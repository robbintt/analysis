---
ver: rpa2
title: Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language
  Models
arxiv_id: '2508.08204'
source_url: https://arxiv.org/abs/2508.08204
tags:
- uncertainty
- measures
- human
- language
- work
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work evaluates inference-time uncertainty quantification measures
  in large language models for alignment with human uncertainty and calibration. Using
  two datasets (2998 Roper survey questions and 38 Pew questions), it compares model
  uncertainty measures against human disagreement patterns.
---

# Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models

## Quick Facts
- arXiv ID: 2508.08204
- Source URL: https://arxiv.org/abs/2508.08204
- Reference count: 14
- Primary result: Entropy-based inference-time uncertainty measures show strong correlation (r ≥ 0.5) with human group uncertainty and demonstrate moderate to strong calibration on MMLU benchmark

## Executive Summary
This work evaluates inference-time uncertainty quantification measures in large language models for alignment with human uncertainty and calibration. Using two datasets (2998 Roper survey questions and 38 Pew questions), it compares model uncertainty measures against human disagreement patterns. The study finds that multiple entropy-based measures—including choice entropy, total entropy, and top-k entropy for various k—show strong correlation (r ≥ 0.5) with human group uncertainty, despite weak alignment with human answer preference. These measures also demonstrate moderate to strong calibration on the MMLU benchmark through both correctness correlation and Jensen-Shannon distance shift analysis. The results identify specific inference-time uncertainty measures that are both human-aligned and well-calibrated, enabling more intuitive uncertainty signals for LLM applications.

## Method Summary
The study evaluates inference-time uncertainty quantification measures in large language models by comparing them against human uncertainty patterns. The authors use two datasets: 2998 Roper survey questions and 38 Pew questions, which contain multiple-choice questions with human answers and uncertainty ratings. They measure correlation between various model uncertainty measures (choice entropy, total entropy, and top-k entropy variants) and human group uncertainty. Calibration is assessed using the MMLU benchmark through correctness correlation and Jensen-Shannon distance shift analysis. The methodology focuses on comparing model uncertainty signals to human uncertainty patterns in survey data, providing a framework for evaluating whether uncertainty measures are human-aligned and well-calibrated.

## Key Results
- Entropy-based uncertainty measures (choice entropy, total entropy, top-k entropy) show strong correlation (r ≥ 0.5) with human group uncertainty
- These measures demonstrate moderate to strong calibration on the MMLU benchmark through correctness correlation
- Model uncertainty measures are poorly aligned with human answer preference despite strong alignment with human uncertainty patterns

## Why This Works (Mechanism)
The entropy-based measures capture the distributional uncertainty in model outputs, which correlates with human disagreement patterns. When humans disagree on answers, the model's probability distribution becomes more uniform, increasing entropy. This mathematical property of entropy makes it naturally aligned with human uncertainty, even when the model's preferred answer doesn't match human consensus. The strong calibration results suggest that these entropy measures not only track human uncertainty but also provide reliable signals about model confidence that correlate with actual correctness.

## Foundational Learning
- Entropy as uncertainty measure: Why needed - provides mathematical framework for quantifying distributional uncertainty; Quick check - calculate entropy for uniform vs peaked distributions
- Jensen-Shannon divergence: Why needed - measures distributional shift for calibration assessment; Quick check - compute JS divergence between two probability distributions
- Correlation analysis: Why needed - quantifies relationship between model and human uncertainty; Quick check - calculate Pearson correlation coefficient between two variables
- Calibration metrics: Why needed - evaluates whether uncertainty signals match actual model performance; Quick check - plot reliability diagrams for model outputs
- Multiple-choice evaluation: Why needed - provides controlled environment for uncertainty measurement; Quick check - verify all questions have discrete answer options
- Human uncertainty modeling: Why needed - establishes ground truth for alignment evaluation; Quick check - analyze variance in human responses across different question types

## Architecture Onboarding
Component map: LLM inference -> probability distribution -> uncertainty measure computation -> human alignment evaluation -> calibration assessment
Critical path: Model output probabilities → Entropy calculation → Correlation with human uncertainty → Calibration validation on MMLU
Design tradeoffs: Using entropy provides interpretable uncertainty signals but may not capture all forms of model uncertainty; multiple-choice format enables controlled evaluation but limits generalizability
Failure signatures: Poor correlation with human uncertainty indicates misalignment; weak calibration suggests uncertainty signals don't reflect actual model performance
First experiments:
1. Calculate choice entropy for a simple binary classification task and verify it increases as model confidence decreases
2. Compare total entropy vs choice entropy on a dataset with known human uncertainty patterns
3. Evaluate calibration of entropy measures on a small subset of MMLU questions before full benchmark analysis

## Open Questions the Paper Calls Out
None

## Limitations
- Pew survey dataset is relatively small (38 questions), potentially limiting generalizability of results
- Exclusive focus on multiple-choice formats constrains applicability to open-ended or complex reasoning tasks
- Does not address temporal variations in human uncertainty or performance across different model architectures

## Confidence
- Human alignment of entropy measures: High confidence (supported by consistent correlations across two independent datasets)
- Calibration on MMLU: Medium to high confidence (validated through multiple metrics, though limited to multiple-choice format)
- Generalizability across domains: Low to medium confidence (requires validation beyond political and factual knowledge questions)

## Next Checks
1. Replicate human alignment experiments across diverse domains including scientific reasoning, creative writing, and multi-step problem solving to test generalizability
2. Conduct temporal validation by measuring how uncertainty measures align with human disagreement patterns across different time periods and cultural contexts
3. Extend calibration evaluation to open-ended generation tasks and measure both traditional metrics (expected calibration error) and task-specific performance impacts when uncertainty is used for model control