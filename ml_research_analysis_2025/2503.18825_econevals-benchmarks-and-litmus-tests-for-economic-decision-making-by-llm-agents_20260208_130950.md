---
ver: rpa2
title: 'EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM
  Agents'
arxiv_id: '2503.18825'
source_url: https://arxiv.org/abs/2503.18825
tags:
- offer
- gemini
- attempt
- workers
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EconEvals, a comprehensive framework for
  evaluating LLM agents on economic decision-making tasks. It presents two novel methodologies:
  benchmarks that test learning from environments in context across procurement, scheduling,
  and pricing tasks; and litmus tests that quantify tradeoff behavior across multiple
  conflicting objectives.'
---

# EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents

## Quick Facts
- arXiv ID: 2503.18825
- Source URL: https://arxiv.org/abs/2503.18825
- Reference count: 40
- One-line primary result: This paper introduces EconEvals, a comprehensive framework for evaluating LLM agents on economic decision-making tasks, showing systematic improvement in capabilities over time.

## Executive Summary
This paper introduces EconEvals, a comprehensive framework for evaluating LLM agents on economic decision-making tasks. It presents two novel methodologies: benchmarks that test learning from environments in context across procurement, scheduling, and pricing tasks; and litmus tests that quantify tradeoff behavior across multiple conflicting objectives. The authors evaluate a broad array of frontier LLMs, finding that more recent models demonstrate improved economic decision-making capabilities over time. They derive economic insights through detailed analysis of LLM behavior beyond aggregate scores, including budget optimization strategies, exploration patterns, and preference reasoning.

## Method Summary
The framework consists of three stylized economic environments (Procurement, Scheduling, Pricing) and three litmus tests (Patience vs. Impatience, Efficiency vs. Equality, Collusiveness vs. Competitiveness). Agents interact with environments via tool-calling over 100 periods, receiving feedback to learn optimal strategies in context. Performance is measured against known optima from underlying economic models, enabling grounded evaluation. Litmus tests output tradeoff preferences while validating that agents can optimize individual objectives (competency) and make consistent choices (reliability). The methodology uses synthetic environments with scalable difficulty levels and requires LLMs to discover strategies through iterative exploration rather than from the initial prompt.

## Key Results
- Systematic improvement in LLM economic decision-making capabilities over time, with newer models showing better performance across all benchmarks
- Varying strengths across different economic tasks, revealing task-specific capabilities and limitations
- Meaningful characterization of LLM preferences through litmus tests, with competency and reliability scores ensuring interpretable tradeoff measurements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn environment interaction enables measurement of in-context learning capabilities that single-shot evaluations cannot capture.
- Mechanism: LLMs act as agents in stylized economic environments (procurement, scheduling, pricing) where optimal strategies must be discovered through iterative exploration and feedback rather than being inferable from the prompt alone. The agent receives feedback on each action and must aggregate this information across periods to improve performance.
- Core assumption: Economic decision-making often requires learning from the environment in context, and this capability is distinct from static knowledge retrieval or single-turn reasoning.
- Evidence anchors:
  - [abstract] "benchmarks that test learning from environments in context across procurement, scheduling, and pricing tasks"
  - [section 3.3.2] "The LLM agent is not given the preferences of the tasks and workers...and must learn information about these preferences indirectly from the blocking-pair feedback."
  - [corpus] Related work (StockBench) evaluates LLM trading agents in multi-turn settings, supporting the multi-turn evaluation paradigm but not specifically addressing in-context learning measurement.
- Break condition: If LLMs can infer optimal strategies from the initial prompt alone without exploration, the benchmark would fail to measure in-context learning.

### Mechanism 2
- Claim: Conflicting objectives reveal latent preference structure in LLM choice behavior when competency and reliability are controlled.
- Mechanism: Litmus tests present stylized decisions with multiple incompatible goals (e.g., efficiency vs. equality). The LLM's position on the tradeoff spectrum is quantified via a litmus score, but only after validating that the LLM can optimize each objective individually (competency) and exhibits consistent choices (reliability).
- Core assumption: LLMs exhibit coherent preferences that generalize across contexts, and these preferences can be meaningfully quantified through stylized tasks.
- Evidence anchors:
  - [abstract] "Each litmus test outputs a litmus score, which quantifies an LLM's tradeoff response, a reliability score, which measures the coherence of an LLM's choice behavior, and a competency score"
  - [section 6.2] "high competency and reliability scores are necessary conditions for an LLM's choice behavior to be meaningfully captured by its corresponding litmus score"
  - [corpus] Corpus evidence is weak for the specific dual-score validation mechanism; "A Framework for Studying AI Agent Behavior" addresses consumer choice but does not propose competency/reliability prerequisites.
- Break condition: If LLMs make choices randomly when faced with conflicting objectives, or if competency is insufficient to execute the task, litmus scores would be uninterpretable.

### Mechanism 3
- Claim: Synthetic environments with known ground-truth optima enable fine-grained, economically grounded performance metrics.
- Mechanism: Unlike end-to-end simulations with undefined optimal performance, these benchmarks are generated from economic models with computable optima (e.g., Gale-Shapley stable matching, nested logit profit maximization). This allows scoring as a percentage of optimal rather than binary success/failure.
- Core assumption: The stylized environments capture transferable economic reasoning capabilities that predict real-world economic decision-making performance.
- Evidence anchors:
  - [section 4] "because each environment is synthetically generated according to an underlying economic model, our benchmarks enjoy two additional properties: scalable difficulty and (economically) grounded evaluation"
  - [section 4] "This in turn allows us to understand, e.g., when a given difficulty level is saturated"
  - [corpus] No direct corpus support; related work (TheAgentCompany, VendingBench) uses end-to-end simulations without ground-truth optima.
- Break condition: If the stylized tasks require substantially different capabilities than real economic decisions, benchmark performance would not predict deployment performance.

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The benchmark environments require agents to act under uncertainty, observing feedback but not the underlying state (e.g., worker/task preferences in scheduling are hidden).
  - Quick check question: Can you explain why a single-period greedy strategy fails in the scheduling benchmark?

- Concept: Stability in Matching Markets (Gale-Shapley)
  - Why needed here: The scheduling benchmark explicitly requires finding stable matchings; understanding blocking pairs is essential to interpret agent feedback and design improvements.
  - Quick check question: What constitutes a blocking pair in a two-sided matching, and why does its absence define stability?

- Concept: Discounted Utility / Intertemporal Choice
  - Why needed here: The Patience vs. Impatience litmus test elicits implied discount rates; interpreting litmus scores requires understanding time preference theory.
  - Quick check question: How would you compute the implied annual interest rate from choices between $100 today and $X at time T?

## Architecture Onboarding

- Component map:
  - Synthetic environments (Procurement -> Scheduling -> Pricing) -> Tool-based agent interaction -> 100-period feedback loop -> Performance scoring
  - Litmus tests (Patience vs. Impatience, Efficiency vs. Equality, Collusiveness vs. Competitiveness) -> Competency/reliability validation -> Litmus score calculation

- Critical path:
  1. Instantiate environment with difficulty-appropriate parameters (e.g., n=10 workers for Basic scheduling vs. n=50 for Hard).
  2. Run agent for 100 periods (benchmarks) or task-specific periods (litmus tests), collecting actions, feedback, and notes.
  3. Compute scores: benchmark scores normalize against known optima; litmus tests compute tradeoff position plus competency/reliability prerequisites.
  4. Validate interpretation: litmus scores are only meaningful if competency ≥0.90 and reliability ≥0.80 (based on observed thresholds in Section 6.2).

- Design tradeoffs:
  - Stylized vs. realistic environments: Stylized enables grounded scoring but may reduce real-world transferability.
  - Tool-based vs. freeform interaction: Tools constrain behavior for reproducibility but may not reflect how LLMs would act with unconstrained interfaces.
  - 100-period horizon vs. shorter: Longer horizons test sustained learning but increase evaluation cost (queries cannot be parallelized due to path-dependency).

- Failure signatures:
  - **Underexploration**: Low exploration rate in stationary environments (e.g., procurement) despite explicit instructions to explore (Section 4.3.4, Appendix F).
  - **Incoherent preferences**: Low reliability scores (<0.80) indicate choices inconsistent with any stable preference structure.
  - **Competency ceiling effects**: Basic difficulty saturated (multiple LLMs achieve 100%) while Hard differentiates, indicating wrong difficulty level for model comparison.

- First 3 experiments:
  1. Run all three benchmarks at Medium difficulty on your target LLM to establish baseline capabilities; compare against the reported scores in Table 4 to validate your implementation.
  2. Run the Efficiency vs. Equality litmus test with both the main prompt and the two competency variants (max efficiency only, max equality only) to verify your scoring pipeline produces values consistent with Table 3.
  3. Ablate the notes tools by disabling write_notes/read_notes and measure performance drop; this quantifies the contribution of cross-period memory to benchmark performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent do cardinal litmus scores (e.g., implied annual interest rates) generalize across contexts?
- Basis in paper: [explicit] Page 36 states, "it is an open question to measure the extent to which these cardinal litmus scores generalize across contexts. We view this as a fruitful direction for follow-up work."
- Why unresolved: The authors currently only claim that litmus scores facilitate generalizable ordinal comparisons, not absolute cardinal predictions.
- What evidence would resolve it: Correlating specific numerical litmus scores with LLM behavior in diverse, unstructured real-world economic scenarios.

### Open Question 2
- Question: How does optimal prompt and scaffolding engineering affect LLM performance on economic decision-making tasks compared to neutral baselines?
- Basis in paper: [explicit] Page 36 notes, "a fruitful direction for further research would be to more optimally engineer these components," referring to prompts and scaffolding.
- Why unresolved: The study deliberately used simple, neutral prompts to ensure fair comparison across models, leaving the potential performance gains from specialized engineering unexplored.
- What evidence would resolve it: Re-evaluating the benchmarks using domain-specific prompts and tool-scaffolding to measure performance deltas.

### Open Question 3
- Question: Do preferences elicited through stylized litmus tests generalize to predict LLM behavior in "wild" or real-world economic decision-making workflows?
- Basis in paper: [explicit] Page 32 notes, "The gold standard for litmus tests is generalizability, that is, an LLM's litmus score in a stylized environment is predictive of some aspect of the LLM's preferences or character 'in the wild'."
- Why unresolved: The authors only provide preliminary evidence of generalizability via correlation with other static benchmarks (e.g., ModelSlant) rather than dynamic real-world environments.
- What evidence would resolve it: A comprehensive study correlating litmus scores with observed LLM agent performance in deployed, unscripted economic applications.

## Limitations

- The framework's reliance on stylized synthetic environments raises questions about real-world transferability, particularly for the Pricing benchmark where the optimal policy requires perfect foresight.
- The 100-period evaluation horizon, while enabling measurement of in-context learning, creates significant evaluation costs and prevents parallelization.
- The competency/reliability thresholds (≥0.90 and ≥0.80) were determined empirically from the studied models but may not generalize to other LLM families or domain-specific models.

## Confidence

- **High confidence**: The benchmark methodology and scoring normalization against known optima are mathematically sound and well-specified. The competency/reliability validation framework for litmus tests is clearly defined.
- **Medium confidence**: The claim that more recent LLMs demonstrate systematically improved economic decision-making capabilities is supported by the presented data but could benefit from additional ablation studies isolating model size vs. architecture improvements.
- **Medium confidence**: The assertion that benchmark performance predicts real-world economic decision-making capability requires further validation beyond the synthetic environments.

## Next Checks

1. Test the framework on domain-specific economic models (e.g., financial forecasting, supply chain optimization) to assess real-world transferability beyond the synthetic environments.
2. Conduct ablation studies comparing tool-based vs. freeform interaction protocols to quantify the impact of scaffolding constraints on economic decision-making performance.
3. Evaluate the competency/reliability thresholds on a diverse set of LLM families (not just frontier models) to validate their generalizability as interpretation filters for litmus test scores.