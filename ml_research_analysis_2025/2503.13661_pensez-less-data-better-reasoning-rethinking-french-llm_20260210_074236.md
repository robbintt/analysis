---
ver: rpa2
title: 'Pensez: Less Data, Better Reasoning -- Rethinking French LLM'
arxiv_id: '2503.13661'
source_url: https://arxiv.org/abs/2503.13661
tags:
- reasoning
- french
- data
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Pensez demonstrates that a small, high-quality, bilingual dataset
  (2,000 samples) can significantly improve both mathematical reasoning and French
  language proficiency in a large language model, achieving up to 20-point gains on
  AIME25 and 12-point gains on a French MATH benchmark. The model outperforms larger
  models trained on vastly more data in knowledge tasks while maintaining competitive
  reasoning performance, showing that strategic data curation and optimized fine-tuning
  can be more effective than scale.
---

# Pensez: Less Data, Better Reasoning -- Rethinking French LLM

## Quick Facts
- arXiv ID: 2503.13661
- Source URL: https://arxiv.org/abs/2503.13661
- Reference count: 40
- Shows 2,000 bilingual samples can achieve +20 AIME25 and +12 French MATH points over base

## Executive Summary
Pensez demonstrates that a small, high-quality, bilingual dataset (2,000 samples) can significantly improve both mathematical reasoning and French language proficiency in a large language model, achieving up to 20-point gains on AIME25 and 12-point gains on a French MATH benchmark. The model outperforms larger models trained on vastly more data in knowledge tasks while maintaining competitive reasoning performance, showing that strategic data curation and optimized fine-tuning can be more effective than scale. This approach challenges the assumption that massive datasets are required for strong reasoning and offers a resource-efficient path to developing high-performing, multilingual models.

## Method Summary
The method fine-tunes Qwen2.5 7B Instruct on a carefully curated bilingual dataset (Pensez-2k) of 2,000 samples, balancing 1,000 English and 1,000 French examples. Data is sourced from multiple collections, filtered for language purity (≥0.95 FastText confidence), deduplicated, and augmented via translation and reasoning chain generation using Llama 3.3 70B. Training uses full-parameter SFT with DeepSpeed ZeRO-3, FlashAttention2, and NEFTune noise (α=5), focusing loss computation on reasoning/solution portions only. The approach targets 60% reasoning and 40% daily tasks, with 5 epochs, batch size 16, and LR 1e-5, achieving results in ~70-76 minutes on 8×H100 GPUs.

## Key Results
- +20 points on AIME25 benchmark compared to base model
- +12 points on French MATH level 5 benchmark
- Outperforms larger models on knowledge tasks (MMLU, BBH, GPQA Diamond, BoolQA) while maintaining strong reasoning

## Why This Works (Mechanism)
The approach works by strategically combining high-quality bilingual data with optimized fine-tuning techniques that focus on reasoning traces. By curating a compact dataset with balanced language coverage and emphasizing reasoning over rote knowledge, the model develops stronger problem-solving capabilities while maintaining language proficiency. The use of NEFTune noise and specialized kernels (MeetKai packing, Liger Kernel) enhances training efficiency and generalization.

## Foundational Learning
- **Bilingual data curation**: Essential for developing multilingual proficiency while maintaining reasoning strength; quick check: verify 1:1 English-French ratio post-augmentation
- **Reasoning trace optimization**: Focusing loss on solution portions improves problem-solving capabilities; quick check: monitor reflection token frequency to detect "overthinking"
- **NEFTune noise injection**: Enhances generalization during fine-tuning; quick check: validate α=5 parameter effectiveness
- **DeepSpeed ZeRO-3 optimization**: Enables efficient training on limited GPU resources; quick check: confirm memory usage and gradient step count (~200)

## Architecture Onboarding

**Component Map**
Qwen2.5 7B Instruct -> Pensez-2k Dataset -> DeepSpeed ZeRO-3 + FlashAttention2 + NEFTune -> SFT Training -> Evaluation (AIME25, French MATH, MMLU, BBH, GPQA Diamond, BoolQA)

**Critical Path**
Data preparation (filtering, translation, augmentation) → Model configuration (special tokens, DeepSpeed setup) → SFT training (5 epochs, loss on reasoning traces) → Evaluation on benchmarks

**Design Tradeoffs**
- Small dataset size (2,000) vs. traditional massive datasets: prioritizes quality over quantity
- Focus on reasoning traces vs. full context: improves problem-solving but may miss contextual nuances
- Bilingual balance (1:1) vs. language-specific scaling: ensures equitable language development

**Failure Signatures**
- "Overthinking": excessive self-reflection loops with repeated "wait," "actually," "let me verify" tokens
- Incorrect answers showing 5-7× higher reflection counts than correct ones
- Bilingual imbalance if French performance lags despite equal dataset representation

**First Experiments**
1. Verify data preparation pipeline produces expected 60% reasoning / 40% daily tasks split
2. Test training configuration with small subset to validate DeepSpeed and kernel optimizations
3. Evaluate baseline model on AIME25 and French MATH to establish comparison points

## Open Questions the Paper Calls Out
None

## Limitations
- Results are specific to French and mathematical reasoning domains, limiting generalization
- Exact prompt templates and sampling strategies are not fully specified, introducing reproducibility uncertainty
- Reliance on external tools (FastText, Llama 3.3 70B) without detailed configuration affects reproducibility

## Confidence

- **High Confidence**: The claim that a small, high-quality, bilingual dataset can improve both mathematical reasoning and French language proficiency is well-supported by the reported benchmark gains (+20 points on AIME25, +12 points on French MATH level 5) and the clear methodology for data curation and fine-tuning.
- **Medium Confidence**: The assertion that this approach outperforms larger models on knowledge tasks is based on reported MMLU, BBH, GPQA Diamond, and BoolQA results, but the exact comparison baselines and conditions are not fully detailed.
- **Low Confidence**: The generalization of these results to other languages, model sizes, or domains is speculative, as the study is narrowly focused on French and mathematical reasoning.

## Next Checks

1. **Replicate Data Preparation**: Implement the full data preparation pipeline, including language filtering with FastText (≥0.95), deduplication, and translation using Llama 3.3 70B, then verify the final 2,000-sample distribution matches the reported 60% reasoning / 40% daily tasks split.

2. **Recreate Training Environment**: Set up the exact training configuration (DeepSpeed ZeRO-3, FlashAttention2, NEFTune noise α=5, bfloat16, MeetKai packing, Liger Kernel) and train the model for 5 epochs with the specified hyperparameters, monitoring for "overthinking" failure modes.

3. **Benchmark Verification**: Evaluate the trained model on AIME25, French MATH level 5, MMLU, BBH, GPQA Diamond, and BoolQA, comparing results to the original paper and checking for the reported +20 and +12 point gains, as well as competitive performance on knowledge tasks.