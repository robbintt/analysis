---
ver: rpa2
title: Improving Reliability and Explainability of Medical Question Answering through
  Atomic Fact Checking in Retrieval-Augmented LLMs
arxiv_id: '2505.24830'
source_url: https://arxiv.org/abs/2505.24830
tags:
- medical
- fact
- atomic
- were
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the problem of hallucinations and low explainability
  in medical question-answering by large language models (LLMs), which limits their
  clinical adoption. The authors introduce an atomic fact-checking framework that
  decomposes LLM-generated responses into discrete, verifiable units called atomic
  facts, each independently verified against an authoritative medical knowledge base.
---

# Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs

## Quick Facts
- arXiv ID: 2505.24830
- Source URL: https://arxiv.org/abs/2505.24830
- Authors: Juraj Vladika; Annika Domres; Mai Nguyen; Rebecca Moser; Jana Nano; Felix Busch; Lisa C. Adams; Keno K. Bressem; Denise Bernhardt; Stephanie E. Combs; Kai J. Borm; Florian Matthes; Jan C. Peeken
- Reference count: 15
- Primary result: Up to 40% overall answer improvement and 50% hallucination detection rate in medical QA through atomic fact checking

## Executive Summary
This study addresses the critical problem of hallucinations and low explainability in medical question-answering systems built on large language models (LLMs). The authors introduce an atomic fact-checking framework that decomposes LLM-generated responses into discrete, verifiable units and independently verifies each against an authoritative medical knowledge base. Evaluation by medical experts across three Q&A datasets showed significant improvements in answer quality and hallucination detection, with the framework also enabling granular traceability to source literature.

## Method Summary
The method builds on retrieval-augmented generation (RAG) to first generate medical Q&A responses, then decomposes these into atomic facts - discrete, self-contained claims that can be independently verified. Each atomic fact is verified against newly retrieved chunks from a medical knowledge base, with incorrect facts rewritten and re-verified through up to three iterative correction loops. The framework uses GPT-4o with four-shot prompts for each step, achieving significant improvements in hallucination detection and answer quality while enabling traceability to source literature.

## Key Results
- Up to 40% overall answer improvement across evaluation sets
- 50% hallucination detection rate compared to baseline approaches
- 75% accuracy in linking atomic facts to relevant guideline passages
- Reduction in false positive rate from 2% to 0% across three correction iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing long-form responses into atomic facts enables targeted error detection that whole-response checking misses.
- Mechanism: An LLM splits each answer into discrete, self-contained claims (e.g., "Trastuzumab is indicated for HER2-positive breast cancer"). Each atomic fact is independently verified against a knowledge base, allowing localized identification of hallucinations and inaccuracies without rejecting entire responses.
- Core assumption: Hallucinations and errors are localized to specific claims rather than distributed across interdependent statements.
- Evidence anchors:
  - [abstract] "This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base."
  - [Results] "Across all evaluation sets, answers were split into a median number of 7 atomic facts (interquartile range: 5–8.75), yielding a total number of 428, 404, 519 facts for the validation (50 Q&A), testing (60 Q&A), and tumorboard (40 Q&A) test sets."
  - [corpus] Related work (UrduFactCheck, FActScore) similarly treats atomic decomposition as foundational for fine-grained verification, though medical-domain applications remain underexplored.
- Break condition: If atomic facts are too interdependent (e.g., qualifications that modify prior claims), decomposition may fragment meaning and produce misleading verdicts.

### Mechanism 2
- Claim: Retrieving fresh evidence for each atomic fact improves verification accuracy compared to reusing initial RAG chunks.
- Mechanism: Rather than validating facts against chunks retrieved for the original question, the framework performs a new retrieval for each atomic fact (7 chunks per fact). This grounds verification in the most relevant passages for each specific claim.
- Core assumption: The optimal evidence for verifying a specific claim differs from the evidence needed to generate the full answer.
- Evidence anchors:
  - [Results] "In the atomic fact correction step, retrieving new chunks for each fact instead of using the initial chunks (used for Q&A) for fact-checking and rewriting increased the overall performance (both precision and sensitivity)."
  - [Results] "A simple Chain-of-Thought (CoT) prompt using GPT4o identified the correct chunk in 75% as the first choice and in 91.9% among the top three chunks."
  - [corpus] Related work on medical knowledge graphs (arxiv:2502.13010) emphasizes retrieval grounding for evolving medical knowledge but does not specifically test per-fact retrieval.
- Break condition: If the knowledge base lacks relevant passages for a fact, verification defaults to LLM judgment without grounding.

### Mechanism 3
- Claim: Iterative correction loops with per-iteration retrieval progressively reduce residual errors without cascading hallucinations.
- Mechanism: Facts labeled "FALSE" are rewritten, then re-verified with fresh retrieval. The loop runs for up to 3 iterations. This progressively catches errors introduced during rewriting and reduces false positive rates.
- Core assumption: Rewriting introduces fewer new errors than it corrects, and errors are progressively eliminated across iterations.
- Evidence anchors:
  - [Results] "Looping through all facts labelled as 'FALSE' further increased the true positive rate, and positive predictive value while reducing the falsification of rewritten facts. The average false positive rate over three evaluation sets reduced from 2% to 1% to 0% throughout 3 iterations of the correction loop."
  - [Results] "Therefore, three iterations were chosen as maximum since that was enough in experiments to correct all unsupported facts."
  - [corpus] Self-Refine (Madaan et al.) uses single-pass feedback; this paper shows smaller models benefit more from decomposed multi-step correction than from unified feedback prompts.
- Break condition: If rewriting consistently introduces new hallucinations faster than it corrects, additional iterations degrade quality.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG):**
  - Why needed here: The framework builds on RAG for initial answer generation and grounding. Without understanding chunk retrieval, embedding similarity, and vector databases, you cannot debug retrieval failures or optimize chunk size.
  - Quick check question: Can you explain why retrieving 7 chunks with cosine similarity might fail to surface a relevant passage for a specific atomic fact?

- **Atomic Fact Decomposition:**
  - Why needed here: The entire framework hinges on splitting responses into verifiable units. You need to understand what makes a fact "atomic" (self-contained, verifiable) versus a compound claim.
  - Quick check question: Given the statement "Trastuzumab is indicated for HER2-positive breast cancer and should be administered with cardiac monitoring," how would you decompose it into atomic facts?

- **Few-Shot Prompting / In-Context Learning:**
  - Why needed here: The pipeline uses 4-shot examples for fact extraction, verdicting, and rewriting. Understanding how prompt design affects consistency and error propagation is critical.
  - Quick check question: Why might increasing few-shot examples from 1 to 4 decrease precision but increase sensitivity in hallucination detection?

## Architecture Onboarding

- **Component map:**
  1. **Knowledge Base:** Oncology guidelines chunked (512 tokens, 100-token overlap), embedded with S-PubMedBERT, stored in ChromaDB.
  2. **Initial RAG Response Generator:** Retrieves 7 chunks for the question, generates answer with 4-shot prompting.
  3. **Atomic Fact Extractor:** LLM splits response into discrete facts (median ~7 per answer).
  4. **Fact Verifier:** For each fact, retrieves 7 fresh chunks, assigns TRUE/FALSE verdict.
  5. **Fact Rewriter:** Rewrites FALSE facts using retrieved evidence; loops up to 3 iterations.
  6. **Response Assembler:** Integrates corrected facts into final answer.

- **Critical path:** Initial RAG generation → Fact decomposition → Per-fact retrieval → Verdicting → Rewriting (looped) → Final assembly. The per-fact retrieval and looping steps dominate latency and cost (~10x tokens vs. initial generation).

- **Design tradeoffs:**
  - **Sensitivity vs. Precision:** 4-shot prompting prioritizes sensitivity (catching more hallucinations) at the cost of more false positives. This is acceptable because false positives can be dismissed during rewriting.
  - **Cost vs. Accuracy:** Fresh retrieval per fact and 3-iteration loops improve accuracy but multiply token usage by ~10x.
  - **Model size:** Smaller models benefit more from this framework than larger models (significant negative correlation between parameter size and improvement magnitude), making it valuable for on-premises deployment.

- **Failure signatures:**
  - **Retrieval gaps:** If the knowledge base lacks relevant passages, the verifier has no grounding and may produce unreliable verdicts.
  - **Error propagation:** Errors can cascade from extraction → verdicting → rewriting. The loop mitigates but does not eliminate this.
  - **Over-correction:** In 0–8% of cases across datasets, answer quality declined due to less relevant chunks retrieved during correction or new hallucinations introduced during rewriting.

- **First 3 experiments:**
  1. **Validate retrieval quality on your domain:** Before deploying, test whether your knowledge base surfaces relevant chunks for sample atomic facts. Use the 75% first-choice accuracy as a benchmark; if significantly lower, investigate chunking strategy or embedding model.
  2. **Calibrate iteration count:** Run ablation tests with 1, 2, and 3 correction loops on a validation set. Measure false positive rate reduction against latency cost to determine optimal stopping point for your use case.
  3. **Compare general vs. domain-specific models:** If considering open-source deployment, test both general-purpose and medical fine-tuned variants. The paper shows mixed results (MedGemma 27B outperforms GPT-4o on fact-checking but not overall rubric scores), so validate on your specific domain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Graph-RAG techniques reduce error propagation and improve robustness compared to the current chunk-based approach?
- Basis in paper: [explicit] The authors state in the Discussion, "Future work could explore approaches to make the process more rigorous, such as Graph-RAG techniques of grounding data into graph structures for more robust generation."
- Why unresolved: The current pipeline relies on sequential LLM steps and text chunking, which risks error accumulation and lacks the structural context graphs provide.
- What evidence would resolve it: A comparative study measuring hallucination rates and factual consistency between the current chunk-based framework and a Graph-RAG implementation.

### Open Question 2
- Question: Can the framework's high token cost and latency be reduced while maintaining high hallucination detection sensitivity?
- Basis in paper: [inferred] The authors note that the "fact-checking and rewriting process uses around 10 times more tokens than just the initial response generation, which increases the costs and the latency."
- Why unresolved: The multi-step nature of atomic fact verification creates a computational bottleneck that may hinder deployment in resource-constrained or real-time clinical settings.
- What evidence would resolve it: An optimized architectural variant that achieves comparable answer improvement and detection rates with significantly lower token consumption.

### Open Question 3
- Question: Does medical fine-tuning consistently improve fact-checking performance compared to general-purpose model counterparts?
- Basis in paper: [inferred] The results showed mixed outcomes, leading the authors to conclude "Medical fine-tuning mostly rather worsened the performance, with MedGemma 27B marking an exception."
- Why unresolved: The benefits of domain specialization for this specific atomic verification task remain ambiguous, as seen in Llama 3 outperforming OpenBioLLM.
- What evidence would resolve it: A comprehensive benchmark across a wider variety of medical and general-purpose models to isolate the specific effect of medical fine-tuning on fact-checking accuracy.

## Limitations
- The framework's computational overhead is significant, using approximately 10x more tokens than baseline RAG approaches.
- Effectiveness has only been demonstrated in oncology-specific contexts; generalization to other medical specialties remains unproven.
- 75% top-1 accuracy for fact-to-chunk tracing still means one in four atomic facts cannot be reliably traced to source literature.

## Confidence
- **High confidence**: The core mechanism of atomic fact decomposition and independent verification is well-supported by quantitative results showing 50% hallucination detection and 40% answer improvement.
- **Medium confidence**: The per-fact retrieval advantage is supported but relies on comparison with a single baseline; broader validation across knowledge domains would strengthen this claim.
- **Medium confidence**: The iterative correction loop benefits are demonstrated but the optimal iteration count (3) may be domain-specific rather than universal.

## Next Checks
1. Conduct latency and cost benchmarking comparing the 5-step framework against baseline RAG approaches to quantify the computational overhead and determine practical deployment thresholds.
2. Test the framework on medical Q&A datasets from different specialties (e.g., cardiology, neurology) to validate cross-domain generalization beyond oncology.
3. Implement and measure a sliding-window approach for atomic fact decomposition to assess whether fragmented facts that depend on context can be preserved without sacrificing verification accuracy.