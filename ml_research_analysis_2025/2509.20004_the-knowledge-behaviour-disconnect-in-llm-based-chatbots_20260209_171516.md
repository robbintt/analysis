---
ver: rpa2
title: The Knowledge-Behaviour Disconnect in LLM-based Chatbots
arxiv_id: '2509.20004'
source_url: https://arxiv.org/abs/2509.20004
tags:
- knowledge
- behaviour
- will
- about
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that large language model-based chatbots exhibit
  a fundamental disconnect between the declarative knowledge they express and the
  linguistic behaviors they perform. The author claims that despite LLMs being able
  to accurately report knowledge about various domains (including rules of chess or
  ethical principles), their actual conversational behavior is not guided by this
  knowledge.
---

# The Knowledge-Behaviour Disconnect in LLM-based Chatbots

## Quick Facts
- arXiv ID: 2509.20004
- Source URL: https://arxiv.org/abs/2509.20004
- Authors: Jan Broersen
- Reference count: 34
- One-line primary result: LLMs exhibit a fundamental disconnect where declarative knowledge does not causally guide linguistic behavior.

## Executive Summary
This paper identifies a fundamental limitation in LLM-based chatbots: while these systems can accurately express declarative knowledge about various domains (chess rules, ethical principles), their actual conversational behavior is not guided by this knowledge. The disconnect arises from the token prediction mechanism that learns patterns without establishing causal connections between content and behavior. The author argues this is not a training issue but an inherent architectural limitation that cannot be resolved through additional training, reinforcement learning, or prompt engineering.

The paper provides empirical support through examples of LLM hallucinations, performative contradictions, and the inability of systems to follow rules they explicitly describe. Despite the LLM's ability to accurately report knowledge about chess rules or ethical principles, it cannot reliably apply this knowledge in behavior. This represents a fundamental challenge for deploying LLMs in knowledge-guided applications where behavior must align with stated principles.

## Method Summary
The paper presents a theoretical argument supported by empirical observations of LLM behavior. The primary methodology involves analyzing LLM outputs across multiple domains to identify patterns where declared knowledge does not align with actual behavior. Specific tests include chess game playing to detect illegal moves despite correct rule knowledge, and behavioral tests where LLMs confirm principles but fail to act accordingly. The analysis focuses on identifying performative contradictions and hallucinations as evidence of the knowledge-behavior disconnect.

## Key Results
- LLMs can accurately report knowledge about chess rules but frequently make illegal moves during gameplay
- Systems confirm principles like "it is prudent to stop responding when users request it" but cannot comply with such requests
- Hallucinations occur where LLM outputs confidently state incorrect information without any knowledge mechanism to constrain divergence
- Current alignment techniques (RLHF, prompt engineering) address behavioral symptoms but cannot eliminate the underlying disconnect

## Why This Works (Mechanism)

### Mechanism 1: Token Prediction Pattern Learning
- Claim: LLMs learn behavioral patterns through token prediction without establishing causal connections between declarative knowledge content and linguistic behavior.
- Mechanism: Self-supervised training on text continuations produces statistical correlations between token sequences but no mechanism for knowledge-to-behavior causation.
- Core assumption: Token prediction architecture cannot spontaneously generate causal links between knowledge representations and behavioral outputs.
- Evidence anchors:
  - [abstract] "This disconnect arises from the token prediction mechanism underlying LLMs, which learns patterns in text without establishing causal connections between content and behavior."
  - [section 5] "We may say that declarative knowledge and behavior correlate in LLMs, but without LLMs having a mechanism that is responsible for the first being a cause for the second."
- Break condition: If an architecture were introduced that explicitly represents knowledge-behavior dependencies rather than pure pattern continuation.

### Mechanism 2: Hierarchical Pattern Accumulation
- Claim: Behavioral patterns are learned in a hierarchy where higher-level patterns depend on lower-level patterns but are progressively more disconnected from declarative knowledge.
- Mechanism: Patterns build from local (spelling→grammar→conceptual sense→truth-telling→logic→rationality→social→ethical), with each level more abstract and less connected to knowledge content.
- Core assumption: Higher-level behavioral patterns are harder to learn and thus more likely to diverge from the declarative knowledge about those behaviors.
- Evidence anchors:
  - [section 3] "The list is not randomly ordered: it goes from behaviours that should be associated with more local patterns in token sequences to ones that should be associated to very global patterns."
  - [section 5, Claim 3] "The higher the level of behaviour, the more the disconnect comes to the fore."
- Break condition: If explicit knowledge constraints were injected at each level during training to enforce consistency.

### Mechanism 3: Alignment Technique Misalignment
- Claim: Current alignment techniques (RLHF, prompt engineering, data curation) address behavioral symptoms but can worsen the underlying disconnect by creating multiple unconnected knowledge sources.
- Mechanism: RLHF trains behavior through reward signals from human feedback; prompt engineering injects behavioral instructions. Neither connects the behavior to the LLM's declarative knowledge—they add parallel, disconnected behavioral patterns.
- Core assumption: Behavioral correction through external signals doesn't create understanding of why behaviors are preferred.
- Evidence anchors:
  - [abstract] "This paper argues this is a fundamental limitation that cannot be eliminated through additional training or alignment techniques like reinforcement learning or prompt engineering."
  - [section 6] "The ethical behaviour learned by the system through the RLHF technique is not based on the ethical knowledge it may report on in conversations; the two simply come from different sources."
- Break condition: If alignment techniques incorporated explicit knowledge-to-behavior mapping rather than purely behavioral shaping.

## Foundational Learning

- Concept: Knowledge-based systems vs. pattern-based systems
  - Why needed here: To understand the fundamental architectural difference the paper is highlighting—classical knowledge-based systems have explicit mechanisms where behavior follows from knowledge representations; LLMs have pattern correlations without causal mechanisms.
  - Quick check question: In a classical knowledge-based system, if the knowledge base contained incorrect chess rules, what would happen to the system's chess-playing behavior?

- Concept: Performative contradictions
  - Why needed here: This is the key diagnostic signal of the disconnect—the paper uses chess illegal moves and apologies-without-behavior-change as empirical evidence. Recognizing these patterns helps identify the disconnect in deployed systems.
  - Quick check question: If an LLM correctly states "it is prudent to stop responding when users request it" but cannot stop responding, what type of failure does this represent?

- Concept: Correlation vs. causation in learning systems
  - Why needed here: The paper's core argument rests on distinguishing between systems where knowledge and behavior merely correlate (both learned from same training data) vs. systems where knowledge causally determines behavior. Understanding this distinction is essential for evaluating alignment approaches.
  - Quick check question: Why can't additional training data convert a correlation into a causal connection, according to the paper's logic?

## Architecture Onboarding

- Component map:
  Core LLM foundation -> Fine-tuning layer -> Alignment layer (RLHF) -> Prompt engineering interface

- Critical path: Training data collection → Unsupervised token prediction → Pattern hierarchy formation → Fine-tuning for functionality → Alignment training → Deployment with prompt engineering (disconnect persists at every stage—none establishes knowledge-behavior causation)

- Design tradeoffs:
  - More training data: Improves correlation between knowledge and behavior but cannot create causation
  - Stronger RLHF alignment: Reduces harmful outputs but creates separate knowledge source disconnected from training-data knowledge
  - Prompt engineering: Provides behavioral control without understanding, adding third disconnected knowledge source
  - Data curation: Can hide disconnect by training on consistent texts, but doesn't establish mechanism

- Failure signatures:
  - Performative contradictions: System correctly states rules/knowledge but behavior violates them (e.g., illegal chess moves)
  - Hallucinations that compound: Once content diverges from training distribution, no knowledge mechanism constrains further divergence
  - Apologies without behavioral change: System responds to error reports with correct apology patterns but repeats identical errors
  - Ethical knowledge-behavior gap: System accurately describes ethical principles but cannot reliably behave ethically

- First 3 experiments:
  1. Chess rule-following test: Query the LLM for complete chess rules (verifying declarative knowledge), then play multiple games and count illegal moves. The disconnect hypothesis predicts rule violations will occur despite perfect rule knowledge. Vary game length to test whether violations increase as game states diverge from training distribution.
  2. Prudence contradiction test: Have the LLM explicitly confirm the principle "it is prudent for LLMs to comply with reasonable user requests to stop responding," then request it stop responding. The disconnect predicts inability to comply despite confirming the principle. This tests whether self-reported knowledge influences self-controlled behavior.
  3. Multi-source alignment gap test: Present ethical scenarios where the LLM provides correct ethical analysis, then engineer subtly different but ethically equivalent scenarios. The disconnect predicts behavioral inconsistency despite equivalent ethical knowledge requirements. This tests whether RLHF alignment creates reliable knowledge-guided behavior or merely pattern-matched responses to specific prompt structures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an architectural mechanism be designed that establishes a causal, two-way link between an LLM's declarative knowledge and its linguistic behavior?
- Basis in paper: [explicit] The conclusion asks how such a mechanism could look, stating the author does not know if it is possible to construct.
- Why unresolved: Current LLMs rely on token prediction which learns patterns without causal links to content.
- What evidence would resolve it: A model that autonomously aligns its actions with its stated knowledge without external supervision.

### Open Question 2
- Question: Do "maximally contradicted" training datasets (where text content contradicts text behavior) result in LLMs that behaviorally follow the patterns rather than the rules?
- Basis in paper: [inferred] The author proposes this as a thought experiment in Section 5 to prove the disconnect but does not conduct the experiment.
- Why unresolved: It is currently a hypothetical argument used to demonstrate the absence of a connection mechanism.
- What evidence would resolve it: Empirical training of an LLM on such data showing it fails to detect internal conflicts.

### Open Question 3
- Question: Does the behavior of LLMs regarding semantic paradoxes demonstrate that meaning is more than use?
- Basis in paper: [explicit] Section 8 outlines a future article investigating this specific claim using semantic paradoxes.
- Why unresolved: The author posits this link but notes the full investigation is reserved for future work.
- What evidence would resolve it: Analysis showing LLMs fail on semantic paradoxes in ways that distinguish between use and content.

## Limitations

- The paper lacks formal architectural proofs showing why token prediction cannot, even in principle, create causal links between knowledge and behavior
- Empirical demonstrations focus on specific domains (chess, ethical principles) without systematic testing across the full hierarchy of behavioral patterns
- Limited quantitative analysis of the degree and distribution of knowledge-behavior disconnect across different LLM versions and architectures

## Confidence

**High Confidence**: The empirical demonstrations of knowledge-behavior disconnect (chess illegal moves, inability to stop responding despite confirming it's prudent) are well-documented and reproducible. The descriptive analysis of how LLMs learn patterns without causal mechanisms is sound.

**Medium Confidence**: The theoretical argument that token prediction architectures cannot establish knowledge-behavior causation. The hierarchical nature of pattern learning and its implications for disconnect severity. The claim that alignment techniques worsen rather than improve the fundamental disconnect.

**Low Confidence**: That this represents an absolute architectural impossibility rather than a current limitation. The specific quantitative relationship between pattern level and disconnect severity. Whether alternative architectures could bridge the disconnect.

## Next Checks

1. **Cross-Architecture Disconnect Test**: Implement a hybrid architecture where LLM outputs are filtered through a symbolic knowledge base that enforces consistency with explicitly stated rules. The disconnect hypothesis predicts this will still fail to eliminate illegal moves and performative contradictions, while architectural impossibility claims predict no improvement.

2. **Temporal Disconnect Decay Analysis**: Track the same LLM across multiple versions/releases over time, measuring illegal move rates and performative contradiction frequency. The hierarchical disconnect claim predicts these should decrease more slowly than basic functionality improvements, with higher-level disconnects persisting longest.

3. **Alignment Technique Causal Tracing**: For RLHF-aligned models, trace specific behavioral outputs back to their knowledge sources—comparing whether ethical behavior traces to training data knowledge versus reward model influence. The multiple-knowledge-sources claim predicts these will be systematically disjoint, with no causal path from declarative knowledge to RLHF-shaped behavior.