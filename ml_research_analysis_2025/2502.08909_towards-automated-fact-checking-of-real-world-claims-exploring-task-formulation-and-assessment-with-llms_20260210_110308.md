---
ver: rpa2
title: 'Towards Automated Fact-Checking of Real-World Claims: Exploring Task Formulation
  and Assessment with LLMs'
arxiv_id: '2502.08909'
source_url: https://arxiv.org/abs/2502.08909
tags:
- evidence
- fact-checking
- performance
- claim
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes baselines for automated fact-checking (AFC)
  using large language models (LLMs) on real-world claims from PolitiFact. The authors
  evaluate Llama-3 models of different sizes (3B, 8B, 70B) across multiple labeling
  schemes (binary, three-class, five-class) in a few-shot setup without fine-tuning.
---

# Towards Automated Fact-Checking of Real-World Claims: Exploring Task Formulation and Assessment with LLMs

## Quick Facts
- arXiv ID: 2502.08909
- Source URL: https://arxiv.org/abs/2502.08909
- Reference count: 40
- LLMs outperform smaller models in classification and justification quality when using retrieval-augmented fact-checking

## Executive Summary
This paper establishes baselines for automated fact-checking (AFC) using large language models (LLMs) on real-world claims from PolitiFact. The authors evaluate Llama-3 models of different sizes (3B, 8B, 70B) across multiple labeling schemes (binary, three-class, five-class) in a few-shot setup without fine-tuning. Claims are analyzed, classified, and explained using retrieval-augmented generation with web evidence. Results show that larger LLMs consistently outperform smaller ones in classification accuracy and justification quality. Evidence integration improves performance across all models, with larger models benefiting most. Classification performance decreases as label complexity increases, but justification quality remains stable across schemes.

## Method Summary
The study evaluates automated fact-checking using Llama-3 models (3B, 8B, 70B) on 17,856 PolitiFact claims from 2007-2024. The AFC task involves three sub-tasks: reasoning (chain-of-thought), verdict classification (2/3/5-class), and explanation generation. The approach uses retrieval-augmented generation with web evidence from Serper API, excluding fact-checking domains. Models operate in one-shot inference without fine-tuning, generating structured JSON outputs. Performance is measured using classification metrics (Accuracy, F1 variants) and TIGERScore for justification quality, with majority voting across three inference passes per claim.

## Key Results
- Larger LLMs (70B) outperform smaller models (3B, 8B) in both classification accuracy and justification quality
- Evidence integration improves performance across all model sizes, with largest gains for 70B model
- Classification accuracy decreases as label complexity increases (binary > three-class > five-class)
- Justification quality remains stable across labeling schemes despite varying classification performance

## Why This Works (Mechanism)
The approach leverages retrieval-augmented generation to ground LLM reasoning in external evidence, addressing the hallucination problem common in fact-checking. Larger models benefit more from evidence integration due to their superior reasoning capabilities and ability to process complex contexts. The structured output format ensures consistent evaluation across models and labeling schemes.

## Foundational Learning
- **PolitiFact labeling scheme**: Understanding the 5-class system (True, Mostly True, Half True, Mostly False, False) is crucial for proper label aggregation
  * Why needed: Paper merges labels (e.g., "Pants on Fire" → "False") and creates binary/3-class variants
  * Quick check: Verify Table 2 label mapping logic before evaluation
- **TIGERScore**: Reference-free metric for assessing explanation quality in fact-checking
  * Why needed: Evaluates justification quality without ground truth explanations
  * Quick check: Ensure correct implementation and threshold settings
- **Serper API web search**: Retrieval mechanism for evidence gathering
  * Why needed: Provides evidence snippets for claims; requires domain filtering
  * Quick check: Validate exclusion of fact-checking domains in results
- **Chain-of-thought reasoning**: Multi-step reasoning process for claim analysis
  * Why needed: Enables structured approach to complex verification tasks
  * Quick check: Monitor reasoning quality and completeness in outputs
- **Majority voting**: Three-pass inference for robust classification
  * Why needed: Reduces variance and improves reliability
  * Quick check: Compare single-pass vs. majority voting performance
- **Token limits**: Context window constraints across model sizes
  * Why needed: Large context (Speaker + Background + Context + Evidence) may cause truncation
  * Quick check: Monitor input token lengths for each model size

## Architecture Onboarding
**Component Map**: Web Search -> Evidence Retrieval -> Prompt Construction -> LLM Inference -> JSON Output -> Evaluation

**Critical Path**: Claim → Serper API search → Evidence snippets → One-shot prompt → LLM inference → Structured output → Classification/Justification evaluation

**Design Tradeoffs**: 
- Larger models provide better performance but higher computational cost
- Web search evidence improves accuracy but introduces latency and API dependencies
- One-shot learning avoids fine-tuning but may limit complex reasoning
- Structured JSON outputs ensure consistency but constrain model flexibility

**Failure Signatures**: 
- Classification errors often occur on nuanced claims requiring contextual understanding
- Justification quality degradation when evidence is insufficient or contradictory
- Token truncation affecting reasoning quality in smaller models
- API rate limits causing incomplete evidence retrieval

**First Experiments**:
1. Test label mapping logic by running sample claims through 2-class and 3-class conversion
2. Validate web search functionality with controlled test claims and domain filtering
3. Compare single-pass vs. majority voting outputs on a small claim subset

## Open Questions the Paper Calls Out
### Open Question 1
How should AFC systems handle "not enough evidence" (NEI) scenarios, and does incorporating NEI labels improve alignment between model verdicts and real-world evidential constraints? The study only evaluated truthfulness-based labels from PolitiFact; NEI labeling requires different annotation methodology and may affect how models reason about claim verifiability. Comparative experiments on datasets with NEI annotations (e.g., FEVER, AVeriTeC) showing whether models can reliably distinguish unsupported claims from verifiable ones would resolve this.

### Open Question 2
How can LLM-generated justifications be structured to align with human fact-checkers' verification workflows and reasoning patterns? Current outputs lack systematic evaluation against professional fact-checking standards, and the relationship between justification structure and expert utility remains unexplored. User studies with professional fact-checkers comparing different justification formats on task efficiency, trust calibration, and error detection rates would provide evidence.

### Open Question 3
What is the extent and impact of hallucinations in LLM-based fact-checking justifications, and can they be systematically detected and mitigated? The study relied on TIGERScore for justification quality, which is reference-free and may not reliably detect factual inconsistencies or fabricated evidence citations. Manual annotation of hallucination types in generated outputs, or development of faithfulness metrics that compare claims against retrieved evidence, would resolve this question.

## Limitations
- Exact one-shot example prompt text is not specified, affecting reproducibility of prompt engineering
- Complete blocklist of excluded fact-checking domains is only partially described
- TIGERScore may not reliably detect factual inconsistencies or fabricated evidence citations
- Web search API dependencies introduce potential reproducibility challenges

## Confidence
**High**: Classification accuracy improvements with larger models, evidence integration benefits
**Medium**: TIGERScore justification quality findings, relative performance across labeling schemes
**Low**: Exact prompt engineering effectiveness, web search API reproducibility

## Next Checks
1. Verify label mapping logic against ground truth to ensure correct aggregation from PolitiFact's 5-class system to binary/3-class schemes
2. Test input token length constraints with full context (Speaker + Background + Context + Evidence) across different model sizes to prevent truncation
3. Validate web search results by implementing the same domain blocklist and comparing retrieved evidence quality with reported exclusions