---
ver: rpa2
title: Object Isolated Attention for Consistent Story Visualization
arxiv_id: '2503.23353'
source_url: https://arxiv.org/abs/2503.23353
tags:
- attention
- character
- arxiv
- isolated
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of maintaining character consistency
  in open-ended story visualization, where generating coherent image sequences from
  narratives requires preserving character identity across multiple scenes. The authors
  propose an enhanced Transformer module with isolated self-attention and cross-attention
  mechanisms that leverage pre-trained diffusion models to improve consistency.
---

# Object Isolated Attention for Consistent Story Visualization

## Quick Facts
- arXiv ID: 2503.23353
- Source URL: https://arxiv.org/abs/2503.23353
- Authors: Xiangyang Luo; Junhao Cheng; Yifan Xie; Xin Zhang; Tao Feng; Zhou Liu; Fei Ma; Fei Yu
- Reference count: 40
- Primary result: 28.91 TIS, 6.52 AQ, 70.28% IIS, 49.63% DS

## Executive Summary
This paper addresses the challenge of maintaining character consistency in open-ended story visualization, where generating coherent image sequences from narratives requires preserving character identity across multiple scenes. The authors propose an enhanced Transformer module with isolated self-attention and cross-attention mechanisms that leverage pre-trained diffusion models to improve consistency. The isolated self-attention refines attention maps to reduce irrelevant focus and highlight key features of the same character, while isolated cross-attention independently processes each character's features using regional prompts and masks derived from the diffusion model's layout. Their training-free approach enables continuous generation of new characters and storylines without re-tuning.

## Method Summary
The method extends SDXL's transformer blocks in the upsampling stages with an additional branch that implements isolated attention mechanisms. First, cross-attention maps from the original branch are processed through Otsu thresholding to generate binary masks that localize characters. For returning characters, stored reference tokens are retrieved. The isolated self-attention applies these masks to constrain each character's tokens to attend only to their own reference features, with re-weighting to amplify attention on relevant tokens. The isolated cross-attention processes each character's description independently using the masks to blend regional features. Finally, the original and extended branches are merged with a consistency factor λ=1.1 to emphasize character consistency while preserving base model quality.

## Key Results
- Achieves 28.91 TIS (Text-Image Similarity), outperforming baselines by 1.56 points
- Reaches 70.28% IIS (Image Identity Similarity), improving over StoryDiffusion's 64.87%
- Scores 6.52 AQ (Aesthetic Quality), maintaining competitive visual appeal
- Obtains 49.63% DS (DreamSim Similarity), demonstrating superior prompt alignment

## Why This Works (Mechanism)

### Mechanism 1: Isolated Self-Attention
- **Claim:** Prevents feature confusion between different characters by constraining attention to same-identity reference tokens.
- **Mechanism:** Binary masks derived from cross-attention maps restrict each image region to attend only to its corresponding character's reference tokens from prior scenes. Re-weighting amplifies attention on reference tokens using normalized cross-attention activation levels.
- **Core assumption:** Feature confusion arises because tokens from different characters mutually attend to each other during concatenated self-attention.
- **Evidence anchors:**
  - [abstract]: "isolated self attention mechanism improves character consistency by refining attention maps to reduce focus on irrelevant areas and highlight key features of the same character"
  - [section III-C]: "we refine the vanilla attention mechanism with two components: an attention mask that ensures each subject attends only to itself, thus preventing feature confusion"
  - [corpus]: Storybooth (FMR 0.52) addresses cross-frame self-attention for consistency but uses different isolation strategy

### Mechanism 2: Isolated Cross-Attention
- **Claim:** Prevents attribute mixing by processing each character's textual description through separate cross-attention passes.
- **Mechanism:** Character-specific prompts (e.g., "girl: blonde hair, black dress") are processed independently, then blended using masks M via Equation 6. Global scene features combine with character-specific features per-region.
- **Core assumption:** Pre-trained diffusion models encode prior knowledge about reasonable spatial layouts that can be extracted from cross-attention maps.
- **Evidence anchors:**
  - [abstract]: "isolated cross attention mechanism independently processes each character's features, avoiding feature fusion"
  - [section III-D]: "we utilize the binary masks M derived in Sec. III-C, following the inherent layout composition of the diffusion model"
  - [corpus]: CharCom addresses identity control through LoRA adapters rather than attention isolation

### Mechanism 3: Mask Generation via Cross-Attention Localization
- **Claim:** Character spatial positions can be approximated from cross-attention maps before final image synthesis.
- **Mechanism:** Cross-attention maps Cm are segmented using Otsu thresholding. Maps are averaged across denoising steps to reduce noise. Lower coefficient-of-variation maps are prioritized when overlaps occur.
- **Core assumption:** Cross-attention activation correlates with subject spatial extent in generated images.
- **Evidence anchors:**
  - [section III-B]: "the cross-attention in the original branch provides an approximate location of the subject"
  - [section III-B]: "attention maps with higher noise tend to segment irrelevant regions"
  - [corpus]: Limited direct corpus evidence for coefficient-of-variation prioritization in related works

## Foundational Learning

- **Concept: Self-Attention in Diffusion Transformers**
  - Why needed here: The method fundamentally modifies self-attention to prevent cross-character feature leakage.
  - Quick check question: Given Q, K, V projections and attention map A = Q × K, how does masking A[:, s:s+nm] = Mm change what each token attends to?

- **Concept: Cross-Attention for Text Conditioning**
  - Why needed here: Cross-attention maps provide both spatial localization and text-image alignment signals.
  - Quick check question: How does cross-attention differ from self-attention in terms of where K and V originate?

- **Concept: Otsu's Thresholding**
  - Why needed here: Converts continuous cross-attention maps to binary masks without learned parameters.
  - Quick check question: What statistical property does Otsu's method maximize when selecting a threshold?

## Architecture Onboarding

- **Component map:**
  - LLM Agent → Scene prompts (S) + ID prompts (C) per character
  - Original Branch → Standard transformer block (provides cross-attention maps Cm)
  - Extended Branch → Isolated self-attention + isolated cross-attention
  - Mask Generator → Otsu(Cm) → Mm
  - Branch Merge → F = FISO × λ + FORI × (1-λ), λ=1.1
  - Base model → SDXL backbone

- **Critical path:**
  1. Original branch forward pass generates cross-attention maps
  2. Otsu thresholding produces character masks Mm
  3. For returning characters: retrieve stored reference tokens Fm
  4. Isolated self-attention applies mask constraint + re-weighting
  5. Isolated cross-attention blends per-character features
  6. Merge branches with λ > 1 to emphasize consistency

- **Design tradeoffs:**
  - λ=1.1 biases toward consistency; higher values risk artifacts (classifier-free guidance analogy)
  - Training-free design enables zero-shot generalization but limits optimization
  - Mask quality directly bounds both isolation mechanisms' effectiveness

- **Failure signatures:**
  - Overlapping character masks → feature bleeding visible in output
  - High coefficient-of-variation attention maps → spurious mask regions
  - λ >> 1.1 → over-saturated consistency, unnatural appearance
  - IIS dropping below 65% indicates isolation failure

- **First 3 experiments:**
  1. Single-character 4-scene generation: Verify IIS > 68% (baseline 64.87%) with all components enabled
  2. Two-character attribute test: Use prompts with contrasting attributes (blonde vs black hair); visually confirm no mixing per Fig. 3
  3. Ablation run: Disable re-weighting (set Re=×); expect IIS drop from 70.28% to ~69% per Table II

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the isolated attention mechanism be effectively adapted for consistent video generation or 3D video generation?
- Basis in paper: [explicit] The conclusion states this approach "could be considered for applications in consistent video generation and 3D video generation."
- Why unresolved: The current architecture is optimized for static image sequences; extending it to video requires addressing temporal coherence and motion dynamics across frames, which the current module does not handle.
- Evidence: Successful integration of the isolated attention module into video diffusion frameworks (e.g., SVD) maintaining temporal consistency metrics.

### Open Question 2
- Question: How does the method perform in scenes with extreme character overlap where cross-attention maps merge significantly?
- Basis in paper: [inferred] The mask generation relies on cross-attention maps and Otsu thresholding, and the paper notes that when maps overlap, they prioritize based on coefficients of variation, suggesting a heuristic for handling conflicts.
- Why unresolved: The isolation mechanism depends on distinct spatial masks; if two characters physically interact (e.g., hugging), the cross-attention maps may become indistinguishable, potentially causing mask leakage or feature confusion.
- Evidence: Qualitative and quantitative results specifically on datasets featuring high levels of physical interaction or occlusion between subjects.

### Open Question 3
- Question: Is the high performance attributable to the attention mechanism or the LLM-based prompt refinement?
- Basis in paper: [inferred] The experimental setup involves using ChatGPT to rewrite prompts to "clarify the descriptions and reduce ambiguity" before processing.
- Why unresolved: It is unclear if the consistency gains stem from the isolated attention or the high-quality, disambiguated prompts provided by the LLM, which removes pronouns and clarifies subjects.
- Evidence: An ablation study comparing results using raw story prompts versus the refined prompts to isolate the contribution of the text preprocessing.

## Limitations
- The method's effectiveness depends on accurate cross-attention map localization, which may fail in complex scenes with character overlap or ambiguous spatial relationships
- The training-free design limits optimization opportunities compared to fine-tuned approaches that could learn dataset-specific consistency patterns
- The coefficient-of-variation prioritization for overlapping masks represents an innovation with limited corpus precedent, making its robustness across different narrative styles uncertain

## Confidence

- **High Confidence**: The core mechanism of isolated self-attention and cross-attention for preventing feature confusion between characters is well-supported by the quantitative metrics (IIS improvement from 64.87% to 70.28%, TIS improvement from 27.35 to 28.91).
- **Medium Confidence**: The training-free design enabling continuous generation without re-tuning is validated through comparisons to StoryDiffusion, ChatGPT4o, and Mini-DALLE3, but the claim assumes the test dataset represents realistic story visualization challenges.
- **Medium Confidence**: The mask generation via cross-attention localization is theoretically sound, but the effectiveness of Otsu thresholding on cross-attention maps and coefficient-of-variation prioritization lacks extensive empirical validation across diverse scenarios.

## Next Checks

1. **Mask Quality Analysis**: Generate and visualize Otsu masks on cross-attention maps across diverse test cases, measuring IoU with ground-truth character segmentations to quantify localization accuracy and identify failure patterns.

2. **Ablation on Re-weighting**: Disable the re-weighting component (set Re=×) and measure IIS degradation to confirm the contribution of attention amplification to consistency gains (expected drop from 70.28% to ~69%).

3. **Coefficient-of-Variation Robustness**: Create test cases with overlapping characters having similar cross-attention activation patterns, then verify the CV prioritization correctly selects the cleaner mask for each character.