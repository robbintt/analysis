---
ver: rpa2
title: 'FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era
  of Large Language Models'
arxiv_id: '2506.21563'
source_url: https://arxiv.org/abs/2506.21563
tags:
- languages
- formosan
- language
- low-resource
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FORMOSANBENCH, the first multi-task benchmark
  for evaluating large language models on three endangered Formosan languages (Atayal,
  Amis, and Paiwan) across machine translation, automatic speech recognition, and
  text summarization. The benchmark reveals a substantial performance gap between
  high-resource and Formosan languages, with LLMs consistently underperforming across
  all tasks.
---

# FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models

## Quick Facts
- arXiv ID: 2506.21563
- Source URL: https://arxiv.org/abs/2506.21563
- Reference count: 12
- This paper introduces the first multi-task benchmark for three endangered Formosan languages (Atayal, Amis, Paiwan), revealing substantial performance gaps across MT, ASR, and summarization tasks.

## Executive Summary
This paper introduces FORMOSANBENCH, the first multi-task benchmark for evaluating large language models on three endangered Formosan languages: Atayal, Amis, and Paiwan. The benchmark spans three tasks—machine translation, automatic speech recognition, and text summarization—using data from Taiwan Indigenous Languages E-Dictionary, Klokâ Digital Platform, and Wikipedia. Across all tasks and models, including zero-shot, 10-shot, and fine-tuned approaches, the results reveal a substantial performance gap between high-resource languages and Formosan languages. BLEU scores for translation remain near zero, ROUGE scores for summarization stay below 20, and even the best-performing ASR task achieves only around 0.3 WER after fine-tuning. These findings underscore the urgent need for more effective adaptation methods and targeted research to support underrepresented languages.

## Method Summary
The benchmark evaluates LLaMA-3.1 (8B/70B), Gemma-1.1-7B-it, Mistral-7B v0.3, NLLB-200, GPT-4o, Whisper, SeamlessM4T, and MMS-1b-all on three tasks using parallel sentence pairs, audio-transcripts, and Wikipedia articles (70/10/20 train/val/test split). Zero-shot, 10-shot (in-context learning), and full fine-tuning (MT/summ: lr=0.001, batch_size=4, 20 epochs; ASR: lr=0.0001, batch_size=16, max 5000 steps) are compared. Metrics are BLEU for MT, WER for ASR, and ROUGE-2/ROUGE-L for summarization. Quality filters remove duplicates, single-word/no-audio entries, and summarization pairs with input < 1.5× summary length.

## Key Results
- Zero-shot and few-shot performance is consistently poor across all tasks, with BLEU scores near zero and ROUGE scores below 20.
- Fine-tuning improves ASR WER from ~0.9 to ~0.3 but translation BLEU remains near zero and summarization ROUGE stays below 20.
- Current adaptation methods (10-shot, fine-tuning) fail to close the performance gap between high-resource and Formosan languages.

## Why This Works (Mechanism)
The benchmark highlights the inadequacy of current large language models for endangered languages due to limited data, poor linguistic structure understanding, and vocabulary coverage. Performance improvements from fine-tuning and few-shot learning are incremental, suggesting that models lack deep semantic understanding and rely on surface-level pattern matching.

## Foundational Learning
- **Few-shot in-context learning**: Demonstrates limited generalization for low-resource languages; quick check: verify prompt formatting and example selection strategy.
- **Fine-tuning**: Shows modest gains but insufficient for closing performance gaps; quick check: monitor loss curves and WER/BLEU progression during training.
- **Automatic speech recognition**: Most improved task via fine-tuning, but still below practical usability; quick check: ensure correct language code usage for each model.
- **Machine translation metrics (BLEU)**: Critical for assessing translation quality; quick check: confirm tokenization and language output correctness.
- **Summarization metrics (ROUGE)**: Measures overlap but not semantic fidelity; quick check: compare ROUGE scores with human evaluation on factual accuracy.

## Architecture Onboarding
- **Component map**: Data collection (E-Dictionary, Klokâ, Wikipedia) -> Pre-processing (filters, splits) -> Model inference (zero-shot, 10-shot, fine-tuning) -> Evaluation (BLEU, WER, ROUGE).
- **Critical path**: Pre-processing → Model selection → Adaptation (fine-tuning/10-shot) → Evaluation.
- **Design tradeoffs**: Broad model coverage vs. limited data; few-shot vs. fine-tuning resource cost; automatic metrics vs. need for human evaluation.
- **Failure signatures**: Near-zero BLEU indicates tokenization or language mismatch; low ROUGE with GPT-4o improvements suggests pattern replication without comprehension; ASR gains but still high WER indicates need for more data or model changes.
- **First experiments**: 1) Run zero-shot baseline across all models and tasks; 2) Execute 10-shot experiments with random seeds; 3) Fine-tune Whisper on ASR and evaluate WER reduction.

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: What adaptation methods beyond few-shot learning and small-scale fine-tuning can substantially close the performance gap between high-resource languages and endangered Formosan languages?
- **Basis in paper**: [explicit] The authors state their "findings underscore the urgent need for more inclusive NLP technologies that can effectively support endangered and underrepresented languages" and that "current models lack sufficient understanding of Formosan linguistic structure and vocabulary despite adaptation efforts."
- **Why unresolved**: All tested adaptation strategies (10-shot, fine-tuning) yielded limited improvements—BLEU remained near zero for MT and ROUGE stayed below 20 for summarization.
- **What evidence would resolve it**: Novel methods achieving BLEU > 10 on translation or ROUGE > 30 on summarization without requiring massive additional data.

### Open Question 2
- **Question**: Can models achieve genuine semantic understanding in low-resource languages, or are improvements merely surface-level pattern replication?
- **Basis in paper**: [explicit] The authors observe that GPT-4o's summarization improvements "stem more from surface-level pattern replication than from genuine semantic understanding," with outputs containing "factual inaccuracies" and "significant grammatical errors."
- **Why unresolved**: Current evaluation metrics (ROUGE, BLEU) cannot distinguish between pattern matching and true comprehension.
- **What evidence would resolve it**: Human evaluation protocols or adversarial tests that assess factual accuracy and grammatical correctness alongside n-gram overlap.

### Open Question 3
- **Question**: How can ASR performance be improved to reach practical usability for Formosan languages beyond the current ~0.3 WER achieved through fine-tuning?
- **Basis in paper**: [inferred] While ASR showed the strongest relative improvement (WER from ~0.9 to ~0.3), the authors note "it still falls short of practical usability" and "further adaptation may still be necessary."
- **Why unresolved**: The paper does not explore whether additional data, architectural changes, or task-specific techniques could push WER below 0.2.
- **What evidence would resolve it**: Demonstrating WER < 0.15 on Formosan ASR tasks through larger-scale fine-tuning, multilingual transfer, or specialized acoustic modeling.

## Limitations
- Fine-tuning and few-shot strategies yield only incremental improvements, failing to close the performance gap.
- Evaluation relies on automatic metrics (BLEU, ROUGE, WER), which may not reflect true semantic understanding or usability.
- Limited exploration of alternative adaptation methods or larger-scale data collection.

## Confidence
- **High**: Core finding that Formosan languages underperform across all tasks even after adaptation is consistently supported.
- **Medium**: Specific numerical improvements (e.g., WER reduction, BLEU, ROUGE scores) may be sensitive to hyperparameters and prompt design.
- **Medium**: Comparison with high-resource languages is based on external literature rather than direct benchmarking.

## Next Checks
1. Re-run 10-shot experiments with multiple random seeds and stratified example selection to verify robustness of reported gains.
2. Test model outputs for language code switching or tokenization failures to explain persistent zero BLEU scores.
3. Extend evaluation to additional Austronesian languages to assess generalizability of the performance gap.