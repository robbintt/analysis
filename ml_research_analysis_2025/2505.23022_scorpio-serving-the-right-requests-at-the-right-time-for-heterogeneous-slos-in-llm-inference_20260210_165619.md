---
ver: rpa2
title: 'SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs
  in LLM Inference'
arxiv_id: '2505.23022'
source_url: https://arxiv.org/abs/2505.23022
tags:
- requests
- tpot
- ttft
- length
- serving
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SCORPIO is an LLM serving system designed to maximize goodput\
  \ and SLO adherence for workloads with heterogeneous service level objectives (SLOs).\
  \ It introduces two specialized guards\u2014a TTFT Guard with least-deadline-first\
  \ reordering and an unattainable SLO rejection mechanism, and a TPOT Guard with\
  \ a VBS-based admission control and credit-based batching\u2014to exploit SLO heterogeneity\
  \ across admission control, queue management, and batch selection."
---

# SCORPIO: Serving the Right Requests at the Right Time for Heterogeneous SLOs in LLM Inference

## Quick Facts
- **arXiv ID**: 2505.23022
- **Source URL**: https://arxiv.org/abs/2505.23022
- **Reference count**: 40
- **Primary result**: Improves system goodput by up to 14.4× and SLO adherence by up to 46.5% compared to state-of-the-art baselines.

## Executive Summary
SCORPIO is an LLM serving system designed to maximize goodput and SLO adherence for workloads with heterogeneous service level objectives (SLOs). It introduces two specialized guards—a TTFT Guard with least-deadline-first reordering and an unattainable SLO rejection mechanism, and a TPOT Guard with a VBS-based admission control and credit-based batching—to exploit SLO heterogeneity across admission control, queue management, and batch selection. Both guards are supported by a predictive module including a sequence length predictor and analytical models. Evaluations show SCORPIO improves system goodput by up to 14.4× and SLO adherence by up to 46.5% compared to state-of-the-art baselines.

## Method Summary
SCORPIO is built as a fork of vLLM and implements two guard mechanisms: a TTFT Guard that prioritizes requests by imminent TTFT deadlines using least-deadline-first reordering and rejects unattainable requests, and a TPOT Guard that uses virtual batch size (VBS) admission control and credit-based batching to manage decode-phase latency. The system relies on a sequence length predictor (fine-tuned OPT-125M) and analytical latency models to estimate prefill and inter-token latencies. The architecture processes requests through predictor → TTFT Guard → TPOT Guard → execution engine pipeline, with VBS admission control preventing cascading TPOT violations under overload.

## Key Results
- Achieves up to 14.4× improvement in system goodput compared to baselines
- Improves SLO adherence by up to 46.5% under heterogeneous workload conditions
- Demonstrates 1.5% better SLO adherence than S3 at low QPS (10 requests/second)
- Analytical models achieve R²≥0.95 and MAPE<8% across settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prioritizing requests by imminent TTFT deadline improves system-wide TTFT SLO attainment under heterogeneous workloads.
- Mechanism: The TTFT Guard implements least-deadline-first (LDF) reordering, placing requests with earlier TTFT deadlines at the queue front. It also rejects requests whose estimated TTFT (sum of prefill times of preceding requests) exceeds their deadline.
- Core assumption: Request SLOs are known at arrival time and prefill latency is predictable from prompt length.
- Evidence anchors:
  - [abstract]: "TTFT Guard, which employs least-deadline-first reordering and rejects unattainable requests"
  - [section 3.3]: "This strategy puts more urgent requests (with earlier TTFT deadline) at the front of the queue, achieving better system-level TTFT attainment."
  - [corpus]: Related systems (JITServe, AccelGen) also prioritize deadline-aware scheduling; however, LDF-specific validation is limited to this paper's experiments.
- Break condition: If request SLOs are homogeneous or unknown, LDF provides no advantage over FCFS.

### Mechanism 2
- Claim: Assigning batching credits proportionally to TPOT slack allows looser-SLO requests to skip iterations, reducing contention for tight-SLO requests.
- Mechanism: Each request earns credits at its TPOT-relative Proportionality (TRP) rate: min(STP(r′))/STP(r). Requests accumulate credit each iteration; only those with credit ≥1.0 are batched. Tight-SLO requests earn credits faster and batch more frequently.
- Core assumption: Skipping iterations for looser-SLO requests does not violate their average TPOT constraint over the generation lifetime.
- Evidence anchors:
  - [abstract]: "TPOT Guard, which utilizes a VBS-based admission control and a novel credit-based batching mechanism"
  - [section 3.4]: "This mechanism ensures that over many steps, the frequency of a request r being batched will converge towards its TRP rate."
  - [corpus]: No direct external validation of credit-based batching found in neighbor papers; concept appears novel to this work.
- Break condition: If all requests have similar TPOT SLOs (TRP≈1 for all), credit-based batching degrades to standard per-iteration batching with added overhead.

### Mechanism 3
- Claim: Virtual Batch Size (VBS) more accurately reflects effective system load when requests are batched intermittently, enabling safer admission control.
- Mechanism: VBS sums the TRP values of all running requests: ΣTRP(r). A new request is admitted only if EstimatedTPOT(VBS(R′), Lavg(R′)) ≤ min(STP(r′) in R′). This prevents cascading TPOT violations under overload.
- Core assumption: The analytical TPOT model (Eq. 4-5) accurately predicts batch-level ITL from |R| and Lavg with R²>0.9.
- Evidence anchors:
  - [section 3.4]: "Since the request r can earn a TRP(r) opportunity to be batched in each iteration if admitted, it can be regarded as a virtual TRP(r) request."
  - [section 4.4 / Table 4]: Analytical model achieves R²≥0.95 and MAPE<8% across settings.
  - [corpus]: VBS concept is not explicitly validated in neighbor literature; closest analog is load-shedding in JITServe but without virtualization.
- Break condition: If TPOT estimator accuracy degrades (e.g., model changes, hardware differences), VBS-based admission may admit too many or too few requests.

## Foundational Learning

- Concept: **Time to First Token (TTFT) vs. Time Per Output Token (TPOT)**
  - Why needed here: These are distinct SLOs; TTFT is dominated by prefill latency (prompt length), while TPOT depends on decode-phase batch size and sequence length.
  - Quick check question: Given a batch of 8 requests with average sequence length 512, which metric would increase if you admitted 4 more requests mid-generation?

- Concept: **Continuous Batching / Paged Attention**
  - Why needed here: SCORPIO builds on vLLM's continuous batching; it assumes requests can be added/removed from the batch at iteration boundaries.
  - Quick check question: What happens to KV cache allocation if a request is skipped for one iteration under credit-based batching?

- Concept: **Service Level Objectives (SLOs) and Goodput**
  - Why needed here: The system optimizes goodput (SLO-compliant requests per second), not raw throughput. Understanding this distinction is critical for interpreting the 14.4× improvement claim.
  - Quick check question: If 100 requests arrive and 90 complete, but only 60 meet their SLOs, what is the SLO adherence rate? What is goodput if T=10s?

## Architecture Onboarding

- Component map: Predictor → TTFT Guard → TPOT Guard → Execution Engine
- Critical path: Request arrival → SeqLen prediction → TTFT estimation → LDF reorder + rejection check → TPOT estimation → VBS admission check → Credit earning → Batch selection → GPU execution
- Design tradeoffs:
  - Predictor colocated with LLM server saves GPU resources but causes 5-20% performance interference (Appendix A.5); separate deployment avoids this.
  - 100-bin equal-width bucketing balances accuracy and resolution; fewer bins inflate accuracy metrics, more bins degrade predictions.
  - Overhead is <0.2% (Table 2), but at low QPS, SCORPIO's complexity can underperform simpler baselines by ~1.5% adherence.
- Failure signatures:
  - Mass TPOT violations under bursty traffic → VBS threshold too loose or TPOT estimator underestimates latency.
  - Excessive rejections under moderate load → TTFT estimator overestimates prefill time or ε (inefficiency coefficient) too high.
  - Credit accumulation stalls for loose-SLO requests → TRP values near zero; check min(STP) calculation.
- First 3 experiments:
  1. **Reproduce ablation (Figure 6)**: Disable TTFT Guard, then TPOT Guard, then both; measure TTFT/TPOT violations and goodput at QPS=14. Confirm interdependence.
  2. **Stress test VBS admission**: Generate synthetic burst at QPS=60 (per Azure trace peak); plot admitted vs. rejected requests over time. Verify no cascading TPOT violations.
  3. **Probe estimator accuracy**: Deploy on different GPU (e.g., H100 vs. A100); re-profile α,β,γ,δ coefficients and measure TPOT estimator R² and MAPE. Document drift.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SCORPIO scheduling logic be effectively integrated with disaggregated architectures that separate prefill and decode phases?
- Basis in paper: [explicit] Appendix A.6 states that integrating the scheduler with "latest optimizations, such as prefill-decode disaggregation," is an interesting direction for future work.
- Why unresolved: The current system assumes a standard continuous batching setup where prefill and decode share the same resources, potentially allowing credit-based batching to manage contention in a way that might not translate directly to physically separated instances.
- What evidence would resolve it: An implementation of SCORPIO on a disaggregated serving framework (e.g., DistServe) that demonstrates maintained or improved SLO adherence without communication overheads negating the scheduling gains.

### Open Question 2
- Question: How can the system dynamically detect low-load conditions to switch to a lower-overhead scheduling strategy?
- Basis in paper: [explicit] Section 4.2 and Appendix A.6 note that the overhead of the predictor and guards causes slight performance degradation at low QPS, suggesting a dynamic switch to a "simpler, lower-overhead scheduling method" as future work.
- Why unresolved: The current design employs a fixed control logic that creates resource contention with the LLM server during idle periods, reducing goodput compared to baselines.
- What evidence would resolve it: A threshold-based or feedback-driven mechanism that disables the predictor and guards at low loads, demonstrating baseline-equivalent performance while retaining SLO guarantees under bursty traffic.

### Open Question 3
- Question: Does processing unattainable requests with lower priority or migrating them improve aggregate system utility compared to immediate rejection?
- Basis in paper: [explicit] Section 3.3 states that requests violating TTFT SLOs are rejected for simplicity, but "other strategies, such as processing these requests with a lower priority or migrating them to a different node, are left as future work."
- Why unresolved: Rejection ensures strict SLO adherence for admitted requests but discards potential value; alternative handling might allow the system to serve more total requests (higher throughput) without significantly degrading the experience of high-priority requests.
- What evidence would resolve it: A comparative analysis measuring system goodput and utility where unattainable requests are queued in a secondary low-priority tier versus the current rejection approach.

## Limitations
- Tight coupling to vLLM's continuous batching architecture limits applicability to other serving systems
- Predictor accuracy degrades with distribution shift in request types
- VBS-based admission control assumes homogeneous hardware performance
- Credit-based batching overhead scales with the number of distinct SLO categories

## Confidence

- **High confidence**: TTFT Guard's LDF reordering improves deadline adherence (validated through ablation and controlled experiments)
- **Medium confidence**: TPOT Guard's credit-based batching maintains long-term average TPOT (theoretical convergence proven but real-world drift not extensively tested)
- **Medium confidence**: Analytical models achieve R²≥0.95 and MAPE<8% (profile-specific coefficients not provided for independent validation)
- **Low confidence**: Performance at extremely low QPS (<10) where overhead dominates (only briefly mentioned in appendix)

## Next Checks

1. **Cross-architecture validation**: Implement SCORPIO's guard mechanisms on a non-vLLM system (e.g., TGI or custom PyTorch serving) to test architecture independence
2. **Distribution shift stress test**: Deploy with synthetic prompt distributions that gradually shift from training data characteristics; measure predictor accuracy decay and SLO adherence degradation over time
3. **Multi-tenant interference study**: Run SCORPIO alongside other GPU workloads (training, batch inference) to quantify performance isolation guarantees under shared resource contention