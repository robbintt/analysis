---
ver: rpa2
title: 'When three experiments are better than two: Avoiding intractable correlated
  aleatoric uncertainty by leveraging a novel bias--variance tradeoff'
arxiv_id: '2509.04363'
source_url: https://arxiv.org/abs/2509.04363
tags:
- bias
- type
- points
- uncertainty
- estimation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes novel active learning strategies that leverage
  a bias-variance tradeoff to efficiently reduce the mean squared error between a
  model distribution and a ground-truth random variable in experimental scenarios
  with heteroskedastic aleatoric uncertainty. The core method calculates the expected
  mean squared error as the sum of epistemic uncertainty, bias squared, and aleatoric
  uncertainty terms, then uses the negative gradient of this error between experimental
  rounds to select informative points for labeling.
---

# When three experiments are better than two: Avoiding intractable correlated aleatoric uncertainty by leveraging a novel bias--variance tradeoff

## Quick Facts
- **arXiv ID:** 2509.04363
- **Source URL:** https://arxiv.org/abs/2509.04363
- **Reference count:** 34
- **Primary result:** Novel active learning strategies using cobias-covariance relationship outperform canonical methods like BALD and Least Confidence in batched settings, particularly when combined with quadratic estimation of the bias matrix.

## Executive Summary
This paper proposes novel active learning strategies that leverage a bias-variance tradeoff to efficiently reduce the mean squared error between a model distribution and a ground-truth random variable in experimental scenarios with heteroskedastic aleatoric uncertainty. The core method calculates the expected mean squared error as the sum of epistemic uncertainty, bias squared, and aleatoric uncertainty terms, then uses the negative gradient of this error between experimental rounds to select informative points for labeling. The authors introduce a novel cobias-covariance relationship that allows quadratic leverage of historical data and provides a natural framework for batching through eigendecomposition. Their difference-based method using this cobias-covariance relationship outperforms canonical methods like BALD and Least Confidence in batched settings, particularly when combined with quadratic estimation of the bias matrix. The approach is demonstrated on both noiseless and noisy systems with uncorrelated and correlated noise, showing superior performance in reducing mean squared error across different model settings.

## Method Summary
The method decomposes expected mean squared error (EMSE) into epistemic uncertainty, bias squared, and aleatoric uncertainty terms. It uses deep ensembles (K=5 networks) to estimate predictive distributions, then employs either direct Gaussian Process estimation or a symmetric neural network for quadratic bias estimation. Acquisition functions target reducible error components using difference operators that cancel aleatoric uncertainty between rounds. For batching, the method performs eigendecomposition of the cobias-covariance matrix to select diverse points aligned with principal eigenvectors. The approach is tested on a 2D toy system with heteroskedastic noise (Types I/II/III) across various acquisition strategies and batch sizes.

## Key Results
- Difference-based PEMSE acquisition outperforms canonical methods like BALD and Least Confidence in batched settings
- Quadratic estimation of bias matrix provides more stable predictions than direct estimation, especially for off-diagonal elements
- Eigendecomposition batching strategy selects more diverse and informative points than top-k selection by acquisition score
- The approach shows superior performance in reducing mean squared error across noiseless and noisy systems with uncorrelated and correlated noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing expected mean squared error (EMSE) into epistemic uncertainty, bias squared, and aleatoric uncertainty enables targeting only reducible error components during active learning.
- Mechanism: The pointwise EMSE decomposition τ_k(x) = σ²_Fk(x) + δ²_k(x) + σ²_Y(x) separates model uncertainty (reducible) from observation noise (irreducible). Acquisition functions that target only the first two terms avoid wasting label budget on regions where noise is inherently high.
- Core assumption: Test-time label noise is independent of the fitted predictor: Cov(F_k(x), Y(x)) = 0 (stated in Section 2.2).
- Evidence anchors:
  - [abstract] "The bias--variance tradeoff can be used to write the expected mean squared error between a model distribution and a ground-truth random variable as the sum of an epistemic uncertainty term, the bias squared, and an aleatoric uncertainty term."
  - [Section 2.2, Eq. 5] Full derivation showing the three-term decomposition.
  - [corpus] Related work on disentangling aleatoric and epistemic uncertainty (e.g., HybridFlow, CLEAR) confirms this decomposition is a recognized approach, though corpus does not directly validate the specific acquisition strategy.
- Break condition: If the independence assumption Cov(F_k(x), Y(x)) = 0 is violated (e.g., model predictions systematically influence which noisy labels are observed), the decomposition may not hold.

### Mechanism 2
- Claim: Difference-based acquisition functions using the κ operator approximate the negative gradient of PEMSE, naturally filtering out intractable aleatoric uncertainty.
- Mechanism: The difference operator κ[g_k](x) := g_{k-1}(x) - g_k(x) measures change between rounds. When applied to τ_k, the aleatoric term σ²_Y(x) cancels (Eq. 9), leaving only changes in reducible components. Regions where τ rapidly decreases indicate areas where bias or epistemic uncertainty is collapsing—these are prioritized.
- Core assumption: Sufficient change occurs between consecutive rounds to make the difference meaningful; with single-point acquisition per round, changes may be too small to detect reliably.
- Evidence anchors:
  - [abstract] "We leverage this relationship to propose novel active learning strategies that directly reduce the bias between experimental rounds."
  - [Section 2.3.2, Eq. 7-9] Derivation showing aleatoric uncertainty cancels in κ(L_k).
  - [Section 4.2] "We hypothesize that the lack of clear benefit using the difference-based methods is because we do not see large changes in either the bias or the PEMSE between two consecutive rounds (only one point was selected per round). Therefore, the benefits should become apparent in a batched setting."
  - [corpus] Corpus does not provide direct validation of difference-based acquisition; mechanism is paper-specific.
- Break condition: If model updates between rounds are too small or too stochastic (e.g., SGD instability), the difference signal becomes noisy and unreliable.

### Mechanism 3
- Claim: The cobias-covariance formulation enables quadratic leverage of historical data through matrix completion, providing O(l²) training pairs from l observations and a principled batching mechanism via eigendecomposition.
- Mechanism: The matrix Ω^(k) = Σ_Fk + Δ_k + Σ_Y captures pairwise relationships across state space. The rank-1 cobias matrix Δ_k = δ_k δ_k^T can be estimated via symmetric neural network Q(x, x*) = ψ(x)^T ψ(x*). Quadratic estimation has single-error-term structure (Eq. 15) versus multiplicative error propagation in direct estimation (Eq. 14). Eigendecomposition of Ω^(k) reveals principal modes; selecting points aligned with top eigenvectors ensures batch diversity.
- Core assumption: The bias structure is sufficiently low-rank and smooth to be captured by the neural network; sufficient historical data exists for matrix completion.
- Evidence anchors:
  - [abstract] "We investigate methods to leverage historical data in a quadratic manner through the use of a novel cobias--covariance relationship, which naturally proposes a mechanism for batching through an eigendecomposition strategy."
  - [Section 3.1, Eq. 11-12] Full cobias-covariance derivation.
  - [Section 3.3, Eq. 16-20] Eigendecomposition batching strategy.
  - [Section 4.2, Figure 5] "In both Type II and Type III scenarios, we see a clear benefit to using quadratic estimation with difference-PEMSE for the eigendecomposition approach because quadratic estimation will achieve higher accuracy predicting off-diagonal elements of Δ_k."
  - [corpus] Corpus papers on uncertainty quantification do not address cobias-covariance directly; this appears to be a novel contribution.
- Break condition: If Δ_k is not approximately low-rank, or if training data is severely limited (< 10-20 points), matrix completion becomes unreliable and eigendecomposition may amplify estimation errors.

## Foundational Learning

- **Bias-variance tradeoff in regression**
  - Why needed here: The entire framework builds on decomposing prediction error into bias (systematic error), variance (model uncertainty), and irreducible noise. Without this foundation, the acquisition functions and cobias-covariance formulation will be opaque.
  - Quick check question: Given a model with high bias but low variance, would collecting more training data help? (Answer: No—high bias indicates model misspecification, not data scarcity.)

- **Epistemic vs. aleatoric uncertainty**
  - Why needed here: The method explicitly targets epistemic uncertainty (reducible with data) while ignoring aleatoric uncertainty (inherent to the observation process). Confusing these leads to wasted experiments on irreducibly noisy regions.
  - Quick check question: If you observe infinite labels at a single point x, which uncertainty type goes to zero? (Answer: Epistemic uncertainty goes to zero; aleatoric uncertainty remains because it's inherent to the measurement process.)

- **Ensemble methods for uncertainty estimation**
  - Why needed here: The framework uses deep ensembles (K=5 networks) to estimate both μ_Fk and σ²_Fk. Understanding how ensembles approximate predictive distributions is essential for implementation.
  - Quick check question: Why use an ensemble rather than a single network with dropout for uncertainty estimation? (Answer: Both can work; ensembles provide more reliable variance estimates but are more computationally expensive.)

## Architecture Onboarding

- **Component map:**
  - Ensemble model F_k -> Bias estimator -> Acquisition selector -> Batching module -> Data store

- **Critical path:**
  1. Initialize ensemble on L_0 (10-100 points)
  2. For each round k: compute μ_Fk, σ²_Fk across X
  3. Estimate bias (direct via GP or quadratic via Q network)
  4. Compute acquisition scores α_k(x) for unlabeled points
  5. If batching: eigendecomposition of Ω^(k), select top-m eigenvector-aligned points
  6. Query labels, update L_k, retrain ensemble

- **Design tradeoffs:**
  - **Direct vs. quadratic estimation**: Direct is simpler but has multiplicative error propagation; quadratic is more stable for batching but requires more training data and computational overhead
  - **Single-point vs. batch acquisition**: Single-point is simpler; batching enables correlated noise handling (Type III) and efficient parallel experiments, but requires eigendecomposition
  - **Ensemble size K**: Larger K improves variance estimation but increases training cost; paper uses K=5
  - **Initial data size**: 10 points is minimal; quadratic estimation struggles below this threshold (Figure 4-5)

- **Failure signatures:**
  - Performance worse than random selection: Likely insufficient initial data (<20 points) or poor bias estimator training
  - Difference-PEMSE underperforms PEMSE: Expected for Type I (no noise to cancel); may indicate batch size too small for meaningful κ signal
  - Batch selections lack diversity: Check eigendecomposition is selecting from distinct eigenvectors, not just top-k by acquisition score
  - Quadratic estimation unstable: Check Q network is trained on lower triangle only (avoid double-counting), validate symmetry Q(x, x*) = Q(x*, x)

- **First 3 experiments:**
  1. **Reproduce Type II single-acquisition results** (Figure 2, middle column): Initialize with 100 points, run 50 iterations of single-point acquisition using PEMSE with direct estimation. Validate that bias-based methods outperform LC/BALD. This confirms basic implementation correctness.
  2. **Test batching with eigendecomposition** (Figure 5 protocol): Initialize with 100 points, run 10 batch iterations of size m=10. Compare PEMSE top-k vs. eigendecomposition batching with quadratic estimation. Verify eigendecomposition provides small but consistent improvement.
  3. **Stress test low-data regime**: Initialize with 10 points, compare direct vs. quadratic estimation with difference-PEMSE in batched setting. Confirm that quadratic estimation is the only non-random method that can beat random selection in this regime (per Section 5 discussion).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the cobias–covariance framework be extended to non-symmetric Bregman divergences such as KL divergence for classification tasks?
- Basis in paper: [explicit] "It would not be challenging to apply our approach to other Bregman divergences, yet we may not be able use the cobias–covariance approach as it is not clear how our approach would work for non-symmetric divergences" (Discussion); "it is not clear how well this approach will work for classification problems with non-symmetric cobias–covariance tradeoffs" (Limitations).
- Why unresolved: The current formulation relies on symmetric matrix structure for eigendecomposition-based batching; non-symmetric divergences break this structure.
- What evidence would resolve it: A modified cobias formulation or alternative batching mechanism for classification that preserves theoretical guarantees.

### Open Question 2
- Question: Does accounting for variable sampling frequency across state space regions improve bias estimator accuracy?
- Basis in paper: [explicit] "we estimate unseen biases leveraging historical realizations, but we do not account for the variation in how many times a specific region of the state space is sampled; specifically, our bias estimator should have appropriately weighted training data" (Discussion).
- Why unresolved: Current implementation treats all historical bias estimates equally regardless of sampling density.
- What evidence would resolve it: Ablation experiments comparing weighted vs. unweighted bias estimation on heterogeneous sampling regimes.

### Open Question 3
- Question: Can function class constraints be incorporated to avoid sampling points that lead to unreliable inferences?
- Basis in paper: [explicit] "we may not wish to sample points in a manner that leads to indescribable inferences outside the function class" (Discussion).
- Why unresolved: Current acquisition functions optimize for MSE reduction without considering whether selected points produce predictions consistent with the model's function class.
- What evidence would resolve it: Modified acquisition functions with function-class regularization, tested on misspecified model scenarios.

## Limitations

- The cobias-covariance framework relies heavily on the rank-1 structure of the bias matrix (Δ_k = δ_k δ_k^T), which may not hold for all real-world problems.
- The quadratic estimation approach requires sufficient historical data for matrix completion—the paper notes instability with fewer than 10-20 points.
- The independence assumption Cov(F_k(x), Y(x)) = 0 is critical but unverified in practical settings.
- The eigendecomposition batching strategy assumes the cobias-covariance matrix is sufficiently well-conditioned for stable decomposition.

## Confidence

- **High confidence**: The three-term decomposition of EMSE (epistemic + bias² + aleatoric) is mathematically sound and validated through the analytical derivation. The cancellation of aleatoric terms in the difference operator κ is rigorously proven.
- **Medium confidence**: The quadratic estimation of bias matrix Δ_k shows theoretical advantages (single-error-term structure vs. multiplicative error), but empirical validation is limited to the synthetic 2D toy problem. Real-world applicability depends on bias structure being approximately low-rank.
- **Medium confidence**: Performance gains over canonical methods (BALD, Least Confidence) are demonstrated but are incremental in single-acquisition settings and only become more pronounced in batched scenarios. The superiority is task-specific and may not generalize to higher-dimensional problems.
- **Low confidence**: The paper's claims about the cobias-covariance relationship being "novel" and "naturally proposing a batching mechanism" are difficult to independently verify without access to the full derivation and proof of uniqueness. The mechanism description is clear but the novelty claim requires broader literature comparison.

## Next Checks

1. **Low-data regime stress test**: Replicate the 10-point initial data scenario to verify that quadratic estimation with difference-PEMSE is the only method that can outperform random selection, as claimed in Section 5 discussion.

2. **Bias structure validation**: For the 2D toy problem, compute the actual rank of Δ_k across iterations and verify it remains close to 1. If the rank deviates significantly, this would challenge the core assumption of the cobias-covariance framework.

3. **Independence assumption test**: In Type III (correlated noise) experiments, compute Cov(F_k(x), Y(x)) empirically across the grid. If this covariance is non-negligible, the theoretical foundation of the decomposition may be violated in practice.