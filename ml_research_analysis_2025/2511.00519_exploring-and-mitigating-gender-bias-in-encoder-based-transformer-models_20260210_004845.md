---
ver: rpa2
title: Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models
arxiv_id: '2511.00519'
source_url: https://arxiv.org/abs/2511.00519
tags:
- bias
- mask
- gender
- word
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses gender bias in encoder-based transformer models
  by introducing MALoR, a novel metric for quantifying bias in contextualized word
  embeddings. The method involves masking gendered terms and occupations to assess
  probability assignments, then mitigating bias through continued pretraining on gender-balanced
  datasets generated via Counterfactual Data Augmentation.
---

# Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models

## Quick Facts
- arXiv ID: 2511.00519
- Source URL: https://arxiv.org/abs/2511.00519
- Reference count: 20
- Primary result: MALoR metric quantifies gender bias in transformer embeddings; CDA-based continued pretraining reduces BERT-base "he-she" bias from 1.27 to 0.08 without degrading downstream task performance

## Executive Summary
This paper introduces MALoR, a novel metric for quantifying gender bias in encoder-based transformer models through Masked Language Modeling probability distributions. The method masks gendered terms and occupations, then compares probability assignments to compute bias scores. The authors demonstrate that continued pretraining on gender-balanced datasets generated via Counterfactual Data Augmentation significantly reduces measured bias across BERT-base, BERT-large, and DistilBERT models, with bias scores dropping from 1.27 to 0.08 for "he-she" associations and from 2.51 to 0.36 for "his-her" associations. Crucially, this mitigation preserves downstream task performance, maintaining SST-2 accuracy across all models.

## Method Summary
The authors develop MALoR to quantify gender bias by computing log₂ ratios of masked token probabilities for male versus female terms across 51 sentence templates and 60 occupations, then taking the mean absolute value. To mitigate bias, they generate gender-balanced datasets via Counterfactual Data Augmentation (CDA) by swapping gendered pronouns and names, then perform continued pretraining using standard MLM objectives with learning rate 2×10⁻⁵ for 200 epochs. The approach is evaluated across three experiments (he-she, his-her, male-female names) on BERT-base, BERT-large, and DistilBERT models, with SST-2 accuracy monitoring to ensure no performance degradation.

## Key Results
- BERT-base "he-she" bias reduced from 1.27 to 0.08 following CDA-based debiasing
- BERT-base "his-her" bias decreased from 2.51 to 0.36 with no SST-2 accuracy loss
- BERT-large and DistilBERT also show significant bias reduction, though DistilBERT exhibits less stable convergence
- No statistically significant performance degradation observed on SST-2 across all debiased models (p-values > 0.01)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MALoR quantifies gender bias by comparing masked token probability distributions for male versus female terms.
- Mechanism: The metric computes log₂(P(male_term)/P(female_term)) for each sentence-occupation pair, averages across 51 sentence templates, then takes the mean absolute value across 60 occupations. This produces a single non-negative score where 0 indicates no bias and higher values indicate stronger bias. The absolute value prevents male-leaning and female-leaning biases from canceling out.
- Core assumption: Masked Language Modeling probability distributions reflect learned gender-occupation associations rather than mere linguistic frequency.
- Evidence anchors:
  - [abstract] "MALoR assesses bias based on model probabilities for filling masked tokens"
  - [Section 4.2, Equation 1] Formal definition of MALoR using log ratio formulation
  - [corpus] Limited direct validation; related work on gender encoding patterns in PLMs (arxiv 2503.06734) uses different methodology, so cross-validation is pending
- Break condition: If a model's vocabulary lacks whole tokens for target words (e.g., RoBERTa splits "engineer" into subwords), the metric cannot compute meaningful probabilities. The paper explicitly excludes RoBERTa and ALBERT from certain experiments for this reason.

### Mechanism 2
- Claim: Continued pretraining on counterfactually augmented data reduces measured gender bias without degrading downstream task performance.
- Mechanism: Counterfactual Data Augmentation (CDA) creates sentence pairs by swapping gendered terms (e.g., "he/she", "his/her", male/female names). When models are further pretrained on this balanced corpus (200 epochs, learning rate 2×10⁻⁵), they learn more neutral associations. The model weights are updated via standard MLM loss, but the training distribution now contains equal representation of both genders for each occupation.
- Core assumption: The bias originates from imbalanced co-occurrence patterns in pretraining data rather than architectural inductive biases.
- Evidence anchors:
  - [abstract] "bias scores for 'he-she' dropped from 1.27 to 0.08, and 'his-her' from 2.51 to 0.36 following our mitigation approach"
  - [Section 4.3] Describes CDA process and continued pretraining setup
  - [Section 5.3, Table 9] SST-2 accuracy maintained across all models (p-values > 0.01, failing to reject null hypothesis of no performance degradation)
  - [corpus] No independent replication of this specific CDA approach found; related debiasing work (arxiv 2506.12527) surveys methods but doesn't validate this technique
- Break condition: If the augmented dataset is too small or lacks sufficient occupation diversity, the model may overfit to specific sentence patterns rather than learning generalizable neutrality. The paper used ~3,200–6,400 sentence pairs per experiment.

### Mechanism 3
- Claim: Larger BERT variants show more stable debiasing convergence than compressed models.
- Mechanism: BERT-base and BERT-large converge within 10% of minimum MALoR scores during training with stable trajectories. DistilBERT, which uses knowledge distillation to compress the model, shows higher variance and fails to reach stable convergence. The assumption is that distillation transfers both useful representations and inherited biases while reducing the model's capacity to learn new balanced patterns.
- Core assumption: Model capacity correlates with ability to unlearn biased associations during continued pretraining.
- Evidence anchors:
  - [Section 5.2.1, Figures 4-6] Visualizations showing BERT models achieving near-neutral (0.5) probability distributions while DistilBERT retains more variability
  - [Appendix 7.1] Learning curve analysis showing BERT convergence vs. DistilBERT instability
  - [corpus] Insufficient evidence; no external studies comparing debiasing stability across model sizes
- Break condition: If debiasing training is stopped too early (before convergence), larger models may retain more of their original bias than smaller models that happened to reach a local minimum faster.

## Foundational Learning

- Concept: **Masked Language Modeling (MLM)**
  - Why needed here: MALoR depends entirely on interpreting the probability distribution a model assigns to masked tokens. Without understanding that MLM forces models to predict missing words based on context, you cannot interpret why high probability for "he" in "The [MASK] works as a nurse" indicates learned gender bias.
  - Quick check question: If a model assigns P("doctor")=0.8 and P("nurse")=0.1 to a masked position in "She works as a [MASK]," what does this suggest about learned associations?

- Concept: **Contextualized vs. Static Word Embeddings**
  - Why needed here: The paper explicitly positions its work as addressing limitations of static embedding debiasing methods. Contextualized embeddings (like BERT's) produce different representations for the same word depending on surrounding context, which requires different bias detection approaches than static methods like Word2Vec or GloVe.
  - Quick check question: Why can't we simply apply cosine-similarity based bias detection (like WEAT) directly to BERT embeddings?

- Concept: **Counterfactual Data Augmentation**
  - Why needed here: The mitigation strategy relies on CDA. Understanding that CDA creates synthetic training examples by swapping protected attributes (gender) helps explain why balanced exposure during pretraining shifts learned associations.
  - Quick check question: If you have 1,000 sentences mentioning "doctors" with "he" and only 100 with "she," how would CDA change this distribution?

## Architecture Onboarding

- Component map:
  MALoR Metric Calculator -> CDA Data Generator -> Continued Pretraining Module -> Downstream Evaluator

- Critical path:
  1. Select target model with WordPiece tokenization (BERT-base, BERT-large, or DistilBERT)
  2. Run pre-debias MALoR across three experiment types (he-she, his-her, male-female names)
  3. Generate CDA dataset specific to each experiment type
  4. Run continued pretraining until MALoR converges (monitor with epoch-by-epoch tracking)
  5. Re-run MALoR on debiased model
  6. Validate on SST-2 to confirm no performance regression

- Design tradeoffs:
  - **Vocabulary compatibility vs. model coverage**: RoBERTa and ALBERT use BPE tokenization that splits occupation terms; they're excluded from full experiments. If you need to debias these models, you must adapt the occupation list or develop subword-aware probability aggregation.
  - **Dataset size vs. debiasing effectiveness**: The paper uses ~6K sentences per experiment. Larger datasets may improve generalization but require more compute. Smaller datasets risk overfitting to specific templates.
  - **Metric sensitivity vs. interpretability**: MALoR aggregates across 51 templates × 60 occupations. This reduces noise but may obscure specific high-bias occupation/template combinations that warrant targeted intervention.

- Failure signatures:
  - **Vocabulary mismatch**: If MALoR scores are "N/A" or unusually low, check whether target words exist as whole tokens in the model's vocabulary
  - **Training instability**: If MALoR oscillates without clear downward trend during debiasing, reduce learning rate or increase batch size
  - **Over-correction**: If post-debias probabilities show uniform ~0.5 for all occupations regardless of context, the model may have lost semantic distinctions (verify with downstream task)
  - **Template leakage**: If debiasing only works on the 51 training templates but fails on new sentence structures, the model has overfit to specific phrasings

- First 3 experiments:
  1. **Reproduce baseline MALoR scores**: Load BERT-base-uncased, run MALoR on he-she experiment with the 51 provided templates and 10 sample occupations. Verify you get scores in the ~1.0-1.5 range matching Table 5.
  2. **Mini-debiasing run**: Create a small CDA dataset (500 sentence pairs) for a single occupation category. Run continued pretraining for 50 epochs and observe MALoR trajectory. Compare full convergence (200 epochs) vs. early stopping.
  3. **Template sensitivity test**: Design 10 new sentence structures not in the original 51. Measure MALoR on original vs. debiased model using only these new templates. If scores are significantly higher than the reported results, the debiasing may not generalize beyond training templates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the MALoR metric and the associated debiasing pipeline be adapted for models like RoBERTa and ALBERT that utilize extensive subword tokenization, which currently prevents accurate probability calculation for occupational terms?
- **Basis in paper:** [explicit] The authors state in Section 4.3.4 and Section 6.2 that "Debiasing experiments could not be performed on RoBERTa and ALBERT due to differences in vocabulary and tokenization," and suggest "vocabulary alignment" as a future direction.
- **Why unresolved:** The current method requires the target occupation (e.g., "engineer") to exist as a single token in the model's vocabulary. In RoBERTa and ALBERT, these words are split into sub-tokens (e.g., "engine" + "er"), distorting the probability distribution needed for the MALoR calculation.
- **What evidence would resolve it:** A modified methodology that aggregates sub-token probabilities or reconstructs semantic meaning from sub-token embeddings, demonstrated by successfully applying MALoR to debias RoBERTa or ALBERT without tokenization errors.

### Open Question 2
- **Question:** Does the effectiveness of the MALoR-based debiasing approach persist across a diverse set of downstream natural language understanding tasks beyond the sentiment analysis task (SST-2) used in this study?
- **Basis in paper:** [explicit] Section 6.2 notes, "We evaluated our models on only one downstream task which is SST-2. Further downstream tasks will be performed on debiased models to further validate their performance."
- **Why unresolved:** While the study confirms performance is maintained on SST-2, it is unclear if the continued pretraining on gender-balanced data causes "catastrophic forgetting" or performance degradation in other complex tasks like Question Answering or Natural Language Inference.
- **What evidence would resolve it:** Benchmark results on the full GLUE or SuperGLUE suites comparing the original and debiased models, showing statistically insignificant changes in accuracy across all tasks.

### Open Question 3
- **Question:** Can the MALoR evaluation metric and Counterfactual Data Augmentation mitigation strategy be effectively translated to modern decoder-based Large Language Models (LLMs)?
- **Basis in paper:** [explicit] The conclusion states, "Since decoder-based transformer models (LLMs) are increasingly replacing encoder-based models, we plan to adapt our framework to these models in future work."
- **Why unresolved:** MALoR relies on Masked Language Modeling (MLM) probabilities, which are intrinsic to BERT-style encoder architectures. Decoder models use causal attention and do not natively support the "fill-in-the-blank" masking logic used to calculate the log-ratio bias score.
- **What evidence would resolve it:** A new adaptation of the MALoR formula suitable for generative likelihoods, applied to a decoder model, showing that the bias mitigation transfer reduces gender associations in text generation tasks without compromising fluency.

## Limitations

- **Vocabulary Coverage Dependency**: The MALoR metric fundamentally requires target words to exist as whole tokens in the model vocabulary, excluding models like RoBERTa and ALBERT that use extensive subword tokenization.
- **Template-Specific Bias Measurement**: The 51 fixed sentence templates may not capture all ways gender bias manifests in contextualized embeddings, potentially missing bias in novel sentence structures.
- **Generalization of CDA Debiasing**: The paper doesn't investigate whether debiasing generalizes beyond the specific templates and occupations used in training, risking overfitting to training contexts.

## Confidence

- **High Confidence**: MALoR as quantifiable metric for gender bias in MLM-based models; Bias reduction magnitude; No SST-2 performance degradation
- **Medium Confidence**: Generalization of debiasing to novel contexts; Comparison across model architectures
- **Low Confidence**: Long-term stability of debiased models; Transferability to multilingual models

## Next Checks

**Check 1: Cross-Contextual Generalization Test**
Design and execute a validation study using 20 novel sentence templates not in the original 51. Measure MALoR on both pre-debiased and post-debiased models using only these new templates for each experiment type. Compare bias reduction magnitude to original template results to assess generalization beyond training contexts.

**Check 2: Subword Tokenization Robustness Analysis**
Implement an adapted MALoR metric that can handle subword tokenization by aggregating probabilities across subword pieces for each target word. Apply this to RoBERTa and ALBERT models using the same 60 occupations and subset of 51 templates. Compare bias scores and debiasing effectiveness to BERT models to determine if vocabulary coverage is a fundamental constraint.

**Check 3: Long-Term Stability Evaluation**
Take the debiased BERT-base model and run an additional 100 epochs of standard MLM pretraining on the original English Wikipedia corpus. Monitor MALoR scores every 10 epochs to track whether bias re-emerges over time. Additionally, test the model on new occupation categories not present in the original debiasing dataset to assess lasting changes.