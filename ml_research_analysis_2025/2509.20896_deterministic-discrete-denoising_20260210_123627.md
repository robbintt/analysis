---
ver: rpa2
title: Deterministic Discrete Denoising
arxiv_id: '2509.20896'
source_url: https://arxiv.org/abs/2509.20896
tags:
- diffusion
- discrete
- denoising
- self
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a deterministic denoising algorithm for discrete
  diffusion models by introducing a variant of the herding algorithm. The key idea
  is to replace the stochastic reverse Markov chain with a deterministic one driven
  by weakly chaotic dynamics, where discrete state transitions are guided by auxiliary
  continuous weight variables.
---

# Deterministic Discrete Denoising

## Quick Facts
- arXiv ID: 2509.20896
- Source URL: https://arxiv.org/abs/2509.20896
- Reference count: 23
- Primary result: A deterministic denoising algorithm for discrete diffusion models using herding-based sampling that improves efficiency and sample quality without retraining.

## Executive Summary
This paper introduces a deterministic denoising algorithm for discrete diffusion models by replacing stochastic categorical sampling with a herding-based approach. The method maintains auxiliary continuous weight variables for each discrete token, using deterministic updates that drive the empirical distribution closer to the target transition probabilities. This drop-in replacement improves generative perplexity by up to 10× and reduces inference steps needed, while maintaining or improving sample quality across text and image generation tasks, as well as combinatorial optimization problems.

## Method Summary
The method replaces stochastic sampling in discrete diffusion models with deterministic herding updates. For each token position, it maintains a discrete state and continuous weight vector. At each denoising step, the algorithm selects the next state via a delayed-switching argmax operation that includes a margin parameter δ to prevent excessive transitions. The weight vector is updated to accumulate the difference between the model's probability distribution and the selected state. This creates a hybrid dynamical system that preserves probability mass piecewise and exhibits weakly chaotic dynamics, enabling efficient deterministic generation from pre-trained models without any architectural changes.

## Key Results
- Up to 10× improvement in generative perplexity (GPT-2 Large) compared to stochastic sampling
- Consistent reduction in FID and IS scores on CIFAR-10 image generation
- Improved solution quality on TSP combinatorial optimization tasks
- Requires fewer inference steps (128 vs 1024) to match stochastic sampling quality

## Why This Works (Mechanism)

### Mechanism 1: Deterministic Herding Replaces Stochastic Sampling
- Claim: Replacing stochastic categorical sampling with deterministic herding-based updates improves generative quality and efficiency in discrete diffusion models.
- Mechanism: The herding algorithm introduces auxiliary continuous weight variables $w_t$ for each discrete token $x_t$. At each denoising step $t$, instead of sampling $x_{t-1} \sim \text{Cat}(p_{t-1})$, the algorithm selects $x_{t-1} = \text{argmax}_x (w_t + p_{t-1})^\top x$, then updates the weight $w_{t-1} = w_t + p_{t-1} - x_{t-1}$. This creates a hybrid dynamical system where the discrete state is guided by accumulated error correction from the continuous weight, driving the empirical distribution closer to the target transition probabilities at a faster convergence rate.
- Core assumption: The herding algorithm's theoretical property of $O(T^{-1})$ convergence for matching feature expectations (vs. $O(T^{-1/2})$ for stochastic sampling) translates to improved fidelity of the generated discrete sequence to the learned denoising trajectory.
- Evidence anchors:
  - [abstract] "The key idea is to derandomize the generative reverse Markov chain by introducing a variant of the herding algorithm, which induces deterministic state transitions driven by weakly chaotic dynamics."
  - [Section 3.1] "This herding-based system reduces the cumulative discrepancy... compared with stochastic sampling, thereby yielding a sample sequence that reflects transition probabilities more faithfully."
  - [corpus] Direct evidence in corpus is weak. Neighboring papers discuss diffusion efficiency but not the herding mechanism directly.
- Break condition: The herding dynamics fail to remain bounded, or the argmax operation becomes unstable for very high-dimensional categorical distributions, causing weight divergence or degenerate samples.

### Mechanism 2: Delayed-Switching Stabilizes Discrete Transitions
- Claim: Introducing a margin $\delta$ in the argmax operation improves sample quality by preventing excessive, rapid switching of discrete states during denoising.
- Mechanism: A modified argmax, $x_{t-1} = \text{argmax}_x (w_t + p_{t-1} + \delta x_t)^\top x$, is used. The term $\delta x_t$ biases toward the current state $x_t$. A state change occurs only if another candidate's value exceeds the current state's value by at least $\delta$, promoting stability and allowing iterative refinement to settle.
- Core assumption: Smoother, more deliberate state transitions in the reverse process lead to more coherent and higher-quality final samples, particularly in dynamic settings like uniform diffusion.
- Evidence anchors:
  - [Section 3.1] "That is, the sample $x_{t-1}$ remains the same as $x_t$ unless another candidate exceeds the objective value by at least $\delta$."
  - [Section 4.1, Figure 1] Shows that increasing $\delta$ improves PPL at the expense of entropy, demonstrating a tunable trade-off.
  - [corpus] Weak direct evidence. No corpus papers explicitly discuss delayed switching in this context.
- Break condition: The margin $\delta$ is set too high (locking generation into suboptimal trajectories) or too low (failing to suppress noisy transitions).

### Mechanism 3: Deterministic Mapping Preserves Probability Mass
- Claim: The herding-based denoising process defines a piecewise isometric mapping that preserves probability mass locally, enabling deterministic generation analogous to ODE-based sampling in continuous diffusion.
- Mechanism: The combined system $(x_t, w_t)$ evolves deterministically from an initial random state $(x_T, w_T)$ to a final sample $(x_0, w_0)$. This mapping is piecewise isometric—small perturbations in initial $w_T$ persist without amplification until they cross a threshold that changes the argmax, at which point the trajectory shifts dramatically.
- Core assumption: The deterministic flow induced by this hybrid dynamical system effectively navigates the discrete state space $V^L$, and its piecewise isometric property is a suitable discrete analog to continuous probability flow ODEs.
- Evidence anchors:
  - [abstract] "...deterministic denoising algorithm... driven by weakly chaotic dynamics."
  - [Section 3.1] "This mapping is not one-to-one, and the probability mass is preserved piecewise... these perturbations cause a drastic shift in the trajectory, reflecting the weakly chaotic dynamics."
  - [corpus] Weak direct evidence. Related work on continuous-time discrete diffusion is mentioned but not this specific mapping property.
- Break condition: The piecewise isometry degrades into highly chaotic behavior (loss of directed denoising) or overly static behavior (trapped in non-equilibrium cycles).

## Foundational Learning

- **Discrete-State Diffusion Models (e.g., D3PM, UDLM)**
  - Why needed here: The proposed algorithm is a drop-in replacement for the *reverse denoising process* of these models. Understanding how data is corrupted via a forward Markov chain and reconstructed via a learned reverse Markov chain is essential.
  - Quick check question: How is the transition probability $p_\theta(x_{t-1}|x_t, t)$ in a discrete diffusion model typically obtained and used to generate a sample at step $t-1$?

- **The Herding Algorithm**
  - Why needed here: This is the core technique being adapted. Understanding its original form—how it deterministically generates samples from a target distribution $p$ by maximizing $w_t^\top x$ and updating $w_{t+1} = w_t + p - x_{t+1}$—is critical.
  - Quick check question: In the standard herding algorithm for a categorical distribution, what is the role of the weight vector $w_t$, and why is its boundedness important?

- **Markov Chains and Stationary Distributions**
  - Why needed here: The paper discusses uniform noise, whose forward process has a uniform stationary distribution. The initialization of the herding weight $w_T$ is justified based on the expected state of the herding system under this uniform distribution.
  - Quick check question: For a uniform noise process, what is the stationary distribution of the forward Markov chain, and how does this relate to the initialization of the noisy sample $x_T$ and the weight $w_T$?

## Architecture Onboarding

- **Component map:**
  - Trained Model ($p_\theta$) -> Probability vector $p_{t-1}$ -> Herding Update Rule -> Discrete state $x_{t-1}$ and weight $w_{t-1}$
  - Weight initialization: $w_T \sim \text{Uniform}([0,1]^K)$ for each token
  - State initialization: $x_T \sim \text{Uniform}(V)$ for each token

- **Critical path:**
  1. Initialize $x_T^{(l)}$ from uniform distribution
  2. Initialize $w_T^{(l)}$ from uniform distribution on $[0,1]^K$
  3. For $t$ from $T$ down to $1$: Get model prediction for $p_{t-1}$; Apply delayed-switching herding update for all $L$ tokens in parallel
  4. Final state $x_0^{(1:L)}$ is the generated sample

- **Design tradeoffs:**
  - **Determinism vs. Diversity:** Margin $\delta$ is critical. $\delta=0$ is most deterministic (best perplexity, lower entropy); larger $\delta$ increases diversity. Tuning required per task
  - **Efficiency vs. Quality:** Fewer steps ($T$) needed—128-step herding can match 1024-step stochastic sampling
  - **Drop-in Nature:** No retraining required, but performance depends entirely on pre-trained model quality

- **Failure signatures:**
  - **Mode Collapse/Repetition:** If $\delta$ is too large or dynamics unfavorable, output may get stuck in repetitive patterns
  - **Divergence/Instability:** Implementation bugs could cause $w_t$ to explode despite theoretical boundedness
  - **Degraded Diversity:** Aggressive deterministic denoising may produce overly generic outputs, reducing distributional metrics

- **First 3 experiments:**
  1. **Reproduce Paper Results on text8/LM1B:** Take pre-trained UDLM model, implement herding update (≈20 lines from Appendix A.1), compare generative perplexity and entropy against baseline stochastic sampling for different $T$ and $\delta$
  2. **Ablation on Delayed Switching ($\delta$):** Run herding with $\delta = 0.0, 0.1, 0.2, 0.5$ on text generation; plot perplexity vs. entropy to visualize trade-off curve
  3. **Apply to Combinatorial Optimization (TSP):** Use official DIFUSCO code, replace Bernoulli sampling with herding-based update (Appendix A.2), evaluate on TSP datasets to assess solution quality improvement

## Open Questions the Paper Calls Out
None

## Limitations
- The method's performance depends critically on proper tuning of the margin parameter $\delta$, which varies across datasets, vocabulary sizes, and inference steps
- The herding dynamics, while theoretically bounded, could potentially diverge in implementation if weight updates are not handled carefully
- The deterministic nature may reduce sample diversity compared to stochastic methods, requiring careful $\delta$ selection to balance quality and diversity

## Confidence
- **Method Effectiveness (High):** Clear improvements in perplexity and sample quality metrics across multiple tasks
- **Theoretical Foundation (Medium):** The herding mechanism is well-established, but the piecewise isometric mapping and chaotic dynamics properties need more rigorous proof
- **Implementation Details (Low):** Critical parameters like $\delta$ selection criteria and exact training configurations for pre-trained models are not fully specified

## Next Checks
1. Verify that weight vectors $w_t$ remain bounded during herding updates across multiple inference runs
2. Implement the delayed-switching herding sampler and confirm it produces the same discrete state transitions as the stochastic baseline for $\delta=0$
3. Test the herding method on a small text8 sample with $T=128$ and compare perplexity/entropy trade-off curves for different $\delta$ values