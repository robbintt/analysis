---
ver: rpa2
title: 'TokenPowerBench: Benchmarking the Power Consumption of LLM Inference'
arxiv_id: '2512.03024'
source_url: https://arxiv.org/abs/2512.03024
tags:
- uni00000157
- uni00000156
- energy
- power
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TokenPowerBench introduces the first comprehensive, open-source
  benchmark for measuring and analyzing the power consumption of large language model
  (LLM) inference. The tool provides a modular, extensible framework with phase-aware
  telemetry that captures energy usage at the GPU, node, and system levels without
  requiring specialized power meters.
---

# TokenPowerBench: Benchmarking the Power Consumption of LLM Inference

## Quick Facts
- arXiv ID: 2512.03024
- Source URL: https://arxiv.org/abs/2512.03024
- Authors: Chenxu Niu; Wei Zhang; Jie Li; Yongjian Zhao; Tongyang Wang; Xi Wang; Yong Chen
- Reference count: 11
- Primary result: First comprehensive, open-source benchmark for measuring LLM inference power consumption

## Executive Summary
TokenPowerBench introduces a modular, extensible framework for measuring and analyzing the power consumption of large language model (LLM) inference. The tool provides phase-aware telemetry that captures energy usage at GPU, node, and system levels without requiring specialized power meters. It supports 15+ open-source LLMs ranging from 1B to 405B parameters and enables systematic exploration of how configuration parameters affect energy efficiency.

The benchmark demonstrates that power consumption varies significantly with model size, prompt length, and inference engine choice. Key findings include approximately 30% energy reduction per token through quantization while improving throughput. TokenPowerBench delivers actionable insights for reducing operational costs and meeting sustainability targets in LLM deployments.

## Method Summary
TokenPowerBench employs a phase-aware telemetry approach to measure power consumption during LLM inference across multiple hardware levels. The framework captures energy usage data at GPU, node, and system levels through standard monitoring interfaces, eliminating the need for specialized power meters. The tool is designed to be modular and extensible, supporting various open-source LLMs from 1B to 405B parameters. Configuration parameters such as batch size, context length, parallelism strategy, and quantization can be systematically varied to analyze their impact on energy efficiency. The benchmark provides a comprehensive framework for understanding how different inference settings affect power consumption patterns during LLM deployment.

## Key Results
- Power consumption varies significantly with model size, prompt length, and inference engine choice
- Quantization can reduce energy per token by approximately 30% while improving throughput
- The benchmark enables systematic exploration of configuration parameters affecting energy efficiency

## Why This Works (Mechanism)
TokenPowerBench works by providing granular, phase-aware telemetry that captures power consumption patterns during different stages of LLM inference. The framework's modular design allows it to interface with various monitoring tools at multiple hardware levels (GPU, node, system) without requiring specialized equipment. This approach enables detailed tracking of energy usage throughout the inference pipeline, from initial prompt processing through token generation. The tool's extensibility allows it to adapt to different model architectures and inference engines, while its systematic parameter variation capabilities reveal how specific configuration choices impact overall energy efficiency.

## Foundational Learning
- **Phase-aware telemetry**: Understanding that different inference phases (prompt processing, attention computation, token generation) have distinct power profiles is crucial for accurate measurement. Quick check: Verify that power measurements capture distinct patterns during different inference stages.
- **Hardware-level monitoring**: Knowledge of how to access and interpret power data from GPUs, CPUs, and system components without specialized meters is essential. Quick check: Confirm monitoring interfaces work across different hardware configurations.
- **Quantization impact**: Understanding how reduced precision affects both computational efficiency and energy consumption is key for optimization. Quick check: Measure energy differences between FP16, INT8, and other quantization schemes.
- **Configuration parameter sensitivity**: Recognizing that batch size, context length, and parallelism strategies significantly affect power consumption enables targeted optimizations. Quick check: Systematically vary one parameter at a time to isolate effects.
- **Model architecture scaling**: Understanding how power consumption scales with parameter count helps predict energy costs for different model sizes. Quick check: Plot power vs. parameter count across the supported model range.
- **Inference engine variations**: Different engines (vLLM, TGI, etc.) have distinct power characteristics even for the same model. Quick check: Compare power profiles across multiple inference engines.

## Architecture Onboarding

**Component Map:** TokenPowerBench -> Monitoring Interfaces -> Power Telemetry -> Analysis Engine -> Configuration Manager -> LLM Inference Engine

**Critical Path:** Configuration Manager → LLM Inference Engine → Power Telemetry → Analysis Engine → Results Output

**Design Tradeoffs:** The framework prioritizes accessibility over absolute precision by using standard monitoring interfaces instead of specialized power meters, enabling broader adoption but potentially sacrificing some measurement accuracy. The modular design trades some performance overhead for extensibility across different hardware and software configurations.

**Failure Signatures:** Measurement inaccuracies may occur due to thermal throttling, background system processes, or inconsistent monitoring interface access. Configuration errors can lead to invalid power measurements or crashes during inference. Hardware compatibility issues may prevent proper monitoring interface initialization.

**First Experiments:**
1. Run a small model (1B parameters) with default settings to verify basic functionality and measurement accuracy
2. Vary batch size from 1 to 32 while keeping other parameters constant to observe scaling effects
3. Compare power consumption between FP16 and INT8 quantization for the same model and configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Hardware dependence affects cross-platform comparisons without normalization methodologies
- Limited validation across different GPU architectures (focused on specific hardware setups)
- Quantization energy savings claims need broader validation across different model families and quantization schemes

## Confidence

| Claim | Confidence |
|-------|------------|
| Phase-aware telemetry methodology | High |
| Relative power consumption measurements within same hardware | High |
| Cross-platform power comparisons | Medium |
| First comprehensive open-source LLM inference power benchmark | Medium |
| 30% quantization energy savings | Medium |

## Next Checks
1. Cross-platform validation: Run TokenPowerBench across multiple GPU architectures (e.g., NVIDIA A100, H100, AMD Instinct) to assess hardware dependency and develop normalization methodologies for cross-platform comparisons.
2. Long-duration stability test: Conduct extended inference sessions (hours to days) to evaluate whether power measurement accuracy degrades over time and assess the impact of thermal throttling on measurements.
3. Real-world workload validation: Compare TokenPowerBench measurements against power consumption in actual production LLM services with mixed workloads, varying request patterns, and concurrent model serving to validate enterprise applicability.