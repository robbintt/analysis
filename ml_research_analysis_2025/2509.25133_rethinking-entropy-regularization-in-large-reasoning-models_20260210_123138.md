---
ver: rpa2
title: Rethinking Entropy Regularization in Large Reasoning Models
arxiv_id: '2509.25133'
source_url: https://arxiv.org/abs/2509.25133
tags:
- entropy
- regularization
- arxiv
- exploration
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large reasoning models trained with reinforcement learning suffer
  from entropy collapse and premature convergence, leading to poor exploration and
  degraded performance. The failure of naive entropy regularization stems from the
  vast action space and long trajectories in these models, which cause global entropy
  explosion.
---

# Rethinking Entropy Regularization in Large Reasoning Models

## Quick Facts
- **arXiv ID:** 2509.25133
- **Source URL:** https://arxiv.org/abs/2509.25133
- **Authors:** Yuxian Jiang; Yafu Li; Guanxu Chen; Dongrui Liu; Yu Cheng; Jing Shao
- **Reference count:** 40
- **Primary result:** SIREN achieves +6.6 maj@k improvement on AIME24/25 over previous entropy-based RLVR approaches

## Executive Summary
Large reasoning models trained with reinforcement learning suffer from entropy collapse and premature convergence, leading to poor exploration and degraded performance. The failure of naive entropy regularization stems from the vast action space and long trajectories in these models, which cause global entropy explosion. SIREN addresses this by introducing selective entropy regularization: a two-step masking mechanism that confines exploration to semantically meaningful tokens (via a top-p mask) and critical reasoning positions (via a peak-entropy mask), combined with self-anchored regularization to stabilize training. Across five mathematical benchmarks, SIREN achieves superior average performance over previous entropy-based RLVR approaches, with a +6.6 maj@k improvement on AIME24/25 using Qwen2.5-Math-7B, and demonstrates greater response diversity while maintaining appropriate entropy levels to prevent premature convergence.

## Method Summary
The paper introduces Selective Entropy Regularization (SIREN) to address entropy collapse in large reasoning models during reinforcement learning. The core innovation is a two-step masking mechanism: first, a top-p mask filters out semantically meaningless tokens by retaining only the top 95% probable tokens; second, a peak-entropy mask identifies critical reasoning positions where token entropy exceeds the 1% quantile across all positions. This selective approach confines entropy regularization to meaningful exploration spaces rather than applying it globally across the entire vast action space. Additionally, SIREN employs self-anchored regularization, where the entropy target is set as the average token entropy from the previous training iteration, preventing premature convergence while maintaining stability. The method is evaluated across five mathematical reasoning benchmarks, demonstrating consistent improvements over previous entropy-based RLVR approaches.

## Key Results
- SIREN achieves superior average performance over previous entropy-based RLVR approaches across five mathematical benchmarks
- On AIME24/25, SIREN delivers a +6.6 maj@k improvement using Qwen2.5-Math-7B
- The method demonstrates greater response diversity while maintaining appropriate entropy levels to prevent premature convergence

## Why This Works (Mechanism)
SIREN works by addressing the fundamental mismatch between global entropy regularization and the structure of reasoning tasks. In long reasoning trajectories with vast action spaces, global entropy regularization fails because it attempts to maintain diversity across all possible token combinations, leading to entropy explosion. The selective masking mechanism solves this by focusing regularization only on semantically meaningful tokens (top-p=0.95) and critical reasoning positions (peak-entropy=0.01 quantile), effectively reducing the exploration space to manageable dimensions. The self-anchored regularization provides dynamic adaptation by setting entropy targets based on previous iteration statistics, creating a moving baseline that prevents premature convergence while maintaining training stability.

## Foundational Learning

**Reinforcement Learning from Verifiable Rewards (RLVR)**: A training paradigm where models learn through trial-and-error with feedback from verifiable correctness signals. Needed to understand why traditional RLVR struggles with reasoning tasks. Quick check: Does the reward function provide step-by-step feedback or only final answer verification?

**Entropy Regularization in RL**: A technique to encourage exploration by penalizing low-entropy policies. Needed to understand why naive application fails in reasoning contexts. Quick check: How does entropy scale with trajectory length and action space size?

**Action Space Explosion**: The combinatorial growth of possible token sequences in language models. Needed to understand why global entropy regularization becomes ineffective. Quick check: What is the effective vocabulary size after top-p masking versus full vocabulary?

**Peak-Entropy Detection**: Identifying positions in a sequence where uncertainty is highest. Needed to understand the critical reasoning positions mask. Quick check: How does peak-entropy position identification correlate with actual reasoning difficulty?

## Architecture Onboarding

**Component Map**: Base LM -> Reinforcement Learning Loop -> Selective Entropy Regularization (Top-p Mask + Peak-Entropy Mask + Self-anchored Target) -> Policy Update

**Critical Path**: Token generation → Entropy calculation → Selective masking → Regularization application → Policy gradient update

**Design Tradeoffs**: The paper trades computational simplicity (global regularization) for effectiveness (selective masking). While selective masking adds computational overhead through entropy calculations and masking operations, it enables successful training where global approaches fail. The choice of static thresholds (top-p=0.95, peak-entropy=0.01) versus dynamic adaptation represents another tradeoff between simplicity and optimal performance.

**Failure Signatures**: Entropy collapse manifests as premature convergence to suboptimal solutions with low diversity. Global entropy explosion appears as unstable training with high variance. Selective masking failures would show either insufficient exploration (too restrictive masks) or continued instability (masks too permissive).

**First Experiments**: 1) Compare training curves with global vs selective entropy regularization on a simple reasoning task. 2) Ablation study removing top-p mask, peak-entropy mask, or self-anchored target individually. 3) Sensitivity analysis varying top-p and peak-entropy thresholds to identify optimal values.

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations

- The analysis could be more rigorous in quantifying how trajectory length and vocabulary size interact to cause entropy regularization failure
- Specific thresholds chosen (top-p=0.95, peak-entropy=0.01) appear somewhat arbitrary without comprehensive ablation studies
- Comparison only includes two baseline methods (REINFORCE and PPO), potentially missing other relevant entropy-regularized approaches

## Confidence

**High confidence** in the empirical observation that naive entropy regularization causes entropy collapse in long-trajectory reasoning tasks.

**Medium confidence** in the effectiveness of the SIREN approach, as results show consistent improvements but evaluation could benefit from more diverse baseline comparisons.

**Medium confidence** in the theoretical explanation of why global entropy regularization fails, as the paper provides intuitive reasoning but lacks formal proofs or comprehensive ablation studies.

## Next Checks

1. Conduct ablation studies varying the top-p threshold (e.g., 0.90, 0.95, 0.99) and peak-entropy threshold (e.g., 0.001, 0.01, 0.1) to determine optimal values and test robustness of the selective masking approach.

2. Compare SIREN against additional entropy-regularized RLVR baselines from recent literature, including approaches that use dynamic or adaptive entropy coefficients, to establish whether the selective masking mechanism provides unique advantages beyond coefficient tuning.

3. Analyze the computational overhead introduced by the selective entropy regularization mechanism and evaluate whether the performance gains justify the additional complexity, particularly in comparison to simpler regularization strategies.