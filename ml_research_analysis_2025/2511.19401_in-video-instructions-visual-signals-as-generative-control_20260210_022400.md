---
ver: rpa2
title: 'In-Video Instructions: Visual Signals as Generative Control'
arxiv_id: '2511.19401'
source_url: https://arxiv.org/abs/2511.19401
tags:
- video
- instructions
- arxiv
- instruction
- in-video
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces In-Video Instructions, a method for controllable
  video generation by embedding visual guidance directly into video frames. Instead
  of relying on text prompts, users provide instructions via overlaid text, arrows,
  or trajectories on the first frame, enabling spatially grounded control over object
  motion and camera behavior.
---

# In-Video Instructions: Visual Signals as Generative Control

## Quick Facts
- arXiv ID: 2511.19401
- Source URL: https://arxiv.org/abs/2511.19401
- Authors: Gongfan Fang; Xinyin Ma; Xinchao Wang
- Reference count: 40
- Primary result: Zero-shot visual instruction method for controllable video generation across multiple models

## Executive Summary
This paper introduces In-Video Instructions, a method for controllable video generation by embedding visual guidance directly into video frames. Instead of relying on text prompts, users provide instructions via overlaid text, arrows, or trajectories on the first frame, enabling spatially grounded control over object motion and camera behavior. The approach works zero-shot across multiple state-of-the-art models including Veo 3.1, Kling 2.5, and Wan 2.2. Experiments show the method reliably interprets embedded instructions, particularly in multi-object scenarios, achieving higher success rates than text-only prompts (e.g., 95.8% vs. 58.3% for stopping actions in complex scenes).

## Method Summary
In-Video Instructions embeds visual control signals directly onto the first frame of a video. Users can place text labels, draw arrows, or sketch trajectories to specify object motions, actions, or camera movements. The method operates as a post-processing step that adds these annotations before passing the frame to existing video generation models. It works across different architectures without requiring model retraining, functioning as a zero-shot approach that leverages the models' existing text recognition capabilities. The visual instructions provide spatial grounding and explicit control over both content and camera behavior, with experiments showing effectiveness in single and multi-object scenarios, sequential instructions, and independent object control.

## Key Results
- Achieves 95.8% success rate for stopping actions in multi-object scenes versus 58.3% with text-only prompts
- Works zero-shot across Veo 3.1, Kling 2.5, and Wan 2.2 models without model modification
- Enables fine-grained control over object motion, sequential multi-object instructions, and camera manipulation through visual annotations

## Why This Works (Mechanism)
The method exploits existing text recognition capabilities in video generation models by embedding instructions directly in the visual input rather than relying on separate text prompts. Models already trained to recognize and respond to text in images can interpret these embedded instructions when they appear in the first frame. This approach provides spatial grounding that text-only prompts lack, allowing users to specify which objects should perform which actions and where. The visual format also enables intuitive control over camera movements through trajectory annotations. By working within the model's existing capabilities rather than requiring architectural changes, the method achieves zero-shot functionality across different models.

## Foundational Learning

**Video Generation Models** - Understanding how modern models like Veo, Kling, and Wan generate videos from text or image inputs is essential for appreciating how embedded instructions can influence outputs. *Quick check*: Review the training objectives and architectures of these models to understand their text processing capabilities.

**Text Recognition in Images** - Modern computer vision models inherently process text within images as part of their training data. *Quick check*: Examine how pre-trained vision models handle text in images and whether they can reliably extract meaning from embedded text.

**Zero-Shot Learning** - The ability to apply techniques across different models without retraining is crucial for practical deployment. *Quick check*: Understand the limitations and capabilities of zero-shot approaches in computer vision tasks.

## Architecture Onboarding

**Component Map**: Input Frame -> Annotation Overlay -> Text Recognition Module -> Motion Generation -> Output Video

**Critical Path**: The method's effectiveness depends on the text recognition module's ability to parse embedded instructions and the motion generation component's capacity to translate these into coherent video sequences.

**Design Tradeoffs**: 
- Pros: Zero-shot functionality, intuitive visual interface, spatial grounding
- Cons: Persistent visual artifacts, potential text-content overlap issues, limited to first-frame guidance

**Failure Signatures**: Instructions may be ignored if text recognition fails, spatial relationships may be misinterpreted, or models may generate artifacts around annotation regions.

**First Experiments**:
1. Test single-object motion control with simple text instructions on each model
2. Evaluate multi-object independent instructions with sequential commands
3. Assess camera control through trajectory annotations in static scenes

## Open Questions the Paper Calls Out
None

## Limitations
- Lacks quantitative metrics beyond success rates for evaluating video quality, temporal coherence, and artifact presence
- Relies on synthetic datasets that may not fully represent real-world complexity or model generalization
- Persistent visual artifacts from text-based annotations limit practical applicability where clean frames are required

## Confidence
- **High Confidence**: Zero-shot functionality across multiple models and reliable interpretation of embedded instructions
- **Medium Confidence**: Higher success rates compared to text-only prompts based on success rate metrics
- **Low Confidence**: Claims about simplicity and intuitiveness lack user study validation

## Next Checks
1. Conduct user studies to evaluate usability, intuitiveness, and effectiveness compared to text-only prompts and other visual control interfaces
2. Develop and apply quantitative metrics for video quality (e.g., Fr√©chet Video Distance) to assess visual fidelity and temporal coherence
3. Test the method on longer videos and diverse real-world datasets to evaluate temporal consistency and scalability