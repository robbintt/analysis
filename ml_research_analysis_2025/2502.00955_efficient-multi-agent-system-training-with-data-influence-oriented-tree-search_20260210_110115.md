---
ver: rpa2
title: Efficient Multi-Agent System Training with Data Influence-Oriented Tree Search
arxiv_id: '2502.00955'
source_url: https://arxiv.org/abs/2502.00955
tags:
- data
- uni00000013
- uni00000048
- influence
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DITS, a data influence-oriented framework
  for training LLM-based multi-agent systems, addressing the misalignment between
  Q-value-based data selection and actual model performance improvement. DITS integrates
  influence scores into MCTS-based data synthesis, prioritizing synthetic data that
  most significantly enhances system performance rather than relying solely on Q-values.
---

# Efficient Multi-Agent System Training with Data Influence-Oriented Tree Search

## Quick Facts
- arXiv ID: 2502.00955
- Source URL: https://arxiv.org/abs/2502.00955
- Reference count: 22
- One-line primary result: DITS achieves 2.1%-2.5% average performance improvements over state-of-the-art methods across eight datasets by prioritizing synthetic data based on influence scores rather than Q-values.

## Executive Summary
DITS introduces a data influence-oriented framework for training LLM-based multi-agent systems that addresses the misalignment between Q-value-based data selection and actual model performance improvement. The framework integrates influence scores into MCTS-based data synthesis, prioritizing synthetic data that most significantly enhances system performance. By deriving an efficient method to estimate influence scores for non-differentiable metrics through inference computations rather than gradient-based approaches, DITS substantially reduces computational overhead while achieving state-of-the-art performance.

## Method Summary
DITS addresses the misalignment between Q-value-based data selection and actual model performance improvement in multi-agent systems by integrating influence scores into MCTS-based data synthesis. The framework generates preference pairs via MCTS, estimates influence scores using single-step gradient descent perturbations with LoRA, combines influence scores with Q-values for ranking, and selects top candidates for training with SFT and DPO. The iterative approach trains from the same initial model but with data synthesized by increasingly capable policies, leading to higher average influence scores and improved final performance.

## Key Results
- DITS achieves 2.1%-2.5% average performance improvements over existing methods across eight datasets
- The framework demonstrates efficient scaling of synthesis computation within the same data synthesis budget
- Experiments show weak correlation (<0.2) between DPO loss and performance metrics, validating the need for influence-based selection

## Why This Works (Mechanism)

### Mechanism 1: Influence-Guided Data Selection Over Q-Value Selection
Q-values estimate expected reward for actions but do not account for training utility; influence scores approximate the marginal impact of a data point on validation-set performance via one-step gradient descent perturbation. This exposes pairs where preferred vs. rejected responses differ more meaningfully for learning. The finite-difference approximation with single-step gradient descent using LoRA captures enough signal about data influence for non-differentiable metrics.

### Mechanism 2: Gradient-to-Inference Conversion for Efficient Influence Estimation
Estimating influence via inference on a validation set with a one-step parameter perturbation is more compute-efficient than full gradient-based influence while still improving selection quality. This replaces classical Hessian-based influence with a perturbed forward pass, avoiding billion-parameter Hessian inverses while using the same inference budget that would otherwise go to additional rollouts.

### Mechanism 3: Iterative Synthesis with Quality Amplification
Iteratively training the model and regenerating synthetic data increases average influence scores and improves final performance. Each iteration trains from the same initial model but with data synthesized by a stronger policy; as model capability improves, it explores higher-value regions of the action space, producing data with higher mean influence.

## Foundational Learning

- Concept: Influence functions for data valuation
  - Why needed here: To understand how DITS re-purposes classical influence ideas via finite-difference approximations for non-differentiable metrics
  - Quick check question: Given a validation metric that cannot be differentiated, how would you approximate the influence of a training point without computing a Hessian inverse?

- Concept: Monte Carlo Tree Search (Selection, Expansion, Simulation, Backpropagation)
  - Why needed here: The base synthesis process is MCTS over multi-agent action sequences; influence scores replace/augment Q-values for selection
  - Quick check question: In DITS, during which MCTS phase does the influence score operate, and what does it replace?

- Concept: Direct Preference Optimization (DPO) basics
  - Why needed here: DITS constructs preference pairs (chosen vs. rejected actions) and trains with DPO; understanding the loss clarifies why DPO loss correlates poorly with downstream metrics
  - Quick check question: Why might minimizing DPO loss not align with improving a downstream F1 score, and how does influence score address this?

## Architecture Onboarding

- Component map: Task sampler → MCTS-based synthesis (Selection/Expansion/Simulation/Backprop) → candidate preference pairs → Influence estimator (validation set forward passes with one-step perturbation) → per-pair influence scores → Scoring combiner (influence + γ·Q) → ranked pairs → Selection filter (Top-α) → training set → Trainer (SFT + DPO) → updated model → Optional: iterate back to synthesis with stronger model

- Critical path: 1) Generate candidate pairs via MCTS (Eq. 2–5), 2) Estimate influence (Eq. 14) on validation set with LoRA one-step perturbation, 3) Combine scores (Eq. 15), select Top-α, train with DPO, 4) If iterating, repeat synthesis with improved model (Algorithm 1)

- Design tradeoffs: Influence-only (γ=0) vs. influence+Q (γ=1): Information Exchange benefits from influence-only; Debate benefits from combined—likely due to metric noise in F1 vs. exact match. Selection ratio α: Too low reduces training diversity; too high includes noise. Perturbation scale (ηϵ): Larger perturbations increase signal but risk instability; too small yields noise.

- Failure signatures: Flat or near-zero influence variance: likely validation set too small or metric too sparse; consider proxy metrics or curriculum. Performance degrades at low α: training data insufficient; increase α or augment with random samples. Influence estimation unstable: check LoRA learning rate and perturbation scale; validate on a held-out subset.

- First 3 experiments: 1) Baseline comparison (Q-value selection vs. influence-only vs. combined) on a single Information Exchange dataset (e.g., HotpotQA) with fixed α=0.5, 2) Ablate perturbation scale (ηϵ) to assess influence estimation stability; monitor influence variance and downstream F1, 3) Sweep selection ratio α (0.1–0.7) on two datasets; identify optimal α and check if it generalizes across tasks.

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research, including further exploring the application of data influence scores to influence the selection step within the tree search process to more efficiently identify beneficial training data. The authors also acknowledge the need to investigate how iterative concentration of high influence scores affects data diversity and whether this leads to model collapse or reduced generalization in downstream tasks.

## Limitations
- The framework relies on finite-difference approximation for influence estimation, which may become unstable for highly discrete or sparse metrics
- Iterative synthesis may lead to concentration of high influence scores at the cost of reduced data diversity
- The method requires careful hyperparameter tuning for perturbation scale, selection ratio, and the influence-Q value combination weight

## Confidence
High: The experimental results demonstrate consistent performance improvements across multiple datasets and tasks, with clear quantitative metrics reported.
Medium: Some implementation details such as exact hyperparameter values for influence estimation and validation set configuration are not fully specified.
Low: The correlation between DPO loss and task metrics is weak (<0.2), suggesting potential misalignment in the training objective.

## Next Checks
1. Verify the correlation between estimated influence scores and actual validation performance improvements by measuring the correlation coefficient across different datasets
2. Test the sensitivity of the framework to validation set size by comparing results using different validation set sizes (e.g., 50 vs. 500 samples)
3. Evaluate the diversity of generated synthetic data across iterations by measuring distinct n-grams or semantic entropy to assess potential model collapse risks