---
ver: rpa2
title: Extending Information Bottleneck Attribution to Video Sequences
arxiv_id: '2501.16889'
source_url: https://arxiv.org/abs/2501.16889
tags:
- maps
- xception
- deepfake
- video
- viba
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work extends the Information Bottleneck Attribution (IBA)
  method to video sequences, creating VIBA, a framework for explainable video classification.
  The authors adapt IBA to two architectures: Xception for spatial features and a
  VGG11-based model for motion dynamics via optical flow.'
---

# Extending Information Bottleneck Attribution to Video Sequences

## Quick Facts
- arXiv ID: 2501.16889
- Source URL: https://arxiv.org/abs/2501.16889
- Authors: Veronika Solopova; Lucas Schmidt; Dorothea Kolossa
- Reference count: 35
- Key outcome: VIBA extends IBA to video sequences for explainable deepfake detection, achieving near-identical performance to baseline models while providing interpretable spatial and temporal explanations

## Executive Summary
This paper introduces VIBA, an extension of the Information Bottleneck Attribution (IBA) method to video classification tasks. The framework injects bottleneck layers into convolutional neural networks (Xception and VGG11-based architectures) to generate interpretable relevance and optical flow maps for explainable deepfake detection. The approach maintains high classification accuracy (81.47% for Xception, 79.87% for VGG11) while providing spatially and temporally consistent explanations. The authors construct a diverse deepfake detection dataset and demonstrate that their method produces explanations with high consistency metrics, though moderate alignment with human annotations suggests room for improvement in capturing human intuition.

## Method Summary
VIBA adapts the IBA method to video sequences by injecting bottleneck layers into convolutional neural networks. The framework processes spatial features using Xception and motion dynamics using a VGG11-based model with optical flow inputs. Bottlenecks are inserted at earlier convolutional layers to capture fine-grained features while maintaining model performance. The approach generates relevance maps for spatial features and optical flow maps for motion information, providing interpretable explanations for video classification decisions. The method is evaluated on a constructed deepfake detection dataset, demonstrating near-identical performance to baseline models while producing consistent and interpretable attribution maps.

## Key Results
- VIBA achieves 81.47% accuracy for Xception and 79.87% for VGG11-based models, nearly identical to baseline performance
- Explanation consistency metrics show high agreement (0.77 for Xception, 0.83 for VGG11) across consecutive frames
- F1 scores comparing automated explanations to human annotations are moderate (45.42% for Xception, 52.72% for VGG11)
- Saliency maps demonstrate temporal and spatial consistency in generated explanations

## Why This Works (Mechanism)
The method works by injecting bottleneck layers into convolutional networks to compress information while preserving task-relevant features. For spatial features, the Xception model processes frames directly, while motion dynamics are captured through optical flow inputs to the VGG11-based model. The bottleneck layers force the network to retain only the most discriminative features for classification, which can then be extracted as interpretable maps. This compression at early layers captures fine-grained spatial and temporal patterns while maintaining classification accuracy. The approach leverages the information bottleneck principle to balance compression and relevance, producing explanations that are both interpretable and aligned with model predictions.

## Foundational Learning
- **Information Bottleneck Principle**: Balances compression and relevance by forcing networks to retain only task-relevant information
  - Why needed: Enables generation of interpretable explanations while preserving classification performance
  - Quick check: Verify that bottleneck insertion doesn't degrade accuracy beyond acceptable thresholds

- **Optical Flow Computation**: Captures motion dynamics between consecutive video frames
  - Why needed: Motion information is crucial for detecting temporal inconsistencies in deepfakes
  - Quick check: Ensure optical flow quality is sufficient for downstream motion analysis

- **Convolutional Neural Networks**: Extract spatial features from video frames
  - Why needed: Standard architecture for video classification tasks
  - Quick check: Validate that feature extraction captures relevant spatial patterns

- **Attribution Methods**: Generate interpretable explanations from model decisions
  - Why needed: Provides insight into model reasoning for classification
  - Quick check: Compare attribution maps with human annotations for consistency

## Architecture Onboarding

Component Map: Video frames -> Xception (spatial features) + Optical flow -> VGG11 (motion features) -> Bottleneck layers -> Classification output

Critical Path: Input video → Frame processing (Xception) + Motion processing (VGG11) → Bottleneck injection → Feature compression → Classification → Attribution map generation

Design Tradeoffs: Early bottleneck insertion captures fine-grained features but may reduce performance if too aggressive; optical flow adds computational overhead but captures essential motion information; dual architecture approach handles both spatial and temporal features but increases model complexity

Failure Signatures: Degraded accuracy from over-aggressive bottleneck compression; poor motion capture from low-quality optical flow; inconsistent explanations across frames indicating temporal instability

Three First Experiments:
1. Test bottleneck layer placement at different depths to find optimal balance between explanation quality and performance
2. Evaluate attribution map quality on synthetic deepfake datasets with known manipulation patterns
3. Compare explanation consistency across different video compression levels and frame rates

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Moderate F1 scores (45.42% for Xception, 52.72% for VGG11) indicate explanations may not fully align with human intuition about deepfake detection
- Small accuracy differences between baseline and VIBA models may not generalize across different datasets or architectures
- Reliance on optical flow introduces computational overhead and potential failure modes in motion-invariable tasks

## Confidence

High confidence claims:
- Performance preservation: Near-identical accuracy scores between baseline and VIBA models are well-documented

Medium confidence claims:
- Explanation consistency: High consistency metrics demonstrated, but moderate alignment with human annotations suggests explanations may not fully capture human reasoning

Low confidence claims:
- Generalizability: Approach validated only on deepfake detection with specific architectures, limiting confidence in broader applicability

## Next Checks

1. Test VIBA on additional video classification tasks beyond deepfake detection (e.g., action recognition, anomaly detection) to assess generalizability across domains and verify if the bottleneck injection approach consistently produces meaningful explanations.

2. Conduct user studies with domain experts to evaluate whether the generated relevance and optical flow maps align with expert knowledge about deepfake detection features, moving beyond the current human annotation comparison.

3. Perform extensive ablation studies across different bottleneck layer positions, network architectures, and video types to determine optimal configurations and identify failure modes where the attribution method may produce misleading explanations.