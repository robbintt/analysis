---
ver: rpa2
title: 'SciML Agents: Write the Solver, Not the Solution'
arxiv_id: '2509.09936'
source_url: https://arxiv.org/abs/2509.09936
tags:
- code
- reasoning
- dataset
- solver
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SciML Agents, a framework for using large
  language models (LLMs) to generate scientific computing code rather than directly
  predicting solutions. The authors propose shifting from learning solution functions
  to leveraging decades of numerical algorithms via code generation.
---

# SciML Agents: Write the Solver, Not the Solution

## Quick Facts
- arXiv ID: 2509.09936
- Source URL: https://arxiv.org/abs/2509.09936
- Reference count: 40
- Primary result: LLMs can generate executable scientific code for simple ODEs with high executability and numerical validity, often without fine-tuning when given sufficient context

## Executive Summary
This paper introduces SciML Agents, a framework that shifts from having LLMs directly predict scientific computing solutions to generating code that leverages established numerical algorithms. The approach addresses the challenge of complex, multi-step reasoning required in scientific computing by utilizing decades of existing algorithmic knowledge rather than attempting to learn solution functions directly. The authors demonstrate that modern LLMs can reliably generate executable scientific code for ordinary differential equations, establishing a new paradigm for evaluating scientific code generation beyond simple syntactic correctness.

## Method Summary
The authors propose a framework where LLMs generate scientific computing code instead of predicting solutions directly. They introduce two key datasets: a diagnostic dataset with adversarial "misleading" problems designed to test symbolic reasoning and solver selection, and ODE-1000, a large-scale benchmark of 1,000 diverse ODE tasks. The evaluation methodology measures both executability (whether the generated code runs) and numerical validity (whether results match reference solutions). The approach leverages instruction-following models' strong in-context learning capabilities, with fine-tuning shown to significantly improve performance for smaller or older models.

## Key Results
- Newer instruction-following models achieve high executability and numerical validity on ODE problems without fine-tuning when given sufficient context
- Fine-tuning significantly improves performance for smaller or older models
- The framework demonstrates that careful prompting and fine-tuning can yield specialized LLM agents capable of reliably solving simple ODE problems
- SciML Agents establish a foundation for evaluating scientific code generation beyond syntactic correctness

## Why This Works (Mechanism)
The framework works by leveraging the extensive repository of numerical algorithms developed over decades in scientific computing, rather than requiring LLMs to learn complex solution functions directly. By generating code that calls established solvers, the approach circumvents the need for LLMs to perform intricate multi-step reasoning and symbolic manipulation themselves. The success stems from modern LLMs' strong code generation capabilities combined with their ability to understand and utilize scientific computing libraries and APIs when provided with appropriate context and guidance.

## Foundational Learning
- **Numerical ODE solvers**: Understanding different numerical methods (Euler, Runge-Kutta, etc.) is crucial for selecting appropriate algorithms based on problem characteristics
  - Why needed: Different ODE problems require different numerical approaches for stability and accuracy
  - Quick check: Can the model distinguish between stiff and non-stiff problems?

- **Scientific computing libraries**: Familiarity with libraries like SciPy, NumPy, and specialized ODE solvers
  - Why needed: Code generation must interface with existing scientific computing infrastructure
  - Quick check: Does generated code correctly import and use required libraries?

- **Problem formulation**: Ability to parse mathematical problem descriptions into computational representations
  - Why needed: Natural language descriptions must be translated into executable code
  - Quick check: Can the model correctly identify initial conditions and parameters?

- **Error handling and validation**: Understanding of numerical precision, convergence criteria, and result verification
  - Why needed: Scientific computing requires rigorous validation of results
  - Quick check: Does the generated code include appropriate error checking?

## Architecture Onboarding

**Component Map**: Problem Description -> Code Generation -> Solver Selection -> Numerical Solution -> Validation

**Critical Path**: The most critical path is from problem description through code generation to solver selection, as incorrect parsing or inappropriate solver choice will propagate errors through the entire pipeline.

**Design Tradeoffs**: The framework trades direct solution prediction for code generation, leveraging existing algorithms rather than learning solution functions. This approach benefits from decades of algorithmic development but requires more complex generation tasks from the LLM.

**Failure Signatures**: Common failures include incorrect library imports, inappropriate solver selection for problem type, syntax errors in generated code, and misinterpretation of mathematical notation or initial conditions.

**Three First Experiments**:
1. Test code generation on increasingly complex ODE problems, from simple linear to nonlinear systems
2. Evaluate the impact of context window size and prompt specificity on generation quality
3. Compare performance of different LLM architectures (instruction-following vs. base models) on the same problem sets

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the generalizability of the approach beyond ODEs, the optimal balance between prompting strategies and fine-tuning, and the development of more sophisticated evaluation metrics that assess code efficiency and numerical stability alongside accuracy.

## Limitations
- The ODE-1000 benchmark represents only a narrow slice of scientific computing challenges, focusing primarily on ordinary differential equations
- The evaluation methodology, while rigorous, may not fully capture the nuanced correctness of generated code across diverse scientific domains
- The success of instruction-following models without fine-tuning suggests strong in-context learning capabilities, but generalizability to more complex scientific problems remains unproven

## Confidence

**High confidence**: The core demonstration that LLMs can generate executable scientific code for simple ODE problems is well-supported by the empirical results. The comparative performance across different model families is clearly documented.

**Medium confidence**: The assertion that this approach represents a fundamental shift from solution learning to algorithm leveraging is conceptually sound but requires broader validation beyond the ODE domain.

**Medium confidence**: The claim about fine-tuning benefits for smaller/older models is supported, but the relative effectiveness compared to prompt engineering strategies needs further exploration.

## Next Checks

1. Evaluate SciML Agents on a broader spectrum of scientific computing problems including partial differential equations, stochastic differential equations, and constrained optimization problems to assess generalizability beyond ODEs.

2. Implement ablation studies systematically varying context length, prompt specificity, and fine-tuning strategies to quantify their relative contributions to model performance across different LLM architectures.

3. Develop and apply more sophisticated evaluation metrics that assess not just numerical accuracy but also code efficiency, numerical stability, and adherence to scientific computing best practices.