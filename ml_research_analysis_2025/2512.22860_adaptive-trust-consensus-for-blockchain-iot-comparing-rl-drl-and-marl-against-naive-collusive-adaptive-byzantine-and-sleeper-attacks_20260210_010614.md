---
ver: rpa2
title: 'Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against
  Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks'
arxiv_id: '2512.22860'
source_url: https://arxiv.org/abs/2512.22860
tags:
- attack
- trust
- learning
- against
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates three reinforcement learning approaches for
  defending blockchain IoT networks against five attack types: Naive Malicious Attack
  (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine
  Fault Injection (BFI), and Time-Delayed Poisoning (TDP). The study compares tabular
  Q-learning, Deep RL with Dueling Double DQN, and Multi-Agent RL under identical
  simulation conditions on a 16-node network.'
---

# Adaptive Trust Consensus for Blockchain IoT: Comparing RL, DRL, and MARL Against Naive, Collusive, Adaptive, Byzantine, and Sleeper Attacks

## Quick Facts
- arXiv ID: 2512.22860
- Source URL: https://arxiv.org/abs/2512.22860
- Reference count: 29
- Primary result: MARL achieves F1=0.85 against coordinated attacks, but all approaches catastrophically fail against Time-Delayed Poisoning (F1=0.11-0.16)

## Executive Summary
This paper evaluates three reinforcement learning approaches for defending blockchain IoT networks against five attack types: Naive Malicious Attack (NMA), Collusive Rumor Attack (CRA), Adaptive Adversarial Attack (AAA), Byzantine Fault Injection (BFI), and Time-Delayed Poisoning (TDP). The study compares tabular Q-learning, Deep RL with Dueling Double DQN, and Multi-Agent RL under identical simulation conditions on a 16-node network. Results show that DRL achieves the highest average detection performance (F1=0.77), while MARL excels against coordinated attacks (F1=0.85 vs DRL's 0.68 against CRA). All agents achieve perfect detection against Byzantine attacks (F1=1.00). Most critically, all approaches fail catastrophically against Time-Delayed Poisoning after sleeper activation, with F1 scores dropping to 0.11-0.16, revealing a fundamental vulnerability in trust-based systems. The research demonstrates that coordinated learning provides measurable advantages against coordinated attacks but that temporal trust-building attacks represent a critical limitation requiring dedicated countermeasures.

## Method Summary
The paper implements a 16-node blockchain IoT network with Bayesian trust-based consensus. Nodes are initialized with Beta(8.0, 8.0) trust distributions and Gaussian noise (σ=0.12). The system uses Thompson Sampling for delegate selection and evaluates three RL approaches: tabular Q-learning with 10/5 bins discretization, Deep RL with Dueling Double DQN ([128,64] hidden layers, Adam lr=5e-4), and Multi-Agent RL with 16 agents sharing neural network parameters synchronized every 10 steps. The 16-dimensional state vector includes trust statistics and operational metrics, with three discrete actions adjusting delegation ratios (×0.9, 1.0, 1.1). Five attack types are simulated: NMA (random malicious behavior), CRA (coordinated trust inflation), AAA (adaptive probing), BFI (malicious consensus injection), and TDP (sleeper agents that activate after trust accumulation). Performance is measured using F1-score, precision, recall, and cumulative reward over 50-100 episodes of 100 steps each.

## Key Results
- DRL achieves highest average detection performance (F1=0.77) across all attack types
- MARL excels against coordinated attacks (F1=0.85 vs DRL's 0.68 against CRA)
- All approaches achieve perfect detection against Byzantine attacks (F1=1.00)
- All approaches catastrophically fail against Time-Delayed Poisoning after sleeper activation (F1=0.11-0.16)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-Agent Reinforcement Learning (MARL) provides superior detection of coordinated collusion (CRA) compared to single-agent approaches.
- **Mechanism:** By deploying independent learning agents at each node with shared neural network parameters, the system creates a distributed observation grid. While a single agent (DRL) might interpret coordinated trust inflation as legitimate consensus, MARL agents collectively identify anomalies through parameter sharing, allowing the policy to learn from diverse local perspectives of the same global attack.
- **Core assumption:** The attack leaves statistical signatures (e.g., compressed trust distribution) that are detectable when observed from multiple agent perspectives but may appear benign locally.
- **Evidence anchors:**
  - [abstract]: "MARL achieves superior detection under collusive attacks (F1=0.85) compared to DRL (0.68) and RL (0.50)."
  - [Section 4.3.3]: "This approach proves particularly effective against coordinated attacks (CRA, BFI) where single-agent methods may be overwhelmed by synchronized adversarial behavior."
  - [corpus]: Weak direct support; neighbor papers (e.g., "SkyTrust") focus on trust dynamics but do not validate MARL specifically for collusion in IoT blockchain.
- **Break condition:** If communication latency prevents timely parameter synchronization, or if attackers mimic honest behavior distributions perfectly, the coordination advantage degrades.

### Mechanism 2
- **Claim:** Fully Homomorphic Encryption with ABAC (FHE-ABAC) mitigates Adaptive Adversarial Attacks (AAA) by obscuring the policy decision boundary.
- **Mechanism:** AAA relies on probing the system to learn defense patterns. FHE-ABAC evaluates access policies on encrypted attributes, meaning the adversary cannot observe which specific attributes or trust thresholds triggered a rejection. This informational asymmetry prevents the attacker from constructing a reliable gradient of the defense mechanism to exploit.
- **Core assumption:** The primary vector for AAA is policy inference; if the attacker shifts to network-level denial of service, this mechanism is insufficient.
- **Evidence anchors:**
  - [Section 4.2]: "By hiding both attributes and policy logic, FHE-secured ABAC prevents adversaries from inferring the decision boundary through repeated probing."
  - [Section 3.2]: "Even if an adversary intercepts the encrypted policy evaluation, they cannot determine which attributes triggered acceptance or rejection."
  - [corpus]: "FIDELIS" validates blockchain protection against poisoning but does not specifically address FHE for policy hiding.
- **Break condition:** If side-channel attacks leak decryption keys or computational overhead creates timing side-channels.

### Mechanism 3
- **Claim:** Deep Reinforcement Learning (DRL) with continuous state representation handles dynamic Byzantine and Adaptive attacks better than tabular RL.
- **Mechanism:** Tabular RL requires discretizing the 16-dimensional state space (e.g., trust mean, variance), losing the nuance required to detect subtle adaptive attacks. DRL (Dueling Double DQN) learns continuous representations, allowing it to distinguish between legitimate trust fluctuations and malicious strategic shifts (e.g., AAA's "slow poisoning").
- **Core assumption:** The neural network architecture is sufficiently deep to capture the non-linear relationships in trust dynamics without overfitting.
- **Evidence anchors:**
  - [Section 7.3.1]: "The discretized state space loses critical information needed to distinguish attack-induced trust patterns... RL fails significantly against adaptive and coordinated threats."
  - [Section 7.3.2]: "DRL... enable[s] DRL to recognize subtle attack signatures that escape tabular methods."
  - [corpus]: "Survey on Strategic Mining" discusses RL for blockchain but highlights scalability issues with tabular methods, supporting the shift to DRL.
- **Break condition:** If the state space dimensionality increases significantly without architectural changes, DRL may suffer from sample inefficiency.

## Foundational Learning

- **Concept: Bayesian Trust & Thompson Sampling**
  - **Why needed here:** The system does not use static reputation scores. It models trust as a Beta distribution ($\alpha, \beta$). You must understand that Thompson Sampling selects delegates based on *sampling* these distributions (balancing exploration of uncertain nodes vs. exploiting known good nodes) to grasp why the system resists simple Sybil attacks but fails against long-term sleeper agents.
  - **Quick check question:** "Why would the system select a node with a lower average trust score over one with a higher score?" (Answer: High uncertainty in the lower-scored node prompts exploration).

- **Concept: The Dueling Network Architecture (D3QN)**
  - **Why needed here:** The paper specifies Dueling Double DQN. You need to understand that this architecture separates the estimation of the *state value* $V(s)$ (how good is the current network health?) from the *action advantage* $A(s,a)$ (how much better is shrinking the delegation pool right now?). This stabilizes learning when the action impact is sparse.
  - **Quick check question:** "In a stable network, which stream changes more rapidly: the Value stream or the Advantage stream?" (Answer: The Advantage stream should approach zero, while the Value stream reflects the high stability).

- **Concept: Parameter-Sharing MARL**
  - **Why needed here:** The MARL implementation isn't just 16 independent agents; they share weights. This means they are effectively instances of the same "brain" experiencing different local realities. This is critical for the "Coordination Hypothesis"—they learn a generalized defense policy that applies across the network.
  - **Quick check question:** "If Agent A encounters a new attack variant, how does Agent B benefit?" (Answer: Through gradient updates/parameter synchronization, Agent B's policy implicitly updates to recognize the variant).

## Architecture Onboarding

- **Component map:** Node Behavior -> Trust Update -> State Vector Generation -> Agent Action (Adjust Ratio) -> Consensus Committee Re-selection
- **Critical path:** `Node Behavior` → `Trust Update` → `State Vector Generation` → `Agent Action (Adjust Ratio)` → `Consensus Committee Re-selection`. *Note: The TDP attack breaks this by corrupting the `Trust Update` step before the Agent can react.*
- **Design tradeoffs:**
  - **Discretization vs. Continuity:** RL (Tabular) is stable but blind to nuance (F1=0.5 on AAA). DRL sees nuance but requires training stability.
  - **Centralized vs. Distributed:** DRL is simpler but weaker against Collusion (0.68). MARL is stronger against Collusion (0.85) but adds synchronization overhead.
  - **Reactiveness vs. Memory:** The current design reacts to recent behavior (Thompson Sampling), which makes it vulnerable to Time-Delayed Poisoning (TDP). There is no long-term "memory" component to flag a sudden behavior shift from a historically "good" node.
- **Failure signatures:**
  - **TDP Catastrophe:** Observe the Reward Curve. If rewards are stable for ~25 episodes and then suddenly tank to near zero without recovery, the "Sleeper" agents have activated. The confusion matrix will show a "trust inversion" (honest nodes classified as malicious).
  - **RL Stagnation:** If training RL on AAA/CRA results in F1 locking at 0.50 (random guess), the discretization is too coarse.
- **First 3 experiments:**
  1. **Baseline Validation (NMA):** Run DRL against Naive Malicious Attack. Verify F1 ≈ 1.00. This confirms the basic trust loop is functioning.
  2. **The Coordination Test (CRA):** Run MARL vs. DRL on Collusive Rumor Attack. Confirm MARL achieves the cited uplift (Target: F1 ≈ 0.85 vs 0.68). This validates the parameter-sharing mechanism.
  3. **The Vulnerability Probe (TDP):** Run any agent against Time-Delayed Poisoning. Monitor the F1 score at Episode 25 (activation). Confirm the drop to ≤ 0.16. Attempt to mitigate by tweaking the trust decay factor γ to see if detection improves.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can blockchain trust systems be redesigned to defend against Time-Delayed Poisoning (TDP) attacks where adversaries build trust before activating?
- **Basis in paper:** [explicit] The authors identify "Temporal Attack Countermeasures" as a critical future direction (Section 10.3), noting that TDP defeated all agents (F1=0.11–0.16).
- **Why unresolved:** Current Bayesian trust models inherently favor recent behavior, making them vulnerable to "sleeper" agents that accumulate genuine positive evidence before attacking.
- **What evidence would resolve it:** An architecture incorporating behavioral consistency analysis or long-term trust memory that maintains F1-scores above 0.80 after sleeper activation.

### Open Question 2
- **Question:** Do the coordination advantages of Multi-Agent RL (MARL) persist in large-scale deployments (hundreds or thousands of nodes)?
- **Basis in paper:** [explicit] Section 10.3 lists "Scalability Analysis" as future work, questioning if MARL advantages persist as agent count increases or if communication overhead dominates.
- **Why unresolved:** The simulation used only 16 nodes; the computational cost and communication latency of MARL may become prohibitive in production-scale IoT networks.
- **What evidence would resolve it:** Empirical results from networks with $N > 100$ nodes showing MARL maintaining superior detection of coordinated attacks without succumbing to latency.

### Open Question 3
- **Question:** Can hybrid defense architectures dynamically switch between DRL and MARL based on detected attack characteristics to optimize performance?
- **Basis in paper:** [explicit] Section 10.3 proposes "Hybrid Defense Architectures," noting that different agents excel against different attacks (e.g., MARL for CRA, DRL for NMA).
- **Why unresolved:** While the paper validates the "coordination hypothesis," it is unknown if a meta-controller can efficiently detect coordination signatures to switch agents in real-time.
- **What evidence would resolve it:** A hybrid system outperforming static DRL and MARL baselines across all five attack families by successfully isolating attack types.

## Limitations
- The catastrophic failure against Time-Delayed Poisoning reveals a fundamental vulnerability in trust-based systems that accumulate historical evidence
- MARL parameter-sharing implementation lacks specification of gradient averaging protocol and replay buffer management
- FHE-ABAC implementation is theoretically described but lacks encryption parameters, key management, or performance benchmarks

## Confidence

- **High Confidence**: DRL achieving superior detection against adaptive attacks (F1=0.77) compared to RL (F1=0.50), given the clear architectural advantage of continuous state representation
- **High Confidence**: MARL outperforming DRL against coordinated attacks (F1=0.85 vs 0.68) under the stated parameter-sharing mechanism
- **High Confidence**: Byzantine Fault Injection being perfectly detected (F1=1.00) by all approaches, as the attack signature is too obvious to miss
- **Medium Confidence**: The 16-node network being representative of real IoT scale, as the paper doesn't validate performance on larger networks or different connectivity topologies
- **Low Confidence**: The claim that Thompson Sampling alone provides sufficient exploration, as the paper doesn't benchmark against epsilon-greedy or Boltzmann exploration strategies

## Next Checks

1. **Attack Persistence Test**: After TDP activation causes F1 collapse, continue training for 50+ episodes. Measure whether agents can recover trust estimates and restore detection capability, or if the system remains permanently compromised.

2. **Trust Initialization Sensitivity**: Repeat TDP experiments with alternative Beta priors (e.g., Beta(4,4), Beta(12,12)) and different noise levels. Quantify how sensitive the catastrophic failure point is to initial trust assumptions.

3. **Communication Overhead Measurement**: Instrument the MARL implementation to measure actual parameter synchronization time and bandwidth usage across 16 agents. Compare against theoretical limits to verify the practical feasibility of the claimed approach.