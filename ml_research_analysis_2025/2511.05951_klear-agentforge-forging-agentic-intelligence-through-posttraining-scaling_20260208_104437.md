---
ver: rpa2
title: 'Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling'
arxiv_id: '2511.05951'
source_url: https://arxiv.org/abs/2511.05951
tags:
- agentic
- training
- arxiv
- scaling
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Klear-AgentForge, a comprehensive open-source
  pipeline for training high-performance agentic models. Starting from Qwen3-8B, the
  authors employ supervised fine-tuning with synthetic data followed by multi-turn
  reinforcement learning to develop models capable of tool use and coding tasks.
---

# Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling

## Quick Facts
- arXiv ID: 2511.05951
- Source URL: https://arxiv.org/abs/2511.05951
- Reference count: 40
- Klear-AgentForge-8B achieves state-of-the-art performance among 8B models on multiple benchmarks including BFCL v3 (71.5), τ-bench Retail (56.7), and SWE-bench Verified (39.4)

## Executive Summary
This paper presents Klear-AgentForge, a comprehensive open-source pipeline for training high-performance agentic models. Starting from Qwen3-8B, the authors employ supervised fine-tuning with synthetic data followed by multi-turn reinforcement learning to develop models capable of tool use and coding tasks. The framework introduces a disaggregated training architecture for efficiency and combines step-wise and outcome-based rewards to mitigate sparse feedback in agent training.

The Klear-AgentForge-8B model achieves state-of-the-art performance among 8B models on multiple benchmarks, including BFCL v3 (71.5), τ-bench Retail (56.7), and SWE-bench Verified (39.4), while remaining competitive with much larger models. The approach addresses the challenge of building agentic intelligence through a systematic posttraining scaling methodology that leverages synthetic data generation and reinforcement learning.

## Method Summary
Klear-AgentForge employs a two-stage training approach: supervised fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement learning (RL). The process begins with the Qwen3-8B foundation model and uses synthetic data generation to create training examples for both stages. The SFT phase focuses on single-turn tool use and coding tasks, while the RL phase extends to multi-turn reasoning scenarios. A key innovation is the disaggregated training architecture, which separates the model into specialized components for improved computational efficiency. The RL training combines step-wise rewards (for intermediate actions) and outcome-based rewards (for final results) to address the challenge of sparse feedback in agent training.

## Key Results
- Klear-AgentForge-8B achieves 71.5 on BFCL v3, 56.7 on τ-bench Retail, and 39.4 on SWE-bench Verified
- State-of-the-art performance among 8B models on all evaluated benchmarks
- Competitive performance with much larger models despite being only 8B parameters
- Demonstrates strong capability in both tool use and coding tasks through multi-turn reasoning

## Why This Works (Mechanism)
The success of Klear-AgentForge stems from addressing two critical challenges in agent training: sparse reward signals and computational inefficiency. The combination of step-wise and outcome-based rewards in the reinforcement learning phase provides more frequent feedback during the learning process, preventing the model from receiving guidance only at task completion. The disaggregated training architecture allows for more efficient computation by separating the model into specialized components, reducing the computational burden while maintaining performance. The synthetic data generation pipeline ensures sufficient training diversity while maintaining control over the training distribution.

## Foundational Learning

**Reinforcement Learning with Sparse Rewards**: Understanding how to structure reward signals in environments where feedback is delayed or infrequent. Why needed: Agent training typically suffers from sparse rewards that make learning inefficient. Quick check: Can the agent learn to optimize both intermediate steps and final outcomes?

**Synthetic Data Generation for Agent Training**: Creating realistic training scenarios that simulate complex tool use and reasoning tasks. Why needed: Real-world training data for agent behaviors is expensive and limited. Quick check: Does the synthetic data distribution match real-world task distributions?

**Disaggregated Model Architectures**: Separating models into specialized components for different aspects of reasoning or task execution. Why needed: Full monolithic models become computationally expensive for complex multi-step reasoning. Quick check: Can specialized components maintain coherent overall behavior?

## Architecture Onboarding

**Component Map**: Synthetic Data Generator -> Supervised Fine-Tuning Pipeline -> Reinforcement Learning Module -> Evaluation Benchmarks
```
Synthetic Data -> SFT -> RL Training -> Evaluation
```

**Critical Path**: The most time-consuming component is the reinforcement learning phase, which requires extensive interaction with environments to collect training data and compute rewards. The synthetic data generation must be carefully designed to ensure coverage of realistic scenarios.

**Design Tradeoffs**: The disaggregated architecture improves computational efficiency but may introduce coordination challenges between specialized components. The choice between step-wise and outcome-based rewards involves balancing immediate feedback against task completion quality.

**Failure Signatures**: 
- Poor performance on multi-turn tasks suggests inadequate RL training or insufficient synthetic data diversity
- High variance in reward signals indicates unstable training or poorly designed reward functions
- Computational bottlenecks in the disaggregated architecture suggest suboptimal component sizing

**First 3 Experiments**:
1. Compare performance with only step-wise rewards versus only outcome-based rewards to quantify their individual contributions
2. Test different ratios of synthetic data versus human-annotated data in the SFT phase
3. Evaluate the disaggregated architecture against a monolithic baseline on computational efficiency and final performance

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on coding and tool-use benchmarks with limited assessment of real-world deployment scenarios or safety considerations
- Training methodology relies heavily on synthetic data generation, which may introduce distribution shifts or biases not captured in benchmark performance
- Disaggregated training architecture has not been validated across different base model families or sizes beyond Qwen3-8B

## Confidence
- Performance claims on established benchmarks: High - supported by specific numerical results on widely-used benchmarks
- Claims about architectural efficiency: Medium - computational advantages demonstrated but not extensively validated across different scenarios
- Generalizability to real-world applications: Low - limited evaluation beyond controlled benchmark environments

## Next Checks
1. Evaluate model performance and robustness on out-of-distribution tasks not represented in the synthetic training data
2. Conduct extensive ablation studies varying the ratio of step-wise to outcome-based rewards to quantify their relative contributions
3. Test the disaggregated training architecture with different base model families (e.g., Llama, Mistral) to assess generalizability of the approach