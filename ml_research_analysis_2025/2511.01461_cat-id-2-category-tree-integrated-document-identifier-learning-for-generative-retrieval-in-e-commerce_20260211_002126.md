---
ver: rpa2
title: 'CAT-ID$^2$: Category-Tree Integrated Document Identifier Learning for Generative
  Retrieval In E-commerce'
arxiv_id: '2511.01461'
source_url: https://arxiv.org/abs/2511.01461
tags:
- category
- retrieval
- loss
- information
- cat-id2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of constructing high-quality\
  \ document identifiers for generative retrieval in e-commerce by incorporating hierarchical\
  \ category information. The authors propose CAT-ID\xB2, which integrates category-tree\
  \ information into document identifier learning through three key modules: Hierarchical\
  \ Class Constraint Loss to enforce category-aware semantic clustering, Cluster Scale\
  \ Constraint Loss to prevent encoding collapse, and Dispersion Loss to ensure document\
  \ distinctiveness."
---

# CAT-ID$^2$: Category-Tree Integrated Document Identifier Learning for Generative Retrieval In E-commerce

## Quick Facts
- **arXiv ID:** 2511.01461
- **Source URL:** https://arxiv.org/abs/2511.01461
- **Reference count:** 40
- **Primary result:** Achieves up to 6.54% recall@10 and 23.37% recall@50 on ESCI-us, with 0.33% increase in average orders per thousand users in online A/B tests

## Executive Summary
This paper addresses the challenge of constructing high-quality document identifiers for generative retrieval in e-commerce by incorporating hierarchical category information. The authors propose CAT-ID², which integrates category-tree information into document identifier learning through three key modules: Hierarchical Class Constraint Loss to enforce category-aware semantic clustering, Cluster Scale Constraint Loss to prevent encoding collapse, and Dispersion Loss to ensure document distinctiveness. The method discretizes semantic embeddings using Residual Quantization Variation Encoder (RQ-VAE) and trains with the combined loss functions. Offline experiments on the ESCI dataset show CAT-ID² significantly outperforms baseline methods, achieving up to 6.54% recall@10 and 23.37% recall@50 on ESCI-us, 15.50% recall@20 and 31.60% recall@50 on ESCI-es, and 13.06% recall@20 and 28.14% recall@50 on ESCI-jp. Online A/B tests demonstrate a 0.33% increase in average orders per thousand users for ambiguous queries and 0.24% for long-tail queries, validating the method's effectiveness in real-world e-commerce search scenarios.

## Method Summary
CAT-ID² learns document identifiers by encoding document-category text pairs through a seq2seq model (BERT/T5) into embeddings, which are then mapped to latent representations via a DNN encoder. These latents are quantized through a Residual Quantization VAE (RQ-VAE) with L=4 layers and codebook size K=256 or 512. The method employs four loss functions: reconstruction loss (L_rq), Hierarchical Class Constraint Loss (L_HCC) using InfoNCE with hard negative mining from same parent categories, Cluster Scale Constraint Loss (L_CSC) using bidirectional KL divergence for uniform codebook usage, and Dispersion Loss (L_Dis) for distinctiveness. The total loss combines these with weights α=0.1, β=0.0001, γ=1.0. The trained RQ-VAE generates document identifiers, which are then used to fine-tune a T5-base LLM for generative retrieval. The method is trained for 300 epochs with batch size 4096 on 8×A100 80GB GPUs, with Sinkhorn post-processing to resolve ID collisions.

## Key Results
- CAT-ID² achieves 6.54% recall@10 and 23.37% recall@50 improvements on ESCI-us compared to baselines
- Online A/B tests show 0.33% increase in average orders per thousand users for ambiguous queries and 0.24% for long-tail queries
- Ablation studies confirm each loss component (HCCL, CSCL, DisL) contributes significantly to performance gains
- Category depth constraint (H < L) is critical for preventing model collapse

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incorporating hierarchical category labels as soft constraints improves the semantic clustering of document identifiers (DocIDs) compared to unsupervised quantization.
- **Mechanism:** The **Hierarchical Class Constraint Loss (HCCL)** applies contrastive learning (InfoNCE) at each quantizer layer. It treats documents sharing a category label at a specific depth as positive pairs and others as negatives. Crucially, it uses a hard negative mining strategy for deeper layers (selecting items from the same parent category but different sub-categories) to force fine-grained separability.
- **Core assumption:** The document set possesses a high-quality, pre-existing hierarchical taxonomy, and the RQ-VAE depth $L$ is sufficient to cover the category depth $H$ without forcing a collapse of the representation space.
- **Evidence anchors:**
  - [abstract] "Hierarchical Class Constraint Loss to integrate category information layer by layer during quantization."
  - [section 3.1] Describes the use of InfoNCE and hard negative mining where "negative examples that belong to the same category in the previous layer but fall into different subcategories in the current layer" are selected.
  - [corpus] *Purely Semantic Indexing for LLM-based Generative Recommendation and Retrieval* highlights general issues with semantic ID conflicts, supporting the need for structured constraints.
- **Break condition:** If the category tree is noisy or the number of categories exceeds the codebook capacity ($|C_1| > K$), this constraint may become impossible to satisfy, leading to optimization instability.

### Mechanism 2
- **Claim:** Enforcing uniform usage of codebook entries prevents "encoding collapse" where distinct documents map to identical identifiers.
- **Mechanism:** The **Cluster Scale Constraint Loss (CSCL)** uses bidirectional KL-divergence to force the average probability distribution of codebook assignments across a batch toward a uniform distribution. This counters the tendency of HCCL to map entire categories to single codebook entries (collapse), thereby ensuring the full representational capacity of the codebook is utilized.
- **Core assumption:** A uniform distribution of codes correlates with higher information density and better retrieval granularity, rather than just adding noise.
- **Evidence anchors:**
  - [abstract] "Cluster Scale Constraint Loss to prevent encoding collapse."
  - [section 3.2] "CSCL penalizes imbalanced assignments by encouraging the average distribution of samples across codebook entries to approach uniformity."
  - [corpus] Weak direct evidence in neighbors; *DOGR* discusses document-oriented contrastive learning but doesn't specifically address the codebook distribution collapse targeted here.
- **Break condition:** If the loss weight ($\gamma$) is too high, it may over-diffuse the semantic clusters, pushing similar items apart solely to satisfy uniformity, degrading recall.

### Mechanism 3
- **Claim:** Maximizing the dissimilarity of reconstructed embeddings ensures distinctiveness of DocIDs for different documents.
- **Mechanism:** The **Dispersion Loss (DisL)** operates on the decoder output ($\hat{d}$). It pushes the reconstructed embedding of a document away from the original embeddings of *all* other documents in the batch (acting as negatives). This ensures that while HCCL pulls categories together, DisL maintains unique "fingerprints" for individual items.
- **Core assumption:** The reconstruction fidelity is high enough that distinctness in the embedding space reliably translates to distinct ID tokens.
- **Evidence anchors:**
  - [abstract] "Dispersion Loss to ensure document distinctiveness."
  - [section 3.3] "This loss pushes all reconstructed embeddings apart, enhancing distinctiveness."
  - [corpus] *Purely Semantic Indexing* notes that existing methods suffer from conflicts where similar documents get identical IDs; DisL directly targets this by enforcing uniqueness.
- **Break condition:** If the embedding dimension is too low or the batch size is too small, the "pushing" force may be insufficient to prevent collisions in the final ID space.

## Foundational Learning

- **Concept: Residual Quantization (RQ-VAE)**
  - **Why needed here:** This is the discretization backbone. Unlike standard VQ-VAE which uses one codebook, RQ-VAE uses a stack of codebooks to refine the quantization error (residual) layer by layer, creating the hierarchical ID sequence.
  - **Quick check question:** How does the residual vector $r^l$ change as you move from the first quantizer layer to the last?

- **Concept: Contrastive Learning (InfoNCE)**
  - **Why needed here:** Both HCCL and Dispersion Loss rely on contrasting positive and negative samples. Understanding how to construct these pairs (e.g., in-category vs. out-of-category) is vital for tuning the model.
  - **Quick check question:** In the HCCL context, does a "hard negative" share the same parent category or a completely different top-level category?

- **Concept: KL Divergence**
  - **Why needed here:** Essential for understanding the Cluster Scale Constraint Loss. The model minimizes the "distance" between the actual code usage distribution and a uniform distribution.
  - **Quick check question:** Why use a bidirectional KL divergence (matching P to Q *and* Q to P) rather than just one direction?

## Architecture Onboarding

- **Component map:** Encoder (BERT/T5 → DNN → Latent z) -> Quantizer (L layers of Codebooks) -> Loss Aggregator (Combines L_rq, L_HCC, L_CSC, L_Dis) -> Post-Processor (Sinkhorn) -> Generator (T5 LLM)

- **Critical path:** The **category depth constraint** ($H < L$). If your e-commerce taxonomy has 7 levels but you set RQ-VAE layers to 4, you cannot fully utilize the hierarchical label information, and the HCCL optimization may fail or degrade.

- **Design tradeoffs:**
  - **Codebook Size ($K$) vs. Dataset Scale:** Small codebooks limit discriminability (collisions), while large codebooks on small datasets cause sparse utilization (underfitting). The paper suggests tuning $K$ based on dataset size (256 vs 512).
  - **HCCL Strength ($\beta$) vs. DisL ($\alpha$):** Too much category constraint collapses the ID space; too much dispersion weakens the category clustering. The "sweet spot" requires DisL to stabilize HCCL.

- **Failure signatures:**
  - **Category Collapse:** Documents in the same category map to the *exact same* ID sequence. *Diagnosis:* CSCL weight ($\gamma$) is too low or codebook size is too small.
  - **ID Fragmentation:** Documents in the same category share no common ID prefix. *Diagnosis:* HCCL weight ($\beta$) is too low, or hard negative mining is not active.
  - **Unstable Training:** Loss spikes during RQ-VAE training. *Diagnosis:* Conflict between reconstruction loss and constraint losses; check stop-gradient operations in RQ-VAE logic.

- **First 3 experiments:**
  1. **Ablation on Loss Components:** Train three variants (w/o HCCL, w/o CSCL, w/o DisL) to verify the specific contribution of each constraint to Recall@10 and collision rates.
  2. **Layer-Distribution Visualization:** Visualize the connection between Category Levels and Codebook Layers (as in Fig 3) to ensure Layer 1 codes map to Level 1 categories, etc.
  3. **Hyperparameter Sensitivity ($\alpha, \beta, \gamma$):** Run a grid search specifically focusing on the balance between $\beta$ (clustering) and $\alpha$ (dispersion) to find the stability region where recall peaks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How robust is CAT-ID$^2$ to noisy or inconsistent category hierarchies compared to the clean, expert-defined labels used in the ESCI dataset?
- **Basis:** [inferred] The paper contrasts its method with unsupervised clustering (RQ-VAE) by relying on "reliable expert-defined labels" (Page 2), assuming the ground truth hierarchy is accurate. Real-world industrial taxonomies often suffer from noise.
- **Why unresolved:** The experiments utilize the curated ESCI dataset; the method's sensitivity to "hard negative mining" (Section 3.1) based on incorrect parent-child relationships is not tested.
- **What evidence would resolve it:** Experiments injecting varying degrees of label noise (e.g., random flipping of category nodes) into the training data to measure the degradation of Recall@k.

### Open Question 2
- **Question:** Why does removing category text from the input embedding cause only a minor performance drop, and does this imply the textual representation is redundant?
- **Basis:** [explicit] In Section 4.3, the authors note that removing category info from the input ("wo cate info") resulted in a "minor performance drop" compared to the significant gains achieved by adding the Hierarchical Class Constraint Loss.
- **Why unresolved:** The authors hypothesize that "rich textual features" in the embeddings reduce reliance on explicit category features, but they do not determine if the category text provides any unique semantic signal beyond the structural loss.
- **What evidence would resolve it:** An ablation study analyzing the embedding space geometry to see if textual category tokens are effectively ignored or if they complement the structural constraints.

### Open Question 3
- **Question:** What are the performance limits when the category tree depth ($H$) approaches or exceeds the RQ-VAE codebook depth ($L$)?
- **Basis:** [explicit] Section 3.1 states, "the maximum depth $H$ of the category tree must be smaller than the maximum depth $L$ of the RQ-VAE" to prevent collapse.
- **Why unresolved:** While Table 6 analyzes total RQ-VAE layers (3, 4, 5), it does not test the boundary condition where the category hierarchy is deeper than the codebook capacity.
- **What evidence would resolve it:** Experiments using datasets with deeper category trees (e.g., $H=5, 6$) while maintaining a shallow codebook (e.g., $L=3, 4$) to observe if the model collapses or prioritizes higher-level distinctions.

## Limitations
- Effectiveness heavily depends on quality and consistency of pre-existing hierarchical category labels
- Computational overhead from multi-loss framework requiring 8×A100 80GB GPUs
- RQ-VAE architecture limitations with fixed codebook size (K=256 or 512) and depth (L=4)
- Category depth constraint (H < L) limits flexibility with varying taxonomy depths

## Confidence
- **High Confidence:** CAT-ID² significantly outperforms baselines on ESCI dataset (recall@10 improvements up to 6.54%)
- **Medium Confidence:** Loss components contribute as claimed (HCCL improves clustering, CSCL prevents collapse, DisL ensures distinctiveness)
- **Low Confidence:** Uniform codebook distribution enforced by CSCL correlates with higher information density lacks direct empirical validation

## Next Checks
1. **Category Label Quality Sensitivity Analysis:** Systematically evaluate CAT-ID² performance using progressively noisier or incomplete category labels to determine the method's robustness to taxonomy quality.

2. **Computational Efficiency Benchmarking:** Conduct detailed analysis of training time, memory usage, and inference latency compared to baseline methods across different dataset sizes, including measurements of Sinkhorn algorithm overhead.

3. **Cross-Domain Generalization Study:** Apply CAT-ID² to document collections from domains without hierarchical category structures using alternative organizational schemas like keywords, topics, or user-defined tags to test method's broader applicability.