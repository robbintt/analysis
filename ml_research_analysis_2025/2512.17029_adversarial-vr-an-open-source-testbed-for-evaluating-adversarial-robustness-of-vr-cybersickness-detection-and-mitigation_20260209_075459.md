---
ver: rpa2
title: 'Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness
  of VR Cybersickness Detection and Mitigation'
arxiv_id: '2512.17029'
source_url: https://arxiv.org/abs/2512.17029
tags:
- cybersickness
- adversarial
- detection
- mitigation
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Adversarial-VR, the first open-source testbed
  for evaluating DL-based cybersickness detection and mitigation systems under adversarial
  conditions. The testbed integrates two state-of-the-art DL models (DeepTCN and Transformer)
  trained on the MazeSick dataset, implements dynamic visual tunneling mitigation
  via Unity, and incorporates three adversarial attacks (MI-FGSM, PGD, C&W) to assess
  model robustness.
---

# Adversarial VR: An Open-Source Testbed for Evaluating Adversarial Robustness of VR Cybersickness Detection and Mitigation

## Quick Facts
- **arXiv ID:** 2512.17029
- **Source URL:** https://arxiv.org/abs/2512.17029
- **Reference count:** 40
- **Primary result:** First open-source testbed evaluating DL-based cybersickness detection under adversarial attacks, showing 5.94x accuracy degradation with C&W attack on Transformer model

## Executive Summary
This paper introduces Adversarial-VR, an open-source testbed for evaluating the robustness of deep learning-based cybersickness detection and mitigation systems against adversarial attacks in virtual reality environments. The testbed integrates state-of-the-art detection models (DeepTCN and Transformer) trained on the MazeSick dataset, implements dynamic visual tunneling mitigation via Unity, and incorporates three adversarial attacks (MI-FGSM, PGD, C&W) to assess model vulnerability. Results demonstrate that adversarial examples can significantly degrade detection accuracy and prevent proper mitigation, with the C&W attack reducing Transformer model accuracy by 5.94x compared to no-attack conditions. The work addresses the critical gap in evaluating adversarial robustness of AI-driven cybersickness systems in immersive VR environments.

## Method Summary
The Adversarial-VR testbed implements two DL architectures (DeepTCN and Transformer) for real-time cybersickness severity classification using VR sensor data (pupil diameter, gaze direction, head orientation) from the MazeSick dataset. Models are trained on 90-timestep sliding windows with Adam optimizer (lr=0.001, batch size 256, 300 epochs) and evaluated under three adversarial attacks (MI-FGSM, PGD, C&W) using the CleverHans library. The system implements dynamic visual tunneling mitigation that adjusts FOV based on severity predictions, and is deployed in a Unity-based VR environment using HTC Vive Pro Eye. Both white-box and black-box attack scenarios are evaluated, with transferability tested between model architectures.

## Key Results
- C&W attack reduces Transformer model accuracy from 95% to 16% (5.94x degradation)
- Adversarial attacks prevent proper mitigation by causing incorrect severity classification
- Transferability exists between models - adversarial examples crafted for Transformer reduce DeepTCN accuracy from 91% to 46%
- Real-time inference latency of 0.0021s/frame enables practical deployment

## Why This Works (Mechanism)

### Mechanism 1
Small, strategically designed perturbations to VR sensor data streams can deceive DL-based cybersickness detection models and trigger inappropriate mitigation. Adversarial examples are crafted using gradient-based optimization (MI-FGSM, PGD, C&W) to perturb multivariate time-series inputs (gaze direction, pupil metrics, head orientation) while remaining within normal sensor value ranges. The perturbed inputs exploit model decision boundaries, causing misclassification of severity levels. The paper reports the C&W attack reduces Transformer model accuracy by 5.94×, with accuracy dropping from 95% to 16% (Table 3).

### Mechanism 2
Adversarial examples generated for one cybersickness detection model architecture transfer effectively to other architectures in black-box scenarios. Transferability arises because different DL models learn similar decision boundaries for the same task. Adversarial perturbations that exploit these shared features fool multiple models. The paper shows adversarial examples crafted for Transformer reduce DeepTCN accuracy from 91% to 46% (C&W), and examples crafted for DeepTCN reduce Transformer accuracy from 95% to 40% (Table 4).

### Mechanism 3
Dynamic visual tunneling mitigation effectiveness depends critically on accurate severity classification; misclassification causes either under-mitigation or over-mitigation. The system maps severity levels (none/low/medium/high) to specific Aperture Fraction and Feathering parameters. Correct classification triggers appropriate FOV restriction. Adversarial misclassification disrupts this mapping—the paper shows an actual "none" severity being predicted as "high" at 105 seconds, causing unnecessary narrow tunneling (AF=0.52f, 36°-80° FOV).

## Foundational Learning

- **Time-series adversarial attacks vs. image-based attacks**
  - Why needed here: Standard adversarial attack tutorials focus on static images. VR sensor data is multivariate time-series with temporal dependencies—perturbations must be crafted differently.
  - Quick check question: Can you explain why applying pixel-level perturbation strategies directly to 90-timestep VR sensor windows would be suboptimal?

- **White-box vs. black-box threat models in ML security**
  - Why needed here: The testbed evaluates both scenarios. Understanding what information the attacker has access to (model weights, gradients, training data) determines attack effectiveness and appropriate defenses.
  - Quick check question: In a black-box scenario, why does transferability matter, and what surrogate model strategies might an attacker use?

- **Dynamic FOV restriction for cybersickness mitigation**
  - Why needed here: This is the mitigation mechanism being attacked. Understanding how peripheral vision restriction reduces sensory conflict helps interpret why incorrect mitigation is harmful.
  - Quick check question: If a user's actual cybersickness is "high" but the model predicts "none," what specific user experience degradation occurs?

## Architecture Onboarding

- **Component map:** Sensor data capture (90Hz) -> REST API transmission -> Cloud model inference (0.0021s/frame) -> Severity prediction -> Mitigation parameter mapping -> Unity shader update -> FOV modification
- **Critical path:** Sensor data capture (90Hz) → REST API transmission → Cloud model inference (0.0021s/frame) → Severity prediction → Mitigation parameter mapping → Unity shader update → FOV modification. Attacks inject at the sensor data stream before cloud transmission.
- **Design tradeoffs:** Cloud vs. edge inference: Cloud enables easier attack injection at communication layer; edge deployment reduces latency but exposes model to direct device compromise. Fixed 90-timestep window: Balances temporal context with inference speed, but creates attack surface for temporal perturbation strategies. Severity granularity (4 levels): Provides mitigation nuance but creates more misclassification opportunities than binary detection.
- **Failure signatures:** Mitigation not triggered when user shows discomfort symptoms → likely "high/medium" misclassified as "none/low". Excessive tunneling during calm navigation → likely "none" misclassified as "medium/high". Rapid FOV oscillations → adversarial perturbations causing unstable predictions across timesteps. Low Pearson correlation (PCC < 0.35) between original and perturbed data streams indicates attack presence.
- **First 3 experiments:**
  1. **Baseline validation:** Run maze simulation with clean data, verify Transformer model achieves ~95% accuracy and appropriate mitigation triggers. Log prediction distribution across severity levels.
  2. **White-box attack injection:** Enable C&W attack module, measure accuracy degradation and mitigation failure rate. Document which severity transitions are most vulnerable (e.g., "high" → "none").
  3. **Black-box transferability test:** Train a surrogate model (e.g., LSTM or simpler CNN) on MazeSick subset, craft adversarial examples against it, evaluate against deployed Transformer. Compare transfer effectiveness to white-box baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What defense mechanisms can effectively detect and mitigate adversarial attacks on DL-based cybersickness detection systems in real-time VR environments?
- **Basis in paper:** Authors state: "we plan to explore defense mechanisms to detect and mitigate adversarial attacks, to strengthen the overall AI-based automatic cybersickness detection and mitigation framework."
- **Why unresolved:** The paper demonstrates vulnerability to attacks but provides no defensive countermeasures; current mitigation only addresses cybersickness symptoms, not adversarial perturbations.
- **What evidence would resolve it:** Implementation and empirical evaluation of adversarial detection/prevention methods (e.g., input sanitization, robust training, anomaly detection) within the testbed showing restored accuracy under attack conditions.

### Open Question 2
- **Question:** How do adversarial perturbations and resulting mitigation failures quantitatively affect human physiological responses, comfort, and task performance during immersive VR sessions?
- **Basis in paper:** Authors acknowledge: "due to time, resource, and budget constraints, we did not conduct a user study... we plan to conduct a comprehensive user study to validate the human-centered impacts of cybersickness and adversarial perturbations."
- **Why unresolved:** All results use pre-recorded dataset and model-level metrics; no human subjects experienced the adversarial manipulation in real-time.
- **What evidence would resolve it:** Controlled user study with participants experiencing both clean and adversarial VR sessions, measuring FMS scores, physiological signals, task completion, and presence questionnaires.

### Open Question 3
- **Question:** Do adversarial vulnerabilities transfer across diverse VR environments (roller coasters, racing, multiplayer) and demographic groups beyond the MazeSick dataset?
- **Basis in paper:** Authors state: "we plan to expand the testbed to include diverse environments... and incorporate datasets with equal gender representation and broader age groups."
- **Why unresolved:** Current evaluation uses single maze environment and MazeSick dataset (gender-imbalanced, age 18-38), limiting generalizability claims.
- **What evidence would resolve it:** Cross-environment and cross-demographic attack evaluation showing consistent or varying vulnerability patterns.

### Open Question 4
- **Question:** Can hybrid attacks combining digital perturbations with physical-world sensor manipulations compromise VR cybersickness systems more effectively than purely digital approaches?
- **Basis in paper:** Authors state: "future work will examine additional attack vectors, including advanced or hybrid strategies combining digital perturbations with physical-world sensor manipulations."
- **Why unresolved:** Current attacks are purely software-based; physical sensor interference (e.g., IR spoofing eye-trackers) remains unexplored.
- **What evidence would resolve it:** Implementation and evaluation of hybrid attack vectors within the testbed framework.

## Limitations
- Attack realism not validated in real-world deployment scenarios; assumes feasibility of sensor data stream interception
- Dataset generalization limited to MazeSick dataset; performance on other VR datasets or real-world data unknown
- Mitigation quantification lacks empirical measurement of actual user experience degradation from adversarial-induced incorrect mitigation

## Confidence
- **High Confidence:** The core methodology for implementing adversarial attacks on time-series cybersickness detection models is technically sound and reproducible. The reported accuracy degradation under attacks (5.94x reduction for C&W on Transformer) is directly measurable from the testbed.
- **Medium Confidence:** The transferability results between DeepTCN and Transformer architectures are internally consistent but may not generalize to other model types or larger-scale applications. The attack implementation details (perturbation bounds, optimization parameters) are specified but may require tuning for different sensor modalities.
- **Low Confidence:** Claims about real-world attack feasibility and user experience impact lack empirical validation beyond the controlled testbed environment. The paper does not address potential defense mechanisms or detection of adversarial attacks.

## Next Checks
1. **Attack Vector Feasibility:** Test the adversarial attack implementation in an end-to-end VR deployment where sensor data is transmitted over realistic network conditions, measuring attack success rates under varying latency and packet loss scenarios.
2. **Cross-Dataset Robustness:** Evaluate the same adversarial attacks on a different cybersickness dataset (e.g., publicly available VR sickness datasets) to assess whether the observed vulnerability patterns hold across diverse data distributions.
3. **Human Factors Validation:** Conduct a small-scale user study measuring actual cybersickness symptoms (using standardized questionnaires) when users experience adversarial-induced incorrect mitigation versus proper mitigation, correlating physiological measurements with subjective reports.