---
ver: rpa2
title: 'Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real
  Instructions?'
arxiv_id: '2509.04292'
source_url: https://arxiv.org/abs/2509.04292
tags:
- instruction
- instructions
- ifeval
- arxiv
- inverse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Inverse IFEval, a benchmark that measures\
  \ LLMs\u2019 ability to override training-induced biases and follow counterintuitive\
  \ instructions. The benchmark includes eight instruction types such as Question\
  \ Correction, Intentional Textual Flaws, Code without Comments, and Counterfactual\
  \ Answering, designed to challenge models beyond standard paradigms."
---

# Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?

## Quick Facts
- arXiv ID: 2509.04292
- Source URL: https://arxiv.org/abs/2509.04292
- Reference count: 40
- Primary result: Even advanced LLMs struggle with counter-intuitive instructions, scoring ~75% on average

## Executive Summary
This paper introduces Inverse IFEval, a benchmark measuring LLMs' ability to override training-induced biases and follow counterintuitive instructions. The benchmark includes eight instruction types designed to challenge models beyond standard paradigms, such as Question Correction, Intentional Textual Flaws, and Counterfactual Answering. Evaluations on 15 leading models show that even advanced LLMs struggle with these out-of-distribution instructions, highlighting the need for future alignment efforts to focus on adaptability under unconventional contexts.

## Method Summary
The benchmark uses a human-in-the-loop pipeline to generate 1012 high-quality questions across 23 domains in both Chinese and English. Each instruction type systematically inverts a specific SFT convention learned during training. Evaluation employs an LLM-as-a-Judge framework with dedicated judge models per instruction type, optimized templates, and enhanced system prompts to achieve 98% accuracy. The dataset is publicly available at Hugging Face.

## Key Results
- Top models score around 75% on average across all instruction types
- Question Correction emerges as a universal weak point, with half of evaluated models scoring below 30%
- Thinking models show significant advantage, with non-thinking variants dropping over 15 points (Qwen3-235B-A22B: 54.22 thinking vs. 40.28 instruct on English)
- Ranking inversions occur between conventional IFEval and Inverse IFEval (Qwen3-235B-A22B-Instruct: 5th → 15th)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models develop cognitive inertia from training on idealized SFT response paradigms, creating systematic blind spots for counter-conventional instructions.
- Mechanism: SFT data annotation follows an "idealized paradigm" where annotators produce responses conforming to predefined formats, correctness norms, and readability standards. This distributional bias becomes ingrained, causing models to default to learned patterns even when explicitly instructed otherwise.
- Core assumption: The mechanism is primarily distributional (training data patterns) rather than architectural or related to model capacity.
- Evidence anchors:
  - [abstract] "cognitive inertia, struggling to follow instructions that conflict with the standardized patterns learned during supervised fine-tuning (SFT)"
  - [section 2.1] "SFT data annotation tends to adhere to an 'idealized paradigm', where annotators construct data by following a predefined, idealized response format"
  - [corpus] "Format Inertia: A Failure Mechanism of LLMs in Medical Pre-Consultation" and "The Price of Format: Diversity Collapse in LLMs" corroborate format-specific training biases as recognized phenomena
- Break condition: If models fine-tuned on explicitly diverse or anti-conventional data still exhibit the same failure patterns, the mechanism may involve deeper architectural constraints or pretraining biases not addressed by SFT.

### Mechanism 2
- Claim: Chain-of-thought reasoning helps models override training-induced biases by enabling reflective processing rather than reflexive pattern matching.
- Mechanism: Thinking models can "reflect on knowledge acquired during SFT" rather than automatically applying learned conventions. Extended computation allows models to detect instruction-context mismatches and suppress dominant but inappropriate response patterns.
- Core assumption: The performance gain derives from explicit reasoning processes, not confounding factors like model scale, training data differences, or compute budget.
- Evidence anchors:
  - [section 3.2.1] "Thinking enables models to reflect on knowledge acquired during SFT, thereby improving performance on Inverse IFEval"
  - [section 3.1] "Non-thinking models (e.g., Qwen3-235B-A22B-Instruct, Qwen3-30B-A3B-Instruct) perform worse than thinking models"
  - [corpus] Corpus evidence is limited; related work on CoT does not specifically isolate bias override as a mechanism
- Break condition: If thinking models fail on novel counter-intuitive instruction types not represented in the current benchmark, or if performance gains disappear when controlling for model scale and training compute.

### Mechanism 3
- Claim: Models face a fundamental instruction fidelity vs. training fidelity conflict when instructions explicitly contradict learned distributional patterns.
- Mechanism: Each of the eight instruction types directly inverts a specific SFT convention (e.g., "provide wrong answers" contradicts correctness training; "no paragraph breaks" contradicts readability norms). Models must suppress strongly reinforced behaviors to comply.
- Core assumption: Failures reflect a genuine conflict between competing objectives rather than instruction comprehension failures or ambiguity.
- Evidence anchors:
  - [section 2.1] Eight types "systematically inverts conventional training paradigms"
  - [appendix A] Each instruction type maps to a specific "SFT Paradigm (Data Inertia)"—e.g., "Training corpora rarely contain inherently flawed questions" for Question Correction
  - [corpus] "The Price of Format: Diversity Collapse in LLMs" provides supporting evidence for format-specific distributional biases
- Break condition: If models continue failing even when instructions are clarified, repeated, or reformulated, suggesting comprehension limitations rather than bias conflict.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) Distributional Bias
  - Why needed here: The paper's core thesis is that SFT creates systematic blind spots; understanding this is essential for interpreting results.
  - Quick check question: Why might training exclusively on "high-quality" annotated data create vulnerabilities for real-world instruction following?

- Concept: Out-of-Distribution (OOD) Instruction Following
  - Why needed here: Inverse IFEval explicitly tests OOD generalization, distinct from knowledge benchmarks like MMLU.
  - Quick check question: What distinguishes OOD instruction-following robustness from conventional benchmark performance?

- Concept: LLM-as-a-Judge Evaluation Framework
  - Why needed here: The paper relies on automated LLM-based scoring with claimed 98% accuracy; understanding limitations is critical.
  - Quick check question: What failure modes arise when using LLMs to evaluate other LLMs, and how might this affect benchmark validity?

## Architecture Onboarding

- Component map:
  Benchmark: 8 instruction types (QC, ITF, CC, CCF, DIA, II, MIM, CA) × 23 domains × 2 languages (Chinese/English, 506 each)
  Evaluation pipeline: Dedicated judge model per instruction type + optimized template structures + enhanced system prompts
  Test subjects: 15 models across closed-source (o3-high, GPT-5-high, Gemini-2.5-pro, Claude-4-Opus) and open-source (Qwen3, DeepSeek-R1, GLM-4.5, Kimi-K2)

- Critical path:
  1. Profile your model's failure distribution across all 8 instruction types (focus on Question Correction and Deliberately Incorrect Answers, where models show largest gaps)
  2. Isolate the thinking mechanism effect by comparing thinking vs. non-thinking variants of the same model family
  3. Run Best-of-N sampling (N=1, 16, 32) to distinguish stochastic failures from systematic limitations

- Design tradeoffs:
  - Synthetic diagnostic vs. real-world utility: Paper acknowledges tasks are intentionally artificial (analogized to IQ tests); unclear how well this transfers to actual user scenarios
  - Judge accuracy vs. complexity: Required dedicated models, template optimization, and system prompt engineering to reach 98% accuracy
  - Language parity: Equal Chinese/English split, but some models (GLM-4.5, Qwen3-235B) show significant cross-lingual performance gaps (8-16 points)

- Failure signatures:
  - Non-thinking models dropping >15 points vs. thinking variants (Qwen3-235B-A22B: 54.22 thinking vs. 40.28 instruct on English)
  - Question Correction as a universal weak point: half of evaluated models score <30%
  - Ranking inversions between IFEval and Inverse IFEval (Qwen3-235B-A22B-Instruct: 5th → 15th)

- First 3 experiments:
  1. Baseline your model on all 8 instruction types; calculate per-type accuracy and identify the largest gaps vs. conventional IFEval performance
  2. If your model supports thinking/non-thinking modes, run paired comparison; quantify the thinking mechanism benefit
  3. Execute Best-of-N (N=1, 16, 32) to assess whether failures are capability-limited or sampling-limited; report the ceiling performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can supervised fine-tuning (SFT) or post-training alignment strategies be modified to mitigate "cognitive inertia" without degrading the model's ability to generate standard, high-quality outputs?
- Basis in paper: [inferred] The Conclusion states the need for "developing methods that mitigate cognitive inertia," and Appendix B.1 suggests that "appropriate post-training" could yield gains, though the paper only evaluates existing models.
- Why unresolved: The paper establishes the benchmark to diagnose the issue but does not propose or test specific training methodologies (e.g., data regularization, adversarial training) to solve the overfitting to the "idealized paradigm."
- What evidence would resolve it: A training ablation study showing that a specific data mixture or loss function improves Inverse IFEval scores while maintaining baseline performance on standard benchmarks like IFEval or MMLU.

### Open Question 2
- Question: Why does the adaptive chain-of-thought (AdaCoT) mechanism fail to improve performance in the Chinese language version, performing worse even than the non-thinking mode?
- Basis in paper: [explicit] Section 3.2.1 notes regarding the Chinese results: "This suggests that the auto-thinking mode still needs further optimization to better suit the Chinese language context."
- Why unresolved: The paper identifies this cross-lingual anomaly in the auto-thinking settings but leaves the underlying linguistic or algorithmic causes undefined.
- What evidence would resolve it: An analysis of the reasoning traces in Chinese vs. English auto-thinking modes, identifying where the logic diverges, followed by a refined AdaCoT implementation that resolves the performance drop.

### Open Question 3
- Question: Does strong performance on Inverse IFEval correlate with better handling of natural, long-tail user requests in production environments?
- Basis in paper: [inferred] The paper argues in Section 2.2 and the Conclusion that the benchmark reflects "real-world challenges" and "long-tail requests," using the analogy that the tasks are like IQ tests (measuring generalization rather than practical utility).
- Why unresolved: The authors establish the analogy but provide no evidence that these specific synthetic, counter-intuitive tasks are predictive of success on actual messy, unconventional user data found in the wild.
- What evidence would resolve it: A correlation study comparing model rankings on Inverse IFEval against human preference ratings or success rates on a dataset of real-world, adversarial user interactions.

## Limitations
- The benchmark's artificial nature raises questions about real-world transferability, as these synthetic tasks may not fully capture the complexity of actual user scenarios.
- Judge model accuracy depends on specific configurations and prompt engineering that aren't fully disclosed, potentially limiting reproducibility.
- The eight instruction types may not comprehensively capture all forms of counter-intuitive instruction-following abilities, leaving potential blind spots.

## Confidence

**High Confidence**: The existence of training-induced distributional biases (Mechanism 1) is well-supported by both the paper's systematic mapping of SFT paradigms and external literature on format diversity collapse in LLMs. The correlation between thinking models and improved performance (Mechanism 2) is reasonably robust given the systematic comparison across model families.

**Medium Confidence**: The instruction fidelity vs. training fidelity conflict (Mechanism 3) is logically sound and well-structured, but relies on the assumption that failures reflect genuine bias conflicts rather than comprehension limitations. This would benefit from additional experiments testing instruction clarification and reformulation.

**Low Confidence**: The claim that these specific eight instruction types comprehensively capture "counter-intuitive ability" may be incomplete, as real-world scenarios could involve more complex or nuanced instruction conflicts not represented in the current benchmark.

## Next Checks

1. **Cross-Benchmark Validation**: Test whether models that perform well on Inverse IFEval also show improved performance on real-world instruction-following tasks with nuanced requirements, such as customer service scenarios or creative writing with specific constraints.

2. **Judge Model Ablation**: Systematically vary the judge model configurations, prompt templates, and scoring rubrics to establish sensitivity analysis and verify that the 98% accuracy claim is robust across different judge implementations.

3. **Instruction Type Expansion**: Design and test at least three new instruction types that target potential blind spots in the current benchmark, particularly focusing on instructions that combine multiple conventional paradigm violations or involve temporal/dynamic constraints.