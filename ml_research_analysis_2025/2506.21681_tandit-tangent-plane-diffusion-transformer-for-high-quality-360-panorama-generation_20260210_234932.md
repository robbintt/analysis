---
ver: rpa2
title: "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360\xB0 Panorama\
  \ Generation"
arxiv_id: '2506.21681'
source_url: https://arxiv.org/abs/2506.21681
tags:
- image
- tangent
- panoramic
- generation
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "TanDiT addresses the challenge of generating high-quality 360\xB0\
  \ panoramic images by decomposing the spherical scene into a structured grid of\
  \ tangent-plane projections, which are then generated jointly using a unified diffusion\
  \ transformer model. Unlike previous approaches that generate views independently\
  \ or rely on multi-branch denoising, TanDiT leverages the spatial relationships\
  \ between tangent planes and employs a model-agnostic refinement step to ensure\
  \ global coherence and loop-consistency."
---

# TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation

## Quick Facts
- arXiv ID: 2506.21681
- Source URL: https://arxiv.org/abs/2506.21681
- Reference count: 40
- Primary result: Achieves state-of-the-art TangentFID of 35.39 and TangentIS of 6.06 on 360° panorama generation

## Executive Summary
TanDiT addresses the challenge of generating high-quality 360° panoramic images by decomposing the spherical scene into a structured grid of tangent-plane projections, which are then generated jointly using a unified diffusion transformer model. Unlike previous approaches that generate views independently or rely on multi-branch denoising, TanDiT leverages the spatial relationships between tangent planes and employs a model-agnostic refinement step to ensure global coherence and loop-consistency. The method achieves state-of-the-art performance across multiple metrics, including TangentFID (35.39) and TangentIS (6.06), outperforming baselines like Diffusion360 and StitchDiffusion. Additionally, TanDiT generalizes well to diverse styles and complex prompts, supports arbitrary resolutions including 4K, and introduces two new panoramic-specific evaluation metrics along with a comprehensive benchmark suite.

## Method Summary
TanDiT generates 360° panoramas by first decomposing the spherical scene into 18 tangent-plane projections arranged in a 3×6 grid, then jointly generating these planes using a LoRA-fine-tuned Stable Diffusion 3.5 Large model. The tangent-plane approach allows the diffusion transformer to leverage its attention mechanism across spatially adjacent views, promoting global coherence. After initial generation and optional super-resolution of each plane, the images are reprojected to equirectangular format and refined through a model-agnostic denoising process that uses circular padding to ensure seamless left-right continuity. The refinement step operates at a high noise timestep (~800) to suppress stitching artifacts while preserving scene structure, making the approach scalable to high resolutions like 4K.

## Key Results
- Achieves TangentFID of 35.39 and TangentIS of 6.06, outperforming state-of-the-art baselines
- Introduces two new panoramic-specific evaluation metrics: TangentFID and TangentIS
- Successfully generates high-resolution panoramas (including 4K) with improved quality and consistency
- Demonstrates superior generalization to diverse styles and complex prompts compared to existing methods

## Why This Works (Mechanism)

### Mechanism 1: Spatial Continuity via Structured Tangent-Plane Grids
- Claim: Decomposing spherical panoramas into adjacent tangent-plane grids allows the DiT's attention mechanism to learn and enforce spatial consistency across views.
- Mechanism: The 3×6 grid arrangement places overlapping regions adjacent to each other, enabling the transformer's self-attention to naturally propagate information between neighboring tangent planes during joint generation.
- Core assumption: The DiT's attention can learn spatial relationships across the grid structure without explicit geometric constraints.
- Evidence anchors:
  - [abstract] "TanDiT leverages the spatial relationships between tangent planes"
  - [section 3.2] "This design enables the DiT to exploit local context across views, promoting globally coherent generation."
  - [corpus] Weak support; related work CubeDiff uses multi-view generation but with independent face processing leading to seam issues.
- Break condition: If attention patterns fail to capture cross-view dependencies, visible discontinuities will appear at tangent plane boundaries.

### Mechanism 2: High-Frequency Artifact Suppression via ERP-Conditioned Refinement
- Claim: Noising an intermediate ERP panorama at a high timestep (~800) and denoising with the pretrained DiT suppresses stitching artifacts while preserving global structure.
- Mechanism: High-timestep noise corrupts high-frequency inconsistencies (seams, misalignments) more than low-frequency scene layout. The pretrained DiT, conditioned on the text prompt, reconstructs a coherent panorama from this partially corrupted signal.
- Core assumption: The chosen timestep balances artifact removal and structural preservation.
- Evidence anchors:
  - [abstract] "model-agnostic refinement step to ensure global coherence and loop-consistency"
  - [section 3.3] "By using a high timestep, we wash out high-frequency mismatches while preserving the scene's overall layout"
  - [corpus] No direct corpus comparison for this specific refinement strategy.
- Break condition: If timestep is too low, artifacts persist; if too high, scene structure degrades (see Table 5).

### Mechanism 3: Loop-Consistency via Circular Padding
- Claim: Circular padding during patched denoising enables information flow between panorama edges, ensuring seamless left-right continuity.
- Mechanism: Padding the left edge with pixels from the right edge (and vice versa) at each denoising step allows the model to condition boundary regions on content from the opposite side.
- Core assumption: This padding strategy provides sufficient context without introducing new artifacts.
- Evidence anchors:
  - [abstract] "ensure global coherence and loop-consistency"
  - [section 3.3] "we adapt a circular padding strategy [8, 9] to ensure that the edge regions receive meaningful context"
  - [corpus] Diffusion360 and PanoDiff use similar circular blending strategies.
- Break condition: If padding width is insufficient, edge inconsistencies remain; if too wide, content duplication may occur.

## Foundational Learning

- Concept: Gnomonic Projection
  - Why needed here: Converts spherical coordinates to flat tangent-plane images, enabling use of standard perspective-image models on 360° data.
  - Quick check question: Can you derive the (x, y) coordinates on a tangent plane given a point's (θ, φ) on the sphere and the tangent point (θ_t, φ_t)?

- Concept: Diffusion Transformer (DiT) Attention Mechanisms
  - Why needed here: Understanding how self-attention propagates information across spatially distant tokens explains why the grid layout promotes coherence.
  - Quick check question: In a double-stream DiT block, how are image and text features integrated before attention?

- Concept: Flow Matching (Rectified Flow)
  - Why needed here: The training objective (Equation 1) differs from traditional denoising score matching.
  - Quick check question: What does the vector field v_θ(z, t) predict in flow matching?

## Architecture Onboarding

- Component map: Text prompt → Tangent-Plane Extractor → VAE Encoder → LoRA-Finetuned SD3.5 DiT → Super-Resolution Module → ERP Stitcher → Noising at T≈800 → Patched Denoising with Circular Padding → Final Panorama

- Critical path: Text prompt → DiT generates tangent grid → SR per plane → ERP stitch → noise at T≈800 → patched denoising with circular padding → final panorama

- Design tradeoffs:
  - 18 vs. fewer/more tangent planes: More planes reduce per-plane distortion but increase grid complexity (Table 9 shows 26 planes degrades performance)
  - Timestep T_high: Lower preserves structure but leaves artifacts; higher improves quality but may alter content (Table 5)
  - Patched vs. full-image denoising: Patched enables 4K generation but may cause content duplication (Figure 19)

- Failure signatures:
  - Polar region artifacts: Tangent planes at poles have higher distortion; may appear inconsistent
  - Left-right seam discontinuity: Indicates insufficient circular padding or too-low refinement timestep
  - Duplicate objects (e.g., two moons): Patched denoising without local caption awareness
  - Texture degradation near boundaries: Caused by non-square ERP aspect ratio; requires patched denoising

- First 3 experiments:
  1. Validate grid learning: Train on 18-plane grids, visualize attention patterns between adjacent planes to confirm spatial dependency capture.
  2. Ablate refinement timestep: Run inference with T_high = 700, 800, 900; quantify tradeoff between artifact removal (DS, TangentFID) and structure preservation (FAED).
  3. Test loop-consistency: Generate panoramas with and without circular padding; measure Discontinuity Score (DS) and visually inspect left-right seam continuity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit geometric loss functions or alignment-aware objectives be integrated into the training phase to enforce consistency across adjacent tangent views, reducing the reliance on the post-hoc refinement step?
- Basis in paper: [explicit] The conclusion states, "the model lacks an explicit mechanism for enforcing consistency across adjacent tangent views," and Appendix I suggests, "alternative strategies such as stronger geometric loss functions, alignment-aware objectives... could address these issues, we leave such explorations to future work."
- Why unresolved: The current method relies on a second, separate refinement stage to harmonize overlapping regions because the primary training objective (flow-matching loss) does not explicitly enforce geometric consistency or pixel-wise alignment between adjacent tangent planes.
- What evidence would resolve it: A modification to the training loss function that minimizes the difference in overlapping regions during the initial generation, resulting in consistent panoramas without the need for the secondary equirectangular-conditioned refinement step.

### Open Question 2
- Question: How can the patched denoising strategy be augmented with local context awareness to prevent the generation of duplicate content in high-resolution outputs?
- Basis in paper: [explicit] Appendix I notes that while patched denoising aids scalability, "it can lead to duplicate content, as each patch is conditioned on the same global caption without local awareness," resulting in artifacts like "two moons generated in different patches."
- Why unresolved: The current method conditions all patches on the global text prompt to maintain scene coherence, but lacks a mechanism to communicate local object placement between patches, leading to semantic repetition in large background areas like skies.
- What evidence would resolve it: A mechanism (e.g., cross-patch attention or layout guidance) that ensures unique semantic instances across the panorama, validated by the absence of duplicate objects in qualitative tests of 4K generation.

### Open Question 3
- Question: Is it possible to unify the generation and refinement stages into a single model to eliminate the need for separate fine-tuned and pre-trained weights during inference?
- Basis in paper: [explicit] The conclusion lists as a limitation: "separate weights are required for the refinement stage. Although our chosen noise level helps preserve most image details, some local changes can still occur."
- Why unresolved: The pipeline currently necessitates switching between a LoRA-fine-tuned model for tangent-grid generation and the original pre-trained DiT for refinement, increasing storage and operational complexity.
- What evidence would resolve it: A single model architecture capable of performing both the initial tangent-plane generation and the subsequent global refinement without switching weights, while maintaining the current quality metrics (e.g., TangentFID).

## Limitations

- The method lacks an explicit mechanism for enforcing consistency across adjacent tangent views during initial generation, requiring a separate refinement stage.
- Patched denoising for high-resolution generation can lead to duplicate content artifacts, such as multiple instances of the same object in different patches.
- The pipeline requires separate weights for the generation and refinement stages, increasing storage and operational complexity.

## Confidence

- High Confidence: The core methodology of tangent-plane decomposition with joint DiT generation, and the refinement strategy using patched denoising with circular padding. The performance claims (TangentFID 35.39, TangentIS 6.06) are supported by the proposed evaluation metrics.
- Medium Confidence: The specific grid layout (18 planes, 3×6 arrangement) and its superiority over alternatives. While Table 9 provides some evidence, the exact impact of different configurations is not fully explored.
- Low Confidence: The precise implementation details of the circular padding in the refinement step and the LoRA fine-tuning configuration, which are essential for reproducing the reported results.

## Next Checks

1. **Grid Layout Validation**: Conduct ablation studies with different tangent plane counts (12, 18, 26) and arrangements to quantify the impact on TangentFID and TangentIS. Verify the 3×6 grid configuration consistently outperforms alternatives.

2. **Refinement Timestep Analysis**: Systematically evaluate the refinement step across a range of timesteps (700, 800, 900) to quantify the tradeoff between artifact removal (measured by DS and TangentFID) and content preservation (measured by FAED). Validate the claimed sweet spot at T≈800.

3. **Circular Padding Effectiveness**: Generate panoramas with and without circular padding in the refinement step. Measure Discontinuity Score (DS) and visually inspect left-right seam continuity. Quantify the minimum padding width required to eliminate visible discontinuities.