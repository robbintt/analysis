---
ver: rpa2
title: When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways
arxiv_id: '2505.24449'
source_url: https://arxiv.org/abs/2505.24449
tags:
- knowledge
- data
- performance
- forgetting
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of enabling large multimodal
  models (LMMs) to maintain up-to-date knowledge in evolving real-world scenarios.
  The authors propose EVOKE, a benchmark for evaluating knowledge injection in LMMs,
  and conduct extensive experiments on existing knowledge injection methods.
---

# When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways

## Quick Facts
- arXiv ID: 2505.24449
- Source URL: https://arxiv.org/abs/2505.24449
- Reference count: 40
- Primary result: Existing knowledge injection methods perform poorly on evolving knowledge tasks, with best accuracy at 56.13%, and supervised fine-tuning causes catastrophic forgetting

## Executive Summary
This paper addresses the critical challenge of maintaining up-to-date knowledge in large multimodal models (LMMs) as real-world information evolves. The authors introduce EVOKE, a comprehensive benchmark designed to evaluate knowledge injection capabilities in LMMs across evolving scenarios. Through extensive experimentation with existing methods like supervised fine-tuning, retrieval-augmented generation, and internet-augmented generation, the paper reveals significant performance limitations and identifies catastrophic forgetting as a major obstacle. The work proposes two promising pathways forward: text knowledge augmentation during training and continual learning techniques, particularly replay and MoELoRA methods.

## Method Summary
The authors developed EVOKE as a benchmark for evaluating knowledge injection in LMMs, then systematically tested existing knowledge injection methods including supervised fine-tuning, retrieval-augmented generation, and internet-augmented generation. They conducted experiments to measure both knowledge acquisition performance and catastrophic forgetting effects across 12 benchmark datasets. The study explored text versus image knowledge augmentation strategies and evaluated various continual learning methods including replay and MoELoRA. Performance was measured using accuracy metrics, with catastrophic forgetting quantified through degradation across benchmark datasets.

## Key Results
- Existing knowledge injection methods (supervised fine-tuning, RAG, internet-augmented generation) achieve only 56.13% accuracy on evolving knowledge tasks
- Supervised fine-tuning causes severe catastrophic forgetting, reducing performance across 12 benchmark datasets by up to 61.93%
- Text knowledge augmentation improves performance while image augmentation degrades it
- Continual learning methods, especially replay and MoELoRA, effectively mitigate forgetting with non-monotonic effects based on replay data size

## Why This Works (Mechanism)
The effectiveness of text knowledge augmentation over image augmentation likely stems from the higher density and specificity of textual information compared to visual representations for conveying evolving knowledge. Text allows for more precise updates to specific facts and concepts, while images may introduce noise or require more complex transformations. The success of replay and MoELoRA methods in mitigating catastrophic forgetting suggests these approaches better preserve previously learned knowledge representations while incorporating new information, possibly through mechanisms that maintain a balance between stability and plasticity in the model's parameters.

## Foundational Learning
- Catastrophic forgetting: The tendency of neural networks to rapidly lose previously acquired knowledge when trained on new tasks. Critical because LMMs must maintain both old and new knowledge simultaneously.
- Knowledge injection: Techniques for updating models with new information without full retraining. Essential for practical deployment where knowledge evolves continuously.
- Continual learning: Methods enabling models to learn sequentially without forgetting. Necessary for lifelong learning scenarios in real-world applications.
- Multimodal representation: How models encode and integrate information from multiple modalities (text, image). Fundamental to understanding LMM behavior during knowledge updates.
- Retrieval-augmented generation: Using external knowledge sources during inference. Important for scenarios where model parameters cannot be updated frequently.
- Supervised fine-tuning: Training on labeled data to adapt model behavior. A baseline approach that reveals the forgetting problem in multimodal contexts.

## Architecture Onboarding

**Component map**: LMM architecture (vision encoder + language model) -> Knowledge injection method (SFT/RAG/IAG) -> Evaluation on EVOKE benchmark -> Performance metrics

**Critical path**: Training pipeline where knowledge injection method modifies LMM parameters, followed by evaluation on evolving knowledge tasks and baseline benchmarks to measure both acquisition and forgetting

**Design tradeoffs**: The choice between updating model parameters (SFT) versus retrieving knowledge externally (RAG) involves balancing computational cost, latency, and the risk of catastrophic forgetting against the overhead of retrieval infrastructure and potential retrieval failures.

**Failure signatures**: Performance degradation across multiple benchmark datasets indicates catastrophic forgetting; low accuracy on evolving knowledge tasks suggests ineffective knowledge injection; inconsistent results across different augmentation strategies reveal sensitivity to input modality.

**3 first experiments**: 1) Compare text vs image augmentation effectiveness on knowledge injection; 2) Test replay buffer size effects on forgetting mitigation; 3) Evaluate MoELoRA against standard fine-tuning for preserving instruction-following capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to specific benchmark datasets, potentially limiting generalizability to broader real-world applications
- Diversity and representativeness of the 12 benchmark datasets for real-world multimodal tasks remains unclear
- Current augmentation strategies show text improving while image degrades performance, suggesting unexplored limitations in multimodal knowledge integration approaches

## Confidence
- High confidence: Existing knowledge injection methods perform poorly on evolving knowledge tasks (56.13% accuracy)
- Medium confidence: Supervised fine-tuning causes catastrophic forgetting with up to 61.93% performance reduction
- Medium confidence: Replay and MoELoRA effectively mitigate forgetting in LMMs

## Next Checks
1. Test proposed methods on additional, more diverse multimodal benchmark datasets to assess generalizability
2. Conduct ablation studies to identify critical components of text knowledge augmentation and explore alternative image augmentation strategies
3. Evaluate long-term effectiveness of replay and MoELoRA methods across multiple sequential knowledge updates to verify sustained forgetting mitigation