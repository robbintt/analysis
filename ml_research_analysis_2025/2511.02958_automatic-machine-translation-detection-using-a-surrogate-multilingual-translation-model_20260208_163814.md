---
ver: rpa2
title: Automatic Machine Translation Detection Using a Surrogate Multilingual Translation
  Model
arxiv_id: '2511.02958'
source_url: https://arxiv.org/abs/2511.02958
tags:
- smatd
- language
- translation
- systems
- tower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of distinguishing human-translated
  (HT) text from machine-translated (MT) text, which is crucial for improving machine
  translation (MT) quality by filtering out synthetic training data. The core method,
  called SMaTD (Surrogate Machine Translation Detection), leverages the internal representations
  from a pre-trained multilingual MT model (NLLB) to classify whether a given translation
  is human or machine-generated.
---

# Automatic Machine Translation Detection Using a Surrogate Multilingual Translation Model

## Quick Facts
- arXiv ID: 2511.02958
- Source URL: https://arxiv.org/abs/2511.02958
- Authors: Cristian García-Romero; Miquel Esplà-Gomis; Felipe Sánchez-Martínez
- Reference count: 19
- Primary result: SMaTD achieves accuracy gains of at least 5 percentage points over baselines, especially for non-English language pairs

## Executive Summary
This paper introduces SMaTD (Surrogate Machine Translation Detection), a novel method for distinguishing human-translated text from machine-translated text using internal representations from a pre-trained multilingual MT model. The approach leverages the hidden states from a specific decoder block of the NLLB model as features for a binary classifier. Experiments demonstrate superior performance over existing methods, particularly for non-English language pairs, and show strong generalization to unseen MT systems. The method addresses a critical need in improving MT quality by filtering synthetic training data.

## Method Summary
SMaTD uses a pre-trained multilingual MT model (NLLB-200-3.3B) as a feature extractor. For each source-target pair, the source is fed to the encoder and the target to the decoder via teacher forcing. The 10th decoder block's hidden states are extracted, projected to a consistent dimension, and fed into a small transformer classifier. The classifier outputs a binary prediction (HT/MT). A variant, SMaTD+LM, optionally combines these representations with a pre-trained language model. The model is trained on WMT news datasets across multiple language pairs with standard Transformer architectures and AdamW optimization.

## Key Results
- SMaTD outperforms state-of-the-art baselines by at least 5 percentage points in accuracy
- Middle decoder blocks (e.g., 10th) provide more informative representations than embedding or final layers
- Multilingual surrogate model provides superior detection for non-English pairs compared to English-centric models
- Strong generalization to unseen MT systems and zero-shot performance on unseen language pairs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hidden states of pre-trained multilingual MT models contain discriminative patterns for HT vs MT classification
- Mechanism: Surrogate model processes source and target via teacher forcing; internal representations from specific decoder block are extracted and classified
- Core assumption: Representations encode meaningful differences between fluent MT and HT text
- Evidence anchors: Abstract states method "leverages internal representations from pre-trained multilingual MT model"; section describes extracting "target token-level representations... from hidden states of chosen decoder block"
- Break condition: Would fail if surrogate representations don't capture distinguishing features, perhaps with extremely high-quality MT or misaligned surrogate

### Mechanism 2
- Claim: Middle decoder blocks provide more informative representations than embedding or final layers
- Mechanism: Classifier performance evaluated using hidden states from different decoder layers; middle layers identified as optimal
- Core assumption: Middle layers capture balance of semantic and structural properties relevant to HT/MT distinction
- Evidence anchors: Abstract mentions "extracts decoder block representations"; section shows "10th block achieves highest accuracy"
- Break condition: Pattern may not hold for decoder-only LLMs or different encoder-decoder architectures

### Mechanism 3
- Claim: Multilingual surrogate model provides superior detection for non-English pairs
- Mechanism: NLLB trained on diverse language pairs provides more robust representations for non-English directions
- Core assumption: English-centric model representations for non-English pairs are less discriminative due to training data imbalance
- Evidence anchors: Abstract shows "especially for non-English pairs"; section notes English-centric baseline "might face greater challenges with non-English language pairs"
- Break condition: If surrogate model is English-centric, performance on non-English pairs would likely degrade

## Foundational Learning

- **Encoder-Decoder Architecture and Teacher Forcing**: Needed to understand how MT model processes input/output and teacher forcing alters decoder operation. Quick check: What are the roles of encoder and decoder, and how does teacher forcing differ from standard autoregressive generation?

- **Hidden States/Internal Representations**: Needed because method uses intermediate layer embeddings as classifier input. Quick check: What do hidden states conceptually represent at a given time step for a given token?

- **Transfer Learning and Surrogate Models**: Needed because pre-trained NLLB is used as feature extractor for different task. Quick check: What is main advantage of using pre-trained surrogate instead of training from scratch?

## Architecture Onboarding

- **Component map**: Source sentence -> Surrogate NLLB tokenizer -> Surrogate encoder -> Surrogate decoder (teacher forcing) -> 10th decoder block hidden states -> Projection layer -> Classifier Transformer -> Feed-forward + sigmoid

- **Critical path**: 1) Source tokenized by surrogate tokenizer, 2) Target tokenized by surrogate tokenizer, 3) Source to surrogate encoder, target to surrogate decoder via teacher forcing, 4) Hidden states extracted from 10th decoder block, 5) States projected to n-dimensional space, 6) Projected sequence fed into classifier Transformer, 7) Classifier's [CLS] token passed to final sigmoid

- **Design tradeoffs**: Surrogate model size (larger better but costlier), decoder block selection (middle empirically best but requires validation), SMaTD vs SMaTD+LM (LM improves performance but adds complexity)

- **Failure signatures**: Random-guess accuracy (~50%) indicates classifier not learning; performance collapse on unseen MT systems suggests overfitting; very high standard deviation indicates instability

- **First 3 experiments**: 1) Baseline reproduction and layer ablation (vary decoder block index), 2) Surrogate model ablation (compare different NLLB sizes), 3) Cross-system generalization (train on one MT system, test on unseen system)

## Open Questions the Paper Calls Out
- How does SMaTD perform on texts from diverse domains outside news (biomedical, legal, colloquial)?
- Would decoder-only surrogate models yield better detection than encoder-decoder NLLB?
- How does detection accuracy vary with non-standard decoding strategies or older architectures?

## Limitations
- Analysis limited to news domain; robustness across other domains unexplored
- Effects of alternative decoding methods and older architectures underexplored
- Reliance on single surrogate system leaves efficacy of other architectures unknown

## Confidence
- **High Confidence**: SMaTD outperforms baselines on standard benchmarks with stated accuracy improvements
- **Medium Confidence**: Multilingual model advantage for non-English pairs needs further validation across different architectures
- **Low Confidence**: Theoretical explanation for why decoder representations contain discriminative features is limited

## Next Checks
1. Perform layer-wise feature analysis using attention visualization and feature importance scoring to understand what classifier learns
2. Test SMaTD with different multilingual MT architectures beyond NLLB to determine if "middle block is best" is general principle
3. Evaluate zero-shot language pair transfer (train on some pairs, test on completely unseen pairs) to understand true multilingual generalization capabilities