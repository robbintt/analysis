---
ver: rpa2
title: Can LLM feedback enhance review quality? A randomized study of 20K reviews
  at ICLR 2025
arxiv_id: '2504.09737'
source_url: https://arxiv.org/abs/2504.09737
tags:
- feedback
- review
- reviewer
- reviews
- reviewers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a large-scale randomized controlled trial at
  ICLR 2025 that deployed a multi-LLM system to provide feedback on peer reviews.
  The Review Feedback Agent targeted vague comments, content misunderstandings, and
  unprofessional remarks, with feedback only being sent after passing automated reliability
  tests.
---

# Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025

## Quick Facts
- arXiv ID: 2504.09737
- Source URL: https://arxiv.org/abs/2504.09737
- Reference count: 40
- Primary result: LLM-generated feedback improved peer review quality, with 27% of reviews updated and incorporating over 12,000 feedback suggestions

## Executive Summary
This paper presents a large-scale randomized controlled trial at ICLR 2025 that deployed a multi-LLM system to provide feedback on peer reviews. The Review Feedback Agent targeted vague comments, content misunderstandings, and unprofessional remarks, with feedback only being sent after passing automated reliability tests. Of the 18,946 reviews that received feedback, 27% were updated, incorporating over 12,000 feedback suggestions. Updated reviews were 80 words longer on average and rated as more informative by blinded researchers. Feedback recipients also showed greater engagement during rebuttals, with longer author-reviewer discussions and higher score revision rates.

## Method Summary
The study implemented a five-stage LLM pipeline using Claude Sonnet 3.5 to generate feedback on peer reviews. Two parallel Actors generated diverse initial feedback, which was merged by an Aggregator, validated by a Critic to remove nitpicky content, and formatted for posting. Reliability tests filtered feedback before sending to reviewers, with a 1-hour delay before posting to OpenReview. The system processed 44,831 reviews across 11,553 papers, with 18,946 receiving feedback. The randomized controlled trial assigned papers to control, half-feedback, or all-feedback groups, with intent-to-treat analysis preserving causal validity despite 7.9% non-compliance.

## Key Results
- 27% of reviews (5,063) were updated after receiving feedback
- Updated reviews incorporated 69.3% of feedback suggestions on average
- Updated reviews were 80 words longer on average and rated as more informative by blinded researchers
- Feedback recipients showed higher engagement during rebuttals with longer discussions and more score revisions

## Why This Works (Mechanism)

### Mechanism 1
Multi-stage LLM pipeline with specialized roles produces higher-quality feedback than single-LLM generation. Two parallel Actors generate diverse initial feedback → Aggregator merges best items → Critic filters for accuracy and removes nitpicky content → Formatter standardizes output. Role separation reduces instruction-following failures and hallucination propagation.

### Mechanism 2
Automated reliability tests act as necessary-but-not-sufficient quality gates, filtering low-quality feedback before human review. Four tests evaluate (1) constructive suggestions present, (2) addresses reviewer not author, (3) doesn't restate reviewer's words, (4) correct formatting. Failed feedback triggers one retry; failure after retry suppresses output entirely.

### Mechanism 3
Randomized assignment with intent-to-treat analysis enables causal attribution of review quality improvements to feedback intervention. Papers randomly assigned to control (no feedback), half-feedback, or all-feedback groups before review period. Intent-to-treat includes reviews selected for feedback but not receiving it (7.9%), diluting effect size but preserving causal validity.

## Foundational Learning

- **Intent-to-treat analysis**: Why needed here - 7.9% of selected reviews didn't receive feedback; ITT analysis preserves randomization benefits despite non-compliance. Quick check - If only 50% of selected reviews received feedback, would comparing "received feedback" vs. control still be valid causal inference?

- **Reliability testing for LLM outputs**: Why needed here - LLMs can hallucinate paper quotes or provide superficial feedback; automated tests catch common failure modes before human exposure. Quick check - What are the tradeoffs between strict tests (high false negatives) and lenient tests (high false positives) in a human-in-the-loop system?

- **Edit distance for change detection**: Why needed here - Distinguishing substantive review updates from typo fixes requires thresholding (edit distance > 5 used here). Quick check - Why might word-level edit distance fail to capture meaningful semantic changes?

## Architecture Onboarding

- **Component map**: Review submitted → 1-hour delay → Actor1 + Actor2 (parallel) → Aggregator → Critic → Formatter → Reliability tests → (pass) Post to OpenReview / (fail) Retry once → (fail) Suppress

- **Critical path**: Review submitted → 1-hour delay → Actor1 + Actor2 (parallel) → Aggregator → Critic → Formatter → Reliability tests → (pass) Post to OpenReview / (fail) Retry once → (fail) Suppress

- **Design tradeoffs**: Two parallel Actors increase diversity but double inference cost (~$0.50/review total); strict reliability tests reduce false positives but suppress ~4% of generated feedback; 1-hour delay allows typo fixes but reduces time for reviewer to act on feedback

- **Failure signatures**: Hallucinated quotes (Critic or reliability test should catch); over-filtering (>20% flagged "already well-written"); format corruption (malformed output fails reliability test); retry exhaustion (monitor 2nd-failure rate)

- **First 3 experiments**: 1) Validate reliability test accuracy with human-labeled feedback items; 2) Ablate single-LLM vs. multi-LLM pipeline; 3) Measure feedback latency impact on update rates

## Open Questions the Paper Calls Out
- Would the Review Feedback Agent be equally effective in scientific domains outside of AI/ML conferences?
- Can reasoning models (vs. current LLMs) generate more nuanced feedback for complex methodological issues?
- Do reviewers who receive AI feedback maintain higher quality standards in subsequent reviews without AI assistance?

## Limitations
- Study limited to one conference (ICLR 2025) and one LLM model (Claude Sonnet 3.5), limiting generalizability
- Cannot assess long-term impacts on reviewer skill development or paper acceptance decisions
- Automated reliability tests may not detect subtle issues like biased feedback or inappropriate suggestions

## Confidence
- **Medium** on causal effect of LLM feedback - RCT design strong but limited to one conference and model
- **Low** on long-term impacts - Study measures immediate effects only, cannot assess lasting behavioral changes
- **High** on technical implementation - Multi-stage pipeline well-documented and measurement methodology sound

## Next Checks
1. Deploy the same feedback system at multiple venues (different conferences, journals, or disciplines) to verify generalizability beyond ICLR computer science papers
2. Track paper acceptance rates and citation impacts for papers receiving feedback-improved reviews versus control groups, and survey reviewers 6-12 months later
3. Conduct human evaluation of 500+ feedback items to measure false positive and false negative rates for each reliability test, and test whether relaxing or tightening specific test criteria improves balance between feedback quality and coverage