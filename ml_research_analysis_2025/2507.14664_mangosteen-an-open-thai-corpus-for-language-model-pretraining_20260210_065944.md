---
ver: rpa2
title: 'Mangosteen: An Open Thai Corpus for Language Model Pretraining'
arxiv_id: '2507.14664'
source_url: https://arxiv.org/abs/2507.14664
tags:
- thai
- data
- language
- pipeline
- fraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of building high-quality Thai
  language corpora for pretraining language models, as existing large-scale datasets
  rely on English-centric or language-agnostic pipelines that fail to capture Thai
  script or cultural nuances, leaving risky content untreated. To solve this, the
  authors introduce Mangosteen, a 47 billion-token Thai corpus built using a Thai-adapted
  Dolma pipeline that includes custom rule-based language identification, revised
  C4/Gopher quality filters, and Thai-trained content filters for removing adult and
  gambling-related content, along with curated non-web sources such as Wikipedia,
  Royal Gazette texts, OCR-extracted books, and CC-licensed YouTube subtitles.
---

# Mangosteen: An Open Thai Corpus for Language Model Pretraining

## Quick Facts
- **arXiv ID**: 2507.14664
- **Source URL**: https://arxiv.org/abs/2507.14664
- **Reference count**: 40
- **Primary result**: Introduces Mangosteen, a 47B-token Thai corpus built with Thai-adapted filtering pipeline, improving SEA-HELM NLG from 3 to 11 points and enabling an 8B Thai model to outperform SEA-LION-v3 and Llama-3.1 on Thai benchmarks.

## Executive Summary
This paper addresses the critical need for high-quality Thai language corpora for pretraining large language models. Existing large-scale datasets rely on English-centric pipelines that fail to capture Thai script characteristics and cultural nuances, leaving problematic content untreated. The authors introduce Mangosteen, a 47 billion-token Thai corpus created through a Thai-adapted Dolma pipeline. This pipeline includes custom rule-based language identification, revised quality filters, and Thai-trained content filters for removing adult and gambling-related content, along with curated non-web sources. Systematic ablation studies demonstrate that the pipeline reduces CommonCrawl from 202 million to 25 million documents while significantly improving downstream NLG performance.

## Method Summary
The authors developed a Thai-adapted version of the Dolma pipeline to process CommonCrawl and FineWeb2 data. Key adaptations include a rule-based Thai language identifier using Unicode character ratios (≥50% Thai characters), modified C4/Gopher quality filters with Thai-specific thresholds, document-level deduplication using Bloom filters and text overlap, and FastText classifiers trained on Thai lexicons for adult and gambling content. The pipeline was validated through GPT-2 ablation studies on 10B tokens, showing incremental improvements from baseline to full pipeline. An 8B-parameter SEA-LION model was then continually pretrained on the full 47.4B token corpus, achieving state-of-the-art performance on Thai benchmarks.

## Key Results
- Mangosteen corpus contains 47.4B tokens from 30.1M documents after processing
- Pipeline reduces CommonCrawl from 202M to 25M documents while raising SEA-HELM NLG from 3 to 11 points
- 8B-parameter model continually pretrained on Mangosteen surpasses SEA-LION-v3 and Llama-3.1 by about 4 points on Thai benchmarks
- All pipeline code, cleaning manifests, corpus snapshot, and checkpoints are released for reproducibility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language-specific filtering improves downstream task performance more efficiently than language-agnostic filtering.
- Mechanism: Thai script and cultural content have unique characteristics (no explicit word boundaries, script-specific Unicode ranges, culturally-bound topics like gambling) that language-agnostic pipelines fail to capture. By tailoring filters to these specifics, the pipeline removes noise while retaining signal.
- Core assumption: The performance gains come from quality improvement per token, not just data quantity reduction.
- Evidence anchors:
  - [abstract] "pipeline trims CommonCrawl from 202M to 25M documents while raising SEA-HELM NLG from 3 to 11"
  - [section 6.1.3] Table 3 shows incremental improvement from baseline (3.09 NLG) through full pipeline (10.60 NLG) on Common Crawl
  - [corpus] No direct corpus comparison; neighbor papers (DCAD-2000, Nemotron-CC-Math) suggest similar domain-specific filtering benefits, but no Thai-specific external validation exists
- Break condition: If language-agnostic pipelines improve to handle Thai script and cultural nuances equivalently, this advantage diminishes.

### Mechanism 2
- Claim: Thai-trained content filters remove culturally-specific undesirable content that English-centric filters miss.
- Mechanism: Adult and gambling content are labeled using Thai lexicons and LLM-identified documents, then classified with FastText. This captures Thai-language gambling sites (illegal in Thailand) that language-agnostic pipelines leave untreated.
- Core assumption: The labeled training data accurately represents the undesirable content categories.
- Evidence anchors:
  - [abstract] "Thai-trained content filters for removing adult and gambling-related content"
  - [section 4.5] "We label documents as belonging to a specific class if they contain three or more distinct words from a predefined list for that class"
  - [corpus] No external corpus evidence; this appears to be a novel contribution
- Break condition: If the training data has systematic biases or gaps, harmful content will leak through or benign content will be over-filtered.

### Mechanism 3
- Claim: Custom language identification using Unicode character ratio outperforms ML-based identifiers for Thai.
- Mechanism: FastText and other language identifiers show overconfidence on documents with low Thai character ratios. A simple rule-based approach (≥50% Thai Unicode characters) is more precise and computationally efficient.
- Core assumption: Thai text can be reliably identified by character composition alone.
- Evidence anchors:
  - [section 4.2] "we compare three language identifiers: langdetect, lingua, and the FastText language identifier model, all three failed to perform this task on Thai text"
  - [appendix B] Figure 3 shows FastText assigns high confidence scores (>0.5) to documents with ≤40% Thai characters
  - [corpus] No corpus comparison available
- Break condition: If mixed-language documents (Thai-English code-switching) become prevalent, the threshold approach may discard valid content.

## Foundational Learning

- Concept: **Data cleaning pipelines for pretraining corpora**
  - Why needed here: The entire contribution is a modified Dolma pipeline; you must understand what Dolma does (language ID, quality filters, deduplication, content filters) to grasp the adaptations.
  - Quick check question: Can you name the four stages of the Dolma pipeline and what each does?

- Concept: **C4 and Gopher quality filtering rules**
  - Why needed here: The paper modifies these rule sets for Thai; you need to know the original rules (word count thresholds, punctuation requirements, duplicate line detection) to understand why modifications were necessary.
  - Quick check question: Why would the C4 "lines with no ending punctuation" rule be inappropriate for Thai?

- Concept: **Ablation studies for data pipeline validation**
  - Why needed here: The paper validates each pipeline component by training GPT-2 on 10B tokens per configuration; understanding this methodology is essential to interpret Table 3.
  - Quick check question: Why did the deduplication step show a temporary drop in SEA-HELM score before content filters restored it?

## Architecture Onboarding

- Component map:
  Raw CommonCrawl / FineWeb2 → Text Extraction (Trafilatura) → Language ID (ThaiCharRatioTagger ≥50% Thai Unicode) → Quality Filters (Modified C4 + Gopher rules) → Deduplication (URL + Document-level text overlap) → Content Filters (FastText classifiers adult, gambling, PII) → Cleaned Corpus (47B tokens)

- Critical path: Language ID → Quality filters → Content filters. These three stages account for most document removal (202M → 25M). Deduplication is secondary in volume impact.

- Design tradeoffs:
  - Rule-based language ID vs. ML-based: Precision gains vs. potential over-filtering of mixed-language content
  - Aggressive word count threshold (200 vs. 50): Removes more low-quality text but may discard valid short documents
  - Document-level vs. paragraph-level deduplication: Thai paragraph boundaries are unreliable due to inconsistent newline usage

- Failure signatures:
  - Gambling content persists in output: Content filter training data may lack coverage of new gambling site patterns
  - Valid Thai documents rejected: Language ID threshold too strict for code-switching content
  - Degraded NLU performance after CPT: Over-filtering removes world-knowledge content (Table 6 shows NLU drop vs. NLG gain)

- First 3 experiments:
  1. **Language ID threshold sensitivity**: Vary Thai character ratio threshold (30%, 50%, 70%) and measure downstream NLG scores to find optimal precision-recall tradeoff.
  2. **Content filter false positive analysis**: Sample 100 rejected documents from each content filter category for human review to identify systematic over-filtering patterns.
  3. **Pipeline stage ablation with different tokenizer**: Repeat Table 3 experiments using Llama-3 tokenizer (instead of GPT-2) to verify that improvements transfer across tokenization schemes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can paragraph-level deduplication be effectively implemented for Thai text when newline characters are unreliable paragraph boundary indicators?
- Basis in paper: [explicit] "Dolma's paragraph-level deduplication is ineffective for Thai web data since the UTF-8 newline (\n) is not always a good indicator of the paragraph boundary in Thai text. Therefore, we only apply deduplication at the document level."
- Why unresolved: The paper implements only document-level deduplication as a workaround, leaving paragraph-level deduplication unaddressed for Thai.
- What evidence would resolve it: Development and ablation of a Thai-specific paragraph segmentation method showing improved downstream performance over document-level deduplication alone.

### Open Question 2
- Question: What explains the trade-off where Thai-focused pretraining improves NLG performance while degrading NLU, and can this be mitigated?
- Basis in paper: [explicit] "Our model outperforms other models on the NLG task, but performs lower than other models on the NLU task... world knowledge will also disappear from our model, resulting in lower performance in NLU."
- Why unresolved: The paper observes and hypothesizes about this trade-off but does not experiment with solutions such as data mixing ratios or curriculum strategies.
- What evidence would resolve it: Experiments varying Thai vs. multilingual/general knowledge data ratios, measuring both NLU and NLG benchmarks.

### Open Question 3
- Question: How can data pipeline hyperparameters (e.g., quality filter thresholds) be optimized without prohibitive computational cost?
- Basis in paper: [explicit] "Ideally, the optimal value for the lower bound should be determined through a series of data ablation experiments. However, due to limited resources and computational constraints, we were unable to perform such extensive evaluations."
- Why unresolved: The paper uses heuristic thresholds based on limited analysis; systematic optimization remains infeasible with current compute requirements.
- What evidence would resolve it: Development of low-compute proxy metrics that correlate well with full-scale downstream performance, validated against ablation studies.

### Open Question 4
- Question: How well do ablation findings from small models (124M parameters) transfer to production-scale models?
- Basis in paper: [inferred] Ablations used GPT-2 124M, while the final model is 8B parameters; the paper acknowledges only one full trial was possible due to compute constraints.
- Why unresolved: No systematic comparison of whether pipeline component rankings remain consistent across scales.
- What evidence would resolve it: Multi-scale ablation experiments comparing relative effectiveness of pipeline components at different model sizes.

## Limitations

- **Limited external validation**: No comparison with other Thai corpora or models beyond the specific benchmarks used.
- **Potential over-filtering concerns**: Aggressive quality filtering may remove world knowledge, leading to NLU degradation (Table 6).
- **Thai-specific scope**: Adaptations are tailored to Thai; effectiveness for other languages or multilingual contexts is unknown.

## Confidence

**High confidence**: Corpus size and composition claims (47.4B tokens from 30.1M documents) are verifiable through released manifests and code. Basic pipeline architecture and document reduction statistics (202M → 25M) are reproducible.

**Medium confidence**: Thai-specific filtering improvements from 3 to 11 SEA-HELM NLG points are supported by ablation studies, but magnitude could vary with different models or Thai evaluation sets.

**Low confidence**: Claims about being the "largest Thai corpus" and "first Thai-adapted" content filters lack comprehensive survey evidence for verification.

## Next Checks

1. **Cross-tokenizer validation**: Repeat the GPT-2 ablation experiments (Table 3) using the Llama-3 tokenizer for both training and evaluation to verify that improvements transfer across tokenization schemes.

2. **Content filter adversarial testing**: Generate 100+ synthetic Thai gambling documents with variations in vocabulary, spelling, and formatting that evade the current word-list-based detection, then measure false negative rates to quantify actual safety coverage gaps.

3. **Knowledge preservation ablation**: Create a controlled experiment where the pipeline is modified to preserve documents containing specific types of world knowledge (identified through topic modeling or entity recognition) and measure the tradeoff between NLU retention and NLG improvement.