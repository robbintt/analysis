---
ver: rpa2
title: 'ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded
  Platforms'
arxiv_id: '2601.08166'
source_url: https://arxiv.org/abs/2601.08166
tags:
- features
- energy
- learning
- performance
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ZeroDVFS, a model-based hierarchical multi-agent
  reinforcement learning framework for thermal- and energy-aware DVFS and task-to-core
  allocation on embedded multi-core platforms. The approach uses two collaborative
  agents to decompose the exponential action space, achieving 358ms latency for subsequent
  decisions and 3.5-8.0s for first decisions including one-time LLM feature extraction.
---

# ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms

## Quick Facts
- arXiv ID: 2601.08166
- Source URL: https://arxiv.org/abs/2601.08166
- Reference count: 40
- Primary result: 7.09× better energy efficiency and 4.0× better makespan than Linux ondemand governor across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 platforms.

## Executive Summary
ZeroDVFS introduces a hierarchical multi-agent reinforcement learning framework for thermal- and energy-aware dynamic voltage and frequency scaling (DVFS) and task-to-core allocation on embedded multi-core platforms. The approach uses two collaborative agents to decompose the exponential action space, achieving 358ms latency for subsequent decisions and 3.5-8.0s for first decisions including one-time LLM feature extraction. An accurate environment model enables zero-shot deployment for new workloads by generating synthetic training data without workload-specific profiling. Experiments demonstrate 7.09× better energy efficiency and 4.0× better makespan than Linux ondemand governor, with first-decision latency 8,300× faster than table-based profiling.

## Method Summary
ZeroDVFS employs hierarchical multi-agent reinforcement learning with two collaborative agents: a Profiler Agent that determines core count and frequency based on workload performance/energy states, and a Temperature Agent that assigns core priorities based on thermal states. The framework uses LLM-based semantic feature extraction to characterize OpenMP programs through 13 code-level features without execution, enabling zero-shot prediction for unseen workloads. A model-based Dyna-Q approach trains regression models to predict next-state thermal and performance metrics, allowing synthetic data generation for zero-shot deployment. The system achieves 358ms latency for subsequent decisions and 3.5-8.0s for first decisions including one-time LLM feature extraction.

## Key Results
- 7.09× better energy efficiency compared to Linux ondemand governor
- 4.0× better makespan performance across multiple embedded platforms
- 358ms latency for subsequent scheduling decisions
- 8,300× faster first-decision latency compared to table-based profiling approaches
- 20× faster convergence through model-based RL with synthetic data generation

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical multi-agent RL decomposes the exponential action space of joint DVFS and core allocation into tractable sub-problems. Two collaborative agents operate sequentially—a Profiler Agent determines core count and frequency based on workload performance/energy states, while a Temperature Agent assigns core priorities based on thermal states. This reduces the action space from O(m^n) to O(m×m + m×n + m) for m cores and n frequency levels.

### Mechanism 2
Model-based RL with an accurate environment model enables zero-shot deployment on new workloads by generating synthetic training data. A Dyna-Q-inspired framework trains regression models (FCN, Conv1D) to predict next-state thermal and performance metrics. During planning phases, these models generate synthetic transitions without real hardware execution. Combined with LLM-extracted features, the model generalizes to unseen programs.

### Mechanism 3
LLM-based semantic feature extraction enables prediction for unseen programs without execution by replacing benchmark identifiers with interpretable code characteristics. Three LLMs extract 13 semantic features (algorithmic complexity, memory access patterns, parallelization characteristics) via zero-shot prompting. These features, combined with 17 Tree-sitter syntactic features, allow the prediction model to generalize to novel programs.

## Foundational Learning

- **Dyna-Q Model-Based RL**: Understanding how direct RL (real environment interaction) integrates with planning (simulated experiences from learned models) is essential for grasping the 20× convergence speedup claim. Quick check: Can you explain why planning with synthetic data reduces sample requirements compared to pure model-free Q-learning?

- **Hierarchical Action Decomposition in RL**: The core architectural decision to use two agents (Profiler + Temperature) rather than one monolithic agent determines the tractability of the action space. Quick check: If you have 6 cores and 12 frequency levels, what is the theoretical action space size for a single-agent approach vs. the hierarchical approach?

- **LLM Feature Extraction for Code Analysis**: The zero-shot generalization claim hinges on understanding how semantic features differ from syntactic features and why they enable transfer. Quick check: Why would "algorithmic complexity O(n log n)" be more useful for generalization than a benchmark identifier "FFT"?

## Architecture Onboarding

- **Component map**: Profiler Agent (D3QN) -> Temperature Agent (D3QN) -> Environment Model (FCN/Conv1D) -> Feature Extraction Pipeline (Tree-sitter -> LLM API)

- **Critical path**: 
  1. New program arrives → Tree-sitter extracts syntactic features (~50ms)
  2. LLM extracts semantic features (3-8s one-time)
  3. RL inference: Profiler Agent selects (cores, freq) → Temperature Agent assigns priorities (358ms)
  4. Action executed on hardware → Observe next state → Store in real replay buffer
  5. Planning phase: Environment model generates synthetic transitions → Store in simulated buffer
  6. Train both D3QN agents on combined real+simulated minibatches

- **Design tradeoffs**: 
  - FCN vs. LSTM for environment model: FCN achieves 2.3ms inference with 0.400 MSE; LSTM achieves better accuracy (0.357 MSE) but 14.2ms latency
  - Single LLM vs. ensemble: DeepSeek alone costs $0.0015/program; all three cost $0.018 but provide consensus validation
  - Planning steps: More steps improve sample efficiency but increase training time; paper uses threshold-based triggering

- **Failure signatures**:
  - High prediction MAPE on new platform (>70%): Indicates domain shift requiring fine-tuning samples (5-20 samples reduce MAPE)
  - Low inter-model agreement on critical features (<40%): Features like "false sharing risk" (14.3% agreement) should be down-weighted
  - Thermal throttling despite Temperature Agent: Reward function may need c_th/c_st recalibration for different thermal thresholds
  - Python inference latency exceeds 400ms: Production deployment requires C++/TensorRT optimization

- **First 3 experiments**:
  1. Validate environment model accuracy: Train FCN on Jetson TX2 profiler data, report MSE on held-out configurations. Target: <0.5 MSE.
  2. Measure convergence speedup: Train MAMBRL D3QN vs. model-free MAMFRL on FFT benchmark. Target: ~20 episodes vs. 400+ for convergence.
  3. Test zero-shot transfer: Train on TX2, evaluate prediction MAPE on Orin NX/RubikPi without fine-tuning. Then add 10 fine-tuning samples.

## Open Questions the Paper Calls Out

- **Multi-file project analysis**: The current pipeline processes single files, potentially missing semantic context across files in large projects. Future work includes extending the framework to handle multi-file software projects through hierarchical analysis or improved concatenation strategies.

- **Concurrent workload scenarios**: The current design focuses on single DAG OpenMP workloads; concurrent tasks introduce complex resource contention and thermal interactions not currently modeled. Extending the framework to handle competing workloads is identified as a primary direction for future work.

- **On-device LLM inference**: Local deployment creates thermal and memory overhead that may conflict with the scheduler's optimization goals. The framework requires unloading the LLM model before the mission-critical workload begins to avoid interfering with thermal efficiency gains.

## Limitations

- Cross-platform transfer reliability shows 64-73% MAPE for zero-shot deployment on new hardware, suggesting significant domain shift that limits the "zero-shot" claim for heterogeneous embedded platforms.
- LLM extraction latency (3-8s one-time) and cost ($0.018 for three-model ensemble) may constrain scalability for large program repositories.
- Single core performance constraint during profiling prevents measurement of core-to-core performance variations, potentially missing important heterogeneity effects.

## Confidence

- **High Confidence**: Hierarchical multi-agent decomposition reduces action space and improves latency (358ms inference, 8,300× faster than table-based profiling).
- **Medium Confidence**: Model-based RL with synthetic data generation achieves 20× faster convergence and enables zero-shot transfer, though cross-platform accuracy degrades significantly.
- **Low Confidence**: Zero-shot deployment claim for entirely new platforms without any profiling data, given the high MAPE observed in cross-platform experiments.

## Next Checks

1. **Domain Shift Validation**: Test ZeroDVFS on a fourth platform (not in the original four) to quantify zero-shot transfer error and validate the few-shot fine-tuning claims.

2. **Feature Robustness Analysis**: Systematically evaluate the impact of removing low-consensus LLM features (e.g., false sharing risk at 14.3% agreement) on prediction accuracy and generalization.

3. **Runtime Overhead Measurement**: Deploy ZeroDVFS on a real-time embedded system and measure the impact of 358ms decision latency on task scheduling deadlines and system responsiveness.