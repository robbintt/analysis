---
ver: rpa2
title: 'TOC-UCO: a comprehensive repository of tabular ordinal classification datasets'
arxiv_id: '2507.17348'
source_url: https://arxiv.org/abs/2507.17348
tags:
- datasets
- class
- classes
- ordinal
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TOC-UCO, a comprehensive repository of 46
  tabular ordinal classification datasets designed to address the lack of standardized
  benchmarking resources in ordinal classification research. The repository improves
  upon previous collections by offering a larger number of datasets with better class
  distributions, natural class imbalances, and diverse numbers of classes (3 to 10).
---

# TOC-UCO: a comprehensive repository of tabular ordinal classification datasets

## Quick Facts
- **arXiv ID:** 2507.17348
- **Source URL:** https://arxiv.org/abs/2507.17348
- **Reference count:** 35
- **Primary result:** Introduces TOC-UCO repository with 46 tabular ordinal classification datasets and demonstrates ordinal-specific methods outperform nominal approaches on ordinal metrics

## Executive Summary
This paper introduces TOC-UCO, a comprehensive repository of 46 tabular ordinal classification datasets designed to address the lack of standardized benchmarking resources in ordinal classification research. The repository improves upon previous collections by offering a larger number of datasets with better class distributions, natural class imbalances, and diverse numbers of classes (3 to 10). Datasets are preprocessed under a unified framework, including k-means-based discretization for regression problems, and stratified train-test partitions are provided for reproducibility.

Baseline experiments using eight different methods (Ridge, Random Forest, XGBoost, and four ordinal-specific models) show that ordinal classifiers like Logistic All-Threshold (LogAT) and the Ensemble Based on unimodal Ordinal (EBANO) outperform nominal methods on ordinal metrics such as AMAE and MMAE, while EBANO achieves the best results on QWK and BACC. This repository provides a robust, reproducible benchmark for advancing ordinal classification research.

## Method Summary
The TOC-UCO repository contains 46 tabular datasets: 24 discretized regression datasets and 22 originally ordinal classification datasets across health, economics, and environmental science domains. All datasets undergo standardized preprocessing including k-means discretization with 5-95 percentile outlier filtering for regression targets, class merging for extreme imbalance, and balanced class weighting. The repository provides 30 fixed stratified 70-30 train-test splits for reproducibility. Eight baseline methods are evaluated including Ridge, Random Forest, XGBoost, MLP, LogAT, MLP-CLM, MLP-T, and EBANO ensemble, with hyperparameters tuned via 3-fold cross-validation on training sets.

## Key Results
- LogAT achieves best performance on ordinal metrics (AMAE: 0.735±0.035, MMAE: 1.086±0.098) among individual models
- EBANO ensemble achieves highest QWK (0.736±0.023) and best overall ranking across all metrics
- Ordinal-specific methods consistently outperform nominal classifiers on ordinal metrics
- K-means discretization produces more natural class distributions than previous equal-frequency approaches

## Why This Works (Mechanism)

### Mechanism 1: K-means discretization preserves natural ordinal structure
K-means clustering identifies natural breakpoints in continuous target distributions by finding cluster centers that minimize within-bin variance. This preserves inherent imbalances rather than artificially forcing equal class sizes. The paper additionally filters outliers (below 5th and above 95th percentiles) before clustering to prevent threshold distortion. This works because real-world ordinal phenomena exhibit non-uniform class distributions where extreme categories (e.g., "severe" or "healthy") naturally occur less frequently than intermediate states.

### Mechanism 2: Ordinal-specific loss functions capture class proximity information
Cumulative link models (CLM) used in LogAT and MLP-CLM model P(Y ≤ k | X) rather than P(Y = k | X), introducing a threshold structure that naturally encodes ordering. The all-thresholds loss penalizes violations of all relevant thresholds simultaneously, ensuring predictions respect ordinal structure. This mechanism works because the cost of misclassification scales with ordinal distance—predicting class 1 when true is 5 incurs higher cost than predicting class 4.

### Mechanism 3: Ensemble over diverse ordinal representations improves robustness
EBANO ensemble (LogAT + MLP-CLM + MLP-T) achieves best QWK and BACC by combining complementary ordinal modeling approaches. Each component captures ordinal structure differently: LogAT uses cumulative links with all-thresholds loss, MLP-CLM uses neural network with CLM output layer, and MLP-T uses soft labels from triangular distributions. Cross-validated weighting based on AMAE ensures the ensemble optimizes for ordinal performance while benefiting from model diversity. This works because different ordinal representations capture complementary aspects of the ordinal relationship, and their errors are not perfectly correlated.

## Foundational Learning

- **Concept: Ordinal vs. Nominal Classification Distinction**
  - Why needed here: The entire repository exists because treating ordered categories (e.g., disease stages, quality ratings) as unordered nominal labels discards valuable structure. Understanding this distinction determines whether ordinal methods are appropriate for your problem.
  - Quick check question: If predicting customer satisfaction {Very Unsatisfied, Unsatisfied, Neutral, Satisfied, Very Satisfied}, explain why predicting "Very Unsatisfied" when true is "Very Satisfied" should be penalized more than predicting "Satisfied."

- **Concept: Imbalanced Ordinal Metrics (AMAE, MMAE, QWK)**
  - Why needed here: The baseline results show different methods excel on different metrics. You cannot interpret results or select models without understanding what each metric captures—AMAE averages per-class MAE, MMAE captures worst-case class, QWK measures agreement with quadratic penalty.
  - Quick check question: Why might a model achieving 90% accuracy still have unacceptable AMAE for a medical staging application?

- **Concept: Cumulative Link Models (CLM)**
  - Why needed here: Two of the top-performing baseline methods (LogAT, MLP-CLM) are CLM-based. Understanding how threshold parameters define class boundaries and how the cumulative probability structure encodes ordering is essential for implementing or extending these approaches.
  - Quick check question: In a CLM with Q=5 classes, how many threshold parameters are estimated, and how do they relate to P(Y ≤ k | X)?

## Architecture Onboarding

- **Component map:**
  - Data layer (46 preprocessed datasets) -> Partition layer (30 fixed stratified splits) -> Preprocessing layer (k-means discretization, class merging) -> Evaluation layer (AMAE, MMAE, QWK, BACC)

- **Critical path:**
  1. Clone repository and load dataset using provided tutorial notebook
  2. Extract indices for specific split (0-29) to get train/test partition
  3. Implement ordinal classifier starting with LogAT baseline
  4. Compute all four metrics across 30 splits
  5. Compare mean±std against Table 6 baselines to validate implementation

- **Design tradeoffs:**
  - Using all 46 datasets provides comprehensive validation but increases compute; selecting by domain or Q value may be appropriate for focused studies
  - AMAE/MMAE align with ordinal structure but may conflict with BACC; paper shows no single method dominates all metrics
  - K-means produces natural distributions but requires access to original regression targets; if only ordinal labels available, this preprocessing is not applicable

- **Failure signatures:**
  - High BACC, poor AMAE/MMAE: Model treating problem as nominal—check if ordinal loss/constraints are active
  - Large std across splits: High variance suggests sensitivity to partition; may need regularization or ensemble
  - Performance degrades with higher Q: Methods may not scale to many classes; consider architecture modifications or class merging
  - Extreme class predictions missing: Model avoiding minority classes; verify class weighting is applied

- **First 3 experiments:**
  1. Implement LogAT with cross-validated regularization strength {10⁻³, 10⁻², ..., 10³} and max iterations {1000, 1500, 3000, 5000} per Table 5; verify AMAE ≈ 0.735±0.035 on TOC-UCO
  2. Compare LogAT (ordinal) against Ridge (nominal) on same splits; quantify performance gap on ordinal metrics vs. BACC to justify ordinal approach
  3. Apply k-means discretization with varying Q values on new regression problem, compute CIR to select optimal class count, then compare resulting distribution against equal-frequency baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of performance metric (ordinal error vs. inter-rater agreement) dictate the optimal model selection on the TOC-UCO benchmark, and is it possible for a single architecture to simultaneously minimize error magnitude (AMAE) and maximize ranking agreement (QWK)?
- Basis in paper: The baseline results explicitly show a divergence where Logistic All-Threshold (LogAT) achieves the best ranking for AMAE and MMAE, while EBANO and XGBoost achieve superior rankings for QWK and BACC.
- Why unresolved: The paper presents these conflicting results as baseline facts but does not investigate the structural reasons why specific ordinal models excel at error minimization while ensemble methods excel at agreement maximization.
- What evidence would resolve it: A comparative analysis of the specific datasets where LogAT outperforms EBANO on AMAE but loses on QWK, potentially involving the analysis of prediction distributions relative to the ground truth ordinal scale.

### Open Question 2
- Question: Does the k-means discretization strategy used for regression datasets introduce a geometric bias that favors centroid-based or linear ordinal classifiers over tree-based ensembles compared to the previous equal-frequency binning approach?
- Basis in paper: The authors replace the "Previous" equal-frequency discretization with k-means to achieve a "natural" distribution, but the subsequent baseline results might reflect the interaction between this specific clustering geometry and the tested classifiers.
- Why unresolved: While the paper demonstrates that the new repository yields different class distributions, it does not isolate the discretization method as an independent variable to test if it systematically alters the relative performance hierarchy of the baseline models.
- What evidence would resolve it: Ablation studies comparing model performance on the same underlying regression data discretized by both k-means and equal-frequency methods to observe shifts in model rankings.

### Open Question 3
- Question: Do soft-labeling approaches (e.g., MLP-T) provide statistically significant performance gains specifically on "assessed ordered categorical" datasets compared to "discretised regression" datasets, given the likelihood of label noise in expert-assessed data?
- Basis in paper: Section 2.2 states that "assessed ordered categorical" problems suffer from noise due to expert subjectivity, and explicitly identifies soft labels as an efficient strategy for such noise, yet the baseline experiments show mixed results for the soft-label model (MLP-T).
- Why unresolved: The average baseline results for MLP-T are not competitive with the top performers, leaving it unclear if soft labels successfully mitigate the specific noise they were designed for in this new benchmark context.
- What evidence would resolve it: A granular analysis of MLP-T performance specifically on the subset of "assessed" datasets versus the "discretised" datasets to check for distinct performance profiles.

## Limitations

- The paper provides weak corpus evidence for the claimed superiority of k-means discretization over equal-frequency binning, relying primarily on a single visualization comparison
- Software version dependencies and exact MLP architecture details remain unspecified, potentially affecting reproducibility of baseline results
- The performance divergence across metrics suggests no single model dominates all evaluation criteria, requiring careful metric selection based on application goals

## Confidence

- **Dataset construction:** High - Well-documented with 30 fixed stratified splits and explicit preprocessing steps
- **Baseline method implementation:** High - Detailed hyperparameter grids and reproducible training procedures provided
- **k-means discretization claims:** Medium - Claims supported by repository data but weak corpus evidence
- **EBANO ensemble superiority:** Medium - Results supported by experimental data but limited corpus evidence on ordinal ensembles specifically

## Next Checks

1. **Discretization method comparison:** Apply both k-means and equal-frequency discretization to a held-out regression dataset, then measure class imbalance preservation (CIR) and compare resulting ordinal classification performance across 30 splits.

2. **Ordinal vs nominal ablation study:** Implement a nominal classifier (Ridge) and ordinal classifier (LogAT) on the same TOC-UCO datasets, measuring the performance gap specifically on AMAE and MMAE to quantify the benefit of ordinal modeling for your specific domain.

3. **Ensemble component analysis:** Train each EBANO component (LogAT, MLP-CLM, MLP-T) individually on TOC-UCO datasets and compute correlation of errors across 30 splits to verify that component diversity justifies the computational overhead of ensembling.