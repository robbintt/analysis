---
ver: rpa2
title: 'RLMEval: Evaluating Research-Level Neural Theorem Proving'
arxiv_id: '2510.25427'
source_url: https://arxiv.org/abs/2510.25427
tags:
- proof
- rlmeval
- theorem
- lean
- projects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RLMEval is a new benchmark for neural theorem proving and proof\
  \ autoformalization targeting research-level mathematics from real Lean formalization\
  \ projects. It focuses on blueprint theorems\u2014major conceptual results rather\
  \ than routine lemmas\u2014from six active Lean projects, totaling 613 theorems."
---

# RLMEval: Evaluating Research-Level Neural Theorem Proving

## Quick Facts
- arXiv ID: 2510.25427
- Source URL: https://arxiv.org/abs/2510.25427
- Authors: Auguste Poiroux; Antoine Bosselut; Viktor Kunčak
- Reference count: 8
- Primary result: 10.3% pass rate on proof autoformalization in normal mode, improving to 16.7% with auxiliary lemmas

## Executive Summary
RLMEval introduces a benchmark for evaluating neural theorem proving and proof autoformalization on research-level mathematics. The benchmark focuses on blueprint theorems—major conceptual results from six active Lean formalization projects—totaling 613 theorems. These theorems represent high-level steps in mathematical development rather than routine lemmas, making them more challenging and realistic for evaluating models' ability to tackle research-level formal mathematics. The evaluation reveals significant performance gaps between current state-of-the-art models and the demands of research-level theorem proving.

## Method Summary
RLMEval evaluates neural theorem proving (NTP) and proof autoformalization on 613 blueprint theorems from six Lean 4 projects. For each theorem, models generate up to 128 proof samples using task-specific prompts and hyperparameters. Proofs are verified via Lean compilation. Two evaluation modes exist: "easy" (access to all project lemmas) and "normal" (blueprint lemmas only). The benchmark uses in-file context truncation for premise access and reports pass@k rates as the primary metric.

## Key Results
- DeepSeek-Prover-V2-7B achieves the highest performance at 10.3% pass rate for proof autoformalization in normal mode
- Access to auxiliary lemmas improves performance by up to 6 percentage points (14.7% to 16.7% pass rate)
- Performance varies significantly by project difficulty, with simpler domains like FLT3 (14.8%) outperforming complex ones like Carleson (2.4%)
- Providing informal proofs offers only modest benefit (~1.5 percentage points)

## Why This Works (Mechanism)

### Mechanism 1: Blueprint Theorem Selection as Difficulty Proxy
Selecting blueprint theorems rather than auxiliary lemmas creates a more challenging evaluation that better predicts research-level utility. Blueprint theorems average 16.6 lines of proof versus 6.6 for auxiliary lemmas, representing deeper reasoning chains and more conceptual integration.

### Mechanism 2: Auxiliary Lemma Access Reduces Search Space
In "easy" mode, models receive context including non-blueprint lemmas already proven in the project. This provides pre-verified building blocks, narrowing the proof search space from discovering both strategy AND lemmas to primarily strategy selection.

### Mechanism 3: Informal Proof Guidance with Diminishing Returns
Providing informal proofs alongside formal statements offers modest performance gains (~1.5 percentage points), suggesting current models can partially but not fully leverage natural language reasoning structure.

## Foundational Learning

- **Concept: Lean 4 Proof Assistant**
  - Why needed here: RLMEval evaluates proofs in Lean 4 syntax; understanding what constitutes a valid Lean proof is essential for interpreting results.
  - Quick check question: Can you explain why a proof that type-checks is considered valid, and what role the `#check` command plays in Lean?

- **Concept: pass@k Evaluation Metric**
  - Why needed here: All results use pass@k; misunderstanding this metric leads to misinterpreting model capabilities.
  - Quick check question: If a model achieves pass@128 = 10%, what does this imply about its pass@1 performance, and why can't you directly compare pass@128 to pass@8 from other benchmarks?

- **Concept: Proof Autoformalization vs. Neural Theorem Proving**
  - Why needed here: RLMEval evaluates both tasks separately; understanding the distinction is necessary for interpreting the evaluation design.
  - Quick check question: Given an informal proof of "the square root of 2 is irrational," what additional information would a model need for proof autoformalization that it wouldn't need for neural theorem proving?

## Architecture Onboarding

- **Component map:**
  LeanInteract -> Blueprint extraction -> Context preparation -> Evaluation harness

- **Critical path:**
  1. Install LeanInteract and configure for target Lean version
  2. Clone target Lean blueprint project and build dependencies
  3. Extract blueprint theorem identifiers from project's informal blueprint
  4. For each theorem: prepare context, invoke model, collect k samples
  5. Verify each sample via Lean compilation, compute pass@k

- **Design tradeoffs:**
  - Easy vs. Normal mode: Easy mode uses all project lemmas (higher pass rates but less realistic)
  - Blueprint-only vs. all theorems: Blueprint filtering increases difficulty but reduces benchmark size
  - In-file context vs. project-wide retrieval: Current design uses simple truncation; more sophisticated RAG could improve but introduces retrieval complexity

- **Failure signatures:**
  - Low pass rates on simpler domains: May indicate context preparation issues or version mismatch
  - Model generates verbose/repetitive proofs: Suggests sampling temperature too high
  - Pass rate doesn't improve with k: Indicates systematic errors rather than exploration issues
  - Compilation errors on known-valid context: Lean version mismatch or dependency issue

- **First 3 experiments:**
  1. Baseline reproduction: Run Llemma-7B on FLT3 (easiest domain) in normal mode with k=32
  2. Context ablation: Compare in-file context vs. statement-only on a 20-theorem subset
  3. Lemma retrieval prototype: Implement simple similarity-based lemma retrieval from full project

## Open Questions the Paper Calls Out

### Open Question 1
Can retrieval-augmented generation or premise-filtering techniques improve model performance on research-level theorem proving beyond simple in-file context truncation? The paper suggests future work should investigate optimal context retrieval strategies for these research-level tasks.

### Open Question 2
How can models better leverage informal proofs that contain cryptic shorthand references without broader project context? The paper notes that informal proofs in RLMEval are often terse and reference external theorems.

### Open Question 3
Can automated lemma discovery or generation techniques close the 6% performance gap between "easy" and "normal" evaluation modes? The paper identifies this disparity as demonstrating the critical role of auxiliary lemmas.

## Limitations
- Small benchmark size (613 theorems) constrains statistical power for comparing performance across evaluation modes
- Cross-entropy-based model selection without direct performance validation creates uncertainty about optimal model choice
- The "blueprint theorem" concept lacks systematic validation across diverse mathematical domains

## Confidence
- **High Confidence**: Comparative performance ranking of models and overall pass rate statistics
- **Medium Confidence**: Mechanism that blueprint theorems are more challenging than auxiliary lemmas
- **Medium Confidence**: Modest benefit of informal proof access (~1.5 percentage points)

## Next Checks
1. Conduct significance testing on performance differences between evaluation modes and across projects using bootstrapping methods
2. Perform expert validation across all six projects to confirm blueprint theorems consistently represent most conceptually challenging results
3. Test whether models selected via cross-entropy outperform or underperform models selected via direct performance on a held-out theorem proving task