---
ver: rpa2
title: 'Optimal word order for non-causal text generation with Large Language Models:
  the Spanish case'
arxiv_id: '2502.14451'
source_url: https://arxiv.org/abs/2502.14451
tags:
- language
- generation
- order
- causal
- non-causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the limitations of causal (left-to-right)
  language models for non-English languages like Spanish, which has flexible word
  order. The authors propose a novel Viterbi algorithm-based method to estimate the
  maximum-likelihood generation order for Spanish text using non-causal language models.
---

# Optimal word order for non-causal text generation with Large Language Models: the Spanish case

## Quick Facts
- **arXiv ID**: 2502.14451
- **Source URL**: https://arxiv.org/abs/2502.14451
- **Reference count**: 40
- **Primary result**: Non-causal LMs generate Spanish text in orders differing from causal models, often favoring non-SVO structures and right-to-left sequences.

## Executive Summary
This paper addresses the limitations of causal language models for non-English languages like Spanish, which has flexible word order. The authors propose a Viterbi algorithm-based method to estimate the maximum-likelihood generation order for Spanish text using non-causal language models. They compare the generation probabilities of this optimal order with those of causal models and analyze the relationship between optimal and causal orders using Spearman's rank correlation. Results show that causal models favor English-like SVO structures, reducing the richness of Spanish syntax. The optimal generation order often differs significantly from causal order and is influenced by sentence structure, suggesting that non-causal models could better capture Spanish linguistic nuances.

## Method Summary
The paper presents a Viterbi algorithm-based methodology for maximum likelihood word order estimation in non-causal language models. The approach treats word ordering as a hidden Markov model where states represent all possible masked token combinations of a sentence. The Viterbi algorithm computes the optimal generation sequence by maximizing the product of transition probabilities from a non-causal language model. The authors compare these optimal generation probabilities against those from a causal model (GPT-2) using the same Spanish corpus. They evaluate the relationship between optimal and causal orders using Spearman's rank correlation coefficient, analyzing how the maximum likelihood estimator differs from the sequential (left-to-right) generation order typical of causal models.

## Key Results
- Causal NLG prefers English-like SVO structures, with SVO having highest causal probability (2.428×10⁻¹⁰) while non-SVO structures have lower probabilities
- Optimal generation order often differs significantly from causal order, with less than 5% of declarative sequences showing Spearman's ρ > 0.8
- The optimal word generation order tends to be closer to right-to-left generation for Spanish, diverging from the left-to-right causal order

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A Viterbi algorithm can estimate the maximum-likelihood generation order for non-causal language models by treating word ordering as a hidden Markov model.
- Mechanism: The algorithm defines states as all possible partial completions of a sentence (masked token combinations) and transitions as filling individual masked positions. It computes the path that maximizes the product of transition probabilities from a non-causal LM, yielding the optimal generation sequence.
- Core assumption: The non-causal language model's conditional probabilities accurately reflect the true likelihood of generating any token given any partially-completed context.
- Evidence anchors:
  - [abstract] "We present a novel Viterbi algorithm-based methodology for maximum likelihood word order estimation."
  - [section 3.2] "We will model our generation order estimation problem as a hidden Markov model (HMM) in which we predict the hidden generation order state sequence X from a state set S including all possible masked token combinations."
  - [corpus] No direct corpus evidence for Viterbi-based NLG ordering; related work uses Viterbi for speech recognition and POS tagging, not generation order.
- Break condition: The state space grows combinatorially with sentence length, making this approach infeasible for long sequences without pruning or approximation.

### Mechanism 2
- Claim: Causal (left-to-right) language models favor SVO word order, reducing the syntactic richness available to languages with flexible word order like Spanish.
- Mechanism: Decoder-only transformers predict tokens autoregressively using only leftward context. This biases generation toward structures where dependencies are locally resolvable—characteristic of English SVO syntax. Spanish's flexible order (VSO, VOS, OVS, etc.) receives lower probability under this constraint.
- Core assumption: The observed probability differences between causal and non-causal models stem from generation order constraints rather than architectural or training differences.
- Evidence anchors:
  - [abstract] "This comparative analysis reveals that causal NLG prefers English-like SVO structures."
  - [section 4.3] Table 1 shows SVO has highest causal probability (2.428×10⁻¹⁰) while non-SVO structures have lower probabilities; Figure 2 shows optimal-causal probability ratios favor non-causal for non-SVO structures.
  - [corpus] Weak direct evidence; corpus neighbors address NLG evaluation and causal inference in other domains, not word order bias.
- Break condition: If models were trained on perfectly balanced multilingual corpora with equivalent tokenization quality, the SVO bias might diminish—but this is not tested here.

### Mechanism 3
- Claim: The optimal non-causal generation order correlates poorly with left-to-right causal order and often trends toward right-to-left generation for Spanish.
- Mechanism: Spearman's rank correlation coefficient (ρ) measures monotonic relationship between optimal Viterbi order and causal order. Low and often negative ρ values indicate the maximum-likelihood sequence frequently generates tokens in non-causal (sometimes reverse) order.
- Core assumption: Spearman's ρ appropriately captures the structural difference between generation orders; alternative metrics might yield different conclusions.
- Evidence anchors:
  - [abstract] "Our results demonstrate that the ideal order predicted by the maximum likelihood estimator is not closely related to the causal order."
  - [section 4.3] "Less than 5% of declarative sequences show ρ > 0.8... majority of values lie on the negative axis... optimal word generation order tends to be closer to right-to-left."
  - [corpus] No corpus evidence for this specific correlation analysis in NLG.
- Break condition: This finding may not generalize to languages with stricter word order (e.g., English) or to longer, more complex sentences beyond the subject-verb-object triplets tested.

## Foundational Learning

- Concept: **Viterbi Algorithm and Hidden Markov Models**
  - Why needed here: The paper's core contribution repurposes Viterbi from sequence decoding to generation order optimization.
  - Quick check question: Given states {A, B, C} and transition probabilities between them, can you trace the most probable path for observation sequence [X, Y, Z]?

- Concept: **Causal vs. Non-Causal (Bidirectional) Language Modeling**
  - Why needed here: The entire comparison depends on understanding how masked attention (non-causal) differs from autoregressive masked attention (causal) in transformers.
  - Quick check question: For the sentence "The cat sat," what context is available to predict "sat" in a causal model versus a bidirectional model?

- Concept: **Spearman's Rank Correlation Coefficient**
  - Why needed here: The paper uses ρ to quantify how much optimal generation order deviates from left-to-right order.
  - Quick check question: If sequence A has ranks [1, 2, 3, 4] and sequence B has ranks [4, 3, 2, 1], what would Spearman's ρ be?

## Architecture Onboarding

- Component map:
  - Input layer -> Non-causal LM (RoBERTa-base-bne) -> Viterbi solver -> Causal LM (GPT-2-base-bne) -> Evaluation layer (Spearman's ρ computation)

- Critical path:
  1. Load sentence and generate all masked states (combinatorial explosion—primary bottleneck)
  2. For each state, query non-causal LM for probability distribution over vocabulary
  3. Extract probability for each masked token, multiply by predecessor's cumulative cost
  4. Backtrace Viterbi path once all states processed
  5. Compute causal probability using same sentence in original order
  6. Calculate ρ between optimal and causal token position rankings

- Design tradeoffs:
  - **Exact vs. approximate Viterbi**: Exact solution requires O(N!·N) state evaluations; beam search or pruning reduces cost but may miss true optimum
  - **Model selection**: RoBERTa/GPT-2 chosen for controlled comparison (same tokenizer, similar size); larger models may show different bias patterns
  - **Sentence length**: Study limited to short S-V-O permutations; scaling requires architectural changes to state management

- Failure signatures:
  - **Memory exhaustion**: Unpruned state space for N>8 tokens exceeds typical GPU memory
  - **Numerical underflow**: Product of many small probabilities requires log-space computation
  - **Tokenizer mismatch**: Different tokenizers between causal/non-causal models invalidate comparison
  - **Zero probability states**: Rare words may receive near-zero probability, collapsing the Viterbi path

- First 3 experiments:
  1. **Sanity check**: Replicate Table 1 on a 50-sentence subset using the published MarIA models; verify SVO probability advantage in causal model.
  2. **Scaling test**: Implement beam-limited Viterbi (beam=5) and compare optimal order against exact solution for N=4 tokens; measure accuracy loss.
  3. **Cross-lingual probe**: Apply the same pipeline to English SVO sentences using GPT-2/BERT; verify that optimal-causal correlation is higher (as Table 3 suggests) than for Spanish.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do complex syntactic structures, such as passive voice and impersonal phrases, influence the maximum likelihood generation order compared to the simple declarative sentences analyzed?
- Basis in paper: [explicit] The authors state, "In future research, we will conduct a more in-depth analysis of the maximum likelihood text generation order for more complex syntactic structures, including passive and impersonal phrases."
- Why unresolved: The current study relies on a dataset limited to 7,044 simple declarative and interrogative sentences derived from subject-verb-object permutations.
- What evidence would resolve it: Applying the Viterbi algorithm to a dataset of complex, compound, and passive Spanish sentences and comparing the resulting optimal orders against the causal baseline.

### Open Question 2
- Question: Can a reinforcement learning agent effectively learn a policy to generate text in the optimal non-causal word order identified by the Viterbi algorithm?
- Basis in paper: [explicit] The authors propose to "incorporate an agent to non-causal NLG to learn the optimal word generation order via reinforcement learning" by modeling the process as a Markov decision process.
- Why unresolved: The current methodology is an *a posteriori* analytical technique for estimating order, not a generative framework capable of producing text.
- What evidence would resolve it: The successful training and deployment of an RL agent that generates Spanish text following the predicted optimal sequences without significant loss in fluency or semantic coherence.

### Open Question 3
- Question: Does the preference for English-like SVO structures in causal models and the divergence of optimal non-causal orders persist across other non-English languages with flexible syntax?
- Basis in paper: [explicit] The authors aim to "encourage comparable investigations in languages other than English using bidirectional transformers."
- Why unresolved: The experimental validation is restricted to the Spanish language, making it unclear if the observed disalignment between causal and optimal orders is unique to Spanish or a general feature of flexible-word-order languages.
- What evidence would resolve it: Replicating the Viterbi-based estimation methodology on other languages with free word order (e.g., Russian, Latin) and analyzing the Spearman's rank correlation coefficients.

## Limitations

- The exponential state space growth makes the Viterbi algorithm computationally infeasible for sentences longer than 8-10 tokens without approximation techniques
- The comparison between causal and non-causal models assumes architectural differences alone drive probability distributions, without controlling for training data or tokenization variations
- The study is limited to short S-V-O permutations and declarative/interrogative sentences, leaving unclear how complex syntactic structures would affect optimal generation order

## Confidence

- **High confidence**: The observed SVO bias in causal models for Spanish text - this is directly measurable from the probability tables and supported by the cross-linguistic comparison showing higher optimal-causal correlation for English
- **Medium confidence**: The general claim that optimal generation order often differs from causal order and trends toward right-to-left - while supported by Spearman's ρ analysis, the magnitude and generalizability beyond short S-V-O structures remains uncertain
- **Low confidence**: The specific mechanism by which the Viterbi algorithm identifies maximum-likelihood generation orders - the implementation details are insufficiently specified to verify the computational approach independently

## Next Checks

1. Implement a controlled reproduction of Table 1 using the MarIA models on a 50-sentence subset to verify the SVO probability advantage in causal models (specifically checking that SVO has higher causal probability than non-SVO structures)
2. Apply the full pipeline to English SVO sentences using the same GPT-2/BERT architecture to test whether optimal-causal correlation increases as predicted for languages with stricter word order
3. Conduct a scaling experiment using beam-limited Viterbi search (beam=5) on N=4 token sentences to measure accuracy loss compared to exact solution and establish practical computational boundaries