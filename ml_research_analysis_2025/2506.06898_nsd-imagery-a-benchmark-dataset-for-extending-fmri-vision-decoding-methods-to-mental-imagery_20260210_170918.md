---
ver: rpa2
title: 'NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods
  to mental imagery'
arxiv_id: '2506.06898'
source_url: https://arxiv.org/abs/2506.06898
tags:
- mental
- imagery
- vision
- images
- brain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NSD-Imagery, a benchmark dataset extending
  the Natural Scenes Dataset (NSD) with fMRI data collected during mental imagery
  tasks. The dataset enables evaluation of vision decoding models on mental imagery,
  addressing the challenge of lower signal-to-noise and spatial resolution in brain
  activity during imagined versus seen images.
---

# NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery

## Quick Facts
- arXiv ID: 2506.06898
- Source URL: https://arxiv.org/abs/2506.06898
- Reference count: 40
- This paper introduces NSD-Imagery, a benchmark dataset extending the Natural Scenes Dataset (NSD) with fMRI data collected during mental imagery tasks.

## Executive Summary
This paper introduces NSD-Imagery, a benchmark dataset extending the Natural Scenes Dataset (NSD) with fMRI data collected during mental imagery tasks. The dataset enables evaluation of vision decoding models on mental imagery, addressing the challenge of lower signal-to-noise and spatial resolution in brain activity during imagined versus seen images. The authors benchmark five recent open-source vision decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, and Takagi et al.) on NSD-Imagery. Results show that while some methods generalize well to mental imagery, performance is largely decoupled from vision reconstruction accuracy. Models with simpler linear decoding architectures and multimodal feature decoding (e.g., Brain Diffuser) generalize better to mental imagery, while complex architectures (e.g., MindEye2) tend to overfit visual training data. Human ratings confirm these findings, with MindEye1 and Brain Diffuser achieving strong identification accuracy for mental image reconstructions.

## Method Summary
The study benchmarks five vision decoding models on the NSD-Imagery dataset, which extends NSD with fMRI data from mental imagery tasks. The models (MindEye1, MindEye2, Brain Diffuser, iCNN, and Takagi et al.) are trained on visual perception data and evaluated zero-shot on imagery data. Key methodological choices include using GLMsingle preprocessed betas, ridge regression vs deep MLP backbones, and multimodal feature decoding. The evaluation uses standard metrics (PixCorr, SSIM, CLIP 2WC) plus human 2AFC identification accuracy as the primary benchmark.

## Key Results
- Models employing simple linear decoding architectures and multimodal feature decoding generalize better to mental imagery, while complex architectures tend to overfit visual training data
- High-dimensional ViT-bigG image embeddings used to drive the SDXL unCLIP generator in MindEye2 tend to break down when decoded from mental images
- Brain Diffuser method appears to be the best of all the methods tested in generalizing to mental images, likely due to its simple and robust ridge regression backbone, as well as the multimodal image and text features employed

## Why This Works (Mechanism)

### Mechanism 1
Simple linear decoding architectures generalize better from seen images to mental imagery than complex architectures. Ridge regression backbones map fMRI voxels to latent embeddings through regularized linear projections, preventing overfitting to high-SNR vision training data and forcing the model to learn robust, low-dimensional correspondences that survive the SNR drop during imagery.

### Mechanism 2
High-dimensional embedding targets degrade catastrophically when decoded from low-SNR imagery signals. MindEye2 maps to ~1280-dimensional ViT-bigG embeddings via deep MLPs. When input SNR drops, the decoder amplifies noise into spurious embedding dimensions, producing semantically incoherent outputs.

### Mechanism 3
Multimodal feature decoding provides redundancy that compensates for imagery signal loss. Brain Diffuser decodes both CLIP image embeddings and CLIP text embeddings from fMRI. Text embeddings capture semantic gist that survives imagery's spatial resolution loss, while image embeddings preserve residual structural information. The diffusion model fuses both.

## Foundational Learning

- **Cross-decoding (perception-to-imagery transfer):** All tested models train on vision data only; mental imagery evaluation is purely zero-shot. Understanding this paradigm is essential for interpreting all results.
- **fMRI betas and GLMsingle preprocessing:** The paper evaluates on GLMsingle preprocessed betas (version 3, fithrf_GLMdenoise_RR). Understanding what betas represent is necessary for data loading.
- **Two-way comparison (2WC) metrics:** Primary evaluation metrics (AlexNet, CLIP, Inception 2WC) are computed against a distractor pool of 11 other NSD-Imagery stimuli—not the usual shared1000 pool.

## Architecture Onboarding

- **Component map:** fMRI betas (nsdgeneral ROI) → Ridge regression/MLP → CLIP-ViT/VGG/ViT-bigG embeddings → Stable Diffusion generator → Reconstructed image
- **Critical path:** 1. Load GLMsingle betas for subjects 1, 2, 5, 7 2. Z-score within each run 3. Map betas → latent embedding via trained decoder 4. Condition diffusion model on decoded embedding 5. Sample N reconstructions; compute metrics over all samples
- **Design tradeoffs:** Ridge regression vs deep MLP (regularization vs capacity); single-modal vs multimodal targets (simpler pipeline vs semantic redundancy); diffusion prior strength (stronger priors improve OOD generalization but hallucinate details)
- **Failure signatures:** Near-chance 2WC with high vision performance (overfitting pattern); structurally coherent but semantically wrong (diffusion prior dominating); color distortions (VGG feature normalization issues)
- **First 3 experiments:** 1. Replicate Brain Diffuser on NSD-Imagery imagery trials; verify ~73% human identification on complex stimuli 2. Ablate text embeddings in Brain Diffuser; quantify semantic metric degradation vs structural metrics 3. Train MindEye2 with L2 regularization on the MLP; measure imagery performance recovery

## Open Questions the Paper Calls Out
None

## Limitations
- The linear assumption linking vision and imagery representations may break for highly abstract or non-visual concepts
- The dataset size (8 subjects, 16 imagery repetitions per stimulus) may constrain model capacity for complex architectures
- Diffusion priors in reconstruction models can introduce hallucinations that artificially boost metrics without faithfully capturing original mental content

## Confidence
- **High:** Simple linear architectures (Brain Diffuser) generalize better to mental imagery than complex models - supported by consistent performance gaps across multiple metrics and human ratings
- **Medium:** Multimodal feature decoding provides semantic redundancy that compensates for imagery signal loss - evidence is strong for complex stimuli but weaker for simple stimuli
- **Low:** Vision reconstruction accuracy predicts mental imagery performance - results show these are largely decoupled, but this may not hold across different model families or stimulus types

## Next Checks
1. Evaluate model performance on a broader stimulus set including abstract/non-visual concepts to test limits of the vision-to-imagery transfer assumption
2. Conduct ablation studies on the diffusion prior strength across all methods to quantify hallucination vs. faithful reconstruction trade-offs
3. Test cross-subject generalization on NSD-Imagery using ZEBRA-like approaches to assess model robustness to individual brain differences