---
ver: rpa2
title: Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity
arxiv_id: '2601.14300'
source_url: https://arxiv.org/abs/2601.14300
tags:
- gradient
- sign
- search
- query
- direction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DPAttack, a novel framework for hard-label
  black-box adversarial attacks that combines a frequency-guided initialization strategy
  with a pattern-driven optimization (PDO) module. The authors provide theoretical
  justification showing that existing sign-flipping attacks like RayS asymptotically
  approximate the sign of the true gradient, establishing a foundation for more efficient
  attack methods.
---

# Gradient Structure Estimation under Label-Only Oracles via Spectral Sensitivity

## Quick Facts
- arXiv ID: 2601.14300
- Source URL: https://arxiv.org/abs/2601.14300
- Reference count: 40
- Authors: Jun Liu; Leo Yu Zhang; Fengpeng Li; Isao Echizen; Jiantao Zhou
- One-line primary result: Introduces DPAttack framework combining frequency-guided initialization with pattern-driven optimization, achieving state-of-the-art hard-label black-box attack performance across multiple datasets and architectures.

## Executive Summary
This paper addresses the challenge of hard-label black-box adversarial attacks by introducing DPAttack, a framework that theoretically grounds sign-flipping attacks as gradient sign recovery processes. The method combines a zero-query frequency-domain initialization using Block-DCT variance statistics with a Pattern-Driven Optimization (PDO) module that preserves spatial coherence during the search. Extensive experiments demonstrate superior performance across CIFAR-10, ImageNet, and ObjectNet, including successful circumvention of state-of-the-art defenses like Blacklight with 0% detection rate.

## Method Summary
DPAttack operates in two stages: First, the Dynamic Direction Module (DDM) computes Block-DCT variance statistics from the clean image to generate a structured initial direction through frequency-domain noise sampling and inverse transform. Second, the Pattern-Driven Optimization (PDO) module constructs a hierarchical search tree from contiguous blocks of identical signs (runs) in the initialization, performing greedy sign-flipping while preserving spatial coherence to reduce query complexity. The approach leverages theoretical insights showing that hierarchical sign-flipping algorithms implicitly approximate the true gradient sign.

## Key Results
- Consistently outperforms state-of-the-art methods in both attack success rate and query efficiency across multiple datasets
- Successfully circumvents Blacklight defense with 0% detection rate
- Demonstrates strong transferability to commercial APIs and CLIP models
- Shows improved performance particularly in low-query regimes (≤50 queries)
- Achieves better efficiency on adversarially trained models compared to baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Frequency-variance initialization yields starting direction closer to decision boundary than random noise
- **Mechanism:** Uses Block-DCT statistics to sample noise proportional to frequency variance, creating structured initial direction that aligns with model sensitivity
- **Core assumption:** Model sensitivity correlates positively with frequency-domain variance of clean image
- **Evidence anchors:** Abstract shows zero-query initialization achieves higher cosine similarity to true gradient sign; Section 4.3.1 demonstrates oscillatory decay of model sensitivity from low to high frequencies
- **Break condition:** Victim model's vulnerability profile uncorrelated with input image's spectral statistics

### Mechanism 2
- **Claim:** Pattern-driven partitioning preserves gradient signal better than rigid dyadic splitting
- **Mechanism:** PDO treats contiguous blocks of identical signs as atomic units, avoiding "gain cancellation" from grouping opposing gradient signs
- **Core assumption:** True gradient sign exhibits local block coherence
- **Evidence anchors:** Abstract states PDO reduces query complexity; Section 4.4 explains dyadic bisection leads to gain cancellation
- **Break condition:** True gradient is highly fragmented at pixel level with no contiguous block structure

### Mechanism 3
- **Claim:** Hard-label sign-flipping attacks are theoretically grounded as gradient sign recovery processes
- **Mechanism:** Derives that hierarchical sign-flipping algorithms implicitly approximate true gradient sign by maximizing loss through boundary minimization
- **Core assumption:** Loss function is locally smooth with subset-wise Lipschitz continuity
- **Evidence anchors:** Section 3 establishes novel theoretical framework; Theorem 2 provides alignment bound
- **Break condition:** Decision boundary is highly non-linear locally, violating first-order Taylor approximation

## Foundational Learning

- **Concept: Hard-Label Oracle Feedback**
  - **Why needed here:** Fundamental constraint where only binary True/False feedback is available, not confidence scores
  - **Quick check question:** If model returns "Dog" with 90% confidence, does this attack setup allow using that "90%" number?

- **Concept: Block-wise Discrete Cosine Transform (BDCT)**
  - **Why needed here:** Decomposes image into frequency bands to identify where to inject perturbation energy based on natural image statistics
  - **Quick check question:** Why use variance of BDCT coefficients rather than coefficients themselves to guide initialization?

- **Concept: Gain Cancellation**
  - **Why needed here:** Explains inefficiency of baseline methods where flipping blocks with opposing gradient signs cancels out net benefit
  - **Quick check question:** If flipping subset of pixels doesn't change output, what does "gain cancellation" suggest about gradient signs inside that subset?

## Architecture Onboarding

- **Component map:** Input (Clean Image, Hard-label Oracle) -> DDM (Init: BDCT variance -> Noise -> Inverse BDCT -> Initial Direction) -> DBS (Config: Dynamic Blocksize Selection) -> PDO (Search: Pattern tree -> Greedy sign-flipping -> Update) -> Output (Adversarial Example)

- **Critical path:** Success relies heavily on DDM initialization; if initial direction doesn't align with true gradient sign, PDO has no spatial coherence to exploit, degrading to random search performance

- **Design tradeoffs:**
  - Block size (w): Small blocks capture details but increase search dimensionality; large blocks are coarse but fast; Dynamic Blocksize Selection automates this tradeoff
  - Generalization vs. Specificity: Clean-image statistics is model-agnostic but requires spectral sensitivity correlation assumption; tailoring to specific models offers marginal gains but fails on others

- **Failure signatures:**
  - Zero-alignment initialization: Flat frequency statistics in clean image cause lack of structure, PDO failure
  - Overhead in low-query regimes: DBS warm-up consumes queries; at <20 queries, overhead may outweigh benefits

- **First 3 experiments:**
  1. Ablation on Initialization: Compare "Ours" vs. "Ours w/o DDM" on ImageNet to quantify frequency prior contribution
  2. Efficiency in Low-Query Regime: Run DPAttack vs. ADBA on ImageNet with Max.Q=50 to validate low-query performance
  3. Defense Evasion: Test DPAttack against Blacklight with/without randomized noise injection to verify 0% detection rate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can frequency-variance prior be adapted to maintain strong performance on adversarially trained models where correlation between clean image statistics and model sensitivity weakens or inverts?
- **Basis in paper:** Table 1 shows adversarially trained WideResNet-50 exhibits negative correlations (-0.17, -0.41) between BFS sensitivity and clean image frequency variance, compared to strong positive correlations (0.80-0.93) for standard models
- **Why unresolved:** Paper observes phenomenon but doesn't propose mechanism to detect such architectures or adapt initialization strategy
- **What evidence would resolve it:** Modified initialization that dynamically detects correlation patterns and adapts sampling distribution accordingly

### Open Question 2
- **Question:** How can PDO be redesigned to achieve comparable efficiency gains under ℓ₂ constraints, given current run-based decomposition is tailored to ℓ∞ structure?
- **Basis in paper:** Paper primarily targets ℓ∞ perturbations, noting in Appendix P that DPAttack is tailored for ℓ∞ characteristics
- **Why unresolved:** Run-length-based sign coherence naturally aligns with ℓ∞ structure; ℓ₂ perturbations permit continuous magnitude variations making discrete sign-space optimization less applicable
- **What evidence would resolve it:** Modified PDO variant operating in continuous perturbation space with theoretical guarantees for ℓ₂ constraints

### Open Question 3
- **Question:** Can more sophisticated stateful defenses detect structural patterns inherent in DPAttack's queries beyond simple fingerprint-based detection?
- **Basis in paper:** Uses randomized variant with Gaussian noise injection to evade Blacklight, achieving 0% detection rate
- **Why unresolved:** Evasion exploits Blacklight's specific hash-based fingerprinting; other defenses could analyze query direction patterns, frequency content, or temporal correlations
- **What evidence would resolve it:** Evaluation against diverse stateful defenses and theoretical characterization of which mechanisms current randomization can/cannot evade

### Open Question 4
- **Question:** What architectural or data characteristics determine optimal BDCT block size w, and can these be predicted without query-based dynamic selection?
- **Basis in paper:** Authors observe optimal w is highly architecture-dependent (w=16 for WideResNet-50 vs w=4 for ViT-B-32) and introduce DBS to dynamically select w
- **Why unresolved:** Paper provides no principled explanation for why specific architectures prefer certain block sizes; DBS adds query overhead
- **What evidence would resolve it:** Theoretical or empirical model predicting optimal w from architectural properties or image characteristics

## Limitations
- Theoretical gradient sign approximation relies on smoothness assumptions that may not hold for highly non-linear decision boundaries
- Frequency initialization mechanism depends on correlation between spectral variance and model sensitivity, which is empirically validated but not universally guaranteed
- PDO effectiveness assumes gradient sign coherence in spatial blocks, which could fail on highly fragmented gradients

## Confidence

- **High Confidence:** Query efficiency improvements over baselines, correctness of BDCT initialization implementation, effectiveness of PDO spatial coherence preservation
- **Medium Confidence:** Theoretical gradient sign approximation framework, Blacklight defense circumvention, generalization to commercial APIs
- **Low Confidence:** Universal applicability of frequency variance correlation assumption, robustness to adaptive defenses beyond Blacklight, performance in extremely low-query regimes

## Next Checks

1. **Robustness to Adversarial Training:** Test DPAttack against diverse set of adversarially trained models (ℓ∞-PGD, ℓ2-C&W) to verify frequency initialization assumption holds when models are specifically trained to be frequency-robust

2. **Gradient Shattering Analysis:** Generate synthetic test cases with known fragmented gradient structures to empirically validate whether PDO maintains advantage when gradient sign exhibits minimal spatial coherence

3. **Adaptive Defense Response:** Implement variant of Blacklight with dynamic noise injection parameters and evaluate whether DPAttack's 0% detection rate persists under adaptive, model-specific noise schedules