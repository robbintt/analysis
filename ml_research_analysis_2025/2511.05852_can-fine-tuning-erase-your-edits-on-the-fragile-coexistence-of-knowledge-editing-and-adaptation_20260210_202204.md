---
ver: rpa2
title: Can Fine-Tuning Erase Your Edits? On the Fragile Coexistence of Knowledge Editing
  and Adaptation
arxiv_id: '2511.05852'
source_url: https://arxiv.org/abs/2511.05852
tags:
- fine-tuning
- edits
- editing
- performance
- memit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines the interaction between knowledge editing and
  fine-tuning in large language models. The authors systematically investigate whether
  fine-tuning preserves or erases edits applied to model parameters, using two knowledge
  editing methods (MEMIT and AlphaEdit) and three fine-tuning approaches (full fine-tuning,
  LoRA, and DoRA) across five models and three datasets.
---

# Can Fine-Tuning Erase Your Edits? On the Fragile Coexistence of Knowledge Editing and Adaptation

## Quick Facts
- arXiv ID: 2511.05852
- Source URL: https://arxiv.org/abs/2511.05852
- Authors: Yinjie Cheng; Paul Youssef; Christin Seifert; Jörg Schlötterer; Zhixue Zhao
- Reference count: 40
- This paper examines whether fine-tuning erases edits applied to model parameters.

## Executive Summary
This paper systematically investigates the interaction between knowledge editing and fine-tuning in large language models. The authors find that fine-tuning generally impairs edits, with larger models showing greater edit stability and AlphaEdit edits being more vulnerable to decay than MEMIT edits. Fine-tuning non-edited layers impairs more edits than fine-tuning edited layers, while selective fine-tuning of edited layers can effectively remove edits at the cost of downstream performance. The study establishes empirical baselines and actionable strategies for integrating knowledge editing with fine-tuning, revealing that current editing methods do not produce edits that reliably survive subsequent adaptation.

## Method Summary
The paper applies two knowledge editing methods (MEMIT and AlphaEdit) to five models (GPT-J, GPT2-XL, Llama2, Llama3.1) across three datasets (zsRE, RePAQ, HotpotQA). After editing, the models undergo fine-tuning using three approaches: full fine-tuning, LoRA, and DoRA. The study evaluates edit preservation through Efficacy (ES), Paraphrase Similarity (PS), and Neighborhood Similarity (NS) metrics, while also measuring downstream task performance on eight benchmarks. Activation space analysis examines directional similarities between edit-induced and fine-tuning-induced changes.

## Key Results
- Fine-tuning generally impairs knowledge edits, with larger models showing greater edit stability
- AlphaEdit edits are more vulnerable to decay than MEMIT edits due to null-space vulnerability
- Fine-tuning non-edited layers impairs more edits than fine-tuning edited layers
- Selective fine-tuning of edited layers can effectively remove edits but harms downstream performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AlphaEdit edits decay faster than MEMIT edits because they occupy null-space directions that fine-tuning gradients do not prioritize.
- Mechanism: AlphaEdit constrains weight perturbations (ΔW) to the null space of Fisher information directions to avoid interfering with unrelated knowledge. Fine-tuning gradients concentrate along high-curvature (non-null) directions, causing null-space edits to shrink or rotate during adaptation.
- Core assumption: Edits placed in low-curvature directions are less stable under subsequent gradient-based updates.
- Evidence anchors:
  - [Section 4.1, p.5]: "AlphaEdit exhibits greater decay after FT... Such a pattern is observed consistently across GPT-J and GPT2-XL. This phenomenon may stem from the Null-Space Vulnerability of AlphaEdit."
  - [Table 2, p.5]: Shows larger average performance decreases for AlphaEdit vs. MEMIT across models.
  - [corpus]: Related work on editing methods exists (e.g., "BalancEdit"), but corpus provides no direct evidence on null-space fragility.
- Break condition: If a fine-tuning method were designed to explicitly preserve null-space directions, AlphaEdit edits might survive longer.

### Mechanism 2
- Claim: Fine-tuning non-edited layers impairs more edits than full fine-tuning because edits rely on distributed representations across layers.
- Mechanism: Factual associations in LLMs emerge from coordinated patterns across many MLP and attention layers. Editing modifies only a subset of weights, creating incomplete shifts in these distributed circuits. Fine-tuning non-edited layers disrupts the coordination needed for the edit to propagate correctly.
- Core assumption: Knowledge is distributed across layers; localized edits require intact downstream processing.
- Evidence anchors:
  - [Section 4.3, p.7]: "fine-tuning non-edited layers provides no benefit in preserving edits... These results suggest that fine-tuning non-edited layers can be a supplementary edit-removal strategy."
  - [Table 5, p.8]: Shows Med_ft_non-edited has lower ES (72%) than Med_ft_all (98%) for 100-edit AlphaEdit on Llama2.
  - [corpus]: No direct corpus evidence on distributed representation mechanism.
- Break condition: If an edit were implemented in a more localized, self-contained manner (e.g., via adapter modules), non-edited layer fine-tuning might preserve it better.

### Mechanism 3
- Claim: Fine-tuning overwrites edits because FT activation shifts are nearly orthogonal to KE activation shifts.
- Mechanism: Edits induce localized, shallow perturbations in activation space. Fine-tuning induces broader activation changes across layers. The directional similarity between edit-induced and FT-induced activation changes is low (near orthogonal), so FT does not reinforce and may erase the edit's representational signature.
- Core assumption: Orthogonal activation changes compete for representation capacity or interfere destructively.
- Evidence anchors:
  - [Section 5, p.8-9]: "fine-tuned models (Med_ft - Mft) shares the lowest similarity, indicating that fine-tuning moves activations in the positive directions nearly orthogonal to the editing direction."
  - [Fig. 3, p.10]: Visualizes layer-wise activation drifts and directional similarities.
  - [corpus]: Weak; corpus neighbors discuss editing robustness but not activation-space orthogonality.
- Break condition: If fine-tuning data were engineered to explicitly query edited facts, FT might move activations in more aligned directions, reinforcing edits.

## Foundational Learning

- Concept: **Knowledge Editing (KE) methods (MEMIT, AlphaEdit)**
  - Why needed here: The paper compares how different editing paradigms respond to fine-tuning. Understanding that MEMIT uses matrix optimization for mass editing while AlphaEdit uses null-space constraints is essential for interpreting their different decay rates.
  - Quick check question: Can you explain why AlphaEdit's null-space constraint might make it more vulnerable to subsequent fine-tuning?

- Concept: **Parameter-Efficient Fine-Tuning (PEFT: LoRA, DoRA)**
  - Why needed here: The paper shows LoRA/DoRA preserve edits better than full fine-tuning on some models. Understanding how these methods decompose and update weights helps explain this difference.
  - Quick check question: How does DoRA's magnitude-direction decomposition differ from LoRA's low-rank approximation?

- Concept: **Distributed Representation Hypothesis in LLMs**
  - Why needed here: The paper invokes this to explain why fine-tuning non-edited layers harms edits. Facts aren't stored in single layers but emerge from coordinated multi-layer activity.
  - Quick check question: If facts are distributed, why does editing specific layers work at all?

## Architecture Onboarding

- Component map:
  - Pre-trained LLM (GPT-J, GPT2-XL, Llama2, Llama3.1) -> Knowledge Editor (MEMIT/AlphaEdit) -> Fine-Tuner (Full FT/LoRA/DoRA) -> Evaluation (ES, PS, NS metrics + downstream tasks)

- Critical path:
  1. Apply KE (MEMIT/AlphaEdit) to base model → Med.
  2. Fine-tune Med on Commonsense/HotpotQA → Med_ft.
  3. Evaluate Med vs. Med_ft on KE metrics (ES, PS, NS) and downstream tasks.
  4. Compare activation patterns (drift, direction) between Med, Mft, Med_ft.

- Design tradeoffs:
  - **Edit removal vs. downstream performance**: Fine-tuning only edited layers removes edits effectively but harms downstream tasks (e.g., HellaSwag drops to 32%).
  - **PEFT vs. full FT**: PEFT preserves edits better but may limit adaptation capacity.
  - **Edit scale vs. stability**: More edits (10k) lead to greater decay, especially for AlphaEdit.

- Failure signatures:
  - Edit decay >40% (full FT on Llama2 with 10k edits).
  - ES drops below 50% post-FT.
  - Downstream task accuracy drops >30% when fine-tuning only edited layers.
  - DeepSeek fails to accept edits (ES ~0.4% with 10k edits).

- First 3 experiments:
  1. **Baseline replication**: Apply MEMIT (1k edits) to GPT-J on zsRE, then LoRA fine-tune on Commonsense. Report ES before/after FT.
  2. **Selective layer ablation**: Edit GPT-J with AlphaEdit (100 edits), then fine-tune only edited layers. Compare ES and downstream performance vs. all-layer FT.
  3. **Activation analysis**: For a single edited fact, extract activations from Med, Mft, and Med_ft at edited and non-edited layers. Compute directional similarity between edit and FT shifts.

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that AlphaEdit's null-space constraint makes it more vulnerable relies on directional analysis but lacks direct proof that FT gradients specifically target null-space directions.
- The distributed representation hypothesis explaining why non-edited layer fine-tuning harms edits is plausible but not rigorously tested with ablation studies on edit localization.
- The orthogonal activation shift hypothesis is based on similarity metrics that may not fully capture complex activation dynamics.

## Confidence
- **High**: Basic empirical findings (FT impairs edits, PEFT preserves better than full FT, more edits = more decay)
- **Medium**: Model size effects (larger models preserve edits better)
- **Medium**: Layer-specific effects (non-edited layer FT harms more than edited layer FT)
- **Low**: Mechanistic explanations (null-space vulnerability, distributed representation, activation orthogonality)

## Next Checks
1. **Null-space gradient analysis**: Measure actual FT gradient projections onto edited vs. null-space directions for both MEMIT and AlphaEdit to directly test the fragility mechanism.
2. **Edit localization ablation**: Systematically vary edit locations (single layer vs. distributed) and measure how this affects sensitivity to non-edited layer fine-tuning.
3. **Activation reinforcement experiment**: Fine-tune with data that explicitly queries edited facts to test whether FT can reinforce rather than erase edits when activation directions align.