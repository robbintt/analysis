---
ver: rpa2
title: Temporal Information Retrieval via Time-Specifier Model Merging
arxiv_id: '2507.06782'
source_url: https://arxiv.org/abs/2507.06782
tags:
- temporal
- queries
- retrieval
- time
- non-temporal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving temporal information
  retrieval without sacrificing performance on non-temporal queries, a problem caused
  by attention bias and catastrophic forgetting in dense retrieval models. The authors
  propose Time-Specifier Model Merging (TSM), a novel method that trains specialized
  retrievers for individual time specifiers (e.g., "in," "after," "between") and merges
  them into a unified model through parameter averaging.
---

# Temporal Information Retrieval via Time-Specifier Model Merging

## Quick Facts
- arXiv ID: 2507.06782
- Source URL: https://arxiv.org/abs/2507.06782
- Reference count: 24
- Achieves 83.49% Recall@20 on TimeQA temporal queries while maintaining non-temporal performance

## Executive Summary
This paper introduces Time-Specifier Model Merging (TSM), a novel approach to temporal information retrieval that addresses the fundamental challenge of attention bias and catastrophic forgetting in dense retrieval models. The method trains specialized retrievers for individual time specifiers (e.g., "in," "after," "between") and merges them through parameter averaging to create a unified model that excels at both temporal and non-temporal queries. Experimental results across four datasets demonstrate significant improvements over existing methods, with TSM achieving superior performance on temporal queries while maintaining strong results on non-temporal benchmarks.

## Method Summary
Time-Specifier Model Merging (TSM) trains specialized retriever models for individual time specifiers rather than attempting to train a single model on all temporal expressions simultaneously. Each specialist model learns to handle queries containing its specific time specifier, allowing focused attention on relevant temporal patterns without interference from other temporal expressions. The method then merges these specialized models through parameter averaging, creating a unified retriever that inherits temporal expertise from each specialist while preserving non-temporal retrieval capabilities. This approach effectively mitigates catastrophic forgetting by maintaining distinct parameter spaces during training and only averaging during the merging phase.

## Key Results
- TSM achieves 83.49% Recall@20 on TimeQA temporal queries, outperforming full fine-tuning, LoRA, routing, and ensembling methods
- Maintains strong non-temporal performance with 53.26% Recall@20 on NQ dataset
- Successfully mitigates catastrophic forgetting through minimal parameter weight changes during merging
- Consistently outperforms baselines across four datasets (TimeQA, Nobel Prize, NQ, and MS MARCO)

## Why This Works (Mechanism)
TSM works by decomposing the complex temporal retrieval problem into manageable sub-problems, each handled by a specialized model. This decomposition prevents attention interference between different time specifiers that commonly occurs in multi-task training. The parameter averaging merging strategy preserves the temporal expertise learned by each specialist while creating a cohesive model that can handle diverse query types. By training specialists independently and only merging parameters after training completion, the method avoids the catastrophic forgetting that typically occurs when training on multiple temporal patterns simultaneously in a single model.

## Foundational Learning
- **Dense Retrieval**: Neural network-based document retrieval using vector representations; needed for modern information retrieval systems; quick check: verify model uses bi-encoder architecture
- **Catastrophic Forgetting**: Degradation of previously learned capabilities when training on new tasks; critical for understanding why multi-task temporal training fails; quick check: measure performance drop on non-temporal queries
- **Parameter Averaging**: Technique for combining model parameters from multiple trained models; enables knowledge transfer between specialists; quick check: verify averaging occurs only after specialist training
- **Time Specifiers**: Linguistic elements indicating temporal relationships (e.g., "before," "after," "between"); fundamental units for temporal query understanding; quick check: ensure all relevant specifiers are covered
- **Recall@K**: Evaluation metric measuring proportion of relevant documents in top-K results; standard metric for retrieval quality; quick check: confirm K=20 for temporal tasks

## Architecture Onboarding

**Component Map**
Parent Retriever -> Time Specifier Specialists (one per specifier) -> Parameter Averaging -> Unified Retriever

**Critical Path**
1. Query encoding through parent retriever backbone
2. Time specifier detection and routing to appropriate specialist
3. Document encoding through specialist-specific parameters
4. Similarity computation and ranking
5. Parameter averaging during merging phase

**Design Tradeoffs**
- Specialist granularity vs. training overhead: More specifiers provide better specialization but increase training complexity
- Parameter sharing vs. independence: Full independence prevents interference but increases memory requirements
- Merging timing: Post-training merging preserves specialization but requires additional inference step

**Failure Signatures**
- Performance degradation on non-temporal queries indicates catastrophic forgetting
- Inconsistent temporal reasoning across different specifiers suggests incomplete specialist training
- High computational overhead during inference suggests inefficient parameter utilization

**First 3 Experiments**
1. Train individual specialists for "in," "after," and "between" specifiers on TimeQA dataset
2. Perform parameter averaging and evaluate merged model on both temporal and non-temporal queries
3. Compare performance against baseline models using standard Recall@20 metric

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability concerns when dealing with large numbers of time specifiers, as each requires individual specialist training
- Computational overhead of maintaining multiple specialist models during training phase
- Potential performance degradation when encountering novel time expressions not seen during training
- Limited evaluation of generalization to truly unseen temporal patterns

## Confidence
- **High**: Core temporal retrieval claims - consistent performance gains across multiple datasets
- **Medium**: Catastrophic forgetting mitigation - primarily measured through parameter changes rather than comprehensive temporal performance metrics
- **High**: Non-temporal query generalization - strong maintained performance on NQ and MS MARCO datasets

## Next Checks
1. Conduct experiments with an expanded set of 50+ time specifiers to evaluate scalability and identify performance degradation patterns
2. Implement comprehensive forgetting metrics that measure retrieval performance across historical time periods to rigorously validate catastrophic forgetting claims
3. Perform cross-dataset generalization tests where models trained on one temporal dataset are evaluated on another to assess true temporal reasoning capabilities versus dataset-specific pattern learning