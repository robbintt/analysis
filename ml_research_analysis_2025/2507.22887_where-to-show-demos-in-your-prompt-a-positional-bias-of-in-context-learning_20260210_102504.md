---
ver: rpa2
title: 'Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning'
arxiv_id: '2507.22887'
source_url: https://arxiv.org/abs/2507.22887
tags:
- qwen
- prompt
- llama3
- position
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uncovers a previously unexplored positional bias in
  in-context learning (ICL) called DPP bias, where the placement of demonstrations
  within a prompt significantly impacts LLM performance. The authors systematically
  evaluate four canonical demo positions across eight tasks and ten LLMs, introducing
  accuracy-change and prediction-change metrics to quantify performance shifts.
---

# Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning

## Quick Facts
- arXiv ID: 2507.22887
- Source URL: https://arxiv.org/abs/2507.22887
- Reference count: 40
- Key finding: Demonstration placement within prompts (DPP bias) significantly impacts LLM performance, with start-of-prompt positions (ssp/esp) yielding more stable and accurate outputs than end-of-message placement (eum).

## Executive Summary
This paper uncovers a previously unexplored positional bias in in-context learning (ICL) called DPP bias, where the placement of demonstrations within a prompt significantly impacts LLM performance. The authors systematically evaluate four canonical demo positions across eight tasks and ten LLMs, introducing accuracy-change and prediction-change metrics to quantify performance shifts. Results show that placing demos at the start of the prompt (ssp/esp) yields more stable and accurate outputs, while end-of-message placement (eum) can flip over 30% of predictions without improving correctness. Smaller models are most sensitive to this bias, though even large models remain affected on complex tasks. No single position is universally optimal, highlighting the need for model- and task-specific prompt tuning.

## Method Summary
The paper evaluates four demonstration placement positions (ssp, esp, sum, eum) across eight benchmarks using ten LLMs ranging from 1.5B to 72B parameters. For each position, 5 demonstrations are sampled from training splits and inserted into chat templates with fixed content but varying spatial location. Predictions are generated using vLLM with 4-bit quantization, temperature=0, and deterministic decoding. Performance is measured via accuracy-change (vs zero-shot) and prediction-change (vs default sum position), with statistical significance assessed using Wilcoxon signed-rank tests. Tasks include classification (AG News, MNLI, ARC, MMLU), QA (SQuAD), reasoning (GSM8K), and summarization (XSUM, CNN/DailyMail).

## Key Results
- End-of-message (eum) placement causes 30-50% prediction volatility without accuracy gains, often degrading performance.
- Start-of-prompt positions (ssp/esp) provide the most consistent accuracy improvements across model scales and task types.
- Larger models show reduced sensitivity to position but remain affected on complex tasks.
- No single optimal position exists; the best placement varies by model and task.

## Why This Works (Mechanism)

### Mechanism 1: Causal Masking-Induced Primacy Bias
Causal-decoder LLMs use autoregressive masking, where earlier tokens exert disproportionate influence on hidden states that condition subsequent predictions. Induction heads concentrate attention on early and "sink" tokens, creating a structural primacy effect. This mechanism assumes attention patterns generalize across model families and sizes tested.

### Mechanism 2: Instruction-Tuning Distributional Priors
Models develop positional preferences based on regularities in their instruction-tuning data, where demonstrations often appear in fixed slots. This creates distributional priors making certain positions more "expected" and effectively utilized. The mechanism assumes shared structural conventions across the instruction-tuning datasets for Qwen, Llama 3, Mistral, and Cohere.

### Mechanism 3: Task-Type Interaction with Positional Sensitivity
The optimal demonstration position varies by task type (classification vs. reasoning vs. generation), with model scale modulating sensitivity. Different tasks require different inductive biases—classification may benefit from early context setting while generative tasks show near-100% prediction volatility even in large models. Larger models develop more robust position-invariant representations, reducing but not eliminating sensitivity.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: DPP bias is a property of ICL; understanding how models learn from demonstrations without weight updates is prerequisite to grasping why position matters.
  - Quick check question: Can you explain how ICL differs from fine-tuning, and why it might be sensitive to prompt structure?

- **Concept: Attention Mechanisms and Induction Heads**
  - Why needed here: The paper attributes DPP bias partly to architectural tendencies in transformer attention, specifically induction heads favoring early tokens.
  - Quick check question: In a causal decoder, why might tokens appearing earlier in the sequence have disproportionate influence on later predictions?

- **Concept: Instruction-Tuning and Chat Templates**
  - Why needed here: The four canonical positions (ssp, esp, sum, eum) are defined relative to system prompts and user messages in chat-style instruction-tuned models.
  - Quick check question: How does the separation of system and user roles in modern LLMs affect where demonstrations can be placed?

## Architecture Onboarding

- **Component map**:
  System Prompt (instructional context) -> User Message (query + optional demos) -> Model Response
  Four DPP Positions: ssp (start of system prompt), esp (end of system prompt), sum (start of user message, default), eum (end of user message)

- **Critical path**:
  1. Define task and select fixed demonstration block (5 examples from training split).
  2. For each DPP position, construct prompt using model's chat template with demos inserted at specified location.
  3. Run inference with temperature=0, deterministic decoding.
  4. Compute task metrics (accuracy, F1, ROUGE-L) and delta metrics vs. zero-shot and vs. default position (sum).
  5. Analyze prediction transitions (correct→incorrect, incorrect→correct) via Sankey-style breakdown.

- **Design tradeoffs**:
  - Control vs. generalization: Holding demo content fixed isolates positional effects but may not reflect real-world prompting where content and position co-vary.
  - Model coverage: Testing 10 models (1.5B–72B) provides scaling insights but omits other architectures (e.g., encoder-decoder).
  - Metric selection: ROUGE and BERTScore for generation are imperfect proxies for true quality; high scores may mask factual errors.

- **Failure signatures**:
  - High prediction volatility (>30%): Indicates model is flipping outputs without accuracy gain—likely eum position on QA/generation tasks.
  - Negative accuracy-change: Demonstrations at a given position hurt performance vs. zero-shot—common for eum on classification.
  - Scale-inverted preferences: Large model (e.g., LLAMA3 70B) showing better performance with eum on arithmetic while small model collapses—suggests task-specific inductive biases.

- **First 3 experiments**:
  1. Baseline Position Sweep: On your target model and task, run all four DPP positions with 5 fixed demos. Record accuracy and prediction-change. Identify if eum causes >20% prediction flips without accuracy gain.
  2. Zero-Shot Comparison: Run the same task without demos. Compare to each DPP position using the Accuracy-Change metric to confirm demos actually help.
  3. Prediction Transition Analysis: For the best and worst DPP positions, manually inspect 10–20 examples that flipped from correct→incorrect and incorrect→correct. Look for patterns in query type, demo similarity, or output format.

## Open Questions the Paper Calls Out

### Open Question 1
Does the DPP bias observed in standard few-shot prompting persist or manifest differently when applied to few-shot chain-of-thought (CoT) prompts? The authors explicitly list extending analysis to "few-shot chain-of-thought prompts" as a primary avenue for future research, as the current study evaluates classification, QA, and summarization tasks but does not systematically test prompts containing reasoning rationales.

### Open Question 2
What specific mechanistic factors—such as attention initialization, decoder primacy, or instruction-tuning data distributions—are causally responsible for the DPP bias? The authors call for "deeper interpretability work" to determine if the bias arises from architectural tendencies (like induction heads) or training corpus conventions, as the paper identifies and quantifies the bias but relies on hypotheses rather than causal evidence.

### Open Question 3
Can automated demo-placement optimization routines that adapt position jointly with content effectively mitigate DPP bias? The authors propose developing "automated demo-placement optimization routines" as a method to achieve more robust in-context learning systems, as the paper demonstrates that no single position is universally optimal and suggests manual tuning, but does not test an automated solution.

## Limitations

- Demo Content Fixedness: Holding demonstration content constant across positions may not reflect real-world prompting where content and position co-vary, potentially underestimating real-world positional sensitivity.
- Generation Task Metric Quality: ROUGE-L and BERTScore may not fully capture degradation in output quality, as they miss factual consistency and coherence while eum placement can flip 30-40% of predictions with maintained high scores.
- Model and Task Coverage: The claim that no single position is universally optimal is supported within the tested corpus but may not hold for tasks outside the eight evaluated categories or for encoder-decoder architectures.

## Confidence

**High Confidence**: The core empirical finding that demonstration position significantly affects performance (30-50% prediction volatility for eum placement) is robust across multiple models and tasks. The Accuracy-Change and Prediction-Change metrics are clearly defined and consistently measured.

**Medium Confidence**: The mechanistic explanation via causal masking and induction heads is plausible but relies on indirect evidence from prior work rather than direct validation within this study. The instruction-tuning distributional prior hypothesis is even more speculative, lacking direct evidence about training corpus structures.

**Low Confidence**: The claim that task-type interaction with positional sensitivity is a primary driver lacks strong supporting evidence. While the paper observes different optimal positions for different tasks, the analysis doesn't establish why classification, reasoning, and generation tasks should respond differently to position beyond surface-level pattern matching.

## Next Checks

1. **Manual Output Quality Audit**: Select 50 examples from eum and ssp positions on XSUM and CNN/DailyMail. Have human annotators rate factual consistency and coherence (not just ROUGE-L). Compare degradation rates to prediction-change metrics to validate whether high volatility correlates with quality drops beyond what automated metrics capture.

2. **Dynamic Demo Content Test**: Run a controlled experiment where you vary both demo content and position simultaneously on AG News and GSM8K. Measure whether the positional effect size changes when demo content is optimized for each position, revealing whether the current results underestimate real-world positional sensitivity.

3. **Attention Pattern Validation**: For one small model (e.g., Llama3-3B) and two tasks showing strong positional bias, extract attention weights during inference for ssp vs eum positions. Analyze whether induction heads indeed show stronger early-token influence in eum placement, providing direct evidence for the proposed mechanism.