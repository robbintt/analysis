---
ver: rpa2
title: Protein Inverse Folding From Structure Feedback
arxiv_id: '2506.03028'
source_url: https://arxiv.org/abs/2506.03028
tags:
- protein
- tm-score
- structure
- recovery
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the inverse folding problem, aiming to design
  amino acid sequences that reliably fold into desired three-dimensional structures.
  The core method leverages Direct Preference Optimization (DPO) to fine-tune an inverse
  folding model using structural feedback from a folding model.
---

# Protein Inverse Folding From Structure Feedback

## Quick Facts
- **arXiv ID:** 2506.03028
- **Source URL:** https://arxiv.org/abs/2506.03028
- **Reference count:** 40
- **One-line primary result:** Protein inverse folding model fine-tuned with DPO using folding model feedback achieves TM-Score of 0.81 and 79.5% improvement on challenging structures via iterative training.

## Executive Summary
This paper addresses the protein inverse folding problem by designing amino acid sequences that reliably fold into desired 3D structures. The core innovation is using Direct Preference Optimization (DPO) with structural feedback from a folding model to fine-tune an inverse folding model. Candidate sequences are generated, their structures are predicted, and pairwise preferences based on TM-Score rankings are used to guide the optimization. The method demonstrates improved sequence recovery and increased average TM-Score from 0.77 to 0.81 on the CATH 4.2 test set, with iterative application yielding 79.5% improvement on challenging structures.

## Method Summary
The method leverages DPO to fine-tune an inverse folding model using structural feedback from a folding model. Candidate sequences are generated from the inverse folding model, their structures are predicted using ESMFold, and TM-Scores are computed to create pairwise preference labels. The DPO objective is combined with supervised fine-tuning (SFT) regularization to prevent model degeneration. The process can be applied iteratively, reinitializing the reference model with current weights each round. The approach uses LoRA adapters for efficient fine-tuning and demonstrates significant improvements in structural fidelity over sequence recovery-based baselines.

## Key Results
- Improved sequence recovery and increased average TM-Score from 0.77 to 0.81 on CATH 4.2 test set
- Iterative DPO application yields 79.5% average TM-Score increase on challenging protein structures
- Ablation shows optimal performance with SFT regularization weight λ = 1.0

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DPO with folding model feedback shifts learning from sequence recovery to structural fidelity
- **Mechanism:** Candidate sequences are scored via TM-Score after structure prediction by folding model; pairwise preferences guide optimization for structural similarity
- **Core assumption:** Folding model provides accurate and discriminative structure predictions as reliable reward signal
- **Evidence anchors:** Abstract states labels used for DPO fine-tuning; Section 3 describes TM-Score preference construction; EnerBridge-DPO shows similar pattern

### Mechanism 2
- **Claim:** SFT loss regularization mitigates model degeneration during DPO
- **Mechanism:** Pure DPO can minimize probabilities of shared tokens in rejected samples; SFT loss preserves probability mass on high-quality chosen sequences
- **Core assumption:** Top 50% TM-Score chosen sequences provide stable regularization signal
- **Evidence anchors:** Section 3 Eq. 10 shows combined loss; Table 2 ablation shows λ = 1 best performance; λ = 0 underperforms

### Mechanism 3
- **Claim:** Iterative DPO refines sampling distribution toward high-quality structural predictions
- **Mechanism:** Each round generates candidates, constructs preferences, and updates via DPO with reinitialized reference model
- **Core assumption:** Model's improved generations provide progressively better training data across rounds
- **Evidence anchors:** Abstract reports 79.5% TM-Score increase from iterative application; Section 3 describes iterative training with weight reinitialization

## Foundational Learning

- **Concept: Protein Inverse Folding vs. Folding**
  - **Why needed here:** Paper builds on inverse mapping g: T → S (structure to sequence), distinct from folding problem f: S → T (sequence to structure)
  - **Quick check question:** Can you explain why maximizing sequence recovery does not guarantee correct folding?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** DPO replaces reinforcement learning from human feedback with direct optimization on preference pairs, simplifying alignment
  - **Quick check question:** How does DPO objective differ from standard supervised fine-tuning, and what role does β play?

- **Concept: TM-Score as Structural Similarity Metric**
  - **Why needed here:** TM-Score (0-1) quantifies structural alignment between predicted and target structures, serving as proxy reward
  - **Quick check question:** Why is TM-Score preferred over RMSD for evaluating fold-level similarity, especially for proteins of different lengths?

## Architecture Onboarding

- **Component map:** Inverse folding model (InstructPLM/ProteinMPNN) with LoRA adapters -> Folding model (ESMFold) for structure prediction -> TM-Align for TM-Score computation -> DPO training loop with combined loss

- **Critical path:**
  1. Sample N sequences per structure from inverse folding model
  2. Predict structures with folding model
  3. Compute TM-Scores via TM-Align; sort and split into chosen (top 50%) and rejected (bottom 50%)
  4. Train with DPO + SFT loss, using LoRA for efficient fine-tuning

- **Design tradeoffs:**
  - More contrastive samples (N) boost recovery but may degrade TM-Score if low-quality pairs dilute signal
  - Higher λ stabilizes training but may limit exploration; λ = 1 worked best
  - ProteinMPNN required wild-type sequences as additional chosen samples to converge (smaller capacity)

- **Failure signatures:**
  - TM-Score stagnation despite increasing training steps may indicate noisy preference pairs
  - Recovery rate rising while TM-Score drops suggests overfitting to sequence-level similarity
  - Loss instability in smaller models may require simpler preference tasks or external high-quality samples

- **First 3 experiments:**
  1. Replicate single-round DPO on CATH 4.2 with InstructPLM; measure TM-Score and recovery vs. baseline
  2. Ablate λ (0, 1, 10) to validate SFT regularization contribution
  3. Run 2-3 rounds of iterative DPO on challenging structures; track TM-Score trajectory and sample diversity

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on folding model predictions as proxy reward signal introduces potential brittleness if predictions are systematically biased
- Method requires access to high-quality folding model, which may not be available for all design contexts
- SFT regularization assumption may break down if folding model predictions are noisy or sequence sampling is degenerate

## Confidence
- **High confidence:** Core DPO mechanism and iterative refinement approach are well-supported by results and methodology
- **Medium confidence:** SFT regularization's role in preventing model degeneration is plausible but optimal λ may vary by model size and data distribution
- **Medium confidence:** 79.5% TM-Score improvement via iterative DPO is well-demonstrated but may not generalize beyond CATH 4.2 without further validation

## Next Checks
1. **Test folding model robustness:** Generate sequences for held-out CATH 4.2 structures using InstructPLM + ESMFold. Compare predicted structure TM-Scores against second independent folding model (AlphaFold-Multimer or RoseTTAFold) to assess consistency.

2. **Validate SFT weight sensitivity:** Run finer-grained ablation on λ (0.1, 0.5, 1.0, 2.0, 5.0) on CATH 4.2 training subset. Measure TM-Score, sequence diversity (Levenshtein distance), and recovery rate to quantify fidelity-generalization tradeoff.

3. **Assess out-of-distribution performance:** Apply iteratively fine-tuned model to design sequences for separate benchmark (TS50/TS500 or antibody design dataset). Measure whether structural gains transfer beyond CATH 4.2 domain.