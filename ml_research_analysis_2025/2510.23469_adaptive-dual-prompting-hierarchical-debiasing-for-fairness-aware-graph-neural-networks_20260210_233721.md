---
ver: rpa2
title: 'Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural
  Networks'
arxiv_id: '2510.23469'
source_url: https://arxiv.org/abs/2510.23469
tags:
- graph
- adprompt
- information
- node
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fairness in graph neural network
  (GNN) adaptation to downstream tasks. Existing graph prompting methods focus on
  utility but overlook fairness concerns, leading to biased predictions across demographic
  groups due to attribute and structural bias in graph data.
---

# Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks

## Quick Facts
- arXiv ID: 2510.23469
- Source URL: https://arxiv.org/abs/2510.23469
- Reference count: 39
- Key outcome: ADPrompt consistently outperforms seven baselines, achieving higher accuracy (e.g., 58.14% on Credit dataset) and lower fairness gaps (e.g., ∆EO=1.08, ∆SP=1.48) compared to state-of-the-art methods.

## Executive Summary
Existing graph prompting methods focus on utility but overlook fairness concerns, leading to biased predictions across demographic groups due to attribute and structural bias in graph data. To address this, the paper proposes Adaptive Dual Prompting (ADPrompt), a hierarchical framework that combines Adaptive Feature Rectification (AFR) to suppress sensitive information in node attributes at the input layer, and Adaptive Message Calibration (AMC) to dynamically adjust information flow during message passing. The two modules are jointly optimized to balance task performance and fairness. Extensive experiments on four datasets with four pre-training strategies show that ADPrompt consistently outperforms seven baselines, achieving higher accuracy and lower fairness gaps compared to state-of-the-art methods.

## Method Summary
ADPrompt is a hierarchical graph prompting framework designed to adapt pre-trained GNNs to downstream tasks while mitigating bias across demographic groups. It consists of two learnable modules: Adaptive Feature Rectification (AFR) that applies soft gating to node attributes to suppress sensitive information at the input layer, and Adaptive Message Calibration (AMC) that dynamically adjusts neighbor message weights during propagation to mitigate structural bias. These modules are optimized jointly with a min-max adversarial objective that encourages final representations to be predictive of task labels while being invariant to sensitive attributes. The approach operates by learning lightweight prompts that modify the input and message-passing process of a frozen pre-trained GNN backbone.

## Key Results
- ADPrompt achieves 58.14% accuracy on Credit dataset with ∆EO=1.08 and ∆SP=1.48, outperforming state-of-the-art methods
- Across four datasets (Credit, German, Pokec_z, Pokec_n), ADPrompt consistently reduces fairness gaps while maintaining or improving accuracy
- The method demonstrates effectiveness with four different pre-training strategies (InfoMax, GraphCL, GAE, BGRL)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Suppressing sensitive information in input node attributes reduces initial bias that would otherwise propagate through the GNN.
- **Mechanism:** A self-gating mechanism learns a personalized attribute prompt $m_i$ for each node $v_i$. This prompt is a vector of dimension $D_x$ (same as input features) with values in $[0, 1]$, generated by a shared attribute projector $\psi$ (a small MLP) conditioned on the node's features: $m_i = \sigma(\psi(x_i))$. The prompt is applied via element-wise multiplication $\tilde{x}_i = m_i \odot x_i$, acting as a soft mask that adaptively attenuates feature dimensions correlated with sensitive attributes.
- **Core assumption:** Sensitive information (e.g., gender, race) is explicitly or implicitly encoded in the input node attributes $X$. Suppressing these features at the input layer is more effective than trying to remove bias from learned representations later, as the bias would be amplified by message passing.
- **Evidence anchors:**
  - [abstract] "To mitigate attribute bias, we design an Adaptive Feature Rectification module that learns customized attribute prompts to suppress sensitive information at the input layer, reducing bias at the source."
  - [section] "In Adaptive Feature Rectification, attribute prompts serve as input-layer gates that filter biased information from node features and mitigate bias at the source." (Page 5)
  - [corpus] Weak/no direct corpus evidence for this specific AFR mechanism in prior work. The paper claims novelty in this approach.
- **Break condition:** If sensitive information is not encoded in node attributes but is primarily structural, AFR will fail to mitigate bias effectively.

### Mechanism 2
- **Claim:** Dynamically adjusting the information flow from neighbors during message passing can mitigate structural bias and prevent bias amplification across layers.
- **Mechanism:** For each edge $(v_i, v_j)$, a structure prompt $e_{ij}^{(l-1)}$ is generated at each layer $l$. This prompt is computed by a structure projector $\phi$ (another small MLP) which takes the concatenated prompted representations of the source and target nodes: $e_{ij}^{(l-1)} = \phi([\tilde{h}_i^{(l-1)} || \tilde{h}_j^{(l-1)}])$. This prompt is added to the neighbor's representation during aggregation: $\tilde{h}_i^{(l)} = \text{AGG}^{(l)}(\tilde{h}_i^{(l-1)}, \{\tilde{h}_j^{(l-1)} + e_{ij}^{(l-1)} : v_j \in N(v_i)\})$. This acts as a learned, edge-specific calibration signal.
- **Core assumption:** Structural bias exists, where connectivity patterns differ across demographic groups (e.g., homophily). The message-passing mechanism in GNNs can amplify this bias, so dynamic, edge-level intervention is needed during propagation.
- **Evidence anchors:**
  - [abstract] "...we propose an Adaptive Message Calibration module that generates structure prompts at each layer, which adjust the message from neighboring nodes to enable dynamic and soft calibration of the information flow."
  - [section] "Our approach performs soft, fine-grained corrections at the information flow level without altering the graph structure... AMC lets each node adaptively regulate how it absorbs neighbor information through learnable structure prompts." (Page 5)
  - [corpus] Edge Prompt Tuning (arXiv:2503.00750) explores edge-level prompts but for general adaptation, not specifically fairness. The corpus lacks strong prior evidence for AMC's fairness-specific dynamic calibration.
- **Break condition:** If the structure prompts over-correct and remove task-relevant structural information, downstream accuracy may degrade significantly.

### Mechanism 3
- **Claim:** Optimizing prompts to make final node representations invariant to sensitive attributes encourages fairness while preserving task-relevant information.
- **Mechanism:** The training objective is a min-max game. The primary objective is to minimize a supervised loss $\mathcal{L}_{Sup}$ (cross-entropy) for the downstream task. A secondary, adversarial objective is to maximize the loss $\mathcal{L}_{Adv}$ of an adversary $\omega$ that tries to predict the sensitive attribute from the final prompted representation $\tilde{h}_i^{(L)}$. The total loss is $\min_{\psi, \phi, \pi} \max_{\omega} \mathcal{L}_{Sup} - \lambda \mathcal{L}_{Adv}$. This forces the prompts to produce representations that are predictive of the label but not the sensitive attribute.
- **Core assumption:** Representations that are statistically independent of sensitive attributes lead to fairer predictions as measured by group fairness metrics like Statistical Parity (SP) and Equal Opportunity (EO).
- **Evidence anchors:**
  - [abstract] "Finally, ADPrompt jointly optimizes the two prompting modules to adapt the pre-trained GNN while enhancing fairness."
  - [section] "ADPrompt enhances fairness by encouraging prompted representations to be independent of sensitive attributes. To this end, we introduce a linear adversary $\omega$..." (Page 6)
  - [corpus] Adversarial learning is a common debiasing technique in representation learning, providing a conceptual anchor, though the paper's specific combination with dual prompts is novel.
- **Break condition:** If the trade-off hyperparameter $\lambda$ is not tuned correctly, the model may either fail to improve fairness (low $\lambda$) or excessively harm task accuracy (high $\lambda$).

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs) and Message Passing**
  - **Why needed here:** ADPrompt is designed for GNNs and operates by intervening in their core message-passing process. Understanding how nodes aggregate information from neighbors is critical to grasping how AMC works.
  - **Quick check question:** How does the output representation of a node in a 2-layer GCN depend on its 2-hop neighborhood?

- **Concept: Pre-training and Adaptation Gap**
  - **Why needed here:** The entire motivation for "graph prompting" is that pre-trained GNN models are not directly suitable for downstream tasks. ADPrompt aims to bridge this gap while adding a fairness objective.
  - **Quick check question:** Why can't we simply fine-tune all parameters of a pre-trained GNN on a downstream task, and what advantage does prompting offer?

- **Concept: Group Fairness Metrics (Statistical Parity & Equal Opportunity)**
  - **Why needed here:** The paper's success is measured by these specific metrics. One must understand their definitions to interpret the results and the goal of the adversarial loss.
  - **Quick check question:** What is the key difference between Statistical Parity and Equal Opportunity? Give a scenario where a model satisfies one but not the other.

## Architecture Onboarding

- **Component map:**
  - Raw feature $x_i$ -> AFR ($m_i \odot x_i$) -> GNN Layer 1 (with AMC prompts) -> ... -> GNN Layer L -> Final representation $\tilde{h}_i^{(L)}$ -> Predictor $\hat{y}_i$ & Adversary $\hat{s}_i$

- **Critical path:** The data flow for a single node $v_i$ is: Raw feature $x_i \to$ AFR ($m_i \odot x_i$) $\to$ GNN Layer 1 (aggregates from neighbors, with AMC prompts added to neighbor messages) $\to$ ... $\to$ GNN Layer L $\to$ Final representation $\tilde{h}_i^{(L)} \to$ Predictor $\hat{y}_i$ & Adversary $\hat{s}_i$.

- **Design tradeoffs:**
  - **Soft vs. Hard Intervention:** AMC uses *additive* prompts ($\tilde{h}_j + e_{ij}$) rather than modifying edge weights or dropping edges. This is less disruptive but may be less powerful for correcting severe structural bias.
  - **Parameter Efficiency vs. Expressiveness:** Prompts are generated by small MLPs ($\psi, \phi$) shared across nodes/edges. This is efficient but might lack capacity for complex, node-specific bias patterns.
  - **Fairness-Accuracy Tradeoff:** Controlled by $\lambda$. The paper shows a clear tradeoff (Fig. 4). This is a classic tension in fair ML.

- **Failure signatures:**
  - **AFR fails:** High fairness gap ($\Delta SP, \Delta EO$) even with high $\lambda$. This suggests bias is not primarily in the input attributes. Investigate structural bias.
  - **AMC fails:** Accuracy drops significantly with little fairness gain. The prompts may be over-correcting or interfering with task-relevant message flow.
  - **Adversary wins:** The adversary $\omega$ achieves very high accuracy in predicting $s$. The prompts are failing to learn invariant representations. Check if $\lambda$ is too low or if the prompts lack capacity.
  - **Catastrophic forgetting:** The model's task accuracy collapses. This can happen if the adversarial loss dominates. Reduce $\lambda$.

- **First 3 experiments:**
  1.  **Ablation Study:** Run ADPrompt on a single dataset (e.g., German Credit) with (a) only AFR, (b) only AMC, and (c) the full model. Compare accuracy and fairness ($\Delta EO, \Delta SP$). This verifies the contribution of each component.
  2.  **Hyperparameter Sensitivity:** Sweep the hyperparameter $\lambda$ (e.g., from 0.1 to 10.0) on a validation set. Plot accuracy vs. fairness gap to characterize the Pareto frontier and select a good operating point.
  3.  **Backbone Generalization:** Run the full ADPrompt pipeline with a different pre-trained GNN backbone (e.g., GAT or GraphSAGE) instead of GCN. Compare performance to the GCN backbone to ensure the method is not architecture-specific.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the computational overhead of ADPrompt scale when applied to large-scale industrial graphs containing millions of nodes?
- Basis in paper: [explicit] The conclusion explicitly states, "Future work will explore scalability to large graphs," noting that current experiments were limited to datasets like Pokec and German.
- Why unresolved: The provided experiments utilize relatively small datasets (maximum ~67k nodes), leaving the efficiency of the dual prompting modules on massive graphs unverified.
- Evidence: Runtime and memory usage benchmarks on standard large-scale benchmarks (e.g., OGB datasets) compared to non-prompting baselines.

### Open Question 2
- Question: Can the hierarchical debiasing mechanism generalize to heterogeneous graphs with multiple node and edge types?
- Basis in paper: [explicit] The authors identify "generalization across diverse graph types" as a direction for future work in the conclusion.
- Why unresolved: The methodology and experiments are restricted to homogeneous graphs, where a single set of feature prompts and structure prompts applies uniformly.
- Evidence: Performance evaluation on heterogeneous information networks (HINs), measuring fairness across different meta-relations.

### Open Question 3
- Question: Is the Adaptive Message Calibration (AMC) module effective for mitigating bias in link prediction or graph classification tasks?
- Basis in paper: [inferred] The paper restricts all evaluations to "node classification tasks" (Page 3, 8), leaving the efficacy of the message-passing interventions on other tasks unknown.
- Why unresolved: AMC modifies neighbor aggregation to calibrate node representations, but it is unclear if this calibration transfers to tasks where the prediction target is an edge or a whole graph.
- Evidence: Application of ADPrompt to link prediction benchmarks using fairness metrics like Demographic Parity on edges.

## Limitations

- The method assumes sensitive information is primarily encoded in node attributes, which may not hold for datasets where structural bias dominates.
- The parameter efficiency gains come at the cost of potential underfitting if the prompt generators lack capacity for complex bias patterns.
- The adversarial training objective introduces optimization challenges that could lead to unstable convergence across different datasets.

## Confidence

- **High confidence:** The AFR and AMC mechanisms are clearly defined and implementable; the min-max optimization framework is standard in adversarial debiasing.
- **Medium confidence:** The empirical superiority over seven baselines is well-supported, but the choice of hyperparameters (particularly λ) and the sensitivity to different pre-training strategies could affect generalizability.
- **Medium confidence:** The claim that prompts enable efficient adaptation while maintaining fairness is supported, but the comparison with full fine-tuning is not directly provided, leaving the efficiency advantage partially unverified.

## Next Checks

1. **Ablation of sensitive attribute encoding:** Test AFR on datasets where sensitive information is primarily structural (e.g., synthetic graphs with homophily-based bias) to verify the mechanism works when bias is not in attributes.
2. **Efficiency benchmarking:** Compare ADPrompt's parameter count and adaptation speed against full fine-tuning of the GNN backbone on a held-out test set to quantify the claimed efficiency gains.
3. **Robustness to λ selection:** Perform a systematic ablation of λ across all datasets and pre-training strategies to establish whether the reported Pareto frontier is consistent or varies significantly with dataset characteristics.