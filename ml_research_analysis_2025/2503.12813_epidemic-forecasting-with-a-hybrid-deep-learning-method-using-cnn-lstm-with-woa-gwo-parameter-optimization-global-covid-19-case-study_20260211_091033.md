---
ver: rpa2
title: 'Epidemic Forecasting with a Hybrid Deep Learning Method Using CNN-LSTM With
  WOA-GWO Parameter Optimization: Global COVID-19 Case Study'
arxiv_id: '2503.12813'
source_url: https://arxiv.org/abs/2503.12813
tags:
- forecasting
- number
- covid-19
- data
- lstm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a hybrid deep learning framework that combines
  Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks
  to forecast COVID-19 case trends across 24 countries on six continents. The CNN
  component extracts spatial features from raw epidemiological data, while the LSTM
  models temporal patterns, improving prediction accuracy.
---

# Epidemic Forecasting with a Hybrid Deep Learning Method Using CNN-LSTM With WOA-GWO Parameter Optimization: Global COVID-19 Case Study

## Quick Facts
- arXiv ID: 2503.12813
- Source URL: https://arxiv.org/abs/2503.12813
- Reference count: 40
- Primary result: Hybrid CNN-LSTM with RS-GWO-WOA optimization outperforms ARIMA and standalone LSTM on 24-country COVID-19 forecasting.

## Executive Summary
This study introduces a hybrid deep learning framework combining Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks for multi-step forecasting of COVID-19 cases across 24 countries. The CNN extracts spatial features from epidemiological time series, while the LSTM models temporal dependencies. A hybrid Whale Optimization Algorithm-Gray Wolf Optimization (RS-GWO-WOA) is employed to fine-tune hyperparameters, yielding statistically significant improvements in predictive accuracy over established benchmarks like ARIMA and standalone LSTM models.

## Method Summary
The framework preprocesses multivariate COVID-19 time series (confirmed cases, deaths, recoveries) via missing value imputation and Min-Max normalization. A CNN-LSTM architecture processes the data: Conv1D layers extract local patterns, MaxPooling reduces dimensionality, Flatten converts features to vectors, RepeatVector reshapes for LSTM input, and a Dense layer outputs forecasts. Hyperparameters (filters, kernel size, pool size, LSTM units) are optimized using a random switcher between GWO and WOA. The model is trained for 100 epochs with batch size 1 and evaluated on 20% held-out test data.

## Key Results
- The hybrid CNN-LSTM with RS-GWO-WOA optimization outperforms ARIMA and standalone LSTM baselines on RMSE, MAE, and $R^2$ metrics.
- The model demonstrates robust forecasting capability across diverse geographical regions (6 continents, 24 countries).
- Statistical significance is achieved in predictive accuracy gains, particularly in long-term forecasting horizons.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** CNN layers extract local features from epidemiological time series before temporal modeling.
- **Mechanism:** 1D convolutional kernels slide over the input sequence to detect local patterns (trend changes, sudden spikes). Pooling reduces dimensionality while preserving salient features. The flattened output is reshaped for LSTM input.
- **Core assumption:** Disease spread dynamics contain detectable local patterns that, when extracted, simplify temporal learning. Assumption: The 1D convolution meaningfully captures "spatial" features even though the input is inherently temporal.
- **Evidence anchors:**
  - [abstract] "The CNN extracts spatial features from raw epidemiological data, while the LSTM models temporal patterns."
  - [section 2.1] "The convolutional layer in a CNN leverages convolution kernels to extract features from input variables."
  - [corpus] Weak direct evidence; related work uses CNN-LSTM for similar forecasting but does not isolate CNN's contribution mechanism.
- **Break condition:** If raw epidemiological data lacks local pattern structure (e.g., purely stochastic noise), CNN feature extraction may add computational overhead without predictive gain.

### Mechanism 2
- **Claim:** LSTM layers capture long-term temporal dependencies after CNN feature extraction.
- **Mechanism:** LSTM uses gated memory cells (forget, input, output gates) to selectively retain or discard information across time steps, enabling learning of extended temporal dependencies in the CNN-extracted feature sequence.
- **Core assumption:** Disease transmission dynamics exhibit long-term dependencies (e.g., multi-week cycles, intervention lag effects) that persist after local feature extraction.
- **Evidence anchors:**
  - [abstract] "LSTM models temporal patterns, yielding precise and adaptable predictions."
  - [section 2.2] Equations 3-8 define forget, input, and output gates; "LSTM models can capture long-term dependencies, making them a good choice for sequence modeling."
  - [corpus] Related work confirms LSTM utility for epidemic time series but provides mixed evidence on whether long-term dependencies are the primary driver vs. short-term autocorrelation.
- **Break condition:** If prediction horizon is very short (1-3 days) and data is dominated by short-term autocorrelation, simpler RNN or ARIMA may suffice; LSTM complexity may overfit.

### Mechanism 3
- **Claim:** Random switching between GWO and WOA during optimization balances exploration and exploitation for hyperparameter tuning.
- **Mechanism:** A random number ε ∈ [0,1] selects between GWO (exploitation-heavy, guided by alpha/beta/delta wolves) and WOA (exploration-heavy, bubble-net strategy) at each iteration. This aims to avoid local optima while maintaining convergence speed.
- **Core assumption:** The hyperparameter search landscape is multimodal; pure GWO or WOA would converge to suboptimal regions. Assumption: Random switching (not adaptive or performance-based switching) is sufficient to balance these dynamics.
- **Evidence anchors:**
  - [abstract] "A hybrid optimization strategy combining the Whale Optimization Algorithm (WOA) and Gray Wolf Optimization (GWO) to fine-tune hyperparameters."
  - [section 2.3.3] "RS-GWO-WOA randomly switches between GWA and WOA in each iteration, so the computational time of finding the optimal solution decreases."
  - [corpus] Limited external validation of RS-GWO-WOA for epidemic forecasting specifically; metaheuristic tuning is common but the specific hybrid's superiority is not independently confirmed.
- **Break condition:** If the hyperparameter space is low-dimensional or nearly convex, simpler grid/random search may be more efficient; hybrid metaheuristic overhead is unjustified.

## Foundational Learning

- **Concept: Time Series Feature Engineering (Sliding Windows, Normalization)**
  - **Why needed here:** The model requires converting raw daily case counts into supervised learning samples via sliding windows. Min-Max normalization is applied for stable gradient descent.
  - **Quick check question:** Can you explain how a sliding window of size `t` transforms a univariate time series into X-y pairs for supervised learning?

- **Concept: Recurrent Neural Network Memory and Vanishing Gradients**
  - **Why needed here:** LSTM is designed to address vanishing gradients in standard RNNs when learning long sequences. Understanding this helps diagnose training failures.
  - **Quick check question:** Why does a vanilla RNN struggle to learn dependencies across 50+ time steps, and how do LSTM gates mitigate this?

- **Concept: Metaheuristic Optimization (Exploration vs. Exploitation Tradeoff)**
  - **Why needed here:** GWO-WOA tuning relies on balancing global exploration and local exploitation. Without this conceptual grasp, interpreting optimizer behavior is difficult.
  - **Quick check question:** In population-based optimization, what is the risk of pure exploitation, and how do algorithms like WOA encourage exploration?

## Architecture Onboarding

- **Component map:** Input -> Min-Max Scaling -> Missing Value Imputation -> CNN (Conv1D -> MaxPooling1D -> Flatten -> RepeatVector) -> LSTM -> Dense Output
- **Critical path:** Data preprocessing -> CNN feature extraction -> LSTM temporal modeling -> Dense prediction. Hyperparameter tuning loop runs offline per country before final training.
- **Design tradeoffs:**
  - **CNN-LSTM vs. ConvLSTM:** Paper uses sequential CNN→LSTM; ConvLSTM integrates convolution within LSTM cells. The former is modular and easier to debug; the latter may better capture spatiotemporal structure but is more complex.
  - **Random switching vs. adaptive switching:** RS-GWO-WOA uses a fixed 0.5 threshold; adaptive methods could dynamically adjust based on improvement rate but increase implementation complexity.
  - **Per-country tuning vs. global hyperparameters:** The paper tunes hyperparameters per country, potentially improving fit but reducing generalizability and increasing compute.
- **Failure signatures:**
  - **High MSE on test set despite low training loss:** Likely overfitting; check regularization, reduce LSTM units, or increase training data proportion.
  - **Optimizer stuck at high RMSE:** RS-GWO-WOA may be trapped in local optima; increase population size or iterations, or verify bounds on hyperparameters.
  - **Erratic predictions across runs:** Stochastic initialization and optimizer randomness; enforce fixed seeds for reproducibility during debugging.
- **First 3 experiments:**
  1. **Baseline replication:** Implement the CNN-LSTM pipeline on a single country (e.g., Italy) without metaheuristic tuning. Use default hyperparameters. Record RMSE, MAE, R². This establishes a baseline.
  2. **Ablation study:** Replace RS-GWO-WOA with grid search or random search for the same hyperparameter space. Compare final RMSE and computational cost to quantify the optimizer's contribution.
  3. **Architecture variant:** Swap the sequential CNN→LSTM for a ConvLSTM layer. Retrain on the same country using identical hyperparameters. Assess whether integrated spatiotemporal processing improves accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the inclusion of demographic external data (age, location, sex) enhance the predictive performance of the hybrid CNN-LSTM model?
- Basis in paper: [explicit] The Conclusion explicitly lists "exploring the impact of external data such as age, location, and sex" as a future direction to improve overall performance.
- Why unresolved: The current study utilized only time-series data of confirmed cases, recoveries, and deaths, without incorporating exogenous demographic variables.
- What evidence would resolve it: Experiments integrating specific demographic datasets into the model's input features, followed by a comparative analysis of error metrics (RMSE, MAE) against the current baseline.

### Open Question 2
- Question: Would an adaptive or performance-based switching strategy outperform the random switch in the RS-GWO-WOA optimization algorithm?
- Basis in paper: [inferred] The authors note in Section 3.3 that the Random Switcher selects algorithms using a random factor "without considering the performance information," which may cause the search to get "stuck in the Local Optima (LO)."
- Why unresolved: The current random approach lacks a feedback mechanism to favor the optimizer (GWO or WOA) that is currently yielding better fitness results.
- What evidence would resolve it: Implementing a dynamic switching mechanism based on iteration fitness improvements and comparing the convergence speed and final accuracy against the random switch method.

### Open Question 3
- Question: Does the proposed framework generalize effectively to other infectious diseases with transmission dynamics distinct from COVID-19?
- Basis in paper: [inferred] While the abstract claims the method is a "versatile method for forecasting epidemic trends," the validation is restricted exclusively to COVID-19 data across 24 countries.
- Why unresolved: Different epidemics may exhibit different spatial-temporal patterns, incubation periods, and intervention impacts that the current hyperparameter tuning (specific to COVID-19 trends) may not capture.
- What evidence would resolve it: Applying the identical CNN-LSTM with RS-GWO-WOA pipeline to datasets of other diseases (e.g., Influenza or Dengue) and evaluating performance against benchmarks.

## Limitations
- The input sequence length (lookback window) is not explicitly specified, a critical hyperparameter for time-series forecasting.
- The internal optimizer (Adam/SGD) and learning rate are not clearly stated in the experimental section.
- The specific COVID-19 dataset source and preprocessing details beyond basic missing value imputation are not provided.

## Confidence
- **High:** The CNN-LSTM hybrid architecture design and its general application to epidemic forecasting.
- **Medium:** The superiority claims over ARIMA and standalone LSTM models, given that exact replication is blocked by unspecified hyperparameters.
- **Low:** The specific contribution of the RS-GWO-WOA optimizer, as external validation of this exact hybrid metaheuristic is limited.

## Next Checks
1. Replicate the baseline CNN-LSTM model on a single country using default hyperparameters to establish performance floors.
2. Implement grid/random search as an ablation to quantify the RS-GWO-WOA optimizer's contribution.
3. Compare the sequential CNN→LSTM architecture against a ConvLSTM variant to assess spatiotemporal integration benefits.