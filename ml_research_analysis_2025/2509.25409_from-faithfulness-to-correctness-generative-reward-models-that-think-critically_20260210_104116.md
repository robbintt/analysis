---
ver: rpa2
title: 'From Faithfulness to Correctness: Generative Reward Models that Think Critically'
arxiv_id: '2509.25409'
source_url: https://arxiv.org/abs/2509.25409
tags:
- reward
- answer
- correctness
- correct
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a Thinking-supervised Reward Model (TRM) that
  addresses the challenge of distinguishing between faithfulness to supporting documents
  and factual correctness in open-domain question answering. TRM employs a structured
  approach where it first evaluates sentence-level faithfulness to documents, then
  performs reasoning to assess correctness, thereby fostering critical thinking abilities.
---

# From Faithfulness to Correctness: Generative Reward Models that Think Critically

## Quick Facts
- **arXiv ID**: 2509.25409
- **Source URL**: https://arxiv.org/abs/2509.25409
- **Reference count**: 25
- **Primary result**: TRM improves F1 score by 6.5% and detection rate by 5.9% for identifying incorrect sentences compared to baselines

## Executive Summary
This paper addresses the challenge of distinguishing between faithfulness to supporting documents and factual correctness in open-domain question answering. The authors introduce a Thinking-supervised Reward Model (TRM) that employs a structured approach where it first evaluates sentence-level faithfulness to documents, then performs reasoning to assess correctness, thereby fostering critical thinking abilities. The model uses explicit sentence-level supervision with dual reward signals (correctness + faithfulness) and structured reasoning trajectories to improve fine-grained error detection over outcome-level rewards.

## Method Summary
The method involves two-stage training: (1) Supervised fine-tuning (SFT) on faithfulness→reasoning→correctness sequences with NLL loss for 4 epochs, followed by (2) reinforcement learning with GRPO using sentence-level rewards that combine correctness, faithfulness, and extra reward for detecting incorrect labels. The reward model (DeepSeek-R1-Distill-Qwen-32B) takes queries, documents, and segmented answers as input, outputting per-sentence faithfulness scores, correctness reasons, and correctness scores. A policy model (Qwen2.5-32B-Instruct) is then optimized using combined TRM and preference rewards to generate better answers.

## Key Results
- TRM achieves 6.5% improvement in F1 score for detecting incorrect sentences compared to baselines
- Detection rate improves by 5.9% for identifying worst answers among candidates
- When integrated into policy optimization, TRM achieves up to 30.3% improvement in answer correctness and 35% increase in usefulness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit sequential evaluation (faithfulness→reasoning→correctness) improves error detection in open-domain QA where verification is inherently ambiguous.
- Mechanism: TRM forces the model to first assess document alignment (faithness), then engage internal knowledge to determine factual accuracy. This prevents conflating "matches the source" with "is actually correct."
- Core assumption: Models can learn to distrust external sources when internal knowledge conflicts, rather than defaulting to document authority.
- Evidence anchors: [abstract] "TRM first assesses the faithfulness of each answer sentence to the supporting documents, and then applies a reasoning step to evaluate sentence-level correctness."

### Mechanism 2
- Claim: Sentence-level supervision with dual reward signals (correctness + faithfulness) improves fine-grained error detection over outcome-level rewards.
- Mechanism: Reward function ri,k = c(i,k) + α·f(i,k) (α=0.5) combines correctness and faithfulness. Additional reward for correctly predicting incorrect labels counteracts 86.86% positive class imbalance.
- Core assumption: Faithfulness assessment quality correlates with downstream correctness prediction accuracy.
- Evidence anchors: [section 2.3] "when a model learns to accurately assess the faithfulness of each intermediate reasoning step, its probability of producing correct answers increases significantly."

### Mechanism 3
- Claim: Structured supervision during SFT creates interpretable reasoning trajectories that make subsequent RL exploration more effective.
- Mechanism: SFT explicitly teaches the Faithfulness→Reasoning→Correctness pattern as target output. RL then uses this as initialization for policy optimization with GRPO.
- Core assumption: The reasoning space for open-domain QA verification is sufficiently constrained that SFT-established patterns transfer better than RL-only exploration.
- Evidence anchors: [section 2.3] "By exposing the model to examples that decompose the reasoning process into interpretable steps, SFT facilitates subsequent stages of training."

## Foundational Learning

- Concept: Faithfulness vs. Correctness distinction
  - Why needed here: The entire TRM architecture hinges on recognizing that "matches documents" ≠ "is factually accurate." Without this, the model cannot perform the critical assessment the paper claims.
  - Quick check question: Given a retrieved document stating "The moon landing occurred in 1965" and an answer "The moon landing was in 1965," is the answer faithful? Is it correct?

- Concept: Class imbalance in reward signals
  - Why needed here: 86.86% of sentences are correct; naive reward optimization will over-predict the majority class. The paper addresses this with additional reward for detecting incorrect labels.
  - Quick check question: If a reward model achieves 90% accuracy on a 90% positive dataset by always predicting "correct," what is its F1 score for the negative class?

- Concept: Generative reward models vs. discriminative scoring
  - Why needed here: TRM produces explicit reasoning outputs (faithfulness score, correctness reason, correctness score) rather than scalar rewards, enabling interpretability and structured supervision.
  - Quick check question: What information is lost when a discriminative reward model outputs only a scalar score compared to a generative model that outputs reasoning traces?

## Architecture Onboarding

- Component map: Query + Documents + Answer → Sentence Separator → TRM (Faithfulness→Reasoning→Correctness) → Policy Model (Qwen2.5-32B-Instruct) → Improved Answer

- Critical path: Data annotation (faithfulness→correctness with reasoning rationale) → SFT training (4 epochs, lr=3e-6) → RL training (800 steps, lr=5e-8, α=0.5) → Policy optimization (TRM + Prefer rewards combined, β weighting, GRPO algorithm)

- Design tradeoffs: Sentence-level vs. answer-level supervision (finer granularity vs. annotation effort), α=0.5 faithfulness weight (document alignment pressure), SFT→RL vs. RL-only (constrained exploration vs. stable initialization)

- Failure signatures: High faithfulness + high correctness for answers contradicting internal knowledge (model hasn't learned critical assessment), F1(incorrect sentences) decreases during RL (reward signal overfitting to majority class), policy generates correct but useless answers (β for Prefer reward too low)

- First 3 experiments: 1) Replicate TRM vs. PRM vs. ORM comparison on held-out test set (F1 incorrect, Detection rate) to validate implementation, 2) Ablate reasoning step (TRM- variant) to confirm contribution of explicit reasoning supervision, 3) Test policy optimization with varying β values (Prefer weight) to identify correctness-usefulness tradeoff frontier

## Open Questions the Paper Calls Out

- Can reinforcement learning be redesigned to avoid overfitting in constrained reasoning spaces where multiple solution paths do not naturally exist? The authors note that "our task offers little room for RL to discover new reasoning strategies."

- What data augmentation or sampling strategies can effectively address the severe class imbalance (86.86% correct vs. 13.14% incorrect sentences) in reward model training? The current approach uses additional reward for incorrect predictions but this is a partial solution.

- How does the TRM approach generalize to domains where faithfulness and correctness are not easily separable, such as creative writing or subjective opinion tasks? The paper focuses exclusively on factual open-domain QA with clear correctness criteria.

## Limitations

- Dataset dependency on proprietary Tencent search logs raises questions about generalization to other retrieval sources
- Limited qualitative analysis of reasoning traces showing their practical utility for debugging or human oversight
- Evaluation relies on GPT-4.1 and human annotators assessing factual correctness, which may not extend to tasks where correctness is inherently subjective

## Confidence

**High Confidence**: The core mechanism of distinguishing faithfulness from correctness is well-grounded and the SFT training procedure is clearly specified with appropriate hyper-parameters.

**Medium Confidence**: The 30.3% correctness improvement claim relies on comparison with baselines that may not represent state-of-the-art reward models, and the effect size across different domains is unclear.

**Low Confidence**: Generalization claims are based on a single proprietary dataset with limited statistical power from the CRUD dataset evaluation (100 test queries).

## Next Checks

1. **Dataset Transferability Test**: Replicate the TRM architecture using an open-source QA dataset (e.g., Natural Questions or HotpotQA) with synthetic document contexts to verify the faithfulness→correctness distinction works outside Tencent's proprietary data.

2. **Ablation of Reasoning Supervision**: Implement a variant that directly predicts correctness from faithfulness without the intermediate reasoning step to quantify the exact contribution of explicit reasoning supervision versus the faithfulness→correctness correlation.

3. **Long-tail Error Analysis**: Conduct a systematic analysis of incorrect predictions to identify whether failures stem from faithfulness assessment errors, reasoning gaps, or knowledge limitations - this would validate whether the architecture addresses the right failure modes.