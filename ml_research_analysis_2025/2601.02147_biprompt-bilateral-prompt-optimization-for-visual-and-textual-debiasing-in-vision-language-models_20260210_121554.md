---
ver: rpa2
title: 'BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in
  Vision-Language Models'
arxiv_id: '2601.02147'
source_url: https://arxiv.org/abs/2601.02147
tags:
- visual
- biprompt
- prompt
- spurious
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces BiPrompt, a bilateral prompt optimization
  framework that addresses visual and textual biases in vision-language models (VLMs)
  like CLIP. While existing methods focus on debiasing only one modality, BiPrompt
  jointly mitigates spurious feature reliance in both visual and textual domains during
  test-time adaptation.
---

# BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models

## Quick Facts
- arXiv ID: 2601.02147
- Source URL: https://arxiv.org/abs/2601.02147
- Authors: Sunny Gupta; Shounak Das; Amit Sethi
- Reference count: 3
- One-line primary result: Joint visual and textual debiasing framework that improves worst-group accuracy in vision-language models during test-time adaptation

## Executive Summary
BiPrompt introduces a bilateral prompt optimization framework that addresses biases in both visual and textual modalities of vision-language models. Unlike existing approaches that focus on single-modality debiasing, BiPrompt jointly mitigates spurious feature reliance during test-time adaptation. The framework employs structured attention-guided erasure for visual debiasing and balanced prompt normalization for textual debiasing, working together to minimize conditional mutual information between spurious cues and predictions. Evaluated across real-world and synthetic benchmarks, BiPrompt demonstrates consistent improvements in both average and worst-group accuracies compared to prior test-time debiasing methods.

## Method Summary
BiPrompt implements a two-pronged approach to debiasing vision-language models. For the visual modality, it uses structured attention-guided erasure to suppress background activations and enforce consistency between causal and spurious regions. For the textual modality, it introduces balanced prompt normalizationâ€”a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. These components work synergistically to steer models toward causal, domain-invariant reasoning without requiring retraining. The framework operates during test-time adaptation, making it a lightweight solution for improving model trustworthiness across diverse bias scenarios.

## Key Results
- Consistently improves both average and worst-group accuracies over prior test-time debiasing methods
- Jointly mitigates spurious feature reliance in visual and textual domains during test-time adaptation
- Demonstrates effectiveness on both real-world and synthetic bias benchmarks

## Why This Works (Mechanism)
BiPrompt works by simultaneously addressing visual and textual biases that VLMs like CLIP rely on during inference. The visual debiasing component uses structured attention to identify and suppress background activations that correlate with target classes but aren't causally related. The textual debiasing component normalizes class embeddings to prevent bias amplification through language priors. By minimizing conditional mutual information between spurious cues and predictions, the framework forces the model to rely on genuinely causal features rather than dataset-specific correlations, resulting in more robust and fair predictions across diverse data distributions.

## Foundational Learning

**Vision-Language Models (VLMs)**: Joint embedding models that map images and text into a shared semantic space, enabling cross-modal understanding and retrieval.
*Why needed*: Forms the foundation for understanding how visual and textual information interact in modern AI systems
*Quick check*: Can explain how CLIP or similar models encode both modalities into the same embedding space

**Test-Time Adaptation**: Methods that adapt pretrained models during inference without additional training data or parameter updates
*Why needed*: Critical for deploying models in real-world scenarios where bias patterns may differ from training distribution
*Quick check*: Can distinguish between training-time and test-time debiasing approaches

**Conditional Mutual Information**: Measures the dependence between spurious features and predictions given the true label
*Why needed*: Provides the theoretical foundation for understanding what the debiasing framework is actually optimizing
*Quick check*: Can explain how minimizing this quantity leads to more robust predictions

**Structured Attention Mechanisms**: Techniques for identifying and manipulating specific feature regions based on their contribution to model predictions
*Why needed*: Enables targeted suppression of background activations that correlate with target classes
*Quick check*: Can describe how attention maps are generated and used for feature manipulation

**Isotropic Semantic Space**: A normalized embedding space where class representations are uniformly distributed without directional bias
*Why needed*: Ensures balanced class representations that don't favor certain groups based on language priors
*Quick check*: Can explain why uniform distribution of class embeddings matters for fairness

## Architecture Onboarding

**Component Map**: Image Embeddings -> Structured Attention Module -> Visual Debiasing -> Masked Features; Text Embeddings -> Prompt Normalization -> Balanced Embeddings; Both -> Joint Prediction Layer -> Debiased Output

**Critical Path**: Input -> VLM Backbone -> Attention Analysis -> Background Suppression -> Normalized Prompts -> Prediction -> Loss Calculation -> Gradient Update (test-time)

**Design Tradeoffs**: The framework balances computational efficiency during test-time adaptation against debiasing strength, opting for lightweight prompt optimization rather than full fine-tuning

**Failure Signatures**: Poor performance on datasets with complex, multi-factor biases; potential degradation on unbiased data when debiasing is too aggressive; computational overhead during inference

**First Experiments**:
1. Evaluate worst-group accuracy improvement on Waterbirds dataset with background bias
2. Test balanced prompt normalization effectiveness on CelebA dataset with gender/race bias
3. Measure computational overhead during test-time adaptation compared to baseline methods

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation of cross-dataset generalization with different bias patterns
- No comparison against domain generalization baselines
- Computational overhead during test-time adaptation not fully characterized

## Confidence
- Claim: Consistently improves average and worst-group accuracies over prior methods
  - Confidence: Medium
- Claim: Enforces consistency between causal and spurious regions through attention-guided erasure
  - Confidence: Medium
- Claim: Balanced prompt normalization creates isotropic semantic space
  - Confidence: Medium

## Next Checks
1. Conduct ablation studies to quantify individual contributions of visual vs. textual debiasing components across diverse bias types
2. Test framework robustness on out-of-distribution datasets with different bias patterns than training data
3. Evaluate computational efficiency and wall-clock time impact during test-time adaptation compared to baseline methods