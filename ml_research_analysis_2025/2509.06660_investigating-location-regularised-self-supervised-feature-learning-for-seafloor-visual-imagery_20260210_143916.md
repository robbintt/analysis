---
ver: rpa2
title: Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor
  Visual Imagery
arxiv_id: '2509.06660'
source_url: https://arxiv.org/abs/2509.06660
tags:
- latent
- views
- representations
- images
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the use of location metadata to improve
  self-supervised feature learning for seafloor visual imagery. Six state-of-the-art
  self-supervised learning (SSL) frameworks were evaluated, including CNN and Vision
  Transformer (ViT) models with varying latent-space dimensionality, across three
  diverse seafloor image datasets.
---

# Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery

## Quick Facts
- arXiv ID: 2509.06660
- Source URL: https://arxiv.org/abs/2509.06660
- Reference count: 32
- Primary result: Location-regularised SSL improves downstream classification by 4.9±4.0% (CNNs) and 6.3±8.9% (ViTs) over standard SSL for seafloor imagery

## Executive Summary
This study demonstrates that incorporating location metadata into self-supervised learning frameworks significantly improves feature extraction for seafloor visual imagery classification. The key innovation is defining positive pairs not as augmented views of the same image, but as physically nearby images within a specified radius. This approach leverages the spatial autocorrelation of seafloor habitats, where nearby locations tend to share similar characteristics. The method was evaluated across six state-of-the-art SSL frameworks, three diverse seafloor datasets, and both CNN and Vision Transformer architectures, showing consistent performance gains particularly for low-dimensional representations.

## Method Summary
The method implements location-regularised self-supervised learning by modifying standard SSL frameworks to sample positive pairs from images taken within a specified radius $r_{loc}$ rather than from augmented views of the same image. For each anchor image $x_i$ with coordinates $(N_i, E_i)$, a positive pair is formed with another image $x_j$ where $\sqrt{(N_i-N_j)^2 + (E_i-E_j)^2} < r_{loc}$. This was applied to six SSL frameworks (SimCLR, SimSiam, MoCo-v2, SwAV, DeepCluster-v2 for CNNs; DINO for ViTs) across three seafloor datasets with georeferenced imagery. Downstream classification used linear SVMs trained on the learned latent representations, with evaluation using macro F1-score.

## Key Results
- Location-regularised SSL consistently improves downstream classification over standard SSL (4.9±4.0% F1 gain for CNNs, 6.3±8.9% for ViTs)
- Low-dimensional CNNs with location-regularisation achieved best performance (0.778±0.115 F1), outperforming best pre-trained CNN by 6.0±1.4%
- High-dimensional ViTs showed strong generalization with pre-training, matching location-regularised models (0.795 F1)
- Multi-crop SSL strategies (SwAV, DeepCluster-v2) underperformed positive-pair methods when using location-regularisation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Location-regularisation improves feature discrimination by enforcing invariance to natural visual variations within habitats rather than pixel-level augmentations
- **Mechanism:** Standard SSL enforces invariance only to pixel-level augmentations, potentially overfitting to specific texture details. Location-regularisation forces the model to learn invariance to natural visual variations (different angles, lighting) inherent to specific seafloor habitats by using distinct but nearby images as positive pairs
- **Core assumption:** Seafloor substrates and habitats vary at spatial scales much larger than a single image frame (1-10 m), implying physically close images share semantic labels
- **Evidence anchors:** Abstract shows consistent improvements; section III explains the proximity assumption; related work on spatial alignment supports generalizability
- **Break condition:** If $r_{loc}$ exceeds average habitat patch size, positive pairs will contain semantically dissimilar content (e.g., sand vs. rock), confusing the model

### Mechanism 2
- **Claim:** Location-regularisation provides stronger utility for low-dimensional latent representations in CNNs than for high-dimensional ones
- **Mechanism:** High-dimensional spaces can naturally separate fine-grained noise, whereas low-dimensional spaces are more constrained. Location metadata groups diverse views of the same semantic class tightly together, effectively "compressing" required feature variability into smaller latent spaces and preventing class collapse
- **Core assumption:** Dimensionality reduction via PCA retains semantic information that can be better separated if intra-class variance is minimized during pre-training
- **Evidence anchors:** Abstract shows low-dim location-regularised CNN outperformed best pre-trained CNN by 6.0±1.4%; section IV.C notes greater enhancement for low-dimensional models
- **Break condition:** If downstream task requires fine-grained distinctions between classes appearing in same location, aggressive compression may lose necessary details

### Mechanism 3
- **Claim:** Architectures with strong pre-trained priors (ViTs) require less dataset-specific optimization than CNNs, reducing relative gain of location-regularisation
- **Mechanism:** High-dimensional ViTs pre-trained on ImageNet exhibit strong generalization due to global self-attention mechanisms that already capture robust structural features. While location-regularisation still helps fine-tuning, the ViT's pre-existing "world model" is sufficient to characterize seafloor imagery without strict spatial constraints needed by CNNs
- **Core assumption:** ViTs pre-trained on large, generic datasets learn transferable structural patterns applicable to underwater domains
- **Evidence anchors:** Abstract notes pre-trained ViTs show strong generalization, matching best location-regularised SSL; section IV.C shows high-dimensional ViTs perform consistently better
- **Break condition:** If target domain is visually distinct from terrestrial data, ViT's pre-trained prior may fail, requiring location-regularised training from scratch

## Foundational Learning

- **Concept:** Contrastive Learning (SimCLR/MoCo)
  - **Why needed here:** To understand how "positive" and "negative" pairs drive feature extraction. The paper modifies the definition of these pairs, which is the core lever for performance improvement
  - **Quick check question:** How does the loss function behave if all inputs are treated as positive pairs (collapse), and how does the paper's location method prevent this?

- **Concept:** Spatial Autocorrelation (Tobler's Law)
  - **Why needed here:** This is the theoretical justification for the "Proximity assumption." One must understand that seafloor habitats are spatially contiguous structures, not random noise
  - **Quick check question:** Does the target survey environment have smooth terrain (validating the assumption) or patchy, chaotic mixed substrates (violating it)?

- **Concept:** Latent Space Dimensionality (PCA)
  - **Why needed here:** The paper explicitly manipulates latent dimensions (512 vs 128) to balance computational cost and feature retention. Understanding this trade-off is crucial for selecting the right model size for deployment
  - **Quick check question:** Why might a lower-dimensional latent space overfit less than a high-dimensional one when data is limited?

## Architecture Onboarding

- **Component map:** AUV imagery + Navigation logs -> Data Loader (implements Eq. 1 spatial queries) -> Encoder (ResNet18 or DINO) -> Projector (MLP head, discarded) -> Latent vectors (128d or 512d/768d) -> Linear Classifier (SVM)

- **Critical path:** The implementation of the spatial sampler (Geo-Sampler). Standard PyTorch/TensorFlow dataloaders assume i.i.d. samples. This architecture requires a custom `get_item` or `batch_sampler` that accesses location metadata to retrieve the "neighbor" image $x_j$ when anchor $x_i$ is drawn

- **Design tradeoffs:**
  - **CNN + Loc-Reg (Low-dim):** Best for efficiency and storage (0.778 F1, small footprint). Requires training from scratch
  - **ViT + Pre-trained (High-dim):** Best for accuracy/zero-shot generalization (0.795 F1). High computational cost (86M params vs 11.7M). Loc-Reg provides marginal gains here

- **Failure signatures:**
  - **Performance Drop vs. Baseline:** $r_{loc}$ likely set too large, grouping distinct habitats (e.g., "Coral" and "Sand") as positive pairs
  - **Mode Collapse (Constant Output):** $r_{loc}$ too small or strict, reverting to standard SSL where augmentations are too easy or too hard to distinguish
  - **ViT Memory Overflow:** Batch size too high for high-dimensional 768-d embeddings; reduce batch size or use gradient checkpointing

- **First 3 experiments:**
  1. **Spatial Scale Sensitivity:** Run sweep on $r_{loc}$ (e.g., [0.5m, 2.0m, 5.0m, 10.0m]) on small subset to identify "habitat scale" before full training
  2. **Dimensionality Ablation:** Train best CNN (GeoCLR) and extract features at 512-d and 128-d. Train linear probe to verify paper's finding that low-dim + loc-reg is sufficient
  3. **Architecture Baseline:** Compare frozen pre-trained ViT (zero-shot) against Loc-Reg fine-tuned ViT on validation set to see if fine-tuning cost is justified for specific habitat

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do multi-crop SSL strategies (e.g., SwAV, DeepCluster-v2) underperform compared to positive-pair methods when using location-regularisation?
- **Basis in paper:** The authors state that "SSLs that used positive pairs outperformed the multi-crop strategy when using location-regularisation" and hypothesise that this "degradation may stem from semantic ambiguity, where local crops may be more susceptible to lacking the global context"
- **Why unresolved:** The paper identifies the performance drop but does not validate the specific mechanism (semantic ambiguity vs. conflicting signals) causing it
- **What evidence would resolve it:** Ablation studies varying crop sizes relative to the spatial distance $r_{loc}$ to determine if overlapping crops mitigate the semantic ambiguity

### Open Question 2
- **Question:** Does location-regularisation provide significant value for Vision Transformers (ViTs) given the strong generalisation of pre-trained models?
- **Basis in paper:** The paper notes that "pre-trained ViTs show strong generalisation, matching the best-performing location-regularised SSL" (0.795 F1-score for both), leaving the utility of dataset optimisation for ViTs uncertain
- **Why unresolved:** While the method improves CNNs significantly, the negligible gain for high-dimensional ViTs suggests pre-training on generic datasets may be sufficient, rendering the complex regularisation redundant for this architecture
- **What evidence would resolve it:** Testing on seafloor datasets with visual domains drastically different from ImageNet to see if location-regularisation provides robustness where zero-shot transfer fails

### Open Question 3
- **Question:** How can the computational constraints of high-performing ViT models be reconciled with the limited power and memory availability of autonomous underwater vehicles (AUVs)?
- **Basis in paper:** The authors conclude that "ViT models are significantly larger... which requires greater computational resources... These constraints need to be considered when designing any real-time robotic applications"
- **Why unresolved:** The study establishes ViTs as top performers but identifies their resource intensity as practical barrier for intended robotic application without proposing efficiency solutions
- **What evidence would resolve it:** Evaluating model distillation techniques or lightweight ViT variants to determine if performance can be maintained within strict energy envelope of operational AUV

## Limitations

- **Hyperparameter Sensitivity:** The optimal location radius $r_{loc}$ is dataset-specific and significantly impacts performance, with no systematic tuning guidelines provided
- **Generalizability to Non-GeoReferenced Data:** The approach fundamentally requires precise location metadata, limiting applicability to legacy datasets or scenarios with imprecise spatial information
- **Comparison to Fully Supervised Learning:** The paper does not benchmark against fully supervised CNNs/ViTs trained on the same datasets, leaving open whether location-regularised SSL closes the gap with supervised methods

## Confidence

- **High Confidence:** The core finding that location-regularisation improves SSL performance over standard SSL is well-supported by consistent F1-score improvements across multiple datasets, architectures, and latent dimensions
- **Medium Confidence:** The architectural recommendations (low-dim CNNs benefit most from location-reg, high-dim ViTs already generalize well) are supported by presented experiments but could benefit from additional ablation studies
- **Low Confidence:** Claims about the mechanism by which location metadata helps (e.g., "compressing feature variability") are plausible but not directly validated through controlled experiments

## Next Checks

1. **Spatial Scale Sensitivity Analysis:** Systematically vary $r_{loc}$ on a subset of data to identify the optimal scale for your specific survey environment before full training

2. **Downstream Task Generalization:** Evaluate the learned representations on a different but related seafloor classification task (e.g., binary sand/rock classification) to test cross-dataset generalization

3. **Supervised Baseline Comparison:** Train a fully supervised ResNet18 and ViT on each dataset and compare their F1-scores against the best location-regularised SSL models to quantify the remaining performance gap