---
ver: rpa2
title: 'EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection'
arxiv_id: '2505.14289'
source_url: https://arxiv.org/abs/2505.14289
tags:
- injection
- agent
- agents
- prompt
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes EVA, a red-teaming framework for evolving indirect
  prompt injection attacks against GUI agents. Unlike static attacks, EVA iteratively
  refines adversarial prompts by monitoring agent attention and adjusting visual cues,
  keywords, and layout to maximize behavioral manipulation.
---

# EVA: Red-Teaming GUI Agents via Evolving Indirect Prompt Injection

## Quick Facts
- arXiv ID: 2505.14289
- Source URL: https://arxiv.org/abs/2505.14289
- Reference count: 40
- Key outcome: Iterative injection refinement exploits GUI agents' attention dynamics, achieving up to 80% attack success vs 48% for static methods

## Executive Summary
This paper introduces EVA, a black-box red-teaming framework that evolves indirect prompt injection attacks against GUI agents. Unlike static attacks, EVA iteratively refines adversarial prompts by monitoring agent attention and adjusting visual cues, keywords, and layout to maximize behavioral manipulation. Tested across six GUI agents and four realistic scenarios, EVA significantly outperforms static baselines and demonstrates strong cross-model transferability, revealing shared vulnerabilities in multimodal agent decision-making.

## Method Summary
EVA is a closed-loop optimization framework that evolves indirect prompt injections against GUI agents without access to their internal weights. It maintains a keyword lexicon with utility scores, generates HTML injections by sampling keywords weighted by their scores, and observes agent responses to classify attacks as success, failure, or invalid. The utility scores are updated based on feedback, reinforcing successful keywords and pruning ineffective ones. The process iterates with periodic introduction of new keywords to avoid local optima. EVA operates in a goal-agnostic setting and evaluates attacks across four scenarios: pop-up injection, chat-based link injection, chat-based payment fraud, and email confirmation injection.

## Key Results
- EVA achieves up to 80% attack success rate, significantly outperforming static baselines (48%)
- Strong cross-model transferability demonstrated: injections evolved on one agent often succeed on others
- Goal-agnostic attacks are less effective but still successful, showing EVA's robustness without task-specific knowledge

## Why This Works (Mechanism)
EVA works by exploiting the attention dynamics of GUI agents through iterative refinement of visual prompts. By continuously monitoring which injections cause agents to click malicious elements and reinforcing the associated keywords and visual layouts, EVA learns to create prompts that maximally capture the agent's attention and manipulate its behavior. The evolving nature allows it to adapt to the specific attention patterns and decision-making processes of different agents, revealing shared vulnerabilities across models.

## Foundational Learning
- **Indirect Prompt Injection**: Malicious instructions embedded in the agent's visual environment rather than the user's text prompt. *Why needed*: This is the core attack vector EVA optimizes. *Quick check*: If a malicious instruction is hidden in an email that a GUI agent reads, is this a direct or indirect prompt injection?
- **Black-Box Optimization**: EVA operates without access to target agent's internal weights or gradients. *Why needed*: The framework must rely solely on observing external actions to guide optimization. *Quick check*: Can EVA directly read the attention maps from the target model? How does it get feedback?
- **Visual Attention in Multimodal Models**: Models assign importance to different regions of an image. *Why needed*: The paper's central claim is that evolving injections exploit the agent's attention dynamics. *Quick check*: Why would a pop-up at the center of the screen be a more effective injection point than a small text link in the corner?

## Architecture Onboarding
- **Component map**: Keyword Lexicon -> Injection Constructor -> Feedback Monitor -> Updater
- **Critical path**:
  1. Initialize Lexicon with seed trigger words and LLM-generated distractors
  2. Generate & Inject: Construct injection and place in GUI environment
  3. Observe: Target agent interacts; Feedback Monitor records action
  4. Evaluate & Update: Boost successful keywords' utility scores if attack succeeded
  5. Iterate: Repeat, with lexicon progressively becoming more effective
- **Design tradeoffs**:
  - Exploration vs. Exploitation: Balance reinforcing known-good keywords with introducing new ones
  - Goal-Agnostic vs. Goal-Aware: EVA can operate without knowing agent's task (more realistic but less effective)
  - Black-Box Efficiency: Iterative approach is slow and resource-intensive compared to single-shot attacks
- **Failure signatures**:
  - Stagnant ASR: Performance plateaus, suggesting local optimum
  - High Invalid Rate: High percentage of ambiguous agent responses
  - No Cross-Model Transfer: Injection fails completely on different models (overfitting)
- **First 3 experiments**:
  1. Baseline Comparison: Run EVA vs static baseline on pop-up injection for one model, measure ASR improvement
  2. Transferability Test: Evolve injection on source model, test on different target model, measure ASR drop
  3. Strategy Ablation: Disable feedback loop's ability to boost successful keywords, compare resulting ASR

## Open Questions the Paper Calls Out
- How can "attention-aware defenses" be practically implemented to mitigate evolving injections without disrupting legitimate GUI interactions? (explicit)
- Do specific internal attention mechanisms or neuron activations in MLLMs correlate with susceptibility to EVA-evolved prompts? (explicit)
- Can evolving injection strategies bypass explicit safety alignment in high-risk scenarios (e.g., payments) where current methods fail? (inferred)

## Limitations
- Evaluations conducted in controlled, simulated HTML environments, not real-world GUI complexity
- High invalid rates for some agents suggest safety filters may limit attack effectiveness
- Exact keyword lexicon size and composition not disclosed, affecting reproducibility
- Cross-model transferability results lack detailed success rate breakdowns

## Confidence
- **High**: Core mechanism (iterative keyword evolution) is clearly described and ASR improvement over static baselines is well-supported
- **Medium**: Claim about exploiting "attention dynamics" is plausible but lacks direct empirical evidence linking attention maps to success
- **Low**: Practical impact in real-world scenarios is uncertain due to controlled evaluation setup and potential safety filter interference

## Next Checks
1. Implement EVA against GUI agent on actual websites via browser automation to measure ASR degradation from interface complexity
2. Run EVA against GPT-4o with safety filters disabled (if possible) and compare payment/email scenario ASR to isolate alignment constraint effects
3. Systematically vary initial keyword lexicon size and composition (10, 50, 100 keywords; curated-only vs curated+random) to measure impact on final ASR and convergence speed