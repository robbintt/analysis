---
ver: rpa2
title: 'SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal
  Bias'
arxiv_id: '2511.13005'
source_url: https://arxiv.org/abs/2511.13005
tags:
- class
- spurious
- prompt
- photo
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of multimodal spurious bias
  in vision-language models like CLIP, where models rely on spurious correlations
  (e.g., background) instead of core features for classification. The authors propose
  SAGE, a training-free method that mitigates this bias through guided prompt selection.
---

# SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias

## Quick Facts
- arXiv ID: 2511.13005
- Source URL: https://arxiv.org/abs/2511.13005
- Authors: Wenqian Ye; Di Wang; Guangtao Zheng; Bohan Liu; Aidong Zhang
- Reference count: 14
- One-line primary result: A training-free prompt selection method that improves worst-group robustness in zero-shot CLIP models by maximizing class separation

## Executive Summary
This paper addresses the challenge of multimodal spurious bias in vision-language models like CLIP, where models rely on spurious correlations (e.g., background) instead of core features for classification. The authors propose SAGE, a training-free method that mitigates this bias through guided prompt selection. SAGE evaluates a diverse set of prompt templates and selects those inducing the largest semantic separation between classes, improving worst-group robustness without external annotations or fine-tuning. Extensive experiments on four benchmark datasets (Waterbirds, CelebA, PACS, VLCS) and five backbone models show that SAGE consistently improves zero-shot accuracy and generalization, achieving the best worst-group accuracy (WGA) and harmonic mean (HM) across models.

## Method Summary
SAGE is a training-free method that improves zero-shot robustness in CLIP models by selecting prompts that maximize semantic separation between classes. For each test image, the method computes separation scores across 80 predefined prompt templates by measuring the difference between maximum and minimum cosine similarities to class text embeddings. The top-K prompts (K=1 default) with highest separation scores are selected for ensemble prediction. This approach shifts text embeddings away from spurious correlations learned during pretraining, allowing the model to focus on object-relevant features rather than contextual features.

## Key Results
- SAGE achieves worst-group accuracy of 56.9% and harmonic mean of 56.9% on Waterbirds, outperforming state-of-the-art zero-shot debiasing methods
- Consistently improves worst-group robustness across five different backbone models including CLIP-ViT-B/32 and CLIP-RN-50
- Outperforms existing methods like BiPrompt and PRISM on multiple benchmarks while maintaining competitive average accuracy
- Shows broad effectiveness across different bias types (background, style) and model architectures

## Why This Works (Mechanism)

### Mechanism 1
Higher separation scores between classes indicate prompts that rely less on spurious correlations. The separation score σ = max(cosine similarity) − min(cosine similarity) measures class discrimination in the joint embedding space. When spurious bias is strong, class probabilities converge yielding low separation; when bias is weak, the margin increases. Maximizing inter-class separation serves as a practical proxy for minimizing spurious feature influence without explicit knowledge of spurious attributes.

### Mechanism 2
Per-image prompt selection adapts to specific visual contexts better than global selection. Each test image receives independent ranking of all M templates; top-K are selected for that image's inference. This allows adaptation to which prompts best separate classes given the particular visual features present. Different images benefit from different prompt phrasings; a one-size-fits-all prompt is suboptimal for spurious bias mitigation.

### Mechanism 3
Semantically diverse prompts shift text embeddings away from spurious correlations learned during pretraining. Prompts with neutral or unusual descriptors produce text embeddings that may not align with the spurious image-text associations learned during CLIP pretraining. Some prompt phrasings are semantically neutral enough to avoid triggering learned spurious associations.

## Foundational Learning

- Concept: Zero-shot classification in CLIP
  - Why needed here: SAGE operates entirely within the zero-shot inference pathway; understanding how CLIP classifies via text-image similarity is essential.
  - Quick check question: Given image embedding v and class text embeddings {u_i}, what is CLIP's prediction rule?

- Concept: Spurious correlations in vision models
  - Why needed here: The paper's premise is that models learn shortcuts (e.g., background → label). This foundation motivates the entire debiasing approach.
  - Quick check question: On Waterbirds, why might a model correctly classify landbirds on land but fail on waterbirds on land?

- Concept: Worst-group accuracy (WGA) as robustness metric
  - Why needed here: SAGE optimizes for WGA, not average accuracy; minority-group performance reveals reliance on spurious features.
  - Quick check question: If a model achieves 90% average accuracy but 20% on a minority group, what does this indicate about its learned features?

## Architecture Onboarding

- Component map: Input image → Vision encoder → Compute M×C similarities → Calculate M separation scores → Select top-K prompts → Ensemble prediction
- Critical path: Input image → vision encoder → compute M×C similarities → calculate M separation scores → select top-K prompts → ensemble prediction
- Design tradeoffs:
  - K=1 vs K>1: K=1 is fastest and optimal for most models, but K=5 helps CLIP-RN-50 on Waterbirds
  - Prompt pool size: 80 prompts used; larger pools increase computation but may improve diversity
  - Per-image selection: Adds O(M×C) similarity computations per inference
- Failure signatures:
  - Low or negative PCC between separation scores and WGA → proxy assumption violated
  - Random selection outperforms SAGE → scoring mechanism unreliable
  - High variance in selected prompts across similar images → noisy scoring
- First 3 experiments:
  1. Replicate Table 2 (Ensemble vs Random vs SAGE) on Waterbirds with CLIP-ViT-B/32; verify WGA improvement and compute PCC between separation scores and WGA.
  2. Run K-ablation (Figure 4) on your target dataset and backbone; confirm K=1 optimal or identify model-specific sweet spot.
  3. Test prompt pool sensitivity: compare full 80-template pool vs reduced 20-template subset; verify robustness to pool composition.

## Open Questions the Paper Calls Out

### Open Question 1
How can SAGE be effectively integrated with small labeled datasets or few-shot learning techniques to further refine performance? The authors state in the Limitations section that "future work could explore integrating SAGE with small labeled datasets to further refine and improve model performance." The current method is strictly zero-shot and training-free by design.

### Open Question 2
To what extent does the performance of SAGE depend on the specific diversity and semantic quality of the predefined set of 80 prompt templates? The Limitations section notes, "the performance of SAGE is contingent on the diversity and quality of the predefined prompt templates." It is unclear if the current set of 80 templates is sufficient for all domains or if specific domains require curated template sets.

### Open Question 3
Can the separation score heuristic effectively mitigate bias in more complex vision-language tasks, such as open-vocabulary detection or segmentation? The authors mention that "extending its evaluation to a broader range of tasks and bias types would provide deeper insights into its generalizability." The current evaluation is restricted to classification tasks.

## Limitations
- Method assumes semantic separation is a reliable proxy for spurious bias mitigation without empirical validation across diverse prompt sets
- Performance depends on the diversity and semantic quality of the predefined prompt templates, which may not generalize to novel domains
- Computational overhead of per-image prompt selection could be significant at scale, though not thoroughly investigated

## Confidence

- **High confidence**: SAGE improves worst-group accuracy on standard benchmarks compared to baseline zero-shot CLIP, as the metrics are directly reported and reproducible
- **Medium confidence**: The claim that SAGE is training-free and broadly applicable across model sizes and architectures, given the ablation results but limited cross-domain testing
- **Low confidence**: The assertion that semantic separation is a universal proxy for spurious bias mitigation, due to lack of systematic correlation analysis between separation scores and WGA across diverse prompt sets

## Next Checks

1. Compute the Pearson correlation coefficient between separation scores and worst-group accuracy across all test images for each dataset; verify positive correlation as a sanity check for the proxy assumption
2. Test whether a reduced prompt pool (e.g., 20 templates) maintains similar debiasing performance to the full 80-template set, assessing sensitivity to prompt pool composition
3. Implement a global prompt selection baseline (same top-K prompts for all images) and compare against SAGE's per-image selection to determine if adaptation overhead is justified