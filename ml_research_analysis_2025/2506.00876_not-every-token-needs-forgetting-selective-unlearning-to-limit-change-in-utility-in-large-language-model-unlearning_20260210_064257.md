---
ver: rpa2
title: 'Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in
  Utility in Large Language Model Unlearning'
arxiv_id: '2506.00876'
source_url: https://arxiv.org/abs/2506.00876
tags:
- unlearning
- forget
- data
- retain
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Selective Unlearning (SU) targets the problem that conventional
  LLM unlearning removes both sensitive and common knowledge indiscriminately, degrading
  general utility. SU introduces two assistant models trained on different data splits
  and uses their prediction score differences to identify and unlearn only tokens
  containing forget-set-specific information.
---

# Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning

## Quick Facts
- arXiv ID: 2506.00876
- Source URL: https://arxiv.org/abs/2506.00876
- Reference count: 9
- Selective Unlearning (SU) improves TOFU retain utility to 0.62 vs. 0.50 for best baseline

## Executive Summary
Selective Unlearning (SU) addresses the fundamental problem that conventional LLM unlearning removes both sensitive and common knowledge indiscriminately, degrading general utility. SU introduces two assistant models trained on different data splits and uses their prediction score differences to identify and unlearn only tokens containing forget-set-specific information. Experiments on TOFU and MUSE-News benchmarks with six baseline methods show SU achieves comparable forget quality to baselines while significantly improving utility preservation on retain data.

## Method Summary
SU trains two assistant models on different data splits (e.g., full data vs. retain-only data), then computes prediction score differences for each token. Tokens with score differences exceeding a threshold are flagged as containing forget-set-specific information and selected for unlearning. The method applies gradient ascent loss only on 5-gram windows around selected tokens while simultaneously applying gradient descent on retain data. This selective approach preserves common tokens that appear in both forget and retain documents, maintaining utility on the retain set while achieving effective forgetting.

## Key Results
- SU (N-Gram) achieves TOFU retain utility of 0.62 vs. 0.50 for best baseline method
- SU improves MUSE retain knowledge memorization to 0.26 vs. 0.21 for best baseline
- SU achieves comparable forget quality to baselines while preserving utility

## Why This Works (Mechanism)

### Mechanism 1: Divergence-Based Token Specificity Detection
Two assistant models trained on different data splits identify forget-set-specific tokens through prediction divergence. Tokens encoding forget-specific information produce different prediction probabilities across the two models, while common tokens yield similar predictions. This allows selective targeting of sensitive information while preserving general knowledge.

### Mechanism 2: Localized Gradient Masking with Context Expansion
SU restricts unlearning loss to 5-gram windows around selected tokens by masking non-selected tokens (`label[idx] = -100`). This localizes parameter updates to minimal necessary regions while ensuring complete phrase removal through context expansion.

### Mechanism 3: Dual-Objective Optimization Balance
Combining gradient ascent on selected forget tokens with gradient descent on retain data creates opposing gradient pressures that localize forgetting effects. This dual optimization preserves utility on retain data while achieving effective forgetting.

## Foundational Learning

### Concept 1: Gradient Ascent for Unlearning
- Why needed here: The core operation that "forgets" by maximizing loss on target tokens—inverse of standard training.
- Quick check question: Why does applying gradient ascent to ALL tokens in a document cause model collapse, but applying it selectively preserves utility?

### Concept 2: N-Gram Language Models
- Why needed here: The paper uses N-gram models as efficient assistant models (20MB memory). Understanding their statistical token prediction helps explain why they work for selection.
- Quick check question: How does a 5-gram model compute token probability differently from an LLM, and why might this simpler model suffice for detecting forget-specific tokens?

### Concept 3: Label Masking in Cross-Entropy Loss
- Why needed here: The mechanism for excluding tokens from loss computation (`label = -100` in PyTorch/HuggingFace conventions).
- Quick check question: What happens to gradient computation for a position where the label is set to -100?

## Architecture Onboarding

### Component Map:
Main Model (θ) ←── Target for unlearning
     │
     ├── Assistant Model 1 (full data trained)
     │       └── Outputs: p₁θ(tᵢ|t<ᵢ)
     │
     ├── Assistant Model 2 (retain data trained)
     │       └── Outputs: p₂θ(tᵢ|t<ᵢ)
     │
     ├── Selection Module
     │       └── |p₁ - p₂| > γ → select token
     │       └── Expand selection to ±2 tokens (5-gram)
     │       └── Generate mask: label[non-selected] = -100
     │
     └── Unlearning Loop
             ├── L_SU = -Σ log(pθ(selected tokens))  [Gradient Ascent]
             └── L_retain = Σ log(pθ(retain tokens))  [Gradient Descent]

### Critical Path:
1. **Offline:** Train assistant models on respective data splits (N-gram: ~20MB; LLM: full model copy)
2. **Per forget document:**
   - Forward pass through both assistants → collect per-token probabilities
   - Compute differences, apply threshold, generate selection mask
   - Expand mask to 5-gram windows around each selected position
   - Forward/backward through main model with masked loss + retain loss
3. **Inference:** Assistants not needed; no additional cost

### Design Tradeoffs:
| Decision | Option A | Option B | Guidance |
|----------|----------|----------|----------|
| Assistant type | N-gram (20MB, fast) | LLM (7B params, semantic) | Paper shows N-gram outperforms on retain utility; start with N-gram |
| Threshold γ | Lower (0.5) | Higher (0.8-0.9) | Higher = fewer tokens selected = better utility but risk incomplete forgetting |
| Assistant splits | Full vs Retain | Full vs Forget | Paper: Full/Retain with γ=0.8 worked best on MUSE; Full/Forget with γ=0.9 on TOFU |

### Failure Signatures:
- **Model outputs empty/gibberish:** Learning rate too high or too many tokens selected → reduce LR, increase threshold
- **Forget info still extractable:** Threshold too high → decrease γ, verify assistant training splits
- **Retain knowledge degraded:** Retain data insufficient or gradient imbalance → increase GD weight, check retain set coverage
- **Selection produces no tokens:** Assistant models have same predictions → verify they're trained on different splits

### First 3 Experiments:
1. **Baseline establishment:** Run GA and NPO+GD on your forget/retain split; record forget quality (ROUGE on forget) and retain utility (Truth Ratio or KnowMem on retain)
2. **Assistant validation:** Train N-gram on full vs retain data; inspect prediction differences on known forget-specific tokens (e.g., entity names) vs. common words—verify divergence pattern
3. **Threshold sweep:** Run SU with γ ∈ {0.5, 0.7, 0.8, 0.9}; plot forget score vs. retain utility to identify Pareto frontier; select based on deployment requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Selective Unlearning (SU) maintain its utility preservation advantages when scaled to LLMs significantly larger than the 7B parameter models tested?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section that due to "cost and resource constraints," they were unable to "extend our experiments to larger scales and bigger LLMs."
- Why unresolved: It is unknown if the token selection mechanism scales efficiently or if the gradient dynamics change substantially in models with tens or hundreds of billions of parameters.
- Evidence: Empirical evaluation of SU on models of size 70B or larger, showing comparable or improved retain utility metrics relative to baselines.

### Open Question 2
- Question: Can the need for two separate assistant models be eliminated or streamlined to reduce the training-time computational overhead?
- Basis in paper: [explicit] The paper notes that the design "naturally infers additional cost at training time" and explicitly encourages future work on "more efficient methods for building selection strategies."
- Why unresolved: While the method saves utility, the requirement to train two auxiliary models (e.g., an N-Gram model on full data and another on retain data) adds complexity to the unlearning pipeline.
- Evidence: A variation of the framework that achieves competitive token selection using a single reference model or pre-existing model states without training new assistants.

### Open Question 3
- Question: Is there a theoretical or automated method to determine the score difference threshold ($\gamma$) without relying on manual hyper-parameter searching?
- Basis in paper: [inferred] The paper states that "final optimal thresholds... are chosen through hyper-parameter searching" (Appendix B.2), and the ablation study shows significant performance variance based on this choice.
- Why unresolved: A reliance on grid search implies that the method might require tuning for every new dataset or model instance, potentially limiting its "plug-and-play" applicability.
- Evidence: An algorithm that adaptively sets the threshold based on the distribution of score differences, eliminating the need for manual searching.

## Limitations
- Limited to text-based benchmarks (TOFU, MUSE-News); effectiveness for other modalities (images, audio) or data types (code, medical records) unexplored
- Requires training two assistant models, adding computational overhead to the unlearning pipeline
- Threshold selection relies on manual hyperparameter searching without theoretical guidance

## Confidence

**High Confidence (9/10)**: The selective unlearning mechanism's ability to preserve utility on retain data while achieving comparable forget quality to baselines. Supported by direct experimental comparisons across two benchmarks with six baselines.

**Medium Confidence (6/10)**: The robustness of the divergence-based selection mechanism across diverse document types and domains. Demonstrates effectiveness on two benchmarks but lacks exploration of edge cases and non-textual data.

**Low Confidence (4/10)**: The claim that the 5-gram window expansion is optimal for all forgetting scenarios. Uses this heuristic without comparing alternative context sizes or adaptive windowing strategies.

## Next Checks

1. **Threshold Sensitivity Analysis**: Systematically sweep γ values on a held-out validation split of your specific dataset to identify the Pareto frontier between forget quality and retain utility. Plot forget score vs. retain utility curves to determine optimal thresholds for your use case.

2. **Selection Quality Inspection**: After training assistant models, manually inspect token-level prediction differences on sample forget documents. Verify that the selection mechanism correctly identifies forget-specific tokens (entity names, sensitive phrases) while excluding common words (articles, prepositions, frequent nouns).

3. **Domain Transfer Validation**: Apply SU to a dataset from a different domain than TOFU/MUSE (e.g., medical records, legal documents, code repositories) and evaluate whether the divergence-based selection remains effective or requires threshold adjustment and potential methodological modifications.