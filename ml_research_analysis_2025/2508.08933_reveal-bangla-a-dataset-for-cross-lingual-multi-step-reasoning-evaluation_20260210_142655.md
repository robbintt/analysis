---
ver: rpa2
title: 'Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation'
arxiv_id: '2508.08933'
source_url: https://arxiv.org/abs/2508.08933
tags:
- reasoning
- bangla
- steps
- language
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reveal-Bangla, a manually translated Bangla
  version of the English Reveal dataset for evaluating multi-step reasoning in language
  models. The dataset contains 104 questions with binary and non-binary types, along
  with reasoning steps and gold answers.
---

# Reveal-Bangla: A Dataset for Cross-Lingual Multi-Step Reasoning Evaluation

## Quick Facts
- **arXiv ID**: 2508.08933
- **Source URL**: https://arxiv.org/abs/2508.08933
- **Reference count**: 40
- **Key outcome**: Reveal-Bangla dataset shows CoT reasoning helps non-binary questions but models struggle with Bangla reasoning steps compared to English

## Executive Summary
This paper introduces Reveal-Bangla, a manually translated Bangla version of the English Reveal dataset for evaluating multi-step reasoning in language models. The dataset contains 104 questions with binary and non-binary types, along with reasoning steps and gold answers. Controlled experiments with small language models (Llama-3.2-1B and BanglaLlama-3.2-1B) show that reasoning context improves accuracy for non-binary questions but models struggle to effectively use Bangla reasoning steps compared to English. The authors also use attribution analysis to show that later reasoning steps have higher influence on model predictions across both languages.

## Method Summary
The paper creates Reveal-Bangla by manually translating 104 questions from the English Reveal dataset, along with associated evidence paragraphs and reasoning steps. Two small language models are evaluated: EngLlama (Llama-3.2-1B-Instruct) and BenLlama (BanglaLlama fine-tuned variant). The evaluation uses two settings: `gen_ans` (answer without reasoning) and `w_cot_gen_ans` (answer with reasoning steps provided). Answers are verified using an NLI model (mDeBERTa-v3) with manual fallback for neutral verdicts. Attribution analysis via ContextCite identifies which reasoning steps most influence model predictions.

## Key Results
- CoT reasoning steps improve accuracy for non-binary questions (+19.2% in English, +3.9% in Bangla) but have uneven effects on binary questions
- Models struggle to employ Bangla reasoning steps effectively, with EngLlama accuracy decreasing when given Bangla CoT (35.6% → 33.7%)
- Later reasoning steps have higher influence on model predictions than earlier steps across both languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-thought (CoT) reasoning steps improve accuracy for complex (non-binary) questions, but provide uneven benefits for simpler (binary) questions.
- Mechanism: Non-binary questions require multi-step logical deduction across evidence; providing explicit reasoning steps decomposes the problem into manageable sub-steps. Binary yes/no questions often need less intermediate reasoning, so CoT adds limited value.
- Core assumption: The reasoning steps provided are logically coherent and attributable to evidence (the paper filters for this).
- Evidence anchors:
  - [abstract] "reasoning context is beneficial for more challenging non-binary questions"
  - [section 4, Results] "while CoT steps have an uneven effect on binary questions, they are consistently beneficial for non-binary ones, across both models and languages"
  - [corpus] MathMist (arXiv:2510.14305) similarly finds mathematical reasoning requires structured logical deduction beyond linguistic understanding.

### Mechanism 2
- Claim: Models struggle to exploit reasoning steps in lower-resource languages (Bangla) compared to higher-resource languages (English), even when the reasoning content is equivalent.
- Mechanism: SLMs have weaker language understanding in lower-resource languages due to limited pretraining data; this bottleneck prevents effective utilization of reasoning context. The paper shows EngLlama's accuracy decreased with Bangla CoT (35.6% → 33.7%), suggesting interference rather than help.
- Core assumption: Translation quality is high (manual translation by native speaker), so performance gaps stem from model language capability, not data quality.
- Evidence anchors:
  - [abstract] "models struggle to employ relevant Bangla reasoning steps effectively"
  - [section 4, Results] "CoT gains for the BenLlama model in Bangla are much milder than for the EngLlama model in English (+3.9% vs. +19.2%)"
  - [corpus] Swahili benchmark paper (arXiv:2509.04516) similarly examines performance disparities between English-trained and natively-trained datasets for lower-resource languages.

### Mechanism 3
- Claim: Later reasoning steps have higher influence on model predictions than earlier steps, suggesting limited context comprehension.
- Mechanism: Attribution via ContextCite uses surrogate linear models to assign importance scores to each step. Higher importance on final steps indicates models rely on answer-adjacent information rather than building understanding from earlier context.
- Core assumption: Assumption: Importance scores from surrogate models faithfully represent model-internal attention to reasoning steps.
- Evidence anchors:
  - [abstract] "later reasoning steps have higher influence on model predictions across both languages"
  - [section 4, Attributing Answers] "in most cases, later steps tend to have a larger influence on the model response... models place higher emphasis on answer-specific information located in later steps"
  - [corpus] No direct corpus evidence on step-level attribution patterns; this is a relatively underexplored area.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) Prompting
  - Why needed here: The entire evaluation framework depends on providing annotated reasoning steps to models and measuring whether they improve answer accuracy.
  - Quick check question: Can you explain why providing intermediate reasoning steps might help with multi-hop questions but not simple lookup questions?

- Concept: Cross-Lingual Transfer in Language Models
  - Why needed here: Understanding why English-centric models struggle with Bangla reasoning despite identical semantic content is central to interpreting the results.
  - Quick check question: What factors determine how well a model trained primarily on English can process reasoning in Bangla?

- Concept: Attribution Methods for Model Interpretability
  - Why needed here: ContextCite attribution is used to analyze which reasoning steps influence predictions, providing mechanistic insight beyond accuracy metrics.
  - Quick check question: How does a surrogate linear model (like ContextCite/LIME) attribute importance to input features, and what are its limitations?

## Architecture Onboarding

- Component map:
  - Dataset: Reveal-Bangla (104 questions, ~355 reasoning steps, 188 evidence paragraphs) with parallel English/Bangla versions
  - Models: EngLlama (Llama-3.2-1B-Instruct) and BenLlama (BanglaLlama fine-tuned variant)
  - Evaluation settings: `gen_ans` (no reasoning provided) vs. `w_cot_gen_ans` (reasoning steps provided)
  - Verification: mDeBERTa-v3 NLI model for answer validation (with manual fallback for neutral verdicts)

- Critical path:
  1. Load question + evidence in target language
  2. (For w_cot_gen_ans) Append annotated reasoning steps to assistant message
  3. Generate answer with greedy decoding (256 token limit)
  4. Verify via NLI entailment/contradiction against gold answer

- Design tradeoffs:
  - Small scale (104 questions) enables controlled analysis but limits statistical power and diversity
  - Manual translation ensures quality but doesn't scale; Google Translate showed errors on sports terminology
  - Binary question skew (70%) may overestimate performance on open-ended reasoning
  - 1B parameter models are computationally accessible but may not reflect larger model behavior

- Failure signatures:
  - EngLlama accuracy decreases with Bangla CoT (35.6% → 33.7%) → language mismatch interference
  - NLI model produces "neutral" verdicts → requires manual verification
  - Models generate answers in wrong script → manual filtering needed (langdetect doesn't support Bangla)

- First 3 experiments:
  1. Replicate the gen_ans vs. w_cot_gen_ans comparison on your own SLM to validate the CoT benefit pattern for non-binary questions.
  2. Test a larger model (e.g., 7B+ parameters) on Reveal-Bangla to assess whether the later-step attribution pattern persists or shifts.
  3. Translate a small subset to another low-resource language and compare CoT gains to establish whether the Bangla findings generalize.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the pattern of later reasoning steps having higher influence on model predictions hold for larger language models (>1B parameters), or does step importance distribution change with model scale?
- Basis in paper: [explicit] "Future research could investigate whether this trend holds with larger model sizes."
- Why unresolved: Only 1B parameter SLMs were evaluated due to computational constraints; scaling behavior for attribution patterns is unknown.
- What evidence would resolve it: Attribution analysis using ContextCite on 7B, 13B, and 70B models on the same Reveal-Bangla dataset.

### Open Question 2
- Question: What language-specific approaches can enhance reasoning capabilities in low-resource languages beyond direct transfer of English-optimized techniques?
- Basis in paper: [explicit] "These findings underscore the need for developing language-specific approaches to enhance reasoning capabilities in low-resource languages, rather than directly transferring techniques optimized for English."
- Why unresolved: The paper demonstrates that direct transfer underperforms (CoT gains in Bangla are +3.9% vs. +19.2% in English) but does not propose alternatives.
- What evidence would resolve it: Comparative evaluation of language-specific prompting strategies, training approaches, or architecture modifications designed for morphologically rich languages.

### Open Question 3
- Question: Do the observed cross-lingual reasoning patterns generalize to other low-resource languages with different linguistic properties and writing systems beyond Bangla?
- Basis in paper: [explicit] "Our findings are specific to the English-Bangla language pair and may not generalize to other low-resource languages with different linguistic properties, writing systems, or relationships to English."
- Why unresolved: Only English-Bangla was tested; morphological richness and script differences may create unique challenges for Bangla.
- What evidence would resolve it: Comparable evaluations on manually translated Reveal subsets for languages like Tamil, Swahili, or Vietnamese.

### Open Question 4
- Question: Can capable multilingual LLMs generate faithful and coherent reasoning chains in Bangla, and how does their quality compare to gold human-translated reasoning steps?
- Basis in paper: [inferred] The gen_cot_ans setting was excluded "due to the poor performance of SLMs on CoT reasoning," leaving model-generated Bangla reasoning unexplored.
- Why unresolved: SLMs failed to generate valid CoT, but larger models may succeed; faithfulness of generated Bangla reasoning remains untested.
- What evidence would resolve it: Human evaluation of GPT-4 or Llama-70B generated reasoning chains in Bangla for logical coherence and attribution.

## Limitations
- Small scale (104 questions) limits statistical power and generalizability
- Binary question skew (70% yes/no) may overestimate performance on open-ended reasoning
- Manual translation ensures quality but prevents scaling; automated approaches show errors on specialized terminology
- 1B parameter models may not reflect behavior of larger language models

## Confidence
- **High confidence**: CoT reasoning steps improve accuracy for non-binary questions; models struggle with Bangla reasoning compared to English; later reasoning steps have higher influence on predictions
- **Medium confidence**: The specific magnitude of CoT benefits and attribution patterns hold across larger models or different datasets
- **Low confidence**: Handling of neutral NLI verdicts and impact of translation quality variations

## Next Checks
1. Replicate the gen_ans vs. w_cot_gen_ans comparison on your own SLM to validate the CoT benefit pattern for non-binary questions, particularly examining whether the binary question interference effect persists.
2. Test a larger model (e.g., 7B+ parameters) on Reveal-Bangla to assess whether the later-step attribution pattern persists or shifts toward more balanced step importance distribution.
3. Translate a small subset to another low-resource language (e.g., Swahili or Nepali) and compare CoT gains to establish whether the Bangla findings generalize beyond a single language pair.