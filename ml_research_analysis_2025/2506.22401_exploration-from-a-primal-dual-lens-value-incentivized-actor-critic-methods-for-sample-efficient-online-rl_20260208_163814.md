---
ver: rpa2
title: 'Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods
  for Sample-Efficient Online RL'
arxiv_id: '2506.22401'
source_url: https://arxiv.org/abs/2506.22401
tags:
- have
- where
- policy
- lemma
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new actor-critic algorithm called Value-Incentivized
  Actor-Critic (VAC) for sample-efficient online reinforcement learning with function
  approximation. The method is inspired by a primal-dual interpretation of the Maximize
  to Explore (MEX) framework, where optimistic regularization emerges naturally from
  a regularized Lagrangian formulation.
---

# Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL

## Quick Facts
- arXiv ID: 2506.22401
- Source URL: https://arxiv.org/abs/2506.22401
- Reference count: 40
- This paper introduces a new actor-critic algorithm called Value-Incentivized Actor-Critic (VAC) for sample-efficient online reinforcement learning with function approximation.

## Executive Summary
This paper introduces Value-Incentivized Actor-Critic (VAC), a new actor-critic algorithm for sample-efficient online reinforcement learning with function approximation. VAC is inspired by a primal-dual interpretation of the Maximize to Explore (MEX) framework, where optimistic regularization emerges naturally from a regularized Lagrangian formulation. Unlike MEX, VAC optimizes a single non-bilevel objective that jointly optimizes both the Q-function and policy using Bellman consistency as a constraint rather than Bellman optimality.

The authors prove that VAC achieves near-optimal regret bounds of $\tilde{O}(dH^2\sqrt{T})$ for episodic linear MDPs and $\tilde{O}(d\sqrt{T}/(1-\gamma)^2)$ for infinite-horizon discounted MDPs. These results match the minimax lower bounds up to factors of $\sqrt{H}$ and $\sqrt{1-\gamma}$ respectively. The theoretical analysis extends to general function approximation settings under appropriate assumptions.

## Method Summary
VAC is an actor-critic algorithm that optimizes a single non-bilevel objective jointly over both the Q-function (critic) and policy (actor). The method uses Bellman consistency as a constraint rather than Bellman optimality, enabling differentiable optimization of both components simultaneously. The algorithm is derived from a primal-dual interpretation of the Maximize to Explore framework, where optimistic regularization emerges naturally from a regularized Lagrangian formulation. Under Linear MDP assumptions, VAC achieves near-optimal regret bounds matching theoretical lower bounds.

## Key Results
- Achieves $\tilde{O}(dH^2\sqrt{T})$ regret for episodic linear MDPs
- Achieves $\tilde{O}(d\sqrt{T}/(1-\gamma)^2)$ regret for infinite-horizon discounted MDPs
- Near-optimal regret bounds matching minimax lower bounds up to $\sqrt{H}$ and $\sqrt{1-\gamma}$ factors
- Theoretical guarantees extend to general function approximation under appropriate assumptions

## Why This Works (Mechanism)

### Mechanism 1: Incentivizing Exploration via Optimistic Regularization
- **Claim:** VAC promotes exploration by naturally incentivizing higher value functions through a regularized Lagrangian formulation, avoiding explicit uncertainty sets.
- **Mechanism:** The algorithm reframes the RL problem as a constrained optimization: maximize the value function subject to the Bellman consistency equation. By introducing dual variables and a regularizer, the dual variables are reparameterized to form a penalty term that biases the Q-function estimate towards higher values (optimism) while fitting the observed data.
- **Core assumption:** The primal-dual reparameterization accurately captures the trade-off between maximizing expected return and satisfying temporal consistency.
- **Evidence anchors:**
  - [abstract]** "optimistic regularization emerges naturally from a regularized Lagrangian formulation."
  - [section 3.1]** Derivation from Eq (13) to Eq (15) shows how the dual variable reparameterization yields the "value-incentivized" objective.
  - [corpus]** Weak direct support; neighbors like "Soft Optimistic Actor Critic" suggest optimism is a valid strategy, but do not detail this specific Lagrangian derivation.
- **Break condition:** If the regularization coefficient $\alpha$ is set too low, the "incentive" term vanishes, and the method collapses to a standard (potentially under-exploring) Actor-Critic.

### Mechanism 2: Tractability via Bellman Consistency Constraints
- **Claim:** Replacing the Bellman optimality constraint (used in prior MEX work) with a Bellman consistency constraint allows for a single, differentiable non-bilevel objective.
- **Mechanism:** Bellman optimality requires a $\max$ operator over actions (a bilevel optimization). By constraining the Q-function to satisfy consistency with a specific policy $\pi$ (Eq 13), the objective becomes jointly differentiable w.r.t. both the Q-function (critic) and the policy (actor).
- **Core assumption:** The policy class is expressive enough that optimizing within it does not severely limit the optimal value function compared to the unrestricted optimal policy.
- **Evidence anchors:**
  - [abstract]** "VAC optimizes a single non-bilevel objective... using Bellman consistency as a constraint rather than Bellman optimality."
  - [section 1.1]** "This formulation preserves the crux of optimistic regularization, while allowing differentiable optimization..."
- **Break condition:** If the policy class is too restrictive, the "model error" (Lemma 7) introduced by using a specific policy class rather than the optimal one may dominate the regret.

### Mechanism 3: Linear MDP Structure for Sample Efficiency
- **Claim:** The method achieves provable sample efficiency by assuming a Linear MDP structure, ensuring that the optimal Q-function lies within the function approximation class.
- **Mechanism:** Under the Linear MDP assumption, the Bellman backup of any linear Q-function remains linear (Bellman completeness). This prevents the "deadly triad" issue where function approximation diverges, allowing the algorithm to bound the estimation error using the "Generalized Eluder Coefficient" (GEC).
- **Core assumption:** The environment dynamics and rewards are linear with respect to a known feature map $\phi$.
- **Evidence anchors:**
  - [section 3.2]** Theorem 1 explicitly assumes Linear MDP (Assumption 1) to prove the regret bound.
  - [appendix B.2.1]** Lemma 6 proves that Linear MDP implies Bellman completeness and realizability.
- **Break condition:** If the environment is non-linear and the features $\phi$ are misspecified, the theoretical guarantees $\tilde{O}(dH^2\sqrt{T})$ likely do not hold, and the "optimistic" value estimates may diverge.

## Foundational Learning

**Concept: Primal-Dual Optimization (Lagrangians)**
- **Why needed here:** VAC is derived by converting a constrained Bellman problem into an unconstrained objective using dual variables. Understanding how constraints turn into regularization terms is essential to grasp why the algorithm looks the way it does.
- **Quick check question:** Can you explain how a hard constraint $g(x) = 0$ becomes a penalty term $\lambda g(x)$ in a Lagrangian, and what role the dual variable $\lambda$ plays?

**Concept: Bellman Consistency vs. Optimality**
- **Why needed here:** The paper's main architectural shift is moving from optimality ($Q^* = T^* Q^*$) to consistency ($Q^\pi = T^\pi Q^\pi$). This distinguishes VAC from MEX and enables the actor-critic structure.
- **Quick check question:** What is the mathematical difference between applying the Bellman *optimality* operator and the Bellman *consistency* operator to a Q-function?

**Concept: Linear MDPs**
- **Why needed here:** The theoretical proofs rely on the property that linear features suffice to represent the transition dynamics. Without this, the complexity scales poorly or becomes unbounded.
- **Quick check question:** In a Linear MDP, how are the transition kernel $P(s'|s,a)$ and reward $r(s,a)$ represented in terms of the feature map $\phi(s,a)$?

## Architecture Onboarding

**Component map:**
Critic ($f$) -> Actor ($\pi$) -> Primal-Dual Objective (joint loss) -> Data Buffer (transitions) -> Bellman consistency loss

**Critical path:**
1. Initialize Critic $f$ and Actor $\pi$.
2. **Joint Update:** Solve the optimization problem (Eq 17) to update $f$ and $\pi$ simultaneously by maximizing the value objective minus the Bellman error penalty.
3. **Interaction:** Run the updated policy $\pi$ to collect new transition data.
4. **Repeat:** Iterate the joint update with the growing dataset.

**Design tradeoffs:**
- **Policy Complexity vs. Tractability:** Using a log-linear policy (Assumption 3) enables theoretical proofs but may limit expressiveness in complex environments compared to deep neural nets.
- **Regularization ($\alpha$):** Must be tuned to balance the "value incentive" (exploration) against the "consistency loss" (exploitation/accuracy).

**Failure signatures:**
- **Optimism Collapse:** If $\alpha$ is too small, the agent exploits too early without covering the state space.
- **Divergence:** If $\alpha$ is too large, the Q-function grows unbounded ("optimism explosion") without grounding in real transitions.
- **Approximation Error:** In non-linear settings, if the features $\phi$ are poor, the Bellman backup may leave the function class, causing systematic bias.

**First 3 experiments:**
1. **Linear MDP Validation:** Run VAC on a synthetic Linear MDP (e.g., linear transition/reward) to verify the $\sqrt{T}$ regret scaling against the theoretical bound in Theorem 1.
2. **Ablation on $\alpha$:** Sweep the regularization parameter $\alpha$ to visualize the transition from under-exploration (low $\alpha$) to high-variance exploration (high $\alpha$) on a sparse reward task.
3. **Constraint Comparison:** Compare VAC (Bellman Consistency) against the baseline MEX (Bellman Optimality) to confirm that the removal of the bilevel optimization improves wall-clock convergence time without degrading sample efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does VAC perform empirically compared to existing exploration baselines in complex environments like those used in deep RL?
- **Basis in paper:** [explicit] The conclusion explicitly states: "Follow-up efforts will focus on empirical validation..."
- **Why unresolved:** The current work focuses exclusively on the theoretical derivation of the algorithm and proving regret bounds, providing no experimental results to demonstrate practical performance or numerical stability.
- **What evidence would resolve it:** Experimental benchmarks on standard control or Atari tasks comparing VAC against MEX, LSVI-UCB, or epsilon-greedy methods.

### Open Question 2
- **Question:** Can the VAC framework and its regret guarantees be extended to multi-agent reinforcement learning settings?
- **Basis in paper:** [explicit] The conclusion lists "...extending the algorithm design to multi-agent settings" as a specific avenue for future development.
- **Why unresolved:** The current analysis is restricted to single-agent MDPs, and the primal-dual formulation relies on a fixed environment, whereas multi-agent settings involve non-stationary dynamics induced by other agents.
- **What evidence would resolve it:** A formal extension of the VAC algorithm to Markov Games with corresponding theoretical guarantees or experimental validation in multi-agent scenarios.

### Open Question 3
- **Question:** Can the regret bounds be tightened to eliminate the $\sqrt{H}$ gap between the current VAC bound and the minimax lower bound?
- **Basis in paper:** [explicit] The discussion of Theorem 1 notes that the bound is "near-optimal up to a factor of $\sqrt{H}$" compared to the lower bound $\tilde{\Omega}(d\sqrt{H^3T})$.
- **Why unresolved:** The analysis or the specific regularization technique used in VAC may introduce horizon dependencies that prevent achieving the information-theoretic limit.
- **What evidence would resolve it:** A modified analysis of VAC or a slightly altered variant of the algorithm that achieves $\tilde{O}(dH^{1.5}\sqrt{T})$ regret.

### Open Question 4
- **Question:** Is the algorithm computationally tractable for general function approximation classes (e.g., deep neural networks) without relying on computationally expensive covering number arguments?
- **Basis in paper:** [inferred] The general function approximation analysis (Appendix B.3) relies on Assumption 7 regarding finite $\epsilon$-nets, and the optimization in (19) requires solving a saddle-point problem, which can be challenging for infinite-dimensional function classes.
- **Why unresolved:** While the theory extends to general function classes via covering numbers, practical implementation with neural networks typically requires guaranteed convergence of first-order methods, which is not proven here.
- **What evidence would resolve it:** A convergence analysis for non-linear function approximation showing that gradient-based optimization finds the requisite approximate optima efficiently.

## Limitations
- Theoretical analysis assumes Linear MDP structure, which may not hold in complex real-world environments where features are misspecified or non-linear.
- The algorithm's performance critically depends on the regularization hyperparameter α, which requires careful tuning to balance exploration vs. exploitation.
- The minimax inner loop for solving the discriminator g adds computational overhead and may be unstable in practice, particularly if the critic f and discriminator g have mismatched learning rates.

## Confidence
- **High confidence**: Theoretical regret bounds under Linear MDP assumptions; the primal-dual formulation and Bellman consistency interpretation.
- **Medium confidence**: Practical implementation details and hyperparameter sensitivity; the empirical performance outside the linear setting.
- **Low confidence**: Generalization to non-linear function approximation without additional assumptions; the exact computational complexity in high-dimensional feature spaces.

## Next Checks
1. **Robustness to Misspecification**: Evaluate VAC on non-linear MDPs with known misspecified features to quantify the gap between theoretical and empirical performance.
2. **Hyperparameter Ablation**: Systematically sweep α and analyze its impact on exploration coverage vs. value function stability across multiple environments.
3. **Computational Overhead**: Measure the runtime per iteration and memory footprint of the inner minimax loop compared to standard actor-critic methods on high-dimensional tasks.