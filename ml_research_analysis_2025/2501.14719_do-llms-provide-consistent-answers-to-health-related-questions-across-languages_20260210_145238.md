---
ver: rpa2
title: Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?
arxiv_id: '2501.14719'
source_url: https://arxiv.org/abs/2501.14719
tags:
- health
- languages
- llms
- answers
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines the consistency of Large Language Models (LLMs)
  in answering health-related questions across English, German, Turkish, and Chinese.
  The authors expanded the HealthFC dataset with Turkish and Chinese translations,
  categorizing questions by disease type, and introduced a novel prompt-based evaluation
  framework to segment and compare responses across languages.
---

# Do LLMs Provide Consistent Answers to Health-Related Questions across Languages?
arXiv ID: 2501.14719
Source URL: https://arxiv.org/abs/2501.14719
Reference count: 24
Primary result: LLMs show significant inconsistencies (40-70%) in health responses across EN, DE, TR, and ZH, especially in clinical guidelines and evidence sections.

## Quick Facts
- arXiv ID: 2501.14719
- Source URL: https://arxiv.org/abs/2501.14719
- Reference count: 24
- Primary result: LLMs show significant inconsistencies (40-70%) in health responses across EN, DE, TR, and ZH, especially in clinical guidelines and evidence sections.

## Executive Summary
This study investigates cross-lingual consistency of LLM responses to health-related questions across English, German, Turkish, and Chinese. The authors expanded the HealthFC dataset with Turkish and Chinese translations and developed a novel prompt-based framework to segment and compare responses across languages. Their analysis revealed significant inconsistencies, particularly in sections referencing clinical guidelines and evidence, with average inconsistency rates ranging from 40-70% across different models and languages. The findings highlight the need for improved cross-lingual alignment in LLMs to ensure accurate and equitable healthcare information delivery.

## Method Summary
The study expanded the HealthFC dataset with Turkish and Chinese translations, creating a multilingual health question corpus. They categorized questions by disease type using ICD10 codes and developed a prompt-based evaluation framework to segment LLM responses into structured discourse components. Four LLMs (ChatGPT-4o, GPT-4o, Llama3-70B, CommandR+) generated responses to questions in all four languages. A separate GPT-4o instance parsed these responses into five discourse sections (Answer Summary, Clinical Guidelines/Evidence, Health Benefits, Individual Considerations, Public Health Advice) and compared them across language pairs to determine consistency levels. The analysis focused on 11 disease categories with at least 20 samples each.

## Key Results
- Significant inconsistencies found across all models: 40-70% average inconsistency rates in discourse sections
- Clinical Guidelines/Evidence section showed highest inconsistency rates (60-70%) across all languages
- English and German (high-resource languages) showed lower inconsistency rates compared to Turkish and Chinese
- Turkish responses were notably shorter, often omitting Clinical Guidelines and Evidence sections
- Chinese responses showed moderate inconsistencies but better maintained Clinical Guidelines content

## Why This Works (Mechanism)
The framework works by first structuring LLM outputs into standardized discourse components using a parsing prompt, then comparing these structured representations across languages. This approach isolates inconsistencies at the discourse level rather than relying on surface-level text comparison, which can miss semantic differences. By using the same evaluation model (GPT-4o) for both parsing and comparison, the method maintains consistency in how answers are interpreted and evaluated. The discourse ontology captures the essential components of medical advice, enabling targeted identification of where cross-lingual alignment fails.

## Foundational Learning
- Concept: Cross-Lingual Alignment
  - Why needed here: This is the core problem the paper addresses. Without alignment, an LLM gives factually different answers to the same question depending on the language, undermining its reliability for global health applications.
  - Quick check question: If you ask "Is coffee good for my heart?" in English and then in Turkish, would you expect the medical advice to be identical or different?

- Concept: Discourse Ontology
  - Why needed here: The paper's key contribution is evaluating responses based on a structured breakdown (Answer Summary, Clinical Guidelines, etc.), not just overall text. Understanding this structure is necessary to interpret the results.
  - Quick check question: A user asks a health question. Does the "Answer Summary" section include all the background context and examples the model provides?

- Concept: High-Resource vs. Low-Resource Languages
  - Why needed here: This distinction is the proposed causal factor for the observed inconsistencies. English and German (high-resource) have vast training data, while Turkish and Chinese have comparatively less, directly impacting model performance.
  - Quick check question: Which language would you expect an LLM to have more "knowledge" about: a language with 100 billion web pages in its training set or one with 10 million?

## Architecture Onboarding
- Component map:
  1. **Expanded HealthFC Dataset:** The source of truth. Contains health questions in EN, DE, and new translations in TR and ZH, categorized by ICD10 codes.
  2. **Subject LLMs:** The systems under test (ChatGPT, GPT-4o, Llama3, CommandR+). They are prompted with the questions to generate long-form answers.
  3. **Evaluation LLM (GPT-4o):** The "judge." It performs two distinct tasks: a) **Parsing:** Splits raw text into structured sections. b) **Consistency-Check:** Compares parsed sections across language pairs.
  4. **Analysis Engine:** Aggregates the structured comparison labels to calculate inconsistency rates per section, per model, and per disease category.

- Critical path: `Dataset Question (e.g., EN, TR) -> Subject LLM -> Two Raw Answers -> Evaluation LLM Parsing -> Two Structured Answers -> Evaluation LLM Comparison -> Consistency Label`.

- Design tradeoffs:
  1. **Automated vs. Human Evaluation:** The study uses GPT-4o for all evaluation. This is scalable but introduces potential model bias. Human annotators were only used for validation (showing moderate to substantial agreement), not for the full evaluation.
  2. **Translation Quality:** The TR and ZH dataset questions were created with Google Translate. While reviewed, this is faster and cheaper than professional medical translation but could introduce subtle errors that affect the Subject LLM's answer.
  3. **Prompting Language:** Prompts for Subject LLMs were given in English for consistency, even when requesting answers in other languages. This improves instruction-following but may not reflect real-world user behavior.

- Failure signatures:
  1. **Parsing Collapse:** The Evaluation LLM fails to map a raw answer to the five discourse sections, leaving key information uncategorized.
  2. **Nuance Loss in Consistency Check:** The Evaluation LLM labels answers as "consistent" even when one contains significantly more detailed evidence than the other, a problem noted in the paper's qualitative analysis.
  3. **Numerical Hallucination:** Subject LLMs provide different statistics (e.g., "3-4 cups of coffee" vs. "1-2 cups") for the same question, which the automated check might miss if not looking for specific factuality.

- First 3 experiments:
  1. **Baseline Inconsistency Audit:** Run 100 randomly selected questions from the HealthFC dataset across all four languages on your target LLM. Use the Evaluation LLM pipeline to get a baseline inconsistency score for each discourse section.
  2. **Cross-Model Comparison:** Compare the inconsistency rates of two different model families (e.g., a closed-source model like GPT-4o vs. an open-source model like Llama3-70B). Hypothesis: Which model architecture leads to more cross-lingual stability?
  3. **Ablation on Translation:** Take 20 questions and provide professionally translated prompts vs. machine-translated prompts. Measure if higher-quality input translations reduce the inconsistency in the Subject LLM's answers. This tests the "input quality" assumption.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Can open-source LLMs detect fine-grained inconsistencies in multilingual health answers more effectively than GPT-4o?
- Basis in paper: [explicit] The authors state in the conclusion: "In future work, we will explore open LLMs instead of GPT-4o to assess fine-grained inconsistencies."
- Why unresolved: The current study relied on GPT-4o for automated evaluation, which showed only moderate agreement with human annotators for nuanced labels.
- What evidence would resolve it: A comparative benchmark evaluating various open-source LLMs against the human-annotated Gold Standard used in this study.

### Open Question 2
- Question: Does cross-lingual inconsistency imply factual inaccuracy or the spread of healthcare misinformation?
- Basis in paper: [explicit] The limitations section notes: "The framework currently does not assess factual accuracy, as ground-truth data is only available in EN and DE."
- Why unresolved: Responses can be semantically inconsistent across languages without necessarily being factually wrong, or conversely, consistent but universally incorrect.
- What evidence would resolve it: Integrating medical ground-truth labels into the multilingual dataset to correlate consistency scores with factual accuracy rates.

### Open Question 3
- Question: To what extent are the observed omissions in non-English responses caused by training data scarcity versus cultural differences in communication styles?
- Basis in paper: [inferred] The discussion hypothesizes that the omission of clinical guidelines in Turkish responses may be "due to differences in training data and communication culture."
- Why unresolved: The study identifies the variance in information density but does not isolate the specific contribution of data availability versus cultural norms.
- What evidence would resolve it: A causal analysis correlating the volume of language-specific training data with the completeness of responses across different cultural contexts.

## Limitations
- The study uses machine-translated prompts rather than professional medical translations, which may introduce subtle biases affecting LLM responses
- Automated evaluation via GPT-4o shows variable agreement rates (Kappa 0.50-0.76) across languages, suggesting potential parsing inconsistencies
- Focus on long-form responses may not generalize to shorter, clinical-query-style interactions common in healthcare settings

## Confidence
- **High**: The core finding that significant inconsistencies exist across discourse sections (40-70% inconsistency rates) is well-supported by the methodology and replicated across multiple models and languages.
- **Medium**: The attribution of inconsistencies primarily to language resource disparities (high-resource vs. low-resource) is plausible but not definitively proven; other factors like translation quality or cultural differences in medical discourse could contribute.
- **Medium**: The specific inconsistency rates per discourse section are reliable within this dataset but may vary with different question sets, model versions