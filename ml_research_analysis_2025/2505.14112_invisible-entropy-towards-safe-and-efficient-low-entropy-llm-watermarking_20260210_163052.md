---
ver: rpa2
title: 'Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking'
arxiv_id: '2505.14112'
source_url: https://arxiv.org/abs/2505.14112
tags:
- entropy
- threshold
- watermarking
- tokens
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a lightweight, safe, and efficient watermarking\
  \ method for large language models, specifically designed for low-entropy scenarios\
  \ where traditional watermarking approaches fail. The key idea is to replace the\
  \ need for the original LLM in entropy calculation with a small, unified feature\
  \ extractor and an entropy tagger that predicts whether the next token\u2019s entropy\
  \ is high or low, along with a threshold navigator that adaptively selects entropy\
  \ thresholds to balance watermark effectiveness and text naturalness."
---

# Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking

## Quick Facts
- arXiv ID: 2505.14112
- Source URL: https://arxiv.org/abs/2505.14112
- Reference count: 40
- This paper introduces a lightweight, safe, and efficient watermarking method for large language models, specifically designed for low-entropy scenarios where traditional watermarking approaches fail.

## Executive Summary
Invisible Entropy (IE) addresses the challenge of watermarking low-entropy text generated by large language models by replacing the need for the original LLM in entropy calculation with a small, unified feature extractor and an entropy tagger. The key innovation is a lightweight binary classifier that predicts whether the next token's entropy is high or low, combined with an adaptive threshold navigator that balances watermark effectiveness and text naturalness. Experiments on HumanEval and MBPP datasets show that this method reduces parameter size by 99% while achieving comparable performance to state-of-the-art watermarking methods.

## Method Summary
IE consists of three main components: a Unified Feature Extractor that converts prefix tokens into embeddings using SimCSE, an Entropy Tagger (small MLP) that predicts high/low entropy via binary classification, and a Threshold Navigator that adaptively selects entropy thresholds. The system trains the Entropy Tagger on MBPP data using ground-truth entropy values computed by the original LLM, then applies KGW-style watermarking selectively based on tagger predictions. The Threshold Navigator searches from high to low thresholds, stopping when watermark ratio decreases as green token count increases, optimizing the balance between detectability and naturalness.

## Key Results
- Achieves 99% parameter reduction (10M vs 1.2B) while maintaining comparable AUROC and Pass@1 metrics
- Improves Unified Effectiveness Score by ~20% through adaptive threshold selection
- Outperforms direct entropy calculation methods in low-entropy scenarios while being 100x more efficient

## Why This Works (Mechanism)

### Mechanism 1: Lightweight Entropy Prediction via Binary Classification
A small MLP classifier predicts whether the next token is high or low entropy without requiring the original LLM. The Entropy Tagger reframes entropy estimation from regression to binary classification, using a Unified Feature Extractor to convert prefix tokens into embeddings. Binary cross-entropy loss trains the tagger using ground-truth labels computed by the original LLM during training only. Core assumption: binary entropy classification is sufficient for selective watermarking.

### Mechanism 2: Threshold Navigator via Monotonicity-Based Search
An adaptive entropy threshold improves the balance between watermark detectability and text naturalness compared to fixed thresholds. The Threshold Navigator searches downward from an initial threshold, computing ratios of green token count change and watermark ratio change. The search stops when both ratios indicate improved trade-off. Theoretical analysis shows this maximizes detection statistic while minimizing Type-II error.

### Mechanism 3: Cross-Model Compatibility via Token Translation
A tokenizer translator enables the entropy tagger to work across different LLM architectures without retraining. The translator converts tokens from any LLM's tokenizer back to raw text, then re-encodes them using the embedding model's tokenizer. The embedding model processes only the final segment, and the last token's embedding represents the prefix context.

## Foundational Learning

- **Concept: Logit-based watermarking (KGW)**
  - Why needed here: IE builds on KGW's green/red list partitioning and logit biasing as the underlying watermark embedding mechanism.
  - Quick check question: Given vocabulary V, hash key h, and green list proportion γ = 0.5, how many tokens receive logit boost δ?

- **Concept: Shannon entropy vs. Spike entropy**
  - Why needed here: The paper uses Shannon entropy for entropy tagger training because its "dispersed distribution offers clearer boundaries" for binary classification.
  - Quick check question: For a probability distribution [0.9, 0.05, 0.05], is the Shannon entropy high or low? Would this token likely be watermarked?

- **Concept: Type-I and Type-II errors in detection**
  - Why needed here: The Threshold Navigator's theoretical justification relies on reducing Type-II error without affecting Type-I error.
  - Quick check question: If z > ẑ for human-written text, what type of error occurs? Does changing τ affect this error rate?

## Architecture Onboarding

- **Component map:** Tokenizer Translator -> Embedding Model (SimCSE) -> Entropy Tagger (MLP) -> Threshold Navigator -> KGW Watermarking Backbone
- **Critical path:** Token generation → Tokenizer Translator → Embedding Model → Entropy Tagger → (if high entropy) Apply watermark via KGW → Threshold Navigator monitors WR and |S|G → Returns final watermarked text
- **Design tradeoffs:** Fixed vs. adaptive threshold (fixed ignores inter-sample variability, adaptive adds computation but improves robustness); Search direction (high→low prioritizes code quality, low→high improves AUROC); Granularity (0.3 increments may miss optimal thresholds)
- **Failure signatures:** Entropy tagger accuracy drops → more misclassifications → degraded AUROC/Pass@1; Threshold navigator stuck → w never < 1 with p > 1 → returns initial threshold; Embedding model truncation too aggressive → loss of context → tagger errors on long dependencies
- **First 3 experiments:** 1) Train MLP on MBPP train split, evaluate accuracy on MBPP validation and HumanEval at τ ∈ {0.3, 0.6, 0.9, 1.2, 1.5}; 2) Run IE with navigator disabled vs. enabled across δ ∈ {1.0, 2.0, 3.0}, measure UES and AUROC gap; 3) Replace Entropy Tagger with StarCoder-3B for entropy estimation, compare AUROC/TPR and parameter count vs. IE

## Open Questions the Paper Calls Out

### Open Question 1
Can the Entropy Tagger be refined to achieve higher precision, particularly by incorporating specific low-entropy tokens like syntax markers? The current tagger shows slight performance degradation compared to precise entropy calculation, attributed to classification inaccuracies on frequent low-entropy tokens.

### Open Question 2
Can adaptive search granularities or alternative search directions enhance the Threshold Navigator's optimization performance? The current fixed search granularity (0.3) may limit optimization flexibility, and alternative search heuristics might yield better balance between watermark naturalness and detectability.

### Open Question 3
How robust is the Invisible Entropy method against diverse adversarial attacks beyond variable name paraphrasing? The current evaluation focuses solely on code generation tasks and variable name paraphrasing, leaving other attack vectors unexplored.

## Limitations

- Entropy Tagger generalization may degrade significantly on out-of-domain data, with accuracy dropping from 83% on MBPP to 66-76% on HumanEval
- Threshold Navigator uses fixed search granularity (0.3) which may miss optimal thresholds for specific samples
- Cross-model compatibility depends on embedding model's ability to capture context despite truncation, potentially losing critical information for long-range dependencies

## Confidence

- **High Confidence**: 99% parameter reduction (10M vs 1.2B) while maintaining comparable metrics is directly measurable and experimentally validated
- **Medium Confidence**: Threshold Navigator improves UES by ~20% through adaptive selection, though theoretical justification for stopping condition needs more rigorous validation
- **Low Confidence**: Entropy Tagger's out-of-domain performance (66-76% on HumanEval) is sufficient for practical watermarking, but relationship between tagger accuracy and final detection metrics isn't fully characterized

## Next Checks

1. **Domain Generalization Test**: Train the Entropy Tagger on MBPP, then evaluate on Python code from GitHub repositories, JavaScript, and natural language text. Measure accuracy degradation and its impact on watermark detection metrics (AUROC, TPR at FPR<5%).

2. **Threshold Navigator Sensitivity Analysis**: Systematically vary the initial threshold (τ=1.0, 1.5, 2.0) and search granularity (Δ=0.1, 0.3, 0.5) across multiple samples. Track how often the navigator finds the theoretical optimum and measure the distribution of selected thresholds.

3. **Context Retention Evaluation**: Create test cases with long-range dependencies (e.g., function definitions spanning >100 tokens). Compare Entropy Tagger accuracy when using full context vs. truncated prefix embeddings. Measure the correlation between truncation length and prediction accuracy.