---
ver: rpa2
title: 'TextureSAM: Towards a Texture Aware Foundation Model for Segmentation'
arxiv_id: '2505.16540'
source_url: https://arxiv.org/abs/2505.16540
tags:
- texture
- segmentation
- sam-2
- texturesam
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitation of Segment Anything Models
  (SAM) in texture-based segmentation tasks, where boundaries are defined by texture
  changes rather than object semantics. The authors propose TextureSAM, a texture-aware
  variant of SAM-2, created by fine-tuning on a texture-augmented version of the ADE20K
  dataset.
---

# TextureSAM: Towards a Texture Aware Foundation Model for Segmentation

## Quick Facts
- arXiv ID: 2505.16540
- Source URL: https://arxiv.org/abs/2505.16540
- Reference count: 35
- Primary result: TextureSAM fine-tuned on texture-augmented ADE20K outperforms SAM-2 by +0.2 mIoU on natural texture datasets and +0.18 mIoU on synthetic texture datasets

## Executive Summary
This paper addresses a fundamental limitation of Segment Anything Models (SAM) in texture-based segmentation tasks, where boundaries are defined by texture changes rather than object semantics. The authors propose TextureSAM, a texture-aware variant of SAM-2 created by fine-tuning on a texture-augmented version of the ADE20K dataset. Using a novel fine-tuning approach that incorporates texture augmentation techniques, they incrementally modify training images to emphasize texture features using a compositional neural texture approach. TextureSAM significantly outperforms SAM-2 on both natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation datasets.

## Method Summary
The method employs Compositional Neural Textures (CNT) to synthesize training data by replacing internal pixels of ground-truth instance masks with textures sampled from the Describable Textures Dataset (DTD). The interpolation coefficient η controls texture strength, with two variants: mild (η ≤ 0.3) and strong (η ≤ 1.0). The fine-tuning process uses sam2_hiera_small on Textured-ADE20K for 19-25 epochs depending on augmentation strength. Modified inference parameters (points_per_side: 32→64, stability_score_thresh: 0.95→0.2) are required for textureSAM to produce valid masks on texture-heavy images.

## Key Results
- TextureSAM achieves +0.2 mIoU improvement over SAM-2 on real-world texture datasets (RWTD)
- Strong augmentation variant (η ≤ 1.0) shows +0.35 mIoU improvement on synthetic texture datasets (STMD)
- Mild augmentation variant (η ≤ 0.3) maintains semantic segmentation capabilities with only -0.1 mIoU drop on standard ADE20K
- Modified inference parameters are essential: default settings result in zero predicted masks for most texture-heavy images

## Why This Works (Mechanism)

### Mechanism 1: Shape-Texture Decoupling via Mask-Conditioned Synthesis
The proposed method shifts a foundation model from shape-based to texture-based segmentation by forcing it to process images where semantic shape is invariant while texture varies stochastically. Using CNT, they replace internal pixels of ground-truth masks with textures from DTD, controlled by interpolation coefficient η. This breaks the correlation between object classes and their expected textures, forcing the model to attend to local texture boundaries rather than shape priors.

### Mechanism 2: Forcing Texture Grouping via Inference Calibration
Improving texture segmentation requires relaxing the model's confidence thresholds to allow it to propose regions that lack strong semantic objectness but possess coherent texture. By increasing points_per_side (32 to 64) and significantly lowering stability_score_thresh (0.95 to 0.2), they force the model to generate denser mask proposals, allowing texture-aware features to surface as valid segments.

### Mechanism 3: Balancing Semantic Retention via Augmentation Strength
The trade-off between semantic segmentation capability and texture sensitivity is directly tunable via texture interpolation coefficient η. The mild variant (η ≤ 0.3) retains most semantic structure, ensuring the model doesn't forget what a "natural" object looks like, while the strong variant (η ≤ 1.0) completely replaces object content with texture. Mild performs better on real-world textured images because it preserves the coupling between shape and texture needed for generalization.

## Foundational Learning

- **Concept: Vision Transformer (ViT) Bias**
  - Why needed here: The paper's premise relies on ViTs (used in SAM) prioritizing global shape over local texture due to patch-based tokenization
  - Quick check question: Why would a standard CNN potentially struggle less with texture segmentation "out of the box" compared to a ViT-based SAM?

- **Concept: Mask Aggregation in Evaluation**
  - Why needed here: Texture regions are often contiguous but lack sharp "objectness" that standard metrics assume
  - Quick check question: If a model splits one textured wall into 5 small masks, does the mIoU metric penalize this heavily? How does "Mask Aggregation" change that evaluation?

- **Concept: Compositional Neural Textures (CNT)**
  - Why needed here: This is the engine of data generation, allowing "mask-aware" texture swapping distinct from standard style transfer
  - Quick check question: Why is standard style transfer (e.g., Gatys et al.) insufficient for creating this dataset? (Hint: consider boundary preservation)

## Architecture Onboarding

- **Component map:** ADE20K (Content) + DTD (Texture Source) → CNT Encoder → Gaussian Representation → Mask-wise Feature Modulation (η) → CNT Decoder → Textured-ADE20K → SAM-2 (Hiera Small) Encoder-Decoder → Modified Inference (points_per_side: 64, stability_score_thresh: 0.2)

- **Critical path:** The Mask-wise Feature Modulation (Section 3.3, Eq 1) is the critical innovation. The system extracts Gaussians from content image, selects those centered only within instance mask, and interpolates their features with texture image features, ensuring texture change is strictly localized to segment of interest.

- **Design tradeoffs:**
  - Mild (η ≤ 0.3) vs. Strong (η ≤ 1.0): Mild retains semantic capabilities (+0.47 mIoU on RWTD, better generalization) while Strong excels only on pure texture boundaries (+0.35 mIoU on STMD) but degrades on natural scenes
  - Inference Sensitivity: Modified inference parameters (low threshold) are necessary for TextureSAM to function but cause original SAM-2 to over-segment on natural images

- **Failure signatures:**
  - Over-fragmentation: Model splits uniform textured area (e.g., grass) into hundreds of tiny blades instead of one region
  - Zero Masks: If inference parameters are not lowered (stability thresh < 0.95), TextureSAM outputs empty predictions on texture-heavy images

- **First 3 experiments:**
  1. Sanity Check (Visual): Run TextureSAM vs. SAM-2 on Synthetic STMD dataset. Verify SAM-2 produces scatter of small segments while TextureSAM produces large, contiguous blocks matching texture boundaries
  2. Generalization Test (Quantitative): Evaluate TextureSAM (η ≤ 0.3) on standard ADE20K validation set. Confirm mIoU doesn't drop >2-3% compared to baseline
  3. Parameter Sweep: Run inference on single natural image from RWTD while sweeping stability_score_thresh (0.1 to 0.9). Observe tipping point where model starts ignoring texture boundaries

## Open Questions the Paper Calls Out

### Open Question 1
Can the trade-off between enhancing texture sensitivity and preserving general semantic segmentation capabilities be eliminated? The conclusion states, "These observations highlight a trade-off between enhancing texture sensitivity and preserving broader segmentation performance." Strong augmentation excels on synthetic data but underperforms on natural images compared to mild variant, suggesting a zero-sum dynamic between generalization and texture specificity.

### Open Question 2
Does the texture-focused fine-tuning necessitate modified inference hyperparameters to produce valid masks? The authors note that using default inference parameters "resulted in no predicted masks for most images," forcing reduction of stability score threshold from 0.95 to 0.2. It's unclear if the model's confidence calibration is fundamentally broken or if the mask decoder architecture cannot reconcile new feature representations.

### Open Question 3
Does the Textured-ADE20K training protocol generalize to specialized domains such as medical imaging or remote sensing? The introduction identifies medical imaging and material classification as critical motivations, but evaluation is limited to natural (RWTD) and synthetic (STMD) datasets. It remains untested whether "texture-awareness" learned from natural textures in DTD transfers to structural and stochastic textures found in radiology or microscopy.

## Limitations
- The need for significantly modified inference parameters (stability threshold 0.95→0.2) creates a regime where the same model cannot be applied to natural images without causing over-segmentation
- Key training hyperparameters (batch size, learning rate schedule, optimizer configuration) are referenced but not included in the paper
- Claims about TextureSAM being a "general solution" for texture segmentation are overstated given the narrow evaluation scope and dataset-specific inference tuning

## Confidence
- **High Confidence**: The core technical contribution (CNT-based texture augmentation with mask-aware feature modulation) is well-specified and reproducible
- **Medium Confidence**: Performance improvements on RWTD (+0.2 mIoU) and STMD (+0.18 mIoU) are reported with appropriate metrics, but inference parameter changes introduce uncertainty about real-world applicability
- **Low Confidence**: Generalization claims to domains like medical imaging are unsupported by evidence beyond the paper's scope

## Next Checks
1. **Cross-Domain Transfer Test**: Evaluate TextureSAM (η ≤ 0.3) on a medical imaging dataset (e.g., ISIC skin lesion segmentation) to assess whether texture augmentation helps in domains with no semantic shape priors
2. **Inference Parameter Sweep**: Systematically vary `stability_score_thresh` (0.1 to 0.95) on RWTD and plot mIoU vs. threshold to identify optimal trade-off point and validate the paper's choice of 0.2
3. **Ablation on Texture Strength**: Train TextureSAM variants with intermediate η values (0.3, 0.5, 0.7) and evaluate on both STMD and ADE20K to map the precise degradation curve and confirm the "mild vs. strong" characterization