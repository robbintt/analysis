---
ver: rpa2
title: 'Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal
  Dataset of Eye and Facial Behaviors'
arxiv_id: '2512.16485'
source_url: https://arxiv.org/abs/2512.16485
tags:
- emotion
- facial
- class
- multimodal
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between facial expression recognition
  (FER) and emotion recognition (ER) by constructing a multimodal dataset called EMER
  that integrates facial expression videos, eye movement sequences, and eye fixation
  maps with multi-view emotion annotations. To capture eye behavior signals non-invasively,
  the dataset uses a stimulus-induced spontaneous emotion elicitation protocol with
  121 participants, resulting in 1,303 high-quality multimodal sequences.
---

# Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors

## Quick Facts
- **arXiv ID**: 2512.16485
- **Source URL**: https://arxiv.org/abs/2512.16485
- **Reference count**: 40
- **Primary result**: EMERT model achieves state-of-the-art performance on EMER dataset by integrating eye behaviors with facial expressions for emotion recognition

## Executive Summary
This paper addresses the fundamental disconnect between facial expression recognition (FER) and emotion recognition (ER) by constructing a novel multimodal dataset called EMER that captures both facial expressions and eye behaviors simultaneously. The dataset includes 1,303 high-quality sequences from 121 participants, collected through a stimulus-induced spontaneous emotion elicitation protocol. The authors propose EMERT, a multimodal transformer architecture that employs modality-adversarial feature decoupling and multitask learning to effectively bridge the emotion gap by modeling eye behaviors as a complementary signal to facial expressions. Experimental results demonstrate significant performance improvements over state-of-the-art methods across seven benchmark protocols.

## Method Summary
The research constructs the EMER dataset through a controlled laboratory protocol where participants view emotionally evocative stimuli while being recorded with synchronized facial expression videos, eye movement sequences, and eye fixation maps. The dataset is then used to train EMERT, a multimodal transformer architecture that processes facial and eye modality streams separately before fusing them through modality-adversarial feature decoupling. This approach explicitly prevents the model from relying solely on facial expressions by introducing adversarial training that forces it to leverage eye behavior information. The system employs multitask learning to simultaneously optimize for both FER and ER tasks, creating a unified framework that captures the nuanced relationship between visible facial expressions and underlying emotional states.

## Key Results
- EMERT significantly outperforms state-of-the-art multimodal methods across seven benchmark protocols on the EMER dataset
- The model demonstrates that eye behaviors provide complementary information to facial expressions for emotion recognition
- Multimodal integration shows consistent improvements across different emotion recognition tasks, validating the importance of capturing both modalities

## Why This Works (Mechanism)
The effectiveness of EMERT stems from its ability to decouple and then strategically recombine information from facial expressions and eye behaviors. By using modality-adversarial training, the model is forced to learn representations that cannot be explained by facial expressions alone, thus capturing subtle emotional cues that may be present in eye movements but absent from facial expressions. The multitask learning framework ensures that both modalities contribute meaningfully to the final emotion prediction, rather than one modality dominating. This architectural choice directly addresses the "emotion gap" by recognizing that facial expressions and eye behaviors often convey different aspects of emotional experience.

## Foundational Learning
- **Multimodal Transformers**: Neural architectures that process multiple input modalities (like text, images, and audio) through self-attention mechanisms. Why needed: To effectively fuse heterogeneous data types (facial expressions and eye movements) that have different temporal and spatial characteristics. Quick check: Verify the model can handle variable-length sequences across modalities.
- **Modality-Adversarial Training**: A technique where separate encoders are trained with adversarial objectives to prevent modality-specific information leakage. Why needed: To ensure the model doesn't rely solely on facial expressions and actually learns to leverage eye behavior information. Quick check: Confirm that eye behavior performance degrades significantly when facial expressions are removed.
- **Stimulus-Induced Emotion Elicitation**: A controlled experimental protocol where participants' emotions are triggered by specific visual or auditory stimuli. Why needed: To generate spontaneous, ecologically valid emotional responses while maintaining experimental control. Quick check: Validate that self-reported emotional states align with stimulus categories.
- **Eye Fixation Maps**: Spatial representations showing where participants' gaze is concentrated during stimulus viewing. Why needed: To capture attention patterns that correlate with emotional processing. Quick check: Ensure fixation maps are properly synchronized with facial expression frames.
- **Multitask Learning for FER and ER**: Joint optimization of facial expression recognition and emotion recognition tasks. Why needed: To create a unified model that understands both the observable expressions and the underlying emotions they represent. Quick check: Verify that performance on one task improves when trained jointly with the other.

## Architecture Onboarding

Component map: Eye Movement Encoder -> Modality-Adversarial Layer -> Fusion Module -> Emotion Classifier; Facial Expression Encoder -> Modality-Adversarial Layer -> Fusion Module -> Emotion Classifier

Critical path: Raw multimodal inputs → Individual modality encoders → Modality-adversarial feature decoupling → Cross-modal fusion → Joint emotion classification

Design tradeoffs: The paper prioritizes controlled data collection over ecological validity, choosing laboratory conditions that ensure high-quality eye tracking data but may limit real-world applicability. The modality-adversarial approach adds training complexity but provides better feature disentanglement compared to simple concatenation or attention-based fusion methods.

Failure signatures: Performance degradation when testing on uncontrolled environments, over-reliance on facial expressions if adversarial training is insufficient, and potential synchronization errors between eye tracking and facial video streams.

Three first experiments:
1. Train EMERT on EMER dataset with only facial expressions (baseline) and compare against full multimodal model
2. Perform ablation study removing the modality-adversarial component to measure its contribution
3. Test model generalization by training on subset of participants and evaluating on held-out subjects

## Open Questions the Paper Calls Out
- **Ecological validity**: The authors acknowledge plans to extend EMER with more diverse and ecologically valid scenarios, suggesting current controlled conditions may limit real-world applicability
- **Large Multimodal Models**: The paper expresses interest in integrating LMMs to further investigate the relationship between FER and ER, indicating uncertainty about whether specialized architectures outperform general-purpose models
- **Cultural generalizability**: While not explicitly stated, the relatively homogeneous participant pool suggests potential limitations in cross-cultural emotion expression patterns

## Limitations
- **Controlled environment constraint**: Laboratory-based data collection may not capture the complexity and variability of real-world emotional expressions
- **Dataset size limitations**: 1,303 sequences from 121 participants, while larger than many physiological datasets, may not capture full demographic and cultural diversity
- **Preprocessing opacity**: Insufficient detail about eye movement and fixation data preprocessing pipeline makes it difficult to assess potential biases or artifacts

## Confidence
- **High confidence** in technical implementation of EMERT model and multimodal dataset construction
- **Medium confidence** in claims about eye behaviors complementing facial expressions, based primarily on performance metrics within single dataset
- **Low confidence** in ecological validity for real-world emotion recognition scenarios

## Next Checks
1. Test EMERT on external multimodal emotion datasets to verify generalizability beyond EMER dataset
2. Conduct ablation studies specifically isolating contribution of eye movement signals versus eye fixation maps to understand which eye behavior features are most valuable
3. Perform cross-cultural validation with participants from diverse demographic backgrounds to assess whether model's performance holds across different populations