---
ver: rpa2
title: 'INTERPOS: Interaction Rhythm Guided Positional Morphing for Mobile App Recommender
  Systems'
arxiv_id: '2506.12661'
source_url: https://arxiv.org/abs/2506.12661
tags:
- user
- recommendation
- sequential
- interaction
- ndcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces INTERPOS, a novel approach to address the
  challenge of user interaction rhythm in mobile app recommendation. Traditional sequential
  recommender systems fail to account for the temporal gaps between user interactions,
  which are particularly pronounced in the mobile app domain compared to other domains.
---

# INTERPOS: Interaction Rhythm Guided Positional Morphing for Mobile App Recommender Systems

## Quick Facts
- arXiv ID: 2506.12661
- Source URL: https://arxiv.org/abs/2506.12661
- Reference count: 40
- Up to 157.3% improvement in NDCG@20 over state-of-the-art

## Executive Summary
INTERPOS addresses a critical gap in sequential recommender systems by incorporating user interaction rhythm—the temporal gaps between consecutive app interactions—into transformer-based architectures. Unlike traditional sequential models that only capture order, INTERPOS introduces rhythm-guided position embeddings that explicitly model the time intervals between interactions. This approach is particularly relevant for mobile app recommendation, where interactions are naturally spaced out over time, creating distinct patterns of engagement that existing models fail to capture.

The method employs three fusion strategies—basic fusion (element-wise sum), MLP-based fusion (concatenate then project), and gated fusion (sigmoid-gated combination)—to integrate rhythm embeddings with traditional positional embeddings. Evaluated across 7 mobile app recommendation datasets, INTERPOS significantly outperforms state-of-the-art models, achieving up to 157.3% improvement in NDCG@20 and 145.62% improvement in HIT@20. The source code is publicly available, enabling reproducibility and further research.

## Method Summary
INTERPOS enhances transformer-based sequential recommenders by morphing positional embeddings with rhythm-guided embeddings that encode temporal gaps between user interactions. The method computes time differences (Δt) between consecutive interactions, normalizes these values per dataset, and learns rhythm embeddings via an embedding table. Three fusion strategies integrate these rhythm embeddings with standard positional embeddings: basic fusion uses element-wise addition, MLP-based fusion concatenates and projects through a neural network, and gated fusion uses sigmoid gating to adaptively combine both signals. This approach is implemented within LightSANs and SASRec architectures using RecBole, with specific hyperparameters including 2 layers, 2 attention heads, hidden size 64-128, and batch size 4096.

## Key Results
- Up to 157.3% improvement in NDCG@20 over state-of-the-art models
- Up to 145.62% improvement in HIT@20 on tested datasets
- Consistent performance gains across all 7 mobile app recommendation datasets

## Why This Works (Mechanism)
Traditional sequential recommenders capture only the order of interactions, missing the crucial temporal spacing between them. Mobile app usage patterns exhibit significant gaps between interactions, creating distinct user engagement rhythms that carry predictive information. By morphing positional embeddings with rhythm embeddings, INTERPOS creates a more nuanced representation that captures both sequence order and temporal spacing, enabling the model to better understand user behavior patterns and make more accurate recommendations.

## Foundational Learning
- **Rhythm Normalization**: Why needed: To ensure time intervals are on comparable scales across users and datasets. Quick check: Verify that normalized Δt values fall within expected range [0,1] after clipping.
- **Positional Embedding Morphing**: Why needed: To combine sequential order information with temporal spacing in a unified representation. Quick check: Confirm merged embeddings have correct dimensionality matching transformer input requirements.
- **Gated Fusion Architecture**: Why needed: To allow the model to learn adaptive weighting between positional and rhythm information. Quick check: Validate that gating weights are learnable and produce outputs within valid range [0,1].

## Architecture Onboarding
- **Component Map**: Input data -> Rhythm Embedding Layer -> Fusion Module (BF/MF/GF) -> Positional Embedding -> LightSANs/SASRec Transformer -> Prediction Layer
- **Critical Path**: The rhythm embedding layer and fusion module are critical, as they directly impact the quality of input representations fed to the transformer.
- **Design Tradeoffs**: Basic fusion is simple but rigid, MLP fusion adds expressiveness but increases parameters, gated fusion is most flexible but requires careful initialization to avoid vanishing gradients.
- **Failure Signatures**: Poor rhythm normalization leading to embedding index errors, dimension mismatches in fusion modules, or overfitting due to increased model complexity.
- **First Experiments**: 1) Validate rhythm embedding layer produces outputs with correct shape and values. 2) Test each fusion strategy with dummy inputs to verify dimension compatibility. 3) Run ablation study comparing individual fusion methods on small dataset subset.

## Open Questions the Paper Calls Out
- Can the rhythm-guided approach maintain performance advantages when applied to high-density interaction domains like e-commerce or short-video streaming where consecutive interactions occur within minutes?
- What specific dataset properties determine whether Basic, MLP, or Gated fusion is optimal, given that results show no clear winner?
- What alternative architectural strategies beyond embedding-layer fusion can effectively incorporate user interaction rhythm into autoregressive recommendation models?

## Limitations
- Performance gains are demonstrated only on mobile app datasets with naturally sparse interactions, not on dense interaction domains
- Rhythm embedding discretization strategy (bin count and mapping) is underspecified, requiring implementation assumptions
- MLP architectures for fusion strategies lack detailed specifications, potentially affecting reproducibility

## Confidence
- **Reproduction Plan Completeness**: Low - critical components like rhythm embedding discretization and fusion MLP architectures are underspecified
- **Implementation Complexity**: Medium - requires integration with RecBole and custom component development
- **Expected Result Fidelity**: Low - claimed improvements depend on correctly implementing underspecified architectural details

## Next Checks
1. Verify rhythm normalization/clipping strategy on MobileRec dataset and confirm embedding bin assignment logic matches implementation
2. Test fusion module forward passes with dummy inputs to validate dimension compatibility and gating behavior in GF
3. Run ablation studies comparing BF, MF, and GF variants on a small subset to confirm they produce distinct outputs and gradients flow correctly