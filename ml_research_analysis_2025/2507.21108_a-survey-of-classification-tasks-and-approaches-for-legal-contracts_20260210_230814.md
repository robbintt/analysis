---
ver: rpa2
title: A Survey of Classification Tasks and Approaches for Legal Contracts
arxiv_id: '2507.21108'
source_url: https://arxiv.org/abs/2507.21108
tags:
- legal
- classification
- contract
- tasks
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews legal contract classification
  (LCC), identifying seven key tasks, fourteen datasets, and thirty-five methodologies.
  It highlights the shift from classical machine learning to deep learning and transformer-based
  approaches.
---

# A Survey of Classification Tasks and Approaches for Legal Contracts

## Quick Facts
- arXiv ID: 2507.21108
- Source URL: https://arxiv.org/abs/2507.21108
- Reference count: 26
- This survey comprehensively reviews legal contract classification (LCC), identifying seven key tasks, fourteen datasets, and thirty-five methodologies.

## Executive Summary
This survey provides a comprehensive overview of legal contract classification (LCC), analyzing the evolution from classical machine learning to transformer-based approaches. The study identifies seven major classification tasks, including topic classification and unfair clause identification, and reviews fourteen datasets spanning different contract types and jurisdictions. Transformer models, particularly BERT-based architectures, have shown significant performance improvements over traditional methods. The survey also highlights critical challenges such as limited multilingual datasets, annotation transparency, and the need for improved model interpretability. Future research directions include expanding to multilingual contexts, exploring encoder-decoder architectures, and developing efficient prompting strategies.

## Method Summary
The survey employs a systematic literature review methodology, analyzing 35 studies published between 2017 and 2024. It categorizes research into seven classification tasks, examines fourteen datasets including LEDGAR and UNFAIR-ToS, and evaluates thirty-five methodologies spanning classical machine learning, deep learning, and transformer-based approaches. The analysis focuses on task definitions, dataset characteristics, evaluation metrics (particularly Macro-F1 for imbalanced data), and performance comparisons. The survey synthesizes findings across studies to identify trends, challenges, and opportunities in LCC, with particular emphasis on the shift toward transformer architectures since 2020.

## Key Results
- Transformer models, especially BERT-based architectures, outperform traditional methods across tasks like topic classification and unfair clause identification
- Pre-training on legal-specific corpora (Legal-BERT, LexLM) conditionally improves performance on nuanced tasks like NLI and unfair clause detection
- Hierarchical modeling approaches are necessary to handle full-length contracts exceeding standard token limits
- Limited multilingual datasets and annotation transparency remain significant challenges
- Model compression techniques and prompting strategies show promise for improving efficiency

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning transformer-based models (specifically BERT architectures) outperforms classical machine learning and standard deep learning for legal contract classification, provided sufficient labeled data exists. The bidirectional attention mechanism captures long-range dependencies and contextual nuances in "legalese" more effectively than Bag-of-Words or RNNs. This works because semantic relationships required for classification are distributed across the text context rather than being captured by isolated keywords. Performance degrades if the dataset is extremely small or if contract structure exceeds the model's maximum context window without adaptation.

### Mechanism 2
Pre-training language models on domain-specific legal corpora (e.g., Legal-BERT, LexLM) conditionally improves performance on nuanced tasks like Natural Language Inference and unfair clause detection. Exposure to legal vocabulary and sentence structures during pre-training allows the model to develop better representations for downstream tasks, reducing data requirements for fine-tuning. This assumes the source legal corpus shares linguistic properties with target contracts. Performance suffers if the pre-training corpus is biased or non-representative.

### Mechanism 3
Hierarchical modeling approaches (processing clauses/provisions before the document) are necessary to handle full-length contracts which exceed standard token limits. By encoding lower-level units first and then aggregating them for document-level classification, the model preserves local context while handling document length constraints. This assumes a contract's meaning is a composite of its clauses, with relevant signals distributed throughout the document. The approach breaks if clause extraction or segmentation fails.

## Foundational Learning

- **Concept: Transformer Attention vs. Classical Features**
  - **Why needed here:** To understand why TF-IDF and SVMs are being replaced; transformers weigh the importance of words based on surrounding context ("self-attention"), which is critical for resolving ambiguous legal phrasing.
  - **Quick check question:** Can you explain why a model might misclassify a clause if it only looks at individual words (TF-IDF) versus the whole sentence context (Attention)?

- **Concept: Multi-label Classification**
  - **Why needed here:** Many LCC tasks (like Unfair Clause Identification) are not mutually exclusive; a single clause can be both "Unilateral Change" and "Arbitration" related.
  - **Quick check question:** Would you use a softmax or sigmoid activation function in the output layer for identifying unfair clauses where multiple labels apply?

- **Concept: The "Legalese" Domain Gap**
  - **Why needed here:** Standard English models (like standard BERT) often fail to grasp the specific deontic modality (obligations vs. permissions) in contracts without domain adaptation.
  - **Quick check question:** How does "pre-training on legal corpora" (Domain Adaptation) change the model's initial state before you even start training on your specific classification task?

## Architecture Onboarding

- **Component map:** Raw Contract Text (PDF/Text) -> Segmentation into Clauses/Provisions -> Pre-trained Transformer (Legal-BERT, RoBERTa) -> Classification Layer (Softmax/Multi-label Sigmoid) -> Macro/Micro F1-Score Evaluation

- **Critical path:**
  1. Data Segmentation: Correctly splitting documents into clauses/provisions is the highest leverage error point
  2. Tokenization: Using legal-aware tokenizers or handling long sequences (sliding window or hierarchical approach)
  3. Metric Selection: Relying on Accuracy is dangerous due to class imbalance; prioritize Macro-F1 or F2-Score

- **Design tradeoffs:**
  - General vs. Domain Models: General models (RoBERTa) are easier to set up but may miss legal nuance. Domain models (Legal-BERT) require more resources but perform better on NLI.
  - Efficiency vs. Performance: Techniques like "Block Expansion" or "Token Pruning" speed up inference but may sacrifice 1-2% accuracy.

- **Failure signatures:**
  - The "Majority Class" Trap: The model predicts "Fair" or "Non-Risky" for everything because "Unfair" clauses are rare (high accuracy, zero recall)
  - Context Truncation: Standard BERT (512 tokens) cuts off the end of long contracts, missing termination clauses located in appendices
  - Jurisprudential Drift: Prompting-based models hallucinating legal rights not present in the text

- **First 3 experiments:**
  1. Baseline Establishment: Train a Linear SVM (TF-IDF) vs. a standard BERT on the LEDGAR dataset to quantify the "Transformer advantage"
  2. Domain Adaptation Check: Compare standard RoBERTa against Legal-BERT on the UNFAIR-ToS task to see if domain pre-training aids unfair clause detection
  3. Context Window Test: Compare a standard BERT (with truncation) against a hierarchical BERT or Longformer on ContractNLI to measure the impact of full-document context

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do encoder-decoder and decoder-only transformer architectures compare to encoder-only models in performance across legal contract classification tasks?
- Basis in paper: Section 8.2 notes that most studies focus on encoder-based models, with "little to no exploration of encoder-decoder models... or decoder-based models."
- Why unresolved: The field currently relies heavily on BERT-style encoders, leaving the potential of generative or seq2seq architectures for classification under-explored.
- What evidence would resolve it: A comparative study fine-tuning architectures like T5 (encoder-decoder) or LLaMA (decoder-only) against BERT baselines on benchmarks like LexGLUE.

### Open Question 2
- Question: Do legal-specific pre-trained language models offer significant performance advantages over general-purpose models for all contract classification tasks?
- Basis in paper: Section 8.2 states the goal is to "determine which tasks benefit most from legal-specific models and which tasks can be adequately handled by general-purpose models."
- Why unresolved: While legal LLMs exist (e.g., Legal-BERT), there is no broad assessment comparing their effectiveness against general models across the diverse range of LCC tasks.
- What evidence would resolve it: A systematic benchmark evaluating the accuracy of legal-specific versus general models (e.g., RoBERTa) on tasks ranging from topic classification to unfair clause detection.

### Open Question 3
- Question: How can contract classification systems be developed to handle multilingual legal environments and cross-jurisdictional variations in document structure?
- Basis in paper: Section 8.2 highlights that models trained only on English often fail in other regions, necessitating research to "expand research to cover multiple languages."
- Why unresolved: Legal terms, structures, and meanings vary significantly between languages and legal systems (e.g., common law vs. civil law), creating a generalization gap.
- What evidence would resolve it: The development and evaluation of cross-lingual models (e.g., XLM-R) on a new corpus of contracts spanning multiple languages and jurisdictions.

## Limitations

- The survey aggregates findings across multiple studies without reporting unified validation protocols, making it difficult to assess whether claimed performance gains are consistent or dataset-specific
- Limited discussion of annotation quality and inter-annotator agreement across the fourteen datasets may mask reliability issues in the underlying labels
- No quantitative analysis of computational costs associated with transformer models versus traditional approaches, despite efficiency being mentioned as a consideration

## Confidence

- **High Confidence:** The dominance of transformer models over classical machine learning approaches is well-supported by multiple studies cited in the survey
- **Medium Confidence:** Claims about domain-specific pre-training (Legal-BERT, LexLM) improving performance are supported by evidence but may vary significantly based on dataset characteristics
- **Low Confidence:** Efficiency comparisons between different model architectures lack quantitative backing, making practical deployment recommendations uncertain

## Next Checks

1. Replicate the LEDGAR classification task using both Legal-BERT and standard BERT to quantify the actual performance gap claimed in the survey
2. Test the annotation reliability of the UNFAIR-ToS dataset by measuring inter-annotator agreement scores across multiple annotators
3. Conduct a computational cost analysis comparing inference times and memory requirements between transformer models and classical approaches on the same hardware