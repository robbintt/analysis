---
ver: rpa2
title: 'Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart
  Personal Assistant'
arxiv_id: '2504.18373'
source_url: https://arxiv.org/abs/2504.18373
tags:
- agent
- intent
- multi-agent
- arxiv
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Auto-SLURP is a benchmark dataset for evaluating multi-agent frameworks
  in smart personal assistants. It extends the SLURP dataset by relabeling data and
  integrating simulated servers and external services to enable end-to-end evaluation
  of language understanding, task execution, and response generation.
---

# Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant

## Quick Facts
- arXiv ID: 2504.18373
- Source URL: https://arxiv.org/abs/2504.18373
- Authors: Lei Shen; Xiaoyu Shen
- Reference count: 15
- Primary result: Auto-SLURP achieves 46% success rate for multi-agent personal assistant frameworks, with intent prediction as primary failure source

## Executive Summary
Auto-SLURP is a benchmark dataset designed to evaluate multi-agent frameworks for smart personal assistants through end-to-end task execution. It extends the SLURP dataset by relabeling data and integrating simulated servers and external services to assess language understanding, task execution, and response generation capabilities. The benchmark covers diverse task domains and reveals significant challenges for state-of-the-art frameworks, with intent prediction identified as the primary bottleneck. Experiments with four representative frameworks show that AgentLite achieves the highest success rate of 46%, while targeted finetuning of the intent agent improves performance by 55%.

## Method Summary
The benchmark extends the SLURP dataset by relabeling slots to capture execution-oriented parameters required for backend API calls. It uses simulated servers and external services (weather, news, search) to enable end-to-end evaluation of multi-agent frameworks. The workflow involves a Program Manager delegating tasks to specialized agents (Intent, Time, URL, Request) for processing user queries across 23 domains. Four frameworks (CamelAI, LangGraph, AutoGen, AgentLite) are evaluated using GPT-4, with an ablation study showing that finetuning Llama-3 8B on the training set improves intent agent performance by 55%.

## Key Results
- AgentLite achieves highest success rate of 46% among four evaluated frameworks
- Intent prediction is the primary failure source (68-69% of failures)
- Finetuning the intent agent with Llama-3 8B improves performance by 55%
- Multi-agent orchestration strategies significantly impact results, with "think and react" patterns outperforming simple aggregation

## Why This Works (Mechanism)

### Mechanism 1
Extending traditional NLU entity slots to include backend execution parameters is necessary for creating executable benchmarks in personal assistant contexts. The dataset transforms SLURP from purely linguistic understanding to actionable schemas by relabeling slots to capture implicit requirements for backend execution, evaluating whether agents can bridge the gap between user intent and function call parameters.

### Mechanism 2
Multi-agent orchestration strategies significantly impact task success rates, with "think and react" patterns and separated prompts outperforming simple aggregation. Frameworks like AgentLite and AutoGen achieve higher success (44-46%) compared to LangGraph (32%) due to better context management and delegation logic.

### Mechanism 3
Systemic performance in multi-agent systems is likely bottlenecked by specific sub-components (intent prediction), and targeted finetuning of these components yields disproportionate system-wide gains. Error analysis identified the Intent Agent as the primary failure source, and replacing it with a finetuned Llama-3 8B model resulted in a 55% relative performance improvement.

## Foundational Learning

- **Concept: Intent Detection & Slot Filling (NLU)**
  - Why needed here: This is the primary failure point identified in the paper. You must understand how LLMs map "email john" to structured data (`intent: email`, `slots: {person: john}`) to debug the 69% failure rate.
  - Quick check question: Can you explain why an LLM might correctly identify an entity but fail to map it to the correct API slot structure?

- **Concept: Multi-Agent Orchestration (Manager-Worker Pattern)**
  - Why needed here: The benchmark evaluates how well a "Manager" agent delegates tasks to specialized workers (Time, URL, Request). Understanding delegation logic is key to optimizing the workflow.
  - Quick check question: What is the difference in context management between a manager who "aggregates results" vs one who "delegates subtasks"?

- **Concept: Simulated Environments & Mock APIs**
  - Why needed here: Auto-SLURP relies on "simulated servers" to verify execution. You need to grasp how to mock external services (Weather, Email) to test an agent's ability to perform function calls without real-world access.
  - Quick check question: How would you design a mock server to validate if a "Send Email" intent was successfully executed?

## Architecture Onboarding

- **Component map:** Program Manager → Intent Agent → Time/Location Agents → URL Agent → Request Agent
- **Critical path:** Input: User Query → Intent Analysis → Param Processing → Routing → Execution
- **Design tradeoffs:** Cost vs. Accuracy: LangGraph is cheaper ($0.14/query) but less accurate (32%), while AutoGen is more expensive ($0.80/query) but more robust (44%)
- **Failure signatures:** Intent Failure (69%), Formatting Failure (Time/Location), Orchestration Loop (Manager routing)
- **First 3 experiments:**
  1. Baseline Run: Run the provided Auto-SLURP workflow on GPT-4o to establish the "Intent Failure" rate locally
  2. Intent Finetuning: Fine-tune Llama-3-8B on the training set and substitute it into the Intent Agent slot
  3. Prompt Engineering Ablation: Compare "Aggregated Context" vs. "Separated Prompts" to measure context separation impact

## Open Questions the Paper Calls Out

### Open Question 1
What specific architectural improvements in generalized orchestration policies are required to exceed the current 46% success rate ceiling without relying on "think and react" heuristics? The paper states that achieving reliable assistants requires "continued progress in... the development of generalized orchestration policies" and robust reasoning approaches, but current frameworks struggle with delegation.

### Open Question 2
To what extent does the performance of multi-agent frameworks on Auto-SLURP's simulated servers correlate with performance in live, real-world application environments? The limitations section notes that "simulated servers... may not fully mimic the behavior of real-world systems," potentially causing discrepancies with live applications.

### Open Question 3
Can the "intent agent" bottleneck be resolved through in-context learning and reasoning alone, or is domain-specific finetuning strictly necessary for robust personal assistant performance? The paper identifies intent prediction as the primary failure source (up to 69% of errors) and shows finetuning improves performance by 55%, but does not test if advanced prompting could achieve similar gains.

## Limitations

- Relabeling process for execution-oriented slots lacks transparency and may not capture true execution complexity
- Framework implementation details are insufficient to explain significant performance differences between approaches
- Reliance on simulated servers and third-party APIs may overstate LLM performance by using idealized mocks

## Confidence

**High Confidence**: Identification of intent prediction as primary failure source (68-69% of failures) is well-supported by systematic error analysis and 55% improvement from finetuning.

**Medium Confidence**: Multi-agent orchestration strategies significantly impact performance, but lacks mechanistic detail to fully validate why specific approaches outperform others.

**Low Confidence**: Claim that Auto-SLURP captures full complexity of end-to-end personal assistant execution is poorly supported due to underspecified relabeling process and simulated server implementations.

## Next Checks

1. **Slot Relabeling Validation**: Compare original SLURP slots to Auto-SLURP's execution-oriented slots across 50 random samples to verify relabeling captures execution requirements.

2. **Framework Implementation Replication**: Implement LangGraph and AutoGen using only paper information (Appendix A), then measure performance gap between reported results and replication.

3. **Server Mock Fidelity Audit**: Test 20 Auto-SLURP queries against both simulated servers and real-world counterparts to measure accuracy gap and assess benchmark overstatement.