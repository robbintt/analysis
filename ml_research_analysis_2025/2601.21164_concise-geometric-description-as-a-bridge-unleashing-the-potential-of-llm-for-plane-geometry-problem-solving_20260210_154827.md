---
ver: rpa2
title: 'Concise Geometric Description as a Bridge: Unleashing the Potential of LLM
  for Plane Geometry Problem Solving'
arxiv_id: '2601.21164'
source_url: https://arxiv.org/abs/2601.21164
tags:
- shape
- line
- point
- segment
- geometric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a novel paradigm for Plane Geometry Problem
  Solving (PGPS) that bridges visual diagrams and textual descriptions using concise
  geometric descriptions. The authors observe that Large Language Models (LLMs) are
  powerful PGPS solvers when visual information is appropriately formulated as textual
  descriptions.
---

# Concise Geometric Description as a Bridge: Unleashing the Potential of LLM for Plane Geometry Problem Solving

## Quick Facts
- arXiv ID: 2601.21164
- Source URL: https://arxiv.org/abs/2601.21164
- Reference count: 40
- Proposes CDL-based paradigm for PGPS, achieving 85.7% accuracy on in-domain benchmarks with only 5.5k training data

## Executive Summary
This work introduces a novel two-stage approach for Plane Geometry Problem Solving (PGPS) that bridges visual diagrams and textual descriptions through concise geometric descriptions in Conditional Declaration Language (CDL). The authors demonstrate that Large Language Models (LLMs) can be powerful PGPS solvers when visual information is appropriately formulated as structured textual descriptions. By training a Multimodal LLM Interpreter to generate CDL representations and using an off-the-shelf LLM solver for reasoning, they achieve state-of-the-art performance while preserving the inherent reasoning capabilities of pre-trained LLMs.

## Method Summary
The method consists of two stages: First, a Multimodal LLM Interpreter (MLLM) is fine-tuned via CoT-augmented Supervised Fine-Tuning (SFT) followed by Group Relative Policy Optimization (GRPO) with CDL matching rewards. The MLLM generates concise geometric descriptions in CDL format from geometric diagrams and text captions. Second, a frozen LLM Solver reasons over the generated CDL to produce final answers. The CDL format uses three types of statements (ConsCDL for topological structure, ImgCDL for visual measurements, TextCDL for problem text) with fixed predicates, which constrains output and reduces the search space for learning. The GRPO stage uses dense, piece-wise CDL matching rewards that provide per-statement feedback, avoiding the sparse binary rewards of outcome-based approaches.

## Key Results
- Achieves 85.7% accuracy on in-domain benchmarks (Formalgeo7k-Rec-CoT) with only 5.5k training data
- Maintains 84.0% accuracy on out-of-domain benchmarks (Unigeo, MathVista)
- Outperforms leading open-source and closed-source MLLMs including Gemini2.5-Pro (81.8%) and Qwen2.5-VL (77.5%)
- Qwen3 30B LLM with ground truth CDL achieves 88.4%, demonstrating sufficiency of LLM reasoning when visual gap is bridged

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Concise structured representations reduce the search space for visual description generation, improving learnability with limited data
- **Evidence:** Expanding ConsCDL with redundant terms degrades validation accuracy from 83.2% to 80.5% despite higher training scores
- **Core assumption:** Geometric problems can be fully specified without redundant natural language descriptions
- **Break condition:** If geometric problems require information not expressible in CDL predicates, performance will plateau

### Mechanism 2
- **Claim:** Dense, piece-wise reward signals enable more effective RL than sparse outcome-based rewards
- **Evidence:** CDL matching rewards alone achieve 83.2% accuracy; adding solution-based rewards drops to 81.3%; solution-based reward alone shows no gain
- **Core assumption:** Accurate CDL statements correlate with correct problem solutions
- **Break condition:** If CDL statements have complex interdependencies where individual accuracy doesn't correlate with solution correctness

### Mechanism 3
- **Claim:** Decoupling visual perception from reasoning preserves pre-trained LLM reasoning capability
- **Evidence:** Qwen3 30B (LLM only) with GT CDL achieves 88.4% vs. Gemini2.5-Pro (MLLM) at 81.8%
- **Core assumption:** Base LLM already possesses sufficient reasoning capability; visual information access is the bottleneck
- **Break condition:** If problems require interleaved visual-reasoning steps, single-pass CDL extraction fails

## Foundational Learning

- **Concept: Conditional Declaration Language (CDL)**
  - **Why needed here:** Understanding the input/output format is prerequisite to any implementation work
  - **Quick check question:** Given a triangle ABC with ∠A=60°, what would the TextCDL entry look like? (Answer: `Equal(MeasureOfAngle(BAC),60)`)

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The RL stage uses GRPO, not standard PPO; differs in how advantages are computed from groups of samples
  - **Quick check question:** In GRPO, how is the advantage for a response computed? (Answer: Normalized deviation from group mean: A_i = (s_i - mean) / std)

- **Concept: Precision/Recall for Structured Outputs**
  - **Why needed here:** The CDL matching reward uses recall and precision as separate signals
  - **Quick check question:** If ground truth has 5 CDL statements and model generates 8 statements with 4 correct matches, what are recall and precision? (Answer: Recall = 4/5 = 0.8, Precision = 4/8 = 0.5)

## Architecture Onboarding

- **Component map:**
  ```
  [Geometric Diagram + Text Caption]
              ↓
  [MLLM Interpreter: Qwen2.5-VL / Qwen3-VL]
    - SFT with CoT-augmented data
    - GRPO with CDL matching rewards
              ↓
  [CDL Output: ConsCDL + ImgCDL + TextCDL]
              ↓
  [LLM Solver: Qwen3 30B — FROZEN]
              ↓
  [Final Answer]
  ```

- **Critical path:**
  1. SFT stage establishes CDL generation capability with CoT reasoning scaffolding
  2. GRPO stage refines output via CDL matching rewards (format, ConsCDL, ImgCDL, TextCDL)
  3. Inference: MLLM generates CDL → LLM reasons to answer

- **Design tradeoffs:**
  - CDL vs. natural language captions: CDL is less expressive but more learnable; natural language is flexible but requires more data
  - ConsCDL CoT only vs. full CoT: CoT for ImgCDL/TextCDL adds tokens without benefit
  - Unified MLLM vs. separate parsers: Unified model handles both diagram and text but trained on only 5.5k samples vs. prior work's 100k+ specialized data

- **Failure signatures:**
  - High training CDL scores but low validation accuracy → overfitting from insufficient conciseness
  - GRPO stage shows no improvement → check reward weight settings or rollout number
  - Correct CDL but wrong answers → LLM solver limitation or CDL-to-solution gap

- **First 3 experiments:**
  1. **Baseline SFT-only:** Train MLLM Interpreter with CoT-augmented SFT only, measure CDL generation and downstream solving accuracy
  2. **Reward ablation:** Run GRPO with subsets of rewards to verify each reward's contribution
  3. **Generalization test:** Train on Formalgeo7k-Rec-CoT, evaluate on Unigeo and MathVista without adaptation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the modular CDL-based paradigm be extended to solid geometry, analytic geometry, or other mathematical domains requiring multimodal reasoning?
- **Basis in paper:** The conclusion states the work may inspire research on complex multimodal reasoning tasks
- **Why unresolved:** CDL was designed specifically for plane geometry; 3D geometric primitives would require fundamentally different formalization
- **What evidence would resolve it:** A formal language for 3D geometry analogously structured to CDL, plus benchmark results on solid geometry datasets

### Open Question 2
- **Question:** How robust is the system when the MLLM Interpreter produces partially incorrect CDL—can error propagation be mitigated?
- **Basis in paper:** Table 2 shows ConsCDL precision/recall around 92-96% after RL, indicating ~4-8% of descriptions contain errors
- **Why unresolved:** No error analysis or failure mode study is provided; the paper does not explore self-correction mechanisms
- **What evidence would resolve it:** An ablation injecting controlled noise into CDL, measuring downstream solver degradation, or a proposed correction/verification module

### Open Question 3
- **Question:** How does the choice of LLM Solver affect overall performance, and is there a compute-performance trade-off?
- **Basis in paper:** Table 12 shows varied performance across solvers (79.4%–83.2%) using the same CDL, but only four LLMs were tested
- **Why unresolved:** The relationship between LLM solver scale/reasoning ability and final PGPS accuracy remains unquantified
- **What evidence would resolve it:** Systematic evaluation across a broader range of LLMs with analysis correlating LLM benchmark scores to PGPS accuracy

### Open Question 4
- **Question:** Can the manual dataset curation process be automated or scaled while maintaining annotation quality?
- **Basis in paper:** Formalgeo7k-Rec-CoT required "four qualified annotators" with "each piece of data independently reviewed by two annotators"
- **Why unresolved:** No discussion of semi-automated verification, synthetic data augmentation, or weak supervision alternatives is provided
- **What evidence would resolve it:** A proposed automated pipeline achieving comparable data quality metrics without human annotation

## Limitations

- Performance claims rely heavily on the quality and availability of Formalgeo7k-Rec-CoT, which is not publicly available
- Manual review process for constructing the dataset and exact CoT generation parser logic are not fully specified
- CDL matching algorithm details and exact prompt formulations for both the MLLM Interpreter and LLM Solver are only partially described

## Confidence

- **High Confidence:** Using concise structured representations (CDL) to reduce the search space for visual description generation is well-supported by empirical evidence showing degradation when redundant terms are added
- **Medium Confidence:** Dense, piece-wise reward signals (CDL matching rewards) are more effective than sparse outcome-based rewards, though solution-based rewards can still provide some benefit when combined
- **Medium Confidence:** Decoupling visual perception from reasoning preserves LLM capabilities, supported by comparative results showing superior performance of the two-stage approach

## Next Checks

1. **Reproduce CDL Generation Quality:** Implement the SFT stage using publicly available geometry datasets and evaluate CDL recall/precision metrics to verify that structured representations indeed improve learnability compared to natural language captions

2. **Validate Reward Design:** Conduct controlled experiments isolating CDL matching rewards versus solution-based rewards on a held-out validation set to confirm that per-statement feedback provides more effective training signals than binary outcome rewards

3. **Test Generalization Across Domains:** Evaluate the trained MLLM Interpreter on geometry problems from different domains (e.g., synthetic vs. real-world diagrams) to assess whether the CDL generation capability generalizes beyond the training distribution, particularly focusing on ConsCDL quality which appears most sensitive to data variations