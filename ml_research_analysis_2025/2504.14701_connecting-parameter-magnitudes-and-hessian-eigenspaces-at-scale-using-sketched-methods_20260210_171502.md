---
ver: rpa2
title: Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched
  Methods
arxiv_id: '2504.14701'
source_url: https://arxiv.org/abs/2504.14701
tags:
- hessian
- overlap
- learning
- parameters
- masks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work connects two independent observations in deep learning:
  (1) parameter magnitude pruning masks stabilize early in training, and (2) the top
  Hessian eigenspace also crystallizes early. To measure similarity between these
  subspaces, the authors develop a methodology using Grassmannian metrics, identifying
  "overlap" as the most interpretable and stable metric.'
---

# Connecting Parameter Magnitudes and Hessian Eigenspaces at Scale using Sketched Methods

## Quick Facts
- arXiv ID: 2504.14701
- Source URL: https://arxiv.org/abs/2504.14701
- Reference count: 40
- One-line primary result: This work connects parameter magnitude pruning masks and Hessian eigenspaces, showing their overlap is consistently above chance-level and increases with network size.

## Executive Summary
This work connects two independent observations in deep learning: (1) parameter magnitude pruning masks stabilize early in training, and (2) the top Hessian eigenspace also crystallizes early. To measure similarity between these subspaces, the authors develop a methodology using Grassmannian metrics, identifying "overlap" as the most interpretable and stable metric. They introduce SEIGH, a matrix-free sketched eigendecomposition algorithm, enabling computation of over 1000 Hessian eigenpairs for networks with 10M+ parameters—an unprecedented scale. Experiments reveal that overlap between parameter magnitude masks and top Hessian eigenspaces is consistently and substantially above chance-level, increasing with network size. This suggests larger parameters align with directions of larger loss curvature, providing new insights into Hessian structure and potential for approximating expensive Hessian quantities via cheap parameter inspection.

## Method Summary
The authors develop SEIGH, a matrix-free sketched eigendecomposition algorithm, to efficiently compute hundreds to thousands of Hessian eigenpairs at scale. They employ Grassmannian metrics to measure subspace similarity between parameter magnitude masks and Hessian eigenspaces, with "overlap" emerging as the most stable and interpretable metric. The methodology enables unprecedented analysis of Hessian structure in large networks, connecting parameter magnitudes to loss curvature directions through empirical measurement of subspace alignment.

## Key Results
- Overlap between parameter magnitude masks and top Hessian eigenspaces is consistently above chance-level across experiments
- This overlap increases with network size, suggesting larger networks show stronger alignment between parameter magnitude and curvature
- SEIGH enables computation of over 1000 Hessian eigenpairs for networks with 10M+ parameters, an order of magnitude improvement in scale

## Why This Works (Mechanism)
The observed alignment between parameter magnitudes and Hessian eigenspaces likely stems from optimization dynamics where SGD naturally concentrates updates along directions of high curvature in the loss landscape. As training progresses, parameters that contribute most to loss reduction (those aligned with large curvature directions) grow in magnitude relative to others. This creates a self-reinforcing relationship where large-magnitude parameters correspond to directions where the loss surface is steep, which are precisely the top Hessian eigenvectors. The early crystallization of both pruning masks and top eigenspaces suggests that the network identifies and stabilizes along these critical directions quickly during training.

## Foundational Learning
- **Hessian eigenspaces**: The eigenvectors of the Hessian matrix represent directions in parameter space along which the loss surface curvature is either positive (convex) or negative (concave). Why needed: Understanding these directions reveals the loss landscape geometry and identifies stable/unstable training directions. Quick check: Hessian eigenvectors with large eigenvalues correspond to directions where small parameter changes cause large loss changes.
- **Grassmannian metrics**: Mathematical tools for measuring distances and angles between subspaces in high-dimensional spaces. Why needed: Direct comparison of eigenspaces requires quantifying how much two sets of vectors span the same subspace. Quick check: The Grassmannian distance between identical subspaces is zero, while orthogonal subspaces have maximum distance.
- **Sketched eigendecomposition**: Approximate methods for computing eigenpairs using random projections to reduce computational complexity. Why needed: Full Hessian eigendecomposition is intractable for large networks due to O(n³) complexity. Quick check: Sketched methods trade some accuracy for massive scalability, enabling analysis of networks with millions of parameters.
- **Parameter magnitude pruning**: Removing network parameters based on their absolute values, typically keeping the largest. Why needed: Provides a simple, parameter-free pruning criterion that correlates with parameter importance. Quick check: Magnitude-based pruning often performs comparably to more complex criteria while being computationally trivial.
- **Subspace overlap**: A specific Grassmannian metric measuring the fraction of shared directions between two subspaces. Why needed: Provides an interpretable measure of how similar two eigenspaces are, ranging from 0 (orthogonal) to 1 (identical). Quick check: Overlap of 0.5 means the subspaces share half their directional content.
- **Matrix-free methods**: Algorithms that operate on linear operators without explicitly forming the full matrix. Why needed: The Hessian matrix is too large to store for modern networks (10M+ parameters → 10¹² entries). Quick check: Matrix-free Hessian-vector products can be computed via automatic differentiation in O(n) time.

## Architecture Onboarding

**Component Map**: SEIGH -> Hessian eigenpairs -> Grassmannian overlap -> Parameter magnitude masks -> Training dynamics

**Critical Path**: The essential workflow is: (1) compute Hessian eigenpairs using SEIGH, (2) extract parameter magnitude masks at various training stages, (3) measure Grassmannian overlap between these subspaces, (4) analyze how overlap evolves during training and scales with network size.

**Design Tradeoffs**: The authors chose sketched eigendecomposition over exact methods to achieve scalability, accepting some approximation error for the ability to analyze 10M+ parameter networks. They selected the overlap metric over alternatives like distance or projection because it proved more stable and interpretable across experiments, despite potentially losing some geometric information.

**Failure Signatures**: The methodology could fail if the Hessian is ill-conditioned (very small gap between top eigenvalues), making sketched methods unreliable. It might also break down for networks with plateaued training where the loss landscape becomes flat, reducing meaningful curvature structure. Additionally, if parameter magnitudes become uniformly distributed (no clear large/small distinction), the pruning masks would lose discriminative power.

**First Experiments**:
1. Run SEIGH on a small network (10K parameters) and compare its top eigenpairs against exact Hessian eigendecomposition to validate accuracy
2. Measure overlap between random parameter masks and Hessian eigenspaces as a baseline to confirm the observed overlap is above chance-level
3. Track how parameter magnitude masks and top Hessian eigenspaces evolve during early training (first 10% of epochs) to verify the claimed early crystallization

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on approximate sketched eigendecomposition may introduce errors, particularly for lower-magnitude eigenvalues
- The Grassmannian overlap metric, while stable and interpretable, remains a proxy for more direct subspace alignment measures
- The study focuses on feedforward networks, leaving generalization to transformers and other architectures untested

## Confidence
- Empirical findings (overlap consistently above chance, increasing with network size): **High**
- Methodological contributions (SEIGH, overlap metric selection): **Medium**
- Theoretical implications (why magnitude aligns with curvature, practical pruning utility): **Low**

## Next Checks
1. Apply SEIGH and overlap analysis to transformer architectures and other modern deep learning models to test generalizability
2. Compare pruning performance when using top Hessian eigenvectors versus parameter magnitude masks to quantify practical utility
3. Conduct ablation studies varying network width/depth and training regimes to isolate factors driving the magnitude-curvature alignment