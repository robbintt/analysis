---
ver: rpa2
title: 'CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare'
arxiv_id: '2512.11437'
source_url: https://arxiv.org/abs/2512.11437
tags:
- language
- high
- medical
- healthcare
- multilingual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "CLINIC introduces a comprehensive multilingual benchmark to evaluate\
  \ the trustworthiness of language models in healthcare across 15 languages and 6\
  \ key domains. It operationalizes five trustworthiness dimensions\u2014truthfulness,\
  \ fairness, safety, privacy, and robustness\u2014through 18 tasks and 28,800 expert-validated\
  \ samples."
---

# CLINIC: Evaluating Multilingual Trustworthiness in Language Models for Healthcare

## Quick Facts
- **arXiv ID:** 2512.11437
- **Source URL:** https://arxiv.org/abs/2512.11437
- **Reference count:** 40
- **Key outcome:** CLINIC introduces a comprehensive multilingual benchmark to evaluate the trustworthiness of language models in healthcare across 15 languages and 6 key domains.

## Executive Summary
CLINIC introduces a comprehensive multilingual benchmark to evaluate the trustworthiness of language models in healthcare across 15 languages and 6 key domains. It operationalizes five trustworthiness dimensions—truthfulness, fairness, safety, privacy, and robustness—through 18 tasks and 28,800 expert-validated samples. Evaluation of 13 models reveals significant performance gaps: models struggle with factual accuracy, demonstrate demographic and linguistic bias, and are vulnerable to privacy breaches and adversarial attacks. Performance degrades notably in low-resource languages. CLINIC provides a standardized, clinically grounded framework for assessing and improving the safety and global applicability of medical AI systems.

## Method Summary
CLINIC evaluates trustworthiness across five dimensions using 18 tasks with 28,800 samples from MedlinePlus and FDA sources, spanning 15 languages across high, mid, and low-resource tiers. The benchmark employs two-step multilingual prompting (English source → multilingual questions) with expert validation and GPT-4o-mini as judge. Thirteen models including proprietary, open-weight, and medical LLMs are assessed using task-specific metrics: accuracy for hallucination, semantic similarity for consistency, RtA rates for safety/privacy, toxicity scores, and leak rates. Performance is analyzed by language tier, domain, and model type.

## Key Results
- Model performance degrades systematically from high-resource to low-resource languages across all trustworthiness dimensions.
- Medical LLMs underperform general-purpose models on trustworthiness metrics despite domain specialization.
- Expert-validated benchmark design successfully exposes real-world vulnerabilities in safety, privacy, and fairness.

## Why This Works (Mechanism)

### Mechanism 1: Language Resource Level Drives Performance Degradation
- **Claim:** Model performance on trustworthiness metrics degrades systematically as language resource level decreases (HR > MR > LR), particularly for honesty, fairness, and privacy tasks.
- **Mechanism:** High-resource languages have more training data and safety alignment exposure; low-resource languages lack sufficient representation during pre-training and instruction tuning, leading to weaker in-context learning and safety responses.
- **Core assumption:** The observed performance gradient reflects genuine capability gaps rather than benchmark artifact.
- **Evidence anchors:**
  - [abstract]: "Performance degrades notably in low-resource languages."
  - [Section 3.6]: "On average, across all tasks, performance follows HR > MR > LR... models perform best on high-resource languages, degrade in mid-resource, and drop sharply in low-resource."
  - [Table 2]: Honesty scores drop from ~94% (HR) to ~90% (LR) for Gemini-2.5-Pro; from ~94% to ~85% for DeepSeek-R1-LLaMA.
  - [corpus]: Related work "Language Models' Factuality Depends on the Language of Inquiry" corroborates that knowledge recall is language-dependent, supporting the cross-lingual transfer hypothesis.
- **Break condition:** If future models show uniform performance across resource tiers, this mechanism would be weakened, suggesting the gap is addressable through training data scaling.

### Mechanism 2: Domain-Specific Medical Models Underperform on Trustworthiness
- **Claim:** Medical LLMs (OpenBioLLM, UltraMedical, MMedLLama) often underperform general-purpose models on trustworthiness dimensions, especially truthfulness and robustness.
- **Mechanism:** Medical fine-tuning optimizes for clinical QA accuracy but may overfit narrow domain data, reducing generalization and safety alignment; smaller parameter counts in some medical models further limit capability.
- **Core assumption:** Underperformance reflects training data and objective mismatch, not benchmark design.
- **Evidence anchors:**
  - [Figure 3]: Medical models show lower hallucination accuracy compared to proprietary and large open-weight models.
  - [Section 3.6]: "medical LLMs hallucinate more and struggle with robustness despite domain training."
  - [Section 3.1]: "medical models generally underperform compared to large open-weight and proprietary models."
  - [corpus]: Weak direct evidence; related surveys don't specifically compare medical vs. general model trustworthiness.
- **Break condition:** If medical models consistently match or exceed general models across all trustworthiness dimensions with similar scale, this mechanism would be refuted.

### Mechanism 3: Expert-Validated Benchmark Design Reveals Real-World Vulnerabilities
- **Claim:** Expert validation and two-step multilingual prompting improve the clinical validity and linguistic quality of benchmark samples, exposing genuine model weaknesses.
- **Mechanism:** Healthcare professionals rate samples for trustworthiness relevance (avg. 3.9/5); two-step prompting grounds multilingual questions in English source passages, improving translation-task quality from 2.5→3.1 (Bengali) and 2.9→3.2 (Hindi).
- **Core assumption:** Expert ratings and native speaker evaluations correlate with actual clinical utility and linguistic fidelity.
- **Evidence anchors:**
  - [Section 2]: "Both doctors consistently rated our trustworthiness dimensions with an average score of 3.9, with an interannotator agreement (Cohen's kappa) of 0.82."
  - [Appendix G]: Translation quality improved with two-step prompting for Bengali (2.5→3.1), Hindi (2.9→3.2), Nepali (4.1→4.25).
  - [Figure 10]: Inter-rater agreement for most tasks ranges from 0.64–0.89, indicating consistent expert judgments.
  - [corpus]: No direct external validation; reliance on internal expert review is a stated limitation.
- **Break condition:** If models show uniformly high performance across all CLINIC tasks without mitigation, this could suggest the benchmark is insufficiently challenging or misaligned with real clinical scenarios.

## Foundational Learning

- **Concept: High/Mid/Low-Resource Languages**
  - **Why needed here:** CLINIC stratifies languages into resource tiers based on training data availability; understanding this distinction is essential for interpreting performance gaps and designing mitigation.
  - **Quick check question:** What distinguishes Swahili (low-resource) from Spanish (high-resource) in the context of LLM training data?

- **Concept: Trustworthiness Dimensions in Healthcare AI**
  - **Why needed here:** The benchmark evaluates five dimensions—truthfulness, fairness, safety, privacy, robustness—each operationalized via specific tasks; understanding these definitions is prerequisite to interpreting results.
  - **Quick check question:** How does "honesty" differ from "truthfulness" in CLINIC's framework?

- **Concept: Adversarial vs. Robustness Testing**
  - **Why needed here:** The benchmark distinguishes adversarial attacks (intentional perturbations) from robustness tests (natural input variation); this distinction informs defense strategy.
  - **Quick check question:** Is "colloquial phrasing" considered an adversarial attack or a robustness test?

## Architecture Onboarding

- **Component map:** MedlinePlus → FDA drug documents → LLM-generated questions (open-ended, MCQ, masked-token) → Expert validation → Two-step multilingual prompting → 28,800 samples across 18 tasks → 13 models → 15 languages → 6 domains → 5 trustworthiness dimensions → Metrics: Accuracy, RtA, similarity, neutrality, leak rate, toxicity → GPT-4o-mini judgment

- **Critical path:** Start with hallucination (FCT, FQT, NOTA) and jailbreak tasks—these show largest inter-model gaps and are most clinically consequential. Then proceed to fairness (stereotype, disparagement) and privacy leakage.

- **Design tradeoffs:**
  - **Breadth vs. depth:** 15 languages × 18 tasks × 6 domains provides coverage but limits sample depth per task-language-domain combination.
  - **Automation vs. human validation:** LLM-based generation with GPT-4o judgment is scalable but introduces model-in-the-loop bias; expert validation is limited to subsets (stated limitation).
  - **Language representation:** Uniform sampling (≈1,920 per language) ensures fairness but may underweight clinically prevalent languages in specific regions.

- **Failure signatures:**
  - **Low OOD RtA for medical LLMs:** Models answer confidently about fictional 2025 drugs, indicating calibration issues.
  - **High privacy leak rates in QwQ-32B (85–87%):** Reveals failure to withhold personal identifiers even in indirect prompts.
  - **Steep LR performance drops:** Consistent 5–15% degradation across truthfulness, fairness, privacy for LR languages.

- **First 3 experiments:**
  1. **Baseline your model on hallucination-FCT and jailbreak-PAIRS across 3 languages** (English, Bengali, Swahili) to establish HR vs. LR gaps.
  2. **Test privacy leakage with indirect prompts** (e.g., "logistical considerations for support group") to measure leak rate; compare with GPT-4o-mini as reference.
  3. **Evaluate fairness-stereotype and fairness-disparagement** on a subset of tasks to quantify bias; if neutrality rate <40% or disparagement RtA <0.5, flag for safety alignment intervention.

**Assumption:** These experiments assume access to CLINIC's evaluation scripts and sample data (GitHub referenced). If unavailable, replicate core tasks using provided prompt templates and metric definitions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can fine-tuning or reinforcement learning methods be adapted to specifically mitigate the hallucination and privacy vulnerabilities observed in low-resource languages?
- Basis in paper: [explicit] The authors explicitly state that "it does not propose concrete remediation techniques" and identify developing mitigation strategies as a key direction for future work.
- Why unresolved: The current paper focuses on diagnosing failures rather than implementing or validating fixes.
- What evidence would resolve it: A study demonstrating improved CLINIC scores in low-resource languages following the application of specific safety alignment or DPO-based remediation techniques.

### Open Question 2
- Question: How do truthfulness and safety metrics shift when extending evaluation to multilingual multimodal healthcare settings compared to the current text-only framework?
- Basis in paper: [explicit] The authors plan to "evaluate healthcare models in settings that combine text and images across multiple languages" in future work.
- Why unresolved: The current version of CLINIC is restricted to text-based evaluation, leaving multimodal trustworthiness unexplored.
- What evidence would resolve it: A multimodal extension of the CLINIC benchmark revealing performance discrepancies in tasks involving medical imaging or visual data across different languages.

### Open Question 3
- Question: To what extent does the exclusive reliance on GPT-4o as an automated judge introduce bias or error in evaluating open-ended responses for low-resource languages?
- Basis in paper: [inferred] The limitations section highlights the "Dependence on GPT-4o for grading" and notes that "a comprehensive human evaluation for all 15 languages remains pending."
- Why unresolved: Without ground-truth human evaluation across all languages, the validity of the automated metric remains uncertain for linguistic contexts where LLM judges may be less reliable.
- What evidence would resolve it: A correlation analysis comparing GPT-4o evaluation scores with human expert judgments for all 15 languages included in the benchmark.

## Limitations

- Evaluation framework relies on LLM-based assessment, introducing model-in-the-loop bias.
- Expert validation was limited to subsets of samples, leaving full clinical validity partially unverified.
- Performance gaps across language tiers may partly reflect benchmark design rather than genuine capability differences.

## Confidence

- **High confidence:** Performance degrades across trustworthiness dimensions as language resource level decreases.
- **Medium confidence:** Medical LLMs underperform general models on trustworthiness, requiring careful disentanglement of model scale effects.
- **Medium confidence:** Benchmark's clinical validity due to expert validation scores and inter-rater agreement, though external validation is limited.

## Next Checks

1. **Replicate the core HR vs. LR performance gap** using three representative tasks (hallucination-FCT, fairness-stereotype, privacy leakage) across English, Bengali, and Swahili to verify the resource tier degradation pattern.

2. **Test GPT-4o judge consistency** by running the same sample set through GPT-4o-mini as secondary evaluator, comparing RtA and similarity scores to quantify potential model-in-the-loop bias.

3. **Validate medical vs. general model performance** by controlling for parameter count (e.g., comparing OpenBioLLM-8B against Qwen-7B variants) while evaluating hallucination and robustness tasks to isolate domain-specific effects.