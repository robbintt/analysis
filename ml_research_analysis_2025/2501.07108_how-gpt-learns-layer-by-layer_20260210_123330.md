---
ver: rpa2
title: How GPT learns layer by layer
arxiv_id: '2501.07108'
source_url: https://arxiv.org/abs/2501.07108
tags:
- feature
- features
- linear
- layer
- tile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes how OthelloGPT, a GPT-based model trained on
  Othello gameplay, constructs internal world models layer by layer. The authors compare
  Sparse Autoencoders (SAEs) and linear probes to interpret the model's learned representations,
  finding that SAEs provide more robust and disentangled insights into compositional
  features.
---

# How GPT learns layer by layer

## Quick Facts
- arXiv ID: 2501.07108
- Source URL: https://arxiv.org/abs/2501.07108
- Authors: Jason Du; Kelly Hong; Alishba Imran; Erfan Jahanparast; Mehdi Khfifi; Kaichun Qiao
- Reference count: 14
- This paper analyzes how OthelloGPT constructs internal world models layer by layer using SAEs and linear probes.

## Executive Summary
This paper analyzes how OthelloGPT, a GPT-based model trained on Othello gameplay, constructs internal world models layer by layer. The authors compare Sparse Autoencoders (SAEs) and linear probes to interpret the model's learned representations, finding that SAEs provide more robust and disentangled insights into compositional features. They observe a hierarchical progression in feature learning, where early layers capture static attributes like board edges, while deeper layers reflect dynamic aspects such as tile flips and board state changes.

## Method Summary
The study trains an 8-layer decoder-only OthelloGPT on next-token prediction for Othello moves. Linear probes and Sparse Autoencoders are then trained on residual stream activations from each layer. SAEs use L1 regularization to enforce sparsity, while linear probes classify tile states (empty/own/enemy). Features are evaluated using AUROC scores, with stability features requiring higher thresholds. Analysis aggregates results across 10 random seeds to assess feature robustness.

## Key Results
- SAEs provide more robust, disentangled insights into compositional features than linear probes
- Hierarchical feature learning observed: early layers capture static attributes like board edges, deeper layers encode dynamic state changes
- Tile stability features emerge most prominently in intermediate layers (layers 2-4)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models construct internal world models through hierarchical layer-wise feature progression, with early layers capturing static structural attributes and deeper layers encoding dynamic state changes.
- Mechanism: The residual stream accumulates representations across layers. Early transformer blocks attend to and encode positional/structural features (board edges, corners). Later blocks compose these with attention to state transitions (tile flips, color changes), building progressively abstract representations.
- Core assumption: This hierarchical progression reflects a general learning dynamic in transformers, not just OthelloGPT-specific behavior.
- Evidence anchors:
  - [abstract] "Early layers capture static attributes like board edges, while deeper layers reflect dynamic tile changes."
  - [section 4.2] "SAEs show more dynamic changes, with activations concentrated in the central tiles and along the edges... capturing the evolving dynamics of central tiles, which tend to flip frequently"
  - [corpus] "Echoes of BERT" paper confirms similar layer-wise linguistic abstraction patterns in transformers generally

### Mechanism 2
- Claim: Sparse Autoencoders (SAEs) disentangle compositional features more effectively than linear probes by enforcing sparsity constraints that distribute representations across interpretable basis vectors.
- Mechanism: SAEs minimize reconstruction error plus L1 penalty on latent activations (λ∥h∥₁). This forces the encoder to represent each input as a sparse combination of basis vectors, making individual features more monosemantic. Linear probes only find features that are linearly separable for classification, missing compositional structure.
- Core assumption: Disentangled sparse features correspond to meaningful concepts in the model's world representation.
- Evidence anchors:
  - [section 2.1] "By penalizing activation magnitude, SAEs force the model to distribute its representations sparsely across the basis vectors which can make features more interpretable"
  - [section 4] "SAEs offer more robust, disentangled insights into compositional features, whereas linear probes mainly detect features useful for classification"
  - [corpus] Weak direct evidence—no corpus papers directly validate SAE disentanglement claims for compositional reasoning

### Mechanism 3
- Claim: Tile stability features—representing strategic permanence—emerge most prominently in intermediate layers (layers 2-4), suggesting concept-specific depth localization.
- Mechanism: Stability (tiles that cannot be flipped) requires integrating multiple board attributes: corner position, edge adjacency, surrounding tile states. Intermediate layers have accumulated enough structural information while retaining enough representational capacity before later layers focus on output prediction.
- Core assumption: Feature emergence at specific depths reflects functional specialization, not training noise.
- Evidence anchors:
  - [section 4.3] "We notice the highest frequency of tile-feature activations in the intermediate layers (layers 2 through 4)... Earlier (layer 1) and later layers (layers 5 through 8) do not appear to learn stability"
  - [section 4.3/Table 1] Features 349 and 108 show concentrated stability-related activations in layers 2-4, minimal in others

## Foundational Learning

- Concept: **Sparse Autoencoders and L1 Regularization**
  - Why needed here: SAEs are the primary interpretability tool. Understanding why sparsity enables disentanglement is essential for evaluating their results.
  - Quick check question: Given an SAE with reconstruction loss + λ∥h∥₁, what happens to feature interpretability if λ → 0? If λ → ∞?

- Concept: **Linear Probes and Representational Accessibility**
  - Why needed here: The paper contrasts SAEs with linear probes. Understanding what linear probes actually measure (linear separability, not mechanistic causation) clarifies why they miss compositional features.
  - Quick check question: If a linear probe achieves 95% accuracy on tile classification at layer 6, what can you conclude? What can you NOT conclude?

- Concept: **Residual Stream Accumulation in Transformers**
  - Why needed here: The paper extracts features from the residual stream after each transformer block. Understanding that this stream accumulates (rather than replaces) information explains why early features remain accessible.
  - Quick check question: In a standard transformer, does the residual stream at layer 4 contain information from layer 1? How does this relate to the hierarchical feature hypothesis?

## Architecture Onboarding

- Component map:
  - OthelloGPT (8-layer decoder-only transformer, 8-head attention, d=512) -> Residual stream extraction after each transformer block -> Linear probes (64-class classification) + SAEs (reconstruction + L1 penalty) -> AUROC feature evaluation -> Visualization as board heatmaps

- Critical path:
  1. Train OthelloGPT on next-token prediction with random valid Othello moves
  2. Extract residual stream activations at each layer for a dataset of board states
  3. Train linear probes and SAEs separately on these activations
  4. For SAEs: identify features with AUROC > 0.7-0.8 for tile color/stability prediction
  5. Aggregate across seeds to find robust features; visualize as board heatmaps

- Design tradeoffs:
  - **SAE sparsity (λ)**: Higher λ → more interpretable but potentially incomplete reconstruction
  - **Number of SAE latent features**: Too few → collapse; too many → redundant/diffuse features
  - **Seed variability**: Paper shows instability across seeds (Tables 1 vs 2); multiple seeds essential for robustness
  - **AUROC threshold**: 0.7 used for tile color, 0.8 for stability—threshold choice affects which features "count"

- Failure signatures:
  - SAE features vary wildly across seeds → features may be artifacts, not model structure
  - Linear probe accuracy flat across layers → model may not be learning hierarchical representations
  - Stability features appear only in layer 1 or layer 8 → contradicts intermediate-layer hypothesis
  - SAE reconstruction loss high → sparsity constraint too aggressive, losing information

- First 3 experiments:
  1. **Baseline probe accuracy**: Train linear probes on all 8 layers for tile classification (empty/own/enemy). Verify accuracy increases with depth (replicate Figure 2).
  2. **SAE feature extraction with seed comparison**: Train SAEs on layers 1-4 with 3+ random seeds. Identify top AUROC features per layer. Check if corner/edge patterns replicate across seeds.
  3. **Stability feature localization**: Compute stability maps for 2000 games. For each SAE feature, compute AUROC for predicting tile stability. Confirm intermediate-layer concentration (layers 2-4 show highest stable-feature counts).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed hierarchical progression from static to dynamic features generalize to larger language models and non-game sequential tasks?
- Basis in paper: [explicit] The authors explicitly state in the Future Work section the need to "extend this analysis to other board game models and LLMs" to determine if the hierarchical feature learning is consistent across tasks.
- Why unresolved: The current study is confined to a small, 8-layer OthelloGPT, limiting the ability to claim universality for the internal representations of larger transformers.
- Evidence: Replicating the SAE and linear probe framework on standard large language models or distinct sequential reasoning tasks to see if early layers consistently capture static attributes.

### Open Question 2
- Question: Do the disentangled features identified by SAEs causally influence the model's move selection, or are they merely correlated epiphenomena?
- Basis in paper: [explicit] The Conclusion notes that "Attribution analysis... could be applied to identify causal features that directly influence move selection."
- Why unresolved: The paper establishes correlations between features and board states (probing), but does not verify if these features are functionally used by the model to predict moves.
- Evidence: Intervention studies (e.g., activation patching or ablation) where specific features are suppressed or amplified to observe changes in the model's next-token prediction accuracy.

### Open Question 3
- Question: Is "tile stability" a primitive feature learned directly by the network, or is it an emergent composite of simpler features like corner and edge configurations?
- Basis in paper: [inferred] In Section 4.3, the authors note that variability across seeds suggests "the features we interpret as 'stability' may be a composition of related but more granular features."
- Why unresolved: The instability of specific feature indices across different seeds implies the model may not have a dedicated "stability" neuron, but rather constructs the concept from sub-components.
- Evidence: Isolating and perturbing corner-detection features to determine if the "stability" feature activations logically degrade or disappear as a result.

## Limitations

- Study focuses on a single 8-layer OthelloGPT, limiting generalizability to larger models
- SAE feature stability across seeds shows significant variability, suggesting potential artifacts
- No external validation of SAE disentanglement effectiveness or game-specific concept localization in other domains

## Confidence

**High confidence**: Linear probe accuracy increases with layer depth, confirming basic hierarchical information accumulation.

**Medium confidence**: The hierarchical feature progression (static → dynamic) reflects genuine model learning rather than artifacts.

**Low confidence**: SAE-identified features represent truly disentangled compositional concepts rather than coincidental correlations.

## Next Checks

1. **Cross-seed stability test**: Train SAEs on 10+ random seeds for layers 1-4. Compute Jaccard similarity of top-50 AUROC features per seed. If overlap <30%, the features may not represent robust model structure.

2. **Ablation causal validation**: Use feature activation ablation to test whether identified SAE features causally influence model outputs. If removing high-AUROC features doesn't change next-token predictions, the features may be epiphenomenal rather than functional.

3. **Scale-up verification**: Apply the same SAE analysis to a 24-layer OthelloGPT. If stability features still concentrate in intermediate layers, this supports the concept-specific depth localization hypothesis beyond the 8-layer case.