---
ver: rpa2
title: 'Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs'
arxiv_id: '2503.23219'
source_url: https://arxiv.org/abs/2503.23219
tags:
- reasoning
- arxiv
- preprint
- aurelia
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AURELIA, a test-time reasoning distillation
  framework for audio-visual large language models (AVLLMs) that enhances their reasoning
  capabilities without requiring additional training or fine-tuning. The method employs
  an actor-critic multi-agent system to iteratively generate structured, step-by-step
  reasoning data, which is then distilled into AVLLMs during inference.
---

# Aurelia: Test-time Reasoning Distillation in Audio-Visual LLMs

## Quick Facts
- arXiv ID: 2503.23219
- Source URL: https://arxiv.org/abs/2503.23219
- Authors: Sanjoy Chowdhury; Hanan Gani; Nishit Anand; Sayan Nag; Ruohan Gao; Mohamed Elhoseiny; Salman Khan; Dinesh Manocha
- Reference count: 40
- One-line primary result: AURELIA achieves up to 100% relative improvement in AVLLM reasoning performance through test-time distillation without training.

## Executive Summary
This paper introduces AURELIA, a test-time reasoning distillation framework that enhances audio-visual large language models (AVLLMs) by generating structured reasoning steps through an actor-critic multi-agent system. The method iteratively refines reasoning until alignment thresholds are met, then distills this reasoning into target AVLLMs during inference. To evaluate AVLLMs comprehensively, the authors present AVReasonBench, a challenging benchmark with 4,500 audio-visual questions across six tasks including the novel AV-GeoIQ task testing geographical and cultural reasoning. Experiments on 18 AVLLMs demonstrate significant reasoning limitations, with AURELIA achieving up to 100% relative improvement in performance.

## Method Summary
AURELIA employs a four-agent system where a Reasoning Generator produces step-by-step reasoning from audio-visual inputs, a Summarizer creates concise captions, an Evaluator scores reasoning quality (1-10), and Feedback guides iterative refinement until alignment thresholds are met. The optimal reasoning is then prepended to target AVLLM inputs during inference, enabling reasoning enhancement without weight updates. The framework uses AVReasonBench for evaluation, comprising 4,500 samples across tasks like Music-AVQA, AVSD, and the novel AV-GeoIQ for geographical reasoning. Closed-source models (Gemini, GPT-4o) serve as reasoning agents, with ablation studies showing performance gains across 15 open-source AVLLMs.

## Key Results
- AURELIA achieves up to 100% relative improvement on AV-GeoIQ task (17.2→34.0 accuracy)
- Consistent gains across all 15 tested open-source AVLLMs, with X-InstructBLIP showing +100% improvement
- Iterative refinement shows convergence benefits: AV-GeoIQ improves from 27.5→38.0 over 5 iterations
- AVReasonBench reveals significant reasoning limitations in current AVLLMs, with baseline models failing on cross-modal reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Actor-Critic Iterative Refinement
Iterative feedback cycles improve reasoning quality until alignment threshold is met. Reasoning Generator produces steps → Summarizer synthesizes caption → Evaluator scores alignment (1-10) → Feedback guides regeneration. Loop continues until score ≥ τ or iterations exhausted. Core assumption: Caption quality positively correlates with reasoning correctness; evaluator scores are reliable proxies for reasoning utility. Evidence anchors: [Section 3.2] specifies termination condition e(t) ≥ β; [Table 5] shows performance increases from iteration 1→5 (e.g., AV-GeoIQ: 27.5→38.0); weak direct corpus evidence for this specific mechanism.

### Mechanism 2: Test-Time Reasoning Injection
Prepending structured reasoning steps to model input improves target AVLLM accuracy without weight updates. Optimal reasoning r* is concatenated with (v, a, q) as context, conditioning M to follow logical pathways rather than relying on implicit associations. Core assumption: Target model has sufficient context window and instruction-following capability to utilize injected reasoning. Evidence anchors: [Abstract] states "achieves up to a 100% relative improvement"; [Table 1] shows X-InstructBLIP +100% on AV-GeoIQ; SightSound-R1 provides convergent evidence for cross-modal reasoning distillation benefits.

### Mechanism 3: Cross-Modal Bias Mitigation via Structured Decomposition
Explicit step-by-step reasoning reduces over-reliance on dominant modalities (visual or auditory). Reasoning steps force sequential attention to specific audio cues, visual elements, and their relationships, preventing premature conclusions from salient single-modality features. Core assumption: Reasoning generator agents have superior multi-modal grounding than target AVLLMs. Evidence anchors: [Figure 1] shows model incorrectly associates instrument with Japan due to visual bias; AURELIA corrects via structured reasoning; AVTrustBench identifies reliability issues supporting bias mitigation motivation.

## Foundational Learning

- Concept: **Actor-Critic Reinforcement Learning**
  - Why needed here: AURELIA's iterative refinement is structurally identical to actor-critic RL, with Evaluator as critic providing scalar rewards.
  - Quick check question: Can you explain why a scalar evaluation score enables gradient-free optimization through iteration?

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: Reasoning steps r* are CoT-style decompositions; understanding CoT transfer is essential for troubleshooting quality.
  - Quick check question: What makes a reasoning chain "structured" vs. unstructured rambling?

- Concept: **Multi-Modal Fusion Architectures**
  - Why needed here: Target AVLLMs must integrate audio, video, and text encoders; knowing where reasoning injection occurs (typically at LLM backbone) is critical.
  - Quick check question: At which layer would you inject reasoning text—encoder, projector, or LLM backbone?

## Architecture Onboarding

- Component map: Input (v, a, q) → [Reasoning Generator R] → r = {steps, justification, answer} → [Summarizer S] → caption c → [Evaluator E] → score e ∈ [1,10] → [Feedback F] → if e < τ: regenerate r → Final: M(v, a, q, r*) → answer s*
- Critical path: Evaluator quality → feedback signal quality → reasoning convergence. Use strong multi-modal models for E (paper uses Gemini 1.5 Pro).
- Design tradeoffs: Higher threshold τ → better reasoning but more iterations/latency (Table 6: τ=10 takes 65.81s vs τ=4 at 23.90s); closed-source agents → higher quality but cost and API dependency; open-source agents → lower cost but ~5-10% accuracy drop.
- Failure signatures: Single-modality dominance in reasoning (Figure 4 left: "dog is silent" assumption); temporal dynamics misunderstanding (Figure 4 right: fails to spot second-highest bass instrument); choice extraction failures requiring GPT-4 fallback.
- First 3 experiments: 1) Baseline sanity check: Run zero-shot AVLLM on AVReasonBench subset (100 samples) without reasoning injection; establish floor accuracy. 2) Iteration ablation: Test T={1,3,5} with fixed τ=8 on single task; verify convergence behavior matches Table 5. 3) Agent combination test: Compare all-Gemini vs. Gemini+GPT-4o hybrid (per Table 4) to confirm multi-model synergy; expect 2-4% gain.

## Open Questions the Paper Calls Out

### Open Question 1
Can open-source LLMs effectively replace proprietary models (e.g., Gemini, GPT-4o) as agents within the AURELIA framework without significant degradation in reasoning quality? [explicit] Section L (Future Work) explicitly states, "A promising future direction would be to replace these proprietary models with open-source alternatives, enhancing accessibility and transparency." Why unresolved: The current implementation relies heavily on the superior multimodal comprehension of closed-source models (Table 4); it is unclear if current open-source models possess sufficient instruction-following and reasoning capabilities to act as effective generators and evaluators in this loop. What evidence would resolve it: Comparative benchmarks running AURELIA using strong open-source models (e.g., LLaMA-3, Qwen) as agents versus the current closed-source baseline on AVReasonBench.

### Open Question 2
Can integrating reasoning capabilities directly into the training phase yield better generalization than test-time distillation? [explicit] Section L (Future Work) suggests "integrating reasoning directly into the training or instruction-tuning phase" so AVLLMs can "inherently develop step-by-step reasoning capabilities." Why unresolved: The current work focuses exclusively on test-time optimization. It remains unknown if training on the generated reasoning data would allow the model to internalize the logic, thereby reducing inference latency and error rates compared to the multi-agent inference pipeline. What evidence would resolve it: Fine-tuning an AVLLM on the reasoning data generated by AURELIA and comparing its zero-shot performance and inference speed against the test-time distillation method.

### Open Question 3
How can the actor-critic feedback mechanism be made robust against initial perceptual errors or modality biases in the Reasoning Generator? [inferred] Section 5.6 (Failure Cases) notes that "an error in interpreting the animal sounds... propagates through the reasoning steps," and Fig. 4 shows the system failing to correct "suboptimal comprehension" of dynamics. Why unresolved: The evaluator relies on the generated caption's coherence with the video/audio. If the generator hallucinates a plausible but incorrect event, the evaluator might reinforce the error rather than correct the initial perception. What evidence would resolve it: An ablation study measuring the correction rate of the feedback loop specifically on samples where the initial reasoning step contains a factual error regarding audio or visual content.

## Limitations
- Dataset validity: AVReasonBench's 4,500 samples across six tasks are novel, but the AV-GeoIQ task introduces cultural/linguistic biases that may not generalize beyond the curated dataset.
- Multi-agent dependency: Performance gains hinge entirely on the quality of closed-source reasoning agents (Gemini, GPT-4o), without open-source ablation studies to verify framework efficacy.
- Evaluation gap: The paper uses accuracy/BLEU/CIDEr metrics but doesn't assess whether reasoning steps themselves are logically sound, conflating correlation with causation in the distillation process.

## Confidence
- **High confidence**: The core mechanism of test-time reasoning injection (Mechanism 2) is well-supported by ablation studies showing consistent gains across 15 open-source models. The 100% relative improvement on AV-GeoIQ is compelling given the controlled experimental setup.
- **Medium confidence**: Iterative refinement through actor-critic loops (Mechanism 1) shows expected convergence patterns in Table 5, but the correlation between evaluator scores and actual reasoning quality remains unverified without human evaluation of the reasoning steps themselves.
- **Low confidence**: Cross-modal bias mitigation claims (Mechanism 3) rely heavily on anecdotal failure cases in Figure 4 rather than systematic analysis of modality-specific errors across the full benchmark.

## Next Checks
1. **Human evaluation of reasoning quality**: Recruit annotators to rate the logical coherence and cross-modal grounding of reasoning steps r* generated by AURELIA, independent of final answer accuracy.
2. **Open-source agent ablation**: Replace Gemini/GPT-4o agents with comparable open-source models (e.g., LLaVA-1.5 for reasoning generation) while keeping the distillation framework constant.
3. **Generalization stress test**: Apply AURELIA to out-of-distribution audio-visual tasks not in AVReasonBench (e.g., emergency vehicle detection from audio-visual cues, or cross-cultural gesture recognition).