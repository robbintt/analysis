---
ver: rpa2
title: 'UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task
  Adaptation'
arxiv_id: '2510.24262'
source_url: https://arxiv.org/abs/2510.24262
tags:
- data
- synthetic
- training
- optimization
- utility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a utility-centric data augmentation framework
  that shifts focus from optimizing intrinsic visual attributes to enhancing task-specific
  utility of synthetic data. The method introduces a weight allocation network to
  efficiently evaluate the task-specific utility of generated samples, then uses this
  feedback to guide a dual-level optimization process: model-level generation capability
  refinement via Direct Preference Optimization, and instance-level generation policy
  tuning of prompts and noise.'
---

# UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation

## Quick Facts
- **arXiv ID**: 2510.24262
- **Source URL**: https://arxiv.org/abs/2510.24262
- **Reference count**: 40
- **Primary result**: 3.87% average accuracy improvement over SOTA across eight benchmark datasets

## Executive Summary
This paper proposes a utility-centric data augmentation framework that shifts focus from optimizing intrinsic visual attributes to enhancing task-specific utility of synthetic data. The method introduces a weight allocation network to efficiently evaluate the task-specific utility of generated samples, then uses this feedback to guide a dual-level optimization process: model-level generation capability refinement via Direct Preference Optimization, and instance-level generation policy tuning of prompts and noise. Extensive experiments on eight benchmark datasets show that the proposed approach achieves consistent gains across multiple architectures and demonstrates strong scalability.

## Method Summary
The framework operates in three stages: (1) Task-Oriented Data Valuation using a meta-learned weight network (MLP with 100-unit hidden layer) that predicts sample utility based on classification loss; (2) Model-Level Optimization applying Direct Preference Optimization to fine-tune the diffusion U-Net using utility-ranked preference pairs; (3) Instance-Level Optimization of prompt embeddings via gradient ascent and noise vectors via asymmetric CFG scale manipulation. The method generates 500 synthetic images per class and trains downstream classifiers (ResNet-50/WideResNet) with standard SGD optimization.

## Key Results
- Achieves 3.87% average accuracy improvement over state-of-the-art methods across eight benchmark datasets
- Demonstrates consistent performance gains across multiple architectures including ResNet-50, WideResNet, and ResNeXt
- Shows scalability with improved results on larger dataset subsets and better data efficiency

## Why This Works (Mechanism)

### Mechanism 1: Meta-Learned Utility Valuation
The weight allocation network (MLP) learns to assign weights to training samples such that minimizing the weighted training loss minimizes the unweighted validation loss. This bi-level optimization approach allows efficient utility estimation without costly model retraining cycles. The network outputs weights in [0,1] where higher values indicate greater utility based on their ability to reduce validation error.

### Mechanism 2: Preference-Aligned Diffusion Fine-Tuning
Direct Preference Optimization shifts the generative model's distribution to produce high-utility samples by learning from utility-ranked pairs. The method constructs preference datasets where winning samples have higher utility scores than losing samples, then fine-tunes the diffusion model to increase the likelihood of generating preferred samples without requiring a separate reward model.

### Mechanism 3: Input-Space Policy Optimization
The framework optimizes generation inputs through two approaches: (1) gradient-based optimization of prompt embeddings to maximize utility scores while maintaining semantic consistency via CLIP regularization, and (2) manipulation of initial noise vectors using asymmetric Classifier-Free Guidance scales to inject semantic information without gradient descent.

## Foundational Learning

- **Meta-Learning (Bi-Level Optimization)**: Essential for understanding how the weight network differentiates through the classifier training process to learn sample weighting. Quick check: If classifier weights are frozen, can the weight network be trained? (No - requires gradient of validation loss w.r.t. weights).
- **Direct Preference Optimization (DPO)**: Replaces standard RL for diffusion model alignment. Quick check: Why is the reference model frozen during DPO training?
- **Diffusion Inversion & CFG**: Critical for understanding noise optimization techniques. Quick check: How does changing CFG scale during inversion vs. denoising affect semantic content of reconstructed noise?

## Architecture Onboarding

- **Component map**: Weight-Net (MLP) → Diffusion Backbone (SD v2.1 U-Net) → Policy Optimizer (gradient + CFG) → Downstream Classifier (ResNet/WideResNet)
- **Critical path**: Pre-training (textual inversion) → Valuation (bi-level optimization) → Alignment (DPO) → Refinement (prompt/noise optimization)
- **Design tradeoffs**: Weight-Net simplicity vs. potential overfitting; DPO for utility vs. LoRA for fidelity; single hidden layer for speed vs. deeper networks for complexity
- **Failure signatures**: Weight collapse (uniform outputs), semantic drift (high utility but nonsensical images), overfitting to specific classifier
- **First 3 experiments**: (1) Train weight network on label-noisy data to verify it assigns lower weights to noisy samples, (2) Ablation comparing MLCO-only vs. ILPO-only generation, (3) t-SNE visualization comparing features from SD v2.1 vs. UtilGen-generated images

## Open Questions the Paper Calls Out

### Open Question 1
Can TODV be adapted for dense prediction tasks like object detection or semantic segmentation? The current scalar feedback from classification losses may be insufficient for spatially complex tasks requiring localization accuracy alongside semantic correctness.

### Open Question 2
How to mitigate bias amplification from the small set of real guiding images used for textual inversion? The framework focuses on maximizing accuracy but doesn't incorporate fairness constraints into the optimization objectives.

### Open Question 3
How dependent is performance on the base diffusion model's prior knowledge of the target domain? The method may struggle with concepts largely absent from the base model's pre-training data, though this limitation is acknowledged.

## Limitations
- Performance depends on validation loss serving as accurate proxy for downstream utility
- Weight network simplicity may limit capture of complex utility relationships
- Preference ranking assumes meaningful utility differences between similar samples

## Confidence

- **High confidence**: Dual-level optimization framework is technically sound; experimental methodology is rigorous; 3.87% improvement is well-supported
- **Medium confidence**: Weight network's ability to estimate utility without retraining is demonstrated but may not generalize to all task types
- **Medium confidence**: DPO fine-tuning effectively shifts generation distribution toward high-utility samples, though stability across tasks needs validation

## Next Checks

1. **Cross-distribution validation**: Test weight network on held-out dataset with known distribution shift to verify utility estimation accuracy across domains
2. **Ablation study**: Compare UtilGen against variants using different weight network architectures (deeper networks, attention-based) to assess optimal complexity
3. **Preference pair quality analysis**: Systematically vary preference selection ratio (ρ) and measure impact on downstream accuracy to identify optimal thresholds for different task types