---
ver: rpa2
title: Evolutionary Neural Architecture Search with Dual Contrastive Learning
arxiv_id: '2512.20112'
source_url: https://arxiv.org/abs/2512.20112
tags:
- architecture
- neural
- search
- predictor
- architectures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DCL-ENAS introduces a dual-stage contrastive learning approach
  to enhance predictor accuracy in evolutionary neural architecture search (ENAS)
  while reducing computational cost. The method employs self-supervised pretraining
  to learn meaningful architecture representations from unlabeled data, followed by
  contrastive fine-tuning to rank architectures based on relative performance.
---

# Evolutionary Neural Architecture Search with Dual Contrastive Learning

## Quick Facts
- arXiv ID: 2512.20112
- Source URL: https://arxiv.org/abs/2512.20112
- Authors: Xian-Rong Zhang; Yue-Jiao Gong; Wei-Neng Chen; Jun Zhang
- Reference count: 40
- Primary result: Achieves 0.05%-0.39% higher validation accuracy than strongest baselines across NASBench benchmarks while requiring only 7.7 GPU-days on ECG task

## Executive Summary
DCL-ENAS introduces a dual-stage contrastive learning approach to enhance predictor accuracy in evolutionary neural architecture search while reducing computational cost. The method employs self-supervised pretraining to learn meaningful architecture representations from unlabeled data, followed by contrastive fine-tuning to rank architectures based on relative performance. This approach achieves state-of-the-art validation accuracy across NASBench-101 and NASBench-201 benchmarks, surpassing strongest baselines by 0.05% to 0.39%. On a real-world ECG arrhythmia classification task, DCL-ENAS improves performance by approximately 2.5 percentage points over manual designs while requiring only 7.7 GPU-days.

## Method Summary
DCL-ENAS implements a two-stage pipeline for efficient neural architecture search. First, a contrastive pre-training stage uses hard encoder (path-based binary vectors) and soft encoder (GIN + path attention) to learn meaningful architecture representations through self-supervised clustering. Second, a contrastive fine-tuning stage employs pairwise ranking loss to optimize predictor accuracy for evolutionary search guidance. The evolutionary search uses tournament selection, crossover, and mutation operators on encoded architectures, with the predictor guiding selection decisions to minimize expensive real evaluations.

## Key Results
- Achieves state-of-the-art validation accuracy across NASBench-101 and NASBench-201 benchmarks
- Surpasses strongest baselines by 0.05% to 0.39% absolute accuracy improvement
- Demonstrates superior Kendall's τ correlation (above 0.6) compared to existing methods
- Reduces computational cost to 7.7 GPU-days on ECG arrhythmia classification task
- Improves Macro-F1 by ~2.5 percentage points over manual designs on ECG dataset

## Why This Works (Mechanism)

### Mechanism 1: Hard Encoder Clusters Architectures via Information Flow
Encoding architectures as binary vectors based on information flow paths enables structural similarity measurement without labels. Each architecture is decomposed into paths from input to output nodes, encoded using fixed operation templates with one-hot vectors for present operations and zero-padding for absent ones. Manhattan distance on these binary vectors quantifies architectural similarity. The core assumption is that architectures with similar path compositions will have similar performance characteristics.

### Mechanism 2: Soft Encoder Learns Grouping Knowledge via Self-Supervised Contrastive Learning
Pre-training the predictor's soft encoder to mimic hard encoder clusters produces meaningful representations without performance labels. K-medoids clustering on hard encodings groups architectures, and the soft encoder (containing GIN-based node attention and transformer-based path attention) is trained via contrastive loss: pull architectures toward their cluster prototype, push away from other prototypes. Crowding distance τ scales the loss based on cluster density.

### Mechanism 3: Pairwise Contrastive Fine-tuning Prioritizes Ranking Over Absolute Prediction
Training the predictor on pairwise ranking rather than MSE regression improves search guidance by focusing on relative ordering. Instead of predicting absolute accuracy, the fine-tuning loss penalizes incorrect rank ordering between architecture pairs. This expands n labeled samples into n(n−1) training signals, prioritizing correct rankings over accurate absolute scores.

## Foundational Learning

- **Concept: Contrastive Learning**
  - Why needed: Core training paradigm for both pre-training (grouping knowledge) and fine-tuning (ranking)
  - Quick check: Given three embeddings E1, E2, E3, can you write a contrastive loss that pulls E1 toward E2 and pushes it from E3?

- **Concept: Graph Neural Networks (specifically GIN)**
  - Why needed: Soft encoder uses Graph Isomorphism Network to extract node features from architecture DAGs
  - Quick check: How does GIN's aggregation differ from simple mean pooling, and why does it matter for distinguishing non-isomorphic graphs?

- **Concept: Evolutionary Algorithm Selection Pressure**
  - Why needed: Understanding how predictor-guided selection replaces expensive fitness evaluation
  - Quick check: If the predictor misranks two architectures, how does this propagate through tournament selection vs. rank-based selection?

## Architecture Onboarding

- **Component map:**
Search Space → Hard Encoder → K-medoids → Cluster Assignments
       ↓
Soft Encoder (GIN + Transformer) ← Pretrain Loss ← Cluster Prototypes
       ↓
Predictor Head (Concat → Project → Full → Score)
       ↓
Pairwise Ranking Loss ← Labeled Architectures
       ↓
Evolutionary Search → Infill Sampling → Real Evaluation → Fine-tune Loop

- **Critical path:**
Implement hard encoder first—validate Path-id uniqueness and Manhattan distance matches intuition on hand-crafted examples. Pre-train soft encoder: verify clustering produces meaningful groups by visualizing t-SNE of hard vs. soft encodings. Fine-tune with pairwise loss: check Kendall's τ on held-out architectures before integrating with evolution.

- **Design tradeoffs:**
Larger batch sizes for pre-training improve cluster quality but increase memory (paper uses 10k-20k architectures). Higher K (cluster count) provides finer-grained supervision but noisier prototypes. Pairwise loss scales O(n²)—paper uses batch size 8192, which may be memory-constrained.

- **Failure signatures:**
Pre-training loss plateaus but soft encoder still produces uniform embeddings → check K-medoids convergence, increase K variance. Fine-tuning Kendall's τ stuck near 0 → pairwise loss may be dominated by easy pairs; consider hard negative mining. Evolution collapses to similar architectures → infill sampling may over-exploit; increase uncertainty weight.

- **First 3 experiments:**
Sanity check: On NASBench-201, pre-train soft encoder with K=10, measure Kendall's τ between hard and soft encoding distances for 1000 random architectures. Ablation: Compare full DCL-ENAS vs. w/o-Pretrain vs. w/o-Contrastive on NASBench-101 with budget 100—replicate Figure 9 curves. Transfer test: Pre-train on NASBench-101, fine-tune on NASBench-201—does cross-benchmark pre-training help or hurt?

## Open Questions the Paper Calls Out

### Open Question 1
Can DCL-ENAS effectively generalize to optimize Large Language Model (LLM) architectures? The conclusion states the "generalization capability of DCL-ENAS to other domains—such as large language model (LLM) architecture optimization—has not yet been empirically validated." This remains unresolved because the current study only validates the method on CNN cell-based search spaces and 1D CNNs for ECG tasks. Empirical results showing DCL-ENAS successfully discovering high-performance Transformer architectures in an LLM search space would resolve this.

### Open Question 2
How does DCL-ENAS perform when jointly optimizing for accuracy, parameter count, and inference FLOPs? The authors note that while the framework is compatible, "future work will jointly optimize accuracy, parameter count, and inference FLOPs under the same compute budget constraint." This is unresolved because the current implementation only optimizes for single-objective validation accuracy using a ranking-based contrastive loss. An experiment adapting the fine-tuning loss for Pareto-based selection, demonstrating a trade-off curve between accuracy and hardware efficiency, would resolve this.

### Open Question 3
Can the computational overhead of the contrastive pre-training phase be reduced to improve scalability? The authors acknowledge that the pre-training stage introduces "non-negligible computational overhead" that "may become a bottleneck that hinders the scalability of the method." This remains unresolved because the paper provides no analysis on minimizing the pre-training epochs or data size required to maintain predictor reliability. An ablation study identifying the minimal effective pre-training duration that preserves the predictor's Kendall-τ correlation score would resolve this.

## Limitations

The hard encoder's path-based representation may miss important architectural features beyond path enumeration, potentially limiting transferability to complex search spaces. The self-supervised pre-training assumes structural similarity correlates with functional similarity—an assumption not rigorously validated across all NAS benchmarks. The pairwise ranking fine-tuning requires substantial memory (batch size 8192) that may be prohibitive for larger architectures.

## Confidence

**High Confidence:** The dual-stage contrastive learning framework improves predictor accuracy (Kendall's τ consistently above 0.6) and reduces search cost (7.7 GPU-days on ECG task). The ablation studies convincingly show the necessity of both pre-training and pairwise fine-tuning stages.

**Medium Confidence:** The superiority of path-based binary encoding over alternative representations, while demonstrated empirically, lacks theoretical justification for why this specific encoding captures architectural semantics effectively.

**Low Confidence:** The transferability of the approach to entirely different search spaces (e.g., transformer architectures, large-scale vision models) remains untested. The computational efficiency claims assume access to the same hardware configurations and may not scale linearly.

## Next Checks

1. **Cross-Benchmark Generalization Test:** Pre-train the soft encoder on NASBench-101, then fine-tune and evaluate on NASBench-201 without retraining from scratch. Measure Kendall's τ and final validation accuracy to quantify transfer learning effectiveness.

2. **Memory-Efficient Ranking Loss:** Implement a hard negative mining strategy for the pairwise loss to reduce batch size requirements from 8192 while maintaining ranking accuracy. Compare Kendall's τ and search performance with the original implementation.

3. **Encoder Ablation on Search Space Variants:** Replace the path-based hard encoder with an alternative (e.g., adjacency matrix flattening) and evaluate whether the contrastive learning framework still improves predictor accuracy on NASBench-101. This tests the robustness of the approach to different structural representations.