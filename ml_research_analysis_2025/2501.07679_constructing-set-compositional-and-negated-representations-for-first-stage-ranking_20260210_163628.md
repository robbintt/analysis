---
ver: rpa2
title: Constructing Set-Compositional and Negated Representations for First-Stage
  Ranking
arxiv_id: '2501.07679'
source_url: https://arxiv.org/abs/2501.07679
tags:
- queries
- query
- retrieval
- representations
- negation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores building set-compositional and negated representations
  for first-stage ranking using Learned Sparse Retrieval (LSR) models. The key contributions
  include: (1) A zero-shot framework that constructs compositional query representations
  using Linear Algebra Operations (LAO) between atomic LSR representations.'
---

# Constructing Set-Compositional and Negated Representations for First-Stage Ranking

## Quick Facts
- arXiv ID: 2501.07679
- Source URL: https://arxiv.org/abs/2501.07679
- Reference count: 40
- One-line primary result: Zero-shot linear algebra operations (LAO) on sparse representations can outperform fine-tuned retrievers on compositional queries, with Disentangled Negation achieving NDCG@10 of 0.258 on Set Difference tasks.

## Executive Summary
This paper tackles the challenge of handling compositional queries (set difference, intersection, union) in first-stage ranking using Learned Sparse Retrieval (LSR) models. The authors introduce a zero-shot framework that constructs compositional query representations through Linear Algebra Operations (LAO) between atomic LSR representations. They propose Disentangled Negation for handling negation queries, Combined Pseudo-Terms (CPT) for intersections, and an improved LSR architecture with negative term scores to enable effective negation handling. The work reveals that zero-shot LAO methods often outperform retrievers fine-tuned on compositional data, highlighting fundamental limitations in both LSR and Dense Retrievers for handling set operations.

## Method Summary
The authors build upon the Splade (Sparse Lexical and Expansion Model) architecture, starting with atomic query representations and applying zero-shot Linear Algebra Operations (LAO) to create compositional representations. They introduce three key mechanisms: Disentangled Negation, which selectively penalizes dimensions corresponding to negated terms without affecting positive ones; Combined Pseudo-Terms (CPT), which models intersections through outer product expansion to capture term co-occurrence; and SNReLU activation with negative term scores to enable true penalization in negation queries. The framework is evaluated on QUEST and NevIR datasets with domain splits to test generalization.

## Key Results
- Disentangled Negation achieves NDCG@10 of 0.258 on Set Difference queries in the QUEST dataset
- CPT improves Intersection performance to NDCG@10 of 0.080, outperforming other zero-shot methods
- Models with negative term scores achieve 42.73% pairwise accuracy on NevIR negation benchmark vs 23.07% for standard models
- Zero-shot LAO methods often outperform retrievers fine-tuned on compositional data, highlighting limitations in current approaches

## Why This Works (Mechanism)

### Mechanism 1: Disentangled Negation via Lexical Grounding
Zero-shot negation works by selectively penalizing only dimensions corresponding to negated concepts. Since LSR representations are lexically grounded (each sparse dimension maps to a vocabulary token), the authors compute R_B* = R_B - 1_A ⊙ R_B (where 1_A indicates dimensions in A), then R_A - R_B*. This formulation penalizes dimensions present in B but not A, preserving all positive query terms and avoiding interference where negating B removes shared semantics with A.

### Mechanism 2: Combined Pseudo-Terms for Intersection Modeling
Intersection queries require modeling joint term co-occurrence that single-dimension lexical representations cannot capture. The authors expand representation from |V| to |V|² dimensions via outer product R_A ⊗ R_B, creating pseudo-terms (e.g., "Colombia∩Venezuela") that score documents based on joint presence. Documents are similarly encoded as R_D ⊗ R_D. This quadratic expansion captures the intersection semantics needed for accurate retrieval.

### Mechanism 3: Negative Term Scores Enable True Penalization
Standard LSR cannot express "documents containing X should be penalized" because all term weights are non-negative. By replacing ReLU with SNReLU (symmetric non-linear with dead zone around zero), the authors enable negative term scores. Aggregation via max-absolute or sum pooling preserves sparsity while permitting both positive and negative signals. This recalibration from "relevant/irrelevant" to "positively relevant/negatively relevant/irrelevant" is essential for effective negation handling.

## Foundational Learning

- **Concept: Learned Sparse Retrieval (LSR) Architecture**
  - Why needed: All mechanisms build on LSR's sparse, lexically-grounded representations (e.g., Splade)
  - Quick check: Can you explain how Splade maps an input token sequence to a sparse |V|-dimensional weight vector using log(1 + ReLU(max_i out_ij))?

- **Concept: Vector Space Set Operations**
  - Why needed: Zero-shot framework relies on algebraic operations that approximate set-theoretic semantics
  - Quick check: Why does simple vector subtraction (A - B) fail for negation when A and B are semantically related?

- **Concept: Interference in Negation**
  - Why needed: Understanding why negation is hard motivates the disentangled approach
  - Quick check: When negating "European monarchs" from "Books about monarchs," why would naive subtraction also remove the "monarchs" concept?

## Architecture Onboarding

- **Component map**: `f_QE` (LSR query encoder) -> `f_CQE` (compositional encoder via LAO) -> Scoring function -> Retrieval
- **Critical path**: 
  1. Decompose compositional query into atomic queries + operation type (A \ B, A ∩ B, A ∪ B)
  2. Encode each atomic query via `f_QE`
  3. Apply LAO: Disentangled Negation for \, CPT for ∩, MaxPool for ∪
  4. Retrieve using inverted index (standard or expanded for CPT)
  5. Score and rank

- **Design tradeoffs**:
  - Zero-shot LAO vs. fine-tuned: LAO offers control and interpretability; fine-tuning may capture nuances but requires labeled compositional data
  - CPT index size: ~|V|² expansion improves intersection NDCG@10 from 0.069 to 0.080 but increases storage
  - SNReLU calibration: Negative scores improve negation handling but degrade atomic query performance (NDCG 0.284 → 0.182) without sufficient training data

- **Failure signatures**:
  - High semantic overlap between A and B causing Disentangled Negation to still affect positive terms
  - Intersection queries returning documents matching only one atomic query (low precision)
  - SNReLU models failing to converge or producing near-zero weights when negation training data is limited

- **First 3 experiments**:
  1. Implement Disentangled Negation on a pretrained Splade model; evaluate on QUEST Set Difference queries, varying interference levels
  2. Build CPT index for a subset of corpus; measure NDCG@10 and R@100 on intersection queries vs. MaxPool baseline; track index size ratio
  3. Fine-tune Splade with SNReLU activation on NevIR (1896 negation queries); compare pairwise accuracy vs. standard Splade; analyze weight distributions for negated vs. positive terms

## Open Questions the Paper Calls Out
1. Can late-interaction models (e.g., ColBERT) construct set-compositional representations more effectively than the single-vector LSR and Dense Retrievers studied in this paper?
2. Can pre-training techniques or LLM-based data generation make LSR models with negative term scores effective in data-scarce settings?
3. Is it possible to develop an efficient query encoder that performs query decomposition (splitting atomic queries and operators) with high accuracy, removing the need for external LLMs?

## Limitations
- The zero-shot framework's performance heavily depends on the lexical grounding assumption—sparse representations must have disentangled semantics per dimension for Disentangled Negation to work
- CPT's quadratic expansion creates practical scaling concerns: while effective for small vocabularies, it may not generalize to production-scale retrieval with larger corpora
- The SNReLU model's effectiveness hinges on having sufficient negation-labeled training data; the paper notes this is "challenging to achieve in Quest" with only 236 negation queries

## Confidence
- **High confidence**: Disentangled Negation mechanism and its effectiveness on Set Difference queries (NDCG@10 of 0.258)
- **Medium confidence**: CPT's improvement for intersections (NDCG@10 of 0.080). While the results show clear gains over baselines, the improvement is modest and the method's scalability is uncertain
- **Low confidence**: SNReLU model's general applicability. The significant performance drop on atomic queries (NDCG 0.284 → 0.182) and the reliance on limited negation training data make it unclear whether the benefits outweigh the costs in practical settings

## Next Checks
1. **Interference stress test**: Systematically vary the semantic overlap between positive and negative query terms in Set Difference tasks to identify the breaking point for Disentangled Negation
2. **CPT scalability analysis**: Measure NDCG@10 and R@100 as the number of combined pseudo-terms increases from 5 to 50, tracking index size growth and query latency to identify practical limits
3. **SNReLU calibration study**: Compare the effect of different ε thresholds in the SNReLU activation function on both negation performance and atomic query quality across multiple fine-tuning runs