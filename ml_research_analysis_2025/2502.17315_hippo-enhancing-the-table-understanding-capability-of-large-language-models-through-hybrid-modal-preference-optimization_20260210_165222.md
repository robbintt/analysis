---
ver: rpa2
title: 'HIPPO: Enhancing the Table Understanding Capability of Large Language Models
  through Hybrid-Modal Preference Optimization'
arxiv_id: '2502.17315'
source_url: https://arxiv.org/abs/2502.17315
tags:
- table
- hippo
- representations
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HIPPO, a hybrid-modal preference optimization
  approach for enhancing the table understanding capability of multi-modal large language
  models (MLLMs). HIPPO represents tables using both text-based and image-based representations,
  and optimizes MLLMs to learn more comprehensive table information from these multiple
  modalities.
---

# HIPPO: Enhancing the Table Understanding Capability of Large Language Models through Hybrid-Modal Preference Optimization

## Quick Facts
- **arXiv ID:** 2502.17315
- **Source URL:** https://arxiv.org/abs/2502.17315
- **Reference count:** 22
- **Primary result:** 4% improvement on table question answering and table fact verification tasks

## Executive Summary
This paper introduces HIPPO, a hybrid-modal preference optimization approach for enhancing the table understanding capability of multi-modal large language models (MLLMs). HIPPO represents tables using both text-based (Markdown) and image-based representations, and optimizes MLLMs through Direct Preference Optimization (DPO) to learn more comprehensive table information from these multiple modalities. The method employs a modality-consistent sampling strategy to enhance response diversity and mitigate modality bias during DPO training. Experimental results demonstrate significant improvements over various table reasoning models, with the approach showing particular effectiveness in enabling MLLMs to learn richer semantics across table representations of different modalities.

## Method Summary
HIPPO implements hybrid-modal preference optimization by representing tables as both Markdown text and rendered images, then sampling multiple responses from the MLLM using text-only, image-only, and combined inputs. For each training sample, the method generates 10 candidate responses per modality at temperature 1.0, then selects the ground truth as the positive example and the most frequent incorrect response across all modalities as the negative example. This modality-consistent sampling strategy helps prevent modality bias during training. The MLLM (MiniCPM-V-2.6) is then optimized using the DPO loss function with LoRA parameter-efficient fine-tuning. The approach shows improved performance not only on multi-modal inputs but also on unimodal table representations, demonstrating generalization of the learned reasoning capabilities.

## Key Results
- Achieves 4%+ improvement over various table reasoning models on TQA and TFV tasks
- Shows significant effectiveness in enabling MLLMs to learn richer semantics across table representations of different modalities
- Demonstrates enhanced reasoning abilities based on unimodal table representations (text-only and image-only)
- Achieves over 1% improvement specifically attributed to the modality-consistent sampling method

## Why This Works (Mechanism)

### Mechanism 1: Hybrid-Modal Representation Sampling for DPO
Sampling responses from both text-based (Markdown) and image-based table representations for Direct Preference Optimization (DPO) training enhances MLLM reasoning by exposing it to complementary semantic signals. HIPPO samples multiple responses (K per modality) using text-only, image-only, and combined (text+image) inputs. This creates a diverse set of candidate answers. The ground truth is selected as the positive example, and the most frequent incorrect answer from the sampled set is selected as the negative example. The MLLM is then optimized using the DPO loss function to increase the likelihood of the ground truth and decrease the likelihood of the chosen negative response. Different modalities (text vs. image) emphasize different semantic features of a table (e.g., arithmetic values vs. visual layout/color cues), and a DPO framework can effectively leverage this diversity to improve preference learning.

### Mechanism 2: Modality-Consistent Negative Sampling
Selecting the most frequent incorrect response from across all modalities as the negative example for DPO training helps mitigate modality bias and leads to more robust optimization. Instead of randomly picking a negative response, HIPPO calculates the frequency of all incorrect responses generated across text, image, and combined inputs. The most common incorrect answer is chosen as the negative example. This "modality-consistent" sampling avoids inadvertently biasing the model against a specific modality, which could happen if negatives were consistently sampled from a single modality. A frequent incorrect answer across modalities represents a more fundamental reasoning failure or a common "distractor," rather than a modality-specific artifact. Training against such a negative is more effective.

### Mechanism 3: Improved Semantic Extraction per Modality
The HIPPO training paradigm improves the model's ability to extract semantics not only from multi-modal inputs but also from unimodal inputs, demonstrating generalization. By training on diverse preference pairs derived from multiple modalities, the model's internal representations are refined. This leads to better performance even when only a single modality (text or image) is provided at inference time. The paper shows increased Jaccard similarity (more consistent answers) and decreased Chain-of-Thought similarity (more diverse reasoning paths adapted to the modality) for HIPPO compared to baselines. The learned preference for correct reasoning generalizes across modalities; the model isn't just learning to map a combined input to an output but is learning better fundamental reasoning skills applicable to any representation of the table.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed: HIPPO's core training method is DPO. You must understand that DPO optimizes a policy (the LLM) by increasing the likelihood of preferred outputs (positive examples) and decreasing the likelihood of dispreferred outputs (negative examples) relative to a reference model, without needing a separate reward model.
  - Quick check: How does DPO differ from Reinforcement Learning from Human Feedback (RLHF)?

- **Concept: Multi-Modal Large Language Models (MLLMs)**
  - Why needed: The paper uses MLLMs (like MiniCPM-V) as the base architecture. Understanding that these models can process both text and images is crucial. The paper leverages their ability to ingest a table as both a Markdown string and a screenshot.
  - Quick check: What is the typical input format for an MLLM when processing a document with both text and images?

- **Concept: Table Representation (Text vs. Image)**
  - Why needed: The entire premise depends on the idea that text (Markdown) and image (screenshot) representations of a table provide different information. Text is good for precise arithmetic and direct text matching, while images preserve layout, styling (e.g., cell colors), and spatial relationships. The method tries to combine these strengths.
  - Quick check: For a question requiring the summation of a specific column in a large table, which representation would likely be more robust for an LLM? Why?

## Architecture Onboarding

- **Component map:** Input Processor -> MLLM Backbone -> Sampling Engine -> Preference Pair Constructor -> DPO Trainer
- **Critical path:**
  1. Data Generation: For each training sample (table, question, ground truth), generate a diverse set of candidate answers using the MLLM from all three modalities. This step is computationally intensive.
  2. Pair Construction: Identify the positive (ground truth) and the most frequent negative (incorrect) answer. This step is the key to modality-consistent sampling.
  3. DPO Training: Fine-tune the MLLM using the generated preference pairs. This adjusts the model's policy to favor the correct reasoning and disfavor the common errors.

- **Design tradeoffs:**
  - Computational Cost vs. Performance: The data generation phase requires multiple forward passes (3 modalities * K samples) per training example, which is expensive upfront but leads to a more effective training signal.
  - Complexity vs. Generalization: The approach is more complex than standard supervised fine-tuning (SFT) but shows better performance on both multi-modal and unimodal evaluation, indicating a more robust model.
  - Modality-Consistent vs. Random Sampling: The paper's ablation shows that modality-consistent sampling for negatives is critical. Random sampling risks introducing modality bias, which hurts performance on unimodal inputs.

- **Failure signatures:**
  - Low-quality negative samples: If the most frequent incorrect answer is not representative of the model's common failure modes, the DPO signal will be weak.
  - Modality bias: If negatives are consistently sampled from one modality, the model may learn to ignore or down-weight that modality, degrading unimodal performance.
  - Insufficient sampling: If K is too small, the set of incorrect answers may not be diverse enough, and the most frequent negative may be a poor choice.

- **First 3 experiments:**
  1. Reproduce Ablation on Sampling Strategy: Implement both the "modality-consistent" negative sampling and a "random" sampling baseline. Run them on a subset of the training data and compare validation accuracy. This confirms the core claim about mitigating modality bias.
  2. Unimodal Performance Check: Train the HIPPO model and then evaluate its performance separately on text-only and image-only test sets. Compare this to a baseline model trained only on multi-modal data. This validates the generalization claim.
  3. Analysis of Generated Negatives: Visualize the most frequent incorrect answers generated during the sampling phase. Categorize them (e.g., arithmetic error, entity mismatch, format issue) to understand what failure modes the model is learning to avoid. This provides insight into the mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- The core claim of 4% improvement relies on ablation studies comparing against random sampling baselines, but the paper lacks direct comparison against other strong table reasoning methods beyond brief mentions
- The modality-consistent sampling strategy's effectiveness depends on the existence of common incorrect answers across modalities, which may not hold for all table reasoning tasks
- The claimed generalization to unimodal inputs is supported by correlation analysis but lacks direct ablation studies isolating this effect

## Confidence
- **High Confidence:** The technical implementation of hybrid-modal preference optimization and the general framework for combining text/image table representations
- **Medium Confidence:** The specific benefits of modality-consistent negative sampling and the 4% improvement claim
- **Low Confidence:** The generalization mechanism to unimodal inputs and the claim that different modalities emphasize complementary semantic features

## Next Checks
1. Replicate the ablation study on negative sampling strategies using a held-out validation set to confirm that modality-consistent sampling provides statistically significant improvement over random sampling baselines.

2. Evaluate unimodal performance isolation by training separate models on text-only and image-only data, then comparing their performance to HIPPO's unimodal evaluation results to verify the claimed generalization.

3. Analyze the diversity of generated negative samples by categorizing incorrect responses (arithmetic errors, entity mismatches, etc.) to confirm that the most frequent negatives represent meaningful common failure modes rather than artifacts of the sampling process.