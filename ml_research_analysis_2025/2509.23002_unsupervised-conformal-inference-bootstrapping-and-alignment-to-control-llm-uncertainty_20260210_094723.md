---
ver: rpa2
title: 'Unsupervised Conformal Inference: Bootstrapping and Alignment to Control LLM
  Uncertainty'
arxiv_id: '2509.23002'
source_url: https://arxiv.org/abs/2509.23002
tags:
- batch
- conformal
- wang
- zhang
- factuality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces an unsupervised conformal inference framework\
  \ for black-box large language models (LLMs) that operates entirely on sampled outputs\
  \ without requiring true labels. The framework combines a Gram-matrix-based atypicality\
  \ score derived from response embeddings, batched and bootstrapped conformal procedures\
  \ (BB-UCP) for stable calibration, and conformal alignment to calibrate a global\
  \ strictness parameter \u03C4 so that a user-defined predicate (e.g., factuality\
  \ improvement) holds on unseen batches with probability at least 1\u2212\u03B1."
---

# Unsupervised Conformal Inference: Bootstrapping and Alignment to Control LLM Uncertainty

## Quick Facts
- **arXiv ID:** 2509.23002
- **Source URL:** https://arxiv.org/abs/2509.23002
- **Reference count:** 40
- **Primary result:** Introduces label-free, test-time gating for LLMs using bootstrapped conformal inference and unsupervised alignment to reduce hallucinations.

## Executive Summary
This paper presents a fully unsupervised conformal inference framework for black-box large language models that operates without true labels. The method combines a Gram-matrix-based atypicality score derived from response embeddings, batched and bootstrapped conformal procedures (BB-UCP) for stable calibration, and conformal alignment to calibrate a global strictness parameter τ so that a user-defined predicate (e.g., factuality improvement) holds on unseen batches with probability at least 1−α. Experiments on multiple QA datasets show that BB-UCP achieves near-nominal coverage with tighter and more stable thresholds than split UCP, while consistently reducing hallucination severity. Conformal alignment reliably yields batch-level factuality gains, with median severity reductions up to 0.209 on the hardest datasets.

## Method Summary
The framework introduces an unsupervised conformal inference approach for black-box LLMs using only sampled outputs. It starts by constructing a Gram matrix from LLM response embeddings to compute an atypicality score for each response. These scores are then used in batched and bootstrapped conformal prediction (BB-UCP) to produce calibrated, goal-aligned decision thresholds without requiring true labels. A conformal alignment step further calibrates a global strictness parameter τ so that a user-defined predicate (e.g., factuality improvement) holds with high probability on unseen batches. This enables label-free, API-compatible test-time gating that transforms geometric signals into calibrated decisions for trustworthy LLM deployment.

## Key Results
- BB-UCP achieves near-nominal coverage with tighter and more stable thresholds than split UCP on multiple QA datasets.
- Conformal alignment reliably yields batch-level factuality gains, with median severity reductions up to 0.209 on the hardest datasets.
- The approach provides a label-free, API-compatible test-time gating mechanism that reduces hallucination severity.

## Why This Works (Mechanism)
The method works by leveraging geometric properties of LLM response embeddings to estimate response atypicality without labels. By constructing a Gram matrix from embeddings and computing normalized inner products, it quantifies how typical or atypical each response is relative to the batch. This unsupervised signal is then calibrated via conformal inference to produce statistically valid coverage guarantees. The bootstrapping and batching further stabilize the calibration, reducing variance and improving threshold quality compared to standard split conformal methods. Finally, conformal alignment adjusts the strictness parameter to ensure that the user's quality predicate (e.g., improved factuality) is satisfied on new batches, bridging the gap between geometric calibration and task-specific performance.

## Foundational Learning

### Gram-matrix-based atypicality
- **Why needed:** Provides a label-free measure of response "usualness" by comparing embeddings via inner products.
- **Quick check:** Verify atypicality scores correlate with known low-quality responses on a held-out set.

### Batched and bootstrapped conformal prediction (BB-UCP)
- **Why needed:** Stabilizes threshold estimation by aggregating over multiple resampled batches, reducing variance.
- **Quick check:** Compare BB-UCP threshold stability and coverage to standard split conformal on synthetic data.

### Conformal alignment
- **Why needed:** Calibrates the strictness parameter τ so that a user-defined predicate holds with high probability on new batches.
- **Quick check:** Confirm alignment yields the target coverage for the chosen predicate on a validation batch.

## Architecture Onboarding

### Component map
Response embeddings -> Gram matrix -> Atypicality scores -> BB-UCP calibration -> Thresholds -> Conformal alignment (τ) -> Batch-level decisions

### Critical path
Embeddings → Gram matrix → Atypicality → BB-UCP → Threshold → Alignment → Deployment

### Design tradeoffs
- Unsupervised signal (atypicality) vs. need for reliable quality proxy
- Batch size vs. threshold stability and coverage guarantees
- Alignment precision vs. computational overhead

### Failure signatures
- Atypicality scores not discriminative → poor coverage
- Small batch sizes → unstable thresholds
- Misaligned predicate → failure to improve desired metric

### First 3 experiments
1. Run BB-UCP on a small QA dataset and compare coverage to split conformal.
2. Apply conformal alignment and measure predicate satisfaction rate.
3. Vary batch size and observe threshold stability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the framework be adapted to maintain valid coverage guarantees under distribution shift or temporal non-stationarity?
- **Basis in paper:** [Explicit] The "Limitations and outlook" section states: "violations due to drift or covariate shift motivate weighted or covariate-aware variants."
- **Why unresolved:** The current theoretical guarantees rely strictly on batch exchangeability (i.i.d. batches), which breaks down in dynamic deployment scenarios.
- **What evidence would resolve it:** Development and testing of a weighted or adaptive UCP variant on a dataset with injected concept drift, demonstrating maintained coverage without manual recalibration.

### Open Question 2
- **Question:** How robust is the Gram-matrix atypicality score to the choice of embedding model and normalization strategy?
- **Basis in paper:** [Explicit] The authors note: "Performance depends on embedding quality and normalization, underscoring the need for robustness audits and principled model/embedding selection."
- **Why unresolved:** The experiments primarily use `all-MiniLM-L6-v2`; sensitivity to domain-specific or higher-dimensional embeddings remains unquantified.
- **What evidence would resolve it:** Ablation studies comparing quantile stability and coverage across diverse embedding architectures (e.g., domain-specific vs. general) and normalization techniques.

### Open Question 3
- **Question:** Can the conformal alignment procedure effectively generalize to multimodal settings such as vision or audio?
- **Basis in paper:** [Explicit] The conclusion identifies "extending alignment to multimodal settings" as a "promising direction."
- **Why unresolved:** It is currently unknown if the inner-product energy logic translates effectively to the geometry of image or audio embeddings used in Vision-Language Models (VLMs).
- **What evidence would resolve it:** Application of the BB-UCP framework to a multimodal task (e.g., image captioning) to verify if the geometric signal correlates with hallucination severity.

### Open Question 4
- **Question:** How can the predicate design be extended to optimize for multi-metric trade-offs, such as balancing factuality against utility?
- **Basis in paper:** [Explicit] The paper states that "predicate design... invites cost-aware utilities and multi-task calibration."
- **Why unresolved:** The current alignment procedure calibrates a single strictness parameter ($\tau$) against a single predicate (e.g., factuality lift).
- **What evidence would resolve it:** A multi-objective calibration experiment that derives simultaneous guarantees for conflicting metrics (e.g., verbosity vs. accuracy).

## Limitations
- Performance depends on the quality and discriminative power of the Gram-matrix atypicality score, which may conflate semantically different responses with similar geometric properties.
- The method assumes the unsupervised predicate (e.g., factuality improvement) can be reliably computed; errors in this proxy directly affect coverage and alignment validity.
- Results are reported on specific QA datasets; generalization to other domains or substantially different LLM architectures is untested.
- The stability improvements over split conformal methods are shown empirically but may depend on batch size, sampling strategy, and specific LLM API.

## Confidence
- **High**: BB-UCP achieves near-nominal coverage with tighter and more stable thresholds for the tested datasets and setups.
- **Medium**: Conformal alignment reliably yields batch-level factuality gains, though these depend on the accuracy of the unsupervised predicate and may vary with task complexity.
- **Medium to Low**: The broader assertion that this framework provides a general, label-free test-time gating mechanism for trustworthy LLM deployment requires further validation across diverse tasks and models.

## Next Checks
1. Test the atypicality score and conformal procedures on non-QA domains (e.g., summarization, code generation) to assess generalization.
2. Perform ablation studies on the predicate choice and accuracy, and evaluate how errors in the unsupervised factuality metric propagate to coverage and calibration.
3. Conduct experiments with varying batch sizes and sampling strategies to quantify the stability and tightness of BB-UCP relative to split conformal methods under different operational conditions.