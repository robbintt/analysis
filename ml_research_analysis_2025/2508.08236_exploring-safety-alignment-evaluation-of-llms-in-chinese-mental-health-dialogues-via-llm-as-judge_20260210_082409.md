---
ver: rpa2
title: Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues
  via LLM-as-Judge
arxiv_id: '2508.08236'
source_url: https://arxiv.org/abs/2508.08236
tags:
- evaluation
- health
- mental
- safety
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating safety alignment
  in LLM responses to high-risk mental health dialogues, where no gold-standard answers
  exist. The authors propose PsyCrisis-Bench, a reference-free benchmark that uses
  a prompt-based LLM-as-Judge approach guided by expert-defined reasoning chains from
  crisis intervention theory.
---

# Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge

## Quick Facts
- **arXiv ID**: 2508.08236
- **Source URL**: https://arxiv.org/abs/2508.08236
- **Reference count**: 31
- **Primary result**: LLM-as-Judge evaluation achieves 0.45 agreement with experts vs 0.1-0.2 for baselines

## Executive Summary
This paper addresses the challenge of evaluating safety alignment in LLM responses to high-risk mental health dialogues, where no gold-standard answers exist. The authors propose PsyCrisis-Bench, a reference-free benchmark that uses a prompt-based LLM-as-Judge approach guided by expert-defined reasoning chains from crisis intervention theory. The evaluation employs binary point-wise scoring across five safety dimensions—empathy, emotional regulation strategies, concern exploration, risk assessment, and referral to external resources—to ensure traceability and interpretability. Experiments on 3,600 judgments show the method achieves significantly higher agreement with expert assessments (0.45 vs. 0.1–0.2) and produces more interpretable rationales compared to baselines. The publicly released dataset covers self-harm, suicidal ideation, and existential distress in Chinese-language real-world contexts, offering a valuable resource for safety-critical LLM evaluation.

## Method Summary
The authors develop PsyCrisis-Bench, a reference-free benchmark for evaluating LLM safety alignment in mental health dialogues. The approach uses a prompt-based LLM-as-Judge system guided by expert-defined reasoning chains derived from WHO's psychological crisis intervention theory. Each response is evaluated across five binary safety dimensions using structured prompts that generate interpretable rationales. The method avoids the need for gold-standard answers by focusing on whether responses demonstrate key safety attributes rather than comparing against optimal responses. The evaluation was conducted on 600 Chinese mental health dialogues covering self-harm, suicidal ideation, and existential distress, with each dialogue judged 6 times to produce 3,600 total judgments.

## Key Results
- LLM-as-Judge achieves 0.45 agreement with expert assessments, significantly outperforming baseline methods (0.1-0.2 agreement)
- The approach produces highly interpretable rationales through structured reasoning chains
- Binary point-wise scoring across five safety dimensions provides clear traceability for safety evaluation

## Why This Works (Mechanism)
The method works by leveraging expert-defined reasoning chains that encode established psychological crisis intervention principles into structured evaluation prompts. By avoiding reference-based comparison and instead assessing whether responses contain specific safety attributes, the approach sidesteps the impossibility of defining "correct" answers in mental health crises. The binary point-wise scoring system ensures clear decision boundaries while maintaining interpretability through generated rationales that explain each judgment.

## Foundational Learning
- **Mental health crisis intervention theory**: Provides the theoretical foundation for defining safety attributes; needed to establish what constitutes a safe response in high-risk situations; quick check: verify criteria align with WHO guidelines
- **Reference-free evaluation**: Enables assessment without gold-standard answers; needed because optimal responses don't exist for crisis situations; quick check: confirm all evaluation criteria are attribute-based rather than comparative
- **LLM-as-Judge paradigm**: Uses language models to evaluate other models; needed to scale safety evaluation beyond human capacity; quick check: validate judge agreement with human experts
- **Binary point-wise scoring**: Simplifies complex safety judgments into clear decisions; needed for traceability and interpretability; quick check: ensure scoring criteria are unambiguous
- **Structured reasoning chains**: Guides the judge through systematic evaluation; needed to produce interpretable rationales; quick check: verify all dimensions are consistently addressed

## Architecture Onboarding

### Component Map
PsyCrisis-Bench Dataset -> Expert-defined Reasoning Chains -> LLM-as-Judge Prompts -> Safety Dimension Scores -> Interpretable Rationales

### Critical Path
The critical path flows from dataset to final scores: dialogue inputs are processed through reasoning chain-guided prompts that systematically evaluate each safety dimension, producing binary scores and rationales that together form the safety assessment.

### Design Tradeoffs
The reference-free approach sacrifices the ability to identify optimal responses in favor of practical safety evaluation. Binary scoring provides clarity but may oversimplify nuanced situations. The prompt-based LLM-as-Judge prioritizes transferability over potential gains from model fine-tuning.

### Failure Signatures
The method shows leniency bias, with judges consistently rating responses safer than experts. Hallucinations occur when judges assume helpful content exists without verification. These failures manifest as inflated safety scores and potentially unjustified positive assessments.

### First 3 Experiments
1. Test LLM-as-Judge agreement on edge cases where responses are ambiguous or partially address safety criteria
2. Compare evaluation consistency across different LLM judge models to assess bias and variability
3. Validate that interpretable rationales actually capture the reasoning behind safety judgments by having experts review them

## Open Questions the Paper Calls Out
### Open Question 1
- **Question**: How does the LLM-as-Judge evaluation paradigm perform when applied to multi-turn mental health dialogue scenarios rather than single-turn interactions?
- **Basis in paper**: [explicit] The authors state in the Limitations section: "While multi-turn evaluation is widespread in real-world scenarios, it will be explored in future work, which requires high-quality multi-turn datasets and careful handling of annotation complexity."
- **Why unresolved**: The current study is constrained to single-turn interactions to ensure safety at the utterance level, but real-world counseling involves context accumulation and dynamic interaction which the current benchmark does not evaluate.
- **What evidence would resolve it**: The construction of a multi-turn mental health dialogue dataset with expert safety annotations and a demonstration of the evaluation method's consistency across conversational context.

### Open Question 2
- **Question**: To what extent do the expert-derived reasoning chains and safety criteria generalize to multilingual and cross-cultural mental health contexts?
- **Basis in paper**: [explicit] The authors note: "...extending the benchmark to multilingual and cross-cultural settings will be critical for improving its generalizability and applicability across diverse populations."
- **Why unresolved**: The current dataset and safety principles (derived from WHO guidelines and Chinese experts) are specific to the Chinese language and cultural context, potentially limiting their validity in regions with different expressions of distress or clinical norms.
- **What evidence would resolve it**: Experiments applying the PsyCrisis-Bench framework to non-Chinese datasets and measuring the agreement between the model's judgments and local human experts in those languages.

### Open Question 3
- **Question**: Can fine-tuning the judge model achieve significantly higher alignment with human experts compared to the current prompt-based approach?
- **Basis in paper**: [explicit] The authors state: "Future work will explore fine-tuning the LLM-as-Judge to further enhance its performance."
- **Why unresolved**: The current method prioritizes lightweight, transferable prompt engineering; it remains unknown if optimizing the model's weights specifically for this safety task would yield higher accuracy or reduce the "moderate" correlation gap.
- **What evidence would resolve it**: A comparative study measuring the inter-annotator agreement (Pearson/Spearman correlation) of a fine-tuned evaluator against the prompt-based baseline using the same expert-annotated test set.

### Open Question 4
- **Question**: Can the "leniency bias" and hallucination of helpful content observed in LLM-as-Judge evaluations be mitigated through grounding techniques?
- **Basis in paper**: [inferred] The Failure Case Analysis notes that "model-assigned safety alignment scores tend to be consistently higher than expert ratings" and that models may hallucinate helpful content (e.g., assuming strategies are present when they are not).
- **Why unresolved**: The paper identifies that the judge often assumes a response is safe or helpful without strict verification, a fundamental limitation of relying solely on generative LLMs as judges without external verification.
- **What evidence would resolve it**: Experiments integrating retrieval-augmented generation (RAG) or strict entailment checks to force the judge to cite specific text spans before awarding positive safety scores.

## Limitations
- Binary scoring system may oversimplify nuanced mental health interactions
- Single-turn evaluation doesn't capture the complexity of real-world counseling conversations
- Reliance on prompt-based LLM-as-Judge without exploring fine-tuning or ensemble methods

## Confidence
- **High**: Performance metrics (0.45 vs 0.1-0.2 agreement with experts)
- **Medium**: Generalizability across cultural contexts
- **Medium**: Relative effectiveness compared to specialized mental health models
- **Low**: Long-term reliability and temporal consistency

## Next Checks
1. Cross-cultural validation using non-Chinese mental health dialogues to test the framework's adaptability to different cultural contexts and healthcare norms
2. Comparison against specialized mental health LLMs (e.g., domain-trained models) rather than only generic LLMs to establish relative effectiveness
3. Longitudinal study assessing temporal consistency of LLM-as-Judge evaluations and potential model drift over time