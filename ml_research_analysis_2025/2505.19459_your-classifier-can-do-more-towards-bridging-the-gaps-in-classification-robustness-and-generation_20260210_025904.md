---
ver: rpa2
title: 'Your Classifier Can Do More: Towards Bridging the Gaps in Classification,
  Robustness, and Generation'
arxiv_id: '2505.19459'
source_url: https://arxiv.org/abs/2505.19459
tags:
- adversarial
- robustness
- training
- clean
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the triple trade-off between classification
  accuracy, adversarial robustness, and generative capability in deep learning models.
  While Joint Energy-based Models (JEMs) can achieve high accuracy and generation,
  they lack robustness compared to adversarial training (AT), which sacrifices accuracy
  and generation.
---

# Your Classifier Can Do More: Towards Bridging the Gaps in Classification, Robustness, and Generation

## Quick Facts
- arXiv ID: 2505.19459
- Source URL: https://arxiv.org/abs/2505.19459
- Reference count: 40
- Primary result: EB-JDAT achieves SOTA robustness (68.76%, 35.63%, 32.40% under AutoAttack) while maintaining near-original accuracy and generation quality

## Executive Summary
This paper addresses the fundamental triple trade-off between classification accuracy, adversarial robustness, and generative capability in deep learning models. While Joint Energy-based Models (JEMs) excel at accuracy and generation but lack robustness, and adversarial training (AT) provides robustness at the cost of accuracy and generation, the authors propose Energy-based Joint Distribution Adversarial Training (EB-JDAT). The method bridges these gaps by explicitly aligning the energy distributions of clean, adversarial, and generated samples through a unified min-max optimization framework, demonstrating state-of-the-art performance across all three metrics on standard benchmarks.

## Method Summary
EB-JDAT extends JEMs by modeling the joint probability distribution of clean data, adversarial examples, and class labels through energy-based optimization. The method introduces a min-max framework where adversarial examples are sampled as high-energy points near clean data, then the model is trained to minimize the energy gap between clean and adversarial samples while maintaining the clean-generated energy alignment. This is achieved through three weighted gradient components: one for clean data distribution, one for adversarial distribution, and one for classification accuracy. The approach is compatible with various JEM variants and demonstrates improved stability compared to standard JEM training through the inclusion of adversarial distribution gradients.

## Key Results
- Achieves 68.76% robustness under AutoAttack on CIFAR-10 while maintaining 90.12% clean accuracy
- Demonstrates 35.63% robustness on CIFAR-100 with only slight degradation in generation quality (FID)
- Shows 32.40% robustness on 64×64 ImageNet subset with comparable performance to specialized methods

## Why This Works (Mechanism)

### Mechanism 1
The trilemma between accuracy, robustness, and generation stems from misaligned energy distributions among clean, adversarial, and generated data. Adversarial Training aligns clean and adversarial energies (improving robustness but hurting accuracy), while JEMs align clean and generated energies (improving generation/accuracy but lacking robustness). EB-JDAT explicitly aligns all three distributions via joint probability maximization. If the energy function fails to correlate with sample likelihood, aligning distributions will not yield semantic improvements.

### Mechanism 2
Standard AT minimizes conditional loss p(y|x̃), while EB-JDAT approximates the full adversarial distribution p(x̃|x) by searching for high-energy adversarial examples and then minimizing their energy gap with clean data. This pulls adversarial samples back into the high-density manifold. If adversarial examples exist inside the data manifold (in-distribution), the premise of "pulling back" from high-energy regions fails.

### Mechanism 3
Training stability relies on a weighted combination of three gradient components: h₁ (clean data distribution), h₂ (adversarial distribution), and h₃ (classification). Specifically, h₂ prevents model collapse while h₁ maintains generative quality. Setting weights improperly causes gradient conflicts, leading to mode collapse or degraded robustness.

## Foundational Learning

- **Energy-based Models (EBMs)**: The method reinterprets classifier logits as an energy function E(x) = -log p(x). Understanding this is required to grasp how "low energy" equates to "high likelihood" and why minimizing energy gaps aligns distributions. *Quick check: If a sample has lower energy than another, is it more or less likely under the model distribution?*

- **Stochastic Gradient Langevin Dynamics (SGLD)**: The paper uses SGLD to sample from the model distribution (generation) and to craft adversarial examples. It introduces noise to escape local minima during sampling. *Quick check: Why is the noise term ε necessary when performing gradient descent for sampling in an EBM?*

- **Adversarial Training (AT) as a Saddle Point Problem**: EB-JDAT extends the standard min-max formulation of AT. You must understand the "inner maximization" (finding the worst attack) vs. "outer minimization" (training the model) to see how EB-JDAT modifies the inner loop using energy gradients. *Quick check: In standard AT, what is maximized in the inner loop (loss or probability)? How does EB-JDAT change the quantity being maximized?*

## Architecture Onboarding

- **Component map**: Backbone (WRN28-10) -> Logits f_θ(x) -> Energy Layer E_θ(x) = -log Σ exp(f_θ(x)[y]) -> Two SGLD Samplers -> Loss Aggregator -> Gradients h₁ + h₂ + h₃

- **Critical path**: 1) Sample clean batch {x, y} 2) Run Standard SGLD to generate "fake" samples x⁻ (for h₁) 3) Run Adversarial SGLD (Inner Max) initialized near x to find high-energy x̃ (for h₂) 4) Compute Cross-Entropy on x̃ (for h₃) 5) Sum gradients: h_total = h₁ + h₂ + h₃ and update weights

- **Design tradeoffs**: Robustness vs. Speed (increasing adversarial sampling steps improves robustness but increases training time linearly); Stability vs. Diversity (reducing weight w₂ might improve accuracy/FID but risks model collapse); Dataset Complexity (method demonstrated on CIFAR/ImageNet subsets; high-resolution data may suffer from "sharp probability distribution" instability)

- **Failure signatures**: Model Collapse (sudden spike in FID or generation of uniform noise, often linked to incorrect w₂ or insufficient SGLD steps); Robustness Overfitting (high PGD accuracy but low AutoAttack accuracy, suggesting the model overfits to the specific attack used in training)

- **First 3 experiments**: 1) Sanity Check (CIFAR-10): Reproduce Table 6 (Ablation). Train with w₂=0 vs w₂=1 to verify stability differences. 2) Hyperparameter Sensitivity: Run ablation on Adversarial Sampling Steps (K ∈ {1, 5, 10}) as per Figure 5 to find the stable region before collapse. 3) Integration Test: Apply the EB-JDAT wrapper to a different backbone (e.g., a smaller WRN) to verify modularity and compute the "Robustness vs. Accuracy" trade-off curve.

## Open Questions the Paper Calls Out

- Can EB-JDAT be stabilized for full-resolution, high-dimensional datasets where current energy-based methods struggle? The authors acknowledge that training on complex, high-dimensional data remains challenging due to sharp probability distributions and low-density guidance issues, but performance on full-resolution data is unverified.

- Is the "slight degradation" in accuracy and generation quality compared to non-robust baselines an unavoidable cost of adversarial robustness? While the method bridges the gap, there is still "slight degradation... This reflects an intrinsic trade-off" between robustness and accuracy/generation.

- Can the reliance on Stochastic Gradient Langevin Dynamics (SGLD) be replaced to improve training efficiency? The method relies on SGLD for both data generation and adversarial sampling, which adds computational overhead and sensitivity to hyperparameters.

## Limitations

- Training instability on complex, high-dimensional data due to sharp probability distributions and low-density guidance issues
- Slight degradation in accuracy and generation quality compared to specialized non-robust baselines, reflecting an intrinsic trade-off
- Computational overhead from SGLD sampling for both generation and adversarial example creation

## Confidence

- **High confidence**: Experimental results showing superior AutoAttack robustness (68.76%, 35.63%, 32.40%) while maintaining near-original accuracy and FID scores are well-documented
- **Medium confidence**: The mechanism of aligning energy distributions through three-gradient formulation is theoretically sound but relies on assumptions about energy landscapes not fully validated
- **Low confidence**: Claims about compatibility with "various JEM variants" and specific claims about preventing model collapse are primarily supported by ablation studies on specific instantiations

## Next Checks

1. **Energy Landscape Validation**: Design an experiment to visualize and quantify the energy distributions of clean, adversarial, and generated samples before and after EB-JDAT training to directly test the core hypothesis about energy alignment

2. **SGLD Hyperparameter Sensitivity**: Systematically vary SGLD step sizes and iteration counts for both standard and adversarial sampling to determine the stable operating region and identify failure thresholds

3. **High-Dimensional Scalability Test**: Apply EB-JDAT to higher resolution datasets (e.g., CIFAR-10 at 128×128) and measure the "sharp probability distribution" phenomenon quantitatively to validate the limitations claim