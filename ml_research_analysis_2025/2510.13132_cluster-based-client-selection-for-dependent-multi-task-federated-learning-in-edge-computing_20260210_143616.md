---
ver: rpa2
title: Cluster-Based Client Selection for Dependent Multi-Task Federated Learning
  in Edge Computing
arxiv_id: '2510.13132'
source_url: https://arxiv.org/abs/2510.13132
tags:
- task
- client
- time
- total
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoDa-FL addresses client selection in multi-task federated learning
  within mobile edge computing environments, where heterogeneous clients, non-IID
  data, and task dependencies complicate efficient training. The framework introduces
  EMD-based clustering to group clients with similar data distributions, reducing
  computational complexity and improving communication efficiency.
---

# Cluster-Based Client Selection for Dependent Multi-Task Federated Learning in Edge Computing

## Quick Facts
- arXiv ID: 2510.13132
- Source URL: https://arxiv.org/abs/2510.13132
- Reference count: 15
- Primary result: CoDa-FL reduces total completion time by 5.05% while maintaining higher accuracy across all tasks in multi-task FL with dependencies

## Executive Summary
CoDa-FL introduces a cluster-based client selection framework for multi-task federated learning in mobile edge computing environments with heterogeneous clients and task dependencies. The method clusters clients based on data distribution similarity using Earth Mover's Distance (EMD), reducing intra-cluster heterogeneity and improving convergence speed. A directed acyclic graph (DAG)-based scheduling mechanism ensures task dependencies are respected while enabling parallel execution within dependency layers. The framework combines EMD-based clustering, PPO reinforcement learning for task-to-cluster assignment, and FedAvg for local training, achieving better scalability and efficiency than baseline approaches.

## Method Summary
The method operates in three phases: first, clients are clustered based on their label distribution similarity using EMD and agglomerative hierarchical clustering. Second, a PPO-based scheduler assigns available tasks to idle clusters while respecting DAG dependencies. Third, selected clients perform local FedAvg training and upload gradients to the server. The clustering step reduces the convergence bound by minimizing intra-cluster EMD, while the DAG scheduling enables parallel execution within dependency layers. The approach balances scalability (polynomial complexity) against optimality (avoiding exponential search), achieving practical performance improvements over non-cluster-based selection methods.

## Key Results
- Total completion time reduced by 5.05% compared to closest baseline
- Maintains higher accuracy across all tasks while meeting individual accuracy thresholds
- Demonstrates theoretical relationship between intra-cluster EMD and convergence rounds
- Achieves polynomial complexity (O(U²Q)) versus exponential complexity of optimal selection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clustering clients by data distribution similarity reduces the upper bound on convergence rounds proportionally to intra-cluster Earth Mover's Distance.
- Mechanism: The paper derives that required training rounds R_v(ε_v) grows as O(L_d · Σ q_v,C_i · Δ_C_i / ε_v), where Δ_C_i is cluster-level EMD. By grouping clients with similar label distributions, intra-cluster EMD decreases, directly tightening the convergence bound. Algorithm 1 computes pairwise L1-norm distances between normalized label histograms and applies agglomerative hierarchical clustering.
- Core assumption: Local loss functions have bounded gradient variance and bounded distribution divergence (Section III). Also assumes label distributions sufficiently represent data heterogeneity.
- Evidence anchors:
  - [abstract] "We derive a direct and explicit relationship between intra-cluster EMD and the number of training rounds required for convergence"
  - [section] Equation (25): R_v(ε_v) = O((... + L_d · Σ_i q_v,C_i Δ_C_i) / (με_v))
  - [corpus] Related work PQFed and ColNet address heterogeneity but do not derive EMD-to-convergence bounds; weak direct corpus validation of this specific relationship.
- Break condition: If client data distributions shift significantly during training (concept drift), or if label histograms are poor proxies for feature-level heterogeneity (e.g., same labels, different features), the EMD-to-convergence relationship may degrade.

### Mechanism 2
- Claim: Organizing inter-dependent tasks as a Directed Acyclic Graph with layer-wise parallel execution minimizes total completion time while respecting precedence constraints.
- Mechanism: Tasks are partitioned into L layers where all tasks within a layer execute in parallel, but layer l+1 cannot begin until all tasks in layer l complete. The optimization objective (Eq. 15) minimizes T_total = Σ_l max_{v∈S_l} T_l,v, where the max captures stragglers within each layer.
- Core assumption: Task dependencies are known a priori and static; no task failure or preemption occurs.
- Evidence anchors:
  - [abstract] "We incorporate a directed acyclic graph-based task scheduling mechanism to effectively manage task dependencies"
  - [section] Section II.A: "The DAG is partitioned into L layers, where each layer l ∈ L = {1,2,...,L} contains all tasks that can be executed in parallel"
  - [corpus] VR-VFL addresses client selection under imperfect CSI but does not model task dependencies; limited corpus validation for DAG-based FL scheduling.
- Break condition: If task durations are highly variable or if new tasks arrive dynamically, static DAG scheduling becomes suboptimal. Deadlock is not possible with proper DAG, but latency spikes occur if critical path tasks are misassigned.

### Mechanism 3
- Claim: Reinforcement learning (PPO) with cluster-latency awareness produces near-optimal task-to-cluster assignments without exhaustive search.
- Mechanism: The state encodes task readiness (not ready/ready/running/completed) and cluster occupancy. PPO learns a policy π_θ that maximizes cumulative reward (negative makespan). This avoids O(V^U / V!) combinatorial explosion by learning from trajectory experience.
- Core assumption: Processing time matrix proc_time is predictable from Equations 12-14; channel conditions and compute capacities are stable enough for learned policy to generalize.
- Evidence anchors:
  - [section] Algorithm 2: "Initialize PPO policy π_θ... Sample action a_t ~ π_θ(·|s_t)"
  - [section] Section V: "CoDa-FL reduces the total time by 5.05% compared to the closest baseline"
  - [corpus] AFBS uses buffer-based selection in semi-asynchronous FL; Adaptive Client Selection via Q-Learning (neighbor paper) uses Whittle Index for selection—supports RL applicability but not DAG-dependent scheduling.
- Break condition: If proc_time estimates diverge from reality (e.g., channel fading, stragglers), PPO policy may misallocate. Retraining or online adaptation would be required.

## Foundational Learning

- Concept: **Earth Mover's Distance (EMD) for categorical distributions**
  - Why needed here: Central to the clustering mechanism; quantifies how far a client's label distribution deviates from global/cluster distributions.
  - Quick check question: Given two clients with label distributions P_u = [0.7, 0.3] and P_v = [0.4, 0.6], compute the L1-norm EMD.

- Concept: **FedAvg convergence on non-IID data**
  - Why needed here: The convergence analysis extends standard FedAvg bounds by incorporating distribution divergence (Γ_v term).
  - Quick check question: In FedAvg, what happens to convergence when local datasets are highly non-IID? What term in Equation (22) captures this effect?

- Concept: **DAG scheduling and the critical path**
  - Why needed here: Total completion time is determined by the critical path through the task dependency graph; understanding layer parallelism is essential.
  - Quick check question: Given tasks A→B, A→C, B→D, C→D, what is the minimum number of layers? Which tasks are on the critical path if B takes 10s and C takes 5s?

## Architecture Onboarding

- Component map: Edge Server -> Clustering Module -> DAG Scheduler -> Clients U -> Edge Server
- Critical path:
  1. Collect client label histograms → compute EMD matrix → cluster clients (one-time setup).
  2. For each FL round: DAG scheduler assigns available tasks to idle clusters.
  3. Selected clients download global model → local E-step SGD → upload gradients.
  4. Server aggregates, checks accuracy threshold τ_v, advances DAG layer when all tasks in layer complete.
- Design tradeoffs:
  - **Cluster count N**: More clusters → finer-grained scheduling but higher O(NV) assignment complexity and smaller per-cluster data.
  - **Local steps E**: More steps → less communication but larger client drift term (E−1)²G² in convergence bound.
  - **Non-clustered selection**: Faster total time (427.5s vs 717.4s in experiments) but O(V^U/V!) complexity—computationally prohibitive for U=100, V=4.
- Failure signatures:
  - **High intra-cluster EMD**: Slower convergence per Eq. (25); may miss accuracy thresholds.
  - **Straggler-dominated layers**: max_{u∈C} t^r_u (Eq. 14) bottleneck; one slow client delays entire layer.
  - **PPO policy divergence**: If proc_time estimates drift, schedule becomes suboptimal; retrain or fall back to heuristic scheduling.
- First 3 experiments:
  1. **Validate EMD-clustering effect**: Run CoDa-FL with varying cluster counts (N=2,5,10); measure intra-cluster EMD and convergence rounds per task. Confirm inverse relationship predicted by Eq. (25).
  2. **Ablate DAG scheduling**: Compare PPO scheduler vs. round-robin vs. random task-cluster assignment on the same DAG structure; isolate scheduling contribution to total time reduction.
  3. **Stress test heterogeneity**: artificially skew client label distributions (e.g., 90% single-class clients); verify that clustering maintains performance vs. RC baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the performance gap between CoDa-FL and the theoretically optimal non-cluster-based selection be reduced without incurring the prohibitive computational complexity associated with exhaustive search?
- Basis in paper: [inferred] The paper notes that a non-cluster-based method achieved a total time of 427.5s, significantly outperforming CoDa-FL (717.4s), but was computationally infeasible due to the exponential search space (≈ 4^100).
- Why unresolved: The current approach trades optimality for polynomial complexity (O(U^2Q)), leaving a substantial efficiency gap compared to the theoretical upper bound.
- What evidence would resolve it: A modified heuristic or approximation algorithm that bridges the time-to-completion gap while maintaining polynomial time complexity.

### Open Question 2
- Question: Does the relationship between intra-cluster Earth Mover's Distance (EMD) and convergence speed hold for heterogeneous tasks involving non-image data or significantly different model architectures?
- Basis in paper: [inferred] The experimental validation relies exclusively on four similar image classification datasets (KMNIST, MNIST, FashionMNIST, QMNIST) with identical model architectures.
- Why unresolved: The EMD calculation and convergence bounds (Eq. 25) are derived and tested on tasks with similar feature spaces; it is unclear if the clustering efficiency transfers to tasks with disparate data modalities (e.g., text vs. sensor data).
- What evidence would resolve it: Numerical experiments showing CoDa-FL’s performance on multi-modal task sets or tasks with divergent model complexities.

### Open Question 3
- Question: How does the O(U^2) complexity of the agglomerative clustering mechanism impact the viability of CoDa-FL in massive-scale IoT deployments with millions of clients?
- Basis in paper: [inferred] The method utilizes agglomerative hierarchical clustering on a pairwise distance matrix, which has quadratic complexity (O(U^2 log U)), but the experiments are limited to U=100 clients.
- Why unresolved: While cheaper than exhaustive search, pairwise distance calculations for millions of devices may still pose a significant bottleneck for the edge server.
- What evidence would resolve it: Scalability analysis or a modified clustering approach (e.g., approximate hierarchical methods) tested on client pools exceeding 10,000 devices.

## Limitations
- Computational complexity of clustering mechanism (O(U²)) may limit scalability to massive IoT deployments
- Performance gap remains compared to theoretically optimal non-cluster-based selection methods
- Experimental validation limited to similar image classification tasks with identical model architectures

## Confidence

- **High**: Clustering mechanism (EMD-based), DAG scheduling structure, FedAvg baseline method, theoretical convergence bound derivation
- **Medium**: PPO scheduler performance (dependent on simulation specifics), total time improvement claim (5.05%), handling of task dependencies in practice
- **Low**: Generalization to arbitrary DAGs, robustness to concept drift or dynamic task arrivals, specific hyperparameter choices (N, E)

## Next Checks

1. **Validate EMD-Clustering Convergence Claim**: Reproduce the experiment varying the number of clusters (N=2, 5, 10) and measure the resulting intra-cluster EMD and the number of convergence rounds required for each task. Confirm the inverse relationship predicted by the theoretical bound (Eq. 25).

2. **Ablate DAG Scheduling Contribution**: Implement and compare CoDa-FL's PPO scheduler against simple baselines like round-robin or random task-cluster assignment on the same DAG structure. Measure the isolated contribution of the PPO scheduler to the total completion time reduction.

3. **Stress Test Data Heterogeneity Robustness**: Generate client datasets with extreme label skew (e.g., 90% of clients having a single dominant class). Run CoDa-FL and measure if the clustering mechanism maintains performance relative to a non-clustered baseline, verifying robustness to high heterogeneity.