---
ver: rpa2
title: Statutory Construction and Interpretation for Artificial Intelligence
arxiv_id: '2509.01186'
source_url: https://arxiv.org/abs/2509.01186
tags:
- rule
- response
- rules
- interpretation
- interpretive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Interpretive ambiguity in AI alignment principles leads to inconsistent\
  \ model behavior. This paper proposes two computational frameworks\u2014interpretive\
  \ constraints and rule refinement\u2014to reduce such ambiguity by mirroring legal\
  \ mechanisms for statutory interpretation."
---

# Statutory Construction and Interpretation for Artificial Intelligence

## Quick Facts
- **arXiv ID**: 2509.01186
- **Source URL**: https://arxiv.org/abs/2509.01186
- **Reference count**: 40
- **Primary result**: Prompt-based interpretive constraints and rule refinement significantly improve judgment consistency across AI models, reducing entropy to near zero on held-out WildChat scenarios.

## Executive Summary
Interpretive ambiguity in AI alignment principles leads to inconsistent model behavior. This paper proposes two computational frameworks—interpretive constraints and rule refinement—to reduce such ambiguity by mirroring legal mechanisms for statutory interpretation. Experiments on 5,000 WildChat scenarios show both methods significantly improve judgment consistency across models, with entropy reduced to near zero in refined rules.

## Method Summary
The paper uses a panel of LLMs as "reasonable interpreters" to evaluate AI behavior against constitutional rules. Two interventions are tested: (1) interpretive constraints that inject legal canons (e.g., Narrow, Broad, Harmonization) into judge prompts to constrain interpretations, and (2) iterative rule refinement that minimizes entropy across development scenarios. The framework maps AI alignment to legal institutions, identifying gaps in current pipelines and proposing computational safeguards. Entropy reduction is measured across 5,000 held-out WildChat scenarios.

## Key Results
- Interpretive constraints reduce average entropy from ~0.2 to ~0.05 across five high-entropy rules
- Rule refinement achieves near-zero entropy (~0.01) on held-out scenarios through prompt-based iteration and GRPO optimization
- Harmonization strategy performs best for rules with conflicting interpretations; Narrow strategy works well for rules requiring strict compliance
- Both interventions significantly improve judgment consistency across diverse model types (Qwen, Llama, Gemma)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based interpretive constraints can reduce inter-model disagreement on rule compliance judgments.
- Mechanism: A panel of LLMs is treated as a set of reasonable interpreters. Injecting a specific interpretive strategy (e.g., Narrow, Broad, or legal canons) into the judge prompt constrains the space of valid interpretations each model can adopt. This reduces variance in how models map a fixed rule-scenario pair to a binary judgment, lowering entropy across the panel.
- Core assumption: Models reliably follow the specified interpretive strategy; the strategy meaningfully shapes internal reasoning toward consistent application.
- Evidence anchors:
  - [abstract] "prompt-based interpretive constraints that reduce inconsistency in rule application... both interventions significantly improve judgment consistency across a panel of reasonable interpreters."
  - [section] Figure 5 shows the "No Interpretation" baseline has the highest average entropy across five rules; specifying strategies reduces entropy.
  - [corpus] Related but not confirming: "NRR-Phi: Text-to-State Mapping for Ambiguity Preservation in LLM Inference" discusses premature semantic collapse under ambiguity.
- Break condition: If models fail to adhere to the prompted strategy or if a strategy is ill-suited to a rule, entropy may not decrease. Figure A7 shows strategy effectiveness is rule-dependent; some strategies can increase net entropy.

### Mechanism 2
- Claim: Iterative rule refinement using entropy-minimization as an objective can produce rules that yield more consistent judgments, including on held-out scenarios.
- Mechanism: A refiner model generates candidate revisions of a rule. A judge model, prompted with multiple interpretive strategies, evaluates each candidate on high-entropy development scenarios. The revision minimizing average entropy is selected. This process is repeated via prompt-based iteration or policy gradient optimization (GRPO) with a combined judge-consistency and edit-distance reward.
- Core assumption: The refined rule maintains the original intent and that entropy reduction on the development set generalizes to unseen test scenarios.
- Evidence anchors:
  - [abstract] "entropy reduced to near zero in refined rules."
  - [section] Figures 7 and 8 show entropy dropping from ~0.2–0.4 to ~0.01–0.1 on 5k held-out scenarios.
  - [corpus] No direct corpus evidence for this specific entropy-minimization refinement method.
- Break condition: If the refiner overfits to development scenarios or introduces substantial meaning shifts, the rule may lose alignment with original intent or fail to generalize. Appendix A5 notes some prompt-based revisions were overly narrow or meaning-shifted.

### Mechanism 3
- Claim: Mapping AI alignment stages to legal institutions (legislation, adjudication, enforcement) identifies structural gaps and motivates corresponding computational safeguards.
- Mechanism: The CAI pipeline is conceptually mapped to law: rule creation → legislation; critique/preference judgment → adjudication; weight updates → enforcement. This reveals missing safeguards: no legislative history, no interpretive canons, no appellate review. The mapping directly informs the design of the computational interventions: rule refinement mirrors administrative rulemaking; interpretive constraints mirror judicial canons.
- Core assumption: The problem of interpretive ambiguity in AI is sufficiently analogous to legal interpretation to justify adapting institutional solutions.
- Evidence anchors:
  - [abstract] "Drawing on legal theory, we identify key gaps in current alignment pipelines by examining how legal systems constrain ambiguity."
  - [section] Sections 2–3 explicitly develop the legal analogy and identify missing safeguards.
  - [corpus] "On Verifiable Legal Reasoning" underscores legal reasoning complexity but does not validate this specific institutional analogy.
- Break condition: If AI enforcement (gradient updates) or context differs fundamentally from legal enforcement, borrowed mechanisms may not address root causes. The paper notes this is an initial framework with limitations (Section 9).

## Foundational Learning

- Concept: Shannon entropy as a disagreement metric across a panel of interpreters
  - Why needed here: The paper quantifies interpretive ambiguity as the entropy of the empirical judgment distribution across reasonable interpreters; understanding this is key to interpreting all results.
  - Quick check question: If five judges vote [Yes, Yes, Yes, Yes, Yes], what is the entropy? What about [Yes, No, Yes, No, Yes]?

- Concept: Legal canons of statutory interpretation
  - Why needed here: The 12 interpretive strategies used as constraints are adapted from these canons (e.g., Ordinary Meaning, Omitted Case, Harmonization). Familiarity aids in selecting and understanding strategies.
  - Quick check question: How might the "Negative Implication" canon ("expressio unius est exclusio alterius") change whether a behavior is judged compliant with a rule?

- Concept: Constitutional AI (CAI) alignment pipeline stages
  - Why needed here: The framework targets specific gaps in the CAI pipeline (rule creation, application, enforcement). Distinguishing these stages is necessary to apply interventions correctly.
  - Quick check question: In CAI, at which pipeline stage does the model generate a critique of its own response based on a constitutional principle?

## Architecture Onboarding

- Component map:
  - Judges Panel: Multiple LLMs (e.g., Qwen2.5-32B, Llama-70B, Gemma) or one model with multiple strategies; output binary compliance judgments.
  - Interpretive Strategies: A set of 12 prompts, each encoding a legal canon or theory; applied via the judge prompt.
  - Refiner: A smaller LLM (e.g., Qwen2.5-7B) that proposes rule revisions based on high-entropy scenarios.
  - Entropy Calculator: Aggregates judge panel outputs into an empirical distribution and computes Shannon entropy per scenario and on average.
  - Scenario Sets: Strain, Sdev, Stest, Shigh-ent derived from WildChat; partitioned and filtered.

- Critical path:
  1. Assemble scenario pool (e.g., WildChat) and rules (e.g., adapted Anthropic constitution).
  2. Identify high-entropy rules by evaluating all rules on a sample with the judge panel.
  3. For interpretive constraints: Apply selected strategies to judge prompts for high-entropy rules and measure entropy reduction on Stest.
  4. For rule refinement: Sample high-entropy scenarios (Shigh-ent) for selected rules; run iterative prompt-based or GRPO-based refinement using Sdev; evaluate final revisions on held-out Stest.

- Design tradeoffs:
  - Judge panel diversity vs. compute: A diverse panel captures more interpretive variance but increases inference cost. Using one model with multiple strategies is cheaper but may miss inter-model differences.
  - Refiner model size vs. quality: Larger refiners may produce better revisions but are slower. GRPO-based refinement requires training but can generalize to multiple rules; prompt-based is simpler but rule-specific.
  - Edit-distance regularizer weight (β): Higher β keeps revisions closer to the original text but may limit entropy reduction; lower β allows more aggressive changes but risks meaning drift.

- Failure signatures:
  - High entropy persists after constraint injection: The chosen strategy may be incompatible with the rule or models are not following the strategy.
  - Refined rule entropy low but meaning drifts: Check human evaluation scores; edit-distance regularizer may be too weak.
  - Inconsistent strategy adherence: Analyze judgment flip rates (Figure A5) to see if models are sensitive to strategy prompts.
  - Overfitting to development scenarios: Evaluate on Stest early and often; ensure Shigh-ent and Sdev are representative.

- First 3 experiments:
  1. Replicate interpretive constraint entropy reduction: Take one high-entropy rule (e.g., Rule 21), run the 5-judge panel with and without the top-performing strategy (e.g., Harmonization), and compare average entropy.
  2. Single-rule prompt-based refinement: Run the prompt-based refinement loop for one rule (e.g., Rule 47) for 5 iterations and plot entropy on Sdev per iteration; inspect revised text for meaning preservation.
  3. Ablate edit-distance reward in GRPO: Train two GRPO-based refiners (β=0.0, β=0.2) for one rule; compare entropy reduction and textual similarity to the original rule.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the rule-refinement pipeline scale to large, multi-rule systems while guaranteeing alignment with the rulemaker's original intent?
- Basis in paper: [explicit] The authors state that "Adding intent-based constraints and scaling from small rule sets to larger, multi-judge systems remain open tasks."
- Why unresolved: The current framework prioritizes minimizing entropy (consistency) but does not implement specific mechanisms to verify that refined rules preserve the semantic intent of the original constitution.
- What evidence would resolve it: Experiments measuring semantic drift against human-annotated intent scores when applying the refinement pipeline to full constitutions with 50+ interacting rules.

### Open Question 2
- Question: Do interpretive constraints and rule refinement generalize to specialized domains (e.g., legal or medical) and proprietary model families?
- Basis in paper: [explicit] The limitations section notes, "Applying the framework to broader domains and additional model families is an essential next step."
- Why unresolved: The study relies exclusively on general conversational data (WildChat) and a specific panel of open-source models (Qwen, Llama, Gemma), leaving domain-specific performance unverified.
- What evidence would resolve it: Evaluation of entropy reduction and judgment accuracy on domain-specific benchmarks (e.g., legal bar exams) and closed-source models like GPT-4.

### Open Question 3
- Question: How does incorporating non-legal or culturally diverse normative traditions affect the space of reasonable interpretations and judgment consistency?
- Basis in paper: [explicit] The authors acknowledge their "twelve interpretive strategies... reflect subjective choices" based on US law and suggest "Other normative traditions (ethical, political, cultural) could yield different readings."
- Why unresolved: The current "reasonable interpreter" is defined using canons derived from US statutory interpretation (Scalia and Garner), potentially biasing the definition of consistency toward Western legal norms.
- What evidence would resolve it: A comparative study measuring judgment variance when models are prompted with interpretive strategies derived from diverse ethical frameworks or non-Western legal traditions.

## Limitations
- The framework is validated on a specific dataset (WildChat) and model family, limiting generalizability to other domains or proprietary models.
- The entropy-minimization objective may not guarantee preservation of the original rule's semantic intent, risking meaning drift in refined rules.
- The legal analogy, while conceptually useful, is not empirically tested for depth and may not fully capture AI-specific enforcement challenges.

## Confidence
- **High confidence**: The core mechanism of using interpretive constraints to reduce entropy is empirically validated across 5,000 scenarios and multiple model types. The entropy reduction is large and consistent.
- **Medium confidence**: The rule refinement process, especially the GRPO variant, is well-motivated and shows promise, but relies on a reward function that balances entropy and edit distance without fully validating meaning preservation.
- **Medium confidence**: The legal analogy is conceptually sound and useful for framing, but its depth is not empirically tested; it serves as a motivation rather than a proven component of the solution.

## Next Checks
1. Test entropy generalization: Evaluate refined rules on an independent, held-out set of scenarios from a different source (e.g., not WildChat) to check for overfitting.
2. Validate meaning preservation: Conduct a larger-scale human evaluation (e.g., >50 annotators) to measure whether entropy-minimizing revisions alter the original rule's intent.
3. Probe strategy robustness: Systematically vary judge model prompts and judge panel composition to determine how sensitive entropy reduction is to prompt phrasing and model diversity.