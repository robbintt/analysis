---
ver: rpa2
title: Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware
  Unlearning with Proxy Constraint
arxiv_id: '2508.20443'
source_url: https://arxiv.org/abs/2508.20443
tags:
- unlearning
- egup
- forget
- retain
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EGUP (Entanglement-Guided Unlearning with Proxy Constraint) is
  a novel unlearning framework for LLMs that addresses excessive forgetting by leveraging
  inter-sample and intra-sample entanglement to adaptively guide the unlearning process.
  The method uses embedding-space similarity to modulate unlearning strength for each
  sample and tracks representation drift across iterations to prevent over-forgetting.
---

# Towards Mitigating Excessive Forgetting in LLM Unlearning via Entanglement-Aware Unlearning with Proxy Constraint

## Quick Facts
- arXiv ID: 2508.20443
- Source URL: https://arxiv.org/abs/2508.20443
- Reference count: 40
- One-line primary result: EGUP achieves up to 0.7934 forget quality with 0.4259 model utility under 5% forgetting, outperforming baselines and approaching full retraining performance.

## Executive Summary
EGUP (Entanglement-Guided Unlearning with Proxy Constraint) is a novel unlearning framework for LLMs that addresses excessive forgetting by leveraging inter-sample and intra-sample entanglement to adaptively guide the unlearning process. The method uses embedding-space similarity to modulate unlearning strength for each sample and tracks representation drift across iterations to prevent over-forgetting. A proxy constraint generated via in-context learning provides a soft regularization signal to prevent the model from deviating too far from its expected behavior on forget samples. EGUP is compatible with existing gradient-based unlearning objectives and serves as a plug-and-play enhancement.

## Method Summary
EGUP combines three mechanisms: (1) inter-sample entanglement reweighting that adjusts per-sample forgetting strength based on cosine similarity to retain data embeddings, (2) intra-sample entanglement tracking that monitors representation drift across iterations using contrastive loss, and (3) ICL-generated proxy constraints that penalize over-unlearning by comparing current performance against expected behavior. The framework operates as a wrapper around gradient-based unlearning objectives (GA, GD, NPO) by modifying the forget loss with sample-specific weights and adding regularization terms. During each iteration, the method computes averaged retain embeddings, calculates entanglement measures for each forget sample, generates proxy answers via ICL, and applies weighted gradients with additional constraints to achieve balanced unlearning.

## Key Results
- Achieves 0.7934 forget quality with 0.4259 model utility on TOFU-5% setting, significantly outperforming baseline NPO+GD (0.0878/0.4158)
- Maintains utility while improving forget quality across all three forget rates (1%, 5%, 10%) on TOFU benchmark
- Demonstrates robustness across multiple model architectures including Phi-1.5, LLaMA2-7B, and ICLM-7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Samples semantically closer to retained knowledge require less aggressive unlearning effort, reducing over-forgetting on entangled samples.
- Mechanism: At each iteration, compute cosine similarity between each forget sample embedding and the averaged retain embedding. Transform via exp(-sim) and softmax to produce per-sample weights. Higher similarity → lower weight → gentler gradient updates on that sample.
- Core assumption: Forget samples similar to retain data in embedding space would be well-predicted by a retrained model, so they need minimal modification.
- Evidence anchors:
  - [abstract]: "entanglement-awareness guided loss reweighting determines the forgetting effort of each sample by measuring its similarity to retain samples in the embedding space"
  - [section 5.1]: "w_i ← exp(−sim_i)" and "samples with lower similarity to the retain data require stronger unlearning"
  - [corpus]: GUARD paper addresses similar data attribution for unlearning; FROC mentions risk-optimized control—both suggest entanglement-aware weighting is an active research direction, but no direct validation of this specific formulation exists externally.
- Break condition: If retain embeddings shift dramatically during unlearning (e.g., due to retain loss being unstable), the similarity signal degrades and reweighting becomes unreliable.

### Mechanism 2
- Claim: Tracking representation drift across iterations smooths the unlearning trajectory and prevents catastrophic parameter jumps.
- Mechanism: Compute intra-sample entanglement as sim_prev = cos(E^t_x, E^{t-1}_x). Add contrastive loss L_c = -log[exp(sim)/(exp(sim) + exp(sim_prev))] that encourages moderate divergence. Early iterations: high sim_prev → stronger gradient acceleration. Later: lower sim_prev → diminishing contrastive penalty → natural stabilization.
- Core assumption: Cross-batch interference causes global representation drift; monitoring it provides a signal for adaptive modulation.
- Evidence anchors:
  - [section 5.1]: "Intra-sample entanglement captures the temporal evolution of each forget embedding across iterations"
  - [Figure 3]: Shows cross-batch interference—loss on untouched samples rises while embedding similarity declines during unlearning
  - [corpus]: No direct external validation of this specific temporal regularization approach found in related papers.
- Break condition: If learning rate is too high, representations may shift too rapidly for sim_prev to provide meaningful regularization before damage occurs.

### Mechanism 3
- Claim: ICL-generated proxy answers approximate the retrained model's expected behavior, providing a soft boundary that penalizes over-unlearning before it occurs.
- Mechanism: Use an LLM with few-shot exemplars to generate proxy answers for forget samples. During unlearning, if current forget loss drops below proxy loss (model "overshoots" expected ignorance), add penalty P = μ × (proxy_loss - forget_loss). This activates as an early warning signal.
- Core assumption: ICL can simulate what a model would predict if it had never seen the forget data, providing a target distribution to constrain against.
- Evidence anchors:
  - [section 5.2]: "By penalizing deviations from these proxy targets, the mechanism provides principled guidance to constrain unnecessary loss escalation"
  - [Figure 4]: Penalty loss rises concurrently with retain loss spikes around step 18 in 5% setting, serving as "early warning signal"
  - [corpus]: Related work (e.g., Forgetting-MarI, Constrained Entropic Unlearning) explores regularization-based unlearning, but ICL-generated proxy constraints specifically are not validated externally.
- Break condition: If proxy answers are poorly calibrated (e.g., ICL exemplars don't match target distribution), the constraint may either over-regularize (under-unlearning) or fail to activate (over-unlearning).

## Foundational Learning

- **Concept: Gradient-based unlearning objectives (GA, GD, NPO)**
  - Why needed here: EGUP is designed as a plug-and-play wrapper around existing objectives. Understanding how gradient ascent (GA), gradient difference (GD), and negative preference optimization (NPO) work is essential to see where EGUP's reweighting and constraints are applied.
  - Quick check question: Can you explain why NPO uses only negative responses as supervision while GD includes a retain loss term?

- **Concept: Embedding-space representations and cosine similarity**
  - Why needed here: Both inter-sample and intra-sample entanglement rely on computing sentence-level embeddings and their cosine similarities. Without this, the weighting mechanism is opaque.
  - Quick check question: Given two embedding vectors a and b, how would you compute their cosine similarity, and what does a value of 0.9 vs -0.3 mean semantically?

- **Concept: In-Context Learning (ICL) for proxy generation**
  - Why needed here: The proxy constraint depends on prompting an LLM with few-shot exemplars to generate expected outputs. Understanding ICL helps diagnose when proxy answers might be unreliable.
  - Quick check question: What factors affect the quality of ICL outputs, and how might exemplar selection bias proxy answers?

## Architecture Onboarding

- **Component map:**
  Forget Dataset (D_f) -> Embedding Extraction -> Inter-sample Entanglement (cos with avg retain embedding) -> Loss Reweighting (w_i = softmax(k × exp(-sim))) -> Weighted Unlearning Loss
  Retain Dataset (D_r) -> Avg Embedding -> (used for inter-sample sim)
  Previous Iteration -> Intra-sample Entanglement (cos with prior embedding) -> Contrastive Loss (L_c)
  Proxy Generator (ICL LLM) -> Proxy Answers for D_f -> Proxy Constraint Penalty (P)
  Total Loss = Weighted Forget + α×Retain + μ×L_c + P

- **Critical path:**
  1. Compute averaged retain embedding once per iteration (cheap if pre-cached).
  2. For each forget sample in batch: compute embedding, inter-sample similarity, intra-sample similarity (requires caching prior iteration embeddings), and loss weight.
  3. Generate proxy answers if not pre-computed (expensive; can batch).
  4. Compute all loss terms and backprop.

- **Design tradeoffs:**
  - **Proxy generation cost vs. quality**: Batching ICL queries reduces overhead but may reduce per-sample calibration quality.
  - **Embedding cache memory vs. freshness**: Storing prior-iteration embeddings is required for intra-sample entanglement but increases memory.
  - **Temperature k**: Higher k sharpens weight distribution (more selective unlearning); lower k smooths it (more uniform). Paper uses grid search {0.5, 1, 2}.
  - **Penalty weight μ**: Controls strength of proxy constraint. Too high → under-unlearning; too low → over-unlearning risk. Paper uses {0.001, 0.005}.

- **Failure signatures:**
  - Forget quality near 0 with retain quality also near 0: Over-unlearning; check if proxy penalty never activates (μ too low) or retain loss term is too weak (α too small).
  - Forget quality low, retain quality high: Under-unlearning; check if proxy penalty activates too early (μ too high) or weights are too conservative (k too low).
  - Instability in loss curves: Check learning rate and whether intra-sample entanglement signal is noisy (embedding dimension issues or insufficient model capacity).

- **First 3 experiments:**
  1. **Reproduce baseline**: Run NPO+GD on TOFU-5% with Phi-1.5 without EGUP to establish baseline forget quality and model utility. Verify your numbers match paper's F.Q.=0.0878, M.U.=0.4158.
  2. **Ablate entanglement only**: Run EGU (no proxy constraint) with NPO+GD to isolate inter- and intra-sample contribution. Expect improved forget quality but some retain degradation per Figure 8.
  3. **Full EGUP with proxy**: Add proxy constraint with μ=0.001 and k=1. Measure forget quality and model utility; target is F.Q.~0.79 with M.U.~0.43 per Table 1. If results diverge, check proxy answer quality manually.

## Open Questions the Paper Calls Out
- Can entanglement guidance be extended to the token level for finer-grained unlearning in semantically dense domains like books?
- Can proxy constraints be made self-adaptive rather than relying on fixed penalty weights and manual hyperparameter tuning?
- How robust is the ICL-based proxy constraint when the external LLM has limited alignment with the target model's behavior distribution?

## Limitations
- Embedding-space similarity may not accurately capture semantic relevance between forget and retain samples in high-dimensional spaces
- Proxy constraint quality depends heavily on ICL exemplar selection and prompt engineering, with no characterization of failure modes
- Intra-sample entanglement tracking assumes predictable representation drift, which may not hold for models with different architectures or learning rates

## Confidence
- **High confidence**: The core framework design (inter-sample and intra-sample entanglement weighting) is mathematically sound and the loss formulation is clear.
- **Medium confidence**: The effectiveness of the proxy constraint mechanism depends heavily on ICL quality, which isn't fully characterized in the paper.
- **Medium confidence**: The embedding similarity approach for measuring entanglement is reasonable but may not generalize across domains with different semantic structures.

## Next Checks
1. **Sensitivity analysis on proxy constraint**: Systematically vary the number and quality of ICL exemplars to measure impact on forget quality and model utility—does EGUP performance degrade gracefully with worse proxy answers?
2. **Embedding space validation**: Visualize forget/retain embedding distributions before and after unlearning to verify that similarity-based weighting actually tracks meaningful semantic relationships.
3. **Cross-model robustness**: Apply EGUP to a non-Phi model (e.g., LLaMA2-7B) and measure whether the same hyperparameters (k=1, μ=0.001) achieve comparable performance, or if entanglement patterns differ significantly.