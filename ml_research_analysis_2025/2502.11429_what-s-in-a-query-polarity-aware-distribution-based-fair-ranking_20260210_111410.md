---
ver: rpa2
title: 'What''s in a Query: Polarity-Aware Distribution-Based Fair Ranking'
arxiv_id: '2502.11429'
source_url: https://arxiv.org/abs/2502.11429
tags:
- fairness
- ranking
- attention
- query
- individual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces DistFaiR, a new fairness metric for amortized
  ranking that measures the divergence between attention and relevance distributions
  across sequences of queries. It identifies divergence measures where individual
  fairness upper bounds group fairness, showing empirically that optimizing individual
  fairness often improves group fairness.
---

# What's in a Query: Polarity-Aware Distribution-Based Fair Ranking

## Quick Facts
- arXiv ID: 2502.11429
- Source URL: https://arxiv.org/abs/2502.11429
- Authors: Aparna Balagopalan, Kai Wang, Olawale Salaudeen, Asia Biega, Marzyeh Ghassemi
- Reference count: 40
- Primary result: Introduces DistFaiR, a fairness metric measuring divergence between attention and relevance distributions across query sequences

## Executive Summary
This paper introduces DistFaiR, a novel fairness metric for amortized ranking that measures the divergence between attention and relevance distributions across sequences of queries. The work identifies divergence measures where individual fairness upper bounds group fairness, showing empirically that optimizing individual fairness often improves group fairness. The authors also highlight fairwashing risks when query polarity is ignored and propose polarity-aware extensions to fairness metrics. Experiments across synthetic and real-world datasets demonstrate that DistFaiR outperforms baselines in reducing worst-case unfairness while maintaining or improving group fairness.

## Method Summary
The authors propose a distribution-based fairness framework that evaluates ranking systems across sequences of queries rather than individual queries. DistFaiR measures the divergence between attention distributions (how the model ranks items) and relevance distributions (ground truth user preferences) for each group, using measures like KL divergence and Hellinger distance. The framework introduces polarity-aware extensions that account for positive versus negative query contexts, addressing fairwashing risks. The approach involves computing divergence measures across query sequences, aggregating results by group, and optimizing for both individual and group fairness objectives through custom loss functions.

## Key Results
- DistFaiR outperforms baselines in reducing worst-case unfairness while maintaining or improving group fairness
- Individual fairness optimization leads to improvements in group fairness under specific divergence measures
- Query polarity significantly impacts fairness measurements, with polarity-aware metrics preventing fairwashing
- Empirical validation across synthetic and real-world (Yelp) datasets shows consistent improvements

## Why This Works (Mechanism)
The mechanism works by capturing the distributional properties of ranking outcomes across multiple queries rather than relying on single-query evaluations. By measuring divergence between attention and relevance distributions, the framework identifies systematic biases that emerge over query sequences. The individual fairness bound on group fairness occurs because certain divergence measures (like KL divergence and Hellinger distance) have mathematical properties that constrain group-level disparities when individual-level disparities are minimized. The polarity-aware extensions work by separately analyzing positive and negative query contexts, revealing when systems appear fair in aggregate but exhibit discriminatory patterns within specific contexts.

## Foundational Learning

**Divergence measures (KL divergence, Hellinger distance)** - Why needed: These quantify the difference between probability distributions of rankings. Quick check: Verify that the chosen divergence satisfies the data processing inequality for the fairness bounds to hold.

**Amortized ranking** - Why needed: Evaluates ranking systems across sequences rather than single queries to capture systematic patterns. Quick check: Ensure the query sequence length is sufficient to capture distributional differences.

**Individual vs group fairness** - Why needed: Individual fairness focuses on similar individuals receiving similar treatment, while group fairness ensures statistical parity across groups. Quick check: Confirm that individual fairness bounds apply to your chosen divergence measure.

**Query polarity** - Why needed: Positive and negative queries (e.g., "best restaurants" vs "restaurants to avoid") can reveal different fairness patterns. Quick check: Analyze whether your dataset contains meaningful query polarity distinctions.

**Distribution matching** - Why needed: The core optimization objective aligns attention and relevance distributions. Quick check: Validate that the matched distributions remain valid probability distributions after optimization.

## Architecture Onboarding

**Component map:** Data → Divergence Computation → Aggregation → Optimization → Fair Ranking Model
```
Data Preprocessing -> Divergence Calculation -> Group Aggregation -> Loss Optimization -> Model Update
```

**Critical path:** The most critical computational path is the divergence calculation across query sequences, as it directly impacts both the fairness measurement and the optimization objective.

**Design tradeoffs:** The framework trades computational efficiency for more comprehensive fairness evaluation. Computing divergences across sequences is more expensive than single-query metrics but captures systematic biases more effectively.

**Failure signatures:** The model may fail when divergence measures are sensitive to small sample sizes in certain groups, or when query sequences don't adequately represent the population distribution.

**First experiments:** 1) Compare single-query vs sequence-based fairness metrics on synthetic data with known biases, 2) Test sensitivity of different divergence measures to sample size variations, 3) Evaluate polarity-aware vs polarity-agnostic metrics on balanced vs imbalanced datasets.

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Theoretical guarantees primarily apply to specific divergence measures (KL divergence, Hellinger distance) with stronger distributional assumptions than acknowledged
- Empirical validation relies heavily on synthetic data with limited real-world diversity (primarily Yelp dataset)
- Generalizability across different ranking domains and query types remains uncertain
- The framework's computational overhead may limit scalability to large-scale ranking systems

## Confidence
- **High** confidence in identifying fairwashing risks when query polarity is ignored
- **Medium** confidence in empirical performance improvements of DistFaiR over baselines given limited real-world evaluation scope
- **Medium** confidence in theoretical claims connecting individual and group fairness bounds due to specific distributional assumptions

## Next Checks
1) Test the framework across diverse real-world ranking datasets beyond Yelp, particularly in high-stakes domains like hiring or education
2) Evaluate robustness to different divergence measures and their sensitivity to distributional assumptions
3) Conduct ablation studies to isolate the contribution of query polarity awareness versus other design choices in the proposed metrics