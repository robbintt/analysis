---
ver: rpa2
title: 'Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward
  Models'
arxiv_id: '2506.09532'
source_url: https://arxiv.org/abs/2506.09532
tags:
- wang
- reward
- reasoning
- athena-prm
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Athena-PRM, a multimodal process reward model
  designed to evaluate step-by-step reasoning in complex problems. To address the
  challenge of generating high-quality process-labeled data efficiently, the authors
  propose leveraging prediction consistency between weak and strong completers to
  filter noisy labels, significantly reducing computational costs while improving
  label accuracy.
---

# Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models

## Quick Facts
- arXiv ID: 2506.09532
- Source URL: https://arxiv.org/abs/2506.09532
- Authors: Shuai Wang; Zhenhua Liu; Jiaheng Wei; Xuanwu Yin; Dong Li; Emad Barsoum
- Reference count: 40
- One-line primary result: Athena-PRM achieves state-of-the-art performance on VisualProcessBench with only 5K samples, outperforming models trained on 300K+ samples.

## Executive Summary
Athena introduces a multimodal process reward model that evaluates step-by-step reasoning in complex problems. The key innovation is a data-efficient training approach that uses prediction consistency between weak and strong completers to filter noisy process labels, reducing data requirements from hundreds of thousands to just 5,000 high-quality samples. Combined with ORM initialization and negative data up-sampling, Athena-PRM achieves superior performance across multiple evaluation scenarios including test-time scaling, direct step evaluation, and reward-ranked fine-tuning.

## Method Summary
Athena-PRM is trained using a three-stage approach: first, an outcome reward model (ORM) is trained on 600K query-response pairs; second, a process reward model (PRM) is initialized from the ORM and fine-tuned on 5K consistency-filtered step-labeled samples; third, negative examples are up-sampled by 2× during PRM training. Process labels are generated through Monte Carlo estimation using weak (3B) and strong (72B) completer pairs, with only steps where both agree being retained. The model is evaluated using minimum step reward aggregation for Best-of-N ranking across 7 multimodal and 2 text benchmarks.

## Key Results
- Achieves state-of-the-art F1-score on VisualProcessBench with only 5K samples
- Outperforms models trained on 300K+ samples while requiring 1/45 the GPU hours for synthesis
- Demonstrates consistent improvements across test-time scaling, direct step evaluation, and reward-ranked fine-tuning
- Shows performance scaling from 5K to 60K samples with diminishing returns beyond 60K

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Filtering step labels through prediction consistency between weak and strong completers substantially improves label quality while reducing data requirements.
- Mechanism: Monte Carlo estimation produces labels biased by completer capability—a strong completer can reach correct answers from incorrect intermediate steps, while a weak completer may fail even with correct steps. The paper retains only steps where both completers assign the same label, removing completer-dependent bias. This yields a small, high-quality dataset where labels reflect intrinsic step correctness rather than completer strength.
- Core assumption: Ground-truth step correctness is independent of which completer performs the estimation; disagreement indicates unreliable labels.
- Evidence anchors:
  - [abstract]: "leveraging prediction consistency between weak and strong completers to filter noisy labels, significantly reducing computational costs while improving label accuracy"
  - [section 2.2, Table 6]: Accuracy improves from 78.2% (weak-only) and 83.4% (strong-only) to 94.1% with consistency filtering using Mistral-7B (weak) and Qwen2.5-72B (strong).
  - [corpus]: Related PRM papers (VisualPRM, MM-PRM) do not validate this specific weak-strong filtering mechanism; corpus evidence is absent.
- Break condition: If weak and strong completers share systematic biases (e.g., both fail on the same step types), consistency could amplify shared errors rather than filter noise.

### Mechanism 2
- Claim: Initializing PRMs from pre-trained ORMs improves step-level evaluation accuracy.
- Mechanism: ORMs trained on large-scale sample-level data capture coarse-grained patterns of solution quality. Initializing PRMs from this checkpoint treats ORM training as "pre-training" and subsequent PRM training on step-level data as "fine-tuning," enabling PRMs to leverage outcome-level signals when step-level supervision is sparse.
- Core assumption: Outcome supervision implicitly encodes learnable signal about intermediate step quality that transfers to step-level prediction.
- Evidence anchors:
  - [section 2.3]: "initializing PRMs with ORMs trained on large-scale sample-level annotated data improves performance"
  - [Table 5]: PRM with ORM initialization achieves 74.8 vs. 74.1 without (+0.7 on MathVista).
  - [corpus]: No corpus papers directly validate ORM-to-PRM transfer; evidence is paper-internal only.
- Break condition: If ORM relies on spurious outcome-level correlations (e.g., answer length), transfer may harm PRM calibration.

### Mechanism 3
- Claim: Up-sampling negative step labels improves PRM performance by mitigating label imbalance.
- Mechanism: Process-labeled datasets are positively imbalanced (79–81% correct steps). Without correction, models can develop a positive prediction bias. Up-sampling negative examples equalizes the learning signal, improving discrimination on error detection.
- Core assumption: Training imbalance translates to model bias that harms step-level discrimination; rebalancing improves generalization.
- Evidence anchors:
  - [Table 5]: Adding up-sampling to ORM-initialized PRM improves from 74.8 to 75.2 on MathVista (+0.4).
  - [Table 9]: Athena-5K has 79% positive, 21% negative labels; Table 10 shows 2× up-sampling is optimal.
  - [corpus]: No corpus validation of negative up-sampling for PRMs; evidence is paper-internal only.
- Break condition: If negative samples are over-represented relative to test distribution, the model may become overly conservative in scoring steps.

## Foundational Learning

- Concept: Monte Carlo (MC) estimation for process labels
  - Why needed here: Athena's filtering mechanism directly targets noise in MC-based labels; understanding MC estimation is prerequisite to understanding why consistency filtering helps.
  - Quick check question: Given a reasoning step, how would a completer estimate its correctness by sampling multiple trajectories to the final answer?

- Concept: Outcome vs. Process Reward Models
  - Why needed here: The ORM→PRM transfer mechanism assumes ORMs encode useful signal for step-level evaluation; distinguishing ORM and PRM objectives clarifies the transfer hypothesis.
  - Quick check question: What does an ORM predict versus what does a PRM predict for a given solution?

- Concept: Best-of-N test-time scaling
  - Why needed here: The primary evaluation scenario in the paper is selecting the highest-reward solution among N candidates; understanding this setup is essential to interpret performance gains.
  - Quick check question: In a Best-of-N setting, how should a PRM aggregate step-level scores into a solution-level ranking?

## Architecture Onboarding

- Component map: Dataset collection → multi-policy response sampling → dual-completer MC estimation → consistency filtering → ORM training → PRM fine-tuning → evaluation in Best-of-N, direct judgment, and reward-ranked fine-tuning
- Critical path: Data curation pipeline aggregates multimodal and text datasets, samples 8 responses per query using multiple policy models, applies filtering (length, n-gram dedup), generates step labels via Monte Carlo with weak/strong completers, filters through consistency check, trains ORM on 600K samples, initializes PRM from ORM, fine-tunes on 5K filtered samples with negative up-sampling, and aggregates rewards using minimum step reward for Best-of-N ranking.
- Design tradeoffs:
  - Data scale vs. quality: 5K filtered samples match 300K vanilla MC-labeled samples in performance but require 1/45 GPU hours for synthesis
  - Completer selection: 3B/72B pairing used; combining two weak or two strong completers also improves accuracy (82–86%) but weak-strong achieves highest (94–95% per Table 6)
  - Up-sampling rate: 2× optimal; higher rates (5×, 10×) showed no further gain
- Failure signatures:
  - Low consistency rate: If weak and strong completers rarely agree, the filtered dataset may be too small to train effectively
  - ORM overfitting: If ORM training saturates on easy patterns, PRM fine-tuning may not benefit from transfer
  - Positive prediction bias: If validation shows high false-positive rate on step correctness, increase negative up-sampling
- First 3 experiments:
  1. Validate consistency filtering on a held-out set with human-annotated step labels; measure precision/recall of retained vs. filtered labels
  2. Compare three training configurations on a fixed validation set: PRM from scratch, PRM from ORM checkpoint, and PRM from ORM with negative up-sampling
  3. Sweep Best-of-N (N=4,8,16,32,64) on MathVista to verify scaling behavior and confirm PRM-minimum aggregation outperforms PRM-last and PRM-product

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does the consistency filtering strategy fail to benefit from using more than two completers?
- **Basis in paper:** [explicit] Appendix C.1 states that the authors explored using more completers to further improve label quality but "did not observe any significant improvement, despite the increased computational cost."
- **Why unresolved:** The paper notes the empirical failure of this extension but does not provide a theoretical or ablation-based explanation for why pairwise consistency is sufficient or why additional signals become redundant.
- **What evidence would resolve it:** An analysis of the overlap in error patterns between multiple completers, showing whether additional models provide redundant rather than novel supervisory signals.

### Open Question 2
- **Question:** Does the performance of Athena-PRM continue to scale linearly or plateau beyond 60K samples?
- **Basis in paper:** [inferred] Section 3.4 and Figure 3 demonstrate performance scaling from 5K to 60K samples, showing consistent growth, but the experiment stops at this threshold.
- **Why unresolved:** While the paper argues for the data efficiency of 5K samples, the upward trend at 60K suggests that larger data regimes might yield further gains, leaving the saturation point undefined.
- **What evidence would resolve it:** Training and evaluation results for Athena-PRM using datasets of 100K, 250K, and 500K filtered samples.

### Open Question 3
- **Question:** How sensitive is the method to the specific capability gap between the weak and strong completers?
- **Basis in paper:** [inferred] The method relies on the assumption that strong completers succeed where weak ones fail (Page 2, Figure 1), but Table 6 only shows results for specific pairs (e.g., 7B vs 72B) without analyzing the optimal margin of capability.
- **Why unresolved:** It is unclear if a smaller gap (e.g., 7B vs 8B) or a different architecture pairing would yield similar filtering accuracy or if a strict "weak vs strong" hierarchy is required.
- **What evidence would resolve it:** A systematic ablation study testing label accuracy across various model size pairings (e.g., 3B/7B, 7B/32B, 32B/72B).

## Limitations
- The consistency filtering mechanism depends on the critical assumption that ground-truth step correctness is independent of which completer performs the evaluation
- ORM initialization effectiveness relies on outcome-level signals genuinely transferring to step-level prediction without introducing spurious correlations
- The claim that 5K samples match 300K vanilla MC-labeled samples in performance is based on internal comparisons that may not generalize across different model architectures
- The negative up-sampling mechanism assumes training imbalance directly causes model bias, which may not hold if test distribution differs from training distribution

## Confidence
- **High confidence**: Claims about dataset composition and basic training procedures are well-specified and reproducible
- **Medium confidence**: The weak-strong consistency filtering mechanism is supported by internal results (94.1% accuracy in Table 6) but lacks external validation from the corpus and depends on the critical assumption about label independence
- **Medium confidence**: ORM initialization and negative up-sampling show consistent internal improvements (+0.7 and +0.4 on MathVista) but are only validated within the paper without corpus support
- **Low confidence**: The claim that 5K samples match 300K vanilla MC-labeled samples in performance is based on internal comparisons that may not generalize across different model architectures or problem domains

## Next Checks
1. **Validate consistency filtering with human annotations**: Take a random subset of steps from Athena-5K and have human annotators label step correctness independently. Compare agreement rates between human labels and both weak/strong completer labels to measure precision/recall of the consistency filter and test the independence assumption.

2. **Cross-model transfer of ORM initialization**: Train an ORM on the same Athena-600K dataset using a different base model (e.g., LLaVA-Next-7B instead of Qwen2.5-VL-7B), then initialize a PRM from this checkpoint and evaluate on the same benchmarks. This would test whether the ORM-to-PRM transfer effect is model-dependent or represents a more general phenomenon.

3. **Systematic bias analysis in weak-strong completers**: For a stratified sample of reasoning failures, analyze whether weak and strong completers fail on the same step types (visual parsing, arithmetic, etc.). If systematic shared biases exist, this would indicate the consistency filter may not reliably remove noise and could instead amplify completer-specific blind spots.