---
ver: rpa2
title: Enhancing Korean Dependency Parsing with Morphosyntactic Features
arxiv_id: '2503.21029'
source_url: https://arxiv.org/abs/2503.21029
tags:
- korean
- parsing
- morphosyntactic
- features
- syntactic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a UniDive dataset for Korean that integrates
  Universal Dependencies and Universal Morphology to enhance morphosyntactic representation.
  The dataset addresses inconsistencies in existing Korean linguistic resources by
  incorporating fine-grained morphological features into syntactic annotation, particularly
  for an agglutinative language with rich inflectional morphology and flexible word
  order.
---

# Enhancing Korean Dependency Parsing with Morphosyntactic Features

## Quick Facts
- arXiv ID: 2503.21029
- Source URL: https://arxiv.org/abs/2503.21029
- Reference count: 9
- Primary result: Korean dependency parsing accuracy improves by 14.09 LAS points with explicit morphosyntactic features in encoder-only models, and 2.6 LAS points in decoder-only models

## Executive Summary
This paper introduces UniDive, a Korean dependency parsing dataset that integrates Universal Dependencies with Universal Morphology to provide fine-grained morphosyntactic features. The work addresses inconsistencies in existing Korean linguistic resources by correcting POS mislabeling and extracting detailed morphological features (CASE, MOOD, TENSE, VOICE, etc.) from the agglutinative surface forms. Experiments with both encoder-only (UDPipe) and decoder-only (Bllossom-3B) models demonstrate that explicit morphological information significantly improves parsing accuracy, with the largest gains observed in the encoder-only architecture. The dataset is distributed under CC BY-SA 4.0 license.

## Method Summary
The method involves two main stages: dataset preparation and model training. First, the Korean Universal Dependencies treebank undergoes POS correction (fixing common misclassifications like NOUN→PROPN and VERB→ADJ) and morphosyntactic feature extraction using rule-based mappings from morpheme-level analysis to UniMorph feature tags. Second, two model types are trained: UDPipe 1 as an encoder-only baseline, and Bllossom-3B fine-tuned via instruction tuning in Alpaca format. The decoder-only model uses AdamW-8bit optimizer with cosine learning rate schedule, bfloat16 precision, and gradient accumulation on a single NVIDIA A6000 GPU.

## Key Results
- Encoder-only UDPipe model improves from 50.24 to 64.33 LAS (+14.09 points) with morphosyntactic features
- Decoder-only Bllossom-3B improves from 84.37 to 86.97 LAS (+2.6 points) with morphosyntactic features
- POS corrections include ADV→NOUN (3,607 instances) and NOUN→PROPN (2,654 instances)
- Experiments use single NVIDIA A6000 GPU with ~47 minutes per epoch

## Why This Works (Mechanism)

### Mechanism 1
Explicit morphosyntactic feature injection improves dependency parsing accuracy, with larger gains in encoder-only architectures. UniMorph-derived features (CASE, MOOD, TENSE, VOICE, etc.) are attached to tokens and made available during training, giving the parser direct access to grammatical cues that would otherwise need to be inferred from surface forms. Core assumption: Korean's agglutinative morphology encodes syntactic relations in suffixes; exposing these explicitly reduces ambiguity for models with weaker implicit morphological knowledge.

### Mechanism 2
Systematic correction of POS mislabeling (especially NOUN vs PROPN, VERB vs ADJ) improves dependency structure learning. Manual and rule-based verification re-aligns XPOS/UPOS tags with lexical identity rather than contextual function, reducing label noise in training data. Core assumption: Noisy POS tags propagate errors into dependency relations; cleaner labels yield cleaner supervised signal.

### Mechanism 3
Instruction tuning adapts decoder-only LLMs to structured dependency output, with morphosyntactic features providing complementary refinement. Korean UniDive data is converted to instruction–input–output triples; loss is computed only on output tokens, teaching the model to generate CoNLL-style parses given enriched token sequences. Core assumption: Decoder-only models already capture some morphology from pretraining; explicit features act as refinements rather than primary signals.

## Foundational Learning

- **Universal Dependencies (UD) annotation scheme** - Why needed: The entire UniDive dataset builds on UD; understanding HEAD, DEPREL, UPOS, and FEATS columns is prerequisite to reading/writing CoNLL-U files. Quick check: Given a Korean sentence, can you identify which word is the root and label at least two dependency relations (e.g., nsubj, obj)?

- **Korean agglutinative morphology (suffix-based case, mood, tense markers)** - Why needed: Morphosyntactic features in UniDive are extracted from surface suffixes; without this, feature assignment rules will be opaque. Quick check: In "학교에서 공부한다" (studies at school), which suffix marks case and which marks mood/tense?

- **Instruction tuning for structured generation** - Why needed: The decoder-only experiments use Alpaca-style IT format; understanding loss masking on output-only tokens is essential for reproduction. Quick check: In an (instruction, input, output) triple, on which portion should cross-entropy loss be computed, and why?

## Architecture Onboarding

- **Component map**: Korean GSD treebank → POS/morphology correction → UniMorph feature extraction → CoNLL-U with enriched FEATS → UDPipe/Bllossom-3B training → UAS/LAS evaluation

- **Critical path**: 1) Verify UD Korean GSD treebank integrity and splits; 2) Apply POS/morphology corrections per Section 5.1 rules; 3) Extract and attach UniMorph features using Table 1 rules; 4) Train UDPipe baseline with/without FEATS; 5) Train Bllossom-3B with/without FEATS in IT format; 6) Evaluate UAS/LAS; 7) Ablate feature subsets

- **Design tradeoffs**: Encoder-only vs decoder-only (encoder benefits more from explicit features but requires separate parsing infrastructure; decoder is easier to deploy but shows smaller gains); Rule-based vs predicted features (current work assumes gold features; production would need prediction module); Full fine-tuning vs LoRA (single A6000 suggests LoRA may be necessary)

- **Failure signatures**: LAS does not improve with features → check FEATS column population; Decoder generates malformed CoNLL-U → inspect instruction formatting; POS corrections degrade performance → verify corrections align with UD v2 guidelines

- **First 3 experiments**: 1) Reproduce UDPipe baseline (no MS features) on Korean GSD; confirm LAS ≈50, then add MS features; target LAS ≈64; 2) Ablate feature groups: train with only CASE, only MOOD, etc.; 3) Convert small subset to IT format and fine-tune Bllossom-3B for 1–2 epochs; verify output format validity

## Open Questions the Paper Calls Out

- Can a sentence-level feature prediction pipeline that refines word-level UniMorph predictions using syntactic dependency context achieve parsing accuracy comparable to gold-standard morphosyntactic features?

- Would fine-grained semantic classification of Korean conjunctive verbal endings (causality, contrast, sequence, condition) provide additional improvements to dependency parsing accuracy beyond current morphosyntactic features?

- How can the UniDive framework be extended to handle non-canonical Korean constructions such as elliptical structures, periphrastic constructions, and multi-function morphemes more effectively?

- To what extent does the 14.09 LAS improvement observed with encoder-only models versus the 2.6 LAS improvement with decoder-only models reflect a fundamental architectural difference in how morphosyntactic features should be integrated?

## Limitations

- Dataset quality concerns due to reliance on manual and rule-based corrections to existing UD resources without external validation

- Incomplete feature extraction specification - Table 1 provides examples but lacks complete deterministic rules for all Korean morphological patterns

- Hyperparameter inconsistencies between text (10 epochs, 4 gradient accumulation) and Table 4 (1 epoch, 16 gradient accumulation)

- Limited evaluation scope using only the Korean GSD treebank for both training and testing

## Confidence

- **High confidence**: Explicit morphosyntactic features improve encoder-only dependency parsing accuracy (14.09 LAS improvement well-supported)
- **Medium confidence**: Decoder-only models benefit from morphosyntactic features (2.6 LAS improvement supported but modest)
- **Low confidence**: Claims about relative importance of different feature types cannot be independently verified due to lack of ablation studies

## Next Checks

1. **Ablation study of feature types** - Train UDPipe with only CASE features, only MOOD features, only TENSE features, and combinations thereof to quantify the contribution of each morphosyntactic category to parsing performance.

2. **Cross-treebank generalization** - Evaluate the best-performing model on an independent Korean dependency treebank (e.g., UD-KSL or second language Korean treebanks) to assess whether improvements generalize beyond the training domain.

3. **Alternative decoder architectures** - Implement a transformer-based sequence-to-sequence parser (e.g., BART or T5) and compare its performance with both the encoder-only and decoder-only baselines when trained with identical morphosyntactic feature enhancements.