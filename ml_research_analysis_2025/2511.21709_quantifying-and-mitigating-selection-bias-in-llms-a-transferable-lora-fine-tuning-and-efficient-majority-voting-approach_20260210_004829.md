---
ver: rpa2
title: 'Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning
  and Efficient Majority Voting Approach'
arxiv_id: '2511.21709'
source_url: https://arxiv.org/abs/2511.21709
tags:
- bias
- lora
- dataset
- across
- permutations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses selection bias in LLMs during multiple-choice
  question answering, where models favor options based on position or symbols rather
  than content. The authors introduce a novel label-free Permutation Bias Metric (PBM)
  that quantifies prediction inconsistencies across all answer permutations, along
  with Batch Question-Context KV caching (BaQCKV) for efficient majority voting and
  LoRA-1 fine-tuning that uses PBM as a differentiable loss.
---

# Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach

## Quick Facts
- **arXiv ID:** 2511.21709
- **Source URL:** https://arxiv.org/abs/2511.21709
- **Reference count:** 9
- **Primary result:** 90% token reduction with BaQCKV while maintaining zero bias

## Executive Summary
This work addresses selection bias in LLMs during multiple-choice question answering, where models favor options based on position or symbols rather than content. The authors introduce a novel label-free Permutation Bias Metric (PBM) that quantifies prediction inconsistencies across all answer permutations, along with Batch Question-Context KV caching (BaQCKV) for efficient majority voting and LoRA-1 fine-tuning that uses PBM as a differentiable loss. Experiments across four MCQ datasets show that BaQCKV reduces token usage by up to 90% while maintaining zero bias, and LoRA-1 decreases PBM bias by 58% and improves accuracy consistency by 27% on average, outperforming baselines like PriDe, BNP, and Gray. The methods are efficient, scalable, and transferable across domains.

## Method Summary
The authors propose three complementary approaches to mitigate selection bias in LLM multiple-choice question answering. First, they introduce the Permutation Bias Metric (PBM), a label-free method that measures prediction inconsistencies by generating all permutations of answer options and calculating the standard deviation of predicted probabilities. Second, they develop Batch Question-Context KV caching (BaQCKV), which caches key-value pairs for context and question tokens across all permutations to reduce redundant computation during majority voting. Third, they introduce LoRA-1 fine-tuning, which incorporates PBM as a differentiable loss function to train the model to minimize selection bias. The combined approach addresses both computational efficiency and bias mitigation while maintaining transferability across different domains.

## Key Results
- BaQCKV reduces token usage by up to 90% while maintaining zero bias
- LoRA-1 fine-tuning decreases PBM bias by 58% and improves accuracy consistency by 27% on average
- Outperforms baseline methods (PriDe, BNP, Gray) across four MCQ datasets
- Methods demonstrate efficiency, scalability, and transferability across domains

## Why This Works (Mechanism)
The approach works by recognizing that selection bias manifests as inconsistent predictions across different permutations of answer options. By measuring this inconsistency through PBM, the model can be trained to produce stable predictions regardless of answer ordering. BaQCKV enables efficient computation of majority voting across permutations by reusing cached context representations, while LoRA-1 fine-tuning directly optimizes the model to minimize the PBM metric. The combination addresses both the computational bottleneck of permutation-based methods and the fundamental bias in the model's decision-making process.

## Foundational Learning
- **Permutation Bias Metric (PBM):** Measures prediction inconsistency across all answer permutations using standard deviation of probabilities - needed to quantify selection bias without labels, quick check: verify PBM correlates with actual bias across diverse datasets
- **Majority Voting:** Aggregates predictions across permutations to identify stable answers - needed to neutralize position-based bias, quick check: test voting accuracy vs individual predictions
- **LoRA Fine-Tuning:** Parameter-efficient adaptation using low-rank matrix decomposition - needed for efficient bias mitigation without full fine-tuning, quick check: measure parameter count vs performance tradeoff
- **KV Caching:** Stores intermediate computation results to avoid redundancy - needed for efficient permutation processing, quick check: measure token reduction vs computation overhead
- **Permutation Generation:** Creates all possible orderings of answer options - needed to expose bias through different presentations, quick check: verify factorial scaling matches theoretical predictions
- **Differentiable Loss Functions:** Enables gradient-based optimization of bias metrics - needed to train models using PBM as objective, quick check: verify gradient flow during LoRA training

## Architecture Onboarding

### Component Map
PBM Calculation -> BaQCKV Caching -> LoRA-1 Fine-tuning

### Critical Path
Input MCQ -> Generate Permutations -> Compute PBM Scores -> Majority Vote (via BaQCKV) -> LoRA-1 Optimization

### Design Tradeoffs
- PBM requires factorial computation but provides label-free bias measurement
- BaQCKV trades memory for computation efficiency
- LoRA-1 balances bias reduction with parameter efficiency
- Permutation generation enables bias detection but scales poorly with option count

### Failure Signatures
- High PBM scores indicate persistent selection bias
- BaQCKV cache misses suggest insufficient context reuse
- LoRA-1 training instability indicates problematic loss landscape
- Majority voting inconsistencies reveal insufficient permutation coverage

### First Experiments
1. Measure PBM scores on baseline model across all four datasets
2. Benchmark BaQCKV token reduction vs vanilla majority voting
3. Compare LoRA-1 performance against full fine-tuning baseline

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- PBM metric requires generating all permutations, scaling factorially with option count
- Evaluation limited to four MCQ datasets, may not capture full bias diversity
- Transfer learning claims need validation across domains with different bias characteristics
- LoRA-1 approach requires access to permutation data during fine-tuning

## Confidence
- **High:** BaQCKV efficiency results (90% token reduction is directly measurable)
- **Medium:** PBM metric effectiveness in quantifying selection bias
- **Medium:** LoRA-1 fine-tuning results (58% PBM reduction, 27% accuracy improvement)

## Next Checks
1. Test PBM and LoRA-1 on a dataset with 5+ answer options to verify factorial scaling claims and measure computational overhead
2. Evaluate the approach on non-MCQ question formats to assess domain transferability beyond multiple-choice settings
3. Conduct ablation studies isolating the individual contributions of PBM-based fine-tuning versus majority voting to quantify their relative effectiveness