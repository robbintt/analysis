---
ver: rpa2
title: 'RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex
  Problems'
arxiv_id: '2510.09227'
source_url: https://arxiv.org/abs/2510.09227
tags:
- regex
- length
- regexes
- llms
- equivalent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces RegexPSPACE, the first benchmark to evaluate
  large language models (LLMs) and large reasoning models (LRMs) on PSPACE-complete
  regex problems. It targets two core tasks: regex equivalence decision (RegexEQ)
  and regex minimization (RegexMin).'
---

# RegexPSPACE: A Benchmark for Evaluating LLM Reasoning on PSPACE-complete Regex Problems

## Quick Facts
- **arXiv ID**: 2510.09227
- **Source URL**: https://arxiv.org/abs/2510.09227
- **Reference count**: 40
- **Key outcome**: First benchmark to evaluate LLMs and LRMs on PSPACE-complete regex problems, revealing significant struggles with minimization tasks due to spatial computational limits

## Executive Summary
This paper introduces RegexPSPACE, the first benchmark to evaluate large language models (LLMs) and large reasoning models (LRMs) on PSPACE-complete regex problems. It targets two core tasks: regex equivalence decision (RegexEQ) and regex minimization (RegexMin). The benchmark is built using a double-exponential space exploration to construct over a million regex instances, labeled with rigorous filtering. Extensive experiments on 6 LLMs and 5 LRMs reveal that models struggle significantly with minimization, often failing to produce equivalent or shorter regexes, while performing better on equivalence tasks. Common failure patterns include verbosity, repetition, and premature stopping. These findings highlight the intrinsic difficulty of PSPACE-complete reasoning and the current limits of LLMs under spatial constraints, offering a new framework for evaluating advanced reasoning capabilities beyond NP-level challenges.

## Method Summary
The benchmark is constructed through bottom-up enumeration of regular expressions up to depth 3, followed by equivalence class partitioning using DFA comparison to group regexes with identical language acceptance. Minimal regexes are identified within each class, and rigorous filtering selects non-minimal examples with class sizes ≥10 and sufficient positive/negative pairs. The resulting dataset contains 1.27M regexes (LRD) with train/validation/test splits, plus 50k unlabeled regexes (URMT) for out-of-distribution testing. Six LLMs and five LRMs are evaluated using zero-shot and five-shot prompting with greedy decoding, with reasoning models receiving extended thinking tokens. Performance is measured using minimality, equivalence, length ratio (RegexMin) and accuracy, F1-score (RegexEQ).

## Key Results
- Models struggle significantly with RegexMin, achieving low minimality rates (often <20%) while performing better on RegexEQ equivalence tasks
- Common failure patterns include verbosity, repetition, and premature stopping, particularly in Llama, DeepSeek, and high-reasoning-mode models
- Increasing token budgets from 1,024 to 4,096 improves minimality for some models (Qwen3-A3B from 3.44% to 17.63%) but does not eliminate fundamental reasoning limitations
- Fine-tuned models show limited generalization to longer regexes, suggesting performance is memorization-limited rather than reasoning-based

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PSPACE-complete problems expose spatial computational limits of LLMs that NP-level benchmarks cannot reveal.
- Mechanism: PSPACE-complete problems require exploring exponentially larger search spaces than NP problems. LLMs have finite context windows that function as limited memory (akin to Non-erasing Turing Machines). When forced to traverse massive state spaces under this constraint, models cannot maintain sufficient exploration depth, leading to failure patterns that would not appear on tractable NP tasks.
- Core assumption: Context window length is the primary bottleneck for PSPACE-complete reasoning, not just parameter count or training data.
- Evidence anchors:
  - [abstract] "Yet their computational limits, particularly spatial complexity constrained by finite context windows, remain poorly understood. While recent works often focus on problems within the NP complexity class, we push the boundary..."
  - [section A.4] "Under this abstraction, LLMs can be viewed as Non-erasing Turing Machines (NETMs)... simulating PSPACE-complete problems sequentially with LLMs would require EXPSPACE"
  - [corpus] No direct corpus validation; related work (THINK-Bench) addresses reasoning efficiency but not PSPACE complexity specifically
- Break condition: If models with larger context windows (e.g., 1M+ tokens) show dramatically improved PSPACE performance, the mechanism may be partially correct but context-length dependent rather than fundamental.

### Mechanism 2
- Claim: RegexMin is harder than RegexEQ for current models because minimization requires generative search across equivalent expressions, not just binary decision.
- Mechanism: Equivalence decision maps to a binary classification output (True/False). Minimization requires: (1) generating a valid regex, (2) ensuring equivalence, (3) achieving minimal length—a generative task requiring systematic exploration of shorter candidates. Models fail because they lack explicit search mechanisms and instead rely on pattern-matching heuristics that do not generalize to the combinatorial space of equivalent expressions.
- Core assumption: The difficulty gap stems from task structure (generative vs. discriminative), not merely from dataset bias or prompt design.
- Evidence anchors:
  - [abstract] "models struggle significantly with minimization, often failing to produce equivalent or shorter regexes, while performing better on equivalence tasks"
  - [section 5.2] "equivalence requires only a binary decision of True or False, whereas minimization requires producing a valid regex that is not only equivalent to the query but also shorter in length"
  - [section 5.2] "few-shot prompting primarily reinforces surface-level regularities rather than enabling deeper optimization"
  - [corpus] LR²Bench notes similar reflective reasoning challenges in constraint satisfaction, suggesting generative search is a broader limitation
- Break condition: If models trained specifically on equivalence-preserving transformations (rather than end-to-end minimization) achieve high minimality scores, the bottleneck may be training methodology rather than architectural limits.

### Mechanism 3
- Claim: Repetition and verbosity failure modes arise from probability distribution collapse during extended token generation for complex reasoning.
- Mechanism: When models attempt to "reason through" PSPACE problems, they generate long token sequences. Without external memory or backtracking mechanisms, the model's attention over its own output creates feedback loops where high-probability tokens reinforce themselves, leading to repetitive patterns or excessive wordiness that exhausts token budgets without converging to solutions.
- Core assumption: Repetition is not merely a decoding artifact but reflects the model's inability to maintain coherent exploration paths over long generations.
- Evidence anchors:
  - [abstract] "Common failure patterns include verbosity, repetition, and premature stopping"
  - [section 5.2] "reasoning models often separate the thinking process into dedicated thinking tokens... models can become trapped in a repetitive probability distribution"
  - [section 5.3] "repetition is particularly pronounced in Llama, DeepSeek, and the high reasoning mode of gpt-oss"
  - [corpus] THINK-Bench explicitly addresses "overthinking" and redundant token generation in LRMs, corroborating this as a general phenomenon
- Break condition: If repetition rates remain constant even when token budgets are drastically increased (as tested with Qwen3-A3B), the mechanism correctly identifies distribution collapse rather than mere token exhaustion.

## Foundational Learning

- Concept: **PSPACE-completeness and polynomial space complexity**
  - Why needed here: The paper's central claim is that PSPACE-complete problems serve as a more rigorous test of computational capacity than NP problems. Understanding why PSPACE ⊇ NP and what "polynomial space" means is essential to interpret the benchmark's significance.
  - Quick check question: Can you explain why determining a winning strategy in generalized chess is PSPACE-complete rather than NP-complete?

- Concept: **Regular languages and finite automata equivalence**
  - Why needed here: The benchmark tasks (equivalence and minimization) are grounded in formal language theory. The dataset construction relies on converting regexes to DFAs and comparing minimized forms—understanding this pipeline is necessary to evaluate dataset validity.
  - Quick check question: Given two regexes, what is the standard algorithmic approach to determine equivalence, and why is it computationally expensive?

- Concept: **Tree-length minimality for regex expressions**
  - Why needed here: The paper defines regex length as the number of nodes in the expression tree (counting operators and concatenations). This is the ground truth for evaluating minimization success, distinct from character-count metrics.
  - Quick check question: For the regex `a*b* + a?`, what is its tree length according to the paper's definition?

## Architecture Onboarding

- Component map:
  - Dataset Construction Pipeline: Bottom-up regex enumeration (Algorithms 1, 4) → Equivalence class partitioning via DFA comparison (Algorithm 2) → Minimal regex labeling (Algorithm 5) → Filtering for benchmark (4 criteria: non-minimal, class size ≥10, positive/negative examples ≥20, isomorphism deduplication)
  - LRD (Labeled Regex Dataset): 1.27M regexes, depth ≤3, length ≤15, with train/validation/test splits
  - URMT (Unlabeled Regex Minimization Test): 50k regexes, depth 4-6, length up to 127, for OOD generalization testing
  - RegexPSPACE Benchmark: 1,685 filtered test instances with paired equivalent/non-equivalent regexes
  - Evaluation Metrics: Minimality, Equivalence, Length Ratio (for RegexMin); Accuracy, F1-score (for RegexEQ)
  - Model Evaluation Harness: Greedy decoding, 1024 answer tokens for non-reasoning models, 4096 thinking + 1024 answer tokens for reasoning models, answer extraction from `\boxed{}` tags

- Critical path:
  1. Dataset construction is the bottleneck—enumerating regexes up to depth 4 requires 1.6 trillion candidates (Table 5), making ground-truth labeling intractable beyond depth 3.
  2. Equivalence class partitioning (Algorithm 2) uses string-acceptance heuristics to reduce O(n²) comparisons, but this is still exponential-time.
  3. Benchmark filtering reduces 50k test regexes to 1,685 instances, ensuring non-trivial problems while maintaining tractable evaluation.

- Design tradeoffs:
  - **Alphabet size (|Σ|=4) vs. problem complexity**: Smaller alphabet limits regex expressiveness but keeps enumeration tractable. Larger alphabets cause double-exponential growth (Table 5).
  - **Depth restriction (≤3) vs. evaluation validity**: Shallow regexes may not fully test reasoning capabilities, but deeper regexes cannot be ground-truth labeled.
  - **Formal regex syntax vs. practical regex**: Paper enforces formal syntax (no `|` for union, no character classes) to ensure theoretical validity, but this diverges from real-world regex usage that LLMs were trained on.
  - **Greedy decoding vs. sampling**: Paper uses greedy for reproducibility, but notes sampling might help escape repetition loops—tradeoff between determinism and performance.

- Failure signatures:
  - **Repetition failure**: Model generates identical or near-identical token sequences until hitting token limit. Prevalent in DeepSeek, Llama, gpt-oss-high. Detected by checking for repeated n-grams in output.
  - **Token exhaustion**: Model generates coherent reasoning but terminates at token limit without answer. Common in phi-4-reasoning, gpt-oss-low. Indicates model requires more exploration than budget allows.
  - **Invalid syntax**: Model outputs practical regex notation (`|`, `[a-z]`, `+` for repetition) that cannot be parsed. Common in non-reasoning models unfamiliar with formal syntax requirements.
  - **False negatives in RegexEQ**: Models systematically biased toward outputting "False" (non-equivalent), likely because verifying equivalence requires more computation than rejecting it.

- First 3 experiments:
  1. **Baseline evaluation on RegexPSPACE** using the provided evaluation harness (Section G.3): Run your target model on both RegexMin and RegexEQ with zero-shot and five-shot prompts (Figures 6-9). Report all five metrics. Compare against Table 1 baselines to establish where your model sits relative to Qwen-14B and gpt-oss-low.
  2. **Ablation on token budget**: Following the analysis in Appendix H.2 (Table 12), evaluate whether your model's failures are due to token exhaustion or fundamental reasoning limits. Increase answer token limit from 1024 to 4096 and measure: (a) change in success rate, (b) average tokens generated for successful vs. failed outputs, (c) repetition rate change.
  3. **OOD generalization test on URMT**: Evaluate on the 50k unlabeled depth 4-6 regexes (Table 4 examples). Since ground truth is unavailable, report equivalence rate (using DFA comparison via FAdo library) and length ratio. This tests whether performance on RegexPSPACE generalizes to longer, more complex inputs—or if performance is memorization-limited as seen in fine-tuned models (Table 7-8).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the reasoning capabilities demonstrated on regex-based PSPACE problems generalize to other distinct PSPACE-complete domains, such as Quantified Boolean Formulas (QBF) or game strategy?
- **Basis in paper:** [explicit] The Limitations section states, "Although there are many PSPACE-complete problems, performance on regex tasks may not directly translate to other domains," and Future Works calls for broader exploration.
- **Why unresolved:** The study focuses exclusively on regex tasks; it is unknown if the "massive exploration" required for regex minimization maps effectively to the logic of other PSPACE problems.
- **What evidence would resolve it:** Evaluation of the same set of models on a newly constructed benchmark of non-regex PSPACE-complete problems showing correlated performance metrics.

### Open Question 2
- **Question:** How can the benchmark be extended to handle regular expressions with larger alphabet sizes and greater expression depths despite the computational tractability barriers to generating ground truth labels?
- **Basis in paper:** [explicit] The Limitations section notes that "Extending to longer regex remains nearly intractable, yet it also represents a meaningful direction for future research" due to double-exponential search space growth.
- **Why unresolved:** Current dataset construction relies on exhaustive enumeration and partitioning of equivalence classes, which becomes computationally impossible as depth or alphabet size increases.
- **What evidence would resolve it:** The development of heuristic verification methods or approximate minimization algorithms that allow for the labeling of deeper, more complex regexes.

### Open Question 3
- **Question:** What architectural or decoding strategies are required to prevent Large Reasoning Models (LRMs) from falling into repetitive probability distributions or premature stopping during the massive exploration required by PSPACE tasks?
- **Basis in paper:** [inferred] The authors observe that "sampling-based decoding... does not constitute a fundamental solution" to the common failure patterns of "verbosity, repetition, and premature stopping."
- **Why unresolved:** The paper identifies the failure mode (repetition loops) but does not propose a solution to the fundamental tendency of models to get stuck during the extensive generation needed for these tasks.
- **What evidence would resolve it:** A proposed intervention (e.g., specialized attention mechanisms or decoding constraints) that significantly reduces the repetition failure rate while maintaining or improving minimization success.

## Limitations
- **Ground-truth labeling scalability**: The double-exponential growth of regex space (1.6 trillion candidates for depth 4) makes comprehensive ground-truth construction infeasible beyond depth 3, limiting evaluation to shallow regexes (≤15 chars).
- **Formal syntax vs. training distribution mismatch**: The benchmark enforces strict formal regex syntax incompatible with practical regex notation that LLMs were trained on, creating an artificial evaluation gap.
- **Context window as primary bottleneck**: While the paper argues PSPACE-completeness exposes spatial limits, it's unclear whether token exhaustion is the fundamental constraint or merely a symptom of deeper architectural limitations in search and planning.

## Confidence
- **High**: The benchmark construction methodology (DFA-based equivalence partitioning, rigorous filtering criteria) is sound and reproducible. The observed failure patterns (repetition, verbosity, token exhaustion) are consistently reported across multiple models.
- **Medium**: The claim that PSPACE-complete problems better reveal spatial computational limits than NP problems is supported by theoretical arguments but requires empirical validation across diverse problem domains beyond regex.
- **Low**: The assertion that RegexMin is inherently harder than RegexEQ due to task structure (generative vs discriminative) lacks systematic ablation studies isolating task complexity from model architecture and training effects.

## Next Checks
1. **Context window ablation study**: Systematically evaluate model performance across increasing token budgets (1K, 2K, 4K, 8K) to distinguish between token exhaustion and fundamental reasoning limits. Track: (a) success rate improvement curve, (b) repetition rate reduction, (c) average generation length before termination.

2. **Cross-domain PSPACE benchmarking**: Adapt the benchmark framework to other PSPACE-complete domains (e.g., quantified boolean formulas, generalized geography games) to test whether observed failure patterns are regex-specific or general spatial reasoning limitations.

3. **Architecture scaling analysis**: Evaluate whether performance gaps between models (e.g., Qwen2.5-14B vs gpt-oss-low) correlate with architectural differences (attention mechanisms, memory layers) rather than parameter count alone, using models with matched parameters but different designs.