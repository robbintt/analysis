---
ver: rpa2
title: "Statistical Inference for Linear Functionals of Online Least-squares SGD when\
  \ $t \\gtrsim d^{1+\u03B4}$"
arxiv_id: '2510.19734'
source_url: https://arxiv.org/abs/2510.19734
tags:
- lemma
- proof
- logt
- logd
- assumption
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes non-asymptotic Berry-Esseen bounds for\
  \ linear functionals of online least-squares stochastic gradient descent (SGD) in\
  \ the growing-dimensional regime. The key result shows that a central limit theorem\
  \ holds for SGD iterates when the number of iterations t grows as t \u2273 d^{1+\u03B4\
  } for any \u03B4 0, significantly extending prior works that required t \u2273 d^{3/2}."
---

# Statistical Inference for Linear Functionals of Online Least-squares SGD when $t \gtrsim d^{1+δ}$

## Quick Facts
- **arXiv ID:** 2510.19734
- **Source URL:** https://arxiv.org/abs/2510.19734
- **Reference count:** 40
- **Primary result:** Establishes non-asymptotic Berry-Esseen bounds for linear functionals of online least-squares SGD with improved dimensional scaling $t \gtrsim d^{1+\delta}$ over prior $t \gtrsim d^{3/2}$ methods.

## Executive Summary
This paper develops a fully online framework for statistical inference on linear functionals of SGD iterates in the growing-dimensional regime. The key innovation is achieving a central limit theorem for SGD iterates when the number of samples $t$ scales as $t \gtrsim d^{1+\delta}$ for any $\delta > 0$, significantly improving over prior work requiring $t \gtrsim d^{3/2}$. The method operates in $O(td)$ time and $O(d)$ memory, compared to $O(td^2 + d^3)$ for covariance-inversion methods, while maintaining assumption-lean conditions on the data generating process.

## Method Summary
The approach centers on expressing the linear functional $\langle a, \theta_t \rangle$ as a sum of a martingale difference sequence, enabling the application of martingale Berry-Esseen bounds that avoid the quadratic error terms inherent to OLS methods. The online variance estimator $\hat{V}_t$ is computed using a block-wise backward update scheme that partitions the data stream and computes a backward-in-time recursion for vectors $u_{i,t}$. This allows the calculation of the variance functional without storing the full history. The method uses a step-size schedule $\eta_i = \eta d^{-1/2} i^{-\alpha}$ with $\alpha \in (1/2, 1)$ and operates through an SGD update loop followed by a backward pass over the most recent block of data.

## Key Results
- Achieves CLT for SGD iterates with improved dimensional scaling $t \gtrsim d^{1+\delta}$ compared to prior $t \gtrsim d^{3/2}$ requirements
- Provides first fully online and data-driven framework for constructing confidence intervals in the near-optimal scaling regime
- Operates in $O(td)$ time and $O(d)$ memory, significantly more efficient than $O(td^2 + d^3)$ for covariance-inversion methods
- Develops online variance estimator with high-probability deviation bounds for the asymptotic variance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The proposed method achieves improved dimensional scaling ($t \gtrsim d^{1+\delta}$) over prior projection-parameter inference ($t \gtrsim d^{3/2}$) by avoiding explicit empirical covariance inversion.
- **Mechanism:** The authors express the linear functional $\langle a, \theta_t \rangle$ as a sum of a martingale difference sequence (Lemma A.2). This allows the application of martingale Berry-Esseen bounds (Theorem A.1) which do not incur the "quadratic" error terms $\sim \|\hat{A} - A\|^2$ inherent to Ordinary Least Squares (OLS) methods. In OLS, these error terms scale as $d^{3/2}/t$, forcing a stricter $t \gtrsim d^{3/2}$ requirement; the SGD martingale structure bypasses this bottleneck.
- **Core assumption:** The step-size $\eta_i$ decays as $i^{-\alpha}$ for $\alpha \in (1/2, 1)$, and the data satisfies specific moment bounds (Assumption 2.1).
- **Evidence anchors:**
  - [abstract] "significantly extending prior works that required $t \gtrsim d^{3/2}$"
  - [appendix H] "Why does the online SGD based method achieve significantly better dimension scaling?... SGD based estimator naturally has a sum of martingale difference sequence structure"
  - [corpus] Neighbor paper "Avoiding the Price of Adaptivity" discusses inflation costs in adaptive linear inference; this paper suggests a structural path to avoid such costs in specific regimes.

### Mechanism 2
- **Claim:** The online variance estimator $\hat{V}_t$ converges to the true asymptotic variance using a block-wise backward update scheme.
- **Mechanism:** The variance estimation relies on partitioning the data stream into blocks of size $t_0$. The estimator computes a backward-in-time recursion for vectors $u_{i,t}$ (Remark 4), allowing the calculation of the variance functional $\eta^2_i (Y_i - X_i^\top \beta^*)^2 (u_{i,t}^\top X_i)^2$ without storing the full history.
- **Core assumption:** The spectral norm of the covariance matrix $\eta \lambda_{\min}(A)$ is bounded below by a constant (Assumption 3.1), and the halfway iterate $\theta_{t/2}$ provides a sufficiently accurate proxy for $\beta^*$.
- **Evidence anchors:**
  - [section 3] "Modified construction when $t$ is not known in advance... use a dyadic batching strategy"
  - [remark 4] "Observe that the sequence of row vectors... satisfies a simple recursion"
  - [corpus] "Online Inference for Quantiles" (neighbor paper) also leverages specific Markov chain structures for inference, paralleling the time-series approach here.

### Mechanism 3
- **Claim:** The bias of the SGD iterate relative to the true parameter $\beta^*$ becomes negligible relative to the variance under the specified step-size regime.
- **Mechanism:** The bias $|E\langle a, \theta_t \rangle - \langle a, \beta^* \rangle|$ decays exponentially as $e^{-\eta \lambda_{\min}(A) d^{-1/2} t^{1-\alpha}}$, whereas the standard deviation scales as $\sim \sqrt{\text{Var}}$. By choosing $\alpha$ appropriately, the bias term vanishes faster than the Berry-Esseen error rate, allowing the centered CLT to hold.
- **Core assumption:** The initialization $\theta_0$ and true parameter $\beta^*$ satisfy a polynomial growth condition relative to noise and dimensions.
- **Evidence anchors:**
  - [theorem 2.2] Bounding the bias-correction term explicitly.
  - [section 2.2] "Assumption 2.1... conditions on $\lambda_{\min}(A)$... are required to control the non-asymptotic behavior"

## Foundational Learning

- **Concept: Martingale Central Limit Theorems (CLT)**
  - **Why needed here:** The paper’s core theoretical contribution is a non-asymptotic Berry-Esseen bound derived specifically for martingale difference sequences. Standard CLTs assume independence; martingale CLTs handle the specific dependence structure of SGD updates.
  - **Quick check question:** Can you explain why the sequence $M_t = \langle a, \theta_t \rangle - E[\langle a, \theta_t \rangle | \mathcal{F}_{t-1}]$ forms a martingale difference?

- **Concept: Berry-Esseen Bounds**
  - **Why needed here:** This is the metric used to quantify the convergence rate to Gaussianity. It provides the explicit non-asymptotic error term ($d_K$) that determines the required sample size scaling $t \gtrsim d^{1+\delta}$.
  - **Quick check question:** What does the Berry-Esseen bound measure that the standard CLT does not?

- **Concept: Stochastic Gradient Descent Dynamics**
  - **Why needed here:** Understanding the step-size $\eta_i = \eta d^{-1/2} i^{-\alpha}$ and its impact on the bias-variance tradeoff is critical for the practical implementation of the algorithm.
  - **Quick check question:** Why must the step-size parameter $\alpha$ be chosen in the interval $(1/2, 1)$ for the variance to scale correctly?

## Architecture Onboarding

- **Component map:** SGD Core -> Variance Estimator (Backward Pass) -> Batching Logic -> Confidence Interval Construction

- **Critical path:**
  1. Initialize $\theta_0$.
  2. Run SGD forward for $t$ steps to obtain $\theta_t$ (point estimate) and $\theta_{t/2}$ (pilot estimate for variance).
  3. Execute the backward update loop for the last $t_0$ steps to compute $\hat{V}_t$.
  4. Construct Confidence Interval: $\langle a, \theta_t \rangle \pm z_{\alpha/2} \sqrt{\hat{V}_t}$.

- **Design tradeoffs:**
  - **$\alpha$ Selection:** Lower $\alpha$ (closer to $1/2$) allows for faster statistical efficiency (lower variance) but requires stricter dimension scaling ($t \gtrsim d^2$ roughly). Higher $\alpha$ allows larger dimensions relative to samples but increases variance.
  - **Memory vs. Complexity:** The variance estimator is $O(td)$ time and $O(d)$ memory, but requires a "backward" pass over the most recent block, which implies retaining that block in memory or reading it from disk.

- **Failure signatures:**
  - **Ill-conditioned Data:** If $\lambda_{\min}(A)$ is very small, the bias decay slows down, potentially breaking the assumption that bias is negligible (Theorem 2.2).
  - **Dimension Scaling Violation:** If $t$ is too small relative to $d$ (specifically $t \lesssim d^{1+\delta}$), the high-order error terms in the Berry-Esseen bound (Theorem 2.3) dominate, and the confidence intervals will undercover.

- **First 3 experiments:**
  1. **Scaling Validation:** Generate synthetic data with $t = C \cdot d^{1.1}$ (where $\delta = 0.1$). Vary $C$ and plot the coverage probability of the constructed CIs against the theoretical bound.
  2. **Ablation on $\alpha$:** Fix $t, d$ and vary $\alpha \in (0.5, 1.0)$. Measure the width of the CI and the bias magnitude $|\langle a, \theta_t \rangle - \langle a, \beta^* \rangle|$ to verify the tradeoff described in Remark 1.
  3. **Comparison vs. OLS:** Compare the coverage and width of the proposed online SGD-CI against a standard batch OLS CI (requiring matrix inversion) as $d$ approaches $t^{0.9}$. Verify if OLS fails/degrades while SGD remains valid.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the growing-dimensional central limit theorem and inference framework be extended to relatively tame non-convex problems, specifically phase retrieval and matrix sensing?
- Basis in paper: [explicit] The conclusion states, "It is also of great interest to extend the analysis to... relatively tamer non-convex problems like phase retrieval and matrix sensing."
- Why unresolved: The current analysis relies on the specific structure of least-squares SGD (linear stochastic approximation), which does not directly translate to the non-convex landscapes of phase retrieval or matrix sensing.
- What evidence would resolve it: A non-asymptotic Berry-Esseen bound for online SGD in these non-convex settings that maintains the improved $t \gtrsim d^{1+\delta}$ scaling.

### Open Question 2
- Question: Can this online inference methodology be adapted for growing-dimensional robust regression problems despite the challenges of non-smoothness?
- Basis in paper: [explicit] The conclusion lists "growing-dimensional robust regression problems" as an area of interest, noting the "main complication being handling the subtleties arising due to non-smoothness."
- Why unresolved: The proof techniques in the current paper rely on the differentiability and linear structure of the least-squares loss, which are absent in robust regression settings (e.g., absolute loss).
- What evidence would resolve it: An online variance estimator and CLT valid for robust regression losses that handle non-smoothness without compromising the computational efficiency or dimensional scaling.

### Open Question 3
- Question: How can one optimally choose the moment parameter $p$ for finite sample sizes $t$ and $d$ to minimize the Berry-Esseen bound constants?
- Basis in paper: [explicit] Remark 2 notes that while higher moments $p$ improve asymptotic rates, the constant $C_2$ grows as $e^{pK}$; the authors state, "Choosing the value of $p$ optimally for a given finite $t, d$ is left as interesting future work."
- Why unresolved: The paper treats $p_{max}$ as a fixed absolute constant to suppress dependence, leaving the trade-off between the asymptotic rate improvement and the explosion of constants for finite samples unexplored.
- What evidence would resolve it: A theoretical derivation or heuristic algorithm that selects $p$ based on the specific values of $t$ and $d$ to optimize the finite-sample approximation error.

## Limitations
- The requirement $t \gtrsim d^{1+\delta}$ still represents a growing-dimensional regime, though significantly improved over the $d^{3/2}$ barrier. The bound may not hold for smaller $t/d$ ratios.
- The variance estimator requires a backward pass over the most recent $t_0$ samples, which may not be fully online in streaming settings where data cannot be revisited.
- The method assumes bounded spectral norm of the covariance matrix and requires careful tuning of the step-size parameter $\alpha$ and scalar $\eta$, which may be sensitive to problem specifics.

## Confidence
- **High confidence** in the theoretical framework: The martingale structure, Berry-Esseen bounds, and the variance estimator construction are rigorously derived with explicit non-asymptotic guarantees.
- **Medium confidence** in practical applicability: While the asymptotic theory is sound, the constants involved in the bound scaling are not specified, and the backward pass requirement may limit streaming applicability.
- **Medium confidence** in the numerical validation: The paper provides limited empirical evidence. The neighbor papers suggest similar methods have been validated, but direct empirical verification of this specific contribution is sparse.

## Next Checks
1. **Empirical Coverage Validation:** Generate synthetic data with varying $d$ and $t$ ratios to verify the claimed $t \gtrsim d^{1+\delta}$ scaling by plotting empirical coverage probabilities of the constructed CIs.
2. **Computational Efficiency Benchmark:** Implement and compare the runtime and memory usage of the proposed online SGD method against standard batch OLS covariance inversion methods for varying $d$ and $t$.
3. **Robustness to Hyperparameter Tuning:** Systematically vary the step-size parameter $\alpha$ and scalar $\eta$ to assess the sensitivity of the CI coverage and width to these choices.