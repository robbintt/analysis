---
ver: rpa2
title: 'Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded
  Devices: A Survey'
arxiv_id: '2510.00078'
source_url: https://arxiv.org/abs/2510.00078
tags:
- arxiv
- dynamic
- memory
- preprint
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides the first systematic characterization of adaptive
  and resource-efficient agentic AI systems for mobile and embedded devices. It addresses
  the fundamental tension between the growing complexity of foundation models and
  the limited resources of deployment environments by proposing a unified taxonomy
  spanning elastic inference, test-time adaptation, dynamic multimodal integration,
  and agentic AI applications.
---

# Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey

## Quick Facts
- **arXiv ID:** 2510.00078
- **Source URL:** https://arxiv.org/abs/2510.00078
- **Reference count:** 40
- **Primary result:** First systematic characterization of adaptive, resource-efficient agentic AI systems for mobile/embedded devices.

## Executive Summary
This survey addresses the fundamental tension between the growing complexity of foundation models and the limited resources of mobile and embedded deployment environments. It proposes a unified taxonomy spanning elastic inference, test-time adaptation, dynamic multimodal integration, and agentic AI applications. The work maps enabling techniques across a hardware-software spectrum, identifying key challenges in accuracy-latency-communication trade-offs and robustness under distribution shifts, while highlighting future opportunities in algorithm-system co-design and collaborative edge deployment.

## Method Summary
The survey systematically characterizes adaptive and resource-efficient agentic AI systems through a comprehensive framework that balances user goals with dynamic device constraints. It aggregates techniques into elastic FM inference (dynamic prompts, scalable depth/width, KV cache management) and test-time adaptation (PEFT, memory augmentation, interactive learning). The methodology involves analyzing existing literature to construct a unified taxonomy, mapping techniques onto a hardware-software spectrum, and identifying key challenges and future opportunities without proposing novel algorithms.

## Key Results
- Proposes a unified taxonomy spanning elastic inference, test-time adaptation, dynamic multimodal integration, and agentic AI applications
- Maps enabling techniques into a hardware-software spectrum addressing accuracy-latency-communication trade-offs
- Identifies key challenges in robustness under distribution shifts and highlights future opportunities in algorithm-system co-design
- Offers a comprehensive framework for scalable, adaptive, and resource-efficient agentic AI balancing user goals with dynamic device constraints

## Why This Works (Mechanism)

### Mechanism 1: Elastic Inference via Dynamic Routing
Dynamically skipping layers or exiting early based on input complexity can reduce computational cost while preserving accuracy for simpler inputs. The system employs confidence-based estimators or gating networks to evaluate intermediate representations. If confidence exceeds a threshold, the inference loop terminates, bypassing subsequent transformer layers. This works because not all input tokens require the full depth of the model to generate a correct output.

### Mechanism 2: Parameter-Efficient Test-Time Adaptation (PEFT)
Adapting a Foundation Model to non-stationary environments is feasible on-device by updating only a small fraction of weights rather than full parameters. The FM backbone is frozen while lightweight trainable modules (e.g., low-rank matrices) are injected. During deployment, only these modules are updated via backpropagation or gradient-free methods using streaming data. This works because the pre-trained backbone contains sufficient generalizable knowledge while only task-specific nuances require adjustment.

### Mechanism 3: Heterogeneous Memory and Compute Offloading
Agentic AI can sustain responsiveness under memory pressure by dynamically managing KV caches and offloading computation across heterogeneous processors. Instead of keeping all Key-Value states in high-bandwidth GPU memory, the system evicts low-importance tokens to CPU RAM or compresses them. Compute is split between power-hungry GPUs and efficient NPUs based on operator intensity. This works because memory bandwidth and capacity are stricter bottlenecks than raw compute throughput for generative inference on edge devices.

## Foundational Learning

- **Concept: Foundation Models (FMs) vs. Agents**
  - **Why needed here:** The paper distinguishes between the static "cognitive core" (the FM) and the dynamic "sensing-action loop" of the agent. Understanding this separation is necessary to grasp why "elastic inference" is needed versus "adaptation."
  - **Quick check question:** Can you explain why a static FM fails to meet the requirements of an "Agentic AI" system in a dynamic environment?

- **Concept: The Memory Wall & KV Cache**
  - **Why needed here:** Section III-E and III-F focus heavily on KV cache management. A learner must understand that in autoregressive transformers, memory grows with sequence length, which is the primary bottleneck on mobile devices.
  - **Quick check question:** How does the memory requirement of a Transformer scale during the decoding phase of a long conversation, and why does this necessitate "swapping" or "compression"?

- **Concept: PEFT (Parameter-Efficient Fine-Tuning) Types**
  - **Why needed here:** Section IV classifies PEFT into Additive (Adapters), Selective (masking weights), and Re-parameterized (LoRA). Distinguishing these is critical for choosing the right adaptation strategy.
  - **Quick check question:** Why might "Re-parameterized" PEFT (like LoRA) be preferred over "Additive" PEFT for a latency-sensitive mobile deployment?

## Architecture Onboarding

- **Component map:** Hardware (SoC/Memory) -> System Scheduling (DAG Compiler/Runtime) -> Model (FM + External Memory) -> Agent (Sensing-Decision-Action) -> Network (Distributed coordination)
- **Critical path:** Input (Perception) -> Encoder (Tokenization) -> KV Cache Management (Bottleneck) -> Dynamic Routing (Elastic Inference) -> Output (Action)
- **Design tradeoffs:**
  - Accuracy vs. Latency: Using quantization or early exiting reduces latency but risks accuracy drops
  - Compute vs. Memory: Recomputing KV cache saves memory but increases CPU cycles
  - Adaptability vs. Stability: Online test-time adaptation handles distribution shifts but risks catastrophic forgetting
- **Failure signatures:**
  - OOM (Out of Memory) Spikes: Caused by memory fragmentation or uncontrolled KV cache growth during long-horizon tasks
  - Stale Cognition: Agent hallucinations due to "weight mismatches" when distribution shifts occur without test-time adaptation
  - Communication Deadlock: In distributed multi-agent systems, static model partitioning causes pipeline bubbles if intermediate features exceed bandwidth
- **First 3 experiments:**
  1. Implement a confidence threshold gate on a standard LLM and measure TTFT and accuracy degradation as the exit layer is moved earlier
  2. Profile memory consumption of a standard decoder versus a "KV Cache Offloading" implementation during long conversations
  3. Fine-tune a small adapter (LoRA) on a simulated stream of new data while keeping the backbone frozen and monitor for catastrophic forgetting

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can foundation model architectures achieve elastic inference specifically for real-time perception–action loops under strict hardware constraints?
- **Basis in paper:** Section VII.A identifies the "absence of elastic architectural adaptation" and the lack of joint resource–model co-optimization for perception, reasoning, and action submodules
- **Why unresolved:** Current FMs excel at symbol processing but lack the necessary real-time adaptability and independent scaling capabilities required for asynchronous, resource-constrained embedded deployments
- **What evidence would resolve it:** Development of meta-controllers that dynamically assemble task-specific FMs and modular operators supporting composable, elastic scaling across heterogeneous devices

### Open Question 2
- **Question:** How can autonomous agents perform responsive online adaptation of foundation models without access to labeled data or cloud connectivity?
- **Basis in paper:** Section VII.C highlights the challenge of sustaining performance "when full retraining is infeasible, data streams are unlabeled, and source data is inaccessible"
- **Why unresolved:** Existing methods struggle with error propagation and catastrophic forgetting under memory and compute limits, while current autoregressive architectures are brittle for long-horizon reasoning
- **What evidence would resolve it:** Algorithms that successfully integrate parameter-efficient fine-tuning (PEFT) with label-free streaming and device-aware scheduling to minimize forgetting and computational overhead

### Open Question 3
- **Question:** How can distributed multi-modal sensing resolve the inherent latency–accuracy–communication trade-off caused by asynchronous sensor streams?
- **Basis in paper:** Section VII.D notes that "waiting for slow modalities inflates latency, while discarding them reduces accuracy," creating a trade-off current synchronous models cannot resolve
- **Why unresolved:** Current synchronous, full-modality MLLMs fail to capture "modality affinity" and cannot efficiently handle the redundancy and asynchrony of real-world sensor data
- **What evidence would resolve it:** Generalizable models that evaluate modality affinity under bandwidth limits and non-blocking imputation techniques that maintain accuracy while minimizing communication costs

## Limitations
- The work is a survey aggregating existing techniques rather than proposing novel mechanisms, making effectiveness dependent on cited work validity
- Specific implementation details for "Dynamic FM Models" and "Dynamic Routing" policies are not provided, requiring consultation of original papers
- Exact hardware configurations and thermal constraints for trend analysis in Fig. 1 are not detailed enough for precise replication

## Confidence
- **High Confidence:** The characterization of fundamental tension between FM complexity and device constraints, and classification of enabling techniques into elastic inference and test-time adaptation categories
- **Medium Confidence:** The proposed unified taxonomy and specific mapping of techniques onto hardware-software spectrum, though practical impact may vary by use case
- **Low Confidence:** Precise quantitative impact of combining all proposed techniques on a single system, as the survey does not present integrated experimental results

## Next Checks
1. **Validate the Elastic Inference Mechanism:** Implement a confidence-based early exit policy on a quantized LLM (e.g., LLaMA-2-7B-4bit) on an embedded device. Measure and plot the trade-off between TTFT, throughput, and accuracy degradation as the exit layer is moved earlier.

2. **Stress-Test the KV Cache Management:** Profile memory consumption and latency of a standard decoder against an implementation with aggressive KV cache eviction or compression during a long conversation (e.g., 8192 tokens).

3. **Benchmark the PEFT Adaptation:** Fine-tune a LoRA adapter on a simulated stream of non-stationary data while keeping the backbone frozen. Monitor for catastrophic forgetting on the original task distribution and measure the memory and latency overhead of the adaptation process.