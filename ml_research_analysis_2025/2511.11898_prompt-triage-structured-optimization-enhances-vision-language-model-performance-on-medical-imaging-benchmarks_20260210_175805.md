---
ver: rpa2
title: 'Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance
  on Medical Imaging Benchmarks'
arxiv_id: '2511.11898'
source_url: https://arxiv.org/abs/2511.11898
tags:
- skin
- prompt
- medical
- optimization
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates automated prompt optimization for vision-language
  models on medical imaging tasks, addressing the gap between general model performance
  and specialized medical benchmarks. The authors implement prompting pipelines for
  five medical imaging tasks across radiology, gastroenterology, and dermatology,
  evaluating 10 open-source VLMs using four prompt optimization techniques.
---

# Prompt Triage: Structured Optimization Enhances Vision-Language Model Performance on Medical Imaging Benchmarks

## Quick Facts
- arXiv ID: 2511.11898
- Source URL: https://arxiv.org/abs/2511.11898
- Reference count: 17
- Primary result: Automated prompt optimization achieves 53% median relative improvement on medical imaging tasks

## Executive Summary
This study evaluates automated prompt optimization for vision-language models (VLMs) on medical imaging tasks, addressing the gap between general model performance and specialized medical benchmarks. The authors implement prompting pipelines for five medical imaging tasks across radiology, gastroenterology, and dermatology, evaluating 10 open-source VLMs using four prompt optimization techniques. Optimized pipelines achieved a median relative improvement of 53% over zero-shot prompting baselines, with the largest gains ranging from 300% to 3,400% on tasks where zero-shot performance was particularly low. SIMBA emerged as the most effective optimizer, delivering the highest relative improvements in 56% of model-task evaluations. These results demonstrate that inference-time prompt optimization can significantly improve medical VLM performance without model retraining or proprietary data access, enabling scalable, weight-agnostic performance improvements for clinical applications.

## Method Summary
The study evaluates 10 open-source VLMs across five medical imaging tasks using DSPy's declarative programming framework. Four optimization techniques (BootstrapFewShotRandomSearch, MIPROv2, SIMBA, GEPA) are applied to hand-crafted baseline prompts that include chain-of-thought reasoning instructions. Each optimizer trains on 500 randomly sampled examples per task and is evaluated on held-out test sets using task-specific metrics. The optimization process is weight-agnostic, focusing on prompt refinement rather than model retraining. Results are reported as mean ± 95% confidence intervals across three bootstrap trials to account for variance.

## Key Results
- SIMBA delivered the highest relative improvements in 56% of model-task evaluations
- Optimized pipelines achieved 53% median relative improvement over zero-shot baselines
- Largest gains ranged from 300% to 3,400% on tasks with particularly low initial performance
- In 22% of evaluations, optimized smallest models surpassed their family's largest model baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative feedback-driven prompt refinement outperforms static prompt design for medical VLMs
- Mechanism: SIMBA evaluates prompt candidates on mini-batches, uses execution feedback to guide instruction/example edits, and converges on combinations that capture diagnostic features the baseline prompts miss
- Core assumption: The model's embedded knowledge is sufficient; the bottleneck is prompt formulation, not capability
- Evidence anchors:
  - [abstract] "SIMBA emerged as the most effective optimizer, delivering the highest relative improvements in 56% of model-task evaluations"
  - [section 4] "SIMBA performs better because its iterative updates incorporate immediate feedback, allowing it to converge on prompts that capture the subtle diagnostic features critical for medical visual reasoning"
  - [corpus] MMedAgent-RL demonstrates similar gains from multi-agent reflective optimization for medical reasoning
- Break condition: When context windows truncate optimized prompts (observed in CheXpert with smaller models), removing critical input portions

### Mechanism 2
- Claim: Reflective instruction tuning alone can achieve substantial gains even without few-shot examples
- Mechanism: GEPA reflects on execution traces to generate feedback, then applies evolutionary search to preserve strongest instruction candidates—enabling improvement when labeled examples are scarce or images exceed context capacity
- Core assumption: Models can follow richer procedural instructions when they are systematically refined against task metrics
- Evidence anchors:
  - [abstract] "largest gains ranging from 300% to 3,400% on tasks where zero-shot performance was particularly low"
  - [section 4] "GEPA achieved the second-highest performance relying solely on instruction tuning"
  - [corpus] Limited direct corpus evidence for instruction-only optimization in medical VLMs
- Break condition: When task requires visual pattern recognition that cannot be verbally described (e.g., subtle radiographic findings)

### Mechanism 3
- Claim: Optimized smaller models can match or exceed unoptimized larger models within the same family
- Mechanism: Prompt optimization surface latent capabilities in smaller models that baseline prompts fail to activate, compressing the effective capability gap
- Core assumption: Baseline prompts underutilize model capacity; optimization recovers this margin
- Evidence anchors:
  - [section 3] "In 22% of evaluations, the best-performing optimized program for a model series' smallest size model surpassed its family's largest model baseline"
  - [section 4] "optimized llama3.2-11B reached 61.53% exact-match on DDI Skin Tone, far exceeding the baseline performance of the much larger llama3.2-90B at 15.4%"
  - [corpus] Generalist vs. specialist VLM comparison work (arXiv:2506.17337) suggests optimization narrows specialization gaps
- Break condition: When task complexity scales beyond smaller model's fundamental reasoning capacity regardless of prompt quality

## Foundational Learning

- Concept: **DSPy declarative programming paradigm**
  - Why needed here: Understanding that prompts become composable, optimizable modules rather than static strings
  - Quick check question: Can you explain how a DSPy "module" differs from a traditional prompt template?

- Concept: **In-context learning vs. weight modification**
  - Why needed here: Grasping why these techniques work without training—examples and instructions reshape activations at inference time
  - Quick check question: What constraint does weight-agnostic optimization impose on what can be improved?

- Concept: **Chain-of-thought reasoning in multimodal contexts**
  - Why needed here: All baseline prompts explicitly instructed CoT; optimizers refine how reasoning is scaffolded
  - Quick check question: Why might CoT instructions fail for medical images even when they work for text-only tasks?

## Architecture Onboarding

- Component map:
  Task definitions -> Baseline prompts (CoT instructions) -> DSPy optimizers (SIMBA, GEPA, MIPROv2, RandomSearch) -> Task-specific evaluation metrics -> 10 VLMs across Qwen/Gemma/Llama families

- Critical path: Define task signature → set baseline prompt → configure optimizer (500 train samples, 3 bootstrap trials) → run optimization → evaluate on held-out test set

- Design tradeoffs:
  - **SIMBA vs. MIPROv2**: SIMBA's iterative feedback yields higher gains (1.5× compute overhead) vs. MIPROv2's Bayesian approach (~1× overhead)
  - **Few-shot vs. instruction-only**: Few-shot helps when images fit context; GEPA's instruction-only approach works when context is constrained
  - **Optimizer selection**: SIMBA for maximum performance, RandomSearch for speed-critical deployment (2.5× overhead but simpler)

- Failure signatures:
  - Context window truncation removing input portions (CheXpert on smaller models)
  - Zero standard deviation across bootstrap trials (small test sets causing convergence to identical prompts)
  - Optimized prompts matching but not exceeding baseline (3 cases observed)

- First 3 experiments:
  1. Replicate SIMBA optimization on single task (e.g., DDI Skin Tone) with smallest model to verify setup and observe prompt evolution
  2. Compare SIMBA vs. GEPA on task with limited context capacity (CheXpert) to quantify instruction-only vs. few-shot tradeoff
  3. Test cross-task transfer: apply DDI-optimized prompts to new dermatology subset to assess generalization vs. overfitting

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can prompt optimization techniques effectively scale to multimodal compound AI systems that simultaneously process medical images and clinical text (e.g., radiology images with patient history)?
- Basis in paper: [explicit] "Future research could investigate more complex multimodal compound AI systems, such as those that simultaneously interpret medical scans and clinical notes... by optimizing both visual and textual prompts together."
- Why unresolved: This study optimized prompts for image-only inputs; compound systems require joint optimization across modalities, introducing additional complexity in prompt space and inter-modality reasoning.
- What evidence would resolve it: Experiments applying SIMBA or GEPA to systems combining medical images with structured clinical notes, demonstrating whether cross-modal prompt optimization yields comparable gains.

### Open Question 2
- Question: How can prompt optimization overcome context window constraints for high-resolution medical imaging tasks where few-shot examples exceed available token budgets?
- Basis in paper: [explicit] "In the CheXpert task, few-shot examples often contributed little to performance, and the large image data per input frequently exceeded the context window, preventing optimizers from effectively applying strategies like instruction tuning or iterative feedback."
- Why unresolved: Medical images consume substantial context capacity, limiting in-context learning; current optimizers lack mechanisms to dynamically compress or prioritize visual context.
- What evidence would resolve it: Development and evaluation of context-aware optimization strategies (e.g., image summarization, dynamic example selection) on tasks where baseline optimization failed due to token limits.

### Open Question 3
- Question: Do optimized prompts generalize across model families, or are they highly specific to the architecture and training corpus of the target VLM?
- Basis in paper: [inferred] The study shows different optimizers perform best for different model-task combinations, but does not test whether prompts optimized for one model transfer to others.
- Why unresolved: Understanding transferability would determine whether institutions must re-optimize for each deployed model or can share optimized prompts across systems.
- What evidence would resolve it: Cross-validation experiments applying prompts optimized on one model family (e.g., Qwen) to others (e.g., Gemma, Llama) on the same tasks, measuring performance degradation.

## Limitations
- Optimizer hyperparameters (trial counts, batch sizes, early stopping criteria) are not fully specified, requiring reliance on DSPy defaults or code inspection
- Context window constraints significantly impacted results, with smaller models experiencing input truncation during few-shot optimization
- High variance across bootstrap trials on small test sets (±40 points) raises questions about statistical power and stability

## Confidence

**High confidence**: The core finding that SIMBA delivers the highest relative improvements (56% of evaluations) is well-supported by consistent performance patterns across multiple model-task combinations.

**Medium confidence**: The claim of 53% median relative improvement over baselines is statistically supported but sensitive to bootstrap variance and test set size.

**Low confidence**: The assertion that optimized smaller models can exceed unoptimized larger models within the same family, while empirically observed, lacks theoretical explanation for why prompt optimization would overcome fundamental capacity differences.

## Next Checks
1. **Hyperparameter sensitivity analysis**: Systematically vary optimizer trial counts, batch sizes, and early stopping criteria to determine whether reported improvements persist across parameter ranges or are artifacts of specific settings.

2. **Cross-domain generalization test**: Apply the best-performing prompts from each medical specialty (radiology, gastroenterology, dermatology) to held-out datasets from different specialties to quantify overfitting versus genuine capability transfer.

3. **Context capacity stress test**: Evaluate all model-optimizer combinations with progressively truncated context windows (simulating smaller models or longer inputs) to map the boundary where few-shot optimization degrades into instruction-only optimization, and verify that GEPA's performance claims hold under these constraints.