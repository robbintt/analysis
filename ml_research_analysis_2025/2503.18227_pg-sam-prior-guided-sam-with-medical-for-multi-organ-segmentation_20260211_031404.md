---
ver: rpa2
title: 'PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation'
arxiv_id: '2503.18227'
source_url: https://arxiv.org/abs/2503.18227
tags:
- medical
- segmentation
- image
- arxiv
- pg-sam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PG-SAM addresses the accuracy degradation of SAM when applied to
  medical image segmentation by introducing a fine-grained modality prior aligner
  that leverages specialized medical knowledge from LLMs. The method enhances modality
  alignment through medical LLM-generated text and fine-tuned CLIP embeddings, while
  a novel decoder improves feature extraction via multi-level fusion and iterative
  mask optimization.
---

# PG-SAM: Prior-Guided SAM with Medical for Multi-organ Segmentation

## Quick Facts
- arXiv ID: 2503.18227
- Source URL: https://arxiv.org/abs/2503.18227
- Reference count: 39
- State-of-the-art performance on Synapse dataset with 84.79% Dice fully supervised, 75.75% few-shot

## Executive Summary
PG-SAM addresses the accuracy degradation of SAM when applied to medical image segmentation by introducing a fine-grained modality prior aligner that leverages specialized medical knowledge from LLMs. The method enhances modality alignment through medical LLM-generated text and fine-tuned CLIP embeddings, while a novel decoder improves feature extraction via multi-level fusion and iterative mask optimization. PG-SAM achieves state-of-the-art performance on the Synapse dataset, with mean Dice scores of 84.79% in fully supervised settings and 75.75% in few-shot scenarios. The method demonstrates superior boundary precision (HD95=12.35) and efficiency compared to existing approaches.

## Method Summary
PG-SAM integrates medical-specific knowledge into SAM through a Prior-Guided Align module that combines LLM-generated medical text descriptions with fine-tuned CLIP embeddings to improve modality alignment. The architecture features a multi-level feature fusion strategy in the decoder, enhancing spatial and semantic feature extraction. An iterative mask refinement process further optimizes segmentation boundaries. The approach operates in both fully supervised and few-shot settings, demonstrating significant performance improvements over baseline SAM and existing medical segmentation methods.

## Key Results
- State-of-the-art performance on Synapse dataset: 84.79% Dice (fully supervised), 75.75% Dice (few-shot)
- Superior boundary precision with HD95=12.35
- Enhanced efficiency compared to existing medical segmentation approaches

## Why This Works (Mechanism)
PG-SAM leverages specialized medical knowledge from LLMs to bridge the domain gap between general-purpose SAM and medical imaging requirements. The medical LLM generates fine-grained textual descriptions of organ structures and imaging modalities, which are then embedded using fine-tuned CLIP models to create modality-specific priors. These priors guide the segmentation process by improving the alignment between visual features and anatomical structures. The multi-level feature fusion strategy combines contextual and local information at different scales, while the iterative mask optimization refines boundaries through progressive refinement, resulting in more accurate and precise segmentation masks.

## Foundational Learning

**Medical Image Segmentation**: Understanding anatomical structures in medical imaging requires domain-specific knowledge about organ morphology, imaging modalities, and pathological variations. Why needed: Medical images have different characteristics than natural images, requiring specialized feature extraction and interpretation.

**CLIP Embeddings**: Contrastive Language-Image Pre-training enables semantic understanding by mapping images and text into a shared embedding space. Why needed: Allows integration of textual medical knowledge with visual features for improved modality alignment.

**Iterative Mask Refinement**: Progressive optimization of segmentation masks through multiple refinement steps. Why needed: Medical segmentation requires high precision at organ boundaries where small errors can have clinical significance.

**Few-Shot Learning**: Learning from limited annotated examples through knowledge transfer and prior information. Why needed: Medical data annotation is expensive and time-consuming, requiring models to perform well with minimal supervision.

**Multi-level Feature Fusion**: Combining features from different spatial scales and semantic levels. Why needed: Medical structures vary in size and complexity, requiring both global context and local detail for accurate segmentation.

## Architecture Onboarding

**Component Map**: Medical LLM Text Generation -> CLIP Embedding Fine-tuning -> Prior-Guided Align Module -> SAM Encoder Features -> Multi-level Feature Fusion Decoder -> Iterative Mask Refinement -> Final Segmentation

**Critical Path**: The Prior-Guided Align module serves as the critical path, transforming LLM-generated text through fine-tuned CLIP embeddings into modality priors that guide the entire segmentation pipeline. This component bridges the domain gap and enables SAM to effectively process medical images.

**Design Tradeoffs**: The approach trades computational overhead from LLM integration and iterative refinement for significantly improved segmentation accuracy. While the method introduces additional complexity through medical knowledge integration, it achieves superior performance with reasonable efficiency gains compared to fully retraining medical segmentation models.

**Failure Signatures**: The method may struggle with rare anatomical variations not well-represented in the LLM training data, leading to inaccurate priors. Additionally, the quality of CLIP embeddings depends heavily on the medical fine-tuning process, and poor fine-tuning could propagate errors throughout the segmentation pipeline.

**First Experiments**:
1. Test LLM text generation quality by manually evaluating medical descriptions against expert annotations
2. Validate CLIP embedding quality by measuring cosine similarity between generated and reference medical image-text pairs
3. Benchmark Prior-Guided Align module performance with synthetic medical images before full pipeline integration

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to Synapse dataset, restricting generalizability to other medical imaging domains
- Lack of ablation studies to isolate contributions of individual components (LLM priors vs. CLIP fusion)
- Computational efficiency claims not substantiated with detailed timing and memory usage metrics

## Confidence
- **High confidence**: State-of-the-art performance on Synapse dataset (84.79% Dice fully supervised, 75.75% few-shot)
- **Medium confidence**: LLM integration methodology and multi-level fusion architecture
- **Low confidence**: Cross-domain generalization and efficiency claims without empirical validation

## Next Checks
1. Evaluate PG-SAM on at least two additional medical imaging datasets (e.g., BTCV, Medical Segmentation Decathlon) to assess cross-domain performance and robustness
2. Conduct component ablation studies comparing: (a) baseline SAM, (b) PG-SAM without LLM priors, (c) PG-SAM without CLIP embedding fusion, and (d) full PG-SAM to isolate performance contributions
3. Perform comprehensive computational analysis including inference time, memory footprint, and GPU utilization across different hardware configurations (RTX 3090, A100, and CPU-only inference) to validate efficiency claims