---
ver: rpa2
title: Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention
  Adversarial Dual AutoEncoders
arxiv_id: '2511.20480'
source_url: https://arxiv.org/abs/2511.20480
tags:
- data
- learning
- detection
- active
- anomaly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents ALADAEN, a novel framework that combines deep
  adversarial dual autoencoders with active learning and GAN-based augmentation to
  detect Advanced Persistent Threats (APTs) in highly imbalanced provenance data.
  ALADAEN learns robust normal behavior representations through a dual-autoencoder
  architecture with attention and adversarial training, then iteratively improves
  detection by selectively querying uncertain samples for oracle labeling and augmenting
  the labeled normal pool with synthetic data generated by a GAN.
---

# Ranking-Enhanced Anomaly Detection Using Active Learning-Assisted Attention Adversarial Dual AutoEncoders

## Quick Facts
- **arXiv ID:** 2511.20480
- **Source URL:** https://arxiv.org/abs/2511.20480
- **Reference count:** 30
- **One-line primary result:** ALADAEN achieves 100%+ nDCG improvements over state-of-the-art methods for APT detection in highly imbalanced provenance data.

## Executive Summary
ALADAEN introduces a novel framework for detecting Advanced Persistent Threats (APTs) in provenance graph data by combining dual adversarial autoencoders with active learning and GAN-based augmentation. The framework addresses the challenge of detecting rare attacks (<0.004% of data) by learning robust normal behavior representations and iteratively improving detection through selective querying of uncertain samples. Evaluated on 40 real-world datasets from the DARPA Transparent Computing program, ALADAEN significantly outperforms classical and deep learning baselines while maintaining computational efficiency suitable for operational security environments.

## Method Summary
The ALADAEN framework integrates an Attention Adversarial Dual AutoEncoder (ADAEN) with active learning and GAN-based data augmentation. ADAEN consists of two autoencoders: AE1 with attention layers for robust normal behavior representation, and AE2 as a reconstruction-refiner/discriminator. The model is trained on binary process feature vectors using a combined reconstruction and adversarial loss. Active learning starts with 20% labeled normal data, iteratively queries uncertain samples near the 80th percentile reconstruction error threshold, labels them through oracle queries, generates synthetic normal samples using a GAN, and retrains the model. This process continues for up to 40 iterations, achieving near-optimal performance with minimal labeled data.

## Key Results
- Achieves nDCG scores exceeding 100% improvement over state-of-the-art methods in some configurations
- Reaches near-optimal performance (nDCG close to 1.0) in as few as 5 active learning iterations
- Effectively detects APTs constituting less than 0.004% of the dataset
- Maintains computational efficiency suitable for operational SOC environments

## Why This Works (Mechanism)
ALADAEN works by learning discriminative normal behavior representations through the dual autoencoder architecture, where attention mechanisms focus on critical features and adversarial training enhances robustness. The active learning component efficiently identifies uncertain samples that are most informative for model improvement, while GAN-based augmentation expands the labeled normal dataset with synthetic examples. This combination addresses the extreme class imbalance by focusing computational resources on the most challenging samples while maintaining a strong normal behavior baseline.

## Foundational Learning
- **Dual Autoencoder Architecture**: Why needed - captures both reconstruction and adversarial perspectives of normal behavior. Quick check - verify both AEs produce meaningful reconstructions on normal data.
- **Attention Mechanisms in Autoencoders**: Why needed - focuses on discriminative features while suppressing noise. Quick check - validate attention weights highlight relevant features.
- **Active Learning Query Strategies**: Why needed - efficiently selects most informative samples for labeling. Quick check - confirm 80th percentile threshold selects genuinely uncertain samples.
- **GAN-based Data Augmentation**: Why needed - expands limited labeled normal data with synthetic examples. Quick check - verify generated samples maintain binary constraints and diversity.
- **Provenance Graph Analysis**: Why needed - understands system behavior through process relationships. Quick check - confirm feature vectors capture meaningful process interactions.
- **Extreme Class Imbalance Handling**: Why needed - addresses detection challenges when attacks represent <0.01% of data. Quick check - validate model maintains high recall for rare attacks.

## Architecture Onboarding

**Component Map:** Input Data -> ADAEN (AE1+Attention+AE2) -> Reconstruction Errors -> Active Learning Loop -> Oracle Queries -> GAN Augmentation -> Retraining

**Critical Path:** ADAEN training → Reconstruction error computation → 80th percentile threshold selection → Oracle query → GAN generation → Model retraining

**Design Tradeoffs:** The dual autoencoder provides robustness but increases computational complexity; attention mechanisms improve feature selection but add parameters; active learning reduces labeling costs but requires careful threshold management; GAN augmentation expands data but risks introducing synthetic noise.

**Failure Signatures:** GAN mode collapse producing non-binary outputs; threshold drift causing selection of uninformative samples; overfitting to initial 20% normal subset; reconstruction errors failing to distinguish subtle attack patterns.

**Three First Experiments:**
1. Train ADAEN on 20% labeled normal subset and evaluate baseline reconstruction quality
2. Implement active learning loop with 80th percentile threshold and validate uncertainty selection
3. Generate synthetic samples using GAN and verify binary constraints and diversity metrics

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- GAN architecture details are not specified, making faithful reproduction difficult
- Query batch size Q is referenced but not numerically defined in the algorithm
- Attention mechanism integration is ambiguous for binary vector inputs
- 20% cold start subset may not capture all normal system behaviors across diverse environments

## Confidence

**High Confidence:**
- Dual autoencoder architecture design and loss function formulation
- Active learning query strategy using 80th percentile threshold
- Evaluation methodology using nDCG metric and DARPA datasets

**Medium Confidence:**
- Overall framework integration and component interactions
- Implementation of active learning loop with iterative retraining

**Low Confidence:**
- GAN architecture specifications and training details
- Exact batch sizing for oracle queries
- Specific attention mechanism implementation for binary vectors

## Next Checks
1. Implement the dual autoencoder with attention layer and validate reconstruction quality on the 20% labeled normal subset before proceeding to active learning
2. Test the 80th percentile threshold selection strategy on a held-out validation set to verify it captures genuinely uncertain samples rather than easy negatives
3. Validate the GAN synthetic data generation by checking binary value constraints and diversity metrics before integrating into the active learning loop