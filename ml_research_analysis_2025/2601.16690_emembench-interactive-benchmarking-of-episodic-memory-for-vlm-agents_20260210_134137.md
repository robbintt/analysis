---
ver: rpa2
title: 'EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents'
arxiv_id: '2601.16690'
source_url: https://arxiv.org/abs/2601.16690
tags:
- step
- memory
- question
- action
- game
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EMemBench is a programmatic benchmark for evaluating long-term\
  \ episodic memory in agents through interactive games. Instead of fixed question\
  \ sets, it generates QA instances from each agent\u2019s own trajectory, using underlying\
  \ game signals to compute verifiable ground truths."
---

# EMemBench: Interactive Benchmarking of Episodic Memory for VLM Agents

## Quick Facts
- **arXiv ID**: 2601.16690
- **Source URL**: https://arxiv.org/abs/2601.16690
- **Reference count**: 40
- **Primary result**: Trajectory-conditioned question generation with deterministic ground truth enables fully automatic evaluation of episodic memory; induction and spatial reasoning remain hardest, especially for VLM agents.

## Executive Summary
EMemBench introduces a programmatic benchmark for evaluating long-term episodic memory in agents through interactive games. Unlike static question sets, it generates questions from each agent's own trajectory, using underlying game signals to compute verifiable ground truths. The benchmark covers 15 Jericho text games and one visual survival game (Crafter), with questions balanced across seven memory skills. Across models and memory agents, results show significant room for improvement, particularly for visually grounded episodic memory tasks.

## Method Summary
EMemBench generates questions from an agent's specific trajectory using deterministic templates and game state signals. For text games (Jericho), it logs structured observations and uses BFS for spatial answers. For visual games (Crafter), it captures cropped frames and logs player coordinates, health, inventory, and game events. Questions are categorized into seven ability types (single/multi-hop recall, induction, temporal, spatial, logical, adversarial). The benchmark supports query-horizon control to normalize difficulty across agents. Memory agents use retrieval-augmented architectures (Mem0, LangMem, A-MEM) with configurable top-k settings.

## Key Results
- Induction and spatial reasoning are persistent bottlenecks, with accuracy below 35% on both text and visual games
- Persistent memory modules yield gains for open-weight backbones on text games but improvements are less consistent for VLMs
- Visually grounded episodic memory remains an open challenge, with spatial reasoning peaking at only 24.3% accuracy
- Performance correlates negatively with trajectory length, validating the need for horizon control

## Why This Works (Mechanism)

### Mechanism 1: Trajectory-Conditioned Question Generation
The system records agent trajectories and game states, then generates questions specifically from what each agent experienced. This ensures evaluation measures "what happened to me" rather than "what happens in this game," creating a more valid test of episodic memory. If agents follow identical paths, the evaluation collapses to a static benchmark.

### Mechanism 2: Deterministic Ground Truth via Game Signals
Instead of human annotation or LLM judgment, the benchmark accesses structured game state (coordinates, inventories, maps) to compute exact answers. Algorithms like BFS calculate precise answers (e.g., "6 steps up") which are compared against agent outputs. This removes annotation noise but depends on reliable state mapping.

### Mechanism 3: Query Horizon Control
Restricting the time window for question generation normalizes difficulty and prevents stronger agents from simply surviving longer and getting harder questions. By bounding questions to specific steps (e.g., 1-50), the benchmark controls for trajectory length variance, ensuring scores reflect memory quality rather than game-playing skill.

## Foundational Learning

- **Episodic vs. Semantic Memory**: The benchmark differentiates between general knowledge and personal experience. Quick check: Does this question require knowing a general rule of the game, or remembering a specific event that happened at step 45?

- **In-Context vs. Persistent Memory**: The paper compares storing history in the prompt versus external storage. Quick check: Is the agent recalling information because it's currently visible in the prompt window, or retrieving it from an external store?

- **Partially Observable Environments**: Visual games limit view to local crops, necessitating spatial memory integration. Quick check: Can the agent answer questions about parts of the map it visited 100 steps ago but cannot currently see?

## Architecture Onboarding

- **Component map**: Agent Interface -> Game Env -> State Logger -> QA Generator -> Evaluator

- **Critical path**:
  1. Execution: Agent plays game; State Logger captures full trajectory Ï„ and state S
  2. Instantiation: QA Generator builds indices and runs templates to create test set
  3. Retrieval/Answering: Agent answers questions using memory mechanism
  4. Scoring: Rule-based matcher computes accuracy

- **Design tradeoffs**:
  - Template vs. Free-form QA: Templates enable deterministic ground truth but risk lower linguistic diversity
  - Visual vs. Text: Text games allow precise state tracking; Visual games introduce grounding noise but test multimodal memory
  - QHC Settings: Standard evaluation reflects realistic usage but adds length variance; Horizon Control ensures fairness but may truncate complex memories

- **Failure signatures**:
  - Retrieval Failure: Agent answers "not answerable" for questions about clearly present steps
  - Grounding Hallucination: Agent invents objects not present in visual frames or confuses terrain types
  - Induction Deficit: Agent fails to generalize patterns despite having full history access

- **First 3 experiments**:
  1. Baseline Run: In-Context agent on text game to establish difficulty floor and verify logging pipeline
  2. Memory Ablation: Mem0/A-MEM module on "Multi-hop" and "Temporal" questions to measure specific gains
  3. Horizon Stress Test: QHC=50 vs QHC=-1 to determine if performance drops stem from context limits or retrieval failures

## Open Questions the Paper Calls Out

### Open Question 1
What architectural innovations in memory modules are needed to achieve consistent gains for VLM agents on visually grounded episodic memory tasks? Current memory modules show inconsistent or negative gains on visual games; visual spatial reasoning peaks at only 24.3% accuracy.

### Open Question 2
How can induction and spatial reasoning capabilities be improved in memory-augmented agents given they remain the primary bottlenecks? These abilities require aggregating scattered evidence over long horizons and maintaining spatial coherence under partial observability.

### Open Question 3
How generalizable are the findings to interactive environments beyond the 16 games tested? Template-based question generation is environment-dependent, limiting ecological validity claims.

## Limitations
- Limited environment coverage (15 text games + 1 visual game) restricts generalizability
- Template-based generation may create evaluation artifacts through repetitive phrasing patterns
- Visual game evaluation depends on accurate image-to-game-state mapping that introduces potential noise sources

## Confidence
- **High confidence**: Trajectory-conditioned evaluation mechanism and deterministic ground truth computation are technically sound
- **Medium confidence**: Claims about induction and spatial reasoning being universally hardest need more cross-environment validation
- **Medium confidence**: Memory module effectiveness differences between text and visual domains are documented but require deeper failure mode analysis

## Next Checks
1. Test the benchmark with a broader range of games (including more visual environments) to assess whether current findings generalize beyond the specific domains evaluated
2. Conduct human evaluation of generated questions to verify that template-based generation maintains sufficient diversity and challenge across ability types
3. Perform ablation studies on horizon control settings (QHC=25, QHC=100) to determine the optimal balance between fairness and long-term memory assessment