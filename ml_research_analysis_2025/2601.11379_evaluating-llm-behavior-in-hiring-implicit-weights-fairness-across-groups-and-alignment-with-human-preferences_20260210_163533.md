---
ver: rpa2
title: 'Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups,
  and Alignment with Human Preferences'
arxiv_id: '2601.11379'
source_url: https://arxiv.org/abs/2601.11379
tags:
- profiles
- profile
- brief
- experience
- scoring
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework to evaluate how large language
  models (LLMs) assign weights to different attributes in hiring decisions. Using
  a synthetic dataset of freelancer profiles and project briefs, the authors employ
  a full factorial design to systematically vary candidate and recruiter characteristics.
---

# Evaluating LLM Behavior in Hiring: Implicit Weights, Fairness Across Groups, and Alignment with Human Preferences

## Quick Facts
- **arXiv ID**: 2601.11379
- **Source URL**: https://arxiv.org/abs/2601.11379
- **Reference count**: 32
- **Key outcome**: Introduces framework to evaluate LLM implicit weights in hiring; finds Gemini 2.0 Flash prioritizes productivity signals with minimal average discrimination but significant intersectional effects.

## Executive Summary
This paper presents a framework for evaluating how large language models assign weights to different attributes in hiring decisions. Using a synthetic dataset of 10,800 freelancer profiles generated through full factorial design, the authors systematically vary candidate and recruiter characteristics to estimate causal effects on LLM scoring. The study finds that while Gemini 2.0 Flash prioritizes core productivity signals like skills and experience, it shows minimal average discrimination against minority groups. However, intersectional analysis reveals that the LLM applies different evaluation standards across demographic subgroups, with productivity signals carrying varying weights depending on the candidate's demographic profile.

## Method Summary
The authors employ a full factorial experimental design to generate synthetic freelancer profiles by systematically varying 10 attributes (name origin, gender, age, education, industry alignment, experience, skills, platform reputation, rate, profile completion) across all possible combinations. They create 16 project briefs from a real freelance platform and use Gemini 2.0 Flash via Vertex AI API to score each profile-brief pair on a 0-10 scale with temperature=0. The study uses OLS regression with clustered standard errors to estimate causal effects of each attribute, then tests for heterogeneity through interaction models across demographic subgroups and brief characteristics.

## Key Results
- LLM prioritizes productivity signals: Skills mismatch (-0.69), experience below requirements (-2.25), and no platform reputation (-1.19) show strongest effects
- Minimal average discrimination: No significant average penalty for Arabic, Chinese, or minority names; minimal gender effects
- Significant intersectional effects: Different weights applied to productivity signals across demographic groups (e.g., stricter industry alignment for women, more tolerance for experience gaps for Arabic profiles)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Full factorial design enables causal identification of LLM decision weights.
- Mechanism: Systematic variation of all attributes independently breaks natural correlations, allowing OLS coefficients to be interpreted as marginal causal effects.
- Core assumption: LLM evaluation logic is stable across experimental space and additivity holds.
- Evidence anchors: [abstract] "apply a full factorial design to estimate how a LLM weighs different match-relevant criteria"; [section 3.2] "This controlled design breaks natural correlations... allowing us to isolate the marginal effect of each attribute"
- Break condition: If interaction effects dominate main effects (R² drops significantly below 0.90), causal interpretation becomes unreliable.

### Mechanism 2
- Claim: LLM approximates weighted additive scoring function for hiring evaluations.
- Mechanism: Gemini 2.0 Flash assigns quantifiable penalties/bonuses to profile attributes relative to reference candidate.
- Core assumption: OLS provides reasonable linear approximation of possibly non-linear decision function.
- Evidence anchors: [abstract] "The LLM prioritizes core productivity signals... but shows minimal average discrimination"; [section 4.1] "the high explanatory power of the OLS regression (R² = 0.90) suggests that the LLM behaves... as if it applies a weighted sum of profile attributes"
- Break condition: If ranking tasks show dramatically different weight distributions, additive assumption is task-dependent.

### Mechanism 3
- Claim: Intersectional subgroup analysis reveals heterogeneous evaluation criteria that average effects mask.
- Mechanism: Interaction terms between demographic indicators and productivity signals show LLM applies different standards across groups.
- Core assumption: Observed interaction effects reflect systematic evaluation logic rather than random noise.
- Evidence anchors: [abstract] "intersectional effects reveal that productivity signals carry different weights across demographic subgroups"; [section 4.2] "These patterns reveal that the evaluation logic of the LLM doesn't simply apply uniform criteria"
- Break condition: If interaction coefficients are statistically insignificant or magnitude << main effects, subgroup differences may be noise.

## Foundational Learning

- **Concept: Full Factorial Experimental Design**
  - Why needed here: The paper's causal claims rest on this design principle for enabling causal inference.
  - Quick check question: If profiles with Arabic names also systematically had lower experience in the dataset, could you isolate the causal effect of name alone?

- **Concept: OLS as Descriptive Linear Approximation**
  - Why needed here: The paper explicitly states it does not assume the LLM follows a linear model, yet uses OLS for interpretability.
  - Quick check question: Why does R² = 0.90 support using OLS descriptively but not prove the LLM is "actually linear"?

- **Concept: Intersectionality in Algorithmic Fairness**
  - Why needed here: The paper's key finding is that average effects mask subgroup-specific discrimination patterns.
  - Quick check question: If a model shows no average gender penalty but penalizes women without platform badges more heavily than men without badges, is it fair?

## Architecture Onboarding

- **Component map:**
  Synthetic data generator -> LLM scoring interface -> Regression analysis pipeline -> Extension framework

- **Critical path:**
  1. Define attribute levels from real platform data
  2. Generate all factorial combinations
  3. Match each profile to each brief (172,800 pairs)
  4. Score each pair 3× with LLM
  5. Run OLS regression
  6. Test interaction effects by subgroup and brief characteristics

- **Design tradeoffs:**
  - Internal validity vs. external validity: Synthetic factorial design enables causal inference but profiles are not representative of real distributions
  - Dataset size vs. attribute granularity: Each additional attribute level multiplies profile count; authors limited to 10,800 profiles
  - Single model vs. generalizability: Only Gemini 2.0 Flash tested; bias patterns may vary across models

- **Failure signatures:**
  - R² drops significantly (< 0.80): Suggests LLM behavior is non-additive or prompt/design issue
  - Large score variance across 3 repeated runs: Indicates temperature/decoding instability
  - Demographic main effects >> productivity effects: Would indicate overt bias
  - Interaction effects reverse in ranking task: Appendix C shows this may occur; indicates task-sensitivity

- **First 3 experiments:**
  1. Replicate with second occupation: Authors did this (SEO writing, Appendix D)—observed similar patterns with slightly stronger skill penalties and emergent gender penalty
  2. Test alternative LLM: Paper explicitly lists this as a limitation; compare Gemini against other models using identical prompts
  3. Compare scoring vs. ranking: Appendix C preliminary analysis suggests ranking amplifies demographic weights—formalize with full factorial ranking design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do LLM biases and implicit weights shift when the evaluation task changes from absolute scoring to relative ranking?
- Basis in paper: [explicit] The authors note that while scoring is the primary focus, "biases that appear negligible in scoring may be amplified in ranking," warranting formal analysis.
- Why unresolved: The study mainly evaluated profiles individually; preliminary ranking tests suggested different feature importance but were not the primary method.
- What evidence would resolve it: A full factorial study using pairwise or listwise ranking prompts to measure interaction effects in a competitive evaluation setting.

### Open Question 2
- Question: To what extent do the observed evaluation logic and intersectional fairness issues generalize across different model architectures?
- Basis in paper: [explicit] The authors state their findings are specific to Gemini 2.0 Flash and "cannot be generalized to other architectures, model sizes, or training paradigms."
- Why unresolved: Different training data or alignment techniques could result in significantly different implicit weights for demographic or productivity signals.
- What evidence would resolve it: Replicating the exact experimental framework on a diverse set of LLMs (e.g., GPT-4, Claude, Llama) to compare causal effects.

### Open Question 3
- Question: How do LLMs interpret implicit economic signals, such as penalizing low daily rates as indicators of incompetence?
- Basis in paper: [inferred] The authors observe that the LLM penalizes low rates, suggesting it infers "lower competence or shorter experience," and explicitly call for future work to explore how LLMs infer such signals.
- Why unresolved: It is unclear if the model is applying economic signaling theory or mimicking biased correlations from training data regarding freelancer pricing.
- What evidence would resolve it: Experiments that disentangle price from quality signals or probe the model's chain-of-thought reasoning regarding pricing decisions.

### Open Question 4
- Question: Can LLMs be aligned with human recruiter preferences without replicating human biases?
- Basis in paper: [explicit] The authors identify "a critical open question is whether LLMs should be aligned with human preferences and how to do so without replicating human biases."
- Why unresolved: Human recruiters exhibit documented discrimination; aligning models to human feedback (RLHF) or preferences risks reinforcing these inequalities.
- What evidence would resolve it: Implementing the proposed Incentivized Resume Rating (IRR) with human recruiters to compare their implicit weights against the LLM, followed by fairness-aware fine-tuning.

## Limitations
- Single model tested: Findings specific to Gemini 2.0 Flash and may not generalize across LLM architectures
- Synthetic data constraint: Profiles represent all possible combinations rather than real-world distributions
- Limited occupation scope: Focus on technical writing may not reflect patterns in other industries or job types

## Confidence
- **High confidence**: Methodological framework and causal identification approach (R² = 0.90 supports additive approximation)
- **Medium confidence**: Average effect estimates across minority groups (minimal discrimination observed)
- **Low confidence**: Generalizing intersectional interaction patterns beyond tested occupation and model

## Next Checks
1. Replicate the full factorial experiment with a second occupation (e.g., software development) to test generalizability of interaction patterns
2. Compare Gemini 2.0 Flash against multiple LLM models (GPT-4, Claude, etc.) using identical prompts and profiles
3. Extend the framework to human recruiters via Incentivized Resume Rating to enable systematic comparison of algorithmic vs. human decision logic