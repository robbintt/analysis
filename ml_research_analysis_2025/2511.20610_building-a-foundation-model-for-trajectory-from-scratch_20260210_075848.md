---
ver: rpa2
title: Building a Foundation Model for Trajectory from Scratch
arxiv_id: '2511.20610'
source_url: https://arxiv.org/abs/2511.20610
tags:
- foundation
- trajectory
- mobility
- gpt-2
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This tutorial demonstrates how to build a minimal trajectory foundation
  model from scratch, adapting GPT-2 for spatiotemporal data. It walks through the
  complete process: from reworking GPT-2''s embedding, positional encoding, and attention
  mechanisms to handle GPS trajectories, to implementing a delta-encoding training
  scheme and masked prediction tasks.'
---

# Building a Foundation Model for Trajectory from Scratch

## Quick Facts
- arXiv ID: 2511.20610
- Source URL: https://arxiv.org/abs/2511.20610
- Reference count: 16
- Primary result: Tutorial demonstrates building a minimal trajectory foundation model from scratch, adapting GPT-2 for spatiotemporal data

## Executive Summary
This tutorial provides a hands-on guide for building a trajectory foundation model by adapting GPT-2 for spatiotemporal data. It walks through the complete process from reworking embeddings and positional encodings to implementing delta-encoding training schemes and masked prediction tasks. The implementation draws on techniques from state-of-the-art models like TrajFM and TrajGPT, showing how these can be integrated into a working prototype. The resulting educational model enables trajectory completion, next-step prediction, and transfer learning, with modular PyTorch code provided for reproducibility.

## Method Summary
The tutorial adapts GPT-2 for trajectory data by replacing its tokenizer and embedding layer with custom parsers that project continuous (latitude, longitude, timestamp) inputs into a shared latent space. It implements delta encoding for stable autoregressive learning, where the model predicts relative changes between consecutive points rather than absolute coordinates. The architecture uses 2 transformer blocks instead of GPT-2's 12 for faster iteration, and introduces masked trajectory modeling to enable multi-task transfer. The model supports both next-step prediction and masked completion tasks, drawing on techniques like rotary positional embeddings and patching from recent literature.

## Key Results
- Successfully adapted GPT-2 architecture for continuous spatiotemporal trajectory data
- Implemented delta encoding scheme that stabilizes autoregressive training
- Created a masked trajectory modeling variant enabling multi-task transfer learning
- Demonstrated practical implementation with modular PyTorch code for SIGSPATIAL community

## Why This Works (Mechanism)

### Mechanism 1: Delta Encoding for Stable Autoregressive Learning
- Claim: Predicting relative changes between consecutive trajectory points produces more stable training targets than absolute coordinates.
- Mechanism: Instead of outputting raw (latitude, longitude, timestamp) triplets, the model learns to predict Δlat, Δlon, Δt—normalizing the output distribution and reducing variance across geographic regions.
- Core assumption: Trajectory increments follow more consistent statistical patterns than absolute positions, and sequential dependencies are preserved under differencing.
- Evidence anchors: [abstract], [section 3]
- Break condition: When trajectories contain discontinuous jumps or when cumulative error from sequential predictions exceeds tolerance for long-horizon forecasting.

### Mechanism 2: Continuous Spatiotemporal Projection Replaces Discrete Tokenization
- Claim: Learnable linear projections map continuous (lat, lon, time) inputs into a shared latent space more effectively than discretizing coordinates into token vocabularies.
- Mechanism: Raw coordinates are normalized (centered around geographic mean), time is decomposed into cyclic features (day-of-week, hour, minute), and a fully-connected layer projects the concatenated vector into the model's embedding dimension.
- Core assumption: Spatial and temporal features share representational structure that can be learned jointly, and normalization preserves transferability across geographic regions.
- Evidence anchors: [abstract], [section 3], [section 4.1]
- Break condition: When input features have drastically different scales not normalized, or when embedding dimension is insufficient to separate semantically distinct locations.

### Mechanism 3: Masked Trajectory Modeling Enables Multi-Task Transfer
- Claim: Masking strategies adapted from language models allow a single pre-trained trajectory encoder to support diverse downstream tasks (completion, travel-time estimation, path inference).
- Mechanism: During pre-training, random trajectory segments or specific feature dimensions (spatial only, temporal only) are masked; the model learns to reconstruct missing elements from bidirectional context.
- Core assumption: Trajectory structure is locally predictable and global context improves reconstruction—similar to masked language modeling assumptions in NLP.
- Evidence anchors: [abstract], [section 3], [section 4.1]
- Break condition: When masked segments exceed typical trajectory autocorrelation length, or when masking destroys temporal ordering required for causal prediction.

## Foundational Learning

- **Latent Space Representations**
  - Why needed here: Foundation models operate by projecting raw inputs into a learned vector space where semantic relationships are preserved; understanding this is prerequisite to interpreting embeddings, attention outputs, and transfer learning.
  - Quick check question: Can you explain why two trajectory points with similar latent vectors might still have different absolute coordinates?

- **Positional Encoding (Absolute vs. Relative)**
  - Why needed here: Trajectories are inherently ordered sequences; the paper contrasts GPT-2's learned positional encoding, original Transformer sinusoidal encoding, and RoPE—each handles position differently and affects how the model captures long-range dependencies.
  - Quick check question: Why might RoPE (rotary positional embedding) be better suited for long trajectories than absolute positional encodings?

- **Autoregressive vs. Masked Prediction Objectives**
  - Why needed here: The tutorial shows how next-step prediction (autoregressive) and masked completion differ in what context they permit the model to see; this choice determines which downstream tasks transfer well.
  - Quick check question: If you wanted a model to estimate total travel time from a partial trajectory, which training objective would be more appropriate and why?

## Architecture Onboarding

- **Component map:**
  Raw Trajectory (lat, lon, t triplets) -> Preprocessing: Normalization + Time Decomposition -> Delta Encoding -> Projection Layer -> Positional Encoding -> Stacked Transformer Blocks -> Output Head -> Loss Computation

- **Critical path:**
  1. Verify preprocessing preserves reconstructability—build a tiny autoencoder to confirm normalized inputs can be decoded back to original coordinates
  2. Validate delta encoding does not introduce numerical instability (check Δ distributions are bounded)
  3. Confirm streaming data loader yields correctly shaped batches before scaling training

- **Design tradeoffs:**
  - 2 vs. 12 transformer blocks: Paper reduces to 2 for faster iteration and reduced overfitting on small datasets, at cost of modeling capacity for complex trajectories
  - Continuous coordinates vs. discrete regions: Classification simplifies training but loses spatial granularity
  - Absolute vs. delta targets: Deltas stabilize training but accumulate error over long horizons; absolute coordinates avoid compounding error but require larger output range

- **Failure signatures:**
  - Model outputs collapse to mean (predicting zero deltas) → learning rate too low or loss weighting unbalanced
  - Predicted trajectories drift geographically → positional encoding not integrating properly or normalization statistics mismatched
  - Training loss plateaus early with high variance → check for NaN values in streaming loader or malformed time features

- **First 3 experiments:**
  1. Autoencoder sanity check: Train a 1-layer autoencoder on preprocessed trajectory points to verify that your normalization + projection pipeline preserves information
  2. Minimal overfit test: Train the 2-block model on a single trajectory until loss approaches zero; confirms model capacity and data pipeline are functional before scaling
  3. Masking ablation: Compare next-step prediction vs. 15% random segment masking on a held-out validation set to quantify which objective transfers better to trajectory completion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does representing trajectories as continuous coordinates provide better generalization than discrete region sequences for foundation models?
- Basis in paper: [inferred] The paper contrasts TrajGPT's discrete region prediction with TrajFM and the tutorial's continuous coordinate approach, noting the trade-offs but not resolving which is superior for transferability.
- Why unresolved: The paper is a tutorial comparing existing methods rather than a comparative study evaluating the fundamental performance difference between discrete and continuous representations across diverse downstream tasks.
- What evidence would resolve it: A standardized benchmark comparing both encoding strategies on identical pre-training data and downstream tasks would resolve this.

### Open Question 2
- Question: How does the efficiency of the tutorial's delta encoding compare to Rotary Positional Embeddings (RoPE) in handling long-range spatiotemporal dependencies?
- Basis in paper: [inferred] The tutorial utilizes delta encoding for simplicity but does not empirically validate its effectiveness against the more complex RoPE mechanisms described in the literature review.
- Why unresolved: The paper implements delta encoding for simplicity but does not empirically validate its effectiveness against the more complex RoPE mechanisms described in the literature review.
- What evidence would resolve it: Ablation studies on long trajectory sequences comparing model convergence rates and error accumulation using delta encoding versus RoPE would provide an answer.

### Open Question 3
- Question: Can "patching" significantly reduce computational overhead for trajectory models without sacrificing the granular accuracy required for high-frequency GPS data?
- Basis in paper: [explicit] Section 4.3 mentions TimesFM's patching approach as a way to reduce sequence length and computational overhead, contrasting it with standard point-by-point processing.
- Why unresolved: The paper identifies patching as a noteworthy, complementary technique but does not integrate it into the tutorial model or analyze its specific impact on trajectory data granularity.
- What evidence would resolve it: An analysis of inference latency and memory usage against granular reconstruction error for patched versus non-patched trajectory models would resolve this.

## Limitations
- Tutorial lacks rigorous empirical validation of delta encoding effectiveness and masking strategy benefits
- Educational model's simplicity (2 transformer blocks) limits practical utility for complex trajectory patterns
- Claims about regional transferability of normalized embeddings not systematically tested across diverse geographic regions

## Confidence
- **High Confidence:** Core architectural modifications (removing GPT-2 tokenizer, adding custom projection layer, implementing delta encoding) are technically sound and follow established foundation model principles
- **Medium Confidence:** Delta encoding mechanism's effectiveness for stable autoregressive learning is theoretically justified but lacks empirical validation
- **Low Confidence:** Claims about regional transferability of normalized embeddings and superiority of continuous coordinates over discrete region prediction are stated without systematic comparison

## Next Checks
1. **Delta Encoding Ablation Study:** Train identical models using both delta encoding and absolute coordinate prediction on the same dataset, comparing training stability, convergence speed, and long-horizon prediction accuracy.
2. **Masking Strategy Comparison:** Implement and compare multiple masking strategies (random point masking, segment masking, feature-wise masking) against pure next-step prediction, measuring transfer performance on trajectory completion, travel-time estimation, and path inference tasks.
3. **Cross-Regional Transferability Test:** Train the model on trajectories from one geographic region, then evaluate on completely different regions without fine-tuning. Compare performance when using normalized vs. absolute coordinates to validate the regional transferability assumption.