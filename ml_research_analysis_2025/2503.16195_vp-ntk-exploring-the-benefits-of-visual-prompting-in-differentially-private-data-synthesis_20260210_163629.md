---
ver: rpa2
title: 'VP-NTK: Exploring the Benefits of Visual Prompting in Differentially Private
  Data Synthesis'
arxiv_id: '2503.16195'
source_url: https://arxiv.org/abs/2503.16195
tags:
- data
- private
- generative
- visual
- vp-ntk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the benefits of visual prompting (VP) in differentially
  private (DP) data synthesis, addressing the challenge of generating high-quality
  synthetic data under strict privacy constraints. The authors propose VP-NTK, a hybrid
  approach that integrates visual prompting with DP-NTK, a DP generative model leveraging
  neural tangent kernels.
---

# VP-NTK: Exploring the Benefits of Visual Prompting in Differentially Private Data Synthesis

## Quick Facts
- **arXiv ID**: 2503.16195
- **Source URL**: https://arxiv.org/abs/2503.16195
- **Reference count**: 22
- **Primary result**: VP-NTK achieves 0.707-0.769 accuracy on CelebA-Gender under (1,10⁻⁵)-DP with 64×64 resolution, outperforming state-of-the-art DP generative models.

## Executive Summary
This paper introduces VP-NTK, a novel approach that combines visual prompting with differentially private neural tangent kernel (DP-NTK) for synthetic data generation. The method leverages pre-trained generators and feature extractors, with trainable noise prompts added to features to align synthetic and private data distributions. By freezing most model parameters and only optimizing low-dimensional prompts, VP-NTK achieves strong utility under strict privacy constraints. Experiments on CelebA datasets demonstrate significant improvements over existing DP generative models, particularly for high-resolution images where other methods struggle due to high model capacity requirements.

## Method Summary
VP-NTK is a hybrid approach that integrates visual prompting with DP-NTK, a DP generative model leveraging neural tangent kernels. The method employs a pre-trained conditional generator (IC-GAN) and feature extractor (ResNet18 pre-trained on Tiny-ImageNet), with visual prompts (trainable noise) added to the features to better align synthetic and private data distributions. DP guarantees are maintained through a one-time Gaussian perturbation of the mean embedding rather than per-step gradient clipping. The model optimizes these prompts using a mixed loss combining empirical maximum mean discrepancy (MMD) with cosine similarity to improve feature distribution alignment while preserving privacy.

## Key Results
- VP-NTK achieves 0.707 and 0.769 classification accuracy for CelebA-Gender under (1, 10⁻⁵)-DP with 64×64 resolution, outperforming state-of-the-art DP generative models.
- The method effectively handles high-resolution images (128×128, 256×256), where other DP methods struggle due to high model capacity requirements.
- Ablation studies show mixed MMD + cosine similarity loss significantly outperforms MMD-only (79.24% vs 58.08% accuracy on CelebA-Gender), with optimal hyperparameters being κ=16, η=10⁻², and α=0.05.

## Why This Works (Mechanism)

### Mechanism 1: Frozen Pre-trained Feature Space with Trainable Prompts
VP-NTK reuses frozen pre-trained generators and feature extractors to reduce the parameter space requiring DP noise injection. Only the visual prompts—trainable per-class noise added to features—are optimized. This constrains the DP-sensitive optimization to a low-dimensional subspace rather than full model weights, preserving more signal under tight privacy budgets.

### Mechanism 2: Mean Embedding Matching with Gaussian Noise Perturbation
DP guarantees are achieved by perturbing the aggregated mean embedding rather than individual gradients. The true mean embedding μ̂ₚ = (1/m)Σϕ(xᵢ)yᵢᵀ is computed from private data, then perturbed as μ̃ₚ = μ̂ₚ + N(0, 4σ²/m²·I) via the Gaussian mechanism. Synthetic data is generated to minimize ||μ̃ₚ − μ̂Q||²_F, paying the privacy cost once rather than per training step.

### Mechanism 3: Mixed Loss with Cosine Similarity Regularization
VP-NTK combines MMD with cosine similarity to improve alignment between synthetic and private feature distributions. The mixed loss addresses potential divergence between empirical MMD (computed from finite samples) and true MMD by adding cosine similarity, which provides complementary signal for feature-direction alignment.

## Foundational Learning

- **Concept**: Differential Privacy (ε, δ)-guarantees
  - Why needed here: Understanding why VP-NTK perturbs mean embeddings rather than training weights directly; interpreting Table I's ε=1 vs. ε=10 results.
  - Quick check question: Why does lower ε require more noise, and how does VP-NTK avoid per-step noise accumulation?

- **Concept**: Neural Tangent Kernel (NTK) Feature Embeddings
  - Why needed here: The feature extractor ϕ(x) = ∇θf(x;θ)/||∇θf(x;θ)|| defines the embedding space where distribution matching occurs.
  - Quick check question: What property of NTK embeddings makes them suitable for kernel-based distribution comparison (MMD)?

- **Concept**: Visual Prompting / Input Perturbation
  - Why needed here: The core innovation—trainable noise added to features, not weights—requires understanding how input-space prompts differ from weight-space fine-tuning.
  - Quick check question: Why does freezing the generator and feature extractor preserve DP guarantees while training prompts?

## Architecture Onboarding

- **Component map**:
  Private Data → Frozen Feature Extractor (ResNet18) → Mean Embedding μ̂ₚ → Gaussian Noise Addition → Perturbed Embedding μ̃ₚ
                                                        ↓ (MMD + Cosine Loss)
  Frozen Generator (IC-GAN) ← Random z, Mapped Labels → Synthetic Features → + Per-Class Visual Prompts (trainable)

- **Critical path**:
  1. Load pre-trained IC-GAN generator and ResNet18 feature extractor (frozen)
  2. Define label mapping (random, fixed before training to preserve DP)
  3. Compute private mean embedding with Gaussian perturbation (one-time privacy cost)
  4. Initialize per-class visual prompts (κ-scaled noise)
  5. Optimize prompts via mixed MMD + cosine loss with regularization α

- **Design tradeoffs**:
  - κ (noise coefficient): Too low → insufficient adaptation; too high → overfitting. Optimal range: 4–16.
  - η (learning rate): 10⁻² optimal; lower → slow convergence; higher → instability.
  - α (regularization): Minimal impact; set to 0.05 as default.
  - Loss: Mixed MMD + cosine strongly outperforms MMD-only.

- **Failure signatures**:
  - Accuracy ~50% on binary classification: Check if label mapping is correctly applied.
  - High variance across runs: κ may be too high, causing prompt overfitting.
  - Degraded performance at higher resolutions: Verify feature extractor supports target resolution.

- **First 3 experiments**:
  1. Reproduce Table I (CelebA-Gender, 64×64, ε=1) with κ=16, η=10⁻², α=0.05, mixed loss to validate baseline.
  2. Ablate loss function (MMD-only vs. cosine-only vs. mixed) on CelebA-Hair to confirm Table VI's pattern holds for multiclass.
  3. Test resolution scaling (64×64 → 128×128 → 256×256) with fixed ε=10 to verify Table II's trend.

## Open Questions the Paper Calls Out
- **Open Question 1**: Does layer-wise visual prompt tuning offer advantages over input-only visual prompting in DP data synthesis?
- **Open Question 2**: Does VP-NTK generalize beyond face datasets to other high-resolution image domains?
- **Open Question 3**: Can a principled method be developed for selecting optimal hyperparameters (κ, η, α) without violating DP constraints?
- **Open Question 4**: How does VP-NTK compare against recent DP diffusion-based generative models for high-resolution synthesis?

## Limitations
- The paper relies on pre-trained IC-GAN and ResNet18 without providing training details, creating reproducibility challenges.
- Exact Gaussian mechanism noise calibration from (ε,δ) parameters is assumed but not explicitly shown.
- Random label mapping, while necessary for DP, may introduce unnecessary randomness if private labels are available.
- Evaluation relies solely on downstream classification accuracy without assessing sample diversity or fidelity metrics.

## Confidence
- **High confidence**: Visual prompting mechanism and its integration with frozen pre-trained models; DP-NTK mean embedding framework; mixed loss function benefits
- **Medium confidence**: High-resolution performance claims (256×256 results depend on IC-GAN quality); generalization beyond CelebA datasets
- **Low confidence**: Exact implementation details enabling the reported results (IC-GAN checkpoint, training hyperparameters, evaluation pipeline)

## Next Checks
1. Implement VP-NTK with different pre-trained backbones (e.g., ResNet50 instead of ResNet18) to test whether visual prompting gains are backbone-dependent.
2. Independently compute Rényi DP accounting for mean embedding perturbation, label mapping randomness, and prompt optimization to verify claimed (ε,δ) bounds.
3. Evaluate synthetic samples using Fréchet Inception Distance (FID) and precision-recall metrics alongside classification accuracy to assess sample quality and diversity comprehensively.