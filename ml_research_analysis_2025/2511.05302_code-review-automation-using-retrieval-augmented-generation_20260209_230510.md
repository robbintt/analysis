---
ver: rpa2
title: Code Review Automation using Retrieval Augmented Generation
arxiv_id: '2511.05302'
source_url: https://arxiv.org/abs/2511.05302
tags:
- code
- review
- generation
- retrieval
- reviews
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automating code reviews,
  which are essential for software quality but labor-intensive. The authors propose
  Retrieval-Augmented Reviewer (RARe), a novel method that combines retrieval-based
  and generative approaches using Retrieval-Augmented Generation (RAG) to improve
  review quality by incorporating external domain knowledge.
---

# Code Review Automation using Retrieval Augmented Generation

## Quick Facts
- **arXiv ID:** 2511.05302
- **Source URL:** https://arxiv.org/abs/2511.05302
- **Reference count:** 40
- **Key outcome:** RARe achieves BLEU-4 scores of 12.32 and 12.96 on two benchmark datasets, outperforming state-of-the-art approaches through retrieval-augmented generation

## Executive Summary
This paper addresses the challenge of automating code reviews, which are essential for software quality but labor-intensive. The authors propose Retrieval-Augmented Reviewer (RARe), a novel method that combines retrieval-based and generative approaches using Retrieval-Augmented Generation (RAG) to improve review quality by incorporating external domain knowledge. RARe uses a dense retriever to find relevant reviews from the codebase and enriches the input for a neural generator, leveraging large language models (LLMs) to produce contextually informed reviews. The method outperforms state-of-the-art approaches on two benchmark datasets, achieving BLEU-4 scores of 12.32 and 12.96, respectively. Human evaluation and interpretability analysis further validate RARe's effectiveness, demonstrating its practical utility and reliability in generating accurate and relevant code reviews.

## Method Summary
RARe combines dense retrieval with generative language models to automate code review generation. The system retrieves relevant code reviews from a training corpus using either Dense Passage Retrieval (DPR) or GPM Dense Retrieval (GDR), then augments the input prompt with the retrieved review before passing it to a neural generator (Llama 3.1 8B Instruct). For DPR, code and review are encoded separately using CodeBERT and GraphCodeBERT respectively. For GDR, CodeBERT embeddings are used with Cosine similarity, followed by GPM re-ranking. The retrieved review is incorporated into the prompt template to guide generation. The approach is evaluated on two benchmark datasets: CodeReviewer (multi-language, ~138k samples) and Tufano (Java only, ~168k samples).

## Key Results
- RARe achieves BLEU-4 scores of 12.32 on CodeReviewer and 12.96 on Tufano datasets
- Outperforms state-of-the-art methods on both datasets using the same evaluation metrics
- Human evaluation confirms generated reviews are accurate and relevant
- Interpretability analysis validates the effectiveness of retrieval-augmented generation

## Why This Works (Mechanism)
The method works by leveraging retrieval-augmented generation to combine the precision of retrieval-based approaches with the flexibility of generative models. By retrieving relevant examples from the codebase and incorporating them into the generation prompt, RARe provides contextual grounding that helps the generator produce more accurate and relevant code reviews. This addresses the limitation of pure generative approaches that may lack domain-specific knowledge, while avoiding the brittleness of pure retrieval methods that depend on exact matches.

## Foundational Learning
- **Dense Passage Retrieval (DPR):** Neural retrieval method using separate encoders for query and document. Needed to find semantically similar code reviews. Quick check: Verify DPR retrieves relevant reviews for diverse code snippets.
- **GPM Dense Retrieval (GDR):** Combines CodeBERT embeddings with Gestalt Pattern Matching for re-ranking. Needed for improved multi-language code retrieval. Quick check: Compare GDR vs DPR retrieval accuracy on mixed-language dataset.
- **Retrieval-Augmented Generation (RAG):** Framework combining retrieval with generative models. Needed to provide contextual grounding for generation. Quick check: Measure performance drop when removing retrieval augmentation.
- **LoRA Fine-tuning:** Parameter-efficient fine-tuning method. Needed to adapt large language models without full fine-tuning. Quick check: Verify LoRA convergence within specified epochs.
- **BLEU-4, ROUGE-L, METEOR metrics:** Standard evaluation metrics for text generation. Needed to measure review quality. Quick check: Ensure metric implementations match paper specifications.

## Architecture Onboarding

**Component Map:** Code Snippet -> Retriever (DPR/GDR) -> Retrieved Review -> Generator (Llama 3.1) -> Code Review

**Critical Path:** Retriever finds relevant review -> Review is incorporated into prompt -> Generator produces review comment

**Design Tradeoffs:** 
- Dense retrieval vs sparse retrieval: Dense offers semantic understanding but may miss exact matches
- Single vs multiple retrieved reviews: Single simplifies prompt engineering but may limit diversity
- Large vs small language model: Larger models offer better quality but higher computational cost

**Failure Signatures:**
- Low BLEU scores due to verbose or off-topic generation
- Retrieval mismatch when retrieved review is for different language or context
- Model generates summaries instead of concise review comments

**Three First Experiments:**
1. Build FAISS index with CodeBERT embeddings and validate Top-1 retrieval accuracy on validation set
2. Compare RARe performance with and without retrieval augmentation using baseline generator
3. Test cross-language retrieval effectiveness on CodeReviewer dataset

## Open Questions the Paper Calls Out
- **Efficiency optimization:** How can RARe be optimized to handle increased input length and latency compared to retrieval-only methods? The paper admits RARe lacks an "efficiency advantage compared to the retrieval-only method," making practical deployment difficult.
- **Real-world deployment:** How does RARe perform when deployed in real-world developer workflows regarding acceptance rates and productivity? Current results rely on offline datasets and automated metrics, lacking validation of practical utility in live engineering environments.
- **Sparse retrieval strategies:** What is the impact of using sparse or hybrid retrieval strategies on the accuracy of RARe? Dense retrievers might miss exact lexical matches that sparse methods capture, potentially limiting the relevance of retrieved examples for the generator.

## Limitations
- Exact LoRA fine-tuning configuration underspecified, particularly rank parameter and target layers
- DPR training configuration lacks explicit hyperparameter details beyond references to prior work
- Prompt engineering strategy omits critical system-level instructions for Llama 3.1 instruction following

## Confidence
- **High Confidence:** Core methodology and benchmark results are clearly articulated and reproducible
- **Medium Confidence:** Reported BLEU-4 scores appear competitive though direct comparison is limited
- **Medium Confidence:** Human evaluation and interpretability analyses support findings but sample sizes are sparse

## Next Checks
1. Implement retrieval pipeline with CodeBERT embeddings and validate Top-1 retrieval accuracy on held-out validation set
2. Conduct ablation studies testing whether retrieved reviews genuinely improve generation quality
3. Test model performance across programming languages in CodeReviewer dataset to verify multi-language effectiveness