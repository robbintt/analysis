---
ver: rpa2
title: Propositional Logic for Probing Generalization in Neural Networks
arxiv_id: '2506.08978'
source_url: https://arxiv.org/abs/2506.08978
tags:
- generalization
- pattern
- patterns
- neural
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the systematic generalization abilities
  of neural networks on propositional logic tasks. The authors create a balanced dataset
  to avoid learning shortcuts and systematically test how well different architectures
  (Transformers, GCNs, LSTMs) can handle unseen combinations of logical operators,
  particularly negation.
---

# Propositional Logic for Probing Generalization in Neural Networks

## Quick Facts
- arXiv ID: 2506.08978
- Source URL: https://arxiv.org/abs/2506.08978
- Reference count: 40
- Primary result: Transformers fail to generalize negation compositionally without structural biases

## Executive Summary
This paper investigates systematic generalization in neural networks using propositional logic tasks. The authors create a balanced dataset to prevent learning spurious correlations and systematically test how different architectures handle unseen combinations of logical operators. While all models achieve high in-distribution accuracy, they struggle significantly with compositional generalization when negation is involved. Transformers, in particular, fail to apply negation to novel operator combinations unless explicit structural biases are introduced through tree positional encodings or recurrence.

## Method Summary
The authors construct Prop35Balanced, a propositional logic dataset with 800K training examples and 10K test examples, balanced across tree structures to prevent directional shortcuts. They systematically test generalization by removing specific parent-child operator patterns (like NOT before OR) from training while preserving output distributions. Three architectures are evaluated: standard Transformers with absolute or tree positional encodings, 16-layer Graph Convolutional Networks, and 6-layer LSTMs. All models are trained using standard seq2seq approaches with cross-attention decoders, and evaluated on semantic accuracy (generating any satisfying assignment).

## Key Results
- All architectures achieve >87% in-distribution accuracy but fail on held-out negation patterns
- Transformers show 20-25% accuracy on unseen negation patterns unless structural biases are added
- Tree positional encodings and recurrence improve generalization for most patterns but fail on NOT applied to OR
- GCNs require 16 layers to match 6-layer LSTM performance on this task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Structural biases (tree positional encodings, recurrence, or graph structure) improve compositional generalization on unseen negation patterns in some but not all cases.
- **Mechanism:** Tree-based encodings and recurrence force the model to process input according to its hierarchical structure rather than learning position-specific heuristics. This creates pressure to learn operator-level representations that transfer to novel combinations.
- **Core assumption:** Systematic operator representations exist and can be learned when the architecture constrains the hypothesis space appropriately.
- **Evidence anchors:**
  - [abstract] "Transformers fail to apply negation compositionally, unless structural biases are introduced."
  - [Section 6] "Tree positional encodings help systematic generalization for (P2) and (P3), but performance on these patterns still lags significantly behind compared to the rest of the patterns."
  - [Section 6] "The LSTM-based models also show improvements over a standard Transformer on these unseen patterns, even though they do not have access to the underlying graph structure."
  - [corpus] Weak direct support; related work on GNN expressiveness (arXiv:2505.08021) discusses logic-GNN correspondence but not negation specifically.
- **Break condition:** Negation applied to OR-operator (Pattern P1: ¬(A ∨ B)) fails across all architectures—even with structural biases—suggesting some operator combinations resist current inductive bias approaches.

### Mechanism 2
- **Claim:** Balancing the dataset across tree structures prevents models from learning directional shortcuts that masquerade as reasoning.
- **Mechanism:** The original Prop35 dataset had systematically larger right subtrees (avg 5.5 tokens) vs. left subtrees (2.7 tokens). Models trained on this imbalanced data showed 25%+ accuracy drops when tested on inverted trees. Random subtree rotation during data preparation eliminates this spurious correlation.
- **Core assumption:** Models will exploit any statistical regularity in training data to minimize loss, even when that regularity is task-irrelevant.
- **Evidence anchors:**
  - [Section 3.2] "We find that training on imbalanced data results in a ≥25% drop in accuracy when models are tested on inverted trees."
  - [Appendix A.1, Table 6] Models trained on Prop35Balanced maintain 94.5% semantic accuracy on both original and inverted test sets.
  - [corpus] No direct corpus support for this specific balancing mechanism.
- **Break condition:** If the target domain has genuine structural asymmetries (e.g., natural language where right-branching is legitimate), aggressive balancing may harm in-distribution performance.

### Mechanism 3
- **Claim:** Pattern-based held-out training enables fine-grained diagnosis of which operator compositions models have learned versus memorized.
- **Mechanism:** By rewriting formulas using De Morgan's laws or removing sentences, specific parent-child operator combinations (e.g., NOT before AND) can be eliminated from training while preserving output distribution. Models that truly learned operator semantics should generalize; those relying on n-gram patterns will fail.
- **Core assumption:** Propositional logic operators have compositional semantics that, if learned, should transfer to any syntactically valid combination.
- **Evidence anchors:**
  - [Section 5] "Since all possible combinations of tokens are present in Prop35, we design seven alternative training sets. For each new training set, exactly one pattern is left out."
  - [Section 6] "Models that were trained without ! &, ! |, or ! xor all failed to apply negation to the binary operator when the pattern is reintroduced at test time."
  - [corpus] SCAN and COGS benchmarks (cited in Related Work) use similar held-out pattern methodology for compositional generalization testing.
- **Break condition:** When patterns are functionally redundant (e.g., NOT applied to a variable already seen with other variables), models may generalize without true compositionality—Pattern P4 shows this.

## Foundational Learning

- **Concept: Polish (prefix) notation**
  - **Why needed here:** All input formulas use prefix notation (operators precede operands) to eliminate parentheses ambiguity. Understanding this is necessary to interpret held-out patterns like `! & A` versus `& ! A`.
  - **Quick check question:** Given the formula `& ! a | b c`, what is the operator precedence and tree structure?

- **Concept: Satisfying assignments (partial vs. complete)**
  - **Why needed here:** The task requires generating any satisfying assignment, not matching a specific ground truth. A partial assignment that satisfies the formula (e.g., `a=1` for `a ∨ b`) is semantically correct even if incomplete.
  - **Quick check question:** For `a ∧ b`, is `{a=1}` a satisfying assignment? Why or why not?

- **Concept: Systematic vs. productive generalization**
  - **Why needed here:** The paper distinguishes length generalization (Prop50—productive) from novel operator combination generalization (held-out patterns—systematic). Conflating these leads to misinterpreting results.
  - **Quick check question:** If a model trained on formulas with ≤5 variables works on formulas with 6 variables, is that systematic or productive generalization?

## Architecture Onboarding

- **Component map:** Input formula (Polish notation) -> Encoder (Transformer/GCN/LSTM) -> Hidden representations -> Cross-attention -> Decoder (Transformer) -> Output tokens (variable assignments)

- **Critical path:**
  1. Data preparation: Apply subtree balancing rotation (50% random flip)
  2. Create held-out training splits by rewriting/removing target patterns
  3. Train for 128 epochs with Noam scheduler, 4000 warmup steps
  4. Evaluate semantic accuracy (any satisfying assignment) not just syntactic match

- **Design tradeoffs:**
  - Tree positional encodings vs. absolute: Tree helps systematic generalization but requires parsing input into tree structure first
  - GCN vs. Transformer: GCN explicitly encodes structure but requires 16 layers vs. 6 for comparable performance
  - Dataset size vs. pattern control: Full dataset (800K) includes all patterns; controlled splits reduce to ~500K-750K depending on pattern frequency

- **Failure signatures:**
  - Negation ignored: Model outputs same assignment for `¬ϕ` as for `ϕ` (classified as "B" behavior in Section 6.1)
  - Variable-specific negation: Model generalizes NOT to unseen variables (P4 works) but not unseen operators (P1-P3 fail)
  - Syntactic overfitting: High syntactic accuracy (exact match) but lower semantic accuracy suggests memorizing solver outputs rather than learning logic

- **First 3 experiments:**
  1. **Reproduction baseline:** Train Transformer with absolute positional encodings on Prop35Balanced, evaluate on standard test set. Target: ≥90% semantic accuracy.
  2. **Negation generalization test:** Train on Pattern P1-excluded split (`! |` removed), test on templated sentences containing `¬(A ∨ B)`. Expect near-zero accuracy for vanilla Transformer.
  3. **Architecture ablation:** Compare Transformer-tree vs. GCN vs. LSTM on Pattern P3 (`! xor`). GCN/LSTM should show 60-80% accuracy vs. ~25% for absolute Transformer.

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond those addressed in the study.

## Limitations
- Limited architectural scope: Only three architectures tested (Transformers, GCNs, LSTMs) without exploring more recent architectures
- Domain specificity: Results may not generalize to other logical systems or larger-scale reasoning tasks
- Sample size constraints: Experiments focus on a narrow propositional logic domain without broader validation

## Confidence
- **High confidence:** Dataset balancing methodology effectively eliminates directional shortcuts (supported by clear quantitative evidence across multiple models)
- **Medium confidence:** Structural biases improve compositional generalization for most negation patterns, though failure on Pattern P1 suggests fundamental limitations rather than implementation issues
- **Low confidence:** Claims about systematic operator representation learning—evidence shows improved performance with structural biases but does not prove the models have learned abstract operator semantics versus sophisticated pattern matching

## Next Checks
1. **Cross-pattern generalization test:** Train models on Pattern P4 (negation applied to variables) and test on Patterns P1-P3 (negation applied to operators). If performance transfers, this suggests learning abstract negation rather than variable-specific patterns.
2. **Ablation study on tree parsing:** Compare tree positional encodings where the parser is perfect versus noisy. This isolates whether improvements come from better input representation or the structural bias itself.
3. **Scaling experiment:** Train on the full 800K dataset and test held-out patterns. Current results use reduced datasets; observing whether structural advantages persist at scale will test robustness of the findings.