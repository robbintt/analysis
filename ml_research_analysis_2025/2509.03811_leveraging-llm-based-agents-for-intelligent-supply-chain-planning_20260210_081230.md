---
ver: rpa2
title: Leveraging LLM-Based Agents for Intelligent Supply Chain Planning
arxiv_id: '2509.03811'
source_url: https://arxiv.org/abs/2509.03811
tags:
- planning
- supply
- chain
- task
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Generative AI-powered agentic framework for
  intelligent supply chain planning, deployed at JD.com to address inefficiencies
  in data processing, coordination, and decision-making across sales, inventory, and
  operations. The system automates task decomposition, data retrieval, and analysis,
  enabling natural language-driven plan generation and real-time adaptation.
---

# Leveraging LLM-Based Agents for Intelligent Supply Chain Planning

## Quick Facts
- **arXiv ID**: 2509.03811
- **Source URL**: https://arxiv.org/abs/2509.03811
- **Reference count**: 24
- **Primary result**: Framework reduces planning cycle time from 120 minutes to 5 minutes, improves planning accuracy by 22%, and increases stock fulfillment rates by 2-3%.

## Executive Summary
This paper presents a Generative AI-powered agentic framework for intelligent supply chain planning, deployed at JD.com to address inefficiencies in data processing, coordination, and decision-making across sales, inventory, and operations. The system automates task decomposition, data retrieval, and analysis, enabling natural language-driven plan generation and real-time adaptation. In deployment, it reduced planners' weekly manual workload from 120 minutes to 5 minutes, improved planning accuracy by 22%, and increased stock fulfillment rates by 2-3% during peak events, translating to a GMV uplift of approximately RMB 2 million. The framework demonstrates scalable, resilient, and adaptive supply chain planning through human-machine collaboration.

## Method Summary
The framework implements a multi-agent architecture driven by a fine-tuned LLM. It uses RAG with hybrid search (Dense + Sparse vectors) on domain SOPs, two-level agents (Intent Classification → Task Orchestration → Task Execution), and atomic code generation (Filter, Transform, Groupby, Sort) for data analysis. The system supports natural language queries, executes iterative execute-evaluate-refine loops, and includes real-time plan correction monitoring.

## Key Results
- Reduced planners' weekly manual workload from 120 minutes to 5 minutes
- Improved planning accuracy by 22% (plans with <5% deviation)
- Increased stock fulfillment rates by 2-3% during peak events

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework reduces planning cycle time by decomposing high-level natural language requests into executable sub-tasks through a hierarchical agent structure.
- Mechanism: A Task Orchestration Agent leverages LLM reasoning to parse strategic intent, retrieves relevant Standard Operating Procedures via RAG, and generates a structured task list. Sub-agents execute each sub-task sequentially, with results fed back to refine remaining tasks.
- Core assumption: LLMs can reliably map ambiguous business requests to domain-specific workflows when augmented with retrieval-augmented SOPs.
- Evidence anchors:
  - [abstract] "automates task decomposition, data retrieval, and analysis, enabling natural language-driven plan generation"
  - [section 3.4.2] "task orchestration... breaks the request down into multiple system-executable subtasks"
  - [corpus] Related work "Large Language Models for Supply Chain Decisions" (arXiv:2507.21502) similarly positions LLMs as semantic interpreters for complex supply chain decisions.

### Mechanism 2
- Claim: Structured data retrieval via text-to-SQL and atomic code generation enables scalable, interpretable analysis across heterogeneous supply chain data sources.
- Mechanism: The Data Acquisition Agent converts natural language parameters into SQL queries using slot extraction and embedding-based matching. The Data Analysis Agent generates Python code composed of four atomic primitives (Filter, Transform, Groupby, Sort), each produced in a separate reasoning cycle to reduce hallucination risk and enhance traceability.
- Core assumption: Decomposing code generation into atomic operations improves reliability compared to monolithic generation, and text-to-SQL achieves sufficient accuracy for enterprise databases.
- Evidence anchors:
  - [section 3.5.1] "transforms intent-recognized natural language input into structured and executable parameters" via slot extraction
  - [section 3.5.2] "iterative inference strategy: each reasoning cycle produces the code for a single atomic step"

### Mechanism 3
- Claim: Continuous feedback loops between task execution and plan refinement enable adaptive response to real-time supply chain volatility.
- Mechanism: After each sub-task completes, the Task Update mechanism integrates execution results as observations, prompting the Task Orchestration Agent to prune completed tasks, resequence remaining items, or inject new sub-tasks. The Plan Correction Agent monitors execution metrics against planned targets and generates remedial recommendations through counterfactual reasoning.
- Core assumption: Real-time feedback can be processed fast enough to influence decisions within planning cycles, and the agent can distinguish signal from noise in volatile demand environments.
- Evidence anchors:
  - [section 3.6] "execute–evaluate–refine loop ensures that the overarching plan is not static, but continuously reconstructs itself"
  - [section 3.7] "detects abnormal deviations... before generating recommendations for plan updates"

## Foundational Learning

- Concept: Supply Chain Planning Domains
  - Why needed here: The framework operationalizes four interconnected planning domains (sales, inventory, inbound, operations), each with distinct KPIs and workflows. Understanding their interdependencies is essential to interpret how the agent orchestrates cross-functional decisions.
  - Quick check question: Can you explain why a sales plan forecast directly constrains inventory allocation decisions?

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: The framework anchors LLM reasoning in domain-specific SOPs and knowledge bases via RAG. This mechanism is critical for translating general LLM capabilities into supply-chain-accurate outputs.
  - Quick check question: How does retrieving a Standard Operating Procedure document before task decomposition improve the relevance of the generated sub-task list?

- Concept: Agent Memory Architectures
  - Why needed here: The system uses both short-term memory (conversational context) and long-term memory (historical decisions, user preferences) to maintain coherence across multi-step planning cycles.
  - Quick check question: What type of information would be stored in short-term versus long-term memory during a weekly sales planning session?

## Architecture Onboarding

- Component map:
  Foundation Model -> Query Enhancement Module -> Intent Classification Agent -> Task Orchestration Agent -> Data Acquisition Agent -> Data Analysis Agent -> Task Update -> Plan Correction Agent

- Critical path: User natural language input → Query Enhancement → Intent Classification → Task Orchestration (with SOP retrieval) → Data Acquisition (SQL generation) → Data Analysis (code execution) → Task Update → Final Plan Output. The iterative loop repeats until all sub-tasks complete.

- Design tradeoffs:
  - Atomic code primitives (Filter, Transform, Groupby, Sort) improve reliability and traceability but may limit expressiveness for complex analytical operations
  - RAG-based SOP retrieval ensures domain alignment but introduces dependency on knowledge base quality and retrieval accuracy
  - Iterative task refinement enables adaptability but adds latency compared to single-pass execution

- Failure signatures:
  - Orchestration generates irrelevant or unexecutable sub-tasks when SOP coverage is incomplete
  - Text-to-SQL produces incorrect queries due to schema ambiguity or embedding mismatches
  - Code generation yields non-executable scripts or logical errors when task complexity exceeds atomic primitive composability
  - Iterative loop stalls when observations contain insufficient signal to refine task list

- First 3 experiments:
  1. **SOP Coverage Audit**: Map a sample of 20 historical planning requests to available SOPs; measure retrieval relevance and identify gaps where RAG fails to return applicable procedures.
  2. **Text-to-SQL Accuracy Test**: Manually construct 30 natural language queries with known correct SQL outputs; compare agent-generated SQL against ground truth on precision/recall of retrieved data.
  3. **Atomic Code Execution Validation**: Execute agent-generated analysis code on a held-out dataset; measure execution success rate and correctness of analytical outputs compared to reference implementations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed framework maintain robustness and accuracy when extended to cross-enterprise orchestration involving external stakeholders?
- Basis in paper: [explicit] The conclusion states the authors "envision these principles extending beyond the planning domain" to areas like sourcing, suggesting a need for "autonomously orchestrated" chains.
- Why unresolved: The current study validates the framework within a single enterprise (JD.com). It does not test performance across organizational boundaries where data sovereignty, heterogeneity, and latency differ significantly.
- What evidence would resolve it: Empirical results from a multi-agent deployment coordinating planning between distinct legal entities or across a supply network with varying data standards.

### Open Question 2
- Question: How does the framework handle complex "what-if" scenarios that require counterfactual reasoning beyond the available historical data or pre-defined tools?
- Basis in paper: [inferred] While the framework supports plan correction via simulation, the Data Analysis Agent relies on a grammar of four atomic primitive operations (Filter, Transform, Groupby, Sort) and existing algorithmic tools.
- Why unresolved: It is unclear if this restricted grammar allows the agent to synthesize novel decision logic for unprecedented market disruptions that do not map to existing database schemas or pre-validated solvers.
- What evidence would resolve it: Performance metrics on out-of-distribution tasks requiring the agent to generate novel code or logic for scenarios not represented in the training data.

### Open Question 3
- Question: To what extent are the reported efficiency gains derived from the Generative AI's reasoning versus the mere automation of data retrieval?
- Basis in paper: [inferred] The evaluation compares a 2023 manual baseline (requiring 20 mins acquisition, 40 mins processing) against the 2024 agent system. The control group relied on "manual extraction" and "repetitive tasks."
- Why unresolved: It is difficult to isolate the specific value of the LLM's *semantic reasoning* from the generic benefits of automating the SQL queries and data processing that previously existed as manual bottlenecks.
- What evidence would resolve it: A controlled A/B test comparing the LLM agent against a non-generative, rule-based automation script to isolate the marginal gain from semantic understanding.

## Limitations
- Evaluation relies heavily on internal JD.com deployment metrics without independent validation
- Framework's effectiveness is contingent on high-quality SOP documentation, creating potential bottlenecks
- Restriction to four atomic operations for data analysis may prove insufficient for complex analytical workflows

## Confidence

- **High Confidence**: The task decomposition mechanism and agent architecture design are well-specified with clear operational procedures. The atomic code generation approach is theoretically sound and directly implementable.

- **Medium Confidence**: The claimed performance improvements are based on internal deployment data. While the methodology is plausible, independent validation would strengthen these claims. The RAG integration appears robust but depends on knowledge base quality that is not externally verifiable.

- **Low Confidence**: The scalability claims to enterprise-wide deployment are based on a single case study without comparative analysis against alternative approaches. The real-time adaptation mechanism's effectiveness in highly volatile demand scenarios lacks rigorous testing.

## Next Checks

1. **External Validation Study**: Deploy the framework in a different enterprise context (different industry or company size) to assess generalizability of the reported performance improvements. Measure task completion time, planning accuracy, and fulfillment rates against baseline manual processes.

2. **Knowledge Base Coverage Analysis**: Conduct systematic evaluation of SOP coverage across diverse planning scenarios. Measure the percentage of novel requests that cannot be adequately addressed due to missing or inadequate documentation, and assess the framework's fallback behavior.

3. **Latency and Scalability Testing**: Implement comprehensive performance benchmarking under varying load conditions. Measure end-to-end planning cycle time, including RAG retrieval, task decomposition, and iterative refinement steps. Identify latency bottlenecks and test the framework's behavior when context windows are exceeded.