---
ver: rpa2
title: 'Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating
  Math Problems with LLMs'
arxiv_id: '2509.17701'
source_url: https://arxiv.org/abs/2509.17701
tags:
- language
- llms
- were
- english
- arabic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces an automated multilingual pipeline for evaluating
  LLM-generated math solutions across English, German, and Arabic. Using curriculum-aligned
  problems and LLM judges, it found that English solutions were consistently rated
  highest, while Arabic was often ranked lowest.
---

# Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs

## Quick Facts
- **arXiv ID**: 2509.17701
- **Source URL**: https://arxiv.org/abs/2509.17701
- **Reference count**: 24
- **Primary result**: Automated pipeline reveals consistent linguistic bias, with English solutions ranked highest and Arabic lowest across GPT-4o-mini, Gemini, and Qwen models

## Executive Summary
This study introduces an automated multilingual pipeline to evaluate LLM-generated math solutions across English, German, and Arabic using LLM judges. The pipeline generates curriculum-aligned problems, translates them, produces solutions from three different LLMs, and employs a comparative evaluation framework where multiple LLM judges rank solution quality. Results show consistent language-based performance gaps, with English solutions rated highest, German in the middle, and Arabic often ranked lowest. The findings highlight persistent linguistic bias in educational AI outputs and underscore the need for more equitable, multilingual model development.

## Method Summary
The pipeline uses curriculum-aligned math problems translated into English, German, and Arabic. Three LLM models (GPT-4o-mini, Gemini, Qwen) generate solutions for each problem in each language. A panel of LLM judges, including Claude 3.5 Haiku, evaluates solutions using comparative ranking with structured rubrics for clarity, accuracy, and pedagogical effectiveness. A held-out judging strategy prevents self-evaluation bias. Majority voting aggregates rankings, and technical term extraction ensures pedagogical relevance. Manual audit validates translations.

## Key Results
- English solutions consistently received the highest rankings across all judge evaluations
- Arabic solutions were frequently ranked lowest, with German solutions typically falling in between
- Performance disparities align with training data representation and linguistic structural complexity
- The comparative LLM-as-judge framework produced rankings consistent with human preferences

## Why This Works (Mechanism)

### Mechanism 1
LLM-as-judge comparative evaluation produces rankings consistent with human preferences for educational content quality assessment. Multiple LLM judges independently rank solutions using structured rubrics, with majority voting aggregating preferences to reduce individual variance. Assumes LLM judges correlate sufficiently with human evaluators in educational contexts. Evidence from related work supports comparative assessment alignment with human preferences, though direct validation in this multilingual context is lacking.

### Mechanism 2
Performance disparities across languages correlate with training data representation and linguistic structural complexity. English-centric training data produces more fluent outputs in English, while German's compound words and Arabic's rich morphology and RTL script introduce processing challenges. Assumes observed quality differences stem from model capability gaps rather than evaluation artifacts. Evidence includes consistent English > German > Arabic ranking and studies showing performance drops in low-resource languages.

### Mechanism 3
Held-out judging strategy mitigates self-evaluation bias where models might favor their own outputs. When evaluating solutions from Model X, Model X is excluded from the judge panel and replaced with a neutral model. Assumes models show detectable preference for their own outputs. Evidence includes the explicit implementation of this strategy, though direct validation of its effectiveness in this context is limited.

## Foundational Learning

- **Position Bias in Comparative Evaluation**
  - Why needed: LLM judges can be influenced by presentation order, with earlier options potentially receiving higher rankings
  - Quick check: When presenting three options repeatedly in fixed order, do earlier positions receive higher average rankings?

- **Majority Voting for Aggregation**
  - Why needed: Individual LLM judges may disagree or show idiosyncratic preferences, requiring aggregation for reliability
  - Quick check: Does requiring 2+ judges to agree before declaring a winner reduce noise compared to averaging scores?

- **Training Data Representation and Language Performance**
  - Why needed: LLMs are trained on corpora with unequal language representation, affecting output quality in lower-resource languages
  - Quick check: Have you checked the training corpus language distribution for your target languages before deploying multilingual systems?

## Architecture Onboarding

- **Component map**: Exercise Generator (GPT-4o, manual) → Translator (GPT-4o-mini API) → Solution Generator (3 LLMs) → Technical Term Extractor (GPT-4o-mini) → Judge Panel (4 LLMs with rotation) → Aggregator (majority voting)
- **Critical path**: Exercise generation → Translation + audit → Solution generation (parallel across 3 models × 3 languages) → Judge evaluation (with held-out rotation) → Majority aggregation → Analysis
- **Design tradeoffs**: Automation vs. human evaluation (chose automation for scalability); comparative ranking vs. direct scoring (chose comparative for alignment with human preferences); single judge vs. panel (chose panel for robustness); manual translation audit (adds quality assurance but limits scale)
- **Failure signatures**: High "TIE" rates suggest judges cannot distinguish quality; consistent rankings may indicate shared bias; extreme model-specific patterns suggest incomplete debiasing; justifications citing surface features over content indicate poor pedagogical evaluation
- **First 3 experiments**: 1) Baseline replication on 50-exercise subset to verify English > German > Arabic pattern; 2) Position bias probe with fixed order to quantify correlation with ranking; 3) Judge calibration check comparing LLM rankings against 20 human-annotated exercises to calculate agreement rate

## Open Questions the Paper Calls Out

### Open Question 1
Can targeted fine-tuning or culturally informed prompt design effectively mitigate performance disparities in underrepresented languages like Arabic? The study used standard prompts and commercial models without optimization for specific linguistic contexts, leaving the effectiveness of targeted approaches unresolved.

### Open Question 2
Do linguistic biases in math problem solving persist across other academic domains or different language families? The study was restricted to K-10 mathematics, and it's unknown whether English favoritism amplifies or reduces in subjects requiring different reasoning styles.

### Open Question 3
Does LLM-as-judge evaluation systematically penalize morphologically complex languages like Arabic compared to human evaluation standards? The study relies entirely on LLM judges without validating rankings against human teacher assessments for specific pedagogical criteria.

## Limitations

- Primary methodological limitation is reliance on LLM judges without direct validation against human evaluators in this specific multilingual educational context
- Training data representation bias introduces confounding, making it difficult to attribute quality differences to model capability gaps versus evaluation artifacts
- Held-out judging strategy assumes self-evaluation bias exists but the magnitude in educational contexts remains empirically uncertain

## Confidence

**High Confidence**: Pipeline architecture is technically sound and reproducible; comparative evaluation methodology follows established best practices

**Medium Confidence**: Observed language performance disparities reflect real differences in output quality; ranking pattern is consistent and statistically robust

**Low Confidence**: Attribution of quality differences to training data representation versus evaluation artifacts; degree of LLM-human judge correlation; effectiveness of held-out strategy in eliminating self-evaluation bias

## Next Checks

1. **Human-LLM Agreement Validation**: Compare LLM judge rankings with human expert evaluations on 50 held-out exercises; calculate inter-rater reliability to validate LLM judgments as pedagogical quality proxies

2. **Training Data Analysis**: Analyze underlying LLM training corpus language distribution, focusing on formal educational content representation in English, German, and Arabic; correlate training data proportions with observed performance gaps

3. **Cross-Model Consistency Probe**: Evaluate whether language performance pattern persists using different LLM architectures (e.g., LLaMA, Mistral) as generators; consistent patterns strengthen training data hypothesis, while divergent results suggest model-specific artifacts