---
ver: rpa2
title: Benchmarking the Influence of Pre-training on Explanation Performance in MR
  Image Classification
arxiv_id: '2306.12150'
source_url: https://arxiv.org/abs/2306.12150
tags:
- methods
- data
- explanation
- performance
- lesions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new benchmark for evaluating the performance
  of explainable AI (XAI) methods in the context of MR image classification. The key
  innovation is the creation of a synthetic dataset where artificial lesions are overlaid
  on real MR images, with the ground truth for explanations being the exact location
  of these lesions.
---

# Benchmarking the Influence of Pre-training on Explanation Performance in MR Image Classification

## Quick Facts
- arXiv ID: 2306.12150
- Source URL: https://arxiv.org/abs/2306.12150
- Reference count: 18
- Key outcome: Models pre-trained on MRI data produce better and more stable explanations across a range of classification accuracies compared to models pre-trained on natural images, with high variability in explanation performance across different XAI methods.

## Executive Summary
This paper introduces a new benchmark for evaluating explainable AI (XAI) methods in MR image classification using a synthetic dataset with artificial lesions and known ground truth. The study investigates how transfer learning affects explanation performance by comparing models pre-trained on ImageNet versus MRI data, then fine-tuned to varying degrees. The main finding is that within-domain pre-training (MRI) leads to better explanation performance than out-of-domain pre-training (ImageNet), especially with limited fine-tuning, while also revealing significant variability in XAI method quality.

## Method Summary
The study uses a synthetic dataset of MR brain images with artificial lesions, where the ground truth for explanations is the exact lesion location. VGG-16 models are pre-trained either on ImageNet or on MRI data (within-domain sex classification), then fine-tuned on the lesion classification task at five different degrees of fine-tuning (1conv through all convolutional blocks). Seven XAI methods are applied to correctly classified test samples, and explanation performance is measured as precision: the overlap between top-n important pixels and ground-truth lesion masks.

## Key Results
- Models pre-trained on MRI data show better explanation performance than those pre-trained on ImageNet, particularly with limited fine-tuning.
- Explanation performance generally improves with higher fine-tuning degrees but may plateau or slightly reverse at high fine-tuning levels.
- XAI methods applied to the same model differ vastly in explanation correctness, even for correctly classified examples.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Within-domain pre-training produces better explanation performance than out-of-domain pre-training, particularly when fine-tuning is limited.
- Mechanism: Models pre-trained on ImageNet encode domain-irrelevant representations (e.g., cats, dogs) in early layers. When fine-tuning is shallow (1conv, 2conv), these representations remain frozen and influence backward-propagated attribution maps, contaminating explanations with non-diagnostic features. Within-domain MRI pre-training provides representations that align more closely with the target task's feature space, reducing this contamination.
- Core assumption: The negative impact of domain-mismatched pre-training on explanation quality operates independently of classification performance (the paper attempts to control for this confound).
- Evidence anchors:
  - [abstract] "models pre-trained on ImageNet show worse explanation performance compared to within-domain pre-training, especially for lower degrees of fine-tuning"
  - [section 5.2, p.7-8] "ImageNet pre-training leads to worse explanations across all XAI methods for lower degrees of fine-tuning (1conv and 2conv), where large parts of the models are prohibited to depart from the internal representations learned on the ImageNet data"
  - [corpus] Neighbor papers on transfer learning in deep vision (arXiv:2205.09904, cited 36Ã—) confirm transfer efficacy depends on domain similarity, but do not directly address explanation performance.
- Break condition: If classification accuracy and explanation performance are not separable (i.e., the observed effect is purely a byproduct of accuracy differences), the mechanism weakens. The paper's supplementary analysis controlling for accuracy suggests the effect persists, but this remains conditional.

### Mechanism 2
- Claim: Increasing fine-tuning depth improves explanation performance up to a point, after which returns diminish or reverse.
- Mechanism: Fine-tuning progressively unfreezes convolutional blocks, allowing the model to overwrite pre-trained representations with task-specific features. Early layers shift toward domain-relevant edge/texture patterns; deeper layers adapt to higher-level lesion discrimination. However, excessive fine-tuning may lead to overfitting or representational drift that degrades attribution alignment with ground-truth lesion locations.
- Core assumption: The plateau/reversal is not purely an artifact of the specific XAI methods evaluated, but reflects a genuine representational tradeoff.
- Evidence anchors:
  - [abstract] "explanation performance improves with higher fine-tuning degrees but may plateau or slightly reverse at high fine-tuning levels"
  - [section 5.2, p.8] "explanation performance generally increases with higher degrees of fine-tuning. However, depending on the XAI method used, and the corpus used for pre-training, this trend plateaus or even slightly reverses at a high degree of fine-tuning (4conv)"
  - [corpus] No direct corpus evidence on fine-tuning depth vs. explanation quality; neighbor papers focus on classification transfer, not XAI.
- Break condition: If the plateau is driven by evaluation noise or specific to VGG-16 architecture, generalization is limited. Broader architecture testing required.

### Mechanism 3
- Claim: XAI methods applied to the same underlying model differ substantially in explanation correctness, even for correctly classified examples.
- Mechanism: Each XAI method operationalizes "importance" differently (gradient-based, propagation-based, perturbation-based). The paper measures correctness via alignment between top-n important pixels and ground-truth lesion masks (precision). Methods that produce diffuse or grid-like artifacts (e.g., Deconvolution on ImageNet-pretrained models) may highlight features that improve prediction but do not correspond to class-discriminative regions.
- Core assumption: Ground-truth explanation correctness is well-defined as overlap with synthetic lesion regions; this excludes potentially valid model reasoning that relies on contextual features.
- Evidence anchors:
  - [abstract] "popular XAI methods applied to the same underlying model differ vastly in performance, even when considering only correctly classified examples"
  - [section 5.1, p.6-7] Qualitative analysis shows "particularly noisy explanations" with Deconvolution for ImageNet-pretrained models, including grid-like patterns unrelated to lesions
  - [corpus] arXiv:2512.05937 discusses saliency methods and ground-truth evaluation in AV perception, supporting the broader pattern that saliency alignment with known important regions is a meaningful but imperfect proxy.
- Break condition: If models learn valid non-lesion features (suppressor variables) that genuinely aid prediction, the ground-truth definition penalizes correct model behavior. The paper acknowledges this but does not resolve it.

## Foundational Learning

- Concept: Transfer learning in CNNs
  - Why needed here: The paper's central question is how pre-training source (ImageNet vs. within-domain MRI) and fine-tuning depth affect explanation quality. Without understanding transfer learning basics (pre-training, layer freezing, fine-tuning), the experimental design is opaque.
  - Quick check question: Given a CNN pre-trained on ImageNet, what happens to early vs. late layer weights when fine-tuning only the final two convolutional blocks on a medical imaging task?

- Concept: XAI methods (saliency, gradient-based attribution)
  - Why needed here: The paper compares eight XAI methods (Integrated Gradients, GradientSHAP, LRP, DeepLIFT, Saliency, Deconvolution, Guided Backpropagation). Understanding how each generates importance maps is necessary to interpret the performance differences.
  - Quick check question: Why might gradient-based methods produce different importance maps than propagation-based methods for the same input and model?

- Concept: Precision-based evaluation for explanation correctness
  - Why needed here: The paper defines explanation performance as precision: overlap between the top-n most intense attribution pixels and the ground-truth lesion mask. This is a specific, task-grounded metric, not a generic XAI quality measure.
  - Quick check question: If a model correctly classifies an image but the XAI method highlights a region adjacent to the lesion (high overlap but not exact), would precision increase or decrease compared to a method that highlights the lesion exactly?

## Architecture Onboarding

- Component map:
  - VGG-16 backbone (5 convolutional blocks, each followed by max-pooling) with ImageNet or within-domain MRI pre-training
  - Fine-tuning controlled at 5 levels: 1conv through all (unfreezing progressively more blocks)
  - XAI methods from Captum library: gradient-based (Saliency, Integrated Gradients, GradientSHAP), propagation-based (LRP, DeepLIFT, Deconvolution, Guided Backpropagation)
  - Ground-truth pipeline: synthetic lesion overlay on HCP MRI slices with binary masks

- Critical path:
  1. Pre-train VGG-16 on either ImageNet (out-of-domain) or MRI sex classification (within-domain)
  2. Fine-tune on lesion classification (regular vs. irregular) with specified layer unfreezing
  3. Apply XAI methods to correctly classified test samples
  4. Compute explanation precision against synthetic lesion ground-truth

- Design tradeoffs:
  - Synthetic lesions enable ground-truth evaluation but may not capture all clinically relevant features; real lesion variability is higher
  - Precision metric rewards exact spatial overlap but penalizes models that may use valid contextual cues
  - VGG-16 is an older architecture; findings may not generalize to ResNets, Vision Transformers, or attention-based models

- Failure signatures:
  - Grid-like or brain-outline artifacts in heat maps (especially Deconvolution on ImageNet models): indicates domain mismatch in learned filters
  - High classification accuracy with low explanation precision: model may rely on non-lesion features (suppressor variables)
  - Explanation quality degradation at intermediate fine-tuning levels: suggests representational conflict between pre-trained and task-specific features

- First 3 experiments:
  1. Replicate the pre-training comparison with a modern architecture (e.g., ResNet-50 or EfficientNet) to test generalization beyond VGG-16.
  2. Extend the ground-truth framework to include non-lesion regions that are correlated with class labels (suppressor variables), to explicitly measure whether XAI methods misattribute importance.
  3. Evaluate whether within-domain pre-training on a different task (e.g., age prediction instead of sex classification) produces similar explanation quality benefits, or whether task-specific feature overlap matters more than domain per se.

## Open Questions the Paper Calls Out
- Does the relationship between pre-training domain and explanation performance generalize to modern architectures beyond VGG-16, such as ResNets or Vision Transformers?
- How do these XAI benchmarks correlate with performance on real clinical data where ground-truth "explanations" are based on expert annotations?
- Can a specific fine-tuning strategy optimize the trade-off where out-of-domain (ImageNet) pre-training yields higher classification accuracy but lower explanation performance compared to within-domain pre-training?

## Limitations
- The synthetic nature of lesions and ground truth may not capture all clinically relevant features or real-world pathology complexity.
- Findings are limited to VGG-16 architecture and may not generalize to modern architectures like ResNets or Vision Transformers.
- The study focuses on correctly classified examples, which may not fully capture explanation performance in clinically relevant failure cases.

## Confidence
- **High confidence**: The finding that XAI methods produce highly variable explanation quality even for correctly classified examples, and the observation of domain-mismatched artifacts (grid patterns) in ImageNet-pretrained models.
- **Medium confidence**: The claim that within-domain pre-training produces better explanation performance than out-of-domain pre-training, as this effect persists when controlling for accuracy but may still contain confounds.
- **Medium confidence**: The claim about optimal fine-tuning depth, given limited evidence and potential architecture-specific effects.

## Next Checks
1. Replicate the pre-training comparison with a modern architecture (e.g., ResNet-50 or EfficientNet) to test generalization beyond VGG-16.
2. Extend the ground-truth framework to include non-lesion regions that are correlated with class labels (suppressor variables), to explicitly measure whether XAI methods misattribute importance.
3. Evaluate whether within-domain pre-training on a different task (e.g., age prediction instead of sex classification) produces similar explanation quality benefits, or whether task-specific feature overlap matters more than domain per se.