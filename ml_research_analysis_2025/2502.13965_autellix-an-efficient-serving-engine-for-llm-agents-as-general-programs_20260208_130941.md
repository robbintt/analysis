---
ver: rpa2
title: 'Autellix: An Efficient Serving Engine for LLM Agents as General Programs'
arxiv_id: '2502.13965'
source_url: https://arxiv.org/abs/2502.13965
tags:
- calls
- programs
- autellix
- program
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Autellix addresses the inefficiency of existing LLM serving systems
  when handling complex, dynamic agentic programs composed of multiple LLM calls and
  external interrupts. It introduces a program-aware scheduling framework that treats
  programs as first-class entities, using cumulative execution times to prioritize
  and preempt LLM calls, thereby minimizing head-of-line blocking at both call and
  program levels.
---

# Autellix: An Efficient Serving Engine for LLM Agents as General Programs

## Quick Facts
- arXiv ID: 2502.13965
- Source URL: https://arxiv.org/abs/2502.13965
- Authors: Michael Luo; Xiaoxiang Shi; Colin Cai; Tianjun Zhang; Justin Wong; Yichuan Wang; Chi Wang; Yanping Huang; Zhifeng Chen; Joseph E. Gonzalez; Ion Stoica
- Reference count: 40
- Primary result: Autellix achieves 4-15× higher throughput than vLLM for LLM agent serving

## Executive Summary
Autellix addresses the inefficiency of serving dynamic agentic programs composed of multiple LLM calls and external interrupts. Traditional LLM serving systems treat each call independently, causing head-of-line blocking that severely impacts program-level performance. Autellix introduces program-aware scheduling that treats entire programs as first-class entities, using cumulative execution times to prioritize and preempt LLM calls. The system employs two novel algorithms: PLAS for single-threaded programs and ATLAS for multi-threaded DAG-structured programs, both optimizing resource allocation based on program-level statistics and critical path analysis. Evaluations demonstrate significant improvements in throughput (4-15×) and latency reduction across diverse workloads, models, and engine configurations.

## Method Summary
Autellix is a serving layer built on top of vLLM v0.6.1 that implements program-aware scheduling through a process table tracking per-program service time, wait time, and thread metadata. The system uses PLAS (Program-Level Attained Service) for single-threaded programs, assigning priorities inversely proportional to cumulative execution time, and ATLAS (Adaptive Thread-level Attained Service) for multi-threaded programs that approximates critical paths via maximum cumulative service time. A locality-aware load balancer routes long requests to program-assigned engines for KV-cache locality while short requests go to least-loaded engines. The scheduler uses discretized priority queues with anti-starvation mechanisms, and a bulk KV-cache swap kernel reduces PCIe overhead during preemption.

## Key Results
- 4-15× higher throughput compared to vLLM across various workloads
- 2.5× improvement on LATS (multi-threaded MCTS workload) using ATLAS
- Up to 1.4× higher throughput from locality-aware load balancing vs. Round Robin
- Linear scaling to 8 GPUs with consistent performance gains
- P99 latency reduced by 20-70% depending on workload and load

## Why This Works (Mechanism)

### Mechanism 1: Program-Level Attained Service (PLAS)
- Claim: Prioritizing LLM calls based on their program's cumulative execution time reduces both call-level and program-level head-of-line blocking.
- Mechanism: PLAS assigns each incoming LLM call a priority inversely proportional to its parent program's total prior service time. Calls from programs with less accumulated runtime are scheduled first using a global process table. Calls are binned into K priority queues based on discretized service times, with demotion on quantum exhaustion.
- Core assumption: Programs with less accumulated service time are more likely to complete soon; favoring them reduces average end-to-end latency.
- Evidence anchors:
  - [abstract]: "minimizing head-of-line blocking at both call and program levels"
  - [§3.1, Figure 6]: Quantifies wait/exec ratio showing short programs wait disproportionately under FCFS/MLFQ
  - [corpus]: "Serve Programs, Not Prompts" (FMR 0.62) advocates similar program-centric serving
- Break condition: If workload has uniform program lengths or minimal interleaving, PLAS provides marginal gains over standard preemptive scheduling.

### Mechanism 2: Critical Path Estimation for Multi-Threaded Programs (ATLAS)
- Claim: Approximating the critical path via maximum cumulative service time across threads improves makespan for DAG-structured programs.
- Mechanism: ATLAS maintains a single scalar per program—the longest observed critical path. Each new call inherits this as initial priority. On completion, the scalar updates only if the call's path is longer. This groups parallel calls from the same program, preventing stragglers from delaying completion.
- Core assumption: The critical path dominates program completion time; non-clairvoyant estimation via max parent priority + runtime is sufficient.
- Evidence anchors:
  - [§4.2.1, Figure 9]: Illustrates how ignoring critical path increases makespan from 11 to 14 units
  - [§6.3]: ATLAS outperforms MLFQ by 2.5× on LATS (multi-threaded MCTS workload)
  - [corpus]: Weak direct evidence; no corpus papers explicitly address DAG-aware LLM scheduling
- Break condition: If programs have highly unbalanced sub-DAGs or frequent external interrupts that invalidate path estimates, ATLAS may misprioritize.

### Mechanism 3: Locality-Aware Load Balancing
- Claim: Routing based on input length and program-affinity reduces KV-cache recomputation while maintaining load balance.
- Mechanism: Short requests (≤2048 tokens) are load-balanced to least-used engines (high cache hit rates from shared system prompts). Longer requests are pinned to their program's assigned engine to preserve prefix locality.
- Core assumption: Intra-program calls share significant prefix (≥90% cache hit); inter-program calls share only system prompt.
- Evidence anchors:
  - [§3.2, Figure 7]: Shows intra-program cache hit rates >90% across input lengths; inter-program decays exponentially
  - [§6.4, Figure 14]: Autellix achieves up to 1.4× higher throughput vs. Round Robin/Least Used
  - [corpus]: "Agentic Plan Caching" (FMR 0.57) addresses related but distinct caching strategies
- Break condition: If programs have minimal prefix overlap or engines are highly heterogeneous, pinning may cause load skew.

## Foundational Learning

- Concept: **Head-of-Line (HoL) Blocking**
  - Why needed here: Autellix's core contribution is distinguishing call-level vs. program-level HoL blocking and mitigating both.
  - Quick check question: In Figure 2c, why does MLFQ still cause 18 units of waiting time despite preempting long calls?

- Concept: **KV-Cache and Prefix Caching**
  - Why needed here: Load balancer decisions hinge on cache hit rates; understanding prefill/decode phases explains recomputation costs.
  - Quick check question: Why do short requests achieve high cache hits across engines while long requests don't (Figure 7)?

- Concept: **Preemptive Scheduling (MLFQ)**
  - Why needed here: Autellix extends MLFQ with program-aware priority assignment; understanding demotion/quantum helps parse Algorithm 1.
  - Quick check question: What is the anti-starvation condition (Line 26) and why does it reset only per-call metrics, not program-level?

## Architecture Onboarding

- Component map:
  Frontend -> Process Table -> Scheduler -> Load Balancer -> LLM Engines

- Critical path:
  1. User program calls `start_session` → entry added to process table
  2. LLM call arrives → load balancer assigns engine → scheduler bins by priority
  3. Call executes → updates process table (service time, possibly critical path)
  4. Program completes → `end_session` removes entry

- Design tradeoffs:
  - Synchronous scheduling: Real-time priority updates vs. batching overhead (mitigated by multi-step scheduling, N-step intervals)
  - Discretized queues: Reduces context switches vs. loses fine-grained priority precision
  - Anti-starvation threshold β: Lower β improves fairness but may reduce throughput by promoting long programs

- Failure signatures:
  - OOM under MLFQ: Too many high-priority queue entries cause memory contention (§6.5.1)
  - Load skew: If pinning long requests causes imbalance, monitor per-engine queue depths
  - Stale process table entries: If `end_session` isn't called (crash), entries persist—implement timeout cleanup

- First 3 experiments:
  1. Reproduce Figure 6: Run ShareGPT workload under FCFS/MLFQ/PLAS; measure wait/exec ratio distribution to validate HoL mitigation
  2. Ablate load balancer: Compare Autellix's policy vs. Round Robin on multi-engine setup with ShareGPT; isolate cache locality gains
  3. Stress test anti-starvation: Vary β threshold and measure P95/P99 latencies (Figure 13) to find fairness-throughput tradeoff point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can compiler-level optimizations, such as branch prediction or speculative execution, be effectively integrated into Autellix to execute future LLM calls while prior calls are still completing?
- Basis in paper: [explicit] Section 7 (Discussion & Future Work) states, "Anticipating [a program's] immediate next steps can be practical... enabling compiler optimizations such as branch prediction and speculative execution... We defer such optimizations to future works."
- Why unresolved: Autellix is currently designed as a non-clairvoyant system that dynamically constructs the execution graph only at runtime, making no assumptions about future steps.
- What evidence would resolve it: An evaluation of an enhanced scheduler that predicts subsequent calls, measuring the reduction in end-to-end latency compared to the current runtime-only approach.

### Open Question 2
- Question: How can the Autellix frontend be secured against user modification to ensure session integrity in production environments?
- Basis in paper: [explicit] Section 5 (Implementation) notes, "As a research prototype, the current frontend lacks safeguards against user modification of the underlying package; addressing this limitation remains future work."
- Why unresolved: The current implementation relies on a library users import, which allows them to potentially alter the underlying package, compromising the stateful session management required for the scheduler.
- What evidence would resolve it: A production-ready implementation of the frontend API that maintains session integrity and prevents tampering without sacrificing the ease of integration with existing agent frameworks.

### Open Question 3
- Question: Can Autellix be adapted to improve the efficiency of distributed reinforcement learning (RL) systems used for post-training reasoning models?
- Basis in paper: [explicit] Section 7 suggests that by reducing the makespan for batch sampling, Autellix "immediately benefits distributed post-training systems" like those used for DeepSeek-R1.
- Why unresolved: The paper's evaluation focuses exclusively on inference serving workloads (e.g., Chatbot, MCTS) and does not test the system within an iterative RL training loop.
- What evidence would resolve it: Benchmarks from a distributed RL training setup comparing the throughput of on-policy sampling using Autellix versus standard serving systems.

### Open Question 4
- Question: What specific heuristics or partial prior knowledge could bridge the performance gap between Autellix and the optimal Shortest Remaining Processing Time (SRPT) policy?
- Basis in paper: [inferred] Section 6.5.3 shows a "noticeable gap" between Autellix and the clairvoyant SRPT policy in simulation, suggesting that the lack of prior knowledge limits optimal performance.
- Why unresolved: Autellix relies on non-clairvoyant scheduling (PLAS/ATLAS), and while it outperforms other agnostic baselines, it does not match the theoretical optimum provided by SRPT.
- What evidence would resolve it: Simulation results testing hybrid scheduling policies that utilize minimal hints (e.g., estimated token counts) to determine if they can close the gap to SRPT without requiring full execution traces.

## Limitations

- Hyperparameter sensitivity: Performance depends on unspecified parameters (number of queues K, time quanta, anti-starvation threshold β) without sensitivity analysis
- Critical path estimation accuracy: ATLAS uses non-clairvoyant approximation that may diverge from true critical path in complex workloads
- Limited evaluation scope: Does not test scenarios with heterogeneous engines or very large-scale deployments where communication overhead might be significant

## Confidence

- High Confidence: The core mechanism of program-level scheduling (PLAS) and its ability to reduce head-of-line blocking is well-supported by both theoretical analysis and empirical results (4-15x throughput improvement)
- Medium Confidence: The critical path approximation in ATLAS and locality-aware load balancing are reasonable approaches but their effectiveness depends on workload characteristics that aren't fully characterized
- Medium Confidence: Scalability claims are demonstrated but don't explore scenarios with heterogeneous engines or very large-scale deployments where network overhead might become significant

## Next Checks

1. **Hyperparameter Sweep**: Systematically vary K (number of priority queues), time quanta, and β (anti-starvation threshold) to identify optimal configurations and measure performance sensitivity across different workload types

2. **Critical Path Accuracy Analysis**: Instrument ATLAS to track the divergence between estimated and actual critical paths across multiple workload runs, quantifying how often the approximation leads to suboptimal scheduling decisions

3. **Heterogeneous Engine Evaluation**: Test Autellix with engines of different performance characteristics (e.g., mix of A100 and H100 GPUs) to evaluate how well the locality-aware load balancer handles engine heterogeneity and whether it maintains its throughput advantages