---
ver: rpa2
title: Allocating Variance to Maximize Expectation
arxiv_id: '2502.18463'
source_url: https://arxiv.org/abs/2502.18463
tags:
- variables
- variance
- lemma
- algorithm
- objective
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents efficient approximation algorithms for maximizing
  the expected maximum of Gaussian random variables under a total variance budget
  constraint. The core method uses a modified chaining argument to show that in the
  optimal variance allocation, most variables receive negligible variance.
---

# Allocating Variance to Maximize Expectation

## Quick Facts
- **arXiv ID:** 2502.18463
- **Source URL:** https://arxiv.org/abs/2502.18463
- **Reference count:** 40
- **Primary result:** PTAS for maximizing expected maximum of Gaussian random variables under variance budget

## Executive Summary
This paper presents efficient approximation algorithms for maximizing the expected maximum of Gaussian random variables under a total variance budget constraint. The core method uses a modified chaining argument to show that in the optimal variance allocation, most variables receive negligible variance. This structural insight enables polynomial-time approximation schemes (PTAS) for single-set problems and an O(log n)-approximation algorithm for multi-set problems.

## Method Summary
The paper develops algorithms that exploit the concentration of variance in optimal solutions. For single sets, it uses grid search over a polynomial-sized set of candidate variables and variance values, leveraging a modified chaining argument to bound the error from excluding variables with small variance. For multiple sets, it groups variables by variance magnitude and applies greedy submodular maximization within each group, iterating over O(log n) variance levels to achieve an O(log n)-approximation.

## Key Results
- PTAS for computing optimal variance allocation with independent Gaussians
- PTAS for correlated Gaussian variables with polynomial runtime in n and 1/ε
- O(log n)-approximation algorithm for multiple sets using greedy submodular maximization
- Characterization of concavity and variance concentration in optimal allocations as set sizes increase

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Variables with variance below ε² contribute negligibly O(ε√(ln(1/ε))) to the expected maximum, enabling search space reduction to O(1/ε²) variables.
- **Mechanism:** Groups variables by variance magnitude (powers of 2), applies exponential moment bounds per group, exploits total variance budget to show tighter bound O(ε√(ln(1/ε))) than standard chaining's O(ε√(ln n)). This enables polynomial-time grid search over only O(1/ε²) candidate variables.
- **Core assumption:** Total variance budget bounded (∑σ²ᵢ = 1) and ε fixed constant.
- **Evidence anchors:**
  - [abstract] "modified chaining argument to show that in the optimal variance allocation, most variables receive negligible variance"
  - [section 2.1] Lemma 2.1: "E max(0, max_{i∈[m]} Y_i) = O(ε√(ln(1/ε)))" with total variance constraint enabling tighter bound vs. Talagrand's O(ε√(ln n))
  - [corpus] Limited direct evidence; variance-dependent bounds appear in related bandit/MDP work but not this specific chaining modification
- **Break condition:** If total variance unbounded or ε decreases with n, the O(1/ε²) search space becomes super-polynomial.

### Mechanism 2
- **Claim:** Optimal variance allocation concentrates on Θ(1/p) variables with Ω(p) variance as set density p increases.
- **Mechanism:** High density increases per-variable coverage across sets; concentrating variance on fewer variables maximizes per-set max contribution while diluting variance across many variables yields lower expected maximum per set. Lower bound construction shows Θ(1/p) concentrated variables achieve Ω(m√p) objective; spreading variance more thinly yields only O(m√(pε ln(1/ε))) for small ε.
- **Core assumption:** Random graph structure (Erdős-Rényi) with high density p; n,m → ∞.
- **Evidence anchors:**
  - [abstract] "it concentrates on a small subset of variables as |S_j| increases"
  - [section 1.3] Theorem 1.6: "Θ(1/p) variables allocated variance Ω(p)" for random instances with probability > 1-δ
  - [section B.1] Proof constructs allocation with σ²ᵢ = p for first 1/p variables, shows this achieves Ω(m√p) while dispersed allocation achieves only O(m√(pε ln(1/ε)))
  - [corpus] No directly comparable concentration results found
- **Break condition:** For small sets (e.g., |Sⱼ| = 2 with cycle structure, Theorem 2.5), uniform allocation σ²ᵢ = 1/n is optimal, breaking concentration.

### Mechanism 3
- **Claim:** Multi-set variance allocation reduces to O(log n) parallel submodular maximization problems, each solvable with (1-1/e)-approximation greedy algorithm.
- **Mechanism:** Variables grouped by variance magnitude (powers of 4). One group captures Ω(1/log n) of optimal objective. Within each group, variance fixed at 4⁻ᵏ, so objective becomes set function f(S) = E[max_{i∈S} X_i] where X_i ∼ N(0, 4⁻ᵏ). Lemma 2.7 proves f is submodular; greedy selection under cardinality constraint yields constant-factor approximation. Best across O(log n) groups gives O(log n)-approximation overall.
- **Core assumption:** Non-negative means; submodularity of expected maximum function (proven Lemma 2.7).
- **Evidence anchors:**
  - [abstract] "O(log n)-approximation algorithm for general m > 1"
  - [section 2.3] Lemma 2.7: "f(S) = E max_{i∈[n]} X_i... is a submodular function"
  - [section 2.3] Algorithm 3 iterates k ∈ [0, log₂n], applies greedy selection per variance level
  - [corpus] Submodular maximization well-studied [Nemhauser et al., 1978] with established approximation guarantees
- **Break condition:** If means can be negative or if submodularity fails for correlated case without independence, greedy guarantee degrades.

## Foundational Learning

- **Concept:** Expected maximum of Gaussian random variables E[max_i X_i] for X_i ∼ N(μᵢ, σ²ᵢ)
  - **Why needed here:** Core objective function; intuition that variance increases expected maximum (high variance → heavier tail → larger max) but with budget constraint creates optimization trade-off
  - **Quick check question:** For n i.i.d. N(0,1) variables, does E[max] grow as √(ln n) or linearly in n?

- **Concept:** Chaining arguments for bounding suprema of stochastic processes
  - **Why needed here:** Lemma 2.1's modified chaining is central to PTAS; understanding standard chaining (grouping by variance, union bounds) clarifies why total variance budget enables tighter bound
  - **Quick check question:** Why does standard chaining give O(ε√(ln n)) while modified version achieves O(ε√(ln(1/ε)))?

- **Concept:** Submodular functions and greedy approximation
  - **Why needed here:** Multi-set algorithm relies on submodularity (Lemma 2.7); greedy (1-1/e)-approximation for cardinality-constrained maximization enables O(log n) overall approximation
  - **Quick check question:** For submodular f with f(∅)=0, does greedy selection with cardinality constraint k guarantee f(S_greedy) ≥ (1-1/e)·OPT?

## Architecture Onboarding

- **Component map:**
  - VarAlloc PTAS: Grid search over variables × variance values → objective evaluation
  - CorrVarAlloc PTAS: Grid search over variables × variance values × covariance submatrices → PSD validation → objective evaluation
  - GraphVarAlloc: Outer loop over variance levels → greedy submodular selection → best across levels
  - Supporting lemmas: Chaining bound (2.1), smoothness (2.2/2.4), variance scaling (2.6), submodularity (2.7), correlation gap (2.8)

- **Critical path:** For multi-set production use: (1) filter singleton sets (constant contribution), (2) transform to zero-mean objective (constant-factor loss), (3) iterate variance levels 4⁻ᵏ for k ∈ [0, ⌈log₂n⌉], (4) per level, greedily add variables maximizing marginal gain until budget exhausted, (5) return best across levels. Key bottleneck is marginal gain computation E[max_{i∈Sⱼ} X_i] for each candidate addition.

- **Design tradeoffs:**
  - PTAS grid granularity: Finer grid (smaller ε) → better approximation but O(ε⁻⁵) complexity growth (independent case) or O(ε⁻²¹) (correlated case)
  - Multi-set: O(log n) approximation vs. exact exponential; greedy O(n·m) per variance level vs. optimal NP-hard
  - Correlation: Allowing full covariance Σ vs. diagonal constraint trades O(ε⁸·⁵) finer grid requirement for potential objective improvement; Lemma 2.8 bounds gap ≤ 2e/(e-1) ≈ 1.58

- **Failure signatures:**
  - **PTAS timeout:** Grid size exponential in 1/ε²; symptom: runtime grows as (ε⁻³)^(ε⁻²) for n > ε⁻². Fix: increase ε or cap variables considered.
  - **Multi-set poor approximation:** Small sets (|Sⱼ| = O(1)) may require uniform variance allocation; O(log n) guarantee still holds but constant factors large. Symptom: OPT ≈ m/√n but algorithm returns ≪ OPT/log n. Fix: detect small-set regime, use uniform baseline.
  - **Covariance non-PSD:** Grid search may generate non-semidefinite matrices. Symptom: algorithm skips valid candidates or crashes. Fix: validate PSD constraint via Cholesky decomposition or eigenvalue check before objective evaluation.

- **First 3 experiments:**
  1. **Single-set validation:** Generate VarAlloc instance with n=100, μ=0, compute OPT via PTAS with ε∈{0.1, 0.05, 0.01}. Compare E[max] from PTAS output vs. brute-force enumeration for small n=10. Verify additive error ≤ ε. Plot runtime vs. 1/ε to confirm polynomial scaling.
  2. **Concentration verification:** For random GraphVarAlloc with n=m=1000, p∈{0.1, 0.3, 0.5, 0.7, 0.9}, compute optimal allocation via PTAS (small p) or multi-set algorithm (large p). Measure fraction of variables with σ²ᵢ > p/10. Confirm Θ(1/p) concentration as p increases per Theorem 1.6.
  3. **Multi-set approximation ratio:** Construct worst-case cycle instance (Theorem 2.5: n variables, n sets with Sⱼ={Xⱼ, Xⱼ₊₁}, uniform variance 1/n optimal). Run Algorithm 3, compare returned objective vs. theoretical OPT = n·(μ + √(2/(πn))). Measure actual approximation ratio; expect degradation for small n due to greedy suboptimality on non-monotone submodular objectives with overlapping sets.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is there a Polynomial Time Approximation Scheme (PTAS) or constant-factor approximation for the GraphVarAlloc (multiple sets) problem?
- **Basis in paper:** [explicit] Section 2.3 states that the PTAS technique for the single-set setting no longer works for multiple sets, and the authors provide only an O(log n)-approximation.
- **Why unresolved:** The scale of the optimal objective can vary significantly with set sizes (e.g., small sets prevent the use of grid search), breaking the concentration arguments used for the single-set case.
- **What evidence would resolve it:** An algorithm achieving a $(1+\epsilon)$-approximation or constant factor for GraphVarAlloc, or a proof of APX-hardness for the multi-set instance.

### Open Question 2
- **Question:** Do the structural results (concavity, concentration) and efficient algorithms extend to non-Gaussian distributions, such as sub-Gaussian variables?
- **Basis in paper:** [inferred] The problem formulation (Section 1.1) and key proofs (Lemma 2.1) rely strictly on Gaussian tail bounds and specific properties of the normal distribution.
- **Why unresolved:** The modified chaining argument depends on the specific decay of Gaussian tails under a total variance budget; different distributions may lack the same concentration properties.
- **What evidence would resolve it:** A proof generalizing Lemma 2.1's error bound to sub-Gaussian variables or a counter-example showing non-concentration of variance for heavy-tailed distributions.

### Open Question 3
- **Question:** Is the O(log n) approximation ratio tight for the GraphVarAlloc problem?
- **Basis in paper:** [inferred] Theorem 1.3 establishes an O(log n) approximation, but the paper does not provide a lower bound or hardness result that matches this logarithmic factor.
- **Why unresolved:** The logarithmic factor arises from a rounding technique that groups variances into log n buckets; it is unclear if this is an artifact of the algorithm or a fundamental complexity barrier.
- **What evidence would resolve it:** A hardness reduction showing that achieving an $o(\log n)$ approximation is NP-hard, or an improved algorithm breaking the logarithmic barrier.

## Limitations

- The PTAS for correlated Gaussians (Algorithm 2) has high computational complexity O(ε⁻²¹) due to fine-grained covariance discretization
- The O(log n) approximation for multiple sets may be loose for small problem instances with few variables
- The concentration results in Theorem 1.6 rely on random graph structure assumptions that may not hold for arbitrary set systems

## Confidence

- **VarAlloc PTAS:** High - well-established chaining and grid search techniques with rigorous proofs
- **CorrVarAlloc PTAS:** High - follows similar structure to VarAlloc with additional covariance handling
- **GraphVarAlloc O(log n)-approx:** High - builds on well-studied submodular maximization with greedy algorithms
- **Theorem 1.6 concentration:** Medium - relies on asymptotic behavior and random graph structure assumptions

## Next Checks

1. **Runtime scaling verification:** Implement Algorithm 1 with ε∈{0.1, 0.05, 0.01} and measure runtime on instances with n∈{100, 200, 500, 1000}. Confirm polynomial scaling and identify the dominant factor (grid size vs. subset enumeration).

2. **Approximation ratio empirical validation:** Construct random GraphVarAlloc instances with varying density p and compare Algorithm 3's output against PTAS solutions for small instances. Measure actual approximation ratio and verify O(log n) behavior.

3. **Concentration behavior testing:** For random instances with n=m∈{100, 500, 1000} and p∈{0.1, 0.3, 0.5, 0.7, 0.9}, compute optimal allocations and measure the fraction of variables receiving significant variance (σ²ᵢ > p/10). Verify Θ(1/p) concentration as predicted.