---
ver: rpa2
title: A Pipeline of Augmentation and Sequence Embedding for Classification of Imbalanced
  Network Traffic
arxiv_id: '2502.18909'
source_url: https://arxiv.org/abs/2502.18909
tags:
- network
- traffic
- data
- flow
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of imbalanced network traffic classification,
  where certain application classes dominate the dataset, leading to poor performance
  on minority classes. The authors propose a pipeline that combines data augmentation
  using LSTM and KDE with a novel FS-Embedding scheme to improve classification accuracy
  and reduce model complexity.
---

# A Pipeline of Augmentation and Sequence Embedding for Classification of Imbalanced Network Traffic

## Quick Facts
- arXiv ID: 2502.18909
- Source URL: https://arxiv.org/abs/2502.18909
- Reference count: 35
- Primary result: FS-Embedding reduces model parameters by >50% while maintaining/improving accuracy vs. one-hot encoding

## Executive Summary
This paper addresses the problem of imbalanced network traffic classification, where certain application classes dominate the dataset, leading to poor performance on minority classes. The authors propose a pipeline that combines data augmentation using LSTM and KDE with a novel FS-Embedding scheme to improve classification accuracy and reduce model complexity. Experiments on a real-world dataset with 19 application classes show that the proposed augmentation pipeline improves classification F1 scores from 89.61% to 94.96% using transformers, while the FS-Embedding scheme reduces model parameters by over 50% compared to one-hot encoding.

## Method Summary
The proposed method addresses imbalanced network traffic classification through two key innovations: (1) data augmentation using LSTM networks to generate synthetic sequential features (packet direction, TCP window size) and KDE to sample numerical features for underrepresented classes, and (2) FS-Embedding that treats source/destination ports and packet direction as "words" in a flow-as-sentence representation, replacing sparse one-hot encoding with dense vector representations. The approach generates synthetic flows by training class-specific LSTMs on packet sequences and fitting KDE estimators to numerical features, then creates an FS-Embedding lookup table based on port frequencies per application class. The method was evaluated using a transformer encoder classifier, showing improved F1 scores and reduced model complexity compared to CRNN baselines with one-hot encoding.

## Key Results
- Classification F1 scores improved from 89.61% to 94.96% using the proposed augmentation pipeline with transformers
- FS-Embedding reduced model parameters by over 50% (458K to 35K) while maintaining or improving accuracy
- The approach effectively addresses class imbalance, with minority classes showing particularly strong improvements (e.g., F1 from 0.67 to 0.81 for class 10)
- Transformer encoder converged faster (300 epochs) compared to CRNN baseline (1000 epochs) with similar parameter counts

## Why This Works (Mechanism)

### Mechanism 1: LSTM-based Sequential Pattern Generation for Augmentation
- Claim: Training class-specific LSTMs on packet direction and TCP window sequences can generate synthetic flows that preserve application-specific temporal patterns.
- Mechanism: For each underrepresented class, an LSTM learns the probability distribution of the next packet direction (or window size) conditioned on prior values in the sequence. During generation, the model samples from this learned distribution autoregressively, producing new sequences up to 20 packets. An end-of-sequence token signals flow termination.
- Core assumption: Packet direction and TCP window size sequences within each application class follow learnable temporal dependencies that can be decoupled from numerical features and independently sampled.
- Evidence anchors: [abstract] "generate artificial data using Long Short-Term Memory (LSTM) networks and Kernel Density Estimation (KDE)"; [section III.A.1] "The sequence pattern generation scheme requires training an LSTM network to generate instances for each class separately... The generated direction is then fed into the LSTM to generate the probabilities for the next step."

### Mechanism 2: Kernel Density Estimation for Independent Numerical Feature Sampling
- Claim: Non-parametric density estimation via KDE can accurately model the distribution of numerical flow features (inter-arrival time, payload length, ports) for sampling synthetic values.
- Mechanism: For each numerical feature per class, KDE estimates the probability density function using a Gaussian kernel with bandwidth selected via Silverman's rule. Synthetic samples are drawn from the learned PDF independently for each feature.
- Core assumption: Numerical features are independently and identically distributed within each class, and their marginal distributions can be captured without modeling inter-feature correlations.
- Evidence anchors: [abstract] "generates synthetic data for underrepresented classes using LSTM for sequential features... and KDE for numerical features"; [section III.A.2] "KDE, also known as the Parzenâ€“Rosenblatt window, is one of the most well-known methods for estimating the probability density function (PDF) of a dataset."

### Mechanism 3: FS-Embedding for Semantic Port and Direction Representation
- Claim: Treating source port, destination port, and packet direction as a "word" within a "flow-as-sentence" representation enables the model to learn semantic relationships between port patterns and application classes, improving generalization while reducing parameters.
- Mechanism: Ports are mapped to characters based on frequency per application class via a lookup table. Each packet's SP, DP, and direction are concatenated into a token (e.g., "AE0"). These tokens form a sequence representing the flow. An embedding layer learns dense vector representations during the classification task, replacing sparse one-hot encoding.
- Core assumption: Port-direction combinations exhibit semantic regularities (e.g., port 80 with specific direction patterns correlates with HTTP) that can be captured in a low-dimensional embedding space.
- Evidence anchors: [abstract] "FS-Embedding replaces one-hot encoding by treating source/destination ports and packet direction as 'words' in a sentence-like flow representation"; [section III.B] "Flows in a network resemble sentences in a language... In word embedding, words with similar meanings are represented similarly in a learned representation of text."

## Foundational Learning

- Concept: **Class Imbalance and F1 Score**
  - Why needed here: The paper's core problem is imbalanced traffic classes (SSL = 37%, RDP < 0.16%). Accuracy is misleading; F1 is reported per-class.
  - Quick check question: If a classifier predicts only the majority class (SSL), what would accuracy be? Why is F1 more informative here?

- Concept: **Autoregressive Sequence Generation**
  - Why needed here: The LSTM augmentation generates packet sequences step-by-step, conditioning each prediction on prior outputs.
  - Quick check question: What happens if the LSTM samples an end-of-sequence token too early or never samples it?

- Concept: **Learned Embeddings vs. One-Hot Encoding**
  - Why needed here: FS-Embedding claims to reduce parameters (~458K to ~35K) while maintaining accuracy by replacing sparse representations with dense vectors.
  - Quick check question: For a vocabulary of 65536 port values, what is the dimensionality of one-hot vs. a 128-dimensional embedding?

## Architecture Onboarding

- Component map:
  - Preprocessing -> Augmentation Module -> FS-Embedding Layer -> Transformer Classifier
  - Feature extraction -> LSTM+KDE generation -> Port-to-character lookup -> Embedding layer -> Transformer encoder

- Critical path:
  1. Identify minority classes (F1 < threshold on baseline model)
  2. Train per-class LSTM on direction and window sequences
  3. Fit KDE for each numerical feature per class
  4. Generate synthetic flows by combining sampled sequences and numerical values
  5. Build FS-Embedding lookup table from training data port frequencies
  6. Train Transformer encoder with FS-Embedded inputs

- Design tradeoffs:
  - **Augmentation vs. Oversampling**: Augmentation learns data distribution; oversampling duplicates existing samples. Paper claims augmentation outperforms sampling (F1: 0.67 vs. 0.54 for class 10).
  - **FS-Embedding vs. One-Hot**: FS-Embedding reduces parameters by >50% but requires port frequency analysis and lookup table construction. One-hot is simpler but scales poorly with large port spaces.
  - **Transformer vs. CRNN Baseline**: Transformer converges faster (300 vs. 1000 epochs) with similar parameter counts (~458K) but requires more memory for attention computation.

- Failure signatures:
  - Generated flows with invalid port-direction combinations (e.g., server port appearing as client source)
  - Embedding collapse if certain port tokens rarely appear in training
  - Minority class overfitting if augmentation produces insufficient diversity
  - Transformer attention instability on very short sequences (<5 packets)

- First 3 experiments:
  1. **Baseline replication**: Train CRNN on original imbalanced data with one-hot encoding; record per-class F1 to identify minority classes needing augmentation.
  2. **Ablation on augmentation method**: Compare LSTM+KDE augmentation vs. random oversampling vs. SMOTE-like interpolation on the same minority classes using the same classifier.
  3. **Embedding dimension sweep**: Train Transformer with FS-Embedding at 32, 64, 128, 256 dimensions; plot accuracy vs. parameter count to validate the claimed efficiency gain.

## Open Questions the Paper Calls Out
- **None specified** - The paper does not explicitly call out open questions or limitations in the manuscript.

## Limitations
- The methodology assumes independence between sequential and numerical features during augmentation, which may not hold in practice and could produce unrealistic synthetic flows
- The FS-Embedding approach relies on port frequency stability within classes, which may not generalize across different network environments or datasets
- The paper lacks cross-dataset validation and does not compare against GAN-based augmentation methods that could potentially offer better synthetic data generation

## Confidence

- **High Confidence**: F1 score improvements (89.61% to 94.96%) and parameter reduction claims (~458K to ~35K) are directly supported by reported experiments on the same dataset
- **Medium Confidence**: LSTM sequence generation and KDE sampling mechanisms are theoretically sound but lack empirical validation of independence assumptions
- **Low Confidence**: Claims about FS-Embedding enabling "semantic learning of port relationships" are plausible but not directly validated through ablation studies or cross-dataset testing

## Next Checks

1. **Feature Correlation Analysis**: Measure pairwise correlations between sequential and numerical features in minority classes to quantify the independence assumption violation risk in the LSTM+KDE augmentation pipeline.
2. **Cross-Dataset Port Mapping Stability**: Apply the trained FS-Embedding model to a different network traffic dataset and measure port vocabulary coverage and classification performance degradation to assess generalization limits.
3. **Synthetic Sample Realism Test**: Compare statistical distributions (e.g., Kolmogorov-Smirnov test) and visualization of real vs. synthetic flows for minority classes to identify unrealistic patterns in the generated data.