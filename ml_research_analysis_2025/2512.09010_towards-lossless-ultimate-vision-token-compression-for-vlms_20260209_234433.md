---
ver: rpa2
title: Towards Lossless Ultimate Vision Token Compression for VLMs
arxiv_id: '2512.09010'
source_url: https://arxiv.org/abs/2512.09010
tags:
- visual
- luvc
- tokens
- pruning
- token
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the computational inefficiency of visual language
  models (VLMs) caused by high-resolution image and video token redundancy. The authors
  propose LUVC, a training-free framework that combines orthogonal iterative merging
  (OIM) in the visual encoder with spectrum pruning units (SPU) in the LLM.
---

# Towards Lossless Ultimate Vision Token Compression for VLMs

## Quick Facts
- **arXiv ID**: 2512.09010
- **Source URL**: https://arxiv.org/abs/2512.09010
- **Reference count**: 40
- **Primary result**: LUVC achieves ~2× inference speedup across LLaVA-OV, InternVL2.5, and Qwen2VL models with negligible accuracy loss via training-free vision token compression

## Executive Summary
This paper addresses the computational inefficiency of visual language models (VLMs) caused by high-resolution image and video token redundancy. The authors propose LUVC, a training-free framework that combines orthogonal iterative merging (OIM) in the visual encoder with spectrum pruning units (SPU) in the LLM. OIM preserves spatial structure while merging tokens in 2D, and SPU uses FFT-based low-pass filtering to prune high-frequency tokens progressively. Experiments show LUVC achieves ~2× inference speedup across LLaVA-OV, InternVL2.5, and Qwen2VL models with negligible accuracy loss. It maintains strong performance on diverse tasks including video, images, charts, and documents, and outperforms FastV, PACT, and VTW baselines. The approach is broadly compatible and supports immediate deployment.

## Method Summary
LUVC is a training-free framework that compresses vision tokens in VLMs through two main components. The Orthogonal Iterative Merger (OIM) operates in the visual encoder, merging tokens orthogonally along row and column axes separately to preserve 2D spatial structure. This 2D-aware approach avoids the spatial collapse that occurs with 1D flattening methods. The Spectrum Pruning Unit (SPU) operates in the LLM, applying FFT-based low-pass filtering to progressively prune high-frequency tokens across multiple layers. By the final layer, all visual tokens are eliminated, having transferred their information to the multimodal representations. The method requires no fine-tuning and is compatible with various VLM architectures.

## Key Results
- Achieves ~2× inference speedup across multiple VLM architectures including LLaVA-OV, InternVL2.5, and Qwen2VL
- Maintains strong performance across diverse tasks: video (VideoMME), images (MMStar, POPE), charts (ChartQA), and documents (DocVQA)
- Outperforms baseline methods FastV, PACT, and VTW on both speed and accuracy metrics
- Successfully compresses video tokens from 8192 to 2048 with minimal accuracy degradation

## Why This Works (Mechanism)

### Mechanism 1: Spatial-Structure Preservation via Orthogonal Merging
The Orthogonal Iterative Merger (OIM) performs merging operations along row and column axes separately rather than treating tokens as a flattened 1D sequence. This 2D-aware approach preserves the spatial grid structure required by projectors that use pixel shuffling or 2D pooling. The core assumption is that the projector relies on explicit 2D spatial layouts, and disrupting this layout (as done in 1D clustering methods) degrades performance. Evidence shows that 2D projectors rely on spatial structure, which 1D merging disrupts. The break condition is if the VLM uses a purely 1D projector, where structural preservation may not offer significant advantages.

### Mechanism 2: Position-Invariant Pruning via Spectral Analysis
The Spectrum Pruning Unit (SPU) transforms visual tokens into the frequency domain using FFT, applies a low-pass filter via Hamming window, and prunes high-frequency tokens based on energy calculations. This approach avoids the position bias and FlashAttention incompatibility inherent in attention-score methods. The core assumption is that visual semantics are concentrated in low-frequency components while high-frequency components are largely noise/redundancy. Evidence shows that spectrum curves remain position-invariant, contrasting with the upward trend in attention scores near text regions. The break condition is if a task relies heavily on high-frequency texture details rather than object semantics.

### Mechanism 3: Progressive Cross-Modal Fusion
LUVC applies SPU progressively across LLM layers, systematically compressing visual tokens until complete elimination at the final layer. The core assumption is that LLM layers can extract and absorb necessary visual information early enough that physical tokens can be discarded later without memory loss. Evidence shows that visual tokens become redundant in later generation stages. The break condition is if the LLM is very shallow or visual reasoning requires deep referencing of raw visual data at the output layer.

## Foundational Learning

- **Concept: Quadratic Complexity of Attention**
  - Why needed: Understanding why token reduction is critical. Attention scales O(n²), where n is token length. High-res images create massive n (e.g., 8000 tokens for video), making prefilling the primary latency bottleneck.
  - Quick check: If an input has 1000 visual tokens, roughly how does computation change if reduced to 500 tokens? (Answer: Computation roughly quarters, 1000² → 500²).

- **Concept: FlashAttention Constraints**
  - Why needed: Many pruning methods rely on reading the attention matrix (QK^T). FlashAttention optimizes speed by not materializing this full matrix, breaking compatibility with attention-based pruners. LUVC uses frequency analysis to bypass this.
  - Quick check: Why can't standard attention-based pruning methods (like FastV) easily work with FlashAttention enabled? (Answer: They need the explicit attention weights, which FlashAttention avoids storing).

- **Concept: Frequency Domain Analysis (FFT)**
  - Why needed: The core innovation of SPU relies on spectral properties. FFT converts spatial signals to frequencies. "Low frequency" corresponds to smooth, general structures, while "high frequency" corresponds to sharp edges or noise.
  - Quick check: In the context of this paper, does a "high-frequency visual token" likely contain the main object's shape or fine-grained noise/texture? (Answer: Noise/Texture).

## Architecture Onboarding

- **Component map**: Visual Encoder -> OIM -> Projector -> LLM -> SPU
- **Critical path**: The implementation of the SPU within the LLM forward pass. You must ensure the FFT/IFFT operations are differentiable (if training) or low-latency (if inference-only), and that the pruning mask (Top-K selection based on energy) correctly indexes the hidden states without shifting the text tokens.
- **Design tradeoffs**:
  - OIM Rate vs. SPU Start: Merging too much in visual encoder destroys detail needed for dense tasks (charts). Starting SPU too early might lose context before fusion.
  - Speed vs. Accuracy: Different schedules for video (aggressive OIM) vs. documents (no OIM, late SPU).
- **Failure signatures**:
  - Position Bias (Baseline): High scores for image tokens adjacent to text tokens, regardless of semantic importance.
  - Spatial Collapse: 1D merging before 2D projector shatters image grid, causing garbage outputs.
  - Over-Smoothing: Low-pass filter threshold too low causes generic captions lacking specific details.
- **First 3 experiments**:
  1. OIM Integrity Check: Run OIM on visual encoder and visualize reconstructed token grid before projector. Verify spatial continuity compared to 1D flatten+merge baseline.
  2. SPU Robustness Analysis: Plot "Spectral Energy" vs. "Token Index" for sample image. Confirm position-invariant distribution compared to sloping attention score curve.
  3. End-to-End Latency Profiling: Measure prefill latency with and without LUVC on video task (high token count). Verify ~2× speedup claim and check accuracy drop on simple VQA benchmark.

## Open Questions the Paper Calls Out
None

## Limitations
- Critical hyperparameters underspecified: low-pass filter cutoff threshold σ_t, exact OIM layer indices, and bipartite soft matching implementation details
- Requires different hyperparameter schedules for different task types (e.g., disabling OIM for documents)
- Performance characteristics vary significantly across task domains, with some tasks more sensitive to token reduction than others

## Confidence

**High Confidence**: The fundamental mechanism of combining 2D-aware token merging with frequency-domain pruning is sound and well-supported by theoretical framework. The core insight that spatial structure preservation and position-invariant pruning address known limitations of existing methods is convincing.

**Medium Confidence**: The claimed ~2× speedup across diverse VLM architectures is supported by results, but exact magnitude depends on unspecified hyperparameters. The claim of "negligible accuracy loss" is reasonable given progressive fusion design, but specific threshold varies by task.

**Low Confidence**: The generalization claim to "charts, documents, and video" tasks is supported by experimental results, but the paper acknowledges different tasks require different hyperparameter schedules. Specific performance characteristics for each task domain would need independent verification.

## Next Checks

1. **Hyperparameter sensitivity analysis**: Systematically vary σ_t (low-pass filter cutoff) and measure its impact on both speed and accuracy across different task types. This would help establish guidelines for setting this critical parameter and validate whether the "negligible accuracy loss" claim holds across the full hyperparameter space.

2. **Cross-architecture robustness test**: Implement LUVC on at least two additional VLM architectures beyond those tested in the paper (e.g., LLaVA-1.5 and Qwen2VL variants). Measure whether the ~2× speedup and accuracy preservation claims generalize to architectures with different visual encoder designs (ViT vs. SigLIP) and projector types.

3. **Ablation of progressive vs. single-shot pruning**: Compare the proposed progressive SPU approach (pruning across multiple LLM layers) against a single-shot pruning strategy where all visual tokens are pruned at once after the visual encoder. This would validate the claimed advantage of "incremental fusion" and help quantify the benefit of the more complex progressive approach.