---
ver: rpa2
title: Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive
  Learning
arxiv_id: '2506.00936'
source_url: https://arxiv.org/abs/2506.00936
tags:
- molecular
- graph
- prediction
- learning
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of accurately predicting molecular
  metabolic stability (MS) for drug discovery, focusing on two key limitations in
  current graph neural network approaches: incomplete modeling of bond-level topological
  features and lack of reliable uncertainty quantification. The authors propose TrustworthyMS,
  a novel contrastive learning framework that integrates dual molecular representations
  (atom-centric and bond-centric graphs), dual-view contrastive learning to align
  molecular topology with bond-interaction semantics, and evidential uncertainty quantification
  using Beta-Binomial subjective logic.'
---

# Uncertainty-Aware Metabolic Stability Prediction with Dual-View Contrastive Learning

## Quick Facts
- arXiv ID: 2506.00936
- Source URL: https://arxiv.org/abs/2506.00936
- Authors: Peijin Guo; Minghui Li; Hewen Pan; Bowen Chen; Yang Wu; Zikang Guo; Leo Yu Zhang; Shengshan Hu; Shengqing Hu
- Reference count: 27
- Primary result: TrustworthyMS achieves 0.622 MCC in HLM classification and 0.833 P-score in HL regression, with 46.1% higher robustness on out-of-distribution data

## Executive Summary
This paper addresses key limitations in graph neural network approaches for metabolic stability prediction by proposing TrustworthyMS, a dual-view contrastive learning framework. The method explicitly models bond-level topological features through a novel molecular graph topology remapping module, aligns molecular topology with bond-interaction semantics via dual-view contrastive learning, and provides calibrated uncertainty estimates using Beta-Binomial subjective logic. Evaluated on HLM classification, HL regression, and out-of-distribution datasets, the framework achieves state-of-the-art performance while offering reliable uncertainty quantification essential for drug discovery applications.

## Method Summary
TrustworthyMS integrates three key innovations: a molecular graph topology remapping module that constructs a bond-centric graph from the original atom-centric graph by creating nodes for "atom-bond-atom" triplets, a dual-view graph contrastive learning component that aligns embeddings from both views through mutual information maximization, and an evidential uncertainty quantification head that outputs Beta distribution parameters for calibrated predictions. The model uses a hybrid loss function combining graph reconstruction, contrastive alignment, and evidential prediction objectives, trained end-to-end on SMILES-derived molecular graphs with specified atom and bond features.

## Key Results
- Achieves 0.622 MCC on HLM binary classification, outperforming existing methods by significant margins
- Achieves 0.833 P-score on HL regression with z-score normalized targets
- Demonstrates 46.1% higher robustness on out-of-distribution clinical candidate and rat microsomal datasets
- Provides calibrated uncertainty estimates where high-uncertainty predictions correlate with errors

## Why This Works (Mechanism)

### Mechanism 1
Explicitly modeling bond-level topological features via a dual-view graph architecture improves molecular representation for metabolic stability prediction. The Molecular Graph Topology Remapping (MGTR) module constructs a secondary "bond-centric" graph ($G_r$) from the original atom-centric molecular graph ($G$), where nodes represent "atom-bond-atom" triplets and edges connect nodes sharing common atoms. This encoding captures higher-order bond relationships that standard atom-centric message passing misses. Performance gains would break if computational costs outweigh benefits on simpler molecules.

### Mechanism 2
Aligning molecular topology and bond-interaction views via contrastive learning enhances representation robustness. The Dual-View Graph Contrastive Learning (DVCL) module maximizes mutual information between atom-centric and bond-centric embeddings while pushing apart different molecules' embeddings. This forces learning of features consistent across both views, filtering out view-specific noise. The mechanism fails if one view is noisy or uninformative, causing negative transfer, or if the temperature parameter is improperly tuned.

### Mechanism 3
Beta-Binomial subjective logic provides calibrated uncertainty estimates for both classification and regression tasks. The Evidential Uncertainty Quantification (EBUQ) module outputs Beta distribution parameters ($\alpha, \beta$) representing belief about the target, deriving belief, disbelief, and uncertainty masses for classification or using distribution variance for regression uncertainty. The mechanism fails if uncertainty is multi-modal or doesn't fit Beta distribution assumptions, or if uncertainty penalty weighting is improper.

## Foundational Learning

- **Concept: Graph Neural Networks (GNNs) for Molecular Property Prediction**
  - Why needed here: This is the core architecture. Understanding standard message-passing is essential to grasp why the paper introduces a bond-centric graph to overcome its limitations.
  - Quick check question: Can you explain the standard message-passing paradigm in GNNs and identify why it might be insufficient for capturing bond-level features?

- **Concept: Contrastive Learning**
  - Why needed here: This is the primary signal for aligning the dual views. Grasping how the model learns by pulling positive pairs together and pushing negative pairs apart is critical.
  - Quick check question: In this paper's dual-view framework, what constitutes a "positive pair" and a "negative pair" for the contrastive loss?

- **Concept: Epistemic Uncertainty & Evidential Deep Learning**
  - Why needed here: This is the key problem and solution. Understanding why a model should output a distribution over its predictions rather than a point estimate is fundamental.
  - Quick check question: Why is quantifying epistemic (model) uncertainty important for drug discovery, and how does modeling a Beta distribution over the output achieve this?

## Architecture Onboarding

- **Component map:** SMILES -> Molecular Graph Topology Remapping -> Dual GIN Encoders -> Dual-View Contrastive Learning -> Evidential Uncertainty Quantification -> Prediction

- **Critical path:** Graph Construction: SMILES â†’ ($G$, $G_r$) is the most critical preprocessing step. Encoding & Alignment: Both graphs pass through encoders, and their embeddings are pulled together via the contrastive loss. Prediction & Calibration: The embeddings are used to predict Beta parameters, from which a point estimate and uncertainty are derived and optimized via the evidential loss.

- **Design tradeoffs:**
  - Computational Cost vs. Richness: Dual-view architecture doubles graph encoding workload versus the gain in representation power from explicit bond modeling
  - Calibration vs. Accuracy: Loss function balances prediction accuracy with uncertainty penalty, requiring careful tuning to avoid overconfident errors or uniformly high uncertainty
  - Bond Graph Granularity: Fixed representation of bond-interactions as nodes in a new graph determines the granularity of the secondary view

- **Failure signatures:**
  - Representation Collapse: Contrastive loss goes to zero but downstream accuracy is random, indicating all embeddings have collapsed to a single point
  - Overconfidence: Model predicts high probability for wrong class with very low reported uncertainty, indicating evidential calibration failure
  - Unstable Uncertainty: Uncertainty estimates vary wildly across runs, suggesting evidential learning process is not converging properly

- **First 3 experiments:**
  1. Reproduce Ablation Study: Train full model and versions with each key component (MGTR, DVCL, EBUQ) removed to validate individual contributions
  2. Sensitivity Analysis on Lambda: Run parameter sweep on contrastive loss weight ($\lambda$) to observe effect on alignment-prediction balance
  3. OOD Generalization Test: Train on primary dataset and evaluate on out-of-distribution dataset to verify robustness and uncertainty claims, checking if high-uncertainty samples correlate with errors

## Open Questions the Paper Calls Out

### Open Question 1
Can the TrustworthyMS architecture be effectively generalized to predict other ADME properties beyond metabolic stability? The conclusion states that future work will "extend this architecture to broader ADME property prediction," but current experimental validation is restricted to metabolic stability datasets.

### Open Question 2
Can the model's computational efficiency be optimized to handle industrial-scale virtual screening? The authors identify "optimizing its computational efficiency for large-scale virtual screening" as a goal for future work, as the dual-view framework introduces additional computational overhead compared to standard single-view GNNs.

### Open Question 3
How does the Beta-Binomial subjective logic compare to other uncertainty quantification techniques for molecular property prediction? While the paper introduces a specific UQ method, it does not benchmark it against other established uncertainty methods like MC Dropout or Deep Ensembles.

## Limitations

- Bond graph construction complexity introduces significant computational overhead compared to standard single-view GNNs
- Beta-Binomial subjective logic uncertainty quantification lacks validation against alternative uncertainty quantification methods
- Model performance on out-of-distribution data, while improved, still shows room for enhancement in extreme cases

## Confidence

- **High Confidence:** Classification and regression performance metrics (MCC=0.622, P-score=0.833) on benchmark datasets are clearly specified and reproducible
- **Medium Confidence:** General dual-view contrastive learning approach is validated by related work, but specific bond-node remapping technique lacks direct precedent
- **Low Confidence:** Efficacy of Beta-Binomial subjective logic for uncertainty quantification in this specific application is weakly supported by the corpus

## Next Checks

1. Ablation of the Bond Graph: Train and compare a version of the model using only the standard atom-centric graph to isolate the benefit of the MGTR module

2. Uncertainty Calibration Test: Perform a reliability diagram analysis on the out-of-distribution data to verify that reported uncertainty scores are truly calibrated and correlate with prediction error

3. Computational Complexity Audit: Profile memory and time cost of the dual-view architecture versus a strong single-view baseline to quantify the practical tradeoff