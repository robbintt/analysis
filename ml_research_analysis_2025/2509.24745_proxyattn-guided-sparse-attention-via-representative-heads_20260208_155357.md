---
ver: rpa2
title: 'ProxyAttn: Guided Sparse Attention via Representative Heads'
arxiv_id: '2509.24745'
source_url: https://arxiv.org/abs/2509.24745
tags:
- attention
- heads
- sparse
- arxiv
- proxyattn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the quadratic complexity bottleneck of attention
  mechanisms in large language models for long-text tasks. It proposes ProxyAttn,
  a training-free sparse attention algorithm that achieves fine-grained block importance
  estimation by leveraging the similarity among attention heads.
---

# ProxyAttn: Guided Sparse Attention via Representative Heads

## Quick Facts
- arXiv ID: 2509.24745
- Source URL: https://arxiv.org/abs/2509.24745
- Reference count: 29
- Primary result: Training-free sparse attention algorithm achieving up to 10.3x attention acceleration and 2.4x prefilling speedup on long-context tasks

## Executive Summary
ProxyAttn addresses the quadratic complexity bottleneck of attention mechanisms in large language models for long-text tasks. The method proposes a training-free sparse attention algorithm that achieves fine-grained block importance estimation by leveraging the similarity among attention heads. By using a small number of representative proxy heads to approximate attention scores for all heads, combined with a block-aware dynamic budget allocation that accounts for varying sparsity across heads, ProxyAttn significantly accelerates long-context processing while maintaining competitive performance.

## Method Summary
ProxyAttn is a training-free sparse attention algorithm that partitions attention heads into groups and computes attention scores using a representative proxy head formed by average pooling queries and keys within each group. The method applies max pooling to obtain block-level importance scores and uses these scores to guide sparse attention. To account for varying sparsity among heads, ProxyAttn proposes a block-aware dynamic budget estimation method that estimates the sparsity of each head by computing attention scores for the last query block and determining the minimum number of top-scoring blocks required to reach a cumulative probability threshold. This yields a per-head budget that is used for mask generation during sparse attention computation.

## Key Results
- Achieves up to 10.3x attention speedup and 2.4x prefilling speedup on long-context tasks
- Maintains competitive performance on RULER, InfiniteBench, and LongBench-v2 benchmarks
- Outperforms existing coarse-grained sparse attention methods while being training-free
- Effective on both Llama3.1-8B and Qwen2.5-7B models with different GQA configurations

## Why This Works (Mechanism)

### Mechanism 1: Head-Level Score Approximation via Proxy Heads
Using a small number of pooled "proxy" attention heads can accurately estimate token-level importance scores for all heads by exploiting the consistency in token focus across attention heads, particularly in long texts where multiple heads focus on large intersections of tokens.

### Mechanism 2: Block-Aware Dynamic Budget Allocation
Assigning a unique, dynamically calculated token budget to each attention head improves performance over static allocation by recognizing that the primary difference between attention heads is their sparsity (how few tokens they attend to), not their overall focus.

### Mechanism 3: Efficient Fine-Grained Estimation
Compressing along the head dimension enables more accurate (fine-grained) block importance estimation at lower computational cost than compressing along the sequence dimension, allowing token-level dot-products for the proxy head while maintaining efficiency.

## Foundational Learning

- **Concept: Block Sparse Attention**
  - Why needed here: This is the core optimization technique ProxyAttn implements
  - Quick check: How does processing attention in blocks enable hardware efficiency compared to processing individual sparse elements?

- **Concept: Group-Query Attention (GQA)**
  - Why needed here: The paper explicitly states it aligns its head grouping granularity with GQA key groups
  - Quick check: In GQA, multiple query heads share a single key head. How does this affect how you group heads for the proxy attention calculation?

- **Concept: Prefilling vs. Decoding**
  - Why needed here: The paper targets the prefilling stage for optimization and uses full attention for decoding
  - Quick check: Why is the prefilling stage the primary target for optimization in long-context inference, compared to the decoding stage?

## Architecture Onboarding

- **Component map:**
  1. Head Pooling & Selection: Partition heads into groups and create proxy Q/K via average pooling
  2. Proxy Score Calculator: Computes attention scores for proxy heads with stride, applies softmax and max pooling
  3. Dynamic Budget Estimator: Computes per-head budget using last query block and cumulative probability threshold
  4. Mask Generator: Generates binary sparse mask using unified scores and per-head budget
  5. Sparse Attention Kernel: Executes attention computation using generated mask

- **Critical path:**
  1. Dynamic Budget Estimator must run first to determine budgets
  2. Proxy Score Calculator provides ranking for mask generation
  3. Mask Generator combines ranking and budgets to create final mask
  4. Sparse Attention Kernel executes computation

- **Design tradeoffs:**
  - Number of Proxy Heads (g): Higher g increases accuracy but also overhead; paper finds g=1 for Llama and g=4 for Qwen sufficient
  - Stride: Larger stride reduces estimation cost but may harm accuracy; paper uses stride=4
  - Threshold (γ): Lower threshold increases sparsity and speed but risks performance loss; paper uses 0.90 or 0.95

- **Failure signatures:**
  - Performance collapse at high sparsity indicates dynamic budget estimator may be miscalibrated
  - No speedup suggests estimation overhead or kernel implementation is inefficient
  - Shape mismatch errors occur if GQA grouping alignment is incorrect

- **First 3 experiments:**
  1. Baseline Overhead Check: Measure estimation latency on 128K sequence to confirm <10% of full attention time
  2. Hyperparameter Sensitivity Sweep: Test γ in [0.90, 0.95, 0.99] and g in [1, 2, 4] to find Pareto frontier
  3. Ablation on Dynamic Budgeting: Compare ProxyAttn with dynamic budgeting against fixed budget version

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can proxy head-based sparse attention be effectively extended to the decoding phase of LLMs, where KV cache management becomes the primary bottleneck?
- Basis in paper: The conclusion states "We will further investigate the utility of the proxy head specifically within the decoding phase of LLMs."

### Open Question 2
- Question: What is the precise relationship between GQA grouping ratios and the optimal number of proxy heads required to maintain performance?
- Basis in paper: The paper observes that Llama3.1-8B needs only 1 proxy head while Qwen2.5-7B needs approximately 4 proxy heads, but only hypothesizes this connection.

### Open Question 3
- Question: Does the observed head similarity pattern hold across more diverse model architectures (e.g., different attention mechanisms like MLA, attention-free architectures) and training objectives?
- Basis in paper: Experiments are limited to two model families with similar transformer architectures, and the authors note the similarity observation "corroborates the findings" of other work without establishing architectural boundaries.

## Limitations
- The head-similarity assumption may not hold across diverse model architectures and tasks
- Dynamic budget estimation relies on the last query block being representative of the full sequence's sparsity pattern
- Performance gains may be contingent on specific GQA implementation details

## Confidence
- **High Confidence**: Empirical performance claims (10.3x acceleration, 2.4x prefilling speedup, competitive RULER scores) are well-supported by experimental results
- **Medium Confidence**: Head-level score approximation mechanism is theoretically sound but requires validation across diverse model families
- **Low Confidence**: Efficiency claim that head-dimension compression is superior to sequence-dimension compression lacks direct comparative evidence

## Next Checks
1. **Head Similarity Validation**: Conduct quantitative analysis of attention head similarity across different model families to validate the core assumption that proxy heads can accurately approximate individual head importance scores.

2. **Dynamic Budget Estimator Robustness**: Test the dynamic budget estimation mechanism on sequences with known heterogeneous attention patterns to verify that the last query block is representative of the full sequence.

3. **Architecture-Agnostic Performance**: Evaluate ProxyAttn on a non-GQA model to determine whether performance gains generalize to broader attention architectures beyond the specific model families tested.