---
ver: rpa2
title: Learning Parameterized Skills from Demonstrations
arxiv_id: '2510.24095'
source_url: https://arxiv.org/abs/2510.24095
tags:
- skills
- continuous
- skill
- discrete
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DEPS learns parameterized skills from multitask expert demonstrations
  using a three-level hierarchy: a discrete skill selector, a continuous parameter
  selector, and a low-level subpolicy. The method employs temporal variational inference
  with information-theoretic regularization and an aggressive 1D state compression
  bottleneck to ensure temporally extended, semantically meaningful, and adaptable
  skills.'
---

# Learning Parameterized Skills from Demonstrations

## Quick Facts
- **arXiv ID:** 2510.24095
- **Source URL:** https://arxiv.org/abs/2510.24095
- **Reference count:** 40
- **Key outcome:** DEPS learns parameterized skills from multitask expert demonstrations using a three-level hierarchy, achieving up to 0.34 mean success rate on out-of-distribution tasks compared to 0.10 for PRISE.

## Executive Summary
DEPS (DEep Parameterized Skills) learns reusable, interpretable parameterized skills from multitask demonstrations by combining discrete skill selection with continuous parameter modulation. The method employs temporal variational inference with information-theoretic regularization and an aggressive 1D state compression bottleneck to ensure temporally extended, semantically meaningful, and adaptable skills. Evaluation on LIBERO and MetaWorld benchmarks shows DEPS significantly outperforms multitask behavior cloning and skill learning baselines, achieving strong generalization to out-of-distribution tasks while discovering intuitive skills like object grasping with continuous parameters defining grasp locations.

## Method Summary
DEPS uses a three-level hierarchy: a discrete skill selector, a continuous parameter selector, and a low-level subpolicy. The method employs temporal variational inference with information-theoretic regularization and an aggressive 1D state compression bottleneck to ensure temporally extended, semantically meaningful, and adaptable skills. The subpolicy receives only a scalar compressed state derived from robot proprioception, forcing the model to use discrete skills and continuous parameters to resolve ambiguity. Evaluation shows DEPS significantly outperforms multitask behavior cloning and skill learning baselines, achieving up to 0.34 mean success rate on out-of-distribution tasks.

## Key Results
- DEPS achieves 0.34 mean success rate on LIBERO-OOD compared to 0.10 for PRISE and 0.15 for continuous-only skill learning
- The 1D state compression ablation shows 0.34 mean success vs 0.13 (2D), 0.05 (3D), and 0.19 (uncompressed)
- DEPS discovers interpretable skills like object grasping where continuous parameters define grasp locations, with smooth monotonic variation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggressive state compression to 1D forces latent variables to encode meaningful skill information, improving generalization.
- **Mechanism:** The subpolicy receives only a scalar state s′_t = tanh(w(k_t,z_t) · s_proj_t + b(k_t,z_t)) derived from robot proprioception. This bottleneck makes the compressed state alone insufficient to determine actions, compelling the model to use discrete skills k_t and continuous parameters z_t to resolve ambiguity.
- **Core assumption:** Skills correspond to families of trajectories lying on low-dimensional manifolds, where a 1D "progress index" suffices for within-skill execution.
- **Evidence anchors:** [Section 4.2] states this forces k_t and z_t to encode task-relevant information; [Section 5.5/Appendix F] shows 1D achieves 0.34 vs 0.13 (2D), 0.05 (3D), and 0.19 (uncompressed) on LIBERO-OOD.
- **Break condition:** If proprioceptive state is unavailable or highly non-informative, the projection may fail to provide a useful trajectory index.

### Mechanism 2
- **Claim:** Temporal variational inference with structured latent variables learns interpretable, temporally-extended skills from demonstrations.
- **Mechanism:** A bidirectional GRU variational network q(κ, ζ|τ, l) infers posterior distributions over skills given full trajectories. The training objective maximizes a variational lower bound with KL divergence terms that regularize the variational posterior toward autoregressive policies π_K and π_Z. Continuous parameters are predicted per discrete skill instance (not per timestep), preventing rapid oscillations.
- **Core assumption:** Demonstration trajectories naturally decompose into sequences of discrete skills with stable continuous parameterizations within each skill invocation.
- **Evidence anchors:** [Section 4.1] derives the variational lower bound; [Section 5.6] shows DEPS discovers intuitive parameterized skills like grasping, moving, and releasing objects.
- **Break condition:** If demonstrations lack temporal structure or require rapid skill switching (sub-second), per-skill parameter prediction may be too coarse.

### Mechanism 3
- **Claim:** Parameterized skills (discrete + continuous) outperform purely discrete or purely continuous skill representations for generalization.
- **Mechanism:** Discrete skills provide structured, interpretable abstractions, while continuous parameters enable flexible modulation. This two-level hierarchy combines interpretability with expressiveness: the discrete skill acts as a "selector" indexing into disjoint continuous parameter subspaces.
- **Core assumption:** Skills share continuous parameter semantics across tasks (e.g., "grasp location" meaning is consistent regardless of object/environment).
- **Evidence anchors:** [Section 1] emphasizes structured and interpretable nature; [Appendix H/Table 8] shows DEPS achieves 0.34 mean success vs discrete-only (0.06) and continuous-only (0.15) on LIBERO-OOD.
- **Break condition:** If continuous parameters encode task/environment-specific information rather than skill-relevant information, generalization degrades.

## Foundational Learning

- **Concept: Variational Inference (VI)**
  - Why needed here: DEPS uses VI to approximate intractable marginalization over skill sequences. Understanding ELBO, KL divergence, and the tradeoff between reconstruction accuracy and latent regularization is essential.
  - Quick check question: Can you explain why maximizing the ELBO is equivalent to minimizing KL(q||p) between approximate and true posteriors?

- **Concept: Options/Skills Framework**
  - Why needed here: DEPS builds on the options framework for temporal abstraction. Understanding how skills are temporally extended, how they terminate, and how they compose is foundational.
  - Quick check question: What is the difference between a primitive action and an option (skill) in hierarchical RL?

- **Concept: Information Bottlenecks**
  - Why needed here: The 1D compression is an intentional information bottleneck. Understanding how constrained information flow forces useful representations is critical for debugging skill quality.
  - Quick check question: Why would reducing observation dimensionality improve generalization to unseen tasks?

## Architecture Onboarding

- **Component map:** Variational network q → Discrete policy π_K → Continuous policy π_Z → Compression MLP f_compress → Subpolicy π_A
- **Critical path:**
  1. Pretraining: Sample trajectories → variational network infers (k, z) → compute ELBO loss with KL terms + norm penalty
  2. Finetuning: Sample (k, z) from autoregressive policies π_K, π_Z (not q) → optimize same ELBO
  3. Inference: π_K → π_Z → compress state → π_A → action
- **Design tradeoffs:**
  - **1D vs higher-dimensional compression**: 1D maximizes generalization (0.34) but may hurt in-distribution long-horizon tasks (LIBERO-10: 0.08 vs BC 0.07)
  - **Per-skill vs per-timestep continuous parameters**: Per-skill prevents overfitting but may limit expressiveness for long trajectories
  - **Number of discrete skills K=10**: Paper notes K=20 improves results (0.47 vs 0.34), suggesting hyperparameter sensitivity
- **Failure signatures:**
  - **Collapse to single skill**: Including k_t as direct input to subpolicy can cause this
  - **Continuous parameter overfitting**: Large-magnitude parameters indicate memorization; mitigate with Skill Parameter Norm Penalty (λ_norm=0.1)
  - **No skill segmentation**: If discrete skills change every timestep, reduce KL weight or check demonstration quality
- **First 3 experiments:**
  1. **Reproduce 1D compression ablation**: Train DEPS with 1D, 2D, 3D, and no compression on LIBERO-OOD. Verify 1D > 2D > 3D and all > uncompressed.
  2. **Visualize skill segmentation**: For 5-10 demonstration trajectories, plot discrete skill assignments over time. Check for intuitive segmentation (grasp → move → release).
  3. **Continuous parameter sensitivity analysis**: For a fixed discrete skill (e.g., grasp), sweep continuous parameter z and visualize resulting grasp locations. Verify smooth, monotonic variation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can DEPS effectively learn parameterized skills in environments where explicit low-dimensional proprioceptive robot states are unavailable?
- **Basis in paper:** [explicit] Appendix J states the method relies on proprioceptive state for compression and notes, "we leave an evaluation of this to future work."
- **Why unresolved:** The current compression strategy projects robot state vectors into a 1D index; it is untested whether raw visual observations can replace this without destabilizing the latent space.
- **What evidence would resolve it:** Successful benchmark performance on environments providing only image observations, without access to ground-truth robot state vectors.

### Open Question 2
- **Question:** Can a residual policy with access to full observations effectively close the performance gap on long-horizon, in-distribution tasks?
- **Basis in paper:** [explicit] Appendix J suggests this remedy for the trade-off between generalization and accuracy: "We leave a more rigorous evaluation of this potential shortcoming and its remedies to future work."
- **Why unresolved:** DEPS currently underperforms on LIBERO-10 (in-distribution) likely due to information loss from aggressive state compression.
- **What evidence would resolve it:** Experiments showing that a residual policy conditioned on full observations can restore high success rates on long-horizon tasks while retaining OOD generalization.

### Open Question 3
- **Question:** Can the optimal number of discrete skills (K) be determined automatically rather than treated as a fixed hyperparameter?
- **Basis in paper:** [inferred] Appendix G (Table 7) reveals that increasing the skill limit K from 10 to 20 significantly boosts mean success rates (0.34 to 0.47), indicating high sensitivity to this manually tuned parameter.
- **Why unresolved:** The method currently requires the user to specify an upper bound on skills, risking under- or over-segmentation of the behavior space.
- **What evidence would resolve it:** A non-parametric extension or adaptive mechanism that identifies the necessary number of skills dynamically during pretraining.

## Limitations
- The 1D state compression mechanism lacks extensive theoretical grounding and may not hold for more complex manipulation tasks with irregular skill boundaries
- Performance on pure vision-based control scenarios is untested, as the method relies heavily on robot proprioception for compression
- High hyperparameter sensitivity to the number of discrete skills (K) suggests the architecture may not scale robustly to more diverse skill sets

## Confidence

- **High confidence:** The variational inference framework and its application to skill learning is well-established, with the ELBO derivation following standard practice. The ablation showing 1D compression outperforms higher dimensions is robust across multiple runs.
- **Medium confidence:** The interpretability claims rely on visual inspection of skill segmentation and parameter sweeps, which are somewhat subjective. The generalization claims on out-of-distribution tasks are strong but based on limited test scenarios.
- **Low confidence:** The assertion that discrete+continuous parameterization is universally superior lacks comparison against other hybrid representations or learned gating mechanisms that could achieve similar results.

## Next Checks

1. Test DEPS on vision-only control scenarios by removing proprioception and using image-based compression. Measure skill quality degradation and identify failure modes.
2. Conduct systematic hyperparameter sweeps for K (discrete skills) and compression dimension across multiple task families to quantify robustness and identify scaling limits.
3. Compare DEPS against alternative hybrid skill representations such as learned gating networks or adaptive discretization schemes to validate the claimed superiority of the discrete+continuous design.