---
ver: rpa2
title: 'SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps
  in Materials Foundation Models'
arxiv_id: '2601.22312'
source_url: https://arxiv.org/abs/2601.22312
tags:
- materials
- across
- reasoning
- consistency
- structural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models for materials science often fail under geometric
  scale shifts, producing predictions that violate physical invariants. SCALAR is
  a benchmark that evaluates scale generalization, structural hallucination, consistency,
  and reasoning in materials foundation models using controlled supercell expansion
  and truncation across 100,000 structures from DFT-validated unit cells.
---

# SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models

## Quick Facts
- arXiv ID: 2601.22312
- Source URL: https://arxiv.org/abs/2601.22312
- Reference count: 13
- Large language models for materials science often fail under geometric scale shifts, producing predictions that violate physical invariants.

## Executive Summary
Materials foundation models frequently fail under geometric scale shifts, generating predictions that violate physical invariants. SCALAR is a benchmark that evaluates scale generalization, structural hallucination, consistency, and reasoning in materials foundation models using controlled supercell expansion and truncation across 100,000 structures from DFT-validated unit cells. Three tasks are defined: CIF-to-property prediction, a physics-grounded Chain-of-Thought variant, and inverse retrieval from properties to crystal candidates. Evaluation metrics capture numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse models show large, model-dependent shifts under explicit reasoning—often reducing hallucination and error but destabilizing consistency or validity. Few-shot and CoT prompting yield heterogeneous, non-guaranteed improvements. Results demonstrate that geometric scale generalization cannot be inferred from accuracy alone.

## Method Summary
SCALAR creates a controlled dataset of nanoparticle structures by expanding DFT-validated unit cells to 20×20×20 supercells, then carving spherical nanoparticles at radii R∈{10,...,30}Å. This yields 25–18,123 atoms per structure spanning the transition from surface-dominated clusters to near-bulk behavior. The benchmark evaluates three tasks: CIF-to-property prediction, physics-grounded Chain-of-Thought reasoning, and inverse retrieval from properties to crystal candidates. Models are tested under zero-shot, few-shot, and CoT prompting. Evaluation metrics include MAE for accuracy, hallucination rate (constraint violations + self-consistency std across N=5 queries), consistency (std), reasoning quality (Spearman ρ on scale trends), parse error rate, and retrieval regret. The dataset uses SO(3) rotation sampling with geodesic spacing to create ID/OOD splits that test generalization beyond memorized structures.

## Key Results
- Scale generalization failures manifest as hallucinations (constraint violations) and consistency breakdowns across models
- CoT prompting reduces error for some models but destabilizes consistency and reasoning for others
- Geometry-native GNNs show 3-4× error inflation under OOD scales despite geometric equivariance
- Few-shot and CoT prompting yield heterogeneous, non-guaranteed improvements
- Inverse retrieval performance depends on k candidates and physical distance metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Controlled geometric scale transformations isolate physical reasoning failures that accuracy metrics miss
- Mechanism: Supercell replication (20×20×20) followed by spherical carving at radii R∈{10,...,30}Å creates paired unit-cell → nanoparticle representations spanning 4–18,123 atoms, enabling measurement of whether models preserve invariants (density bounds, composition ratios) under scale shifts
- Core assumption: Materials models should maintain crystallographic identity and physical constraints across scale transformations of the same underlying structure
- Evidence anchors:
  - [abstract] "controlled supercell expansion and truncation across 100,000 structures from DFT-validated unit cells"
  - [section 3.1] "yields a controlled family of clusters per composition, spanning the transition from strongly surface-dominated (small R) to near-bulk behavior (large R)"
  - [corpus] "Stress-Testing Multimodal Foundation Models for Crystallographic Reasoning" addresses similar spatial generalization challenges
- Break condition: If models achieve low error via pattern matching without physical reasoning, hallucination and consistency metrics will degrade under OOD radii despite stable accuracy

### Mechanism 2
- Claim: Hallucination in scientific domains can be operationalized as constraint violations and cross-query inconsistency
- Mechanism: The benchmark quantifies hallucination via (1) constraint violations (negative densities, volumes exceeding theoretical bounds) and (2) self-consistency failures across N=5 repeated queries per sample, separating confident-but-wrong predictions from mere inaccuracy
- Core assumption: Scientific hallucinations differ from linguistic ones—they appear locally plausible but violate global physical invariants under distribution shifts
- Evidence anchors:
  - [abstract] "producing predictions that violate physical invariants"
  - [section 1] "confident predictions that violate underlying physical invariants, particularly under distribution shifts induced by changes in structural scale"
  - [corpus] Weak direct support; "How Large Language Models are Designed to Hallucinate" addresses structural causes but not physical constraints
- Break condition: If hallucination metrics conflate formatting errors with physical reasoning failures, diagnostic signal is diluted

### Mechanism 3
- Claim: Chain-of-Thought reasoning trades off error reduction against consistency destabilization
- Mechanism: CoT prompting is evaluated via Spearman rank correlation between predicted property deltas across radii and ground-truth changes, revealing that better explanations do not guarantee stable predictions
- Core assumption: Reasoning quality (trend alignment) and output stability (cross-query consistency) are partially orthogonal capabilities
- Evidence anchors:
  - [abstract] "often reducing hallucination and error but destabilizing consistency or validity"
  - [section 4.2] "better intermediate explanations do not reliably translate into more stable or physically consistent predictions"
  - [corpus] "Eidoku: A Neuro-Symbolic Verification Gate for LLM Reasoning" supports reasoning-consistency decoupling
- Break condition: If CoT improvements stem from output formatting rather than physical understanding, gains will not transfer to novel structures

## Foundational Learning

- Concept: Crystallographic Representations (CIF Format)
  - Why needed here: Understanding unit cells as periodic abstractions defined by lattice parameters, atomic positions, and space group symmetry is prerequisite to interpreting the "infinite crystal → finite nanoparticle" transformation
  - Quick check question: Given a unit cell with lattice constant a=4Å, why does a 20×20×20 supercell yield dimension 80Å, and why might spherical carving at R=10Å miss atoms near cell boundaries?

- Concept: Intensive vs. Extensive Properties
  - Why needed here: Density (intensive) should remain approximately scale-invariant while atom count (extensive) scales as ~R³; conflating these leads to misinterpreting error patterns
  - Quick check question: If nearest-neighbor distance changes <3% across scales but density errors exceed 30%, which property type better diagnoses physical reasoning failures?

- Concept: SO(3) Rotation Sampling with Geodesic Spacing
  - Why needed here: ID/OOD splits are defined by quaternion-based rotation separation (εID=8°, εOD=8° margins), not random sampling—understanding this is essential to interpret generalization results
  - Quick check question: Why does enforcing minimum geodesic distance between rotations ensure that ID and OOD splits probe generalization rather than memorization?

## Architecture Onboarding

- Component map:
  Dataset Pipeline -> Evaluation Tasks -> Metrics Layer -> Baselines
  Phase I (supercell expansion + spherical carving at R∈{10–30}Å) -> Phase II (SO(3) rotation sampling via unit quaternions with geodesic spacing) -> Phase III (split-aware partitioning: SID={13,15,17,20,24,27}Å, SOOD={10,11,29,30}Å) -> CIF→property prediction (forward) -> physics-grounded CoT reasoning (trend analysis) -> inverse retrieval (k-candidate selection from target properties) -> Numeric error (MAE) -> hallucination (constraint violations + self-consistency std across N=5) -> reasoning quality (Spearman ρ + delta penalty) -> parse validity -> retrieval regret (normalized L2 distance to optimal candidate) -> Analytical scaling laws (intensive properties: ~2% error; extensive: 30–35%) -> geometry-native GNNs (SchNet, CGCNN, E(3)NN—show 3–4× error inflation under OOD scales)

- Critical path:
  1. Load DFT-validated unit cell CIFs from hydrogen-storage materials
  2. Expand to 20×20×20 supercells (Li ≥ 2Rmax + Δ for Rmax=30Å)
  3. Carve spherical nanoparticles, yielding 25–18,123 atoms per structure
  4. Apply split-specific rotations with exclusion constraints (ID: 8° margin from training; OOD: 8° from training ∪ ID)
  5. Prompt models for property predictions (density, volume, nearest-neighbor distance)
  6. Compute hallucination/consistency/reasoning metrics; compare ID vs. OOD degradation

- Design tradeoffs:
  - No surface relaxation: Isolates pure scale effects but excludes realistic surface reconstructions (limits ecological validity for nanoparticles)
  - Rotation splits probe parsing, not geometry: Target properties are rotation-invariant; rotation separation tests text-processing robustness (acknowledged limitation)
  - Hydrogen-dominated dataset (55% of atoms): Relevant for energy storage but may constrain conclusions to hydride chemistries
  - Log-transformation of metrics: Enables cross-metric comparison but obscures absolute error magnitudes

- Failure signatures:
  - High accuracy + high hallucination: Memorization without constraint enforcement (e.g., negative densities)
  - CoT reduces errors but inflates consistency std: Reasoning improves averages while destabilizing individual predictions
  - Parse errors >50% (LLaMA 4 Maverick: 32–96%): Formatting failures dominate signal
  - OOD >> ID gaps (E(3)NN: 106%→313% MAPE): Scale generalization failure in geometry-native models

- First 3 experiments:
  1. Zero-shot vs. few-shot scaling comparison: Test 0/1/3-shot prompting on high-accuracy models (Gemini, GPT-5) vs. high-consistency models (Claude) under OOD radii to identify whether in-context examples reduce hallucination without sacrificing stability
  2. CoT tradeoff characterization: Measure error reduction vs. consistency degradation per model; identify architectures where explicit reasoning is net-beneficial vs. destabilizing
  3. Inverse retrieval difficulty sweep: Evaluate top-1 accuracy and physical regret at k∈{3,5,10} candidates; test whether low physical distance (consistent predictions) correlates with correct retrieval

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What architectural or training factors determine whether Chain-of-Thought prompting improves versus degrades physical reasoning in materials foundation models?
- Basis in paper: [explicit] "Few-shot and CoT prompting yield heterogeneous, non-guaranteed improvements" and "CoT is not a monotonic or universally beneficial intervention."
- Why unresolved: The paper documents that CoT reduces errors for some models but degrades reasoning and consistency for others, without identifying the mechanistic causes of this variability.
- What evidence would resolve it: Controlled ablation studies varying model scale, pretraining data composition, and prompting formats, coupled with attention analysis to identify where CoT reasoning diverges from physical constraints.

### Open Question 2
- Question: Do the geometric scale generalization failures observed in SCALAR persist when properties are computed via quantum-mechanical methods rather than classical geometric derivations?
- Basis in paper: [explicit] Limitations section states "evaluated properties are derived from classical geometric computations and do not reflect quantum-mechanical observables."
- Why unresolved: The benchmark isolates scale effects using deterministic geometric calculations, but real materials exhibit quantum confinement, surface reconstruction, and electronic structure changes that may introduce different failure modes.
- What evidence would resolve it: Extension of SCALAR with DFT-computed properties (e.g., band gaps, formation energies) for the same nanoparticle structures, enabling direct comparison of model performance on derived versus quantum-mechanical targets.

### Open Question 3
- Question: Can the scale generalization failures demonstrated in both LLMs and geometry-native models be attributed to fundamental extrapolation limits of supervised learning, or do they reflect remediable architectural gaps?
- Basis in paper: [explicit] "Geometry-native graph neural networks...exhibit substantial scale-dependent degradation" with CGCNN degrading from 185% to 464% MAPE on atom count.
- Why unresolved: The paper shows that even models with geometric inductive biases fail under scale shifts, but does not determine whether this reflects insufficient equivariance, training distribution coverage, or a deeper limitation in how neural architectures extrapolate beyond training supports.
- What evidence would resolve it: Systematic evaluation of scale-aware architectures (e.g., size-augmented GNNs, physics-informed constraints) trained on multi-scale data with explicit extrapolation bounds, compared against theoretical generalization guarantees.

### Open Question 4
- Question: To what extent do the benchmark conclusions generalize beyond hydrogen-rich chemistries to broader materials classes?
- Basis in paper: [explicit] Limitations section notes "the dataset includes a substantial fraction of hydrogen-containing materials...and generalization of conclusions across hydride versus non-hydride chemistries requires further investigation."
- Why unresolved: Hydrogen-dominated structures (55.07% of atoms) may induce specific failure patterns related to light-element handling or hydrogen-storage-optimized training data that do not transfer to transition-metal oxides, semiconductors, or other industrially relevant chemistries.
- What evidence would resolve it: Construction of matched SCALAR-style benchmarks for non-hydride material families (e.g., perovskites, intermetallics, oxides) with identical evaluation protocols, enabling controlled comparison of scale generalization across chemical regimes.

## Limitations
- Hydrogen-dominated dataset (55% of atoms) may overrepresent hydride chemistry
- Surface relaxation excluded to isolate scale effects but removes realistic nanoparticle physics
- Rotation-based ID/OOD split tests parsing robustness rather than geometric generalization
- Benchmark does not address composition-structure-property relationship learning

## Confidence
- High confidence: Systematic hallucination and consistency failures across diverse models (LLaMA, Gemini, GPT, Claude, E(3)NN)
- Medium confidence: CoT tradeoff patterns depend heavily on model architecture and training
- Low confidence: Physical reasoning quality based on trend correlation alone may reflect formatting rather than understanding

## Next Checks
1. Cross-chemistry validation: Apply SCALAR to non-hydride materials (oxides, metals, organics) to test whether hallucination patterns generalize beyond hydrogen storage chemistry
2. Surface physics integration: Incorporate surface relaxation effects and test whether models can learn physically accurate nanoparticle geometries when provided with relaxed structures
3. Multi-task generalization: Extend evaluation to composition-to-structure and property-to-structure tasks to assess whether scale generalization failures reflect broader limitations in learning structure-property relationships