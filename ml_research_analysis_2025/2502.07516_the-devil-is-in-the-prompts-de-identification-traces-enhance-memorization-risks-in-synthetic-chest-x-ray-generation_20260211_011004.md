---
ver: rpa2
title: 'The Devil is in the Prompts: De-Identification Traces Enhance Memorization
  Risks in Synthetic Chest X-Ray Generation'
arxiv_id: '2502.07516'
source_url: https://arxiv.org/abs/2502.07516
tags:
- memorization
- prompts
- diffusion
- data
- de-identification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates training data memorization in text-to-image
  diffusion models for synthetic chest X-ray generation using the MIMIC-CXR dataset.
  The authors conduct the first systematic analysis of memorization at both prompt
  and token levels, revealing that de-identification markers ("") are the most memorized
  elements.
---

# The Devil is in the Prompts: De-Identification Traces Enhance Memorization Risks in Synthetic Chest X-Ray Generation

## Quick Facts
- arXiv ID: 2502.07516
- Source URL: https://arxiv.org/abs/2502.07516
- Reference count: 31
- One-line primary result: De-identification markers ("___") are the most memorized elements in synthetic chest X-ray generation, and inference-time mitigation strategies fail to reduce memorization risks.

## Executive Summary
This study investigates training data memorization in text-to-image diffusion models for synthetic chest X-ray generation using the MIMIC-CXR dataset. The authors conduct the first systematic analysis of memorization at both prompt and token levels, revealing that de-identification markers ("___") are the most memorized elements. Their analysis shows that while these markers are intended to protect patient privacy, they actually create spurious correlations that enhance memorization risks. The study finds that existing inference-time mitigation strategies (random word addition, random number addition, and complete removal) are ineffective at reducing memorization of these markers, with average L2 distances between generated samples remaining nearly unchanged (0.38 original vs. 0.42-0.45 with mitigation). The authors propose actionable recommendations including using randomized de-identification markers and recaptioning datasets to enhance privacy preservation. The work highlights a critical flaw in standard anonymization practices and provides a foundation for developing better memorization mitigation techniques.

## Method Summary
The study uses the MIMIC-CXR dataset (filtered subset of 110K samples; 21,373 unique prompts contain de-identification markers) to analyze training data memorization in text-to-image diffusion models. The authors employ the RadEdit model with biomedical text encoder (BiomedCLIP) and SDXL VAE to compute a memorization detection score (dmem) that measures the L2 distance between text-conditional and unconditional noise predictions across all denoising timesteps. They identify the top 1 percentile of prompts as memorized and perform token-level analysis to determine which tokens contribute most to memorization. The study tests three inference-time mitigation strategies (Random Word Addition, Random Number Addition, and complete removal of de-identification markers) by computing mean L2 distances between 50 generated samples with different seeds.

## Key Results
- De-identification markers ("___") are the most memorized elements in synthetic chest X-ray generation
- Existing inference-time mitigation strategies (random word addition, random number addition, and complete removal) are ineffective at reducing memorization of these markers
- Average L2 distances between generated samples remain nearly unchanged with mitigation (0.38 original vs. 0.42-0.45 with mitigation)
- Memorization creates spurious correlations where the model learns to associate de-identification markers with specific image features

## Why This Works (Mechanism)

### Mechanism 1: Text-Conditional Noise as Memorization Signal
Memorized prompts cause the model to overfit to a fixed denoising track, making predictions predominantly reliant on text-conditioning rather than initial noise. For non-memorized prompts, generated images vary with different seeds. For memorized prompts, the model ignores initial noise and produces nearly identical outputs across seeds. The memorization detection metric dmem computes the L2 distance between noise predictions with and without text conditioning across all timesteps.

### Mechanism 2: Spurious Correlation Through Unique, Frequent Tokens
Uniform de-identification markers create retrieval-key behavior that triggers memorization of associated training samples. The "___" token is (1) lexically unique within MIMIC-CXR's corpus, and (2) appears in 21,373 unique prompts. This combination creates a spurious correlation pathway where the model learns to associate the marker with specific image features, enabling retrieval of particular training samples.

### Mechanism 3: Inference-Time Mitigation Failure via Learned Representations
The model has already encoded the spurious correlation during training. Removing or replacing tokens at inference modifies the prompt embedding but does not affect the model's internal representation pathways that were established during training. The near-identical L2 distances (0.38 vs. 0.42-0.45) confirm the intervention cannot decouple the learned association.

## Foundational Learning

- **Diffusion Model Denoising Trajectories**: Understanding that memorized prompts force the model onto a fixed denoising path independent of initial noise is essential for interpreting the detection metric.
  - Quick check: If you generate 10 images with the same prompt but different seeds, and all outputs are nearly identical, what does this indicate about the model's relationship with that prompt?

- **Text-Conditional vs. Unconditional Guidance**: The dmem metric measures the difference between text-guided and unguided noise predictions; understanding this contrast is necessary to interpret memorization scores.
  - Quick check: Why does a high dmem value suggest stronger memorization rather than just stronger text-guidance?

- **Spurious Correlations in Training Data**: The core finding is that privacy-protecting markers inadvertently create spurious patterns the model exploits; understanding this concept helps identify similar risks in other datasets.
  - Quick check: If a dataset uses "[REDACTED]" uniformly across all samples where names were removed, could this create a similar memorization risk?

## Architecture Onboarding

- **Component map**: Text Encoder (TE) -> VAE (VE) -> U-Net (εθ) -> dmem computation
- **Critical path**:
  1. Extract unique prompts from dataset
  2. For each prompt, compute text-conditional noise at each denoising timestep
  3. Compute dmem = (1/T) × Σ ||εθ(xt, ep) - εθ(xt, e∅)||²
  4. Rank prompts by dmem score; top 1% flagged as memorized
  5. For token-level analysis, decompose contribution per token within memorized prompts

- **Design tradeoffs**:
  - Using RadEdit (trained on MIMIC-CXR) ensures domain-relevant memorization detection but limits generalization to other datasets
  - Top 1% threshold is arbitrary; could miss moderately memorized samples or over-flag edge cases
  - Inference-time mitigation tested only three strategies; other approaches (e.g., prompt paraphrasing, temperature scaling) remain unexplored

- **Failure signatures**:
  - L2 distance between generated samples < 0.5 despite mitigation attempts → memorization not resolved
  - Identical outputs across 10+ different seeds for the same prompt → high confidence memorization
  - dmem scores showing heavy-tailed distribution with clear separation between top percentile and rest → confirms memorization is concentrated in specific prompts

- **First 3 experiments**:
  1. Reproduce dmem distribution on a held-out subset of MIMIC-CXR to validate the memorization detection pipeline before applying to full dataset
  2. Test whether randomized de-identification markers (e.g., "___A1", "___B2") reduce dmem scores by training a small diffusion model on a modified dataset subset
  3. Evaluate recaptioning intervention by using an in-domain VLM to rewrite memorized prompts, then measure if dmem scores decrease for the same image-prompt pairs

## Open Questions the Paper Calls Out

### Open Question 1
Does replacing uniform de-identification markers with randomized symbols effectively reduce the spurious correlations that lead to training data memorization?
- Basis in paper: [explicit] The authors recommend that "dataset curators should refrain from using a uniform de-identification marker" and suggest randomizing marker symbols to enhance caption diversity.
- Why unresolved: The paper proposes this solution as an actionable strategy for stakeholders but does not experimentally validate whether randomized markers successfully mitigate memorization in practice.
- What evidence would resolve it: A comparative study measuring memorization scores (dmem) in models trained on datasets with uniform markers versus those trained on datasets with randomized markers.

### Open Question 2
Can recaptioning datasets with in-domain Vision-Language Models (VLMs) effectively eliminate the memorization risks associated with de-identification traces?
- Basis in paper: [explicit] The authors suggest that "employing an in-domain vision-language model (VLM) can refine the language and augment the information density of the captions" to improve diversity.
- Why unresolved: This is presented as a recommendation for model developers; the study does not test the impact of recaptioning on the model's reliance on specific tokens like the "___" marker.
- What evidence would resolve it: Experiments training diffusion models on VLM-recaptioned versions of MIMIC-CXR to verify if the reliance on de-identification tokens decreases without losing semantic fidelity.

### Open Question 3
What specific training-time mitigation strategies can effectively address memorization caused by de-identification markers, given that inference-time interventions fail?
- Basis in paper: [explicit] The authors conclude that "removing memorization from trained models is a complex task" and state that findings "indicate a deeper underlying issue that must be addressed at the training level."
- Why unresolved: The study tested inference-time methods (random word/number addition, removal) and found them ineffective, leaving the development of training-time solutions as an open challenge.
- What evidence would resolve it: The development and benchmarking of new training-time regularization techniques or data filtering methods using the list of memorized prompts released by the authors.

## Limitations
- The memorization detection mechanism (dmem) assumes text-conditional noise magnitude reliably correlates with memorization strength, but this relationship hasn't been independently validated in medical imaging contexts
- The analysis is based on a filtered subset of MIMIC-CXR (110K samples) with unspecified filtering criteria, potentially introducing selection bias
- Inference-time mitigation strategies tested are limited to three simple approaches, leaving more sophisticated methods unexplored

## Confidence
- **High Confidence**: Identification of de-identification markers ("___") as highly memorized elements is well-supported by token-level analysis showing their unique frequency and lexical properties
- **Medium Confidence**: The mechanism linking spurious correlations to memorization through unique, frequent tokens is plausible but relies on assumptions about token frequency driving memorization
- **Low Confidence**: The generalizability of the dmem memorization detection metric to other medical imaging datasets and domains remains uncertain

## Next Checks
1. Reproduce dmem distribution: Validate the memorization detection pipeline by computing dmem scores on a held-out subset of MIMIC-CXR not used in the original analysis
2. Test randomized marker intervention: Implement randomized de-identification markers (e.g., "___A1", "___B2") and train a small diffusion model on a modified dataset subset to measure if dmem scores decrease
3. Evaluate recaptioning effectiveness: Use an in-domain VLM to rewrite memorized prompts with semantically equivalent but structurally different text, then measure if dmem scores decrease for the same image-prompt pairs