---
ver: rpa2
title: 'To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample
  Efficiency in Active Learning Models'
arxiv_id: '2507.15381'
source_url: https://arxiv.org/abs/2507.15381
tags:
- learning
- accuracy
- amax
- palm
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "PALM is a mathematical model that characterizes active learning\
  \ (AL) trajectories through four interpretable parameters: achievable accuracy (Amax),\
  \ coverage efficiency (\u03B4), early-stage performance (\u03B1), and scalability\
  \ (\u03B2). It provides a predictive description of AL behavior from partial observations,\
  \ enabling estimation of future performance and principled comparisons across different\
  \ strategies."
---

# To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models

## Quick Facts
- arXiv ID: 2507.15381
- Source URL: https://arxiv.org/abs/2507.15381
- Reference count: 40
- Key outcome: PALM provides a predictive mathematical model for AL trajectories using four interpretable parameters, validated across diverse datasets and methods.

## Executive Summary
PALM introduces a mathematical model that characterizes active learning trajectories through four interpretable parameters: achievable accuracy (Amax), coverage efficiency (δ), early-stage performance (α), and scalability (β). The model provides a predictive description of AL behavior from partial observations, enabling estimation of future performance and principled comparisons across different strategies. By treating labeled samples as objects covering portions of feature space, PALM reveals crucial insights into learning efficiency, data space coverage, and the scalability of AL methods.

## Method Summary
PALM is a parametric model that characterizes AL learning curves through four parameters estimated via nonlinear least squares regression. The model decomposes accuracy into contributions from covered and uncovered regions of data space, with coverage efficiency δ quantifying per-sample contribution. It requires 4+ budget-accuracy observations to fit the four parameters (Amax, δ, α, β) and predicts full trajectories. The approach was validated on CIFAR-10/100 and ImageNet variants with ResNet architectures, using SGD training with cosine learning rate decay and comparing multiple AL strategies with and without SSL embeddings.

## Key Results
- PALM accurately predicts full AL trajectories from limited early observations (requires ~1000 samples for CIFAR-10)
- SSL embeddings significantly increase coverage efficiency δ (from 0.094 to 0.535 for Margin sampling) and reduce early-stage delay α
- MoCov3 embeddings achieved the highest coverage efficiency and Amax, while BYOL showed slow convergence requiring larger budgets
- The model generalizes effectively across datasets, budgets, and AL strategies, enabling cost-effective strategy selection

## Why This Works (Mechanism)

### Mechanism 1: Coverage-Driven Accuracy Decomposition
The model treats labeled samples as randomly placed objects covering portions of feature space. Each labeled sample contributes δ (coverage efficiency) to expected coverage EC = 1 - (1 - δ)^B. Accuracy in covered regions (AC) combines with generalization in uncovered regions (AUC) to produce overall accuracy A = AC·PC + AUC·(1 - PC). This assumes labeled samples provide independent, approximately uniform coverage of relevant regions.

### Mechanism 2: Exponential Budget Scaling with Adjustable Dynamics
The four-parameter model A = Amax·(1 - (1 - δ)^((B/b + α)^β)) captures asymptotic performance ceiling (Amax), per-sample coverage contribution (δ), effective starting point offset (α), and accuracy scaling rate (β). This parametric form enables prediction of full learning curves from early observations through nonlinear regression.

### Mechanism 3: SSL Embedding Enhancement of Coverage Efficiency
Self-supervised pretrained embeddings (SimCLR, MoCo, BYOL) provide semantically structured feature spaces where sample selection better aligns with true class boundaries. This increases per-sample coverage contribution (δ) because selected samples represent broader regions, and reduces α because the model starts with better representations.

## Foundational Learning

- **Random Covering Problem / Coverage Probability**: Understanding how independent objects cover a space is essential for interpreting δ and the exponential coverage formula. Quick check: If δ = 0.1 and B = 10 labeled samples, what is the expected coverage fraction? (Answer: EC ≈ 1 - 0.9^10 ≈ 0.651)

- **Nonlinear Least Squares Parameter Estimation**: PALM requires fitting the 4-parameter model to observed budget-accuracy pairs. Quick check: What is the minimum number of budget-accuracy observations needed to estimate Amax, δ, α, and β? (Answer: At least 4 distinct budget points)

- **Active Learning Selection Strategies**: Understanding selection criteria (e.g., uncertainty sampling selects low-confidence samples, typicality selects representative samples) is necessary to interpret parameter differences. Quick check: Which AL strategy category does TypiClust belong to, and what property is it designed to maximize? (Answer: Typicality-based; selects samples most representative of the data distribution)

## Architecture Onboarding

- **Component map**: PALM Parameter Estimation Module -> AL Strategy Interface -> SSL Embedding Extractor -> Learning Curve Fitting Pipeline -> Budget Normalization Layer

- **Critical path**: 1) Initialize with pretrained SSL embeddings (optional but recommended for higher δ), 2) Run AL loop for minimum 4 iterations with budget increments b, 3) Collect (B_i, test_accuracy_i) pairs at each iteration, 4) Apply constrained nonlinear regression, 5) Validate fit quality on held-out iterations

- **Design tradeoffs**: Early stopping vs. fit reliability (fewer observations enable earlier decisions but increase parameter uncertainty), embedding quality vs. computational cost (MoCov3 shows highest δ/Amax but requires expensive pretraining), batch size vs. iteration granularity (smaller b provides more observation points but increases training overhead)

- **Failure signatures**: Unstable parameter estimates (δ oscillating between iterations), δ → 0 or δ → 1 with poor fit, α < 0 without justification, large discrepancy between predicted and actual in later iterations, BYOL-like linear plateauing requiring larger budgets

- **First 3 experiments**: 1) Baseline validation on CIFAR-10/100 without SSL, comparing predicted vs. actual final accuracy, 2) SSL embedding comparison with fixed AL strategy (Margin sampling with SimCLR vs. MoCov3 vs. BYOL on ImageNet-50), 3) Limited budget prediction test (fit PALM using only first 5-10% of budget, predict accuracy at 50% budget)

## Open Questions the Paper Calls Out

### Open Question 1
How does the depletion of the unlabeled pool and the resulting increase in sample noise affect PALM's parameter estimation accuracy in the final stages of active learning? The current model assumes an infinite pool, and the paper notes "minor deviations" in later CIFAR-100 rounds due to progressive exhaustion of informative examples.

### Open Question 2
Can PALM accurately predict the long-term performance of active learning strategies that exhibit delayed learning dynamics (e.g., BYOL) when only early-stage, linear-growth data is available? The paper observes that PALM fits BYOL curves as if plateaued due to slow early progress, but notes potential acceleration with larger budgets.

### Open Question 3
Does the PALM formulation require modification to model scenarios where the initial accuracy in uncovered regions is substantial, such as when using foundation models with strong zero-shot capabilities? The derivation relies on the assumption that accuracy in uncovered regions is negligible, which may not hold for modern foundation models.

## Limitations

- Assumes an effectively infinite unlabeled pool; deviations occur when pool is exhausted
- Requires at least 4 distinct budget-accuracy observations for reliable parameter estimation
- Model mis-specification risk if learning curves exhibit non-monotonic behavior or distribution shift

## Confidence

- Mechanism 1 (Coverage decomposition): Medium - supported by mathematical derivation but weak direct empirical validation
- Mechanism 2 (Parametric curve prediction): High - validated through extensive experiments across multiple datasets and methods
- Mechanism 3 (SSL embedding enhancement): High - clear quantitative evidence of δ and α improvements

## Next Checks

1. Validate PALM fit quality on CIFAR-10/100 without SSL embeddings using the specified training protocol (SGD with Nesterov momentum, cosine LR decay)
2. Reproduce the δ improvement from 0.094 to 0.535 when comparing Margin sampling with and without SSL embeddings on ImageNet-50
3. Test PALM's predictive capability using only 5-10% of total budget to assess early-stage decision-making utility