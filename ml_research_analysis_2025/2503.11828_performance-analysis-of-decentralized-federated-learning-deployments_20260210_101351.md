---
ver: rpa2
title: Performance Analysis of Decentralized Federated Learning Deployments
arxiv_id: '2503.11828'
source_url: https://arxiv.org/abs/2503.11828
tags:
- data
- device
- linear
- non-iid
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive performance analysis of decentralized
  federated learning (DFL) under various network topologies, non-IID data distributions,
  and training strategies. The authors conduct both theoretical convergence analysis
  and extensive experimental evaluations across six DFL deployment configurations
  (continuous/aggregate linear, ring, star, and mesh) using five different models
  including traditional ML (SVM, logistic regression), deep learning (ResNet, DistilBERT),
  and a large language model (MiniGPT-4).
---

# Performance Analysis of Decentralized Federated Learning Deployments

## Quick Facts
- **arXiv ID:** 2503.11828
- **Source URL:** https://arxiv.org/abs/2503.11828
- **Reference count:** 20
- **Primary result:** DFL models converge to global optimum under IID conditions but degrade with non-IID data; star and mesh topologies show superior robustness to non-IID distributions.

## Executive Summary
This paper presents a comprehensive performance analysis of decentralized federated learning (DFL) under various network topologies, non-IID data distributions, and training strategies. The authors conduct both theoretical convergence analysis and extensive experimental evaluations across six DFL deployment configurations (continuous/aggregate linear, ring, star, and mesh) using five different models including traditional ML (SVM, logistic regression), deep learning (ResNet, DistilBERT), and a large language model (MiniGPT-4).

The theoretical analysis establishes that DFL models converge to the global optimum under IID data conditions for strongly convex functions, but convergence rates degrade as non-IID data distribution increases. Experimental results validate this finding, showing that all models achieve high F1 scores (0.95-0.99) matching baseline performance under IID conditions across all topologies. However, under non-IID data, convergence becomes topology-dependent, with star and mesh topologies demonstrating superior robustness due to their concurrent training nature, while linear and ring topologies show convergence only on devices with better label balance.

## Method Summary
The study evaluates six DFL configurations (Continuous Linear/Ring, Aggregate Linear/Ring/Star/Mesh) using five models (SVM, Logistic Regression, ResNet-18, DistilBERT, MiniGPT-4) on four datasets with controlled non-IID label skew (Levels 1-3). Experiments run on a single NVIDIA Quadro RTX 8000 simulating 5 devices with SGD optimizer. Convergence is measured via epochs to flatten and F1 scores, with non-IID setup defined by specific label fraction distributions across devices.

## Key Results
- All models achieve F1 scores of 0.95-0.99 matching baseline performance under IID conditions across all topologies
- Convergence degrades proportionally with non-IID data heterogeneity, with Z term directly impacting convergence bounds
- Star and mesh topologies demonstrate superior robustness to non-IID distributions compared to linear and ring topologies due to concurrent parameter aggregation
- Non-convex models (DNNs, LLMs) exhibit transient loss fluctuations during inter-device parameter transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Convergence degradation scales proportionally with non-IID data heterogeneity (quantified as Z in theoretical bounds).
- Mechanism: In derived convergence bounds (Eq. 16, 18, 26, 29), the term Z = F* - F*_k directly appears as an additive penalty. When Z = 0 (IID), the gap to optimal is bounded by constants. As Z increases, the bound loosens, causing divergence between actual and optimal loss.
- Core assumption: Local objective functions are µ-strongly convex and L-smooth (convex models only; non-convex DNNs observed empirically).
- Evidence anchors:
  - [abstract] "convergence rate is inversely proportional to the degree of non-IID data distribution"
  - [section IV, Eq. 16] Shows Z term directly impacting convergence bound: E[F(wk) - F*] ≤ ... + 2ηkZ
  - [corpus] Adjacent work (arXiv:1907.02189, Li et al.) confirms FedAvg convergence degradation under non-IID; corpus papers consistently treat label skew as primary heterogeneity source.
- Break condition: If Z becomes arbitrarily large (e.g., complete label partition across devices), the bound provides no meaningful guarantee; convergence may fail entirely.

### Mechanism 2
- Claim: Concurrent aggregation topologies (star, mesh) provide superior robustness to non-IID data compared to sequential topologies (linear, ring).
- Mechanism: In star/mesh, devices aggregate parameters from ALL peers each round (Eq. 30), enabling immediate averaging of heterogeneous gradients. In linear/ring, parameters flow sequentially; early devices with skewed data propagate biased updates before corrective signals arrive.
- Core assumption: Sufficient training epochs per device; aggregation rounds T must be large enough for averaging effects to accumulate.
- Evidence anchors:
  - [abstract] "Star and mesh topologies demonstrate superior robustness to non-IID distributions compared to linear and ring topologies"
  - [section VII, Tables X-XIV] Under non-IID Level 2, star/mesh show consistent convergence across all devices; linear/ring show "NC" (no convergence) on multiple devices
  - [corpus] S-VOTE (arXiv:2501.19279) discusses client selection improving DFL under heterogeneity, implicitly supporting topology-dependent robustness.
- Break condition: If non-IID is extreme (Level 3: devices missing entire label classes), even star/mesh may oscillate; the paper notes star/mesh experienced "oscillation" in Level 3 binary classification scenarios.

### Mechanism 3
- Claim: Non-convex models (DNNs, LLMs) exhibit loss fluctuations during inter-device parameter transfer; this is a structural property, not a bug.
- Mechanism: Deep residual connections and transformer attention mechanisms create non-linear optimization landscapes. When parameters transfer between devices with different data distributions, gradient directions shift, causing transient loss increases (observed as 1-5 epoch fluctuations).
- Core assumption: Model architectures contain non-linear residual/attention paths (ResNet, DistilBERT, MiniGPT-4).
- Evidence anchors:
  - [section VII] "since ResNet and DistilBERT are not convex models... the loss exhibits fluctuations over 1 to 5 epochs"
  - [section VII, Tables VI-VIII] Non-convex models show variable convergence epochs per device vs. stable patterns in convex models
  - [corpus] Adjacent DFL work on TinyML (arXiv:2501.04817) focuses on linear models; non-convex dynamics remain under-explored in corpus.
- Break condition: Fluctuations are transient; if they persist beyond early training or diverge, indicates learning rate too high or severe data imbalance.

## Foundational Learning

- **Concept: IID vs. Non-IID Data in Federated Settings**
  - Why needed here: The entire paper's conclusions hinge on understanding how data heterogeneity (Z) affects distributed optimization. Without this, convergence bounds and topology recommendations are unmotivated.
  - Quick check question: If five devices each hold all 10 MNIST digit classes but with different proportions (e.g., Device 1 has 90% zeros, Device 2 has 80% ones...), is this IID or non-IID?

- **Concept: Strong Convexity and L-Smoothness**
  - Why needed here: All theoretical convergence proofs assume µ-strong convexity and L-smoothness. These properties guarantee unique global minima and bounded gradient changes—essential for deriving Eq. 16's bound.
  - Quick check question: A function f is L-smooth if ||∇f(x) - ∇f(y)|| ≤ L||x - y||. What does this imply about the Hessian's spectral norm?

- **Concept: FedAvg Aggregation**
  - Why needed here: Aggregate training strategies (linear, ring, star, mesh) use weighted averaging similar to FedAvg: w_agg = Σ(s_i × w_i) / Σs_i. Understanding this weighting scheme is necessary to interpret Eq. 20, 30.
  - Quick check question: If three devices have sample counts [100, 200, 300] and parameters [w1, w2, w3], what is the aggregated parameter after one FedAvg round?

## Architecture Onboarding

- **Component map:**
  DFL Deployment = Network Topology + Training Strategy + Data Distribution
  Topologies: Linear (chain) | Ring (circular) | Star (hub-spoke) | Mesh (fully-connected)
  Training Strategies: Continuous (sequential parameter passing) | Aggregate (FedAvg-style weighted averaging)
  Valid Combinations (6 total): 1. Continuous Linear 2. Continuous Ring 3. Aggregate Linear 4. Aggregate Ring 5. Aggregate Star 6. Aggregate Mesh

- **Critical path:**
  1. Define N devices and partition dataset with controlled non-IID level (use KL divergence for binary, label fractions for multi-class)
  2. Select topology + strategy combination
  3. Initialize all devices with identical w0
  4. Execute local SGD epochs per device (follow epoch allocation rules in Section VI)
  5. Transfer/aggregate parameters according to topology protocol
  6. Monitor loss curves for convergence (flat or train-val intersection)

- **Design tradeoffs:**
  - Linear/Ring: Lower bandwidth (single neighbor communication), but sequential bottleneck and poor non-IID robustness. Choose only for IID data or resource-constrained networks.
  - Star: Concurrent training with single aggregation point. Best balance of robustness and simplicity. Central device becomes potential bottleneck; select highest-capability node.
  - Mesh: Maximum robustness (full peer-to-peer) but highest communication overhead (O(N²) connections). Reserve for high-reliability, moderate N scenarios.

- **Failure signatures:**
  - "NC" pattern in convergence tables: Device never converges—indicates data too skewed for sequential topologies. Switch to star/mesh.
  - Persistent loss oscillation (>5 epochs): Non-convex model with excessive non-IID. Reduce learning rate or apply gradient clipping.
  - Late-device convergence in linear (e.g., 400+ epochs vs. 83 for first): Sequential propagation delay. Normal behavior; allocate sufficient total epochs.

- **First 3 experiments:**
  1. Baseline sanity check: Deploy SVM with IID data on all 6 topologies using Table III hyperparameters. Verify F1 ≈ 0.95 and convergence patterns match Tables IV-V. Purpose: Validate implementation against published results.
  2. Non-IID stress test: Deploy ResNet-18 on aggregate star vs. continuous ring with Level 2 non-IID ([0.1, 0.3, 0.5, 0.7, 0.9] label fractions). Compare final F1 scores against Table XIV (expect ~0.97 vs. ~0.95 gap). Purpose: Reproduce topology robustness finding.
  3. Break condition exploration: Using logistic regression with Level 3 non-IID ([1, 0, 0.7, 1, 0]—complete label absence on some devices), test whether aggregate mesh oscillates as claimed. Monitor for divergent loss trajectories. Purpose: Identify practical deployment boundaries.

## Open Questions the Paper Calls Out

- **How can software-defined networking (SDN) be leveraged to realize agile DFL deployments?**
  - Basis: The Conclusion states, "In the future, we plan to leverage software-defined networking to realize agile DFL deployments."
  - Unresolved because: Current work focuses on static configurations without implementing dynamic network management.
  - Resolution evidence: Implementation of an SDN-controlled DFL system demonstrating dynamic topology adjustment and resulting performance metrics.

- **How does device heterogeneity in computation and communication affect the convergence rates of the analyzed topologies?**
  - Basis: Theoretical analysis explicitly assumes a "homogeneous environment" with identical device capabilities.
  - Unresolved because: Derived convergence bounds and experimental validations rely on uniform device performance.
  - Resolution evidence: Convergence analysis and empirical tests on a testbed with heterogeneous hardware profiles.

- **How do proxy and idle nodes influence the convergence efficiency of large-scale cross-device DFL?**
  - Basis: Authors state they "do not consider proxy or idle nodes in our evaluation," limiting scope to trainer and aggregator roles.
  - Unresolved because: Study excludes these roles to isolate topology effects, leaving their impact on training strategies unknown.
  - Resolution evidence: Evaluation of the six DFL configurations incorporating nodes with intermittent availability or relay-only functions.

## Limitations

- Scalability boundaries: 5-device focus may not scale predictably to hundreds of nodes in mesh topologies (O(N²) communication overhead).
- Non-convex model assumptions: Convergence proofs rely on convexity properties that don't hold for DNNs/LLMs; empirical observations lack theoretical guarantees.
- Extreme non-IID regime: Level 3 non-IID causes oscillation even in robust topologies, but paper doesn't provide thresholds for when DFL becomes infeasible.

## Confidence

- **High confidence:** IID convergence across all topologies; F1 scores matching baseline performance; topology robustness hierarchy under non-IID.
- **Medium confidence:** Non-IID convergence degradation mechanisms; aggregate aggregation formula interpretation.
- **Low confidence:** MiniGPT-4 fine-tuning configuration; exact initialization seeds; scalability predictions beyond 5 devices.

## Next Checks

1. **Topology stress test:** Deploy aggregate mesh with 10+ devices under Level 2 non-IID; measure communication overhead and convergence stability vs. 5-device baseline to validate scalability claims.
2. **Extreme non-IID threshold:** Systematically increase label absence in Level 3 until star/mesh fails; identify exact failure point and test whether device exclusion strategy restores convergence.
3. **Non-convex bound extension:** Adapt convergence analysis from Li et al. (arXiv:1907.02189) to DFL linear/ring topologies; derive empirical bounds for DNN loss fluctuation duration and compare against observed 1-5 epoch patterns.