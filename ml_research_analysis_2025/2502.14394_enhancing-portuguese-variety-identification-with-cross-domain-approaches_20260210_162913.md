---
ver: rpa2
title: Enhancing Portuguese Variety Identification with Cross-Domain Approaches
arxiv_id: '2502.14394'
source_url: https://arxiv.org/abs/2502.14394
tags:
- language
- portuguese
- corpus
- training
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of distinguishing between European
  and Brazilian Portuguese in NLP applications. The authors compiled a cross-domain
  corpus (PtBrVarId) from multiple sources spanning six domains, and developed a language
  variety identification (LVI) classifier.
---

# Enhancing Portuguese Variety Identification with Cross-Domain Approaches

## Quick Facts
- arXiv ID: 2502.14394
- Source URL: https://arxiv.org/abs/2502.14394
- Reference count: 15
- Best model achieves F1 scores of 84.97% on DSL-TL and 77.25% on FRMT benchmarks

## Executive Summary
This study addresses the challenge of distinguishing between European and Brazilian Portuguese in NLP applications. The authors compiled a cross-domain corpus (PtBrVarId) from multiple sources spanning six domains, and developed a language variety identification (LVI) classifier. They investigated the impact of delexicalization (masking named entities and thematic content) and proposed a two-step training protocol to improve cross-domain generalization. The best-performing model, a BERT-based classifier with intermediate delexicalization, achieved F1 scores of 84.97% on DSL-TL and 77.25% on FRMT benchmarks, outperforming N-gram baselines and representing the current state of the art for this task. The code, corpus, and models are publicly released to support further research.

## Method Summary
The authors compiled a silver-labeled corpus (PtBrVarId) from six domains: Journalistic, Legal, Politics, Web, Social Media, and Literature. They implemented a two-step training protocol where Step 1 trains on one domain and validates on all other domains to find optimal hyperparameters for cross-domain generalization, then Step 2 retrains on all domains using those hyperparameters with undersampling. Delexicalization was applied to the training set only, probabilistically masking named entities (via NER) and thematic content (via POS tags) with optimal parameters PPOS=0.6 and PNER=0.0. The best model was a BERTimbau classifier fine-tuned with these techniques, evaluated on DSL-TL and FRMT benchmarks.

## Key Results
- BERT-based classifier with intermediate delexicalization achieved 84.97% F1 on DSL-TL and 77.25% F1 on FRMT benchmarks
- N-gram models improved by 13 percentage points on FRMT when trained on delexicalized corpus
- Two-step training protocol outperformed single-domain and naive multi-domain approaches for cross-domain generalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intermediate delexicalization improves cross-domain generalization by forcing models to learn variety-specific linguistic features rather than domain-specific shortcuts.
- Mechanism: By probabilistically masking named entities (via NER) and thematic content (via POS tags) during training, the model cannot rely on spurious correlations—such as associating "Cameroun" with specific varieties—and must instead learn morphological and syntactic patterns that genuinely distinguish European from Brazilian Portuguese.
- Core assumption: The linguistic cues that distinguish varieties are present even when named entities and thematic nouns are removed; models trained on raw text exploit shortcuts that fail under domain shift.
- Evidence anchors:
  - [abstract]: "investigated the impact of delexicalization (masking named entities and thematic content)"
  - [section 6]: "training in the delexicalized corpus improved the F1 score by approximately 13 and 10 percentage points for the N-gram and BERT models, respectively" on FRMT
  - [corpus]: No direct corpus neighbor evidence for delexicalization in LVI; mechanism is paper-specific
- Break condition: If delexicalization removes too many tokens (high PPOS), models lose necessary signal; full delexicalization historically reported to reduce effectiveness (Sharoff et al., 2010).

### Mechanism 2
- Claim: A two-step training protocol yields better hyperparameters for cross-domain generalization than single-domain or naive multi-domain training.
- Mechanism: Step 1 trains on one domain and validates on all other domains (held-out), selecting hyperparameters that maximize out-of-domain performance. Step 2 retrains on all domains using those hyperparameters. This explicitly optimizes for transfer rather than in-domain fit.
- Core assumption: Hyperparameters that work best for cross-domain transfer in Step 1 will generalize when scaling to the full multi-domain corpus in Step 2.
- Evidence anchors:
  - [abstract]: "proposed a two-step training protocol to improve cross-domain generalization"
  - [section 4]: "Step one is used to find the best hyperparameters to train the model so ensure the generalization capability of the model"
  - [corpus]: Weak corpus evidence; no neighbor papers explicitly validate this protocol
- Break condition: If domains are too dissimilar, hyperparameters from any single domain may not transfer; undersampling in Step 2 may discard useful signal from over-represented domains.

### Mechanism 3
- Claim: BERT-based classifiers outperform N-gram baselines for Portuguese LVI, particularly when combined with delexicalization.
- Mechanism: BERTimbau (pretrained on Brazilian Portuguese) captures contextual morphological and syntactic patterns that distinguish varieties, while N-gram models rely on surface token statistics that are more vulnerable to domain-specific vocabulary.
- Core assumption: Pretrained Portuguese representations encode variety-distinguishing features even when the pretraining corpus is predominantly Brazilian Portuguese.
- Evidence anchors:
  - [abstract]: "best-performing model, a BERT-based classifier... achieved F1 scores of 84.97% on DSL-TL and 77.25% on FRMT benchmarks, outperforming N-gram baselines"
  - [section 6]: "BERT model outperforms the N-gram model across all scenarios"
  - [corpus]: Neighbor paper "Amadeus-Verbo" (FMR 0.60) confirms strong LLM performance on Portuguese tasks, supporting transformer effectiveness
- Break condition: If pretraining bias toward Brazilian Portuguese is too strong, the model may systematically misclassify European Portuguese; this is noted as future work.

## Foundational Learning

- **Language Variety Identification (LVI)**:
  - Why needed here: This is the core task—distinguishing linguistically similar varieties (European vs. Brazilian Portuguese) rather than different languages.
  - Quick check question: Can you explain why LVI is harder than standard language identification?

- **Delexicalization**:
  - Why needed here: Understanding how masking entities and POS tags prevents shortcut learning is essential for interpreting the results.
  - Quick check question: What is the difference between delexicalizing named entities vs. thematic content (POS)?

- **Cross-Domain Generalization**:
  - Why needed here: The paper's central claim is that their protocol improves transfer to unseen domains; understanding domain shift is prerequisite.
  - Quick check question: Why would a model trained on news articles fail on social media text for the same task?

## Architecture Onboarding

- **Component map**: Silver-labeled corpus (6 domains) → Data cleaning pipeline → Delexicalization module (spaCy NER/POS) → BERTimbau fine-tuning (or N-gram + Naive Bayes baseline) → Binary classifier (EP/BP)
- **Critical path**: (1) Corpus compilation and silver-labeling heuristics; (2) Delexicalization with tuned (PPOS, PNER) probabilities; (3) Two-step training protocol; (4) Evaluation on DSL-TL and FRMT benchmarks
- **Design tradeoffs**: Higher delexicalization reduces domain shortcuts but may remove genuine signal; BERT is more effective but computationally expensive (52h grid search vs. 3h for N-grams); undersampling balances domains but discards data
- **Failure signatures**: (1) N-gram model collapsing on FRMT entity bucket (46.95% F1)—relies on entities, not variety markers; (2) Low Fleiss' Kappa in Literature domain (0.23)—texts blend EP/BP features; (3) High PPOS (>80%) degrades performance for both architectures
- **First 3 experiments**:
  1. Replicate the (PPOS, PNER) grid search on a single domain to identify optimal delexicalization levels before full training.
  2. Train N-gram and BERT models on raw vs. delexicalized corpora; compare performance on the FRMT entity bucket specifically.
  3. Execute the two-step protocol with held-out domain validation; verify that Step 1 hyperparameters improve Step 2 generalization on the manually annotated test partition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does expanding the corpus to include less-resourced varieties, specifically African Portuguese, impact the robustness and discrimination accuracy of the LVI system?
- Basis in paper: [explicit] Section 7 states, "the corpus should be expanded to include other less-resourced Portuguese varieties, particularly African Portuguese."
- Why unresolved: The current study and dataset (PtBrVarId) are restricted to European and Brazilian Portuguese due to the lack of available resources for other varieties.
- What evidence would resolve it: Performance metrics (F1 scores) and error analysis of a model trained on a dataset augmented with African Portuguese data.

### Open Question 2
- Question: To what extent does the specific variety of the pre-trained model (e.g., BERTimbau trained on BP) introduce bias, and can a balanced pre-training approach improve LVI performance?
- Basis in paper: [explicit] Section 7 notes, "it is crucial to explore the impact of the pretrained model selection, as the language variety on which the model was originally trained may introduce bias."
- Why unresolved: The current implementation relies on BERTimbau, which is pre-trained predominantly on Brazilian Portuguese, potentially skewing the model's baseline features.
- What evidence would resolve it: A comparative study fine-tuning models initialized with BP-specific, EP-specific, and multi-varietal pre-trained weights on the same LVI task.

### Open Question 3
- Question: Does applying delexicalization to the validation or inference set improve model consistency compared to the current approach of only delexicalizing the training set?
- Basis in paper: [explicit] Section 4 states, "We leave the study of the impact of delexicalizing the validation set on the effectiveness of the model for future research."
- Why unresolved: The authors simulated real-world conditions by keeping validation sets unaltered, but they did not test if delexicalizing all inputs would further reduce spurious feature reliance.
- What evidence would resolve it: Benchmark results comparing the current model against a variant where inputs are delexicalized during both training and inference phases.

## Limitations

- Delexicalization effectiveness is sensitive to probability thresholds, with the optimal balance between removing shortcuts and preserving variety features not fully explored
- The two-step training protocol assumes single-domain validation will generalize to multi-domain training without empirical validation across multiple held-out domain combinations
- BERT model's performance advantage may be partially attributable to domain-specific pretraining bias toward Brazilian Portuguese

## Confidence

**High confidence**: The empirical observation that BERT-based models outperform N-gram baselines for Portuguese LVI (F1 scores of 84.97% vs 78.19% on DSL-TL). This claim is directly supported by the reported benchmark results and consistent across multiple evaluation scenarios.

**Medium confidence**: The claim that intermediate delexicalization improves cross-domain generalization. While the reported F1 improvements (13 percentage points for N-grams, 10 for BERT on FRMT) are substantial, the mechanism relies on the assumption that variety-distinguishing features survive masking, which is not directly validated through ablation studies or feature importance analysis.

**Medium confidence**: The two-step training protocol's effectiveness for cross-domain hyperparameter selection. The protocol is logically sound and shows performance improvements, but the lack of ablation testing with alternative hyperparameter selection methods (random search, Bayesian optimization) or validation on additional held-out domain combinations limits definitive conclusions about its superiority.

## Next Checks

1. **Ablation study on delexicalization thresholds**: Systematically vary PPOS from 0.1 to 0.9 in increments of 0.1 while keeping PNER=0.0, and measure the impact on F1 scores across all six domains in the test partition. This would quantify the trade-off between removing domain-specific shortcuts and preserving variety-distinguishing linguistic features.

2. **Cross-domain hyperparameter sensitivity analysis**: Implement the two-step protocol using different held-out domain combinations (e.g., validate on Legal instead of Social Media) and compare the resulting hyperparameters and final F1 scores. This would test whether the protocol's effectiveness is robust to domain selection or if certain domains are better validators than others.

3. **Pretraining bias assessment**: Fine-tune the same BERT architecture on the delexicalized corpus but initialize with a multilingual BERT model rather than BERTimbau, then compare performance on European Portuguese identification. This would isolate whether the performance advantage stems from BERT's architecture or from domain-specific pretraining advantages.