---
ver: rpa2
title: 'Squeeze and Excitation: A Weighted Graph Contrastive Learning for Collaborative
  Filtering'
arxiv_id: '2504.04443'
source_url: https://arxiv.org/abs/2504.04443
tags:
- learning
- weightedgcl
- graph
- contrastive
- views
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the limitation in existing Graph Contrastive\
  \ Learning (GCL) approaches for recommendation, which assign equal weights to all\
  \ features within perturbed views, neglecting the varying significance of different\
  \ features. To overcome this, the authors propose WeightedGCL, a framework that\
  \ incorporates a robust perturbation strategy\u2014applying perturbations only to\
  \ the final GCN layer\u2014and integrates a Squeeze-and-Excitation Network (SENet)\
  \ to dynamically assign weights to perturbed view features."
---

# Squeeze and Excitation: A Weighted Graph Contrastive Learning for Collaborative Filtering

## Quick Facts
- **arXiv ID:** 2504.04443
- **Source URL:** https://arxiv.org/abs/2504.04443
- **Reference count:** 30
- **Primary result:** Achieves up to 24.45% improvement in NDCG@20 and 20.58% in Recall@20 on Alibaba dataset through dynamic feature weighting in GCL.

## Executive Summary
This paper addresses a key limitation in Graph Contrastive Learning (GCL) for recommendation: existing approaches assign equal weights to all features within perturbed views, ignoring that different features have varying importance. The authors propose WeightedGCL, which combines a robust perturbation strategy (applying noise only to the final GCN layer) with a Squeeze-and-Excitation Network (SENet) to dynamically assign weights to perturbed view features. This allows the model to focus on crucial features while reducing the impact of less relevant information. Extensive experiments on three real-world datasets (Amazon, Pinterest, and Alibaba) demonstrate significant performance improvements over competitive baselines.

## Method Summary
WeightedGCL builds upon LightGCN by incorporating two key innovations: a robust perturbation strategy and dynamic feature weighting via SENet. The model applies random noise only to the final GCN layer outputs, creating two contrastive views while preserving stable structural information from earlier layers. The SENet module then dynamically weights these perturbed embeddings through a squeeze (global averaging) and excitation (multi-layer feed-forward network) process. The final loss combines standard BPR loss for recommendation with InfoNCE loss for contrastive learning, forcing the model to learn representations invariant to noise but sensitive to weighted features.

## Key Results
- WeightedGCL achieves up to 24.45% improvement in NDCG@20 on the Alibaba dataset
- The model outperforms competitive baselines across all three datasets (Amazon, Pinterest, Alibaba)
- Ablation studies confirm that both the final-layer perturbation and SENet weighting are critical for performance
- The method shows particular effectiveness on sparse datasets, with Alibaba (99.993% sparsity) benefiting significantly

## Why This Works (Mechanism)

### Mechanism 1: Final-Layer Robust Perturbation
Applying random noise strictly to the final GCN layer ($L$) rather than all layers stabilizes representation learning by preserving early-stage aggregation signals. Standard GCNs aggregate neighbor information layer-by-layer, and if noise is injected at layer 0 or 1, it propagates and amplifies through subsequent aggregations, corrupting the structural identity of the node. By restricting perturbation $\Delta$ to $E^{(L)}$, the model creates contrastive views that vary in final feature space but share identical, stable structural foundations from layers $0$ to $L-1$.

### Mechanism 2: Dynamic Feature Recalibration (SENet)
Dynamically weighting embedding channels via Squeeze-and-Excitation (SE) blocks allows the model to emphasize informative features and suppress noise within the contrastive view. Instead of treating all embedding dimensions equally, the SE block compresses the perturbed view into a summary statistic (Squeeze), passes it through a feed-forward network (Excitation), and outputs a scaling vector $T$. This vector is applied element-wise to the perturbed embeddings, acting as a learnable attention mechanism for the feature dimensions.

### Mechanism 3: Weighted Contrastive Alignment
Contrasting two independently weighted-and-perturbed views forces the encoder to learn representations invariant to noise but sensitive to the weighted features. The model generates two views, $\bar{R}$ and $\tilde{R}$, using independent noise and SE-weighting, and minimizes InfoNCE loss to pull these views together while pushing apart views of other nodes. Because the SE block suppresses "less relevant information" before the loss calculation, the alignment focuses specifically on the "crucial features."

## Foundational Learning

- **Concept: Graph Contrastive Learning (GCL)**
  - **Why needed here:** WeightedGCL is a modification of standard GCL. You must understand that GCL relies on maximizing agreement between augmented views of the same node (self-supervision) to solve data sparsity.
  - **Quick check question:** How does the InfoNCE loss distinguish between a "positive" pair and a "negative" pair in this architecture?

- **Concept: Squeeze-and-Excitation (SE) Networks**
  - **Why needed here:** This is the core novelty ("Weighted"). You need to understand that SE blocks perform channel-wise attention (recalibrating feature strength) rather than spatial attention.
  - **Quick check question:** In the "Squeeze" step (Eq. 5), how is the spatial information of the node embeddings condensed into a single statistic per channel?

- **Concept: LightGCN Backbone**
  - **Why needed here:** The paper uses LightGCN as the base encoder. You must know that LightGCN removes non-linearities and feature transformations, relying solely on neighbor aggregation.
  - **Quick check question:** According to Eq. 2, how is the final representation $F$ computed from the layer embeddings?

## Architecture Onboarding

- **Component map:** Input Graph -> LightGCN Backbone (L layers) -> Final Layer Perturbation -> SENet (Squeeze → Excitation → Recalibrate) -> BPR Loss + InfoNCE Loss
- **Critical path:** The gradient flow from the InfoNCE loss back through the Excitation network ($W_K \dots W_1$) is critical. If the Excitation network is too deep (high granularity), gradients may vanish or training becomes unstable.
- **Design tradeoffs:**
  - Perturbation Depth: The paper argues for final-layer only. Perturbing all layers introduces too much instability, but perturbing none collapses the model.
  - Excitation Granularity ($K$): More layers (W-G4) allow finer weighting but increase training difficulty. W-G3 is generally safer.
- **Failure signatures:**
  - Collapse to Random: If $\lambda_c$ is too low or perturbation is removed, performance drops to near zero (Table 1: R@20 drops from ~0.13 to 0.01).
  - Over-regularization: If the contrastive weight $\lambda_c$ is too high, the model may focus too much on view alignment and lose personalization (BPR signal).
- **First 3 experiments:**
  1. **Sanity Check (Ablation):** Run `WGCL-w/o pert` vs. `WeightedGCL` on a subset of Amazon data to confirm that removing perturbation causes the performance collapse described in Section 3.3.
  2. **Hyperparameter Sensitivity:** Perform a grid search on temperature $\tau$ vs. contrastive weight $\lambda_c$ (Fig 3). Specifically, verify if $\tau=0.2$ is indeed optimal for Amazon/Pinterest but $\tau=0.8$ is needed for Alibaba (sparse data requires higher temperature).
  3. **Granularity Test:** Compare W-G1 (single layer excitation) vs. W-G3 (three layers) to verify if the performance gain justifies the extra parameters in the Excitation network.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does perturbing specific intermediate layers (e.g., $L-1$) provide a performance trade-off distinct from perturbing only the final layer or all layers simultaneously?
- **Basis in paper:** The ablation study compares the proposed "final layer" perturbation against "all layers" perturbation and no perturbation, but it does not evaluate the effect of perturbing individual intermediate layers.
- **Why unresolved:** It is unclear if the final layer is uniquely optimal for perturbation or if intermediate layers could offer similar robustness benefits without the potential instability of perturbing the entire network.

### Open Question 2
- **Question:** Is there a theoretical relationship between dataset characteristics (e.g., sparsity or interaction volume) and the optimal granularity (number of FFN layers) of the excitation network?
- **Basis in paper:** Table 3 shows that the optimal excitation strategy varies by dataset (W-G4 is best for Amazon, while W-G3 is best for Pinterest and Alibaba), but the paper provides no explanation for this variance.
- **Why unresolved:** The authors empirically select the best granularity but do not investigate the underlying data properties that cause one granularity level to outperform another.

### Open Question 3
- **Question:** How does WeightedGCL perform when the random uniform noise perturbation is replaced with alternative augmentation strategies such as edge dropout or structured noise?
- **Basis in paper:** Section 2.3 defines the perturbation strategy strictly as adding random noise matrices sampled from $U(0, 1)$, without comparing against other perturbation methods common in graph contrastive learning.
- **Why unresolved:** The paper claims the method improves robustness, but it attributes this success to the location (final layer) and weighting (SENet) of the noise, leaving the impact of the nature of the noise unexplored.

## Limitations

- The final-layer-only perturbation strategy, while theoretically sound, may be brittle on extremely sparse datasets where final embeddings may have already lost critical structure.
- The SENet weighting is the key novelty, but the paper provides limited empirical analysis of what the excitation weights actually learn, making it difficult to confirm whether they are capturing meaningful feature importance.
- The temperature hyperparameter shift for Alibaba (0.8 vs 0.2) is mentioned but not explained, suggesting the model's behavior is highly dataset-dependent.

## Confidence

- **High confidence:** The ablation study showing performance collapse without perturbation (WGCL-w/o pert) is well-documented and replicable.
- **Medium confidence:** The improvement claims (24.45% NDCG@20 on Alibaba) are supported by tables, but the results are tightly coupled to specific hyperparameter tuning per dataset, which may not generalize.
- **Low confidence:** The interpretability of the SENet weights and the mechanism by which they improve contrastive learning is weakly supported by the paper.

## Next Checks

1. **SENet Weight Analysis:** Visualize and analyze the excitation weights learned on a sample dataset. Check if they are sparse, concentrated on specific channels, or converge to uniform values.
2. **Perturbation Sensitivity:** Systematically vary the noise magnitude (not just presence/absence) and test if there's an optimal range for each dataset, particularly for Alibaba.
3. **Transfer Learning Test:** Train WeightedGCL on a dense dataset (Amazon) and evaluate its performance on a held-out sparse test set to assess generalization.