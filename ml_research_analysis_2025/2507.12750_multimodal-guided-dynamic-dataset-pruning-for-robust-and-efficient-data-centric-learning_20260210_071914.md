---
ver: rpa2
title: Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric
  Learning
arxiv_id: '2507.12750'
source_url: https://arxiv.org/abs/2507.12750
tags:
- training
- data
- selection
- learning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a dynamic dataset pruning framework that improves
  training efficiency by adaptively selecting high-quality samples based on both task-specific
  loss and cross-modality semantic consistency. The method uses CLIP-based semantic
  supervision to filter out noisy or inconsistent samples, addressing limitations
  of existing static pruning methods.
---

# Multimodal-Guided Dynamic Dataset Pruning for Robust and Efficient Data-Centric Learning

## Quick Facts
- arXiv ID: 2507.12750
- Source URL: https://arxiv.org/abs/2507.12750
- Reference count: 18
- Primary result: Dynamic pruning framework improves accuracy with fewer samples by balancing task loss and cross-modality consistency

## Executive Summary
This paper proposes a dynamic dataset pruning framework that improves training efficiency by adaptively selecting high-quality samples based on both task-specific loss and cross-modality semantic consistency. The method uses CLIP-based semantic supervision to filter out noisy or inconsistent samples, addressing limitations of existing static pruning methods. By optimizing a selection score through dual-supervision, the framework dynamically adjusts data composition during training. Experiments on CIFAR-10/100 and ImageNet-1k show consistent improvements over state-of-the-art baselines, achieving higher accuracy with fewer samples.

## Method Summary
The framework combines task supervision (sT = L(fθ(xi), yi)) with cross-modality consistency (sC = τ·zi·zt/(||zi||||zt||)) to dynamically prune datasets during training. A selection score s is optimized via Ls = (1/||s||0)·s·(λsC - sT), initialized as ones, and samples near the median score are selected each epoch. CLIP's image and text encoders are adapted with linear adapters fine-tuned using InfoNCE loss, while CLIP weights remain frozen. All modality features are precomputed once for efficiency. The method achieves robust pruning even under 20% label noise.

## Key Results
- Consistent accuracy improvements over baselines on CIFAR-10/100 and ImageNet-1k at 30%, 50%, and 70% selection ratios
- Robust to 20% label noise while maintaining pruning efficiency
- Lightweight and scalable approach suitable for resource-constrained scenarios

## Why This Works (Mechanism)
The dual-supervision mechanism works by balancing task-specific difficulty with cross-modality semantic consistency. Task loss identifies hard or mislabeled samples, while CLIP-based semantic consistency ensures selected samples align with their class semantics across modalities. This prevents pruning of useful samples that may have high task loss due to noise or ambiguity, while removing truly problematic samples that fail both criteria.

## Foundational Learning
- **CLIP cross-modal embeddings**: Required to compute semantic consistency between image and text representations; quick check: verify CLIP zero-shot accuracy on target dataset
- **Selection score optimization**: Balances task loss and semantic consistency; quick check: visualize score distribution and selection ratio per epoch
- **Median-based selection**: Dynamically adjusts pruning threshold; quick check: monitor actual selection ratio vs target ratio

## Architecture Onboarding
- **Component map**: CLIP encoders -> adapter training -> feature precomputation -> dual-supervision score optimization -> dynamic selection -> model training
- **Critical path**: Score computation (sT, sC) → score update → sample selection → model training
- **Design tradeoffs**: Precomputing features improves efficiency but reduces adaptability to dataset shifts; frozen CLIP weights ensure stability but limit domain adaptation
- **Failure signatures**: 
  - Accuracy drops below baseline → λ misbalanced or adapter poorly aligned
  - Selection ratio drifts from target → median tolerance window inappropriate
  - Slow convergence → score optimizer learning rate too low
- **First experiments**:
  1. Implement sensitivity analysis on λ to find optimal balance
  2. Verify adapter fine-tuning improves zero-shot CLIP accuracy
  3. Test different median tolerance windows (±5%, ±10%, ±15%) for selection stability

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Sensitive to hyperparameter λ balancing task loss and semantic consistency
- Performance depends on quality of pretrained CLIP model and adapter training
- Median-based selection may miss edge cases with extreme scores

## Confidence
- **High confidence**: Overall framework combining task loss and cross-modality consistency is clearly described and validated
- **Medium confidence**: Selection score optimization formulation specified, but critical hyperparameters (λ, optimizer type, learning rate) missing
- **Low confidence**: Adapter training details and exact median-based selection criterion insufficiently specified

## Next Checks
1. Implement sensitivity analysis on λ to find optimal balance between task loss and semantic consistency
2. Verify adapter fine-tuning by checking zero-shot CLIP accuracy improvement on target dataset
3. Experiment with different median tolerance windows (±5%, ±10%, ±15%) to determine impact on selection ratio stability and final accuracy