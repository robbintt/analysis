---
ver: rpa2
title: Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese
  Grammar
arxiv_id: '2505.19599'
source_url: https://arxiv.org/abs/2505.19599
tags:
- psych
- predicate
- which
- japanese
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates whether pretrained language models understand
  nuanced Japanese grammar rules by measuring perplexity on minimal pairs of grammatical
  and ungrammatical sentences involving the "first person psych predicate restriction."
  Weblab is the only tested model that consistently assigns lower perplexity to grammatical
  sentences than ungrammatical ones, which the authors attribute to its uniformly
  poor tokenization forcing consistent handling of Japanese text. When restricted
  to well-tokenized sentences, Llama 3's performance improves by an order of magnitude
  (28x improvement).
---

# Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar

## Quick Facts
- **arXiv ID:** 2505.19599
- **Source URL:** https://arxiv.org/abs/2505.19599
- **Reference count:** 5
- **Primary result:** Only Weblab consistently assigns lower perplexity to grammatical Japanese sentences than ungrammatical ones, which the authors attribute to its uniformly poor tokenization forcing consistent handling of Japanese text.

## Executive Summary
This paper evaluates whether pretrained language models understand nuanced Japanese grammar rules by measuring perplexity on minimal pairs of grammatical and ungrammatical sentences involving the "first person psych predicate restriction." Weblab is the only tested model that consistently assigns lower perplexity to grammatical sentences than ungrammatical ones, which the authors attribute to its uniformly poor tokenization forcing consistent handling of Japanese text. When restricted to well-tokenized sentences, Llama 3's performance improves by an order of magnitude (28x improvement). Machine translation experiments confirm that models without consistent tokenizers struggle to produce grammatically correct Japanese sentences with psych predicates in third person, often producing ungrammatical or semantically incorrect outputs. The findings suggest that more consistent tokenizers could help language models better follow nuanced Japanese grammar rules.

## Method Summary
The paper evaluates Japanese language models on their understanding of the "first person psych predicate restriction" by generating template-based minimal sentence pairs (grammatical vs. ungrammatical) and computing perplexity using base models from Huggingface (Mistral-7B, Llama 2-7B, Llama 3-8B, Weblab-10B, Swallow-7B, Swallow-MS-7B-v0.1). The evaluation uses sentences with 2 subjects (watashi/I, haha/Mother), 10 psych predicates (itai, samui, tsurai, kokorobosoi, atsui, kanashii, sabishii, ureshii, hazukashii, kurushii), 10 non-psych predicates, and two forms (direct -i, evidential -sou). The authors analyze tokenizer fertility and byte fallback rates to understand why models differ in performance, finding that consistent tokenization—even if uniformly poor—enables better grammar pattern recognition than inconsistent tokenization.

## Key Results
- Weblab consistently assigns lower perplexity to grammatical sentences than ungrammatical ones, while other models show the opposite pattern
- When restricted to well-tokenized sentences, Llama 3's performance improves by 28x
- Models without consistent tokenizers struggle to produce grammatically correct Japanese sentences with psych predicates in third person, often producing ungrammatical or semantically incorrect outputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Uniform tokenization—whether uniformly good or uniformly bad—enables more consistent grammar pattern recognition than inconsistent tokenization.
- **Mechanism:** When tokenization varies in quality across grammatical constructions, token probability differences reflect tokenizer artifacts rather than linguistic acceptability. Uniform tokenization ensures probability differences stem from learned grammar patterns, not byte-fallback-induced probability crashes.
- **Core assumption:** Models learn grammar through token-level probability distributions; when these distributions are corrupted by tokenizer artifacts, grammatical judgments become unreliable.
- **Evidence anchors:** "We give evidence that Weblab's uniformly bad tokenization is a possible root cause for its good performance"; "Weblab may have performed best on these tasks because it was trained using a tokenizer for English models as-is... it tokenizes every sentence uniformly poorly, and does not have grammar-specific tokenization issues"
- **Break condition:** If a model's training data contains sufficiently many examples of a grammatical construction to override tokenization noise, the mechanism may not hold.

### Mechanism 2
- **Claim:** Byte fallback causes order-of-magnitude probability reductions that make grammatically correct tokens appear artificially unlikely.
- **Mechanism:** When a tokenizer lacks vocabulary for a character sequence, it falls back to byte-level encoding. These byte tokens have extremely low probabilities because they appear infrequently during training. For Japanese, this creates systematic under-estimation of certain grammatical forms—specifically adjectives ending in "shii" when combined with evidential expressions.
- **Core assumption:** The probability assigned to byte-fallback tokens during inference reflects their training distribution, not the grammatical correctness of the underlying sequence.
- **Evidence anchors:** "Those ending in 'shii' such as ('kanashii,' 'sabishii,' 'ureshii,' 'hazukashii,' 'kurushii') caused byte fallback when used in conjunction with evidential expressions, thus giving these tokens extremely low probability"; Weblab byte fallback rate: 0.66 vs Llama 3: 0.08
- **Break condition:** If byte fallback tokens are upweighted during training or if the model has learned to compose meaning from byte sequences, the probability penalty may diminish.

### Mechanism 3
- **Claim:** Models will circumvent tokenization barriers by substituting alternative grammatical constructions, even when the circumlocution is unnatural.
- **Mechanism:** When the most natural output sequence contains low-probability tokens (due to byte fallback), models with sufficient grammatical knowledge select alternative phrasings that avoid those tokens. This reveals the model "knows" the grammar rule but cannot express it naturally due to tokenization constraints.
- **Core assumption:** The model's willingness to use circumlocution indicates retained grammatical knowledge that is tokenization-constrained, not absent.
- **Evidence anchors:** "Language models will use alternative grammar patterns in order to produce grammatical sentences when tokenization issues prevent the most natural sentence from being output"; Weblab produces evidential expressions 47-90% of the time for "cold" and "embarrassed," while Llama 3 produces them 0-32% of the time
- **Break condition:** If the model lacks knowledge of alternative constructions, it may produce ungrammatical output rather than circumlocution.

## Foundational Learning

- **Concept: Tokenization fertility and byte fallback**
  - Why needed here: Understanding why Weblab's "bad" tokenizer outperforms Llama 3's "better" tokenizer requires knowing that fertility (tokens per word) and byte fallback rates measure tokenizer quality, and that consistency matters more than raw quality for grammatical pattern learning.
  - Quick check question: If a tokenizer has fertility 1.0 for Japanese but falls back to bytes for evidential markers, will it produce consistent probability estimates across grammatical and ungrammatical sentences?

- **Concept: First person psych predicate restriction (Japanese grammar)**
  - Why needed here: The paper's evaluation hinges on this specific grammar rule—certain predicates describing internal states (cold, sad, happy) can only directly describe the first person. Third-person descriptions require evidential markers like "-sou" (seems) or "-garu" (showing signs of).
  - Quick check question: Given "Watashi wa samui" (I feel cold) is grammatical, what marker must be added to make "Haha wa samu-" grammatical?

- **Concept: Perplexity as grammaticality proxy**
  - Why needed here: The paper uses perplexity differences between minimal pairs (grammatical vs ungrammatical) to infer grammatical knowledge. Lower perplexity on grammatical sentences indicates the model "knows" the rule.
  - Quick check question: If a model assigns perplexity 1,000 to a grammatical sentence and 700 to its ungrammatical counterpart, what does this suggest about the model's grammatical knowledge?

## Architecture Onboarding

- **Component map:**
  Input text → Tokenizer → Token IDs → Model (base or instruct) → Token probabilities
       ↓
  Byte fallback? → Probability penalty (orders of magnitude)
       ↓
  Perplexity calculation → Grammaticality judgment (compare minimal pairs)

- **Critical path:**
  1. Identify target grammar rule with clear minimal pairs
  2. Generate test sentences using templates with controlled predicates
  3. Tokenize and check for byte fallback (inspect token IDs for values > vocabulary size or byte-range)
  4. Compute per-sentence perplexity for base models (not instruct-tuned)
  5. Compare perplexity distributions: grammatical < ungrammatical indicates success

- **Design tradeoffs:**
  - Uniform byte-level tokenization: Eliminates grammar-specific artifacts but increases sequence length (Weblab fertility 1.23 vs Llama 3's 0.85), potentially degrading long-context reasoning
  - Language-specific tokenizers: Better compression and fluency but introduce inconsistency if vocabulary doesn't cover all grammatical morphemes
  - Paper suggests: Either use uniformly bad tokenizers (bytes only) or ensure uniform good coverage of grammatical morphemes

- **Failure signatures:**
  - Adjective-specific failures: Adjectives ending in "shii" with evidentials trigger byte fallback (kanashii → kanashi-sou breaks at "sou" boundary)
  - Grammar-specific perplexity inversions: Ungrammatical sentences have lower perplexity than grammatical ones
  - Translation outputs: Model produces direct forms when evidentials are grammatically required, or uses semantic mistranslations to avoid the construction entirely

- **First 3 experiments:**
  1. **Tokenization audit:** For your tokenizer, compute fertility and byte fallback rates across a sample of Japanese sentences containing psych predicates in both direct and evidential forms. Identify which predicate-evidential combinations trigger fallback.
  2. **Perplexity minimal pair test:** Generate 100+ sentence pairs using the paper's template structure (first/third person × psych/non-psych × direct/evidential). Compute perplexity ratios for each model. Flag cases where ungrammatical < grammatical.
  3. **Ablation with forced tokenization:** Manually override tokenization for problematic predicates (pre-tokenize evidential forms as single tokens or use character-level fallback) and measure whether perplexity ordering corrects. This tests whether tokenization is the causal mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- The conclusion that uniform tokenization enables better grammar learning relies on comparisons across different base models, making it difficult to isolate tokenization effects from other architectural differences
- The first person psych predicate restriction represents just one Japanese grammar rule, limiting generalizability to other grammatical phenomena
- The paper focuses on perplexity-based evaluation but doesn't examine whether tokenization issues affect actual language generation quality or task performance in Japanese

## Confidence

**High confidence:** The core observation that tokenization inconsistencies cause systematic probability distortions for specific Japanese grammatical constructions. The empirical demonstration that Llama 3's perplexity improves 28x when restricted to well-tokenized sentences is directly measurable and reproducible.

**Medium confidence:** The proposed mechanism that uniform tokenization (even if uniformly poor) enables more reliable grammar learning than inconsistent tokenization. While the byte fallback effects are well-documented, the broader claim about why Weblab outperforms Llama 3 requires additional controlled experiments to rule out other factors.

**Low confidence:** The assertion that more consistent tokenizers "could help language models better follow nuanced Japanese grammar rules" as a general principle. This is a reasonable inference but hasn't been validated across multiple grammar rules or through experiments showing improved generation quality.

## Next Checks

1. **Controlled tokenizer ablation study:** Take a single base model (e.g., Llama 3) and systematically vary its tokenizer between (a) full byte-level, (b) character-level, (c) word-level, and (d) the original tokenizer. Re-run the Japanese grammar evaluation to isolate tokenization effects while holding the model architecture constant.

2. **Cross-linguistic grammar probing:** Extend the evaluation methodology to test tokenization effects on other languages with rich morphology or complex grammatical agreement systems (e.g., Turkish, Finnish, or Russian). This would determine whether the observed phenomenon is specific to Japanese evidential constructions or represents a broader tokenization challenge for morphologically rich languages.

3. **Generation quality assessment:** Beyond perplexity evaluation, measure actual generation quality by fine-tuning models on Japanese grammaticality judgment datasets or evaluating their ability to produce grammatically correct sentences with psych predicates in unconstrained generation tasks.