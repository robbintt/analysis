---
ver: rpa2
title: Fact or Facsimile? Evaluating the Factual Robustness of Modern Retrievers
arxiv_id: '2508.20408'
source_url: https://arxiv.org/abs/2508.20408
tags:
- factual
- base
- retrieval
- arxiv
- factuality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study reveals that dense retrievers and rerankers, key components\
  \ of retrieval-augmented generation (RAG) pipelines, suffer a significant loss in\
  \ factual accuracy compared to their base large language models (LLMs). By pairing\
  \ 12 embedding checkpoints with their corresponding base LLMs and evaluating both\
  \ on a factuality benchmark, the researchers found that retrievers\u2019 accuracy\
  \ dropped by 12 to 43 percentage points (median 28 points), with most falling into\
  \ the 25-35% range versus 60-70% for base models."
---

# Fact or Facsimile? Evaluating the Factual Robustness of Modern Retrievers

## Quick Facts
- **arXiv ID**: 2508.20408
- **Source URL**: https://arxiv.org/abs/2508.20408
- **Authors**: Haoyu Wu; Qingcheng Zeng; Kaize Ding
- **Reference count**: 19
- **Primary result**: Dense retrievers and rerankers suffer 12-43 percentage point accuracy drops compared to their base LLMs on factuality benchmarks

## Executive Summary
This study reveals that dense retrievers and rerankers, key components of retrieval-augmented generation (RAG) pipelines, suffer a significant loss in factual accuracy compared to their base large language models (LLMs). By pairing 12 embedding checkpoints with their corresponding base LLMs and evaluating both on a factuality benchmark, the researchers found that retrievers' accuracy dropped by 12 to 43 percentage points (median 28 points), with most falling into the 25-35% range versus 60-70% for base models. Under high-distractor conditions, top-1 accuracy fell from 33% to 26%, and accuracy plummeted to one-third of its original level when correct completions were paraphrased to mask lexical cues. Statistical tests confirmed decisions were driven by surface-level semantic similarity rather than factual reasoning.

## Method Summary
The researchers paired 12 dense retriever/reranker checkpoints with their corresponding base LLMs and evaluated both on the FACTOR benchmark—a 4-way multiple-choice factual question answering dataset. Base LLMs were evaluated via perplexity scoring on concatenated prefix-completion pairs, while retrievers used cosine similarity between instruction-prefixed query embeddings and candidate completions. Rerankers computed relevance scores between query-candidate pairs. The study tested accuracy gaps across different candidate pool sizes (4 vs 1000) and applied a paraphrase attack using GPT-4.1 to rewrite correct completions while preserving factual truth. Statistical analysis used Mann-Whitney U tests to compare similarity scores for correctly vs incorrectly answered questions.

## Key Results
- Retrievers' accuracy dropped 12-43 percentage points (median 28 points) compared to base LLMs on factuality benchmarks
- Under high-distractor conditions, top-1 accuracy fell from 33% to 26%
- When correct completions were paraphrased to mask lexical cues, accuracy dropped to one-third of original level
- Statistical tests confirmed decisions were driven by surface-level semantic similarity rather than factual reasoning (p < 0.01)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive learning objectives that optimize for semantic similarity inadvertently degrade parametric factual knowledge during retriever fine-tuning.
- Mechanism: The fine-tuning process applies contrastive loss that pulls semantically similar query-passage pairs together while pushing dissimilar ones apart. This optimization pressure systematically favors surface-level semantic matching capabilities over the preservation of factual knowledge encoded in the base LLM's parameters.
- Core assumption: Parametric factual knowledge and semantic matching capabilities compete for representational capacity during fine-tuning.
- Evidence anchors:
  - [abstract]: "These findings reveal a systematic trade-off introduced by contrastive learning for retrievers: gains in semantic retrieval are paid for with losses in parametric factual knowledge"
  - [section 3.1.2]: "This trend strongly suggests that the fine-tuning process for semantic retrieval may inadvertently erode the rich factual knowledge encapsulated within the original base models."
  - [corpus]: Corpus provides limited direct mechanistic validation; related work on factual robustness exists but doesn't confirm the specific contrastive learning trade-off mechanism.

### Mechanism 2
- Claim: Retrievers rely on surface-level semantic proximity (cosine similarity) rather than genuine factual reasoning when ranking candidates.
- Mechanism: The embedding space learned through contrastive training creates high similarity scores for query-completion pairs with lexical or semantic overlap, regardless of factual correctness. Statistical tests show similarity scores are significantly higher (p < 0.01) for correctly ranked ground-truth documents, indicating correlation—not causal reasoning—drives predictions.
- Core assumption: High semantic similarity between query and correct answers in training data creates spurious correlation that retrievers exploit.
- Evidence anchors:
  - [abstract]: "Statistical tests confirmed decisions were driven by surface-level semantic similarity rather than factual reasoning"
  - [section 4.1, Table 3]: "a consistent and statistically significant pattern (p < 0.01) across all evaluated IR models: the similarity scores for ground-truth documents are significantly higher when they are correctly ranked"
  - [corpus]: No corpus papers directly validate the similarity-vs-reasoning distinction; mechanism remains inferred from controlled experiments.

### Mechanism 3
- Claim: Paraphrasing correct answers to reduce lexical overlap with queries causes catastrophic accuracy degradation, revealing dependency on surface cues.
- Mechanism: When GPT-4.1 paraphrased correct completions to preserve factual truth while minimizing similarity to queries, retrievers' previously correct predictions flipped to wrong at >2/3 rate. This demonstrates that retrievers exploit lexical cues (shared terms, phrasing patterns) rather than understanding factual content.
- Core assumption: The paraphrasing process successfully reduces semantic/lexical similarity while preserving factual equivalence (validated by manual case study on 50 instances).
- Evidence anchors:
  - [abstract]: "accuracy plummeted to one-third of its original level when correct completions were paraphrased to mask lexical cues"
  - [section 4.2, Figure 2]: "For both models, accuracy drops to around 30% on questions they originally answered correctly after paraphrasing"
  - [corpus]: Limited corpus validation; related work on embedding robustness to edits exists (Paper 95241) but doesn't specifically test paraphrase attacks on factuality.

## Foundational Learning

- **Concept: Contrastive Learning for Retrieval**
  - Why needed here: The paper's core argument hinges on understanding how contrastive objectives (pulling similar pairs together, pushing dissimilar pairs apart) create the observed factuality-semantic trade-off.
  - Quick check question: Can you explain why maximizing inner-product similarity between queries and relevant passages might degrade factual knowledge?

- **Concept: Embedding Space Geometry**
  - Why needed here: The statistical analysis relies on cosine similarity measurements in embedding space; understanding what similarity represents is critical for interpreting the results.
  - Quick check question: What does high cosine similarity between a query and completion embedding indicate about their relationship in the learned space?

- **Concept: Perplexity-Based Evaluation for LLMs**
  - Why needed here: The paper compares retriever accuracy against base LLMs evaluated via perplexity (selecting lowest-perplexity completion); understanding this baseline is essential.
  - Quick check question: Why does lower perplexity on a prefix-completion pair suggest the LLM considers it more probable/factual?

## Architecture Onboarding

- **Component map:**
  - Base LLMs (Llama-3.1-8B, Mistral-7B, Qwen2 variants, Gemma-2-9B) -> Retrievers (e5-mistral-7b-instruct, SFR-Embedding-Mistral, gte-Qwen2 variants, etc.) -> Rerankers (FollowIR-7B, bge-reranker-v2.5-gemma2-lightweight, mxbai-rerank-large-v2) -> FACTOR Benchmark (4-way multiple choice with 1 correct + 3 distractors)

- **Critical path:**
  1. Select retriever/reranker checkpoint and identify its base LLM
  2. Evaluate base LLM on FACTOR via perplexity scoring (lowest perplexity = prediction)
  3. Evaluate retriever on FACTOR via embedding similarity (highest cosine similarity = prediction) or reranker via relevance scores
  4. Compare accuracy gaps between base and fine-tuned models
  5. Conduct statistical tests (Mann-Whitney U) comparing similarity scores for correctly vs. incorrectly answered questions
  6. Apply paraphrase attack to correct completions and measure accuracy retention

- **Design tradeoffs:**
  - **4-option vs. 1000-option evaluation**: 4-option matches FACTOR's original design but underestimates real-world distractor volume; 1000-option more realistic but computationally expensive
  - **Perplexity vs. embedding-based evaluation**: Different modalities (generative vs. discriminative) may confound direct comparison; paper argues gap is consistent across 12 model pairs, suggesting robust effect
  - **Paraphrase quality validation**: Manual validation on 50 samples balances rigor vs. scale; residual risk that some paraphrases alter meaning

- **Failure signatures:**
  - **Factuality collapse**: Accuracy drops to 25-35% (vs. 60-70% base) across all retriever families
  - **Distractor sensitivity**: Top-1 accuracy drops from 33% to 26% when candidate pool expands from 4 to 1000
  - **Paraphrase vulnerability**: >67% of correct predictions flip after paraphrasing; accuracy drops to ~30% of original
  - **Similarity-correlation pattern**: Consistently higher similarity scores for correctly ranked items (p < 0.01) across all models

- **First 3 experiments:**
  1. **Baseline comparison**: Run FACTOR evaluation on 3 base LLM-retriever pairs (e.g., Mistral-7B → e5-mistral-7b-instruct, Qwen2-7B → gte-Qwen2-7B-instruct, Llama-3.1-8B → ReasonIR-8B) to confirm 20-30 point accuracy degradation pattern in your infrastructure
  2. **Distractor scaling test**: Evaluate SFR-Embedding-Mistral (or equivalent top performer) with candidate pools of 4, 50, 200, 500, 1000 to map accuracy degradation curve; verify 33% → 26% drop is reproducible
  3. **Paraphrase robustness probe**: Apply GPT-4 paraphrasing to 100 FACTOR correct completions, evaluate retriever accuracy retention, and compute Jaccard similarity between original and paraphrased completions to quantify lexical cue reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can contrastive learning objectives be modified to jointly optimize for semantic similarity and factual fidelity without sacrificing retrieval performance?
- Basis in paper: [explicit] Authors conclude that "retrieval objectives that balance similarity with factual fidelity" are needed and call for "factuality-aware retrieval training."
- Why unresolved: The paper demonstrates the trade-off exists but does not propose or test alternative training objectives that could resolve it.
- What evidence would resolve it: A new training loss that incorporates factuality signals (e.g., counterfactual examples or truth labels) showing reduced accuracy gaps between retriever and base LLM while maintaining MTEB retrieval benchmarks.

### Open Question 2
- Question: Does the factuality degradation pattern generalize across factuality benchmarks with different error type distributions beyond FACTOR?
- Basis in paper: [inferred] The study evaluates all models solely on FACTOR, which uses specific distractor types (Entity, Predicate, Circumstance, Coreference, Link).
- Why unresolved: It remains unknown whether the observed 12-43 percentage point degradation is benchmark-specific or reflects a general property of contrastive fine-tuning.
- What evidence would resolve it: Replication of the paired base-vs-retriever evaluation on alternative factuality benchmarks (e.g., TruthfulQA, FActScore) showing consistent degradation magnitudes.

### Open Question 3
- Question: Are retrievers universally vulnerable to paraphrasing attacks, or does robustness vary by paraphrase generation method?
- Basis in paper: [inferred] The paraphrase attack used only GPT-4.1 to rewrite completions, and robustness was tested on only two models (SFR-Embedding-Mistral and mxbai-rerank-large-v2).
- Why unresolved: Whether the two-thirds accuracy flip rate generalizes across diverse paraphrase strategies and model architectures remains untested.
- What evidence would resolve it: Systematic evaluation across multiple paraphrase methods (rule-based, translation-based, diverse LLMs) and additional retriever/reranker models showing consistent vulnerability patterns.

### Open Question 4
- Question: What scaling laws govern retriever factuality degradation as the candidate pool size increases beyond 1,000 options?
- Basis in paper: [inferred] Only the top-performing retriever (SFR-Embedding-Mistral) was tested in the expanded candidate pool setting, dropping from 33% to 26% accuracy.
- Why unresolved: The trajectory of accuracy decline at scale (e.g., 10K, 100K candidates) and whether stronger retrievers exhibit different scaling behaviors is unknown.
- What evidence would resolve it: Evaluation of multiple retrievers across exponentially increasing candidate pool sizes with fitted degradation curves.

## Limitations
- The study demonstrates correlation but cannot establish causation without controlled intervention experiments comparing contrastive vs non-contrastive training
- Paraphrase attack relies on GPT-4.1's ability to preserve factual truth while reducing lexical similarity, though manual validation was limited to 50 samples
- The 4-option FACT-FER benchmark may underestimate real-world distractor volume, though the 1000-option test partially addresses this concern

## Confidence
- **High Confidence**: Factuality accuracy gap between base LLMs and retrievers (28-point median drop) across 12 model pairs; statistical significance of similarity scores for correctly vs incorrectly answered questions (p < 0.01)
- **Medium Confidence**: Paraphrase vulnerability demonstration; high-distractor accuracy degradation; mechanism that contrastive learning trades factuality for semantic matching
- **Low Confidence**: Specific attribution of factuality loss to contrastive objective rather than other fine-tuning factors; generalizability to non-instruction-tuned retrievers

## Next Checks
1. **Intervention experiment**: Train retriever variants using contrastive vs non-contrastive objectives on identical base model to isolate causal effect on factuality
2. **Cross-task generalization**: Test factuality accuracy on non-multiple-choice factual question answering (e.g., open-ended QA) to validate broader robustness claims
3. **Adversarial probing**: Design targeted attacks that maximize semantic similarity while minimizing factual correctness to stress-test the vulnerability mechanism identified