---
ver: rpa2
title: 'EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval
  from Fine-grained Emotions'
arxiv_id: '2509.15986'
source_url: https://arxiv.org/abs/2509.15986
tags:
- emotion
- music
- system
- user
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces EmoHeal, an end-to-end system that delivers
  personalized therapeutic music experiences by analyzing user text input and generating
  real-time audiovisual content. EmoHeal uses a fine-tuned XLM-RoBERTa model to detect
  27 fine-grained emotions from user text, maps these emotions to musical parameters
  via a knowledge graph grounded in music therapy principles, and retrieves audiovisual
  content using the CLaMP3 model to guide users from their current state toward a
  calmer one.
---

# EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions

## Quick Facts
- arXiv ID: 2509.15986
- Source URL: https://arxiv.org/abs/2509.15986
- Reference count: 0
- One-line primary result: End-to-end system delivers personalized therapeutic music via fine-grained emotion detection and audiovisual retrieval, showing significant mood improvement (M=4.12) with strong correlation (r=0.72) between perceived accuracy and therapeutic outcome.

## Executive Summary
EmoHeal is an end-to-end system that delivers personalized therapeutic music experiences by analyzing user text input and generating real-time audiovisual content. The system uses a fine-tuned XLM-RoBERTa model to detect 27 fine-grained emotions from user text, maps these emotions to musical parameters via a knowledge graph grounded in music therapy principles, and retrieves audiovisual content using the CLaMP3 model to guide users from their current state toward a calmer one. A within-subjects study (N=40) demonstrated significant supportive effects, with participants reporting substantial mood improvement and high perceived emotion recognition accuracy.

## Method Summary
EmoHeal employs a neuro-symbolic architecture combining neural emotion detection with expert knowledge for therapeutic music retrieval. The system first processes user text through a fine-tuned XLM-RoBERTa-base model to classify 27 fine-grained emotions using Focal Loss. An emotion-music knowledge graph maps these emotions to six musical parameters through a hybrid approach: expert rules for unambiguous emotions (τ=0.7 threshold) and dynamic weight matrices for complex blends. Musical parameters are converted to natural language prompts and retrieved via CLaMP3's text-audio similarity search against a 600-video library. The three-stage UI presents matching content, transitions through bridging content, and concludes with target calming content following the iso-principle.

## Key Results
- Significant mood improvement reported by participants (M=4.12, p<0.001) on 5-point Likert scale
- High perceived emotion recognition accuracy (M=4.05, p<0.001) indicating users felt understood
- Strong correlation between perceived accuracy and therapeutic outcome (r=0.72, p<0.001) validates fine-grained approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-grained emotion recognition accuracy causally influences therapeutic outcome.
- **Mechanism:** When users perceive accurate identification of their nuanced emotional state (27 categories), they experience a sense of being "understood," which mediates mood improvement. 78% of participants explicitly mentioned feeling "understood."
- **Core assumption:** The correlation (r=0.72) reflects a directional relationship where recognition accuracy drives therapeutic benefit.
- **Evidence anchors:** Abstract correlation finding; Section 4.2 detailed correlation; related work on EEG-based therapy (arXiv:2601.12280) focuses on physiological signals rather than text-based emotion recognition accuracy.
- **Break condition:** If users do not perceive the emotion recognition as accurate (e.g., M<3.5 on 5-point scale), the therapeutic effect would likely diminish regardless of music quality.

### Mechanism 2
- **Claim:** The iso-principle "match-guide-target" narrative structure enables emotional transition from distress toward calm.
- **Mechanism:** The system first presents content matching the user's current emotional state (validation), then progressively shifts musical parameters toward target calming states, avoiding "emotional mismatch" that static approaches create.
- **Core assumption:** Users in vulnerable states benefit more from gradual emotional transition than immediate calming content.
- **Evidence anchors:** Abstract retrieval logic; Section 2 discussion of iso-principle; BREATH study (arXiv:2510.15895) uses physiological signals for similar affective regulation.
- **Break condition:** If the initial "match" stage fails to resonate (e.g., emotion vector misclassification), users may disengage before the "guide" phase completes.

### Mechanism 3
- **Claim:** The hybrid neuro-symbolic architecture produces interpretable emotion-to-music mappings that enhance user trust.
- **Mechanism:** Neural networks capture nuanced text features; the knowledge graph with expert rules and dynamic weight matrix translates emotions into six musical parameters, providing transparency absent in pure neural approaches.
- **Core assumption:** Interpretability of the mapping contributes to perceived system responsiveness.
- **Evidence anchors:** Section 3.2 IF-THEN rules and dynamic adjustment; Section 5 neuro-symbolic approach producing interpretable mappings; WeMusic-Agent (arXiv:2512.16108) combines LLM reasoning with domain knowledge.
- **Break condition:** If the weight matrix encodes incorrect music-emotion relationships, even accurate emotion detection yields inappropriate music.

## Foundational Learning

- **Concept: Multi-label emotion classification with Focal Loss**
  - **Why needed here:** The emotion detection task outputs 27 simultaneous probabilities, not a single class. Standard cross-entropy doesn't handle class imbalance well; Focal Loss (γ=2) down-weights easy examples.
  - **Quick check question:** Given the achieved Macro-F1=0.64 vs Weighted-F1=0.71, which metric better reflects performance on underrepresented emotion classes?

- **Concept: Iso-principle in music therapy**
  - **Why needed here:** The three-stage "match-guide-target" narrative is the core therapeutic mechanism. Without understanding this, the retrieval logic appears arbitrary.
  - **Quick check question:** Why would playing immediately calming music to an anxious user potentially backfire compared to first matching their anxious state?

- **Concept: Contrastive multimodal retrieval (CLaMP3)**
  - **Why needed here:** The system converts musical parameters to natural language prompts, then retrieves audio via text-audio similarity. Understanding the shared embedding space is critical for debugging retrieval failures.
  - **Quick check question:** If retrieval returns semantically similar but therapeutically inappropriate videos, is the failure in the prompt generator, the CLaMP3 encoder, or the video library composition?

## Architecture Onboarding

- **Component map:** User Text → XLM-RoBERTa-base (768-dim CLS) → Linear(768→27) → Sigmoid → 27-d Emotion Vector e → Emotion-Music KG → 6 Music Parameters → Prompt Generator → CLaMP3 Text Encoder → Cosine Similarity → Top-3 Videos from Faiss Index → Three-Stage UI (Matching → Bridging → Target)

- **Critical path:** Emotion detection accuracy → Knowledge graph mapping quality → Prompt clarity → CLaMP3 retrieval relevance. The 0.7 threshold (τ) for expert rules is empirically set; incorrect tuning here bypasses expert curation.

- **Design tradeoffs:**
  - Retrieval-based vs. generative music: Chose retrieval for fidelity and efficiency; trades off infinite personalization for professional quality.
  - 27-class granularity vs. computational cost: Fine-grained emotions require larger knowledge graph; coarse labels would simplify but reduce correlation with outcomes.
  - Single-session study design: Enables controlled evaluation but cannot assess long-term adaptation or habituation effects.

- **Failure signatures:**
  - Low perceived accuracy (M<3.5): Check emotion classifier calibration, especially for mixed/blended emotions where no expert rule fires.
  - Irrelevant video retrieval: Inspect generated prompts—template-based conversion may lose nuance; verify CLaMP3 encoder matches training distribution.
  - High dropout before Stage 3: "Match" phase may be too brief or insufficiently resonant; consider extending based on user feedback.

- **First 3 experiments:**
  1. **Ablation on emotion granularity:** Replace 27-class with 6-class coarse emotions; measure change in accuracy-outcome correlation to test whether fine-grained detection is necessary.
  2. **Threshold sensitivity analysis:** Vary τ (0.5, 0.6, 0.7, 0.8) to measure proportion of sessions using expert rules vs. dynamic adjustment and impact on perceived accuracy.
  3. **Retrieval diversity audit:** For a fixed emotion input, examine top-10 retrieved videos for acoustic feature variance (tempo, spectral centroid); low variance suggests prompt or library limitations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the observed mood improvement and perceived emotion recognition accuracy persist over repeated sessions or longer-term usage?
- **Basis in paper:** The authors explicitly identify the "single-session design" as a limitation in the Discussion section.
- **Why unresolved:** The current study only measures immediate post-intervention effects (N=40) without follow-up, leaving the durability of the "supportive effects" unknown.
- **What evidence would resolve it:** A longitudinal study tracking user mood and sleep quality metrics over multiple weeks of consistent system usage.

### Open Question 2
- **Question:** Can EmoHeal be effectively adapted for clinical populations with severe mental health conditions?
- **Basis in paper:** The paper states "Future work should extend to... clinical integration" and notes the inclusion criteria excluded those requiring clinical intervention.
- **Why unresolved:** The efficacy was validated only on a non-clinical cohort (primarily university students), so the system's safety and effectiveness for severe anxiety or disorders remain untested.
- **What evidence would resolve it:** Randomized controlled trials (RCTs) involving patients with diagnosed anxiety disorders, measuring clinical outcomes alongside user experience.

### Open Question 3
- **Question:** Does incorporating multimodal signals (e.g., physiological data) significantly improve therapeutic outcomes compared to text-only analysis?
- **Basis in paper:** The Conclusion lists "multimodal signals" as a specific direction for future work.
- **Why unresolved:** The current system relies exclusively on user text input, which may suffer from articulation limits or bias, whereas bio-signals could offer objective emotional data.
- **What evidence would resolve it:** A comparative study evaluating emotion recognition accuracy and mood improvement when physiological inputs (heart rate, skin conductance) are added to the text model.

## Limitations
- The controlled single-session design limits understanding of long-term therapeutic efficacy, habituation effects, and adaptation to repeated use.
- The knowledge graph's expert rules and emotion-to-music mapping weights (W) are fixed rather than personalized, potentially limiting efficacy for users with atypical emotional-music associations.
- The CLaMP3 retrieval model's performance depends on the quality and diversity of the 600-video library, which may not represent all therapeutic needs.

## Confidence
- **High confidence:** The emotion classification performance metrics (Macro-F1=0.64, Weighted-F1=0.71) and user study mood improvement scores (M=4.12, p<0.001) are directly measured and statistically validated.
- **Medium confidence:** The correlation between perceived accuracy and therapeutic outcome (r=0.72) is statistically significant but may reflect shared underlying factors rather than causal directionality.
- **Medium confidence:** The three-stage "match-guide-target" narrative structure's therapeutic mechanism assumes iso-principle effectiveness without testing alternative pacing or content delivery approaches.

## Next Checks
1. **Correlation robustness test:** Conduct a within-subjects ablation study comparing 27-class emotion detection against 6-class coarse detection to determine whether the r=0.72 correlation depends on fine-grained emotion granularity.

2. **Threshold sensitivity validation:** Systematically vary the expert rule threshold τ (0.5, 0.6, 0.7, 0.8) across sessions to measure how the proportion of expert rule usage affects both perceived accuracy and therapeutic outcomes.

3. **Longitudinal efficacy assessment:** Implement a 4-week follow-up study with daily EmoHeal usage to evaluate whether therapeutic benefits persist, diminish due to habituation, or improve through system adaptation to individual user patterns.