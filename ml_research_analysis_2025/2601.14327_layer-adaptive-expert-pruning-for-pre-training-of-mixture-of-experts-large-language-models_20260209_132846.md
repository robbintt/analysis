---
ver: rpa2
title: Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large
  Language Models
arxiv_id: '2601.14327'
source_url: https://arxiv.org/abs/2601.14327
tags:
- experts
- expert
- pruning
- laep
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Layer-Adaptive Expert Pruning (LAEP), a method
  for pruning underutilized experts during the pre-training of Mixture-of-Experts
  (MoE) Large Language Models. LAEP identifies underutilized experts in each layer
  based on token distribution statistics and prunes them, then rearranges the remaining
  experts to balance the load across computing devices.
---

# Layer-adaptive Expert Pruning for Pre-Training of Mixture-of-Experts Large Language Models

## Quick Facts
- arXiv ID: 2601.14327
- Source URL: https://arxiv.org/abs/2601.14327
- Reference count: 21
- Reduces MoE model parameters by up to 33.3% and improves training efficiency by up to 48.3% through pre-training expert pruning

## Executive Summary
This paper introduces Layer-Adaptive Expert Pruning (LAEP), a method for pruning underutilized experts during the pre-training of Mixture-of-Experts (MoE) Large Language Models. LAEP identifies underutilized experts in each layer based on token distribution statistics and prunes them, then rearranges the remaining experts to balance the load across computing devices. Experiments show that LAEP reduces model parameters by up to 33.3% and improves training efficiency by up to 48.3%, while maintaining comparable performance to state-of-the-art models. For example, pre-training a 1010B Yuan3.0-1T model with LAEP achieves a 48.3% improvement in training efficiency and a 33.3% parameter reduction compared to the base 1515B model.

## Method Summary
LAEP monitors expert token distributions during pre-training to identify and prune underutilized experts after the distribution stabilizes. The method uses dual constraints: β (cumulative load threshold) identifies experts whose total token contribution is negligible, and α (individual load threshold) prunes experts receiving less than α-fraction of average load. After pruning, experts are rearranged across devices using a greedy load-balancing algorithm to minimize cross-device variance. The approach is applied once during training, after an initial volatile phase when expert loads stabilize, allowing reliable identification of persistently underutilized experts.

## Key Results
- LAEP achieves 48.3% improvement in training efficiency and 33.3% parameter reduction for 1010B Yuan3.0-1T model
- Pruned models maintain comparable performance to baselines with test loss of 1.653 vs 1.661 for 24.5% fewer parameters
- Expert rearrangement contributes 15.9% of total efficiency gain, with pruning providing 32.4%
- Layer-adaptive α values (0.2 for early/late layers, 0.4 for middle layers) with β=0.1 provide optimal balance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert token distributions stabilize after an initial volatile phase, enabling reliable identification of persistently underutilized experts
- Mechanism: During pre-training, token routing evolves through two phases: (1) an initial transition phase where expert loads fluctuate dramatically due to random initialization, lasting several hundred iterations; (2) a stable phase where per-expert token counts converge and relative expert rankings become fixed
- Core assumption: The stable-phase expert ranking reflects intrinsic expert utility rather than transient routing noise
- Evidence anchors: [section 3.1]: "The evolution of expert token allocation during pre-training can be divided into two distinct phases... once the stable phase is reached, the relative ordering of experts by token load becomes largely fixed." [section 3.1, Figure 1]: Max/Min token ratio visualized across layers showing persistent imbalance

### Mechanism 2
- Claim: Pruning underutilized experts during pre-training reduces parameter count without degrading model accuracy
- Mechanism: LAEP prunes experts using dual constraints: (1) β identifies experts whose total token contribution is negligible; (2) α prunes experts receiving less than α-fraction of average load. Underutilized experts contribute minimally to learned representations, so their removal reallocates capacity to better-trained experts
- Core assumption: Experts receiving consistently low token counts have learned less useful representations than high-load experts
- Evidence anchors: [abstract]: "LAEP achieves a 48.3% improvement in training efficiency alongside a 33.3% parameter reduction, while still delivering excellent performance." [section 3.4.1, Table 1]: Pruned model (β=0.1, α=0.4) achieves test loss 1.653 vs. baseline 1.661 with 24.5% fewer parameters

### Mechanism 3
- Claim: Expert rearrangement across devices improves training throughput by reducing computational stragglers in distributed settings
- Mechanism: After pruning, remaining experts are reassigned to computing devices via a greedy load-balancing algorithm. Experts are sorted by token load, then iteratively assigned to minimize cross-device variance
- Core assumption: Token load correlates with actual compute time; load variance translates directly to throughput loss
- Evidence anchors: [section 3.3]: "Experts are first ranked according to their token loads. They are then iteratively rearranged in a greedy manner such that, at each step, the cumulative load variance across devices is minimized." [section 4, Table 5]: Expert rearrangement contributes 15.9% of the 48.3% total efficiency gain

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) routing
  - Why needed here: LAEP relies on understanding how tokens are dispatched to experts via learned router weights and how load imbalance emerges
  - Quick check question: Given a layer with 64 experts and top-k=2 routing, what is the expected token count per expert if routing were perfectly uniform?

- Concept: Expert parallelism
  - Why needed here: The rearrangement mechanism assumes familiarity with distributing experts across GPUs/devices and the synchronization constraints this creates
  - Quick check question: Why does load imbalance across devices cause training slowdown in data-parallel expert distribution?

- Concept: Auxiliary load-balancing losses
  - Why needed here: The paper positions LAEP as a loss-free alternative to methods like DeepSeek-V3's sequence-wise auxiliary loss; understanding this baseline clarifies the tradeoff space
  - Quick check question: What negative side effect does the paper report when auxiliary loss coefficients are set too high (e.g., 0.01)?

## Architecture Onboarding

- Component map: Pre-training loop -> Router (gating network) -> Token distribution tracker -> Stability detector -> Pruning decision module -> Expert rearranger

- Critical path:
  1. Train for several hundred iterations until token distribution enters stable phase (verify via load variance monitoring)
  2. Compute per-expert token counts across all layers
  3. Apply pruning: for each layer, flag experts below β cumulative threshold, then prune those also below α individual threshold
  4. Run rearrangement algorithm (Algorithm 2) to redistribute experts across devices
  5. Resume training with pruned, rebalanced model

- Design tradeoffs:
  - **α vs. β**: β controls global pruning aggressiveness; α provides per-expert granularity. Paper recommends β=0.1 and layer-adaptive α (0.2 for early/late layers, 0.4 for middle layers)
  - **Pruning timing**: Pruning too early risks removing potentially useful experts; pruning too late forfeits efficiency gains
  - **Rearrangement overhead**: Greedy algorithm adds O(E log E) computation; worthwhile only for long training runs

- Failure signatures:
  - Pruned model shows sudden test loss spike → α or β too aggressive; reduce and retry
  - Efficiency gains minimal despite pruning → load imbalance persists across devices; verify rearrangement executed correctly
  - Downstream task performance drops on rare-domain tasks → low-load experts may have encoded specialized knowledge

- First 3 experiments:
  1. **Phase detection validation**: Train a small MoE model (e.g., 10B) and plot per-expert token counts over iterations to confirm two-phase pattern and identify transition point
  2. **Hyperparameter sweep**: Fix β=0.1, vary α ∈ {0.2, 0.4, 0.6} and measure test loss + parameter reduction on held-out validation set
  3. **Ablation: pruning-only vs. pruning+rearrangement**: Run LAEP with and without the rearrangement step to quantify each component's contribution to TFLOPS improvement

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense. However, several important limitations and areas for future work are implied:

- The method's effectiveness across different model scales and architectures beyond the tested configurations
- The impact of iterative pruning throughout training versus one-time pruning after the stable phase
- The potential limitations on downstream task performance for specialized domains where low-load experts might encode rare but critical knowledge

## Limitations
- Hardware configuration and training protocol specifications are not provided, making it difficult to reproduce absolute efficiency gains
- The theoretical foundation for the two-phase load evolution is observational rather than derived from routing dynamics
- The greedy rearrangement algorithm is heuristic with no proof or empirical comparison to optimal assignment
- Only two model sizes (10B, 20B) and one large model (1010B) are tested, limiting generalizability across scales

## Confidence
- **High confidence**: The claim that expert pruning during pre-training reduces parameters without harming accuracy (supported by test loss comparisons in Table 1 and Figure 3)
- **Medium confidence**: The claim that load stabilization enables reliable pruning decisions (observed in figures but not formally justified or tested across architectures)
- **Low confidence**: The absolute efficiency gains (48.3% TFLOPS, 33.3% parameter reduction) due to missing hardware and protocol details

## Next Checks
1. **Phase Detection Validation**: Train a small MoE model (e.g., 10B, 64 experts, 12 layers) on a standard corpus (100B tokens) and plot per-expert token counts per layer over training iterations. Identify the transition point to the stable phase and verify that expert rankings become fixed after this point.

2. **Hyperparameter Sensitivity Analysis**: Fix β=0.1 and sweep α∈{0.2, 0.4, 0.6}. For each setting, measure test loss and parameter reduction on a held-out validation set. Determine the sensitivity of model quality to pruning aggressiveness and identify the optimal α for balancing efficiency and accuracy.

3. **Ablation: Pruning vs. Pruning+Rearrangement**: Run LAEP with pruning only (no rearrangement) and with both pruning and rearrangement. Measure TFLOPS/GPU and test loss for each variant. Quantify the contribution of each component to overall efficiency gains and confirm that rearrangement meaningfully reduces device-level stragglers.