---
ver: rpa2
title: Training Autoencoders Using Stochastic Hessian-Free Optimization with LSMR
arxiv_id: '2504.13302'
source_url: https://arxiv.org/abs/2504.13302
tags:
- training
- algorithm
- mini-batch
- lsmr
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a stochastic Hessian-free optimization method
  for training deep autoencoders that combines the LSMR method with a novel mini-batch
  selection algorithm. The approach addresses two main challenges in HF optimization:
  slow training and overfitting.'
---

# Training Autoencoders Using Stochastic Hessian-Free Optimization with LSMR

## Quick Facts
- arXiv ID: 2504.13306
- Source URL: https://arxiv.org/abs/2504.13306
- Reference count: 3
- Primary result: Achieves rapid training of deep autoencoders with improved generalization error using LSMR-based stochastic Hessian-free optimization

## Executive Summary
This paper presents a stochastic Hessian-free optimization method for training deep autoencoders that addresses two main challenges in HF optimization: slow training and overfitting. The authors replace the traditional conjugate gradient method with LSMR for solving large sparse linear systems, incorporate Chapelle & Erhan's improved preconditioner, and introduce a mini-batch selection algorithm that gradually increases batch size based on variance estimates and validation performance. Experimental results on CURVES, MNIST, and USPS datasets demonstrate that this approach achieves rapid training of deep autoencoders with improved generalization error compared to traditional HF methods, while using significantly less memory during training.

## Method Summary
The method combines several key innovations: LSMR replaces conjugate gradient for solving the Gauss-Newton linear systems, providing faster convergence for least-squares problems and allowing principled early termination based on validation merit function. An adaptive mini-batch selection algorithm starts with small batches and increases size based on gradient variance estimates and validation progress. The approach uses warm-starting LSMR with the previous iteration's search direction, scaled by a decaying factor γ. A Levenberg-Marquardt damping scheme with adaptive λ maintains stability, while Chapelle & Erhan's preconditioner improves convergence. The entire framework is implemented using matrix-vector products computed through forward and backward passes, avoiding explicit Hessian computation.

## Key Results
- Demonstrates rapid training of deep autoencoders with improved generalization error compared to traditional HF methods
- Achieves significant memory savings during training by avoiding explicit Hessian formation
- Shows that LSMR-based approach converges faster than CG-based HF optimization on least-squares problems
- Successfully trains autoencoders on CURVES, MNIST, and USPS datasets with competitive performance

## Why This Works (Mechanism)

### Mechanism 1
Replacing conjugate gradient with LSMR for solving the Gauss-Newton linear systems reduces iteration count while maintaining solution quality. LSMR minimizes the residual of the normal equations (AᵀAx = Aᵀb) over a Krylov subspace, with both ∥rₖ∥₂ and ∥Aᵀrₖ∥₂ decreasing monotonically, permitting principled early termination based on validation loss rather than purely residual-based criteria. This is particularly effective for least-squares formulations where LSMR often converges faster than CG.

### Mechanism 2
Adaptive mini-batch sizing based on gradient variance and validation progress accelerates early training while controlling generalization error. Starting from a small batch n₁, the algorithm estimates ∥Var(∇f_S(w))∥₁ and compares it to θ²∥∇f_S(w)∥₂². If variance exceeds the tolerance, the batch size is increased. Independently, if relative validation improvement r_rel < 0.005, batch size is multiplied by η > 1. This dual trigger keeps gradient estimates accurate as optimization advances while limiting unnecessary data throughput early.

### Mechanism 3
Warm-starting LSMR with the previous iteration's search direction (decayed by γ) accelerates convergence along low-curvature eigendirections. Low-curvature directions persist as descent directions across iterations. By initializing LSMR with γᵢdᵢ₋₁ (γ starts at ~0.7, increases to 0.95), Krylov iterations begin from a partially converged subspace, reducing total work. LSMR can then refine the direction rather than recompute from scratch.

## Foundational Learning

- **Concept: Hessian-free optimization via matrix-vector products**
  - Why needed here: HF never forms the full Hessian; it only requires Hv products computed via forward/backward passes. Understanding this distinction is essential for implementing LSMR's operator-based interface.
  - Quick check question: Given a network with 1M parameters, why can HF update weights while explicit Newton's method cannot?

- **Concept: Gauss-Newton approximation for nonlinear least squares**
  - Why needed here: Autoencoder training minimizes reconstruction error (nonlinear least squares). The Gauss-Newton matrix JᵀJ approximates the Hessian by dropping second-derivative terms, guaranteeing positive semi-definiteness.
  - Quick check question: Why is the Gauss-Newton matrix always positive semi-definite, unlike the true Hessian?

- **Concept: Levenberg-Marquardt damping**
  - Why needed here: The λ²I term interpolates between Gauss-Newton (λ→0) and gradient descent (λ large). Adaptive λ based on ρ (actual/predicted reduction) stabilizes training.
  - Quick check question: If ρ < 0.25, should λ increase or decrease, and why?

## Architecture Onboarding

- **Component map:**
  - Algorithm 1 (main loop) -> Algorithm 2 (BatchSize) -> Algorithm 3 (Precon) -> Algorithm 6 (LSMR) -> Algorithm 4 & 5 (A(d) and AT(U))

- **Critical path:**
  1. Sample mini-batch → compute S_ℓ, S'_ℓ (activations and derivatives)
  2. Build preconditioner c via backward passes with random ±1 vectors
  3. Run LSMR: each iteration calls A(d) and AT(U), checks merit function every ~1.25× iterations
  4. Compute ρ, update λ, perform backtracking line search
  5. Update weights, decay γ, adjust batch size for next iteration

- **Design tradeoffs:**
  - **Memory vs. speed:** Caching S_ℓ, S'_ℓ for all LSMR iterations reduces recomputation but increases memory. The matrix-as-vector-of-matrices representation avoids vectorization overhead.
  - **Preconditioner cost vs. iteration count:** Chapelle & Erhan's preconditioner costs one backward pass per mini-batch instance but can halve LSMR iterations. For very deep networks, verify the tradeoff empirically.
  - **Early termination aggressiveness:** Smaller ftol and miniter yield faster iterations but risk suboptimal directions; larger values increase robustness at computational cost.

- **Failure signatures:**
  - Validation merit function oscillates or increases consistently → LSMR terminates too early; increase miniter or recover
  - λ oscillates wildly instead of decaying → damping schedule too aggressive; use drop = 0.99 rather than 0.5
  - Batch size jumps to maximum immediately → θ too small; increase θ (e.g., 0.2 → 0.5)
  - Training stalls with flat loss → preconditioner may be near-zero; check for numerical underflow in Algorithm 3

- **First 3 experiments:**
  1. **Baseline replication:** Run Algorithm 1 on MNIST with Table 1 hyperparameters; confirm training/test errors within 5% of reported values. Ablate LSMR warm-start (set γ₁ = 0) to measure iteration savings.
  2. **Batch schedule sensitivity:** Vary θ ∈ {0.1, 0.3, 0.5} and η ∈ {1.005, 1.01}; plot total gradient evaluations vs. final test error to find Pareto frontier.
  3. **Preconditioner ablation:** Compare Chapelle & Erhan preconditioner vs. Martens' empirical Fisher diagonal vs. no preconditioning on CURVES; record LSMR iterations per HF step and wall-clock time.

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can the matrix-based LSMR approach be adapted for architectures with weight sharing, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs)?
**Basis in paper:** [Explicit] Section 2 reviews prior work applying Hessian-free optimization to RNNs and CNNs, but the methodology here is restricted to "deep autoencoders."
**Why unresolved:** The proposed matrix calculus (Section 3.1) and matrix-vector product algorithms (Algorithms 4 & 5) are derived specifically for fully-connected weight matrices $W_\ell$, not the structured sparse matrices found in convolutions.
**What evidence would resolve it:** Extending the definition of the space $\mathcal{W}$ and the forward/backward mappings to handle convolutional kernels and demonstrating convergence on a standard CNN benchmark.

### Open Question 2
**Question:** Why do logistic activation functions outperform ReLU in this specific Hessian-free framework?
**Basis in paper:** [Explicit] Section 6 states: "It is worth noting that the logistic activations outperform the ReLU activations in our experiments."
**Why unresolved:** The authors report the empirical observation but do not provide a theoretical explanation regarding the interaction between LSMR, the Gauss-Newton matrix, and the non-smooth nature of ReLUs.
**What evidence would resolve it:** An ablation study analyzing the spectral properties of the curvature matrix and the convergence behavior of LSMR when applied to ReLU-based autoencoders compared to logistic ones.

### Open Question 3
**Question:** Why does the method yield lower generalization accuracy on the synthetic CURVES dataset compared to the baseline, despite improvements on MNIST and USPS?
**Basis in paper:** [Explicit] Table 2 shows the test error on CURVES is 0.214 for the proposed method versus 0.098 for Martens (2010). Section 6 hypothesizes the algorithm's "anti-overfitting" nature may be suboptimal for synthetic data but does not confirm the mechanism.
**Why unresolved:** It is unclear if the dynamic batch sizing or the specific merit function termination criteria (Section 5) halt optimization prematurely on simpler, noise-free distributions.
**What evidence would resolve it:** An analysis of the optimization trajectory on CURVES to determine if the validation merit function $\phi(d)$ causes early termination before the model fits the noise-free data.

## Limitations

- The adaptive mini-batch selection algorithm's effectiveness relies heavily on gradient variance estimates computed on small batches being predictive of larger-batch behavior, which may break down for highly non-stationary objectives or small datasets
- Warm-starting LSMR with the previous iteration's direction assumes the loss landscape changes slowly, which may not hold near sharp curvature changes or saddle points
- The paper lacks ablation studies on the preconditioner's contribution, making it difficult to quantify its impact versus other algorithmic components

## Confidence

- **High confidence** in the core LSMR implementation and its advantages for solving least-squares problems, supported by well-established numerical analysis literature
- **Medium confidence** in the mini-batch selection algorithm's practical effectiveness, as the variance-based criterion is theoretically grounded but its interaction with HF optimization hasn't been thoroughly validated
- **Medium confidence** in the overall approach's superiority, given strong experimental results but limited ablation studies and no comparison to modern first-order optimizers with momentum

## Next Checks

1. Ablate the LSMR warm-start (set γ=0) and measure the increase in iteration count to quantify the convergence benefit
2. Compare training/test error trajectories with and without the Chapelle & Erhan preconditioner to isolate its contribution
3. Test the mini-batch selection algorithm with θ values spanning {0.1, 0.3, 0.5} to identify the optimal tradeoff between gradient variance tolerance and computational efficiency