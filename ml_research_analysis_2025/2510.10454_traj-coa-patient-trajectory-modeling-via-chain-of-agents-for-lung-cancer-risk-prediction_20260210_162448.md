---
ver: rpa2
title: 'Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer
  Risk Prediction'
arxiv_id: '2510.10454'
source_url: https://arxiv.org/abs/2510.10454
tags:
- risk
- cancer
- patient
- lung
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Traj-CoA introduces a multi-agent system to handle long and noisy
  electronic health records for patient trajectory modeling. It processes EHR data
  in manageable, time-aware chunks via a chain of worker agents that extract task-related
  events and update a shared long-term memory module, EHRMem.
---

# Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction

## Quick Facts
- arXiv ID: 2510.10454
- Source URL: https://arxiv.org/abs/2510.10454
- Reference count: 40
- Primary result: Traj-CoA achieves 0.766 AUROC for lung cancer risk prediction using 5 years of EHR data

## Executive Summary
Traj-CoA introduces a multi-agent system to handle long and noisy electronic health records for patient trajectory modeling. It processes EHR data in manageable, time-aware chunks via a chain of worker agents that extract task-related events and update a shared long-term memory module, EHRMem. A manager agent then synthesizes the worker agents' summaries and EHRMem to make final predictions. Evaluated on a lung cancer risk prediction task using five years of EHR data, Traj-CoA achieved an AUROC of 0.766, outperforming zero-shot baselines of four categories and most supervised fine-tuned models. Ablation studies confirm the importance of EHRMem in maintaining critical early events. Traj-CoA demonstrates robust temporal reasoning and generalizability for complex patient trajectory modeling tasks.

## Method Summary
Traj-CoA is a zero-shot Chain-of-Agents framework for patient trajectory modeling that addresses the lost-in-the-middle degradation problem in long EHR sequences. The method uses time-aware chunking (default 8k tokens) to split EHR data into manageable segments processed by sequential worker agents. Each worker extracts potentially task-relevant events into a shared EHRMem module and updates a summary for the next agent. A final manager agent synthesizes all summaries and EHRMem to make risk predictions. The framework uses MedGemma-27B as the base model and requires EHR data in unified XML format with chronological ordering. Evaluation is performed on a proprietary lung cancer risk prediction task with 300 test samples (28 cases, 272 controls) spanning up to 5 years of longitudinal EHR data.

## Key Results
- Achieved 0.766 AUROC for lung cancer risk prediction, outperforming zero-shot baselines across four categories
- EHRMem ablation caused 1.8% AUROC and 8.1% F1 score degradation, confirming its importance
- Optimal chunk size identified at 8k tokens, with performance degrading at both smaller (2k) and larger (16k) sizes
- Model successfully identifies diverse event categories across full 5-year horizon with clinically aligned themes

## Why This Works (Mechanism)

### Mechanism 1: Time-Aware Chunking to Mitigate Lost-in-the-Middle Degradation
Partitioning long EHR sequences into moderate-sized chunks processed by sequential agents reduces attention degradation in middle context positions. XML input is split by timestamp boundaries into chunks ≤k tokens; each worker agent processes one chunk with bounded context. This avoids the quadratic attention scaling and middle-position forgetting observed in vanilla long-context LLMs.

### Mechanism 2: EHRMem Prevents Progressive Abstraction Loss
An external long-term memory module preserves early critical events that would be lost through sequential summarization alone. Worker agents extract potentially task-relevant events into EHRMem with timestamps. Deduplication prevents EHR "copy-forward" noise. Manager agent conditions on both the final summary SC and full memory M for prediction.

### Mechanism 3: Hierarchical Worker-Manager Synthesis Enables Global Temporal Reasoning
Two-stage architecture (sequential workers + final manager) enables integration of local chunk insights with global timeline context. Worker Wi produces summary Si from chunk ci and previous summary Si-1. Manager M receives final summary SC and EHRMem to synthesize trajectory-level reasoning and final prediction.

## Foundational Learning

- **Lost-in-the-Middle Phenomenon**: Explains why naive long-context LLMs fail on extended EHR (64k context degraded to 0.714 AUROC vs. 32k at 0.743). Quick check: If you double the context window of a vanilla LLM on your task, does performance improve or degrade?

- **Sequential Information Aggregation**: Worker chain implements this by passing updated summaries Si-1 → Si across chunks. Quick check: Can you trace how a risk factor from year 1 propagates through the agent chain to the final output?

- **Task-Specific Event Extraction**: EHRMem population uses an "intentionally inclusive" heuristic—workers extract potentially relevant events, not just obviously relevant ones. Quick check: Does your extraction prompt distinguish between "definitely predictive" and "potentially relevant" events?

## Architecture Onboarding

- **Component map**: Preprocessing (EHR → XML) → Chunking (time-aware ≤8k tokens) → Worker Chain (sequential agents) → EHRMem (shared memory) → Manager Agent (synthesis + prediction)

- **Critical path**: 
  1. Format heterogeneous EHR into unified XML preserving temporal order
  2. Chunk by timestamp boundaries under token limit (default 8k)
  3. Initialize EHRMem empty; first worker processes chunk 1 with no prior summary
  4. Each subsequent worker: reads chunk + prior summary + memory context → updates summary + appends new events to memory
  5. Manager: synthesizes SC and M → final risk prediction (1-10 scale)

- **Design tradeoffs**: 
  - Chunk size: Small (2k) → more agent hops, higher abstraction loss; Large (16k) → intra-chunk lost-in-the-middle
  - Memory context window k: Too small → deduplication fails; Too large → prompt bloat
  - Extraction inclusivity: Over-inclusive → noise accumulation; Under-inclusive → missed early signals

- **Failure signatures**: 
  - Over-reliance on recent benign imaging: False negatives from dismissing risk despite stable recent scans
  - Never-smoker underestimation: Model may overweight smoking status absence; environmental/occupational factors under-captured
  - Recency bias without memory: F1 drops 8.1% without EHRMem, indicating early events underweighted

- **First 3 experiments**:
  1. Chunk size sweep: Fix total context at 80k; vary chunk size from 2k to 16k; expect U-shaped curve with optimum near 8k
  2. EHRMem ablation: Run with and without memory module; quantify AUROC/F1 gap to validate mechanism contribution
  3. Context scaling test: Increase max chunks from 5 to 20 (40k→160k tokens); verify AUROC continues improving (unlike vanilla LLM which degrades past 32k)

## Open Questions the Paper Calls Out
- How does Traj-CoA’s event synthesis mechanism differ when processing trajectories for specific subpopulations, such as never-smokers, where standard risk heuristics may fail?
- Does the Traj-CoA framework generalize to multi-institutional data with distinct EHR vendor formats and coding inconsistencies without manual prompt engineering?
- Can automated prompt optimization or reinforcement learning replace the manual crafting of agent instructions while maintaining predictive accuracy?

## Limitations
- Performance claims based on small test set (28 cases, 272 controls) limiting statistical power and generalizability
- Proprietary dataset inaccessible for independent verification of results
- Zero-shot evaluation approach makes direct comparison to supervised methods difficult

## Confidence
- **High confidence**: Core architectural innovation (time-aware chunking + EHRMem + hierarchical worker-manager synthesis) is well-specified and mechanistically sound
- **Medium confidence**: Performance claims plausible but limited by dataset access and small sample size; ablation studies support EHRMem contribution
- **Low confidence**: External validation on different datasets, particularly public EHR sources like MIMIC, has not been demonstrated

## Next Checks
1. Implement Traj-CoA on a public EHR dataset (e.g., MIMIC-IV with cancer registry linkage) and systematically vary chunk sizes (2k, 4k, 8k, 16k) to confirm the U-shaped AUROC curve with peak performance at 8k tokens.

2. Train/evaluate Traj-CoA with and without the EHRMem module on a separate lung cancer dataset to quantify the performance gap (target: ≥1.8% AUROC drop) and validate the mechanism's contribution.

3. Extend the maximum number of chunks beyond 15 (160k tokens) on a dataset with longer patient histories to verify whether AUROC continues improving (unlike vanilla LLMs which degrade past 32k context).