---
ver: rpa2
title: 'MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with
  Fables'
arxiv_id: '2509.12371'
source_url: https://arxiv.org/abs/2509.12371
tags:
- moral
- fables
- morals
- story
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MORABLES is a benchmark for evaluating moral reasoning in LLMs
  using fables and short stories. It presents multiple-choice questions where models
  must select the correct moral from five options, including distractors designed
  to challenge superficial reasoning.
---

# MORABLES: A Benchmark for Assessing Abstract Moral Reasoning in LLMs with Fables

## Quick Facts
- arXiv ID: 2509.12371
- Source URL: https://arxiv.org/abs/2509.12371
- Reference count: 40
- Models struggle with abstract moral reasoning, often relying on thematic matching rather than true inference

## Executive Summary
MORABLES is a benchmark that evaluates LLMs' ability to infer morals from fables and short stories through multiple-choice questions. The benchmark includes 709 fable-moral pairs with four types of distractors designed to challenge superficial reasoning. Experiments show that larger models significantly outperform smaller ones, with scale appearing to be the primary driver of performance rather than explicit reasoning capabilities. Even the best models struggle with nuanced moral inference, often relying on shallow patterns like partial narrative cues and thematic associations.

## Method Summary
The benchmark uses 709 fable-moral pairs from Western literature, presenting models with five moral options (one correct, four distractors) for each story. Models are prompted to select the correct moral ID (A-E) using one-shot prompting and temperature=0. Variants include TF (True/False binary judgments), NOTO (None of the others option), and ADV (adversarial modifications). Evaluation metrics include accuracy, precision/recall/F1, consistency across framings, and human-annotated quality ratings.

## Key Results
- Larger models significantly outperform smaller ones, with a 40+ percentage point gap between Mistral 7B (28.4%) and Llama 3.3 70B (73.6%)
- Reasoning-enhanced models like GPT-o3-mini underperform standard models like GPT-4o, suggesting scale matters more than reasoning ability
- Models frequently select partial-story distractors (13-29% of responses), indicating over-reliance on early narrative cues
- TF-NOTO consistency shows ~20% self-contradiction rates, revealing instability in moral judgments across framings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model scale, not explicit reasoning capabilities, drives moral inference performance on fable-based tasks.
- Mechanism: Larger parameter counts encode richer semantic representations and narrative patterns from training data, enabling better thematic association between stories and morals without explicit reasoning steps.
- Core assumption: Performance gains reflect improved pattern recognition rather than deeper moral understanding.
- Evidence anchors: 40+ percentage point performance gap between models of different sizes; reasoning-enhanced models underperform standard models.

### Mechanism 2
- Claim: Models frequently select morals based on partial narrative cues and character associations rather than holistic story comprehension.
- Mechanism: The "partial-story" distractor is the most frequently selected incorrect option, suggesting models overweight early textual tokens when forming moral judgments.
- Core assumption: Models lack mechanisms to integrate full narrative arcs into abstract conclusions.
- Evidence anchors: Partial-story distractor is most frequent incorrect choice across nearly all models (29.3% for Mistral 7B, 13.1% for Llama 3.3 70B).

### Mechanism 3
- Claim: Task framing and positional biases significantly affect model decisions, revealing instability in moral judgment.
- Mechanism: Models exhibit "attention sinks" at text boundaries and strong aversion to "None of the others" options, leading to ~20% self-contradiction when the same moral is evaluated under different framings.
- Core assumption: RLHF fine-tuning induces sycophantic tendencies - models prefer selecting any plausible option over rejecting all.
- Evidence anchors: Consistency metric shows DeepSeek V3 selects morals it previously rejected in TF ~50% of the time in NOTO; Claude 3.5 shows ~38% inconsistency.

## Foundational Learning

- **Multiple-Choice Question Answering (MCQA) Evaluation**
  - Why needed here: MORABLES uses MCQA format; understanding how models select among options - and how distractors exploit shortcuts - is essential for interpreting results.
  - Quick check question: When a model selects option B at 75% accuracy, what additional evidence would confirm genuine understanding versus pattern matching?

- **Thematic Matching vs. Abstract Reasoning**
  - Why needed here: The paper's central claim is that models perform thematic matching rather than abstract moral reasoning; distinguishing these computationally is non-trivial.
  - Quick check question: If a model correctly matches "The Wolf and the Crane" to "Expect no reward for serving the wicked," what adversarial test could distinguish rote association from inferred principle?

- **Consistency Metrics Across Task Framings**
  - Why needed here: The TF/NOTO consistency metric reveals self-contradiction; understanding this diagnostic is critical for robustness assessment.
  - Quick check question: A model labels a moral "True" in TF but selects it in NOTO when "None of the others" is available - is this sycophancy or calibrated uncertainty?

## Architecture Onboarding

- **Component map**: Dataset (709 fable-moral pairs) -> Distractor generation (4 types) -> Prompt template (one-shot) -> Model API -> Answer extraction -> Evaluation metrics
- **Critical path**: Load fable and 5 moral candidates → Apply prompt template with one example → Extract first generated token as answer → For TF/NOTO: compare against model's own prior judgments to compute consistency
- **Design tradeoffs**: One-shot vs. zero-shot (4-6% accuracy gain with one example, but introduces example-dependence); Next-token vs. log-probability selection (statistically equivalent for open models, unavailable for closed/reasoning models); 5 vs. 8 choices (performance similar; quality of distractors matters more than quantity)
- **Failure signatures**: High partial-story selection rate → model over-weights early narrative tokens; Low NOTO accuracy with high TF recall → sycophantic preference for any option over rejection; Large TF-NOTO inconsistency → unstable internal representations of moral validity
- **First 3 experiments**: 1) Baseline establishment: Run Llama 3.3 70B and GPT-4o on Core variant with one-shot prompting; record per-distractor error distribution to identify dominant failure mode. 2) Consistency stress test: Evaluate the same model on TF then NOTO variants; compute consistency metric to quantify self-contradiction rate. 3) Adversarial robustness probe: Apply ADV variant with all three modifications (character swap + trait injection + appended tautology); compare accuracy drop to baseline to assess memorization vs. reasoning reliance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs perform genuine moral inference on entirely new, unseen narratives, or do high accuracy scores primarily reflect memorization of pre-training data?
- Basis: Authors state a more effective solution would be to create entirely new, unseen short stories to test whether LLMs can still infer a moral in those cases.
- Why unresolved: Current adversarial modifications partially address memorization but cannot definitively distinguish recall from reasoning; classical fables were likely in pre-training corpora.
- What evidence: Performance evaluation on newly authored fables with verified exclusion from all pre-training data, compared against adversarial variants of known fables.

### Open Question 2
- Question: How do LLMs perform on moral inference when evaluated on fables from non-Western cultural traditions with differing ethical frameworks?
- Basis: Dataset is "predominantly sourced from Western stories" and authors acknowledge it is "more than likely biased toward moral lessons that reflect Western values."
- Why unresolved: Future work will expand the dataset to include fables from diverse cultures, facilitating the study of cultural bias and differing ethical perspectives.
- What evidence: Cross-cultural benchmark with fables from diverse traditions, analyzing performance gaps and systematic biases in moral selection across cultures.

### Open Question 3
- Question: What specific mechanisms underlie the finding that scale contributes more to moral inference performance than explicit reasoning capabilities?
- Basis: Reasoning-enhanced models fail to bridge this gap, suggesting that scale – not reasoning ability – is the primary driver of performance.
- Why unresolved: GPT-o3-mini underperforms while DeepSeek R1 performs well; the paper does not explain why reasoning architectures yield inconsistent results or what scale provides that reasoning steps do not.
- What evidence: Controlled experiments matching model sizes across reasoning and non-reasoning variants; analysis of internal representations and attention patterns during moral inference.

## Limitations
- Benchmark relies on Western fable literature, potentially encoding cultural biases that may not transfer to non-Western moral reasoning contexts
- Distractor generation process used GPT-4o, creating potential circularity where distractors may align too closely with model-specific weaknesses
- One-shot prompting protocol introduces example-dependence that may not reflect zero-shot moral reasoning capabilities

## Confidence
- **Scale drives performance over reasoning capabilities**: High confidence - supported by consistent 40+ percentage point gaps between models of different sizes and counterintuitive underperformance of reasoning-enhanced models
- **Models rely on thematic matching rather than abstract reasoning**: Medium confidence - evidenced by partial-story distractor selection and thematic association patterns, but direct evidence distinguishing rote memorization from inference is limited
- **Sycophantic tendencies cause self-contradiction in moral judgments**: Medium confidence - consistency metrics show clear self-contradiction, but the causal link to RLHF fine-tuning requires additional experimental validation

## Next Checks
1. **Cross-cultural validation**: Evaluate MORABLES on non-Western fable collections (e.g., Aesop's Fables vs. Panchatantra) to test whether performance patterns replicate across cultural contexts and identify potential cultural encoding biases.

2. **Reasoning architecture ablation**: Compare reasoning-augmented models (chain-of-thought, value-grounded reasoning frameworks) against scale-matched non-reasoning models on MORABLES to isolate whether architectural modifications can overcome the scale ceiling observed with current reasoning models.

3. **Sycophancy intervention experiment**: Apply direct preference optimization (DPO) to reduce sycophantic tendencies in models, then re-evaluate TF-NOTO consistency on MORABLES to determine if framing sensitivity can be mitigated without degrading core moral inference accuracy.