---
ver: rpa2
title: 'Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question
  Answering'
arxiv_id: '2510.11928'
source_url: https://arxiv.org/abs/2510.11928
tags:
- question
- passage
- topic
- passages
- discrepancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MIND is a user-in-the-loop pipeline for detecting factual and cultural
  discrepancies in multilingual QA knowledge bases. It uses polylingual topic modeling
  to align documents thematically, generates questions in an anchor language, retrieves
  relevant passages in a comparison language, and uses LLMs to classify discrepancies.
---

# Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering

## Quick Facts
- **arXiv ID:** 2510.11928
- **Source URL:** https://arxiv.org/abs/2510.11928
- **Reference count:** 40
- **Primary result:** MIND pipeline detects factual and cultural discrepancies in multilingual QA with F1 scores of 0.91–0.92 on controlled data and reliably surfaces inconsistencies in real-world health datasets.

## Executive Summary
MIND is a user-in-the-loop pipeline designed to detect factual and cultural discrepancies in multilingual question answering knowledge bases. It uses polylingual topic modeling to align documents thematically, generates questions from an anchor language corpus, retrieves relevant passages from a comparison language corpus, and uses large language models to classify discrepancies. Evaluated on bilingual QA data in maternal and infant health (Rosie) and on synthetic and Wikipedia datasets, MIND achieves high F1 scores on controlled data and reliably surfaces real-world inconsistencies, though human review remains necessary for fine-grained validation.

## Method Summary
MIND employs polylingual topic modeling to create a shared thematic space across languages, then generates standalone questions from an anchor corpus using an LLM. These questions are used to retrieve relevant passages from a comparison corpus within specific topic clusters, improving retrieval precision over global vector search. The system classifies discrepancies into four categories—No Discrepancy, Contradiction, Cultural Discrepancy, and Not Enough Info—using a fine-tuned LLM. The pipeline is designed for user-in-the-loop validation, with active learning planned to reduce annotation burden.

## Key Results
- MIND achieves high F1 scores on controlled data (0.91–0.92 weighted).
- Topic-based retrieval improves over baselines (MRR@3: 0.72 vs 0.64 for ANN).
- The system reliably surfaces real-world inconsistencies in maternal and infant health QA data.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constrained retrieval within topic-aligned clusters improves search relevance over global vector search.
- **Mechanism:** The pipeline uses Polylingual Topic Modeling (PLTM) to map both corpora into a shared thematic space. Instead of searching the entire comparison corpus, retrieval is restricted to specific topic clusters (filtered by a relevance threshold $\epsilon$). This filtering reduces the search space to semantically relevant regions, increasing the likelihood that the retrieved passage actually addresses the anchor's question.
- **Core assumption:** Documents in different languages discussing similar themes will converge on similar topic distributions, even if they are not direct translations.
- **Evidence anchors:**
  - [abstract] "Topic-based retrieval improved over baselines (MRR@3: 0.72 vs 0.64 for ANN)."
  - [section 4.4.2] "TB-ENN-W consistently has best retrieval... [vs] ANN baseline."
- **Break condition:** If the topic model fails to align themes, retrieval precision will degrade, resulting in "Not Enough Info" errors.

### Mechanism 2
- **Claim:** Asymmetric question generation and answering surfaces discrepancies that symmetric text comparison misses.
- **Mechanism:** The system generates questions derived *only* from the anchor corpus and then forces the comparison corpus to answer them. By "decontextualizing" the questions (making them standalone), the system tests if the knowledge in the comparison corpus supports the specific claims in the anchor corpus. This mimics a user query rather than a document diff.
- **Core assumption:** A discrepancy exists if a question validly generated from Context A cannot be answered consistently using Context B.
- **Evidence anchors:**
  - [abstract] "generates questions in an anchor language, retrieves relevant passages in a comparison language..."
  - [section 3.3] "...questions... grounded in [anchor passage]... [are] the basis for detecting discrepancies in [comparison corpus]."
- **Break condition:** If the LLM generates questions that are ambiguous or reliant on "anecdotal" context, the comparison corpus will fail to answer, triggering false positives.

### Mechanism 3
- **Claim:** LLMs can separate "factual contradiction" from "cultural discrepancy" via specialized classification prompts.
- **Mechanism:** Standard Natural Language Inference (NLI) typically uses binary labels (Entail/Contradict). MIND introduces a 4-way classification scheme: No Discrepancy, Contradiction, Cultural Discrepancy, and Not Enough Info. The LLM is instructed to label a pair as a "Cultural Discrepancy" if the difference stems from regional norms (e.g., medical guidelines) rather than factual error.
- **Core assumption:** The LLM possesses sufficient world knowledge to distinguish between "wrong" and "culturally different" when provided with the evidence pair.
- **Evidence anchors:**
  - [section 3.6] "We prompt an LLM... to determine whether [answer A] entails... contradicts... or differs due to a cultural discrepancy."
  - [section 4.4.3] "gpt-4o fails to detect any discrepancy type [in the Rosie subset]... while qwen:32b is slightly better at handling discrepancies."
- **Break condition:** If the LLM is not explicitly calibrated for the "Cultural" class, it defaults to "Contradiction" (over-flagging) or "No Discrepancy" (under-flagging).

## Foundational Learning

- **Concept: Polylingual Topic Modeling (PLTM)**
  - **Why needed here:** Standard topic models (LDA) work on single languages. PLTM is required to create a shared "concept space" so the system knows that an English document about "Infants" corresponds to a Spanish document about "Bebés".
  - **Quick check question:** Can you explain how PLTM uses "loose alignment" (or parallel corpora) to infer topic distributions across languages without requiring perfect translations?

- **Concept: Natural Language Inference (NLI)**
  - **Why needed here:** The core of the discrepancy detector relies on NLI logic (Entailment vs. Contradiction). Understanding the limits of standard NLI (which struggles with cultural nuance) is key to understanding why MIND adds a custom class.
  - **Quick check question:** How does the definition of "contradiction" in standard NLI datasets differ from the "cultural discrepancy" class defined in this paper?

- **Concept: Decontextualization**
  - **Why needed here:** To generate valid search queries, the LLM must strip context-dependent references (e.g., "the study") and replace them with absolute terms (e.g., "The 2023 NIH study").
  - **Quick check question:** Why is query decontextualization critical before performing cross-lingual retrieval?

## Architecture Onboarding

- **Component map:** Corpora (Anchor & Comparison) -> PLTM Trainer -> LLM Q-Gen -> FAISS Retriever -> LLM Classifier -> Human UI
- **Critical path:** The **Retrieval** component. If the wrong passage is retrieved from the comparison corpus, the resulting answer will be irrelevant, leading the classifier to mark it as "Not Enough Info" or a false "Contradiction."
- **Design tradeoffs:**
  - **Exact vs. Approximate Search (ENN vs. ANN):** ENN yields higher accuracy (MRR 0.72) but is slower; ANN is faster but less precise.
  - **LLM Selection:** The paper notes `gpt-4o` is better at strict factuality, while `qwen:32b` is better at detecting the nuance of "Cultural Discrepancies," implying a choice between precision and sensitivity.
- **Failure signatures:**
  - **False Contradictions:** Occur when retrieved passages are thematically similar but contain different specific statistics (e.g., "1 in 88" vs "1 in 68") due to source age, not factual error.
  - **False Cultural Flags:** Annotators often marked LLM-flagged "Cultural Discrepancies" as simple "Contradictions," showing the boundary is unstable.
- **First 3 experiments:**
  1. **Validate Topic Alignment:** Train PLTM on a sample of your data. Manually inspect the top words for Topic $k$ in English vs. Spanish to ensure they describe the same concept.
  2. **Ablate Retrieval:** Run the pipeline with standard ANN search vs. Topic-Based ENN. Compare the "Not Enough Info" rate; high rates indicate retrieval failure.
  3. **Calibrate the Classifier:** Run the FEVER-DPLACE-Q synthetic benchmark. If the model fails to distinguish "Contradiction" from "Cultural Discrepancy," refine the system prompt definitions.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can active learning reduce the human annotation burden in MIND by accurately predicting when model uncertainty warrants review?
- **Basis in paper:** [explicit] The authors state they "plan to incorporate active learning, prompting review only when model uncertainty is high" to reduce human effort.
- **Why unresolved:** Current pipeline requires human review for all flagged discrepancies; no uncertainty quantification mechanism exists.
- **What evidence would resolve it:** A study comparing annotation effort and accuracy between full-review and active-learning-guided review on ROSIE-MIND.

### Open Question 2
- **Question:** How does MIND perform in multilingual settings beyond two languages?
- **Basis in paper:** [inferred] The paper notes the approach "naturally extends to multiple comparison corpora" but evaluates only bilingual English–Spanish and English–German pairs.
- **Why unresolved:** No experimental validation of scalability or retrieval quality when aligning three or more languages simultaneously.
- **What evidence would resolve it:** Evaluation on a trilingual or larger corpus with PLTM alignment and discrepancy detection across all language pairs.

### Open Question 3
- **Question:** Would introducing a "contextual difference" category improve classification accuracy for cases that are neither contradictions nor cultural discrepancies?
- **Basis in paper:** [explicit] The authors observe that annotators struggle with borderline cases and suggest "introducing an additional category (e.g., contextual differences) may also help."
- **Why unresolved:** Current four-category scheme conflates regulatory/jurisdictional variation with cultural discrepancies, leading to annotator disagreement.
- **What evidence would resolve it:** Re-annotation of ROSIE-MIND with five categories and comparison of inter-annotator agreement and F1 scores.

## Limitations

- The pipeline's effectiveness depends critically on the quality of polylingual topic alignment; misalignment can lead to retrieval failures.
- The LLM classifier's ability to distinguish "cultural" from "factual" discrepancies remains fragile, with low inter-annotator agreement (κ = 0.30).
- The Rosie maternal health corpus is small (92 QA pairs), limiting statistical confidence in real-world evaluations.

## Confidence

- **High confidence:** Topic-based retrieval improves over ANN baseline (MRR@3: 0.72 vs. 0.64).
- **Medium confidence:** MIND detects discrepancies in real-world Rosie data, though the sample size is limited.
- **Low confidence:** The LLM reliably separates cultural from factual discrepancies, as inter-annotator agreement is poor (κ = 0.30) and model choice significantly affects outcomes.

## Next Checks

1. **Validate PLTM alignment quality:** Train PLTM on a held-out sample of your multilingual corpus. Manually inspect top topic words in both languages to ensure semantic alignment.
2. **Stress-test retrieval with noisy topics:** Deliberately inject misaligned topic clusters. Measure the "Not Enough Info" rate; high rates indicate retrieval fragility.
3. **Refine cultural vs. factual boundaries:** Run the classifier on a subset of Rosie data with explicit cultural vs. factual labels. If the model cannot distinguish them, revise the prompt definitions or add a calibration step.