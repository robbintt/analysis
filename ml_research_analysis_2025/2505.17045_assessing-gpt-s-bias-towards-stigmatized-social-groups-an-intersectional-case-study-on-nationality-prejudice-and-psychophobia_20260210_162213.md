---
ver: rpa2
title: 'Assessing GPT''s Bias Towards Stigmatized Social Groups: An Intersectional
  Case Study on Nationality Prejudice and Psychophobia'
arxiv_id: '2505.17045'
source_url: https://arxiv.org/abs/2505.17045
tags:
- north
- bias
- biases
- scale
- mental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study examines intersectional biases in GPT models toward
  stigmatized social groups by evaluating responses to scenarios involving American
  and North Korean nationalities with various mental disabilities. Using structured
  prompts across five everyday interactions, the research finds that GPT-3.5/4/4o
  models exhibit greater negative bias toward North Koreans compared to Americans,
  particularly when mental disability is a factor.
---

# Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia

## Quick Facts
- arXiv ID: 2505.17045
- Source URL: https://arxiv.org/abs/2505.17045
- Reference count: 0
- Primary result: GPT models exhibit greater negative bias toward North Koreans than Americans, particularly when mental disability is involved, with inconsistent rating behaviors under scale inversion.

## Executive Summary
This study investigates intersectional biases in GPT models toward stigmatized social groups by evaluating responses to scenarios involving American and North Korean nationalities with various mental disabilities. Using structured prompts across five everyday interactions, the research finds that GPT-3.5/4/4o models exhibit greater negative bias toward North Koreans compared to Americans, particularly when mental disability is a factor. Results show inconsistencies in rating methods and stochastic modeling, with North Koreans receiving lower empathy ratings even under flipped scales. The models also make assumptions about cultural differences and discuss mental disabilities without considering symptom spectrums. These findings highlight the need for improved LLM design incorporating diverse cultural perspectives and nuanced understanding of intersectional identities to ensure equitable treatment of global users.

## Method Summary
The study employed a factorial experimental design testing 2 nationalities (American, North Korean) × 6 mental disabilities (Bipolar/Depression/Schizophrenia × remitted/symptomatic) × 5 everyday scenarios (renting, coworker, marriage, neighbor, childcare), creating 60 unique conditions. Each condition was evaluated using Likert scale ratings (0-3) under both regular and flipped scale conditions, with follow-up explanation requests for specific scenarios. The methodology involved three-step prompting: initial rating prompts, scale-flipped repetitions, and qualitative explanation requests. Results were aggregated across multiple trials per condition, with standard deviations and cross-group comparisons computed to identify bias patterns.

## Key Results
- GPT models showed significantly lower empathy ratings for North Korean individuals compared to Americans across all tested scenarios and disabilities.
- Mental disability compounded negative bias, with North Koreans facing greater prejudice when mental health conditions were specified.
- Rating inconsistencies emerged under scale inversion, with North Koreans receiving higher (less favorable) ratings under flipped scales despite semantically equivalent prompts.
- Models made cultural-difference assumptions asymmetrically, generating such explanations for North Korean prompts but not American ones.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intersectional identity combinations amplify negative bias beyond single-attribute bias.
- Mechanism: When stigmatized attributes (nationality + mental disability) co-occur in prompts, model outputs reflect compounded negative associations present in training data, producing lower empathy ratings than either attribute alone would generate.
- Core assumption: Training corpus contains correlated negative sentiment toward multiple stigmatized identities that models learn as joint probability distributions.
- Evidence anchors: [abstract] "Findings reveal significant discrepancies in empathy levels with North Koreans facing greater negative bias, particularly when mental disability is also a factor." [results] "higher ratings (less willingness) applied to North Korean prompts with remitted bipolar, depression, and schizophrenia" [corpus] Related work (Mei et al., 2023 cited in paper) documents bias against 93 stigmatized groups; corpus neighbor "Identifying Features Associated with Bias Against 93 Stigmatized Groups" confirms stigmatized identity bias is active research area with established methodology.
- Break condition: If models were explicitly fine-tuned with counterbalancing data for intersectional groups, this amplification effect should diminish or reverse.

### Mechanism 2
- Claim: Numerical Likert-scale outputs from LLMs exhibit scale-inversion inconsistency, indicating unreliable self-reported preference modeling.
- Mechanism: When the same question is re-prompted with reversed scale labels (0=definitely willing vs. 0=definitely unwilling), models do not produce mirror-image ratings. Instead, they maintain directional bias, suggesting the model anchors on semantic content rather than understanding the rating task structure.
- Evidence anchors: [results] "Figures 2 and 4 illustrate that ratings within the same nationality are not consistent under the two scales." [results] "higher biases towards North Koreans under a flipped scale than regular scale" [corpus] Corpus evidence on scale consistency specifically is weak—no direct neighbors address Likert reliability in LLMs.
- Break condition: If models truly understood ordinal rating tasks, flipped scales would produce inverse ratings (rating on scale A = max - rating on scale B for same prompt content).

### Mechanism 3
- Claim: Models assume a default cultural position and treat certain nationalities as "other," inserting cultural-difference reasoning asymmetrically.
- Mechanism: When asked to explain ratings, GPT generates "cultural differences" rationales for North Korean prompts but not for American prompts, indicating the model has learned an implicit "self" position aligned with American identity and treats North Korean as foreign/other.
- Evidence anchors: [discussion] "GPT-4o made the assumption that they are different from a North Korean individual, but not from an American" [results] "significant cultural differences between myself and a North Korean individual... could lead to misunderstandings" (quoted from GPT-4o response) [corpus] Corpus neighbor "BharatBBQ: A Multilingual Bias Benchmark" documents Western-centric benchmark limitations; "Intersectional Bias in Japanese Large Language Models" confirms cultural positioning varies by model training context.
- Break condition: If prompts explicitly specified the model's assumed cultural position or randomized it, this asymmetry should either appear for both groups or disappear entirely.

## Foundational Learning

- Concept: **Intersectionality in bias analysis**
  - Why needed here: Single-axis bias testing (nationality OR disability alone) would miss the compounded discrimination the paper identifies. You must design evaluations that vary multiple identity attributes factorially.
  - Quick check question: If you test bias for "women" and "Asian" separately, will you automatically detect bias against "Asian women"? Why or why not?

- Concept: **Likert scale validity and inversion testing**
  - Why needed here: The paper demonstrates that LLM numerical outputs may not reflect stable preferences. Understanding scale design helps you avoid mistaking artifact for signal.
  - Quick check question: A model rates X as 2/3 on "willingness" scale. You flip the scale so 2 now means "probably unwilling." If the model still rates X as 2, what does that suggest about its understanding?

- Concept: **Stochasticity in LLM outputs**
  - Why needed here: The paper notes rating variability under re-prompting and calls for more trials. You need to understand that single-prompt evaluations are unreliable.
  - Quick check question: You run a bias test once and get a concerning result. What minimum steps should you take before concluding bias exists?

## Architecture Onboarding

- Component map: Prompt templates (nationality × disability × situation) → [paper: 2 nationalities × 6 disabilities × 5 scenarios] → Scale conditions (Regular vs. flipped Likert) → Explanation prompt → Aggregation layer (Mean ratings, standard deviations) → Comparison logic (American vs. North Korean delta scores)

- Critical path:
  1. Define intersectional attribute matrix (identities × conditions)
  2. Generate prompts with controlled variation
  3. Collect multiple response samples per condition (address stochasticity)
  4. Test both regular and inverted scales
  5. Compute per-group aggregates and cross-group differences
  6. Qualitatively analyze explanation text for asymmetric reasoning

- Design tradeoffs:
  - **Breadth vs. depth**: Paper covers 2 nationalities, 6 disabilities, 5 scenarios (60 conditions) but notes need for more trials—decide whether to expand attribute space or increase samples per condition first.
  - **Quantitative vs. qualitative**: Likert scales enable scalable measurement but miss nuance; explanations reveal reasoning but don't scale. Paper recommends balancing both.
  - **Commercial vs. open models**: Paper uses only GPT variants; notes this limits generalizability but offers easier API access.

- Failure signatures:
  - **Scale anchoring failure**: Same numerical rating under inverted scales → model doesn't understand ordinal task
  - **Explanation asymmetry**: Cultural-difference reasoning appears for one group but not counterpart → implicit default-position bias
  - **High variance across re-prompts**: Standard deviation indicates unreliability (see Tables showing moderate-high deviations)

- First 3 experiments:
  1. **Replicate with increased trials**: Run the paper's prompt matrix with n≥10 samples per condition to establish whether findings hold under statistical scrutiny; compute confidence intervals on rating differences.
  2. **Add neutral nationality control**: Include a third nationality (e.g., Canadian, German) to test whether bias is specifically anti-North Korean or pro-American vs. all others.
  3. **Explicit cultural-positioning prompt**: Prepend prompts with "Assume you are [nationality]" and measure whether asymmetry shifts or disappears—this tests whether default cultural assumption is promptable or baked into weights.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do subtle variations in prompt wording or the inclusion of contextual information alter the expression of intersectional bias in LLMs?
- **Basis in paper:** [Explicit] The authors state they "will also analyze how different prompt formulations (e.g. subtle wording changes or addition of contextual information) influences results."
- **Why unresolved:** The current study utilized a structured prompt format, but results showed high inconsistencies, leaving the sensitivity of models to linguistic nuances untested.
- **What evidence would resolve it:** A comparative analysis of model outputs across identical scenarios using varied syntactic structures and contextual framing.

### Open Question 2
- **Question:** Are the observed biases consistent, inherent model behaviors or artifacts of stochastic variability?
- **Basis in paper:** [Explicit] The authors note the necessity of "conducting more trials to definitely conclude whether these results are expected in specific prompts, especially due to the stochasticity of GPT."
- **Why unresolved:** The current findings are based on limited trials which showed rating inconsistencies, making it difficult to distinguish robust bias from random generation noise.
- **What evidence would resolve it:** Large-scale benchmarking with thousands of iterations to establish statistical significance for specific intersectional prompts.

### Open Question 3
- **Question:** Do intersectional bias patterns persist across non-commercial, open-source models and demographics beyond the American/North Korean dichotomy?
- **Basis in paper:** [Explicit] The authors acknowledge exclusive use of commercial LLMs and plan to "study a wider model spectrum" and "replicate this work under more demographics."
- **Why unresolved:** It is unclear if the "US-centric bias" and rating inconsistencies are unique to GPT models or generalizable to the wider LLM ecosystem and other cultural groups.
- **What evidence would resolve it:** Cross-model evaluation using open-weights architectures (e.g., Llama, Mistral) applied to a diverse set of nationality and disability intersections.

## Limitations
- Restricted to GPT models (3.5, 4, 4o), limiting generalizability across LLM architectures and training approaches.
- Only 5 trials per prompt condition, producing high standard deviations and insufficient statistical power.
- Exclusive focus on American/North Korean nationalities, missing broader demographic patterns and potential cultural variations.

## Confidence
- **High Confidence**: The existence of differential bias against North Korean versus American nationalities in GPT models, supported by consistent directional patterns across multiple scenarios and disabilities.
- **Medium Confidence**: The amplification of bias when mental disability intersects with nationality, as the effect is present but varies considerably in magnitude across conditions and suffers from high variance.
- **Low Confidence**: The specific mechanism of scale-inversion inconsistency, as this finding relies on a single observation of ratings changing under explanation requests rather than systematic testing of the scale-flipping methodology.

## Next Checks
1. **Statistical Replication with Increased Trials**: Re-run the complete prompt matrix with n≥10 trials per condition to establish confidence intervals and determine whether the reported directional biases remain statistically significant under larger sample sizes.

2. **Cross-Model Validation**: Test the same prompt set across at least three additional LLM families (e.g., Claude, LLaMA, Gemini) to determine whether the nationality and disability biases are GPT-specific or represent broader LLM tendencies.

3. **Cultural Positioning Experiment**: Systematically test whether the asymmetric cultural-difference reasoning can be eliminated or reversed by explicitly setting the model's assumed cultural position at the start of each prompt, measuring whether this prompt engineering approach can neutralize the default-position bias.