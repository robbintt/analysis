---
ver: rpa2
title: A Hybrid Multilayer Extreme Learning Machine for Image Classification with
  an Application to Quadcopters
arxiv_id: '2507.08047'
source_url: https://arxiv.org/abs/2507.08047
tags:
- classification
- hml-elm
- object
- each
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Hybrid Multilayer Extreme Learning Machine
  (HML-ELM) for image classification with application to quadcopters. The method combines
  ELM-based autoencoders for unsupervised feature extraction with a Simplified Interval
  Type-2 Fuzzy ELM (SIT2-FELM) classifier using a SC algorithm for efficient type
  reduction.
---

# A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters

## Quick Facts
- arXiv ID: 2507.08047
- Source URL: https://arxiv.org/abs/2507.08047
- Reference count: 8
- Primary result: HML-ELM achieves 97.60% accuracy on object data with faster training than CNNs

## Executive Summary
This paper introduces a Hybrid Multilayer Extreme Learning Machine (HML-ELM) for image classification, specifically applied to UAV object recognition and transport missions. The method combines stacked ELM-based autoencoders for unsupervised feature extraction with a Simplified Interval Type-2 Fuzzy ELM classifier using a computationally efficient SC algorithm. The approach is validated on benchmark datasets (MNIST, CIFAR-10, Fashion-MNIST) and a custom object dataset collected from UAV flights, demonstrating superior performance compared to traditional ELM variants while maintaining faster training times than CNNs.

## Method Summary
The HML-ELM architecture consists of two phases: unsupervised feature extraction using stacked ELM-based autoencoders (ELM-AEs), followed by classification using a Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with an SC algorithm for efficient type reduction. The ELM-AEs use randomly generated orthogonal hidden parameters to project input data into higher-level representations without backpropagation. The SIT2-FELM layer employs Interval Type-2 Fuzzy Sets to handle uncertainty in the data, with the SC algorithm eliminating the need for iterative sorting in type reduction. The system was validated on benchmark datasets and applied to real-world UAV missions for object classification and transport.

## Key Results
- Achieved 97.60% testing accuracy on the custom object dataset
- Demonstrated faster training time (980.3s) compared to CNN (5391.1s) while maintaining competitive accuracy
- Showed superior performance compared to traditional ML-ELM, ELM, and ML-FELM methods
- Successfully deployed in real-time UAV applications for object classification and transport missions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Stacking ELM-based Autoencoders creates high-level feature representation without backpropagation fine-tuning.
- **Mechanism**: $L$ layers of ELM-AEs use randomly generated orthogonal hidden parameters, with output weights calculated analytically via Moore-Penrose pseudoinverse.
- **Core assumption**: Random orthogonal projections preserve sufficient information as universal approximators for feature extraction.
- **Evidence anchors**: Abstract states "unsupervised multilayer feature encoding is achieved by stacking ELM-AEs"; section 3.3 describes orthogonal random parameters computed without optimization.

### Mechanism 2
- **Claim**: SIT2-FELM final layer improves classification robustness against noise and uncertainty.
- **Mechanism**: Uses Interval Type-2 Fuzzy Sets with Upper/Lower Membership Functions to create a Footprint of Uncertainty that better captures noise in UAV images.
- **Core assumption**: Noise in UAV visual feed is better modeled by IT2 interval width than crisp probabilities.
- **Evidence anchors**: Abstract mentions "novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM)"; section 5 states IT2 FLSs outperform type-1 counterparts in dealing with uncertainty.

### Mechanism 3
- **Claim**: SC algorithm enables efficient type-reduction by eliminating iterative sorting.
- **Mechanism**: Reformulates centroid calculation using derivative properties, solving for outputs without Karnik-Mendel sorting bottleneck.
- **Core assumption**: Derivative-based approximation provides sufficient accuracy for type-reduction.
- **Evidence anchors**: Abstract notes "SC algorithm for efficient type reduction"; section 3.3 states SC eliminates sorting need, reducing associated time.

## Foundational Learning

- **Concept**: **Extreme Learning Machine (ELM) Theory**
  - **Why needed here**: Architecture relies on random hidden layer weights with only output weights learned analytically.
  - **Quick check question**: Can you calculate output weights $\beta$ without gradient descent? (Answer: Yes, using $\beta = H^\dagger T$)

- **Concept**: **Interval Type-2 Fuzzy Logic (IT2-FLS)**
  - **Why needed here**: Classification layer uses fuzzy inference system with uncertainty modeling.
  - **Quick check question**: How do IT2 fuzzy sets differ from Type-1 sets? (Answer: IT2 grades are fuzzy sets/intervals, not crisp values)

- **Concept**: **Autoencoders (AE)**
  - **Why needed here**: Feature extraction phase uses ELM-AEs where target equals input.
  - **Quick check question**: What does an autoencoder learn to reconstruct? (Answer: The input data itself)

## Architecture Onboarding

- **Component map**: Image Segmentation → ELM-AE Projection → SIT2-FELM Inference → SC Type-Reduction
- **Critical path**: Raw UAV feed → HSV conversion → Blurring/thresholding → 52x52 patch extraction → ELM-AE layers → SIT2-FELM → Decision logic
- **Design tradeoffs**:
  - Accuracy vs. Speed: HML-ELM achieves ~97.6% accuracy with 980.3s training vs CNN's 99.14% with 5391.1s
  - Noise vs. Complexity: IT2-FELM adds robustness but increases complexity, mitigated by SC algorithm
- **Failure signatures**:
  - High latency (>0.1s) suggests SC algorithm issues or too many hidden nodes
  - Low accuracy on irregular shapes (94.2% vs 98.3% for circles) indicates segmentation preprocessing limitations
- **First 3 experiments**:
  1. Unit Test SC Algorithm against KM algorithm on small dataset
  2. Layer Ablation on Object Data (L=1 vs L=3 ELM-AE layers)
  3. Noise Robustness Check comparing SIT2-FELM vs standard ELM

## Open Questions the Paper Calls Out
- How can HML-ELM be optimized for extremely large datasets given linear computational load growth?
- To what extent does manual HSV thresholding limit robustness in dynamic outdoor lighting?
- Under what conditions does SC type-reduction fail to match KM algorithm accuracy?

## Limitations
- Computational load grows linearly with very large image datasets
- Manual HSV color thresholding may limit outdoor environment robustness
- SC algorithm accuracy vs KM algorithms not fully quantified under high uncertainty

## Confidence
- **High Confidence**: ELM-AE architecture effectiveness (well-established ELM theory)
- **Medium Confidence**: SIT2-FELM accuracy improvements (theoretically justified but complex)
- **Low Confidence**: SC algorithm efficiency claims (no independent validation evidence)

## Next Checks
1. Implement and benchmark SC algorithm against KM type-reduction on fuzzy classification problem
2. Conduct ablation study on ELM-AE layers using provided Object dataset
3. Test system robustness to varying noise levels in input images