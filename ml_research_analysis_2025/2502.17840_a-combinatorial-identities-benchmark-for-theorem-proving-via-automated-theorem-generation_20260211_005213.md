---
ver: rpa2
title: A Combinatorial Identities Benchmark for Theorem Proving via Automated Theorem
  Generation
arxiv_id: '2502.17840'
source_url: https://arxiv.org/abs/2502.17840
tags:
- theorem
- proof
- theorems
- tactic
- choose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of automated theorem proving
  in combinatorial identities, where the scarcity of high-quality training data limits
  the capabilities of large language models (LLMs). To tackle this, the authors construct
  LeanComb, a formalized theorem proving benchmark for combinatorial identities in
  Lean, and develop an automated theorem generator, ATG4CI, which combines LLM-generated
  candidate tactics with a reinforcement learning tree search approach for tactic
  prediction.
---

# A Combinatorial Identities Benchmark for Theorem Proving via Automated Theorem Generation

## Quick Facts
- arXiv ID: 2502.17840
- Source URL: https://arxiv.org/abs/2502.17840
- Reference count: 40
- Models trained on LeanComb-Enhanced dataset achieve up to 26% proof success rate on combinatorial identities, improving over baselines by >17%

## Executive Summary
This paper tackles automated theorem proving for combinatorial identities in Lean 4 by addressing the critical bottleneck of training data scarcity. The authors develop ATG4CI, a framework that combines LLM-generated candidate tactics with reinforcement learning tree search to generate synthetic theorems. Through iterative synthetic data generation and validation, they create LeanComb-Enhanced—a dataset of 260K combinatorial identity theorems with complete formal proofs. Fine-tuned models trained on this dataset demonstrate significantly improved success rates in automated theorem proving for combinatorial identities.

## Method Summary
The method extracts partial proof paths (P3s) from existing Lean proofs, uses an LLM to generate candidate tactics for each proof state, and employs RL-based search (PUCT) to select promising tactics. Validated theorems are added to an enhanced dataset, which is used to fine-tune the LLM in an iterative loop. The framework uses Lean4Kit for state extraction, fine-tunes Llama-3.1-8B with LoRA on NVIDIA L40 GPUs, and validates generated theorems through Lean's type checker with correction mechanisms for incomplete and type errors.

## Key Results
- ATG4CI generates 260K combinatorial identity theorems with complete Lean proofs
- Fine-tuned models achieve 26% Pass@1 success rate on LeanComb test set
- Each model improves success rates by over 17% compared to baseline models
- Iterative generation increases correct theorem proportion from ~17-23% to ~75%

## Why This Works (Mechanism)

### Mechanism 1
Iterative synthetic data generation with validation creates a self-improving training loop that increases theorem diversity and correctness rates. The framework generates candidate theorems from partial proof paths, validates them through Lean's type checker, corrects errors, and feeds validated theorems back into model fine-tuning. This increases the proportion of correct theorems from ~17-23% (iteration 1) to ~75% (iteration 2).

### Mechanism 2
Combining LLM candidate generation with RL-based search (PUCT) outperforms either approach alone for tactic prediction in combinatorial identities. The LLM proposes diverse candidate tactics (up to 16 per state). PUCT balances exploration (trying less-visited tactics) with exploitation (high-value predictions) to select which tactics to pursue.

### Mechanism 3
Partial Proof Paths (P3s) provide productive starting points for theorem generation by exposing intermediate proof states. Rather than generating theorems from scratch, the framework extracts partial proof paths from existing proven theorems—root-to-intermediate-state sequences. Tactic prediction then extends these paths, and when no valid tactic exists, the remaining subgoal becomes a new candidate theorem.

## Foundational Learning

- **Lean 4 tactic-based proving**: The entire framework operates on Lean proof states and tactics. Understanding how `rw`, `simp`, `have`, `assumption` transform goals is essential for interpreting state-tactic pairs.
  - Quick check: Given a goal `⊢ a + b = b + a`, which tactic would you apply first and why?

- **Monte Carlo Tree Search with PUCT**: The RLSCI search uses PUCT to balance exploration/exploitation. Understanding the formula Q(s,t) + c·P(s,t)·√ΣN(s,b)/(N(s,t)+1) explains why some tactics are prioritized.
  - Quick check: If a tactic has been tried 10 times with average reward 0.3, and another tried 2 times with reward 0.6, which does PUCT with c=1.0 prefer?

- **Combinatorial identity structure**: The benchmark focuses on identities like Vandermonde's, binomial sums, and recurrence relations. Recognizing common patterns (summation indices, binomial coefficients) helps debug generation failures.
  - Quick check: What's the key difference between the training set (classical identities from Spivey) and test set (from Gould/Shi)?

## Architecture Onboarding

- **Component map**: LeanComb Training Set → Lean4Kit → P3s Extraction → LLM (Llama3.1-8B) → Candidate Tactics (t=16) → RLSCI Search (PUCT) → Validation → Deduplication → Candidate Theorems → LeanComb-Enhanced ← Fine-tune LLM (repeat)

- **Critical path**:
  1. Data extraction quality: Lean4Kit must correctly capture state-tactic pairs; malformed pairs poison training
  2. Candidate tactic diversity: Too few candidates (t<8) limits exploration; too many (t>16) increases noise
  3. Validation strictness: Type errors and logical errors must be caught; false positives corrupt the enhanced dataset

- **Design tradeoffs**:
  - 8 vs 16 candidate tactics: 8-tactic setting achieves highest new-theorem success rate (41.7%); 16-tactic produces highest total count (260K)
  - Search time (300s): Longer search finds more proofs but slows iteration; paper uses 300s threshold
  - Correction vs discard: Correcting theorems adds 24-31K theorems but requires additional MCTS search

- **Failure signatures**:
  - "Incomplete errors": LLM generates tactics that create valid but unproven subgoals → apply standard MCTS to complete
  - "Type errors": Variable type annotations lost during extraction → re-inject types from original theorem context
  - "Logical errors": Tactics generate contradictory subgoals → discard theorem (unrecoverable)

- **First 3 experiments**:
  1. Reproduce P3 extraction on single theorem: Run Lean4Kit on `idt_182` (the Fibonacci example), verify you extract 22 distinct P3s matching paper claims
  2. Ablate candidate tactic count: Compare theorem yield with t=4, 8, 16 on a 50-theorem subset; expect diminishing returns above 8
  3. Validate correction pipeline: Inject a known type error, verify the correction module catches and fixes it before addition to G*

## Open Questions the Paper Calls Out

### Open Question 1
Can ATG4CI be extended to other specialized mathematical domains beyond combinatorial identities where data scarcity similarly constrains LLM performance? The framework was designed specifically for combinatorial identities; its applicability to domains with different proof structures (e.g., algebraic topology, differential geometry) is untested.

### Open Question 2
How can the framework handle logical errors in candidate theorems more effectively rather than discarding them as "irreparable"? The current correction mechanism addresses incomplete and type errors but lacks methods for repairing logically inconsistent generated theorems.

### Open Question 3
What techniques could improve theorem generation and proof success rates for longer proof sequences beyond the observed peak at 6 prediction steps? The current RL-based search may struggle with credit assignment over longer horizons, and candidate tactic quality degrades with depth.

### Open Question 4
Can models trained on LeanComb-Enhanced data bridge the gap to solving olympiad-level combinatorics problems that currently challenge systems like AlphaProof? The 26% success rate on LeanComb suggests significant room for improvement before tackling competition-level problems with deeper reasoning requirements.

## Limitations

- Data generation scalability limits: The 260K theorem corpus represents only 2-3 generations of iterative generation; the framework's ability to generate increasingly complex theorems from increasingly simpler ones remains unproven
- RLSCI parameter sensitivity: The choice of 16 candidate tactics, 300-second search limit, and cpuct parameter appears somewhat arbitrary and may overfit to the specific distribution of LeanComb theorems
- Validation completeness: Type correctness doesn't guarantee mathematical validity—the framework may generate theorems that are syntactically valid but semantically trivial or mathematically uninteresting

## Confidence

- **High confidence**: The 26% Pass@1 success rate on LeanComb test set represents a genuine improvement over baseline models
- **Medium confidence**: The claim that iterative generation increases theorem correctness from 17-23% to 75% assumes the validation pipeline correctly identifies valid theorems
- **Low confidence**: The assertion that generated theorems are "useful" for training lacks direct evidence of mathematical significance

## Next Checks

1. **Cross-domain transfer validation**: Fine-tune the same model architecture on LeanComb-Enhanced data, then evaluate on miniF2F without further fine-tuning to test whether generated theorems capture general proof patterns

2. **Generation quality analysis**: For theorems generated in iteration 2, analyze their structural complexity versus iteration 1 theorems to compute the proportion that represent genuinely new mathematical relationships

3. **Search algorithm ablation**: Replace RLSCI with pure MCTS or pure LLM candidate generation on the same P3s to quantify the incremental benefit of combining LLM candidates with RL search versus either approach alone