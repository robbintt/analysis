---
ver: rpa2
title: Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes
arxiv_id: '2512.10878'
source_url: https://arxiv.org/abs/2512.10878
tags:
- counterfactual
- class
- data
- counterfactuals
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses model reconstruction using counterfactual
  explanations, focusing on scenarios with limited access to labeled data and counterfactuals.
  The core idea is to treat counterfactuals as soft-labeled samples (0.5) that blend
  features from both classes and use Wasserstein barycenters to compute class prototypes
  that integrate both original data and counterfactuals.
---

# Classifier Reconstruction Through Counterfactual-Aware Wasserstein Prototypes

## Quick Facts
- arXiv ID: 2512.10878
- Source URL: https://arxiv.org/abs/2512.10878
- Reference count: 11
- Primary result: Achieves 96% fidelity on Adult Income with 500 counterfactual queries, outperforming baselines in low-data regimes

## Executive Summary
This paper introduces a method for reconstructing black-box classifiers using limited counterfactual queries and labeled data. The key innovation is treating counterfactuals as soft-labeled samples (0.5) that blend features from both classes, combined with Wasserstein barycenters to compute class prototypes. This approach preserves distributional structure and mitigates decision boundary shifts that occur when counterfactuals are naively treated as hard-labeled training instances. Experiments on four tabular datasets demonstrate superior fidelity compared to baseline methods, especially in low-query regimes, while using a simpler prototype-based architecture than neural network surrogates.

## Method Summary
The method reconstructs a target binary classifier by collecting labeled data and counterfactuals, then computing class prototypes via Wasserstein barycenter optimization. Counterfactuals are assigned soft label 0.5 to represent ambiguous samples near the decision boundary. The barycenter objective minimizes optimal transport cost to both class samples and counterfactuals, with symmetry regularization encouraging balanced prototype placement. Classification is performed by comparing test samples to learned prototypes using Wasserstein distance with a margin threshold. The approach uses logistic regression for the target model, MCCF for counterfactual generation, and POT library for barycenter computation.

## Key Results
- Achieves 96% fidelity on Adult Income dataset with 500 queries, compared to 92% for best baseline
- Outperforms neural network surrogates despite simpler architecture, especially as complexity increases
- Maintains robust performance in low-query regimes (300-500 queries) across all four datasets
- Shows lower variance than baseline methods, particularly on challenging DCCC dataset

## Why This Works (Mechanism)

### Mechanism 1
Assigning counterfactuals a soft label of 0.5 mitigates decision boundary shift that occurs when counterfactuals are naively treated as hard-labeled training instances. Counterfactuals near the decision boundary are explicitly modeled as ambiguous samples blending characteristics of both classes, rather than forcing them into one class. This prevents the surrogate's boundary from being pulled toward the counterfactual distribution in one direction. Core assumption: Counterfactuals genuinely lie close to the true decision boundary of the target model. Break condition: If counterfactuals are poorly generated and do not lie near the true boundary, soft labeling provides no benefit.

### Mechanism 2
Wasserstein barycenters preserve the underlying geometric structure of class distributions, enabling data-efficient prototype learning. By minimizing optimal transport cost to both class samples and counterfactuals, the barycenter captures distributional shape rather than just central tendency. Core assumption: The underlying class distributions have meaningful geometric structure that simple averaging would distort. Break condition: If class distributions are highly irregular or multi-modal without proper initialization, a single barycenter per class may oversimplify.

### Mechanism 3
Symmetry regularization enforces balanced decision boundary placement by penalizing asymmetric counterfactual-to-prototype distances. The term R(Q0,Q1) = (W2(Q0,Pcf) − W2(Q1,Pcf))² forces counterfactuals to be approximately equidistant from both class prototypes, which stabilizes boundary estimation under limited data. Core assumption: Counterfactuals should have symmetric relationship to the true decision boundary. Break condition: If counterfactual queries are severely one-sided (only reject-to-accept), symmetry constraint may be mismatched to data availability.

## Foundational Learning

- **Concept: Wasserstein Distance (Optimal Transport)**
  - Why needed here: Core mechanism for computing barycenters; measures distributional similarity by transport cost rather than pointwise overlap.
  - Quick check question: Why does Wasserstein distance capture geometric structure better than KL divergence for this application?

- **Concept: Prototype-Based Classification**
  - Why needed here: Classification is performed by comparing test samples to learned prototypes using Wasserstein distance.
  - Quick check question: How does distance-to-prototype classification differ from margin-based classifiers like SVMs?

- **Concept: Model Fidelity vs. Accuracy**
  - Why needed here: The evaluation metric measures agreement between surrogate and target predictions, not ground-truth correctness.
  - Quick check question: Why is fidelity the appropriate metric for model extraction rather than classification accuracy?

## Architecture Onboarding

- **Component map:**
  Input layer -> Barycenter solver -> Regularization module -> Classification layer

- **Critical path:**
  1. Query target model to collect D0, D1, Dcf
  2. Solve barycenter optimization (Equation 3) for Q0, Q1
  3. Classify new samples via ŷ = argmin W2(δx, Qc) with margin threshold

- **Design tradeoffs:**
  - λc = 0.5: Equal weighting of class samples and counterfactuals; adjustable if counterfactual quality varies
  - γ = 0.3: Controls symmetry enforcement strength; higher values may over-regularize with imbalanced queries
  - Margin τ: Prevents overconfident predictions near boundary; set to 0 in reported experiments

- **Failure signatures:**
  - Neural network surrogates show fidelity degradation as complexity increases: indicates overfitting in low-query regimes
  - C-CHVAE counterfactuals achieve only 77-83% fidelity: indicates generative-model counterfactuals may not lie on true data manifold for tabular data
  - High variance in baseline methods (±8-9 on DCCC): indicates instability with limited samples

- **First 3 experiments:**
  1. Reproduce Table 1 results on Adult Income with query sizes {300, 400, 500} to validate implementation
  2. Ablate symmetry regularization (γ = 0 vs. γ = 0.3) to isolate its contribution to fidelity
  3. Test robustness by varying λc ∈ {0.3, 0.5, 0.7} to measure sensitivity to counterfactual weighting

## Open Questions the Paper Calls Out

- How does the number of query samples quantitatively impact reconstruction fidelity beyond the low-query regimes tested? The current experiments focus primarily on low-data scenarios (300-500 queries), demonstrating robustness but leaving the scaling behavior with larger query budgets uncharacterized.

- Can the method be extended to multi-class classification scenarios? The problem setting explicitly restricts the label space to Y={0, 0.5, 1}, and the mathematical formulation defines class prototypes only for c ∈ {0, 1}.

- Does incorporating prior knowledge of the target model substantially enhance reconstruction quality? The authors note that "incorporating such prior information could substantially enhance reconstruction quality" and that their current approach assumes no prior knowledge.

## Limitations

- Method performance depends critically on quality of counterfactual generation, with poor counterfactuals degrading soft-labeling benefits
- Limited ablation studies on key hyperparameters (λc, γ) prevent understanding of sensitivity
- Performance on highly imbalanced datasets is not explicitly tested, which could affect barycenter stability

## Confidence

- Soft-labeling mechanism: Medium-High (conceptually sound but dependent on counterfactual quality)
- Wasserstein barycenters: High (established use in distributional learning)
- Symmetry regularization: Medium (no ablation provided)

## Next Checks

1. Perform an ablation study on γ (0 vs 0.3) to isolate symmetry regularization's contribution
2. Test sensitivity to λc ∈ {0.3, 0.5, 0.7} to assess counterfactual weighting robustness
3. Compare against alternative counterfactual generators (e.g., DiCE) to evaluate method robustness to counterfactual quality