---
ver: rpa2
title: 'Filtering with Confidence: When Data Augmentation Meets Conformal Prediction'
arxiv_id: '2509.21479'
source_url: https://arxiv.org/abs/2509.21479
tags:
- data
- augmentation
- arxiv
- prediction
- conformal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a filtering method for synthetic data augmentation
  using conformal prediction. The core idea is to calibrate a quality threshold for
  accepting generated samples, ensuring provable risk control over the number of poor-quality
  samples accepted.
---

# Filtering with Confidence: When Data Augmentation Meets Conformal Prediction

## Quick Facts
- **arXiv ID:** 2509.21479
- **Source URL:** https://arxiv.org/abs/2509.21479
- **Reference count:** 40
- **Primary result:** Up to 40 percentage points (pp) in F1 score over unaugmented baselines and 4 pp over other filtered augmentation baselines.

## Executive Summary
This paper proposes a filtering method for synthetic data augmentation using conformal prediction to calibrate quality thresholds with provable risk control. The method uses a two-stage process: first calibrating a conformal prediction algorithm on a subset of data to account for uncertainty in quality evaluation, then filtering remaining data using the calibrated threshold. Experiments across text, image, and tabular data demonstrate consistent performance improvements, showing that conformal calibration can effectively distinguish high-quality from low-quality synthetic samples while maintaining statistical guarantees on the number of poor-quality samples accepted.

## Method Summary
The method implements a two-stage filtering pipeline for synthetic data augmentation. First, a calibration set with both gold-standard and surrogate quality scores is used to compute non-conformity scores via conformal prediction, establishing a threshold that guarantees control over poor-quality sample acceptance. Second, a conditional conformal prediction mechanism using RKHS-based regression adapts thresholds to individual samples based on their difficulty characteristics. The approach requires no access to internal model logits or extensive retraining, making it practical for real-world deployment. Quality scores are computed using both expensive gold-standard evaluators (like Gemini-Pro) and cheaper surrogates (like Gemini-Flash or learned regressors).

## Key Results
- Achieves up to 40 pp improvement in F1 score over unaugmented baselines
- Provides 4 pp improvement over other filtered augmentation baselines
- Maintains provable risk control with P(L ≤ ρ) ≥ 1−α guarantees
- Shows consistent performance across text, image, and tabular domains

## Why This Works (Mechanism)

### Mechanism 1: Conformal Risk Control for Quality Thresholding
The method calibrates acceptance thresholds via conformal prediction to provide provable bounds on poor-quality synthetic samples accepted. Using a calibration set with gold-standard scores A and surrogate scores Â, it defines non-conformity scores as the minimum threshold ensuring filtered sets contain at most ρ false acceptances. The conformal quantile yields a calibrated threshold guaranteeing P(L ≤ ρ) ≥ 1−α. This relies on exchangeability between calibration and augmentation samples and monotone loss functions.

### Mechanism 2: Surrogate-to-Gold-Standard Regression Alignment
When gold-standard evaluation is expensive, a learned regression model transforms noisy surrogate scores into more reliable quality estimates. The model trains on Dtrain to predict surrogate measure Ã, assuming it's a noisy realization centered at true gold-standard A. Conformal calibration then uses the original surrogate as an unbiased estimator for the regression target, incorporating features like semantic relevance and generation entropy.

### Mechanism 3: Conditional Coverage via RKHS-Based Adaptation
Sample-adaptive thresholds improve filtering when sample difficulty varies, providing localized coverage guarantees. The method fits regularized kernel quantile regression in an RKHS, where for each test sample the threshold is the maximum score such that the fitted coefficient ≤ U (randomized). This adapts thresholds to input characteristics via kernel similarity, offering better performance than marginal CP baselines.

## Foundational Learning

- **Conformal Prediction Fundamentals**: Understanding quantile-based set construction and exchangeability is essential since the entire method rests on CP's finite-sample, distribution-free coverage guarantees. Quick check: Given calibration scores {0.2, 0.5, 0.8, 0.9} and α=0.1, what threshold ensures 90% coverage?

- **Bias-Variance Trade-off in Data Augmentation**: The paper's core motivation is that synthetic data reduces estimator variance but introduces bias if low-quality. Understanding this trade-off clarifies why filtering matters. Quick check: If synthetic samples come from a shifted distribution, does this primarily affect bias or variance of the downstream estimator?

- **Reproducing Kernel Hilbert Spaces (RKHS)**: The conditional CP mechanism uses RKHS functions for adaptive thresholding. The regularization term and representer theorem are essential implementation details. Quick check: Why does the RKHS formulation allow smooth interpolation between samples while controlling complexity?

## Architecture Onboarding

- **Component map:** Data Splitter -> Synthetic Generator -> Quality Scorers -> Non-conformity Score Computer -> Conditional CP Module -> Filter

- **Critical path:**
  1. Generate synthetic candidates with temperature τ > 1 for diversity
  2. Score calibration subset with both A and Â
  3. Compute non-conformity scores S_i for each calibration sample
  4. Fit conditional CP model using kernel W on embeddings (LDA for text, PCA for tabular/images)
  5. Apply learned thresholds to Daug; retain passing samples
  6. Train downstream model on original + filtered synthetic data

- **Design tradeoffs:**
  - Higher τ increases diversity but produces more low-quality samples → stricter filtering needed
  - Smaller ρ (false discovery tolerance) yields stricter filtering but may reject useful diverse samples
  - Larger calibration set improves threshold reliability but reduces data available for augmentation
  - Kernel bandwidth ξ controls locality of conditional adaptation (too small = overfit, too large = marginal)

- **Failure signatures:**
  - Low accepted-sample count: Thresholds too high; check if calibration scores are unusually low or kernel bandwidth misconfigured
  - High coverage violations: Calibration/augmentation distribution mismatch; verify exchangeability
  - No improvement over unfiltered: Temperature too low (low diversity) or gold-standard/surrogate poorly correlated (check scatter plots like Figure 5)
  - Marginal CP outperforms conditional: Kernel choice or embedding space doesn't capture difficulty structure

- **First 3 experiments:**
  1. **Sanity check on calibration**: Split Dcalib further, verify empirical violation rate ≤ α across multiple random splits (replicate Figure 7 for your data)
  2. **Temperature sweep**: Generate at τ ∈ {0.5, 1.0, 1.5, 2.0}, measure both accepted-sample diversity (stable rank) and downstream F1 to find optimal regime
  3. **Ablation: surrogate quality**: Compare using gold-standard A on all data (expensive baseline) vs. surrogate Â with conformal calibration; quantify the gap to understand cost-quality trade-off

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can conformal data augmentation be effectively extended to counterfactual or retrieval-based augmentation settings?
- **Basis in paper:** The conclusion states: "Future directions for improvement include: (1) extending our methodology to other generative settings such as counterfactual or retrieval-based augmentation."
- **Why unresolved:** The current framework assumes data reformation where existing points are transformed; counterfactual and retrieval-based settings involve fundamentally different generation paradigms requiring new quality scoring approaches.
- **What evidence would resolve it:** Demonstration of CondCP filtering on counterfactual generation tasks (e.g., causal inference scenarios) and retrieval-augmented generation pipelines, showing maintained risk control guarantees.

### Open Question 2
- **Question:** How does the method perform under distribution shift between calibration and augmentation data?
- **Basis in paper:** The theoretical guarantees in Lemma 3.1 assume D_calib ∪ D_aug are i.i.d., but real-world augmentation often involves distribution shift when generating for underrepresented regions.
- **Why unresolved:** The conformal guarantees rely on exchangeability; when generated samples systematically differ from calibration distribution, coverage may degrade unpredictably.
- **What evidence would resolve it:** Experiments with controlled distribution shift between calibration and augmentation sets, measuring empirical violation rates and downstream performance degradation.

### Open Question 3
- **Question:** What is the optimal selection strategy for hyperparameters (α, ρ, λ) across different task domains?
- **Basis in paper:** Sensitivity analysis in Appendix D.6 shows performance varies with hyperparameter choices, but no principled selection method is proposed beyond manual tuning.
- **Why unresolved:** The trade-off between diversity (higher ρ, lower λ) and quality (lower ρ, higher λ) appears dataset-dependent without clear theoretical guidance.
- **What evidence would resolve it:** A theoretical or empirical framework mapping dataset characteristics (imbalance ratio, dimensionality, noise levels) to optimal hyperparameter ranges.

### Open Question 4
- **Question:** How can the coverage gap from RKHS approximation in conditional CP be better characterized or reduced?
- **Basis in paper:** Lemma 3.1 shows coverage deviates from nominal level by a gap related to γ and the RKHS function, but practical significance and mitigation strategies remain unclear.
- **Why unresolved:** The coverage gap is estimable but its magnitude across real-world datasets and its impact on downstream augmentation quality is not characterized.
- **What evidence would resolve it:** Systematic analysis of coverage gap magnitude across diverse datasets and investigation of alternative function classes beyond RKHS that may reduce this gap.

## Limitations
- **Exchangeability assumption**: The method requires calibration and augmentation samples to be exchangeable, which may fail under distribution drift.
- **Surrogate quality dependence**: The effectiveness depends on the correlation between cheap surrogate scores and expensive gold-standard evaluations.
- **Kernel design sensitivity**: The conditional CP mechanism's performance depends heavily on kernel choice and embedding space capturing quality-relevant difficulty structure.

## Confidence
- **High Confidence**: The conformal risk control mechanism and its provable guarantees (Sections 3.1-3.2) are well-grounded in established CP theory. The two-stage filtering pipeline is clearly specified and experimentally validated across multiple domains.
- **Medium Confidence**: The surrogate regression alignment (Section 3.3) is theoretically sound but relies on untested assumptions about evaluator bias properties. The conditional CP adaptation shows empirical benefits but the coverage gap from RKHS regularization is not quantified in all experiments.
- **Low Confidence**: The tabular data augmentation quality scoring mechanism (gradient boosting on k-NN similarity) lacks sufficient detail for implementation. The image experiment results are difficult to interpret due to missing qualitative comparisons of accepted vs rejected generations.

## Next Checks
1. **Coverage verification**: Run the conformal calibration on held-out calibration folds to empirically measure violation rates across different ρ values and verify they stay within α bounds.
2. **Distribution drift test**: Generate synthetic data at multiple time points or with different seeds, then compare calibration and augmentation score distributions to detect exchangeability violations.
3. **Kernel sensitivity analysis**: Systematically vary kernel bandwidth ξ and embedding dimensionality to quantify their impact on conditional threshold adaptation effectiveness versus marginal CP baselines.