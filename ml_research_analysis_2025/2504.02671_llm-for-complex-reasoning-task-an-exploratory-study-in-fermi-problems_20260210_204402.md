---
ver: rpa2
title: 'LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems'
arxiv_id: '2504.02671'
source_url: https://arxiv.org/abs/2504.02671
tags:
- questions
- llms
- prompt
- standard
- specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how large language models (LLMs) handle Fermi
  Problems (FPs), a type of mathematical reasoning task that requires numerical estimation
  under real-world constraints. The authors developed an LLM-based pipeline to classify
  and solve FPs, using the TELeR taxonomy to design prompts at different complexity
  levels (0, 2, and 4).
---

# LLM for Complex Reasoning Task: An Exploratory Study in Fermi Problems

## Quick Facts
- **arXiv ID**: 2504.02671
- **Source URL**: https://arxiv.org/abs/2504.02671
- **Reference count**: 20
- **Primary result**: All three tested LLMs (GPT-3.5, GPT-4, Llama 3.0) achieved fp_scores below 0.5 on Fermi Problems, confirming the difficulty of these tasks.

## Executive Summary
This paper explores how large language models handle Fermi Problems (FPs), a type of mathematical reasoning task requiring numerical estimation under real-world constraints. The authors developed an LLM-based pipeline using the TELeR taxonomy to design prompts at different complexity levels (0, 2, and 4) and categorized FPs into "standard" and "specific" questions based on clarity, neutrality, and conciseness. Experiments showed that all tested models achieved fp_scores below 0.5, confirming FPs' inherent difficulty. Standard questions consistently yielded higher accuracy and efficiency than specific ones across all models and prompt levels. Higher-level prompts improved distinguishability of scores, reduced computational hops, and increased consistency.

## Method Summary
The study uses the REALFP dataset (558 test questions) and employs GPT-3.5 to classify each FP as "standard" or "specific" based on clarity, neutrality, and conciseness criteria. For each LLM (GPT-3.5, GPT-4, Llama 3.0), prompts are generated at TELeR levels 0, 2, and 4. The target LLM generates its own prompt for each question based on the TELeR level, then answers using that prompt. Numerical answers are extracted, with re-prompting up to 10 times if no valid number is found. Performance is evaluated using fp_score, which measures how close the answer is to the gold standard within orders of magnitude.

## Key Results
- All three models achieved fp_scores below 0.5, confirming the difficulty of Fermi Problems
- Standard questions consistently yielded higher accuracy and efficiency than specific ones across all models and prompt levels
- Higher-level prompts (Level 4) improved distinguishability of scores, reduced computational hops, and increased consistency
- GPT-4 achieved the highest fp_score of 0.500 at Level 4 on standard questions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Higher-level prompts designed via the TELeR taxonomy improve LLM performance on Fermi Problems by providing structured decomposition guidance and explicit evaluation criteria.
- **Mechanism**: TELeR prompt levels 2 and 4 inject sub-task structures and evaluation directions, reducing the solution space ambiguity that FPs inherently possess. This scaffolds the model's reasoning by breaking estimation problems into sequential computational steps.
- **Core assumption**: LLMs possess latent numerical reasoning capabilities that can be elicited through structured guidance rather than requiring architectural modification.
- **Evidence anchors**: [abstract] "Higher-level prompts improved distinguishability of scores, reduced computational hops, and increased consistency"; [section 5.4] "Prompt level 4 further injects the expected evaluation direction"; [Table 3] GPT-4 fp_score improved from 0.466 (Level 0) to 0.500 (Level 4).

### Mechanism 2
- **Claim**: LLMs perform better on "standard" questions than "specific" questions due to stronger alignment between question semantics and pre-training distribution.
- **Mechanism**: Standard questions use widely recognized concepts with consistent definitions across training corpora. Specific questions contain ambiguous terms or domain-specific definitions that create embedding uncertainty, degrading retrieval and reasoning quality.
- **Core assumption**: Performance gaps reflect parametric knowledge distribution rather than reasoning architecture limitations.
- **Evidence anchors**: [abstract] "Standard questions consistently yielded higher accuracy and efficiency than specific ones across all models and prompt levels"; [Table 4] GPT-3.5 at Level 4 achieved 0.500 on standard vs. 0.353 on specific questions (gap = 0.147); [Table 7] Cross-similarity between standard and specific questions (0.1653) was lower than within-category similarity.

### Mechanism 3
- **Claim**: Higher prompt levels reduce computational inefficiency (measured in "hops") by constraining output format and increasing valid response probability on first attempt.
- **Mechanism**: Structured prompts reduce the likelihood of non-numeric or nonsensical outputs by pre-specifying expected answer formats and reasoning steps, minimizing retry iterations.
- **Core assumption**: "Hops" meaningfully measure efficiency and correlate with model comprehension quality.
- **Evidence anchors**: [section 6.3.2] "Fewer hops indicate a more responsive and efficient model, while a higher number of hops suggests a slower reaction, reflecting weaker comprehension"; [Table 6] GPT-4 standard questions required 92 hops at Level 0 vs. 38 hops at Level 4.

## Foundational Learning

- **Concept: Fermi Problems (FPs)**
  - **Why needed here**: Understanding that FPs require order-of-magnitude estimation under real-world ambiguity—not precise calculation—is essential for interpreting fp_score results and the inherent difficulty ceiling.
  - **Quick check question**: Can you explain why an answer within one order of magnitude receives full credit in Fermi Science Olympiads?

- **Concept: TELeR Taxonomy**
  - **Why needed here**: The prompt design framework (Turn, Expression, Level of Details, Role) defines the experimental independent variable; understanding levels 0/2/4 is necessary to replicate or extend this work.
  - **Quick check question**: What distinguishes a Level 4 prompt from a Level 2 prompt in the TELeR framework?

- **Concept: fp_score Metric**
  - **Why needed here**: The logarithmic scoring function (Equation 1) penalizes magnitude errors progressively; interpreting scores requires understanding that 0.5 does not mean "50% correct."
  - **Quick check question**: If a model's answer deviates by two orders of magnitude from the gold standard, what fp_score does it receive?

## Architecture Onboarding

- **Component map**: Classification Module -> Prompt Generator -> Solver Module -> Validation Loop -> Evaluation Module
- **Critical path**: Classification → Prompt Generation → Solving → Validation Loop → fp_score Computation. The classification step determines downstream performance ceiling; prompt level determines efficiency.
- **Design tradeoffs**:
  - Using same LLM for classification and evaluation vs. independent classifiers (paper chose single classifier for consistency, sacrificing robustness analysis)
  - TELeR Level 4 vs. Level 2: Higher structure improves GPT-family performance but adds prompt generation overhead
  - 10-hop retry limit trades off completeness vs. infinite loop risk
- **Failure signatures**:
  - fp_score < 0.3 with low hop count → model confidently produces wrong magnitude (parametric knowledge gap)
  - High hop count with valid numeric output → prompt fails to constrain format effectively
  - Larger standard/specific gap at higher prompt levels → prompt structure amplifies knowledge distribution bias
- **First 3 experiments**:
  1. **Baseline replication**: Run GPT-3.5 on 50 sampled FPs across all three TELeR levels; verify fp_score distribution matches Table 3
  2. **Classification ablation**: Manually label 30 FPs as standard/specific; compare human labels against GPT-3.5 classifier agreement rate
  3. **Efficiency validation**: Log hop distributions per prompt level; confirm Level 4 reduces mean hops for GPT-family models per Table 6 patterns

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How do attention mechanisms in LLMs correlate with the performance disparity between standard and specific Fermi Problems?
- **Basis in paper**: [explicit] The authors state in the Limitations section that they "specifically consider explaining performance from the perspective of attention mechanisms" and that "interpretability will be our dominant future direction."
- **Why unresolved**: The current study only establishes that a performance gap exists between question types but does not investigate the internal model states or attention patterns that cause this divergence.
- **What evidence would resolve it**: An analysis of attention heatmaps or internal embedding states comparing model processing of "standard" versus "specific" questions to identify where the reasoning divergence occurs.

### Open Question 2
- **Question**: Does incorporating a weighted reward mechanism based on question classification (standard vs. specific) during training enhance LLM performance on Fermi Problems?
- **Basis in paper**: [explicit] The Conclusion suggests a potential direction is to "introduce rewards/penalties based on the difficulty of question types, involving a weighted mechanism in the training process."
- **Why unresolved**: The current work utilizes a zero-shot inference pipeline without fine-tuning; the impact of explicitly weighting the loss function based on the proposed taxonomy remains untested.
- **What evidence would resolve it**: A comparative study fine-tuning an LLM where the loss function applies different weights to "standard" and "specific" questions, compared against a baseline model with uniform weighting.

### Open Question 3
- **Question**: To what extent do the findings regarding TELeR prompt levels and question types generalize to a broader range of large language models?
- **Basis in paper**: [explicit] The Limitations section notes the evaluation was restricted to three models due to budget constraints and that "including a broader range of LLMs in future studies would provide a more comprehensive understanding."
- **Why unresolved**: The observed behaviors (e.g., prompt level 4 improving efficiency) might be specific to the architectures of GPT-3.5/4 and Llama 3.0.
- **What evidence would resolve it**: Replication of the TELeR prompt experiments on distinct model architectures (e.g., Mistral, Gemini, or Claude) to verify if the "standard vs. specific" performance gap and prompt sensitivity persist.

## Limitations

- **Knowledge distribution bias**: The study attributes performance gaps between standard and specific questions to knowledge distribution rather than reasoning capability, but cannot definitively prove this without controlled experiments varying the training corpus.
- **FP score limitations**: While fp_score is designed for Fermi Problems, it may not capture the full spectrum of reasoning quality. A model producing an answer one order of magnitude off receives 0.667, but this may not reflect the quality of the reasoning process that led to that answer.
- **Prompt generation transparency**: The TELeR taxonomy levels 2 and 4 are defined, but the exact prompt used to generate prompts at these levels is not fully specified, limiting reproducibility.

## Confidence

- **High**: LLMs struggle with Fermi Problems (fp_scores < 0.5); higher-level prompts reduce computational hops and increase consistency; standard questions yield higher accuracy than specific ones.
- **Medium**: Performance gaps between standard and specific questions are primarily due to knowledge distribution rather than reasoning capability; TELeR Level 4 prompts are more effective than Level 2 for GPT-family models.
- **Low**: The exact mechanism by which higher-level prompts improve distinguishability of scores is fully understood; the classification of questions as "standard" vs. "specific" is perfectly reliable and reproducible.

## Next Checks

1. **Prompt Generation Replication**: Implement the exact meta-prompt used to generate TELeR Level 2 and Level 4 prompts for a sample of 10 Fermi Problems. Compare the generated prompts with those in Appendix A.3 to verify understanding of the process.
2. **Classification Consistency Test**: Run the GPT-3.5 classification prompt on the same 30 Fermi Problems three times. Calculate the inter-run agreement rate to assess the stability and reliability of the standard/specific classification.
3. **Knowledge Gap Isolation**: Manually create a new set of Fermi Problems that are structurally identical to the original standard questions but contain specific domain knowledge not present in the original set. Test whether performance gaps persist when structure is controlled.