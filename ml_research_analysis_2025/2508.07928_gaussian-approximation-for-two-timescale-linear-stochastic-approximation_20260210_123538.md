---
ver: rpa2
title: Gaussian Approximation for Two-Timescale Linear Stochastic Approximation
arxiv_id: '2508.07928'
source_url: https://arxiv.org/abs/2508.07928
tags:
- lemma
- last
- proof
- where
- holds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes non-asymptotic bounds for the accuracy of
  Gaussian approximation in two-timescale linear stochastic approximation algorithms
  under both martingale difference and Markov noise settings. The authors analyze
  both last iterate and Polyak-Ruppert averaging regimes, deriving bounds in terms
  of convex distance between probability distributions.
---

# Gaussian Approximation for Two-Timescale Linear Stochastic Approximation

## Quick Facts
- **arXiv ID:** 2508.07928
- **Source URL:** https://arxiv.org/abs/2508.07928
- **Reference count:** 40
- **Primary result:** Establishes non-asymptotic bounds for Gaussian approximation accuracy in two-timescale linear stochastic approximation under both martingale and Markov noise, showing rates up to n⁻¹/⁴ and n⁻¹/⁶ respectively.

## Executive Summary
This paper establishes non-asymptotic bounds for the accuracy of Gaussian approximation in two-timescale linear stochastic approximation algorithms under both martingale difference and Markov noise settings. The authors analyze both last iterate and Polyak-Ruppert averaging regimes, deriving bounds in terms of convex distance between probability distributions. A key finding is that the normal approximation rate for the last iterate improves with increased timescale separation, while it decreases in the averaged setting. The paper also provides high-order moment bounds for TTSA errors and demonstrates applicability to reinforcement learning algorithms like GTD and TDC.

## Method Summary
The method analyzes two-timescale stochastic approximation (TTSA) by decomposing the error into a linear martingale component and a nonlinear perturbation. For the last iterate, the authors transform the coupled system into a decoupled form using a change of variables, enabling sequential analysis of the slow and fast components. For the averaged iterate, they analyze the martingale difference sequence formed by the averaged errors. The analysis requires polynomially decaying step sizes with exponents satisfying 1/2 < a < b < 1, where a and b control the decay rates for the slow and fast timescales respectively. The bounds depend on the stability of the system matrices (Hurwitz conditions) and the mixing properties of the noise process.

## Key Results
- Derives non-asymptotic bounds for Gaussian approximation in terms of convex distance between TTSA iterates and Gaussian distributions
- Shows convergence rates of order n⁻¹/⁴ for martingale noise and n⁻¹/⁶ for Markov noise
- Demonstrates that normal approximation rate for last iterate improves with increased timescale separation
- Proves that Polyak-Ruppert averaged iterate requires similar scales for optimal rate
- Applies results to reinforcement learning algorithms including GTD and TDC

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The accuracy of approximating TTSA iterates by a Gaussian distribution is controlled by splitting the error into a linear martingale component and a non-linear perturbation.
- **Mechanism**: The paper utilizes a decomposition strategy where the total convex distance ρ_{Conv}(T, ν) between the statistic T and a Gaussian ν is bounded by the sum of the distance for a linear term W and a perturbation term D involving high-order moments. This reduces the complex dependent TTSA process to the analysis of a standard martingale CLT plus a controllable residual.
- **Core assumption**: The noise terms form a bounded martingale difference sequence or a geometrically ergodic Markov chain.
- **Break condition**: If the high-order moments of the perturbation D do not decay sufficiently fast relative to the linear term, the bound becomes trivial (>1).

### Mechanism 2
- **Claim**: The coupled dynamics of the slow variable θ and fast variable w can be analyzed independently by decoupling the update equations.
- **Mechanism**: By performing a change of variables, the authors transform the coupled TTSA system into a form where the "fast" error w̃_{k+1} does not explicitly depend on θ̃_k in the drift term. This allows the application of single-timescale stability results to bound the iterates sequentially.
- **Core assumption**: The matrices -A_{22} and -Δ (the Schur complement) are Hurwitz, ensuring stability of the individual decoupled processes.
- **Break condition**: If the ratio of step sizes β_k / γ_k does not converge to zero, the decoupling transformation may fail to stabilize the error terms.

### Mechanism 3
- **Claim**: The convergence rate of the Last Iterate improves with increased timescale separation, whereas the Polyak-Ruppert averaged iterate rate degrades (or requires similar scales).
- **Mechanism**: For the Last Iterate, the error bound contains terms favoring separation because the faster convergence of the inner loop w effectively reduces noise in the outer loop θ. For PR Averaging, the optimal rate is achieved when the scales effectively coincide, as large separation introduces bias in the averaging of the coupled processes.
- **Core assumption**: Step size exponents a, b satisfy 1/2 < a < b < 1.
- **Break condition**: If 2b ≤ 1+a, the bounds for the last iterate may fail to converge to zero.

## Foundational Learning

- **Concept: Lyapunov Stability (Hurwitz Matrices)**
  - **Why needed here**: The paper relies on the stability of the system matrices (A_{22} and Δ) to guarantee that the iterates do not diverge and that the decoupled dynamics are contractive.
  - **Quick check question**: Can you verify if a matrix A is Hurwitz by checking the real parts of its eigenvalues?

- **Concept: Martingale Central Limit Theorem (CLT)**
  - **Why needed here**: The analysis reduces the main problem to bounding a linear martingale difference sequence. The CLT provides the baseline Gaussian limit to which the TTSA iterates are compared.
  - **Quick check question**: Does a sequence of random variables with zero conditional expectation (E[X_{n}|F_{n-1}] = 0) form a martingale difference sequence?

- **Concept: Step Size Schedules (Polynomial Decay)**
  - **Why needed here**: The rates (n⁻¹/⁴ vs n⁻¹/⁶) depend critically on the exponents a and b chosen for the step sizes γ_k and β_k.
  - **Quick check question**: If a step size is β_k = c · k^{-b}, does a larger exponent b mean the step size decays faster or slower?

## Architecture Onboarding

- **Component map**: Data stream X_k -> Two recursive updates (θ_k, w_k) -> Optional accumulator (averaged θ) -> Convex distance metric
- **Critical path**:
  1. Define step size sequences β_k, γ_k satisfying 1/2 < a < b < 1
  2. Verify Hurwitz stability of the expected matrices A_{22} and Δ
  3. If using Last Iterate: Maximize separation (b ≈ 1, a ≈ 1/2)
  4. If using PR Averaging: Minimize separation (b ≈ a) for best rate

- **Design tradeoffs**:
  - Last Iterate vs. Averaging: Last iterates are better if you can tune high timescale separation (b ≫ a); Averaging is better if you want the standard n⁻¹/⁴ rate without precise tuning of separation, provided the scales are similar
  - Markov vs. Martingale: Expect a slower rate (n⁻¹/⁶) for Markov noise compared to martingale noise (n⁻¹/⁴) due to the cost of mixing time

- **Failure signatures**:
  - Covariance Collapse: If β_k decays too fast relative to γ_k, or if A_{22} is not invertible/stable
  - Non-convergence: If b ≤ 1/2 or if 2b ≤ 1+a, the specific bounds derived in the paper do not hold
  - Bias Dominance: In PR averaging with large separation, the bias term dominates, preventing the Gaussian approximation from tightening

- **First 3 experiments**:
  1. Sanity Check: Replicate the theoretical rates on a synthetic linear system (martingale noise) by plotting β_n^{-1/2}(θ_n - θ*) and verifying the Gaussian density fit improves as n → ∞
  2. Ablation on Separation: Run the TTSA on a reinforcement learning task (e.g., TDC algorithm) varying the ratio β/γ. Confirm that Last Iterate error decreases as separation increases, while Averaged error increases
  3. Noise Type Comparison: Compare the convergence speed (in terms of n) between i.i.d. sampling (martingale) and a trajectory-based sampler (Markov) to validate the n⁻¹/⁴ vs n⁻¹/⁶ gap

## Open Questions the Paper Calls Out

- **Open Question 1**: Can the Gaussian approximation rates derived in Theorems 1-4 be matched by corresponding lower bounds to prove tightness?
  - **Basis in paper**: Conclusion section explicitly calls for "construction of lower bounds to ensure tightness"
  - **Why unresolved**: The paper currently provides only upper bounds on the convex distance
  - **What evidence would resolve it**: Theoretical derivation of a lower bound for ρ_{Conv} that scales identically to the upper bounds (e.g., order n⁻¹/⁴ or n⁻¹/⁶)

- **Open Question 2**: Can valid confidence intervals for the TTSA solution (θ*, w*) be constructed using a non-asymptotic bootstrap approach or online covariance estimation?
  - **Basis in paper**: Conclusion identifies this as a "natural further research direction" for statistical inference
  - **Why unresolved**: The paper establishes normal approximation accuracy but does not propose or analyze a practical procedure for uncertainty quantification
  - **What evidence would resolve it**: A finite-sample analysis of the coverage probability for bootstrap-based confidence intervals under the specified noise settings

- **Open Question 3**: Can the uniform boundedness assumption (A6) on the matrices A_{ij}(x) and vectors b_i(x) be replaced by high-order moment conditions?
  - **Basis in paper**: Authors state in the discussion of assumptions that A6 is likely replaceable by moment conditions
  - **Why unresolved**: Current proofs rely on boundedness of A_{ij}(x) to utilize the Poisson equation effectively
  - **What evidence would resolve it**: Derivation of the main theorems assuming only E[||A_{ij}(X)||^p] < ∞ for sufficiently large p

## Limitations
- Strong dependence on stability assumptions (Hurwitz conditions on system matrices) which may not hold for all RL applications
- Bounds depend on problem-specific constants that are typically unknown in practice
- Fundamental rate gap (n⁻¹/⁴ vs n⁻¹/⁶) between martingale and Markov noise settings

## Confidence
- **High** for the mechanism of error decomposition into linear and nonlinear components
- **Medium** for the specific convergence rates, as these depend on precise tuning of step size parameters
- **Low** for practical applicability to complex reinforcement learning problems with general function approximation

## Next Checks
1. **Rate Verification**: Implement the theoretical step size schedules on a synthetic linear system and empirically measure the convergence rate of the scaled error statistics to verify the predicted n⁻¹/⁴ (martingale) and n⁻¹/⁶ (Markov) scalings hold in practice.

2. **Timescale Separation Study**: For a concrete reinforcement learning task (e.g., TDC algorithm), systematically vary the ratio β/γ and measure the error in both last iterate and averaged settings to confirm that last iterate error decreases with separation while averaged error increases.

3. **Markov Mixing Impact**: Compare convergence rates between i.i.d. sampling and trajectory-based sampling on the same problem instance to validate that mixing time costs are responsible for the n⁻¹/⁶ vs n⁻¹/⁴ gap in practice.