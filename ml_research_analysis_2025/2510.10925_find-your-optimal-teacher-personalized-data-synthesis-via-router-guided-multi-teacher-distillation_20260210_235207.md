---
ver: rpa2
title: 'Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher
  Distillation'
arxiv_id: '2510.10925'
source_url: https://arxiv.org/abs/2510.10925
tags:
- teacher
- qwen2
- student
- math
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PerSyn introduces a router-guided, prompt-level teacher selection
  mechanism that first routes each prompt to its optimal teacher based on learnability
  and quality, then generates data only for assigned prompts. This "Route then Generate"
  paradigm replaces the inefficient "Generate then Select" baseline approach.
---

# Find Your Optimal Teacher: Personalized Data Synthesis via Router-Guided Multi-Teacher Distillation

## Quick Facts
- arXiv ID: 2510.10925
- Source URL: https://arxiv.org/abs/2510.10925
- Reference count: 40
- PerSyn achieves +7.5% on MATH and +8.7% on IFEval over strong single-teacher baseline

## Executive Summary
PerSyn introduces a novel "Route then Generate" paradigm for personalized synthetic data generation in multi-teacher distillation. Instead of generating responses from all teachers then selecting the best, PerSyn first routes each prompt to its optimal teacher based on a combined reward of student learnability and response quality, then generates data only for assigned prompts. This approach addresses the inefficiency of traditional "Generate then Select" methods and leverages the observation that different prompts have different optimal teachers for different students. Across five student models and six benchmarks in instruction tuning and math reasoning, PerSyn consistently outperforms single-teacher and dataset-level teacher selection baselines.

## Method Summary
PerSyn operates through a three-stage process: First, it collects 2.5K prompts and generates parallel responses from all teacher models in the pool. Second, it computes a combined reward for each response using student learnability (log-likelihood under student model) and quality (via reward model), then trains a router (Qwen2.5-1.5B backbone) using Bradley-Terry pairwise comparisons derived from these rewards. Third, the trained router assigns each prompt in the full dataset to its optimal teacher, which then generates only the assigned prompts. The student model is fine-tuned using SFT on this personalized dataset. The key innovation is prompt-level teacher routing that considers both learnability gaps and quality scores, enabling smaller teachers to be optimal for smaller students despite lower raw capability.

## Key Results
- PerSyn achieves +7.5% on MATH and +8.7% on IFEval over strong single-teacher baseline
- Router trained on only 2.5K parallel responses generalizes effectively to full prompt sets
- Over 95% of prompts routed to smaller teacher models for 1.5B student, revealing optimal teacher depends on student capacity
- Quality signal more critical than learnability signal for router training (ablation shows greater performance drop when removing quality)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prompt-level teacher routing improves student learning over single-teacher or dataset-level teacher selection.
- **Mechanism:** PerSyn assigns each prompt to its optimal teacher based on a combined reward (learnability × student + quality × reward model). This creates a personalized synthetic dataset where each sample matches the student's learning capacity and quality requirements, avoiding both trivial responses and learnability gaps.
- **Core assumption:** Different prompts have different optimal teachers for a given student; optimal teacher varies by student model.
- **Evidence anchors:**
  - [abstract] "PerSyn first assigns each prompt to its optimal teacher via a query-level router that jointly considers student learnability and teacher response quality."
  - [section 2.1] Eq. 2 defines r(y, θ) = (1−α)r_quality + αr_learnability, with α=0.4 by default.
  - [corpus] Merge-of-Thought Distillation (arXiv:2509.08814) observes "different students have different [teacher] preferences" — consistent with prompt-level routing hypothesis.
- **Break condition:** If all prompts shared the same optimal teacher for all students, routing would provide no benefit over single-teacher selection.

### Mechanism 2
- **Claim:** Smaller teacher models are often optimal for small students despite having lower raw capability.
- **Mechanism:** Learnability reward penalizes responses with large distributional gaps from student capabilities. Strong teachers may generate outputs that are overly complex or shifted from student distribution, reducing effective learning even if quality is high.
- **Core assumption:** High-quality responses are not necessarily highly learnable; student log-likelihood captures learnability gap.
- **Evidence anchors:**
  - [section 3.3/Fig.6-7] "over 95% of prompts are routed to smaller teacher models... Qwen2.5-3B-Instruct receives higher allocation than Qwen2.5-7B/14B/32B/72B-Instruct and even Llama-3.1-405B."
  - [abstract] "stronger models are not always optimal teachers, revealing a mismatch between teacher outputs and student learnability."
  - [corpus] Distilling Reasoning into Student LLMs (arXiv:2510.03988) confirms "smaller student LLMs" struggle with reasoning traces from stronger teachers.
- **Break condition:** If student model capacity increases substantially, learnability gap shrinks and larger teachers become more optimal.

### Mechanism 3
- **Claim:** A lightweight router trained on only 2.5K parallel responses can approximate oracle teacher selection.
- **Mechanism:** Bradley-Terry model converts pairwise teacher comparisons into preference probabilities. Router (1.5B backbone) learns to predict which teacher maximizes combined reward without generating full responses at inference time.
- **Core assumption:** Pairwise preference patterns generalize from 2.5K samples to full prompt distribution.
- **Evidence anchors:**
  - [section 2.3] Eq. 3-4 define BT model and BCE loss for router training.
  - [section 3.3/Table 3] PerSyn router matches or exceeds Oracle router performance across 5 student models.
  - [section 3.3] "500K pairwise training samples, constructed from only 2.5K prompts with parallel teacher responses, are sufficient."
  - [corpus] Evidence weak — no corpus papers evaluate router-based distillation at comparable sample efficiency.
- **Break condition:** If prompt distribution shifts significantly from training set, router predictions may degrade.

## Foundational Learning

- **Concept: Log-likelihood for learnability estimation**
  - Why needed here: Core signal for measuring how well student can absorb teacher output; computed via student's token probabilities.
  - Quick check question: Can you compute log p(y_t | y_{<t}, x) for a given response-token sequence using the student model?

- **Concept: Bradley-Terry model for pairwise preferences**
  - Why needed here: Router training converts ranked teacher lists into pairwise comparison data for binary classification.
  - Quick check question: Given teachers A, B, C ranked 1st, 2nd, 3rd, how many pairwise comparisons can you extract?

- **Concept: Reward models for quality scoring**
  - Why needed here: Provides quality signal orthogonal to learnability; required for Eq. 2 combination.
  - Quick check question: Does your reward model output scalar scores comparable across responses, or binary correctness only?

## Architecture Onboarding

- **Component map:**
  Router π (1.5B Qwen2.5) → produces teacher preference distribution o ∈ R^|M|
  Teacher pool M = {M_1...M_n} (15-19 models, 1.5B–405B)
  Student θ (target model to distill)
  Reward model (Skywork-Reward-Llama-3.1-8B for instruction tuning; binary for math)

- **Critical path:**
  1. Collect 2.5K prompts → generate parallel responses from all teachers
  2. Score responses via Eq. 2 (learnability + quality) → create pairwise preference data
  3. Train router via BCE loss (Eq. 4)
  4. Route full prompt set → each teacher generates only assigned prompts
  5. Train student via SFT on personalized dataset

- **Design tradeoffs:**
  - α=0.4 balances quality vs. learnability; higher α = more learnability-focused routing (Fig. 4 shows peak at 0.4)
  - Router size: 1.5B comparable to 3B performance (Fig. 5)
  - Long-CoT teachers (DeepSeek-R1): low allocation but necessary for ~5% complex prompts; forced reassignment causes 1.3% performance drop

- **Failure signatures:**
  - Models trained on Strong baseline (single largest teacher) produce "repetitive reasoning without termination" on math tasks
  - Router over-predicting Long-CoT teachers → excessive output length, degraded student performance
  - Router trained with <2.5K parallel responses → Hit@3 drops significantly (Fig. 5)

- **First 3 experiments:**
  1. Reproduce Table 2 for one student model (Qwen2.5-1.5B) on instruction tuning with Strong, Mix, CAR, and PerSyn baselines.
  2. Ablate α ∈ {0.1, 0.4, 0.7} to verify quality-learnability tradeoff on IFEval.
  3. Compare router with 2.5K vs. 1K parallel responses to validate sample efficiency claim.

## Open Questions the Paper Calls Out

- **Question:** Can PerSyn effectively generalize to domains beyond instruction tuning and math reasoning, such as code generation or multimodal understanding?
  - **Basis in paper:** [explicit] The Limitations section explicitly states it "remains unclear whether PerSyn can generalize to other scenarios, such as code generation, multimodal understanding, and other specialized domains."
  - **Why unresolved:** The current study restricted experiments to text-based instruction tuning and math reasoning tasks, leaving the router's applicability to modalities like vision or code untested.
  - **What evidence would resolve it:** Empirical results from experiments applying the PerSyn routing mechanism to code synthesis benchmarks (e.g., HumanEval) or multimodal tasks.

- **Question:** How does the efficiency and effectiveness of PerSyn scale when applied to significantly larger student models (e.g., 32B or 70B parameters)?
  - **Basis in paper:** [explicit] The authors note that experiments were "limited to student models with up to 14B parameters" and they "have not evaluated larger LLMs (e.g., 32B or 70B) due to computational constraints."
  - **Why unresolved:** Larger student models may exhibit different learnability profiles or smaller gaps with strong teachers, potentially changing the optimal routing distribution.
  - **What evidence would resolve it:** Benchmarks demonstrating student performance and allocation ratios when distilling into 32B or 70B models using the PerSyn router.

- **Question:** Is the optimal weighting between learnability and quality (α) static, or should it be adjusted dynamically based on prompt complexity or student capacity?
  - **Basis in paper:** [inferred] The paper fixes α=0.4 based on aggregate trends (Fig 4), but the analysis implies that student learnability varies significantly by prompt.
  - **Why unresolved:** A global scalar weight may not capture the nuanced trade-off for specific difficult prompts where quality is paramount versus easier prompts where learnability is sufficient.
  - **What evidence would resolve it:** Ablation studies comparing the performance of a fixed α against a router that predicts a prompt-specific α value.

## Limitations

- **Reward Model Quality Dependence:** The combined reward function relies on quality scores from a reward model (Skywork-Reward-Llama-3.1-8B for instruction tuning, binary correctness for math). The paper does not analyze reward model calibration or potential biases.
- **Prompt Distribution Shift:** Router training uses only 2.5K parallel responses, which may not fully capture the diversity of the full prompt set (50K for instruction tuning).
- **Generalization Beyond Tested Models:** While PerSyn demonstrates effectiveness across 5 student models (1.5B-32B) and 6 benchmarks, the results may not generalize to very small models (<1B) or much larger frontier models.

## Confidence

**High Confidence**: PerSyn improves student performance over single-teacher and dataset-level teacher selection baselines. This claim is supported by consistent improvements across multiple student models and benchmarks (e.g., +7.5% on MATH, +8.7% on IFEval). The "Route then Generate" paradigm is empirically validated.

**Medium Confidence**: Smaller teacher models are often optimal for small students due to learnability gaps. While the prompt allocation analysis strongly supports this (95%+ routed to smaller teachers for 1.5B student), the underlying mechanism relies on the learnability reward calculation, which depends on the student's token likelihood model.

**Medium Confidence**: A lightweight router trained on only 2.5K parallel responses can effectively approximate oracle teacher selection. The router achieves competitive performance to the Oracle router, but the sample efficiency claim rests on one training configuration.

## Next Checks

1. **Reward Model Ablation:** Replace Skywork-Reward-Llama-3.1-8B with a simpler reward signal (e.g., GPT-4 zero-shot scoring) and measure the impact on PerSyn performance. This would test the sensitivity of routing to reward model quality and calibrate the importance of the quality component.

2. **Cross-Dataset Router Transfer:** Train the router on 2.5K prompts from one dataset (e.g., OpenR1-Math-220K) and evaluate routing performance on a different dataset (e.g., GSM8K). This would test whether pairwise preference patterns learned from limited samples generalize across prompt distributions.

3. **Extreme Scale Testing:** Apply PerSyn to distill a 1B student from a mix of teachers including very small (0.5B) and very large (405B) models. This would validate whether the learnability-quality tradeoff mechanism holds at model scale extremes and identify any new failure modes.