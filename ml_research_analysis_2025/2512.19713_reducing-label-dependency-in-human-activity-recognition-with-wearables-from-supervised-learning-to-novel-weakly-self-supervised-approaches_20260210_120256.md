---
ver: rpa2
title: 'Reducing Label Dependency in Human Activity Recognition with Wearables: From
  Supervised Learning to Novel Weakly Self-Supervised Approaches'
arxiv_id: '2512.19713'
source_url: https://arxiv.org/abs/2512.19713
tags:
- learning
- data
- supervised
- weakly
- activity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates human activity recognition (HAR) using
  wearable sensors, addressing the challenge of label dependency in machine learning
  approaches. The authors systematically explore and develop six learning paradigms
  spanning the supervision spectrum: fully supervised learning, basic unsupervised
  learning, weakly supervised learning with constraints, multi-task learning with
  knowledge sharing, self-supervised learning based on domain expertise, and a novel
  weakly self-supervised framework that integrates both weak supervision and self-supervision.'
---

# Reducing Label Dependency in Human Activity Recognition with Wearables: From Supervised Learning to Novel Weakly Self-Supervised Approaches

## Quick Facts
- arXiv ID: 2512.19713
- Source URL: https://arxiv.org/abs/2512.19713
- Reference count: 40
- Primary result: Weakly self-supervised approach achieves 99.04% accuracy on PAMAP2 using only 10% labeled data

## Executive Summary
This paper addresses the challenge of label dependency in human activity recognition (HAR) by developing and evaluating six learning paradigms spanning the supervision spectrum. The authors propose a novel weakly self-supervised framework that combines self-supervised pre-training with limited pairwise constraints, achieving competitive performance with minimal labeled data. Experimental results across three benchmark datasets demonstrate that this approach can reach 99.04% accuracy on PAMAP2 using just 10% of labeled data, surpassing fully supervised baselines.

## Method Summary
The weakly self-supervised approach employs a two-stage training process: first, temporal and feature consistency losses enable self-supervised pre-training on unlabeled data; second, the model is fine-tuned using limited pairwise similarity constraints (must-link/cannot-link pairs) in a weakly supervised manner. The method uses a residual autoencoder with temporal consistency (P=5 neighbors), feature consistency (Q=5 kNN), and reconstruction losses with weights [0.3, 0.5, 0.2] in Stage 1, then adds a similarity loss with γ=0.8 weight in Stage 2. The approach is compared against fully supervised TCN+ResNet, basic unsupervised autoencoder, weakly supervised Siamese networks, and multi-task learning paradigms.

## Key Results
- Weakly self-supervised approach achieves 99.04% accuracy on PAMAP2 with 10% labeled data
- Multi-task weakly supervised approach reaches 98.93% accuracy on PAMAP2
- t-SNE visualizations show distinct clustering of activities, validating learned representations
- Temporal consistency alone can degrade performance on complex datasets (REALDISP: 64.01% → 63.41%)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining self-supervised pre-training with limited pairwise constraints achieves competitive HAR performance using only 10% labeled data.
- **Mechanism:** The two-stage framework first learns generalizable representations through temporal and feature consistency losses (self-supervision), then refines these representations using pairwise similarity constraints (weak supervision). This ordering matters: self-supervision establishes a useful initialization before sparse labels refine class boundaries.
- **Core assumption:** Temporally proximate samples and feature-space neighbors share activity semantics, providing valid supervisory signals.
- **Evidence anchors:** [abstract]: "The weakly self-supervised approach employs a two-stage training process that combines temporal and feature consistency losses in the first stage with limited pairwise constraints in the second stage."

### Mechanism 2
- **Claim:** Pairwise similarity constraints provide sufficient supervision for discriminative representation learning without explicit class labels.
- **Mechanism:** Siamese networks process paired inputs through weight-shared branches, and contrastive loss pulls similar pairs closer while pushing dissimilar pairs apart by at least margin δ. This structures the embedding space for downstream clustering without requiring per-sample class labels.
- **Core assumption:** Pairwise relationship labels are easier or cheaper to obtain than full class annotations.
- **Evidence anchors:** [Section 3.3]: "This method trains a Siamese network to provide a similarity metric, enabling activity clustering without the strict requirement of explicitly labeled data."

### Mechanism 3
- **Claim:** Multi-task learning with HAR and person identification improves representation quality through shared knowledge.
- **Mechanism:** A shared encoder feeds two task-specific heads (activity similarity, person similarity). The joint loss balances both objectives, encouraging the shared representation to capture complementary signals: activity-invariant person features and person-invariant activity features.
- **Core assumption:** HAR and person identification are semantically related enough that shared representations benefit both.
- **Evidence anchors:** [Section 3.4]: "HAR and person identification exhibit a strong connection. Integrating them within a single weakly supervised multi-task model is a logical step."

## Foundational Learning

- **Concept:** Contrastive loss with margin
  - **Why needed here:** Core objective for Siamese networks; understanding margin δ and the separate loss terms for similar/dissimilar pairs is essential for debugging weak supervision performance.
  - **Quick check question:** If your model pulls all pairs to zero distance regardless of similarity label, which loss term (Ls or Ld) is likely dominating?

- **Concept:** Reconstruction loss as regularization
  - **Why needed here:** Prevents trivial solutions in self-supervised learning; the ablation shows Φae alone underperforms but remains necessary when combined with consistency losses.
  - **Quick check question:** What happens to representation quality if you remove Φae entirely and only use Φtc and Φfc?

- **Concept:** k-means on learned embeddings
  - **Why needed here:** All non-supervised approaches output embeddings for clustering rather than direct classification; evaluation protocol assumes k-means is appropriate for the learned space.
  - **Quick check question:** If t-SNE visualizations show elongated rather than spherical clusters, would k-means still be optimal?

## Architecture Onboarding

- **Component map:** Raw sensor data → TCN/ResNet encoder → 96-dim embedding → batch norm → dropout → global pooling → activity prediction (supervised) OR Feature extraction → Residual autoencoder encoder [256→256→128→96] → Siamese wrapper → Distance computation → Contrastive loss (weakly supervised)

- **Critical path:** For weakly self-supervised: (1) extract handcrafted features, (2) Stage 1 self-supervised training with Φtc + Φfc + Φae, (3) Stage 2 add Φact_simi with γ=0.8 weight, (4) extract embeddings, (5) apply k-means with k = number of activity classes.

- **Design tradeoffs:** Handcrafted features vs. raw data: Self-supervised and unsupervised use handcrafted features for domain inductive bias; supervised/weakly supervised use raw data directly. Multi-task improves accuracy but conflicts with feature consistency (person-removal) objective—cannot combine with self-supervised stage. Margin δ=1.0: Fixed across experiments; Assumption: dataset-specific tuning might improve results but was not explored.

- **Failure signatures:** Temporal-only self-supervision degrades REALDISP (Table 7): likely due to complex 33-activity dataset with rapid transitions. Unsupervised autoencoder achieves only 63-77%: no task-specific guidance. 1% labels in weakly self-supervised drops accuracy ~14-17 points from 10%: limited supervision signal dominates.

- **First 3 experiments:**
  1. Baseline reproduction: Run supervised TCN and unsupervised autoencoder on PAMAP2; verify you can match reported 95.27% and 77.06% accuracy before attempting hybrid methods.
  2. Ablation on loss weights: Systematically vary α, β, γ in Equation 14 on a held-out validation split; the paper reports α=0.05, β=0.1, γ=0.8 for Stage 2 but does not show sensitivity analysis.
  3. Dataset transfer test: Train weakly self-supervised model on PAMAP2 (9 subjects, 12 activities), evaluate on UCI-Smartphone (30 subjects, 6 activities) without retraining Stage 1; this tests generalization which the paper does not address.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the weakly self-supervised framework generalize to real-world deployment scenarios with sensor displacement, variable placement, and diverse user populations beyond the three benchmark datasets?
- **Basis in paper:** [explicit] The conclusion states: "Future work will consider expanding evaluation to additional datasets and deployment scenarios to further assess generalizability under varied sensor conditions." Additionally, the authors acknowledge using only ideal-placement settings for REALDISP because "real-world deployments introduce additional variability not captured in this setup."
- **Why unresolved:** All experiments used controlled benchmark datasets with standardized sensor configurations, leaving performance under noisy, displaced, or inconsistent real-world sensor conditions untested.
- **What evidence would resolve it:** Evaluation on datasets with intentionally displaced sensors, varied body positions, and longitudinal deployment studies with diverse user demographics.

### Open Question 2
- **Question:** Can Transformer-based architectures (e.g., PatchTST, TimesNet) or multi-scale architectures (e.g., InceptionTime) improve representation quality and label efficiency over the TCN/ResNet backbones used in this work?
- **Basis in paper:** [explicit] The positioning section notes: "In our work, we adopt TCN and ResNet backbones for their proven effectiveness and computational efficiency on sensor data, while noting that these newer architectures present promising directions for future exploration."
- **Why unresolved:** The study deliberately limited architectural exploration to TCN and ResNet for controlled comparison of learning paradigms, leaving potential benefits of more recent architectures unexplored.
- **What evidence would resolve it:** Controlled experiments replacing the TCN/ResNet encoder with Transformer or Inception-based encoders while keeping all other factors constant.

### Open Question 3
- **Question:** How can the temporal consistency assumption be modified to handle activity transitions and complex temporal dynamics that violate local continuity?
- **Basis in paper:** [inferred] The authors note that "activity transitions, such as sit-to-stand or stand-to-walk, tend to produce ambiguous signal patterns that may challenge the assumption of temporal continuity." Additionally, on REALDISP, adding temporal consistency alone decreased performance (64.01% → 63.41%), attributed to "more complex temporal dynamics."
- **Why unresolved:** The temporal consistency loss assumes nearby samples share the same activity class, which breaks during transitions and for activities with rapid state changes.
- **What evidence would resolve it:** Modified temporal consistency formulations that explicitly model transition regions, or adaptive weighting that reduces temporal consistency influence near detected transition boundaries.

## Limitations

- The paper does not report hyperparameter sensitivity analysis or statistical validation across multiple random seeds
- Multi-task approach cannot be combined with weakly self-supervised framework due to conflicting objectives
- Temporal consistency assumption may degrade performance on datasets with frequent activity transitions (REALDISP)

## Confidence

- **High confidence:** The overall effectiveness of the weakly self-supervised two-stage approach is well-supported by results across all three datasets
- **Medium confidence:** The specific mechanism claims are supported by ablation studies but lack external validation from the corpus
- **Low confidence:** The claim that 10% labeled data is sufficient assumes specific data distribution and activity transition patterns

## Next Checks

1. **Hyperparameter sensitivity analysis:** Systematically vary α, β, γ weights in the weakly self-supervised loss across a validation set to identify optimal ratios and show robustness to parameter changes.

2. **Statistical significance testing:** Report mean and standard deviation across at least 5 random seeds for all experimental conditions to establish whether observed improvements are statistically significant.

3. **Transfer learning evaluation:** Train the weakly self-supervised model on one dataset (e.g., PAMAP2) and evaluate directly on another (e.g., UCI-Smartphone) without fine-tuning to test generalization of the learned representations.