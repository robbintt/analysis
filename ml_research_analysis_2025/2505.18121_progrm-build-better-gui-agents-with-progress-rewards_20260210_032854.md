---
ver: rpa2
title: 'ProgRM: Build Better GUI Agents with Progress Rewards'
arxiv_id: '2505.18121'
source_url: https://arxiv.org/abs/2505.18121
tags:
- progress
- steps
- training
- trajectories
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces PROGRM, a progress-based reward model designed
  to improve GUI agents by providing dense intermediate rewards during online reinforcement
  learning. Unlike traditional outcome rewards, which only signal success or failure
  at the end of a task, PROGRM estimates task progress at each step, enabling more
  informative and efficient training.
---

# ProgRM: Build Better GUI Agents with Progress Rewards

## Quick Facts
- arXiv ID: 2505.18121
- Source URL: https://arxiv.org/abs/2505.18121
- Authors: Danyang Zhang; Situo Zhang; Ziyue Yang; Zichen Zhu; Zihan Zhao; Ruisheng Cao; Lu Chen; Kai Yu
- Reference count: 40
- One-line primary result: ProgRM achieves 62.00% success rate on WikiHow GUI tasks, outperforming proprietary models and ORM baselines

## Executive Summary
This paper introduces ProgRM, a progress-based reward model designed to improve GUI agents by providing dense intermediate rewards during online reinforcement learning. Unlike traditional outcome rewards that only signal success or failure at task completion, ProgRM estimates task progress at each step, enabling more informative and efficient training. To address the challenge of progress label annotation, the authors propose an LCS-based self-annotation algorithm that automatically identifies key steps in trajectories and assigns corresponding progress labels without requiring human effort. Evaluated on the WikiHow benchmark, GUI agents trained with ProgRM achieve a 62.00% success rate, outperforming state-of-the-art proprietary models like Claude-3.7-Sonnet (56.00%) and ORM-based approaches.

## Method Summary
The method involves three key components: (1) LCS-based self-annotation that groups successful trajectories by soft-LCS similarity, extracts common subsequences as recipes, and identifies key steps without human annotation; (2) training a progress model (ProgRM) using a lightweight LLM encoder with MLP head and BCE loss on the self-annotated data; (3) online RL training with REINFORCE++ adapted for multi-turn token-level credit assignment, using progress gain rewards. The approach is evaluated on the WikiHow Android benchmark with 577 tasks, achieving superior success rates compared to outcome reward models and proprietary baselines.

## Key Results
- Achieves 62.00% success rate on WikiHow benchmark, outperforming Claude-3.7-Sonnet (56.00%) and ORM (54.00%)
- ProgRM shows 90.67% accuracy on success/failure prediction vs 73.33% for ORM
- Key step progress estimation error of 0.126 for LCS-based method vs 0.036 for environment-reward-based method

## Why This Works (Mechanism)

### Mechanism 1
Dense progress rewards improve RL sample efficiency over sparse outcome rewards. ProgRM estimates `pt = Prog(st; g)` at each step, then computes reward as progress gain `r(p)t = Prog(st; g) − Prog(st−k; g)`. This provides credit for partial task completion even in failed trajectories, unlike ORM which assigns 0 reward to all steps in failed episodes. Core assumption: Progress can be meaningfully quantified along a monotonic trajectory toward task completion.

### Mechanism 2
LCS-based recipe extraction identifies key steps without human annotation. Group successful trajectories by soft-LCS similarity, extract common subsequence as "recipe." Match new trajectories to best recipe via completion ratio. Steps in the matched LCS are key steps; others inherit preceding key-step progress. Core assumption: Successful trajectories for the same task share common behavior patterns that represent genuine progress milestones.

### Mechanism 3
BCE-trained progress model generalizes to unseen trajectories. LLM encoder + MLP head with sigmoid outputs `p̂ ∈ [0,1]`. Trained on self-annotated progress labels using BCE loss. State representation uses action history + current screen observation, enabling sequential context. Core assumption: The self-annotated labels sufficiently approximate true progress for supervised learning to generalize.

## Foundational Learning

- Concept: **Longest Common Subsequence (LCS)**
  - Why needed here: Core algorithm for extracting shared "recipes" from variable-length action trajectories.
  - Quick check question: Can you implement soft-LCS that handles text arguments with partial matches?

- Concept: **REINFORCE++ / Policy Gradient with Baselines**
  - Why needed here: Online RL algorithm adapted for multi-turn LLM agents with token-level credit assignment.
  - Quick check question: How does discount factor differ for intra-turn vs inter-turn transitions?

- Concept: **Dense vs Sparse Rewards**
  - Why needed here: Understanding why outcome rewards fail for long-horizon GUI tasks.
  - Quick check question: What is the credit assignment problem in a 50-step trajectory with only terminal reward?

## Architecture Onboarding

- Component map: Recipe Library Builder -> Key Step Matcher -> Progress Labeler -> Progress Model -> RL Trainer
- Critical path: 1) Collect successful trajectories per task; 2) Build recipe library via soft-LCS grouping; 3) Annotate all trajectories with progress labels; 4) Train ProgRM to convergence; 5) Initialize actor with SFT, then online RL with ProgRM rewards
- Design tradeoffs: History length k=1 performs best; longer k degrades performance. ProgRMEnv outperforms ProgRMLCS but requires environment support. Lightweight self-hosted RM (0.05s latency) vs general-purpose evaluator (5.7s+ latency)
- Failure signatures: High false positive rate in ORM causes misleading high scores for incorrect actions. LCS-based labeling misses key steps when trajectories are heterogeneous. QA tasks suffer after SFT/RL due to reduced general language capability
- First 3 experiments: 1) Train ProgRM on environment-labeled data; verify it reaches 62% SR baseline. 2) Ablation on k: Compare k=1 vs k=3; expect k=3 to degrade as reported in Table 4. 3) Recipe quality audit: For 10 tasks, manually inspect LCS recipes vs ground-truth key steps; measure precision/recall of key step discovery

## Open Questions the Paper Calls Out
- How can the automatic key step discovery algorithm be optimized to close the performance gap between LCS-based progress labels and ground-truth environment labels?
- Can ProgRM generalize effectively to diverse GUI environments beyond the WikiHow benchmark, specifically those lacking well-annotated training tasks?
- Can additional reinforcement learning steps fully restore the general language capabilities required for QA tasks that were diminished during GUI-specific supervised fine-tuning?

## Limitations
- Self-annotation quality presents a significant uncertainty with key step progress estimation error of 0.126 for LCS-based method vs 0.036 for environment-reward-based method
- Generalization to new task types remains untested, with evaluation focused exclusively on WikiHow Android tasks
- Trade-off with general language capability is concerning, as RL training degrades QA performance from 100% to 96.88%

## Confidence
- High Confidence: Dense progress rewards improve RL sample efficiency (Table 2: 62.00% SR vs 54.00% for ORM); REINFORCE++ adaptation works effectively (baseline success against Claude-3.7-Sonnet at 56.00% SR)
- Medium Confidence: LCS-based recipe extraction identifies meaningful key steps (Table 3 shows better progress estimation than ORM); BCE-trained progress model generalizes to unseen trajectories (90.67% accuracy vs 73.33% for ORM)
- Low Confidence: Self-annotation approach scales to arbitrary GUI domains (only validated on WikiHow benchmark); Progress rewards don't compromise general language capabilities (only tested on limited QA subset with modest degradation)

## Next Checks
1. **Recipe Quality Audit**: Manually inspect the LCS recipes extracted for 10 diverse WikiHow tasks, comparing them against ground-truth key steps. Calculate precision and recall to quantify the annotation quality gap.
2. **Cross-Domain Generalization Test**: Evaluate ProgRM-trained agents on at least two different GUI domains (e.g., web-based tasks like WebShop or desktop applications) to test transfer effectiveness.
3. **Progress Signal Ablation**: Train agents using only progress rewards from environment milestones versus LCS-based rewards on a subset of tasks. Compare learning curves and sample efficiency to determine whether the self-annotation approach provides sufficient signal despite its higher error rate.