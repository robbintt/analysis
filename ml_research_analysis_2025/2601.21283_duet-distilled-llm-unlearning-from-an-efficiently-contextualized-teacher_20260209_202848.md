---
ver: rpa2
title: 'DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher'
arxiv_id: '2601.21283'
source_url: https://arxiv.org/abs/2601.21283
tags:
- unlearning
- duet
- knowledge
- arxiv
- forgetting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DUET, a novel distillation-based unlearning
  method that combines the strengths of training-based and in-context unlearning approaches.
  The method uses an efficiently contextualized teacher model to provide supervision
  signals for distilling unlearning behavior into a student model through top-K logit
  alignment.
---

# DUET: Distilled LLM Unlearning from an Efficiently Contextualized Teacher

## Quick Facts
- arXiv ID: 2601.21283
- Source URL: https://arxiv.org/abs/2601.21283
- Authors: Yisheng Zhong; Zhengbang Yang; Zhuangdi Zhu
- Reference count: 40
- One-line primary result: DUET achieves competitive or superior unlearning performance compared to state-of-the-art methods while requiring only query-level data without explicit responses.

## Executive Summary
This paper proposes DUET, a novel distillation-based unlearning method that combines the strengths of training-based and in-context unlearning approaches. The method uses an efficiently contextualized teacher model to provide supervision signals for distilling unlearning behavior into a student model through top-K logit alignment. DUET achieves effective forgetting while preserving general knowledge utility, demonstrating competitive or superior performance compared to state-of-the-art unlearning methods on both Harry Potter and WMDP benchmarks. The approach is data-efficient, requiring only query-level data without explicit responses or refusal templates, and shows robustness against reverse engineering attacks and evaluation format variations.

## Method Summary
DUET addresses LLM unlearning by distilling an in-context teacher's unlearning behavior into a student model through top-K logit alignment. The teacher is a frozen base LLM with an unlearning prefix (e.g., "You are an AI Assistant who has unlearned about Harry Potter..."), while the student is a trainable copy. For each batch mixing forget queries and retain queries, DUET computes teacher logits at the first decoding position, selects the top-K indices from these logits, and aligns student logits to teacher logits using Huber L-1 loss. This unified approach eliminates the need for separate retention losses or explicit refusal responses, requiring only 100-200 forget queries and 100 retain queries for training.

## Key Results
- DUET outperforms baselines like GA, NPO, FLAT, and RMU on Harry Potter and WMDP benchmarks while maintaining higher data efficiency
- Top-K=1000 logit alignment provides optimal balance between forgetting effectiveness and utility preservation
- DUET demonstrates robustness against reverse engineering attacks and evaluation format variations
- The method requires only query-level data without explicit responses or refusal templates

## Why This Works (Mechanism)

### Mechanism 1: Top-K Logit Alignment Concentrates Supervision on Decision-Relevant Tokens
Distilling only the top-K logits from a teacher provides more precise unlearning signals than full-vocabulary alignment. The teacher model produces shifted logits where refusal/uncertainty tokens receive high scores and domain-specific tokens are suppressed. By aligning only these top-K candidates in the student, DUET embeds the refusal pattern without noise from low-probability vocabulary tokens.

### Mechanism 2: In-Context Teacher Provides Semantic Boundary for Forgetting vs. Retention
A prompt-steered teacher model can semantically distinguish queries targeting undesirable knowledge from general-domain queries. The teacher prefix leverages the base model's semantic understanding to refuse targeted queries while leaving unrelated queries unaffected, yielding clean supervision that naturally handles the forgetting-retention boundary.

### Mechanism 3: Parameterized Distillation Confers Robustness Against Prompt Removal Attacks
Embedding the teacher's refusal behavior into student model parameters makes unlearning robust to reverse-engineering attacks that remove or override in-context prompts. DUET distills the pattern of logit shifts into weights, decoupling the unlearning behavior from any runtime prompt.

## Foundational Learning

- **Knowledge Distillation (Teacher-Student Framework)**: DUET's core operation is distilling logit distributions from a prompted teacher to a student model. Without understanding distillation objectives, the alignment loss and top-K selection rationale will be opaque.
  - Quick check: Can you explain why aligning logits is different from aligning token sequences (as in standard supervised fine-tuning)?

- **In-Context Learning and Prompt Engineering**: The teacher's unlearning behavior is entirely controlled by the in-context prefix. Prefix quality directly determines forgetting efficacy and retention.
  - Quick check: What happens if you use a generic "refuse-all" prefix versus a domain-specific one, based on Table 5?

- **Unlearning Problem Formulation (Forget Set vs. Retain Set)**: DUET mixes forget and retain samples in each batch under a unified objective. Understanding the trade-off between forgetting and retention is essential for interpreting metrics and hyperparameter choices.
  - Quick check: Why does adding a separate KL-divergence retention loss (as in GA+KL or NPO+KL) often hurt forgetting effectiveness, whereas DUET's unified distillation does not?

## Architecture Onboarding

- **Component map**: Base LLM -> Teacher (frozen + prefix) -> Student (trainable) -> Top-K selector -> Huber L-1 loss -> Weight update

- **Critical path**: 1) Construct forget queries (100-200 samples) and retain queries (100 samples), 2) Design and validate teacher prefix on held-out probes, 3) For each batch, compute teacher logits with prefix, student logits without prefix, select top-K indices, accumulate Huber loss, and update student weights, 4) Evaluate on expanded forget set (500 samples) and retention set.

- **Design tradeoffs**: K selection (K=1000 optimal; K=1 too sparse, K=5000 too noisy), prefix design (domain-specific outperforms generic), unified vs. separate retention (unified simplifies training but requires careful prefix design).

- **Failure signatures**: Catastrophic utility drop (overly aggressive learning rate or poor prefix), ineffective forgetting (prefix too generic or K poorly chosen), vulnerability to reverse prompts (incomplete distillation).

- **First 3 experiments**: 1) Baseline prefix sweep testing 3-5 prefix variants on a small forget set, 2) K ablation training DUET with K âˆˆ {1, 100, 1000, 5000} on fixed prefix, 3) Robustness check applying reverse prompts to compare R-Forget with and without attack.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can unlearning boundaries be precisely determined when the distinction between forget data and retain data is ambiguous or semantically overlapping?
- Basis in paper: [explicit] "Determining the boundary of unlearning represents a significant challenge, specifically regarding what to forget versus what to retain. When the distinction between forget data and retain data is ambiguous, samples near the boundary exhibit poor unlearning performance."
- Why unresolved: Current methods define boundaries through training data, which creates problems when categories overlap semantically. DUET's prompt-based approach offers a perspective but does not fully resolve boundary ambiguity.
- What evidence would resolve it: Development of automated boundary detection methods that achieve consistent forgetting performance across ambiguous edge cases, with quantitative metrics for boundary precision.

### Open Question 2
- Question: How can LLM unlearning be evaluated comprehensively to verify that knowledge is truly removed rather than superficially refused?
- Basis in paper: [explicit] "How to evaluate whether a model has truly unlearned is another critical challenge... it is difficult to evaluate whether the model merely refuses to answer on the surface while still retaining deep knowledge of the unlearning target."
- Why unresolved: Existing methods like MIA and jailbreak techniques remain insufficient because models can evade detection. Surface-level refusal metrics do not guarantee internal knowledge erasure.
- What evidence would resolve it: Novel evaluation protocols combining probing techniques, behavioral tests, and internal representation analysis that correlate with verified knowledge removal.

### Open Question 3
- Question: Can unlearning methods be made robust against sophisticated jailbreak attacks that reformulate harmful requests into academically or creatively framed questions?
- Basis in paper: [explicit] "While DUET achieves the lowest ASR among all compared approaches, it remains vulnerable to jailbreak attacks... improving the robustness of unlearned models against jailbreak attacks [is] an important direction for future research."
- Why unresolved: Current objective-focused unlearning methods struggle under adversarial prompting. DUET achieved 35.62% overall ASR, indicating significant vulnerability remains.
- What evidence would resolve it: Development of adversarial training or objective formulations that maintain low ASR (e.g., <10%) under diverse jailbreak attack categories while preserving utility.

### Open Question 4
- Question: Does extending logit distillation beyond the first decoding step improve unlearning for complex multi-step reasoning tasks without degrading utility?
- Basis in paper: [explicit] "Studying ability distillation for more complex reasoning tasks (e.g., multi-step mathematics or logical inference) remains an intriguing direction for future research."
- Why unresolved: The paper's experiments show multi-step distillation yields marginal forgetting improvements but causes utility degradation, leaving optimal approaches for complex tasks unknown.
- What evidence would resolve it: Systematic evaluation on multi-step reasoning benchmarks showing whether extended distillation horizons improve forgetting of procedural knowledge while maintaining reasoning capabilities.

## Limitations

- The evaluation relies heavily on in-house query sets with limited detail on construction methodology, making exact replication challenging
- The method requires carefully constructed forget queries that must probe target knowledge specifically, creating a hidden data curation burden
- Prefix design sensitivity means significant practitioner expertise is required to achieve optimal performance
- All evaluations use single-step predictions, without validation that refusal behavior persists through multi-token generation sequences

## Confidence

- **High confidence**: DUET achieves competitive performance on standard unlearning benchmarks (R-Forget, WMDP Acc) compared to GA, NPO, FLAT, and RMU baselines
- **Medium confidence**: DUET demonstrates robustness against reverse engineering attacks and evaluation format variations
- **Medium confidence**: DUET preserves general knowledge utility while achieving effective forgetting

## Next Checks

1. **Multi-step generation robustness test**: Generate complete responses (not just first-token predictions) for both forget and retain queries under various decoding parameters (temperature, top-p). Measure whether refusal patterns persist through full sequences and whether utility degradation occurs in longer outputs.

2. **Cross-domain prefix transferability study**: Systematically test whether teacher prefixes optimized for one domain (Harry Potter) transfer to semantically similar but distinct domains (other copyrighted works). Measure forgetting effectiveness and retention preservation across domain boundaries to assess prefix generalization.

3. **Real-world deployment simulation**: Implement a reverse engineering attack that combines prompt manipulation, adversarial examples, and knowledge extraction techniques beyond simple prefix removal. Evaluate whether DUET's parameter-level distillation maintains robustness against sophisticated attempts to recover unlearned knowledge.