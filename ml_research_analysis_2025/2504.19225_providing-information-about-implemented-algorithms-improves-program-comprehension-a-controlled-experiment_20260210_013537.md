---
ver: rpa2
title: 'Providing Information About Implemented Algorithms Improves Program Comprehension:
  A Controlled Experiment'
arxiv_id: '2504.19225'
source_url: https://arxiv.org/abs/2504.19225
tags:
- participants
- algorithm
- labels
- comprehension
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether annotating source code with algorithm
  labels improves program comprehension. A controlled experiment with 56 participants
  compared a control group to an experimental group receiving algorithm labels.
---

# Providing Information About Implemented Algorithms Improves Program Comprehension: A Controlled Experiment

## Quick Facts
- arXiv ID: 2504.19225
- Source URL: https://arxiv.org/abs/2504.19225
- Reference count: 40
- Primary result: Algorithm labels improved program comprehension by 23% (p=0.040)

## Executive Summary
This controlled experiment demonstrates that annotating source code with algorithm labels (algorithm name + Wikipedia link) significantly improves program comprehension scores by 23% compared to unannotated code. The study involved 56 participants performing comprehension tasks on Java code containing algorithms like Quicksort and Binary Search. While labels didn't reduce completion times (p=0.991), qualitative feedback revealed participants found labels helpful for recognizing code intent and enabling focused external searches. Medium-experience developers showed the largest benefit, suggesting algorithm labels bridge knowledge gaps rather than replace expertise.

## Method Summary
The study used a between-subjects design with 56 participants randomly assigned to control (unannotated code) or experimental (algorithm-labeled code) groups. Participants completed four comprehension exercises involving mental simulation and maintenance tasks on the CoCoME codebase, with exercises including mean calculation, Levenshtein Distance, and debugging Quicksort/Binary Search implementations. Program comprehension scores (0-40.5 points) were measured and analyzed using PERMANCOVA with programming experience as a covariate. Each exercise had strict time limits (17-30 minutes), and scores were independently rated by two assessors.

## Key Results
- Algorithm labels improved program comprehension scores by 6 points (~23%) with statistical significance (p=0.040)
- No significant difference in completion times between groups (p=0.991)
- Medium-experience developers showed the highest relative improvement (8.3 points) compared to high-experience developers (2.3 points)
- Most participants found labels helpful for understanding code intent and enabling focused external searches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Algorithm labels enable top-down comprehension strategies by providing high-level intent before code inspection.
- Mechanism: Labels give developers a "beacon" that triggers hypothesis formation about code purpose. Instead of reading line-by-line, developers verify code matches the labeled algorithm, reducing cognitive load.
- Core assumption: Developers have sufficient prior knowledge of the algorithm to form meaningful hypotheses when cued by its name.
- Evidence anchors: [abstract]: "Qualitative feedback revealed that most participants found labels helpful for recognizing code intent"; [section 4.1]: "This aligns with the concept of top-down comprehension where developers recognize familiar patterns, quickly validate them, and infer the code's meaning instead of resorting to time-consuming bottom-up comprehension"
- Break condition: If developers don't recognize the algorithm name, labels provide no schema to activate—effect nullifies.

### Mechanism 2
- Claim: Labels function as information retrieval scaffolds, enabling targeted external searches.
- Mechanism: Providing both algorithm name and Wikipedia link gives developers precise search terms, reducing time spent on exploratory queries and filtering irrelevant results.
- Core assumption: The linked resource contains information relevant to the comprehension task at hand.
- Evidence anchors: [abstract]: "enabling focused searches" listed as a key qualitative finding; [section 4.1]: "Participants expressed that the labels enabled them to search for additional information online to better understand the algorithms, with many mentioning that reference implementations or pseudocode were particularly useful"
- Break condition: If labels are inaccurate or link to low-quality resources, developers may be misled, potentially harming comprehension.

### Mechanism 3
- Claim: Labels benefit medium-experience developers most because they bridge the gap between partial and complete algorithm recognition.
- Mechanism: High-experience developers already recognize algorithms without labels; low-experience developers lack sufficient foundational knowledge to use labels effectively. Medium-experience developers have enough knowledge to leverage labels but not enough to recognize algorithms unaided.
- Core assumption: The U-shaped benefit curve reflects expertise reversal effect—supports become less helpful as expertise increases.
- Evidence anchors: [abstract]: "especially for participants with medium programming experience"; [section 3.1.2]: "participants with medium programming experience have the highest relative improvement by 8.3 points... improvement is less pronounced for participants with high programming experience with a difference of 2.3 points"
- Break condition: If tasks require debugging beyond algorithm recognition (e.g., business logic errors), labels provide diminishing returns.

## Foundational Learning

- Concept: Top-down vs. bottom-up program comprehension
  - Why needed here: The paper's mechanism hinges on labels shifting comprehension strategy. Without understanding this distinction, the 23% improvement appears magical.
  - Quick check question: When you see a function named `quicksort()`, do you read every line or verify it matches the expected partition-and-recurse pattern?

- Concept: Beacon-based comprehension
  - Why needed here: Labels act as beacons—code cues that trigger recognition. Understanding beacons explains why a simple comment can outperform extensive code reading.
  - Quick check question: What code features typically serve as beacons for you (names, comments, structure)?

- Concept: Confounding variables in controlled experiments
  - Why needed here: The study controls for programming experience (covariate). Applying this research requires understanding why uncontrolled experience differences could masquerade as treatment effects.
  - Quick check question: If a study claimed labels helped but didn't measure participants' prior algorithm knowledge, would you trust the result?

## Architecture Onboarding

- Component map: Algorithm detection -> Label annotation -> IDE integration -> Developer comprehension
- Critical path: Algorithm detection → Label annotation → Developer reads label → Schema activation (if recognized) → Targeted search/verification → Improved comprehension
- Design tradeoffs:
  - Minimal labels (name + link only) vs. enriched labels (complexity, variants, pseudocode) — paper used minimal; participants requested enriched
  - Automatic vs. manual annotation — paper used manual; real tools need automation with precision/recall tradeoffs
  - Universal vs. task-specific labels — not all algorithms equally benefit all tasks
- Failure signatures:
  - Labels for algorithms developers already know: low marginal benefit (ceiling effect for high-experience)
  - Labels for algorithms developers don't recognize: no schema to activate, label becomes noise
  - Inaccurate labels: misleading developers, potentially worse than no labels
  - Over-labeling: comment fatigue, developers ignore all annotations
- First 3 experiments:
  1. Replicate with unfamiliar algorithms: Test whether benefit persists when participants lack prior algorithm knowledge (stress-test Mechanism 1's break condition).
  2. Compare label types: Minimal (name only) vs. enriched (name + complexity + pseudocode) to quantify value of additional context.
  3. Measure actual time-to-comprehension without fixed time limits: The p=0.991 time result may reflect ceiling effects from time caps; unbounded time could reveal speed benefits.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does annotating source code with algorithm labels significantly reduce the time required for comprehension in experimental settings without strict time limits?
- Basis in paper: [explicit] Section 7 states that "More research is needed to fully assess the influence of algorithm labels on the time required to comprehend programs," noting that the current study found no time difference (p=0.991) likely because participants utilized the entire allotted time to maximize points.
- Why unresolved: The study design used fixed time windows for exercises, which introduced a potential ceiling effect that masked any potential speed improvements from the labels.
- What evidence would resolve it: A controlled experiment where participants perform comprehension tasks with open-ended time constraints or where speed is incentivized, measuring the actual duration to completion.

### Open Question 2
- Question: Do enriched algorithm labels (e.g., including implementation variants, time complexity, or step-by-step comments) improve program comprehension more than the minimal labels (name + link) tested in this study?
- Basis in paper: [explicit] Section 4.1 reports participant suggestions to include details like "time and memory complexity" and specific "implementation type," concluding that "future research could focus on the influence of these improvements on program comprehension."
- Why unresolved: The current study intentionally used minimal labels to isolate the effect of algorithm identification, leaving the added value of detailed metadata unknown.
- What evidence would resolve it: A comparative study evaluating comprehension scores between groups provided with minimal labels versus groups provided with enriched, detailed annotations.

### Open Question 3
- Question: How effective is automatic algorithm recognition in supporting specific software engineering tasks beyond general comprehension, such as automated error detection or code optimization?
- Basis in paper: [explicit] Section 4.2 and Section 7 identify "error detection," "code optimization," and "library replacement" as potential use cases suggested by participants, stating these provide "valuable avenues for future research."
- Why unresolved: The current experiment focused solely on mental simulation and maintenance exercises, whereas the suggested use cases represent different workflows (e.g., verification against a standard, performance tuning).
- What evidence would resolve it: User studies or controlled experiments that task developers with specific optimization or debugging scenarios, measuring the efficacy of algorithm recognition tools in those specific contexts.

### Open Question 4
- Question: What is the quantitative prevalence of self-implemented algorithms in industrial codebases across different domains?
- Basis in paper: [explicit] Section 4.3 notes that while self-implementation was found to be common, "further research is needed to better quantify which types of algorithms and domains are most relevant in practice."
- Why unresolved: The current findings rely on qualitative self-reports from a limited number of participants (15 reports) regarding their specific codebases.
- What evidence would resolve it: A large-scale mining study of open-source and proprietary repositories to quantify the frequency of self-implemented algorithms (e.g., sorting, searching) versus standard library usage.

## Limitations
- The study's time analysis may be underpowered due to fixed time limits potentially masking real speed benefits
- Generalizability is limited by focus on specific algorithms in one codebase with German participants
- The 23% improvement may not translate to practical development workflows where developers encounter diverse algorithms

## Confidence
- **High confidence**: Labels improve comprehension scores (p=0.040) and qualitative feedback supports perceived helpfulness
- **Medium confidence**: Medium-experience developers benefit most (U-shaped curve supported but not extensively validated)
- **Low confidence**: Time improvement claims (p=0.991 suggests no effect, but this may reflect ceiling rather than true equivalence)

## Next Checks
1. **Validate the mechanism**: Replicate with unfamiliar algorithms to test whether labels help when participants lack prior knowledge—this directly tests whether top-down comprehension (Mechanism 1) requires existing schema activation.
2. **Test time effects properly**: Conduct unbounded time trials to determine if labels provide real speed benefits beyond what fixed-time experiments can reveal—the current p=0.991 result may be an artifact of time caps.
3. **Generalize across domains**: Test algorithm labels in different codebases (open-source, industrial) and with diverse participant populations to assess whether the 23% improvement holds across contexts and whether cultural/linguistic factors matter.