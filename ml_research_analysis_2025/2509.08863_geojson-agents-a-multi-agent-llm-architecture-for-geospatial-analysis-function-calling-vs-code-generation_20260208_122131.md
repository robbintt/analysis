---
ver: rpa2
title: GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function
  Calling vs Code Generation
arxiv_id: '2509.08863'
source_url: https://arxiv.org/abs/2509.08863
tags:
- geojson
- task
- tasks
- execution
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the limitations of traditional GIS tools
  and general-purpose LLMs in geospatial analysis by proposing GeoJSON Agents, a multi-agent
  LLM architecture specifically designed for geospatial tasks. The framework transforms
  natural language instructions into structured GeoJSON operations using two LLM enhancement
  techniques: Function Calling and Code Generation.'
---

# GeoJSON Agents:A Multi-Agent LLM Architecture for Geospatial Analysis-Function Calling vs Code Generation

## Quick Facts
- **arXiv ID:** 2509.08863
- **Source URL:** https://arxiv.org/abs/2509.08863
- **Reference count:** 40
- **Primary result:** Proposed GeoJSON Agents framework achieves 97.14% accuracy with code generation vs 85.71% with function calling on 70 geospatial tasks

## Executive Summary
This study addresses the limitations of traditional GIS tools and general-purpose LLMs in geospatial analysis by proposing GeoJSON Agents, a multi-agent LLM architecture specifically designed for geospatial tasks. The framework transforms natural language instructions into structured GeoJSON operations using two LLM enhancement techniques: Function Calling and Code Generation. It integrates a Planner agent for task decomposition and a Worker agent for execution via predefined function APIs or dynamically generated Python code. A hierarchical benchmark of 70 tasks was constructed to evaluate both approaches, with experiments conducted using OpenAI's GPT-4o. Results show that the Code Generation-based agent achieved 97.14% accuracy, while the Function Calling-based agent attained 85.71%, both significantly outperforming the best general-purpose model (48.57%).

## Method Summary
The GeoJSON Agents framework employs a Planner-Worker architecture where a Planner agent decomposes natural language instructions into subtasks, and a Worker agent executes them using either Function Calling (selecting from 40 predefined APIs) or Code Generation (dynamically generating Python code with GeoPandas/Shapely). The system was evaluated on a hierarchical benchmark of 70 geospatial tasks (Basic: 40, Intermediate: 20, Advanced: 10) using GPT-4o. For Code Generation, the Worker employs a closed-loop self-repair mechanism, generating code and retrying up to 5 times based on error feedback. For Function Calling, the Worker selects from a closed set of 40 APIs defined in YAML/JSON. The framework was implemented with a web interface (Streamlit/LangGraph) for uploading GeoJSON data and natural language prompts.

## Key Results
- Code Generation-based agent achieved 97.14% accuracy across 70 benchmark tasks
- Function Calling-based agent attained 85.71% accuracy, both significantly outperforming best general-purpose model (48.57%)
- Code Generation offers superior flexibility for complex, open-ended tasks but requires more debugging rounds (1.35 avg vs 1.27 for Function Calling)
- Function Calling provides enhanced execution stability for structured operations but fails on advanced tasks (60% accuracy)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Task Decoupling
- Claim: Separating task planning from execution mitigates the "hallucination" and context-drift issues common in single-agent systems handling long reasoning chains.
- Mechanism: A Planner agent decomposes natural language instructions into subtasks, while a Worker agent executes them in an isolated environment with distinct context, closing the loop via feedback.
- Core assumption: The underlying LLM has sufficient reasoning capability to decompose tasks and distinct context separation prevents error propagation.
- Evidence anchors:
  - [section 3.3.1] "The Planner–Worker architecture... separating planning and execution mitigates the 'hallucination' issue common in single-agent systems."
  - [corpus] *Designing Domain-Specific Agents via Hierarchical Task Abstraction* suggests specialized domains require rigorously structured workflows, aligning with this separation.

### Mechanism 2: Closed-Loop Code Self-Repair
- Claim: Dynamic code generation with iterative debugging achieves higher accuracy on complex, open-ended tasks than static function mapping.
- Mechanism: The Worker generates Python code and executes it in a sandbox. Upon failure (syntax or runtime error), it analyzes the traceback, re-prompts itself, and regenerates code up to a limit (e.g., 5 retries).
- Core assumption: The LLM can diagnose error messages correctly and the execution environment (e.g., GeoPandas) supports the required operations.
- Evidence anchors:
  - [abstract] The Code Generation-based agent achieved 97.14% accuracy vs. Function Calling's 85.71%.
  - [section 3.3.4] Describes the "execute–validate–repair" cycle where the Worker autonomously revises faulty code.

### Mechanism 3: Constrained API Tooling (Function Calling)
- Claim: Restricting the Worker to a predefined library of APIs ensures high stability and lower token consumption for structured, low-to-medium complexity tasks.
- Mechanism: The Worker selects from a closed set of 40 APIs defined in YAML/JSON. This constrains the solution space, reducing syntax errors and execution rounds.
- Core assumption: The user's task maps cleanly to existing API capabilities and the API definitions are unambiguous to the LLM.
- Evidence anchors:
  - [section 4.2.1] Function calling shows high stability (1.19 rounds) on basic tasks but drops to 60% accuracy on advanced tasks.

## Foundational Learning

### Concept: GeoJSON vs. Binary GIS Formats
- Why needed here: The architecture relies on GeoJSON because, unlike binary Shapefiles, it is lightweight, human-readable, and natively compatible with LLM tokenization and JSON-based function calling.
- Quick check question: Can you explain why a text-based format like GeoJSON is superior to a binary format like Shapefile for an LLM's context window?

### Concept: Planner-Worker Architecture
- Why needed here: This study explicitly rejects single-agent "Chain-of-Thought" in favor of a multi-agent split to handle task decomposition (Planner) and tool execution (Worker) separately.
- Quick check question: How does separating the "Planner" from the "Worker" reduce the risk of hallucination in a long GIS workflow?

### Concept: GeoSpatial Libraries (GeoPandas/Shapely)
- Why needed here: The Code Generation agent generates Python code dynamically. Understanding these libraries is required to verify if the generated logic is sound.
- Quick check question: If the Worker generates code to perform a "spatial join" between points and polygons, which library methods would you expect to see in the script?

## Architecture Onboarding

### Component map:
- User Interface -> Planner (LLM) -> Worker (LLM + Sandbox) -> Memory/Context

### Critical path:
1. User uploads GeoJSON
2. Planner uses `reading_gis_metadata` to understand data schema
3. Planner decomposes task -> "Create buffer", "Calculate stats"
4. Worker executes step (generating code or calling API)
5. Worker returns result/error
6. Planner evaluates -> loops or sends "END"

### Design tradeoffs:
- **Code Gen vs. Function Calling:** Code Gen offers flexibility (97.14% accuracy) but requires more debugging rounds (1.35 avg). Function Calling offers stability (1.27 rounds) but fails on complex, open-ended tasks (60% accuracy).
- **Token Efficiency:** Code Gen is generally cheaper per task because it avoids loading a massive API definition into the context window, whereas Function Calling must embed the 40-function YAML documentation.

### Failure signatures:
- **API Mismatch (Function Calling):** Agent invokes `CalculateMainDirectionOfPolygon` when asked for `population_density` due to semantic confusion.
- **Logical Flaw (Code Gen):** Code executes successfully (no error) but output values are `Null` due to a silent logic error in the spatial join step.
- **Hallucination:** The Planner decomposes a task the Worker cannot physically execute (e.g., "interactively edit webpage" in a non-interactive sandbox).

### First 3 experiments:
1. **Basic Buffer Task (Function vs Code):** Run a simple "buffer 100m" task to verify that Function Calling succeeds in 1 round while Code Gen takes 1-2 rounds (generation + validation).
2. **Ablation Study (Single vs Multi-Agent):** Disable the Planner and feed the user prompt directly to the Worker to observe the drop in accuracy (e.g., from 85% to 45%).
3. **Advanced Open-Ended Task:** Request a "population density map" to verify if Function Calling fails due to lack of specific API, while Code Gen succeeds by synthesizing Python code on the fly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can an adaptive hybrid architecture outperform single-method agents by dynamically switching between function calling and code generation based on task complexity?
- Basis in paper: [explicit] The authors explicitly propose developing an "adaptive hybrid model that dynamically selects between function calling and code generation" to leverage the stability of the former and flexibility of the latter.
- Why unresolved: The current study evaluates the two methods in isolation, identifying trade-offs (stability vs. flexibility) but not testing an integrated approach.
- What evidence would resolve it: A comparative experiment showing a hybrid agent achieving higher accuracy or lower execution rounds than isolated agents across the 70-task benchmark.

### Open Question 2
- Question: How do open-source LLMs (e.g., Llama, CodeLlama) compare to commercial models like GPT-4o when fine-tuned for geospatial tasks within the GeoJSON Agents framework?
- Basis in paper: [explicit] The authors identify the "Integration of Open-Source LLMs" as a future direction to address the high operational costs and limited fine-tuning access of the closed-source models used in this study.
- Why unresolved: The experiments exclusively utilized commercial models (GPT-4o, GLM-4), leaving the performance ceiling and cost-efficiency of open-source alternatives within this specific architecture untested.
- What evidence would resolve it: Performance benchmarks of the GeoJSON Agents framework running on fine-tuned open-source models, measuring accuracy against the GPT-4o baseline.

### Open Question 3
- Question: To what extent can domain-specific knowledge graphs or advanced prompt engineering reduce hallucinations in multi-agent geospatial systems during complex tasks?
- Basis in paper: [explicit] The authors identify hallucinations (e.g., invoking incorrect functions like `CalculateMainDirectionOfPolygon` instead of density functions) as a persistent challenge and suggest "incorporating domain-specific knowledge graphs" as a mitigation strategy.
- Why unresolved: While the multi-agent architecture reduces hallucinations compared to single-agent models, the study notes they still occur in advanced tasks, and proposed solutions remain untested.
- What evidence would resolve it: An ablation study showing a significant reduction in semantic error rates (e.g., wrong function selection) when knowledge graphs are integrated into the Planner's reasoning process.

## Limitations

- **API Implementation Gap**: The 40 predefined functions are only described by name and docstring in Appendix 2, not implemented. Reproducers must assume standard GeoPandas behavior, risking mismatches if the paper used custom wrappers.
- **Sandbox Execution Model**: The `execute_code` tool for Code Generation lacks detail on how file I/O, module imports, and security boundaries are enforced. This could lead to different failure modes in reproduction.
- **Task Decomposition Ambiguity**: The Planner's logic is described but not formally specified. Small prompt wording changes could lead to different subtask splits, affecting downstream accuracy.

## Confidence

- **High Confidence**: The empirical accuracy comparison (Code Gen 97.14% vs. Function Calling 85.71% vs. baseline 48.57%) is directly supported by the described experimental protocol and benchmark tasks.
- **Medium Confidence**: The claim that Code Generation is "more flexible for open-ended tasks" while Function Calling is "more stable for structured tasks" is supported by the task breakdown but relies on subjective classification of task complexity.
- **Low Confidence**: The assertion that Function Calling consumes significantly more tokens than Code Generation contradicts earlier text stating both modes are similar; this inconsistency reduces confidence in the token-efficiency analysis.

## Next Checks

1. **Function Calling Edge Case**: Replicate an "advanced" task (e.g., "population density map") to confirm Function Calling fails due to lack of specific API, while Code Generation succeeds by synthesizing Python code on the fly.
2. **Sandbox Security Test**: Verify that the `execute_code` tool properly sandboxes file access and module imports to prevent unintended side effects or privilege escalation.
3. **Single vs Multi-Agent Ablation**: Disable the Planner and feed user prompts directly to the Worker to quantify the drop in accuracy and observe error propagation patterns.