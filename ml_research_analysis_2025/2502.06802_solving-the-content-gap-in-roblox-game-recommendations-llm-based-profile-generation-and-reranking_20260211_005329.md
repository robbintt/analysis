---
ver: rpa2
title: 'Solving the Content Gap in Roblox Game Recommendations: LLM-Based Profile
  Generation and Reranking'
arxiv_id: '2502.06802'
source_url: https://arxiv.org/abs/2502.06802
tags:
- game
- user
- text
- games
- roblox
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a two-stage approach to address the challenge
  of generating high-quality, structured game text features for Roblox's dynamic,
  user-generated content. The first stage involves extracting in-game text and using
  large language models (LLMs) to generate structured game profiles that capture attributes
  like genre, objectives, and gameplay mechanics.
---

# Solving the Content Gap in Roblox Game Recommendations: LLM-Based Profile Generation and Reranking

## Quick Facts
- arXiv ID: 2502.06802
- Source URL: https://arxiv.org/abs/2502.06802
- Reference count: 28
- Improves NDCG Engagement by 4.90% on average compared to baseline model

## Executive Summary
This paper addresses the challenge of generating high-quality, structured game text features for Roblox's dynamic, user-generated content. The authors propose a two-stage approach: first extracting in-game text and using large language models (LLMs) to generate structured game profiles capturing attributes like genre, objectives, and gameplay mechanics; then introducing an LLM-based re-ranking mechanism that leverages these profiles to validate their quality and enhance recommendation relevance by incorporating user-specific preferences. Experimental results demonstrate that this approach significantly improves recommendation quality, with the proposed method outperforming baseline models that rely on developer-provided titles and descriptions alone.

## Method Summary
The method involves extracting in-game text from Roblox games and using GPT-4o to generate structured JSON game profiles containing fields like genre, objectives, mechanics, and target audience. For each user, the system retrieves their 7-day play history, maps games to their profiles, and uses another LLM prompt to summarize user preferences. A third prompt generates a personalized reranking strategy using Chain-of-Thought reasoning. The top 30 candidates from the initial ranking are then re-ordered based on alignment with this strategy and candidate game profiles. The pipeline processes 2,700 users across 18,941 games, comparing NDCG Engagement against a baseline DNN model using only ID and behavior features.

## Key Results
- LLM-based game profile generation from in-game text improves NDCG@10 by 4.90% compared to baseline
- Personalized reranking with LLM-generated strategies outperforms non-personalized reranking
- Title and description-based approaches underperform baseline, demonstrating superiority of in-game text extraction
- Performance degrades for 30-70 percentile users with diverse play histories

## Why This Works (Mechanism)

### Mechanism 1
Extracting raw in-game text and structuring it via LLMs produces higher-quality game profiles than using developer-provided titles/descriptions alone. In-game text (instructions, button prompts, background narrative) is aggregated per game, sampled if token-limited, and fed to an LLM with a structured prompt that forces JSON output covering genre, objectives, mechanics, target audience, features, language, and scale. The prompt explicitly instructs the LLM to ignore noisy text (promotional calls-to-action, social media prompts). Core assumption: Developers naturally embed gameplay-relevant information in in-game text to reduce player drop-off, and LLMs can separate signal from noise without manual filtering. Evidence: Title-based and Title+Desc models underperform baseline, while the proposed model improves NDCG@10 by 4.90% on average.

### Mechanism 2
Deriving user profiles from recent play history (via game profiles, not raw IDs) enables preference extraction that ID-based collaborative filtering cannot capture. For each user, retrieve the last 7 days of played games, map each game to its generated profile Pg, and prompt an LLM to summarize preferences across dimensions (genres, themes, mechanics, play styles). The prompt excludes game IDs since they carry no semantic content. Core assumption: Play history over 7 days is representative of current preferences, and LLMs can generalize from a sequence of structured game profiles to a coherent user preference model. Evidence: The proposed model outperforms versions without personalization, which perform comparably to baseline.

### Mechanism 3
A Chain-of-Thought (CoT)-inspired personalized reranking strategy, generated by the LLM based on the user profile, improves top-30 recommendation relevance compared to generic relevance scoring. The LLM receives the user profile Pu and generates a textual reranking strategy Su that prioritizes certain attributes (e.g., "adventure and obby games with extensive gameplay"). The top 30 candidates from the initial ranking list are then re-ordered by the LLM according to alignment with Su and Pu, using each candidate's game profile. Core assumption: LLMs can follow abstract ranking strategies and compare game profiles consistently; the top 30 positions capture most user attention and engagement. Evidence: Case study shows adventure and obby games moved to top positions following the LLM-generated strategy.

## Foundational Learning

- **Collaborative Filtering vs. Content-Based Recommendation**
  - Why needed: The baseline model relies on ID-based collaborative filtering (sparse ID features, behavior statistics), which fails to capture game semantics. Understanding this contrast clarifies why adding structured content features improves performance.
  - Quick check: Can you explain why a user who plays "adventure" games might receive irrelevant "simulator" recommendations under pure collaborative filtering?

- **LLM Prompt Engineering for Structured Output**
  - Why needed: The pipeline depends critically on prompts that force JSON-formatted game profiles and ranking strategies. Poorly designed prompts would yield unparseable or noisy outputs.
  - Quick check: What prompt constraints would you add to ensure the LLM outputs valid JSON with all required keys?

- **NDCG (Normalized Discounted Cumulative Gain) and Engagement Variants**
  - Why needed: The evaluation uses NDCG Engagement, where relevance scores are derived from playtime rather than binary relevance. Interpreting results requires understanding how this metric penalizes errors at higher ranks.
  - Quick check: If a user plays a recommended game for 5 seconds vs. 5 hours, how does NDCG Engagement reflect this difference compared to standard NDCG?

## Architecture Onboarding

- **Component map**: In-Game Text Extractor -> Game Profile Generator (LLM) -> User Profile Generator (LLM) -> Reranking Strategy Generator (LLM) -> Top-30 Reranker (LLM)

- **Critical path**:
  1. Extract in-game text for each game (offline/batch)
  2. Generate and store game profiles (must complete before serving)
  3. At inference, retrieve user's 7-day play history, map to profiles
  4. Generate user profile and reranking strategy (can be cached per user)
  5. Re-rank top 30 candidates and serve

- **Design tradeoffs**:
  - Top-30 focus vs. full-list reranking: Concentrates compute where user engagement is highest, but may miss improvements at lower ranks
  - Random sampling for long text: Preserves diversity but risks dropping important signals; alternative would be extractive summarization
  - LLM choice: GPT-4o performs best in ablation; smaller models (Llama-8B) fail to follow instructions. Cost/latency vs. quality
  - No fine-tuning on platform data: General LLM knowledge is used; limits ability to incorporate Roblox-specific popularity/stats

- **Failure signatures**:
  - Noisy/empty in-game text: LLM generates vague or default profiles (e.g., "This game appears to be about X but lacks detail")
  - Highly diverse play history (mid-engagement users): User profile and strategy become generic; reranking may not improve (observed in 30-70 percentile drop for NDCG@20/@30)
  - LLM instruction-following failure: Models like Llama-8B produce unusable outputs; requires fallback to baseline or different LLM

- **First 3 experiments**:
  1. Profile Quality Audit: Manually inspect 50-100 generated game profiles against actual gameplay (or human annotations) to measure accuracy of genre, mechanics, and objectives
  2. Ablation on Profile Fields: Remove individual fields (e.g., game_language, game_scale) from the profile and measure impact on NDCG Engagement to identify which attributes drive gains
  3. Percentile-Specific Analysis: Isolate the 30-70 percentile users and test alternative strategies (e.g., weighting recent games more heavily, or clustering play history into preference modes) to address the observed performance drop

## Open Questions the Paper Calls Out

- Does fine-tuning the LLM on Roblox-specific interaction data improve recommendation accuracy by capturing platform-specific popularity trends better than general-purpose models?
- Can incorporating multimodal data sources, such as game visuals and audio cues, into the profile generation pipeline enhance recommendation relevance beyond text-only analysis?
- Would refining game taxonomies to be more granular address the performance drop observed in mid-engagement users (30-70 percentile) who possess diverse play histories?

## Limitations
- Depends on proprietary in-game text data from Roblox, limiting generalizability to other platforms
- Random 50% token sampling for long texts is not empirically validated and may omit critical information
- 7-day play history window is arbitrary and may not capture stable preferences for all user segments
- Evaluation uses only NDCG Engagement, lacking diversity, novelty, or fairness measurements

## Confidence
- **High confidence**: In-game text extraction combined with LLM-based game profile generation improves recommendation quality compared to title/description-only approaches (directly supported by ablation study)
- **Medium confidence**: Personalized LLM-generated reranking strategies improve top-30 recommendations compared to non-personalized reranking (statistically significant but mechanism lacks independent validation)
- **Low confidence**: Generalizability of 7-day play history window and specific LLM prompt formulations to other platforms or user populations (not systematically tested)

## Next Checks
1. Profile Quality Audit: Manually inspect 50-100 generated game profiles against actual gameplay to measure accuracy of genre, mechanics, and objectives
2. Percentile-Specific Analysis: Isolate the 30-70 percentile users and test alternative strategies to address the observed performance drop
3. Ablation on Profile Fields: Remove individual fields from the profile and measure impact on NDCG Engagement to identify which attributes drive gains