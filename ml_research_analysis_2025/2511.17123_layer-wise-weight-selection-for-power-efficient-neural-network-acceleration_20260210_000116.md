---
ver: rpa2
title: Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration
arxiv_id: '2511.17123'
source_url: https://arxiv.org/abs/2511.17123
tags:
- energy
- accuracy
- weight
- layers
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a layer-wise weight selection framework\
  \ for power-efficient neural network acceleration on systolic-array hardware. The\
  \ key innovation is combining a per-layer MAC energy model\u2014based on MSB-Hamming-distance\
  \ grouping of partial-sum transitions and tile-level systolic mapping\u2014with\
  \ an energy\u2013accuracy co-optimized weight restriction algorithm."
---

# Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration
## Quick Facts
- arXiv ID: 2511.17123
- Source URL: https://arxiv.org/abs/2511.17123
- Reference count: 19
- Key outcome: Layer-wise weight selection framework achieving up to 58.6% energy reduction with only 2–3% accuracy loss on systolic-array hardware

## Executive Summary
This paper introduces a layer-wise weight selection framework for power-efficient neural network acceleration on systolic-array hardware. The key innovation is combining a per-layer MAC energy model—based on MSB-Hamming-distance grouping of partial-sum transitions and tile-level systolic mapping—with an energy–accuracy co-optimized weight restriction algorithm. The method applies aggressive compression selectively to high-energy layers while preserving model accuracy. Experiments on LeNet-5 and ResNet architectures show up to 58.6% energy reduction with only 2–3% accuracy loss, outperforming prior power-aware pruning baselines. Ablation studies confirm that layer-wise scheduling and the co-optimized weight selection are critical for achieving superior energy-accuracy trade-offs.

## Method Summary
The proposed framework introduces a per-layer MAC energy model grounded in MSB-Hamming-distance grouping of partial-sum transitions, which quantifies switching activity at the tile level of systolic arrays. This energy model is integrated with a weight restriction algorithm that selectively compresses weights in high-energy layers, co-optimizing for both energy efficiency and model accuracy. The approach is validated on convolutional neural networks (LeNet-5 and ResNet variants), demonstrating significant energy savings while maintaining acceptable accuracy loss. Layer-wise scheduling and co-optimized weight selection are shown to be critical for the observed performance gains.

## Key Results
- Up to 58.6% energy reduction achieved with only 2–3% accuracy loss on systolic-array hardware
- Outperforms prior power-aware pruning baselines across LeNet-5 and ResNet architectures
- Layer-wise scheduling and co-optimized weight selection are critical for superior energy-accuracy trade-offs

## Why This Works (Mechanism)
The method works by exploiting the observation that energy consumption in systolic arrays is dominated by MAC operations, especially in layers with high switching activity. By modeling energy at the per-layer level using MSB-Hamming-distance grouping, the framework accurately captures the relationship between partial-sum transitions and power usage. The co-optimized weight restriction algorithm then selectively compresses weights in layers that contribute most to energy consumption, achieving aggressive savings without sacrificing accuracy. This targeted approach allows for more efficient resource utilization compared to uniform pruning strategies.

## Foundational Learning
- **Systolic array architecture**: Why needed: Understanding data flow and MAC operation distribution for accurate energy modeling. Quick check: Can you explain how data moves through a systolic array and where switching activity is highest?
- **MSB-Hamming-distance grouping**: Why needed: Enables fine-grained modeling of partial-sum transitions for energy estimation. Quick check: How does Hamming distance relate to switching activity in binary representations?
- **Layer-wise energy profiling**: Why needed: Identifies which layers are most energy-intensive and thus prime candidates for compression. Quick check: What metrics are used to determine per-layer energy contribution?
- **Co-optimization of energy and accuracy**: Why needed: Balances aggressive compression with model performance to avoid accuracy collapse. Quick check: How does the algorithm ensure that accuracy loss remains within acceptable bounds?

## Architecture Onboarding
- **Component map**: Energy model (MSB-Hamming-distance grouping) -> Weight restriction algorithm -> Layer-wise scheduling -> Systolic array mapping
- **Critical path**: Energy profiling (layer-wise) → Weight selection (co-optimized) → Pruning/compression → Systolic mapping → Hardware execution
- **Design tradeoffs**: Aggressive per-layer compression vs. global accuracy preservation; model generalizability vs. hardware-specific optimization
- **Failure signatures**: Excessive accuracy loss if high-energy layers are over-compressed; suboptimal energy savings if layer selection is inaccurate; hardware incompatibility if systolic mapping assumptions are violated
- **3 first experiments**: (1) Validate per-layer energy model accuracy on a reference systolic array simulator; (2) Apply weight restriction algorithm to a single high-energy layer and measure impact on energy and accuracy; (3) Perform ablation study removing layer-wise scheduling to quantify its contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Energy model is specific to 8-bit quantized systolic architectures; generalizability to other quantization schemes or precisions is not explored
- Methodology not demonstrated on non-convolutional layers (e.g., depthwise, pointwise, or attention layers), limiting architectural generality
- Contribution of energy model versus pruning strategy is not fully decoupled in ablation studies
- No analysis of robustness to hardware non-idealities (e.g., process variation, supply noise) or dynamic runtime adaptation

## Confidence
- Energy model accuracy and MAC transition counting: **High**
- Layer-wise weight selection effectiveness: **High**
- Energy-accuracy trade-off improvements vs. baselines: **High**
- Generalizability to other architectures/quantizations: **Medium**
- Hardware robustness and scalability: **Low**

## Next Checks
1. Validate the energy model on a different systolic array hardware platform or cycle-accurate simulator with alternative bitwidths (e.g., 4-bit, 16-bit).
2. Apply the framework to transformer-based models or models with depthwise convolutions to assess architectural generality.
3. Perform sensitivity analysis on accuracy under hardware noise or process variation to quantify robustness.