---
ver: rpa2
title: 'The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects
  on User Trust, Exploration, and Workflows'
arxiv_id: '2505.21512'
source_url: https://arxiv.org/abs/2505.21512
tags:
- linkq
- query
- data
- participants
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LinkQ is a KG exploration system that converts natural language
  questions into structured queries with an LLM, designed to help users assess the
  accuracy of both KG queries and LLM responses. The system was developed collaboratively
  with KG practitioners and features five visual mechanisms: an LLM-KG state diagram,
  query editor with explanations, entity-relation ID table, query structure graph,
  and interactive graph visualization of query results.'
---

# The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows

## Quick Facts
- arXiv ID: 2505.21512
- Source URL: https://arxiv.org/abs/2505.21512
- Reference count: 40
- Primary result: LLM-assisted KG query systems can induce overtrust through visual transparency, requiring expertise-adaptive interfaces.

## Executive Summary
This paper presents LinkQ, a KG exploration system that converts natural language questions into structured SPARQL queries using an LLM, with visual mechanisms to help users assess accuracy. Developed collaboratively with KG practitioners, LinkQ features five visual components including an LLM-KG state diagram, query editor with explanations, and interactive graph visualization. A qualitative study with 14 practitioners revealed that users—even KG experts—tended to overtrust LinkQ's outputs due to its "helpful" visualizations, even when the LLM was incorrect. The study also found that users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs, challenging the assumption that these systems are one-size-fits-all. The research highlights the risks of false trust in LLM-assisted data analysis tools and the need for further investigation into visualization's role as a mitigation technique.

## Method Summary
The study employed a qualitative approach with 14 KG practitioners using LinkQ to answer questions from the Mintaka benchmark (120 questions across five types). The system uses a chained prompting protocol where the LLM works with a System component to validate entity/relation IDs against the KG before query generation. Users completed open-ended tasks while thinking aloud, and sessions were recorded and transcribed for analysis. The researchers compared LinkQ's accuracy against GPT-4 baseline and conducted thematic analysis of user workflows and trust dynamics. The study focused on two KGs: Wikidata and BRON (cybersecurity domain).

## Key Results
- Users—even KG experts—tended to overtrust LinkQ's outputs due to its "helpful" visualizations, even when the LLM was incorrect.
- Users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs, challenging the assumption that these systems are one-size-fits-all.
- LinkQ outperformed baseline GPT-4 across all Mintaka question types, with accuracy ranging from 75.0% on multi-hop to 91.7% on comparative questions.

## Why This Works (Mechanism)

### Mechanism 1: Chained Prompting Protocol
- Claim: Decomposing NL-to-query translation into discrete, observable stages with ground-truth validation improves accuracy over end-to-end LLM generation.
- Mechanism: Multi-stage protocol (Question Refinement → KG Exploration → Query Generation → Results Summarization) where the System mediates LLM requests to the KG API, fetching actual entity/relation IDs before query construction.
- Core assumption: Validating IDs against the KG at each stage prevents hallucination propagation into the final query.
- Evidence anchors:
  - [section 4.2] "The LLM and System work together to fuzzy search for entities, find relevant relations, and traverse the KG to identify all the correct IDs."
  - [section 4.4] LinkQ outperformed baseline GPT-4 across all Mintaka question types (e.g., Comparative: 91.7% vs 20.8%; Multi-hop: 75.0% vs 16.7%).
  - [corpus] "Augmented Knowledge Graph Querying leveraging LLMs" similarly leverages LLMs for KG query tasks, suggesting broader applicability of this approach.
- Break condition: Performance degrades on complex intersections (54.2% accuracy) and multi-hop queries beyond 2–3 hops; protocol assumes LLM can correctly identify when to stop exploring.

### Mechanism 2: Visualization-Mediated Trust Calibration
- Claim: Visual transparency mechanisms intended to support verification can paradoxically induce overtrust when outputs appear structurally plausible.
- Mechanism: Visual components (state diagram, query structure graph) align with users' mental models → users infer correctness from structural coherence rather than semantic validation.
- Core assumption: Users conflate "looks reasonable" with "is correct," especially when visual explanations are well-designed.
- Evidence anchors:
  - [abstract] "Users—even KG experts—tended to overtrust LinkQ's outputs due to its 'helpful' visualizations, even when the LLM was incorrect."
  - [section 6.2] "When the query output was incorrect... some users rationalized LinkQ's answer by externalizing circumstances that could justify its (false) output."
  - [corpus] Corpus evidence on this mechanism is limited; related papers focus on query accuracy rather than trust dynamics. "User-Centered AI for Data Exploration" notes expertise variation but doesn't address overtrust directly.
- Break condition: Overtrust emerges when visualizations are polished but lack explicit uncertainty signaling; users with high LLM skepticism and/or domain expertise are somewhat more resistant.

### Mechanism 3: Expertise-Adaptive Workflow Routing
- Claim: Users with different KG/LLM expertise levels develop distinct validation strategies, requiring interface affordances tailored to each profile.
- Mechanism: KG experts inspect query structure graphs and entity-relation tables; non-experts rely on chat panel and external source links, avoiding query inspection entirely.
- Core assumption: One-size-fits-all interfaces cannot adequately support users across the expertise spectrum.
- Evidence anchors:
  - [abstract] "Users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs, challenging the assumption that these systems are one-size-fits-all."
  - [section 5.4] "KG experts cautiously inspected query structures... KG non-experts never inspected queries before executing them."
  - [corpus] "User-Centered AI for Data Exploration" advocates adapting to users' varying expertise levels and analytical needs.
- Break condition: System fails non-experts if query visualizations are mandatory or prominent; fails experts if semantic context (labels, descriptions) is insufficient for deep validation.

## Foundational Learning

- **Knowledge Graph Schema & Query Languages (SPARQL/Cypher)**
  - Why needed: To interpret the Query Editor, Entity-Relation Table, and Query Structure Graph—and to debug when LLM-generated queries fail.
  - Quick check question: Can you read a basic SPARQL SELECT query with a triple pattern and explain which entities/relations it matches?

- **LLM Hallucination Patterns**
  - Why needed: To understand why LinkQ uses chained prompting, hallucination warnings, and ground-truth ID validation.
  - Quick check question: What are two common failure modes when an LLM generates structured queries without external validation?

- **Human-AI Trust Dynamics in XAI**
  - Why needed: To interpret the overtrust findings and understand why transparency can backfire.
  - Quick check question: Why might plausible explanations increase user trust in incorrect outputs?

## Architecture Onboarding

- **Component map:**
  Chat Panel -> LLM-KG State Diagram -> Query Editor -> Entity-Relation Table -> Query Structure Graph -> Results Graph/Table

- **Critical path:**
  User question → Question Refinement (LLM clarifies ambiguities) → KG Exploration (LLM requests IDs; System calls KG API) → Query Generation (few-shot SPARQL training injected) → Execution → Results Summarization (LLM explains output)

- **Design tradeoffs:**
  - Transparency vs. Overtrust: More visual detail aids comprehension but can induce false confidence
  - Automation vs. Control: Fully automated query generation vs. human-in-the-loop validation at each stage
  - Generalizability vs. Optimization: Generic LLM works across KGs (no fine-tuning) but lower peak accuracy than domain-specialized models

- **Failure signatures:**
  - Empty results: Could be correct (no matching data) or LLM error (wrong IDs, schema misunderstanding)—system doesn't currently distinguish
  - Plausible-but-wrong answers: Query structure looks reasonable but retrieves incorrect entities; users rationalize rather than reject
  - Performance degradation: Accuracy drops sharply on intersection queries (54.2%) and multi-hop queries beyond 2–3 hops

- **First 3 experiments:**
  1. **Integrate with your own KG**: Update the system prompt with your schema, configure KG API calls, and run 10–15 questions with known ground-truth answers to establish baseline accuracy.
  2. **Observe expertise-based workflows**: Have 2–3 KG experts and 2–3 non-experts complete identical tasks; log which visual components each group uses and where they fail.
  3. **Test trust calibration interventions**: Add explicit uncertainty indicators (e.g., "this entity was fuzzy-matched; verify") or alternative query suggestions; measure whether overtrust decreases without harming legitimate trust.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does using an LLM assistant reduce the coverage of data exploration compared to traditional manual methods?
- Basis in paper: [explicit] The authors observed users adopting a "deliberate" bottom-up approach rather than broad exploration and stated: "Future work can consider studying this concept directly, e.g., comparing data coverage with and without an LLM."
- Why unresolved: The current qualitative study identified a potential shift in user behavior (belief elicitation) that might limit exploration breadth, but did not quantitatively measure data coverage against a control group.
- What evidence would resolve it: A controlled experiment measuring the volume and diversity of data attributes accessed by users performing the same open-ended tasks with and without LLM assistance.

### Open Question 2
- Question: Can visualization be effectively designed to mitigate overtrust in LLM outputs, rather than unintentionally fostering it?
- Basis in paper: [explicit] The abstract and discussion highlight the "need for further investigation into the role of visualization as a mitigation technique" for false trust, noting that LinkQ's transparency features inadvertently caused users to rationalize incorrect answers.
- Why unresolved: The study found that visual aids intended to explain the system (like the state diagram) increased confidence even when the system was wrong; it is unclear if visualizations can be designed to "challenge" assumptions rather than just clarify processes.
- What evidence would resolve it: A study testing alternative "adversarial" visualization designs (e.g., those highlighting uncertainty or alternatives) to see if they reduce the rate of user acceptance of incorrect LLM outputs.

### Open Question 3
- Question: How do users interpret and react to empty or limited query results generated by an LLM?
- Basis in paper: [explicit] Section 6.3 asks: "When the LLM writes an incorrect query, what happens when results are empty or limited? ... Does this mean a user will believe their analysis is concluded?"
- Why unresolved: The authors observed users rationalizing incorrect non-empty results, but the specific user experience of "empty" results—where the user cannot distinguish between "no data exists" and "LLM query error"—remains an open area for qualitative study.
- What evidence would resolve it: A Wizard-of-Oz or log-analysis study focusing specifically on user strategies and confidence levels when queries return zero results.

## Limitations
- Small sample size (14 practitioners) with limited demographic diversity raises questions about generalizability across different user populations and KG domains.
- Accuracy evaluation conducted on specific KGs (Wikidata, BRON) with controlled question sets; performance on other knowledge graphs or real-world, ambiguous queries remains unknown.
- "Overtrust" phenomenon observed but not experimentally isolated from other factors like confirmation bias or prior domain knowledge.

## Confidence
- **High Confidence**: The chained prompting protocol's technical implementation and accuracy improvements on controlled question sets are well-documented and reproducible. The workflow differences between KG experts and non-experts are clearly observable and align with common HCI patterns.
- **Medium Confidence**: The overtrust mechanism—where visual transparency paradoxically increases false confidence—is supported by qualitative evidence but requires controlled experiments to establish causality and rule out confounding factors like user expertise or task complexity.
- **Low Confidence**: Claims about the generalizability of these trust dynamics to other KG domains, user populations, or LLM-assisted data analysis systems are speculative and not empirically validated beyond the study's scope.

## Next Checks
1. **Replicate trust calibration effects**: Conduct a controlled experiment with 30+ participants randomly assigned to LinkQ with and without uncertainty indicators (e.g., fuzzy-match warnings, alternative query suggestions). Measure overtrust rates on incorrect outputs while controlling for prior KG/LLM expertise.
2. **Test cross-domain generalizability**: Implement the chained prompting protocol with a different KG (e.g., biomedical, financial) and question set. Evaluate whether accuracy gains persist and whether visualization-mediated overtrust emerges in new contexts.
3. **Diagnose performance bottlenecks**: Systematically analyze failures on intersection and multi-hop queries (accuracy drops to 54.2% and below 17% respectively). Log KG API call sequences and LLM decisions to identify whether failures stem from premature exploration termination, multi-hop path discovery limitations, or query generation errors.