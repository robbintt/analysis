---
ver: rpa2
title: Can Interpretation Predict Behavior on Unseen Data?
arxiv_id: '2507.06445'
source_url: https://arxiv.org/abs/2507.06445
tags:
- accuracy
- attention
- hierarchical
- heads
- nested
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether interpretability tools can predict
  out-of-distribution (OOD) model behavior by analyzing attention patterns in Transformers
  trained on a synthetic parentheses-balancing task. The task allows models to learn
  either a simple counting rule (EQUAL-COUNT) or a hierarchical nesting rule (NESTED),
  creating diverse populations of models with different OOD generalization behaviors.
---

# Can Interpretation Predict Behavior on Unseen Data?

## Quick Facts
- arXiv ID: 2507.06445
- Source URL: https://arxiv.org/abs/2507.06445
- Reference count: 40
- This paper investigates whether interpretability tools can predict out-of-distribution (OOD) model behavior by analyzing attention patterns in Transformers trained on a synthetic parentheses-balancing task.

## Executive Summary
This paper investigates whether interpretability tools can predict out-of-distribution (OOD) model behavior by analyzing attention patterns in Transformers trained on a synthetic parentheses-balancing task. The task allows models to learn either a simple counting rule (EQUAL-COUNT) or a hierarchical nesting rule (NESTED), creating diverse populations of models with different OOD generalization behaviors. The authors find that attention patterns tracking hierarchical structure in the input data correlate with the hierarchical NESTED rule on unseen OOD data, even when these patterns are not causally necessary for implementing the rule. Ablation experiments reveal that some attention patterns actually suppress hierarchical generalization when present, and that causal interventions have weak correlation between in-distribution and OOD effects. The findings suggest that interpretability can provide valuable predictive insights about model behavior under distribution shifts, even without establishing causal mechanisms, motivating further work on using interpretations to anticipate model performance on unseen data.

## Method Summary
The authors train a population of small Transformers (1-3 layers, 2-4 attention heads, varying weight decay) on a synthetic parentheses-balancing task where models can learn either a counting-based or hierarchical generalization rule. They generate ambiguous training data compatible with both rules, then test OOD generalization on data that distinguishes between the two rules. The key analysis involves identifying attention heads that track hierarchical structure in the input (depth-tracking heads) and correlating these patterns with OOD accuracy. Ablation experiments systematically remove attention patterns to determine their causal role in implementing the learned rules.

## Key Results
- Hierarchical attention patterns (depth-tracking heads) in in-distribution data strongly correlate with hierarchical NESTED generalization on OOD data (ρ = 0.76)
- Some attention patterns correlate with NESTED but causally suppress it; ablating them improves OOD accuracy
- Causal ablation effects show weak correlation between in-distribution and OOD conditions (ρ = 0.24)
- 1-layer models consistently learn EQUAL-COUNT; 2-layer models prefer NESTED; 3-layer models show highest variance in rule selection

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Attention Patterns Predict OOD Generalization
- Claim: If in-distribution attention exhibits depth-tracking patterns (hierarchical heads), the model is likely to generalize hierarchically on OOD data.
- Mechanism: Hierarchical heads attend to tokens based on their tree depth in the parentheses sequence (negative-depth detectors favor tokens with more closing than opening brackets; sign-matching heads favor tokens matching the final token's depth sign). These patterns reflect hierarchical structure processing, which correlates with learning the NESTED rule.
- Core assumption: Representational geometry provides a proxy for the model's algorithm even when not causally employed.
- Evidence anchors:
  - [abstract] "when in-distribution attention exhibits hierarchical patterns, the model is likely to generalize hierarchically on OOD data"
  - [section 4.2] Figure 4b shows models with ID hierarchical heads have higher OOD accuracy
  - [corpus] Weak direct evidence; related work on Transformers and periodicity generalization (arXiv:2601.22690) shows OOD limitations but doesn't address interpretability-based prediction
- Break condition: If depth-tracking heads appear in models that fail NESTED generalization, the correlation breaks. The paper notes 23% of ID hierarchical heads don't behave as hierarchical heads OOD.

### Mechanism 2: Sign-Matching Heads Suppress Hierarchical Generalization
- Claim: Sign-matching attention heads correlate with NESTED behavior but causally suppress it; ablating them improves OOD accuracy.
- Mechanism: These heads attend to tokens whose depth sign matches the final token's depth. Though associated with NESTED, they interfere with systematic hierarchical generalization under distribution shift.
- Core assumption: The mechanism's function changes between ID and OOD conditions.
- Evidence anchors:
  - [abstract] "attention patterns actually suppress hierarchical generalization when present"
  - [section 4.3.2] Figure 5 shows ablating attention in models with OOD sign-matching heads improves OOD accuracy (points above diagonal)
  - [corpus] No direct corpus evidence for this counterintuitive finding
- Break condition: Models without backup circuits would show different ablation responses. The paper posits redundant circuits compensate ID but fail OOD.

### Mechanism 3: Weight Decay Prunes Vestigial Circuits That Affect OOD Only
- Claim: Regularization eliminates vestigial circuits (e.g., FIRST-SYMBOL heuristic) that have no ID impact but harm OOD generalization.
- Mechanism: Early training develops simple heuristics; without weight decay, these circuits persist as "vestigial" structures suppressed on ID data but active OOD. Weight decay forces the model toward systematic rules.
- Core assumption: Vestigial circuits can be functionally suppressed ID while remaining active OOD.
- Evidence anchors:
  - [section 3.1.1] FIRST-SYMBOL heuristic only occurs in 1-layer models with zero weight decay
  - [section 3.1.1] Figure 3 shows transient heuristic phases during training
  - [corpus] Weak; concept-based models under distribution shifts (arXiv:2504.17921) addresses OOD behavior but not vestigial circuits
- Break condition: If regularization promotes a rule that isn't systematically simpler, the mechanism fails. Section 3.2 notes regularization promotes different rules depending on architecture.

## Foundational Learning

- Concept: **Marr's Levels of Analysis**
  - Why needed here: The paper distinguishes between implementational-level (mechanistic circuits) and algorithmic-level (high-level behavioral rules) understanding. Causal interventions target implementation; correlational analysis targets algorithm.
  - Quick check question: Can you predict OOD behavior from attention patterns without knowing their causal role?

- Concept: **OOD Generalization vs. Systematic Rules**
  - Why needed here: Models can achieve perfect ID accuracy while following different rules OOD (EQUAL-COUNT vs. NESTED). Understanding which rule a model learned requires probing OOD behavior, not just ID performance.
  - Quick check question: Why can't ID validation accuracy distinguish between EQUAL-COUNT and NESTED models?

- Concept: **Correlation vs. Causation in Interpretability**
  - Why needed here: The key finding is that correlational interpretability can predict OOD behavior even when mechanistic interventions give misleading results. Ablation effects are weakly correlated between ID and OOD (ρ = 0.24).
  - Quick check question: If an attention head ablation doesn't affect ID accuracy, does that mean the head is unimportant for OOD generalization?

## Architecture Onboarding

- Component map:
  - Input layer -> Transformer layers (D ∈ {1,2,3}) -> Classification head
  - Hierarchical heads: Depth-tracking attention patterns (negative-depth detectors or sign-matching)
  - Vestigial circuits: FIRST-SYMBOL heuristic (first token determines label)

- Critical path:
  1. Training data is ambiguous (compatible with both EQUAL-COUNT and NESTED)
  2. Model converges on ID with ≥99% accuracy
  3. Internal attention patterns develop (hierarchical or not)
  4. OOD test reveals which rule the model learned
  5. Ablation tests causally probe mechanism importance

- Design tradeoffs:
  - **Depth vs. rule selection**: 1-layer models learn EQUAL-COUNT only; 2-layer models prefer NESTED; 3-layer models show highest variance (inverted U-shape)
  - **Weight decay vs. circuit persistence**: Zero weight decay allows vestigial circuits; higher decay prunes them but affects rule selection differently by depth
  - **Width (heads) vs. expressivity**: Number of heads has no significant effect on OOD rule selection (p > 0.05)
  - **Causal vs. correlational interpretability**: Causal interventions (ablation) show weak ID-OOD correlation; correlational analysis better predicts OOD behavior

- Failure signatures:
  - **FIRST-SYMBOL heuristic**: 1-layer, zero weight decay models show ~55% OOD accuracy (matches first-symbol bias in OOD set)
  - **Sign-matching heads present**: Correlate with NESTED but suppress it; ablation improves OOD accuracy
  - **Ablation-ID-OOD mismatch**: ID accuracy barely changes (<0.5% average) after attention ablation, but OOD accuracy changes dramatically

- First 3 experiments:
  1. **Train population with controlled hyperparameters**: Grid sweep over depth (1-3 layers), width (2-4 heads), weight decay (0, 0.001, 0.01), random seeds for initialization and data order. Verify ID convergence (≥99% accuracy).
  2. **Identify hierarchical heads on ID validation set**: For each model, check if attention heads at EOS position favor negative or non-negative depth tokens on ≥80% of mixed-depth sequences. Classify as negative-depth detectors or sign-matching heads.
  3. **Correlate head types with OOD accuracy and run ablations**: Plot OOD accuracy vs. head presence. Apply uniform attention ablation (flatten all attention to uniform). Compare ID vs. OOD accuracy changes to identify heads that suppress vs. support hierarchical generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can correlational interpretability methods predict OOD behavior in realistic, large-scale language models?
- Basis in paper: [explicit] The authors state: "Our study, however, uses a synthetic setting and it would take further effort to repeat a similar analysis in a realistic scenario" and "Because the models we study are necessarily small, the particular patterns we observe might apply less in large models."
- Why unresolved: The paper demonstrates proof-of-concept only on synthetic parentheses-balancing tasks with small Transformers; real-world LMs involve higher-dimensional representations, more distributed mechanisms, and greater training costs.
- What evidence would resolve it: Successful prediction of OOD generalization using internal representations (e.g., attention patterns, SAE features) in large pretrained models on natural language tasks with measurable distribution shifts.

### Open Question 2
- Question: Why do some correlated internal structures suppress rather than support the generalization rules they predict?
- Basis in paper: [explicit] The authors find that "ablating the attention of models with OOD sign-matching heads actually improves OOD accuracy" and state: "Disentangling module generalization failure like this is a ripe topic for future work."
- Why unresolved: The paper conjectures these heads may be "spandrels (side effects of learning NESTED)" or malfunction under distribution shift, but cannot distinguish between these hypotheses.
- What evidence would resolve it: Training dynamics analysis tracking when sign-matching heads emerge relative to rule acquisition; targeted ablations at different training stages; mechanistic analysis of how these heads interfere with hierarchical processing.

### Open Question 3
- Question: What is the nature and role of redundant backup circuits that compensate for ablations on ID data but fail under distribution shift?
- Basis in paper: [inferred] The authors posit that "models could contain many redundant backup circuits that compensate for ablation ID, but become unreliable under distribution shift" to explain why ablation effects are weakly correlated between ID and OOD conditions (ρ = 0.24).
- Why unresolved: The paper observes the phenomenon (ID accuracy drops only 0.5% after ablation while OOD accuracy changes substantially) but does not identify or characterize these backup circuits.
- What evidence would resolve it: Systematic identification of circuits with similar functional roles; probing whether backup circuits have different data distribution preferences; testing if backup circuits emerge later in training or have different architectural locations.

## Limitations
- The synthetic parentheses task may not generalize to real-world distributions with more complex relationships between features
- The mechanistic explanation for why some correlated structures suppress rather than support generalization rules is speculative
- The approach relies on 1D sequential structure where depth can be computed as a simple function of token position

## Confidence
- **High Confidence**: The core empirical finding that hierarchical attention patterns correlate with OOD generalization (ρ = 0.76 for depth-tracking heads) is well-supported by the experimental results across multiple model configurations
- **Medium Confidence**: The claim that correlational interpretability can predict OOD behavior better than causal interventions is supported, but the weak correlation between ID and OOD ablation effects (ρ = 0.24) could have alternative explanations related to the specific synthetic task design
- **Low Confidence**: The explanation for why sign-matching heads suppress hierarchical generalization is speculative. The paper posits redundant circuits and vestigial structures but doesn't provide direct evidence for these mechanisms

## Next Checks
1. **Cross-task validation**: Apply the same interpretability pipeline to a different synthetic task with multiple valid generalization rules (e.g., simple arithmetic vs. modular arithmetic) to test whether hierarchical attention patterns remain predictive of OOD behavior

2. **Real-world dataset test**: Apply the correlation-based interpretability approach to a real-world classification task where models can learn multiple valid rules (e.g., image classification with texture vs. shape cues) and validate whether attention patterns predict OOD generalization

3. **Architecture generalization**: Test whether the correlation between hierarchical attention and OOD generalization holds for different model architectures (RNNs, MLPs, or larger Transformers) on the same parentheses task to establish whether this is a general phenomenon or specific to the studied architecture