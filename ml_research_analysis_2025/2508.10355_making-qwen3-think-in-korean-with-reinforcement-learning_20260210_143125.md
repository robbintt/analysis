---
ver: rpa2
title: Making Qwen3 Think in Korean with Reinforcement Learning
arxiv_id: '2508.10355'
source_url: https://arxiv.org/abs/2508.10355
tags:
- reward
- korean
- reasoning
- grpo
- cars
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work successfully adapted Qwen3 14B to perform internal reasoning
  in Korean through a two-phase fine-tuning approach. The first phase used supervised
  fine-tuning on a Korean reasoning dataset to establish foundational Korean language
  understanding and logical reasoning skills.
---

# Making Qwen3 Think in Korean with Reinforcement Learning

## Quick Facts
- arXiv ID: 2508.10355
- Source URL: https://arxiv.org/abs/2508.10355
- Authors: Jungyup Lee; Jemin Kim; Sang Park; SeungJae Lee
- Reference count: 40
- Key outcome: Adapted Qwen3 14B to perform internal reasoning in Korean through two-phase fine-tuning (SFT + RL), achieving native Korean thinking capability and substantial performance improvements on reasoning benchmarks

## Executive Summary
This work successfully adapted Qwen3 14B to perform internal reasoning in Korean through a two-phase fine-tuning approach. The first phase used supervised fine-tuning on a Korean reasoning dataset to establish foundational Korean language understanding and logical reasoning skills. The second phase employed reinforcement learning with an Oracle-Guided Dr.GRPO algorithm that incorporated an external oracle judge to ensure stable training and prevent reward hacking. The resulting model demonstrates native Korean thinking capability, as evidenced by qualitative examples showing complete Korean chain-of-thought, while achieving substantial performance improvements on advanced reasoning benchmarks (AIME, GPQA-Diamond, HumanEval) compared to the base model.

## Method Summary
The method uses a two-stage fine-tuning approach: first, supervised fine-tuning (SFT) on 30k Korean reasoning examples establishes Korean language patterns and basic reasoning skills; second, reinforcement learning with Oracle-Guided Dr.GRPO optimizes for correctness within that linguistic framework. The RL phase uses composite rewards (accuracy, format, language consistency, length penalty) and an external oracle judge to prevent reward hacking. Training was conducted on 8×H100 GPUs using the Open-R1 toolkit, with 32k context length and bf16 precision.

## Key Results
- Achieved AIME 2024 accuracy of 83.3% (vs 73.33% post-SFT, 0% base)
- Improved HumanEval to 66.46% (vs 60.36% post-SFT, 56.09% base)
- Maintained KMMLU performance at 60.04% (vs 58.5% base)
- Demonstrated native Korean chain-of-thought reasoning capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-phase training (SFT → RL) enables language-native reasoning by first establishing linguistic patterns before optimizing reasoning trajectories.
- Mechanism: SFT on 30k Korean reasoning examples imprints Korean reasoning patterns and format compliance; RL then optimizes for correctness within that linguistic framework. SFT moved KMMLU from 58.5% to 60.04% and HumanEval from 56.09% to 60.36%, while RL pushed AIME 2024 from 73.33% (post-SFT) to 83.3%.
- Core assumption: Reasoning patterns learned in one language can transfer when the model is incentivized to think in that language, rather than translating internally.
- Evidence anchors:
  - [abstract] "two-stage fine-tuning approach... supervised fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a strong foundation"
  - [Section 2, "Impact of SFT"] Table 1 shows SFT improved HumanEval to 60.36 and GPQA-Diamond to 62.12
  - [Section 4.1] Table 2 shows RL-tuned model achieved AIME 2024: 83.3%, HumanEval: 66.46%

### Mechanism 2
- Claim: Oracle-guided reward calibration prevents reward hacking and policy collapse in GRPO-based RL.
- Mechanism: An external oracle judge (larger/higher-quality model) verifies semantic correctness and adjusts rewards, catching cases where the policy exploits reward signals without genuine problem-solving. Without oracle, accuracy reward collapsed to near zero by step 220; with oracle, it improved steadily to ~0.95.
- Core assumption: The oracle model provides reliable semantic judgments that the base reward function cannot.
- Evidence anchors:
  - [abstract] "introducing an oracle judge model that calibrates the reward signal"
  - [Section 3.1] Figure 3 shows v1 (no oracle) collapsed; v2 (oracle-guided) improved steadily
  - [corpus] Critique-GRPO paper mentions RL with numerical feedback can hit performance plateaus—oracle guidance addresses this

### Mechanism 3
- Claim: Composite reward (accuracy + format + language consistency + length penalty) shapes multi-faceted behavior without trade-offs.
- Mechanism: Each component targets a specific behavior (w_acc=1.0, w_format=0.2, w_lang=0.2, w_overlong=0.2). Language consistency reward (1.0 if Korean, 0.0 otherwise) ensures reasoning stays in Korean. The model maintained format/language rewards at 0.98–1.0 while improving accuracy from 0.55 to 0.85.
- Core assumption: Weighted sum does not create conflicting gradients that destabilize training.
- Evidence anchors:
  - [Section 3.2] "The original reward r for each output was a weighted sum... w_acc = 1.0 and w_format = w_lang = w_overlong = 0.2"
  - [Figure 5] Shows format and language rewards saturated at ~0.99; accuracy improved from ~0.5 to ~0.85
  - [corpus] Related GRPO work (Critique-GRPO) discusses multi-component feedback but corpus does not directly confirm weight tuning strategies

## Foundational Learning

- Concept: **Group Relative Policy Optimization (GRPO) and Dr.GRPO**
  - Why needed here: GRPO samples multiple outputs per query and learns from relative rewards within groups; Dr.GRPO removes normalization biases that cause length exploitation.
  - Quick check question: Why does removing σ_r normalization in Dr.GRPO prevent degenerate behaviors?

- Concept: **Reward function composition**
  - Why needed here: Multi-objective RL requires balancing accuracy, format, language, and length constraints without one dominating.
  - Quick check question: What happens if language weight (w_lang) is set too high relative to accuracy?

- Concept: **Oracle judge integration**
  - Why needed here: Pure verifiable rewards can be gamed; oracle provides semantic ground truth to catch reward hacking.
  - Quick check question: How does oracle judge differ from standard reward model?

## Architecture Onboarding

- Component map:
  Policy model -> Reference model (frozen SFT) -> Reward model (composite) -> Oracle judge (external)

- Critical path:
  1. Curate Korean reasoning SFT data (30k samples, 1:5 reasoning:non-reasoning mix)
  2. Run SFT for 3 epochs, LR=1e-5, verify token accuracy reaches ~85%
  3. Initialize RL from SFT checkpoint, configure Dr.GRPO with oracle judge
  4. Train ~1000 steps with G=12 samples per query, monitor for reward hacking

- Design tradeoffs:
  - Oracle overhead: Adds inference cost per sample; necessary for stability but slows training
  - Weight tuning: Higher w_lang ensures Korean reasoning but may sacrifice accuracy if too dominant
  - SFT data mix: 1:5 ratio balances reasoning injection vs. breadth; more reasoning data may overfit

- Failure signatures:
  - Accuracy reward suddenly drops → likely reward hacking; check oracle integration
  - Language reward drops → model reverting to English; increase w_lang or check language detector
  - Policy loss diverges → reduce learning rate or increase KL penalty (β)

- First 3 experiments:
  1. Reproduce SFT baseline: Train on Korean reasoning data, verify KMMLU and HumanEval improvements match Table 1
  2. Ablate oracle: Run Dr.GRPO with and without oracle judge, compare accuracy reward trajectory (expect collapse without oracle)
  3. Vary language weight: Test w_lang ∈ {0.1, 0.2, 0.5}, measure Korean CoT retention vs. accuracy trade-off

## Open Questions the Paper Calls Out
None

## Limitations
- Undisclosed oracle judge identity (listed candidates: Gemini, ChatGPT, DeepSeek but no specification)
- Limited dataset composition details beyond quantified sample counts
- No sensitivity analysis for composite reward weight tuning

## Confidence
- **High confidence**: Two-phase training framework producing measurable benchmark improvements
- **Medium confidence**: Oracle-guided reward calibration preventing reward hacking (based on Figure 3 comparison)
- **Medium confidence**: Composite reward function design maintaining stable behavior

## Next Checks
1. **Oracle Ablation Study**: Run Dr.GRPO with identical hyperparameters but without the oracle judge, tracking accuracy reward trajectory to confirm the predicted collapse behavior shown in Figure 3.
2. **Language Weight Sensitivity**: Test the RL phase with w_lang ∈ {0.1, 0.5} while holding other parameters constant to quantify the trade-off between Korean consistency and accuracy performance.
3. **Dataset Composition Analysis**: Systematically vary the SFT reasoning:non-reasoning ratio (1:2, 1:5, 1:10) to determine the optimal balance for establishing Korean reasoning patterns without overfitting.