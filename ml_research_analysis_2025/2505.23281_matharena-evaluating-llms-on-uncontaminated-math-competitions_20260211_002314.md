---
ver: rpa2
title: 'MathArena: Evaluating LLMs on Uncontaminated Math Competitions'
arxiv_id: '2505.23281'
source_url: https://arxiv.org/abs/2505.23281
tags:
- competitions
- high
- problems
- problem
- competition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MATHARENA, a benchmark for evaluating LLMs
  on recently released math competitions to avoid contamination from training data.
  The approach uses recurring competitions as a source of high-quality, uncontaminated
  problems, evaluated immediately upon release.
---

# MathArena: Evaluating LLMs on Uncontaminated Math Competitions

## Quick Facts
- **arXiv ID:** 2505.23281
- **Source URL:** https://arxiv.org/abs/2505.23281
- **Reference count:** 40
- **Primary result:** MATHARENA introduces a contamination-free benchmark for LLM math evaluation using recently released competitions, showing top models achieve ~90% on numerical problems and ~40% on proofs.

## Executive Summary
This paper introduces MATHARENA, a benchmark for evaluating LLMs on recently released math competitions to avoid contamination from training data. The approach uses recurring competitions as a source of high-quality, uncontaminated problems, evaluated immediately upon release. The benchmark covers both final-answer and proof-based competitions, totaling 162 problems across seven competitions. Key results include: top models (GPT-5, GROK4, GEMINI-2.5-PRO) achieving ~90% accuracy on numerical problems, outperforming top 1% of humans; evidence of significant contamination in AIME 2024; and proof-based evaluations on IMO 2025 showing top models at ~40%, indicating substantial room for improvement. The benchmark is fully transparent, reproducible, and continuously updated.

## Method Summary
MATHARENA evaluates LLMs on math competitions (final-answer and proof-based) immediately after release to avoid data contamination. The method uses 162 problems from 7 competitions (AIME, HMMT, BRUMO, CMIMC, USAMO, IMO, Project Euler) with data available under CC-BY-NC-SA 4.0 license. Models are queried with competition-specific prompts and evaluated using a dual-path grading system: rule-based sympy parser for numerical answers with LLM judge fallback, and expert human grading for proofs. The approach includes statistical analysis using paired permutation tests for rank confidence intervals and Bernoulli variance for accuracy confidence intervals.

## Key Results
- Top models achieve ~90% accuracy on uncontaminated numerical problems, outperforming top 1% of humans
- Significant contamination detected in AIME 2024, with models performing above human quantile baseline
- Proof-based evaluation on IMO 2025 shows top models at ~40%, demonstrating substantial room for improvement
- Benchmark successfully differentiates model capabilities with 95% confidence intervals despite small competition sizes

## Why This Works (Mechanism)

### Mechanism 1: Temporal Contamination Barrier
Evaluating on competitions released after model training cutoff prevents memorization-based performance inflation by creating an information-theoretic barrier where models cannot have memorized solutions to problems that did not yet exist.

### Mechanism 2: Human Quantile Alignment for Difficulty Calibration
Using human performance quantiles across years provides difficulty-normalized baselines for detecting contamination by mapping model performance to stable human quantile distributions.

### Mechanism 3: Dual-Path Verification for Answer Extraction
Combining rule-based symbolic parsing with LLM-based semantic equivalence checking catches parser failures without sacrificing scalability, using sympy parser for exact equivalence with Gemini-2.5-Flash as backup judge.

## Foundational Learning

- **Data Contamination in Benchmark Evaluation:** Understanding how training data leakage inflates reported capabilities is essential since the entire premise of MathArena is detecting and avoiding contamination.
- **Chain-of-Thought Reasoning in LLMs:** The paper explicitly excludes "non-reasoning models" and tracks performance improvements tied to chain-of-thought releases, making this concept critical.
- **Proof Verification vs Natural Language Proof Evaluation:** MathArena handles both automated and human-graded evaluations, and understanding why proofs resist automation is critical.

## Architecture Onboarding

- **Component map:** Competition Sources → Problem Extraction → Model Inference → Grading Fork (Final-answer: Parser + LLM judge + Manual; Proof-based: Expert human grading) → Post-Processing → Leaderboard
- **Critical path:** The grading fork is the bottleneck, with final-answer competitions scaling well but proof-based competitions requiring expert human time (30+ grader-hours for IMO 2025).
- **Design tradeoffs:** Small N (30-40 problems) vs statistical power, transparency vs contamination risk, human grading vs scalability.
- **Failure signatures:** Truncation detection (short outputs), parser failures (non-standard formatting), contamination indicators (above quantile baseline), proof hallucination (fabricated theorems).
- **First 3 experiments:** 1) Reproduce parser on held-out responses to validate automation, 2) Implement paired permutation test for rank CI to understand statistical methodology, 3) Single-competition contamination check to practice contamination detection.

## Open Questions the Paper Calls Out

- **Scalable automated grading for natural language proofs:** Current methods cannot reliably assess proof correctness and completeness, requiring human expertise.
- **New competition formats for saturated benchmarks:** As top models approach ~90% on numerical problems, new formats are needed where top models achieve <60% accuracy.
- **Detecting partial contamination:** Even new competitions may contain problems with similar versions online, creating partial memorization risks.
- **Closing proof-reasoning gaps:** The large performance gap between final-answer (~90%) and proof-based (~40%) mathematical reasoning remains unresolved.

## Limitations

- **Temporal contamination window vulnerability:** If model providers update closed-source models with newly released competition data before public evaluation, the contamination barrier fails.
- **Human quantile baseline stability assumptions:** Significant difficulty fluctuations between competition years could cause false contamination signals or missed contamination.
- **Proof grading subjectivity:** Mathematical proof evaluation inherently involves subjective judgment about elegance and completeness, potentially affecting reported model performance.

## Confidence

**High Confidence:**
- MATHARENA successfully avoids contamination for competitions evaluated immediately upon release
- Top models achieve ~90% accuracy on uncontaminated numerical problems
- AIME 2024 shows clear contamination evidence (models performing above human quantile baseline)
- Proof-based evaluation reveals substantial capability gaps (top models at ~40% on IMO 2025)

**Medium Confidence:**
- 162 problems across 7 competitions provides sufficient statistical power for model differentiation
- The dual-path grading system maintains accuracy while scaling
- Human quantile alignment reliably calibrates difficulty across competition years

**Low Confidence:**
- The benchmark's ability to detect subtle contamination cases where models only memorize partial solutions
- Long-term sustainability as competitions may be reused in modified forms
- The extrapolation that ~90% on numerical problems represents "superhuman" performance

## Next Checks

1. **Replicate contamination detection methodology:** Implement quantile-alignment analysis on AIME 2024 and 2025 problems to verify contamination patterns.
2. **Test parser robustness on edge cases:** Measure disagreement rates between parser, LLM judge, and ground truth on 100 non-standard formatted responses.
3. **Validate proof grading consistency:** Compute inter-rater reliability across three expert graders for 10 IMO 2025 problems to quantify grader subjectivity impact.