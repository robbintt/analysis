---
ver: rpa2
title: Can ChatGPT Learn My Life From a Week of First-Person Video?
arxiv_id: '2504.03857'
source_url: https://arxiv.org/abs/2504.03857
tags:
- keegan
- data
- summaries
- about
- what
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored whether foundation models can learn personal
  information from first-person video data by having the author wear a camera headset
  for 54 hours and fine-tuning GPT-4o and GPT-4o-mini on automatically generated summaries.
  Both models successfully learned basic facts about the author (age, gender, occupation),
  with GPT-4o correctly deducing more complex details like city of residence and pet
  ownership.
---

# Can ChatGPT Learn My Life From a Week of First-Person Video?
## Quick Facts
- **arXiv ID**: 2504.03857
- **Source URL**: https://arxiv.org/abs/2504.03857
- **Reference count**: 9
- **Primary result**: Foundation models can extract personal information from first-person video but exhibit significant hallucination tendencies

## Executive Summary
This study explored whether foundation models can learn personal information from first-person video data by having the author wear a camera headset for 54 hours and fine-tuning GPT-4o and GPT-4o-mini on automatically generated summaries. Both models successfully learned basic facts about the author (age, gender, occupation), with GPT-4o correctly deducing more complex details like city of residence and pet ownership. However, both models also hallucinated information, inventing names for people in the footage. GPT-4o answered 7 out of 13 personal questions correctly, while GPT-4o-mini only got 2 correct, showing significant performance differences between model sizes.

## Method Summary
The author wore a camera headset continuously for approximately 54 hours over one week, capturing first-person video data of daily activities. The video footage was automatically summarized using computer vision and natural language processing techniques. These summaries were then used to fine-tune two foundation models: GPT-4o and GPT-4o-mini. After fine-tuning, both models were queried with 13 personal questions to evaluate their ability to learn and recall information about the author's life from the video data.

## Key Results
- GPT-4o correctly answered 7 out of 13 personal questions, demonstrating ability to extract complex personal details
- GPT-4o-mini only answered 2 out of 13 questions correctly, revealing significant performance differences between model sizes
- Both models exhibited hallucination tendencies, confidently inventing names for people in the footage despite this information not being present
- Basic demographic information (age, gender, occupation) was successfully extracted by both models

## Why This Works (Mechanism)
Foundation models trained on vast amounts of internet data have learned patterns of human behavior, environments, and social interactions. When exposed to first-person video data, these models can leverage their pre-existing knowledge to contextualize and interpret visual information. The automatic summarization process converts visual patterns into textual representations that foundation models can process effectively. Fine-tuning on personalized video summaries allows the models to adapt their general knowledge to specific individual contexts, enabling extraction of personal details from visual cues about environments, objects, and activities.

## Foundational Learning
- **Egocentric computer vision**: Understanding first-person perspective is crucial because standard object recognition doesn't capture the spatial relationships and context of personal activities. Quick check: Evaluate summary quality on diverse activity types.
- **Foundation model fine-tuning**: Models must balance retaining general knowledge while adapting to personal context. Quick check: Compare performance on personal vs general questions.
- **Information hallucination in LLMs**: Understanding when and why models generate confident but incorrect information is essential for assessing reliability. Quick check: Systematically test confidence calibration.
- **Video-to-text summarization**: Converting visual data to textual format enables foundation models to process egocentric video effectively. Quick check: Measure information loss during summarization.
- **Personal data privacy**: The ability to extract detailed personal information raises important privacy considerations. Quick check: Assess what sensitive information can be inferred.
- **Model size scaling**: Performance differences between GPT-4o and GPT-4o-mini demonstrate the importance of model capacity for complex inference tasks. Quick check: Map performance to parameter count.

## Architecture Onboarding
**Component Map**: Camera Headset -> Video Processing Pipeline -> Automatic Summarization -> Foundation Model Fine-tuning -> Question Answering

**Critical Path**: The most critical path is Video Processing Pipeline -> Automatic Summarization, as errors or information loss at this stage directly limit what foundation models can learn. The quality of generated summaries determines the upper bound of model performance.

**Design Tradeoffs**: Using automatic summarization trades computational efficiency and scalability for potential information loss compared to raw video processing. Fine-tuning existing foundation models leverages pre-trained knowledge but may introduce biases from training data. The choice of summary generation method affects both the types of information that can be extracted and the computational resources required.

**Failure Signatures**: Hallucinations (confident but incorrect information generation), particularly for proper nouns and names, represent a fundamental failure mode. Information loss during summarization manifests as inability to answer questions about specific details. Poor generalization across different activity types indicates limitations in the video processing pipeline.

**3 First Experiments**:
1. Compare summary quality using different video processing techniques (frame-by-frame vs continuous processing)
2. Test model performance on held-out questions about temporal relationships between events
3. Evaluate hallucination frequency when training data contains ambiguous visual information

## Open Questions the Paper Calls Out
None

## Limitations
- Single participant study (n=1) limits generalizability to broader populations
- 54-hour dataset may not capture sufficient variability in contexts and social interactions
- Automatic summarization process introduces additional error layer that wasn't fully characterized
- Evaluation only tested retrieval of existing information, not reasoning or synthesis capabilities

## Confidence
- **High confidence**: Both GPT-4o and GPT-4o-mini can extract basic demographic information (age, gender, occupation) from egocentric video summaries
- **Medium confidence**: GPT-4o can deduce more complex personal details (city of residence, pet ownership) with reasonable accuracy
- **Low confidence**: Performance differences between model sizes (7/13 vs 2/13 correct answers) would need replication with larger samples to validate
- **High confidence**: Both models exhibit hallucination tendencies, confidently generating incorrect information (invented names)

## Next Checks
1. **Replication with diverse participants**: Test the same methodology across 10-20 participants with varied demographics, occupations, and living environments to establish error rate distributions and identify systematic biases.

2. **Controlled hallucination study**: Systematically vary the amount and type of training data to map the relationship between training set size, data diversity, and hallucination frequency, particularly for proper nouns and named entities.

3. **Longitudinal stability assessment**: Conduct the experiment across multiple non-consecutive weeks to evaluate whether models maintain consistent performance over time and identify which types of information show temporal decay versus persistence.