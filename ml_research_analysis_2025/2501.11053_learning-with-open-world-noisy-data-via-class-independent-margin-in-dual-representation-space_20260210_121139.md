---
ver: rpa2
title: Learning with Open-world Noisy Data via Class-independent Margin in Dual Representation
  Space
arxiv_id: '2501.11053'
source_url: https://arxiv.org/abs/2501.11053
tags:
- noise
- learning
- noisy
- open-set
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses Learning with Open-world Noisy Data (LOND),
  a challenging problem where noisy labels may originate from unknown classes during
  both training and inference. To handle closed-set and open-set noise, the authors
  propose a dual representation space learning framework that jointly trains a projection
  network (for shared semantics in prototype space) and a One-Vs-All network (for
  unique semantics in class-independent space).
---

# Learning with Open-world Noisy Data via Class-independent Margin in Dual Representation Space

## Quick Facts
- arXiv ID: 2501.11053
- Source URL: https://arxiv.org/abs/2501.11053
- Reference count: 40
- The method achieves 4.55% average accuracy improvement and 6.17% AUROC improvement on CIFAR80N dataset

## Executive Summary
This paper addresses the Learning with Open-world Noisy Data (LOND) problem, where training data contains both closed-set noise (wrong labels from known classes) and open-set noise (labels from unknown classes). The authors propose a dual representation learning framework that jointly trains a projection network and a One-Vs-All network to capture shared and unique semantics respectively. Through bi-level contrastive learning, consistency regularization, and class-independent margin criteria, the method achieves state-of-the-art performance on both synthetic and real-world noisy datasets.

## Method Summary
The method introduces a dual representation learning framework that operates in both prototype space (shared semantics across classes) and class-independent space (unique semantics for each class). The framework jointly trains a projection network to capture shared semantic features and a One-Vs-All network to model class-specific characteristics. Bi-level contrastive learning enhances the distinction between clean and noisy samples, while consistency regularization improves robustness to open-set noise. The class-independent margin criterion provides a principled way to identify and filter noisy samples during training. Extensive experiments on CIFAR80N, CIFAR100N, and web-crawled datasets demonstrate superior performance compared to existing methods.

## Key Results
- Achieves 4.55% average accuracy improvement on CIFAR80N dataset
- Improves AUROC by 6.17% on CIFAR80N compared to existing methods
- Demonstrates superior performance on real-world web-crawled datasets (Web-Aircraft, Web-Car, Web-Bird)

## Why This Works (Mechanism)
The dual representation framework effectively separates shared semantic information from class-specific features, allowing the model to better distinguish between clean samples and noise. The projection network captures universal features that help identify patterns across different classes, while the One-Vs-All network focuses on unique characteristics that differentiate each class. Bi-level contrastive learning creates stronger discriminative power by comparing samples at multiple levels, and consistency regularization ensures stable predictions across different augmentations. The class-independent margin criterion provides a data-driven threshold for identifying noisy samples based on their representation distances.

## Foundational Learning
- **Contrastive learning**: Needed for learning discriminative representations by pulling similar samples together and pushing dissimilar samples apart. Quick check: Verify that positive pairs share semantic similarity while negative pairs are truly dissimilar.
- **Dual representation learning**: Required to separate shared and unique features for better noise handling. Quick check: Ensure both representation spaces contribute meaningfully to final predictions.
- **Consistency regularization**: Important for improving model robustness to input variations. Quick check: Monitor prediction stability across different augmentations.
- **Margin-based classification**: Provides principled threshold for noise detection. Quick check: Validate that margin values appropriately separate clean from noisy samples.
- **Open-set recognition**: Essential for handling unknown classes in real-world scenarios. Quick check: Test model's ability to reject truly novel classes.
- **Noisy label training**: Fundamental for learning with imperfect supervision. Quick check: Evaluate performance degradation as noise levels increase.

## Architecture Onboarding

Component Map:
Projection Network -> Prototype Space -> Shared Semantics
One-Vs-All Network -> Class-independent Space -> Unique Semantics
Bi-level Contrastive Loss -> Both Spaces
Consistency Regularization -> Both Spaces
Margin Criterion -> Final Prediction

Critical Path:
Input -> Augmentation -> Dual Networks -> Contrastive Learning -> Margin-based Filtering -> Final Prediction

Design Tradeoffs:
- Computational overhead vs. performance gain from dual representation
- Complexity of bi-level contrastive learning vs. simpler alternatives
- Margin threshold selection vs. adaptive approaches
- Training stability with multiple loss components

Failure Signatures:
- Poor separation in prototype space indicates ineffective shared feature learning
- Inconsistent predictions across augmentations suggest weak regularization
- High false positive rate in noise detection points to suboptimal margin criteria
- Performance degradation on clean data indicates over-aggressive noise filtering

First 3 Experiments:
1. Ablation study removing bi-level contrastive learning to measure its contribution
2. Single representation space variant to quantify dual space benefits
3. Fixed vs. adaptive margin threshold comparison for noise detection

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Primarily validated on small-scale datasets (CIFAR variants and web-crawled images)
- Computational overhead of dual representation spaces not thoroughly analyzed
- Limited evaluation on non-image domains and real-world large-scale scenarios
- Heavy reliance on synthetic noise injection may not reflect real-world complexity

## Confidence
- **High confidence**: Method's ability to handle closed-set noise on CIFAR datasets
- **Medium confidence**: Performance claims on web-crawled datasets
- **Low confidence**: Scalability to real-world large-scale applications

## Next Checks
1. Evaluate the method on larger-scale datasets (e.g., ImageNet variants) to assess scalability and computational efficiency
2. Conduct ablation studies on the dual representation framework components to quantify individual contributions to performance
3. Test the method on non-image domains (e.g., text classification with noisy labels) to validate cross-domain applicability