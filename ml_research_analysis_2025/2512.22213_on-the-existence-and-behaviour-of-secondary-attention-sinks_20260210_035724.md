---
ver: rpa2
title: On the Existence and Behaviour of Secondary Attention Sinks
arxiv_id: '2512.22213'
source_url: https://arxiv.org/abs/2512.22213
tags:
- sink
- sinks
- secondary
- attention
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies and analyzes secondary attention sinks\u2014\
  tokens that become attention sinks in middle layers and persist for variable depths,\
  \ unlike primary sinks that emerge early and persist throughout. The study spans\
  \ 11 model families and reveals that secondary sinks are formed by specific MLP\
  \ layers that map token representations to align with the primary sink direction."
---

# On the Existence and Behaviour of Secondary Attention Sinks

## Quick Facts
- arXiv ID: 2512.22213
- Source URL: https://arxiv.org/abs/2512.22213
- Reference count: 7
- This paper identifies and analyzes secondary attention sinks—tokens that become attention sinks in middle layers and persist for variable depths, unlike primary sinks that emerge early and persist throughout.

## Executive Summary
This paper identifies and analyzes secondary attention sinks in transformer models—tokens that emerge as attention sinks in middle layers rather than early layers like the primary BOS sink. The study spans 11 model families and reveals that secondary sinks are formed by specific MLP layers that map token representations to align with the primary sink direction. The ℓ2-norm of these MLP outputs determines both the sink score and lifetime of secondary sinks. The findings show that larger models exhibit more deterministic and frequent sink levels, with secondary sinks compensating for the decay of primary sink strength in middle layers.

## Method Summary
The paper analyzes reasoning traces from DeepSeek-14B on AIME24 and Math datasets, examining hidden states, attention weights, and MLP outputs across 11 model families. Secondary sinks are detected using cosine similarity > 0.95 with the BOS sink direction, combined with ℓ2-norm thresholding. The analysis tracks sink formation, lifetime, and properties across layers, using PCA on sink token representations and t-SNE clustering. No training is performed—the study is purely analytical, examining trained models to understand the emergence and characteristics of secondary attention sinks.

## Key Results
- Secondary attention sinks emerge in middle layers (not early) and persist for variable depths, unlike primary BOS sinks
- Specific middle-layer MLP modules create secondary sinks by mapping token representations to align with the primary sink direction
- The ℓ2-norm of MLP outputs at sink-creation layers determines both sink score and the number of layers the sink persists

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Specific middle-layer MLP modules convert semantically uninformative tokens into secondary attention sinks by mapping their representations to align with the primary sink direction.
- **Mechanism:** Tokens that will become secondary sinks pass through MLP layers with low cosine similarity to the BOS sink. The MLP selectively amplifies components aligned with the BOS sink direction while suppressing misaligned components. Multiple orthogonal principal components (PC1–PC6) are converted into the same sink direction, indicating the MLP encodes a sink-direction mapping function.
- **Core assumption:** The linear representation hypothesis holds—future secondary sinks share a common linear subspace that the MLP exploits to recover the sink direction.
- **Evidence anchors:**
  - [abstract] "these sinks are formed by specific middle-layer MLP modules; these MLPs map token representations to vectors that align with the direction of the primary sink"
  - [Section 4] Figure 4 shows cosine similarity with BOS sink increasing from ~0.2 at MLP input to ~0.95 at MLP output; Figure 5 shows ±αPCi inputs produce outputs aligned or anti-aligned with sink direction
  - [corpus] Related work (Queipo-de Llano et al., 2025) links sinks to compression; Ruscio et al. (2025) ties multi-sink emergence to positional embedding schemes—but neither describes this MLP-driven formation mechanism
- **Break condition:** If MLP weights are ablated at the identified `l_start` layer and sink formation persists, this mechanism is incomplete.

### Mechanism 2
- **Claim:** The ℓ2-norm of the MLP output at the sink-creation layer (`l_start`) causally determines both the attention sink score and the number of layers the sink persists.
- **Mechanism:** Higher MLP output norms produce larger hidden-state norms in subsequent layers, which increase attention weights via softmax scaling. The relationship is log-linear for sink score ratio vs. norm, and monotonic with plateaus for lifetime vs. log-norm. This creates discrete "sink levels" with characteristic (l_start, lifetime) pairs.
- **Core assumption:** The attention score scales sufficiently with query/key norm for norm-driven attention amplification to overcome positional biases.
- **Evidence anchors:**
  - [abstract] "The ℓ2-norm of these vectors determines the sink score of the secondary sink, and also the number of layers it lasts for"
  - [Section 5] Figure 7 (Mid, Right) shows log-linear relationship between MLP output norm and sink-score ratio; lifetime increases monotonically with distinct linear regimes
  - [corpus] Corpus does not provide independent verification of norm-lifetime coupling; this appears to be a novel finding
- **Break condition:** If artificially scaling MLP output norms at `l_start` does not produce proportional changes in sink lifetime, the claimed causal relationship is weaker than proposed.

### Mechanism 3
- **Claim:** Secondary sinks function as compensatory attention anchors that emerge when the primary BOS sink reaches its minimum strength in middle layers.
- **Mechanism:** The BOS sink score exhibits a valley-shaped profile across depth, reaching minimum in middle layers (layer ~22 in DeepSeek-14B). Secondary sinks emerge precisely at this minimum, redistributing attention mass that would otherwise be unanchored. This maintains attention stability across the network's full depth.
- **Core assumption:** A minimum level of sink attention is functionally necessary for stable transformer operation (per prior work on attention sinks preventing "over-mixing").
- **Evidence anchors:**
  - [abstract] "the primary sink weakens in middle layers, coinciding with the emergence of secondary sinks"
  - [Section 5] Figure 7 (Left) shows BOS sink-score valley and secondary sink emergence at the same depth; Finding 3.1 explicitly states "compensating effect"
  - [corpus] Gu et al. (2024) interprets sinks as mitigating over-mixing; Yu et al. (2024) shows multi-token sinks—but compensation for BOS decay is not established in prior work
- **Break condition:** If secondary sinks are surgically removed (attention masked to BOS only) and model performance does not degrade in middle layers, the compensation hypothesis is unsupported.

## Foundational Learning

- **Concept: Attention Sink (Primary/BOS Sink)**
  - Why needed here: The entire analysis contrasts secondary sinks against the well-studied primary sink phenomenon. Without understanding that BOS tokens attract disproportionate attention across all layers, the novelty of middle-layer, variable-lifetime sinks is lost.
  - Quick check question: Can you explain why a semantically uninformative token like BOS would receive high attention across diverse inputs?

- **Concept: RoPE and Positional Embedding Effects on Attention**
  - Why needed here: The paper notes that models using NTK-aware scaled RoPE (lower rotary frequency) show secondary sinks while standard RoPE models often don't. Positional encoding interacts with sink formation.
  - Quick check question: How does reducing the rotary base frequency in RoPE change the relative attention advantage of early sequence positions?

- **Concept: MLP Computational Role in Transformers**
  - Why needed here: The causal mechanism hinges on MLP layers (not attention layers) creating secondary sinks. Understanding that MLPs perform position-wise transformations is essential.
  - Quick check question: In a decoder-only transformer, does the MLP at layer `l` have access to information from other sequence positions?

## Architecture Onboarding

- **Component map:** Hidden state → MLP at `l_start` → Aligned representation with BOS sink → Secondary sink emerges → Persists for `lifetime` layers

- **Critical path:**
  1. Identify `l_start` for your model family (check Appendix Tables 2–3)
  2. Extract MLP outputs at `l_start` for candidate tokens
  3. Measure ℓ2-norm and cosine similarity with BOS sink to predict lifetime and score

- **Design tradeoffs:**
  - Larger models → more deterministic sink levels but potentially more sink levels overall (Qwen3-14B shows 6 levels vs. QwQ-32B's 3)
  - Math/reasoning post-training amplifies secondary sinks (Qwen2-Math vs. Qwen2 base)
  - Not all RoPE-base models exhibit secondary sinks despite similar positional encoding—architecture-specific factors remain unclear

- **Failure signatures:**
  - Secondary sinks absent in LLaMA-3.1, CodeLlama, Mathstral despite appropriate scale
  - Small models (<1.5B) show weak or absent secondary sinks
  - Models with high rotary base (10K) can show sinks (Qwen2-Math-1.5B), but so can models with 1M base (Qwen3)—RoPE base alone does not predict sink presence

- **First 3 experiments:**
  1. **Reproduction check:** Run the cosine similarity > 0.95 detection method on your target model with reasoning traces; verify secondary sinks appear at documented `l_start` layers
  2. **Ablation test:** Zero out MLP outputs at `l_start` for detected secondary sink tokens only; measure change in attention patterns and downstream token predictions
  3. **Norm-scaling intervention:** Artificially scale MLP output norms at `l_start` for non-sink tokens by factors of 2×, 5×, 10×; test if this creates synthetic secondary sinks with proportionally increased lifetimes

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the functional impact of secondary sinks on text generation quality and downstream task performance?
- **Basis in paper:** [explicit] The conclusion states that the "effect on the text generation and downstream performance remains an open question."
- **Why unresolved:** The paper focuses on characterizing the existence, formation, and attention patterns of secondary sinks, but does not evaluate whether their presence improves or degrades model outputs.
- **What evidence would resolve it:** Ablation studies that selectively suppress secondary sinks while measuring changes in perplexity, task accuracy (e.g., on AIME24), and generation coherence.

### Open Question 2
- **Question:** Why do certain model families like CodeLlama fail to exhibit secondary sinks despite using large rotary bases similar to models that do?
- **Basis in paper:** [explicit] Section 2 notes that despite using very large rotary bases, CodeLlama does not exhibit secondary sinks, and states: "The underlying cause of this discrepancy remains an open question."
- **Why unresolved:** While the authors identify a correlation between low rotary frequency and secondary sinks in Qwen models, this correlation breaks down for CodeLlama, suggesting other architectural or training factors are at play.
- **What evidence would resolve it:** A comparative analysis of MLP weight configurations and activation patterns between CodeLlama and Qwen families to identify blocking mechanisms or missing pre-training features.

### Open Question 3
- **Question:** What specific dynamics during pre-training lead to the emergence of secondary sinks?
- **Basis in paper:** [explicit] The conclusion identifies "Investigating the root cause of secondary sinks emergence during pre-training" as a direction for future research.
- **Why unresolved:** The study analyzes trained models and post-training effects (e.g., math fine-tuning) but does not examine the training checkpoints or loss dynamics that initially cause these structures to form.
- **What evidence would resolve it:** Checkpoint analysis throughout the pre-training of models like Qwen or DeepSeek to observe exactly when secondary sink levels emerge and correlate this with training loss phases.

### Open Question 4
- **Question:** Is the emergence of secondary sinks a causal mechanism for supporting enhanced reasoning, or merely a correlated byproduct?
- **Basis in paper:** [inferred] Section 3 suggests secondary sinks "arise as a mechanism to support enhanced reasoning capability," but the paper establishes correlation (they appear after math training) rather than causation.
- **Why unresolved:** The paper demonstrates that secondary sinks compensate for primary sink decay, but it does not prove that this compensation is functionally required for the reasoning capabilities observed in larger models.
- **What evidence would resolve it:** Intervention experiments where secondary sinks are artificially induced in non-reasoning models to see if reasoning performance improves, or conversely, removed from reasoning models to see if performance degrades.

## Limitations

- The paper establishes correlation between MLP outputs and sink formation but does not provide definitive causal proof through targeted interventions
- The study focuses primarily on reasoning traces from specific datasets, which may not represent general model behavior
- The compensation hypothesis is based on temporal correlation rather than experimental validation of functional necessity

## Confidence

- **High Confidence:** The identification of secondary attention sinks as a distinct phenomenon from primary BOS sinks
- **Medium Confidence:** The mechanism describing how specific MLP layers create secondary sinks by mapping representations to align with the primary sink direction
- **Medium Confidence:** The claim that ℓ2-norm of MLP outputs determines both sink score and lifetime
- **Low Confidence:** The compensation hypothesis that secondary sinks emerge specifically to counteract BOS sink decay in middle layers

## Next Checks

1. **Ablation Experiment:** Remove or zero out MLP outputs at the identified `l_start` layers for secondary sink tokens only, then measure whether sink formation is prevented while preserving model functionality on reasoning tasks.

2. **Norm Scaling Intervention:** Systematically scale MLP output norms at `l_start` by controlled factors (2×, 5×, 10×) for non-sink tokens to test whether synthetic secondary sinks with predictable lifetimes can be created, validating the causal relationship between norm and sink properties.

3. **Functional Impact Test:** Remove secondary sinks through attention masking (forcing attention to BOS only) and measure performance degradation specifically in middle layers where compensation is hypothesized to be most important, testing whether the compensation hypothesis is functionally necessary.