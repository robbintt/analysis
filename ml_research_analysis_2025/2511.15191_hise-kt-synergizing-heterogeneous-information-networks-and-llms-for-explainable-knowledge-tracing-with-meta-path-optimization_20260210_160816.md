---
ver: rpa2
title: 'HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable
  Knowledge Tracing with Meta-Path Optimization'
arxiv_id: '2511.15191'
source_url: https://arxiv.org/abs/2511.15191
tags:
- knowledge
- student
- uni00000013
- question
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HISE-KT, a framework that synergizes heterogeneous
  information networks (HINs) and large language models (LLMs) for explainable knowledge
  tracing. It constructs a multi-relationship HIN with five node types and employs
  an LLM to score and filter meta-path instances, reducing noise.
---

# HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization

## Quick Facts
- **arXiv ID:** 2511.15191
- **Source URL:** https://arxiv.org/abs/2511.15191
- **Reference count:** 23
- **Primary result:** AUC up to 0.9749, ACC up to 0.9073 on four datasets

## Executive Summary
This paper introduces HISE-KT, a framework that combines heterogeneous information networks (HINs) with large language models (LLMs) for explainable knowledge tracing. The system constructs a multi-relationship HIN with five node types, uses an LLM to score and filter meta-path instances, and employs a rule-based collaborative filtering mechanism to retrieve similar students for enriched context. The LLM then predicts student performance and generates explainable analysis reports. Experiments on four datasets demonstrate superior predictive performance compared to state-of-the-art baselines while providing interpretable explanations.

## Method Summary
HISE-KT integrates HINs with LLMs to create an explainable knowledge tracing framework. The method constructs a multi-relational HIN with five node types (students, questions, knowledge concepts, student ability, question difficulty) and 14 meta-paths. An LLM scores and filters meta-path instances to reduce noise, while a rule-based collaborative filtering mechanism retrieves similar students based on educational psychology principles. The final LLM module generates predictions and three-sentence analysis reports by integrating the student's history, similar peers' information, and question attributes. The entire framework operates in a zero-shot manner without fine-tuning the LLM.

## Key Results
- Achieves AUC up to 0.9749 and ACC up to 0.9073 on four benchmark datasets
- Ablation studies show significant performance drops when meta-path scorer (w/o MSR) or similar student retriever (w/o SimU) is removed
- Provides interpretable analysis reports that explain prediction rationale
- Outperforms state-of-the-art knowledge tracing methods on all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM-based meta-path scoring reduces noise and improves predictive signal quality
- **Mechanism:** Samples 100 meta-path instances per template, LLM scores each on Question Centrality, Knowledge Concept Relevance, Informativeness, and Node Type Diversity, retains Top-K instances
- **Core assumption:** LLM can reliably distinguish high-quality from noisy meta-path instances
- **Evidence anchors:** Ablation study shows performance drop without scorer; meta-path noise identified as challenge in related literature
- **Break condition:** LLM scoring fails if prompts are flawed or model lacks domain knowledge, leading to arbitrary selection

### Mechanism 2
- **Claim:** Similar student histories improve prediction accuracy and provide grounded explanations
- **Mechanism:** Rule-based collaborative filtering retrieves Top-S similar students using 5-dimensional feature vector and Mahalanobis distance
- **Core assumption:** Peer performance is strong predictor when similarity is measured by ability, history overlap, and knowledge state
- **Evidence anchors:** Ablation study shows significant AUC drop (0.9749 to 0.6911 on Slepemapy) without similar student info
- **Break condition:** Similarity features fail to capture true learning patterns, providing confusing context to LLM

### Mechanism 3
- **Claim:** Structured prompt enables accurate predictions and evidence-backed explanations from frozen LLM
- **Mechanism:** Three-block structured prompt (target student history, target question attributes, similar students' info) elicits JSON prediction and analysis report
- **Core assumption:** General-purpose LLM has sufficient reasoning to perform KT from structured context without fine-tuning
- **Evidence anchors:** Framework uses Qwen-Plus without additional fine-tuning
- **Break condition:** Prompt exceeds context window or LLM reasoning is unreliable, causing inconsistent outputs

## Foundational Learning

**Concept:** Heterogeneous Information Networks (HINs)
- Why needed here: Foundational data structure modeling students, questions, and concepts as nodes to capture multi-relational semantics
- Quick check question: Can you explain the difference between a "node" and a "meta-path" in a HIN?

**Concept:** Meta-Paths
- Why needed here: Core operation using predefined patterns (e.g., Q-U-Q) to traverse HIN and capture specific semantic relationships
- Quick check question: In the context of this paper, what does the meta-path Q-U-A-U-Q represent?

**Concept:** Item Response Theory (IRT)
- Why needed here: Psychometric model generating external node attributes (student ability, question difficulty) that enrich HIN
- Quick check question: What two primary parameters for a question are typically estimated by the IRT-2PL model?

## Architecture Onboarding

**Component map:** IRT & Data Processing -> MRHIN Builder -> Meta-Path Sampler -> LLM Scorer -> Similar Student Retriever -> LLM Predictor

**Critical path:** MRHIN Construction -> Meta-Path Sampling -> LLM Scoring -> Similar Student Retrieval -> LLM Prediction. Final prediction quality depends linearly on quality of filtered meta-paths and relevance of retrieved students.

**Design tradeoffs:**
- Accuracy vs. Interpretability: Trades pure black-box optimization for transparent, evidence-backed explanations
- Cost vs. Performance: LLM-based scoring and prediction introduce higher computational cost and latency
- Zero-Shot vs. Fine-Tuning: Avoids fine-tuning costs but places heavy burden on prompt engineering

**Failure signatures:**
- Degraded AUC near baselines indicates failure in similar student retrieval
- Poor explanations suggest failure in meta-path scoring
- Low IRT correlation indicates ineffective node type diversity scoring

**First 3 experiments:**
1. Reproduce ablation on w/o MSR: Compare full model with random K selection to verify scorer's contribution
2. Test similar student retrieval: Manually inspect Top-S similar students for a target student to verify comparable histories/abilities
3. Probe LLM prediction prompt: Feed crafted examples with/without similar student info to gauge direct impact on prediction quality

## Open Questions the Paper Calls Out
- **Computational overhead:** How does LLM-based meta-path scoring scale compared to traditional random walks?
- **Explanation validation:** Can the quality of natural language explanations be quantitatively validated?
- **Prompt robustness:** Is meta-path scoring mechanism robust to variations in prompt design or LLM architecture?

## Limitations
- Zero-shot LLM approach heavily relies on prompt engineering quality and LLM reasoning capability
- Meta-path sampling uses fixed hyperparameters without justification of optimality
- Similarity retrieval depends on potentially ill-conditioned covariance matrices
- "Explainable" analysis is qualitative and not quantitatively validated against human judgment

## Confidence

**High confidence:** Overall architecture design and ablation studies demonstrating contribution of meta-path scorer and similar student retriever

**Medium confidence:** Reported performance improvements over baselines are plausible but exact reproducibility depends on unprovided implementation details

**Low confidence:** Quality and grounding of generated explanations cannot be verified without human evaluation data or qualitative examples from actual experiments

## Next Checks

1. **Replicate ablation study on meta-path scorer:** Run full model and version with random meta-path selection on small dataset to verify performance gap

2. **Verify similar student retrieval relevance:** Manually inspect top-3 similar students for 10 target students across datasets to confirm comparable ability levels and learning trajectories

3. **Test LLM explanation grounding:** Generate predictions and reports for 5-10 target questions with known difficulty levels, verify explanations correctly reference relevant student history and similar students' performance