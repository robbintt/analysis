---
ver: rpa2
title: 'Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling'
arxiv_id: '2509.24403'
source_url: https://arxiv.org/abs/2509.24403
tags:
- scaling
- question
- reasoning
- database
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Agentar-Scale-SQL, a framework that achieves
  state-of-the-art performance on the BIRD text-to-SQL benchmark by orchestrating
  three types of test-time scaling: internal scaling (via RL-enhanced intrinsic reasoning),
  sequential scaling (through iterative refinement), and parallel scaling (via diverse
  synthesis and tournament selection). The method combines a reasoning generator fine-tuned
  with reinforcement learning and an in-context learning generator, producing diverse
  SQL candidates that are refined for syntax and semantics before being selected by
  a learned reasoning selector.'
---

# Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling

## Quick Facts
- arXiv ID: 2509.24403
- Source URL: https://arxiv.org/abs/2509.24403
- Reference count: 40
- State-of-the-art: 81.67% execution accuracy on BIRD benchmark

## Executive Summary
Agentar-Scale-SQL introduces a framework that achieves state-of-the-art performance on the BIRD text-to-SQL benchmark through orchestrated test-time scaling. The system combines three scaling strategies: internal scaling via RL-enhanced reasoning, sequential scaling through iterative refinement, and parallel scaling using diverse candidate generation. By leveraging a reasoning generator fine-tuned with GRPO, an ICL generator for complementary coverage, and a learned tournament selector, the framework produces and selects SQL candidates that achieve 81.67% execution accuracy and 77.00% reward-based valid efficiency score on the official BIRD leaderboard.

## Method Summary
The framework orchestrates three types of test-time scaling: internal scaling via RL-enhanced reasoning (GRPO fine-tuning on execution rewards), sequential scaling through iterative refinement (SQL Fixer/Revisor), and parallel scaling via diverse synthesis (dual generators with tournament selection). The system preprocesses schemas into DDL and Light formats, uses vector stores for retrieval, and employs two generators (reasoning fine-tuned with GRPO and ICL-based) to produce candidates. These candidates undergo iterative refinement and are selected through a learned tournament mechanism rather than majority voting, achieving state-of-the-art results on BIRD.

## Key Results
- Achieves 81.67% execution accuracy on BIRD test set
- Ranks first on official BIRD leaderboard with 77.00% reward-based valid efficiency score
- Reasoning generator contributes largest performance gain (-4.89 EX when removed)
- Learned selection provides +1.82 EX over self-consistency baseline

## Why This Works (Mechanism)

### Mechanism 1: RL-Enhanced Intrinsic Reasoning (Internal Scaling)
Aligning the generator with execution feedback via Reinforcement Learning (RL) appears to be the primary driver of high accuracy, specifically for complex logic. The framework uses GRPO to fine-tune a reasoning model, receiving rewards based on execution correctness rather than just text generation.

### Mechanism 2: Dual-Generator Complementarity (Parallel Scaling)
Combining a "Reasoning" generator (fine-tuned) with an "ICL" generator (prompting) expands coverage of solvable problems. The system leverages two distinct approaches: a deep analytical reasoner using DDL schemas and a broad pattern-matching ICL model using Light schemas.

### Mechanism 3: Learned Selection via Tournament (Selection Scaling)
Replacing majority voting with a learned, pairwise tournament selector allows the system to distinguish between syntactically valid but semantically incorrect candidates. Candidates are grouped by execution result and compared through a fine-tuned selector trained via RL.

## Foundational Learning

**Concept: Group Relative Policy Optimization (GRPO)**
Why needed: This is the engine of "Internal Scaling," optimizing for relative improvement across generated rollouts for SQL execution rewards.
Quick check: How does the advantage calculation in GRPO differ from standard PPO? (Hint: It uses the mean/std of rewards from multiple outputs of the same prompt).

**Concept: Schema Representation (DDL vs. Light)**
Why needed: The paper attributes performance gains to using the right schema for the right model (DDL for code-fine-tuned models, Light for general LLMs).
Quick check: Why would a general-purpose LLM struggle with a raw DDL schema compared to a Markdown "Light" schema? (Hint: Token efficiency and training data composition).

**Concept: Pass@k and Upper Bound Analysis**
Why needed: To evaluate "Parallel Scaling" effectively, understanding the trade-off between generating more candidates and selector accuracy.
Quick check: If Pass@16 is high but final accuracy is low, which component is failing?

## Architecture Onboarding

**Component map:**
Vector Stores (Cells & Examples) + Schema Formatting -> Task Understanding -> Parallel Generation [Reasoning Gen + ICL Gen] -> Iterative Refinement [Fixer/Revisor] -> Selection [Reasoning Selector Tournament] -> Final SQL

**Critical path:** The Reasoning Generator. While the ICL generator adds diversity, ablation studies show the Reasoning Generator (Internal Scaling) contributes the heaviest performance lift (-4.89 if removed).

**Design tradeoffs:**
- Accuracy vs. Latency: Multiple LLM calls make it unsuitable for real-time apps but optimized for high-stakes ChatBI
- Complexity vs. Generalization: Avoided complex heuristics for "Schema Linking," betting that scaling computation is more robust than hand-crafted rules

**Failure signatures:**
- Cascading Hallucination: Wrong cell values in Step 1 lead to both generators producing logically distinct wrong answers
- Tournament Deadlock: Selector relies entirely on RL training when candidates produce distinct but plausible results

**First 3 experiments:**
1. Compare Learned Reasoning Selector against "Self-Consistency" baseline to isolate tournament mechanism value
2. Run test set using only Reasoning Generator vs. only ICL Generator to profile error profiles
3. Plot EX score vs. Number of Candidates (k) to find diminishing returns point

## Open Questions the Paper Calls Out
1. How can "Exercise-Time Scaling" be implemented to shift computational burden from inference to offline pre-computation phase?
2. Can the current workflow-based approach evolve into a fully autonomous agent that doesn't rely on predefined structures?
3. Does the Orchestrated Test-Time Scaling methodology generalize effectively to broader code generation and reasoning tasks?
4. Can substantial computational overhead and latency be reduced to support real-time applications while maintaining high accuracy?

## Limitations
- Requires substantial compute for parallel candidate generation and multiple RL fine-tuning stages
- Relies on proprietary models (GPT-5) creating reproducibility barriers
- Results demonstrated only on BIRD benchmark, performance on other benchmarks unverified
- Execution-based reward signal may not fully capture semantic correctness

## Confidence
- **High Confidence**: Execution accuracy improvements (81.67% EX) and ablation findings showing reasoning generator's critical role
- **Medium Confidence**: Dual-generator complementarity claims (47 unique ICL vs 12 Reasoning correct samples)
- **Medium Confidence**: Selection scaling benefits (+1.82 EX over baseline)

## Next Checks
1. Test Agentar-Scale-SQL on Spider and other text-to-SQL benchmarks to verify generalization beyond BIRD
2. Measure inference time per query across different candidate counts to quantify practical deployment constraints
3. Conduct targeted evaluation on questions where all generated candidates fail to identify systematic weaknesses