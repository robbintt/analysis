---
ver: rpa2
title: 'Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks'
arxiv_id: '2510.27106'
source_url: https://arxiv.org/abs/2510.27106
tags:
- human
- agreement
- judges
- metrics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models (LLMs) are increasingly used as automated\
  \ judges for evaluating natural language generation (NLG) tasks, but their self-consistency\u2014\
  how reliably they rate the same input across multiple runs\u2014remains underexplored.\
  \ This paper measures intra-rater reliability across three NLG benchmarks: SummaC\
  \ (binary summary factuality), SummEval (multi-metric summary ratings), and MT-Bench\
  \ (conversation ranking)."
---

# Rating Roulette: Self-Inconsistency in LLM-As-A-Judge Frameworks

## Quick Facts
- arXiv ID: 2510.27106
- Source URL: https://arxiv.org/abs/2510.27106
- Reference count: 40
- Primary result: LLM judges show consistently low self-consistency (Krippendorff's Alpha often below 0.8), challenging their reliability as automated evaluators.

## Executive Summary
Large language models are increasingly used as automated judges for evaluating natural language generation tasks, but their self-consistency—how reliably they rate the same input across multiple runs—remains underexplored. This paper measures intra-rater reliability across three NLG benchmarks: SummaC (binary summary factuality), SummEval (multi-metric summary ratings), and MT-Bench (conversation ranking). Results show consistently low self-reliability for all tested LLMs (Llama 3.1, DeepSeek-R1, Qwen 3), with Krippendorff's Alpha often well below the 0.8 threshold for good agreement. Agreement with human judges is similarly modest, especially for subjective metrics like fluency. While taking majority votes over multiple runs can improve agreement, simply disabling sampling degrades performance, revealing a trade-off between consistency and accuracy. These findings suggest that LLM-as-a-judge frameworks are not yet reliable substitutes for human evaluation and should incorporate reliability metrics and variance-reduction strategies to produce more robust results.

## Method Summary
The paper evaluates three large language models (Llama-3.1-70B-Instruct, DeepSeek-R1-Distill-Qwen, Qwen3-32B) as automated judges across three NLG benchmarks. Each model is run three times per example with temperature=0.6 and top_p=0.9-0.95, and outputs are parsed into structured ratings. Intra-rater reliability is measured using Krippendorff's Alpha with appropriate distance functions for each task type. The study compares single-run performance against majority-vote aggregation and no-sampling (temperature=0) conditions, measuring both self-consistency and agreement with human annotations.

## Key Results
- All three LLMs achieved Krippendorff's Alpha well below 0.8 on all benchmarks, with worst performance on subjective metrics like fluency
- Qwen 3 gave identical judgments across all three runs for only 61.3% of MT-bench cases
- Majority voting improved agreement with human judgments, but disabling sampling (temperature=0) consistently degraded accuracy
- The best self-reliability achieved was α=0.788 on SummaC binary classification, still below the 0.8 threshold

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM judges exhibit low intra-rater reliability due to stochastic sampling combined with inherently underdefined evaluation criteria.
- Mechanism: When an LLM evaluates the same content multiple times, token-level sampling randomness propagates through the reasoning chain, producing different final judgments. The effect is amplified when evaluation criteria (e.g., "fluency" or "relevance") lack objective ground truth, allowing small perturbations to shift outputs across rating boundaries.
- Core assumption: The inconsistency stems from both sampling and task subjectivity, not solely from model architecture flaws.
- Evidence anchors:
  - [abstract] "LLM judges have low intra-rater reliability in their assigned scores across different runs... This variance makes their ratings inconsistent, almost arbitrary in the worst case"
  - [Section 4] Qwen 3 gave the same judgment on all 3 runs for only 61.3% of MT-bench cases; even the best model achieved Krippendorff's Alpha of 0.563 on MT-bench, well below the 0.8 threshold
  - [corpus] Related work (TrustJudge, arXiv:2509.21117) confirms score-comparison inconsistencies where lower-rated responses outperform higher-scored ones
- Break condition: If task definitions become objectively measurable (e.g., binary factual consistency with clear criteria), self-reliability improves—as seen with Qwen 3 achieving α=0.788 on SummaC binary classification.

### Mechanism 2
- Claim: Aggregating judgments across multiple runs improves alignment with human evaluations.
- Mechanism: Each run samples from the model's implicit judgment distribution. Majority voting or averaging reduces variance by canceling out random deviations, converging toward the model's "true" preference estimate—similar to self-consistency reasoning.
- Core assumption: The LLM's judgment distribution is centered near a meaningful signal, not uniformly random.
- Evidence anchors:
  - [Section 5.1] "accuracy is higher when computed via majority vote than the expected accuracy for a single run... for Deepseek-R1 and Qwen 3, taking the majority vote gives a higher balanced accuracy than the maximum accuracy achieved by a single run"
  - [Section 6.2] "taking an aggregate of the results of multiple runs, like a simple majority vote, can improve agreement with human judgment"
  - [corpus] Related work (Reference-Free Rating, arXiv:2509.24678) confirms single-response ratings are unstable; multiple samples are needed for reliability
- Break condition: If the underlying judgment distribution is bimodal or adversarially manipulated, aggregation may not converge to correct answers.

### Mechanism 3
- Claim: Disabling sampling (greedy decoding) degrades evaluation accuracy despite improving self-consistency.
- Mechanism: Sampling introduces exploration that allows the model to consider diverse reasoning paths. Removing it forces deterministic but potentially suboptimal outputs, sacrificing the "wisdom of the crowd" effect within a single model.
- Core assumption: Beneficial variance exists in the sampling process that contributes to better average judgments.
- Evidence anchors:
  - [Section 5.1, Table 2] Llama 3.1 drops from 59.1% (sampled) to 58.4% (no sampling); Deepseek-R1 drops from 69.8% to 69.3%; Qwen 3 drops from 79.4% to 79.2%
  - [Section 6.2] "trying to eliminate variance entirely by turning off variance hurts performance"
  - [corpus] Weak direct corpus evidence on this specific trade-off; related work focuses on uncertainty quantification rather than the sampling-accuracy relationship
- Break condition: If future models develop more robust internal representations, greedy decoding may achieve parity with sampled outputs.

## Foundational Learning

- Concept: **Krippendorff's Alpha (chance-corrected agreement)**
  - Why needed here: The paper argues that standard metrics (accuracy, correlation) inflate agreement by ignoring chance. Krippendorff's Alpha corrects for base-rate bias and handles ordinal/nominal data.
  - Quick check question: If two raters both label 95% of items as "positive" and agree on 90%, is their agreement impressive? (Answer: No—chance agreement alone yields ~90.5%.)

- Concept: **Intra-rater vs. Inter-rater Reliability**
  - Why needed here: The paper distinguishes self-consistency (same judge, multiple runs) from agreement between different judges. Most prior work only measures inter-rater reliability; self-reliability is overlooked.
  - Quick check question: An LLM achieves 0.85 agreement with human judges but 0.4 self-reliability. Should you trust single-run judgments? (Answer: No—the high human agreement may be luck.)

- Concept: **Temperature Sampling and Token-Level Variance**
  - Why needed here: LLMs use temperature-controlled sampling during generation. This creates run-to-run variability. Understanding this is essential to interpreting why identical prompts yield different ratings.
  - Quick check question: Setting temperature=0 makes outputs deterministic. Why might this hurt evaluation quality? (Answer: It removes beneficial exploration of reasoning paths.)

## Architecture Onboarding

- Component map:
  Evaluation Harness -> Rating Extraction -> Reliability Compute -> Aggregation Module

- Critical path:
  1. Load benchmark (SummaC/SummEval/MT-bench) and filter for examples with ≥2 human annotations
  2. Run each LLM judge 3× per example with temperature=0.6, top_p=0.9-0.95
  3. Extract ratings via prompt parsing
  4. Compute intra-rater α per model, inter-rater α (LLM vs. human), and accuracy
  5. Compare single-run vs. majority-vote vs. no-sampling conditions

- Design tradeoffs:
  - **More runs** → better reliability estimates but higher compute cost (paper found 3 runs sufficient; no gains beyond 5)
  - **Chance-corrected metrics** → more principled but less intuitive than accuracy
  - **Ensemble aggregation** → improves accuracy but obscures individual run variance

- Failure signatures:
  - α < 0.5: Judgments are unreliable; may be near-random for subjective metrics
  - Large std between runs (e.g., Llama 3.1: 59.1±2.06): High variance makes single-run reports misleading
  - Accuracy >> α: Signals inflated agreement due to class imbalance or chance

- First 3 experiments:
  1. Reproduce intra-rater α for a single model on one benchmark (e.g., Qwen 3 on SummaC) to validate setup; expect α≈0.78
  2. Compare majority-vote vs. single-run accuracy on SummEval to confirm aggregation benefit; expect +1-3% improvement
  3. Test temperature=0 condition to verify performance drop; expect consistent degradation across models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do human judges exhibit self-reliability issues comparable to LLMs when re-evaluating the same content, and can this measure serve as a reliable upper bound for LLM performance?
- Basis in paper: [explicit] The authors state, "we do not know if human judgments, which are taken as the gold standard, suffer from the same consistency issues as LLMs, which should prompt further investigations."
- Why unresolved: Existing benchmarks lack repeated annotations from the same human annotator, making it impossible to quantify human intra-rater reliability.
- What evidence would resolve it: A dataset containing multiple, independent evaluation runs from the same human annotators on identical NLG tasks.

### Open Question 2
- Question: Does the presence of explicit reasoning traces in Large Reasoning Models (LRMs) correlate with higher intra-rater reliability compared to standard LLMs?
- Basis in paper: [explicit] In the Limitations section, the authors note, "we have not explored any relationships between their reasoning traces and their self-reliability" for models like Deepseek-R1.
- Why unresolved: The study treated reasoning models as black-box judges without analyzing the consistency or content of their intermediate reasoning steps.
- What evidence would resolve it: An analysis comparing the consistency of reasoning paths across multiple runs and correlating them with final score stability.

### Open Question 3
- Question: Can probing model internals identify specific layers responsible for score divergence in conflicting runs, and to what extent does prompt structure mitigate this?
- Basis in paper: [explicit] The authors list "investigating the impact of prompt structure on reliability and applying probing the model internals" as specific avenues for future work.
- Why unresolved: The current methodology evaluates LLMs based solely on output behavior, ignoring internal activations or fine-grained prompt sensitivities.
- What evidence would resolve it: Layer-wise probing experiments to detect where representations diverge during inconsistent generations, alongside systematic prompt ablation studies.

### Open Question 4
- Question: To what degree does annotator training or expertise affect self-reliability in human judges compared to crowdsourced workers?
- Basis in paper: [explicit] The authors recommend it is "important to explore how much of an effect training or expertise has on self-reliability."
- Why unresolved: While the paper notes that experts and crowdworkers assign different ratings, it lacks the data to assess which group is more consistent over time.
- What evidence would resolve it: A comparative study measuring the intra-rater reliability of expert annotators versus untrained crowdworkers.

## Limitations
- The study only tested three LLMs on three specific benchmarks, limiting generalizability to other model families or evaluation tasks.
- The temperature=0.6 setting was chosen heuristically without systematic exploration of the full temperature-consistency-accuracy tradeoff space.
- The paper assumes that low self-consistency is inherently problematic, but doesn't fully explore whether some variance might be appropriate for inherently subjective evaluation criteria.

## Confidence

- **High Confidence**: The measurement of intra-rater reliability using Krippendorff's Alpha across multiple benchmarks is methodologically sound and the results are reproducible. The finding that α values fall well below 0.8 is unambiguous.
- **Medium Confidence**: The interpretation that low self-consistency represents a fundamental limitation requiring reliability metrics and variance-reduction strategies is reasonable but assumes these are desirable properties rather than inherent features of subjective evaluation.
- **Low Confidence**: The claim that aggregation strategies can meaningfully improve agreement with human judgment is supported but the magnitude of improvement and its generalizability to other evaluation contexts needs further validation.

## Next Checks

1. **Temperature Sweep Experiment**: Systematically vary temperature from 0.0 to 1.0 and measure the full reliability-accuracy tradeoff curve for each model/benchmark combination to identify optimal sampling regimes.

2. **Prompt Sensitivity Analysis**: Test whether prompt wording variations produce larger reliability differences than sampling randomness alone, helping distinguish between inherent task subjectivity and evaluation methodology issues.

3. **Longitudinal Stability Test**: Run the same evaluation suite monthly over 6-12 months to assess whether reliability patterns are stable across model updates and whether certain evaluation practices consistently outperform others.