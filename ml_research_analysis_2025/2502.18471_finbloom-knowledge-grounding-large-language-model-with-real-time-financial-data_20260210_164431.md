---
ver: rpa2
title: 'FinBloom: Knowledge Grounding Large Language Model with Real-time Financial
  Data'
arxiv_id: '2502.18471'
source_url: https://arxiv.org/abs/2502.18471
tags:
- financial
- data
- arxiv
- dataset
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Financial Agent to address the challenge
  of real-time data access for large language models (LLMs) in finance. The Financial
  Agent uses a knowledge-grounding approach by combining a finetuned LLM (FinBloom
  7B) with a data module containing real-time financial news and tabular data.
---

# FinBloom: Knowledge Grounding Large Language Model with Real-time Financial Data

## Quick Facts
- arXiv ID: 2502.18471
- Source URL: https://arxiv.org/abs/2502.18471
- Reference count: 37
- FinBloom 7B achieves 0.3464 average F1 score on FinBen benchmark

## Executive Summary
FinBloom introduces a Financial Agent architecture that grounds large language models in real-time financial data without frequent retraining. The system combines a finetuned Bloom 7B model trained on 14 million financial news articles and 12 million SEC filings with a data module containing real-time news and tabular financial data. The Financial Agent translates natural language queries into structured data requests, enabling efficient retrieval and contextualization for downstream LLM processing. This approach achieves significantly higher performance than existing financial LLMs on the FinBen benchmark while maintaining low latency suitable for real-time financial decision-making.

## Method Summary
The approach involves two key training phases: (1) pre-training FinBloom 7B on a corpus of 14 million Reuters/DPA news articles and 25% of 12 million SEC filings using QLoRA parameter-efficient fine-tuning, and (2) finetuning the pre-trained model on a custom Financial Context Dataset of 50,000 query-to-structured-request pairs using Prompt Tuning. The Financial Agent extracts relevant financial entities, metrics, and temporal ranges from user queries and generates machine-readable structured data requests. These requests are processed by a Data Module containing semantic search over real-time news and tabular financial data, which returns contextual information enriched with related metrics. The enriched context is then fed to a main LLM for final response generation.

## Key Results
- FinBloom 7B achieves 0.3464 average F1 score on FinBen benchmark across 25 datasets
- Outperforms FinMA-7B (0.3244), FinGPT (0.2136), and CFGPT (0.1652) on the same benchmark
- Financial Agent achieves BLEU score of 0.9614 and ROUGE-L of 0.9771 on 10,000 test samples
- Enables real-time financial data retrieval without frequent model retraining
- Achieves low latency suitable for high-velocity financial data processing

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific pre-training on financial corpora improves context identification for queries. Training on 14M news articles and 12M SEC filings exposes the model to financial terminology, metric interdependencies, and domain-specific language patterns. This creates internal representations that map natural language queries to relevant financial concepts more accurately than general-purpose models.

### Mechanism 2
Fine-tuning on structured query-to-data-request pairs enables translation from natural language to machine-readable retrieval instructions. The Financial Context Dataset contains 50K examples mapping user queries → "Required Data" (companies, metrics, dates) → "Structured Data Request" (formalized format). Fine-tuning on this teaches the model to extract entities, identify metrics and related metrics, and specify temporal ranges.

### Mechanism 3
Decoupling real-time data access from model parameters enables low-latency grounding without retraining. The architecture separates concerns: (1) Data Module maintains streaming tabular/news data, (2) Financial Agent generates structured requests, (3) Data Module retrieves and formats data, (4) Context-enriched query goes to main LLM. This avoids parameter updates for new data.

## Foundational Learning

- **Retrieval-Augmented Generation (RAG)**: Why needed here: FinBloom's architecture is a variant of RAG designed for high-velocity financial data. Understanding standard RAG helps identify where this approach differs (specialized agent for context generation, dual tabular/text repositories). Quick check question: Can you explain why traditional RAG struggles with tabular financial data according to Section 2.2?

- **Parameter-Efficient Fine-Tuning (QLoRA/Prompt Tuning)**: Why needed here: FinBloom uses QLoRA for domain pre-training and Prompt Tuning for agent fine-tuning. Understanding these methods explains how 7B models are trained with limited compute. Quick check question: What is the difference between full parameter fine-tuning and prompt tuning, and why might the latter be preferred for the Financial Agent?

- **Financial Metric Interdependencies**: Why needed here: The "Related Metrics" design (e.g., P/E ratio requires share price AND earnings per share) is central to how the dataset and agent work. Without this understanding, you may not grasp why the dataset includes related metrics. Quick check question: Given a query about "return on equity," what related metrics would you include in a structured data request?

## Architecture Onboarding

- **Component map**: User Query → Financial Agent (FinBloom 7B fine-tuned) → Structured Data Request → Data Module (Tabular DB + News DB) → Retrieved Data → Text Conversion → Enriched Query → Main LLM → Response

- **Critical path**:
  1. Financial Agent accuracy in generating structured requests (BLEU/ROUGE metrics)
  2. Data Module retrieval precision (semantic matching quality)
  3. Context window management (fitting retrieved data + query into main LLM)

- **Design tradeoffs**:
  - Agent size vs. latency: 7B parameter model balances capability with inference speed for real-time use
  - Template-based vs. organic queries: Template generation ensures coverage but may miss edge-case phrasings
  - Human-curated vs. automated related metrics: Human curation ensures accuracy but doesn't scale

- **Failure signatures**:
  - Agent outputs empty or malformed structured requests → check if query type matches training distribution
  - Data Module returns no data → verify date ranges and entity name matching
  - Main LLM ignores context → check if context is properly formatted in the enriched prompt

- **First 3 experiments**:
  1. Ablation on related metrics: Run the agent with/without the "Related Metrics" field to measure impact on downstream response quality
  2. Out-of-template query testing: Create 100 queries that deviate from the 5K templates to measure generalization bounds
  3. Retrieval noise tolerance: Inject irrelevant news articles into retrieved context and measure degradation in main LLM response accuracy

## Open Questions the Paper Calls Out
- Future research could expand this framework into a multi-agent system, integrating additional agents such as financial video analysis tools to further enhance its capabilities.

## Limitations
- Template-based dataset construction may create distributional bias limiting generalization to organic queries
- 7B parameter model may lack reasoning depth for complex multi-step financial analysis
- No extensive testing on adversarial or out-of-distribution query types

## Confidence

- **High Confidence**: Architecture's core mechanism (decoupling real-time data access from model parameters) is well-established; empirical results are clearly reported with appropriate baselines
- **Medium Confidence**: Effectiveness of domain-specific pre-training is supported by relative performance gains, but true impact remains partially uncertain without ablation studies
- **Low Confidence**: Generalizability of template-based dataset to real-world financial queries is uncertain without systematic testing on non-template questions

## Next Checks

1. **Out-of-Distribution Testing**: Evaluate the Financial Agent on 100+ hand-crafted queries that deviate significantly from template structure to measure actual generalization capability

2. **Ablation on Related Metrics**: Remove the "Related Metrics" component from structured requests and measure impact on downstream response quality to quantify claimed improvement

3. **Retrieval Robustness Testing**: Systematically inject irrelevant news articles into retrieved context at varying proportions (10%, 25%, 50%) and measure degradation in main LLM response accuracy to establish noise tolerance threshold