---
ver: rpa2
title: 'Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated
  Survey'
arxiv_id: '2506.04461'
source_url: https://arxiv.org/abs/2506.04461
tags:
- systematicity
- systematic
- arxiv
- representations
- compositionality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey highlights a critical distinction in systematicity
  research: while Fodor and Pylyshyn argue for systematicity of internal representations,
  existing benchmarks primarily test behavioural systematicity. The authors analyze
  key benchmarks in language and vision using Hadley''s taxonomy, finding that many
  only test weak or quasi-systematicity rather than strong systematicity that approximates
  human capabilities.'
---

# Behavioural vs. Representational Systematicity in End-to-End Models: An Opinionated Survey

## Quick Facts
- arXiv ID: 2506.04461
- Source URL: https://arxiv.org/abs/2506.04461
- Reference count: 20
- This survey distinguishes between behavioural systematicity (task performance) and representational systematicity (internal structure), finding most benchmarks only test weak/quasi-systematicity rather than strong systematicity that approximates human capabilities.

## Executive Summary
This survey analyzes the critical distinction between behavioural and representational systematicity in machine learning benchmarks. While Fodor and Pylyshyn argue that true systematicity requires structured internal representations, existing benchmarks primarily test whether models can achieve systematic behavior without necessarily possessing systematic representations. The authors apply Hadley's taxonomy to classify benchmarks into weak, quasi, and strong systematicity levels, finding that many only test weak or quasi-systematicity. They argue that demonstrating systematic behavior doesn't guarantee systematic representations, and emphasize the need for mechanistic interpretability to understand what representations actually drive model performance. The survey concludes that progress toward human-like systematic generalization requires appropriate behavioural claims backed by rigorous mechanistic interpretability.

## Method Summary
The paper provides a theoretical analysis of systematicity in machine learning, applying Hadley's (1994) taxonomy to classify existing benchmarks in language and vision. The authors analyze train/test splits of major benchmarks like SCAN, COGS, and CLEVR to determine which level of systematicity they test. They also review mechanistic interpretability studies that probe whether models develop systematic representations. No experimental procedure is reproduced; this is a conceptual survey that synthesizes theoretical arguments and empirical findings from the literature.

## Key Results
- Most existing benchmarks only test weak or quasi-systematicity, not strong systematicity that approximates human generalization capabilities
- Models can achieve benchmark performance through non-systematic shortcuts (memorization, heuristics) without developing structured representations
- Demonstrating systematic behavior does not guarantee systematic internal representations
- Mechanistic interpretability is essential for verifying whether systematic representations actually drive performance
- Claims of strong behavioural systematicity require custom-trained models due to unknown pre-training exposure

## Why This Works (Mechanism)

### Mechanism 1: Operationalisation Gap Between Behaviour and Representation
- Claim: Observable systematic behaviour does not guarantee systematic internal representations
- Mechanism: Models can achieve benchmark performance through non-systematic shortcuts (memorization, heuristics, entropy detection) without developing structured representations that compose constituents systematically
- Core assumption: Fodor & Pylyshyn's position that true systematicity requires structure-sensitive operations over symbols
- Evidence anchors:
  - [abstract]: "while Fodor and Pylyshyn argue for systematicity of internal representations, existing benchmarks primarily test behavioural systematicity"
  - [section 3.2]: "A model might achieve strong performance on systematicity benchmarks through mechanisms that do not involve systematic internal representations, such as memorization or task-specific heuristics"
  - [corpus]: Limited direct corpus validation; "Explainability Through Systematicity" discusses systematicity as a broader ideal but does not empirically test the behavioural/representational gap
- Break condition: When mechanistic interpretability demonstrates that systematic representations are causally responsible for the observed behaviour (would overcome F&P's challenge)

### Mechanism 2: Hadley's Taxonomy as Benchmark Validation Framework
- Claim: Many existing benchmarks only test weak or quasi-systematicity, not the strong systematicity that approximates human capability
- Mechanism: The taxonomy (weak → quasi → strong) provides testable criteria based on whether words appear in novel syntactic positions during testing that were never seen during training—in any clause depth
- Core assumption: Strong systematicity (processing words in entirely novel syntactic positions) is the appropriate target for human-like generalization
- Evidence anchors:
  - [section 2.2]: "To exhibit strong systematicity, the system must be able to process novel sentences, including those with embedded clauses, even when words appear in syntactic positions that they did not occupy in the training data"
  - [section 4.1]: On COGS/ReCOGS—"None of these transformations alter the meanings of the LFs... nevertheless, ReCOGS evokes much better performance... thus, it is hard to draw any conclusions as to the true systematic generalization capabilities"
  - [corpus]: "Compositional-ARC" assesses systematic generalization in abstract spatial reasoning; corpus evidence on Hadley's taxonomy specifically is limited
- Break condition: When a benchmark explicitly controls for novel syntactic positions at multiple recursion depths and demonstrates passing requires strong systematicity

### Mechanism 3: Mechanistic Interpretability for Representation Verification
- Claim: Probing and circuit analysis can reveal whether systematic representations exist and are causally used, but probing alone is insufficient
- Mechanism: Linear/non-linear probing can identify decodable concepts; causal interventions (ablation, activation patching) are required to verify representations are actually used for task performance
- Core assumption: Decodable representations that are causally necessary indicate genuine systematic representations rather than epiphenomenal byproducts
- Evidence anchors:
  - [section 5.1]: "Feng and Steinhardt (2024) and Feng et al. (2024) discover 'binding vectors' that sufficiently large LLMs use to bind entities"
  - [section 5.2]: "the ability to decode a given property from a model's representations does not guarantee that the model causally relies on this property"
  - [corpus]: "Systematic Abductive Reasoning via Diverse Relation Representations" discusses diverse relation representations but does not directly validate causal interpretability methods
- Break condition: When probing reveals representations but causal intervention shows they're not used for OOD generalization (as in Kobayashi et al. findings)

## Foundational Learning

- **Concept: Compositionality vs. Systematicity**
  - Why needed here: The paper argues systematicity is a specific aspect of compositionality; conflating them leads to misaligned benchmark claims
  - Quick check question: Can you explain why "John loves Mary" → "Mary loves John" tests systematicity specifically, not just compositionality?

- **Concept: Competence vs. Performance (Chomsky)**
  - Why needed here: The behavioural/representational distinction maps to this classical cognitive science distinction; understanding it clarifies why passing benchmarks ≠ possessing the underlying capability
  - Quick check question: If a model fails a systematicity benchmark, does that prove it lacks systematic representations? What's the alternative explanation?

- **Concept: The Binding Problem**
  - Why needed here: Core challenge for systematic representations—how to represent relationships between entities such that the representation generalizes to novel combinations
  - Quick check question: Why is representing "the red square" compositionally harder for neural networks than representing "red" and "square" as independent features?

## Architecture Onboarding

- **Component map**:
  - Benchmark design (train/test distribution control) → Behavioural evaluation level (weak/quasi/strong)
  - Probing methods (linear probes, sparse autoencoders) → Representation discovery
  - Causal interventions (ablation, activation patching) → Verification that representations are used
  - Function vectors / binding mechanisms → Potential systematic representation implementations

- **Critical path**:
  1. Define target systematicity level using Hadley's taxonomy
  2. Design benchmark with explicit train/test controls for that level (track which syntactic positions each word appears in)
  3. Train model from scratch (pre-trained models have unknown training exposure)
  4. Evaluate behavioural performance
  5. Apply mechanistic interpretability to identify candidate systematic representations
  6. Perform causal interventions to verify representations drive behaviour

- **Design tradeoffs**:
  - Synthetic benchmarks (controlled distributions, clear strong-systematicity tests) vs. naturalistic benchmarks (ecological validity, but unknown training exposure)
  - Weak/quasi-systematicity tests (easier to construct and pass) vs. strong systematicity tests (harder, but more meaningful claims)
  - Probing-only analysis (scalable, but epiphenomenal risk) vs. full causal intervention (rigorous, but labour-intensive)

- **Failure signatures**:
  - Benchmark performance jumps dramatically after minor output format changes (e.g., COGS → ReCOGS) without meaning change—indicates spurious correlations in original benchmark
  - High probing accuracy but ablation shows no performance drop—representation is decodable but not used
  - In-distribution generalization succeeds but OOD generalization fails despite decodable latent structure—model learned workaround

- **First 3 experiments**:
  1. Map existing systematicity benchmarks to Hadley's taxonomy by analyzing whether test examples contain words in syntactic positions absent from training data at each recursion depth
  2. Train a Transformer on a controlled strong-systematicity task, then apply probing + causal ablation to determine whether systematic representations emerge and are used
  3. Compare a model that passes behavioural systematicity tests against one augmented with explicit compositional mechanisms (e.g., hypernetwork taking decoded latents as input) on OOD generalization to isolate whether representational systematicity drives performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the evaluation of systematicity change under alternative syntactic theories like Tree-Adjoining Grammars or Combinatory Categorial Grammar?
- Basis in paper: [explicit] The authors state that a full treatment of systematicity under alternative syntactic theories represents an "important direction for future work" beyond the phrase structure grammar framework used in their analysis.
- Why unresolved: Current benchmarks and analyses predominantly assume phrase structure grammar, potentially overlooking systematicity dimensions relevant to other syntactic formalisms.
- What evidence would resolve it: Development of benchmarks or theoretical frameworks that map Hadley's levels of systematicity to alternative grammatical architectures.

### Open Question 2
- Question: How can researchers verify that systematic representations are causally utilized by the model rather than merely being decodable via probing?
- Basis in paper: [explicit] The survey notes that probing evidence is mixed and criticized because "the ability to decode a given property... does not guarantee that the model causally relies on this property."
- Why unresolved: Models may achieve behavioural systematicity using non-systematic shortcuts while systematic representations remain dormant or unused during out-of-distribution tasks.
- What evidence would resolve it: Causal intervention studies (e.g., activation patching or ablation) demonstrating that specific representations are functionally necessary for systematic generalization.

### Open Question 3
- Question: How can strong behavioural systematicity be reliably assessed in large-scale pre-trained models without knowledge of their training data?
- Basis in paper: [explicit] The authors conclude that claims of strong behavioural systematicity "may need to be limited to custom trained models" because researchers are "blind to what they see during training."
- Why unresolved: Hadley's strong systematicity requires testing words in syntactic positions unseen in training; this is impossible to verify if the pre-training corpus is unknown or contaminated.
- What evidence would resolve it: Methods for detecting data contamination or new evaluation protocols that guarantee novelty relative to a model's training distribution.

## Limitations

- The analysis relies heavily on author judgment rather than formal criteria for classifying benchmarks
- The connection between systematic representations and human-like generalization remains largely theoretical
- Claims about mechanistic interpretability lack specific probe architectures and evaluation thresholds
- Unknown pre-training exposure makes strong systematicity claims difficult for large-scale models

## Confidence

**High Confidence**: The distinction between behavioural and representational systematicity is well-established in the literature, with clear theoretical grounding in Fodor and Pylyshyn's original arguments and supported by empirical examples of models passing benchmarks through non-systematic mechanisms.

**Medium Confidence**: The classification of specific benchmarks using Hadley's taxonomy is methodologically sound, but depends on detailed examination of train/test splits that may not be fully documented for all benchmarks. The conclusion that many benchmarks only test weak/quasi-systematicity appears reasonable but requires careful case-by-case verification.

**Low Confidence**: Claims about the relationship between systematic representations and human-like generalization capabilities remain largely theoretical. While the survey identifies this as a critical research direction, empirical validation that systematic representations drive human-like generalization in end-to-end models is limited.

## Next Checks

1. **Benchmark Classification Validation**: For each major benchmark analyzed (SCAN, COGS, COVR, etc.), formally document whether test examples contain words in syntactic positions absent from training data at each recursion depth, using a standardized checklist derived from Hadley's taxonomy.

2. **Mechanistic Interpretability Replication**: Select one benchmark that claims to test strong systematicity, train a model from scratch on controlled data, then apply probing + causal ablation to determine whether systematic representations emerge and are causally responsible for OOD generalization.

3. **Representation-Behavior Causality Test**: Compare a model that passes behavioural systematicity tests against one augmented with explicit compositional mechanisms (e.g., hypernetwork taking decoded latents as input) on OOD generalization to isolate whether representational systematicity drives performance.