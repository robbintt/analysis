---
ver: rpa2
title: Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive
  Ability
arxiv_id: '2503.04111'
source_url: https://arxiv.org/abs/2503.04111
tags:
- generalization
- networks
- network
- then
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of understanding why over-parameterized
  deep neural networks generalize well, despite classical uniform generalization bounds
  failing to explain this phenomenon. The authors propose a new approach based on
  the expressive ability of neural networks that minimize or approximately minimize
  empirical risk.
---

# Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive Ability

## Quick Facts
- **arXiv ID**: 2503.04111
- **Source URL**: https://arxiv.org/abs/2503.04111
- **Reference count**: 40
- **Primary result**: Proposes expressive ability framework explaining generalization of over-parameterized networks via lower bounds dependent on ratio of data expression width to model width.

## Executive Summary
This paper addresses the fundamental question of why over-parameterized deep neural networks generalize well despite classical uniform generalization bounds predicting overfitting. The authors introduce a novel framework based on the "expressive ability" of networks that minimize empirical risk. The key insight is that generalization depends not on the raw size of the model, but on the ratio between the minimum width required to express the data distribution (W_0) and the actual network width (W). This framework explains several empirical phenomena in deep learning and provides new theoretical insights into the role of over-parameterization and loss function choice.

## Method Summary
The paper establishes a new generalization bound based on the expressive ability of two-layer ReLU networks minimizing empirical risk. The method involves defining a hypothesis space H_W with weight clipping constraint [-1, 1], introducing the concept of "expressive width" W_0 as the minimum width required to express the data distribution with confidence c, and proving that population accuracy is lower-bounded by terms involving the ratio W_0/W. The experiments validate these theoretical predictions using MNIST dataset with two-layer fully connected networks, varying network width and training data volume while enforcing the weight clipping constraint.

## Key Results
- Sample complexity bound shows N ≥ Ω(W_0^2) and W ≥ Ω(W_0) are sufficient for high population accuracy
- Over-parameterization improves generalization when W >> W_0 through the W_0/W term in the bound
- "Bad" loss functions can cause 50% accuracy regardless of width or data abundance
- Robust generalization requires higher expressive cost (W_0) than standard generalization

## Why This Works (Mechanism)

### Mechanism 1: Expressive Width Ratio Bound
The population accuracy of a network minimizing empirical risk is lower-bounded by a term containing the ratio of the "minimum expressive width" (W_0) to the actual width (W). The authors prove that generalization error scales with O(W_0/W) rather than solely with model parameters. As the actual network width W grows large relative to the minimum width W_0 required to express the data distribution, this error term diminishes. This theoretically decouples generalization from the raw parameter count, explaining why over-parameterized models can generalize. Core assumption: The data distribution D can be expressed by a two-layer network of width W_0 with confidence c.

### Mechanism 2: Distribution-Dependent Sample Complexity
The required number of training samples N is primarily determined by the intrinsic complexity of the data distribution (expressive cost W_0), not the size of the over-parameterized model (W). The paper establishes a lower bound where generalization fails if N is less than the VC-dimension of the hypothesis space needed to represent the distribution. For ReLU networks, this implies N must generally exceed Ω(W_0), irrespective of the actual model size W. Core assumption: The hypothesis space must be able to shatter the specific data points representing the distribution boundaries.

### Mechanism 3: Loss Function Geometry Constraint
The choice of loss function acts as a hard constraint on generalization; "bad" loss functions can prevent generalization regardless of width or data abundance. Minimizing "bad" losses (e.g., strictly decreasing concave functions or those with finite minima like MSE in specific contexts) may yield networks that minimize empirical risk but fail to capture the correct classification margin on the population. The geometry of the loss pushes the optimizer toward solutions that do not align with the expressive confidence c. Core assumption: The loss function determines the landscape of empirical risk minimizers.

## Foundational Learning

- **Concept: Rademacher Complexity & VC-Dimension**
  - **Why needed here:** The paper explicitly contrasts its bounds against these classical measures. Understanding that classical theory predicts over-parameterized models should overfit (high VC-dim) is necessary to appreciate why the authors introduce "expressive ability" (W_0) as a superior metric.
  - **Quick check question:** Why does the classical VC-dimension bound fail to explain the success of over-parameterized networks?

- **Concept: Empirical Risk Minimization (ERM)**
  - **Why needed here:** The theorems apply specifically to networks F ∈ M, defined as those minimizing empirical risk. The results are conditional on the network actually finding or approximating this minimum.
  - **Quick check question:** Does the paper guarantee that SGD *finds* the global minimum, or does it assume the network F is already in the set of minimizers?

- **Concept: Expressive Ability (Width W_0)**
  - **Why needed here:** This is the central parameter of the paper's theory. It represents the complexity of the *data distribution* rather than the model.
  - **Quick check question:** If a distribution requires width W_0=100 to express, how wide does the paper suggest your actual network W should be to ensure generalization?

## Architecture Onboarding

- **Component map:** Hypothesis Space (H_W) -> Expressive Cost (W_0) -> Loss Function -> Population Accuracy
- **Critical path:**
  1. Estimate expressive cost W_0 (or ensure it is small relative to available resources)
  2. Select actual network width W >> W_0 (specifically W ≥ Ω(W_0))
  3. Ensure sample size N ≥ Ω(W_0^2)
  4. Train using ERM with cross-entropy

- **Design tradeoffs:**
  - **Over-parameterization vs. Efficiency:** Increasing W improves the W_0/W term in the generalization bound, but offers diminishing returns once W >> W_0
  - **Robustness vs. Standard Accuracy:** The paper notes that robust generalization requires higher expressive cost (W_0) than standard generalization. Designing for robustness requires significantly more capacity and data
  - **Assumption:** The theory relies on the existence of a "positive separation bound" in the data distribution (Definition 3.1)

- **Failure signatures:**
  - **Robustness Failure:** The network generalizes well but lacks robustness. Likely cause: The expressive cost for robust features (W_0^robust) is much higher than for standard features, and the current width W is insufficient for the robust task
  - **Stagnation:** Increasing data N does not improve accuracy. Likely cause: The network width W is too small relative to W_0 (bottlenecked by the W_0/W term)
  - **Random Baseline:** Model sits at 50% accuracy despite training. Likely cause: Use of a "bad" loss function (Theorem 6.8) or data lacking the separation property

- **First 3 experiments:**
  1. **Vary Width (W):** Train networks of increasing width on a fixed dataset (N). Verify if accuracy saturates/improves as predicted by the W_0/W term, rather than degrading due to overfitting
  2. **Vary Data (N):** For a fixed large width W, vary dataset size. Confirm that performance scales with √(N) until hitting the ceiling imposed by the W_0/W term
  3. **Loss Function Ablation:** Compare Cross-Entropy vs. a "bad" loss (e.g., specific concave strictly decreasing functions) on a synthetic distribution to validate the breakdown predicted in Theorem 6.8

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the generalization bounds for deep neural networks (depth > 2) be refined to be polynomial in network parameters rather than exponential?
  - Basis in paper: [explicit] Appendix K states, "this bound is relatively loose, and how to obtain a bound that is polynomial in W_0, D_0, c is an important question."
  - Why unresolved: The preliminary extension to deep networks provided in the appendix yields an exponential dependence (e^(-WD)), which fails to explain the efficiency of deep learning compared to the tight bounds derived for two-layer networks.
  - What evidence would resolve it: A theoretical derivation of a sample complexity bound for multi-layer networks where the error term decreases polynomially with width and depth, or a proof that such polynomial bounds are impossible under the current expressive ability framework.

- **Open Question 2:** How can the minimum expressive capacity (width W_0 and confidence c) required to represent a data distribution be practically estimated for use in sample complexity calculations?
  - Basis in paper: [explicit] The "Limitation and future work" section states, "A more accurate estimate of the cost required to represent a given data distribution is needed."
  - Why unresolved: The theoretical bounds provided (e.g., Theorem 4.4) depend on the parameters W_0 and c, but determining the "expression cost" of an arbitrary real-world distribution is currently an abstract challenge without a defined algorithmic solution.
  - What evidence would resolve it: An algorithm or statistical method to estimate W_0 for standard benchmark distributions, or theoretical bounds relating W_0 to measurable properties of the data density (e.g., Lipschitz constants or noise levels).

- **Open Question 3:** What distributional characteristics determine the optimal activation function for minimizing sample complexity based on expressive ability?
  - Basis in paper: [inferred] Section 5.2 demonstrates that different activation functions (ReLU vs. sin(πx)) require vastly different widths (W_0) to express the same distribution, but Remark 5.8 notes that "we cannot provide a general conclusion for any distribution."
  - Why unresolved: While the paper provides specific counter-examples showing advantages for periodic functions on specific distributions, there is no general rule linking data structure (e.g., frequency, sparsity) to the activation function that minimizes expression cost.
  - What evidence would resolve it: A theoretical framework characterizing which activation functions minimize W_0 for specific classes of data distributions (e.g., smooth manifolds vs. discrete sets), validated by experiments comparing sample complexity.

## Limitations

- The theoretical framework relies on idealized assumptions including strict weight clipping to [-1, 1], which may not reflect typical training dynamics
- The analysis focuses on two-layer networks, which may not capture the complexity of modern deep architectures
- The definition of "expressive ability" W_0 as a distribution property is theoretically elegant but challenging to estimate in practice
- The separation between model capacity and data complexity, while theoretically sound, may not account for all real-world factors affecting generalization

## Confidence

- **High Confidence**: The core mechanism linking generalization to the ratio W_0/W (Mechanism 1) and the identification of loss function geometry as a constraint (Mechanism 3) are well-supported by the theoretical proofs and experimental validation on MNIST
- **Medium Confidence**: The sample complexity bound dependent on W_0 (Mechanism 2) is theoretically rigorous but its practical implications depend heavily on accurate estimation of W_0, which is non-trivial for complex distributions
- **Low Confidence**: The extension of these theoretical insights to deep networks with multiple layers and the practical estimation of expressive cost for real-world datasets remain largely unexplored

## Next Checks

1. **Empirical Validation on Modern Architectures**: Test whether the W_0/W generalization principle extends to deep convolutional networks on standard benchmarks (CIFAR-10/100) beyond the two-layer MNIST experiments

2. **Expressive Cost Estimation**: Develop and validate practical methods for estimating W_0 for real-world datasets, moving beyond the theoretical definition to enable actionable insights

3. **Loss Function Landscape Analysis**: Conduct a systematic study of how different loss functions (beyond the "bad" examples identified) affect the geometry of the empirical risk minimizer and its generalization properties