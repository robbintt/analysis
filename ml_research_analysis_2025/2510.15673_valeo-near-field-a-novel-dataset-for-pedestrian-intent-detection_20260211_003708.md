---
ver: rpa2
title: 'Valeo Near-Field: a novel dataset for pedestrian intent detection'
arxiv_id: '2510.15673'
source_url: https://arxiv.org/abs/2510.15673
tags:
- pedestrian
- dataset
- pose
- data
- vehicle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces the Valeo Near-Field dataset, a novel resource\
  \ for detecting pedestrians\u2019 intentions as they approach a parked vehicle.\
  \ It includes synchronized multimodal data\u2014fisheye camera feeds, LiDAR scans,\
  \ ultrasonic sensors, and motion capture-based 3D body poses\u2014collected across\
  \ real-world indoor and outdoor environments."
---

# Valeo Near-Field: a novel dataset for pedestrian intent detection

## Quick Facts
- arXiv ID: 2510.15673
- Source URL: https://arxiv.org/abs/2510.15673
- Reference count: 31
- Primary result: Introduces VNF dataset with synchronized multimodal data for near-field pedestrian perception, showing 3D positioning error of 0.32 m (±0.08) and pose estimation MPJPE of 180-201 mm

## Executive Summary
The Valeo Near-Field dataset introduces synchronized multimodal sensor data for pedestrian intent detection near parked vehicles. It includes fisheye camera feeds, LiDAR scans, ultrasonic sensors, and motion capture-based 3D body poses collected across real-world indoor and outdoor environments. The dataset features detailed annotations of 3D body joint positions synchronized with camera images and accurate 3D pedestrian positions extracted from LiDAR data. A subset of 51 sequences is publicly released to support research on pedestrian detection, 3D pose estimation, and intent prediction.

## Method Summary
The dataset combines four fisheye cameras, four LiDAR sensors, ultrasonic sensors, and motion capture suits to collect synchronized multimodal data around parked vehicles. Motion capture data is corrected for drift using periodic LiDAR-based manual annotations at 3 fps. The baseline pipeline uses YOLOX for detection, fisheye-adapted ViTPose for 2D keypoints, geometry-driven attention for 3D uplift, and transformer-based methods for final 3D localization. The system processes data in a modular fashion, with detection, pose estimation, and localization handled by separate components.

## Key Results
- Average 3D pedestrian positioning error of 0.32 m (±0.08)
- 0.86 m accuracy in the 5–10 m zone
- Skeleton pose estimation errors (MPJPE) of 201 mm (±32) at 0–5 m and 180 mm (±44) at 5–10 m
- Height estimation errors averaging 18.1 cm (±9.8) at close range, rising to 21.25 cm (±11.3) at 15–20 m

## Why This Works (Mechanism)

### Mechanism 1
Multi-view fisheye triangulation improves 3D pedestrian localization accuracy when pedestrians are visible in overlapping camera fields of view. Four fisheye cameras provide overlapping coverage, and when a pedestrian appears in multiple views, independent 3D pose estimates are associated via bipartite graph matching, with line-of-sight intersection refining position estimates. Accuracy in the 5–10 m zone is higher than in the 0–5 m zone because pedestrians in the 5–10 m range are often visible to two cameras, whereas very close pedestrians are typically captured by only one. Break condition: Open vehicle doors invalidate calibration; single-camera visibility at 0–5m range degrades accuracy (observed: 0.62 vs 0.86 accuracy).

### Mechanism 2
Motion capture suit data, corrected via periodic LiDAR-based manual annotation, provides sufficiently accurate 3D pose ground truth despite open-loop positional drift. The MVN Awinda suit operates open-loop, accumulating positional error over time. Manual LiDAR annotations at 3 fps provide position/orientation corrections that align motion suit data to vehicle coordinates, reducing drift impact while preserving joint-angle accuracy. Break condition: Long sequences without LiDAR visibility; rapid accumulation of drift exceeds correction frequency.

### Mechanism 3
Fisheye-aware 2D pose estimation followed by geometry-aware 3D uplift produces distance-dependent pose accuracy, with error increasing at range boundaries. YOLOX detects bounding boxes; ViTPose (fisheye-fine-tuned) estimates 2D keypoints; keypoints project from fisheye to pinhole geometry; geometry-driven attention mechanism uplifts to 3D pose. Pixel resolution decreases with distance, reducing keypoint precision. Break condition: Partial occlusion at 0–5m (body parts outside FoV); extreme fisheye distortion at image edges degrades 2D keypoint detection.

## Foundational Learning

- **Fisheye camera models and distortion correction:**
  - Why needed here: The entire pipeline depends on accurate fisheye-to-pinhole projection; misunderstanding distortion models will propagate errors through 3D uplift.
  - Quick check question: Can you explain why a pinhole camera model fails for keypoints near fisheye image edges, and what radial distortion parameters correct this?

- **Multi-view geometry and triangulation:**
  - Why needed here: 3D localization from multiple cameras requires understanding epipolar geometry, line-of-sight intersection, and association across views.
  - Quick check question: Given two camera poses and a 2D keypoint in each image, sketch how to compute the 3D point via triangulation.

- **Temporal synchronization of heterogeneous sensors:**
  - Why needed here: The dataset combines sensors with different clock domains; understanding synchronization error bounds is critical for fusion.
  - Quick check question: If camera runs at 30fps and LiDAR at 10Hz, what is the maximum temporal misalignment without interpolation, and how would you detect synchronization drift?

## Architecture Onboarding

- **Component map:**
  Fisheye cameras (4×) → YOLOX (detection) → ViTPose-fisheye (2D keypoints) → Fisheye→Pinhole projection → Geometry-driven attention [6] (3D pose uplift) → Bipartite matching (pose-location association) → Cross-view transformer [24] (3D localization) → Ultrasonic (close range) → Near-field refinement (optional)

- **Critical path:** 2D keypoint detection accuracy → fisheye projection correctness → multi-view association success → final 3D localization precision. Errors compound through the pipeline.

- **Design tradeoffs:**
  - Door-mounted cameras: Provide wider FoV but calibration breaks when doors open (observed in 0–5m zone degradation)
  - 3 fps LiDAR manual annotation: Reduces motion suit drift but labor-intensive; unsuitable for real-time deployment
  - Assumption: Constant pedestrian height for initial 3D localization: Simplifies triangulation but introduces ~10–21cm height error (Table 3)

- **Failure signatures:**
  - Single-camera visibility (0–5m): Accuracy drops to 0.62; MPJPE spikes to 201mm; body parts outside FoV
  - Open doors: Calibration invalidated; camera positions shift relative to vehicle frame
  - Female participants: Systematic height overestimation (35cm error at 0–5m vs 17.7cm for males) suggests dataset bias toward male morphology in training

- **First 3 experiments:**
  1. Reproduce baseline metrics on public 51-sequence test set: Run the described pipeline (YOLOX + ViTPose-fisheye + [6] + [24]) and verify MPJPE ~180–200mm and localization ADE ~0.32m. Document any deviations.
  2. Ablate multi-view benefit: Evaluate 3D localization using single-camera vs. multi-camera association. Quantify accuracy delta between 0–5m (typically single-camera) and 5–10m (typically dual-camera) zones.
  3. Characterize door-open failure mode: Identify frames with open doors in the dataset; measure localization/pose error specifically for these cases. Test simple mitigation (exclude door-mounted cameras when door state detected).

## Open Questions the Paper Calls Out

- **Open Question 1:** Can an end-to-end processing pipeline jointly optimizing detection, pose, and intent prediction provide superior robustness compared to the modular baseline? The conclusion explicitly encourages exploring "end-to-end processing pipelines that jointly optimize all these tasks for improved robustness." This remains unresolved because the provided baseline relies on a successive pipeline of distinct models, which may propagate errors across stages.

- **Open Question 2:** How can 3D localization accuracy be maintained when open vehicle doors invalidate camera calibration or occlude the pedestrian? In Section 4.3, the authors note accuracy drops in the 0–5 m zone and state, "We leave it to future users of the dataset to refine measurements in cases where door openings cause these issues." This is unresolved because the baseline assumes static extrinsic calibration, which breaks when door-mounted cameras move, and lacks specific handling for the resulting occlusions.

- **Open Question 3:** How does the limitation to stationary ego-vehicle scenarios impact the generalizability of intent detection models? The conclusion lists the focus on "stationary vehicle interactions" as a primary limitation, identifying dynamic scenarios as a direction for "future work." This is unresolved because the dataset currently lacks motion profiles for the ego-vehicle, making it impossible to validate algorithm performance during driving maneuvers.

## Limitations
- Reliance on manual LiDAR annotation for motion capture drift correction limits scalability and real-time applicability
- Demographic imbalance (61 males, 7 females) introduces potential bias in height estimation, with female participants showing systematic overestimation errors up to 35cm
- Door-mounted camera configuration creates calibration vulnerabilities when vehicle doors open, degrading accuracy in close-range scenarios

## Confidence

- **High confidence:** Multi-view fisheye triangulation mechanism (supported by quantitative evidence showing 5–10m accuracy improvement)
- **Medium confidence:** Motion capture drift correction approach (engineering solution with limited external validation)
- **Medium confidence:** Baseline performance metrics (requires access to pretrained models for exact reproduction)

## Next Checks
1. Characterize demographic bias by computing gender-stratified height estimation errors across the entire dataset, not just the test subset
2. Implement door-open detection and measure the specific degradation in 0–5m zone accuracy when doors are open versus closed
3. Validate the multi-view benefit by systematically ablating single-camera scenarios and quantifying the accuracy drop in the critical 0–5m range