---
ver: rpa2
title: 'CIC: Circular Image Compression'
arxiv_id: '2407.15870'
source_url: https://arxiv.org/abs/2407.15870
tags:
- image
- figure
- dlpr
- cdlpr
- compression
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the performance degradation in learned image
  compression (LIC) when applied to out-of-sample, out-of-distribution, or out-of-domain
  testing images, caused by the inherent difference between training and testing datasets.
  To overcome this limitation, the authors propose a Circular Image Compression (CIC)
  approach that incorporates a closed-loop architecture, inspired by automatic control
  theory, to minimize the gap between training and testing images and enhance LIC
  performance.
---

# CIC: Circular Image Compression

## Quick Facts
- arXiv ID: 2407.15870
- Source URL: https://arxiv.org/abs/2407.15870
- Reference count: 40
- Primary result: CIC achieves state-of-the-art reconstruction performance by applying closed-loop feedback to pretrained SIC models, particularly effective for out-of-distribution images

## Executive Summary
The paper addresses performance degradation in learned image compression when applied to out-of-distribution testing images by proposing a Circular Image Compression (CIC) approach. CIC introduces a closed-loop architecture inspired by automatic control theory that iteratively refines reconstructions by feeding back residuals through a control loop. The method proves that steady-state error approaches zero using Taylor series expansion and functions as a Plug-and-Play post-processing solution that can be built on any existing SIC method.

## Method Summary
CIC wraps any pretrained Serial Image Compression (SIC) encoder-decoder pair in a closed feedback loop. The framework computes residuals between current and decoded images, scales them by a coefficient matrix Λ(t), and integrates them to form a control term added to the initial reconstruction. This iterative process continues until convergence or a fixed iteration count. The method leverages local linearization via Taylor expansion to prove that steady-state error approaches zero, while requiring no retraining of the base SIC model.

## Key Results
- CIC outperforms eight state-of-the-art open-source SIC algorithms across five public image compression datasets
- Maximum PSNR improvement of 1.7142 dB on CLIC2021 Test dataset
- Particularly effective for out-of-distribution images with dark backgrounds, sharp edges, high contrast, grid shapes, or complex patterns
- Achieves consistent improvements across all tested datasets while maintaining Plug-and-Play compatibility with existing models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Closing the feedback loop reduces reconstruction error iteratively toward a steady-state near zero
- Mechanism: CIC wraps encoder-decoder in a closed loop where residual f_r(t) = f(t) - f_d(t) is computed, scaled by Λ(t), and integrated to form control term f_c(t). Reconstruction follows: f(t) = f_d0 + ∫₀ᵗ Λ(τ)·f_r(τ)dτ
- Core assumption: Nonlinear encoding-decoding function can be locally approximated by linear term via Taylor expansion around f_0
- Evidence anchors: Abstract proves steady-state error approaches zero via Taylor expansion; Section 3.5 derives convergence condition ||I - Δt·Λ·U||_F < 1 leading to lim(t→∞) r(t) = 0
- Break condition: If ||I - Δt·Λ·U||_F ≥ 1, convergence fails and error may diverge or oscillate

### Mechanism 2
- Claim: CIC functions as Plug-and-Play post-processing wrapper around any pretrained SIC model without retraining
- Mechanism: CIC treats encoder and decoder as fixed black-box components, iteratively refining output through residual feedback at test time
- Core assumption: Pretrained SIC model provides reasonable initial reconstruction within basin of convergence
- Evidence anchors: Abstract states CIC possesses Post-Training and Plug-and-Play properties; Section 1 describes building upon existing pretrained models
- Break condition: Severely corrupted base model outputs may amplify artifacts rather than correct them

### Mechanism 3
- Claim: CIC provides larger PSNR/SSIM gains on out-of-distribution images with specific structural characteristics
- Mechanism: Open-loop SIC models underperform on images with dark backgrounds, sharp edges, high contrast, grid shapes, or complex patterns. CIC's closed-loop refinement iteratively corrects these systematic errors
- Core assumption: Residual f_r(t) contains signal information correlated with reconstruction error
- Evidence anchors: Abstract highlights effectiveness on specific image types; Section 4.2.2 shows visual improvements on Kodak (sharp edges), CLIC2021 Test (dark backgrounds/high contrast), CLIC2021 Validation (grid shapes)
- Break condition: If base model artifacts are perceptually structured rather than pixel-wise errors, residual feedback may not converge to meaningful correction

## Foundational Learning

- **Closed-loop vs. Open-loop Control Systems**: Understanding how negative feedback reduces steady-state error is essential for CIC's core innovation. Quick check: Given system with output y(t) and reference r(t), what happens to error e(t) = r(t) - y(t) when negative feedback with sufficient gain is applied?

- **Taylor Series Expansion for Local Linearization**: CIC uses first-order Taylor approximation to analyze nonlinear loop equation. Understanding validity conditions for linearization is critical. Quick check: For NF(x) ≈ NF(x₀) + (∂NF/∂x)|_{x₀}·(x - x₀), when does this approximation break down?

- **Plug-and-Play Optimization in Imaging**: CIC treats EN/DE as fixed modules. Familiarity with modular optimization frameworks helps understand why this works without retraining. Quick check: What constraints must a denoiser or decoder satisfy for Plug-and-Play iterative refinement to converge?

## Architecture Onboarding

- **Component map**: Input f_0 -> EN (Encoder) -> RP (Representation) -> QT (Quantization) -> EC (Entropy Coding) -> ED (Entropy Decoding) -> RC (Reconstruction) -> DE (Decoder) -> f_d0 (Initial decoded image) -> Summator (residual f_r(t) = f(t) - f_d(t)) -> Multiplier (scales by Λ(t)) -> Integrator (accumulates to f_c(t)) -> Output f(t)

- **Critical path**:
  1. Input original image f_0 to base SIC encoder → encoded representation f_e0
  2. Decode f_e0 → initial reconstruction f_d0
  3. Initialize f(0) = 0 or f_d0
  4. For each iteration t: Encode f(t) → f_e(t), decode → f_d(t), compute residual f_r(t) = f(t) - f_d(t), update f(t+Δt) = f(t) + Δt·Λ(t)·f_r(t)
  5. Return f(N) after N iterations

- **Design tradeoffs**:
  - Computation vs. Quality: CIC doubles computation (requires encoding at each iteration); paper suggests N = 1-10 iterations. Start with N = 3-5
  - Λ(t) selection: Paper proposes Λ(t) = η·I where η is scalar constant. η must satisfy: 1/(D·Δt·μ(t)) < η < 1/(Δt·μ(t))
  - Initialization: f(0) = f_d0 (warm start) typically converges faster than zero initialization

- **Failure signatures**:
  - Divergence/oscillation: If ||f(t)|| grows unbounded or oscillates, reduce η
  - No improvement: If ΔPSNR ≈ 0 after 3 iterations, either base model is near-optimal or residual contains no correctable signal
  - Artifact amplification: If reconstructed images show enhanced noise or blockiness, residual may amplify quantization noise; try reducing N or adding regularization

- **First 3 experiments**:
  1. Reproduce single-image convergence: Run CDLPR on kodim01.png with τ=7, track PSNR/SSIM vs. iteration count (N = 1, 3, 5, 10). Plot ||f(t) - f_0||₂ to verify convergence behavior matches Equation 19
  2. Ablate integration constant η: On held-out validation set, sweep η ∈ {-1.0, -0.5, 0.5, 1.0} and measure mean PSNR gain. Identify stable range per Equation 18 condition
  3. Stress test on out-of-distribution images: Apply CIC to 5 manually selected images with dark backgrounds, grid patterns, or high contrast. Compare CIC gains vs. base SIC; document failure cases

## Open Questions the Paper Calls Out

- **Open Question 1**: Can CIC be effectively integrated into the In-Training procedure of deep neural networks rather than operating solely as a Post-Training module? The current study strictly utilizes CIC as a Plug-and-Play, Post-Training solution.

- **Open Question 2**: Does CIC maintain superior reconstruction performance when applied to specialized image domains such as medical imaging or remote sensing? Current experiments are limited to general photography datasets.

- **Open Question 3**: Can advanced control theories, such as fuzzy logic, improve CIC's convergence rate or steady-state error compared to the current Proportion-Integration control strategy? The current implementation relies on basic PI control.

- **Open Question 4**: How can CIC's computational complexity be reduced to make it viable for applications with constrained computing resources? The paper explicitly notes doubling computation load as a limitation.

## Limitations

- **Convergence Robustness**: Proof relies on linear approximation via Taylor expansion; linearization assumptions may break down for highly nonlinear mappings or low-bitrate representations

- **Computational Overhead**: Requires N+1 encoding/decoding passes versus 1 for standard SIC, potentially doubling inference time without adequate efficiency tradeoff analysis

- **Coefficient Matrix Sensitivity**: While Λ(t) = η·I is proposed, paper doesn't explore adaptive or image-specific coefficient selection or systematic investigation of η tuning requirements

## Confidence

- **High Confidence**: CIC's Plug-and-Play architecture - Mathematical framework and implementation are clearly described, fundamental to method's design
- **Medium Confidence**: CIC's effectiveness on out-of-distribution images - Experimental results show consistent gains but corpus lacks direct evidence for closed-loop feedback specifically addressing OOD generalization
- **Medium Confidence**: Convergence proof via Taylor expansion - Mathematical derivation appears sound but limited empirical validation of linearization assumptions across diverse conditions reduces confidence

## Next Checks

1. **Convergence Boundary Testing**: Systematically vary integration constant η and iteration count N across diverse image set (varying content, bitrate, quality). Document minimum/maximum η values ensuring stable convergence and identify failure modes when conditions are violated

2. **Computational Cost-Benefit Analysis**: Measure inference time and memory usage for CIC versus base SIC across different N values and image resolutions. Calculate PSNR gain per unit computation time to quantify efficiency tradeoff

3. **Generalization to Base Model Variations**: Apply CIC to SIC models with different architectures (VAE, GAN-based, transformer-based) and training objectives (MSE, MS-SSIM, GAN loss). Assess whether closed-loop refinement consistently improves reconstruction quality regardless of base model design choices