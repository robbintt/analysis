---
ver: rpa2
title: 'COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language
  Navigation'
arxiv_id: '2503.24065'
source_url: https://arxiv.org/abs/2503.24065
tags:
- navigation
- state
- space
- cosmo
- conf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces COSMO, a novel hybrid architecture that combines
  selective state space models (SSMs) with transformers for vision-and-language navigation
  (VLN). The method addresses the high computational costs and suboptimal performance
  of transformer-based VLN models, particularly on lengthy instructions.
---

# COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation

## Quick Facts
- **arXiv ID**: 2503.24065
- **Source URL**: https://arxiv.org/abs/2503.24065
- **Reference count**: 40
- **Primary result**: Hybrid SSM-transformer VLN model achieves competitive performance with 15.5% of DUET's parameters and 9.3% of its FLOPs

## Executive Summary
COSMO addresses the high computational costs of transformer-based vision-and-language navigation (VLN) models by introducing a hybrid architecture that combines selective state space models with transformers. The method introduces two VLN-tailored SSMs: Round Selective Scan (RSS) for spatial modeling and Cross-modal Selective State Space Module (CS3) for cross-modal interaction. RSS captures inter-token relationships within a single scan, while CS3 adapts the selective mechanism to a dual-stream structure for effective multimodal fusion. Experiments on REVERIE, R2R, and R2R-CE datasets show that COSMO achieves competitive navigation performance with significant reductions in computational costs—only 15.5% of parameters and 9.3% of FLOPs compared to the baseline DUET.

## Method Summary
COSMO proposes a hybrid architecture that combines selective state space models with transformers for efficient vision-and-language navigation. The method introduces RSS (Round Selective Scan) for spatial modeling and CS3 (Cross-modal Selective State Space Module) for cross-modal interaction. RSS processes spatial information by flipping and concatenating single scans to capture inter-token relationships. CS3 uses a dual-stream structure with selective mechanisms: it receives language tokens from the text encoder and visual tokens from the visual encoder, processes them through state space models, and uses a gate to control information flow. The architecture processes language and vision features through these SSMs, then uses transformer layers for action decision-making. The model is pre-trained with speaker-augmented data (100k steps) and fine-tuned on target datasets (20k steps for R2R/REVERIE, 30 epochs for R2R-CE).

## Key Results
- Achieves competitive navigation performance with only 15.5% of DUET's parameters and 9.3% of its FLOPs
- Shows particular effectiveness on long-instruction datasets like REVERIE
- Maintains strong performance across discrete (R2R, REVERIE) and continuous (R2R-CE) navigation environments

## Why This Works (Mechanism)
COSMO's selective memorization mechanism addresses the computational inefficiency of transformers in VLN by replacing expensive cross-attention operations with efficient state space models. The dual-stream architecture (RSS for spatial, CS3 for cross-modal) allows selective processing of information streams, reducing computational overhead while maintaining performance. The flip-concatenation in RSS captures bidirectional relationships within sequences, and the gated output in CS3 controls information flow between modalities. This selective approach is particularly effective for long instructions where traditional transformers struggle with quadratic complexity.

## Foundational Learning

**Selective State Space Models**: Used for efficient sequence modeling with linear complexity. Why needed: Replace expensive transformer attention in VLN. Quick check: Verify RSS and CS3 implement state space computation with state size=16.

**Dual-stream Architecture**: Separate processing paths for text and visual modalities. Why needed: Maintain modality-specific feature extraction before fusion. Quick check: Confirm CS3 has distinct text and visual streams with independent state space modules.

**Flip-Concatenation**: Sequence processing that considers bidirectional relationships. Why needed: Capture inter-token relationships within single scans. Quick check: Verify RSS processes flipped-concatenated sequences before class token aggregation.

**Gated Information Flow**: Control mechanism for cross-modal interaction. Why needed: Prevent information overload and maintain selective processing. Quick check: Ensure CS3 uses gates to control output from state space models.

## Architecture Onboarding

**Component Map**: Text Encoder -> RSS -> CS3 -> Transformer Layers -> Action Decision

**Critical Path**: Visual features + Language tokens → RSS/CS3 selective modules → Transformer decision layers → Action prediction

**Design Tradeoffs**: 
- Selective mechanisms reduce computational cost but require careful gating to maintain performance
- Dual-stream design preserves modality-specific features but adds architectural complexity
- State space size=16 balances efficiency and representational capacity

**Failure Signatures**: 
- Direct substitution of vanilla Mamba or single-stream SSM causes SR to drop to ~32% (from 51%)
- Long-instruction performance degrades without proper selective memorization
- Concatenated sequence approaches fail to maintain competitive performance

**3 First Experiments**:
1. Replace DUET cross-modal layers with RSS+CS3 and verify parameter reduction to 15.5%
2. Test RSS+CS3 on REVERIE long instructions and measure SR improvement
3. Measure actual FLOPs on R2R and confirm 9.3% of baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Architectural specificity limits generalizability to other VLN architectures
- Computational savings based on theoretical FLOPs, not empirical runtime measurements
- Heavy reliance on speaker-augmented pre-training data (100k steps)

## Confidence
- **High Confidence**: Hybrid architecture specification and experimental validation on multiple VLN datasets
- **Medium Confidence**: Computational efficiency claims based on theoretical calculations
- **Medium Confidence**: Improvements on long instructions demonstrated but not fully analyzed

## Next Checks
1. Measure actual inference time and memory usage of COSMO on standard GPU/CPU setup
2. Replace DUET backbone with PRESS or HAMT and assess adaptation of RSS+CS3 modules
3. Train COSMO without speaker-augmented pre-training to quantify its contribution