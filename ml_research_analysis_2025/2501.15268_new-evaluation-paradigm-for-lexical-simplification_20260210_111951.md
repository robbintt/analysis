---
ver: rpa2
title: New Evaluation Paradigm for Lexical Simplification
arxiv_id: '2501.15268'
source_url: https://arxiv.org/abs/2501.15268
tags:
- complex
- sentence
- word
- words
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a new evaluation paradigm for lexical simplification
  (LS) by addressing limitations in existing datasets when evaluating large language
  model (LLM)-generated simplifications. It proposes a human-machine collaborative
  annotation method to create an all-in-one LS dataset that includes all complex words
  in a sentence along with their suitable substitutes.
---

# New Evaluation Paradigm for Lexical Simplification

## Quick Facts
- **arXiv ID**: 2501.15268
- **Source URL**: https://arxiv.org/abs/2501.15268
- **Reference count**: 15
- **Primary result**: Multi-LLM collaboration (CoLLS) achieves F1 scores up to 0.611 and F1-20 scores up to 0.280 on lexical simplification tasks, outperforming single-prompt methods

## Executive Summary
This paper introduces a new evaluation paradigm for lexical simplification (LS) that addresses limitations in existing datasets when evaluating large language model (LLM)-generated simplifications. The authors propose a human-machine collaborative annotation method to create an all-in-one LS dataset containing all complex words in sentences along with their suitable substitutes. They explore various LLM-based approaches, including single-prompt, in-context learning, chain-of-thought, and a novel multi-LLM collaboration framework called CoLLS that simulates the traditional LS pipeline through specialized roles and majority voting. Experimental results demonstrate that multi-LLM collaboration significantly outperforms existing baselines, with CoLLS achieving substantial improvements in both F1 and F1-20 metrics across different text genres.

## Method Summary
The study constructs a new LS dataset through human-machine collaborative annotation, where three diverse LS methods (LSBert, GPT3.5, Gemini1.0) generate pseudo-substitute pools and human annotators validate suitability with LLM-assisted recommendations. For the LS task itself, they test four approaches: single-prompt with few-shot learning, chain-of-thought prompting, and a multi-LLM collaboration framework called CoLLS. CoLLS decomposes the LS pipeline into three specialized roles—Complex Word Analyst, Substitute Generator, and Sentence Comparator—each executing N independent prompts per step, with final outputs requiring agreement from at least m of N predictions. The framework is evaluated on 400 sentences from CWI 2018 shared task datasets across three genres, measuring performance using F1 score and a new F1-20 metric that weights complex words by annotator agreement.

## Key Results
- CoLLS achieves F1=0.611 and F1-20=0.280 on the News dataset, significantly outperforming GPT3.5(COT) with F1=0.524 and F1-20=0.225
- Multi-LLM collaboration consistently outperforms single-prompt methods across all tested datasets and metrics
- Ablation studies show that CoLLS performance stabilizes at N=3 prompts per step, with N=1 performing worst
- The human-machine collaborative annotation achieves 95% coverage and >94% precision in substitute validation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-LLM collaboration (CoLLS) outperforms single-prompt LLM methods by decomposing the LS pipeline into specialized roles and aggregating decisions through majority voting
- Mechanism: Three LLM roles—Complex Word Analyst, Substitute Generator, and Sentence Comparator—each execute N independent prompts per step. Final outputs require agreement from at least m of N predictions (default: m=2, N=3). This filters idiosyncratic errors while preserving consistent predictions across reasoning paths
- Core assumption: Aggregating diverse reasoning paths reduces individual model errors without requiring model fine-tuning; the assumption holds that LLMs make uncorrelated errors across different prompt formulations
- Evidence anchors:
  - [abstract]: "Experimental results demonstrate that LS based on multi-LLMs approaches significantly outperforms existing baselines."
  - [section 5.2]: "The multi-LLM collaboration method CoLLS significantly outperforms other LLM-based methods... CoLLS(GPT3.5) achieves F1=0.611, F1-20=0.280 on News dataset vs. GPT3.5(COT) F1=0.524, F1-20=0.225."
  - [section 5.3]: "When N is set to 1, CoLLS performs the worst. When N is set to 3 or higher, the results stabilize."
  - [corpus]: Related work on multi-agent prompting (arXiv:2506.11681) supports hybrid multi-agent architectures for sentence simplification, though direct comparison to CoLLS is not available

### Mechanism 2
- Claim: Human-machine collaborative annotation produces higher-coverage, higher-quality LS datasets than manual annotation alone by leveraging automated methods for candidate generation and humans for validation
- Mechanism: Three diverse LS methods (LSBert, GPT3.5, Gemini1.0) generate pseudo-substitute pools. Human annotators validate suitability (simplicity + meaning preservation) with LLM-assisted recommendations. Coverage increases because automated methods propose substitutes humans may not recall
- Core assumption: Humans can more reliably judge substitute quality than generate substitutes from memory; automated methods provide broader semantic coverage than human recall
- Evidence anchors:
  - [section 3.4]: "The human annotators provide 342 distinct substitutions and 325 substitutions belonged to the substitutions provided in the dataset. The coverage is equivalent to 95% (325/342)."
  - [section 3.4]: "High precision rate of above 94% (779/828) indicates the high quality of the substitutions."
  - [section 3.4]: "LLM can provide effective information to supplement the annotator's work... consistency test shows 0.763-0.930 agreement between human and LLM judgments."
  - [corpus]: No directly comparable corpus evidence on human-machine annotation efficiency for LS; related work focuses on LLM-as-judge approaches (arXiv:2512.06228) rather than human-in-the-loop annotation

### Mechanism 3
- Claim: Chain-of-thought (COT) prompting provides modest but consistent improvements over standard few-shot prompting by explicitly decomposing the LS task into interpretable steps
- Mechanism: COT prompts instruct models to: (1) identify complex words, (2) generate simpler alternatives, (3) replace while preserving structure. This explicit reasoning reduces task confusion compared to direct "simplify this sentence" instructions
- Core assumption: Explicit step decomposition reduces reasoning errors compared to implicit task understanding; models benefit from structured task framing even without intermediate output verification
- Evidence anchors:
  - [section 5.2]: "The use of COT-based LLM method brings a slight performance improvement. GPT3.5(COT) F1=0.541 vs. GPT3.5 F1=0.524 on WikiNews."
  - [section 4.1]: "We find that guiding the model through COT technique can enhance the quality of text revisions."
  - [corpus]: Related work (arXiv:2210.00720) supports complexity-based prompting for multi-step reasoning, but specific LS gains are context-dependent

## Foundational Learning

- Concept: **Lexical Simplification Pipeline (CWI → SG → SR)**
  - Why needed here: CoLLS explicitly mirrors this traditional pipeline with LLM roles; understanding the decomposition is prerequisite to understanding why multi-LLM collaboration helps
  - Quick check question: Can you explain why substitute ranking (SR) differs from the Validation step in CoLLS?

- Concept: **Majority Voting / Self-Consistency in LLMs**
  - Why needed here: All three CoLLS stages rely on N-prompt voting; ablation shows N=1 performs worst. Understanding ensemble reasoning is critical
  - Quick check question: Given N=5 prompts with m=3 agreement threshold, how many prompts must agree for a substitute to be accepted?

- Concept: **F1 vs. F1-20 Metrics (Difficulty-Weighted Evaluation)**
  - Why needed here: The paper introduces F1-20 to weight complex words by annotator agreement (Hi in Eq. 6-7). Interpreting results requires understanding why unweighted F1 is insufficient
  - Quick check question: If Model A simplifies 5 words each marked complex by 2 annotators, and Model B simplifies 3 words each marked complex by 15 annotators, which has higher F1-20?

## Architecture Onboarding

- Component map: Input Sentence → [CWI Module] → [SG Module] → [Validation Module] → Final Substitute

- Critical path: CWI → SG → Validation. Errors propagate forward; missed complex words in CWI cannot be recovered. Validation step contributes more to F1-20 than F1 (ablation: N=0 for Validation drops F1-20 but not F1 significantly)

- Design tradeoffs:
  - N (number of prompts per step): Higher N increases robustness but linearly increases cost and latency. Paper shows N=3 is sufficient; N>3 yields diminishing returns
  - m (agreement threshold): Lower m increases recall but risks false positives; higher m increases precision but may miss valid simplifications
  - Prompt diversity: Paper uses 2/4/6 demonstrations across three LLMs to ensure diversity. Homogeneous prompts reduce voting benefits

- Failure signatures:
  - Low NumCW with high CorrectCW: CWI module is conservative (high precision, low recall)—consider lowering m threshold
  - High NumCW with low CorrectCW: CWI module over-identifies complex words—raise m threshold or improve CWI prompts
  - Large gap between F1 and F1-20: Model simplifies easy complex words but misses difficult ones—review SG candidate quality for low-frequency vocabulary
  - Validation step N=0 shows minimal F1 change but F1-20 drop: Validation filters contextually inappropriate substitutes that affect harder words

- First 3 experiments:
  1. **Reproduce ablation on N**: Run CoLLS with N=1,2,3,4,5 on WikiNews subset. Verify F1 stabilizes at N≥3. Cost: ~5× inference compared to single-prompt baseline
  2. **Test prompt diversity hypothesis**: Compare CoLLS with homogeneous prompts (same 4 demonstrations for all N) vs. heterogeneous prompts (2/4/6 demonstrations). Expect 2-5% F1 degradation with homogeneous prompts
  3. **Validate dataset quality claim**: Sample 50 instances from constructed dataset; have independent annotator evaluate substitute precision. Target: >90% precision (paper reports 94%). If <85%, review human-machine annotation workflow for systematic errors

## Open Questions the Paper Calls Out
- Can the human-machine collaborative annotation method and the CoLLS framework be effectively adapted to construct datasets and perform lexical simplification for morphologically rich or low-resource languages?
- Does the performance of the multi-LLM collaboration framework (CoLLS) scale effectively when applied to a significantly larger dataset with broader text genres (e.g., medical or legal texts)?
- What is the computational cost and latency trade-off between the multi-step CoLLS framework and single-prompt LLM methods in a real-time deployment scenario?

## Limitations
- Dataset construction methodology lacks critical implementation details about how annotator disagreements were resolved and how LLM-assisted recommendations were integrated
- Performance comparisons across different LLMs are presented without ablation studies that isolate model effects from framework effects
- The evaluation focuses exclusively on single-word substitutions, excluding multi-word expressions and context-dependent simplifications

## Confidence
**High confidence**: The core finding that multi-LLM collaboration (CoLLS) outperforms single-prompt methods is well-supported by ablation studies showing N=1 performs worst and N≥3 stabilizes. The majority voting mechanism is clearly specified and the F1 score improvements (0.524→0.611 on News dataset) are substantial and consistent across multiple baselines.

**Medium confidence**: The dataset quality metrics (95% coverage, >94% precision) are based on internal consistency checks between human and LLM judgments but lack external validation. The 76-93% agreement rates are promising but the evaluation methodology for determining "correct" substitutes isn't fully transparent.

**Low confidence**: The F1-20 metric's effectiveness in capturing true simplification difficulty is theoretically sound but practically untested. The paper assumes that annotator agreement correlates with word complexity, but doesn't validate this assumption against established complexity measures or demonstrate that F1-20 better predicts downstream comprehension outcomes.

## Next Checks
1. **External dataset validation**: Apply CoLLS to an independently constructed LS dataset (e.g., from CWI 2018 test set or another source) without human-assisted annotation. Compare F1 scores to those reported on the constructed dataset to isolate framework performance from dataset quality effects.

2. **Multi-word expression ablation**: Run CoLLS on a subset of sentences containing complex multi-word expressions (identified via dependency parsing or POS patterns). Measure performance degradation compared to single-word simplifications to quantify the impact of this scope limitation.

3. **Cross-model consistency test**: Implement CoLLS with three different LLMs (e.g., GPT-3.5, Llama-3-70B, and Claude) and measure performance variance across model combinations. This would reveal whether CoLLS's benefits are model-agnostic or depend on specific model characteristics like instruction-following ability or reasoning depth.