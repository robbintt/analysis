---
ver: rpa2
title: 'InfoAgent: Advancing Autonomous Information-Seeking Agents'
arxiv_id: '2509.25189'
source_url: https://arxiv.org/abs/2509.25189
tags:
- tool
- search
- arxiv
- query
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InfoAgent is a deep research agent built to tackle long-horizon,
  multi-evidence information-seeking problems. We introduce a data synthesis pipeline
  that constructs hard queries by building entity trees, sampling subtrees, and applying
  entity fuzzification, forcing models to perform deep reasoning over multiple documents.
---

# InfoAgent: Advancing Autonomous Information-Seeking Agents

## Quick Facts
- arXiv ID: 2509.25189
- Source URL: https://arxiv.org/abs/2509.25189
- Reference count: 40
- InfoAgent achieves 15.3% accuracy on BrowseComp, outperforming prior open-source models

## Executive Summary
InfoAgent is a deep research agent designed for long-horizon, multi-evidence information-seeking problems. The authors introduce a data synthesis pipeline that systematically increases question difficulty through entity tree construction, subtree sampling, and entity fuzzification. To ensure stable training, they implement a custom self-hosted search and browsing infrastructure, avoiding reliance on commercial APIs. InfoAgent is post-trained from Qwen3-14B using supervised fine-tuning to instill long-horizon search behaviors, followed by reinforcement learning to improve reasoning-driven tool use. The model demonstrates strong performance on benchmarks like BrowseComp, BrowseComp-ZH, and Xbench-DS, and shows cross-lingual generalization despite being trained on English data.

## Method Summary
The authors build a two-stage training pipeline for InfoAgent. First, they synthesize 14k training trajectories by constructing entity trees from Wikipedia, sampling subtrees, and applying entity fuzzification to create hard queries that require multi-hop reasoning. They implement a custom self-hosted search infrastructure using Google/Bing/Brave APIs with BM25 and embedding reranking, plus GPT-4o-mini for snippet generation. InfoAgent is then post-trained from Qwen3-14B using supervised fine-tuning (2 epochs, lr=2e-5, 32k context) on these trajectories, followed by reinforcement learning with GRPO (5 epochs, lr=1e-6, 16k context, binary reward) on filtered samples. The RL stage uses 8 trajectories per group and filters data by pass@4 ∈ [0.25, 0.75].

## Key Results
- 15.3% accuracy on BrowseComp benchmark
- 29.2% accuracy on BrowseComp-ZH (cross-lingual generalization)
- 40.4% accuracy on Xbench-DS benchmark
- Custom search tool achieves 10x higher accuracy than Wiki Retriever baseline

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Entity fuzzification forces multi-hop reasoning by removing direct entity lookups.
- **Mechanism:** The pipeline builds entity trees from Wikipedia, then applies three-stage fuzzification (entity→generic description, numbers→ranges, semantic rephrasing). This creates optimization problem: min|q(K)| s.t. v∉p(K), where constraints K make shallow retrieval insufficient, requiring the agent to chain multiple search operations.
- **Core assumption:** Removing surface-level cues compels models to learn search strategies rather than pattern matching.
- **Evidence anchors:**
  - [abstract]: "we build entity trees and apply sub-tree sampling with entity fuzzification to systematically increase question difficulty"
  - [Section 3.1, Equation 3]: Defines the constraint optimization that ensures "it extremely difficult for the model to identify the target entity through shallow pattern matching"
  - [corpus]: Related work (WebDancer, DeepDive) lacks this systematic difficulty escalation—no fuzzification mechanism mentioned in neighbors
- **Break condition:** If fuzzification is too aggressive, questions become unsolvable; if too weak, models shortcut with single-hop retrieval.

### Mechanism 2
- **Claim:** Two-stage training (cold-start SFT → RL) is necessary because base models lack agentic search capabilities.
- **Mechanism:** SFT on 14k synthesized trajectories (mean 20.3 tool calls) instills long-horizon behavior. RL (GRPO on 5.7k filtered samples, pass@4 ∈ [0.25, 0.75]) refines reasoning-driven tool selection via binary correctness rewards.
- **Core assumption:** Agentic search requires distinct skills (planning, backtracking, multi-query formulation) absent from pretrained models.
- **Evidence anchors:**
  - [abstract]: "cold-start supervised finetuning to instill long-horizon search behaviors, followed by reinforcement learning which significantly improves reasoning-driven tool use"
  - [Section 4.3, Figure 5]: Without SFT, model achieves lower reward, fewer tool calls, and higher repeated-query ratio during RL
  - [corpus]: DeepSeek-R1, o1 show SFT→RL effective for reasoning, but InfoAgent specifically targets *agentic tool use*—distinct from pure reasoning
- **Break condition:** If SFT data quality is poor or trajectories are too short (<10 tool calls), RL fails to converge to effective strategies.

### Mechanism 3
- **Claim:** Self-hosted search infrastructure with LLM-generated snippets improves training stability and upper-bound performance.
- **Mechanism:** Custom search tool crawls pages, applies BM25+embedding+reranker pipeline, then uses GPT-4o-mini to generate 60-word query-focused snippets. Browse tool returns 2048-token chunks. Redis caching reduces latency; fallback to paid APIs handles long-tail requests.
- **Core assumption:** High-quality, consistent retrieval signals are necessary for RL to learn effective search policies.
- **Evidence anchors:**
  - [abstract]: "dedicated self-hosted search infrastructure, enhancing transparency of agent environments"
  - [Section 4.3, Table 2]: Model trained with custom tool achieves 10.0% on BrowseComp vs. 1.0% with Wiki Retriever; inference with custom tool maintains performance
  - [corpus]: No comparable infrastructure analysis in neighbor papers—most rely on commercial APIs or static corpora
- **Break condition:** If snippet generation is inaccurate or latency is too high, RL rollouts become bottlenecked; cache miss rate spikes degrade throughput.

## Foundational Learning

- **Concept: ReAct Framework**
  - **Why needed here:** InfoAgent operates by interleaving reasoning traces with tool calls (search/browse) in an action cycle. Understanding this loop is essential for debugging trajectories.
  - **Quick check question:** Can you trace how an observation from a search tool gets incorporated into the next reasoning step?

- **Concept: GRPO (Group Relative Policy Optimization)**
  - **Why needed here:** RL stage uses GRPO with normalized advantages computed across 8 trajectories per question. Binary rewards require understanding how sparse feedback shapes policy.
  - **Quick check question:** Why would the authors filter for pass@4 ∈ [0.25, 0.75] instead of using all data?

- **Concept: Entity Linking & Fuzzification**
  - **Why needed here:** Data synthesis depends on NER to build entity trees and fuzzification to obscure entities. Without this, you can't reproduce or extend the training data.
  - **Quick check question:** If you fuzzify "Albert Einstein" to "a famous physicist," what search strategies must an agent use to recover the original entity?

## Architecture Onboarding

- **Component map:**
  Wikipedia Entities → Entity Tree Builder → Fuzzifier → QA Generator → o3 Trajectory Synthesis → Custom Search Tool ←→ InfoAgent (Qwen3-14B + SFT + RL) ←→ BrowseComp/Xbench Evaluation

- **Critical path:**
  1. Data synthesis quality (fuzzification level, trajectory length ≥10 tool calls)
  2. SFT cold-start effectiveness (instills basic search patterns)
  3. RL filtering (pass@4 range ensures appropriate difficulty)
  4. Search tool latency/quality (Redis cache hit rate, snippet relevance)

- **Design tradeoffs:**
  - **Long vs. short SFT trajectories:** Long (≥10 calls) improves accuracy but causes 71-80% out-of-context errors under 32k limit. Authors recommend mixing short trajectories to mitigate.
  - **Custom tool vs. Wiki Retriever:** Custom tool achieves 10x higher accuracy but requires significant engineering; Wiki Retriever is simpler but caps performance.
  - **Process reward vs. outcome reward:** Authors tested recall-based bonus rewards (λ=0.5, 0.9) but found no improvement over binary correctness—suggests intermediate signals may not help with current setup.

- **Failure signatures:**
  - **Excessive tool calls exhausting context:** Model trained on long trajectories hits 80% OOC on BrowseComp. Mitigation: truncate at 16k during RL rollout.
  - **Repeated queries without progress:** Without SFT, repeat-query ratio stays high (~8-10%) vs. ~2% with SFT. Indicates model doesn't learn diverse search strategies.
  - **Low cache hit rate during RL:** Long-tail trajectories block rollout. Mitigation: fallback to paid APIs for 15% of requests.

- **First 3 experiments:**
  1. **Ablate SFT cold-start:** Train RL-only model from Qwen3-14B base on same data. Expected: lower reward convergence, fewer tool calls, higher repeat-query ratio (confirmed by Figure 5).
  2. **Compare search tools:** Train two models—one with custom tool, one with Wiki Retriever—using identical SFT+RL pipeline. Evaluate on BrowseComp. Expected: custom tool achieves ~10% vs. ~1% (Table 2).
  3. **Vary trajectory length in SFT:** Train separate models on <10 vs. ≥10 tool call subsets. Measure accuracy and OOC rate. Expected: longer trajectories improve accuracy but increase OOC (Table 3).

## Open Questions the Paper Calls Out
None

## Limitations
- Core evaluation relies on synthetic data and benchmark-specific metrics, which may not reflect real-world performance on genuinely novel queries.
- Effectiveness of entity fuzzification depends heavily on the balance between difficulty and solvability—aggressive fuzzification could create unanswerable questions that inflate performance gaps.
- Self-hosted search infrastructure introduces significant engineering complexity that may not be necessary if commercial APIs offer sufficient stability.

## Confidence
- **High Confidence**: Accuracy improvements on BrowseComp (15.3%) and Xbench-DS (40.4%) are directly measured from the paper's reported experiments.
- **Medium Confidence**: The necessity of two-stage training is supported by ablation showing RL-only models underperform, but could benefit from more extensive comparison with alternative training schedules.
- **Medium Confidence**: The claim that self-hosted search improves stability is based on comparison with Wiki Retriever, but lacks analysis of long-term API reliability or cost trade-offs.

## Next Checks
1. **Ablation of SFT cold-start**: Train an RL-only model from Qwen3-14B base on the same synthesized data to verify whether SFT is truly necessary for effective search strategy learning.
2. **Real-world deployment test**: Deploy InfoAgent on genuinely novel queries outside the benchmark suite to measure generalization and identify failure modes not captured by synthetic evaluation.
3. **Search tool reliability analysis**: Measure the impact of commercial API downtime or rate limiting on InfoAgent's performance compared to the self-hosted infrastructure under varying cache miss rates.