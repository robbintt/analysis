---
ver: rpa2
title: Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed
  Predictive Pipelines
arxiv_id: '2505.07857'
source_url: https://arxiv.org/abs/2505.07857
tags:
- intent
- language
- detection
- urdu
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a few-shot learning framework for Urdu intent
  detection that combines contrastive learning with a prototype-informed attention
  mechanism. By re-training multilingual and Urdu-specific language models using unlabeled
  Urdu data, the approach generates domain-aware sentence representations that improve
  downstream intent detection.
---

# Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines

## Quick Facts
- arXiv ID: 2505.07857
- Source URL: https://arxiv.org/abs/2505.07857
- Reference count: 40
- Few-shot learning framework combining contrastive learning with prototype-informed attention achieves up to 98.25% F1-score on ATIS and 84.42% on Web Queries in 4-way 5-shot settings.

## Executive Summary
This paper introduces LLMPIA, a few-shot learning framework for Urdu intent detection that leverages contrastive learning and prototype-informed attention mechanisms. By re-training multilingual and Urdu-specific language models using unlabeled Urdu data, the approach generates domain-aware sentence representations that significantly improve downstream intent detection performance. The framework was evaluated on two benchmark datasets under 4-way 1-shot and 4-way 5-shot settings, demonstrating substantial improvements over existing methods.

## Method Summary
The approach combines two phases: (1) LLMRCL - re-training pre-trained language models using Masked Language Modeling (MLM) with 25% token masking and Self-Supervised Contrastive Learning (SCL) on unlabeled Urdu data to generate domain-aware representations, and (2) PIA - a prototype-informed attention mechanism that creates class prototypes from support sets and uses Feature Interaction Attention (FIAT) to refine these prototypes before comparing them to queries via cosine similarity. The framework was evaluated on ATIS and Web Queries datasets under various few-shot settings with mutually exclusive class splits.

## Key Results
- Achieved F1-scores of 83.28% (4-way 1-shot) and 98.25% (4-way 5-shot) on ATIS dataset
- Achieved F1-scores of 76.23% (4-way 1-shot) and 84.42% (4-way 5-shot) on Web Queries dataset
- Outperformed state-of-the-art methods by 53.55% F1-score on Web Queries under standard train/test splits
- LLMRCL re-training consistently improved performance across all data splits (25%, 50%, 75% seen classes)

## Why This Works (Mechanism)

### Mechanism 1: Domain-Adaptive Representation Learning (LLMRCL)
Re-training generic pre-trained language models using unlabeled Urdu data via contrastive learning significantly improves semantic representation quality for downstream tasks. The LLMRCL component combines MLM with Self-Supervised Contrastive Learning, forcing the model to distinguish semantic relationships between text pairs without human labels, refining the vector space for Urdu nuances.

### Mechanism 2: Prototype-Informed Attention (PIA)
Mapping few-shot queries to class prototypes via an attention mechanism outperforms direct embedding comparison. The PIA module generates prototype representations for each intent class from the support set, using a Feature Interaction Attention layer to capture inner-sentence and inner-class interactions before comparison with the query.

### Mechanism 3: Angular Metric Alignment
Cosine similarity is the most robust metric for mapping query embeddings to prototypes in this high-dimensional Urdu intent space. The model projects queries and prototypes into a shared space and uses cosine similarity to measure directional alignment, which remains stable despite variations in embedding vector length.

## Foundational Learning

- **Concept: Few-Shot N-way K-shot Learning**
  - Why needed: The paper evaluates performance on 4-way 1-shot and 4-way 5-shot settings. Understanding how to construct "Support" and "Query" sets from limited data is fundamental.
  - Quick check question: In a "4-way 1-shot" task, how many distinct classes and examples per class are present in the Support Set?

- **Concept: Contrastive Learning (SimCLR style)**
  - Why needed: The LLMRCL phase relies on this. The paper uses self-supervised contrastive learning where the model must pull positive pairs closer and push negative pairs apart.
  - Quick check question: How does the model generate "positive" and "negative" pairs from unlabeled text data?

- **Concept: Prototype Networks**
  - Why needed: The core of the "PIA" mechanism. Rather than classifying a query directly, the model computes a mean embedding (prototype) for each class and classifies based on distance to these prototypes.
  - Quick check question: What is the mathematical operation performed to create a class prototype from a set of sentence embeddings?

## Architecture Onboarding

- **Component map:** Embedding Layer (re-trained PLM) -> LLMRCL Stage (MLM Head + Contrastive Loss) -> PIA Stage (FIAT Layer -> PI Layer -> Adaptive Layer -> Metric Layer)
- **Critical path:** 1) Re-train base model using LLMRCL on combined Urdu corpus; 2) Create episodes with N-way K-shot Support + Query sets; 3) Forward pass through re-trained model -> FIAT -> PI Layer -> Adaptive Layer -> Cosine Similarity; 4) Optimize with Cross-Entropy + Contrastive Regularization loss.
- **Design tradeoffs:** Multilingual models (MuRIL, BERT-104) often generalized better than Urdu-specific models (RoBERTa-small-Urdu) in few-shot scenarios after re-training; DeBERTa and DistilBERT offered efficiency but showed high bias and lower performance.
- **Failure signatures:** High bias (Type II errors) with DeBERTa showing lower recall than precision; performance collapse in 1-shot settings without LLMRCL re-training; prototypes skewed by outlier support set samples.
- **First 3 experiments:** 1) Baseline Reproduction: Implement LLMRCL using bert-base-104-languages on Web Queries and compare F1-scores against "Pre-train" baseline; 2) Metric Ablation: Run PIA pipeline with Euclidean Distance instead of Cosine Similarity on ATIS dataset; 3) Bias Analysis: Evaluate DeBERTa-base-100-languages vs. MuRIL-base-17-languages on 75% seen class split to confirm "High Bias" signature.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do advanced generative language models (LLMs) like DeepSeek, Gemini, and GPT compare to the proposed LLMPIA framework for Urdu intent detection?
  - Basis: Conclusion states Urdu intent detection landscape "lacks the potential exploration of advanced generative language models including DeepSeek, Gemini and GPT."
  - Why unresolved: Current study focused exclusively on encoder-based pre-trained models without evaluating zero-shot/few-shot capabilities of generative decoder architectures.
  - What evidence would resolve it: Comparative benchmark evaluating generative LLMs on ATIS and Web Queries datasets against LLMPIA framework's F1-scores.

- **Open Question 2:** Does training diverse language models on significantly larger Urdu corpora improve performance of the end-to-end predictive pipeline?
  - Basis: Conclusion identifies future direction to "train diverse types of language models on large Urdu language data and use these models in an end-to-end intent detection predictive pipeline."
  - Why unresolved: Current research relied on existing pre-trained models and re-trained them using available unlabeled data, without exploring impact of training on massive native Urdu datasets.
  - What evidence would resolve it: Experiments comparing current model performance against models pre-trained from scratch on large-scale Urdu corpus within LLMPIA pipeline.

- **Open Question 3:** Can the proposed framework be adapted to handle intent classes with extremely scarce data (fewer than six samples)?
  - Basis: Section 5.1 notes that "Classes with six or fewer queries excluded from dataset as such small sample sizes are not suitable for few-shot intent detection."
  - Why unresolved: Methodology explicitly filtered out ultra-low resource classes, meaning framework's robustness on intents with minimal available examples remains untested.
  - What evidence would resolve it: Evaluation of LLMPIA pipeline on unfiltered dataset containing intent classes with 1-5 total samples to assess performance degradation or adaptation strategies.

## Limitations

- The framework's performance gains hinge on underexplored dimensions of pair construction in contrastive learning and the size/characteristics of the unlabeled Urdu corpus used for pre-training.
- The temperature parameter Ï„ in similarity computation is unspecified, which could materially affect attention mechanism calibration.
- The generalization of the prototype-informed attention mechanism across diverse low-resource languages is inferred but not empirically tested.

## Confidence

- **High Confidence**: The core claim that re-training PLMs with contrastive learning improves few-shot Urdu intent detection is well-supported by ablation studies and comparison against strong baselines.
- **Medium Confidence**: The superiority of cosine similarity over Euclidean distance is demonstrated empirically but lacks theoretical grounding within the paper.
- **Low Confidence**: The generalization of the prototype-informed attention mechanism across diverse low-resource languages is inferred but not tested.

## Next Checks

1. **Pair Generation Ablation**: Modify LLMRCL contrastive loss to use both semantic similarity and random lexical shuffling as positive pair generators. Compare downstream few-shot intent detection F1-scores to isolate whether semantic or syntactic structure drives gains.

2. **Unlabeled Corpus Scaling Study**: Train same MuRIL-based pipeline on increasing fractions (1%, 10%, 100%) of unlabeled Urdu corpus. Plot F1-score vs. corpus size to identify point of diminishing returns and detect potential overfitting.

3. **Cross-Lingual Prototype Robustness**: Apply LLMPIA pipeline to a second low-resource language (e.g., Swahili or Hindi) using same two-phase training regime. Evaluate whether F1-score gains observed in Urdu replicate, particularly focusing on prototype quality in 1-shot vs. 5-shot settings.