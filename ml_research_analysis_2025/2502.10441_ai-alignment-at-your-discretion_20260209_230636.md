---
ver: rpa2
title: AI Alignment at Your Discretion
arxiv_id: '2502.10441'
source_url: https://arxiv.org/abs/2502.10441
tags:
- discretion
- human
- principles
- alignment
- principle
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the concept of alignment discretion, which
  refers to the latitude afforded to annotators in deciding which AI model outputs
  are "better" or "safer." This discretion remains largely unexamined, posing risks
  of arbitrary decisions and models failing to mimic this discretion. The authors
  formalize alignment discretion by drawing parallels with judicial discretion in
  legal systems and propose metrics to systematically analyze when and how discretion
  is exercised.
---

# AI Alignment at Your Discretion

## Quick Facts
- arXiv ID: 2502.10441
- Source URL: https://arxiv.org/abs/2502.10441
- Reference count: 40
- Key outcome: Formalizes "alignment discretion" and reveals excessive, unexamined subjectivity in AI safety alignment datasets, where models trained on these datasets develop their own discretion patterns.

## Executive Summary
This paper introduces the concept of "alignment discretion"—the latitude annotators have in deciding which AI model outputs are "better" or "safer." Drawing parallels with judicial discretion in legal systems, the authors formalize this concept and propose metrics to systematically analyze when and how discretion is exercised in AI alignment. By applying these metrics to safety alignment datasets like HH-RLHF and PKU-SafeRLHF, they reveal layers of discretion previously unaccounted for. The results show that annotators exercise significant discretion, and algorithms trained on these datasets develop their own forms of discretion in interpreting and applying principles, challenging the purpose of having explicit principles in alignment processes.

## Method Summary
The authors formalize alignment discretion by defining metrics to classify preference pairs into Consensus (principles agree), Conflict (principles disagree), and Indifference (principles are irrelevant). They use a strong LLM (GPT-4o) as an oracle to evaluate model responses against a set of 21 principles, then calculate "Discretion Arbitrariness" (frequency of disagreeing with principle consensus) and "Principle Priority" weights derived from pairwise principle conflicts. The Discretion Discrepancy metric measures divergence between annotator and model priority rankings using Kendall tau distance. The methodology is applied to safety alignment datasets, revealing excessive discretion and discrepancies between human preferences and model behaviors.

## Key Results
- Human annotators show ~28.9% arbitrariness when disagreeing with principle consensus in HH-RLHF dataset
- Reward models successfully learn human discretion patterns but fail to transfer them to LLMs through standard RLHF
- Models fine-tuned with RLHF develop significantly different principle priority rankings compared to human annotators

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The *Principle Agreement State* (Consensus, Conflict, Indifference) acts as a filter to identify when alignment is determined by rules versus when it requires subjective discretion.
- **Mechanism:** An oracle model evaluates pairs of model responses against a defined constitution (set of principles). If principles agree (Consensus), the "correct" output is objectively determined. If principles disagree (Conflict), the decision is pushed to a "discretionary" state where the annotator implicitly prioritizes one principle over another.
- **Core assumption:** The oracle model (e.g., GPT-4o) can accurately interpret and apply abstract principles to specific contexts without introducing significant reasoning errors or self-bias.
- **Evidence anchors:**
  - [abstract] "We present a set of metrics to systematically analyze when and how discretion in AI alignment is exercised..."
  - [section 5.1] Defines the three cases: Consensus, Conflict, and Indifference based on principle-specific preferences.
  - [corpus] "Statutory Construction and Interpretation for AI" supports the mechanism that ambiguity in principles necessitates interpretation, mirroring the paper's reliance on legal theory.
- **Break condition:** If the principles are defined such that they are always "Indifferent" to the specific data pairs, or if the Oracle fails to distinguish between them, the mechanism cannot differentiate random choice from principled discretion.

### Mechanism 2
- **Claim:** Reward Models (RMs) function as "discretion repositories" that capture human prioritization logic, but standard RLHF fails to fully transfer this specific hierarchy to the final Language Model (Policy).
- **Mechanism:** RMs are trained on human preference data, effectively learning the weights (priorities) humans assign to conflicting principles. However, when a Policy is optimized via RL to maximize the RM's score, it learns to achieve high reward *without* necessarily replicating the RM's internal principle ranking, resulting in "Discretion Discrepancy."
- **Core assumption:** High reward scores correlate with the intended alignment behavior, even if the model's internal reasoning (discretion) differs from the human's.
- **Evidence anchors:**
  - [abstract] "...algorithms trained on these datasets develop their own forms of discretion in interpreting and applying these principles..."
  - [section 6.2] "Reward models show promise in learning human discretion patterns... [but] significant discrepancy when transferring this to LLMs."
  - [corpus] Corpus signals regarding "Incentivizing High-Quality Human Annotations" suggest that if the reward signal (discretion) isn't high quality or transferable, the mechanism fails.
- **Break condition:** If the RL optimization process includes a regularization term that specifically penalizes deviation from the RM's principle hierarchy, rather than just score maximization.

### Mechanism 3
- **Claim:** Measuring "Arbitrariness" (disagreeing with Principle Consensus) exposes the "Kangaroo Court" risk where alignment processes legitimize outcomes that are effectively random or biased.
- **Mechanism:** By calculating the probability that an annotator disagrees with a case where principles unambiguously favor one response (Consensus), the system quantifies the noise or anti-normative behavior in the dataset. High arbitrariness suggests that the "ground truth" labels are unreliable for training principle-following agents.
- **Core assumption:** A "Consensus" of principles actually represents the desired ground truth, and disagreement is an error or bias rather than a superior nuanced interpretation.
- **Evidence anchors:**
  - [section 5.2] Defines "Discretion Arbitrariness" as $Pr(\text{disagreement} \mid \text{Consensus})$.
  - [table 1] Shows human arbitrariness is ~28.9% on HH-RLHF, indicating high noise.
  - [corpus] "It's only fair when I think it's fair" highlights how bias alignment can undermine fairness, relevant to how arbitrary discretion affects outcomes.
- **Break condition:** If the principle set $C$ is flawed or incomplete, what appears as "Arbitrariness" may actually be the annotator correctly following a valid principle missing from the set.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - **Why needed here:** The paper critiques the standard RLHF pipeline as the primary vector where "discretion" is injected and potentially distorted during the transfer from RM to Policy.
  - **Quick check question:** Can you explain the role of the Reward Model as an intermediary between human preferences and the final LLM policy?

- **Concept: Judicial Discretion (Hart/Dworkin)**
  - **Why needed here:** The paper borrows heavily from legal philosophy to define "alignment discretion," specifically the concept that rules (principles) require interpretation when they conflict.
  - **Quick check question:** How does the legal concept of "Supremacy" (one right overriding another) differ from a simple "error" in classification?

- **Concept: Bradley-Terry-Luce (BTL) Model**
  - **Why needed here:** This is the mathematical framework used to map preference data into probabilities, which underpins how the Reward Model is trained and how "quality" is inferred.
  - **Quick check question:** In the BTL model, what does a probability of 0.5 imply about the difference in latent "quality" between two responses?

## Architecture Onboarding

- **Component map:** Principle Set ($C$) -> Oracle ($Pref_{oracle}$) -> Annotator ($Pref_a$) -> Metrics Engine -> Discretion Gap
- **Critical path:** The selection of the **Oracle** and the **Principle Set**. If the Oracle cannot reliably distinguish nuances (e.g., it is indifferent to most pairs), or if the Principles are too vague, the classification of "Conflict" vs. "Consensus" becomes noise, rendering the Arbitrariness metric invalid.
- **Design tradeoffs:**
  - **Oracle Selection:** Using a strong model like GPT-4o ensures high-quality classification but introduces self-consistency bias (it agrees with itself), potentially underestimating arbitrariness for models similar to it.
  - **Granularity:** A small set of broad principles increases "Indifference" (no principle applies), while a large set increases "Conflict" (principles overlap and contradict).
- **Failure signatures:**
  - **High Indifference:** The principle set is irrelevant to the dataset domain.
  - **High Discretion Discrepancy:** The RLHF process is effectively "reward hacking"—finding outputs that please the RM without adhering to the principle hierarchy.
  - **High Arbitrariness:** The dataset labels are low quality or the annotators are ideologically misaligned with the defined constitution.
- **First 3 experiments:**
  1. **Oracle Consistency Check:** Run the principle evaluation on a subset of data using a different Oracle model (e.g., Claude or Llama) to measure how sensitive the "Consensus" classification is to the choice of judge.
  2. **Dataset Audit:** Apply the Arbitrariness metric to your internal alignment datasets to identify specific annotators or segments with high conflict/consensus deviation before training.
  3. **RM-to-Policy Transfer:** Train a Reward Model and calculate its Supremacy matrix. Then perform RLHF and calculate the Supremacy matrix of the resulting Policy. Measure the Kendall tau distance to quantify the "Discretion Loss" during optimization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can human discretion captured by reward models be reliably transferred to Large Language Models (LLMs)?
- Basis in paper: [explicit] The introduction states that "translating human discretion from reward models to LLMs is an open problem" and suggests RLHF may be insufficient.
- Why unresolved: The experiments show that while fine-tuned reward models mirror human discretion well, LLMs fine-tuned using these reward models develop significantly different principle priorities.
- What evidence would resolve it: An alignment algorithm that trains an LLM to exhibit a principle priority ranking with a low Discretion Discrepancy relative to the reward model.

### Open Question 2
- Question: Can the Discretion Discrepancy (DD) metric be directly minimized as a training objective?
- Basis in paper: [explicit] In Section 5.3, the authors define the Discretion Discrepancy metric and explicitly state: "We leave the metric’s direct minimization for future work."
- Why unresolved: The paper currently uses the metric only as an audit tool to characterize existing divergence rather than as a loss function to constrain model behavior.
- What evidence would resolve it: A training methodology that incorporates the DD metric into the optimization process and successfully reduces the Kendall tau distance between model and human rankings.

### Open Question 3
- Question: How do distinct demographic or cultural communities exercise alignment discretion differently?
- Basis in paper: [explicit] The authors state in Section 2: "We hope that future work analyzes how different communities exercise their power of discretion."
- Why unresolved: The current analysis focuses on aggregate human preferences in safety datasets without isolating how principle prioritization varies across diverse social or political backgrounds.
- What evidence would resolve it: An application of the paper's metrics to a multicultural dataset (e.g., PRISM) revealing distinct principle supremacy hierarchies for different demographic groups.

## Limitations

- Reliance on GPT-4o as oracle may introduce circular reasoning and systematic underestimation of discretion discrepancies
- Principle set derived from Collective Constitutional AI may not generalize across different alignment philosophies or cultural contexts
- Does not address how to resolve conflicts when principles themselves are fundamentally incompatible

## Confidence

- **High Confidence:** The mechanism of measuring arbitrariness against principle consensus (Mechanism 3) is well-defined and directly testable. The observation that HH-RLHF shows ~28.9% arbitrariness is robust.
- **Medium Confidence:** The claim that reward models capture human discretion patterns but fail to transfer them to policies (Mechanism 2) is supported by the analysis, but the underlying assumption that high RM scores should correlate with principle hierarchy fidelity needs more validation.
- **Low Confidence:** The assertion that discretion "remains largely unexamined" in the field overall, while likely true in formal literature, may overstate the novelty given existing discussions of annotation subjectivity in ML.

## Next Checks

1. **Oracle Consistency Audit:** Run the principle evaluation on a subset of data using different strong LLMs (Claude, Llama-3) to measure sensitivity of "Consensus" classification to judge selection.
2. **Principle Relevance Validation:** Calculate the proportion of Indifference cases across different dataset domains to test whether principles are appropriately granular for the task.
3. **Transfer Fidelity Test:** Train a Reward Model and measure the Kendall tau distance between its principle priority weights and those of the RLHF-optimized policy to quantify discretion loss.