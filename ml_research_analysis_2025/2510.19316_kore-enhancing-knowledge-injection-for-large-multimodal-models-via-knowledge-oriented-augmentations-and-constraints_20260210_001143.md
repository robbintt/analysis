---
ver: rpa2
title: 'KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented
  Augmentations and Constraints'
arxiv_id: '2510.19316'
source_url: https://arxiv.org/abs/2510.19316
tags:
- knowledge
- performance
- kore
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KORE is a synergistic method for knowledge injection into large
  multimodal models, addressing the challenge of learning new knowledge while preserving
  old knowledge. The method combines knowledge-oriented augmentations that automatically
  convert individual knowledge items into structured and comprehensive formats, and
  constraints that minimize interference with previous knowledge by initializing adapters
  in the null space of covariance matrices.
---

# KORE: Enhancing Knowledge Injection for Large Multimodal Models via Knowledge-Oriented Augmentations and Constraints

## Quick Facts
- **arXiv ID**: 2510.19316
- **Source URL**: https://arxiv.org/abs/2510.19316
- **Reference count**: 40
- **Primary result**: KORE achieves 12.63+ improvement in CEM and 21.27 in F1-Score over LoRA on EVOKE while preserving old knowledge across 12 retention benchmarks

## Executive Summary
KORE addresses the challenge of knowledge injection in large multimodal models, where models must learn new knowledge while preserving previously acquired capabilities. The method combines knowledge-oriented augmentations that convert raw knowledge into structured dialogues and instruction tasks, with constraints that project adapter weights into the null space of pre-trained knowledge's covariance matrix. This synergistic approach enables superior performance in adapting to new knowledge (e.g., 18.53+ improvement in knowledge adaptation) while effectively mitigating catastrophic forgetting across all retention benchmarks.

## Method Summary
KORE operates through two synergistic mechanisms: knowledge-oriented augmentations and null space constraints. The augmentation pipeline automatically converts individual knowledge items into structured multi-round dialogues and visual question-answer pairs, generating approximately 75K dialogue samples and 46K VQA samples from the original knowledge. The constraint mechanism extracts activations from LMM linear layers on representative samples, computes their covariance matrix, and uses SVD to project adapter weights into the null space, ensuring gradient updates don't interfere with prior knowledge patterns. This approach modifies the standard LoRA architecture by freezing the adapter matrix A in the null space while training only the B matrix.

## Key Results
- KORE achieves 12.63+ improvement in CEM and 21.27 in F1-Score over LoRA on EVOKE benchmark
- Full-FT fine-tuning degrades LLaVA-v1.5-7B's MME score from 66.63 to 34.17, while KORE maintains performance
- KORE preserves old knowledge across all 12 retention benchmarks with less than 5% decline compared to baseline models
- The method demonstrates effectiveness across multiple base models including LLaVA-v1.5-7B/13B and Qwen2.5-VL-7B

## Why This Works (Mechanism)

### Mechanism 1
Converting raw knowledge into structured multi-round dialogues and instruction tasks improves knowledge internalization beyond surface-level augmentation. The augmentation pipeline builds a "knowledge tree" with dialogue trunks and task branches, generating ~75K dialogue samples and ~46K VQA samples from original knowledge, forcing the model to learn conceptual relationships rather than surface patterns. Core assumption: Structured, interconnected training data promotes "knowledge internalization" rather than "data memorization." Break condition: If general augmentation methods (synonym replacement, image rotation) matched KORE's 18.53 improvement in knowledge adaptation.

### Mechanism 2
Projecting adapter weights into the null space of pre-trained knowledge's covariance matrix constrains updates to directions that preserve prior knowledge. Covariance matrix C = XX^T captures activation patterns from pre-trained knowledge. SVD extracts null space Û where Û^T C ≈ 0. Projecting W₀ through P = ÛÛ^T initializes adapter A in null space, then freezing A ensures AC ≈ 0, meaning gradient updates via B don't interfere with prior activations. Core assumption: Covariance matrices of linear layer activations meaningfully encode pre-trained knowledge patterns. Break condition: If random adapter initialization or standard LoRA with regularization achieved comparable retention scores.

### Mechanism 3
The combination of structured augmentation and null space constraint together optimize the adaptation-retention trade-off better than either alone. Augmentation maximizes new knowledge acquisition through comprehensive coverage; constraint minimizes interference with prior knowledge through orthogonal gradient directions; together they form a synergistic system where each addresses a different objective. Core assumption: Knowledge adaptation and retention compete for model capacity and require explicit mechanisms to balance. Break condition: If Full-FT with augmentation alone achieved KORE's combined performance across both adaptation and retention metrics.

## Foundational Learning

- **Concept: Catastrophic Forgetting in Sequential Learning**
  - Why needed here: This is the core problem KORE addresses—models losing previously learned capabilities when acquiring new knowledge
  - Quick check question: What happens to LLaVA-v1.5's MME score after Full-FT fine-tuning on new knowledge? (Answer: drops from 66.63 to 34.17)

- **Concept: Null Space of a Matrix**
  - Why needed here: Understanding the constraint mechanism requires grasping what null spaces are and why projecting into them preserves certain properties
  - Quick check question: If matrix A is in the null space of covariance matrix C, what is the product AC? (Answer: approximately zero matrix)

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: KORE builds on LoRA architecture; understanding the base mechanism (W* = W₀ + BA) is essential before the null space modification
  - Quick check question: In standard LoRA, what happens to the original weight matrix W₀ during fine-tuning? (Answer: It remains frozen; only B and A are trained)

## Architecture Onboarding

- **Component map:**
  ```
  KORE-AUGMENTATION Pipeline: Raw knowledge → GPT-4o dialogue generation → Google Image Search + CLIP filtering → KORE-74K dataset
  KORE-CONSTRAINT Module: Sample activation data → Extract activations X → Compute C = XX^T → SVD decomposition → Extract Û (null space) → Project W₀ → Initialize B, A via SVD(W₀ÛÛ^T) → Freeze A
  Modified Training Loop: Standard LoRA training with frozen A matrix (only B updates during backprop)
  ```

- **Critical path:**
  1. Covariance matrix extraction from representative samples (paper uses 64 samples per category from OneVision)
  2. SVD decomposition and null space approximation (computational bottleneck—O(d³) for d×d covariance)
  3. Weight projection and adapter initialization before any training
  4. Training with augmented KORE-74K dataset

- **Design tradeoffs:**
  - **Rank selection**: Higher rank (256 vs 235) improves both adaptation and retention but increases parameters (369M vs 340M trainable)
  - **Covariance sample source**: General samples vs. task-specific samples (Table 13 shows task-specific improves targeted retention by ~7 points)
  - **Augmentation depth**: More comprehensive augmentation (10 dialogues, 5 VQA per item) improves adaptation but 10x+ dataset expansion

- **Failure signatures:**
  - **Overfitting to training data**: Very low loss on EVOKE but poor CEM/F1 generalization (Full-FT exhibits this)
  - **Constraint failure**: Sharp retention drops in MME/MMBench (>20 point decline indicates null space projection ineffective)
  - **Insufficient augmentation**: CEM <20 on EVOKE despite training suggests knowledge not being internalized

- **First 3 experiments:**
  1. **Baseline reproduction**: Train standard LoRA and KORE(r=235) on EVOKE, evaluate on all 12 retention benchmarks to confirm the adaptation-retention gap (expect ~12+ CEM improvement over LoRA with retention within 5% of original)
  2. **Ablation validation**: Run W/o Augmentation and W/o Constraint variants on LLaVA-v1.5-7B to verify each component's contribution (expect 15-20 point drops in respective metrics)
  3. **Covariance source test**: Build covariance matrices from 256 MME-specific samples vs. general samples, compare MME retention scores (expect ~7 point improvement with targeted constraint as shown in Table 13)

## Open Questions the Paper Calls Out

### Open Question 1
Can the computational cost of KORE be reduced by identifying specific critical linear layers for covariance computation without compromising knowledge retention performance? The current method applies constraints to all linear layers, which is resource-intensive; the sensitivity of the retention capability to layer selection is unknown. What evidence would resolve it: An ablation study showing retention performance (e.g., on MME or POPE) when applying the null space constraint only to attention layers or specific MLP layers versus the full model.

### Open Question 2
How can the KORE-augmentation pipeline be modified to effectively detect and filter hallucinations introduced by the GPT-4o generation process? The current pipeline relies on the automated generation of multi-round dialogues and VQA data, potentially embedding false knowledge that the model subsequently "internalizes." What evidence would resolve it: A comparison of knowledge adaptation accuracy using a "gold-standard" human-verified dataset versus the current automated pipeline to quantify the impact of GPT-4o hallucinations.

### Open Question 3
Does the null space projection method remain effective for sequential knowledge injection where the model undergoes multiple, successive fine-tuning stages? The methodology computes the covariance matrix C based on the pre-trained weights (W₀) to protect "pre-trained knowledge." It does not explicitly discuss how to update C to protect previously injected knowledge in a continuous learning sequence. What evidence would resolve it: An experiment performing sequential fine-tuning on distinct datasets (e.g., EVOKE followed by another news dataset) to see if retention of the first injected dataset degrades.

### Open Question 4
Would integrating reinforcement learning or knowledge graph structures into the augmentation process significantly improve the "internalization" of complex knowledge compared to the current tree-like structure? The current augmentation creates "profound" but individual knowledge trees; it is unclear if modeling inter-knowledge relationships via graphs or RL rewards yields better generalization. What evidence would resolve it: Evaluation on a dataset requiring complex multi-hop reasoning across injected knowledge items to compare standard KORE augmentation against a graph-enhanced version.

## Limitations

- Extracting covariance matrices from all linear layers is computationally expensive and resource-intensive, suggesting future work should identify critical layers to reduce consumption
- The augmentation process relies on GPT-4o, which may introduce hallucinations into the training data that the model subsequently learns
- The current augmentation is confined to individual knowledge units rather than modeling inter-knowledge relationships, which could limit complex reasoning capabilities

## Confidence

- **High confidence**: The adaptation-retention tradeoff is empirically validated through extensive experiments showing KORE outperforms baselines across multiple benchmarks (CEM improvements of 12.63+ and retention scores within 5% of original)
- **Medium confidence**: The mechanism of null space projection preserving knowledge is supported by mathematical derivation and visualization, but the theoretical foundation connecting covariance patterns to "knowledge" is not rigorously established
- **Medium confidence**: The augmentation pipeline's effectiveness is demonstrated through performance improvements, but the contribution of specific components (dialogue depth, VQA format) to knowledge internalization is not isolated

## Next Checks

1. **Covariance source validation**: Systematically test whether using task-specific covariance samples versus general samples provides consistent retention improvements across all 12 retention benchmarks, not just MME
2. **Null space approximation sensitivity**: Evaluate how KORE's performance varies with different rank selections for the null space approximation and with different numbers of samples used to compute the covariance matrix
3. **Ablation of augmentation components**: Conduct a finer-grained ablation study isolating the contribution of multi-round dialogues versus VQA generation to determine which augmentation format drives the 18.53+ CEM improvement on EVOKE