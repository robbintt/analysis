---
ver: rpa2
title: 'InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction'
arxiv_id: '2403.19652'
source_url: https://arxiv.org/abs/2403.19652
tags:
- motion
- human
- arxiv
- interaction
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces InterDreamer, a novel zero-shot framework
  for generating 3D human-object interaction (HOI) sequences guided by text descriptions.
  The key insight is decoupling interaction semantics and dynamics, leveraging large
  language models and pre-trained text-to-motion models for semantic alignment, while
  using a world model to learn object dynamics from contact vertices.
---

# InterDreamer: Zero-Shot Text to 3D Dynamic Human-Object Interaction

## Quick Facts
- arXiv ID: 2403.19652
- Source URL: https://arxiv.org/abs/2403.19652
- Authors: Sirui Xu; Ziyin Wang; Yu-Xiong Wang; Liang-Yan Gui
- Reference count: 40
- Primary result: Zero-shot text-to-3D human-object interaction generation using semantic-dynamics decoupling and LLM-based prompt rewriting

## Executive Summary
InterDreamer introduces a novel zero-shot framework for generating 3D human-object interaction sequences from text descriptions without requiring paired text-interaction training data. The key innovation is decoupling interaction semantics from dynamics, using LLMs and pre-trained text-to-motion models for semantic alignment while learning object dynamics from contact vertices. The framework leverages large language models to bridge the distribution gap between interaction prompts and motion model training data, while a learned world model predicts object motion based on local contact vertex information. Evaluated on BEHAVE, OMOMO, and CHAIRS datasets, InterDreamer achieves superior interaction quality metrics compared to baselines, demonstrating strong generalization to novel objects and out-of-distribution text prompts.

## Method Summary
InterDreamer operates through a pipeline that separates high-level semantic planning from low-level dynamic control. The high-level planner uses an LLM to extract object category, contact body part, and rewrite text prompts to align with pre-trained motion model distributions. The low-level control generates human actions via text-to-motion models, retrieves initial object states from interaction databases, and uses a world model to predict object dynamics based on contact vertex features through cross-attention mechanisms. An optimization loop iteratively refines poses to minimize penetration and contact losses. The system is trained on BEHAVE dataset and evaluated on BEHAVE, OMOMO, and CHAIRS datasets, with segment length m=4 frames and 500 epochs of world model training.

## Key Results
- Achieves lower FID score (0.151 vs 0.219 baseline) on interaction quality
- Demonstrates better CMD score (443 vs 484) for contact alignment
- Shows strong generalization to novel objects and out-of-distribution text prompts

## Why This Works (Mechanism)

### Mechanism 1: Semantic-Dynamics Decoupling via Asymmetric Knowledge Sources
Generating 3D human-object interactions without paired text-interaction data is possible because high-level interaction semantics (the "what") can be decoupled from low-level object dynamics (the "how"). The framework separates generation into two streams: semantic stream uses LLM to rewrite prompts and extract contact logic for text-to-motion models, while dynamics stream uses a learned World Model to predict object motion based on human contact vertices. These streams synchronize through initial state retrieval and optimization loops. This works because text-to-motion models trained on single-human data can produce valid interaction poses if prompts are normalized, and object motion is locally deterministic based on contact vertices.

### Mechanism 2: Distribution Alignment via LLM "Retrieval-Augmented" Prompting
Pre-trained motion models fail on HOI prompts due to distribution gaps between standard motion captions ("a person walks") and interaction descriptions ("someone pulls a chair"). The High-Level Planning module bridges this gap by having the LLM parse input text into object category, contact body part, and simplified description. This simplification rewrites complex HOI commands into the style of HumanML3D dataset, projecting out-of-distribution interaction requests into the known manifold of the motion generator. The LLM possesses sufficient common sense physical reasoning to identify appropriate contact points, assuming the target motion distribution covers the geometric requirements.

### Mechanism 3: Generalizable Dynamics via Contact-Vertex Abstraction
A dynamics model trained on limited HOI datasets can generalize to novel objects by ignoring global object geometry and focusing on local contact vertices. The World Model samples N vertices on the human body and projects them to the object surface, learning a mapping from local spatial trajectories to object displacement. By abstracting action as "vertex velocities" and state as "vertex-to-surface vectors," the model learns interaction physics (pushing/pulling) that is agnostic to whether the object is a specific chair or box, provided contact topology is similar. This works because object motion is dictated primarily by local forces at contact points.

## Foundational Learning

- **Concept: Markov Decision Processes (MDP)**
  - Why needed here: The paper explicitly formulates interaction generation as an MDP where object state is $s_t$ and human action is $a_t$. Understanding state transitions ($s_{t+1} \sim P(s_{t+1}|s_t, a_t)$) is required to grasp the World Model's role.
  - Quick check question: Can you identify what represents the "Action" and what represents the "State" in the InterDreamer architecture?

- **Concept: Cross-Attention in Transformers**
  - Why needed here: The World Model uses cross-attention to condition object trajectory features on human vertex features. You need to understand how one modality (human vertices) guides generation of another (object motion).
  - Quick check question: In the World Model's attention block `Attn(G(...), F(...))`, which input represents the query (object trajectory) and which represents the key/value (human control)?

- **Concept: In-Context Learning (LLMs)**
  - Why needed here: The High-Level Planning relies on Few-Shot and Chain-of-Thought prompting to extract contact info. You must distinguish this from fine-tuning; the LLM is not retrained, but guided via prompt window.
  - Quick check question: Why does the paper prefer "simplifying and modifying the description" rather than just asking the LLM to generate the motion directly?

## Architecture Onboarding

- **Component map:** Text → LLM (High-Level Planner) → (Object Category, Body Part, Simplified Text) → Text-to-Motion → Human Action sequence → World Model (with contact vertices) → Object State → Optimization Loop → Final Poses

- **Critical path:** The inference loop does not generate the full sequence at once. It generates a segment of human motion, then queries the World Model to update the object. If this feedback loop desynchronizes, the optimization step must resolve it, or the sequence will exhibit jitter.

- **Design tradeoffs:**
  - Vertex Abstraction vs. Full Geometry: Using only contact vertices allows generalization to unseen objects (zero-shot) but loses global context, potentially leading to physically inaccurate accelerations
  - Retrieval vs. Generation: Initial states are retrieved from a database rather than generated, ensuring high-quality initial contact but limiting the initial pose space to what exists in training data

- **Failure signatures:**
  - Jitter/Drift: Occurs if World Model prediction error accumulates; look for objects "sliding" slightly out of hands
  - Semantic Drift: LLM rephrasing fails to capture nuance, resulting in person "picking up" when they should have "kicked"
  - Penetration: Optimizer fails to resolve collisions in complex geometry (e.g., hands inside cup handles)

- **First 3 experiments:**
  1. Validate Distribution Alignment: Run prompts with and without "High-Level Planning" (LLM rewriting) directly through standard Motion Generator (like MotionGPT). Visualize difference in semantic fidelity.
  2. World Model Generalization Test: Train World Model only on "backpack" interactions from BEHAVE and test inference on "chair" objects. Check if vertex-based control still produces plausible "carrying" motions.
  3. Ablation on Vertex Count (N): Vary number of sampled contact vertices in World Model input. Test if using only 1 vertex (point contact) degrades performance compared to default setting.

## Open Questions the Paper Calls Out
None

## Limitations
- Zero-shot generalization scope is primarily limited to common household items with similar geometric properties to BEHAVE training set
- Framework heavily depends on LLM's ability to rewrite prompts in style compatible with pre-trained motion models, creating fragile dependency chain
- Evaluation focuses on visual metrics and geometric measures but does not assess physical plausibility in terms of realistic force application or momentum conservation

## Confidence
**High Confidence:**
- Semantic-dynamics decoupling mechanism effectively reduces data requirement for HOI generation
- Contact-vertex abstraction provides practical generalization strategy for object dynamics
- LLM-based prompt rewriting successfully bridges distribution gap between interaction descriptions and standard motion captions

**Medium Confidence:**
- Zero-shot generalization extends meaningfully beyond training distribution to truly novel object categories
- Iterative optimization reliably resolves physical inconsistencies between human motion and object dynamics
- Vertex-based dynamics model captures sufficient physical reasoning for common interaction types

**Low Confidence:**
- Framework generalizes to highly articulated objects or interactions requiring fine-grained manipulation
- System maintains stability for long interaction sequences (>10 seconds) without accumulating prediction errors
- LLM rewriting strategy remains effective across diverse interaction domains beyond household manipulation

## Next Checks
1. **Object Category Generalization Stress Test:** Evaluate InterDreamer on objects requiring significantly different interaction strategies than those in BEHAVE (tools, sports equipment, clothing). Measure whether contact-vertex abstraction still produces plausible dynamics or if the model overfits to chair/box-like geometries.

2. **Physical Plausibility Benchmark:** Implement physics-based validation using a simulator (e.g., PyBullet) to check whether generated sequences respect conservation of momentum, realistic force magnitudes, and human balance constraints. Compare against baseline methods on both geometric metrics and physical plausibility scores.

3. **Long-Horizon Stability Analysis:** Generate interaction sequences of 30+ seconds and analyze error accumulation patterns. Track whether World Model's prediction errors compound over time, causing objects to drift from intended trajectories, and whether optimization loop can compensate for these errors in extended interactions.