---
ver: rpa2
title: Large Language Models are Few-shot Multivariate Time Series Classifiers
arxiv_id: '2502.00059'
source_url: https://arxiv.org/abs/2502.00059
tags:
- time
- series
- data
- learning
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes LLMFew, a framework for few-shot multivariate
  time series classification that leverages pre-trained knowledge in large language
  models (LLMs). The key innovation is a Patch-wise Temporal Convolution Encoder (PTCEnc)
  that aligns time series data with the textual embedding input of LLMs, combined
  with LoRA fine-tuning to enhance the LLM's feature representation learning.
---

# Large Language Models are Few-shot Multivariate Time Series Classifiers

## Quick Facts
- **arXiv ID:** 2502.00059
- **Source URL:** https://arxiv.org/abs/2502.00059
- **Reference count:** 40
- **Primary result:** LLMFew achieves 125.2% and 50.2% improvement in 1-shot classification accuracy on Handwriting and EthanolConcentration datasets respectively

## Executive Summary
This paper introduces LLMFew, a framework that leverages pre-trained large language models (LLMs) for few-shot multivariate time series classification. The key innovation is a Patch-wise Temporal Convolution Encoder (PTCEnc) that aligns time series data with LLM embeddings, combined with LoRA fine-tuning to enhance feature learning. The approach demonstrates strong performance across 10 datasets, significantly outperforming traditional models in 1-shot and few-shot scenarios. This work shows that LLM-based methods can overcome data scarcity in industrial applications where labeled data is limited.

## Method Summary
LLMFew processes multivariate time series by first dividing each univariate series into patches, which are then passed through a PTCEnc (stacked causal convolution blocks with dilation) to align with the LLM's embedding space. The encoded patches are fed into a pre-trained LLM backbone (e.g., GPT-2) with LoRA fine-tuning applied to the Query, Key, and Value attention parameters. An MLP classification head with skip connection produces the final output. The model is trained with cross-entropy loss and evaluated on 10 UEA Archive datasets using N-way-K-shot classification.

## Key Results
- Achieves 125.2% improvement in 1-shot classification accuracy on Handwriting dataset
- Demonstrates 50.2% improvement on EthanolConcentration dataset in 1-shot setting
- Outperforms traditional models across various datasets in few-shot scenarios
- Shows strong generalization from pre-trained knowledge without requiring source domain data

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLM pre-trained knowledge provides a functional prior for sequential modeling that transfers to time series data.
- **Mechanism:** Treats LLM's pre-trained textual knowledge as a substitute for source domain data, aligning time series inputs to leverage existing sequential dependency modeling.
- **Core assumption:** Sequential patterns in text (syntax, long-range dependencies) share structural similarity with temporal patterns in time series.
- **Evidence anchors:** Abstract states LLM knowledge overcomes data scarcity; section 4.2.1 shows transformer-based time series models fail without pre-trained knowledge.
- **Break condition:** Complex domain-specific physical laws (e.g., fluid dynamics) with no linguistic analog may fail to generalize.

### Mechanism 2
- **Claim:** PTCEnc is strictly necessary to align time series patches with LLM's semantic space; direct projection is insufficient.
- **Mechanism:** Uses stacked causal convolutions with dilation to capture local temporal features before they reach the LLM, acting as a translation layer.
- **Core assumption:** Time series data contains local temporal dynamics that must be explicitly encoded before global sequence modeling.
- **Evidence anchors:** Section 3.2.2 describes PTCEnc alignment; section 4.2.5 shows removing PTCEnc causes significant accuracy drops (34.3% to 19.3% on Handwriting).
- **Break condition:** If patching strategy destroys signal (e.g., stride too large), PTCEnc will encode noise.

### Mechanism 3
- **Claim:** LoRA allows efficient few-shot feature learning while preventing catastrophic forgetting or overfitting from full fine-tuning.
- **Mechanism:** Injects trainable rank-decomposition matrices into LLM's attention layers (Q, K, V), restricting parameter updates to low-dimensional subspace.
- **Core assumption:** Adaptation required for time series classification lies in a low-dimensional intrinsic subspace.
- **Evidence anchors:** Section 3.2.3 describes LoRA application; section 4.2.5 shows "Frozen" variant underperforms, validating backbone utility.
- **Break condition:** If rank r is too low, model cannot capture inter-class differences; if too high, may overfit to few samples.

## Foundational Learning

- **Concept:** **Few-shot Learning (N-way-K-shot)**
  - **Why needed here:** Central constraint—model trained with only K samples per class, making standard deep learning impossible due to overfitting.
  - **Quick check question:** If you have 5 classes and only 1 example per class, what is the "N-way-K-shot" setting? (Answer: 5-way-1-shot).

- **Concept:** **Low-Rank Adaptation (LoRA)**
  - **Why needed here:** Paper relies on LoRA to fine-tune LLM—freezes main weights W and learns separate matrix ΔW = BA (rank r ≪ d).
  - **Quick check question:** Does LoRA retrain all parameters of the LLM? (Answer: No, it injects and trains small adapter matrices).

- **Concept:** **Causal Convolution & Dilation**
  - **Why needed here:** PTCEnc uses this to process time series—causal convolutions only look at past data, dilation increases receptive field exponentially.
  - **Quick check question:** Why use a dilation factor of 2^d in the encoder? (Answer: To capture long-range dependencies without excessive computational cost).

## Architecture Onboarding

- **Component map:** Input: Multivariate Time Series (M variables, L length) → Patching: Splits into patches (P length, S stride) → PTCEnc: Causal Conv Encoder (D depth, dilation 2^d) → Aligns to LLM embedding dim → LLM Backbone: Pre-trained Decoder with LoRA in Attention (Q, K, V) → Head: MLP + Softmax for classification

- **Critical path:** PTCEnc alignment is most fragile component—paper compares simple 1D conv (failure) vs. stacked causal conv (success). Patching parameters and LoRA rank determine success.

- **Design tradeoffs:**
  - **LLM Size vs. Data Size:** Table 3 shows larger LLMs (Llama3 8B) don't always beat smaller ones (GPT2-small) on small datasets. Guidance: Start with smaller backbone (GPT-2 Small/Medium) for few-shot to avoid overfitting and reduce memory.
  - **Patching Granularity:** Shorter patches capture fine details but increase sequence length (memory cost). Longer patches lose granularity.

- **Failure signatures:**
  - **High Dimensionality:** Struggled with PEMS-SF dataset (963 features)—LLM struggles to model dense inter-variable relationships compared to GNNs.
  - **Frozen Backbone:** Using LLM purely as feature extractor (Frozen baseline) results in performance drop vs. LoRA, indicating pre-trained weights alone aren't perfectly aligned.

- **First 3 experiments:**
  1. **Baseline Validation:** Run 1-shot on Handwriting dataset using GPT-2 Small. If no significant jump over Transformer baseline (~+20%), check PTCEnc implementation (skip connections, dilation).
  2. **Ablation (Encoder vs. Projection):** Replace PTCEnc with simple Linear layer ("w/o PTCEnc"). Verify performance drop (should be drastic)—confirms mechanism learning temporal features, not just projecting dimensions.
  3. **LoRA Rank Sensitivity:** Test ranks r ∈ {4, 8, 16} on EthanolConcentration dataset. Verify higher r doesn't degrade performance in 1-shot (overfitting check).

## Open Questions the Paper Calls Out

- **Open Question 1:** How can LLM-based frameworks better model inter-dimensional relationships in high-dimensional multivariate time series?
  - **Basis in paper:** [Explicit] Section 4.2.1 notes lower performance on PEMS-SF (963 features) and asks "how to model relationships between features in MTSC task as dimensions increase..."
  - **Why unresolved:** LLMFew achieved non-significant improvement (0.7%) on high-dimensional traffic dataset vs. other datasets.
  - **What evidence would resolve it:** Architectural extension capturing cross-dimension dependencies with statistically significant gains on high-dimension benchmarks.

- **Open Question 2:** Which specific aspects of LLMs contribute most to learning temporal representations: pre-trained knowledge or underlying architecture?
  - **Basis in paper:** [Explicit] Conclusion states intent to "further investigate which aspects of LLMs are most effective for learning temporal representations..."
  - **Why unresolved:** While pre-trained knowledge helps, paper doesn't disentangle benefits of Transformer architecture vs. linguistic knowledge.
  - **What evidence would resolve it:** Comparative study evaluating same architecture with pre-trained weights vs. random initialization on few-shot MTSC tasks.

- **Open Question 3:** Can a universal patching strategy be developed that adapts to varying data periodicities without manual tuning?
  - **Basis in paper:** [Inferred] Section 4.2.7 states "we have yet to find a patch length that is universally applicable to all datasets," noting periodicity depends on acquisition methods.
  - **Why unresolved:** Framework relies on manually tuned hyperparameters (patch length, stride) for specific datasets.
  - **What evidence would resolve it:** Self-adaptive patching module maintaining consistent accuracy across UEA archive without dataset-specific hyperparameter selection.

## Limitations

- **Hyperparameter transparency:** Critical values for LoRA rank, PTCEnc architecture, and patching parameters not specified per dataset
- **High-dimensionality challenge:** Poor performance on PEMS-SF dataset (963 features) suggests LLM struggles with dense inter-variable relationships
- **Baseline comparison scope:** Claims of LLM superiority rest on specific baseline comparisons that may shrink with newer specialized time series architectures

## Confidence

- **High Confidence:** LoRA adaptation of pre-trained LLMs to time series is well-established (supported by PEFT-MuTS); PTCEnc necessity clearly demonstrated in ablation study
- **Medium Confidence:** General claim that LLMs can be effective few-shot time series classifiers is supported, but specific performance numbers may be sensitive to unreported hyperparameters
- **Low Confidence:** Assertion that larger LLMs don't consistently outperform smaller ones is only partially supported—Table 3 shows mixed results without clear guidance on when to choose which size

## Next Checks

1. **Hyperparameter Sensitivity Sweep:** Systematically vary LoRA rank (r ∈ {4, 8, 16}) and PTCEnc depth on Handwriting dataset. Verify performance degrades predictably when either component is weakened, confirming stated mechanisms.

2. **Cross-Domain Transfer Test:** Apply best-performing model (from Handwriting) to dataset with very different characteristics (e.g., JapaneseVowels or SpokenArabicDigits) without retraining. Measure performance drop to quantify LLM's generalization beyond training data.

3. **PEMS-SF Feature Reduction Experiment:** Reduce 963 features of PEMS-SF to subset (50-100 most informative features using feature importance scores). Retrain and evaluate to determine if LLM's failure is due to dimensionality or specific nature of traffic data.