---
ver: rpa2
title: A Shared Geometry of Difficulty in Multilingual Language Models
arxiv_id: '2601.12731'
source_url: https://arxiv.org/abs/2601.12731
tags:
- language
- difficulty
- cross-lingual
- across
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates how problem difficulty is encoded across
  languages in large language models. The authors train linear probes on the AMC subset
  of Easy2Hard, translated into 21 languages, to decode difficulty from model activations
  at each layer.
---

# A Shared Geometry of Difficulty in Multilingual Language Models

## Quick Facts
- **arXiv ID**: 2601.12731
- **Source URL**: https://arxiv.org/abs/2601.12731
- **Reference count**: 12
- **Primary result**: Difficulty representations in LLMs exhibit a depth-dependent trade-off: shallow layers support cross-lingual transfer while deep layers optimize within-language accuracy.

## Executive Summary
This work investigates how problem difficulty is encoded across languages in large language models. The authors train linear probes on the AMC subset of Easy2Hard, translated into 21 languages, to decode difficulty from model activations at each layer. They find that difficulty representations emerge in two stages: shallow layers support strong cross-lingual generalization but achieve lower within-language accuracy, while deeper layers optimize for language-specific performance but fail to transfer. Across all evaluated models, the best cross-lingual transfer occurs around layer 15, whereas same-language probing peaks around layer 30, revealing a depth-dependent trade-off between shared and language-specific difficulty signals.

## Method Summary
The authors train linear Ridge regression probes on residual stream activations from multilingual LLMs to predict problem difficulty scores. Using the AMC math benchmark translated into 21 languages, they extract activations at each layer for all tokens, focusing on the final token position. Probes are trained separately per language and per layer, with regularization selected via cross-validation. The same problems are used across all languages with consistent train/test splits. Performance is evaluated using Spearman rank correlation, comparing same-language accuracy against cross-lingual transfer to quantify the depth-dependent trade-off.

## Key Results
- Cross-lingual transfer peaks around layer 15 while same-language performance peaks around layer 30
- Shallow-layer probes achieve strong cross-lingual generalization despite lower within-language accuracy
- Deep-layer probes achieve high same-language accuracy but exhibit poor cross-lingual generalization
- The transfer-optimal layer preserves near-optimal within-language performance (0.014 Spearman drop) while transfer drop is much larger (0.177)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Problem difficulty is first encoded in a language-agnostic representation at shallow layers that supports cross-lingual transfer.
- Mechanism: Residual stream activations at early-to-middle layers (~layer 15 in LLaMA-3.1-8B) contain a shared difficulty direction that linear probes can decode regardless of input language. This representation emerges before language-specific processing dominates.
- Core assumption: The linear decodability of difficulty implies a structured representation rather than spurious correlation; this is supported but not causally proven in this work.
- Evidence anchors:
  - [abstract] "Probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance."
  - [section 4.1] "Cross-lingual transfer peaks much earlier (around layer 14)" while "diagonal cells concentrate tightly around a single later (deep) layer."
  - [corpus] Lugoloobi & Russell (2025) demonstrate linear decodability of difficulty in English, which this paper extends multilingually.
- Break condition: If difficulty signals were purely surface-level linguistic features rather than meta-cognitive, shallow-layer transfer would not generalize to typologically distant languages with different surface forms.

### Mechanism 2
- Claim: Deep layers refine difficulty representations in a language-specific manner, improving within-language accuracy but degrading cross-lingual alignment.
- Mechanism: As information flows through transformer layers, the shared difficulty signal is progressively specialized toward the target language's output space. This specialization improves monolingual prediction but breaks cross-lingual geometric alignment.
- Core assumption: The depth-dependent divergence reflects functional specialization rather than representational degradation.
- Evidence anchors:
  - [abstract] "Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization."
  - [section 4.2] "For LLaMA-3.1-8B, fixing probes at the diagonal optimal layer leads to a mean reduction of 0.177 Spearman ρ under transfer."
  - [corpus] Li et al. (2025) find deeper layers become increasingly language-specific with reduced cross-lingual representational similarity.
- Break condition: If deep-layer difficulty signals were language-agnostic, same-language and cross-lingual optima would converge at the same layer.

### Mechanism 3
- Claim: The transfer-optimal layer retains near-optimal within-language performance, but the reverse is not true.
- Mechanism: The language-agnostic difficulty direction established at shallow layers is preserved through later processing; deep layers add language-specific information but do not overwrite the shared representation.
- Core assumption: The shared representation remains accessible in residual stream, not erased by later processing.
- Evidence anchors:
  - [section 4.2] "Evaluating probes monolingually at the off-diagonal-optimal layer results in only a negligible loss in same-language performance (0.014 Spearman ρ)."
  - [Table 1] In-language drop is consistently small across all models (0.007–0.015), while transfer drop is large (0.055–0.192).
  - [corpus] Limited direct corpus evidence on this asymmetric preservation property.
- Break condition: If shallow representations were overwritten rather than augmented by deep layers, probing at transfer-optimal layers would show substantial within-language degradation.

## Foundational Learning

- **Linear Probing with Ridge Regression**
  - Why needed here: The entire methodology depends on training linear classifiers on residual activations to decode continuous difficulty scores.
  - Quick check question: Can you explain why Spearman correlation (rank-based) is preferred over MSE for evaluating probe predictions?

- **Residual Stream Architecture**
  - Why needed here: Probes extract from the residual stream at the final token position; understanding how residual connections preserve information across layers is essential.
  - Quick check question: Why does the final token position capture the most informative representation for difficulty probing?

- **Cross-Lingual Transfer in Representation Space**
  - Why needed here: The core finding hinges on measuring how well probes trained on one language transfer to another, which requires understanding representational alignment.
  - Quick check question: What does it mean geometrically for two languages to "share a difficulty direction" in activation space?

## Architecture Onboarding

- **Component map**:
  Input: AMC math problems (translated to 21 languages) -> Tokenized prompts -> Model forward pass -> Residual stream extraction (all layers, final token) -> Per-layer Ridge regression probes -> Difficulty score predictions -> Spearman evaluation against ground truth

- **Critical path**:
  1. Dataset preparation with language-consistent train/test splits (same problems, different languages)
  2. Activation extraction using TransformerLens at final token per prompt
  3. Per-layer probe training with regularization selection via held-out validation
  4. Cross-lingual evaluation matrix construction (train-lang × test-lang × layer)

- **Design tradeoffs**:
  - Shallow vs. deep extraction: Shallow (layer ~15) optimizes cross-lingual applications; deep (layer ~30) optimizes monolingual accuracy
  - Final token vs. pooled representation: Final token chosen based on prior work showing strongest difficulty signal
  - Ridge regularization strength: Higher values (100-1000) help prevent overfitting on smaller language subsets

- **Failure signatures**:
  - Translation quality degradation: If COMET-Kiwi scores fall below ~0.6, cross-lingual probe transfer degrades (observed for Swahili at 0.61)
  - Layer misalignment: Using deep-layer probes for cross-lingual transfer causes ~0.18 Spearman drop
  - Small model capacity: LLaMA-3.2-1B shows compressed layer separation (optimal layers at 6 and 12 vs. 15 and 30 for 8B)

- **First 3 experiments**:
  1. Replicate Figure 1 for your target model: Extract activations across all layers, train probes per-language, plot cross-lingual Spearman as heatmap to identify transfer-optimal layer.
  2. Ablate translation quality: Train on English, test on high-scoring (Italian, 0.825) vs. low-scoring (Swahili, 0.613) translations to quantify translation-noise sensitivity.
  3. Test domain transfer: Apply the same probing methodology to non-math tasks (e.g., commonsense reasoning) to assess whether the two-stage geometry generalizes beyond mathematical difficulty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the cross-lingual geometry of difficulty representations extend beyond mathematical problem solving to domains with more subjective or context-dependent difficulty, such as commonsense reasoning, programming, or open-ended generation?
- Basis in paper: [explicit] Authors state in Limitations: "It therefore remains unclear whether the same cross-lingual geometry of difficulty extends to domains where difficulty is more subjective or context-dependent, such as commonsense reasoning, programming, or open-ended generation."
- Why unresolved: The study is confined to the AMC math benchmark, which offers well-calibrated human difficulty labels but represents a narrow behavioral slice with relatively objective difficulty.
- What evidence would resolve it: Replicate the probing methodology on benchmarks with subjective difficulty (e.g., commonsense QA, code generation tasks) and assess whether the same shallow-layer cross-lingual transfer and deep-layer specialization pattern emerges.

### Open Question 2
- Question: Can difficulty steering vectors learned in one language be applied to steer model behavior in another language?
- Basis in paper: [explicit] Authors state in Limitations: "Whether analogous interventions transfer across languages—for example, by steering a model using difficulty vectors learned in one language and applied to another—remains an open question, which we leave to future work."
- Why unresolved: Probing establishes presence and alignment of difficulty signals, but does not demonstrate causal transfer of interventions across languages; prior causal evidence exists only for English.
- What evidence would resolve it: Train difficulty steering vectors on one language and evaluate their effect on hallucination reduction or response quality when applied to inputs in different target languages.

### Open Question 3
- Question: Does the depth-dependent trade-off between shared and language-specific difficulty representations hold across diverse model architectures beyond decoder-only LLMs?
- Basis in paper: [explicit] Authors note in Limitations: "it is not yet clear whether the same representational structure is present in all model" after acknowledging evaluation was limited to a small set of instruction-tuned, decoder-only models.
- Why unresolved: Only four decoder-only models were evaluated; encoder-decoder, MoE, or differently trained architectures may organize difficulty representations differently.
- What evidence would resolve it: Apply the same probing methodology to encoder-decoder models (e.g., mT5), mixture-of-experts architectures, or models with substantially different training objectives.

## Limitations
- The study is confined to mathematical problem difficulty, leaving unclear whether the depth-dependent geometry generalizes to domains with subjective or context-dependent difficulty.
- Only four decoder-only, instruction-tuned models were evaluated, limiting conclusions about the universality of the two-stage representation geometry.
- The paper does not establish whether difficulty steering vectors transfer across languages, focusing only on representational alignment rather than causal intervention.

## Confidence
- **High confidence**: The existence of depth-dependent trade-offs between cross-lingual transfer and same-language accuracy, supported by consistent patterns across multiple models and languages.
- **Medium confidence**: The universality of the two-stage geometry as a fundamental LLM property, pending broader validation across tasks and architectures.
- **Medium confidence**: The claim that shallow layers preserve near-optimal within-language performance, though this relies on a single quantitative observation that could benefit from additional validation.

## Next Checks
1. **Task Generalization**: Apply the same probing methodology to non-mathematical tasks (e.g., commonsense reasoning, reading comprehension) to determine whether the depth-dependent difficulty geometry generalizes beyond mathematical problem-solving.

2. **Architecture Ablation**: Test the two-stage pattern in models with different architectural choices (e.g., Mamba, Hyena, or sparse transformers) to assess whether this geometry is specific to standard transformer designs.

3. **Probe Geometry Analysis**: Conduct principal component analysis on the learned probe weights across layers to directly visualize whether difficulty representations become more language-specific at deeper layers, providing geometric confirmation of the depth-dependent specialization hypothesis.