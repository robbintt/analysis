---
ver: rpa2
title: 'Overshoot: Taking advantage of future gradients in momentum-based stochastic
  optimization'
arxiv_id: '2501.09556'
source_url: https://arxiv.org/abs/2501.09556
tags:
- overshoot
- momentum
- weights
- adam
- gradients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Overshoot is a novel momentum-based stochastic optimization method
  that improves convergence by calculating gradients at model weights shifted in the
  direction of the current momentum, rather than at current weights. The method is
  motivated by the observation that consecutive model updates have similar directions,
  making future model weights more relevant for gradient estimation.
---

# Overshoot: Taking advantage of future gradients in momentum-based stochastic optimization

## Quick Facts
- arXiv ID: 2501.09556
- Source URL: https://arxiv.org/abs/2501.09556
- Reference count: 12
- Key outcome: Novel momentum-based optimizer that computes gradients at weights shifted in momentum direction, achieving 15%+ faster convergence across diverse tasks.

## Executive Summary
Overshoot is a momentum-based stochastic optimization method that improves convergence by calculating gradients at model weights shifted in the direction of the current momentum, rather than at current weights. The approach exploits the observation that consecutive model updates have similar directions, making future model weights more relevant for gradient estimation. Implemented efficiently for both SGD and Adam optimizers with zero memory overhead, Overshoot consistently outperforms standard momentum and Nesterov's momentum across diverse tasks including CNNs, transformers, and regression models, achieving at least 15% reduction in training steps while improving test performance.

## Method Summary
Overshoot computes gradients at weights shifted by the momentum vector (θ′ = θ + γ·update) rather than current weights. The method maintains base weights (θ) and overshoot weights (θ′) with an overshoot factor γ controlling the look-ahead distance. This decouples the look-ahead from the momentum coefficient, allowing independent tuning. The approach is implemented efficiently in two variants: SGDO (Algorithm 2) and AdamO (Algorithm 3) with a delay mechanism for AdamO to stabilize early training. Final model weights are recovered by adjusting for the overshoot offset.

## Key Results
- Consistently outperforms classical momentum and Nesterov's accelerated gradient by 15%+ reduction in training steps
- Improves test performance while accelerating convergence across diverse tasks (MLP, CNN, transformers)
- Optimal overshoot factor varies by task, with Awd metric suggesting γ≈2.5 (β₁=0.9) or γ≈5 (β₁=0.95)
- Zero memory overhead implementation for both SGD and Adam optimizers

## Why This Works (Mechanism)

### Mechanism 1: Anticipatory Gradient Estimation via Momentum Extrapolation
Computing gradients at weights shifted along the momentum vector yields more relevant estimates for future optimization steps than computing at current weights. The method exploits the observation that consecutive updates in momentum-based optimizers have similar directions (positive expected cosine similarity), so the overshoot point is predictably closer to future optimization positions.

### Mechanism 2: Decoupled Look-Ahead Factor from Momentum Coefficient
Separating the look-ahead distance (γ) from the momentum decay rate (μ or β₁) allows independent tuning of gradient anticipation versus gradient accumulation. Unlike Nesterov momentum where look-ahead equals μ, Overshoot introduces γ as a free parameter, unifying three SGD variants: γ=0 → Classical Momentum; γ=μ → NAG; γ=μ(1-μ)⁻¹ → Vanilla SGD.

### Mechanism 3: Weighted Distance Minimization for Gradient Relevance
Overshoot reduces the average weighted distance (Awd) between gradient computation points and target model weights, increasing relevance of accumulated momentum gradients. The paper introduces Awd(γ) = (1/N) Σᵢ Σⱼ ||θᵢ − θ′ⱼ||·w(i,j) measuring gradient relevance, hypothesizing that minimizing Awd correlates with faster convergence.

## Foundational Learning

- **Momentum-based optimization (Polyak momentum, Nesterov Accelerated Gradient)**: Why needed: Overshoot builds directly on classical momentum theory and is positioned as an extension and unification of CM and NAG. Understanding how momentum accumulates past gradients is essential to grasp why shifting gradient computation matters.
  - Quick check question: Can you explain why NAG computes gradients at θ − ημm rather than at θ, and what advantage this provides over classical momentum?

- **Stochastic gradient descent dynamics in non-convex landscapes**: Why needed: The core assumption about consecutive update similarity depends on local landscape geometry. Understanding how SGD navigates valleys, saddle points, and noisy gradients helps identify when Overshoot will help versus hurt.
  - Quick check question: In what training scenarios would you expect consecutive SGD updates to have low cosine similarity?

- **Bias-variance tradeoff in gradient estimation**: Why needed: Overshoot trades computing the exact gradient at current weights for an estimated gradient at a predicted future point. This is fundamentally a bias-variance decision depending on trajectory predictability.
  - Quick check question: If momentum direction becomes unpredictable, would Overshoot increase gradient bias, variance, or both?

## Architecture Onboarding

- **Component map**: Base weights (θ) → Overshoot weights (θ′ = θ + γ·update) → Gradient computation at θ′ → Update rule with derived coefficients mc, gc → Final base weight recovery

- **Critical path**:
  1. Initialize: θ₀ = θ′₀, set γ, τ (AdamO only), momentum parameters
  2. Each step: (a) Compute gradient at overshoot weights: g_t = ∇f(θ′_{t-1}); (b) Update momentum; (c) Compute derived coefficients mc, gc; (d) Update weights: θ_t = θ_{t-1} − η(mc·m_t + gc·g_t)
  3. Training end: Recover base weights as θ_t + ηγm_t

- **Design tradeoffs**:
  - Higher γ: More aggressive look-ahead with potentially faster convergence, but increased instability risk if update direction changes
  - Efficient implementation vs. general form: Efficient version eliminates base weights during training (saving computation) but loses ability to monitor training loss at actual weights
  - AdamO approximation: Uses dt+1 ≈ dt approximation for efficiency (Eq. 10), accurate for large t but requires delay (τ) early

- **Failure signatures**:
  - Instability with high γ: Training loss oscillates or diverges; reduce γ or increase τ
  - Poor early training (AdamO): If τ too low, overshoot applied before momentum stabilizes, causing erratic updates
  - No improvement over baseline: May indicate low consecutive update similarity; measure update cosine similarity

- **First 3 experiments**:
  1. Baseline comparison: Implement SGDO with γ=5 on CIFAR-10/ResNet; compare convergence speed and final accuracy against CM, NAG, and Adam. Measure steps-to-target-loss and test accuracy.
  2. γ sensitivity sweep: Test γ ∈ {1, 3, 5, 7, 10} plotting training loss convergence and Awd metric to identify if arg min Awd(γ) correlates with fastest convergence.
  3. AdamO delay ablation: Test τ ∈ {0, 25, 50, 100} on transformer fine-tuning to determine optimal delay for tasks with high initial gradient variance.

## Open Questions the Paper Calls Out

- **Can the empirical superiority be formally proven with theoretical convergence guarantees?** The paper only shows empirical evidence and acknowledges that theoretical proof could better support the superiority over classical momentum and NAG.

- **How can the overshoot factor γ be adapted dynamically during training?** The paper suggests dynamically adjusting γ could maximize benefits but currently uses fixed γ, noting that optimal γ varies by task and hyperparameter settings.

- **What alternative past-gradient weighting scheme would better reflect gradient relevance?** The current exponential decay (μ^(i−j) for SGD, β1^(i−j) for Adam) may not be optimal for Overshoot since there's no monotonous relationship between "gradient age" and relevance.

- **How does Overshoot interact with learning rate schedulers and other optimization techniques?** Evaluations were conducted on default settings without incorporating learning rate schedulers or hyperparameter fine-tuning.

## Limitations

- The optimal overshoot factor γ varies significantly by task, suggesting effectiveness depends on specific training dynamics
- The Awd metric provides theoretical justification but lacks strong empirical validation as a reliable predictor of optimal γ
- The AdamO approximation is only asymptotically accurate, potentially affecting early training stability despite the delay mechanism

## Confidence

- **High Confidence**: Empirical demonstration of consistent improvements across diverse tasks with statistical significance
- **Medium Confidence**: Anticipatory gradient mechanism is plausible but relies on assumption of positive consecutive update similarity
- **Low Confidence**: Awd metric as theoretical justification lacks strong empirical validation and may not predict optimal γ

## Next Checks

1. **Update Similarity Analysis**: Measure cosine similarity between consecutive SGD updates across different training stages and tasks, correlating with Overshoot performance to validate the core assumption about update direction predictability.

2. **Awd vs. Convergence Correlation**: Systematically test whether minimizing the Awd metric (through γ tuning) correlates with minimizing steps-to-target-loss across multiple tasks to validate or challenge the theoretical foundation.

3. **Early Training Stability**: Evaluate AdamO performance without the delay mechanism (τ=0) across various tasks to quantify the importance of postponing overshoot until momentum stabilizes.