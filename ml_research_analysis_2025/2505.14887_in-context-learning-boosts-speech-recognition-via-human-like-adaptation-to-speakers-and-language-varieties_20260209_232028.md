---
ver: rpa2
title: In-Context Learning Boosts Speech Recognition via Human-like Adaptation to
  Speakers and Language Varieties
arxiv_id: '2505.14887'
source_url: https://arxiv.org/abs/2505.14887
tags:
- speech
- speaker
- speakers
- across
- adaptation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: In-context learning significantly improves ASR robustness across
  diverse English speakers and varieties using Phi-4-Multimodal. The framework interleaves
  transcribed audio-text pairs as exemplars before transcribing target audio, requiring
  no model updates or additional training data.
---

# In-Context Learning Boosts Speech Recognition via Human-like Adaptation to Speakers and Language Varieties

## Quick Facts
- arXiv ID: 2505.14887
- Source URL: https://arxiv.org/abs/2505.14887
- Reference count: 32
- In-context learning improves ASR robustness across diverse English speakers with 19.7% average WER reduction

## Executive Summary
This paper introduces a novel in-context learning framework that adapts speech recognition models to individual speakers and language varieties without updating model weights or requiring additional training data. The approach interleaves transcribed audio-text pairs as exemplars before transcribing target audio, leveraging Phi-4-Multimodal's capabilities. Results demonstrate significant improvements in robustness across four diverse English corpora spanning native American English, major world language varieties, Spanish heritage varieties, and global accent diversity. The framework achieves 5.4-36.4% relative WER reduction with an average improvement of 19.7%, with most pronounced gains for low-resource varieties.

## Method Summary
The framework uses in-context learning by interleaving transcribed audio-text pairs (exemplars) before transcribing target audio. It employs Phi-4-Multimodal with speech modality fine-tuned on English audio-text pairs. The context construction involves selecting exemplar speakers, extracting 3-10 utterances per speaker, and concatenating these with target audio as interleaved audio-text pairs. This approach requires no model updates or additional training data, making it practical for real-world deployment. The method was evaluated across four corpora with diverse speaker populations and language varieties.

## Key Results
- 19.7% average relative WER reduction across all corpora and conditions
- Most pronounced improvements for low-resource varieties (35.2% and 39.2% gains for German and Indian speakers)
- Adaptation benefits speaker-specific improvements at 4-6 examples (19.6% relative gain) before shifting to variety-level generalization
- Largest gains occur within first few examples (25-30 seconds of speech), paralleling human perceptual adaptation patterns

## Why This Works (Mechanism)
In-context learning enables rapid speaker and variety adaptation by providing the model with exemplar audio-text pairs that capture speaker-specific acoustic patterns, phonetic features, and language variety characteristics. The framework leverages the model's ability to quickly adjust its internal representations based on observed patterns, similar to human perceptual adaptation. By interleaving transcribed examples with target audio, the model can effectively re-weight phonetic features and adjust its recognition parameters for the specific speaker or variety being processed.

## Foundational Learning
- **In-context learning**: Understanding how models adapt behavior based on provided examples without weight updates. Why needed: Core mechanism enabling speaker adaptation without retraining. Quick check: Verify model can learn simple patterns from few examples.
- **Phonetic feature adaptation**: Recognition that acoustic-phonetic properties vary across speakers and varieties. Why needed: Explains why adaptation improves performance. Quick check: Analyze error patterns before and after adaptation.
- **Multimodal representation**: How audio and text modalities are integrated in Phi-4-Multimodal. Why needed: Framework relies on model's ability to process interleaved audio-text pairs. Quick check: Confirm model can process both modalities simultaneously.
- **Speaker-specific acoustic patterns**: Understanding that individual speakers have unique acoustic signatures. Why needed: Foundation for personalized adaptation approach. Quick check: Measure acoustic distance between speakers.
- **Language variety characteristics**: Recognition that different English varieties have distinct phonetic and lexical patterns. Why needed: Explains variety-level adaptation benefits. Quick check: Compare WER reduction across variety pairs.

## Architecture Onboarding

**Component map:** Phi-4-Multimodal (audio encoder, text decoder, multimodal fusion) -> In-context adapter (exemplar selection, context construction) -> Speech recognition output

**Critical path:** Audio input → Multimodal encoder → Context integration (exemplars + target) → Text decoding → Output transcription

**Design tradeoffs:** Static context selection vs. dynamic adaptation, exemplar quantity vs. latency, variety-specific vs. speaker-specific focus, oracle transcripts vs. model-generated context

**Failure signatures:** Minimal adaptation when exemplars poorly match target speaker, degradation with too many diverse exemplars, plateau effects indicating model capacity limits

**First experiments:** 1) Test exemplar selection with varying acoustic similarity metrics, 2) Evaluate context construction with different exemplar quantities (3-10), 3) Compare adaptation speed across speaker distances

## Open Questions the Paper Calls Out

**Open Question 1:** Can unsupervised or self-transcription methods for generating context examples match the performance of ground-truth transcribed examples?
The paper relies on accurately transcribed context, which may not be practical in real-world scenarios. Future work should explore unsupervised or self-transcription for context generation and active context selection.

**Open Question 2:** Do ICL adaptation effects generalize to spontaneous speech, cross-lingual settings, and streaming inference?
All experiments used read speech, and the paper calls for extending the framework to spontaneous speech, cross-lingual settings, and streaming applications while probing the precise mechanisms of variety-specific adaptation.

**Open Question 3:** What are the precise mechanisms underlying variety-specific adaptation in multimodal language models?
The authors call for probing the precise mechanisms of variety-specific adaptation and hypothesize that ICL potentially involves rapid re-weighting of phonetic features based on observed speaker-specific patterns.

**Open Question 4:** Why do baseline performance disparities between native and non-native speakers persist even after ICL adaptation?
Despite 19.7% average WER reduction, significant gaps remain for certain varieties, with SAA showing a nearly 10-fold difference between native (1.2%) and non-native (11.4%) zero-shot WERs that adaptation does not eliminate.

## Limitations
- Results confined to Phi-4-Multimodal, preventing generalization to other models or architectures
- Evaluation spans only English varieties, leaving unknown whether similar gains would occur for multilingual scenarios
- Performance disparities between native and non-native speakers persist despite adaptation benefits
- Does not investigate long-term adaptation effects or potential catastrophic forgetting when switching between speakers or varieties

## Confidence
- **High** for effectiveness of in-context learning within tested conditions
- **Medium** for claimed parallels with human perceptual adaptation due to limited behavioral comparison data
- **Low** for generalization across languages, models, and real-world deployment scenarios

## Next Checks
1. Test the in-context learning approach on additional ASR architectures (e.g., Whisper, Wav2Vec) to assess model dependency and robustness of findings
2. Evaluate cross-linguistic adaptation by testing non-English varieties and multilingual scenarios to determine if adaptation patterns hold across languages
3. Conduct ablation studies varying exemplar presentation order, speaker similarity metrics, and shot count ranges to better understand the mechanisms driving speaker-to-variety generalization transitions