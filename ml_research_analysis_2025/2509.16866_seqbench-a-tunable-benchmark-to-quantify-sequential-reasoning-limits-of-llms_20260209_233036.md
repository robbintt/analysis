---
ver: rpa2
title: 'seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs'
arxiv_id: '2509.16866'
source_url: https://arxiv.org/abs/2509.16866
tags:
- reasoning
- figure
- door
- performance
- room
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces seqBench, a tunable benchmark designed to
  systematically evaluate sequential reasoning limits in large language models (LLMs)
  by independently controlling logical depth, backtracking requirements, and noise
  ratio in pathfinding tasks. The benchmark reveals that all evaluated LLMs exhibit
  exponential performance collapse as logical depth increases, with a model-specific
  characteristic path length L0 (ranging from 85.7 for Gemini-2.5-Flash to 1.6 for
  Llama-3.2-3B) beyond which success rates approach zero.
---

# seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs

## Quick Facts
- arXiv ID: 2509.16866
- Source URL: https://arxiv.org/abs/2509.16866
- Reference count: 20
- All evaluated LLMs exhibit exponential performance collapse as logical depth increases, with characteristic path lengths ranging from 1.6 to 85.7

## Executive Summary
seqBench is a tunable benchmark designed to systematically evaluate sequential reasoning limits in large language models (LLMs) by independently controlling logical depth, backtracking requirements, and noise ratio in pathfinding tasks. The benchmark reveals that all evaluated LLMs exhibit exponential performance collapse as logical depth increases, with a model-specific characteristic path length L0 (ranging from 85.7 for Gemini-2.5-Flash to 1.6 for Llama-3.2-3B) beyond which success rates approach zero. Performance also degrades significantly with increased backtracking steps and noise ratio, while fact shuffling has minimal impact. Detailed error analysis shows models frequently fail by omitting critical steps rather than taking illegal shortcuts, with failures tending to occur earlier in longer problems, suggesting difficulties with global planning. Even the most recent SOTA models, including OpenAI's GPT-5, demonstrate these same failure patterns. The seqBench framework provides a valuable diagnostic tool for understanding fundamental reasoning bottlenecks in LLMs, highlighting that current architectures struggle with coherent multi-step inference despite their ability to process vast amounts of information.

## Method Summary
seqBench generates synthetic pathfinding problems on 2D grids using Kruskal's algorithm to create acyclic mazes. The "Rewind Construction" algorithm places keys and locked doors to guarantee solvability while controlling backtracking requirements. Facts describing the maze structure are compiled into natural language with tunable noise ratios. Models are prompted with instructions, few-shot examples, and problem facts, then evaluated on exact match success, progress ratio, precision, and recall. The primary metric is characteristic path length L0, derived from exponential decay fitting of success rates against logical depth. The dataset and code are publicly available on HuggingFace.

## Key Results
- All evaluated LLMs show exponential performance collapse with characteristic path lengths ranging from 1.6 to 85.7
- Performance degrades significantly with increased backtracking steps and noise ratio, but minimally with fact shuffling
- Models primarily fail by omitting critical steps rather than hallucinating illegal actions, showing high precision but low recall
- Failures tend to occur earlier in longer problems, suggesting global planning difficulties rather than pure local error accumulation
- Even the most recent SOTA models demonstrate the same exponential decay patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sequential reasoning accuracy collapses exponentially with logical depth, following a model-specific characteristic length scale L₀.
- Mechanism: Each reasoning step introduces an approximately independent error probability p; over L steps, success rate decays as P(L) ≈ exp(-L/L₀) where L₀ = 1/p characterizes the model's reasoning horizon. The log-linear relationship in Figure 1 suggests this quasi-independence of per-step errors.
- Core assumption: Errors compound multiplicatively rather than catastrophically at a single step; assumes no strong error correction or recovery mechanisms.
- Evidence anchors:
  - [abstract] "accuracy collapses exponentially beyond a model-specific logical depth"
  - [Section 2.2] "Plotting success rates on a semi-logarithmic (log-y) scale against L reveals an approximately linear decay trend... errors may accumulate with a degree of independence at each reasoning step"
  - [corpus] CogniLoad (arXiv:2509.18458) similarly isolates task length, complexity, and distractors to study reasoning limits, supporting the controlled decomposition approach.
- Break condition: If models develop robust self-correction or verification mechanisms that catch and recover from errors mid-chain, the exponential decay assumption would underpredict performance on longer chains.

### Mechanism 2
- Claim: Models fail earlier in longer problems because anticipated global complexity degrades initial planning quality.
- Mechanism: When presented with a problem requiring 80 steps vs. 20 steps, the model's first-error distribution shifts leftward—errors occur at step 5 more frequently in the 80-step case. This suggests the model struggles to maintain coherent global planning when the full problem scope is salient, rather than purely accumulating local errors.
- Core assumption: The model encodes some representation of total problem complexity at inference time; this representation interferes with early-step reasoning quality.
- Evidence anchors:
  - [Section 2.4] "as the total required path length (L) of a problem increases, models tend to make critical errors more frequently even at the earliest steps... an error at an early step (e.g., step 5) becomes substantially more likely when the model is attempting to solve an 80-step problem versus a 20-step problem"
  - [Section 2.4] "suggesting a struggle with global planning or maintaining coherence over longer horizons, rather than just an accumulation of local errors"
  - [corpus] Limited direct corpus evidence; primarily an empirical observation from this work.
- Break condition: If prompting strategies decompose problems into independent sub-problems that mask total complexity, this effect should attenuate.

### Mechanism 3
- Claim: Models fail primarily by omitting required actions rather than hallucinating illegal ones.
- Mechanism: The transformer generates plausible local continuations but loses track of global constraints (e.g., "must pick up key before door"). This manifests as high precision (few hallucinated actions) but low recall (missed critical sub-goals). The mechanism suggests attention over long action sequences fails to maintain dependency awareness.
- Core assumption: The model's generative process optimizes for local plausibility over global constraint satisfaction; no explicit working memory for pending sub-goals.
- Evidence anchors:
  - [Section 2.4] "detailed analysis reveals that LLMs often fail by omitting critical sub-goals... while precision generally remains high... recall and progress ratio plummet with increasing path length"
  - [Section 2.4] "models frequently fail by omitting critical steps rather than taking illegal shortcuts"
  - [corpus] ZebraLogic (arXiv:2502.01100) notes similar constraint-satisfaction failures, though in combinatorial rather than sequential settings.
- Break condition: If models are augmented with explicit state tracking or external memory for pending goals, omission rates should decrease.

## Foundational Learning

- Concept: **Exponential decay with characteristic scale (L₀)**
  - Why needed here: The paper quantifies model limits via L₀ (ranging 1.6–85.7); understanding this parameter is essential for interpreting performance cliffs and comparing models.
  - Quick check question: If Model A has L₀=10 and Model B has L₀=20, at what path length does Model A's success rate drop to ~37%? What about Model B?

- Concept: **Backtracking in search/planning**
  - Why needed here: seqBench operationalizes backtracking via key-door dependencies; understanding why revisiting prior states is harder than linear planning clarifies one complexity dimension.
  - Quick check question: In a pathfinding task, what makes "retrieve key, return to door, unlock, proceed" harder than simple linear traversal?

- Concept: **Precision vs. Recall in sequential generation**
  - Why needed here: The paper diagnoses failure mode via precision/recall asymmetry; distinguishing hallucination (low precision) from omission (low recall) enables targeted debugging.
  - Quick check question: If a model generates 50 actions but the ground truth has 60, and 40 of the 50 are correct, what are precision and recall?

## Architecture Onboarding

- Component map:
  Maze Generator -> Rewind Construction -> Fact Compiler -> Evaluator

- Critical path:
  1. Specify parameters: grid size (N×M), target backtracks (B_target), noise ratio (N_target)
  2. Run Algorithm 1 → maze graph + locked doors + key placements + path skeleton
  3. Compile facts → inject distractors per N_target
  4. Construct prompt (instructions + 3 few-shot examples + facts)
  5. Query model (T=1.0, top-p=0.95, 5 runs/instance)
  6. Parse output → compute metrics → fit L₀ if analyzing depth scaling

- Design tradeoffs:
  - Synthetic vs. linguistic diversity: Grid tasks minimize confounding factors but may not transfer to all domains
  - Minimal search complexity: By design, seqBench isolates sequential reasoning from combinatorial search; this trades realism for diagnostic clarity
  - Backtracking constraint: Current implementation limits to simple dependency chains (one key per door, no nested door dependencies); extensible but increases generation complexity

- Failure signatures:
  - Adjacency errors: Model "jumps" between unconnected rooms (Figure 9)
  - Omission of key pickup: Model proceeds to locked door without retrieving required key (high precision, low recall)
  - Early-first-error shift: In long problems, first violation occurs disproportionately early, indicating global planning breakdown
  - Noise sensitivity: Success drops sharply as distracting facts increase; shuffle alone has minimal effect

- First 3 experiments:
  1. Establish baseline L₀: Sample 40 problems per L-bin (L=10–100), run 5 trials each, fit exponential decay to extract your model's characteristic length. Compare to published values (e.g., Llama-3.3-70B: 10.2, Gemini-2.5-Flash: 85.7).
  2. Isolate backtracking impact: Fix L∈[40,60], vary B=0→5, plot success rate vs. B. Confirm monotonic decline and observe token count increase (reasoning effort).
  3. Noise sensitivity profile: Fix L=50, B=2, vary noise ratio N=0→1.0, measure success and progress ratio. Expect near-linear degradation; verify shuffle has minimal effect when N=0.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural or training factors determine the model-specific characteristic path length (L₀)?
- Basis in paper: [explicit] The Limitations section explicitly calls for systematic investigation to disentangle how L₀ is influenced by model architecture, scale (parameters, training data), fine-tuning, and inference-time computation.
- Why unresolved: While the paper establishes the existence of L₀ as a metric for reasoning limits, it does not isolate which specific model components or training regimes cause L₀ to vary (e.g., from 1.6 to 85.7).
- What evidence would resolve it: Controlled ablation studies across model scales and architectures correlating specific interventions with shifts in the L₀ decay constant.

### Open Question 2
- Question: Does the ability to use external tools or backtrack programmatically mitigate the exponential performance collapse observed in raw LLMs?
- Basis in paper: [explicit] The authors list evaluating agentic systems capable of tool use as a key avenue for future research to see if failure modes persist when models can externalize sub-problems.
- Why unresolved: The current evaluation focused on raw model inference without agentic loops, leaving the potential for external state management to alleviate sequential reasoning bottlenecks untested.
- What evidence would resolve it: Benchmarking agentic frameworks on seqBench instances with high logical depth to see if external tool invocation flattens the exponential decay curve.

### Open Question 3
- Question: Does visual problem presentation (e.g., maze images) alter the sequential reasoning failure modes compared to text-only descriptions?
- Basis in paper: [explicit] The paper notes in the Limitations section that it did not investigate whether similar failure modes arise when problems are presented visually, acknowledging multimodal capabilities could influence outcomes.
- Why unresolved: Current findings are strictly bound to textual fact lists; it is unknown if visual grounding reduces the cognitive load associated with the "burden of anticipated complexity."
- What evidence would resolve it: Evaluating multimodal models on the newly released maze image dataset to compare characteristic path lengths (L₀) against text-only baselines.

## Limitations

- Synthetic tasks may not fully capture real-world sequential reasoning complexity involving richer contextual reasoning, temporal dependencies, or ambiguous goals
- Exponential decay model assumes independent per-step errors, which may not hold for all reasoning types or prompting strategies
- Benchmark focuses on pathfinding with simple dependency chains, potentially missing more complex multi-agent or nested dependency scenarios

## Confidence

- High confidence: The exponential decay pattern with characteristic length L₀ is empirically robust across multiple model families and scales
- Medium confidence: The interpretation of early-failure shift as global planning breakdown, while supported by data, requires additional validation on non-synthetic tasks
- Medium confidence: The precision/recall analysis accurately captures failure modes, but may not generalize to domains where hallucination is more costly than omission

## Next Checks

1. Test seqBench's noise sensitivity and depth scaling on non-synthetic sequential reasoning tasks (e.g., multi-step code generation or procedural text understanding) to assess domain transfer
2. Implement and evaluate prompting strategies that explicitly decompose problems into sub-tasks to determine if early-failure shift persists when total problem complexity is masked
3. Compare seqBench-derived L₀ values with performance on established reasoning benchmarks (GSM8K, MATH) to validate L₀ as a predictor of real-world reasoning capacity