---
ver: rpa2
title: Code-Driven Planning in Grid Worlds with Large Language Models
arxiv_id: '2505.10749'
source_url: https://arxiv.org/abs/2505.10749
tags:
- actions
- grid
- direction
- move
- current
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an iterative programmatic planning (IPP) framework
  that uses large language models (LLMs) to generate interpretable Python programs
  as action policies for solving grid-based planning tasks. Instead of directly generating
  action sequences, IPP synthesizes executable code that maps environment states to
  action sequences and improves these programs through an iterative refinement process
  based on performance feedback.
---

# Code-Driven Planning in Grid Worlds with Large Language Models

## Quick Facts
- arXiv ID: 2505.10749
- Source URL: https://arxiv.org/abs/2505.10749
- Reference count: 40
- One-line primary result: Code-driven planning via iterative refinement improves LLM performance on grid tasks by 10% to 10× over direct action generation, achieving state-of-the-art results.

## Executive Summary
This paper introduces Iterative Programmatic Planning (IPP), a framework that uses large language models to generate executable Python code as policies for grid-based planning tasks. Instead of directly generating action sequences, IPP synthesizes interpretable code that maps environment states to actions and refines these programs through iterative improvement based on performance feedback. The approach was evaluated on GRASP (energy collection) and MiniGrid (navigation and manipulation) benchmarks using six leading LLMs, showing consistent improvements over direct code generation across five of six models. IPP established new state-of-the-art results for GRASP and significantly outperformed direct solution elicitation from GPT-o3-mini.

## Method Summary
IPP treats LLMs as code generators rather than planners, synthesizing Python functions that map environment observations to action sequences. The framework employs three prompting strategies: Direct Generation (full task description), Pseudocode Extension (starting from a greedy skeleton), and Step-by-Step (curriculum-based). The iterative refinement loop executes generated code on training instances, identifies worst-performing cases, and prompts the LLM to refine the code based on these failures. This process continues until performance converges or degrades, creating reusable policies that generalize across grid instances.

## Key Results
- IPP improved performance over direct code generation by 10% to 10× across five of six evaluated models
- Established new state-of-the-art result for GRASP benchmark
- Outperformed direct solution elicitation from GPT-o3-mini by 63% on MiniGrid to 116% on GRASP
- Code generation has higher initial prompting costs but significantly lower amortized cost than per-instance inference methods

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthesizing code policies allows for generalized problem-solving across grid instances where direct action elicitation fails.
- **Mechanism:** The framework shifts the LLM's role from a "planner" (generating one-time action sequences) to a "programmer" (synthesizing a reusable function $f$). By mapping environment observations to actions via Python code, the system leverages the LLM's strength in code generation to create deterministic, executable logic that applies to varying initial states.
- **Core assumption:** The LLM possesses sufficient coding proficiency to translate spatial reasoning and task rules into syntactically correct and logically sound control flow (e.g., loops, conditionals).
- **Evidence anchors:**
  - [abstract] "Instead of directly generating action sequences, IPP synthesizes executable code that maps environment states to action sequences..."
  - [section 2] "Code-driven planning offers a compromise by forcing the LLM to provide an explicit policy... encoded as an interpretable program."
  - [corpus] Related work (e.g., "Code Driven Planning with Domain-Adaptive Critic") suggests a broader trend of using code as an interface for agent control, though this specific paper focuses on the iterative refinement aspect.
- **Break condition:** If the task requires semantic understanding or common sense reasoning not easily reducible to algorithmic state manipulation, code synthesis may fail to capture the necessary nuance.

### Mechanism 2
- **Claim:** Iterative refinement based on failure cases approximates a non-gradient optimization process, improving policy robustness.
- **Mechanism:** The Iterative Refinement (IR) loop executes the generated code on a training set and identifies the $k$ worst-performing instances ($D_{fail}$). These failures act as a "heuristic gradient," guiding the LLM to modify the code $f^{(t)}$ to fix logical bugs or edge-case handling without requiring model weights to be updated.
- **Core assumption:** The LLM can perform "debugging" via in-context learning; specifically, it can correlate execution failures with specific lines of logic in the generated code when presented with error context.
- **Evidence anchors:**
  - [abstract] "includes an iterative refinement mechanism that updates code based on task performance feedback."
  - [section 2.1, Algorithm 2.2] "We identify the k worst-performing instances... and prompt the LLM to refine the current program."
  - [section 5] "Even models with a weak initial program benefit significantly from refinement, highlighting the value of execution-based feedback."
- **Break condition:** If the failure cases are contradictory or require fundamental architectural changes to the code structure (rather than local bug fixes), the iterative loop may oscillate or diverge.

### Mechanism 3
- **Claim:** Structured prompting strategies (curriculum and pseudocode) reduce the complexity of the synthesis search space.
- **Mechanism:** By providing a "Greedy" pseudocode skeleton or starting with simplified environments (Step-by-Step), the system primes the LLM with a valid control flow baseline. This reduces the cognitive load on the model, requiring it only to adapt or extend existing logic rather than generating complex strategies from scratch.
- **Core assumption:** The provided pseudocode or curriculum steps are sufficiently aligned with the optimal solution path; a poor prior could mislead the model.
- **Evidence anchors:**
  - [section 2] "Pseudocode Extension... narrows the LLM's search space, allowing the model to use symbolic priors while preserving flexibility."
  - [section 4, Figure 3] "GPT-o3-mini achieves steady gains... while Claude-3.7 improves... demonstrating the benefit of curriculum-based synthesis."
- **Break condition:** If the task complexity increases non-linearly, the step-by-step curriculum might fail if the intermediate steps do not generalize to the final complex state.

## Foundational Learning

- **Concept:** **Policy vs. Plan**
  - **Why needed here:** The core distinction in this paper. A *Plan* is a linear sequence of actions (e.g., "Move Up, Move Right") valid only for a specific instance. A *Policy* (in this context, a Python function) is a mapping from state to action (e.g., "If wall ahead, turn left") that generalizes across instances.
  - **Quick check question:** Does the generated output rely on hardcoded coordinates (Plan) or relative logic (Policy)?

- **Concept:** **State Tracking in Code**
  - **Why needed here:** Grid worlds require tracking position, inventory (keys/energy), and obstacles. The LLM must generate code that explicitly maintains these variables (e.g., `current_pos`, `holding_key`) to function correctly.
  - **Quick check question:** In the generated code, is the agent's position updated after every move calculation?

- **Concept:** **Amortized Inference**
  - **Why needed here:** The paper argues for cost efficiency. While generating code is expensive (high initial tokens), the generated Python code is "free" to run on new instances (zero marginal LLM cost), unlike Chain-of-Thought which requires LLM inference for every new grid.
  - **Quick check question:** If I run 1,000 instances of the same task, why is IPP cheaper than CoT?

## Architecture Onboarding

- **Component map:** Prompt Constructor -> LLM Engine -> Execution Sandbox -> Evaluator -> Refinement Loop -> Prompt Constructor
- **Critical path:** The reliability of the **Execution Sandbox** and the quality of the **Failure Feedback**. If the error messages or failure cases provided to the LLM are ambiguous, the refinement loop cannot function.
- **Design tradeoffs:**
  - **Interpretability vs. Optimality:** The code is human-readable (high interpretability) but may be sub-optimal compared to a highly tuned RL policy (lower optimality in complex dynamics).
  - **Setup Cost vs. Run Cost:** High token cost during the synthesis phase vs. near-zero cost during deployment.
- **Failure signatures:**
  - **Hallucinated Logic:** Code uses libraries not present in the sandbox or actions not allowed by the environment.
  - **State Drift:** The code fails to update internal variables (e.g., energy level) consistently with environment transitions.
  - **Refinement Collapse:** The model deletes working code to fix a minor edge case, reducing overall performance (seen in some DeepSeek-R1 results).
- **First 3 experiments:**
  1. **Sanity Check (Direct Generation):** Prompt the model to write a `solve()` function for the simplest GRASP configuration (no obstacles, 4-direction movement). Verify if the code runs without syntax errors.
  2. **Loop Verification (Iterative Refinement):** Run the generated code on 10 random grids. Pick the worst result, feed it back to the model with the prompt "Fix this bug," and measure the performance delta.
  3. **Generalization Test:** Train/Synthesize code on a "Small" grid size. Execute this same code (without changes) on a "Large" grid size to test if the policy generalizes or overfits to the training dimensions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does iterative refinement consistently degrade performance for some models (e.g., DeepSeek-R1 showed up to -20% decline) while dramatically improving others?
- Basis in paper: [explicit] The authors explicitly note that "DeepSeek-R1 uniquely exhibits consistent performance declines (up to –20%) across all settings, suggesting potential incompatibility with the IR strategy."
- Why unresolved: The paper does not investigate model-specific factors (e.g., training data, reasoning patterns) that cause this divergence.
- What evidence would resolve it: Ablation studies comparing model architectures and fine-tuning approaches; analysis of failure modes in refinement prompts.

### Open Question 2
- Question: Can the IPP framework generalize beyond deterministic, fully observable grid environments to continuous or partially observable domains?
- Basis in paper: [explicit] The authors state they "assume the transition function is deterministic and fully defined" and that models can "make mistakes in tracking the agent's state... especially for more complex tasks or when new situations arise."
- Why unresolved: The evaluation was limited to discrete 2D grid benchmarks; no experiments tested stochastic transitions, partial observability, or continuous action spaces.
- What evidence would resolve it: Empirical validation on robotics simulators or partially observable Markov environments with the same IPP framework.

### Open Question 3
- Question: How can the feedback mechanism in iterative refinement be made more precise to isolate specific code errors rather than providing only aggregate performance signals?
- Basis in paper: [explicit] "The way the model uses performance feedback is not very precise. It only sees whether the program did better or worse overall, without knowing exactly what part of the code caused the problem."
- Why unresolved: The current approach uses only task-level metrics (energy, reward) without localization to specific code regions.
- What evidence would resolve it: Integration of execution traces, line-level error annotations, or test case diagnostics into the refinement feedback loop.

### Open Question 4
- Question: What is the optimal curriculum design for step-by-step prompting, and why does performance sometimes decline when adding complexity (e.g., GPT-o1 peaked at Step 2 then dropped)?
- Basis in paper: [inferred] Figure 3 shows non-monotonic performance across curriculum steps, but the paper does not analyze causes or propose optimal curriculum sequencing.
- Why unresolved: Curriculum order was arbitrary; no systematic study of how complexity ordering affects final policy quality.
- What evidence would resolve it: Ablation experiments varying curriculum step order and complexity granularity; analysis of when accumulated code becomes harder to modify.

## Limitations

- The iterative refinement mechanism shows dramatic performance variations across models, with some experiencing consistent degradation
- The approach assumes deterministic, fully observable environments and may not generalize to stochastic or continuous domains
- The interpretability benefits are assumed rather than empirically validated through human studies

## Confidence

- **High Confidence:** The core mechanism of synthesizing Python code from LLMs for grid tasks is well-established. The GRASP and MiniGrid environments are clearly defined, and the performance improvements over direct action elicitation (63-116%) are measurable and significant.
- **Medium Confidence:** The effectiveness of iterative refinement varies dramatically across models (10% to 10× improvements), suggesting the mechanism works but is highly model-dependent. The cost amortization argument is theoretically sound but requires empirical validation in production settings.
- **Low Confidence:** Claims about interpretability benefits are difficult to verify without human studies. The paper asserts that code is more interpretable than black-box plans, but this remains an assumption rather than a demonstrated result.

## Next Checks

1. **Failure Case Reproduction:** Recreate the iterative refinement loop using the exact failure cases from the paper's training set. If available, obtain the dataset link to ensure faithful reproduction of the IR mechanism.

2. **Cost-Benefit Analysis:** Measure the total token costs for Direct Generation + IR synthesis versus running 100-1000 instances of the same task with CoT prompting. Include both initial synthesis costs and marginal execution costs to validate the amortization claim.

3. **Interpretability Verification:** Conduct a small user study comparing human understanding of generated Python policies versus raw action sequences. Test whether developers can debug or modify the generated code more easily than interpreting CoT traces.