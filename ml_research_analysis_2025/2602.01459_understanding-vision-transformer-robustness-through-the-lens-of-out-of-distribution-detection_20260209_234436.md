---
ver: rpa2
title: Understanding vision transformer robustness through the lens of out-of-distribution
  detection
arxiv_id: '2602.01459'
source_url: https://arxiv.org/abs/2602.01459
tags:
- quantization
- vision
- attention
- have
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Vision transformers are prone to performance drops under low-bit
  quantization, especially for out-of-distribution (OOD) tasks. This work evaluates
  quantized DeiT, DeiT3, and ViT models on common OOD datasets and finds that pretraining
  on ImageNet-22k causes greater sensitivity to 4-bit quantization errors than ImageNet-1k-only
  training, resulting in significant drops in AUPR-out scores.
---

# Understanding vision transformer robustness through the lens of out-of-distribution detection

## Quick Facts
- arXiv ID: 2602.01459
- Source URL: https://arxiv.org/abs/2602.01459
- Reference count: 34
- Primary result: Large-scale pretraining (ImageNet-22k) makes ViT models more sensitive to low-bit quantization errors, reducing OOD detection performance, while data augmentation improves quantization robustness.

## Executive Summary
This work investigates how quantization affects vision transformer (ViT) robustness for out-of-distribution (OOD) detection. The authors evaluate quantized DeiT, DeiT3, and ViT models on standard OOD datasets, finding that pretraining on ImageNet-22k leads to greater sensitivity to 4-bit quantization errors compared to ImageNet-1k-only training. While ViT models trained only on ImageNet-1k show better stability under quantization, DeiT3 models pretrained on ImageNet-22k maintain strong OOD detection performance despite significant accuracy drops. The study reveals that quantization-induced OOD robustness varies significantly with pretraining scale and model architecture.

## Method Summary
The study evaluates three ViT architectures (DeiT, DeiT3, ViT-S16) under post-training quantization (PTQ) via RepQ-ViT at multiple precisions (FP32, W8A8, W6A6, W4A8, W8A4, W4A4). Models are tested with both ImageNet-22k pretraining and ImageNet-1k-only training. Performance is measured on ImageNet-1k validation (Top-1 accuracy, NLL) and six OOD datasets (ImageNet-O, Textures, Places365, SUN, iNaturalist, OpenImage-O) using AUPR-out with maximum softmax probability (MSP) as the baseline detector. Attention rollout visualizations are generated by discarding the lowest 0.9 ratio pixels and taking the maximum across attention heads.

## Key Results
- ImageNet-22k-pretrained models show significantly larger accuracy drops and AUPR-out reductions at 4-bit quantization compared to ImageNet-1k-only variants
- DeiT3 models pretrained on ImageNet-22k maintain strong OOD detection performance despite large accuracy drops under quantization
- Data augmentation in training improves stability under quantization for OOD detection
- Attention visualizations suggest high-norm outlier tokens dominate at lower precision, affecting OOD detection performance

## Why This Works (Mechanism)
The mechanism behind the observed quantization robustness differences appears to involve how large-scale pretraining (ImageNet-22k) creates models with more complex feature representations that are less tolerant to quantization errors. When these models are quantized to 4-bit precision, the quantization noise disproportionately affects the high-frequency features learned from the larger dataset, leading to larger accuracy drops. Data augmentation appears to regularize the feature space, creating more quantization-tolerant representations even in models trained on larger datasets.

## Foundational Learning
- **Post-training quantization (PTQ)**: Quantization method applied after training without fine-tuning; needed to reduce model size and computation while maintaining performance
- **Maximum softmax probability (MSP) baseline**: Simple OOD detection method using maximum softmax score; serves as baseline for OOD detection performance
- **AUPR-out**: Area under precision-recall curve with OOD as positive class; primary metric for OOD detection performance
- **Attention rollout**: Method to visualize token attention patterns; helps understand how quantization affects model's attention mechanisms
- **W8A8, W4A4 notation**: Weight and activation quantization precision (e.g., W8A4 means 8-bit weights, 4-bit activations); determines quantization granularity
- **Calibration set**: Small dataset used to determine quantization parameters; affects final quantized model quality

## Architecture Onboarding

**Component map**: Model weights -> RepQ-ViT quantization -> Performance evaluation -> Attention visualization

**Critical path**: Quantization → Performance evaluation → Analysis; the quantization step directly determines downstream performance metrics

**Design tradeoffs**: Large-scale pretraining (IN22k) improves in-distribution performance but increases sensitivity to quantization errors for OOD tasks, while data augmentation provides more robust quantization behavior

**Failure signatures**: High variance in AUPR-out scores across seeds at 4-bit precision, significant accuracy drops without proportional OOD performance gains, attention patterns dominated by outlier tokens

**First experiments**:
1. Verify baseline FP32 accuracy matches reported values before quantization
2. Compare AUPR-out variance across 3 seeds at 4-bit precision for IN22k vs IN1k models
3. Generate attention rollout maps for one model-dataset pair to observe outlier token behavior at different precisions

## Open Questions the Paper Calls Out
None

## Limitations
- Exact RepQ-ViT hyperparameters (calibration set size, reparameterization strategy) are unspecified
- MSP implementation details (softmax temperature, preprocessing) are not fully described
- Image preprocessing for attention visualization is not specified

## Confidence
- High confidence in: (1) IN22k-pretrained models being more sensitive to 4-bit quantization than IN1k-only; (2) DeiT3 maintaining strong OOD performance despite accuracy drops; (3) data augmentation improving quantization robustness
- Medium confidence in attention-based explanations due to qualitative nature of analysis
- Major uncertainties include exact RepQ-ViT hyperparameters, MSP implementation details, and image preprocessing for attention visualization

## Next Checks
1. Verify accuracy baselines at FP32 across all model variants match reported values
2. Reproduce AUPR-out trends at 4-bit with 3 seeds, checking variance across seeds
3. Generate attention rollout maps for at least one model and dataset to qualitatively confirm outlier token dominance at lower precision