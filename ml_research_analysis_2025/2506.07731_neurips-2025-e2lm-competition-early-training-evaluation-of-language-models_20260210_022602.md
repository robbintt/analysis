---
ver: rpa2
title: 'NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models'
arxiv_id: '2506.07731'
source_url: https://arxiv.org/abs/2506.07731
tags:
- tokens
- billion
- score
- benchmarks
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The competition addresses the challenge of evaluating scientific
  knowledge acquisition in small language models (SLMs) during early training stages,
  where existing benchmarks fail to provide meaningful performance signals. The proposed
  solution involves designing novel evaluation tasks specifically tailored to measure
  progressive scientific knowledge development in SLMs, with a focus on reasoning
  and domain-specific understanding.
---

# NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models

## Quick Facts
- arXiv ID: 2506.07731
- Source URL: https://arxiv.org/abs/2506.07731
- Reference count: 40
- Primary result: Competition framework identifies MMLU-var (completion-style) as most effective benchmark for early-stage SLM scientific knowledge evaluation

## Executive Summary
The NeurIPS 2025 E2LM Competition addresses the challenge of evaluating scientific knowledge acquisition in small language models during early training stages, where traditional benchmarks fail to provide meaningful performance signals. The proposed solution involves designing novel evaluation tasks specifically tailored to measure progressive scientific knowledge development in SLMs, with a focus on reasoning and domain-specific understanding. The evaluation framework uses three metrics—Signal Quality, Ranking Consistency, and Scientific Knowledge Compliance—to identify benchmarks that effectively track learning dynamics during the critical 0-200B token training window.

## Method Summary
The competition uses three small language models (0.5B, 1B, 3B parameters) with intermediate checkpoints up to 200B tokens, trained on either web-only or knowledge-enriched data mixtures. Evaluation employs lm-evaluation-harness framework with completion-style prompt formatting to improve early-stage signal quality. The global scoring combines three metrics: Signal Quality (monotonic improvement + temporal stability), Ranking Consistency (model architecture stability), and Scientific Knowledge Compliance (domain relevance). MMLU-var serves as baseline, converting multiple-choice to completion format. Models are evaluated via log-likelihood scoring of answer candidates separately.

## Key Results
- MMLU-var achieves highest total score (71.7%) with balanced performance across all three metrics
- Completion-style prompts improve Signal Quality from 0.29 (standard MMLU) to 0.959
- Scientific-mix trained models significantly outperform web-only variants on MMLU-var while performing similarly on HellaSwag
- Traditional benchmarks like MATH show near-random performance throughout early training

## Why This Works (Mechanism)

### Mechanism 1: Prompt Format Adaptation Improves Early-Stage Signal Quality
- Claim: Converting multiple-choice benchmarks to completion/close-style format produces more informative learning signals during early training (0-200B tokens).
- Mechanism: Early-trained SLMs struggle with multiple-choice reasoning but assign higher log-likelihood to correct answers in completion format, reducing linguistic complexity while preserving knowledge assessment.
- Core assumption: Log-likelihood ranking reflects underlying knowledge acquisition before explicit reasoning capabilities emerge.
- Evidence anchors: MMLU-var achieves 0.959 Signal Quality vs 0.29 for standard MMLU; early SLMs give higher likelihood to correct answers in completion format.

### Mechanism 2: Scientific Knowledge Acquisition Separates from General Language Fluency
- Claim: Scientific knowledge can be isolated as a learnable capability distinct from general language understanding during early training.
- Mechanism: Models trained on knowledge-rich data mixtures outperform web-only models on MMLU-var but perform similarly on HellaSwag, creating measurable scientific knowledge signals.
- Core assumption: Performance gaps reflect genuine knowledge acquisition differences rather than memorization or format sensitivity.
- Evidence anchors: Knowledge-mix models significantly outperform web-only variants on MMLU-var; both perform similarly on HellaSwag.

### Mechanism 3: Multi-Metric Evaluation Identifies Benchmarks That Track Learning Dynamics
- Claim: Combining signal quality, ranking consistency, and domain compliance produces robust evaluation framework for early training.
- Mechanism: Each metric captures different failure modes—noisy signals, unstable rankings, or off-target measurement. Weighted combination prioritizes signal quality and scientific relevance.
- Core assumption: Benchmarks scoring high on all three metrics at 200B tokens remain valid indicators at 1T tokens.
- Evidence anchors: MMLU-var achieves highest total score (71.7%) with balanced performance across all metrics.

## Foundational Learning

- Concept: **Evaluation Harness Architecture**
  - Why needed here: Competition uses lm-evaluation-harness framework; submissions must integrate custom tasks. Understanding task registration and metric computation is essential.
  - Quick check question: Can you explain how to register a custom task in lm-evaluation-harness and specify which model checkpoints to evaluate?

- Concept: **Language Model Training Dynamics and Scaling Laws**
  - Why needed here: Evaluation signals change dramatically between early (0-200B) and late training stages. Understanding capability acquisition rates informs benchmark design.
  - Quick check question: At approximately what token count do models typically begin showing meaningful performance on standard benchmarks like MMLU, and why does this occur later than language fluency benchmarks?

- Concept: **Benchmark Contamination and Leakage Detection**
  - Why needed here: Competition includes leakage check to detect when answers appear verbatim in prompts. SciQ had 82% leakage rate, potentially explaining strong signal.
  - Quick check question: How would you detect whether a benchmark's questions contain answer information within the prompt context, and what statistical threshold would you use to flag problematic samples?

## Architecture Onboarding

- Component map: Provided models (0.5B, 1B, 3B) with two architectures each (deep/wide variants) → Data mixtures (Web-only vs Scientific-Mix) → lm-evaluation-harness framework → Three-metric scoring system → Hidden test set (200B-1T tokens)

- Critical path:
  1. Set up free-tier Colab/Kaggle environment (3B model needs ~6GB GPU memory)
  2. Load checkpoints via starting kit notebooks (0-Basic through 6-Submission_examples)
  3. Design/adapt benchmark with 100-15,000 samples, ensure Python evaluation metric
  4. Integrate task into lm-evaluation-harness (notebook 4)
  5. Compute local Signal Quality score (notebook 5)
  6. Submit to Codabench for global scoring with hidden checkpoints
  7. Iterate based on leaderboard feedback

- Design tradeoffs:
  - Sample size vs. signal quality: Larger benchmarks may dilute signal; competition limits to 15K samples
  - Difficulty alignment: Too difficult (MATH shows ~1% accuracy) or too easy (SciQ with leakage) benchmarks fail; target grade-school to early high-school complexity
  - Domain breadth vs. compliance: Focused scientific benchmarks score higher on compliance but risk narrow applicability
  - Completion vs. multiple-choice: Completion format improves early signal but may not transfer to standard protocols

- Failure signatures:
  - Noisy/flat learning curves: Benchmark too difficult (MATH shows ~1% accuracy throughout)
  - Zero compliance score: Benchmark measures commonsense/language rather than scientific knowledge
  - Ranking inconsistency: Different architectures swap rankings across checkpoints
  - Leakage detected: Answers appear in prompts (SciQ 82% rate)

- First 3 experiments:
  1. Reproduce MMLU-var baseline: Run notebook 3 to evaluate MMLU-var on provided 1B checkpoints, plot learning curves
  2. Ablate prompt format: Take existing scientific benchmark (ARC-Easy), create completion-format variant, compare learning curve smoothness
  3. Test difficulty gradient: Create benchmark variants at 3 difficulty levels, evaluate which produces strongest Signal Quality while maintaining Scientific Compliance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the lack of discriminative signal in early SLM training caused primarily by deficit in acquired scientific knowledge, or by linguistic complexity and formatting of current benchmark prompts?
- Basis in paper: Authors explicitly ask if "SLMs are not learning any scientific knowledge," if "benchmark questions are too difficult," or if "noise is due to... how the questions are formatted" (Section 1.1).
- Why unresolved: While MMLU-var suggests format matters, extent to which prompt engineering compensates for model capacity limits versus actual knowledge acquisition remains unclear.
- Evidence: Ablation studies systematically decoupling knowledge depth from prompt linguistic complexity on models between 0–200B tokens.

### Open Question 2
- Question: How can evaluation benchmarks be structured to align with a model's "developmental stage" rather than just testing final capabilities?
- Basis in paper: Paper argues evaluation strategies should account for learning dynamics, stating learners should not be assessed on advanced concepts before mastering basics (Section 1.2).
- Why unresolved: Current benchmarks are static; paper identifies need for tasks that probe "intermediate reasoning capabilities" that evolve over time.
- Evidence: Curriculum-based benchmark suite where tasks unlock or adapt based on model performance at specific training token counts.

### Open Question 3
- Question: Do optimal evaluation signals and data mixtures identified in small models (0.5B–3B) reliably transfer to predicting training dynamics of larger models?
- Basis in paper: Paper claims "Transfer to large models" as key impact but acknowledges this is goal rather than proven result, citing external surveys (Section 1.1).
- Why unresolved: Uncertain if ranking consistency and signal quality metrics validated on SLMs scale to predict behavior of frontier models.
- Evidence: Comparative experiments demonstrating data mixtures optimized via SLM early-training signals result in superior performance when scaled to 7B+ parameter models.

## Limitations

- Framework rests on untested assumptions about early training evaluation that warrant caution, including the relationship between log-likelihood ranking and knowledge assessment
- Competition focuses exclusively on small language models (0.5B-3B parameters), limiting generalizability to larger models where training dynamics may differ substantially
- Leakage detection revealed 82% of SciQ samples contained answer information in prompts, raising questions about validity of existing scientific benchmarks

## Confidence

**High Confidence**: Observation that standard multiple-choice benchmarks produce noisy, non-discriminative signals during early training is well-supported; completion-style adaptation improving signal quality is reasonably justified through MMLU-var results.

**Medium Confidence**: Claim that scientific knowledge acquisition can be isolated as distinct capability is supported by differential performance between data mixtures, but underlying assumption about performance gaps reflecting genuine knowledge differences remains unverified.

**Low Confidence**: Specific weighting of evaluation metrics (α=0.5, 0.1, 0.4) lacks empirical justification and may not generalize across different benchmark types or model architectures.

## Next Checks

1. Test Prompt Format Transferability: Evaluate whether models trained with completion-style prompts show improved performance on standard multiple-choice benchmarks after 1T tokens, or if format-specific training creates persistent evaluation gap.

2. Validate Knowledge Separation Across Architectures: Compare scientific knowledge acquisition signals across different model architectures and parameter scales to determine whether observed separation is architecture-independent.

3. Longitudinal Benchmark Stability Analysis: Track top-scoring benchmarks identified through three-metric framework at 200B tokens through full 1T token training period to verify whether initial high scores predict sustained validity.