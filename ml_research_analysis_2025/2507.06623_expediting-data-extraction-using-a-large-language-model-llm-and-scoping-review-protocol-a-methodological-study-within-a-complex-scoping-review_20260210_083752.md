---
ver: rpa2
title: 'Expediting data extraction using a large language model (LLM) and scoping
  review protocol: a methodological study within a complex scoping review'
arxiv_id: '2507.06623'
source_url: https://arxiv.org/abs/2507.06623
tags:
- data
- extraction
- review
- source
- protocol
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study tested whether a scoping review protocol could expedite\
  \ data extraction using a large language model (LLM). Two LLM approaches\u2014one\
  \ with an extended protocol and one with a simple protocol\u2014were compared against\
  \ human baseline extraction from 10 evidence sources."
---

# Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review

## Quick Facts
- arXiv ID: 2507.06623
- Source URL: https://arxiv.org/abs/2507.06623
- Reference count: 40
- LLM-assisted data extraction shows high accuracy for simple citation details but poor performance for complex, subjective data items

## Executive Summary
This study evaluated whether large language models (LLMs) could expedite data extraction in scoping reviews when guided by structured protocols. Two LLM approaches were tested against human baseline extraction across 10 evidence sources, focusing on both simple citation details and complex data items. While LLMs achieved high accuracy (83.3%–100%) for citation details, performance dropped sharply for complex and subjective data items (9.6%–15.8%). Overall precision was high (>90%), but recall remained low (<25%), indicating frequent omissions and misattributions. LLM review of human-extracted data provided minimal error detection (5% of deliberately introduced errors). The findings suggest protocol-based LLM extraction may assist with simple, structured tasks but is unreliable for complex or subjective data and not suitable for review data verification.

## Method Summary
The study compared LLM-assisted data extraction against human baseline extraction within a scoping review context. Two LLM approaches were employed: one using an extended protocol and another using a simple protocol. Both were tested on 10 evidence sources, with accuracy measured for citation details and complex data items. The LLMs were prompted to extract data using structured protocols, and their outputs were compared against human-extracted data. Additionally, an LLM was used to review human-extracted data to assess its utility for error detection. Accuracy, precision, and recall were the primary metrics evaluated.

## Key Results
- High accuracy for citation detail extraction (83.3%–100%) for both LLM approaches
- Sharp decline in accuracy for complex, subjective data items (9.6%–15.8%)
- High precision (>90%) but low recall (<25%) overall, indicating frequent omissions and misattributions
- LLM review of human-extracted data detected only 5% of deliberately introduced errors

## Why This Works (Mechanism)
LLM performance in data extraction is highly dependent on the complexity and subjectivity of the task. Simple, structured data items (like citation details) align well with LLM capabilities, enabling high accuracy. However, complex and subjective data items require nuanced understanding and context, which current LLM protocols struggle to capture, leading to significant drops in performance. The high precision but low recall pattern suggests that while LLM outputs are reliable when generated, they frequently miss relevant data or misattribute information.

## Foundational Learning
- **Data extraction accuracy**: Why needed—to assess LLM reliability in extracting review data; Quick check—compare LLM and human outputs for same data items
- **Protocol-guided extraction**: Why needed—to standardize LLM prompts and improve consistency; Quick check—evaluate accuracy differences between extended and simple protocols
- **Recall vs. precision**: Why needed—to understand LLM completeness and reliability; Quick check—measure proportion of relevant data captured vs. total relevant data
- **Error detection capability**: Why needed—to evaluate LLM utility for review verification; Quick check—introduce known errors and measure detection rate
- **Subjectivity handling**: Why needed—to assess LLM performance on nuanced data; Quick check—compare accuracy for objective vs. subjective data items
- **Data item complexity**: Why needed—to identify task types suitable for LLM assistance; Quick check—stratify accuracy by data item complexity

## Architecture Onboarding
- **Component map**: Human extractors -> LLM extraction (extended/simple protocols) -> LLM review of human data -> Accuracy/precision/recall evaluation
- **Critical path**: Protocol design -> LLM prompting -> Data extraction -> Output comparison -> Performance assessment
- **Design tradeoffs**: Protocol complexity vs. extraction accuracy; recall vs. precision; human vs. LLM verification
- **Failure signatures**: High omission rate (low recall), misattribution of data, poor handling of subjective context
- **First experiments**:
  1. Test LLM accuracy on a larger, more diverse corpus of review materials
  2. Compare LLM and human extractors head-to-head for same data items
  3. Evaluate LLM performance on data items with increasing subjectivity and complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation based on only 10 evidence sources, limiting generalizability
- Low recall (<25%) raises concerns about completeness in comprehensive reviews
- LLM error detection capability is minimal, not suitable for review verification
- No inter-rater reliability assessment among human extractors

## Confidence
- Simple, structured data extraction (citation details): **High**
- Moderately complex tasks: **Medium**
- Subjective or context-heavy data items: **Low**

## Next Checks
1. Test LLM performance across a larger, more diverse corpus of review materials to assess generalizability
2. Evaluate LLM accuracy for data items with higher subjectivity or contextual complexity
3. Conduct head-to-head comparisons between LLM and human extractors for the same data items to quantify relative performance and identify specific error patterns