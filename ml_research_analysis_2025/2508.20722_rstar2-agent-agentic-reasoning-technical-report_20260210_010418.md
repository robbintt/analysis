---
ver: rpa2
title: 'rStar2-Agent: Agentic Reasoning Technical Report'
arxiv_id: '2508.20722'
source_url: https://arxiv.org/abs/2508.20722
tags:
- reasoning
- tool
- training
- code
- agentic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: rStar2-Agent is a 14B math reasoning model trained with agentic
  reinforcement learning to achieve frontier-level performance. It demonstrates advanced
  cognitive behaviors, such as thinking carefully before using Python coding tools
  and reflecting on code execution feedback to autonomously explore, verify, and refine
  intermediate steps in complex problem-solving.
---

# rStar2-Agent: Agentic Reasoning Technical Report

## Quick Facts
- arXiv ID: 2508.20722
- Source URL: https://arxiv.org/abs/2508.20722
- Authors: Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, Mao Yang
- Reference count: 12
- Key outcome: rStar2-Agent is a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. It demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) an efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. The model boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.

## Executive Summary
rStar2-Agent is a 14B parameter model that achieves state-of-the-art performance on mathematical reasoning benchmarks through agentic reinforcement learning. The model demonstrates advanced cognitive behaviors including careful tool selection, autonomous exploration, and iterative refinement of problem-solving steps. Unlike traditional reasoning models that rely on lengthy Chain-of-Thought, rStar2-Agent learns to use Python coding tools effectively, achieving high accuracy with significantly shorter responses. The training methodology combines efficient infrastructure, a novel RL algorithm (GRPO-RoC), and a unique non-reasoning fine-tuning approach to produce a model that surpasses much larger competitors while using minimal computational resources.

## Method Summary
The rStar2-Agent methodology consists of three main stages: First, a non-reasoning SFT stage fine-tunes the base model on function call, instruction, and chat examples without explicit reasoning training. Second, a three-stage GRPO-RoC reinforcement learning process trains the model to use Python tools effectively, starting with 8K context length and progressing to 12K and harder problems. The GRPO-RoC algorithm uses a Resample-on-Correct strategy that filters successful rollouts for tool errors while preserving failure mode diversity. The training infrastructure employs a load-balanced rollout scheduler that dynamically allocates requests based on KV-cache availability across 64 MI300X GPUs, enabling efficient multi-turn agentic rollouts. The final model achieves frontier-level performance on AIME and MATH benchmarks while maintaining response length efficiency.

## Key Results
- Achieves 80.6% pass@1 on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B)
- Demonstrates advanced cognitive behaviors including careful tool selection and iterative refinement
- Trains to peak performance in only 510 RL steps within one week on 64 MI300X GPUs
- Generalizes to alignment, scientific reasoning, and agentic tool-use tasks beyond mathematics

## Why This Works (Mechanism)

### Mechanism 1: Asymmetric Rollout Filtering (GRPO-RoC)
- **Claim:** Agentic reinforcement learning produces shorter, higher-quality reasoning trajectories if the training algorithm selectively filters successful rollouts for tool-use errors while preserving the diversity of failure modes.
- **Mechanism:** Standard outcome-based RL often reinforces "lucky" trajectories that contain tool errors but happen to produce a correct final answer. The Resample-on-Correct (RoC) strategy addresses this by oversampling 2G rollouts and downsampling the positive (correct) group to retain only those with minimal environment noise (low tool error ratio), whereas negative groups are uniformly sampled to maintain diverse error signals. This creates a gradient toward "clean" tool usage.
- **Core assumption:** Assumption: High-quality positive trajectories exist within the initial sampling distribution and can be identified via heuristic metrics like tool error rates.
- **Evidence anchors:**
  - [Section 2.2.3]: "RoC first oversamples a larger group of rollouts... Positive trajectories are filtered to retain only the highest-quality ones... while negative trajectories are uniformly downsampled."
  - [Figure 4]: Shows GRPO-RoC reduces tool call errors in positive trajectories over time compared to naive GRPO.
  - [corpus]: Specific validation for the RoC algorithm is absent in the provided corpus; related work focuses on general agentic capabilities.
- **Break condition:** If the base model lacks sufficient tool proficiency to generate *any* error-free successful rollouts early in training, the filtering mechanism may discard all positive signals, stalling learning.

### Mechanism 2: Dynamic KV-Cache Aware Scheduling
- **Claim:** Multi-turn agentic rollouts can be trained efficiently on limited GPU resources by dynamically allocating requests based on real-time KV-cache capacity rather than static batch allocation.
- **Mechanism:** Multi-turn tool use creates highly variable sequence lengths. Static allocation causes "head-of-line blocking" where GPUs sit idle waiting for the longest rollout to finish, or KV-cache overflow leading to costly recomputation. The Load-Balanced Rollout Scheduler monitors available KV-cache memory on each GPU and dispatches requests dynamically, ensuring compute density remains high despite variable turn lengths.
- **Core assumption:** Assumption: The overhead of monitoring cache and dispatching asynchronously is lower than the latency incurred by GPU starvation and eviction/recomputation loops.
- **Evidence anchors:**
  - [Section 3.2]: Describes how static allocation leads to "GPU idle time" and "KV cache overflow," while the dynamic scheduler assigns requests based on available capacity.
  - [Figure 7]: Visualizes the difference between static (idle gaps) and dynamic (dense) GPU utilization.
  - [corpus]: Corpus papers on large models (EXAONE, LongCat) imply infrastructure scaling challenges but do not detail this specific scheduling mechanism.
- **Break condition:** If the inference engine cannot accurately report free KV-cache slots in real-time, or if dispatch latency is high, dynamic scheduling may fail to prevent out-of-memory (OOM) errors.

### Mechanism 3: Non-Reasoning Cold Start
- **Claim:** Initializing RL with a model fine-tuned only for instruction following (not complex reasoning) encourages the discovery of more efficient, tool-augmented reasoning paths than starting with a model already biased toward long Chain-of-Thought (CoT).
- **Mechanism:** Prior "reasoning-heavy" SFT teaches models to solve problems using internal tokens. By skipping this and using Non-Reasoning SFT, the model starts with shorter response lengths. RL then learns to extend reasoning *only* when necessary, often offloading computation to the code environment rather than generating lengthy internal CoT.
- **Core assumption:** Assumption: Pre-trained base models possess latent reasoning capabilities that are better unlocked by exploration than by imitating existing long-form reasoning data.
- **Evidence anchors:**
  - [Section 4.1]: "Unlike prior works... we begin with a non-reasoning SFT stage... without enhancing reasoning. This avoids potential SFT overfitting."
  - [Table 2]: Compares the recipe (No Reasoning SFT, 8K->12K length) against methods like MiMo (Reasoning SFT, 32K->48K length).
  - [corpus]: No direct comparison in corpus; related works (e.g., EXAONE 4.0) integrate reasoning modes but do not explicitly argue against reasoning SFT.
- **Break condition:** If the base model's latent reasoning is too weak, the RL phase may fail to converge on correct solutions without the "warm start" of reasoning demonstrations.

## Foundational Learning

### Concept: Group Relative Policy Optimization (GRPO)
- **Why needed here:** This is the base RL algorithm. It estimates advantages by comparing different rollouts (group) for the same prompt, removing the need for a separate value model (critic).
- **Quick check question:** Can you explain why GRPO uses a group of outputs per prompt rather than a single output?

### Concept: Agentic Tool Loop (Multi-turn Rollout)
- **Why needed here:** Understanding the interaction loop (LLM -> Code -> Result -> LLM) is critical. The paper treats code execution as an environment interaction, not just text generation.
- **Quick check question:** What specific feedback signals does the environment return to the model after a tool call?

### Concept: Outcome-Only Rewards
- **Why needed here:** The paper strictly avoids process rewards to prevent "reward hacking," relying on the final answer correctness to drive learning.
- **Quick check question:** Why might outcome-only rewards be insufficient in a noisy environment without the RoC modification?

## Architecture Onboarding

### Component map:
Rollout Scheduler (CPU) -> Inference Engine (SGLang, GPU) -> Environment Service (isolated CPU workers) -> RL Trainer (VERL)

### Critical path:
The latency of the Environment Service (executing code) is the primary bottleneck in the rollout loop. Efficient asynchronous dispatch from the scheduler to the isolated environment workers is essential to keep GPUs utilized.

### Design tradeoffs:
- **Oversampling Factor:** The paper uses 2x oversampling (32 -> 16). Increasing this improves the chance of finding clean positive samples but linearly increases rollout compute cost.
- **Isolation vs. Speed:** The environment service isolates execution (safety) but adds IPC overhead (Redis/Dispatch) compared to local execution.

### Failure signatures:
- **Runaway Response Length:** If GRPO-RoC is disabled, you will see average response lengths balloon (8k -> 16k+) with high tool error rates inside correct trajectories.
- **KV-Cache Thrashing:** If using static allocation, expect to see significant "recomputation" warnings in logs and low GPU utilization metrics.

### First 3 experiments:
1. **Baseline Validation:** Run GRPO with tools but *without* RoC on a small batch. Verify that "correct" trajectories contain high tool-error rates (validating the noise problem).
2. **Scheduler Stress Test:** Compare static vs. dynamic scheduling on a batch with highly variable turn counts (e.g., complex math problems). Measure GPU idle time.
3. **Ablation on SFT:** Train two small modelsâ€”one with the paper's "non-reasoning" SFT and one with standard "reasoning" SFT. Compare the convergence speed and final response length.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do the high-entropy reflection behaviors and low-entropy coding tool usage patterns generalize to non-coding tools?
- **Basis in paper:** [explicit] The conclusion states, "How this phenomenon generalizes to other non-coding tools remains an open question for future work."
- **Why unresolved:** The current study is restricted to a Python code interpreter environment, making it unclear if the observed cognitive behaviors (e.g., entropy patterns) are unique to formal code execution.
- **What evidence would resolve it:** Experiments applying the rStar2-Agent framework to agentic tools like web search or API calls, followed by an entropy analysis of the resulting trajectories.

### Open Question 2
- **Question:** Is the observed training collapse after peak accuracy a fundamental limitation of the 14B model capacity or an artifact of the RL recipe?
- **Basis in paper:** [inferred] Section 5.3 notes that continued RL training led to collapse, stating, "We hypothesize the root cause is model capacity... this failure mode has not been reported publicly."
- **Why unresolved:** The authors tested various interventions (temperature scaling, length extension) without success, leaving the exact mechanism of this collapse undetermined.
- **What evidence would resolve it:** Comparative studies across varying model sizes (e.g., 7B vs. 32B) to see if the collapse point correlates with parameter count, or successful stabilization via modified optimization objectives.

### Open Question 3
- **Question:** Can the "Resample-on-Correct" (RoC) strategy remain effective when positive trajectories cannot be easily filtered for "quality" via execution errors?
- **Basis in paper:** [inferred] The paper explicitly ties RoC's success to filtering trajectories with "minimal tool-induced errors" (Section 2.2.3) in a deterministic environment.
- **Why unresolved:** The method assumes that "noisier" successful trajectories are lower quality. This assumption might fail in stochastic environments where error-prone paths still yield high-quality reasoning.
- **What evidence would resolve it:** Applying GRPO-RoC to tasks with stochastic tool outputs (e.g., web search) or subjective rewards to see if quality filtering improves or degrades performance.

## Limitations
- Environment noise filtering assumes tool errors are identifiable and filterable via simple heuristics, which may not generalize to domains beyond math
- Training data contamination concerns remain due to unspecified verification methodology for the "Math-Verifier" tool
- Resource scalability is limited as the reported 64 MI250X GPU setup with 512 batch size may not be accessible to all researchers
- Model capacity limitations appear to cause training collapse after peak performance, suggesting inherent reasoning ceilings

## Confidence

**High Confidence:**
- GRPO-RoC reduces tool errors in positive trajectories compared to vanilla GRPO
- Non-reasoning SFT leads to more efficient reasoning trajectories than reasoning SFT
- Load-balanced scheduling improves GPU utilization over static allocation

**Medium Confidence:**
- The combination of all three innovations is necessary for achieving state-of-the-art performance
- The model's generalization to non-math tasks (alignment, scientific reasoning) is meaningful rather than coincidental
- The 510-step training limit represents the maximum reasoning capability of the base model

**Low Confidence:**
- The exact impact of each innovation in isolation (lack of complete ablation studies)
- Whether the results would replicate with different base models or on non-math reasoning tasks
- The long-term stability of the learned policy beyond the reported training steps

## Next Checks
1. **Ablation Study Extension:** Perform a complete ablation study removing each innovation (GRPO-RoC, dynamic scheduling, non-reasoning SFT) individually and in combination to quantify their independent contributions to final performance.

2. **Cross-Domain Transfer:** Test rStar2-Agent on reasoning tasks outside mathematics (e.g., logical reasoning, commonsense reasoning) to validate the claimed generalization beyond math problems.

3. **Robustness to Environment Noise:** Introduce controlled variations in the code execution environment (different Python versions, library versions) to test whether the GRPO-RoC filtering remains effective when tool error patterns change.