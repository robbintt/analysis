---
ver: rpa2
title: 'FedPaI: Achieving Extreme Sparsity in Federated Learning via Pruning at Initialization'
arxiv_id: '2504.00308'
source_url: https://arxiv.org/abs/2504.00308
tags:
- pruning
- sparsity
- learning
- training
- fedpai
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FedPaI introduces a novel federated learning framework that leverages
  pruning at initialization (PaI) to achieve extreme sparsity levels up to 98% while
  maintaining model accuracy. Unlike conventional iterative pruning methods that progressively
  remove weights during training, FedPaI identifies optimal sparse connections at
  the start of training by analyzing gradient flow, eliminating the need for repeated
  pruning cycles and reducing communication overhead.
---

# FedPaI: Achieving Extreme Sparsity in Federated Learning via Pruning at Initialization

## Quick Facts
- arXiv ID: 2504.00308
- Source URL: https://arxiv.org/abs/2504.00308
- Reference count: 32
- Achieves 98% sparsity while maintaining model accuracy in federated learning

## Executive Summary
FedPaI introduces a novel federated learning framework that leverages pruning at initialization (PaI) to achieve extreme sparsity levels up to 98% while maintaining model accuracy. Unlike conventional iterative pruning methods that progressively remove weights during training, FedPaI identifies optimal sparse connections at the start of training by analyzing gradient flow, eliminating the need for repeated pruning cycles and reducing communication overhead. The framework supports both structured and unstructured pruning, enabling flexibility for different hardware environments. Experimental results demonstrate that FedPaI outperforms existing efficient FL approaches, achieving 6.4× to 7.9× training acceleration while maintaining accuracy comparable to unpruned baselines, even under challenging non-IID data distributions.

## Method Summary
FedPaI implements pruning at initialization (PaI) using gradient-based importance scoring (GraSP) to identify critical sparse connections before training begins. The framework generates personalized client-side masks using local data batches, then performs masked local training and uploads only non-zero weights. Server-side aggregation employs Top-κ magnitude selection to maintain sparse downloads. The method supports both unstructured pruning (client-side, personalized masks) and structured pruning (server-side, fixed mask), with unstructured mode showing superior performance at extreme sparsity levels under non-IID conditions.

## Key Results
- Achieves 98% sparsity while maintaining accuracy comparable to unpruned baselines
- Outperforms existing efficient FL approaches by 6.4× to 7.9× training acceleration
- Personalized client-side pruning improves accuracy by 1-5% under non-IID data distributions
- Sparsity-aware aggregation maintains sparse downloads without accuracy loss

## Why This Works (Mechanism)

### Mechanism 1: Gradient Flow Preservation via Pruning at Initialization
- Identifying sparse connections before training begins preserves learning capacity better than magnitude-based iterative pruning at high sparsity levels.
- Uses GraSP importance scores computed via Hessian-gradient products to identify connections that maximize gradient flow.
- Core assumption: Gradient flow at initialization is a valid proxy for long-term learning capacity in sparse networks.

### Mechanism 2: Personalized Client-Side Pruning for Non-IID Adaptation
- Generating client-specific sparsity masks using local data improves model capacity under heterogeneous data distributions compared to global server-side masks.
- Each client samples from their local dataset to compute personalized importance scores.
- Core assumption: Non-IID data distributions have sufficiently distinct feature representations that benefit from personalized connectivity patterns.

### Mechanism 3: Sparsity-Aware Aggregation for Download Efficiency
- Top-κ magnitude selection after averaging preserves the most important learned connections while maintaining sparse downloads.
- Naive FedAvg averaging makes the global model dense whenever any client has a non-zero weight.
- Core assumption: Weight magnitude in the averaged model reflects connection importance aggregated across clients.

## Foundational Learning

- **Federated Averaging (FedAvg)**: Why needed: FedPaI modifies the standard aggregation rule; understanding baseline FedAvg is essential to grasp why naive averaging destroys sparsity. Quick check question: If three clients have masks [1,0], [0,1], [1,0] and weights [2,0], [0,3], [4,0], what does FedAvg produce versus Top-κ(FedAvg)?

- **Gradient Flow and the Hessian-Gradient Product**: Why needed: The core pruning criterion uses Hg to approximate connection sensitivity; intuition about why this captures "learning capacity" is prerequisite. Quick check question: Why might a weight with small magnitude still be critical for gradient propagation?

- **Structured vs. Unstructured Pruning Trade-offs**: Why needed: FedPaI offers both modes; understanding hardware implications (sparse matrix support vs. dense tensor ops) determines deployment choices. Quick check question: On a standard GPU without sparse tensor cores, which pruning type yields actual speedup?

## Architecture Onboarding

- **Component map**: Server -> Clients (10% participation) -> Server (aggregation)
- **Critical path**: 1. Initialization → Server broadcasts dense initial weights; 2. Client-side PaI → Each client computes personalized mask mi via GraSP on local data batch; 3. Local training → Masked SGD updates for E local epochs; 4. Upload → Transmit only non-zero weights; 5. Aggregation → Server applies Top-κ (unstructured) or standard avg (structured); 6. Download → Transmit sparse global model
- **Design tradeoffs**: FedPaI-U (unstructured, client-side) vs. FedPaI-S (structured, server-side); client-side personalization improves accuracy but increases client compute overhead
- **Failure signatures**: Training collapse at high sparsity with magnitude-based pruning; convergence instability under non-IID; download bandwidth not reducing
- **First 3 experiments**: 1. Sanity check—IID, 50% sparsity: Run FedPaI-U vs. unpruned FedAvg on CIFAR-10 with VGG19, IID partition; 2. Non-IID stress test—α=0.1, 98% sparsity: Compare FedPaI-U vs. FedPaI-S vs. LotteryFL; 3. Hardware acceleration validation: Profile FedPaI-S training time per round vs. FedPaI-U on target hardware

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can FedPaI maintain its extreme sparsity and accuracy advantages when applied to Transformer-based architectures (e.g., ViT, BERT) in federated learning?
- Basis in paper: The Introduction explicitly identifies the evolution from CNNs to Transformers as a driver for resource constraints, yet the Experimental Evaluation restricts validation to VGG19 and ResNet18.
- Why unresolved: The PaI mechanism relies on gradient flow analysis (GraSP) or early mask stabilization (EBT), which may exhibit different dynamics in attention layers compared to convolutional layers.
- What evidence would resolve it: Experimental results on standard FL benchmarks (e.g., FedNLP, CIFAR-100 with ViT) showing convergence curves and sparsity-accuracy trade-offs for Transformer models.

### Open Question 2
- Question: How can the structured pruning variant (FedPaI-S) be enhanced to match the robustness of unstructured pruning (FedPaI-U) under extreme data heterogeneity?
- Basis in paper: The paper states that FedPaI-S "starts to degrade at sparsity levels exceeding 95%" under extreme non-IID settings ($\alpha=0.1$) because server-side masks fail to capture diverse local features, whereas FedPaI-U remains robust.
- Why unresolved: Structured pruning is critical for accelerating training on standard hardware without specialized sparse-compute support, making its poor performance in highly heterogeneous environments a significant deployment barrier.
- What evidence would resolve it: A modified structured pruning mechanism (e.g., personalized structured masks) that maintains accuracy comparable to FedPaI-U (within ~2%) at 95% sparsity when $\alpha < 0.1$.

### Open Question 3
- Question: Can FedPaI be effectively combined with quantization techniques to create a joint compression framework that exceeds the efficiency of pruning alone?
- Basis in paper: Table I compares FedPaI against quantization baselines (e.g., AdaQuantFL) as separate competing methods, but does not explore a hybrid approach, despite quantization being a standard complementary technique.
- Why unresolved: Combining extreme sparsity (98%) with low-bit quantization often leads to training instability or accuracy collapse in decentralized settings due to compounded gradient noise.
- What evidence would resolve it: A joint FedPaI-Quantization implementation that achieves stable convergence with a total compression ratio exceeding 100x (e.g., 98% sparse + 4-bit weights).

### Open Question 4
- Question: What is the actual energy consumption and latency performance of FedPaI on real edge devices with limited compute resources?
- Basis in paper: The Methodology section notes that the FL environment is "simulated... on Nvidia A100 GPUs" and estimates acceleration based on profiling rather than physical deployment on edge hardware.
- Why unresolved: Simulation on high-performance GPUs abstracts away critical edge constraints such as memory bandwidth bottlenecks, thermal throttling, and the overhead of sparse matrix handling on non-specialized mobile CPUs.
- What evidence would resolve it: On-device benchmarks (e.g., on Raspberry Pi or Jetson Nano) measuring wall-clock training time per round and total energy consumption in Joules.

## Limitations

- GraSP-based importance scoring relies on a single batch for mask generation, which may not capture full data distribution under severe non-IID conditions
- Hessian-gradient product computation assumes local convexity around initialization, which may not hold for deep networks
- Server-side structured pruning shows reduced accuracy at 98% sparsity under extreme non-IID, suggesting limitations in this mode

## Confidence

- **High Confidence**: Gradient flow preservation via PaI outperforms iterative magnitude pruning at extreme sparsity (validated by Figure 8 convergence comparison)
- **Medium Confidence**: Client-side personalized pruning significantly improves non-IID adaptation (supported by Figure 5 accuracy gains, but batch size effects unknown)
- **Medium Confidence**: Sparsity-aware aggregation maintains sparse downloads without accuracy loss (mechanism described but independent validation needed)
- **Low Confidence**: FedPaI-S structured mode performance at 98% non-IID (limited data points, shows degradation)

## Next Checks

1. **Gradient Flow Validation**: Run ablation comparing FedPaI-U (GraSP) vs. magnitude-based PaI at 95% sparsity on CIFAR-10; expect >5% accuracy gap confirming gradient flow preservation

2. **Personalization Sensitivity**: Vary local data batch size (Db) from 1 to 100 samples in FedPaI-U; measure accuracy degradation under α=0.1 non-IID to quantify personalization robustness

3. **Aggregation Mechanism Stress Test**: Compare Top-κ magnitude aggregation vs. random Top-κ selection after FedAvg; expect significant accuracy drop with random selection, validating magnitude proxy assumption