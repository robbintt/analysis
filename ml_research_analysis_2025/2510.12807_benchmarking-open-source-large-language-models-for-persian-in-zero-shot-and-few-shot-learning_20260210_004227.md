---
ver: rpa2
title: Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and
  Few-Shot Learning
arxiv_id: '2510.12807'
source_url: https://arxiv.org/abs/2510.12807
tags:
- persian
- language
- performance
- tasks
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive benchmark of open-source large
  language models (LLMs) for Persian natural language processing tasks, evaluating
  both zero-shot and few-shot learning paradigms across sentiment analysis, named
  entity recognition, reading comprehension, and question answering. The study assesses
  11 prominent open-source models including Gemma2, GLM4, Qwen variants, Llama models,
  and others using established Persian datasets such as ParsiNLU and ArmanEmo.
---

# Benchmarking Open-Source Large Language Models for Persian in Zero-Shot and Few-Shot Learning

## Quick Facts
- arXiv ID: 2510.12807
- Source URL: https://arxiv.org/abs/2510.12807
- Reference count: 33
- Key outcome: Gemma2 consistently outperforms other models across nearly all Persian NLP tasks in both zero-shot and few-shot learning paradigms

## Executive Summary
This paper presents a comprehensive benchmark of 11 open-source large language models for Persian natural language processing tasks, evaluating both zero-shot and few-shot learning paradigms across sentiment analysis, named entity recognition, reading comprehension, and question answering. The study assesses prominent models including Gemma2, GLM4, Qwen variants, Llama models, and others using established Persian datasets such as ParsiNLU and ArmanEmo. Gemma2 consistently outperforms other models across nearly all tasks in both learning paradigms, with particularly strong performance in complex reasoning tasks. However, most models struggle with token-level understanding tasks like Named Entity Recognition, highlighting specific challenges in Persian language processing.

## Method Summary
The study evaluates 11 open-source LLMs (7B-70B parameters) on Persian NLP tasks using standardized zero-shot and 5-shot prompting approaches. Models are tested on established Persian datasets including ParsiNLU (reading comprehension, entailment, sentiment, MT), ArmanEmo (emotion), ArmanNER (NER), Persian MMLU, and Persian News Summary/XLSummary. Zero-shot evaluation uses natural language instructions, while few-shot uses 5 randomly selected examples per task. Inference is performed with temperature=0.1, greedy decoding for classification, and beam width=4 for generation tasks, using Hugging Face Transformers v4.30.2 and PyTorch v2.0.1 with FP16 mixed precision.

## Key Results
- Gemma2 achieves highest average performance (0.61) across all tasks and paradigms
- Few-shot learning yields 13.8% average performance improvement over zero-shot
- Most models struggle with Named Entity Recognition, showing significant performance gaps
- Complex reasoning tasks benefit most from few-shot prompting (up to 17.3% improvement)

## Why This Works (Mechanism)

### Mechanism 1: Few-Shot In-Context Learning Enhancement
- Claim: Few-shot prompting improves Persian NLP performance by approximately 13.8% on average
- Mechanism: In-context examples provide task-specific patterns that activate cross-lingual knowledge representations already present in the model
- Core assumption: Models have acquired sufficient multilingual representations during pretraining that can be triggered by demonstration examples
- Evidence anchors: [abstract] "few-shot learning generally yields better results for most models, with an average performance improvement of 13.8%"; [section 4.1] "models generally performed better in few-shot learning scenarios"
- Break condition: Token-level prediction tasks show minimal improvement (NER averaged only 7.2% gain)

### Mechanism 2: Cross-Lingual Transfer via Architectural and Pretraining Quality
- Claim: Gemma2's consistent outperformance suggests architectural and pretraining advantages that enhance Persian cross-lingual transfer
- Mechanism: Enhanced multilingual representations and potentially higher-quality Persian data during pretraining create better aligned token embeddings and attention patterns
- Core assumption: The performance differential stems from model design and training rather than evaluation artifacts
- Evidence anchors: [abstract] "Gemma 2 consistently outperforms other models across nearly all tasks"; [section 5.1] "This exceptional cross-lingual generalization capability"
- Break condition: Performance advantages may diminish on tasks requiring explicit cultural knowledge

### Mechanism 3: Persistent Token-Level Processing Gap for Persian Morphosyntax
- Claim: Token-level tasks like NER remain challenging across all models due to Persian's morphological complexity
- Mechanism: Persian's right-to-left script, zero-width non-joiners, and complex morphology create tokenization ambiguities that prompt-based approaches cannot resolve
- Core assumption: The difficulty is inherent to Persian's linguistic structure rather than merely insufficient training data
- Evidence anchors: [abstract] "most models struggle with token-level understanding tasks like Named Entity Recognition"; [section 5.3] "Models demonstrated particular difficulty with entity boundary detection"
- Break condition: Specialized Persian tokenizers or character-level architectures could potentially mitigate boundary detection issues

## Foundational Learning

- Concept: Zero-Shot vs. Few-Shot Learning Paradigms
  - Why needed here: The paper's evaluation framework hinges on comparing these approaches; understanding their mechanisms explains the 13.8% average improvement
  - Quick check question: Why does few-shot learning improve reading comprehension by 17.3% but NER by only 7.2%?

- Concept: Cross-Lingual Transfer in Multilingual LLMs
  - Why needed here: Explains why English-dominant models achieve non-trivial Persian performance and what factors enhance transfer quality
  - Quick check question: What architectural or pretraining characteristics might explain why Gemma2 achieves 0.61 average score while Llama3.2 achieves only 0.21?

- Concept: Persian Linguistic Challenges (Morphology and Orthography)
  - Why needed here: Critical for understanding the 18.7% performance gap versus English benchmarks and persistent NER difficulties
  - Quick check question: How do Persian's right-to-left script and morphological fusion affect token-level entity boundary detection?

## Architecture Onboarding

- Component map: Input Persian text -> Standardized prompt templates (zero-shot: task instruction only; few-shot: 5 examples + instruction) -> 11 evaluated models (7B-70B parameters) -> Temperature=0.1 generation, greedy decoding for classification, beam width=4 for translation/summarization -> Task-specific metrics calculation (Accuracy/F1, BLEU, ROUGE, Exact Match + F1) -> NVIDIA A100 GPUs, Hugging Face Transformers v4.30.2, PyTorch v2.0.1 with FP16

- Critical path: 1. Prompt standardization across all models (ensures fair comparison) 2. Few-shot example selection (5 examples per task, randomly sampled with diversity constraints) 3. Inference execution with fixed hyperparameters 4. Metric calculation per task type using datasets library 5. Statistical significance testing (paired t-tests, p<0.01 threshold)

- Design tradeoffs: Standardized prompts vs. model-optimized prompts (chose standardized for comparability); 5-shot vs. more examples (5-shot balances context window constraints with demonstration diversity); Task-specific decoding strategies (greedy for deterministic classification, beam search for fluent generation)

- Failure signatures: NER scores collapse in zero-shot (0.00-0.13 range for most models); Multi-token entity errors 27.8% higher than single-token entities; Idiomatic expression misinterpretation 34.5% higher error rate than literal text; Semantic disambiguation fails 23.7% more often for neutral vs. contradictory pairs

- First 3 experiments: 1. Run zero-shot baseline on ParsiNLU reading comprehension and sentiment analysis across all 11 models to establish initial capability tiers 2. Add 5-shot examples systematically and measure per-task delta to identify which task types benefit most from in-context learning 3. Conduct fine-grained error analysis on NER outputs, categorizing failures by entity type (PER/LOC/ORG), token count, and orthographic variation

## Open Questions the Paper Calls Out

- Question: How do dedicated Persian-centric models (e.g., PersianMind, Maral) compare against the top-performing general multilingual model Gemma 2 on these specific benchmarks?
  - Basis in paper: [explicit] The authors explicitly call for expanding evaluation to "emerging Persian-specialized LLMs" to determine the benefits of language-specific pretraining versus general approaches
  - Why unresolved: This study focused on prominent general open-source models and did not include the emerging class of Persian-adapted architectures
  - What evidence would resolve it: A direct benchmark comparison of PersianMind and Maral against Gemma 2 using the ParsiNLU and ArmanEmo datasets

- Question: To what extent can advanced prompting strategies like Chain-of-Thought (CoT) or Retrieval-Augmented Generation (RAG) optimize performance over the standardized few-shot prompting used in this study?
  - Basis in paper: [explicit] Section 6.3 suggests that "exploring dynamic prompt tuning, chain-of-thought reasoning, and retrieval-augmented generation specifically optimized for Persian" could yield performance gains
  - Why unresolved: The current methodology relied on standardized prompts without extensive optimization, leaving the potential of complex reasoning chains or external knowledge retrieval untested
  - What evidence would resolve it: Comparative results using CoT and RAG prompting techniques on the low-scoring reasoning and reading comprehension tasks

- Question: What specific morphosyntactic features of Persian cause the "fundamental limitations" observed in token-level Named Entity Recognition (NER)?
  - Basis in paper: [inferred] The paper notes persistent NER struggles (Section 5.2) and suggests future "linguistic analysis" focused on "morphosyntactic phenomena" to identify architectural limitations
  - Why unresolved: The error analysis identified boundary detection issues but did not isolate the specific linguistic rules responsible for the 27.8% higher error rate in multi-token entities
  - What evidence would resolve it: A fine-grained linguistic analysis correlating specific Persian grammatical structures with failure rates in token classification tasks

- Question: Can parameter-efficient fine-tuning (PEFT) methods like LoRA or P-tuning significantly narrow the 18.7% performance gap between Persian and English without full retraining?
  - Basis in paper: [explicit] Section 6.3 proposes "investigating efficient parameter-efficient fine-tuning approaches (e.g., LoRA, P-tuning) specifically for Persian"
  - Why unresolved: The study evaluated frozen models via zero-shot and few-shot inference, without applying gradient-based adaptation techniques
  - What evidence would resolve it: Performance metrics of LoRA-adapted versions of the benchmarked models on the Persian MMLU and reasoning tasks

## Limitations

- Prompt standardization across 11 diverse models may not leverage each model's specific strengths or prompt-tuning capabilities
- 5-shot example selection process lacks detail on diversity criteria or seed values, potentially introducing evaluation variance
- Paper does not specify exact model checkpoints or parameter scales beyond the 7B-70B range, affecting reproducibility

## Confidence

- **High Confidence**: Gemma2's consistent outperformance across paradigms and tasks (0.61 average score vs. 0.21 for Llama3.2); 13.8% average improvement from few-shot learning is directly measured
- **Medium Confidence**: Attribution of performance differences to architectural and pretraining quality is reasonable but requires deeper architectural analysis
- **Low Confidence**: 18.7% performance gap between Persian and English benchmarks is cited but not directly measured in this study

## Next Checks

1. **Prompt Optimization Study**: Conduct ablation experiments testing both standardized prompts and model-specific optimized prompts for each of the 11 evaluated models to quantify the trade-off between comparability and absolute performance.

2. **Token-Level Architecture Investigation**: Design controlled experiments comparing standard decoder-based models against character-level or specialized Persian tokenizers on NER and other token-boundary-sensitive tasks to isolate whether the 27.8% higher error rate for multi-token entities stems from model architecture or prompt methodology.

3. **Cross-Lingual Transfer Analysis**: Systematically evaluate the same 11 models on parallel English-Persian datasets (when available) using identical prompt templates to directly measure the 18.7% performance differential and identify which task types and model architectures show the most severe cross-lingual degradation.