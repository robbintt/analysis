---
ver: rpa2
title: 'Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs
  without Training'
arxiv_id: '2601.07359'
source_url: https://arxiv.org/abs/2601.07359
tags:
- uni00000015
- uni00000014
- attention
- uni00000018
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses a key inconsistency in Multimodal Large Language\
  \ Models (MLLMs) where, despite correct visual understanding in deeper layers, final\
  \ predictions are often misled by noisy attention from earlier layers\u2014termed\
  \ \"seeing it right but saying it wrong.\" To resolve this, the authors propose\
  \ DualPD, a training-free dual-perspective decoding refinement strategy that improves\
  \ visual grounding without additional training. DualPD operates in two ways: (1)\
  \ layer-wise attention-guided contrastive logits, which identify and compare logits\
  \ between layers with the largest attention shift to capture evolving visual understanding,\
  \ and (2) head-wise information filtering, which suppresses low-contribution attention\
  \ heads to reduce noise."
---

# Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training

## Quick Facts
- **arXiv ID**: 2601.07359
- **Source URL**: https://arxiv.org/abs/2601.07359
- **Reference count**: 40
- **Primary result**: Training-free dual-perspective decoding refinement improves visual grounding in MLLMs, achieving up to 2.5% average accuracy gains across six benchmarks.

## Executive Summary
This work addresses a fundamental inconsistency in Multimodal Large Language Models (MLLMs) where deeper layers correctly ground visual information but final predictions are corrupted by noisy attention from earlier layers—the "seeing it right but saying it wrong" problem. The authors propose DualPD, a training-free refinement strategy that operates through two complementary mechanisms: layer-wise contrastive amplification of evolving visual understanding and intra-layer attention filtering of low-contribution heads. Experiments on LLaVA and Qwen-VL models across six multimodal benchmarks demonstrate consistent accuracy improvements without requiring additional training.

## Method Summary
DualPD is a training-free inference-time refinement method that improves visual grounding in MLLMs by addressing the disconnect between correct visual understanding in deeper layers and noisy final predictions. The method operates in two stages: (1) Inter-layer contrastive attention—computes Hellinger distance between attention maps of adjacent layers, selects the layer with maximum attention shift as "basic layer," and computes contrastive logits by subtracting this layer's logits from the final layer's logits; (2) Intra-layer attention refine—scores attention heads by L2 norm of their pre-softmax attention maps, suppresses bottom 15% heads using soft decay factor γ=0.9, and applies this filtering before computing the final contrastive logits. The approach requires capturing intermediate activations during the forward pass but adds no training overhead.

## Key Results
- **Consistent gains**: DualPD achieves average accuracy improvements of up to 2.5% across six multimodal benchmarks (GQA, VQAv2, OKVQA, VizWiz, TextVQA, DocVQA).
- **Generalizability**: The method works effectively across different model architectures including LLaVA-1.5/1.6 and Qwen-VL models, demonstrating broad applicability.
- **Training-free**: DualPD requires only a forward pass and inference-time computation, avoiding the computational overhead of training separate refinement models.

## Why This Works (Mechanism)

### Mechanism 1: Inter-layer Contrastive Amplification
Deep layers acquire correct visual grounding that gets obscured by accumulated noise from earlier layers; contrasting logits amplifies the correct signal by computing Δz = z^(lt) - z^(lb) where lt is the final layer and lb is the "basic" layer selected via maximum attention shift. This high-pass filters semantic evolution, isolating visual information from later stages while suppressing static noisy priors. The core assumption is monotonic evolution of visual grounding with depth. Break condition: if visual grounding is not monotonic (e.g., middle layers more accurate than final), simple subtraction may fail or amplify intermediate noise.

### Mechanism 2: Attention-Guided Dynamic Layer Selection
Static layer selection is suboptimal compared to identifying the layer with maximum visual attention shift. DualPD calculates Hellinger distance between attention maps of adjacent layers and selects the basic layer lb where this shift is maximized, identifying the point of largest visual transition. The core assumption is that largest attention shift correlates with most significant semantic update regarding visual input. Break condition: if "largest shift" corresponds to distraction rather than correct reasoning, the selected basic layer provides poor contrastive baseline.

### Mechanism 3: Intra-layer Head-wise Noise Filtering
Not all attention heads contribute equally; low-activity heads introduce noise that dilutes attention signal. DualPD scores heads by L2 norm of pre-softmax attention maps and applies soft mask (decay factor γ=0.9) to logits of heads with lowest scores (bottom 15%), suppressing their influence. The core assumption is that head importance correlates directly with L2 norm of attention weights. Break condition: if critical reasoning feature relies on sparse attention pattern (resulting in low L2 norm), this mechanism might accidentally suppress correct signal.

## Foundational Learning

- **Logit Lens / Layer-wise Decoding**: Understanding how to extract predictions from intermediate layers is essential for computing Δz. Quick check: Can you explain how to project a hidden state from layer 16 into vocabulary space using the model's unembedding matrix?

- **Hellinger Distance**: This metric quantifies "shift" in attention and is preferred over L2 or Cosine for probability distributions because it's bounded and sensitive to overlap. Quick check: Why is Hellinger distance often preferred over KL Divergence for measuring similarity between two attention distributions that might have zero values?

- **Soft Masking / Logit Decay**: The paper uses soft decay (multiplying logits by 0.9) rather than hard pruning. Quick check: If a head's logits are multiplied by 0.9 before Softmax, how does this affect the probability distribution compared to completely removing the head?

## Architecture Onboarding

- **Component map**: Forward Pass Hook -> Attention Map Extractor -> Hellinger Distance Calculator -> Layer Selector -> Head Scorer (L2 norm) -> Logit Projector -> Refinement Engine (soft masking + contrastive subtraction)

- **Critical path**: The selection of the basic layer (lb). If this selection points to a layer too close to the final layer, the contrastive signal Δz will be weak, failing to suppress noise.

- **Design tradeoffs**: Efficiency vs. Accuracy (requires storing intermediate activations and computing logits for all layers, increasing memory/compute overhead); Suppression Ratio (hard-cutting bottom 15% of heads may require tuning for different architectures).

- **Failure signatures**: Performance drop on text-heavy tasks where visual-attention-based layer selection might degrade linguistic coherence; instability when basic layer is consistently selected as very early layer, making contrast too aggressive.

- **First 3 experiments**: (1) Reproduce Static vs. Dynamic Selection by plotting accuracy on VQAv2 while forcing lb to be layer 1, 5, 10... etc., comparing against Hellinger-based dynamic selection; (2) Ablation on γ by varying decay factor from 0.0 to 1.0 on bottom 15% heads to verify non-linear performance curve; (3) Cross-Model Generalization by applying exact hyperparameters optimized for LLaVA-1.5 directly to Qwen-VL without tuning.

## Open Questions the Paper Calls Out
None explicitly stated in the provided materials.

## Limitations
- The core premise of monotonic visual grounding evolution remains unproven as universal property of MLLMs and may fail when intermediate layers capture more accurate visual context than final layers.
- The head importance metric (L2 norm of attention weights) assumes direct correlation between attention magnitude and contribution, potentially failing for sparse or diffuse attention patterns encoding critical information.
- The fixed suppression ratio (15%) and decay factor (γ=0.9) are heuristic choices that may require task-specific tuning, contradicting the "training-free" claim when optimal parameters vary by task.

## Confidence
- **High confidence**: Implementation details of attention distance computation (Hellinger), head scoring (L2 norm), and contrastive logit subtraction are mathematically sound and reproducible.
- **Medium confidence**: Claim of consistent 2.5% average gains across six benchmarks is supported by reported results but depends on whether the phenomenon generalizes beyond tested models and tasks.
- **Low confidence**: Assertion that DualPD is "training-free" while requiring inference-time computation of all intermediate activations and logits—effectively doubling computational overhead—represents semantic distinction that may not align with practical deployment constraints.

## Next Checks
1. **Layer-wise Accuracy Profile**: For each benchmark, plot raw accuracy of layer-wise predictions to verify monotonic visual grounding assumption and identify cases where middle layers outperform final layers.

2. **Cross-Domain Generalization**: Apply DualPD to non-VQA tasks (e.g., image captioning or visual reasoning) where the phenomenon may manifest differently, testing broader applicability.

3. **Ablation of Attention Shift Metric**: Replace Hellinger distance with alternative metrics (KL divergence, Jensen-Shannon) for layer selection and compare performance degradation to establish criticality of specific distance metric.