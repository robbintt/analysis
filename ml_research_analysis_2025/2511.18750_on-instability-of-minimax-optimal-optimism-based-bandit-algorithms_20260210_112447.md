---
ver: rpa2
title: On Instability of Minimax Optimal Optimism-Based Bandit Algorithms
arxiv_id: '2511.18750'
source_url: https://arxiv.org/abs/2511.18750
tags:
- page
- lemma
- bandit
- algorithms
- proof
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the stability properties of minimax optimal
  optimism-based bandit algorithms, focusing on the Lai-Wei stability condition. The
  authors establish general structural conditions under which optimism-based bandit
  algorithms violate stability, proving that widely used algorithms including MOSS,
  Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and
  Anytime KL-UCB-SWITCH are unstable when rewards are 1-sub-Gaussian.
---

# On Instability of Minimax Optimal Optimism-Based Bandit Algorithms

## Quick Facts
- **arXiv ID:** 2511.18750
- **Source URL:** https://arxiv.org/abs/2511.18750
- **Reference count:** 0
- **Primary result:** Widely used minimax optimal optimism-based bandit algorithms violate stability conditions and fail to exhibit asymptotic normality of sample means for 1-sub-Gaussian rewards.

## Executive Summary
This paper investigates the stability properties of minimax optimal optimism-based bandit algorithms, focusing on the Lai-Wei stability condition. The authors establish general structural conditions under which optimism-based bandit algorithms violate stability, proving that widely used algorithms including MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH are unstable when rewards are 1-sub-Gaussian. Numerical simulations demonstrate that sample means fail to exhibit asymptotic normality for all these algorithms. The results suggest a fundamental tension between stability and minimax optimality, raising the open question of whether simultaneously stable and minimax optimal bandit strategies exist.

## Method Summary
The authors conduct numerical simulations to demonstrate instability by implementing nine optimism-based bandit algorithms (MOSS, Anytime-MOSS, Vanilla-MOSS, ADA-UCB, OC-UCB with ε=0.1, KL-MOSS, KL-UCB++, KL-UCB-SWITCH, and Anytime KL-UCB-SWITCH) on a two-armed bandit problem with N(0,1) reward distributions for both arms. They run 5000 independent simulations with T=10000 pulls each, computing empirical relative frequencies of arm pulls and coverage of Wald confidence intervals for mean reward estimation. The simulation results are compared against theoretical predictions of instability derived from the Lai-Wei stability condition and structural analysis of optimism-based algorithms.

## Key Results
- All nine tested optimism-based algorithms violate the Lai-Wei stability condition for 1-sub-Gaussian rewards
- Sample means fail to exhibit asymptotic normality across all algorithms in numerical simulations
- Wald confidence intervals show undercoverage at all nominal levels tested (0.80-0.99)
- Instability arises from algorithms maintaining constant exploration bonuses in later rounds (t ≥ T/2K)
- Fundamental tension exists between achieving minimax optimality and stability

## Why This Works (Mechanism)
The instability mechanism stems from the structure of optimism-based algorithms that maintain constant exploration bonuses when the number of remaining rounds is sufficiently large. These algorithms violate the Lai-Wei stability condition, which requires that the probability of selecting suboptimal arms vanishes asymptotically. The constant bonus regime creates persistent exploration behavior that prevents convergence to optimal arms, leading to sample mean distributions that deviate from asymptotic normality.

## Foundational Learning
- **Lai-Wei stability condition:** A necessary condition for asymptotic normality requiring P(a(t) ≠ a*) → 0; needed to establish theoretical instability framework
- **Optimism principle:** Algorithms select arms based on upper confidence bounds; needed to understand algorithm structure
- **Sub-Gaussian rewards:** Distributional assumption (σ=1) ensuring concentration properties; needed for theoretical analysis
- **Wald confidence intervals:** Statistical tool for assessing normality; needed to demonstrate empirical instability
- **KL divergence bounds:** Used in KL-based algorithms for tighter confidence bounds; needed for implementing KL-UCB variants
- **Minimax optimality:** Achieves optimal worst-case regret; needed to understand the stability-optimality tradeoff

## Architecture Onboarding
**Component map:** Bandit environment -> Algorithm implementation -> Arm selection -> Reward generation -> Statistics tracking -> CI coverage analysis

**Critical path:** Algorithm UCB calculation → Arm selection → Reward observation → Empirical mean update → CI computation → Coverage verification

**Design tradeoffs:** Theoretical stability vs. minimax regret optimality; numerical precision in KL optimization vs. implementation simplicity; simulation runtime vs. statistical power

**Failure signatures:** Wald CIs showing correct coverage would contradict instability claims; convergence to optimal arm in early rounds would mask instability; numerical errors in KL bounds could invalidate KL-UCB results

**First experiments:**
1. Verify symmetric N(0,1) initialization for both arms across all simulation runs
2. Check KL divergence numerical precision by comparing against analytical bounds
3. Validate UCB bonus calculations match theoretical formulas for each algorithm

## Open Questions the Paper Calls Out
None specified in the reproduction notes.

## Limitations
- Exact nominal coverage levels and specific points plotted on coverage curves remain unspecified
- Numerical precision of KL divergence bounds in KL-based algorithms may vary across implementations
- Procedure for computing empirical coverage (fixed T vs. averaged windows) lacks specification

## Confidence
**High:** Theoretical framework establishing instability via Lai-Wei condition and structural arguments is rigorous and independent of simulation details
**Medium:** Simulation implementation details are partially underspecified, particularly KL bounds precision and exact coverage calculation procedures

## Next Checks
1. Reproduce the N(0,1) two-armed bandit simulations with all listed algorithms, focusing on KL-UCB variants where numerical KL optimization is critical
2. Verify Wald CI coverage at multiple nominal levels (0.80-0.99) and compare against reported undercoverage
3. Test whether identical arm means are truly maintained across all simulation runs, as instability result relies on symmetric reward distributions