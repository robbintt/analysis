---
ver: rpa2
title: 'Deep Progressive Training: scaling up depth capacity of zero/one-layer models'
arxiv_id: '2511.04981'
source_url: https://arxiv.org/abs/2511.04981
tags:
- layer
- training
- loss
- progressive
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies progressive training for deep neural networks,
  focusing on depth expansion to reduce computational cost while maintaining performance.
  It introduces zero/one-layer progressive training, where small models are first
  trained then expanded by adding new layers.
---

# Deep Progressive Training: scaling up depth capacity of zero/one-layer models

## Quick Facts
- **arXiv ID**: 2511.04981
- **Source URL**: https://arxiv.org/abs/2511.04981
- **Reference count**: 40
- **Primary result**: Zero/one-layer progressive training achieves ~5× speedup compared to training deep models from scratch while maintaining performance.

## Executive Summary
This work introduces zero/one-layer progressive training, a method for efficiently training deep neural networks by starting with shallow models and expanding depth during training. The approach leverages small initial models to minimize early computation, then expands to full depth at a carefully chosen timing. Theoretical analysis shows this behaves like projected gradient descent with good initialization, while empirical results demonstrate 5× speedup across GPT2, ResNet, and MoE architectures with minimal loss degradation.

## Method Summary
The method trains a small model (0 or 1 layer) with warmup-stable-decay schedule, then expands depth by adding new layers at the bottom of the stack using random or copying initialization. Key innovations include muP-based hyperparameter transfer enabling zero-shot scaling, and WSD schedules that preserve mixing time during the stable phase. The expansion timing is determined by measuring mixing time tmix through pilot experiments, then setting τ = T_stable − tmix. The approach works across architectures and supports single-stage expansion, avoiding complexity of multi-stage approaches.

## Key Results
- Zero/one-layer expansion achieves ~5× speedup compared to training deep models from scratch
- WSD schedule enables expansion up to 80% of training without convergence penalty
- Random and copying initialization both preserve muP spectral scaling for feature learning
- Single-stage expansion (0→12) matches or exceeds multi-stage (0→2→12) efficiency
- The method works across GPT2, ResNet, and MoE architectures

## Why This Works (Mechanism)

### Mechanism 1: Zero/One-Layer Depth Expansion Minimizes Pre-Expansion Compute
The small model learns representations during the cheap early phase. After depth expansion at τ, the grown model requires a fixed amount of data (tmix) to converge to the same loss as fixed-size training. Since tmix is measured in tokens not iterations, and τ can be pushed to ≈0.8T under WSD, total compute ≈ 0.02·Nlarge·0.8T + Nlarge·0.2T ≈ 0.2× the fixed-size cost.

### Mechanism 2: WSD Schedule Preserves Mixing Time During Stable Phase
Theoretical analysis shows the loss difference contains a term minimized by smaller η before τ. Empirically, tmix(τ) ≈ 30k iterations (≈16B tokens) regardless of τ under WSD, but tmix grows explosively under cosine once decay begins.

### Mechanism 3: Random/Copying Initialization Preserves muP Feature Learning
muP theory requires consistent activation element sizes across layers: ||Al||2/√nl ∼ ||Al+1||2/√nl+1. Random initialization naturally satisfies spectral norms ∼ √ratio. Zero initialization violates this (||Wl||* = 0), breaking gradient flow and feature learning despite being function-preserving.

## Foundational Learning

- **Concept: Maximal Update Parameterization (muP)**
  - **Why needed here**: Explains why learning rate and other HPs transfer from 39M to 7B model without retuning; guides initialization choice.
  - **Quick check question**: Given a 1-layer model with optimal LR=0.01, what LR should you use for the 12-layer expansion?

- **Concept: Residual Network Function Preservation**
  - **Why needed here**: Understanding why inserting layers doesn't break forward pass; critical for expansion ordering decisions.
  - **Quick check question**: Why does zero-initializing a new layer in a ResNet preserve the function, but the same operation in a vanilla MLP does not?

- **Concept: Mixing Time vs. Convergence Time**
  - **Why needed here**: Distinguishes when progressive training "catches up" (mixing) from final convergence; determines valid expansion timing.
  - **Quick check question**: If you expand at τ=0.8T with 40k iterations remaining, and mixing time is 30k iterations, will you match fixed-size training?

## Architecture Onboarding

- **Component map**: Embedding -> Source model (0/1 layer) -> Expansion operation (add layers at bottom) -> Target model (full depth) -> Optimizer (Muon-NSGD) -> WSD schedule
- **Critical path**:
  1. Train source model with WSD from iteration 0
  2. Determine mixing time tmix via early-stop pilot (expand right after warmup, measure iterations to match fixed-size loss)
  3. Set expansion time τ = T_stable − tmix
  4. At τ: expand depth with random (0-layer) or copying (1-layer) initialization
  5. Continue training with unchanged hyperparameters through stable then decay phase

- **Design tradeoffs**:
  - **0-layer vs. 1-layer source**: 0-layer maximizes compute savings but may have higher loss spike; 1-layer provides smoother transition
  - **Random vs. copying**: Random avoids ordering complexity; copying provides marginally faster mixing for 1-layer
  - **Single-stage vs. multi-stage**: Multi-stage (0→2→12) is equivalent or worse than single-stage (0→12) due to mixing behavior composition
  - **Optimizer state handling**: Inheriting, copying, or resetting all work; copying may be less stable

- **Failure signatures**:
  - **Loss spike >2× at expansion**: Likely using zero initialization or expanding during decay phase
  - **Final loss 5%+ worse than fixed-size**: Expansion too late (τ>0.8T) or using cosine schedule
  - **No mixing after 2× tmix tokens**: Learning rate mismatch; verify muP scaling applied correctly
  - **Training instability post-expansion**: Check that random layers inserted at bottom not top

- **First 3 experiments**:
  1. **Pilot mixing time estimation**: Train fixed-size 12-layer and progressive (expand at iteration 2k post-warmup) for 25k iterations; measure iterations until losses converge. This gives tmix for your data/optimizer combo.
  2. **Expansion timing sweep**: With tmix known, run 3-5 progressive trainings varying τ/T (0.5, 0.6, 0.7, 0.8) under WSD; confirm robustness plateau. Expect all τ≤0.8T to reach same final loss.
  3. **Initialization ablation**: Compare random vs. copying vs. zero initialization for 1→12 expansion at τ=0.5T. Verify random/copying match, zero fails (or use copying_zeroL if function-preservation required).

## Open Questions the Paper Calls Out

- **Can progressive training efficiently scale both width and depth simultaneously?**
  - The paper expects future work to explore scaling both dimensions, noting that zero/one-layer expansion demonstrates specific efficiency gains but leaves interaction with width expansion unexplored.

- **Do the convergence guarantees hold for non-convex loss landscapes?**
  - The theoretical framework relies on convex optimization assumptions, explicitly noting that deep learning is non-convex despite similar dynamics, leaving the applicability of the convex loss bounds to large-scale non-convex settings as an open question.

- **What is the optimal strategy for handling optimizer states during depth expansion?**
  - While the paper finds that copying, inheriting, or resetting optimizer states all seem to work, prior works show mixed results and copying is less stable, without establishing a theoretically optimal method for transferring momentum/variance data to new layers.

## Limitations
- Theoretical framework relies on convex optimization assumptions that may not fully capture deep network dynamics
- Method effectiveness depends critically on WSD schedule and muP scaling, which may not transfer seamlessly to all architectures
- 5× speedup claim is specific to studied configurations and may vary significantly with different model sizes, datasets, or training objectives

## Confidence

- **High Confidence**: Compute savings claims (80% reduction, 5× speedup) supported by direct experimental measurements and Pareto frontier analysis in Figure 8.
- **Medium Confidence**: WSD schedule preservation of mixing time during stable phase. While theoretical derivation and empirical measurements support this, the exact robustness window (τ≤0.8T) may depend on specific implementation details.
- **Low Confidence**: muP initialization requirements and hyperparameter transfer across vastly different model scales. The paper assumes muP scaling applies uniformly from 39M to 7B parameters, but this extreme scale gap lacks corpus validation.

## Next Checks

1. **Mixing Time Robustness**: Run controlled experiments varying expansion timing τ across 0.5T to 0.9T under WSD schedule, measuring not just final loss but also intermediate mixing curves to validate the claimed τ≤0.8T window.

2. **Architecture Transfer**: Apply zero/one-layer expansion to architectures beyond the paper's scope (e.g., ViT, LSTM-based models) to test the generality of muP scaling and WSD schedule requirements across different model families.

3. **Scale-Extreme Validation**: Train progressive models from 39M to 10B+ parameters, systematically varying the scale gap to identify where muP hyperparameter transfer breaks down and whether new initialization strategies are needed for extreme scale differences.