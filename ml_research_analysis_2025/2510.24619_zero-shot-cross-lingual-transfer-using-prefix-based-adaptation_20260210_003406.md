---
ver: rpa2
title: Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation
arxiv_id: '2510.24619'
source_url: https://arxiv.org/abs/2510.24619
tags:
- prefix
- tuning
- llama
- language
- lora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates prefix-based adaptation methods\u2014soft\
  \ prompts, prefix tuning, and Llama Adapter\u2014for zero-shot cross-lingual transfer\
  \ in decoder-only LLMs. The authors compare these methods with LoRA across models\
  \ from 1B to 24B parameters on benchmarks including XNLI, XQUAD, Belebele, and MGSM\
  \ in 35+ high- and low-resource languages."
---

# Zero-Shot Cross-Lingual Transfer using Prefix-Based Adaptation

## Quick Facts
- **arXiv ID:** 2510.24619
- **Source URL:** https://arxiv.org/abs/2510.24619
- **Reference count:** 12
- **Primary result:** Prefix tuning and Llama Adapter outperform LoRA for zero-shot cross-lingual transfer, achieving up to 6% higher accuracy on XNLI and 13% on XQUAD with fewer parameters.

## Executive Summary
This paper evaluates prefix-based adaptation methods—soft prompts, prefix tuning, and Llama Adapter—for zero-shot cross-lingual transfer in decoder-only LLMs. The authors compare these methods with LoRA across models from 1B to 24B parameters on benchmarks including XNLI, XQUAD, Belebele, and MGSM in 35+ high- and low-resource languages. Prefix tuning and Llama Adapter consistently outperform LoRA, with prefix tuning achieving up to 6% higher accuracy on XNLI and 13% on XQUAD with Llama 3.1 8B. Performance gains are consistent across language families and scripts, with improvements of up to 37% on low-resource languages in Belebele. Prefix-based methods require only 1.23M trainable parameters yet match or exceed full fine-tuning performance, while avoiding catastrophic forgetting seen with LoRA.

## Method Summary
The paper evaluates three prefix-based parameter-efficient fine-tuning methods (soft prompt tuning, prefix tuning, and Llama Adapter) against LoRA for zero-shot cross-lingual transfer. Models are fine-tuned on English-only data from SQuAD, MNLI, and GSM8K, then evaluated zero-shot on XNLI, XQUAD, Belebele, and MGSM across 35+ languages. Prefix tuning adds 10 learnable prefix tokens to attention keys and values across 30 layers (1.23M parameters), while Llama Adapter uses zero-initialized gating to stabilize training. All methods freeze base model weights to preserve multilingual capabilities, with evaluation metrics including accuracy, F1, and exact match scores.

## Key Results
- Prefix tuning and Llama Adapter outperform LoRA on XNLI and XQUAD benchmarks across all evaluated models
- Prefix tuning achieves up to 6% higher accuracy on XNLI and 13% on XQUAD with Llama 3.1 8B
- Prefix-based methods require only 1.23M trainable parameters while matching or exceeding full fine-tuning performance
- Performance improvements are consistent across language families and scripts, with gains up to 37% on low-resource languages in Belebele

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Prefix-based methods outperform LoRA for zero-shot cross-lingual transfer because they preserve the pretrained model's multilingual representations while steering task behavior.
- **Mechanism:** Prefix tuning injects learnable key-value pairs at multiple transformer layers. These prefix tokens serve as additional context vectors in attention computation: the query attends to both learned prefixes and actual input, producing attention distributions that blend prefix influence with input content. This modifies attention behavior without touching base weights.
- **Core assumption:** The multilingual LLM has already acquired transferable language-agnostic representations during pretraining that prefix tuning can selectively activate.
- **Evidence anchors:** "prefix-based techniques...are less explored, especially for zero-shot transfer in decoder-only models"; "By adding context vectors while keeping the base model frozen, these methods preserve the LLM's inherent multilingual capabilities"; Limited direct corpus support; related work on language-specific neurons suggests neurons don't facilitate transfer, which aligns with prefix methods avoiding neuron-level modification.
- **Break condition:** If source language training data contains language-specific patterns that don't generalize (e.g., English-specific idioms), prefix may overfit to source-language surface features rather than task logic.

### Mechanism 2
- **Claim:** Full fine-tuning on English degrades cross-lingual performance because it causes catastrophic forgetting of multilingual knowledge.
- **Mechanism:** Direct weight updates during fine-tuning shift model parameters toward English-specific task optimization. Since the base model's multilingual capabilities emerge from distributed weight configurations, modifying these weights degrades representations for non-English languages that relied on the original parameterization.
- **Core assumption:** Multilingual knowledge in decoder-only LLMs is fragile and distributed across weights, not modularly isolated.
- **Evidence anchors:** "Full fine-tuning degrades target language performance due to catastrophic forgetting"; Full fine-tuning achieves 37.74 F1 on XQUAD vs. 78.11 for prefix tuning with Llama 3.1 8B; Franken-Adapter similarly argues for modular adaptation to avoid interfering with multilingual capabilities.
- **Break condition:** If full fine-tuning were regularized to explicitly preserve multilingual activations (e.g., via Elastic Weight Consolidation on language-specific weights), degradation might be mitigated—this paper did not explore this.

### Mechanism 3
- **Claim:** Zero-initialized gating (Llama Adapter) stabilizes prefix training by preventing random prefix values from disrupting attention patterns early in training.
- **Mechanism:** Llama Adapter initializes prefix influence to zero via a learnable gating scalar. The gate gradually scales prefix contributions during optimization, allowing the model to integrate prefix signals smoothly rather than abruptly perturbing attention distributions.
- **Core assumption:** Random prefix initialization causes early-training instability that harms convergence on cross-lingual tasks.
- **Evidence anchors:** "replaces the standard attention mechanism with a zero-initialized variant...mitigates instabilities that often arise from randomly initialized prefix tokens"; Llama Adapter matches or exceeds prefix tuning on XNLI, XQUAD, and MGSM; No direct corpus validation of zero-initialization specifically for multilingual transfer.
- **Break condition:** If prefix length is very short (e.g., 1-2 tokens), random initialization may not cause sufficient disruption to require zero-init gating; standard prefix tuning may suffice.

## Foundational Learning

- **Concept: Attention Key-Value Separation**
  - **Why needed here:** Prefix tuning operates by injecting learned keys and values into attention, not just prepending tokens to input. Understanding that K and V projections can be manipulated independently of the input sequence is essential.
  - **Quick check question:** In standard self-attention, are keys and values computed from the same input? Can prefix tuning inject different K/V pairs than what the input sequence would produce?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The central advantage of prefix methods over fine-tuning is avoiding this phenomenon. Knowing what causes forgetting (weight drift from task-specific optimization) clarifies why freezing base weights matters.
  - **Quick check question:** If you fine-tune an LLM on English-only task data, why would its performance on unrelated tasks or languages degrade?

- **Concept: Parameter-Efficient Fine-Tuning (PeFT) Paradigm**
  - **Why needed here:** LoRA, prefix tuning, and soft prompts all share the goal of minimizing trainable parameters. Distinguishing where parameters are added (weight matrices vs. attention layers vs. input embeddings) determines their inductive biases.
  - **Quick check question:** LoRA adds trainable low-rank matrices to projection weights; prefix tuning adds learnable vectors to attention keys/values. Which approach directly modifies the model's computational graph vs. modifying its inputs?

## Architecture Onboarding

- **Component map:**
Input Layer -> [Soft Prompt Tuning: prepend learnable embeddings here] -> Transformer Layer × N -> [Prefix Tuning: concatenate learned P_K, P_V to K, V for layers L to N] -> [Llama Adapter: same as prefix tuning + zero-init gating scalar per layer] -> Multi-Head Attention -> Feed-Forward Network (frozen) -> Output Layer

- **Critical path:**
1. Freeze all base model weights
2. Initialize prefix tokens (random or zero-init with gating)
3. Forward pass: compute prefix K/V projections, concatenate with input K/V
4. Attention uses extended [P_K; K_H] and [P_V; V_H] matrices
5. Backpropagation updates only prefix parameters
6. Inference: prefixes remain attached; no LoRA weight merging needed

- **Design tradeoffs:**
- Prefix length vs. parameters: 10 tokens across 30 layers = 1.23M params (sweet spot per Tables 8-9). Longer prefixes increase capacity but risk overfitting to source language.
- Layer depth: Adapting top layers only (L=30 of 32) performs best—deeper layers encode more task-relevant abstractions, earlier layers retain linguistic features.
- Soft prompts vs. prefix tuning: Soft prompts are simplest (input-only) but underperform on XQUAD (Table 2: 33.6 avg F1 vs. 78.1). Use for extreme parameter constraints; otherwise prefer prefix tuning.

- **Failure signatures:**
- Low-resource languages with complex scripts: MGSM shows degradation for Bengali, Telugu, Swahili (Table 3)—prefix may not transfer reasoning patterns without some target-language exposure.
- Temperature/top-p sensitivity: Higher temperature improves F1 but drops exact match (Figure 3)—indicates diversity-accuracy trade-off in generation tasks.
- LoRA rank scaling doesn't help: LoRA-r128 (75.5M params) still underperforms prefix tuning (1.23M params)—problem isn't capacity, it's weight modification approach.

- **First 3 experiments:**
1. Baseline comparison: Train LoRA (r=4), soft prompts, prefix tuning, and Llama Adapter on English XNLI. Evaluate on 3 high-resource (German, Spanish, French) and 3 low-resource (Swahili, Urdu, Thai) languages. Confirm prefix tuning > LoRA by 3-6% margin.
2. Layer depth ablation: Insert prefixes at [10, 20, 30, 32] layers for Llama 3.1 8B on XQUAD. Expect peak at 30 layers per Table 8.
3. Cross-script stress test: Evaluate prefix-tuned model on Belebele languages with non-Latin scripts (Cyrillic: Russian; Arabic: Sindhi; Ethiopic: Amharic). Compare against LoRA to verify script-agnostic gains shown in Table 4a.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does prefix-based adaptation specifically preserve language-agnostic representations in the deeper layers of the model, thereby preventing the catastrophic forgetting of multilingual capabilities observed in full fine-tuning?
- **Basis in paper:** The authors hypothesize in the Conclusion that the success of prefix tuning stems from "learning language-agnostic behaviors" and explicitly plan to "investigate why prefix-tuning is effective through attention visualization and representation probing."
- **Why unresolved:** While the paper demonstrates superior performance by prefix methods over LoRA and full fine-tuning, it does not provide mechanistic evidence or internal model analysis to support the hypothesis that freezing base weights preserves cross-lingual knowledge representation.
- **What evidence would resolve it:** A probing study (e.g., using centered kernel alignment or linear classifiers) on the hidden states of frozen layers to show that prefix tuning maintains higher language-agnostic representation similarity across languages compared to LoRA or full fine-tuning.

### Open Question 2
- **Question:** How does the zero-shot cross-lingual transfer performance of prefix-based methods change when using a high-resource non-English language (e.g., Chinese or French) or a low-resource language as the source for fine-tuning?
- **Basis in paper:** The paper states in the Limitations section: "our evaluations used only English as the source language. Analyzing other source languages could offer deeper insights into the methods’ cross-lingual capabilities."
- **Why unresolved:** The study establishes a strong baseline for English-centric transfer, but it is unclear if the 6% gain over LoRA holds when the source language has a different script or morphological structure, or if prefix tuning is more robust to low-resource source data.
- **What evidence would resolve it:** A set of experiments fine-tuning on XNLI/SQuAD equivalents in languages like Chinese, Arabic, or Swahili and evaluating the transfer performance across the remaining target languages.

### Open Question 3
- **Question:** Can prefix-based adaptation outperform LoRA in open-ended cross-lingual generation tasks such as summarization or machine translation, where fluency is prioritized over classification accuracy?
- **Basis in paper:** The authors note in the Limitations that they plan to "explore more diverse response generation tasks (e.g. summarization and translation)."
- **Why unresolved:** The current evaluation relies heavily on discriminative tasks (XNLI, Belebele) or span-extraction (XQUAD). It is unknown if the limited parameter count of prefix methods (1.23M) is sufficient to steer the model for complex generative tasks where LoRA's weight modifications might offer an advantage.
- **What evidence would resolve it:** Benchmark results on generative datasets (e.g., XL-Sum or FLORES) comparing prefix tuning against LoRA using metrics like BLEU, ROUGE, and chrF.

### Open Question 4
- **Question:** What specific architectural or training modifications are required to address the performance degradation of prefix methods on complex reasoning tasks (MGSM) in very low-resource languages like Telugu and Swahili?
- **Basis in paper:** The authors observe in Section 5 (Analysis and Ablations) that on the MGSM benchmark, "performance degraded for very low-resource languages like Swahili, Telugu, and Bengali," and they plan to "explore improving response generation for low-resource languages" in future work.
- **Why unresolved:** The paper establishes that while prefix tuning generally outperforms LoRA, it struggles specifically with mathematical reasoning in specific low-resource scripts. The paper does not determine if this is a tokenization issue, a data scarcity issue, or a limitation of the prefix capacity.
- **What evidence would resolve it:** Ablation studies testing hybrid approaches (e.g., LoRA + Prefix) or increased prefix lengths specifically for these low-resource reasoning tasks to identify if the bottleneck is representational capacity or linguistic alignment.

## Limitations
- Evaluation focuses on decoder-only LLMs from a single family (Llama variants), limiting generalizability to other architectures
- Does not explore prefix-based methods on truly low-resource languages where even English training data is limited
- Experiments use relatively short training durations (2 epochs) and fixed hyperparameters that may not be optimal for all language pairs

## Confidence

**High Confidence:**
- Prefix tuning and Llama Adapter outperform LoRA on XNLI and XQUAD benchmarks (supported by direct experimental comparisons across multiple models and language pairs)
- Full fine-tuning degrades cross-lingual performance compared to parameter-efficient methods (consistent degradation observed across multiple tasks and models)
- Prefix tuning achieves superior performance with fewer parameters than LoRA (direct parameter-count comparisons with performance metrics)

**Medium Confidence:**
- Zero-initialized gating in Llama Adapter provides stability benefits (observed performance matches prefix tuning but direct ablation of zero-init vs. random init is not shown)
- Layer depth placement (adapting deeper layers) is optimal (consistent with prior work but not explicitly ablated across all tasks)
- Prefix methods preserve multilingual representations (mechanism is plausible but not directly measured)

**Low Confidence:**
- Performance gains on truly low-resource languages (MGSM shows degradation for several low-resource languages, suggesting limits to transfer)
- Temperature and top-p settings are optimal (tuning was not exhaustive, and Figure 3 shows sensitivity)
- Prefix-based methods will generalize to other LLM families (evaluation limited to Llama models)

## Next Checks

1. **Direct catastrophic forgetting quantification:** Measure multilingual knowledge retention by fine-tuning on English-only data with full fine-tuning, LoRA, and prefix methods, then comparing performance degradation across all languages using embedding similarity metrics or language-specific probes.

2. **Cross-architecture generalization test:** Implement prefix tuning on an encoder-decoder model (e.g., mBERT or mT5) and evaluate zero-shot cross-lingual transfer on XNLI to verify if the performance pattern holds beyond decoder-only LLMs.

3. **Low-resource language stress test:** Evaluate prefix-tuned models on languages with minimal pretraining data (e.g., languages with <1M Wikipedia tokens) and compare against few-shot learning baselines to determine the practical limits of zero-shot transfer.