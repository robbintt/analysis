---
ver: rpa2
title: 'Better Hessians Matter: Studying the Impact of Curvature Approximations in
  Influence Functions'
arxiv_id: '2509.23437'
source_url: https://arxiv.org/abs/2509.23437
tags:
- training
- hessian
- k-fac
- depth
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates how Hessian approximation quality impacts\
  \ influence function-based training data attribution. The authors decompose K-FAC\
  \ into three approximation layers\u2014GGN substitution, block-diagonal approximation,\
  \ and Kronecker factorisation\u2014and evaluate each layer's contribution to attribution\
  \ accuracy."
---

# Better Hessians Matter: Studying the Impact of Curvature Approximations in Influence Functions

## Quick Facts
- arXiv ID: 2509.23437
- Source URL: https://arxiv.org/abs/2509.23437
- Reference count: 40
- Better Hessian approximations yield higher influence function attribution accuracy

## Executive Summary
This work systematically evaluates how different curvature approximation layers affect influence function-based training data attribution. Through controlled experiments on MLPs of varying depth and width, the authors decompose K-FAC into three approximation layers—GGN substitution, block-diagonal approximation, and Kronecker factorization—and quantify each layer's contribution to attribution accuracy. The study reveals that while all approximation layers degrade attribution quality, Kronecker factorization accounts for ~60% of the total error gap, making it the dominant bottleneck in practical implementations.

## Method Summary
The paper evaluates influence function attribution quality across different Hessian approximation methods: Exact Hessian, GGN, Block-Diagonal GGN, EK-FAC, and K-FAC. Experiments use UCI Digits dataset (1,797 samples) with MLPs of varying depths (1, 4, 8) and widths (32, 64, 128), trained with SGD for 10, 100, and 1000 epochs. Attribution quality is measured via Linear Data-modelling Score (LDS) using Spearman correlation between predicted and actual group effects. The authors compute approximation error using relative error of the inverse curvature matrix and decompose error contributions across approximation layers.

## Key Results
- Better Hessian approximations yield higher attribution accuracy (higher LDS scores)
- Kronecker factorization error accounts for ~60% of total approximation error gap
- Cross-layer coupling increases with depth, making block-diagonal approximation increasingly costly
- EK-FAC improves over K-FAC but cannot close the gap due to basis misalignment

## Why This Works (Mechanism)

### Mechanism 1: GGN Substitution as Stable Curvature Proxy
- Claim: Replacing the Hessian with the Generalized Gauss-Newton (GGN) matrix provides a positive-semidefinite curvature approximation that avoids unstable second-derivative terms while preserving most curvature information near convergence.
- Mechanism: The GGN decomposition H(θ) = G(θ) + R(θ) separates output-space curvature (G) from parameter non-linearities (R). Near optima where ∇θJ(θ) ≈ 0, the residual R(θ) becomes negligible irrespective of model curvature. For piecewise-linear activations (ReLU), R(θ) = 0 almost everywhere since second derivatives vanish in regions with stable activation patterns.
- Core assumption: The curvature from output space (GGN term) dominates over parameter-space non-linearities (residual term) during the training regimes where influence functions are applied.
- Evidence anchors:
  - [abstract] "Approximations such as Generalised Gauss-Newton (GGN) and Kronecker-Factored Approximate Curvature (K-FAC) have been proposed to make influence computation tractable"
  - [Section 3.2.1] "For exponential-family likelihoods, G coincides with the Fisher information matrix... when parameters θ yield near-optimal predictions for all training examples, the gradient of the loss with respect to outputs vanishes: gi(θ) ≈ 0 for all i. In such cases, the residual R(θ) becomes negligible"
  - [corpus] Paper "Better Training Data Attribution via Better Inverse Hessian-Vector Products" (arXiv:2507.14740) shows improved attribution through better iHVP approximations, supporting the importance of curvature quality
- Break condition: Early in training when the model is far from optima, the residual R(θ) can be substantial and non-positive-semidefinite, potentially removing useful curvature information beyond mere instability suppression.

### Mechanism 2: Block-Diagonal Decoupling of Cross-Layer Curvature
- Claim: Partitioning the GGN into layer-wise blocks enables tractable inversion while sacrificing cross-layer coupling information that scales with network depth.
- Mechanism: The block-diagonal approximation G ≈ diag(G₁, ..., Gₗ) discards off-diagonal blocks Gᵢⱼ = (1/N)Σₖ Jᵢ(xₖ)ᵀHᵤ⁽ᵏ⁾Jⱼ(xₖ) that quantify Hᵤ-weighted alignment between parameter blocks' output sensitivities. The computational cost drops from O(D³) to O(Σᵢdᵢ³), enabling parallel block-wise inversion.
- Core assumption: Cross-layer coupling is weak enough that discarding off-diagonal blocks preserves the curvature directions most relevant for influence estimation.
- Evidence anchors:
  - [abstract] "The dominant source of error comes from within-block Kronecker factorisation, accounting for ~60% of the total error gap in most settings"
  - [Section 3.2.2] "Cross-layer coupling interpretation: These off–diagonal terms quantify, in an H(u)–weighted inner product, how similarly two parameter blocks move the model's outputs"
  - [Section 4.2] "cross-layer coupling (off-block mass) decreases mildly over training and increases strongly with depth... the LDS–error slope is flatter at late epochs (weaker cross-block terms) and steeper in deeper networks (stronger cross-block terms)"
  - [corpus] Limited direct corpus support for block-diagonal specific analysis in influence functions
- Break condition: Deep networks exhibit substantially larger off-block mass (cross-layer coupling increases monotonically with depth), violating the weak-coupling assumption and accounting for increasing LDS losses per unit error.

### Mechanism 3: Kronecker Factorization with Spectral Correction
- Claim: Factoring each block as Gₗ ≈ Aₗ₋₁ ⊗ Sₗ (assuming independence between activations and pre-activation gradients) enables efficient inversion, with eigenvalue correction (EK-FAC) recovering spectral fidelity but not basis alignment.
- Mechanism: K-FAC assumes Ãₗ = E[ãₗ₋₁ãₗ₋₁ᵀ] and Sₗ = E[DsₗDsₗᵀ] are independent, enabling (A ⊗ S)⁻¹ = A⁻¹ ⊗ S⁻¹. This loses cross-covariance structure—parameters rarely activating also rarely receiving large gradients. EK-FAC corrects per-direction scaling as S* = diag(E[(Uᵀgₗ)²]) but retains K-FAC's Kronecker eigenbasis Uₐ ⊗ Uₛ.
- Core assumption: The activation-gradient independence assumption holds sufficiently that the Kronecker eigenbasis approximates the true curvature eigenspace.
- Evidence anchors:
  - [abstract] "Eigenvalue correction (EK-FAC) improves performance over K-FAC but cannot close the gap to unfactored methods due to basis misalignment"
  - [Section 3.2.3] "These products λᵢᴬλⱼˢ are generally not the true variances of the full block along (Uₐ ⊗ Uₛ)'s directions, leading to systematic curvature misestimation"
  - [Section 4.2] "EK-FAC and K-FAC operate in the same Kronecker eigenbasis (identical basis-overlap curves)... moving from EK-FAC to K-FAC primarily introduces spectral mis-scaling rather than basis error"
  - [corpus] Paper "Bayesian Influence Functions for Hessian-Free Data Attribution" (arXiv:2509.26544) proposes alternative approach avoiding Hessian inversion entirely, suggesting limitations of factorization approaches
- Break condition: As depth increases, the Kronecker eigenbasis diverges from the true block eigenvectors (basis overlap declines), and no diagonal correction in the fixed Kronecker basis can recover the lost off-diagonal curvature mass.

## Foundational Learning

- **Concept: Influence Functions and Inverse Hessian-Vector Products**
  - Why needed here: Influence functions approximate leave-one-out retraining effects using τIF = ∇θm(zq, θ*)ᵀH⁻¹∇θL(zm, θ*), requiring tractable computation of inverse Hessian-vector products for large, ill-conditioned Hessians.
  - Quick check question: Given a model with 1M parameters, why can't we directly compute H⁻¹v, and what two broad approximation regimes address this?

- **Concept: Curvature Matrices (Hessian vs. GGN vs. Fisher)**
  - Why needed here: The paper decomposes approximation error through Hessian → GGN → Block-GGN → K-FAC/EK-FAC, requiring understanding of what curvature information each matrix captures and discards.
  - Quick check question: For a network with ReLU activations at a local minimum, what is the relationship between the Hessian H and the GGN G, and why might they still differ during training?

- **Concept: Kronecker Product Algebra and Eigenstructure**
  - Why needed here: K-FAC exploits (A ⊗ S)⁻¹ = A⁻¹ ⊗ S⁻¹ for efficient inversion, and understanding why EK-FAC corrects eigenvalues but not eigenvectors requires knowing that Kronecker products have eigenstructure (Uₐ ⊗ Uₛ)(Λₐ ⊗ Λₛ)(Uₐ ⊗ Uₛ)ᵀ.
  - Quick check question: If A has eigenvectors Uₐ and S has eigenvectors Uₛ, what are the eigenvectors of A ⊗ S, and why can't eigenvalue correction fix misaligned eigenvectors?

## Architecture Onboarding

- **Component map:** Exact Hessian H (full D×D, intractable) → GGN G (positive semidefinite, linearized) → Block-diagonal GGN (layer-wise blocks, cross-layer coupling discarded) → EK-FAC/K-FAC blocks (Aₗ₋₁ ⊗ Sₗ factorization with/without eigenvalue correction)

- **Critical path:** Training duration → GGN quality improves (residual R decreases) → Block-diagonality holds better (cross-layer coupling decreases) → Kronecker basis alignment improves → Overall approximation error ↓, LDS ↑. Depth → Cross-layer coupling ↑, basis alignment ↓ → Error concentrated in block-diagonal and factorization steps.

- **Design tradeoffs:**
  - Computational cost: Hessian (intractable) > GGN (O(D³)) > Block-GGN (O(Σᵢdᵢ³)) > EK-FAC ≈ K-FAC (O(Σᵢ(dᵢ² + per-block eigendecomposition)))
  - Attribution fidelity: Hessian ≳ GGN > Block-GGN > EK-FAC > K-FAC
  - Error concentration: ~60% from Kronecker factorization, ~10-25% from block-diagonalization (increases with depth), ~0-4% from GGN substitution (decreases with training)
  - Practical recommendation per paper: EK-FAC for most settings; Block-GGN when depth is large and attribution accuracy is critical

- **Failure signatures:**
  - Early training: Large LDS drop from Hessian→GGN (≈72% at epoch 10) despite small error share (≈3.5%)
  - Deep networks: GGN→Block-GGN LDS penalty increases as cross-layer mass grows
  - Any depth: K-FAC→EK-FAC gap persists because both share Kronecker eigenbasis that diverges from true curvature eigenvectors
  - Numerical instability: Pseudo-inverse threshold ε = 10⁻⁴ with no damping may discard small-magnitude curvature modes

- **First 3 experiments:**
  1. **Baseline error decomposition**: For your target architecture, compute the incremental error shares (Hessian→GGN→Block-GGN→EK-FAC→K-FAC) using Eq. 9's approximation error metric. Identify which layer dominates (>50% indicates factorization is primary bottleneck; >30% in block-diagonal step indicates depth-related coupling).
  2. **Training duration sweep**: Track LDS and approximation error at epochs {10, 100, 1000} (or equivalents for your training timeline). Expect methods to cluster near convergence with diminishing marginal LDS gains from additional curvature fidelity.
  3. **Numerical stability ablation**: Sweep pseudo-inverse threshold ε ∈ [10⁻⁸, 10⁻²] and damping λ ∈ [10⁻⁸, 10⁻¹]. Log solver pathologies (non-convergence, extreme IHVP norms) to separate numerical effects from approximation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the dominance of within-block Kronecker factorisation error persist in architectures with weight sharing, such as CNNs and Transformers?
- Basis in paper: [explicit] The authors note the evaluation excludes CNNs and transformers, whose curvature structure differs due to weight sharing and attention mechanisms.
- Why unresolved: The study restricts experiments to multi-layer perceptrons (MLPs) on a small dataset to keep the retraining protocol computationally tractable.
- What evidence would resolve it: Replicating the error decomposition analysis on convolutional or attention-based models to compare the relative error shares of factorization versus block-diagonalization.

### Open Question 2
- Question: How does the choice of numerical regularisation—specifically Tikhonov damping versus eigenvalue truncation thresholds—alter the observed relationship between approximation error and influence fidelity?
- Basis in paper: [explicit] The authors state they did not ablate the truncation threshold ε or compare against pure Tikhonov damping, leaving sensitivity to these controls unknown.
- Why unresolved: The study relied on a fixed pseudo-inverse approach with a hard threshold and no damping to ensure fair comparison between methods, precluding an analysis of regularisation effects.
- What evidence would resolve it: A systematic ablation sweeping damping λ and threshold ε values to measure their impact on Linear Data-modelling Score (LDS) and approximation error.

### Open Question 3
- Question: Do the method ordering and error decomposition findings generalise to large-scale settings, such as Large Language Models (LLMs)?
- Basis in paper: [explicit] The authors admit results may not transfer to larger models due to the restricted depth, width, and dataset size used in the experiments.
- Why unresolved: Very wide networks may operate closer to the Neural Tangent Kernel (NTK) regime where residual curvature is smaller, potentially changing the relative benefits of different approximations.
- What evidence would resolve it: Applying the same error decomposition methodology to a large-scale model to verify if the inverse relationship between curvature error and attribution quality holds.

## Limitations

- The study's restriction to MLPs on UCI Digits limits generalizability to modern architectures like CNNs and Transformers.
- The exclusive use of eigenvalue thresholding (ε=10⁻⁴) without Tikhonov damping prevents separation of numerical stabilization effects from approximation quality.
- Claims about GGN substitution preserving curvature information lack rigorous bounds on residual magnitude, particularly for non-exponential-family losses or early training phases.

## Confidence

- **High Confidence**: The inverse relationship between curvature approximation error and LDS scores is well-established through controlled experiments across multiple dimensions (training duration, depth, width).
- **Medium Confidence**: The decomposition of error contributions (60% from Kronecker factorization, 10-25% from block-diagonalization, 0-4% from GGN substitution) relies on the specific approximation error metric and may not generalize to all architectures or datasets.
- **Low Confidence**: The claim that "GGN substitution preserves most curvature information" lacks rigorous bounds on residual magnitude, particularly for non-exponential-family likelihoods or early training phases.

## Next Checks

1. **Numerical Stability Ablation**: Systematically vary ε ∈ [10⁻⁸, 10⁻²] and λ ∈ [10⁻⁸, 10⁻¹] across all methods to separate numerical effects from approximation quality. Log solver convergence failures and IHVP norm stability.

2. **Loss Function Diversity**: Test the GGN mechanism across non-exponential-family losses (e.g., contrastive losses, adversarial objectives) to verify the assumption that residual R becomes negligible near convergence holds broadly.

3. **Architecture Generalization**: Apply the error decomposition framework to convolutional and transformer architectures to validate whether the 60%/10-25%/0-4% error distribution holds or varies significantly with architectural inductive biases.