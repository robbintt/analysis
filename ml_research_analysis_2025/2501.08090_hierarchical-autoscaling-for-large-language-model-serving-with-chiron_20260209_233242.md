---
ver: rpa2
title: Hierarchical Autoscaling for Large Language Model Serving with Chiron
arxiv_id: '2501.08090'
source_url: https://arxiv.org/abs/2501.08090
tags:
- batch
- requests
- serving
- chiron
- request
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Chiron introduces hierarchical autoscaling for large language
  model serving, addressing inefficiencies in previous systems that don''t consider
  service-level objectives (SLOs). It employs local and global autoscalers: the local
  autoscaler dynamically adjusts batch sizes based on latency and throughput metrics,
  while the global autoscaler scales instances using request queuing and multiplexing
  strategies.'
---

# Hierarchical Autoscaling for Large Language Model Serving with Chiron

## Quick Facts
- arXiv ID: 2501.08090
- Source URL: https://arxiv.org/abs/2501.08090
- Reference count: 18
- Primary result: Hierarchical autoscaling achieves up to 90% higher SLO attainment and 70% better GPU efficiency for LLM serving

## Executive Summary
Chiron introduces hierarchical autoscaling for large language model serving, addressing inefficiencies in previous systems that don't consider service-level objectives (SLOs). It employs local and global autoscalers: the local autoscaler dynamically adjusts batch sizes based on latency and throughput metrics, while the global autoscaler scales instances using request queuing and multiplexing strategies. Experiments show Chiron achieves up to 90% higher SLO attainment and improves GPU efficiency by up to 70% compared to existing solutions, demonstrating significant improvements in resource utilization and request throughput.

## Method Summary
Chiron implements a two-tier autoscaling system for LLM serving with mixed interactive and batch workloads. The local autoscaler per instance dynamically adjusts maximum batch size using latency-based and throughput-based backpressure metrics. The global autoscaler manages instance scaling through request queuing and multiplexing, maintaining a global queue for batch requests and scaling instances based on estimated queue waiting time. The system uses mixed instances that can serve both interactive and batch requests, enabling opportunistic use of over-provisioned capacity. The backend uses vLLM with Llama 3.1 8B/70B models on NVIDIA A100 GPUs, handling requests classified as either interactive (tight SLOs) or batch (relaxed SLOs).

## Key Results
- Achieves up to 90% higher SLO attainment compared to baselines
- Improves GPU efficiency by up to 70% through better resource utilization
- Maintains stable performance under varying arrival rates and burstiness

## Why This Works (Mechanism)

### Mechanism 1: Local Autoscaling with Adaptive Batch Sizes
The local autoscaler monitors latency-based backpressure (LBP = observed ITL / ITL SLO) and throughput-based backpressure (TBP = previous throughput / current throughput). If combined local backpressure exceeds 1, batch size is halved to reduce ITL and avoid SLO violations. If backpressure is below 1, batch size increases proportionally using EWMA to maximize throughput without violating SLOs. Core assumption: Inter-token latency and throughput vary predictably with batch size, allowing reactive control loop to converge to optimal batch size without offline profiling.

### Mechanism 2: Global Autoscaling with Request Queuing and Multiplexing
Batch requests are queued and assigned to request groups with similar TTFT SLOs. The global autoscaler estimates waiting time for each group using statistical averaging of output token distributions and continuous batching throughput. Instances are added only when waiting time is projected to exceed TTFT SLO (Batch Backpressure, BBP), allowing batch requests to multiplex on over-provisioned capacity before scaling. Core assumption: Continuous batching leads to stable, predictable token generation throughput; output token lengths can be modeled as normal distribution for accurate waiting time estimation.

### Mechanism 3: Mixed Instances for Multiplexing and Over-Provisioning
Dedicated mixed instances serve both interactive and batch requests, enabling opportunistic use of over-provisioned capacity. Interactive instances are over-provisioned to handle arrival spikes. Mixed instances absorb these spikes for interactive requests while processing batch requests during idle periods. If interactive requests arrive, batch requests on mixed instances are preempted and their KV cache is migrated to CPU memory for fast restart. Core assumption: Over-provisioning is necessary due to long model load times; preemption overhead is low enough to not negate throughput gains.

## Foundational Learning

Concept: PagedAttention and KV Cache Management
- Why needed here: Chiron relies on dynamic KV cache allocation to enable preemption and fast restart on mixed instances
- Quick check question: How does PagedAttention reduce memory fragmentation compared to static KV cache allocation?

Concept: Continuous Batching and Iterative Scheduling
- Why needed here: Global autoscaler's waiting time estimation assumes stable throughput from continuous batching
- Quick check question: Why does continuous batching help amortize the memory-bound decoding phase in LLM inference?

Concept: Service Level Objectives (TTFT and ITL)
- Why needed here: Chiron's autoscalers are explicitly driven by TTFT and ITL SLOs
- Quick check question: What is the difference between Time to First Token (TTFT) and Inter-Token Latency (ITL), and which one is more critical for streaming chat applications?

## Architecture Onboarding

Component map: Request arrival -> Global Queue -> Request Classifier (Interactive/Batch) -> Interactive Instances / Batch Instances / Mixed Instances -> Local Autoscaler (per instance) -> Global Autoscaler (cluster-level)

Critical path: Interactive request arrival -> routed to interactive or mixed instance -> local autoscaler adjusts batch size to meet ITL SLO -> if mixed instance is preempted, batch request is re-queued with KV cache preserved. Batch request arrival -> queued -> waiting time estimated -> if BBP > 0, batch instances added; otherwise, multiplexed on mixed instances.

Design tradeoffs:
- Over-provisioning level (Θ) vs. GPU cost: Higher Θ handles burstier interactive traffic but wastes capacity if batch demand is low
- Preemption vs. throughput: Frequent preemptions save interactive SLOs but may reduce batch throughput due to restart overhead
- Online batch size adaptation vs. stability: Reactive autoscaling converges without profiling but may oscillate if metrics are noisy

Failure signatures:
- SLO violations for interactive requests under high burstiness: Indicates Θ is set too low or mixed instances are saturated
- Stuck batch queue with no scaling: BBP estimation may be too conservative due to inaccurate output token distribution modeling
- High hysteresis (frequent scaling up/down): Request group granularity may be too fine; grouping parameters need tuning

First 3 experiments:
1. Vary interactive arrival rate (Poisson) and measure ITL SLO attainment vs. Llumnix baseline to validate local autoscaler
2. Introduce large batch queue (e.g., 1M requests) and compare GPU-hours consumed and TTFT SLO attainment between Chiron and Llumnix to validate global autoscaler and multiplexing
3. Increase arrival burstiness (Gamma distribution, varying CV) and observe when over-provisioning level (Θ) becomes insufficient to stress-test mixed instance preemption mechanism

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can queue waiting time estimation be refined for small queue sizes where statistical averaging assumptions fail?
- Basis in paper: Section 6.3 states that "when request queues are small, statistical averaging effects of continuous batching do not hold," leading to conservative estimates and reduced accuracy
- Why unresolved: Current estimator relies on Central Limit Theorem, which requires sufficient requests (approx. 2000) to maintain high accuracy (R² > 0.99)
- What evidence would resolve it: Modified estimation algorithm maintaining high R² values (>0.9) even when queue depth is orders of magnitude smaller than convergence threshold

### Open Question 2
- Question: What is the performance overhead of the KV cache migration mechanism during request eviction in mixed instances?
- Basis in paper: Section 3 mentions utilizing "fast restart by saving the KV cache by migrating it to CPU memory" to prevent throughput drops, but does not quantify migration latency cost
- Why unresolved: While mechanism prevents re-computation, paper does not measure PCIe bandwidth contention or latency incurred by moving large KV caches to CPU memory during preemption
- What evidence would resolve it: Breakdown of eviction latency comparing re-computation time vs. migration time for various model sizes (e.g., Llama 70B)

### Open Question 3
- Question: How does Chiron's performance compare to fully elastic autoscaling when serving models with fast loading times?
- Basis in paper: Appendix A.1 notes that "If model loading times are below the TTFT SLO... the need for over-provisioning and the global autoscaler reduces," suggesting current hierarchical approach may be sub-optimal for small models
- Why unresolved: Paper focuses on large models (Llama 8B/70B) with long load times; does not evaluate if complexity of hierarchical backpressure is necessary for models that can be loaded on demand
- What evidence would resolve it: Comparative analysis of Chiron vs. standard reactive autoscaling on models with load times < 3 seconds

## Limitations
- Hierarchical approach adds complexity that may be unnecessary for small models with fast loading times
- Statistical waiting time estimation accuracy degrades significantly for small request queues (<2000 requests)
- KV cache migration overhead during preemption is not quantified and could impact throughput at high preemption frequencies

## Confidence

High Confidence: The core observation that hierarchical autoscaling improves SLO attainment compared to flat scaling approaches is well-supported by experimental results. Separating interactive and batch request handling based on different SLO requirements is sound and aligns with established principles in QoS-aware systems.

Medium Confidence: Specific backpressure calculations and thresholds (LBP/TBP combined > 1 triggers scaling actions) appear empirically derived but lack sensitivity analysis. Statistical waiting time estimation method is theoretically sound but practical accuracy depends heavily on quality of historical token distribution data.

Low Confidence: Scalability to extremely large clusters (>50 GPUs) is not demonstrated. Paper does not address potential cascading failures where global scaling decisions based on queue estimates could lead to over-provisioning across many instances. Interaction between local and global autoscalers during rapid workload transitions is not thoroughly explored.

## Next Checks

1. **Sensitivity Analysis of Backpressure Thresholds:** Systematically vary LBP/TBP threshold values (currently >1 triggers scaling actions) across multiple hardware configurations and model sizes to determine if proposed values are optimal or merely sufficient for tested scenarios.

2. **Queue Size Impact on Waiting Time Estimation:** Conduct experiments with varying queue depths (particularly <1000 requests where CLT assumptions break down) to measure accuracy gap between estimated and actual waiting times, and quantify resulting SLO violations.

3. **Preemption Overhead Characterization:** Measure actual latency and throughput impact of KV cache migration during mixed instance preemptions under different preemption frequencies, establishing operational limits beyond which multiplexing benefit is negated.