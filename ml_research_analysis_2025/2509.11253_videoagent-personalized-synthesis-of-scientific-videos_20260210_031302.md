---
ver: rpa2
title: 'VideoAgent: Personalized Synthesis of Scientific Videos'
arxiv_id: '2509.11253'
source_url: https://arxiv.org/abs/2509.11253
tags:
- video
- content
- scientific
- quality
- static
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: VideoAgent addresses the challenge of automating scientific video
  generation by proposing a multi-agent framework that orchestrates both static slides
  and dynamic animations through a conversational interface. The system parses source
  papers into fine-grained multimodal assets and uses a planner to determine when
  to employ static slides or dynamic animations based on user requirements.
---

# VideoAgent: Personalized Synthesis of Scientific Videos

## Quick Facts
- arXiv ID: 2509.11253
- Source URL: https://arxiv.org/abs/2509.11253
- Authors: Xiao Liang; Bangxin Li; Zixuan Chen; Hanyue Zheng; Zhi Ma; Di Wang; Cong Tian; Quan Wang
- Reference count: 0
- Primary result: VideoAgent achieves near-human knowledge transfer (87.5% human eval) and outperforms commercial services on narration (PPL: 18.08), visual (VLM-as-Judge: 8.03), and synchronization (VLM-as-Judge: 6.58) quality.

## Executive Summary
VideoAgent introduces a multi-agent framework for generating personalized scientific videos from academic papers, combining static slides and dynamic animations with synchronized narration. The system parses source papers into structured multimodal assets, uses a planner to determine content-appropriate visual formats, and employs audio-duration-driven synchronization for seamless audio-visual alignment. Evaluated on 50 AI conference papers, VideoAgent variants significantly outperform commercial services across narration, visual, and synchronization quality metrics, while achieving near-human knowledge transfer effectiveness.

## Method Summary
VideoAgent operates through a four-stage pipeline: (1) Document Parser extracts text and visuals from PDFs using Docling and Marker, creating a JSON asset library; (2) Requirement Analyzer conducts dialogue to generate JSON configuration; (3) Personalized Planner creates storyboards and generates python-pptx slides or python-manim animations via code generation; (4) Multimodal Synthesizer generates narration via TTS and composites all elements using MoviePy with audio-duration-driven timing. The framework is evaluated using SciVidEval, combining automated metrics (narration, visual, synchronization) with human evaluation on knowledge transfer.

## Key Results
- Narration quality: VideoAgent variants achieve perplexity of 18.08 and Rouge-L of 0.16
- Visual quality: VLM-as-Judge scores reach 8.03 for visual assessment
- Knowledge transfer: Near-human performance with 99.5% automated and 87.5% human evaluation accuracy
- Synchronization: VLM-as-Judge synchronization score of 6.58, showing effective audio-visual alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured multimodal asset decomposition enables accurate content retrieval and reduces downstream processing costs.
- Mechanism: Document Parser uses Docling for PDF parsing, Marker for Markdown conversion, and summarization agent for semantic compression, storing assets in JSON with textual descriptions for visual elements.
- Core assumption: Semantic compression preserves sufficient information for downstream video generation without losing critical scientific content.
- Evidence anchors: Abstract states "fine-grained asset library"; section 2.1 describes JSON storage preserving global semantics while reducing token cost.

### Mechanism 2
- Claim: Dual-path visual generation (static slides + dynamic animations) enables content-appropriate format selection.
- Mechanism: Personalized Planner evaluates each chapter to decide between python-pptx static slides or python-manim dynamic animations based on content complexity.
- Core assumption: LLMs can reliably determine which content benefits from animation vs. static presentation.
- Evidence anchors: Abstract mentions "orchestrates a narrative flow that synthesizes both static slides and dynamic animations"; section 2.3 describes dynamic animation synthesis with detailed JSON plans.

### Mechanism 3
- Claim: Audio-duration-driven visual timing ensures audio-visual synchronization without manual alignment.
- Mechanism: Multimodal Synthesizer generates narration first via TTS, then uses audio clip duration to determine slide display time or animation playback speed through MoviePy compositing.
- Core assumption: TTS-generated narration timing is stable and predictable enough to serve as the synchronization anchor.
- Evidence anchors: Abstract mentions "multimodal content synchronization"; section 2.4 describes duration-driven timing for slides and animations.

## Foundational Learning

- Concept: **Manim programmatic animation**
  - Why needed here: Dynamic animations are generated via python-manim code; understanding Manim's coordinate system, scene composition, and timing primitives is essential for debugging animation generation.
  - Quick check question: Can you write a Manim scene that animates a rectangle moving across the screen while text fades in?

- Concept: **VLM-as-Judge evaluation paradigm**
  - Why needed here: SciVidEval uses GPT-4o as a VLM judge for visual quality, synchronization, and knowledge transfer; understanding prompt design and scoring biases is critical for interpreting results.
  - Quick check question: What are two failure modes when using a VLM to score visual-semantic alignment?

- Concept: **Document parsing pipelines (PDF → structured JSON)**
  - Why needed here: The system depends on accurate extraction of figures, tables, equations, and text relationships from PDFs; parsing errors cascade through the entire pipeline.
  - Quick check question: What information is lost when converting a two-column academic PDF with floating figures to linear Markdown?

## Architecture Onboarding

- Component map: Document Parser (Docling → Marker → JSON) → Requirement Analyzer (LLM dialogue → JSON config) → Personalized Planner (Storyboard → python-pptx + python-manim) → Multimodal Synthesizer (TTS → MoviePy compositing)

- Critical path: Paper PDF → Asset Library (JSON) → User Requirements (JSON config) → Storyboard → Generated Code → Video Assembly. The Planner is the orchestration bottleneck; failures here affect all downstream outputs.

- Design tradeoffs:
  - Gemini-2.5 Pro vs. GPT-4o vs. Qwen-2.5VL: Table 1 shows Gemini-2.5 Pro achieves best visual quality (8.03) and synchronization (6.58); Qwen-2.5VL has lowest synchronization score (2.87)
  - Static vs. dynamic generation: Animations require more compute and code generation complexity but improve concept explanation for algorithmic content
  - Automated vs. human evaluation: Human evaluation limited to 10 papers (subset); automated VLM-as-Judge scales but may not capture nuanced knowledge transfer failures

- Failure signatures:
  - Low Asset Match Accuracy: Indicates document parser failed to correctly extract/associate figures and tables
  - High Perplexity narration: Suggests summarization agent produced non-fluent text or requirement analyzer misconfigured style parameters
  - Low CLIP-Score/VLM synchronization: Indicates audio-visual timing misalignment, likely from TTS variability or incorrect duration mapping
  - Animation code execution errors: Manim code generation failures, often from overly complex scene plans

- First 3 experiments:
  1. Run VideoAgent end-to-end on a single CVPR paper from the benchmark; inspect the JSON asset library, storyboard, and generated code files at each stage to trace the transformation pipeline
  2. Ablate the dynamic animation component by forcing all content to static slides; compare VLM-as-Judge scores and human evaluation on knowledge transfer for algorithm-heavy papers
  3. Substitute the LLM backend (GPT-4o → Gemini-2.5 Pro) while holding all other components constant; quantify the delta in visual quality, synchronization, and knowledge transfer scores using Table 1 metrics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can robust, automated metrics be developed to specifically evaluate the semantic fidelity and aesthetic quality of synthesized dynamic animations?
- Basis in paper: [explicit] The authors state in the conclusion: "For future work, developing robust metrics to evaluate the quality of synthesized videos, especially dynamic animations, remains an important area of investigation."
- Why unresolved: Current evaluation relies heavily on VLM-as-Judge scores for static frames or general synchronization, lacking granular metrics to verify if a dynamic animation accurately represents the underlying algorithm or mathematical concept.
- What evidence would resolve it: Creation of a benchmark sub-suite containing ground-truth animation logic paired with automated tools capable of verifying temporal semantic consistency in motion graphics.

### Open Question 2
- Question: To what extent does the VideoAgent framework generalize to scientific domains outside of Artificial Intelligence, such as theoretical physics or organic chemistry?
- Basis in paper: [inferred] The paper restricts its data collection and evaluation solely to "AI research papers" from conferences like CVPR and NeurIPS, while claiming to solve "scientific video synthesis" broadly.
- Why unresolved: The performance of the Document Parser (using Docling/Marker) and the animation code generation may vary significantly when parsing non-standard chemical notation, complex physics diagrams, or mathematical proofs not common in AI literature.
- What evidence would resolve it: Experimental results applying VideoAgent to papers from non-CS domains (e.g., Nature Chemistry) with a corresponding analysis of parsing errors and animation hallucination rates.

### Open Question 3
- Question: How can the framework mitigate factual hallucinations in dynamic animations when the underlying code generation model lacks sufficient reasoning capabilities?
- Basis in paper: [inferred] The paper notes that "the factual and aesthetic quality of its dynamic animations relies on the underlying large models," and the pipeline translates abstract plans into executable code without an intermediate verification step.
- Why unresolved: While the system can generate code to animate a concept, there is no explicit mechanism mentioned to formally verify that the generated animation accurately reflects the source paper's logic before video synthesis.
- What evidence would resolve it: An ablation study or modified pipeline introducing a "code verifier" agent, showing a reduction in semantic errors within the generated animations compared to the baseline.

## Limitations

- Human evaluation limited to only 10 papers (20%), making statistical significance uncertain for knowledge transfer claims
- Heavy reliance on LLM code generation introduces brittleness - failures in manim animation generation can cascade through the pipeline
- Benchmark dataset of 50 papers limits generalizability across diverse scientific domains beyond AI literature

## Confidence

- **High Confidence**: Document parsing and asset extraction pipeline demonstrates robust performance with quantitative metrics provided
- **Medium Confidence**: Dual-path visual generation mechanism shows promising results but depends heavily on LLM judgment quality
- **Low Confidence**: Audio-duration synchronization approach shows weakest automated evaluation scores, suggesting timing alignment remains challenging

## Next Checks

1. **Ablation Study on Dynamic Content Selection**: Systematically vary the percentage of content forced to static slides versus dynamic animations across different paper types and measure impact on knowledge transfer scores and visual quality metrics

2. **TTS Timing Robustness Analysis**: Test the synchronization mechanism across multiple TTS providers and voice speeds to quantify the impact of audio duration variability on audio-visual alignment quality

3. **Cross-Domain Generalization Test**: Evaluate VideoAgent on papers from non-CV domains (biology, physics, social sciences) to assess whether the content classification and animation generation mechanisms generalize beyond computer vision literature