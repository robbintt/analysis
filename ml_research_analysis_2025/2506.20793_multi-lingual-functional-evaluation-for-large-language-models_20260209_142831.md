---
ver: rpa2
title: Multi-lingual Functional Evaluation for Large Language Models
arxiv_id: '2506.20793'
source_url: https://arxiv.org/abs/2506.20793
tags:
- languages
- performance
- language
- across
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors created cross-lingual functional benchmarks (CL-IFEval\
  \ and CL-GSMSym) by translating English instruction-following and symbolic math\
  \ evaluation templates into five additional languages. They found that functional\
  \ benchmarks revealed much larger performance gaps between languages compared to\
  \ static data benchmarks\u2014up to 34% difference in instruction-following tasks\u2014\
  while static benchmarks showed gaps under 5%."
---

# Multi-lingual Functional Evaluation for Large Language Models

## Quick Facts
- arXiv ID: 2506.20793
- Source URL: https://arxiv.org/abs/2506.20793
- Reference count: 15
- Primary result: Functional benchmarks revealed up to 34% larger performance gaps between languages compared to static benchmarks

## Executive Summary
This paper introduces cross-lingual functional benchmarks (CL-IFEval and CL-GSMSym) by translating English instruction-following and symbolic math templates into five additional languages. The study finds that functional benchmarks—which use templated generation with variables and distractors—expose much larger performance gaps between languages (up to 34%) compared to static benchmarks (under 5%). Model rankings also shift significantly between static and functional evaluations, with certain models showing consistent performance across languages while others exhibit substantial inconsistency. The findings suggest that static benchmarks may overestimate model capabilities, particularly for low-resource languages.

## Method Summary
The authors created cross-lingual functional benchmarks by translating English IFEval prompts (541 items) and GSM-Symbolic templates (5000 items from 100 templates) into French, Spanish, Hindi, Arabic, and Yoruba using Google Translate with spot-checking. They evaluated multiple open-weight models plus GPT-4o-mini and Claude Sonnet 3.5 on both functional benchmarks and static benchmarks (M-MMLU, MGSM, Belebele). CL-IFEval uses strict/loose accuracy at prompt and instruction levels, while CL-GSMSym uses 8-shot accuracy with language-specific chain-of-thought prompting. The key innovation is using templated generation with variables and distractors rather than fixed input-output pairs.

## Key Results
- Performance gaps between languages reached 34% in functional benchmarks versus under 5% in static benchmarks
- Model rankings shifted significantly between static and functional evaluations
- Arabic and English showed among the most robust performance across models
- Yoruba consistently showed the poorest performance across all models
- Aya-Expanse-32B had a 23.47% performance gap on CL-IFEval versus only 5.89% on static benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Functional benchmarks based on templated generation expose performance brittleness that static benchmarks miss.
- **Mechanism:** Unlike static benchmarks with fixed input-output pairs, functional benchmarks use templates containing variables ($X$) and distractors ($D$). This forces the model to process the prompt structurally rather than relying on memorized sequences or surface-level patterns. The ground truth is calculated dynamically via a function $f(X)$, decoupling the verification from the specific phrasing of the input.
- **Core assumption:** Performance drops in functional settings indicate a failure in robust instruction processing or reasoning, rather than just translation noise.
- **Evidence anchors:**
  - [Page 2] Defines functional evaluation: "input permutations... generated through a fixed template... ground truth... calculated using literal template-based functional mappings."
  - [Page 4] Results show a 24% performance drop between static M-GSM and functional CL-GSM Symbolic in English, suggesting static benchmarks may inflate capability.
  - [Corpus] Related work (MGSM-Pro) notes GSM-Symbolic showed "strong evidence of high variance," reinforcing that template-based stress tests reveal instability.
- **Break condition:** If template translation introduces semantic drift that breaks the logic of the question (e.g., changing a math problem's meaning), the performance gap may measure translation quality rather than model reasoning.

### Mechanism 2
- **Claim:** Translating functional templates (rather than static datasets) isolates cross-lingual generalization of instruction-following logic.
- **Mechanism:** By translating the instruction prompt templates (e.g., "Create a riddle using $N$ words") into target languages, the evaluation tests whether the model can map the *instructional intent* to the correct output function ($f(X)$) regardless of the language of the prompt. This measures the portability of the model's "reasoning engine" across linguistic boundaries.
- **Core assumption:** The translated templates retain the semantic constraints and difficulty of the original English templates.
- **Evidence anchors:**
  - [Page 3] Methodology: "translate 541 English IFEval prompts... as well as 5000 question-answer pairs generated from 100 templated symbolic problem variants."
  - [Page 2] Figure 1 illustrates how variables like $\{n1, n2\}$ and distractors $\{color1, color2\}$ are embedded in the template.
- **Break condition:** If the model fails to understand the template structure in a specific language due to tokenization issues or lack of training data, the mechanism breaks down into a resource-availability test rather than a reasoning test.

### Mechanism 3
- **Claim:** Static benchmarks correlate poorly with functional performance, specifically exaggerating model capability in high-resource languages.
- **Mechanism:** Static benchmarks often rely on multiple-choice (M-MMLU) or specific phrasing (Belebele) which may be susceptible to data contamination or pattern matching. Functional benchmarks require generative execution of constraints. The paper suggests that high scores on static benchmarks do not guarantee the robust execution of instructions, revealing a "robustness gap."
- **Core assumption:** High performance on static benchmarks is partially driven by surface-level pattern matching or contamination, which functional benchmarks mitigate via permutation.
- **Evidence anchors:**
  - [Page 4] "Aya-expanse-32B... has a 5.89% average error gap [static]... but a 23.47% performance gap... on the functional benchmark."
  - [Page 2] Cites critique that static benchmarks are "devoid of realistic cultural context" and subject to "data contamination."
  - [Corpus] The corpus signals indicate related research in multilingual evaluation, but specific evidence for the *divergence* between static/functional is primarily isolated to the source paper.
- **Break condition:** If the functional benchmark's verification logic (e.g., regex for counting words) fails for specific languages (like Yoruba agglutination), the low correlation could be an artifact of the metric, not the model.

## Foundational Learning

- **Concept: Functional vs. Static Evaluation**
  - **Why needed here:** The paper's core thesis rests on the distinction between static (fixed dataset) and functional (templated, generative) evaluation. Without understanding this, the performance gaps reported are uninterpretable.
  - **Quick check question:** If I change a name in a math problem from "Sally" to "Bob" and the model fails, is this a failure detected by static or functional evaluation?

- **Concept: Cross-Lingual Transfer**
  - **Why needed here:** The study evaluates how well capabilities (math, instruction following) learned in one language (English) transfer to others (Yoruba, Hindi). Understanding this helps explain why Arabic (medium resource) sometimes outperforms expectations.
  - **Quick check question:** Why might a model perform well on translation but poorly on instruction-following in the same non-English language?

- **Concept: Template Variables & Distractors**
  - **Why needed here:** The mechanism relies on populating templates with $X$ (variables that change the answer) and $D$ (distractors that shouldn't). Understanding this separation is crucial for designing the evaluation logic.
  - **Quick check question:** In the apple math problem (Fig 1), does the *color* of the apple change the ground truth $f(X)$? Should the model be penalized for mentioning it?

## Architecture Onboarding

- **Component map:** Template Engine -> Localization Layer (Google Translate + Spot-check) -> Inference Interface -> Verifier (deterministic function $f(X)$)
- **Critical path:** The validity of the entire system depends on the **Template Translation Quality**. If the logical constraint ("Answer in capital letters") is mistranslated in the template, the verifier will unfairly penalize the model.
- **Design tradeoffs:**
  - **Translation Method:** The authors use Google Translate (speed/coverage) vs. Human Translation (accuracy). This introduces noise, specifically in low-resource languages (Yoruba).
  - **Metric Strictness:** The paper uses "Strict" vs. "Loose" accuracy. Strict is more reliable but brittle; Loose is forgiving but may hallucinate compliance.
  - **Language Scope:** Focusing on 6 languages allows deeper analysis but limits generalizability to the "long tail" of languages.
- **Failure signatures:**
  - **Semantic Drift:** Translation errors in math problems (e.g., metric/imperial confusion) leading to unsolvable prompts.
  - **Instruction Amnesia:** Models ignoring specific constraints (e.g., "use 10 words") in non-English languages, specifically the "Start/End" category failure in Yoruba (Page 5).
  - **Verifier Mismatch:** The regex/parser for the verifier failing to parse non-English formatting (e.g., different decimal separators or quotation marks).
- **First 3 experiments:**
  1. **Sanity Check (Round-Trip Translation):** Take the translated templates, translate them back to English, and verify the logic holds. This quantifies translation noise.
  2. **Per-Template Variance Analysis:** Run the same model on the 50 samples of the 10 math templates (as done in Fig 9-11) to identify which *types* of reasoning (e.g., probabilistic inference) are most fragile to language switching.
  3. **Distractor Ablation:** Run the functional benchmarks with distractors ($D$) removed vs. included to measure the specific cost of "noise" in the prompt for each language.

## Open Questions the Paper Calls Out
1. Can the functional evaluation paradigm be successfully extended to assess multilingual competence in code generation and multi-modal instruction following?
2. Can the functional evaluation framework be utilized effectively as a training signal to improve the robustness of multilingual models?
3. Do proprietary, closed-weight models exhibit the same large discrepancies between static benchmark performance and functional performance as the open-weight models analyzed?

## Limitations
- Translation quality uncertainty: Google Translate with spot-checking may introduce noise, particularly for low-resource languages like Yoruba
- Limited language scope: Only 6 languages tested, limiting generalizability to the broader linguistic landscape
- Verification metric brittleness: Strict verification may penalize valid outputs that don't match exact patterns, especially across languages

## Confidence
- **High Confidence:** The relative ranking shifts between static and functional benchmarks (e.g., Aya-Expanse-32B's 23.47% performance gap on CL-IFEval vs. 5.89% on static benchmarks)
- **Medium Confidence:** The claim that functional benchmarks better reveal cross-lingual performance gaps
- **Low Confidence:** The interpretation that Yoruba's poor performance represents a "worst-case" language scenario rather than a template translation failure

## Next Checks
1. **Round-Trip Translation Validation:** Translate each functional template to the target language and back to English, then compare against the original for semantic drift.
2. **Per-Instruction Failure Analysis:** For CL-IFEval, categorize failed prompts by instruction type (Create, Start/End, Use, Answer) and language.
3. **Distractor Sensitivity Testing:** Run the functional benchmarks with distractors ($D$) systematically removed vs. included for each language.