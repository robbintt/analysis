---
ver: rpa2
title: Multimodal Online Federated Learning with Modality Missing in Internet of Things
arxiv_id: '2505.16138'
source_url: https://arxiv.org/abs/2505.16138
tags:
- modality
- learning
- data
- missing
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning from multimodal
  data in IoT environments where sensor failures cause missing modalities during training.
  The authors propose Multimodal Online Federated Learning (MMO-FL) and introduce
  the Prototypical Modality Mitigation (PMM) algorithm to handle missing modalities
  through prototype learning.
---

# Multimodal Online Federated Learning with Modality Missing in Internet of Things

## Quick Facts
- arXiv ID: 2505.16138
- Source URL: https://arxiv.org/abs/2505.16138
- Reference count: 36
- Key outcome: PMM achieves superior accuracy in multimodal online federated learning with missing modalities, with sublinear regret bounds

## Executive Summary
This paper addresses the challenge of learning from multimodal data in IoT environments where sensor failures cause missing modalities during training. The authors propose Multimodal Online Federated Learning (MMO-FL) and introduce the Prototypical Modality Mitigation (PMM) algorithm to handle missing modalities through prototype learning. PMM constructs and updates class-specific prototypes for each modality, which can substitute for missing modalities during training. Theoretical analysis shows that PMM achieves sublinear regret bounds. Experiments on UCI-HAR and MVSA-Single datasets demonstrate that PMM outperforms baselines including zero-filling and partial modality training, achieving superior accuracy even compared to settings with complete modalities.

## Method Summary
MMO-FL extends federated learning to multimodal online settings with missing modalities. The PMM algorithm consists of two components: Online Prototypes Construction (OPC) where clients compute local class-conditional prototypes as feature averages, and Online Prototypes Substitution (OPS) where missing modality features are reconstructed using persistent global prototypes. The server aggregates models via FedAvg and maintains persistent prototypes updated through cumulative moving averages. Clients with missing modalities download and use these prototypes to substitute absent features during training. The framework supports quantization and delayed updates to reduce communication overhead while maintaining good performance.

## Key Results
- PMM outperforms zero-filling and partial modality training baselines on UCI-HAR and MVSA-Single datasets
- Achieves comparable or superior accuracy to full-modality settings even with missing modalities
- Sublinear regret bounds proven with O(√T) regret under complete modalities, degraded by linear term under missing modalities
- Quantization with b=2 bits achieves substantial communication reduction with minor accuracy loss
- Performance degrades significantly when missing rate λ > 0.7

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prototype-based substitution can compensate for missing modalities in online federated learning.
- Mechanism: PMM constructs modality-specific class prototypes by averaging feature representations from available data across clients. When a modality is missing, the algorithm substitutes the absent features with the corresponding global prototype based on the inferred class label from available modalities.
- Core assumption: The law of large numbers applies—aggregated prototypes converge toward true class-conditional feature distributions as data accumulates, making them suitable approximations for missing modality features.
- Evidence anchors:
  - [section VI] "We define the local prototype as the average value of the features extracted by the modality encoder" (Eq. 15)
  - [section VI] "When modality m is absent in the current global round, the corresponding feature representations are synthesized using the persistent global prototypes"
  - [corpus] Related work FedProto [28] demonstrates prototype exchange effectiveness in heterogeneous FL, but corpus lacks direct validation of prototype-based modality reconstruction in online settings.
- Break condition: If missing modalities persist at very high rates (λ > 0.7), prototype updates become too infrequent to maintain representational accuracy, causing performance degradation (Fig. 5).

### Mechanism 2
- Claim: Sublinear regret bounds are achievable in MMO-FL, but missing modalities introduce an additional linear regret term proportional to the missing rate.
- Mechanism: Theoretical analysis shows that with learning rate η = O(1/√T), MMO-FL achieves O(√T) regret under complete modalities. However, missing modalities add an O(T(1-β)) term where β is the minimum available modality ratio, because gradient updates for missing modalities are zeroed out.
- Core assumption: The convexity of loss functions (Assumption 1) and Lipschitz continuity of gradients (Assumption 2) hold; Assumption 5 guarantees β > 0 (clients never lose all modalities).
- Evidence anchors:
  - [section V] Theorem 3 provides regret bound: "RegT ≤ K‖Θ1,0 − Θ∗‖²/2ηE + (5 − 2β)ηTK(M+1)L² + 2ηTDK(M+1)²φσL + 2(1-β)(M+1)TDKσL/E"
  - [section V] "If we set β = 1, meaning no modality is missing... MMO-FL clearly achieves a regret bound of O(√T)"
  - [corpus] No corpus papers provide comparative regret analysis for multimodal online FL.
- Break condition: If β approaches 0 (frequent complete modality loss), the linear term dominates and sublinear regret is not achievable.

### Mechanism 3
- Claim: Prototype quantization and delayed updates can reduce communication overhead with acceptable performance trade-offs.
- Mechanism: Prototypes are less sensitive to precision than raw gradients because they serve as coarse semantic approximations. Uniform scalar quantization with b bits reduces transmission size, while delayed updates skip prototype construction in some rounds.
- Core assumption: Prototype substitution is inherently approximate, so additional quantization error introduces marginal degradation.
- Evidence anchors:
  - [section VI Remark 2] "the precision of this substitution may not be highly sensitive... quantization technique can be applied"
  - [section VII] Fig. 7 shows b=2 bits achieves substantial communication reduction with minor accuracy loss on UCI-HAR
  - [corpus] FedSAUC [arxiv 2504.04867] addresses communication efficiency in edge FL but through update control rather than quantization.
- Break condition: Excessive quantization (b < 2) or very long update delays may cause prototype drift, degrading substitution quality.

## Foundational Learning

- **Federated Learning (FedAvg protocol)**:
  - Why needed here: MMO-FL extends horizontal FL with multimodal architectures and online data streams. Understanding client-server communication, local gradient descent, and model aggregation is prerequisite.
  - Quick check question: Can you sketch the FedAvg update rule and explain why local iterations (E > 1) introduce optimization challenges?

- **Online Convex Optimization and Regret Analysis**:
  - Why needed here: The theoretical contribution relies on regret bounds to characterize learning performance over streaming data. The paper proves O(√T) regret under specific assumptions.
  - Quick check question: What does a sublinear regret bound imply about an online algorithm's asymptotic performance relative to a fixed optimal strategy?

- **Prototype Learning in Neural Networks**:
  - Why needed here: PMM uses class-conditional feature averages as prototypes. Understanding how prototypes capture semantic information and enable knowledge transfer is essential.
  - Quick check question: How does a prototype differ from a learned class embedding in a standard classification head?

## Architecture Onboarding

- **Component map**:
  - Modality Encoders (θ₁...θₘ) -> Head Encoder (θ₀) -> Prediction
  - Client-side: Feature extraction, prototype computation, local training
  - Server-side: Model aggregation, prototype maintenance, global prototype broadcast

- **Critical path**:
  1. Round t begins → Client collects data, detects modality availability Mᵗ_k
  2. If full modalities → Train locally, compute local prototypes pᵗ,ᵐ_{k,c} via Eq. 15
  3. If missing modalities → Download ¯Pᵗ, substitute via Eq. 19, continue training
  4. Upload local model (and prototypes if available)
  5. Server aggregates models via Eq. 7, updates persistent prototypes via Eq. 17
  6. Broadcast global model and prototype collection

- **Design tradeoffs**:
  - Quantization level (b): Lower bits reduce communication but may degrade prototype quality—start with b=4
  - Update delay interval: Longer delays reduce overhead but slow prototype refinement—empirically test DL=2
  - Feature dimension: Must be consistent across modalities for prototype alignment (paper uses 128-dim)

- **Failure signatures**:
  - High missing rate (λ > 0.7): Accuracy drops significantly as prototypes cannot be reliably updated
  - Severe non-IID (α < 1): Local prototypes may diverge; persistent global prototypes help but don't fully mitigate
  - Complete modality loss (β = 0): Training halts—ensure Assumption 5 is enforced in deployment

- **First 3 experiments**:
  1. **Reproduce FM vs PM vs ZF vs PMM comparison** on UCI-HAR with λ=0.5, α=10 (Fig. 4a). Verify PMM achieves comparable or superior accuracy to full-modality baseline.
  2. **Ablation on missing rate λ**: Sweep λ ∈ {0.3, 0.5, 0.7} to identify performance cliff point for your data regime.
  3. **Communication efficiency test**: Compare full-precision prototype upload vs. quantized (b=4) vs. delayed update (DL=2) to quantify overhead reduction vs. accuracy trade-off on your target edge hardware.

## Open Questions the Paper Calls Out

- **Question:** How does the MMO-FL framework perform when deployed on physical IoT testbeds with genuine real-time multimodal data streams, as opposed to the simulated online environments used in this study?
  - **Basis in paper:** [explicit] The Conclusion states, "In future work, we aim to integrate real-time IoT multimodal data and develop practical testbeds to evaluate and enhance the MMO-FL framework."
  - **Why unresolved:** The current experiments (Section VII) rely on static datasets (UCI-HAR, MVSA-Single) artificially converted into streaming data, which may not capture the full volatility and noise of physical sensors.
  - **What evidence would resolve it:** Empirical results from a hardware deployment showing test accuracy and communication latency under actual sensor failure conditions.

## Limitations

- Prototype substitution quality degrades significantly when missing modality rates exceed 70%
- Regret bounds assume convexity and Lipschitz continuity that may not hold for deep multimodal architectures
- Current analysis only considers synchronized missing modalities across all clients

## Confidence

- **High confidence**: Prototype substitution mechanism works effectively for moderate missing rates (λ ≤ 0.5) based on empirical results showing PMM outperforms baselines
- **Medium confidence**: Sublinear regret bounds are achievable, but the additional linear term from missing modalities may dominate in practice when β is small
- **Medium confidence**: Communication efficiency gains from quantization are real but come with accuracy trade-offs that require careful calibration

## Next Checks

1. **Robustness to high missing rates**: Systematically test PMM performance when λ ∈ {0.3, 0.5, 0.7, 0.9} to identify the exact threshold where prototype substitution fails and understand whether alternative strategies (e.g., adaptive prototype aging) could extend the effective range.

2. **Non-IID stress test**: Vary the Dirichlet parameter α ∈ {0.1, 1, 5, 10} to quantify how client heterogeneity impacts prototype quality and substitution accuracy, particularly for minority classes that may be underrepresented in some clients' local data.

3. **Communication efficiency benchmarking**: Compare uniform scalar quantization against alternative approaches (vector quantization, sparsification, or adaptive bit allocation) across different data modalities to determine if current quantization schemes are optimal for preserving prototype semantic information.