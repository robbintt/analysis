---
ver: rpa2
title: Effective Training Data Synthesis for Improving MLLM Chart Understanding
arxiv_id: '2508.06492'
source_url: https://arxiv.org/abs/2508.06492
tags:
- chart
- data
- code
- arxiv
- charts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a modular pipeline for synthesizing chart
  data and QA pairs to improve multimodal large language models' chart understanding.
  By separating data generation from chart function creation, conditioning subplot
  generation on previous subplots, diversifying visual elements, filtering low-quality
  charts, and generating QA pairs with GPT-4o, the authors create the effective chart
  dataset (ECD) containing over 10k chart images and 300k QA pairs across 25 themes
  and 29 chart types.
---

# Effective Training Data Synthesis for Improving MLLM Chart Understanding

## Quick Facts
- **arXiv ID:** 2508.06492
- **Source URL:** https://arxiv.org/abs/2508.06492
- **Reference count:** 40
- **Primary result:** Modular pipeline creates ECD dataset improving MLLM chart understanding by 11.96% average on CharXiv and up to 31.58% on ECDBench

## Executive Summary
This paper introduces a modular pipeline for synthesizing chart data and QA pairs to improve multimodal large language models' chart understanding. By separating data generation from chart function creation, conditioning subplot generation on previous subplots, diversifying visual elements, filtering low-quality charts, and generating QA pairs with GPT-4o, the authors create the effective chart dataset (ECD) containing over 10k chart images and 300k QA pairs across 25 themes and 29 chart types. The ECD consistently improves model performance across six benchmarks, with average accuracy gains of 11.96% on CharXiv and up to 31.58% on the constructed ECDBench, surpassing existing chart training datasets in both data diversity and effectiveness.

## Method Summary
The ECD dataset is created through a 5-step pipeline: (1) separate data and function generation for single plots using GPT-4o, (2) conditional sequential subplot generation where later plots are conditioned on earlier ones, (3) visual diversification through random code modifications, (4) quality filtering using GPT-4o ratings on visual clarity and semantic coherence, and (5) QA generation with confidence filtering. The process generates ~10.5k images and ~321k QA pairs using 25 academic themes and 29 chart types. The key innovation is decoupling data generation from chart function code, allowing LLMs to focus on creating diverse, non-linear data distributions rather than simultaneously solving code syntax and data logic.

## Key Results
- ECD achieves 11.96% average accuracy gain on CharXiv benchmark compared to models trained on real charts
- ECD improves performance on all six tested benchmarks with up to 31.58% gain on ECDBench
- ECD has the lowest FID (60.74) compared to other synthetic datasets when measured against CharXiv
- ECD contains 10.5k images and 321k QA pairs with 10-15 descriptive and 10-15 reasoning questions per chart

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupling data generation from chart function code increases the complexity and realism of synthesized charts.
- **Mechanism:** By separating the structural definition (human-defined functions) from the semantic content (LLM-generated data), the system reduces the cognitive load on the generator, allowing it to focus on creating diverse, non-linear data distributions rather than simultaneously solving code syntax and data logic.
- **Core assumption:** LLMs generate higher-quality, noisy data when not burdened with generating the plotting code structure simultaneously.
- **Evidence anchors:** [abstract] "we separate data and function creation for single plot generation..." [section 3.1] "...we strategically decompose the task... By focusing on a step-by-step generation, we ensure the improvement on data distributions..."

### Mechanism 2
- **Claim:** Sequential conditioning of subplots improves semantic coherence, mimicking real scientific multi-panel figures.
- **Mechanism:** Instead of generating subplots independently, the pipeline generates later subplots conditioned on the data of earlier ones. This forces a thematic relationship between panels (e.g., subplot 2 presents a different view of the data in subplot 1), aligning the synthetic structure with authentic scientific storytelling.
- **Core assumption:** Real-world chart understanding requires reasoning across semantically related visual panels, not just isolated plots.
- **Evidence anchors:** [abstract] "...condition the generation of later subplots on earlier ones for multi-subplot figures..." [section 3.2] "This conditional generation strengthens the relationships between subplots, emulates human-designed scientific figures..."

### Mechanism 3
- **Claim:** Visual diversification and quality filtering bridge the distribution gap between synthetic and real charts (FID reduction).
- **Mechanism:** Random injection of visual elements (annotations, noise, zooms) increases visual entropy, while a GPT-4o-based filter removes low-quality artifacts (clutter, misalignment). This dual process shifts the synthetic distribution closer to the complex distribution of real scientific charts (CharXiv).
- **Core assumption:** Higher visual entropy and "semantic coherence" ratings correlate with improved MLLM fine-tuning performance on real benchmarks.
- **Evidence anchors:** [abstract] "...visually diversify the generated figures, filter out low quality data..." [Fig 3] Shows ECD has the lowest FID (60.74) compared to other synthetic datasets when measured against CharXiv.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT)**
  - **Why needed here:** The ECD dataset is designed specifically as an SFT dataset to teach open-source MLLMs to handle chart reasoning, distinct from pre-training.
  - **Quick check question:** How does SFT on synthetic data differ from pre-training on real-world web scrapes in terms of distribution alignment?

- **Concept: Fréchet Inception Distance (FID)**
  - **Why needed here:** The paper uses FID to quantitatively claim that ECD is "more realistic" than other synthetic datasets by measuring the distance between feature distributions.
  - **Quick check question:** Does a lower FID score guarantee better downstream QA performance, or just better visual similarity?

- **Concept: Conditional Generation**
  - **Why needed here:** Understanding how the pipeline generates multi-panel plots requires grasping how inputs for step $N$ depend on outputs from step $N-1$.
  - **Quick check question:** In the context of subplot generation, what is the "condition" provided to the LLM?

## Architecture Onboarding

- **Component map:** Theme Selector -> Function Library -> Data Generator (LLM) -> Renderer -> Diversifier (LLM) -> Filter (LLM) -> QA Generator (LLM)

- **Critical path:** Data Generation -> Rendering -> Diversification -> QA Generation. (Filtering runs in parallel to quality control)

- **Design tradeoffs:**
  - **Modularity vs. Simplicity:** Maintaining separate functions and data generators is harder to pipeline than end-to-end generation but yields higher data complexity (Table 12).
  - **Cost vs. Quality:** Using GPT-4o for generation, diversification, filtering, and QA is expensive (~$2.15k total) compared to template-based methods, but yields higher accuracy.

- **Failure signatures:**
  - **Low Entropy/High FID:** Indicates the diversification step is failing or being filtered out too aggressively.
  - **Hallucinated QA:** GPT-4o generates questions about elements not visually present in the rendered chart (mitigated but not eliminated by confidence filtering).
  - **Visual Clutter:** Over-diversification leads to unreadable charts that pass the code check but fail visual inspection.

- **First 3 experiments:**
  1. **Ablate Generation Strategy:** Compare "Code + Data Joint Generation" vs. "Separate Generation" (Table 12) on FID and Entropy to verify the modularity hypothesis.
  2. **Ablate Diversification:** Train models on datasets with/without visual diversification to measure the impact on CharXiv accuracy.
  3. **Filtering Threshold Test:** Vary the retention threshold for the "visual clarity" filter to find the optimal balance between dataset size and data quality.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a domain-specific vision encoder for charts be developed or adapted to provide a more accurate metric for evaluating the realism and effectiveness of synthetic chart datasets compared to current natural-image-based metrics?
- **Basis in paper:** [explicit] Section I (Discussions and Limitations) states, "The community still lacks a proper chart vision encoder to better compute indicators that require feature extraction," noting that FID relies on InceptionNet-V3 pretrained on ImageNet, creating a "domain gap."
- **Why unresolved:** Current evaluation metrics (FID) utilize feature extractors designed for natural images, which fail to capture chart-specific semantic nuances like text layout, logical structure, and data density, leading to a potential misalignment between metric scores and actual training utility.
- **What evidence would resolve it:** The development of a chart-specific feature extractor that correlates more strongly with downstream MLLM accuracy on complex reasoning benchmarks than FID does.

### Open Question 2
- **Question:** To what extent do specific low-level visual attributes—such as font sizes, text formats, and color schemes—independently contribute to the training effectiveness of synthetic datasets compared to high-level structural complexity?
- **Basis in paper:** [explicit] Section I (Discussions and Limitations) notes that FID and entropy are imperfect indicators and poses the question: "Other factors such as text formats, font sizes, and even colors may also influence the training set quality."
- **Why unresolved:** The current study focuses on modular generation and complexity (entropy) but treats visual attributes primarily as random diversification steps rather than independent variables to be systematically analyzed for their impact on model learning.
- **What evidence would resolve it:** A controlled ablation study varying only specific visual styles (e.g., font sizes or color palettes) in the ECD while holding data complexity constant to measure the resulting variance in MLLM benchmark performance.

### Open Question 3
- **Question:** What is the optimal ratio of descriptive to reasoning QA pairs during fine-tuning to ensure consistent improvements across diverse benchmark distributions without causing performance regressions in specific tasks like binary classification?
- **Basis in paper:** [explicit] Section 5.2 highlights performance drops in "Binary" questions for some models and suggests "the data mixture ratio during fine-tuning might not have been fully optimized," while Section 5.4 discusses the imperfect match between ECD's definitions and benchmark definitions.
- **Why unresolved:** The paper demonstrates that mixing descriptive and reasoning data is beneficial (ablation study), but fine-tuned models still suffer minor performance degradation on specific benchmarks (e.g., ChartBench), indicating the current ratios or data distributions are suboptimal for generalization.
- **What evidence would resolve it:** A systematic grid search over descriptive-to-reasoning ratios during the fine-tuning of various MLLM architectures, followed by evaluation on the full suite of benchmarks to identify a generalized optimal mixture.

## Limitations

- The paper relies heavily on GPT-4o for multiple pipeline stages, creating scalability bottlenecks and potential bias in the synthetic data distribution
- Filtering thresholds ("above dataset mean") and diversification parameters are not fully specified, making exact replication challenging
- The 2.3% hallucination rate in generated QA pairs, while mitigated by confidence filtering, remains an unresolved quality issue

## Confidence

- **High confidence:** The effectiveness of separate data/function generation (Table 12 results clearly support the mechanism), the improvement over existing synthetic datasets on all six benchmarks, and the quantitative FID/entropy measurements showing ECD's superior visual complexity
- **Medium confidence:** The causal relationship between sequential subplot conditioning and semantic coherence improvements, as the paper provides mechanism but limited ablation studies specifically isolating this effect
- **Medium confidence:** The long-term generalization of ECD-trained models to real-world charts beyond the tested benchmarks, given the synthetic-to-real distribution gap, though FID measurements provide supporting evidence

## Next Checks

1. **Ablation of GPT-4o dependency:** Replace GPT-4o with open-source models (e.g., Llama 3) for one stage (e.g., data generation) to measure performance impact and assess scalability concerns

2. **Cross-domain generalization test:** Evaluate ECD-trained models on charts from domains not present in the 25 training themes to measure true generalization beyond the training distribution

3. **Cost-effectiveness analysis:** Calculate the computational and financial cost per high-quality chart image to compare against template-based methods and assess practical deployment feasibility