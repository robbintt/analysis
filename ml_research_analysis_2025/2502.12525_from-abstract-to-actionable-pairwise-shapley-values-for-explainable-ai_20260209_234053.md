---
ver: rpa2
title: 'From Abstract to Actionable: Pairwise Shapley Values for Explainable AI'
arxiv_id: '2502.12525'
source_url: https://arxiv.org/abs/2502.12525
tags:
- feature
- shapley
- values
- features
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving interpretability
  in Shapley value-based feature attribution methods for explainable AI. The authors
  propose Pairwise Shapley Values, a novel framework that grounds feature attributions
  in explicit, human-relatable comparisons between proximal data instances, using
  single-value imputation for efficiency.
---

# From Abstract to Actionable: Pairwise Shapley Values for Explainable AI

## Quick Facts
- **arXiv ID**: 2502.12525
- **Source URL**: https://arxiv.org/abs/2502.12525
- **Authors**: Jiaxin Xu; Hung Chau; Angela Burden
- **Reference count**: 40
- **Primary result**: Pairwise Shapley Values improve interpretability by grounding feature attributions in explicit comparisons between proximal data instances, outperforming traditional methods in efficiency and intuitive explanations across diverse regression and classification tasks.

## Executive Summary
This paper addresses the challenge of improving interpretability in Shapley value-based feature attribution methods for explainable AI. The authors propose Pairwise Shapley Values, a novel framework that grounds feature attributions in explicit, human-relatable comparisons between proximal data instances, using single-value imputation for efficiency. Unlike traditional methods relying on abstract baselines or computationally intensive calculations, their approach mimics human decision-making strategies like comparative market analysis in real estate. The method was evaluated on diverse regression and classification tasks (home pricing, polymer property prediction, drug discovery), demonstrating enhanced interpretability through more intuitive explanations, improved model insights (e.g., monotonicity rates), and computational efficiency. The pairwise approach consistently outperformed existing Shapley-based methods while maintaining robustness across different similarity algorithms for reference selection.

## Method Summary
The Pairwise Shapley Values framework introduces a novel approach to feature attribution by computing Shapley values between pairs of proximal data instances rather than using abstract baselines. The method employs single-value imputation to efficiently approximate marginal contributions, significantly reducing computational complexity compared to traditional Shapley value calculations. By grounding attributions in concrete comparisons between similar data points, the approach aligns with human decision-making patterns, such as comparative market analysis. The framework was evaluated across multiple regression and classification tasks, including home pricing, polymer property prediction, and drug discovery, demonstrating improved interpretability and computational efficiency while maintaining robustness across different similarity algorithms for reference selection.

## Key Results
- Pairwise Shapley Values consistently outperformed traditional Shapley-based methods in interpretability across diverse tasks including home pricing, polymer property prediction, and drug discovery.
- The method demonstrated computational efficiency through single-value imputation while maintaining robust performance across different similarity algorithms for reference selection.
- Enhanced model insights were achieved, including improved understanding of monotonicity rates, through more intuitive pairwise comparisons between proximal data instances.

## Why This Works (Mechanism)
The framework works by replacing abstract baseline comparisons with explicit pairwise comparisons between proximal data instances, making feature attributions more intuitive and human-relatable. Single-value imputation provides computational efficiency while approximating marginal contributions accurately enough for practical use. The method leverages similarity algorithms to select appropriate reference points, ensuring that comparisons remain meaningful and contextually relevant. By mimicking human decision-making strategies like comparative market analysis, the approach creates explanations that align with how people naturally reason about feature importance.

## Foundational Learning
**Shapley Value Theory**: Traditional method for feature attribution in cooperative game theory applied to ML interpretability. Needed because it provides a theoretically sound foundation for fair feature contribution attribution. Quick check: Verify that the pairwise adaptation preserves the key Shapley axioms (efficiency, symmetry, dummy player, additivity).

**Imputation Methods**: Techniques for handling missing values in feature sets during marginal contribution calculations. Needed because efficient approximation is crucial for computational feasibility. Quick check: Compare single-value imputation accuracy against more expensive methods on validation datasets.

**Similarity Metrics for Reference Selection**: Algorithms to identify proximal data instances for meaningful pairwise comparisons. Needed because the quality of reference selection directly impacts attribution interpretability. Quick check: Test robustness across different similarity metrics (Euclidean, cosine, Mahalanobis) on the same datasets.

## Architecture Onboarding

**Component Map**: Data Instance -> Similarity Algorithm -> Reference Selection -> Marginal Contribution Calculation -> Pairwise Shapley Value -> Attribution Output

**Critical Path**: The essential computation flow follows: input data instance → similarity-based reference selection → single-value imputation for marginal contributions → pairwise Shapley value calculation → interpretable feature attributions.

**Design Tradeoffs**: The method trades computational complexity (from exponential to polynomial time) against potential approximation error from single-value imputation. This represents a deliberate choice prioritizing practical usability and interpretability over theoretical exactness.

**Failure Signatures**: Potential failure modes include: poor reference selection leading to meaningless comparisons, imputation bias affecting attribution accuracy, and loss of global feature importance patterns when focusing on local pairwise comparisons.

**First 3 Experiments**: 1) Benchmark pairwise vs traditional Shapley on home pricing dataset for both computational time and attribution interpretability. 2) Evaluate robustness across different similarity metrics using polymer property prediction task. 3) Test scalability by applying to high-dimensional drug discovery dataset while monitoring attribution quality degradation.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability limited to evaluated domains (home pricing, polymer properties, drug discovery) without extensive testing on highly complex or high-dimensional data.
- Single-value imputation, while computationally advantageous, may introduce approximation errors that aren't fully characterized.
- The choice of similarity algorithms for reference selection could significantly impact results, though the method showed some robustness across different approaches.

## Confidence
**High**: Computational performance and efficiency claims are well-supported by consistent results across multiple datasets and tasks.

**Medium**: Generalizability claims require further validation on diverse, high-dimensional datasets beyond the specific domains tested.

**Low**: The paper doesn't extensively characterize the approximation errors introduced by single-value imputation or provide comprehensive sensitivity analysis for similarity metric selection.

## Next Checks
1. Test the Pairwise Shapley framework on high-dimensional datasets (e.g., genomics, image features) to assess scalability and interpretability preservation.
2. Conduct user studies with domain experts to validate whether the pairwise explanations actually improve decision-making compared to traditional Shapley methods.
3. Perform ablation studies varying the imputation strategy and similarity metrics to quantify their impact on attribution quality and computational efficiency.