---
ver: rpa2
title: 'GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection'
arxiv_id: '2512.09396'
source_url: https://arxiv.org/abs/2512.09396
tags:
- information
- arxiv
- automation
- gair
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GAIR is a multimodal large language model-based GUI automation
  agent framework designed to integrate capabilities from heterogeneous models. The
  core idea is to use multiple GUI-specific models for information extraction while
  employing a general-purpose MLLM for joint reasoning, decision-making, and group
  reflection.
---

# GAIR: GUI Automation via Information-Joint Reasoning and Group Reflection

## Quick Facts
- **arXiv ID**: 2512.09396
- **Source URL**: https://arxiv.org/abs/2512.09396
- **Reference count**: 28
- **Primary result**: Achieves 78.4% success rate on UI-I2E-Bench and 91.0% on ScreenSpot benchmarks using fewer parameters than state-of-the-art models.

## Executive Summary
GAIR is a multimodal large language model-based framework for GUI automation that combines specialized models for information extraction with a general-purpose model for joint reasoning and decision-making. The framework employs a novel group reflection mechanism where the general-purpose model requests targeted re-extraction from specialized models when initial information is insufficient. GAIR was evaluated on two benchmarks covering web, mobile, and desktop platforms, demonstrating superior performance with fewer parameters compared to existing approaches.

## Method Summary
GAIR employs a multi-model architecture where GUI-specific MLLMs (UI-TARS-7B-DPO, InfiGUI-R1-3B, UGround-V1-7B) extract element information from screenshots while a general-purpose MLLM (Qwen2.5-VL-7B) coordinates reasoning and decision-making. The framework operates in three stages: observation (specialized models extract data), reasoning/decision (general-purpose model integrates and decides), and group reflection (if information is insufficient, the coordinator provides targeted instructions to specialized models for re-extraction). This approach leverages the complementary strengths of specialized perception models and general-purpose reasoning models.

## Key Results
- GAIR achieves 78.4% success rate on UI-I2E-Bench benchmark, outperforming state-of-the-art models with fewer parameters
- On ScreenSpot benchmark, GAIR reaches 91.0% success rate across web, mobile, and desktop platforms
- Ablation studies show significant improvements from both multi-model integration (76.5% without reflection) and group reflection (78.4% with reflection)

## Why This Works (Mechanism)

### Mechanism 1: Information-Joint Reasoning
Aggregating observations from multiple specialized models through a general-purpose coordinator improves task coverage and decision quality compared to any single specialized model. GUI-specific models trained on different distributions capture complementary aspects of interface semantics and element localization, which the general-purpose model synthesizes and integrates.

### Mechanism 2: Group Reflection for Targeted Re-Extraction
The coordinator can request task-specific re-extraction from specialized models when initial information is insufficient, reducing premature or incorrect action selection. Upon detecting ambiguity, the general-purpose model issues targeted instructions to each specialized model based on their known strengths, then re-integrates refined outputs before final decision-making.

### Mechanism 3: Division of Labor Between Specialized and General-Purpose Models
Separating perception-heavy tasks (element extraction, grounding) into specialized models and reasoning-heavy tasks (integration, planning) into a general-purpose model leverages the respective training advantages of each. Specialized models handle GUI-layout parsing informed by fine-tuning on GUI corpora, while the general-purpose model contributes broader world knowledge and semantic reasoning.

## Foundational Learning

### Concept: GUI Grounding and Referring
Why needed: GAIR relies on specialized models' ability to map natural language instructions to precise screen coordinates. Understanding bounding-box vs. point-based grounding clarifies what each model outputs and how errors propagate.
Quick check: Given a screenshot and instruction "click the submit button," can you distinguish between a model that returns a textual description vs. one that returns pixel coordinates?

### Concept: Multi-Turn Dialogue and Context Accumulation
Why needed: The general-purpose model uses multi-turn dialogue to maintain context across observation, reasoning, and optional reflection stages. Misunderstanding state management leads to incorrect integration of prior model outputs.
Quick check: If the coordinator receives conflicting element labels from two specialized models, how should it structure a follow-up prompt to disambiguate without discarding valid information?

### Concept: Error Detection vs. Correction in Agentic Systems
Why needed: Group reflection introduces an explicit error-avoidance step before action execution. Distinguishing between detection (identifying ambiguity) and correction (resolving it via targeted queries) is critical for interpreting ablation gains.
Quick check: In a reflection cycle, what is the minimum information delta required for the coordinator to justify exiting reflection and committing to an action?

## Architecture Onboarding

### Component map
GUI-Specific MLLMs (multiple) -> General-Purpose MLLM (coordinator) -> Reflection Controller (logical state) -> Execution Interface

### Critical path
1. Screenshot + instruction ingested by all specialized models in parallel
2. Specialized models return structured observations (text + optional coordinates)
3. Coordinator integrates observations, assesses sufficiency; if sufficient, proceeds to action; if not, triggers reflection
4. In reflection, coordinator generates tailored instructions per specialized model; re-extraction occurs; updated observations are re-integrated
5. Final action is selected and executed; results may feed back into next observation cycle

### Design tradeoffs
- **Latency vs. Accuracy**: Each reflection round adds inference cost; early-exit heuristics can reduce unnecessary rounds but risk false confidence
- **Model Heterogeneity vs. Integration Complexity**: More diverse specialized models increase coverage but complicate conflict resolution and instruction formatting
- **Parameter Budget**: Using multiple 7B-class models (~21-30B total) vs. single 72B model; GAIR claims competitive performance with smaller total parameters but deployment complexity increases

### Failure signatures
- **Persistent Disagreement**: Specialized models return incompatible element interpretations across reflection rounds without convergence
- **Instruction Misalignment**: Coordinator's targeted hints are ignored or misinterpreted by specialized models
- **Over-Reflection**: System enters repeated reflection cycles without reaching confidence threshold, exceeding latency budgets

### First 3 experiments
1. **Single-Model Baseline vs. Multi-Model Integration**: Run same benchmark with only best-performing specialized model vs. full multi-model pipeline (no reflection) to isolate gains from joint reasoning
2. **Reflection Ablation**: Compare multi-model integration with and without group reflection to quantify error-avoidance contributions
3. **Per-Model Instruction Targeting**: Log content of coordinator-generated instructions during reflection; manually annotate whether instructions correctly address identified information gaps; correlate instruction quality with downstream success

## Open Questions the Paper Calls Out

### Open Question 1
**Question**: How can the GAIR framework be effectively adapted for GUI automation in vertical professional fields, such as CAD or video editing, where specific training data is scarce?
**Basis**: The paper acknowledges that GUI automation tasks in vertical fields are not yet well defined and identifies exploring solutions for these specific scenarios as primary future work.
**Why unresolved**: Current evaluation relies on general-purpose benchmarks; GUI-specific datasets and models for professional disciplines must be constructed to fully leverage the framework's potential in those domains.

### Open Question 2
**Question**: What is the trade-off between GAIR's improved accuracy and its inference latency compared to monolithic single-model approaches?
**Basis**: The framework requires running multiple GUI-specific models plus a general-purpose model, and potentially repeating this cycle during group reflection.
**Why unresolved**: While the paper highlights reduced "time latency" from error avoidance, it does not quantify the computational cost or wall-clock time added by the ensemble architecture itself compared to single-pass models.

### Open Question 3
**Question**: How robust is the general-purpose model's ability to discriminate valid information when the majority of GUI-specific models provide conflicting or incorrect data?
**Basis**: The analysis shows the decision correct rate drops to 56.8% when only one model is correct, but doesn't fully explore scenarios where multiple specific models hallucinate or agree on an error.
**Why unresolved**: The "Information Alignment" challenge implies the general model must judge reliability, but experiments only test conditions where 1 or 2 models are correct, leaving "all models wrong" or "majority wrong" edge cases unverified.

## Limitations
- Exact prompt templates, reflection-triggering criteria, and reflection instruction content are unspecified, preventing faithful replication
- Performance improvements depend on proprietary model configurations and fine-tuned weights not publicly available
- The reflection mechanism's efficacy relies on coordinator's ability to accurately diagnose information gaps, which may not generalize beyond evaluated benchmarks

## Confidence
- **High confidence**: The core architectural design is clearly specified and internally consistent; division-of-labor rationale is well-supported by ablation results
- **Medium confidence**: Claimed performance improvements are based on published benchmarks but external validation is needed; reflection mechanism's error-avoidance benefits are demonstrated but specific success conditions are not fully characterized
- **Low confidence**: Reproducibility of exact results is uncertain due to unspecified implementation details; generalization to unseen GUI types and real-world deployment scenarios remains unproven

## Next Checks
1. **Prompt template reconstruction**: Reverse-engineer the observation, integration, and reflection prompts by testing described behavior on simplified examples; validate that reconstructed prompts reproduce multi-model coordination and reflection-triggering
2. **Reflection threshold calibration**: Empirically determine information sufficiency threshold by varying confidence cutoffs and measuring reflection frequency vs. success rate; identify optimal trade-offs between accuracy and latency
3. **Cross-platform generalization**: Test GAIR on GUI samples from platforms not represented in UI-I2E-Bench or ScreenSpot (e.g., enterprise software, custom mobile apps); compare performance degradation against single-model baselines to quantify domain robustness