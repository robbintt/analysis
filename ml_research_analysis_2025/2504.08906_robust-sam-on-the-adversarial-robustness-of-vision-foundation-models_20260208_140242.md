---
ver: rpa2
title: 'Robust SAM: On the Adversarial Robustness of Vision Foundation Models'
arxiv_id: '2504.08906'
source_url: https://arxiv.org/abs/2504.08906
tags:
- adversarial
- attack
- robustness
- arxiv
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of the Segment Anything
  Model (SAM) to adversarial attacks by proposing a comprehensive robustness framework.
  The key contributions are a cross-prompt adversarial attack method that achieves
  higher attack success rates across both point and box prompts, and a few-parameter
  adversarial defense using Singular Value Decomposition (SVD) that improves robustness
  while preserving performance.
---

# Robust SAM: On the Adversarial Robustness of Vision Foundation Models

## Quick Facts
- arXiv ID: 2504.08906
- Source URL: https://arxiv.org/abs/2504.08906
- Reference count: 4
- Primary result: Cross-prompt attack achieves higher ASR across both point and box prompts; SVD-based defense improves mIoU by at least 15% using only 512 trainable parameters

## Executive Summary
This paper addresses the vulnerability of Segment Anything Model (SAM) to adversarial attacks by proposing a comprehensive robustness framework. The key contributions are a cross-prompt adversarial attack method that achieves higher attack success rates across both point and box prompts, and a few-parameter adversarial defense using Singular Value Decomposition (SVD) that improves robustness while preserving performance. By adapting only 512 parameters, the proposed defense method achieves at least a 15% improvement in mean Intersection over Union (mIoU) against various adversarial attacks.

## Method Summary
The framework consists of two components: (1) a cross-prompt adversarial attack that targets common key features shared between point and box prompts, achieving higher attack success rates through simultaneous optimization for both prompt types; and (2) an SVD-based defense that adapts only singular values of convolutional layers while keeping all other parameters frozen, enabling robust fine-tuning with minimal parameter updates. The attack uses PGD-style optimization with a step size of α (unspecified) over 20 iterations, while the defense trains for 500 epochs with Adam optimizer (lr=1e-3) on adversarial examples.

## Key Results
- Cross-prompt attack achieves higher attack success rates than individual point or box prompt attacks
- SVD-based defense improves mIoU by at least 15% against various adversarial attacks using only 512 trainable parameters
- Cross-prompt attack demonstrates superior transferability to SAM 2 compared to existing approaches
- Defense preserves clean performance while significantly improving robustness

## Why This Works (Mechanism)

### Mechanism 1: Cross-Prompt Feature Disruption
The attack identifies common key features shared between point and box prompts by zeroing each feature channel and measuring output differences for both prompt types. The intersection of top-K influential channels for point and box prompts identifies shared vulnerable features. Adversarial noise is optimized to push these shared features toward negative values, disrupting both prompt pathways simultaneously. This works because features with high mutual influence across prompt types are the bottleneck for model predictions.

### Mechanism 2: Singular Value Decomposition for Constrained Parameter Adaptation
The defense decomposes convolutional layer weights via SVD into orthogonal basis matrices U, V and diagonal singular values P. Only P is updated via gradient descent while U, V, and all other model parameters remain frozen. This restricts adaptation to rescaling existing feature directions rather than creating new ones, allowing robustness improvements while maintaining the original feature space structure.

### Mechanism 3: Feature Distribution Shift for Noise Mitigation
Adapting singular values shifts the feature distribution to counteract perturbations introduced by adversarial noise. By adjusting P, the model effectively rescales feature contributions to suppress noise-amplified dimensions and emphasize stable dimensions. The defense loss encourages the model to output masks matching ground truth, forcing P to learn scaling factors that neutralize adversarial perturbations.

## Foundational Learning

- **Concept: Adversarial Attacks (PGD/FGSM)**
  - Why needed: The cross-prompt attack builds on gradient-based methods like PGD. Understanding how adversarial examples exploit model gradients is prerequisite to comprehending the attack pipeline.
  - Quick check: Can you explain why PGD uses multiple iterations with a step size α, and how the Clip function constrains perturbation magnitude?

- **Concept: Singular Value Decomposition (SVD)**
  - Why needed: The defense method relies entirely on SVD to decompose weight matrices and constrain adaptation. Understanding orthogonal matrices, singular values, and low-rank structure is essential.
  - Quick check: Given a matrix W = UPV^T, what happens to the transformation if you only modify diagonal elements of P while keeping U and V fixed?

- **Concept: Vision Transformer (ViT) Encoder Architecture**
  - Why needed: SAM uses a ViT-based encoder to extract global features. The attack targets feature maps from this encoder; the defense modifies its final convolutional layers.
  - Quick check: In a ViT, how are image patches processed to produce feature maps, and where do convolutional layers typically appear in the architecture?

## Architecture Onboarding

- **Component map:** Input image X → SAM Encoder → Feature maps F → Key Feature Selector → C_common → Noise optimization → Adversarial example X_adv → SAM Encoder (with SVD-decomposed conv layers) → Decoder → Predicted mask

- **Critical path:** 1) Input image X → SAM Encoder → Feature maps F; 2) For attack: F → Key Feature Selector → C_common → Noise optimization via L_adv → Adversarial example X_adv; 3) For defense: X_adv → SAM Encoder (with SVD-decomposed conv layers) → Decoder → Predicted mask → Loss L_def → Gradient update on P only

- **Design tradeoffs:**
  - Fewer trainable parameters (512) vs. expressiveness: SVD constraint prevents catastrophic forgetting and maintains clean performance but may limit robustness gains compared to full fine-tuning
  - Cross-prompt attack strength vs. detectability: Higher perturbation intensity ε increases ASR but makes adversarial examples more perceptible
  - Top-K selection for key features: Small K yields focused attacks on critical features; large K increases attack coverage but may dilute effectiveness

- **Failure signatures:**
  - Low ASR under cross-prompt attack: Check if C_common is too small (point/box features don't overlap)
  - Clean performance degradation after defense: Indicates overfitting to adversarial examples
  - High robustness on SA1B but poor on COCO/DAVIS: Suggests distribution shift

- **First 3 experiments:**
  1. Reproduce cross-prompt attack ASR: Take 100 images from VOC, run attack pipeline with ε=16/255, compute ASR for point-only, box-only, and cross-prompt attacks
  2. Ablate trainable parameter count: Compare defense performance when adapting 128 vs. 256 vs. 512 singular values
  3. Test transferability to SAM 2: Apply cross-prompt attack (trained on SAM) directly to SAM 2 on video frames

## Open Questions the Paper Calls Out

### Open Question 1
Can the proposed SVD-based defense be effectively extended to SAM 2 to robustly handle temporal consistency in video segmentation? The defense evaluation is restricted to image segmentation datasets on the original SAM, while SAM 2 relies on temporal memory attention mechanisms that may require different parameter constraints.

### Open Question 2
Does the cross-prompt attack methodology transfer to semantic prompt modalities, such as text or mask inputs? The paper focuses exclusively on point and box prompts, and it's unclear if the identified feature channels overlap with those activated by text embeddings or dense mask prompts.

### Open Question 3
Would applying SVD adaptation to the Transformer attention blocks yield superior robustness compared to adapting only convolutional layers? The authors select convolutional layers but don't justify excluding Transformer components, which process global interactions and might correct feature perturbations more effectively.

## Limitations
- Cross-prompt attack transferability claims lack quantitative validation for SAM 2
- Defense performance improvements primarily demonstrated on VOC, with potential generalization limitations to COCO and DAVIS
- Attack success rate metric (mIoU < 0.5 for both prompt types) is binary and may not capture partial segmentation failures

## Confidence
- **High Confidence:** Basic attack/defense framework implementation and parameter counts (512 trainable parameters via SVD)
- **Medium Confidence:** Claim of 15% mIoU improvement against adversarial attacks on VOC dataset
- **Low Confidence:** Cross-prompt attack transferability to SAM 2 and generalization to other datasets beyond VOC

## Next Checks
1. Apply the cross-prompt attack (trained on SAM) directly to SAM 2 on a held-out video dataset. Measure ASR and compare against single-prompt attacks to verify transferability claims.

2. Test the defense method on COCO and DAVIS datasets. Report mIoU improvements and compare against the VOC results to quantify generalization limitations.

3. Compare the SVD-based defense against alternative few-parameter methods like weight normalization or low-rank adaptation. Evaluate both robustness gains and clean performance impact.