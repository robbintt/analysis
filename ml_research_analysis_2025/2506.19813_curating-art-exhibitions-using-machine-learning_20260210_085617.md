---
ver: rpa2
title: Curating art exhibitions using machine learning
arxiv_id: '2506.19813'
source_url: https://arxiv.org/abs/2506.19813
tags:
- paintings
- artworks
- european
- museum
- canvas
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the application of machine learning techniques
  to automate the curation of art exhibitions. The research proposes four distinct
  models that learn from existing exhibitions curated by human experts to replicate
  similar curatorial decisions.
---

# Curating art exhibitions using machine learning

## Quick Facts
- arXiv ID: 2506.19813
- Source URL: https://arxiv.org/abs/2506.19813
- Reference count: 16
- Primary result: Machine learning models can achieve reasonable accuracy in art exhibition curation, with engineered features performing comparably to large language models

## Executive Summary
This study investigates how machine learning can automate the curation of art exhibitions by learning from human-curated examples. The research develops four distinct models that use different approaches to replicate curatorial decisions, ranging from text vectorization techniques to OpenAI embeddings and direct neural network architectures. The findings demonstrate that carefully engineered features and neural network designs can achieve performance comparable to large language models like GPT, while offering greater resource efficiency. The work provides evidence that AI can meaningfully assist in the complex task of art curation.

## Method Summary
The study employs four machine learning models trained on existing art exhibitions curated by human experts. The models use different approaches: self-contained text vectorization of artwork metadata, OpenAI-based embeddings, direct neural network embedding output to artwork metadata, and a full OpenAI approach. Each model learns to predict curatorial decisions by analyzing patterns in how artworks were selected and arranged in historical exhibitions. The models are evaluated based on their ability to replicate these curatorial choices, with accuracy measured against the original human-curated selections.

## Key Results
- Models achieved reasonable accuracy in selecting artworks and metadata for exhibitions
- Some models outperformed random selection by significant margins
- Engineered features and carefully designed neural network architectures achieved performance comparable to large language models like GPT
- The full OpenAI approach demonstrated strong performance but required substantially more computational resources

## Why This Works (Mechanism)
The machine learning models succeed in art curation by learning patterns and relationships from existing human-curated exhibitions. The models identify correlations between artwork characteristics, metadata attributes, and successful curatorial arrangements. By analyzing large numbers of curated exhibitions, the algorithms develop an understanding of what combinations of artworks and metadata typically work well together. The success of engineered features demonstrates that domain-specific knowledge and careful feature design can capture essential curatorial patterns without requiring massive computational resources.

## Foundational Learning
- Text vectorization and embeddings (why needed: to convert artwork metadata into numerical representations the models can process; quick check: verify embeddings preserve semantic relationships between artworks)
- Neural network architecture design (why needed: to create models that can learn complex patterns in curatorial decisions; quick check: test model performance with different architectural configurations)
- Feature engineering principles (why needed: to create meaningful numerical representations of curatorial concepts; quick check: validate that engineered features correlate with human curation decisions)
- Evaluation metrics for curatorial tasks (why needed: to measure model performance in a way that reflects curatorial quality; quick check: compare model selections against expert human evaluations)

## Architecture Onboarding

Component Map:
Text Preprocessing -> Feature Engineering -> Model Training -> Evaluation -> Deployment

Critical Path:
The critical path runs through feature engineering to model training, as the quality of engineered features directly impacts model performance. Poor feature engineering will limit even sophisticated model architectures, while good features can enable simpler models to perform well.

Design Tradeoffs:
The study balances model complexity against computational efficiency. While large language models like GPT offer strong performance, they require substantial resources. The engineered feature approaches achieve comparable results with significantly less computational overhead, making them more practical for real-world deployment. The tradeoff involves sacrificing some contextual understanding for efficiency and scalability.

Failure Signatures:
Models may fail when encountering exhibition themes or art styles not represented in the training data. Poor feature engineering can lead to models that miss important curatorial patterns or make decisions based on spurious correlations. Overfitting to specific curatorial styles in the training data can result in models that perform well on similar exhibitions but poorly on diverse or novel themes.

First Experiments:
1. Test feature engineering approaches on a held-out subset of the training data to validate their effectiveness
2. Compare model performance across different exhibition themes to identify potential biases or limitations
3. Evaluate model decisions against human curator assessments to validate qualitative aspects of curation

## Open Questions the Paper Calls Out
The study does not explicitly identify open questions, but several areas remain unexplored. The generalizability of these approaches to different cultural contexts and exhibition styles requires investigation. The long-term effectiveness of AI-curated exhibitions in terms of visitor engagement and satisfaction has not been assessed. The potential for these models to develop novel curatorial approaches beyond what they learned from human examples remains an open area for research.

## Limitations
- Limited generalizability to exhibitions with different styles, themes, or cultural contexts not represented in training data
- Evaluation metrics focus on selection accuracy rather than qualitative aspects like thematic coherence or visitor engagement
- Comparison with large language models may not fully capture the nuanced decision-making capabilities of human curators

## Confidence
- Core findings: Medium (methodology sound but limited by dataset scope)
- Model performance claims: Medium (results demonstrated but evaluation framework has limitations)
- Resource efficiency claims: Medium (comparative analysis performed but broader applicability uncertain)

## Next Checks
1. Test the models on independent exhibition datasets from different museums, time periods, and cultural contexts to assess generalizability
2. Conduct blind evaluations where human curators compare AI-selected exhibitions against human-curated ones, measuring both accuracy and qualitative aspects
3. Implement A/B testing in actual museum settings, comparing visitor engagement and satisfaction between AI-assisted and traditional curation approaches