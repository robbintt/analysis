---
ver: rpa2
title: 'Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes'
arxiv_id: '2511.01096'
source_url: https://arxiv.org/abs/2511.01096
tags:
- event
- hawkes
- events
- processes
- process
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes the Hyper Hawkes Process (HHP), a new family
  of marked temporal point process models that combine strong predictive performance
  with interpretable structure. HHP extends classical Hawkes processes by lifting
  dynamics into a latent space and using a hypernetwork to generate time- and history-dependent
  dynamics, while retaining the linear recurrence that enables event-level attribution.
---

# Hyper Hawkes Processes: Interpretable Models of Marked Temporal Point Processes

## Quick Facts
- arXiv ID: 2511.01096
- Source URL: https://arxiv.org/abs/2511.01096
- Reference count: 40
- Primary result: Combines strong predictive performance with interpretable structure for marked temporal point processes

## Executive Summary
This paper introduces the Hyper Hawkes Process (HHP), a novel family of marked temporal point process models that bridge the gap between predictive accuracy and interpretability. HHP extends classical Hawkes processes by incorporating a latent space and hypernetwork architecture that generates time- and history-dependent dynamics while preserving the linear recurrence property essential for interpretability. The model achieves state-of-the-art or near state-of-the-art performance across seven real-world datasets while maintaining the ability to attribute predictions to individual past events.

## Method Summary
HHP operates by lifting the temporal dynamics into a latent space through a hypernetwork that conditions the intensity function on both the current time and the complete event history. The model retains the linear recurrence property of classical Hawkes processes, enabling direct attribution of predictions to specific past events. This is achieved through a carefully designed architecture where the base intensity and kernel functions are modulated by learned latent representations, allowing for complex, history-dependent dynamics while maintaining computational tractability for interpretability analysis.

## Key Results
- Achieves state-of-the-art log-likelihood performance across seven real-world datasets
- Outperforms both classical Hawkes baselines and neural temporal point process models in RMSE and mark prediction accuracy
- Demonstrates superior interpretability through event-level attribution using leave-one-out estimators and particle-based analysis
- Shows robust performance across diverse domains including social media, healthcare, and financial applications

## Why This Works (Mechanism)
HHP works by combining the expressive power of neural networks with the structural properties of classical Hawkes processes. The hypernetwork architecture allows the model to learn complex, history-dependent intensity functions while the latent space lifting enables rich temporal dynamics. The key innovation is maintaining linear recurrence in this expanded space, which preserves the mathematical structure necessary for event-level attribution. This design allows HHP to capture non-linear dependencies and long-range interactions that classical Hawkes processes cannot, while neural models cannot easily attribute to specific past events.

## Foundational Learning
1. **Temporal Point Processes**: Stochastic processes where events occur at irregular time points - needed for modeling discrete events over continuous time; quick check: verify events occur at specific timestamps with no simultaneous occurrences
2. **Hawkes Processes**: Self-exciting point processes where past events increase future intensity - needed for capturing temporal dependencies; quick check: confirm intensity increases after event occurrences
3. **Marked Point Processes**: Extension where events have associated marks or labels - needed for multi-dimensional event characterization; quick check: verify each event has associated categorical/continuous mark
4. **Linear Recurrence**: Property where intensity depends linearly on past events - needed for tractable inference and interpretability; quick check: confirm intensity calculation scales linearly with event history
5. **Hypernetworks**: Neural networks that generate parameters for other networks - needed for adaptive, history-dependent dynamics; quick check: verify hypernetwork outputs valid model parameters

## Architecture Onboarding

**Component Map**: Input Time/History -> Hypernetwork -> Latent Space -> Intensity Function -> Predictions

**Critical Path**: Time t + Event History H -> Hypernetwork φ -> Latent Representation z -> Intensity λ(t|H) -> Event Prediction

**Design Tradeoffs**: 
- Flexibility vs interpretability: neural components provide expressivity while linear recurrence enables attribution
- Computational cost vs accuracy: full history processing enables rich dependencies but increases complexity
- Model complexity vs generalization: additional parameters improve fit but risk overfitting

**Failure Signatures**:
- Performance degradation on very long sequences due to linear scaling
- Interpretability breakdown when latent space becomes too complex
- Training instability from hypernetwork parameter coupling

**First Experiments**:
1. Train on synthetic Hawkes data with known ground truth to verify basic functionality
2. Ablation study removing hypernetwork to test necessity of latent space
3. Test leave-one-out attribution on controlled synthetic data with clear event dependencies

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Computational complexity scales linearly with event history length, potentially limiting application to very long sequences
- Particle-based interpretability methods rely on heuristic sampling that may miss important counterfactual scenarios
- Hypernetwork introduces additional hyperparameters requiring careful tuning for stable training

## Confidence

**High Confidence**: Empirical performance improvements are well-supported by comprehensive experiments across seven diverse datasets, with clear advantages in log-likelihood, RMSE, and mark prediction accuracy over multiple baseline approaches.

**Medium Confidence**: Interpretability claims are promising but depend on specific analytical tools whose effectiveness across all domains requires further validation, particularly in complex real-world scenarios.

**Low Confidence**: Generalizability to extreme conditions (millions of events, highly imbalanced marks, or extreme temporal irregularity) remains untested based on the moderate-sized, relatively balanced datasets used in current experiments.

## Next Checks
1. **Scalability Benchmark**: Evaluate HHP's runtime and accuracy on datasets with 100K+ events, comparing against truncated methods and approximate inference techniques
2. **Cross-Domain Robustness**: Test HHP on datasets with extreme mark imbalance (>100:1) and highly irregular temporal patterns to assess robustness beyond balanced datasets
3. **Interpretability Stress Test**: Apply leave-one-out and particle-based analysis to synthetic data with known mechanisms, then systematically perturb data to evaluate how well interpretability tools recover true dynamics under varying conditions