---
ver: rpa2
title: The Real Barrier to LLM Agent Usability is Agentic ROI
arxiv_id: '2505.17767'
source_url: https://arxiv.org/abs/2505.17767
tags:
- agents
- agent
- agentic
- time
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLM agents are technically capable but underused in mass-market\
  \ applications. The paper argues that usability depends on Agentic ROI\u2014a measure\
  \ of the information gain and time savings an agent provides relative to its cost."
---

# The Real Barrier to LLM Agent Usability is Agentic ROI

## Quick Facts
- arXiv ID: 2505.17767
- Source URL: https://arxiv.org/abs/2505.17767
- Authors: Weiwen Liu; Jiarui Qin; Xu Huang; Xingshan Zeng; Yunjia Xi; Jianghao Lin; Chuhan Wu; Yasheng Wang; Lifeng Shang; Ruiming Tang; Defu Lian; Yong Yu; Weinan Zhang
- Reference count: 21
- One-line primary result: LLM agents achieve high usability in coding and research domains but fail in mass-market applications due to negative Agentic ROI in low-complexity tasks.

## Executive Summary
LLM agents are technically capable but face fundamental usability barriers in mass-market applications. The paper introduces "Agentic ROI" as the key metric, measuring the ratio of information gain and time savings to total cost. Empirical surveys across five domains reveal that usability strongly correlates with Agentic ROI (r=0.95), with high-ROI domains like coding achieving strong benefits while low-ROI domains like e-commerce see negligible value due to high prompting overhead relative to baseline task times.

## Method Summary
The study surveyed 34 participants (14 AI practitioners, 20 end-users) across five domains using an online questionnaire. Participants rated information gain on a 10-point scale, reported time savings in minutes, and rated usability on a 10-point scale with free-text explanations. Scores were averaged per domain and normalized to [0,1]. Cost was estimated by dividing monthly subscription fees by maximum allowable monthly tasks, also normalized. Agentic ROI was calculated as (Information Gain × Time Savings) / Cost.

## Key Results
- Agentic ROI strongly correlates with usability ratings (r=0.95) across five domains
- High-ROI domains (coding, research) achieve strong time savings and quality gains
- Low-ROI domains (office work, e-commerce) show negligible benefit due to high prompting overhead
- Proposed zigzag development trajectory: scale up capability first, then scale down costs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Usability depends on value gained relative to interaction cost, not raw capability
- **Mechanism:** Agentic ROI = (Information Gain × Time Savings) / Cost. Usability fails when agent interaction time approaches baseline human time. In fast tasks like e-commerce, any agent overhead creates negative ROI.
- **Core assumption:** Users abandon systems introducing friction even with high output quality
- **Evidence anchors:**
  - [abstract]: "Agentic ROI strongly correlates with usability ratings (r=0.95)"
  - [section]: Section 3.2 notes low-ROI domains where "time required to instruct... can paradoxically exceed... manual execution"
  - [corpus]: *The Cost of Dynamic Reasoning* highlights agentic workflows impose significant operational costs

### Mechanism 2
- **Claim:** Sleep-time compute increases Information Gain without degrading Time Savings
- **Mechanism:** Decoupling offline user understanding from online serving allows pre-computation during idle periods, improving quality without increasing interaction latency
- **Core assumption:** User context remains stable between offline computation and next query
- **Evidence anchors:**
  - [section]: Section 4.1.1 proposes sleep-time compute "decouples task completion into two phases"
  - [abstract]: "Scaling up to improve information gain... via sleep-time compute"
  - [corpus]: *Agentic Reasoning* papers support decoupled planning phases

### Mechanism 3
- **Claim:** Zigzag trajectory (scale up then scale down) maximizes ROI
- **Mechanism:** After achieving capability threshold, reduce Cost through memory retrieval and specialized models rather than expensive real-time reasoning
- **Core assumption:** Users satisfied with quality make cost and speed primary adoption factors
- **Evidence anchors:**
  - [section]: Section 4.2 suggests "shifting from compute-bound inference to memory retrieval... reduces latency and cost"
  - [abstract]: "Scaling down to reduce cost through memory retrieval, specialized models"
  - [corpus]: *STRIDE* advocates selecting simplest effective AI modality to avoid over-engineering

## Foundational Learning

- **Concept: Baseline Human Time ($T_0$)**
  - **Why needed here:** Denominator in time savings calculation; measures current human task speed
  - **Quick check question:** "Does the target task currently take 5 seconds (low $T_0$) or 5 hours (high $T_0$)?"

- **Concept: Agent Interaction Time ($T_{Agent}$)**
  - **Why needed here:** "Hidden tax" of agentic systems including prompting, clarifying, correcting
  - **Quick check question:** "How much time is spent managing the agent vs. enjoying the result?"

- **Concept: Sleep-time Compute**
  - **Why needed here:** Architectural solution to latency/cost trap; distinguishes async thinking from instant interaction
  - **Quick check question:** "Can we pre-calculate needs while users are idle, or must we compute only when they hit enter?"

## Architecture Onboarding

- **Component map:** User History & Context -> Sleep-time Processor (offline analysis, simulations) -> Online Agent (real-time execution with specialized/generalist models) -> Evaluator (Agentic ROI Monitor)

- **Critical path:**
  1. Measure human baseline ($T_0$ and $Q_0$) for target domain
  2. Implement Sleep-time processor to pre-compute context and reduce $T_{Agent}$
  3. Route simple queries to specialized/memory-based models to reduce Cost

- **Design tradeoffs:**
  - Generalist vs. Specialized: High capability/high cost vs. low cost/narrow scope
  - Proactive vs. Reactive: Reduces $T_{Agent}$ but risks wrong intent inference
  - Latency vs. Quality: More compute improves quality but may reduce time savings

- **Failure signatures:**
  - "Paradox" Failure: Agent execution time > Manual execution time (Low ROI)
  - Stale Memory: Sleep-time compute contradicts current real-time context
  - ROI Blindness: Optimizing benchmark accuracy while token cost makes service unviable

- **First 3 experiments:**
  1. **ROI Baseline Audit:** Measure $T_0$ vs. $T_{Agent}$ for 3 workflows (calendar, code, shopping); calculate current ROI
  2. **Sleep-time Simulation:** Implement background job analyzing logs to predict next query; measure if pre-loading reduces $T_{Agent}$ or improves $Q_{Agent}$
  3. **Cost/Quality Regression:** Route 50% traffic to smaller specialized model; compare ROI (Quality drop vs. Cost drop) against generalist

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the research community develop standardized benchmarks and datasets that accurately estimate Agentic ROI across diverse tasks, domains, and user populations?
- **Basis in paper:** [explicit] Authors call for "collaboration on shared datasets, benchmarks, and tools for estimating Agentic ROI across tasks, domains, and user populations" in Section 5
- **Why unresolved:** Current benchmarks focus on raw capability rather than utility-driven metrics like information gain and time savings relative to cost; no standardized framework exists for personalized ROI evaluation
- **What evidence would resolve it:** Shared evaluation framework quantifying $T_0$, $Q_0$, and cost across user expertise levels, validated against real-world usability studies

### Open Question 2
- **Question:** To what extent can sleep-time compute perform deep, multi-hop reasoning over user history to reduce interaction time without prohibitive offline computational costs?
- **Basis in paper:** [inferred] Section 4.1.1 proposes scaling sleep-time compute to infer latent user intents, but current implementations (e.g., OpenAI Pulse) are limited to summarization
- **Why unresolved:** Unclear if computational expense of continuous simulations yields net positive ROI compared to cost savings during online interaction
- **What evidence would resolve it:** Comparative studies showing deep sleep-time reasoning achieves higher user satisfaction or lower $T_{Agent}$ with total cost maintaining competitive Agentic ROI

### Open Question 3
- **Question:** Can scaling down phase (model distillation, specialization) sufficiently lower costs to make LLM agents viable in low-ROI domains like e-commerce and office work?
- **Basis in paper:** [inferred] Paper identifies "usability gap" where high-demand domains have low ROI due to minimal baseline human time; proposes scaling down trajectory but doesn't demonstrate cost reduction alone overcomes low information gain
- **Why unresolved:** In low-ROI domains, $T_{Agent}$ often exceeds $T_0$; uncertain if cheaper specialized models can improve instruction speed/accuracy enough to flip ROI positive
- **What evidence would resolve it:** Deployment data from specialized, low-cost agents in mass-market tasks showing positive Agentic ROI curve driven by cost efficiency and reduced interaction friction

## Limitations
- Small-scale survey (N=34) may not generalize across broader populations or task variations
- Agentic ROI metric depends on subjective self-reporting rather than objective measurements
- Cost estimation uses rough subscription-based proxies rather than actual usage-based billing data
- Zigzag trajectory is theoretically sound but lacks empirical validation beyond survey correlations

## Confidence

- **High Confidence:** Correlation between Agentic ROI and usability ratings (r=0.95) well-supported by survey data; observation that low-ROI domains show negligible benefit due to high prompting overhead is clearly documented
- **Medium Confidence:** Mechanism linking sleep-time compute to improved ROI through decoupled reasoning is logically sound but relies on unstated assumptions about data stability; generalizability of five-domain analysis to other task categories remains uncertain
- **Low Confidence:** Proposed zigzag development trajectory is presented as prescriptive framework without empirical evidence of effectiveness in actual agent development cycles

## Next Checks

1. **Objective ROI Measurement:** Conduct controlled experiments measuring actual task completion times and output quality in coding and research domains, comparing human baseline to agent performance with precise time and quality metrics rather than self-reported estimates

2. **Cross-Domain Scalability Test:** Apply Agentic ROI framework to additional domains (healthcare triage, financial planning, legal document review) to test whether observed correlation holds beyond original five domains and whether zigzag trajectory emerges empirically

3. **Cost Sensitivity Analysis:** Replace subscription-based cost estimates with actual token-usage billing data from multiple LLM providers across different model sizes, testing whether proposed scaling-down benefits (specialization, memory retrieval) actually reduce operational costs in practice