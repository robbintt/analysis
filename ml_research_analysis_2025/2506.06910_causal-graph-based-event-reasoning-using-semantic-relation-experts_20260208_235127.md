---
ver: rpa2
title: Causal Graph based Event Reasoning using Semantic Relation Experts
arxiv_id: '2506.06910'
source_url: https://arxiv.org/abs/2506.06910
tags:
- causal
- event
- events
- pairs
- pair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper tackles the challenge of event reasoning in NLP, where\
  \ identifying causal connections between events remains difficult for large language\
  \ models (LLMs), impacting tasks like event forecasting and timeline understanding.\
  \ To address this, the authors propose a collaborative approach to generate causal\
  \ event graphs, where LLM-based \"experts\" focus on specific semantic relations\u2014\
  temporal, discourse, pre-condition, and commonsense\u2014and engage in multiple\
  \ rounds of discussion to reach a consensus on the global causal structure."
---

# Causal Graph based Event Reasoning using Semantic Relation Experts

## Quick Facts
- **arXiv ID**: 2506.06910
- **Source URL**: https://arxiv.org/abs/2506.06910
- **Reference count**: 40
- **Primary result**: Collaborative LLM-based experts outperform baselines in generating causal event graphs and downstream event reasoning tasks

## Executive Summary
This paper addresses the challenge of identifying causal connections between events in natural language, a task where current LLMs struggle due to the complexity of integrating multiple semantic threads (temporal, discourse, preconditions, commonsense). The authors propose a novel approach using specialized LLM "experts" for different semantic relations that collaborate through multi-round discussions to generate coherent causal graphs. These graphs are then used for downstream tasks like event likelihood prediction and forecasting, achieving competitive results without fine-tuning while providing explainable predictions.

## Method Summary
The approach uses a multi-agent collaboration system where four specialized LLM-based experts (Temporal, Discourse, Precondition, Commonsense) independently identify causal links based on their semantic focus, then engage in up to three rounds of discussion to refine and converge on a global causal structure. A final judge LLM synthesizes the outputs into a consolidated causal graph. For downstream tasks, events are inserted into the graph to assess likelihood, with the insertion path serving as explanation. The method uses GPT-4o and Llama-70B without fine-tuning, relying on carefully crafted prompts for each agent.

## Key Results
- Collaborative expert approach outperforms direct prompting and pairwise baselines on CRAB dataset (Graph-level Macro F1: 69.0% vs 59.3% for direct baseline)
- Shows competitive performance on downstream tasks (ForecastQA, EEL, next event prediction) without fine-tuning
- Generates informative and coherent explanations for event likelihood predictions
- Achieves higher precision in identifying non-causal pairs compared to baselines

## Why This Works (Mechanism)

### Mechanism 1: Semantic Specialization with Collaborative Consensus
Decomposing causal graph generation into specialized semantic perspectives improves accuracy compared to monolithic prompting. Four LLM-based "experts" independently identify causal links based on their specific semantic relation (temporal, discourse, precondition, commonsense), then engage in multi-round discussions to challenge, revise, and converge on a global causal structure. A final "judge" expert synthesizes the final graph. This assumes causality is multi-faceted and different semantic relations provide complementary and sometimes corrective evidence for causal links.

### Mechanism 2: Graph-Based Explanations for Downstream Reasoning
Explicit causal graphs serve as an effective intermediate representation for improving performance on event reasoning tasks like forecasting and likelihood prediction. A causal graph is first generated, then for a query event the model attempts to "insert" it into the graph by identifying its causes and effects. If the event can be coherently placed, it is deemed "likely." The path through the graph containing the event serves as an explanation. This assumes event likelihood is primarily determined by its causal fit with other observed events.

### Mechanism 3: Transitivity-Aware Global Context
Generating causal links with global context and transitivity constraints reduces incoherent predictions. The system processes all events from a given context together, with experts prompted to consider the "global causal structure" and avoid contradictions. The graph is built to enforce transitivity (if A causes B and B causes C, then A causes C), which is noted to be an issue in simpler pairwise approaches. This assumes causal relations in a narrative are coherent and transitive, and that local pairwise predictions often lack this global consistency.

## Foundational Learning

- **Causal Graphs (Directed Acyclic Graphs - DAGs)**: Core data structure representing events as nodes and causal relations as directed edges. Why needed: This is the foundation for representing and reasoning about event causality.
  - Quick check: What is the problem if the model generates a cycle in the causal graph (e.g., Event A causes Event B, and Event B causes Event A)?

- **Semantic Relations (Temporal, Discourse, etc.)**: Understanding what each expert focuses on. Why needed: Each expert's specialization (temporal precedence, entity sharing, conditional necessity, implicit knowledge) provides complementary evidence for causal links.
  - Quick check: Which expert would be most useful for identifying that "The city faced a resource shortage" enabled "local officials requested help"?

- **LLM-based Multi-Agent Collaboration**: The "mechanism" relies on a specific agentic workflow. Why needed: Understanding how experts pass messages, what they share, and how a "judge" resolves disputes is crucial for implementation.
  - Quick check: In the paper's setup, does the "judge" have access to the full debate history or only the final sets of causal links from each expert?

## Architecture Onboarding

- **Component map**: Event Extractor -> Expert Agents Pool (Temporal, Discourse, Precondition, Commonsense) -> Collaboration Loop -> Judge Agent -> Downstream Task Solver

- **Critical path**: The accuracy of the final causal graph is the primary bottleneck, depending heavily on initial event extraction and quality of expert agent reasoning. If experts hallucinate or miss links, the judge cannot recover them.

- **Design tradeoffs**:
  - **Accuracy vs. Cost**: More rounds of expert collaboration and detailed prompts improve graph quality but dramatically increase token usage and cost (Debate approach costs ~30x more than Direct)
  - **Consistency vs. Complexity**: Enforcing global consistency and transitivity is more computationally complex than simple pairwise prediction but yields more coherent outputs
  - **Assumption**: The paper uses GPT-4o and Llama-70B. A tradeoff is the reliance on very large, expensive proprietary models; performance on smaller, open-source models is not guaranteed

- **Failure signatures**:
  - High disagreement among experts leading to incoherent judge synthesis
  - Transitivity violations (A->B and B->C but missing A->C)
  - Causal conflation (confusing correlation with causation)
  - Misplaced query events deemed unlikely even when they should be likely

- **First 3 experiments**:
  1. **Ablation Study**: Replicate "Experts wo collab" vs "Collab with experts" on small CRAB subset to verify collaboration performance gain
  2. **Expert Role Analysis**: Manually inspect 10-20 debate traces to validate whether Temporal expert reasons about time and Commonsense expert brings outside knowledge
  3. **Downstream Task Verification**: Pick examples from EEL task, manually generate simplified causal graph, and test event insertion heuristic for explanation coherence

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the collaborative causal graph generation framework be extended to model graded judgments of causality strength rather than binary causal links? The current approach doesn't capture causality strength and modeling graded judgments could provide richer representation for reasoning.

- **Open Question 2**: How can the computational cost of the multi-agent debate approach be reduced while maintaining accuracy? The debate approach is significantly more expensive than direct or pairwise baselines, and reducing this cost is important future work.

- **Open Question 3**: Does reliance on GPT-4 for evaluating the Explainable Event Likelihood (EEL) task introduce systematic bias favoring the proposed GPT-4-based expert method? The study relies heavily on GPT-4 to judge explanations, which may exhibit bias despite similar trends found in small human validation.

## Limitations

- Reliance on LLM-generated events and causal links introduces additional layer of potential hallucination
- Performance tightly coupled to quality and scale of LLM, with significant accuracy drop using Llama-70B vs GPT-4o
- Evaluation primarily on CRAB dataset (2.7k pairs, news articles) - performance on more complex, multi-domain, or long-form narratives untested
- System is reasoning tool requiring human verification, not knowledge extraction system

## Confidence

- **High**: Multi-agent collaboration demonstrably improves performance over direct prompting baseline on CRAB dataset; ablation study provides strong evidence for contribution of each component
- **Medium**: Downstream task results are competitive with state-of-the-art models, but measured by pairwise comparison against small set of models on specific benchmarks; absolute performance and generality less certain
- **Low**: Factual accuracy and real-world applicability of generated causal graphs - outputs require human verification

## Next Checks

1. **Expert Role Validation**: Manually inspect 20-30 debate traces from collaboration loop, categorizing reasoning provided by each expert to validate semantic specialization functionality

2. **Transitivity Enforcement Audit**: Log all transitive triples (A->B, B->C implies A->C) identified by judge and check random sample of 50 triples for correctness to verify graph utility for reasoning

3. **Small-Scale Reproduction**: Implement core pipeline (event extraction + expert collaboration + judge synthesis) on subset of 10-20 CRAB articles and measure BAcc and F1 scores to confirm mechanism works as described