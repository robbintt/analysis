---
ver: rpa2
title: 'Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal
  Contexts'
arxiv_id: '2510.14351'
source_url: https://arxiv.org/abs/2510.14351
tags:
- character
- dilemma
- reasoning
- moral
- hero
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces Beyond One World, a benchmark for evaluating
  how well large language models role-play character-specific versions of superheroes
  across multiple timelines and moral contexts. The dataset spans 30 heroes and 90
  canon-specific versions, with two tasks: Canon Events (testing factual recall) and
  Moral Dilemmas (testing ethical reasoning).'
---

# Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts

## Quick Facts
- arXiv ID: 2510.14351
- Source URL: https://arxiv.org/abs/2510.14351
- Reference count: 20
- This work introduces Beyond One World, a benchmark for evaluating how well large language models role-play character-specific versions of superheroes across multiple timelines and moral contexts.

## Executive Summary
This work introduces Beyond One World, a benchmark for evaluating how well large language models role-play character-specific versions of superheroes across multiple timelines and moral contexts. The dataset spans 30 heroes and 90 canon-specific versions, with two tasks: Canon Events (testing factual recall) and Moral Dilemmas (testing ethical reasoning). Models are scored on canonical accuracy, reasoning fidelity, and alignment between internal reasoning ("thinking") and outward behavior ("acting"). Experiments show that chain-of-thought prompting can improve coherence for weaker models but may reduce accuracy for stronger ones, and that cross-version generalization within a character remains difficult. The benchmark reveals that models often excel at either thinking or acting, but rarely both, exposing gaps in multiversal consistency and trustworthy role-play.

## Method Summary
Beyond One World evaluates LLMs on version-specific character role-play across two tasks: Canon Events (MCQ factual recall) and Moral Dilemmas (binary ethical choices). The benchmark uses 30 heroes with 90 versions across childhood/pre-hero/hero phases, totaling 1,346 MCQs and 1,080 dilemmas. Models generate responses with or without chain-of-thought, then GPT-4o-mini segments outputs into <thinking>/<acting> spans. Sonnet 3.7 judges fidelity using character attributes, and all-mpnet-base-v2 computes cosine similarity for Think-Act Matching. The benchmark tests 7 models (GPT-4o-mini, Gemini 2.0-Flash, Gemini-2.5-Flash-Thinking, Sonnet 3.5/3.7, DeepSeek r1/v3) at temperature 0.6.

## Key Results
- Chain-of-thought prompting improves narrative coherence for weaker models but reduces canonical accuracy for stronger ones
- Cross-version generalization within a character remains a major obstacle, with substantial accuracy drops
- Models often excel at either thinking or acting, but rarely both, exposing gaps in multiversal consistency

## Why This Works (Mechanism)

### Mechanism 1: Differential CoT Effects by Model Capability
- Claim: Chain-of-thought prompting improves narrative coherence for weaker models but can reduce canonical accuracy for stronger models.
- Mechanism: Explicit reasoning steps in stronger models appear to trigger over-generation or off-canon elaborations that degrade factual consistency, while weaker models benefit from structured reasoning scaffolds that constrain their outputs.
- Core assumption: Stronger models have sufficient parametric knowledge that CoT reasoning distracts or diverges from stored canon.
- Evidence anchors: [abstract] "chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones"; [section 5.1, Table 3] 4o-mini gains +0.020 accuracy with CoT while sonnet3.5 drops -0.053; gemini2.5-flash-think drops dramatically (-0.275); [section 5.4] "CoT improves coherence for weaker models but can harm stronger ones by over-generating or straying from canon"
- Break condition: If models are fine-tuned on canon-specific corpora with CoT supervision, the negative effect on stronger models may diminish.

### Mechanism 2: Think-Act Alignment as Trustworthiness Proxy
- Claim: Separating internal deliberation ("thinking") from outward decisions ("acting") and measuring their alignment provides a proxy for role-play trustworthiness.
- Mechanism: Embedding-based cosine similarity between tagged `<thinking>` and `<acting>` spans quantifies whether a model's internal reasoning justifies its external behavior. Higher alignment suggests coherent character portrayal even if factual accuracy varies.
- Core assumption: Semantic similarity between thinking and acting spans captures behavioral consistency; low similarity indicates post-hoc rationalization or disconnected decision-making.
- Evidence anchors: [abstract] "Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness"; [section 4.2] Describes pipeline: GPT-4o-mini segments responses, Sonnet 3.7 judges, all-mpnet-base-v2 computes cosine similarity; [section 5.3, Table 3] r1 shows largest CoT improvement in cosim (+0.075) despite accuracy drop, suggesting improved reasoning-action coherence; [corpus] Related work on psychological alignment (PsyMem) suggests fine-grained character modeling improves consistency, but does not directly validate think-act matching as a metric
- Break condition: If thinking and acting spans are artificially aligned via prompt engineering rather than genuine reasoning, the metric may overestimate trustworthiness.

### Mechanism 3: Cross-Version Generalization Failure via Timeline Conflation
- Claim: Models struggle to maintain version-specific distinctions when evaluating across multiple incarnations of the same character.
- Mechanism: Models conflate overlapping information across timelines because parametric knowledge does not cleanly partition universe-specific facts. Without explicit grounding in source material metadata, models default to the most salient or frequently occurring version.
- Core assumption: Cross-version failure reflects inadequate contextual anchoring rather than insufficient model capacity.
- Evidence anchors: [abstract] "cross-version generalization within a character remains a major obstacle"; [section 5.2, Figure 3] Cross-character accuracy substantially lower than within-character; gemini2.5-flash-think shows steepest drop (0.388 dilemma cross); [section 5.4] "multiversal coherence is not trivially solved by scale or reasoning, but requires models to anchor their decisions in fine-grained contextual cues"; [corpus] TimeChara (Ahn et al., 2024) shows similar point-in-time hallucination issues; corpus evidence weak for specific mechanism of timeline conflation
- Break condition: If models are provided explicit version-specific retrieval context (RAG), cross-version generalization may improve substantially.

## Foundational Learning

- Concept: **Persona-grounded dialogue conditioning**
  - Why needed here: The benchmark assumes models can adopt character personas via prompt-based conditioning. Without understanding how persona prompts constrain generation, the think-act separation results are uninterpretable.
  - Quick check question: Can you explain why adding "You are playing the role of Tony Stark" to a prompt changes model behavior beyond surface-level keyword matching?

- Concept: **Embedding-based semantic similarity**
  - Why needed here: The Think-Act Matching metric relies on cosine similarity between sentence embeddings. Understanding what this captures (and fails to capture) is essential for interpreting trustworthiness scores.
  - Quick check question: If two texts have high cosine similarity but express contradictory intentions, would the Think-Act Matching metric detect the inconsistency?

- Concept: **Hallucination in knowledge-intensive generation**
  - Why needed here: Canon Events task directly tests whether models hallucinate off-canon facts when prompted with CoT. Understanding hallucination mechanisms is prerequisite for interpreting why stronger models degrade with CoT.
  - Quick check question: When a model generates confident but incorrect facts about a character's backstory, is this more likely retrieval failure or generation-time confabulation?

## Architecture Onboarding

- Component map: Prompting layer -> Inference layer -> Segmentation layer -> Judging layer -> Alignment layer
- Critical path: Prompt construction → Model inference → Response segmentation → LLM-as-judge scoring → Embedding-based alignment calculation
- Design tradeoffs: Using GPT-4o-mini for segmentation trades accuracy for cost; may misclassify thinking/acting boundaries; LLM-as-judge with character attributes introduces subjectivity but enables fine-grained personality alignment assessment; multiple-choice format for Canon Events limits probing depth but enables scalable evaluation
- Failure signatures: Low cosim + high accuracy: Model gives correct answers but with incoherent internal reasoning (trustworthiness risk); High cosim + low accuracy: Model produces consistent but wrong characterizations (hallucination with confidence); Sharp cross-character accuracy drop: Model failing to distinguish version-specific context (timeline conflation)
- First 3 experiments:
  1. Baseline CoT ablation: Run Canon Events and Dilemmas with and without CoT across all 7 models to replicate the differential effect pattern; document which models show accuracy gains vs. losses
  2. Cross-version transfer probe: Take Spider-Man variants (CID 4, 5, 6) and systematically evaluate each on questions from other versions; measure conflation patterns by error type
  3. Think-Act boundary validation: Manually inspect 50 segmented responses to verify GPT-4o-mini classification accuracy; if >15% misclassification, replace with human annotation or alternative segmentation approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit structured world knowledge injection (e.g., knowledge graphs or retrieval-augmented canon databases) improve cross-version generalization within the same character across different timelines?
- Basis in paper: [explicit] The paper states "cross-version generalization within a character remains a major obstacle" and shows substantial accuracy drops in cross-character evaluation (Figure 3). The discussion suggests "future work should explore integrated reasoning–persona modeling, potentially combining structured world knowledge with dynamic narrative alignment."
- Why unresolved: Current models conflate overlapping but divergent character versions (e.g., different Spider-Man iterations), indicating they lack fine-grained mechanisms to anchor decisions to specific universe constraints.
- What evidence would resolve it: Experiments comparing baseline prompting against RAG-enhanced or knowledge-graph-grounded approaches on cross-version tasks, showing improved accuracy on the cross-character evaluation protocol.

### Open Question 2
- Question: Why does chain-of-thought prompting improve coherence for weaker models but reduce canonical accuracy for stronger models, and can this trade-off be mitigated?
- Basis in paper: [explicit] Finding (1) states "chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones." Table 3 shows sonnet3.5 drops from 0.704 to 0.651 accuracy with CoT, while 4o-mini gains +0.020.
- Why unresolved: The paper notes this "echoes prior findings on CoT's mixed effects in knowledge-intensive tasks" but does not identify the mechanism causing hallucination or off-canon elaboration in stronger models.
- What evidence would resolve it: Ablation studies varying CoT length and constraining reasoning traces to canon-only content; analysis of what types of canonical facts are most affected.

### Open Question 3
- Question: What architectural or training interventions can bridge the gap where models excel at either thinking or acting but rarely both?
- Basis in paper: [explicit] Finding (3) states "models often excel at either thinking or acting, but rarely both." Figure 4 shows gemini2 has high thinking (3.67) but lower acting, while sonnet3.7 excels at acting (3.65) with moderate thinking (3.03). The discussion notes "bridging this gap is key for trustworthy role-play agents."
- Why unresolved: The Think-Act Matching metric reveals misalignment but the paper does not propose solutions to achieve simultaneous high performance on both dimensions.
- What evidence would resolve it: Training objectives that jointly optimize reasoning fidelity and action alignment; multi-task fine-tuning with explicit think-act consistency losses evaluated on both Canon Events and Moral Dilemmas.

## Limitations

- Benchmark relies heavily on LLM-as-judge for segmentation and scoring, introducing subjectivity
- Cross-character evaluation may be confounded by prompt ambiguity when models encounter unfamiliar version-specific details
- Dataset construction process may introduce unintended correlations between phase and moral reasoning patterns

## Confidence

**High confidence**: Differential CoT effects by model capability (strong empirical support from multiple models showing consistent patterns), Canon Events accuracy trends, Think-Act Matching correlation with trustworthiness concerns.
**Medium confidence**: Cross-version generalization failure mechanism (supported by accuracy drops but mechanism attribution is inferred), the specific negative impact of CoT on stronger models (sample size limited to 7 models).
**Low confidence**: The exact scoring criteria for LLM-as-judge (incomplete specification), the generalizability of Think-Act Matching as a trustworthiness metric beyond the evaluated models.

## Next Checks

1. **Segmentation validation study**: Manually annotate 100 randomly selected responses to verify GPT-4o-mini's thinking/acting segmentation accuracy. If error rate exceeds 15%, implement human annotation for a subset of responses and compare resulting cosim distributions.
2. **Judge rubric standardization**: Develop and document a detailed scoring rubric for Sonnet 3.7 judge, including concrete examples of 0-5 scores across multiple dimensions. Test inter-judge reliability by having human experts score 50 responses using the same rubric.
3. **Cross-character generalization probe**: Conduct controlled experiments isolating timeline conflation by providing explicit version metadata in prompts. Compare cross-character accuracy with and without version-specific grounding to determine whether the failure stems from context ambiguity or fundamental model limitations.