---
ver: rpa2
title: 'Grounded Gesture Generation: Language, Motion, and Space'
arxiv_id: '2507.04522'
source_url: https://arxiv.org/abs/2507.04522
tags:
- motion
- gesture
- dataset
- generation
- referential
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal dataset and framework for grounded
  gesture generation, combining synthetic pointing gestures with real conversational
  data from VR environments. The dataset, over 7.7 hours in length, is standardized
  in the HumanML3D format and includes synchronized motion, speech, and 3D scene information.
---

# Grounded Gesture Generation: Language, Motion, and Space

## Quick Facts
- arXiv ID: 2507.04522
- Source URL: https://arxiv.org/abs/2507.04522
- Authors: Anna Deichler; Jim O'Regan; Teo Guichoux; David Johansson; Jonas Beskow
- Reference count: 38
- Key outcome: Fine-tuning OmniControl on a combined synthetic and real dataset improves spatial control and motion naturalness for grounded pointing gestures.

## Executive Summary
This paper introduces a multimodal dataset and framework for grounded gesture generation, combining synthetic pointing gestures with real conversational data from VR environments. The dataset, over 7.7 hours in length, is standardized in the HumanML3D format and includes synchronized motion, speech, and 3D scene information. The authors fine-tune OmniControl, a diffusion-based motion generation model, on this data to produce spatially controllable pointing gestures. Results show that fine-tuning consistently improves motion quality and control accuracy, with the synthetic-only model achieving the lowest errors for wrist joints. This work lays a foundation for research in situated gesture generation and grounded multimodal interaction, addressing the gap in spatially grounded, context-aware gesture synthesis.

## Method Summary
The authors combine synthetic pointing gestures (1,135 clips) with real conversational gestures from the MM-Conv VR dataset (5,115 clips) to create a 7.7-hour dataset in HumanML3D format. They fine-tune OmniControl, a diffusion-based motion generation model, on three dataset splits: synthetic only (ST), referential+synthetic (REF+ST), and all data (ALL). Fine-tuning uses a composite loss with spatial guidance to enforce joint position constraints during denoising. The model is evaluated using FID for motion naturalness and Control L2 error for spatial accuracy on pelvis and wrist joints.

## Key Results
- Fine-tuning OmniControl consistently improves motion quality and control accuracy across all dataset splits
- The synthetic-only fine-tuned model achieves the lowest wrist joint errors (Control L2 0.058-0.060)
- REF+ST split shows best balance between naturalness (FID 0.65-0.82) and full-body coordination
- Fine-tuned models outperform the base OmniControl model on both FID and Control L2 metrics

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning a diffusion-based motion model on task-specific, spatially grounded data improves joint-level control accuracy while maintaining motion naturalness. OmniControl uses a composite loss during denoising: `L_total = L_denoise + λ_spatial * L_spatial + λ_realism * L_realism`. The spatial loss `L_spatial = ‖x^j_t − g^j_t‖²` forces controlled joint positions toward target coordinates at each diffusion timestep, while the realism term preserves natural trajectories. Fine-tuning adapts the pretrained HumanML3D weights to the distribution of pointing gestures. Core assumption: Position-based joint constraints can capture the communicative intent of deictic gestures. Evidence: ST-only fine-tuned model achieves FID 0.65-0.82 and Control L2 0.058-0.060 for wrists vs. base model's 3.82-4.60 and 0.114-0.116.

### Mechanism 2
Synthetic pointing gesture data, generated via VLM-prompted utterances and aligned with motion using the Hungarian algorithm, can complement scarce real grounded gesture data. A VLM (GroundingGPT) receives rendered scene views and generates exophoric demonstrative utterances. TTS synthesizes speech with voice cloning. WhisperX extracts word-level timestamps, and the Hungarian algorithm matches utterances to motions based on demonstrative location and duration. Core assumption: VLM-generated referential expressions approximate the distribution and timing of natural demonstrative utterances in situated dialogue. Evidence: Full pipeline description; 1,406 audio samples matched to 1,135 motions; WER<0.3 filtering applied.

### Mechanism 3
Modular decoupling of perception (reference resolution) and generation (motion synthesis) improves scalability and interpretability for embodied agents. A perception module (VLM or 3D scene graph) extracts spatial cues (e.g., object centroids, bounding boxes) and projects them to world coordinates. The motion generator conditions on these cues via OmniControl's spatial guidance, producing gestures grounded to detected locations. Core assumption: Intermediate spatial representations (centroids, boxes) are sufficient for gesture grounding; end-to-end learning is not required. Evidence: "Modular systems, where perception and generation are decoupled, offer better scalability and interpretability."

## Foundational Learning

- **HumanML3D motion representation**: All dataset integration and OmniControl training depend on the 263-dimensional pose vector (root velocities, joint positions, rotations, contacts) sampled at 20 fps. Why needed: Conversion pipeline and training setup require understanding this format. Quick check: Can you explain why pelvic translation is encoded as frame-to-frame displacement while other joints are pelvis-relative?

- **Diffusion models for motion synthesis**: OmniControl is a diffusion model; understanding denoising timesteps, guidance, and loss composition is essential for fine-tuning and debugging. Why needed: The fine-tuning procedure and evaluation depend on diffusion model mechanics. Quick check: What does the spatial guidance term do at each denoising timestep?

- **Deictic gesture and exophoric reference**: The task is referential grounding—"this/that" pointing toward objects—distinct from beat or iconic co-speech gestures. Why needed: Understanding the communicative intent and spatial requirements of pointing gestures. Quick check: How does an exophoric demonstrative differ from an endophoric one, and why does that matter for spatial cue extraction?

## Architecture Onboarding

- **Component map**: [Scene/VLM] → Spatial Cue (object centroid/box) → [OmniControl + fine-tuned weights] ← [Audio/Text] → Text encoder → Conditioned motion output (SMPL-X)

- **Critical path**:
  1. Convert raw BVH/marker data → SMPL-X → HumanML3D format (Section 3.1)
  2. Generate synthetic utterances and align with pointing motions (Section 3.2)
  3. Fine-tune OmniControl on ST / REF+ST / ALL splits with spatial joint constraints
  4. Evaluate using FID (naturalness) and Control L2 (accuracy) vs. base model

- **Design tradeoffs**:
  - ST-only: Best wrist precision but limited conversational diversity
  - REF+ST: Better pelvis/full-body coordination; slight wrist accuracy drop
  - ALL: Maximum diversity; modest precision tradeoff
  - Position vs. directional loss: Current L_spatial optimizes joint position; pointing tasks may require elbow→wrist→target directional alignment (Section 5)

- **Failure signatures**:
  - Base model produces punching motion instead of pointing (sample24.mp4)
  - Fine-tuned model overfits synthetic distribution; unnatural motion on conversational references
  - Spatial cues extracted from incorrect objects; gesture points to wrong location
  - Long or complex referential utterances (>20s) filtered out; coverage gaps

- **First 3 experiments**:
  1. **Baseline replication**: Run OmniControl base vs. fine-tuned on ST split; measure FID and Control L2 for pelvis, left/right wrist. Confirm reported improvements.
  2. **Directional loss ablation**: Replace position-based L_spatial with directional alignment loss (elbow→wrist unit vector vs. wrist→referent vector). Evaluate pointing accuracy on held-out test set.
  3. **Perception robustness test**: Inject noise into spatial cue inputs (centroid jitter, wrong object selection). Measure gesture grounding error sensitivity; identify cascading failure thresholds.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can spatial loss functions be redesigned to enforce directional alignment (e.g., elbow-to-wrist vector) rather than absolute joint position matching to better capture the communicative intent of pointing? The authors note that current evaluation does not reflect desired behavior because "the goal is not to match absolute joint positions but to produce accurate pointing directions." They suggest "redefining the spatial loss in OmniControl to enforce directional alignment... rather than relying solely on positional targets."

- **Open Question 2**: What is the optimal spatial semantic representation (e.g., bounding boxes, object centroids, or 3D scene graphs) required for gesture generation to balance spatial grounding with semantic nuance? The paper asks, "does the gesture module require the same spatial granularity as humanoid locomotion, or a more semantically nuanced representation?" and proposes investigating "the effect of different spatial representations, such as bounding boxes, keypoints, or object centroids, on gesture quality."

- **Open Question 3**: Does pretraining on "socially relevant" motion subsets or integrating with multimodal conversational datasets (e.g., BEAT2) improve grounded gesture generation compared to pretraining on general-purpose motion data like AMASS? The Discussion states, "pretraining on general motion corpora like AMASS may be suboptimal for communicative behaviors. Subsampling AMASS to retain only socially relevant sequences, or integrating our data with multimodal datasets like BEAT2, could support more effective representation learning."

## Limitations
- The position-based spatial loss does not guarantee proper pointing direction, only joint position accuracy
- Reliance on synthetic data for pointing gestures raises questions about domain transfer to natural conversational contexts
- Evaluation metrics focus on local joint accuracy rather than holistic pointing effectiveness (does the gesture actually direct attention to the intended referent?)

## Confidence
- **High confidence**: The improvement in FID and Control L2 metrics from fine-tuning is well-demonstrated and reproducible
- **Medium confidence**: The claim that synthetic data can meaningfully augment real grounded gesture data is supported but lacks external validation
- **Medium confidence**: The modular perception-generation architecture is theoretically justified but not demonstrated to provide practical advantages over end-to-end approaches

## Next Checks
1. Conduct a user study evaluating whether fine-tuned gestures actually succeed at referential communication (do observers understand what is being pointed at?) beyond joint position accuracy
2. Implement and test the directional loss variant suggested in Section 5 to address the pointing direction limitation
3. Evaluate model performance on out-of-distribution scenarios, such as complex spatial scenes with multiple objects or occluded referents, to test the robustness of the perception module