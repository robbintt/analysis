---
ver: rpa2
title: Spatially Grounded Explanations in Vision Language Models for Document Visual
  Question Answering
arxiv_id: '2507.12490'
source_url: https://arxiv.org/abs/2507.12490
tags:
- document
- image
- docvqa
- https
- grid
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EaGERS introduces a training-free, model-agnostic pipeline that
  generates spatial natural language explanations via a vision language model, grounds
  these explanations to specific document regions using multimodal embeddings (BLIP,
  CLIP, ALIGN) and majority voting over a configurable grid, and re-queries the model
  on a masked image restricted to those regions to produce the final answer. This
  approach enhances transparency and reproducibility in DocVQA without requiring fine-tuning.
---

# Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering

## Quick Facts
- arXiv ID: 2507.12490
- Source URL: https://arxiv.org/abs/2507.12490
- Authors: Maximiliano Hormazábal Lagos; Héctor Cerezo-Costas; Dimosthenis Karatzas
- Reference count: 24
- Primary result: EaGERS achieves 74.50% EM and 83.31% ANLS on DocVQA validation, outperforming base model (71.17% EM, 82.90% ANLS)

## Executive Summary
EaGERS introduces a training-free, model-agnostic pipeline that generates spatial natural language explanations via a vision language model, grounds these explanations to specific document regions using multimodal embeddings (BLIP, CLIP, ALIGN) and majority voting over a configurable grid, and re-queries the model on a masked image restricted to those regions to produce the final answer. This approach enhances transparency and reproducibility in DocVQA without requiring fine-tuning. Experiments on the DocVQA dataset show that EaGERS outperforms the base model on Exact Match accuracy and Average Normalized Levenshtein Similarity, with the best configuration (50 cells grid, 15% margin) achieving 74.50% EM and 83.31% ANLS, while maintaining model performance and adding spatial interpretability.

## Method Summary
EaGERS generates spatial natural language explanations from a VLM given (image, question), then partitions the document into a grid and uses an ensemble of three multimodal embedding models (BLIP, CLIP, ALIGN) to compute cosine similarities between the explanation and each grid cell. Majority voting across the three embeddings selects the top 30% of cells most relevant to the explanation. A masking layer then blacks out non-selected regions (with optional 15% margin expansion), and the VLM is re-queried with the masked image and question to produce the final answer. This process adds spatial grounding without fine-tuning the base model.

## Key Results
- Best configuration (50-cell grid, 15% margin) achieves 74.50% EM vs base model's 71.17% EM
- ANLS improves from 82.90% to 83.31% with EaGERS
- 5×10 grid outperforms 5×5 grid (74.50% vs 64.20% EM at 0% margin)
- 15% margin expansion provides 8 percentage point EM improvement over 0% margin (66.67% vs 74.50% EM)

## Why This Works (Mechanism)

### Mechanism 1: Natural Language Explanations as Spatial Queries
VLM-generated spatial descriptions can function as semantic queries to locate relevant document regions, even when the model is not explicitly trained for grounding. The VLM receives (image, question) and produces a natural language explanation describing where to find the answer visually—not the answer itself. This explanation is embedded and compared against grid cell embeddings to identify relevant regions. Core assumption: VLMs pretrained on vision-language corpora can verbalize spatial locations with sufficient accuracy to guide region selection. Break condition: When VLM explanations reference irrelevant areas, which degrades downstream fidelity.

### Mechanism 2: Ensemble Multimodal Embedding with Majority Voting
Combining multiple pretrained vision-language embedders with majority voting reduces individual model biases and improves region selection robustness. Three embedding models (BLIP, CLIP, ALIGN) independently encode both the explanation text and each grid cell image. Cosine similarity ranks cells per embedder; majority voting across the three rankings determines final region selection (top 30% of cells). Ties resolved by average similarity. Core assumption: Different contrastive pretraining corpora produce complementary alignment biases that ensemble voting can mitigate. Break condition: When all three embedders systematically misalign explanation semantics with correct visual regions, or when inter-embedder agreement is very low.

### Mechanism 3: Spatial Masking for Constrained Re-generation
Masking non-relevant regions forces the VLM to generate answers from only validated regions, improving both accuracy and traceability. After region selection, all cells outside the selected set R are masked black (with configurable 15% margin expansion). The same VLM is re-queried with (masked_image, question) to produce the final answer, now explicitly grounded to visible regions. Core assumption: Reducing visual context to relevant regions reduces distraction and improves answer precision without losing necessary information. Break condition: When relevant content spans grid boundaries and is partially masked—mitigated but not eliminated by margin expansion.

## Foundational Learning

### Multimodal Contrastive Learning (CLIP/BLIP/ALIGN paradigm)
Why needed: The entire region selection mechanism depends on understanding how contrastive pretraining aligns text and image embeddings in shared latent space. Cosine similarity between explanation embeddings and cell embeddings is the core selection signal. Quick check: Can you explain why CLIP uses a contrastive loss that pushes paired text-image embeddings together while pushing non-paired examples apart in the same batch?

### Grid-based Spatial Partitioning and Boundary Effects
Why needed: Fixed grids introduce quantization artifacts at cell boundaries. The 15% margin is a direct response to content falling between cells. Quick check: Given a 5×10 grid over a document, what happens to a text region that spans the border between cells (3,4) and (3,5)? How does margin expansion affect this?

### Ensemble Voting and Rank Aggregation
Why needed: Majority voting across three embedder rankings is the fusion strategy. Understanding tie-breaking and rank aggregation is essential for debugging selection failures. Quick check: If BLIP ranks cell A as #1, CLIP ranks it #5, and ALIGN ranks it #3, while cell B is ranked #2 by all three—which cell wins under majority voting?

## Architecture Onboarding

### Component map
Input: (document_image, question) -> [1] VLM (Qwen2.5-VL-3B) -> spatial_explanation (text) -> [2] Grid Partitioner -> m×n cells (+ optional margin expansion) -> [3] Embedding Ensemble (BLIP, CLIP, ALIGN) -> [4] Similarity Ranker -> per-embedder cell rankings (cosine similarity) -> [5] Majority Voting -> aggregated ranking -> top-k cells (k = 30% of cells) -> [6] Masking Layer -> masked_image (non-selected cells = black) -> [7] VLM Re-query -> (masked_image, question) -> final_answer

### Critical path
1. Explanation quality: If the VLM hallucinates or references wrong regions, the entire pipeline fails downstream.
2. Embedding alignment: Cosine similarity must meaningfully correlate explanation semantics with visual content.
3. Margin configuration: 15% margin is critical for boundary content; 0% margin shows degraded performance.

### Design tradeoffs
| Decision | Option A | Option B | Tradeoff |
|----------|----------|----------|----------|
| Grid size | 25 cells (5×5) | 50 cells (5×10) | Finer granularity improves localization but increases embedding computation |
| Margin | 0% | 15% | Margin handles boundary content but may include irrelevant regions |
| k selection | Fixed 30% | Adaptive | Fixed is simpler and reproducible; adaptive could handle variable document complexity |
| Grid type | Fixed uniform | Adaptive (object detection) | Fixed simplifies implementation; adaptive handles irregular layouts better |

### Failure signatures
- Low EM/ANLS despite high region overlap: VLM explanation is spatially accurate but semantically wrong about what to extract.
- High variance in inference time: CV ranges from 20-37%—suggests document complexity variability affecting explanation generation.
- Systematic boundary misses: Relevant content consistently at cell edges → increase margin or switch to adaptive grid.
- Inter-embedder disagreement: Low α across BLIP/CLIP/ALIGN rankings → one or more embedders poorly suited to document domain.

### First 3 experiments
1. Baseline establishment: Run the base VLM (Qwen2.5-VL-3B or equivalent) directly on DocVQA validation split to measure EM/ANLS without EaGERS. Expected: ~71% EM, ~83% ANLS.
2. Grid granularity ablation: Compare 5×5 (25 cells) vs 5×10 (50 cells) with 0% margin to isolate granularity effects. Expected: 50 cells outperforms 25 cells (66.67% vs 64.20% EM).
3. Margin impact study: Within the 50-cell configuration, compare 0% vs 15% margin to quantify boundary handling benefit. Expected: 15% margin shows ~8 percentage point EM improvement (66.67% → 74.50% EM).

## Open Questions the Paper Calls Out

### Open Question 1
Would adaptive or content-aware grid partitioning (e.g., using object detectors or layout analysis) outperform fixed uniform grids for documents with irregular layouts or variable aspect ratios? Basis: Authors state fixed grid configurations may not generalize well to documents with irregular layouts and will investigate adaptive grid partitioning. Why unresolved: Only fixed 5×5 and 5×10 grids tested. What evidence: Comparative experiments on diverse document layouts using detection-based region partitioning, measuring ANLS, EM, and region-selection fidelity.

### Open Question 2
How frequently do VLM-generated spatial explanations fail to align with ground-truth answer regions, and what is the quantitative impact on final answer accuracy? Basis: Authors note performance degrades when explanations reference irrelevant areas and future work should evaluate frequency and impact of such inaccuracies. Why unresolved: No systematic analysis on explanation grounding quality or error propagation. What evidence: Annotation of explanation-to-region alignment on a sample subset, correlation analysis between explanation accuracy and final EM/ANLS.

### Open Question 3
Would alternative embedding fusion strategies such as Reciprocal Rank Fusion (RRF) provide more robust region selection than the current majority voting approach? Basis: Authors state they will evaluate more robust fusion strategies like RRF and quantify agreement using Krippendorff's alpha. Why unresolved: Only majority voting tested; inter-embedder agreement unexplored. What evidence: Ablation study comparing majority voting vs. RRF vs. weighted similarity aggregation, with Krippendorff's α quantifying embedder agreement and downstream EM/ANLS impact.

### Open Question 4
Can the 2.4× inference time overhead (17.48s vs 7.21s) be reduced while maintaining or improving the modest accuracy gains observed with EaGERS? Basis: Table 1 shows inference time increases and authors explicitly note efficiency improvements are needed. Why unresolved: No caching, parallelization, or early-exit strategies explored. What evidence: Profiling of pipeline bottlenecks, experiments with embedding caching, adaptive early-stopping, or reduced re-query iterations, reporting latency-accuracy Pareto curves.

## Limitations
- The approach assumes VLM-generated spatial descriptions are sufficiently accurate for region selection, but performance degrades when explanations reference irrelevant areas or miss boundary content
- The ensemble voting mechanism's effectiveness depends on complementary biases across BLIP, CLIP, and ALIGN that weren't empirically validated in this specific document context
- The 15% margin expansion mitigates but doesn't eliminate boundary issues where relevant content spans grid boundaries

## Confidence
- High confidence: The EaGERS pipeline architecture and its ability to improve EM/ANLS on DocVQA without fine-tuning
- Medium confidence: The spatial explanation generation mechanism's reliability across diverse document layouts
- Low confidence: The ensemble embedding approach's generalizability beyond the specific BLIP/CLIP/ALIGN combination tested

## Next Checks
1. Test EaGERS with alternative VLMs (e.g., GPT-4V, Gemini) to assess whether explanation quality varies significantly by model
2. Evaluate performance on document types not represented in DocVQA (handwritten forms, technical diagrams, low-quality scans) to test generalizability
3. Measure inter-embedder agreement (Krippendorff's α) across diverse document layouts to quantify ensemble robustness thresholds