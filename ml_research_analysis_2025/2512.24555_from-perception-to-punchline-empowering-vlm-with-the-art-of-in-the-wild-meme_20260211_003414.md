---
ver: rpa2
title: 'From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme'
arxiv_id: '2512.24555'
source_url: https://arxiv.org/abs/2512.24555
tags:
- meme
- humor
- arxiv
- human
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of meme generation\u2014a creative,\
  \ subjective multimodal task requiring nuanced visual understanding and humor\u2014\
  which is difficult to address with standard image-to-caption methods due to its\
  \ open-ended, group-wise nature. To bridge this gap, the authors propose HUMOR,\
  \ a framework that guides VLMs through hierarchical Chain-of-Thought (CoT) reasoning\
  \ and aligns them with group-wise human preferences."
---

# From Perception to Punchline: Empowering VLM with the Art of In-the-wild Meme

## Quick Facts
- **arXiv ID:** 2512.24555
- **Source URL:** https://arxiv.org/abs/2512.24555
- **Reference count:** 40
- **Primary result:** HUMOR-CoT achieves 2.68 humor score vs 2.39 for base, with strong zero-shot generalization

## Executive Summary
This paper addresses meme generation—a creative, subjective multimodal task requiring nuanced visual understanding and humor. Standard image-to-caption methods fail due to the open-ended, group-wise nature of meme humor. The authors propose HUMOR, a framework that guides VLMs through hierarchical Chain-of-Thought reasoning and aligns them with group-wise human preferences. By decoupling template-level intent from context-level grounding, using pairwise reward modeling within coherent meme groups, and applying group-wise reinforcement learning, HUMOR preserves theoretical humor bounds while achieving strong empirical performance. The approach demonstrates significant improvements over baselines in human evaluations of humor, readability, context robustness, and human-likeness.

## Method Summary
The HUMOR framework consists of three stages: (1) Hierarchical CoT SFT training where the model first generates multiple reasoning paths for template intent, then anchors one path to ground-truth captions using actual user context; (2) Pairwise reward model training that operates within meme groups sharing the same template, learning a latent humor functional consistent with human ranking; (3) GRPO RL fine-tuning that uses the group-wise reward to guide policy updates while enforcing KL-divergence trust regions to prevent performance degradation. The method uses Qwen2.5-7B-Instruct as base model with LoRA fine-tuning, and Keye-VL-8B for reward modeling, trained on 3,713 crawled memes with reverse-engineered user requirements.

## Key Results
- HUMOR-CoT achieves 2.68 humor score vs 2.39 for base model in human evaluation
- Strong zero-shot generalization to unseen meme templates
- Human-likeness of 91.5% vs 75.7% for base model (Gemini-2.5-pro classification)
- Context robustness demonstrated through improved performance on context-swapped inputs
- Kendall's τ for reward model ranking consistency validates group-wise preference modeling

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Multi-path Reasoning Anchoring
Decoupling template-level intent from context-level grounding via multi-path exploration preserves a conditional lower bound on humor quality compared to single-path supervision. The model generates multiple reasoning paths and anchors one path to ground-truth captions using actual user context, preventing collapse into superficial mappings while ensuring high-quality paths retain probability mass. Core assumption: high-quality reasoning paths retain significant probability mass during exploration phase.

### Mechanism 2: Group-wise Subjective Reward Modeling
Restricting pairwise preference comparisons to within coherent meme groups creates a robust proxy for human humor preference that global scoring misses. Humor is subjective and lacks global scale, so training on pairs within the same template group learns a latent humor functional consistent with human ranking, avoiding cross-context noise. Core assumption: human judgments of humor are reliable within groups but not comparable across groups.

### Mechanism 3: KL-Bounded Policy Optimization (GRPO)
Using group-wise reward to guide policy updates while enforcing KL-divergence trust region prevents expected humor score from degrading beyond theoretical bounds. The optimization pushes the policy toward reward-ranked distribution while constraining drift from reference policy using KL penalty, with theoretical analysis proving expected humor cannot drop more than $\sqrt{\tau/2}$ if KL constraint is satisfied. Core assumption: reward model is rank-consistent and humor function is bounded.

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) Prompting**
  - Why needed here: Builds on standard CoT but enforces hierarchical structure (template → context) to avoid single-step failures in capturing metaphorical intent
  - Quick check: Can you explain why "Image → Punchline" mapping might fail vs "Image → Intent → Context → Punchline"?

- **Concept: Pairwise Ranking / Bradley-Terry Model**
  - Why needed here: Reward model relies on pairwise comparisons rather than absolute scores to handle subjectivity
  - Quick check: Why would "8/10 funniness" be noisier than "Is Meme A funnier than Meme B"?

- **Concept: Proximal Policy Optimization (PPO) / Trust Regions**
  - Why needed here: GRPO variant constrains updates to prevent catastrophic forgetting of humor capabilities learned in SFT stage
  - Quick check: What happens to policy performance if you remove KL-divergence constraint during RL on noisy reward?

## Architecture Onboarding

- **Component map:** Input (Template Image + User Tags) → Generator (Hierarchical CoT: Explore Paths → Anchor Path) → Caption → Reward Model (Group-wise Pairwise Scores → EBC Ranking) → Optimizer (GRPO Loss → Update Generator weights)

- **Critical path:** Reward Model reliability is bottleneck. If group-wise pairwise comparisons don't align with human preference, RL optimizes wrong objective.

- **Design tradeoffs:**
  - Diversity vs. Stability: Multi-path CoT increases creativity but requires stronger Reward Model
  - Absolute vs. Relative Evaluation: Explicitly rejects absolute scoring in favor of group-wise ranking

- **Failure signatures:**
  - Mode Collapse: Outputs exact training captions regardless of new tags
  - Safe but Dull: High readability but low humor
  - Literalism: Ignores visual metaphor, just describes image

- **First 3 experiments:**
  1. Sanity Check (CoT): Single-Path vs. Hierarchical Multi-Path, measure Context-Swap Distance
  2. Reward Model Validation: Compute Kendall's τ between RM ranking and human MaxDiff ranking (target τ > 0.6)
  3. Ablation on KL: Run GRPO with different β values, plot Expected Humor vs. KL Divergence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the framework prevent the model from defaulting to literal interpretations when user prompts contain overly specific or technical keywords?
- Basis: Appendix L.4 notes overly specific nouns cause abandonment of humorous scenarios for surface-level puns
- Evidence needed: Improved humor scores on technical/narrow prompts compared to baseline

### Open Question 2
- Question: Can effectiveness of group-wise preference optimization be decoupled from semantic strength of base VLM?
- Basis: Section 5.4 shows weaker models show "near-chance level" results, suggesting heavy reliance on base model alignment
- Evidence needed: Modified reward/training strategy achieving significant rank consistency on weaker base models

### Open Question 3
- Question: Can safety constraints be effectively integrated into RL fine-tuning phase to prevent sensitive content generation?
- Basis: Appendix L.3 identifies high-risk outputs and suggests future work with training-time safety constraints
- Evidence needed: Decrease in "high-risk" classification rate without external filtering, achieved via safety-aware reward shaping

## Limitations

- **Human Preference Modeling Assumption**: The local comparability assumption within meme templates may not hold across all categories, limiting cross-template generalization.
- **Theoretical vs. Empirical Gap**: Theoretical bounds rely on idealized assumptions about reward model consistency that may not hold in practice due to noise and distribution shift.
- **Data Representativeness**: The 3,713-meme dataset may not capture full diversity of meme humor, and VLM-based reverse-engineering of user requirements could introduce systematic bias.

## Confidence

- **High Confidence**: Hierarchical CoT framework and pairwise ranking within groups are well-supported by literature and align with multimodal reasoning principles. Human evaluation results are compelling.
- **Medium Confidence**: Theoretical analysis provides solid foundation but assumptions need empirical validation. GRPO mechanism depends heavily on reward model quality.
- **Low Confidence**: Exact impact of hyperparameters unclear without detailed ablation studies. Reverse-engineering process could introduce systematic bias.

## Next Checks

1. **Reward Model Robustness Test**: Generate memes for 20 templates (5 per template), have humans rate them, compute Kendall's τ between reward model ranking and human ranking for each template individually to test local comparability assumption.

2. **KL-Divergence Sensitivity Analysis**: Run GRPO with β values {0.01, 0.1, 1.0, 10.0}, plot expected humor vs. KL divergence from reference policy to empirically validate theoretical bounds and identify optimal settings.

3. **Cross-Template Transfer Experiment**: Evaluate trained reward model's pairwise predictions on meme pairs from different templates, measure accuracy and analyze failure cases to determine if cross-template modeling could improve edge case performance.