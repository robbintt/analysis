---
ver: rpa2
title: Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks
arxiv_id: '2510.00962'
source_url: https://arxiv.org/abs/2510.00962
tags:
- english
- rules
- questions
- accuracy
- grammatical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether large language models (LLMs) perform
  worse when answering multiple-choice questions written in non-Standard American
  English (SAE) dialects versus SAE. Using grammatical perturbation tools, questions
  from three benchmark QA datasets (BoolQ, SciQ, MMLU) are transformed into six English
  dialects (African American, Appalachian, Chicano, Indian, Singaporean, Southern).
---

# Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks

## Quick Facts
- **arXiv ID**: 2510.00962
- **Source URL**: https://arxiv.org/abs/2510.00962
- **Reference count**: 39
- **Primary result**: LLM performance drops up to 20 percentage points on dialectal variants compared to SAE, with three specific grammatical rules explaining most degradation

## Executive Summary
This paper examines whether large language models (LLMs) perform worse when answering multiple-choice questions written in non-Standard American English (SAE) dialects versus SAE. Using grammatical perturbation tools, questions from three benchmark QA datasets (BoolQ, SciQ, MMLU) are transformed into six English dialects (African American, Appalachian, Chicano, Indian, Singaporean, Southern). Performance drops by up to 20 percentage points across dialects compared to SAE, with the largest declines for Singaporean and African American English. Further analysis of individual grammatical rules reveals that three specific rules—existential "it," zero copula, and "y'all"—account for the majority of degradation across multiple dialects. These findings highlight targeted grammatical structures as key contributors to dialectal bias and suggest model improvements could focus on these high-impact rules to reduce disparities.

## Method Summary
The study uses Multi-VALUE, a rule-based SAE-to-dialect transformer, to convert questions from three benchmark datasets (BoolQ, SciQ, MMLU) into six dialectal variants while keeping answers unchanged. The perturbed questions are evaluated using zero-shot inference on three LLMs (Gemma-2B, Mistral-7B, GPT4o-mini) via LM Eval Harness. Performance is measured as accuracy differentials between SAE and dialectal variants, with further analysis decomposing degradation by individual grammatical rules. Perplexity analysis using FineWeb provides distributional evidence for why certain dialects show higher degradation.

## Key Results
- LLMs show up to 20 percentage point accuracy reduction on dialectal variants compared to SAE
- Singaporean and African American English show the largest performance declines
- Three grammatical rules (existential "it," zero copula, "y'all") individually account for at least 45% of degradation, collectively explaining 64-85% across American dialects
- Perplexity increases substantially for dialectal variants, with Singaporean English showing 2713-3517% increases on SciQ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM performance degrades on dialectal text because specific grammatical structures are underrepresented in training corpora, causing tokenization and parsing failures.
- Mechanism: Non-SAE grammatical rules (existential "it," zero copula, "y'all") create token sequences with higher perplexity; models trained predominantly on SAE assign lower probability to these structures, reducing effective context for downstream tasks.
- Core assumption: The Multi-VALUE perturbations approximate authentic dialectal writing patterns; training data underrepresentation is the primary driver.
- Evidence anchors:
  - [abstract] "up to a 20% reduction in accuracy" with "three specific grammar rules... explain[ing] the majority of performance degradation"
  - [section 3.1] Perplexity analysis shows "substantial increases in perplexity when SAE is transformed into dialectal variants," with Singaporean English showing 2713-3517% perplexity increases
  - [corpus] Related work on AAE grammatical feature tagging (arXiv:2502.06004) confirms LLMs struggle with dialectal feature identification, supporting the underrepresentation hypothesis
- Break condition: If dialectal text were added to training data at scale and degradation persisted, this mechanism would be weakened.

### Mechanism 2
- Claim: A small subset of high-impact grammatical rules disproportionately drives dialectal bias, meaning targeted mitigation could yield outsized gains.
- Mechanism: Obligatory grammatical rules (always applied when grammatically possible) differ in their impact on model internals; existential "it," zero copula, and "y'all" each account for 45-85% of degradation in American dialects because they alter syntactic structure fundamental to parsing.
- Core assumption: Rule-level decomposition generalizes beyond the Multi-VALUE perturbation framework to authentic dialectal text.
- Evidence anchors:
  - [section 3.2] "each of these three rules individually account for at least 45% of the degradation" and "64-85% of overall dialectal degradation" for American dialects
  - [section 4] "9 out of 20 of the observed statistically-significant grammar rules overlap across dialects"
  - [corpus] Weak direct corpus evidence on rule-level mitigation transfer; related work focuses on holistic dialect handling rather than rule decomposition
- Break condition: If models trained on these three rules showed no cross-dialect transfer, the shared-rule hypothesis would fail.

### Mechanism 3
- Claim: Co-occurring grammatical rules produce interaction effects that exceed simple additive degradation.
- Mechanism: Multiple simultaneous grammatical perturbations create compounded parsing ambiguity (e.g., null prepositions + dropped copula + relativizer changes); the model's attention mechanism cannot isolate individual rule contributions.
- Core assumption: The interaction effects observed in synthetic perturbations reflect real-world dialectal complexity.
- Evidence anchors:
  - [appendix A.4] For Singaporean English, combining three rules yields -5.77% accuracy vs. -8.25% additive expectation, showing "an interaction effect in grammar rules co-occurring"
  - [table 9-10] Interaction gaps range from -0.79% to +2.48%, indicating non-linear compounding
  - [corpus] No direct corpus evidence on rule interaction effects; this remains underexplored
- Break condition: If interaction effects disappeared with larger models or different tokenizers, the mechanism would be model-architecture dependent rather than fundamental.

## Foundational Learning

- Concept: **Perplexity as proxy for out-of-distribution detection**
  - Why needed here: The paper uses perplexity to explain why Singaporean English shows highest degradation (3517% increase on SciQ); understanding this metric helps diagnose which dialectal features are most unfamiliar to models.
  - Quick check question: If a dialectal variant has 10x higher perplexity than SAE, would you expect proportional accuracy degradation? Why or why not?

- Concept: **Grammatical rule obligatoriness vs. optionality**
  - Why needed here: The paper distinguishes obligatory rules (always applied when grammatically possible) from optional rules; this affects which rules to prioritize for mitigation.
  - Quick check question: Why might obligatory rules be better targets for bias mitigation than optional rules?

- Concept: **McNemar's test for paired categorical comparisons**
  - Why needed here: The paper uses McNemar's test to identify which grammar rules cause statistically significant accuracy drops; understanding this helps evaluate whether observed degradations are meaningful.
  - Quick check question: What does a significant McNemar's test tell you about two conditions that a simple accuracy difference does not?

## Architecture Onboarding

- Component map:
  - Multi-VALUE -> LM Eval Harness -> Gemma-2B/Mistral-7B/GPT4o-mini -> Accuracy metrics

- Critical path:
  1. Load benchmark dataset (SAE questions only)
  2. Apply dialectal perturbations via Multi-VALUE (questions only; answers unchanged)
  3. Filter to questions where at least one rule applies
  4. Run zero-shot evaluation on SAE baseline
  5. Run zero-shot evaluation on each dialectal variant
  6. Subset to SAE-correct questions for disparity measurement
  7. Decompose by individual grammatical rules

- Design tradeoffs:
  - **Synthetic vs. natural dialectal text**: Multi-VALUE provides controlled perturbations but may not capture authentic dialectal writing; prior work (Lin et al., 2025) finds human-written AAE causes even larger degradation
  - **Multiple-choice vs. open-ended**: MC provides cleaner measurement but underestimates real-world harm; open-ended tasks likely show larger gaps
  - **Question-only perturbation**: Answers left in SAE creates artificial scenario; real users would write both in dialect

- Failure signatures:
  - **Rule overlap confounds**: If multiple rules apply to the same question, attributing degradation to specific rules requires careful subsetting
  - **Small model variance**: Gemma-2B shows highest degradation but also highest baseline variance on MMLU (34.1% SAE accuracy)
  - **Perturbation rejection**: Some questions cannot be processed by Multi-VALUE; these are excluded from both SAE and dialectal counts

- First 3 experiments:
  1. **Reproduce the three-rule decomposition** on a held-out dataset (e.g., HellaSwag) to validate that existential "it," zero copula, and "y'all" generalize as high-impact rules
  2. **Fine-tune on synthetic rule-perturbed QA pairs** using only the three high-impact rules; measure cross-dialect transfer to test the paper's mitigation hypothesis
  3. **Scale experiment with larger model** (e.g., Llama-3-70B or GPT-4o) to determine if degradation persists at higher scale or if larger models absorb dialectal variation naturally

## Open Questions the Paper Calls Out

- **Question 1**: Can training models on QA pairs perturbed with a small set of high-impact grammatical rules (existential "it," zero copula, "y'all") effectively mitigate dialectal performance disparities across multiple dialects?
  - Basis: The authors explicitly call for future work to investigate bias mitigation methods focused on individual, high-impact grammatical structures
  - Why unresolved: The paper identifies which rules cause degradation but does not test whether targeted training on these rules actually reduces bias in practice
  - What evidence would resolve it: Pre/post training comparisons showing reduced accuracy gaps between SAE and dialectal variants after fine-tuning on perturbed QA pairs

- **Question 2**: How do dialectal performance degradations compare between synthetic grammatical perturbations (Multi-VALUE) and naturally-occurring dialectal text written by native speakers?
  - Basis: The authors acknowledge Multi-VALUE "rules are applied following set probabilities of occurrence... and thus could be debatably similar to a true speaker or writer of that dialect"
  - Why unresolved: The study relies entirely on synthetic perturbations; ecological validity for real-world dialectal usage remains unknown
  - What evidence would resolve it: Comparative evaluation using matched corpora of naturally-occurring dialectal text versus synthetic perturbations on the same benchmark tasks

## Limitations

- Synthetic perturbations may not fully capture authentic dialectal writing patterns, potentially underestimating real-world disparities
- The three identified high-impact rules show consistent degradation but rule overlap confounds make precise attribution challenging
- Results reflect only questions where grammatical rules apply (0-100% per dialect), excluding dialectal variation that doesn't trigger Multi-VALUE rules

## Confidence

- **High Confidence**: The core finding that dialectal bias exists and causes measurable performance degradation (up to 20 percentage points) is well-supported by systematic evaluation across three benchmarks and six dialects
- **Medium Confidence**: The identification of three specific grammatical rules as primary drivers of degradation is convincing but relies on the Multi-VALUE perturbation framework's accuracy
- **Low Confidence**: The generalization of these findings to authentic dialectal text remains uncertain, as human-written African American English causes even larger degradation than Multi-VALUE perturbations

## Next Checks

1. **Cross-Dataset Rule Decomposition Validation**: Apply the same three-rule decomposition methodology to an independent benchmark (e.g., HellaSwag or ARC) to verify that existential "it," zero copula, and "y'all" consistently emerge as high-impact rules across different QA domains and question types

2. **Fine-tuning Transfer Experiment**: Fine-tune a model using only synthetic training data with the three high-impact rules perturbed, then evaluate cross-dialect transfer to test whether mitigation on these rules generalizes to dialects where they may not be primary features (e.g., Singaporean English)

3. **Scale Dependency Investigation**: Evaluate the same dialectal perturbations on larger models (Llama-3-70B, GPT-4o, or Claude) to determine whether degradation persists at higher scale or if larger models naturally absorb dialectal variation, which would inform whether current limitations are architectural or scale-dependent