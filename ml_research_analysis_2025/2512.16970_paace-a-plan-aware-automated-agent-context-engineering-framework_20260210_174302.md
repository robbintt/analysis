---
ver: rpa2
title: 'PAACE: A Plan-Aware Automated Agent Context Engineering Framework'
arxiv_id: '2512.16970'
source_url: https://arxiv.org/abs/2512.16970
tags:
- context
- paace
- compression
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PAACE addresses context management challenges in long-horizon LLM
  agents by introducing plan-aware compression that preserves task dependencies and
  instruction coherence. The framework combines synthetic workflow generation with
  outcome-level distillation to learn next-k-task relevance and structure-aware compression.
---

# PAACE: A Plan-Aware Automated Agent Context Engineering Framework

## Quick Facts
- **arXiv ID:** 2512.16970
- **Source URL:** https://arxiv.org/abs/2512.16970
- **Reference count:** 8
- **Primary result:** Plan-aware compression achieves up to 40% context reduction while improving accuracy on long-horizon LLM agent tasks

## Executive Summary
PAACE addresses context management challenges in long-horizon LLM agents by introducing plan-aware compression that preserves task dependencies and instruction coherence. The framework combines synthetic workflow generation with outcome-level distillation to learn next-k-task relevance and structure-aware compression. Evaluated on AppWorld, OfficeBench, and multi-hop QA benchmarks, PAACE achieves higher accuracy than all baselines while reducing peak context length by up to 40% and cumulative attention load by up to 60%. The distilled PAACE-FT model retains 97% of the teacher's performance with over an order of magnitude lower inference cost, demonstrating practical applicability. Results indicate that plan-aware compression not only improves efficiency but also acts as a regularization mechanism, enhancing reasoning stability and reducing instruction drift in multi-step workflows.

## Method Summary
PAACE uses a two-stage approach: (1) Teacher compression pipeline where GPT-OSS-120B generates plan-aware compressed contexts using evolved prompts and next-k conditioning, filtering successful compressions via semantic equivalence (θ=0.85) and LLM judge approval; (2) Student distillation where Qwen3-4B-Instruct learns to replicate compression from (Π_{t:t+k}, C_t) → C̃_t pairs via causal LM loss. The framework generates ~1.2M synthetic workflows (~9.5B tokens) with 5-30 step plans spanning document workflows, web navigation, and multi-hop QA. Next-k parameter is set to 2 for tool-centric tasks and 3 for multi-hop QA based on task characteristics.

## Key Results
- Achieves up to 40% reduction in peak context length and 60% reduction in cumulative dependency across benchmarks
- Outperforms all baselines on AppWorld, OfficeBench, and 8-Objective QA while maintaining higher accuracy
- PAACE-FT retains 97% of teacher performance with >10x inference cost reduction

## Why This Works (Mechanism)
PAACE leverages plan-awareness to compress context while preserving task dependencies across workflow steps. The framework's effectiveness stems from conditioning compression on next-k tasks (k=2 for tool-centric, k=3 for multi-hop QA), ensuring relevant information is retained for upcoming steps. The synthetic workflow generator (PAACE-Syn) creates diverse training data with varying plan lengths, noise injection, and tool interactions, enabling the model to learn robust compression strategies. The evolutionary prompt optimization fine-tunes compression instructions based on success rate, semantic equivalence, and compression ratio metrics, while the distillation stage transfers this capability to a smaller, efficient model without sacrificing performance.

## Foundational Learning
- **Plan-aware compression**: Compressing context based on task plans and dependencies rather than just recent history; needed to maintain coherence across multi-step workflows; quick check: verify compressed context retains all constraints/variables mentioned in next-k tasks
- **Next-k conditioning**: Including k future tasks when making compression decisions; needed to ensure relevant information isn't prematurely discarded; quick check: measure accuracy drop when varying k from 1 to 4
- **Synthetic workflow generation**: Creating large-scale training data with controlled plan lengths and tool interactions; needed to train compression models without expensive human annotations; quick check: validate generated workflows cover target domain distributions
- **Semantic equivalence filtering**: Using sentence embeddings to ensure compressed outputs maintain meaning; needed to prevent information loss during compression; quick check: verify 85% similarity threshold catches most semantic degradations
- **Causal LM distillation**: Training student model with masked language modeling on teacher's successful compressions; needed to transfer compression capability efficiently; quick check: monitor student perplexity on held-out compression examples
- **Evolutionary prompt optimization**: Iteratively improving compression prompts based on performance metrics; needed to discover effective compression strategies automatically; quick check: track prompt population improvement over optimization generations

## Architecture Onboarding

**Component Map**
PAACE-Syn -> Teacher Compression -> Filter (Semantic+Judge) -> Distillation -> PAACE-FT

**Critical Path**
Workflow generation → Teacher compression with next-k conditioning → Semantic equivalence validation → LLM judge approval → Distillation training → Inference deployment

**Design Tradeoffs**
- Teacher model choice (GPT-OSS-120B vs. alternatives) impacts synthetic data quality and student performance
- Next-k parameter balances compression aggressiveness with information preservation
- Semantic threshold (0.85) trades off compression ratio against output quality
- LLM judge adds computational overhead but improves compression reliability

**Failure Signatures**
- Empty compressed contexts or failure to compress indicate over-aggressive strategies
- Accuracy degradation over workflow steps suggests instruction drift
- Low semantic equivalence scores reveal information loss
- High compression ratios with poor task performance indicate superficial compression

**First Experiments**
1. Validate synthetic workflow generation by sampling 100 workflows and checking plan complexity distribution
2. Test teacher compression pipeline on 10 workflows with varying k values and measure semantic equivalence
3. Run full distillation pipeline on 1,000 examples and evaluate student compression quality vs. teacher

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on proprietary GPT-OSS-120B teacher model that is unavailable for reproduction
- Prompt evolution mechanism lacks sufficient detail for exact replication
- Domain-specific workflow templates and noise injection strategies not fully specified
- LLM judge model and evaluation prompts are not provided

## Confidence
- **Compression Efficiency Claims:** High confidence - directly measurable from reported metrics
- **Accuracy Claims on Benchmarks:** Medium confidence - well-defined metrics but teacher dependency introduces uncertainty
- **PAACE-FT Distillation Performance:** Medium confidence - clear methodology but exact training setup unspecified
- **Regularization and Stability Claims:** Low confidence - qualitative benefits lack rigorous quantitative validation

## Next Checks
1. **Teacher Model Substitution Impact Test:** Reproduce framework using GPT-4 or Claude-3.5-Sonnet as teacher, then measure student performance drop on AppWorld/OfficeBench tasks
2. **Prompt Evolution Reproducibility Check:** Implement steady-state evolutionary optimization with varying population sizes (10, 50, 100) and measure variance in compression success rates
3. **Next-k Sensitivity Analysis:** Systematically vary k (1, 2, 3, 4) across domains and measure changes in compression ratio, task accuracy, and instruction retention