---
ver: rpa2
title: Do LLM hallucination detectors suffer from low-resource effect?
arxiv_id: '2601.16766'
source_url: https://arxiv.org/abs/2601.16766
tags:
- mtrex
- languages
- across
- language
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines whether hallucination detection methods suffer
  from the low-resource effect that affects LLM performance in non-English languages.
  The authors created a multilingual factual QA benchmark (mTREx) by translating English
  TREx into German, Hindi, Bengali, and Urdu, and combined it with Global-MMLU to
  evaluate four LLMs (7B-70B parameters) across five tasks and five languages.
---

# Do LLM hallucination detectors suffer from low-resource effect?

## Quick Facts
- **arXiv ID**: 2601.16766
- **Source URL**: https://arxiv.org/abs/2601.16766
- **Reference count**: 40
- **Key outcome**: Hallucination detector performance degrades much less than LLM task accuracy in low-resource languages

## Executive Summary
This paper investigates whether hallucination detection methods are affected by the low-resource problem that impacts LLM performance in non-English languages. The authors created a multilingual factual QA benchmark by translating English TREx into German, Hindi, Bengali, and Urdu, and evaluated four LLMs (7B-70B parameters) across five languages. They tested three hallucination detection methods and found that while LLM task accuracy drops significantly in low-resource languages, hallucination detector performance degradation is often much smaller - sometimes less than 10% of the task accuracy drop.

## Method Summary
The authors created mTREx by translating English TREx into German, Hindi, Bengali, and Urdu, and combined it with Global-MMLU to evaluate four LLMs (7B-70B parameters) across five languages. They tested two internal artifact-based methods (MAM using self-attention and fully connected activations) and two sampling-based black-box methods (SelfCheckGPT and Semantic Entropy). The TPHR metric was introduced to quantify the relative degradation between hallucination detector and LLM task performance.

## Key Results
- LLM task accuracy drops over 60% in Bengali and Urdu compared to English
- Hallucination detector performance degradation is often less than 10% of task accuracy drop
- TPHR values >1 in low-resource languages indicate HD performance remains stable
- MAM methods consistently outperformed sampling-based methods across all languages

## Why This Works (Mechanism)
The paper hypothesizes that hallucination detection, being a simpler binary classification task, is less affected by tokenization inefficiencies and low-resource language challenges compared to the complex token generation process. The binary nature of the task may require less nuanced language understanding than full QA generation.

## Foundational Learning
- **Multilingual tokenizers**: Required for processing low-resource languages; quick check: verify tokenizer coverage for target languages
- **Self-attention mechanisms**: Used in MAM method to detect hallucinations; quick check: analyze attention patterns across languages
- **Sampling-based detection**: Methods like SelfCheckGPT generate multiple responses to detect hallucinations; quick check: evaluate sampling efficiency across languages
- **Binary classification vs generation**: Hallucination detection is simpler than full QA; quick check: compare complexity of binary vs generative tasks
- **Cross-lingual transfer**: Zero-shot transfer from English to low-resource languages is poor; quick check: measure transfer gap across language pairs
- **Tokenization efficiency**: Low-resource languages may suffer from poor tokenization; quick check: compare tokenization coverage and accuracy

## Architecture Onboarding
- **Component map**: LLM (QA task) -> Hallucination Detector (binary classification)
- **Critical path**: Generate answer → Extract internal activations/sampling → Classify hallucination probability
- **Design tradeoffs**: Internal artifact methods (MAM) vs sampling-based methods; cross-lingual transfer vs in-language supervision
- **Failure signatures**: Poor tokenization, insufficient multilingual pretraining data, zero-shot transfer limitations
- **First experiments**: 1) Compare tokenization coverage across languages, 2) Test cross-lingual transfer gap, 3) Evaluate MAM vs sampling method performance

## Open Questions the Paper Calls Out
None

## Limitations
- Small number of languages tested (five total, with only two truly low-resource languages)
- Focus on QA-style factual verification tasks may not generalize to other applications
- Largest model (70B parameters) may have access to more multilingual pretraining data

## Confidence
- High confidence in core empirical finding that hallucination detector degradation < LLM task degradation in low-resource languages
- Medium confidence in hypothesis that binary classification simplicity drives robustness
- Medium confidence in cross-lingual transfer results given limited zero-shot evaluation

## Next Checks
1. Replicate the study with additional low-resource languages (e.g., African, indigenous American languages) to validate the pattern across a broader linguistic spectrum
2. Test hallucination detection methods on non-QA tasks (summarization, reasoning) to determine if the low-resource robustness generalizes beyond factual verification
3. Conduct ablation studies on tokenization effects by comparing models with and without specialized multilingual tokenizers to isolate the contribution of tokenization to performance differences