---
ver: rpa2
title: 'Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based
  zero-shot retrieval'
arxiv_id: '2506.18316'
source_url: https://arxiv.org/abs/2506.18316
tags:
- retrieval
- citation
- dense
- paragraph
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of citation discovery in the
  SCIDOCA 2025 shared task, where the goal is to predict the correct citation from
  a pool of candidate abstracts for a given paragraph. The primary difficulties arise
  from the length of the abstracts and the high similarity among candidate abstracts,
  making it challenging to identify the exact citation.
---

# Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval

## Quick Facts
- **arXiv ID:** 2506.18316
- **Source URL:** https://arxiv.org/abs/2506.18316
- **Reference count:** 16
- **Primary result:** Relation-based retrieval achieves 0.8506 precision for top-10 retrieval in citation discovery task

## Executive Summary
This paper addresses the SCIDOCA 2025 shared task on citation discovery, where the goal is to predict the correct citation abstract from a pool of candidates given a query paragraph. The primary challenge stems from the length of abstracts and high similarity among candidates, making traditional retrieval methods inadequate. The authors propose a two-stage approach that first extracts relational features from the query using an LLM, retrieves top-k candidates based on these features, then uses another LLM to identify the final citation from the filtered subset.

The method demonstrates significant improvements in precision (0.8506 for top-10) compared to traditional TF-IDF and dense retrieval approaches. While recall is lower than dense retrieval, the integration of LLM inference with relation-based retrieval achieves a balanced F1 score of 0.2912, comparable to dense retrieval but with a different precision-recall trade-off. These results highlight the potential of structured relational information in enhancing citation discovery tasks.

## Method Summary
The proposed method combines relation-based retrieval with Large Language Models (LLMs) to improve citation prediction accuracy. The system first extracts relational features from the given paragraph using Mistral-7B-Instruct-v0.3, then retrieves the top-k most similar abstracts based on these extracted features. From this subset, another LLM performs final inference to identify the most relevant citation. The approach leverages structured relational information to guide the retrieval process, reducing noise and improving precision. Experiments conducted on 1,000 randomly selected queries from the SCIDOCA dataset demonstrate the effectiveness of this two-stage approach.

## Key Results
- Relation-based retrieval achieves 0.8506 precision for top-10 retrieval
- LLM inference with relation-based retrieval achieves F1=0.2912, comparable to dense retrieval (F1=0.2884)
- Raw relation-based retrieval recall is ~0.21, but LLM inference improves recall to 0.4626
- Traditional methods like TF-IDF and Dense Vector Retrieval underperform on this task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Extracting structured relational triples from query paragraphs improves retrieval precision by filtering on semantic relationships rather than surface-level keyword matching.
- Mechanism: LLM receives paragraph → prompted to extract (entity, relation, entity) triples → these structured features index into candidate pool → retrieves abstracts matching relational patterns rather than term overlap.
- Core assumption: The correct citation shares meaningful relational structure with the query paragraph, not just vocabulary.
- Evidence anchors:
  - [abstract]: "first retrieves the top-k most similar abstracts based on extracted relational features from the given paragraph"
  - [section]: "relation-based retrieval approach demonstrates a substantial improvement in precision (e.g., 0.8506 for top-10)"
  - [corpus]: Document-level relation extraction literature (DocRED, edge-oriented graphs) supports that cross-sentence relations capture document semantics better than sentence-level features.
- Break condition: When paragraphs lack clear entity relationships, or when the correct citation frames the same concepts with different relational structures.

### Mechanism 2
- Claim: A two-stage retrieval-then-inference pipeline achieves better F1 balance than single-stage methods by concentrating LLM reasoning on a filtered candidate set.
- Mechanism: Stage 1 retrieves top-k using relation-based matching (high precision, lower recall) → Stage 2 LLM performs deeper contextual comparison on smaller set → reduces comparison noise while leveraging LLM's discriminative capacity.
- Core assumption: The correct citation exists within the top-k retrieved candidates; precision matters more than recall at retrieval stage.
- Evidence anchors:
  - [abstract]: "From this subset, we leverage a Large Language Model (LLM) to accurately identify the most relevant citation"
  - [section]: Table 2 shows LLM inference with relation-based retrieval achieves F1=0.2912, comparable to dense retrieval (F1=0.2884) but via different precision-recall trade-off.
  - [corpus]: Re-ranking literature (Nogueira & Cho, citation [9]) confirms that cross-encoder reranking on retrieved subsets improves precision.
- Break condition: When correct citation falls outside top-k (recall failure propagates to final prediction).

### Mechanism 3
- Claim: LLM-based final inference improves recall over raw retrieval scores by incorporating joint reasoning over paragraph-abstract pairs.
- Mechanism: LLM receives query paragraph + each retrieved abstract → computes textual similarity AND relational coherence → outputs single best-matching abstract ID.
- Core assumption: The LLM can reliably distinguish between highly similar abstracts when explicitly prompted to compare against relational features.
- Evidence anchors:
  - [abstract]: "leverage a Large Language Model (LLM) to accurately identify the most relevant citation"
  - [section]: "integrating LLMs with retrieval significantly enhances recall compared to raw retrieval scores" — raw relation-based recall ~0.21 vs LLM inference recall 0.4626
  - [corpus]: RAG literature (Gao et al., citation [4]) supports combining retrieval with generative LLM reasoning for knowledge-intensive tasks.
- Break condition: When candidate abstracts are semantically indistinguishable even for LLM, or when prompt design fails to elicit proper comparison reasoning.

## Foundational Learning

- **Concept: Document-Level Relation Extraction (DocRE)**
  - Why needed here: The core innovation extracts relations across sentence boundaries, not just within single sentences.
  - Quick check question: Given "BERT was introduced in 2018. It achieved state-of-the-art on GLUE." can you identify a cross-sentence relation triple?

- **Concept: Precision-Recall Trade-off in Retrieval**
  - Why needed here: The paper explicitly optimizes for precision at retrieval stage, accepting lower recall, then recovers recall via LLM inference.
  - Quick check question: If precision=0.85 at top-10 but recall=0.21, what happens to both metrics if you expand to top-50?

- **Concept: Zero-Shot LLM Prompting for Structured Extraction**
  - Why needed here: The system uses Mistral-7B without task-specific fine-tuning; effectiveness depends on prompt design.
  - Quick check question: How would you phrase a prompt to extract (subject, predicate, object) triples from a scientific paragraph?

## Architecture Onboarding

- **Component map:**
  Input Layer -> Extraction Module -> Retrieval Module -> Inference Module -> Output Layer

- **Critical path:**
  Query paragraph → Relation extraction prompt → LLM → Extracted triples → Similarity search over candidate pool → Top-k abstracts → Citation prediction prompt → LLM → Final citation ID

- **Design tradeoffs:**
  - Higher k: Improves recall, degrades precision (Table 1 shows top-20 precision drops to 0.1576)
  - Relation-based vs Dense retrieval: Precision-focused vs recall-focused first stage
  - Single LLM for both stages (current) vs specialized models: Simpler deployment vs potential performance gains

- **Failure signatures:**
  - High precision, very low recall at retrieval: Relation extraction too restrictive or top-k too small
  - LLM inference doesn't improve over raw retrieval: Prompt not guiding comparison effectively
  - Performance degrades with longer abstracts: Context window or attention issues in LLM

- **First 3 experiments:**
  1. Replicate baseline comparison: TF-IDF vs Dense vs Relation-based retrieval at k=10, 15, 20; measure precision/recall/F1
  2. Ablate LLM inference: Compare raw retrieval F1 vs retrieval + LLM inference for each method
  3. Vary relation extraction prompt: Test whether simpler keyphrase extraction vs full triple extraction changes precision/recall balance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid retrieval models dynamically optimize the trade-off between high precision and high recall based on specific paragraph characteristics?
- Basis in paper: [explicit] The authors state, "For future work, we aim to explore hybrid retrieval models that dynamically adjust the recall-precision trade-off based on paragraph characteristics."
- Why unresolved: The current implementation forces a static choice; relation-based retrieval offers high precision (0.8506) but lower recall, whereas dense retrieval offers high recall but lower precision.
- What evidence would resolve it: Development and evaluation of an adaptive system that outperforms static baselines by switching or blending retrieval strategies per query.

### Open Question 2
- Question: Does incorporating graph-based relational reasoning improve retrieval effectiveness by capturing deeper contextual dependencies?
- Basis in paper: [explicit] The conclusion proposes "incorporating graph-based relational reasoning could further improve retrieval effectiveness by capturing deeper contextual dependencies across documents."
- Why unresolved: The proposed method extracts relation triples independently and uses them for filtering, without modeling the complex structural interactions between the query and the candidate documents.
- What evidence would resolve it: A comparative study showing F1 score improvements when graph-based structures are integrated into the retrieval or ranking pipeline compared to the current triple-based approach.

### Open Question 3
- Question: Is the low recall in the relation-based stage an inherent limitation of the extraction method or a result of the specific LLM used?
- Basis in paper: [inferred] The authors note that relation-based retrieval "may also discard some relevant documents" and relies solely on "Mistral-7B-Instruct-v0.3" for extraction.
- Why unresolved: The paper does not analyze if the missed relevant documents (false negatives) resulted from the model's failure to extract relations or the retrieval mechanism's inability to match them.
- What evidence would resolve it: An error analysis of the retrieval drop-off or experiments using varied LLM sizes to isolate the bottleneck in the relation extraction phase.

## Limitations
- The exact LLM prompts for relation extraction and citation selection are not disclosed, making faithful reproduction challenging
- The conversion process from extracted relational triples to retrieval queries remains unspecified
- Dense retrieval baseline implementation details (model architecture, pooling strategy) are absent
- The method achieves high precision but lower recall, potentially missing relevant citations

## Confidence
- **High confidence:** The two-stage retrieval-then-inference architecture and its precision-recall trade-off characterization
- **Medium confidence:** The specific F1 score of 0.2912 being "comparable to dense retrieval" - depends on exact baseline implementation
- **Medium confidence:** The claim that relation-based retrieval "significantly outperforms" traditional methods - precision improvement is clear, but practical significance depends on full precision-recall curve

## Next Checks
1. **Prompt validation:** Test whether the claimed precision improvement persists when using simpler keyphrase extraction instead of full relational triple extraction
2. **Recall floor analysis:** Systematically measure what percentage of ground-truth citations fall outside the top-20 retrieved candidates to quantify the fundamental limitation of the precision-focused approach
3. **Abstract length sensitivity:** Evaluate performance degradation when candidate abstracts exceed typical lengths to assess scalability constraints