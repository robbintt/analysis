---
ver: rpa2
title: 'Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior'
arxiv_id: '2505.00439'
source_url: https://arxiv.org/abs/2505.00439
tags:
- validation
- instances
- instance
- size
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting policies that generalize
  well from small training instances to large test instances in per-domain planning.
  The key insight is that dynamic generation of validation instances during training,
  rather than using fixed validation sets, can improve policy selection for scaling
  behavior.
---

# Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior

## Quick Facts
- arXiv ID: 2505.00439
- Source URL: https://arxiv.org/abs/2505.00439
- Reference count: 12
- This paper addresses policy selection for per-domain planning, showing that dynamic validation instance generation during training improves scaling behavior from small to large instances.

## Executive Summary
This paper tackles the challenge of selecting policies that generalize well from small training instances to larger test instances in per-domain planning. The key insight is that static validation sets can be misleading for evaluating scaling behavior, as they don't capture how policies perform on progressively larger instances. The authors propose a dynamic validation approach that generates validation instances on-the-fly during training, continuing until policy coverage drops below 30%. This method consistently outperforms traditional fixed-set validation approaches across 9 planning domains, yielding policies with better scaling behavior and more efficient solutions.

## Method Summary
The proposed method generates validation instances dynamically during training, starting with small instances and progressively increasing their size. The process continues as long as the policy maintains sufficient coverage (above 30%) on these instances. This approach contrasts with traditional methods that use fixed validation sets throughout training. The authors also refine the evaluation methodology by incorporating statistical coverage with confidence intervals to better assess scaling behavior. The framework was evaluated using graph neural network policies across 9 different planning domains, demonstrating consistent improvements in scaling metrics.

## Key Results
- Dynamic validation achieved better "Scale" (largest instance size with sufficient performance) in all 9 tested domains
- Dynamic validation showed improved "SumCov" (total coverage across all sizes) across all domains
- Policies selected via dynamic validation found plans of equal or shorter length while solving more instances

## Why This Works (Mechanism)
Dynamic validation works because it provides more informative feedback during training about how policies will perform on larger instances. Traditional fixed validation sets can mislead the training process by focusing on performance metrics that don't correlate well with scaling behavior. By generating validation instances that grow progressively during training and stopping when coverage becomes uninformative (below 30%), the method ensures that the selected policy has been validated on instances that reflect the true scaling challenges it will face at test time.

## Foundational Learning
- **Per-domain planning**: Why needed - Planning domains have unique characteristics requiring specialized policies; Quick check - Each domain needs separate training and validation
- **Scaling behavior evaluation**: Why needed - Policies must generalize from small to large instances; Quick check - Coverage metrics across instance sizes
- **Coverage threshold**: Why needed - Determines when validation instances become uninformative; Quick check - 30% threshold empirically validated
- **Graph neural networks in planning**: Why needed - Can capture relational structure in planning problems; Quick check - Used as policy architecture in experiments
- **Validation instance generation**: Why needed - Provides realistic test scenarios during training; Quick check - On-the-fly generation vs. fixed sets

## Architecture Onboarding

**Component Map:**
- Training instances -> Policy training -> Dynamic validation generation -> Coverage evaluation -> Policy selection
- Static validation loss -> Traditional policy selection (alternative path)
- Coverage metrics (Scale, SumCov) -> Performance evaluation

**Critical Path:**
The critical path is: Training instances → Policy training → Dynamic validation generation → Coverage evaluation → Policy selection. This path ensures that policies are selected based on their ability to maintain coverage on increasingly large instances, rather than being misled by static validation performance.

**Design Tradeoffs:**
- Dynamic vs. static validation: Dynamic provides more informative feedback but requires on-the-fly instance generation
- Coverage threshold selection: 30% is heuristic but balances informativeness with computational efficiency
- Instance size progression: Gradual increase ensures stable learning while testing scaling limits

**Failure Signatures:**
- Poor scaling behavior when using fixed validation sets
- Coverage dropping rapidly as instance size increases
- Policies optimized for small-instance performance that fail on larger instances

**First Experiments to Run:**
1. Compare dynamic validation against fixed validation with varying coverage thresholds
2. Test different policies (e.g., transformers, attention-based) with dynamic validation
3. Evaluate computational overhead of on-the-fly validation generation

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental scope limited to 9 planning domains with specific graph neural network architectures
- 30% coverage threshold is heuristic and may not generalize across all problem distributions
- Evaluation focuses on coverage metrics without detailed analysis of solution quality trade-offs
- Computational overhead of dynamic validation generation not thoroughly analyzed

## Confidence

**Policy selection methodology**: High - Dynamic validation approach is well-justified and experimentally validated across multiple domains.

**Coverage threshold selection**: Medium - The 30% threshold is reasonable but lacks theoretical derivation and may require tuning for different scenarios.

**Generalization across domains**: Medium - Results are strong within tested domains but limited in scope; may not generalize to all planning domains or policy types.

## Next Checks

1. Test dynamic validation with diverse policy architectures (e.g., transformers, attention-based models) to assess generalizability beyond graph neural networks.

2. Evaluate computational overhead of on-the-fly validation instance generation versus static validation sets to understand practical trade-offs.

3. Conduct ablation studies on the coverage threshold parameter (30%) to determine sensitivity and identify optimal values for different domain types.