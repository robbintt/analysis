---
ver: rpa2
title: 'SINAI at eRisk@CLEF 2022: Approaching Early Detection of Gambling and Eating
  Disorders with Natural Language Processing'
arxiv_id: '2509.14806'
source_url: https://arxiv.org/abs/2509.14806
tags:
- task
- posts
- early
- eating
- days
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The SINAI team participated in eRisk@CLEF 2022, addressing early
  detection of gambling and eating disorders using NLP. For gambling detection (Task
  1), they employed sentence embeddings from RoBERTa-large combined with features
  like volumetry, lexical diversity, complexity, and emotion detection.
---

# SINAI at eRisk@CLEF 2022: Approaching Early Detection of Gambling and Eating Disorders with Natural Language Processing

## Quick Facts
- arXiv ID: 2509.14806
- Source URL: https://arxiv.org/abs/2509.14806
- Reference count: 16
- SINAI team achieved second place in both eRisk@CLEF 2022 tasks with F1=0.808 for gambling detection and strong performance in automatic EDE-Q completion

## Executive Summary
The SINAI team participated in eRisk@CLEF 2022, addressing early detection of gambling and eating disorders using natural language processing. For gambling detection (Task 1), they employed sentence embeddings from RoBERTa-large combined with features like volumetry, lexical diversity, complexity, and emotion detection. A binary classification model achieved second place with an F1 score of 0.808 out of 41 teams. For eating disorders (Task 3), they used text similarity estimation via transformer embeddings to fill EDE-Q questionnaires, achieving second place out of 3 teams. The gambling model leveraged linguistic features alongside embeddings, while the eating disorder approach relied on heuristic similarity thresholds to answer questionnaire items.

## Method Summary
The gambling detection approach used RoBERTa-large sentence embeddings (1024-dim) concatenated with 79 handcrafted linguistic features (volumetry, lexical diversity, complexity, emotion scores) from the last 50 user posts. These were processed through a feedforward neural network (128 hidden units, dropout 0.5, ReLU) for binary classification. The eating disorder approach computed cosine similarity between user posts (last 28 days) and EDE-Q questionnaire items, using threshold-based heuristics (0.35-0.4) to map similarity scores to ordinal responses. Both tasks used RoBERTa embeddings via spaCy's sentence-transformers, with gambling detection trained on 81 positive users and ~2000 negative users.

## Key Results
- Task 1 (Gambling): F1 score of 0.808, achieving second place out of 41 teams
- Task 3 (Eating Disorders): MAE of 2.60, achieving second place out of 3 teams
- Binary classification outperformed regression (F1 0.808 vs 0.546) for gambling detection
- Heuristic similarity thresholds showed limitations with MZOE > 0.85 indicating >85% incorrect predictions for eating disorders

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining sentence embeddings with interpretable linguistic features improves pathological gambling detection.
- Mechanism: RoBERTa-large produces 1024-dimensional sentence embeddings; these are concatenated with 79 normalized features (volumetry, lexical diversity, complexity, emotion scores) and passed through a feedforward network with one hidden layer (128 units, dropout 0.5, ReLU). The fusion allows semantic patterns from embeddings to complement explicit markers like emotional expression and text complexity.
- Core assumption: The last 50 posts per user capture sufficient signal for gambling pathology; linguistic markers co-occur with disorder-related language.
- Evidence anchors:
  - [abstract] "approach presented in Task 1 is based on the use of sentence embeddings from Transformers with features related to volumetry, lexical diversity, complexity metrics, and emotion-related scores"
  - [section 2.2] "the 50 most recent posts were taken after evaluating different sizes"
  - [corpus] Related paper (SINAI at eRisk@CLEF 2023) shows continued use of this hybrid approach, suggesting reproducibility.
- Break condition: If users post infrequently or pathological signals appear earlier than the last 50 posts, detection may miss early markers; feature normalization could obscure absolute differences in writing patterns.

### Mechanism 2
- Claim: Binary classification with softmax outperforms regression with a fixed threshold for gambling detection.
- Mechanism: Binary classification outputs two class scores processed through softmax, allowing the model to learn class boundaries dynamically. Regression uses sigmoid output with a fixed 0.5 threshold, which constrains decision boundaries and may not align with optimal separation.
- Core assumption: The training data distribution is sufficiently balanced and the softmax decision boundary generalizes to unseen users.
- Evidence anchors:
  - [section 2.4] "the binary configuration is more convenient to train the system... determining the threshold after applying a sigmoid function on the output logit is not something the model can learn, as it is fixed on 0.5"
  - [section 2.3] Run 2 (binary) achieved F1=0.808 vs Run 0 (regression) F1=0.546
  - [corpus] Weak external validation; corpus papers focus on different tasks (depression, claim detection).
- Break condition: On severely imbalanced datasets, softmax may still produce biased class probabilities; calibration techniques may be needed.

### Mechanism 3
- Claim: Text similarity between user posts and questionnaire items can approximate EDE-Q responses for eating disorder severity.
- Mechanism: Posts from the last 28 days are encoded; cosine similarity is computed between each post and each questionnaire question. For day-based questions, posts exceeding a similarity threshold count toward day ranges. For scale-based questions, the highest similarity maps to ordinal responses (e.g., 0.6→0.65 → "markedly").
- Core assumption: Higher semantic similarity between a post and a questionnaire item indicates the user is expressing that symptom; the 28-day window aligns with EDE-Q instructions.
- Evidence anchors:
  - [section 3.2] "we computed the similarity between each post in a user's history with each question in the EDE-Q Questionnaire"
  - [section 3.4] MZOE > 0.85 indicates >85% incorrect predictions; MAE scores (2.60) suggest responses are not far from ground truth.
  - [corpus] No directly comparable similarity-based questionnaire completion papers found.
- Break condition: Users may discuss symptoms indirectly or sarcastically; similarity thresholds are heuristic and may not transfer across populations.

## Foundational Learning

- Concept: Sentence embeddings (RoBERTa-large)
  - Why needed here: Encodes user posts into dense vectors capturing semantic meaning; serves as the backbone for both tasks.
  - Quick check question: Can you explain why RoBERTa-large (1024 dims) might capture different information than feature-based TTR or readability scores?

- Concept: Lexical diversity metrics (TTR, MSTTR, HDD, MTLD)
  - Why needed here: Quantifies vocabulary variation; pathological users may exhibit distinct patterns (e.g., repetitive language about gambling).
  - Quick check question: Why might TTR alone be insufficient for short texts, and how does MTLD address this?

- Concept: Threshold-based decision vs learned decision boundaries
  - Why needed here: Understanding why fixed thresholds (0.5 sigmoid) underperform versus learned softmax boundaries informs model selection.
  - Quick check question: What happens to regression-based detection if positive cases are rare in the training set?

## Architecture Onboarding

- Component map: User posts (last 50, concatenated) -> RoBERTa-large tokenizer (max 512 tokens) -> [CLS] embedding (1024) + spaCy features (volumetry, TTR variants, complexity) + emotion scores (DistilBERT/BERT) (79 total) -> Concatenation (1103) -> FFNN (128 hidden, dropout 0.5, ReLU) -> Output layer (2 classes for binary, 1 for regression)

- Critical path:
  1. Data preprocessing (post selection, URL removal for Run 1)
  2. Feature extraction (parallel: embeddings + linguistic features)
  3. Fusion and classification
  4. Threshold tuning for early detection (ERDE optimization)

- Design tradeoffs:
  - 50 posts vs fewer: captures more signal but delays detection; affects ERDE/latency metrics
  - Binary vs regression: binary improves F1 but may sacrifice calibrated probability scores
  - Heuristic thresholds for Task 3: interpretable but brittle; no learning from ground truth questionnaires

- Failure signatures:
  - F1 drops significantly when using regression (0.546) vs binary (0.808) -> check decision boundary
  - Task 3 MZOE > 0.85 -> similarity-based heuristics failing to capture symptom expression
  - High ERDE with low latency-speed -> model triggers too late; reduce post window or adjust thresholds

- First 3 experiments:
  1. Ablation study: Remove each feature group (volumetry, lexical diversity, complexity, emotions) and measure F1 impact to quantify contribution.
  2. Threshold sweep for Task 3: Test similarity thresholds from 0.25-0.50 in 0.05 increments; track MAE and MZOE changes.
  3. Post window analysis: Compare 25, 50, 75 most recent posts to evaluate early detection tradeoff (ERDE vs F1).

## Open Questions the Paper Calls Out

- To what extent do the individual linguistic feature categories (volumetry, lexical diversity, complexity, emotion scores) contribute to gambling detection performance compared to pure transformer embeddings? The paper notes that "Further analysis is needed to determine how non-embeddings features contribute to the performance beyond pure end-to-end models" and plans to "analyze in depth the linguistic features considered in Task 1 to understand to what extent they contribute."

- What are the specific failure modes in automatic EDE-Q questionnaire completion, and which question types or content areas are most challenging for similarity-based approaches? The authors state they "would like to perform an error analysis to identify the main weaknesses of our system" and note that questions related to food restriction (RS) and shape preoccupation (SCS) had the highest error rates.

- Can supervised learning or alternative NLP approaches outperform similarity-based heuristics for automatic questionnaire completion when no labeled training data is available? The paper mentions exploring "other Natural Language Processing models" as future work, noting the task was "only test" with no training data provided.

## Limitations

- The gambling detection approach assumes the last 50 posts contain sufficient signal for pathology, which may not hold for all users and delays early detection.
- The eating disorder questionnaire completion relies entirely on heuristic similarity thresholds without learning from ground truth, making it brittle to language variation.
- The 79 linguistic features lack full specification in the paper, creating reproducibility barriers.
- Task 3's strong performance (second place) may reflect limited competition (3 teams) rather than robustness.

## Confidence

- **High confidence**: Binary classification outperforming regression for gambling detection (F1 0.808 vs 0.546); RoBERTa-large embeddings capturing semantic patterns relevant to pathology; the 50-post window selection based on empirical evaluation.
- **Medium confidence**: Feature fusion improving over embeddings alone (no ablation study presented); similarity thresholds (0.35-0.4) for questionnaire completion being optimal (heuristics not validated); 28-day window for Task 3 aligning with EDE-Q requirements.
- **Low confidence**: Generalization of Task 3 approach to other questionnaire types; robustness across different user populations; ERDE metrics reflecting true early detection capability given fixed post window.

## Next Checks

1. **Ablation study on linguistic features**: Remove each feature category (volumetry, lexical diversity, complexity, emotions) individually and measure F1 impact to quantify contribution and test the fusion hypothesis.

2. **Post window sensitivity analysis**: Compare performance using 25, 50, and 75 most recent posts to quantify the early detection tradeoff and identify the optimal window for minimizing ERDE while maintaining detection accuracy.

3. **Threshold optimization for Task 3**: Systematically test similarity thresholds from 0.25-0.50 in 0.05 increments on the Task 3 development set, tracking MAE and MZOE changes to validate whether the heuristic approach can be improved through data-driven calibration.