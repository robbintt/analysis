---
ver: rpa2
title: 'SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation
  Models'
arxiv_id: '2601.08623'
source_url: https://arxiv.org/abs/2601.08623
tags:
- saferedir
- unlearning
- unsafe
- content
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SafeRedir addresses the challenge of removing unsafe concepts from
  image generation models without requiring retraining or degrading benign generation
  quality. The method intervenes in the prompt embedding space by detecting unsafe
  content through multi-modal analysis of prompt embeddings, image latents, and diffusion
  timesteps, then redirecting only the unsafe tokens toward safe semantic regions
  using learned direction vectors, token masks, and adaptive scaling.
---

# SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models

## Quick Facts
- arXiv ID: 2601.08623
- Source URL: https://arxiv.org/abs/2601.08623
- Authors: Renyang Liu; Kangjie Chen; Han Qiu; Jie Zhang; Kwok-Yan Lam; Tianwei Zhang; See-Kiong Ng
- Reference count: 40
- Primary result: SafeRedir achieves 99.84% Forget Success Rate while maintaining 6.68% CLIP Score Difference Rate across multiple unlearning tasks

## Executive Summary
SafeRedir addresses the challenge of removing unsafe concepts from image generation models without requiring retraining or degrading benign generation quality. The method intervenes in the prompt embedding space by detecting unsafe content through multi-modal analysis of prompt embeddings, image latents, and diffusion timesteps, then redirecting only the unsafe tokens toward safe semantic regions using learned direction vectors, token masks, and adaptive scaling. Evaluations across multiple unlearning tasks (NSFW, artistic style, object removal) show SafeRedir achieves a Forget Success Rate of 99.84%, maintains high content preservation with a CLIP Score Difference Rate of 6.68%, and exhibits strong robustness to adversarial prompts while preserving image quality and generalizing effectively across diverse diffusion backbones.

## Method Summary
SafeRedir operates by hooking into the diffusion generation pipeline at the cross-attention conditioning interface. It first encodes the prompt using CLIP's text encoder, then at each denoising step extracts the current latent representation and timestep embedding. A multi-modal classifier fuses these three modalities through cross-attention to detect unsafe content. When unsafe prompts are detected, a redirector module predicts per-token direction vectors, adaptive scaling factors, and soft masks to selectively modify only the unsafe token embeddings before they reach the U-Net's cross-attention layers. The approach requires no model retraining and can be applied to any diffusion model with accessible text conditioning interfaces.

## Key Results
- Achieves 99.84% Forget Success Rate across multiple unlearning tasks (NSFW, Van Gogh style, Church removal)
- Maintains high content preservation with 6.68% CLIP Score Difference Rate
- Shows strong adversarial robustness with only 0.08% ASR on MMA dataset
- Demonstrates plug-and-play capability across four diffusion models (SD v1.4, v1.5, OpenJourney, Realistic Vision) with >94% FSR

## Why This Works (Mechanism)

### Mechanism 1: Multi-modal Context Fusion for Unsafe Detection
Combining prompt embeddings, image latents, and diffusion timesteps enables detection of unsafe content that single-modality approaches miss, particularly adversarially paraphrased prompts. A cross-attention module fuses encoded latent features, timestep embeddings, and prompt embeddings through multi-scale attention, with the fused representation feeding an MLP classifier that predicts safe/unsafe at each denoising step. Text-only detection achieves 51.68% accuracy on MMA adversarial dataset, while text+latent+timestep achieves 74.72%.

### Mechanism 2: Token-Level Adaptive Redirection
Localizing intervention to sensitive tokens via learned masks and adaptive scaling preserves benign semantics while redirecting unsafe content. Three components work together: a delta generator predicts per-token direction vectors, an alpha predictor produces scaling factors in [0,1], and a mask predictor produces soft masks in [0,1]. Token-wise masked redirection achieves better FSR/LPIPS balance than fixed scaling approaches.

### Mechanism 3: Inference-Time Hooking Without Model Modification
Intervening only at the cross-attention conditioning interface enables model-agnostic deployment without retraining. SafeRedir hooks into the diffusion pipeline at three points: prompt encoder output capture, U-Net forward interception, and cooldown schedule logic to prevent oscillation. The approach transfers effectively across different diffusion backbones without requiring model modification.

## Foundational Learning

- **Diffusion latent space dynamics**: Understanding that early timesteps contain near-Gaussian noise (latents ~50% accuracy for detection) while semantics materialize later is critical for knowing when intervention is feasible. *Quick check: At diffusion step 10 of 1000, would you expect latent-based detection to be reliable? Why or why not?*

- **Cross-attention conditioning in diffusion models**: SafeRedir works by replacing `encoder_hidden_states` fed to cross-attention layers—you must understand this interface to implement hooks correctly. *Quick check: In Stable Diffusion's U-Net, what role does the cross-attention layer play in conditioning generation on text prompts?*

- **Cosine similarity in embedding spaces**: Pseudo-ground-truth token masks are derived from cosine similarity between safe/unsafe token embeddings (threshold τ=0.2). *Quick check: Given two token embeddings with cosine similarity 0.95, would they likely be flagged for redirection? What does high similarity imply about semantic content?*

## Architecture Onboarding

- **Component map**: Prompt encoder -> Detector (ResidualSEBlocks + timestep encoder + cross-attention fusion -> MLP classifier) -> Redirector (shared multi-modal encoder -> three parallel heads: Δ generator with LoRA adapter, α predictor MLP, mask predictor self-attention+MLP) -> U-Net cross-attention conditioning interface

- **Critical path**: 1. Encode prompt → p_emb 2. At each timestep t: extract z_t, encode with detector, fuse with p_emb and t 3. If detector flags unsafe AND cooldown==0: compute Δ, α, m → redirect embedding 4. Pass p̂_emb to U-Net cross-attention as `encoder_hidden_states` 5. If redirected, set cooldown=K steps

- **Design tradeoffs**: Higher α scaling → better forgetting but risk of semantic drift and quality degradation (Table IV: α=3.0 achieves 99.87% FSR but FID degrades to 200.72); Strict masking → preserves benign content but may miss distributed unsafe semantics; Early-timestep intervention → more influence on generation but less reliable detection

- **Failure signatures**: Oscillation artifacts (images flicker between safe/unsafe across steps → increase cooldown K); Over-sanitization (benign subjects disappear → check mask predictor is not over-predicting); Adversarial bypass (unsafe content still appears → detector may be text-reliant; verify latent encoder receives correct z_t)

- **First 3 experiments**: 1. Detector ablation: Run detection with text-only vs. text+latent+timestep on IGMU validation set. Expected: multi-modal should achieve >95% accuracy, text-only should drop on paraphrased prompts. 2. Scaling sensitivity: Vary α_scale from 0.5 to 3.0 on NSFW task, plot FSR vs. FID tradeoff curve to find operating point. 3. Transfer test: Train on SD v1.4 data, deploy on OpenJourney without modification. Verify FSR >95% on held-out unsafe prompts to confirm plug-and-play claim.

## Open Questions the Paper Calls Out

### Open Question 1
Can SafeRedir's embedding-space redirection approach be effectively extended to transformer-based (e.g., DiT) or autoregressive image generation architectures, which have fundamentally different conditioning mechanisms than U-Net-based diffusion models? The conclusion states: "Future research directions include extending SafeRedir to additional generative model families beyond the Stable Diffusion series, such as transformer-based or autoregressive architectures." This remains unresolved as SafeRedir was designed and evaluated exclusively on U-Net-based Stable Diffusion variants.

### Open Question 2
How effectively can SafeRedir disentangle and redirect sensitive concepts that are deeply entangled with abstract or global scene semantics (e.g., political bias, implicit violence) rather than localized attributes like nudity or specific objects? The conclusion suggests exploring advanced techniques for disentangling sensitive concepts that are closely tied to abstract or global scene semantics. The current evaluation focuses on NSFW (local attribute), artistic style (global but concrete), and objects, but abstract concepts without clear visual correlates may require fundamentally different detection and redirection mechanisms.

### Open Question 3
How does SafeRedir perform when multiple distinct sensitive concepts must be unlearned simultaneously, and does token-level masking scale to handle concept interference or conflicts? While the paper evaluates individual unlearning tasks separately, its performance on concurrent multi-concept unlearning with potentially interfering redirection directions is not investigated. The redirection vectors are learned per concept, and multiple concepts may require incompatible semantic shifts.

### Open Question 4
Can reinforcement-guided or task-adaptive tuning strategies improve SafeRedir's ability to dynamically adjust redirection strength and direction for diverse prompt contexts? The conclusion suggests incorporating task-adaptive tuning or reinforcement-guided redirection strategies may enhance flexibility and safety compliance. The current adaptive scaling uses a learned MLP with sigmoid activation and a fixed gate mechanism, but more sophisticated adaptive mechanisms could potentially improve the safety-preservation trade-off.

## Limitations
- The methodology assumes unsafe concepts manifest distinctly in text embeddings and diffusion latents across timesteps, which may not hold for all prompt types
- Sensitive semantics must be concentrated in specific tokens that can be isolated via cosine similarity thresholds; distributed unsafe content may be missed
- The plug-and-play claim across diverse diffusion backbones rests on similar cross-attention interfaces, which may not hold for architectures like SD v2.x

## Confidence

**High Confidence Claims** (evidence anchors well-supported):
- SafeRedir achieves high FSR (>99%) on controlled unlearning tasks
- Multi-modal detection outperforms text-only baselines on adversarial prompts
- Transfer capability across diffusion models trained on different datasets
- Token-level redirection provides better content preservation than global scaling

**Medium Confidence Claims** (some evidence but limited scope):
- Robustness to adversarial paraphrasing (MMA dataset shows improvement but is synthetically generated)
- No quality degradation claim (FID scores are acceptable but not state-of-the-art)
- Plug-and-play capability across diverse backbones (tested on 4 models, but all share similar architecture lineage)

**Low Confidence Claims** (minimal direct evidence):
- Effectiveness against novel adversarial patterns not seen in training
- Performance on truly open-domain prompts outside curated safe/unsafe pairs
- Scalability to much larger models or different text encoder architectures

## Next Checks

1. **Out-of-Distribution Robustness Test**: Deploy SafeRedir on prompts from a completely different domain (e.g., medical imaging descriptions, legal document generation) containing novel terminology and evaluate FSR on both intended and unintended content. This validates whether the multi-modal detector generalizes beyond curated safe/unsafe pairs.

2. **Architectural Transfer Stress Test**: Apply SafeRedir to a diffusion model with fundamentally different text encoding (e.g., SD v2.x with CLIP-ViT/Local instead of ViT-L/14) and document any modifications required to the prompt encoder or embedding dimension handling. This directly tests the plug-and-play claim across architectural variations.

3. **Adversarial Prompt Evolution**: Generate a new set of adversarial prompts using different attack strategies (e.g., synonym substitution, prompt injection, context poisoning) not represented in the MMA dataset, and measure FSR degradation. This reveals whether the multi-modal detection relies too heavily on specific adversarial patterns seen during training.