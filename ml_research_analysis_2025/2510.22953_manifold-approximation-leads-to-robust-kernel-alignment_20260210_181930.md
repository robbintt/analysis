---
ver: rpa2
title: Manifold Approximation leads to Robust Kernel Alignment
arxiv_id: '2510.22953'
source_url: https://arxiv.org/abs/2510.22953
tags:
- kernel
- alignment
- kcka
- learning
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Manifold-approximated Kernel Alignment (MKA),
  a new method for comparing representations that incorporates manifold geometry into
  the alignment task. The authors propose a non-linear, non-Mercer kernel derived
  from manifold approximation and provide a theoretical framework for MKA.
---

# Manifold Approximation leads to Robust Kernel Alignment

## Quick Facts
- **arXiv ID**: 2510.22953
- **Source URL**: https://arxiv.org/abs/2510.22953
- **Reference count**: 40
- **Primary result**: MKA is more robust to hyperparameters and dimensionality than existing alignment methods while maintaining competitive performance across vision, NLP, and graph domains

## Executive Summary
This paper introduces Manifold-approximated Kernel Alignment (MKA), a new method for comparing representations that incorporates manifold geometry into the alignment task. The authors propose a non-linear, non-Mercer kernel derived from manifold approximation and provide a theoretical framework for MKA. Through extensive empirical evaluations on synthetic datasets and real-world examples, they demonstrate that MKA is more consistent under varying dimensionality and shapes that preserve topology. The method shows improved robustness to hyperparameters compared to existing approaches like CKA, RTD, and IMD. MKA performs competitively across diverse tasks in vision, natural language processing, and graph domains, while requiring less parameter tuning. The authors also show that MKA perceives neural network representations differently than CKA, with block structures being less pronounced. Overall, the paper presents a promising direction for representation comparison that could find applications in neuroscience, neural decoding, and graph learning.

## Method Summary
MKA builds a kernel matrix using k-nearest neighbor graphs to approximate the underlying manifold structure of representations. For each point, it computes local parameters ρᵢ (minimum neighbor distance) and σᵢ (determined via bisection to satisfy row-sum constraints). The resulting asymmetric kernel Kᵁ applies locally-adaptive exponential weighting within neighborhoods. Unlike CKA which uses full centering, MKA employs row-centering with a fixed row-sum constraint D = 1 + log₂(k). The alignment score is computed via a simplified closed-form formula derived from Theorem 4.1. This approach captures local manifold geometry rather than global density-weighted distances, making MKA more robust to hyperparameters while maintaining bounded outputs between 0 and 1.

## Key Results
- MKA achieves perfect ranking of topological similarity on synthetic datasets (Swiss-roll vs S-curve, rings/clusters) where CKA fails
- MKA shows consistent performance across k ∈ {15, 50, 100, 200} while kCKA exhibits unstable behavior
- On the ReSi benchmark, MKA achieves best mean rank of 2.33 compared to CKA (3.67), RTD (4.67), and IMD (4.67) across vision, NLP, and graph domains
- MKA requires less hyperparameter tuning than competing methods while maintaining competitive performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Constraining kernel matrices to k-nearest neighbor graphs improves topological consistency in representation comparison.
- Mechanism: The manifold-approximated kernel (Kᵁ) restricts pairwise relationships to local neighborhoods via k-NN, then applies locally-adaptive exponential weighting with parameters ρᵢ and σᵢ calibrated per-point. This captures local manifold geometry rather than global density-weighted distances that dominate standard CKA.
- Core assumption: Representations worth comparing share local neighborhood structure even if global geometries differ.
- Evidence anchors:
  - [abstract] "incorporates manifold geometry into the alignment task"
  - [section 5.1] MKA properly captures alignment between Swiss-roll and S-curve at r=0.5 while CKA(σ=M) fails; MKA "is very robust to the parameter" k compared to kCKA
  - [corpus] No direct validation of this specific mechanism in corpus; corpus contains CKA applications but not manifold-approximated variants
- Break condition: When representations have fundamentally different local structures (e.g., different cluster counts), MKA alignment will correctly decrease—but this may be undesirable if you need invariance to certain topological transformations.

### Mechanism 2
- Claim: Row-sum normalization to a fixed constant (D = 1 + log₂(k)) reduces sensitivity to hyperparameter choices.
- Mechanism: Each kernel row sums to the same value regardless of local density variations. This prevents outlier points with unusual neighbor distributions from dominating the alignment score, and imposes implicit rank ordering within neighborhoods.
- Core assumption: Equalizing "influence" across samples via row normalization yields more meaningful similarity than raw distance-based weighting.
- Evidence anchors:
  - [section 4] "This constraint fixes the row of the kernel matrix to a constant and makes the kernel less sensitive to lone outliers"
  - [section 5.2, Fig. 2e] MKA provides correct ranking for all values of k in rings experiment, while kCKA fails at k≥200
  - [corpus] No corpus papers validate this row-normalization mechanism directly
- Break condition: If your application specifically requires sensitivity to density variations (e.g., detecting mode collapse), this normalization may mask relevant signal.

### Mechanism 3
- Claim: Row-centering (not full centering) with row-sum constraints enables simplified closed-form computation.
- Mechanism: Under the row-sum constraint, Theorem 4.1 shows MKA reduces to a simple inner product formula without needing explicit centering matrix operations. This removes the O(n³) component from centering while maintaining bounded outputs (Corollary 4.2: 0 < MKA < 1 when D < √N).
- Core assumption: Row-centering alone is sufficient bias removal for this application; the authors acknowledge in Appendix D that this leaves residual bias but argue it remains meaningful.
- Evidence anchors:
  - [section 4, Theorem 4.1] Exact simplification formula provided
  - [section 5.6] Complexity analysis shows all methods remain O(n³) in n, but MKA has lower constant factors
  - [corpus] Corpus evidence weak; no papers validate the row-centering approximation
- Break condition: For very small datasets where D ≥ √N, bounds may not hold—verify empirically.

## Foundational Learning

- Concept: **Centered Kernel Alignment (CKA)**
  - Why needed here: MKA is explicitly positioned as a fix for CKA's limitations. You cannot understand what MKA improves without understanding CKA's HSIC-based formulation and its sensitivity to kernel bandwidth.
  - Quick check question: Can you explain why CKA with RBF kernel at large σ approximates linear CKA (Theorem 3.1)?

- Concept: **k-Nearest Neighbor Graphs for Manifold Approximation**
  - Why needed here: MKA's kernel is built on k-NN graphs. Understanding how neighborhood size k trades off local vs. global structure is essential for hyperparameter selection.
  - Quick check question: What happens to manifold approximation when k is set too small vs. too large?

- Concept: **Non-Mercer (Indefinite) Kernels**
  - Why needed here: MKA uses a non-symmetric, non-positive-semidefinite kernel. Standard kernel theory assumes Mercer kernels; understanding why MKA still works requires knowing that HSIC doesn't require PSD kernels.
  - Quick check question: Why doesn't MKA require a positive semidefinite kernel matrix?

## Architecture Onboarding

- Component map:
  Input representations X,Y → k-NN graph construction → Local parameter computation (ρᵢ, σᵢ via bisection) → Kernel matrix assembly (Kᵁ, Lᵁ) → Simplified alignment computation (Theorem 4.1) → Scalar similarity score

- Critical path:
  1. k-NN search (dominates runtime: O(n²(d + log k)))
  2. σᵢ bisection search per point (O(nk log(Δ/ε)))
  3. Sparse inner products (O(nk))

- Design tradeoffs:
  - **k selection**: Paper shows robustness across k=15-200, but recommends k=100 as default. Smaller k = more local, larger k = more global structure captured.
  - **Distance metric**: Paper uses Euclidean; other metrics may be appropriate for non-Euclidean data.
  - **Approximate k-NN**: For large n, approximate nearest neighbor methods can reduce quadratic cost—paper mentions but doesn't benchmark.

- Failure signatures:
  - **Oscillatory behavior under varying k**: If alignment scores fluctuate wildly as k changes, check for data with ambiguous local structure or insufficient samples.
  - **Scores stuck near boundaries (0 or 1)**: Verify D < √N; if violated, bounds from Corollary 4.2 don't apply.
  - **Disagreement with CKA on "obviously similar" representations**: This may be expected—MKA de-emphasizes block structures from dominant datapoints (Section 5.5).

- First 3 experiments:
  1. **Swiss-roll vs. S-curve sanity check**: Replicate Figure 1 with your implementation. Verify MKA peaks at r=0.5 and remains stable across k values.
  2. **Ablation on row-sum constraint**: Compare MKA against a variant without the 1 + log₂(k) constraint to quantify the robustness benefit.
  3. **Your own representations**: Apply MKA to compare layers of a network you're studying. Compare MKA's block structure patterns against CKA—do you see the "less pronounced blocks" effect the authors report?

## Open Questions the Paper Calls Out

- **Open Question 1**: Can effective resistance or diffusion distance-based kernel functions enhance MKA's ability to capture manifold geometry?
  - Basis in paper: The authors explicitly list "effective resistance" and "diffusion distance" as specific avenues for future work to explore different kernel functions.
  - Why unresolved: The current implementation relies exclusively on a UMAP-inspired kernel based on Euclidean distances within k-NN graphs.
  - What evidence would resolve it: A comparative study benchmarking MKA utilizing these alternative kernels against the current Euclidean-based implementation on the ReSi benchmark and synthetic topology datasets.

- **Open Question 2**: Do advanced debiasing techniques improve the statistical properties of MKA compared to the current row-centering approach?
  - Basis in paper: The conclusion suggests future work should "focus on additional debiasing techniques" beyond the simple row-wise centering currently employed.
  - Why unresolved: The paper notes that unlike CKA, MKA does not perform column-wise centering, leaving potential bias terms in the estimation.
  - What evidence would resolve it: Empirical evaluation of MKA using debiasing methods (e.g., from Sucholutsky et al.) to determine if they reduce estimation bias without compromising the manifold geometry preservation.

- **Open Question 3**: What factors cause MKA to underperform CKA in specific NLP tasks despite its advantages in vision and graph domains?
  - Basis in paper: The results show CKA is a "clear winner" in the NLP domain while MKA performs best in vision, suggesting the method's effectiveness varies by data modality.
  - Why unresolved: The paper establishes this performance gap empirically on the ReSi benchmark but does not provide a theoretical explanation for why manifold approximation is less effective for language model representations.
  - What evidence would resolve it: An ablation study analyzing the geometry of NLP embeddings versus vision features to identify if specific topological properties (e.g., lack of distinct manifold structure) limit MKA's utility in language tasks.

## Limitations

- The empirical robustness claims rely heavily on synthetic datasets and ReSi benchmarks; real-world domain shifts (e.g., distribution drift in production) remain untested
- No ablation studies on the choice of distance metric (Euclidean vs. alternatives) for non-Euclidean data like graphs
- The computational advantage over CKA is not quantified empirically—theoretical O(n³) bounds suggest similar scaling

## Confidence

- **High confidence**: MKA is implementable and produces scores in [0,1] with the described procedure; the manifold-approximation mechanism works as specified
- **Medium confidence**: MKA shows consistent hyperparameter robustness on synthetic datasets; competitive ReSi benchmark performance is validated
- **Low confidence**: Generalization to unseen domain shifts; sensitivity to non-Euclidean distance metrics; practical runtime advantage over CKA

## Next Checks

1. **Distribution shift test**: Apply MKA to representations before/after controlled domain shift (e.g., different image datasets, sentiment domains) to verify robustness beyond ReSi
2. **Metric sensitivity analysis**: Replace Euclidean distance with cosine/Manhattan for graph/sequence data; measure alignment score stability and downstream utility
3. **Large-scale runtime benchmark**: Compare MKA vs. CKA on datasets with N>10,000 to measure actual computational overhead and scalability limits