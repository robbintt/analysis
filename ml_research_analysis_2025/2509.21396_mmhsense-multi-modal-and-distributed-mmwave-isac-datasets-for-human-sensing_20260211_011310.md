---
ver: rpa2
title: 'mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human Sensing'
arxiv_id: '2509.21396'
source_url: https://arxiv.org/abs/2509.21396
tags:
- mmwave
- isac
- datasets
- task
- sensing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces mmHSense, a set of open labeled mmWave ISAC
  datasets for human sensing research, addressing the lack of publicly available datasets
  in this domain. The datasets cover various applications including gesture recognition,
  pose estimation, localization, and person identification, collected using both COTS
  devices and custom experimental platforms across multiple environments and users.
---

# mmHSense: Multi-Modal and Distributed mmWave ISAC Datasets for Human Sensing

## Quick Facts
- arXiv ID: 2509.21396
- Source URL: https://arxiv.org/abs/2509.21396
- Reference count: 16
- Introduces open labeled mmWave ISAC datasets for human sensing with 97.75% gesture recognition accuracy

## Executive Summary
This paper introduces mmHSense, a comprehensive set of open labeled mmWave ISAC datasets designed to address the scarcity of publicly available resources for human sensing research. The datasets encompass diverse applications including gesture recognition, pose estimation, localization, and person identification, collected using both commercial off-the-shelf devices and custom experimental platforms across multiple environments and users. By providing multi-modal signal features such as CSI, beam SNR, and power per beam, along with bi-static and multi-static configurations, the datasets enable researchers to advance deep learning techniques in mmWave ISAC systems.

The paper demonstrates the datasets' utility through various experimental tasks, showcasing high performance in gesture recognition (97.75% accuracy with ResNet18), pose estimation (6.2 cm MPJPE), and person identification (80.50% accuracy). Additionally, the study highlights the effectiveness of LoRA-based fine-tuning in maintaining performance on prior tasks while reducing computational overhead, making the datasets valuable for both advancing algorithmic research and addressing practical challenges in mmWave ISAC deployment.

## Method Summary
The mmHSense datasets were collected using a combination of COTS mmWave devices and custom experimental platforms across diverse environments including labs, corridors, and living rooms. Multiple users performed various gestures, poses, and movements while data was captured in both bi-static and multi-static configurations. The datasets incorporate rich signal features including Channel State Information (CSI), beam Signal-to-Noise Ratio (SNR), and power measurements per beam pair. Data preprocessing included filtering, normalization, and augmentation techniques to enhance robustness. Deep learning models including ResNet18, EfficientNet, and Transformer-based architectures were trained and evaluated on tasks such as gesture recognition, pose estimation, and person identification, with performance metrics reported for each application.

## Key Results
- Gesture recognition achieved 97.75% accuracy using ResNet18 on the 5GmmGesture dataset
- Pose estimation reached 6.2 cm MPJPE on the DISAC-mmVRPose dataset
- Person identification demonstrated 80.50% accuracy in multi-user scenarios
- LoRA-based fine-tuning maintained performance while reducing computational overhead

## Why This Works (Mechanism)
The mmHSense datasets work effectively because they capture rich, multi-modal mmWave signal features that encode spatial and temporal information about human movements. The combination of CSI, beam SNR, and power measurements provides complementary perspectives on the same physical phenomena, enabling deep learning models to learn robust representations. The diverse collection environments and multiple users create datasets with sufficient variability to train models that generalize well. The bi-static and multi-static configurations allow capturing different aspects of signal reflections and multipath effects, providing more complete information about human presence and movement.

## Foundational Learning
- **Channel State Information (CSI)**: Time-domain channel measurements capturing signal reflections; needed to understand how signals interact with human bodies; quick check: visualize amplitude and phase variations across subcarriers
- **Beam SNR (Signal-to-Noise Ratio)**: Quality metric for directional beams; needed to assess signal strength and reliability; quick check: plot SNR distribution across different beam directions
- **Power per Beam Pair**: Energy measurements for specific Tx-Rx beam combinations; needed to understand directional signal propagation; quick check: create heatmaps of power distribution across the spatial domain
- **Domain Adaptation**: Techniques for handling user-specific variations; needed because different users create different signal patterns for the same gestures; quick check: test accuracy drop when training on one user and testing on another
- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning method; needed to update models for new tasks without full retraining; quick check: compare parameter count and inference time between full fine-tuning and LoRA
- **Multi-static Configuration**: Multiple receivers distributed in space; needed to capture comprehensive spatial information; quick check: analyze how adding receivers improves localization accuracy

## Architecture Onboarding

Component Map: Signal Acquisition -> Preprocessing -> Feature Extraction -> Deep Learning Model -> Task-specific Output

Critical Path: The most critical path is from signal acquisition through preprocessing to feature extraction, as the quality of these stages directly impacts model performance. Raw mmWave signals must be properly filtered and normalized before feature extraction to ensure consistent input to deep learning models.

Design Tradeoffs: The choice between using raw time-domain signals versus extracted features involves balancing computational complexity against information preservation. Raw signals retain maximum information but require more processing power, while extracted features reduce dimensionality but may lose subtle patterns. The bi-static vs multi-static configurations tradeoff involves deployment complexity against sensing accuracy.

Failure Signatures: Common failure modes include multipath interference causing signal ambiguity, occlusions leading to incomplete data, and user-specific variations creating domain gaps. These manifest as sudden drops in accuracy, inconsistent predictions across similar gestures, and poor generalization to unseen users.

First Experiments:
1. Visualize CSI amplitude and phase patterns for different gestures to understand signal characteristics
2. Train a simple CNN on extracted features to establish baseline performance
3. Test domain adaptation techniques by training on one user and evaluating on another

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can domain adaptation techniques be refined to handle the significant domain gap caused by user-specific differences in mmWave ISAC gesture recognition, where conventional methods like ADDA often fail?
- Basis in paper: Section IV.a states that conventional methods often fail because "source features of one gesture may be mapped to target features of another gesture" due to variations across users.
- Why unresolved: The high variability in user gestures and environments creates a domain gap that current adaptation techniques cannot bridge effectively.
- What evidence would resolve it: Development of a domain adaptation algorithm that maintains high accuracy (>90%) on the 5GmmGesture dataset when tested on completely unseen users.

### Open Question 2
- Question: What is the specific trade-off between sensing accuracy and communication overhead when varying the number of spatial and temporal Tx/Rx beams in 5G mmWave ISAC systems?
- Basis in paper: Section IV.c suggests using the 5GmmGesture dataset to analyze "how the number of Tx and Rx beams... impact the sensing performance and communication overhead."
- Why unresolved: The paper provides the dataset (Power per Beam Pair) but does not quantify the cost of increasing beam density relative to the gain in gesture recognition fidelity.
- What evidence would resolve it: A comparative analysis plotting communication latency/throughput against gesture classification accuracy for different beam configurations.

### Open Question 3
- Question: How can neural networks be distributed across multiple devices in multi-static ISAC setups to reduce data transfer bottlenecks without compromising sensing fidelity?
- Basis in paper: Section IV.d identifies "split inference" as a research opportunity supported by the datasets to "reduce sensing data transfer bottlenecks."
- Why unresolved: Transferring raw high-frequency data from multiple distributed receivers to a central node is resource-intensive; optimal split points for the network are undefined.
- What evidence would resolve it: Implementation of a split inference model on the DISAC-mmVRPose dataset that achieves comparable MPJPE (approx. 6.2 cm) with significantly reduced bandwidth usage.

## Limitations
- Performance metrics are based on specific deep learning architectures and may not generalize across all potential applications
- The extent of environmental variability and its impact on algorithm robustness remains incompletely characterized
- Real-world deployment challenges including occlusions, multipath interference, and varying user behaviors could impact performance

## Confidence
- Dataset utility and application claims: High
- Performance metrics for specific tasks: Medium
- Generalization across environments and users: Medium
- LoRA effectiveness claims: Medium

## Next Checks
1. Evaluate dataset performance across additional deep learning architectures beyond those tested to establish broader model compatibility
2. Conduct cross-environment validation to quantify performance degradation in scenarios not included in the original dataset collection
3. Test LoRA-based fine-tuning on a wider range of hardware platforms and model sizes to verify computational overhead claims across different computational constraints