---
ver: rpa2
title: 'AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning'
arxiv_id: '2506.21612'
source_url: https://arxiv.org/abs/2506.21612
tags:
- representation
- sampling
- graph
- contextual
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AdaptGOT addresses the challenge of effective POI embedding learning
  by integrating multiple contextual information (geographical, co-occurrence, and
  textual) through a novel mixed sampling strategy and an adaptive encoder-decoder
  architecture. The model employs a Mixture-of-Expert (MoE) mechanism to dynamically
  adjust POI representations across tasks without retraining, while minimizing Jensen-Shannon
  divergence to preserve topological consistency.
---

# AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning

## Quick Facts
- **arXiv ID:** 2506.21612
- **Source URL:** https://arxiv.org/abs/2506.21612
- **Reference count:** 40
- **Primary result:** AdaptGOT achieves up to 48.3% improvement in Recall@5 for POI recommendation and 23.8% for next POI recommendation by integrating multi-context sampling with adaptive MoE architecture.

## Executive Summary
AdaptGOT introduces a pre-trained model for POI representation learning that addresses the limitations of existing methods by integrating geographical, co-occurrence, and textual contexts through an adaptive encoder-decoder architecture. The model employs a novel mixed sampling strategy that generates multiple contextual subgraphs and a Mixture-of-Experts mechanism to dynamically adjust representations for different downstream tasks without retraining. By minimizing Jensen-Shannon divergence, AdaptGOT preserves topological consistency while enabling zero-shot cross-city transfer learning. Experiments demonstrate superior performance across three tasks on real-world datasets.

## Method Summary
AdaptGOT consists of three core components: mixed sampling, GOT representation learning, and an adaptive encoder-decoder with MoE. The mixed sampling generates four subgraphs using KNN, density-based, importance-based, and category-aware strategies. The GOT encoder computes geographical, co-occurrence, and textual embeddings, fusing them through a modified attention mechanism that incorporates spatial and behavioral proximity. The adaptive aggregator uses MoE to select relevant subgraph representations per task, regularized by JS divergence to maintain global graph topology. Pre-training uses masked reconstruction with weighted loss across all modalities.

## Key Results
- Achieves 48.3% improvement in Recall@5 for POI recommendation in Los Angeles dataset
- Improves next POI recommendation by 23.8% in New York dataset
- Outperforms state-of-the-art pre-trained models like SpaBERT and CTLE across all evaluation metrics

## Why This Works (Mechanism)

### Mechanism 1: Multi-Context Entropy Maximization
The model combines four sampling strategies (KNN, density, importance, category) to enhance POI embedding expressivity by increasing node label entropy and reducing feature conflict. By concatenating features from diverse topologies, AdaptGOT lowers the probability of feature duplication in the 1-WL isomorphism test, enabling better discrimination between structurally similar POIs. The diversity from geographical versus categorical sampling translates into non-redundant signal rather than noise.

### Mechanism 2: GOT Relative Attention Fusion
AdaptGOT modifies attention scores by computing element-wise products of Query, Key, Geographical embedding, and Co-occurrence embedding. This forces the model to weigh neighbor importance based simultaneously on semantic similarity and spatial/behavioral proximity. The integration captures complex interrelations better than summing independent embeddings, with geographical distance and co-occurrence frequency serving as valid inductive biases for weighting semantic attention.

### Mechanism 3: MoE-Driven Topological Consistency
The Mixture-of-Expert gating mechanism allows dynamic selection of relevant subgraph representations for specific tasks while preserving global topology through JS divergence minimization. The adaptive aggregator learns sparse distributions over experts associated with different subgraph contexts, ensuring that downstream tasks benefit from appropriate structural views without drifting from the original graph distribution. This enables zero-shot cross-city transfer learning while maintaining generalizability.

## Foundational Learning

- **Concept: 1-WL (Weisfeiler-Leman) Isomorphism Test**
  - Why needed here: The paper justifies its mixed sampling approach by proving it improves the 1-WL test's discriminative power, which implies the GNN can distinguish different graph structures effectively.
  - Quick check question: If two POIs have the same category and neighbor count, why would adding "textual context" help the 1-WL test distinguish them?

- **Concept: Mixture of Experts (MoE)**
  - Why needed here: AdaptGOT relies on MoE to switch between geographical, co-occurrence, and textual contexts without retraining the whole model.
  - Quick check question: In AdaptGOT, what does the "Gating Network" (G) actually output, and how does the "Importance Loss" prevent it from selecting the same expert for every POI?

- **Concept: Jensen-Shannon (JS) Divergence**
  - Why needed here: This mathematical constraint ensures that sampled subgraphs don't lose the structural properties of the full POI graph.
  - Quick check question: Why is JS divergence used here instead of KL divergence? (Hint: Symmetry).

## Architecture Onboarding

- **Component map:** Input -> Mixed Sampler (4 subgraphs) -> GOT Encoder (Geo+Occ+Text) -> MoE Adaptive Aggregator -> Decoder (masked reconstruction)
- **Critical path:** Mixed Sampling → GOT Attention. If sampling generates irrelevant neighbors, the attention mechanism (which multiplies Geo/Occ weights) will propagate noise rather than signal.
- **Design tradeoffs:** Expressivity vs. Efficiency (4 sampling strategies increase theoretical expressivity but require more processing); Adaptivity vs. Stability (MoE allows adaptivity but needs JS divergence loss to prevent overfitting).
- **Failure signatures:** Over-smoothing (check embedding visualization for uniform distribution); Expert Collapse (monitor expert selection distribution and importance loss CV²).
- **First 3 experiments:** 1) Ablate Sampling: Remove one strategy and measure Recall drop to validate Theorem 1. 2) Attention Analysis: Visualize attention weights for "Next POI" vs. "POI Recommendation" tasks. 3) Cross-City Transfer: Train on NYC, test on Tokyo to evaluate zero-shot transfer performance.

## Open Questions the Paper Calls Out

- **Question:** How does the computational overhead of the MoE mechanism and mixed sampling strategies impact real-time inference latency compared to lighter pre-trained models?
  - Basis: Complex architecture with four sampling strategies and MoE layer increases theoretical time complexity.
  - Evidence needed: Empirical benchmarks comparing wall-clock time and memory usage during pre-training and fine-tuning.

- **Question:** Can the GOT representation module effectively incorporate visual features alongside text, geography, and co-occurrence without destabilizing adaptive weighting?
  - Basis: Current architecture restricts to GOT information, excluding visual modalities despite their potential value.
  - Evidence needed: Ablation studies adding visual embeddings to observe impact on MoE balance loss and metrics.

- **Question:** Does eliminating explicit time encoding in favor of spatio-temporal co-occurrence limit performance on tasks requiring fine-grained temporal dynamics?
  - Basis: Aggregating check-ins into co-occurrence counts may discard sequential nuance needed for time-sensitive predictions.
  - Evidence needed: Evaluation on tasks specifically focused on strict temporal sequences.

## Limitations
- Theoretical claims about 1-WL test improvements lack empirical validation of whether four sampling strategies produce genuinely complementary information
- JS divergence regularization importance shown through performance drops rather than evidence of preventing overfitting in practice
- Sentiment analysis component described abstractly without specifying implementation details (lexicon, model, or heuristic)

## Confidence
- **High confidence:** Multi-context embedding integration improves performance over single-context baselines
- **Medium confidence:** Specific four-sampling strategy combination is optimal (alternative combinations might yield similar results)
- **Medium confidence:** MoE with JS divergence is necessary for cross-task adaptability (alternative regularization methods could work)
- **Low confidence:** Theoretical claims about 1-WL test improvements directly translate to practical embedding quality

## Next Checks
1. **Sampling strategy ablation with correlation analysis:** Remove each strategy individually and measure Recall drop, but also compute pairwise correlation coefficients between generated neighborhoods to verify they produce genuinely diverse information.

2. **JS divergence ablation with overfitting analysis:** Compare validation loss trajectories and embedding similarity distributions (e.g., via t-SNE) between models with and without JS divergence loss to determine if it prevents overfitting versus simply regularizing weight norms.

3. **Cross-context attention visualization:** For specific POI pairs, visualize modified attention weights across geographical, co-occurrence, and textual modalities to verify the model appropriately weights different contexts based on task requirements.