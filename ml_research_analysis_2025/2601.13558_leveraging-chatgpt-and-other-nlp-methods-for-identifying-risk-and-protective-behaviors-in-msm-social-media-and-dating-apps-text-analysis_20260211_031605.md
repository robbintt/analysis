---
ver: rpa2
title: 'Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective
  Behaviors in MSM: Social Media and Dating apps Text Analysis'
arxiv_id: '2601.13558'
source_url: https://arxiv.org/abs/2601.13558
tags:
- data
- risk
- were
- text
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the use of text data from social media and
  dating apps to predict risk and protective behaviors among men who have sex with
  men (MSM). Machine learning models were trained using features derived from ChatGPT
  embeddings, BERT embeddings, LIWC analysis, and a custom dictionary-based approach
  for identifying risk terms.
---

# Leveraging ChatGPT and Other NLP Methods for Identifying Risk and Protective Behaviors in MSM: Social Media and Dating apps Text Analysis

## Quick Facts
- arXiv ID: 2601.13558
- Source URL: https://arxiv.org/abs/2601.13558
- Reference count: 40
- Key outcome: ChatGPT embeddings combined with LIWC and BERT achieved F1 scores of 0.78 for predicting monthly binge drinking and >5 sexual partners, demonstrating the potential of social media text data for public health risk assessment in MSM populations.

## Executive Summary
This study evaluated the use of text data from social media and dating apps to predict risk and protective behaviors among men who have sex with men (MSM). Machine learning models were trained using features derived from ChatGPT embeddings, BERT embeddings, LIWC analysis, and a custom dictionary-based approach for identifying risk terms. The models achieved strong performance in predicting monthly binge drinking and having more than five sexual partners, with F1 scores of 0.78. Moderate performance was observed in predicting PrEP use and heavy drinking, with F1 scores of 0.64 and 0.63, respectively. The findings demonstrate the potential of social media and dating app text data to provide valuable insights into risk and protective behaviors and highlight the effectiveness of large language model-based methods in supporting scalable and personalized public health interventions for MSM.

## Method Summary
The study analyzed text messages from Grindr, Tinder, Instagram, Snapchat, Twitter, and Reddit from 160 MSM participants who provided at least 30 days of data and 1000 messages each. Text was preprocessed to include only sent messages from the past six months. Four types of features were extracted: ChatGPT embeddings (text-embedding-ada-002), BERT embeddings (Twitter-sentiment pretrained), LIWC psychological features, and custom risk word frequencies from a 330-word dictionary. Feature selection was performed using Fisher scores with divide-and-conquer K search, and three classifiers (Logistic Regression, Gradient Boosting, SVM) were evaluated using leave-one-out cross-validation. The methodology focused on isolating risk-related content before embedding to improve signal-to-noise ratio.

## Key Results
- ChatGPT embeddings combined with LIWC and BERT achieved F1 scores of 0.78 for predicting monthly binge drinking and >5 sexual partners
- Moderate performance for PrEP use (F1=0.64) and AUDIT-C high drinking (F1=0.63) was observed
- Risk word filtering before embedding improved performance compared to embedding full user corpora
- Fisher Score-based feature selection was necessary to prevent overfitting given the high-dimensional feature space and small sample size

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Isolating specific vocabulary (risk words) prior to generating embeddings improves the signal-to-noise ratio for behavior prediction compared to embedding full user corpora.
- **Mechanism:** By filtering text to retain only terms from a predefined risk dictionary before applying the embedding model (ChatGPT), the vector representation concentrates on behavioral indicators rather than conversational filler. This "semantic filtering" ensures that high-dimensional vectors represent the presence of risk concepts rather than general linguistic style.
- **Core assumption:** The semantic meaning of risk is carried primarily by specific lexical items rather than their surrounding conversational context.
- **Evidence anchors:**
  - [section] (Page 5, Algorithm 2): "By directly focusing on the risk words, the embeddings become more representative... simultaneously eliminating the noise introduced by other non-risk content."
  - [section] (Table V): Shows "GPT RiskW" (risk words only) contributing the highest average feature weight compared to "GPT" (all messages) across most labels.
- **Break condition:** If users discuss risk behaviors using exclusively coded, non-dictionary language (e.g., novel slang), the dictionary filter will discard these signals before the embedding model can interpret them contextually.

### Mechanism 2
- **Claim:** Combining dense vector representations (LLM embeddings) with sparse psychological markers (LIWC) captures distinct behavioral signatures that neither feature set captures alone.
- **Mechanism:** LLM embeddings capture latent semantic context, while LIWC features capture explicit psychological categories (e.g., "analytical thinking"). The fusion allows the model to correlate specific psychological states with behavioral risk, improving upon the "black box" nature of pure deep learning features.
- **Core assumption:** Risk behaviors manifest through both the semantic content of messages and the psycholinguistic style of the user.
- **Evidence anchors:**
  - [abstract] "Combining ChatGPT embeddings with LIWC and BERT and using the most correlated features improved performance on the prediction task."
  - [section] (Page 3): LIWC computes features describing "social and psychological states... [and] emotional tone."
- **Break condition:** If the LIWC dictionary categories do not align with the specific subcultural language used by the target demographic (MSM), the sparse features add noise rather than signal.

### Mechanism 3
- **Claim:** Fisher Score-based feature selection is necessary to prevent overfitting when training high-dimensional models on small sample sizes (N=160).
- **Mechanism:** The raw feature space (BERT 768D + GPT 1536D + LIWC) vastly exceeds the sample size. Fisher Scores iteratively rank features by their inter-class variance, retaining only those that successfully distinguish between risk classes (e.g., Binge vs. No Binge), reducing dimensionality to a robust subset.
- **Core assumption:** The most predictive features for a specific individual generalize across the study population (i.e., a "global" best-feature-set exists).
- **Evidence anchors:**
  - [section] (Page 6, Algorithm 3): "Implementing the Fisher score selection enabled us to focus on the most informative features, hence, efficiently preventing overfitting."
  - [section] (Table IV): Shows performance improved from F1 0.64 (Naive) to 0.78 (Improved) for binge drinking after selection.
- **Break condition:** If the "most informative" features are highly non-linear or dependent on complex interactions (which Fisher scores may miss), the linear selection process might discard useful features.

## Foundational Learning

- **Concept: Leave-One-Out Cross-Validation (LOOCV)**
  - **Why needed here:** With only 160 participants, standard train/test splits would result in high variance. LOOCV uses N-1 participants for training and 1 for testing, cycling through all, maximizing the training data available for each evaluation step.
  - **Quick check question:** Why is LOOCV preferred over a standard 80/20 split when N=160 and feature dimensions are >1000?

- **Concept: Embedding Vectors (text-embedding-ada-002)**
  - **Why needed here:** The paper converts text into 1536-dimensional vectors. Understanding that these vectors represent "semantic closeness" is key to why "risk words" can be mathematically correlated with survey answers.
  - **Quick check question:** Does the embedding capture the *presence* of a word or the *meaning* of a sentence, and how does averaging these vectors affect the representation of a user's entire history?

- **Concept: Class Imbalance (AUDIT-C High vs. Low)**
  - **Why needed here:** The dataset is imbalanced (e.g., 35% High Risk vs. 65% Low Risk for AUDIT-C). The paper reports F1 scores rather than Accuracy to handle this imbalance.
  - **Quick check question:** Why would reporting "Accuracy" be misleading if 90% of the population were "Low Risk"?

## Architecture Onboarding

- **Component map:** Raw text (JSON/HTML) -> User-level CSVs -> Filter for User ID, Date, Message (exclude received) -> Extract features (Dictionary: Risk Word frequencies; Semantic: Extract Risk Words -> OpenAI API `text-embedding-ada-002` -> Average vectors; Psychometric: LIWC analysis -> Average scores) -> Calculate Fisher Scores -> Select Top-K features -> SVM (Linear Kernel) -> Probability Output

- **Critical path:** The **Risk Word extraction -> GPT Embedding** pipeline (Algorithm 2). This is where the paper claims the highest "information gain." If this step is implemented incorrectly (e.g., embedding the whole message instead of just the risk words), performance degrades.

- **Design tradeoffs:**
  - *Naive vs. Modified Embedding:* The "Naive" approach (embedding whole message blocks) failed because API token limits and noise diluted the signal. The "Modified" approach (extracting risk words only) is computationally cheaper and more accurate but risks losing context.
  - *Model Complexity:* The team chose Logistic Regression, Gradient Boosting, and SVM. They favored interpretability and robustness on small data over complex Deep Neural Networks (DNNs), which would likely overfit.

- **Failure signatures:**
  - *Low F1 on PrEP/Heavy Drinking:* The paper notes that less frequent behaviors or those with subtler linguistic markers (PrEP use) yielded lower scores (0.64). This suggests the features derived from "Risk Words" are highly sensitive to alcohol/sex terms but less sensitive to medication adherence.
  - *Overfitting:* If the Fisher Score threshold is not strictly calculated on the training set inside the LOOCV loop, data leakage would occur (Page 6 warning).

- **First 3 experiments:**
  1. **Baseline Reproduction:** Train a Logistic Regression model using *only* the "Risk Word" frequencies (no embeddings) to establish a performance floor.
  2. **Ablation Study:** Train the best-performing model (SVM) using *only* "GPT Risk Words" embeddings vs. *only* "GPT All Messages" embeddings to validate the paper's central claim about noise reduction.
  3. **Feature Stability:** Run the Fisher Score selection on 5 random subsets of the data to see if the "Top-K" features remain consistent, testing the robustness of the selected signal.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Do personalized public health interventions derived from automated text-based risk detection significantly reduce harmful drinking and sexual risk behaviors among MSM compared to standard care?
- **Basis in paper:** [explicit] The Conclusion states that "further research is required to evaluate the effectiveness of interventions based on social media text data."
- **Why unresolved:** This study focused on the feasibility of predicting behaviors using NLP, not on measuring the longitudinal health outcomes of interventions based on those predictions.
- **What evidence would resolve it:** A randomized controlled trial comparing health outcomes of participants receiving text-analysis-based interventions versus a control group.

### Open Question 2
- **Question:** Does the predictive performance of ChatGPT embeddings for risk behaviors improve or stabilize when applied to datasets significantly larger than the current sample of 160 users?
- **Basis in paper:** [explicit] The Discussion notes that "to fully explore the power of ChatGPT Embeddings, additional analysis on larger datasets may unveil even more potential."
- **Why unresolved:** The study was limited by a small sample size (n=160), which restricts the ability to validate the generalizability and robustness of the LLM-based features.
- **What evidence would resolve it:** Replication of the methodology using a dataset with thousands of participants to assess changes in F1 scores and model robustness.

### Open Question 3
- **Question:** To what extent does the inclusion of multimedia data, such as images or internet search history, improve the predictive accuracy of models for PrEP use and sexual risk behaviors beyond text-only analysis?
- **Basis in paper:** [explicit] The Limitations section suggests that "incorporating image data exchanged between individuals could have significantly enhanced our predictive capabilities."
- **Why unresolved:** iOS platform restrictions and consent protocols limited data collection to text messages, excluding potentially rich visual or behavioral data sources.
- **What evidence would resolve it:** A comparative study on a platform (e.g., Android) that allows for the collection and integration of image and search data alongside text.

## Limitations
- The performance gap between predicted behaviors and clinically actionable risk assessment remains unclear - the paper does not establish clinical validity thresholds or evaluate false positive/negative costs in real-world screening contexts.
- The 330-item risk word dictionary and 33 risk categories are not provided, preventing independent validation of the core "semantic filtering" mechanism.
- The model's generalizability is uncertain - training on 160 MSM participants from a single study may not capture demographic or cultural variations in risk language.

## Confidence
- **High Confidence**: BERT + LIWC + risk word frequency features show consistent moderate performance across multiple behaviors (PrEP F1=0.64, AUDIT-C F1=0.63).
- **Medium Confidence**: ChatGPT embeddings show strong performance for alcohol/sexual risk behaviors (F1=0.78) but the specific advantage of "risk word only" filtering over full-text embeddings requires replication with provided dictionary.
- **Low Confidence**: Clinical utility claims - the paper does not demonstrate that these predictions would meaningfully improve public health interventions beyond existing survey methods.

## Next Checks
1. **Dictionary Verification**: Replicate the "risk word only" embedding approach using the published 330-word list on an independent MSM text corpus to confirm the noise-reduction mechanism.
2. **Clinical Threshold Analysis**: Apply the best-performing model (SVM with F1=0.78 for binge drinking) to calculate positive/negative predictive values against established clinical screening criteria for hazardous drinking.
3. **Demographic Generalizability Test**: Train the feature extraction pipeline on text from MSM participants of different ages, ethnicities, or geographic regions to assess whether the LIWC + BERT feature combination maintains predictive power across populations.