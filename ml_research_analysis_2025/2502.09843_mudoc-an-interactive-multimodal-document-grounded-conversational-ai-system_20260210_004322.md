---
ver: rpa2
title: 'MuDoC: An Interactive Multimodal Document-grounded Conversational AI System'
arxiv_id: '2502.09843'
source_url: https://arxiv.org/abs/2502.09843
tags:
- text
- image
- images
- mudoc
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MuDoC is a multimodal conversational AI system that generates document-grounded
  responses by interleaving text and figures from PDF documents. It uses GPT-4o with
  embedding-based retrieval (DPR and CLIP) to extract and rank relevant text chunks
  and images, and replaces figure references in responses with actual figures from
  the source document.
---

# MuDoC: An Interactive Multimodal Document-grounded Conversational AI System

## Quick Facts
- arXiv ID: 2502.09843
- Source URL: https://arxiv.org/abs/2502.09843
- Authors: Karan Taneja; Ashok K. Goel
- Reference count: 5
- Generates multimodal, document-grounded responses with interleaved text and figures from PDFs

## Executive Summary
MuDoC is a multimodal conversational AI system that generates document-grounded responses by interleaving text and figures from PDF documents. It uses GPT-4o with embedding-based retrieval (DPR and CLIP) to extract and rank relevant text chunks and images, and replaces figure references in responses with actual figures from the source document. The system features an interactive interface enabling navigation to source content by clicking on figures or text paragraphs, plus summarize and ELI10 (Explain Like I'm 10) tools for text selection. Testing with a 357-page AI textbook showed relevant image inclusion and effective navigation, though some figure placement issues, hallucinations, and short text mappings were observed.

## Method Summary
The system preprocesses PDF documents using Mask R-CNN for layout analysis, Tesseract OCR for text extraction, and GPT-3.5 for text cleaning/summarization. Images are captioned and described using GPT-4 Vision. DPR and CLIP embeddings are computed for text chunks and images respectively. During runtime, user queries trigger retrieval of top-5 text chunks (via DPR cosine similarity) and top-5 images (via combined DPR/CLIP scoring). GPT-4o generates responses with HTML image tags that are replaced with actual figures during rendering. The ReactJS + Flask interface enables navigation to source content via stored bounding boxes for images and DPR embedding similarity for text paragraphs.

## Key Results
- Relevant images were successfully included in responses during testing with a 357-page AI textbook
- Source content navigation was effective, with clickable figures and text paragraphs linking to original locations
- Figure placement in responses was sometimes suboptimal, with excessive text between images and some figures only loosely connected to topics
- Hallucinations occurred more frequently when multiple images were provided to GPT-4o

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding-based retrieval enables relevant text and image extraction from long documents by matching query semantics to pre-computed content representations.
- Mechanism: DPR context encoder creates dense embeddings for text chunks (raw, cleaned, summary), while CLIP provides aligned image-text embeddings. Cosine similarity between query embeddings and pre-computed content embeddings ranks relevance. For images, the system combines DPR and CLIP scores (mean of max similarities across representations) to select top-5 candidates.
- Core assumption: Semantic similarity in embedding space correlates with actual relevance to user queries; GPT-4o can refine selection from retrieved candidates.
- Evidence anchors:
  - [abstract] "uses GPT-4o with embedding-based retrieval (DPR and CLIP) to extract and rank relevant text chunks and images"
  - [section: Response Generation] "Maximum cosine similarity across raw, cleaned, and summary text is used as the text chunk score, and the top-5 chunks are used as retrieval output"
  - [corpus] Related work on multimodal retrieval (MMDocIR, PatentLMM) confirms embedding-based approaches for document figure retrieval, though direct comparison to MuDoC's specific dual-encoder approach is not available in corpus.
- Break condition: If query vocabulary diverges significantly from document vocabulary, or if figures lack descriptive captions, retrieval quality degrades. Paper notes: "hallucinations occur more often when multiple images are provided to GPT-4o."

### Mechanism 2
- Claim: Interleaved text-image responses emerge from prompting GPT-4o to reference retrieved images using structured HTML tags that are post-processed into actual figures.
- Mechanism: GPT-4o tool calls feature triggers retrieval queries. Retrieved images are appended as user messages with filenames/captions. System instructions direct GPT-4o to insert `<img>` tags with filenames. During rendering, a substitution step replaces tags with actual image snippets and generated captions, producing blog/article-style multimodal output.
- Core assumption: GPT-4o can appropriately judge when visuals are "useful" and select correct images from retrieval results; HTML tag substitution preserves response coherence.
- Evidence anchors:
  - [abstract] "replaces figure references in responses with actual figures from the source document"
  - [section: Response Generation] "The system message contains instructions to... refer to image using HTML image and caption tag and a description in the text"
  - [corpus] No direct corpus evidence on interleaved generation from document-grounded systems; related work (DreamLLM, ANOLE) focuses on generated rather than retrieved images.
- Break condition: When GPT-4o references non-existent filenames or misaligns image references with text content. Paper observes: "figure placement in responses is not always optimal" and "some figures were only loosely connected to the topics."

### Mechanism 3
- Claim: Clickable response elements enable source verification by mapping response paragraphs and images back to original document locations via stored bounding boxes and embedding similarity.
- Mechanism: For images, stored bounding boxes from preprocessing enable direct navigation. For text paragraphs (which are rephrased, not copied), post-processing computes cosine similarity between paragraph DPR embeddings and raw text snippet embeddings. Paragraphs above threshold and 100 characters get mapped to source locations; clicking scrolls and highlights the source for 3 seconds.
- Core assumption: Rephrased text retains sufficient semantic overlap with source for embedding-based mapping; users benefit from source verification rather than relying solely on AI output.
- Evidence anchors:
  - [abstract] "interactive interface enabling navigation to source content by clicking on figures or text paragraphs"
  - [section: User Interface] "Paragraphs with a length below 100 characters or similarity scores below a threshold are not mapped"
  - [corpus] Corpus lacks comparative systems with source navigation features; this appears novel among document-grounded conversational systems.
- Break condition: Short or generic paragraphs fail mapping. Paper notes: "paragraphs in some responses were mapped to short headings in the document instead of a complete text snippet."

## Foundational Learning

- Concept: Dense Passage Retrieval (DPR)
  - Why needed here: DPR provides the primary text retrieval mechanism; understanding bi-encoder architecture (query encoder vs. context encoder) and why embeddings are pre-computed is essential for debugging retrieval failures.
  - Quick check question: Given a user query about "neural network backpropagation," would DPR retrieve chunks containing "gradient descent" if the document never uses both terms together?

- Concept: CLIP (Contrastive Language-Image Pre-training)
  - Why needed here: CLIP enables cross-modal retrieval where text queries find relevant images; understanding that CLIP aligns image and text in shared embedding space explains why image captions/descriptions improve retrieval.
  - Quick check question: Why does MuDoC compute both CLIP and DPR embeddings for image captions, and what would happen if only one were used?

- Concept: Document Layout Analysis (DLA)
  - Why needed here: The entire preprocessing pipeline depends on accurate region classification (text/figure/table); errors here propagate to retrieval and response quality.
  - Quick check question: If DLA misclassifies a figure caption as body text, which downstream steps would fail and how would the user experience be affected?

## Architecture Onboarding

- Component map:
  - **Preprocessing**: PDF → DLA (Mask R-CNN) → OCR (Tesseract) → Text cleaning/summarization (GPT-3.5) → Image captioning (GPT-4 Vision) → Embedding generation (DPR + CLIP) → Index storage
  - **Runtime**: Query → DPR/CLIP query embedding → Cosine similarity retrieval → GPT-4o with tool calls → Response with `<img>` tags → Tag substitution → UI render
  - **Post-processing**: Paragraph-to-source mapping via DPR embedding similarity
  - **UI**: ReactJS frontend + Flask backend + PDF.js viewer + SSE streaming

- Critical path:
  1. DLA accuracy → extraction quality → embedding quality → retrieval relevance
  2. Image caption quality → CLIP retrieval accuracy → GPT-4o image selection
  3. System prompt design → GPT-4o tool call behavior → response format compliance

- Design tradeoffs:
  - Chunk size (2000 chars, 500 overlap): Larger chunks provide context but may dilute relevance signals; smaller chunks improve precision but lose context
  - Top-5 retrieval vs. fewer: More candidates give GPT-4o selection flexibility but increase hallucination risk (paper observes this with multiple images)
  - Threshold for text navigation mapping: Higher threshold reduces false mappings but leaves more paragraphs non-clickable

- Failure signatures:
  - Hallucinated image descriptions when multiple images provided → reduce retrieved image count
  - Figures placed in separate sections with excessive text → adjust system prompt for tighter integration
  - Paragraphs mapping to headings instead of substantive text → increase minimum character threshold or require semantic density check
  - Non-clickable short paragraphs → expected behavior below 100 chars; consider merging adjacent short segments

- First 3 experiments:
  1. Retrieval ablation: Compare retrieval quality using DPR-only vs. CLIP-only vs. combined scoring on a held-out query set; measure precision@5 for both text and images
  2. Image count sensitivity: Test response quality with top-1, top-3, top-5 retrieved images; measure hallucination rate and user preference in blind comparison
  3. Navigation accuracy evaluation: Sample 50 clicked paragraphs; manually verify if highlighted source text is semantically equivalent to response paragraph; tune similarity threshold based on results

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can response quality and educational effectiveness of multimodal document-grounded systems be quantitatively evaluated?
- Basis in paper: [explicit] "In future work, we will quantitatively evaluate its performance in terms of response quality and its effectiveness in education as a tool for learners."
- Why unresolved: Current evaluation is based solely on qualitative observations from informal user feedback on a single document.
- What evidence would resolve it: Controlled user studies with measurable learning outcomes, response accuracy metrics, and comparison to baselines.

### Open Question 2
- Question: What is the optimal number of retrieved images to balance visual relevance against hallucination risk in interleaved responses?
- Basis in paper: [inferred] Authors observe "hallucinations occur more often when multiple images are provided to GPT-4o" and suggest "reducing the number of retrieved images may improve visual coherence."
- Why unresolved: Current top-5 retrieval threshold was not systematically tested; trade-off between coverage and coherence remains unquantified.
- What evidence would resolve it: Ablation study varying retrieved image counts (1–10) with human ratings of relevance, coherence, and hallucination frequency.

### Open Question 3
- Question: How can text-to-source mapping be improved to avoid linking response paragraphs to uninformative short headings?
- Basis in paper: [inferred] Authors report "paragraphs in some responses were mapped to short headings in the document instead of a complete text snippet which can lead to perceptual difficulty in navigation."
- Why unresolved: Current approach uses DPR embedding similarity with fixed thresholds (100 characters, similarity score cutoff) without semantic validation.
- What evidence would resolve it: Testing alternative mapping strategies (e.g., chunk-level alignment, semantic classifiers) with navigation success rates.

### Open Question 4
- Question: How well does MuDoC generalize to documents beyond technical textbooks (e.g., reports, manuals, scientific papers)?
- Basis in paper: [inferred] System was tested on only one document—a 357-page AI textbook—with no evaluation on other domains or document structures.
- Why unresolved: Layout analysis relies on PubLayNet-trained models; retrieval and response quality may vary with different figure types, layouts, and content densities.
- What evidence would resolve it: Evaluation across diverse document corpora with varying lengths, domains, and visual complexity.

## Limitations

- Evaluation based solely on qualitative observations from informal user feedback on a single 357-page AI textbook, without quantitative metrics or diverse document types
- System shows sensitivity to retrieved image count, with hallucinations increasing when multiple images are provided to GPT-4o
- Paragraph-to-source mapping fails for short paragraphs and sometimes maps to uninformative headings rather than substantive text

## Confidence

- Document-grounded conversational system design: High
- Embedding-based retrieval effectiveness: High
- Multimodal response generation: Medium
- Source navigation implementation: Medium
- Quantitative evaluation validity: Low

## Next Checks

1. Implement the preprocessing pipeline end-to-end with a sample PDF and verify that layout analysis, OCR, and embedding generation complete successfully
2. Test retrieval quality by running sample queries and checking if retrieved text chunks and images are semantically relevant to the queries
3. Validate the paragraph-to-source mapping by selecting paragraphs from generated responses and confirming they highlight the correct source content in the document viewer