---
ver: rpa2
title: 'OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application
  Workflows'
arxiv_id: '2508.09124'
source_url: https://arxiv.org/abs/2508.09124
tags:
- task
- agents
- tasks
- agent
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OdysseyBench, a comprehensive benchmark for
  evaluating language agents on long-horizon workflows across multiple office applications
  including Word, Excel, PDF, Email, and Calendar. The benchmark addresses the limitation
  of existing atomic task benchmarks by requiring agents to identify essential information
  from long-horizon interaction histories and perform multi-step reasoning across
  various applications.
---

# OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows

## Quick Facts
- arXiv ID: 2508.09124
- Source URL: https://arxiv.org/abs/2508.09124
- Authors: Weixuan Wang; Dongge Han; Daniel Madrigal Diaz; Jin Xu; Victor Rühle; Saravan Rajmohan
- Reference count: 31
- Evaluates LLM agents on long-horizon workflows across Word, Excel, PDF, Email, and Calendar applications

## Executive Summary
OdysseyBench introduces a comprehensive benchmark for evaluating language model agents on complex, multi-application office workflows. The benchmark addresses limitations of existing atomic task evaluations by requiring agents to perform multi-step reasoning across different applications while managing long interaction histories. The authors developed HomerAgents, an automated framework that generates realistic workflow scenarios through systematic environment exploration and task synthesis, creating both derived tasks from existing atomic benchmarks and entirely new complex workflows.

The benchmark demonstrates that task complexity increases with the number of applications involved, leading to performance degradation in state-of-the-art LLM agents. By incorporating multi-day dialogues with extensive interaction histories, OdysseyBench provides a more realistic assessment of agents' capabilities in handling real-world office scenarios compared to traditional atomic task benchmarks. The systematic approach to benchmark generation and evaluation offers a scalable methodology for developing more challenging and realistic LLM evaluation frameworks.

## Method Summary
The HomerAgents framework automates long-horizon workflow benchmark generation through two complementary approaches. HomerAgents+ transforms existing atomic tasks into complex, contextually rich scenarios by expanding task scopes and introducing realistic environmental interactions. HomerAgents-Neo generates entirely new complex tasks from scratch through systematic exploration of the application environment. The framework uses GPT-4 for task generation and evaluation, ensuring consistency and scalability. Each task is documented through multi-day dialogues with minimum requirements of five days per task and ten utterances per dialogue, creating realistic interaction histories that agents must navigate.

The benchmark consists of two splits: OdysseyBench+ with 300 tasks derived from real-world use cases and OdysseyBench-Neo with 302 newly synthesized complex tasks. Evaluation involves comprehensive testing of state-of-the-art LLM agents across these tasks, measuring performance degradation as task complexity increases. The methodology emphasizes semantic compression and coherent aggregation as essential components for effective multi-step reasoning across applications.

## Key Results
- OdysseyBench effectively challenges state-of-the-art LLM agents with complex, multi-application workflows
- Performance degrades as the number of applications involved in tasks increases
- Semantic compression and coherent aggregation are essential for successful multi-step reasoning
- Multi-day dialogue structure with extensive interaction histories provides more realistic assessment than atomic task benchmarks

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its systematic approach to generating realistic, complex workflows that mirror real-world office scenarios. By requiring agents to navigate multiple applications while managing long interaction histories, OdysseyBench exposes limitations in current LLM architectures' ability to perform sustained reasoning and context management. The automated generation process ensures scalability while maintaining consistency in task difficulty and realism.

## Foundational Learning

1. **Multi-step reasoning across applications**
   - Why needed: Real-world office tasks require coordinating actions across multiple software tools
   - Quick check: Agent successfully completes workflows requiring at least three different applications

2. **Semantic compression**
   - Why needed: Long interaction histories require effective summarization to maintain context
   - Quick check: Agent maintains task continuity after compressing 10+ previous interactions

3. **Temporal reasoning**
   - Why needed: Multi-day dialogues require understanding and managing time-based dependencies
   - Quick check: Agent correctly sequences actions across multiple days

4. **Context management**
   - Why needed: Real workflows involve maintaining situational awareness across extended interactions
- Quick check: Agent recalls and applies information from earlier in the conversation

## Architecture Onboarding

**Component Map**: HomerAgents+ -> Task Transformation -> Scenario Generation -> Dialogue Synthesis
HomerAgents-Neo -> Environment Exploration -> Task Generation -> Dialogue Synthesis
Evaluation Pipeline -> GPT-4 Assessment -> Performance Metrics

**Critical Path**: Environment Exploration → Task Generation → Dialogue Synthesis → Agent Evaluation → Performance Analysis

**Design Tradeoffs**: Automated generation enables scalability but may miss nuanced real-world complexities; GPT-4 evaluation ensures consistency but introduces potential circularity

**Failure Signatures**: Performance degradation with increasing application count; inability to maintain context across multi-day dialogues; failure to identify essential information in long histories

**First Experiments**:
1. Test single-agent performance on OdysseyBench+ tasks
2. Evaluate multi-agent collaboration on OdysseyBench-Neo workflows
3. Measure performance impact of varying interaction history lengths

## Open Questions the Paper Calls Out
None

## Limitations
- Automated task generation may not fully capture real-world workflow complexity and ambiguity
- Heavy reliance on GPT-4 for both task generation and evaluation creates potential circularity
- Benchmark scope limited to five office applications, unclear applicability to other domains
- Multi-day dialogue structure may not fully represent asynchronous real office communication

## Confidence
- **High**: Technical implementation of HomerAgents framework and benchmark generation pipeline
- **Medium**: Claims about performance degradation with increasing application complexity
- **Medium**: Comparative assessment of OdysseyBench versus atomic task benchmarks

## Next Checks
1. Conduct human evaluation studies to validate GPT-4's task complexity ratings and performance assessments
2. Test OdysseyBench with additional LLM agents beyond those evaluated in the paper
3. Perform ablation studies to quantify the contribution of individual components in the HomerAgents framework to overall benchmark quality