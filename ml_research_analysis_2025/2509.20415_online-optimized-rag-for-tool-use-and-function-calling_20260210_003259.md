---
ver: rpa2
title: Online-Optimized RAG for Tool Use and Function Calling
arxiv_id: '2509.20415'
source_url: https://arxiv.org/abs/2509.20415
tags:
- tool
- retrieval
- algorithm
- arxiv
- embeddings
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Online-Optimized RAG, a deployment-time framework
  that improves retrieval-augmented generation (RAG) systems by continuously adapting
  embedding models using minimal feedback. The method addresses the problem of embedding
  misalignment due to imperfect models or noisy descriptions, which can lead to incorrect
  tool/function retrieval and task failure.
---

# Online-Optimized RAG for Tool Use and Function Calling

## Quick Facts
- arXiv ID: 2509.20415
- Source URL: https://arxiv.org/abs/2509.20415
- Reference count: 40
- Primary result: Improves RAG systems by continuously adapting embeddings using minimal feedback, achieving 8.05% accuracy gain on UltraTool

## Executive Summary
This paper introduces Online-Optimized RAG (ORAG), a deployment-time framework that improves retrieval-augmented generation systems by continuously adapting embedding models using minimal feedback. The method addresses embedding misalignment issues that arise from imperfect models or noisy descriptions, which can lead to incorrect tool/function retrieval and task failure. ORAG applies lightweight online gradient updates to item embeddings after each interaction, requiring no changes to the underlying LLM and adding negligible latency. The framework supports single/multi-hop retrieval, dynamic tool inventories, and K-retrieval with reranking.

## Method Summary
ORAG treats retrieval as online multiclass classification with bandit feedback, where only the selected tool's success/failure is observed. For each query, it samples a tool based on current softmax probabilities, executes the task, and observes binary success/failure. The core innovation is an importance-weighted gradient estimator that performs unbiased updates despite only observing feedback for the chosen tool. The algorithm updates all item embeddings: successful tools move toward the query, failed tools move away, and unselected tools also move away. Theoretical analysis shows performance depends on initialization quality, with better initial models converging faster. The method supports both single and multi-hop retrieval scenarios.

## Key Results
- Improves text-embedding-v4 performance by 8.05% on UltraTool benchmark
- Enhances multi-hop QA accuracy from 0.55 to 0.68 on MultiHopRAG
- Outperforms static embeddings across diverse benchmarks including ToolRet, FiQA, and MultiHopRAG
- Demonstrates consistent improvements in tool selection accuracy and task success

## Why This Works (Mechanism)

### Mechanism 1: Importance-Weighted Gradient Estimation for Bandit Feedback
The method performs unbiased gradient updates using only binary success/failure feedback through an importance-weighted estimator. The gradient for item $i$ incorporates the selection probability $p_{t,i}$ in the denominator to correct for sampling bias, allowing updates without knowing the optimal tool.

### Mechanism 2: Embedding-Space Geometry Correction via Cosine Similarity
The algorithm directly modifies tool embeddings in vector space. When a selected tool succeeds, its embedding moves toward the query (increasing similarity). When it fails, the embedding moves away. This shifts the embedding geometry to better align tool descriptions with query intent distributions.

### Mechanism 3: Performance Guarantees via Initialization Quality Dependence
Theoretical analysis derives an $O(\sqrt{T})$ regret bound that explicitly depends on initialization quality. Better initial embeddings (smaller distance to optimum) lead to lower regret, providing a formal basis for why strong pre-trained models accelerate learning.

## Foundational Learning

- **Concept: Online Convex Optimization (OCO)** - Needed to understand why the algorithm uses gradient descent and how regret bounds prove learning occurs. Quick check: Can you explain why a sublinear regret bound implies average performance approaches the best fixed strategy?

- **Concept: Multi-Armed Bandit (MAB) Feedback** - Essential for understanding why the importance-weighted estimator is necessary when only the selected tool's outcome is observed. Quick check: How does a stochastic bandit differ from an adversarial one in the context of changing query distributions?

- **Concept: Softmax Classification & Cross-Entropy Loss** - Critical for understanding how retrieval is cast as multiclass classification and why gradient updates improve accuracy. Quick check: How does cross-entropy act as a surrogate for 0-1 loss, and why is minimizing it aligned with maximizing accuracy?

## Architecture Onboarding

- **Component map**: Retriever (stores embeddings, computes probabilities) -> Executor/LLM (executes task) -> Feedback Observer (converts outcome to binary) -> Online Optimizer (computes gradients, updates embeddings)

- **Critical path**: Query arrival → Embedding ($q_t$) → Retriever ($p_t$) → Sample tool ($i_t$) → Execute → Feedback → **Gradient Update** → Update $\Theta$. The gradient update step adds negligible latency.

- **Design tradeoffs**:
  - Update All vs. Update Selected: Full update (O(I) operations) is more accurate but expensive; single-item update is O(1) but potentially slower
  - Learning Rate: Small constant ensures stability but may be slow; decaying schedule converges but requires tuning
  - Session Scope: Aggregate updates across all users (universal adaptation) or per-user (personalization)

- **Failure signatures**:
  - Divergence/Instability: Embeddings explode in magnitude. Remedy: Project to unit ball or reduce learning rate
  - Catastrophic Forgetting: Unused tools become irretrievable. Observation: All unselected tools move away on failure
  - No Improvement: Cumulative loss doesn't decrease. Diagnosis: Check feedback reliability or initialization quality
  - Sensitivity to Initialization: Heavily dependent on initial model quality; poor base models require many updates

- **First 3 experiments**:
  1. Baseline Reproduction: Replicate Table 1 on 500 UltraTool queries, measure Recall@10 before/after 1000 updates
  2. Ablation on Feedback Noise: Inject noise into feedback (0.1, 0.2 flip probability), plot accuracy vs. noise rate
  3. Initialization Sensitivity Check: Compare three initializations (strong, weak, random), plot cumulative loss over time

## Open Questions the Paper Calls Out

1. How does incorporating probability calibration techniques affect convergence guarantees and empirical performance?
2. How robust is the online optimization process to noisy or adversarial binary feedback signals?
3. What are the performance trade-offs of the efficient "chosen-item-only" update variant for large tool catalogs?

## Limitations
- Requires reliable binary feedback that may not always correlate with optimal tool selection
- Performance heavily depends on initialization quality, potentially requiring many updates for poor base models
- Importance-weighted gradient estimator can become unstable with low-probability samples

## Confidence

**High Confidence**: Theoretical analysis and basic algorithmic framework are well-founded
**Medium Confidence**: Empirical improvements demonstrated, but limited ablation studies on feedback noise and initialization
**Low Confidence**: Claims about negligible latency and ease of deployment lack detailed measurement; "gpt-4.1-nano" reranker identifier appears non-standard

## Next Checks

1. **Feedback Noise Robustness**: Systematically vary noise rate in binary feedback (0-50%) and measure degradation in final accuracy
2. **Initialization Transferability**: Test same ORAG algorithm starting from three initialization extremes on identical datasets, measuring convergence speed and final performance
3. **Long-term Deployment Stability**: Run simulated deployment with non-stationary tool distributions and measure if embeddings for unused tools become irretrievable