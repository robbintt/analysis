---
ver: rpa2
title: 'AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley
  Value Estimation'
arxiv_id: '2512.12597'
source_url: https://arxiv.org/abs/2512.12597
tags:
- tools
- tool
- agentshap
- importance
- shapley
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AgentSHAP addresses the challenge of explaining which external
  tools actually contribute to LLM agent responses. It uses Monte Carlo Shapley value
  estimation to compute tool importance scores by treating the agent as a black box
  and testing responses with different tool subsets.
---

# AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation

## Quick Facts
- arXiv ID: 2512.12597
- Source URL: https://arxiv.org/abs/2512.12597
- Reference count: 22
- Primary result: First practical method for computing tool-level explainability in LLM agents using black-box Monte Carlo Shapley estimation

## Executive Summary
AgentSHAP addresses the critical challenge of explaining which external tools actually contribute to LLM agent responses. The method treats the agent as a black box and uses Monte Carlo Shapley value estimation to compute importance scores for each tool by testing responses with different tool subsets. Requiring no access to model internals, AgentSHAP works with any LLM and provides actionable insights for debugging, optimization, and trust calibration. Experiments on the API-Bank benchmark demonstrate strong performance with 0.945 consistency across runs, 13x quality differences when removing important versus unimportant tools, and 86-100% top-1 accuracy across experiments.

## Method Summary
AgentSHAP computes tool importance scores through Monte Carlo Shapley value estimation by treating the LLM agent as a black box. The method first obtains a baseline response using all available tools, then performs leave-one-out evaluations for each tool to establish individual contributions. Using a sampling ratio ρ=0.5, it randomly selects subsets of tools (coalitions) and measures response quality via cosine similarity between tool-subset responses and the baseline using text embeddings. The Shapley values are computed by averaging marginal contributions across sampled coalitions, providing a principled attribution of each tool's importance to the final response.

## Key Results
- 0.945 consistency across runs on 9 prompts from API-Bank level-1 test set
- 13x quality difference when removing highest versus lowest SHAP-valued tools
- 7x SHAP gap between expected and irrelevant tool scores when injecting 4 unrelated tools
- 86-100% top-1 accuracy across three experiments, with irrelevant tools achieving 100% accuracy

## Why This Works (Mechanism)
AgentSHAP works by leveraging the Shapley value framework from cooperative game theory to attribute credit to individual tools within an LLM agent system. By treating the agent as a black box and systematically evaluating responses with different tool subsets, it captures the marginal contribution of each tool to the final output. The Monte Carlo sampling approach makes this computationally feasible for larger tool sets, while the use of semantic similarity between embeddings provides a stable proxy for response quality that doesn't require task-specific metrics.

## Foundational Learning

**Shapley Values in Cooperative Game Theory**
*Why needed*: Provides the mathematical foundation for fairly attributing contributions among multiple elements (tools) working together
*Quick check*: Verify understanding of marginal contribution averaging across all possible coalitions

**Monte Carlo Estimation**
*Why needed*: Makes Shapley computation tractable for large tool sets where exact computation is exponential
*Quick check*: Confirm that sampling ratio ρ=0.5 balances accuracy with computational cost

**Semantic Similarity via Text Embeddings**
*Why needed*: Provides a model-agnostic way to measure response quality without requiring task-specific metrics
*Quick check*: Test that cosine similarity between embeddings correlates with human-perceived response quality

## Architecture Onboarding

**Component Map**: LLM Agent -> Tool Calling Interface -> Monte Carlo Sampler -> Response Embedder -> Shapley Calculator

**Critical Path**: Tool subset selection → Agent response generation → Embedding computation → Similarity calculation → SHAP value computation

**Design Tradeoffs**: Black-box approach trades interpretability of internal model states for generality across any LLM; embedding-based quality measurement trades task-specific accuracy for model-agnostic evaluation

**Failure Signatures**: High variance in SHAP values indicates insufficient sampling; low consistency across runs suggests embedding instability; poor top-1 accuracy reveals tool selection errors

**First 3 Experiments**: 1) Run consistency test on 9 API-Bank prompts across 3 trials, 2) Remove highest vs lowest SHAP tool and measure quality drop, 3) Inject 4 irrelevant tools and verify 7x SHAP gap

## Open Questions the Paper Calls Out

**Open Question 1**: Can AgentSHAP be extended to capture pairwise or higher-order Shapley interactions between tools, revealing synergies where combinations outperform individual tools?
Basis: Current formulation computes marginal contributions for single tools only
Evidence needed: Modified AgentSHAP outputting interaction SHAP values validated on synergistic tasks

**Open Question 2**: How can tool importance be tracked across multi-turn conversations where relevant tools may shift dynamically over time?
Basis: Current method treats each prompt independently
Evidence needed: Temporal extension evaluated on multi-turn dialogue benchmarks

**Open Question 3**: Can causal analysis of tool call sequences reveal ordering effects that AgentSHAP's set-based attribution misses?
Basis: AgentSHAP treats tools as a set, ignoring sequential dependencies
Evidence needed: Augmented method attributing importance to ordered subsequences tested on ordering-sensitive tasks

**Open Question 4**: What sampling strategies or approximations could enable real-time AgentSHAP explanations during live agent execution?
Basis: Current approach requires ~130 API calls for 8 tools, making latency too high for interactive use
Evidence needed: Adaptive sampling methods achieving comparable accuracy with fewer evaluations

## Limitations
- Embedding-based similarity may not capture task-specific correctness
- Assumes tool contributions are independent and additive
- API-Bank benchmark may not generalize to all LLM agent use cases
- Requires ~130 API calls for 8 tools, limiting real-time applications

## Confidence
- **High Confidence**: Monte Carlo Shapley estimation algorithm is mathematically sound; consistency results demonstrate reliable computation
- **Medium Confidence**: Faithfulness experiments are compelling but depend on quality metric used; semantic similarity may miss nuanced correctness differences
- **Medium Confidence**: SHAP gap between expected and irrelevant tools is promising but assumes irrelevant tools should have near-zero contribution

## Next Checks
1. **Sensitivity Analysis**: Vary Monte Carlo sampling ratio ρ and number of samples to test stability of SHAP values and consistency metrics
2. **Task-Specific Quality Evaluation**: Replace embedding-based similarity with human evaluation or task-specific correctness metrics for subset of queries
3. **Tool Interaction Testing**: Design experiments with conditional tool dependencies to verify AgentSHAP captures these rather than treating tools as independent contributors