---
ver: rpa2
title: Better Training Data Attribution via Better Inverse Hessian-Vector Products
arxiv_id: '2507.14740'
source_url: https://arxiv.org/abs/2507.14740
tags:
- training
- influence
- learning
- which
- functions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of accurately computing inverse
  Hessian-vector products (iHVPs), a key computational step in training data attribution
  (TDA) methods like influence functions. The authors propose ASTRA, an algorithm
  that uses the EKFAC preconditioner to accelerate Stochastic Neumann series iterations
  (SNI) for more accurate iHVP approximations.
---

# Better Training Data Attribution via Better Inverse Hessian-Vector Products

## Quick Facts
- arXiv ID: 2507.14740
- Source URL: https://arxiv.org/abs/2507.14740
- Authors: Andrew Wang; Elisa Nguyen; Runshi Yang; Juhan Bae; Sheila A. McIlraith; Roger Grosse
- Reference count: 40
- Primary result: ASTRA preconditioned SNI converges faster than vanilla SNI for inverse Hessian-vector products, improving training data attribution performance (LDS ~0.5–0.6 with ensembling).

## Executive Summary
This paper tackles the challenge of accurately computing inverse Hessian-vector products (iHVPs), a key computational step in training data attribution (TDA) methods like influence functions. The authors propose ASTRA, an algorithm that uses the EKFAC preconditioner to accelerate Stochastic Neumann series iterations (SNI) for more accurate iHVP approximations. Unlike standard EKFAC-based methods, ASTRA combines the computational efficiency of preconditioning with the theoretical consistency of iterative methods. The paper demonstrates that ASTRA significantly improves TDA performance—measured by Linear Data Modeling Score (LDS)—across various settings, including challenging architectures like convolutional networks. ASTRA also requires fewer iterations and less hyperparameter tuning than SNI, converging faster and achieving higher correlation with ground-truth retraining outcomes.

## Method Summary
ASTRA accelerates inverse Hessian-vector product computation for training data attribution by combining EKFAC preconditioning with stochastic Neumann iterations. The method first computes EKFAC statistics (activations and pseudo-gradients) over the training data to build a Kronecker-factored approximation of the curvature. For each query point, ASTRA initializes the iterative process using the EKFAC solution and then refines it through preconditioned SNI updates that solve a regularized least-squares problem. The algorithm requires only 200 iterations compared to thousands for vanilla SNI, while preserving information in low-curvature directions that are critical for accurate influence estimation. EKFAC statistics are computed once and shared across all queries, making the approach computationally efficient.

## Key Results
- ASTRA achieves Linear Data Modeling Score (LDS) of 0.52-0.57 on FashionMNIST with ensembling, compared to 0.25-0.33 for vanilla SNI
- Requires only 200 iterations versus 1000+ for SNI, with faster convergence in low-curvature directions
- EKFAC preconditioning reduces hyperparameter sensitivity and implicit damping compared to truncated SNI
- Low-curvature directions (eigenvalues 10^-4 to 10^-2) play a critical role in TDA performance
- ASTRA-IF outperforms both standard EKFAC-IF and vanilla SNI-IF across MLP and CNN architectures

## Why This Works (Mechanism)

### Mechanism 1: Preconditioning Accelerates Convergence
Applying EKFAC as a preconditioner rescales gradient updates to improve the conditioning of the Hessian, allowing larger learning rates and faster convergence. The Kronecker-factored approximation captures enough curvature structure to effectively condition the problem without excessive computational cost.

### Mechanism 2: Recovery of Low-Curvature Information
Standard truncated SNI suppresses low-curvature directions through implicit damping proportional to 1/(αJ). ASTRA converges faster in these directions, preserving critical information for predicting model behavior that truncated methods lose.

### Mechanism 3: Iterative Refinement of Approximate Curvature
ASTRA combines fast EKFAC approximation with iterative refinement, using SNI to correct residual errors rather than computing the full solution from scratch. This "best of both worlds" approach achieves better accuracy than EKFAC alone with fewer iterations than SNI alone.

## Foundational Learning

- **Concept: Inverse Hessian-Vector Products (iHVP)**
  - Why needed: This is the fundamental computational bottleneck in influence functions
  - Quick check: Why can't we simply compute the Hessian matrix explicitly for modern neural networks to get the inverse?

- **Concept: Preconditioning**
  - Why needed: ASTRA's core novelty is using EKFAC as a preconditioner
  - Quick check: How does a preconditioner P change the update rule for gradient descent, and why does this speed up convergence for ill-conditioned problems?

- **Concept: Stochastic Neumann Series (SNI)**
  - Why needed: This is the baseline iterative method being improved
  - Quick check: In SNI, what is the risk of truncating the series too early, and how does ASTRA affect the number of terms needed?

## Architecture Onboarding

- **Component map:** EKFAC statistics computation -> Eigendecomposition module -> ASTRA solver (iterative core) -> Influence score computation

- **Critical path:**
  1. Compute EKFAC statistics over training data (once)
  2. Perform eigendecomposition to build preconditioner
  3. For each query: initialize θ₀ using EKFAC, run ASTRA iterations
  4. Compute influence scores via dot products with training gradients

- **Design tradeoffs:**
  - Memory vs. Speed: EKFAC requires O(D) storage for layer-wise statistics
  - Iterations vs. Accuracy: ASTRA needs hundreds vs. thousands for vanilla SNI
  - Damping (λ): Critical for stability; ASTRA allows lower effective damping

- **Failure signatures:**
  - Slow convergence: Preconditioner statistics stale or incorrect
  - High variance: Batch size too small during SNI steps
  - Poor LDS: Model not converged or convexity assumptions violated

- **First 3 experiments:**
  1. Implement 1-step ASTRA to verify it matches standard EKFAC-IF results
  2. Plot convergence curves comparing ASTRA vs. SNI loss and LDS
  3. Ablate damping factors to validate the implicit damping hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal method for computing the average Generalized Gauss-Newton (GGN) Hessian (G_ℓ) within the ASTRA-SOURCE framework to maximize performance? The authors leave to future work to explore various ways to compute the average GGN, which may further improve performance. Evidence would require a comparative ablation study measuring LDS across different GGN averaging schemes.

### Open Question 2
How can the computational cost of scanning the full training dataset for gradient dot products be reduced or approximated efficiently for ASTRA? The authors identify this as an orthogonal but important issue they do not address. Evidence would require proposing and validating an approximate search algorithm that reduces dot product complexity without degrading attribution accuracy.

### Open Question 3
Under what specific conditions does improving iHVP accuracy fail to translate into better TDA performance due to other theoretical bottlenecks? The paper notes that benefits may not materialize until other bottlenecks are resolved, such as training stochasticity or convexity violations. Evidence would require a theoretical or empirical decomposition of total error into solver error and approximation error.

## Limitations
- EKFAC effectiveness uncertain for deeper architectures beyond basic CNNs and MLPs
- Computational benefits depend on efficient eigendecomposition scaling with model size
- Study focuses primarily on LDS metric, requiring additional validation for real-world applications

## Confidence

- **High confidence:** ASTRA converges faster than vanilla SNI (supported by quantitative iteration comparisons and convergence plots)
- **Medium confidence:** Low-curvature directions are critical for TDA quality (supported by ablation studies but relies on a single metric)
- **Medium confidence:** ASTRA requires fewer hyperparameters than SNI (supported by tuning results but limited hyperparameter exploration)
- **Low confidence:** EKFAC preconditioning generalizes equally well to all neural network architectures (primarily validated on MLPs and basic CNNs)

## Next Checks

1. **Architecture scaling test:** Implement ASTRA on deeper CNNs (ResNet-18/50) and transformers to verify EKFAC preconditioning effectiveness and computational benefits hold for modern architectures

2. **Low-curvature importance validation:** Systematically ablate low-curvature directions using the projection method and verify their impact on multiple TDA tasks beyond LDS (e.g., actual data poisoning detection or label error identification)

3. **Memory-efficiency analysis:** Profile ASTRA's memory usage during EKFAC eigendecomposition and SNI iterations, comparing against both vanilla SNI and LoRIF baselines to quantify the practical scalability tradeoffs