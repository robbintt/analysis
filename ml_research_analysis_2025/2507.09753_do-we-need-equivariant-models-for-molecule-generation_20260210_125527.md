---
ver: rpa2
title: Do we need equivariant models for molecule generation?
arxiv_id: '2507.09753'
source_url: https://arxiv.org/abs/2507.09753
tags:
- oxmol
- equivariance
- equivariant
- generation
- molecule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether non-equivariant 3D convolutional
  neural networks (CNNs) trained with rotation augmentations can learn equivariance
  and match the performance of explicitly equivariant models for molecule generation.
  The authors introduce a loss decomposition that separates prediction error from
  equivariance error and evaluate how model size, dataset size, and training duration
  affect performance across denoising, molecule generation, and property prediction
  tasks.
---

# Do we need equivariant models for molecule generation?

## Quick Facts
- arXiv ID: 2507.09753
- Source URL: https://arxiv.org/abs/2507.09753
- Reference count: 40
- Primary result: CNNs with rotation augmentation learn reconstruction equivariance easily but require larger models for generative equivariance

## Executive Summary
This paper investigates whether non-equivariant 3D CNNs can match equivariant GNNs for molecule generation by learning equivariance through rotation augmentations. The authors introduce a loss decomposition separating prediction error from equivariance error and evaluate how model size, dataset size, and training duration affect performance across denoising, molecule generation, and property prediction tasks. Key findings show that CNNs easily learn equivariance during denoising reconstruction even with small models, limited data, and few training epochs, but for generative tasks, only larger models trained on more data produce consistent outputs across rotated inputs.

## Method Summary
The study compares VoxMol, a 3D CNN with 111M parameters, against E3NN, an SE(3)-equivariant model, on molecule generation tasks. VoxMol uses 4D voxel tensors [8 channels × 64 × 64 × 64] with rotation/translation augmentations and noise injection (σ=0.9) for denoising training. The models are evaluated on GEOM-Drugs (1.1M train/146K val/146K test) using reconstruction equivariance error, Walk-Jump Sampling for generation, and property prediction metrics. The analysis includes varying model sizes (7M, 28M, 111M), training durations, and data subsets.

## Key Results
- CNNs learn reconstruction equivariance through augmentation even with small models and limited data
- Generative equivariance requires larger models (111M vs 7M parameters) and complete training data
- CNNs learn redundant parallel representations rather than recognizing rotated inputs as equivalent in latent space

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Non-equivariant CNNs can learn approximate equivariance through rotation augmentation during training, producing consistent outputs across rotated inputs despite lacking architectural equivariance guarantees.
- **Mechanism:** Random rotation augmentations applied during training create many views of the same molecular structure. The model learns to map these different views to consistent outputs by optimizing the reconstruction loss across all augmented versions. This produces functionally equivariant behavior without architectural constraints.
- **Core assumption:** The model has sufficient capacity to learn multiple representation pathways for different orientations, and the training distribution adequately covers the rotation space.
- **Evidence anchors:**
  - [abstract]: "CNNs easily learn equivariance during denoising reconstruction even with small models, limited data, and few training epochs."
  - [section 4.1, Figure 2]: Both E3NN (equivariant by design) and VoxMol trained with rotation augmentations achieve equivariant reconstruction error lower than the ground truth reconstruction upper bound.
  - [corpus]: "Training Dynamics of Learning 3D-Rotational Equivariance" (arXiv:2512.02303) provides complementary analysis of how quickly symmetry-agnostic models learn to respect symmetries.
- **Break condition:** For generative tasks, smaller models (7M parameters) or models trained on limited data (<50% of dataset) produce inconsistent outputs across rotated seed molecules, showing degraded molecular stability and shifted property distributions (Table 1, Table 2).

### Mechanism 2
- **Claim:** Learned equivariance in CNNs operates through redundant parallel representations rather than true rotational equivalence in latent space.
- **Mechanism:** The CNN encodes rotated versions of the same molecule to different latent embeddings but still produces similar outputs. This suggests the model learns separate processing pathways for different orientations rather than learning rotation-invariant features.
- **Core assumption:** The model has enough capacity to maintain multiple redundant encoding pathways that converge to similar outputs through different decoder paths.
- **Evidence anchors:**
  - [section 4.1, Figure 6]: "Cosine similarity between latent embeddings of the same molecules under different rotations" shows low similarity except at 0°/360°, indicating rotated molecules are embedded differently despite similar reconstructions.
  - [section 1]: "CNNs learn redundant parallel representations rather than recognizing rotated inputs as equivalent, potentially explaining their higher parameter demands compared to equivariant graph neural networks."
  - [corpus]: Direct corpus evidence for this mechanism is weak; no directly comparable analysis of latent representation redundancy was found in neighbor papers.
- **Break condition:** When model capacity is reduced (7M vs 111M parameters), the model cannot maintain sufficient redundant pathways, leading to inconsistent generative behavior under rotation.

### Mechanism 3
- **Claim:** Equivariance learning difficulty scales with task complexity—reconstruction/denoising tasks learn equivariance easily, while generative tasks require substantially more model capacity and training data.
- **Mechanism:** Reconstruction is a direct input-output mapping where the denoising loss directly penalizes inconsistencies. Generation involves sampling from a learned distribution through multi-step MCMC, where small equivariance errors compound and the loss signal is less direct.
- **Core assumption:** Assumption: The reconstruction loss saturates before fully learning features necessary for robust generation; auxiliary losses may be needed.
- **Evidence anchors:**
  - [section 5]: "Since generative models are trained only with a simple reconstruction loss which saturates easily, it may not fully allow the model to learn features useful for good performance on generation and prediction tasks."
  - [section 4.2, Table 2]: VoxMol variants with no augmentation, smaller size (28M), or limited training data (50% subset) show higher KL divergence in chemical property distributions between rotated and unrotated generations.
  - [corpus]: "Scalable Non-Equivariant 3D Molecule Generation via Rotational Alignment" (arXiv:2506.10186) addresses similar scalability concerns with equivariant diffusion models.
- **Break condition:** Generative tasks with seeded lead compounds require full-scale models (111M parameters) trained on complete datasets; below this threshold, property distributions shift significantly under rotation (KL divergence increases from ~0.002 to ~0.05 for properties like LogP).

## Foundational Learning

- **Concept: Equivariance vs Invariance**
  - Why needed here: The paper fundamentally compares models that are equivariant by architecture (f(Rx) = Rf(x)) against models that learn approximate equivariance. Understanding this distinction is essential for interpreting the experimental results.
  - Quick check question: If you rotate a molecule input by 90°, should the output rotate by 90° (equivariance) or stay the same (invariance)?

- **Concept: Walk-Jump Sampling (Neural Empirical Bayes)**
  - Why needed here: The generation process uses this two-step sampling: a "walk" step (Langevin MCMC) explores the noisy manifold, then a "jump" step applies the denoiser. Understanding this clarifies why generation is more sensitive to equivariance errors than denoising alone.
  - Quick check question: In the walk-jump sampling process, which step introduces stochasticity that could amplify small equivariance errors?

- **Concept: Voxel Representation of Molecules**
  - Why needed here: The paper uses 4D tensors [c × l × l × l] where c=8 atom channels and l=64 grid length. This discrete grid representation differs from point cloud representations used by equivariant GNNs, affecting how rotational augmentations are applied.
  - Quick check question: How does representing atoms as Gaussian densities on a voxel grid affect the application of rotation augmentations compared to point cloud representations?

## Architecture Onboarding

- **Component map:**
  Input pipeline: Molecule coordinates → Gaussian density voxelization (64³ grid, 8 atom channels, 0.25Å resolution) → random rotation/translation augmentation → noise injection (σ=0.9)
  VoxMol encoder: 3D U-Net with 4 resolution levels, self-attention on lowest 2 resolutions (111M parameters)
  E3NN variant: SE(3)-equivariant 3D U-Net using steerable CNNs (0.5M parameters tested—significantly smaller due to training cost)
  Output heads: Denoising reconstruction head + optional property prediction MLP head (avg pooling → 3 linear layers with LayerNorm, dropout 0.1, Shifted SoftPlus)

- **Critical path:**
  1. Data loading and voxelization (GEOM-Drugs: 1.1M train / 146K val / 146K test molecules)
  2. Augmentation: random rotation + translation per sample per iteration
  3. Forward pass through 3D U-Net encoder-decoder
  4. Loss computation: MSE between denoised output and ground truth voxels
  5. For property prediction: additional MLP head with regression loss

- **Design tradeoffs:**
  - VoxMol (111M params, non-equivariant): Faster training (2h total), high generation quality, but learns redundant representations requiring more capacity
  - E3NN (0.5M params, equivariant): 45× longer per-epoch training time (90min vs 2h for full VoxMol), mathematically guaranteed equivariance, but did not match VoxMol generation quality in experiments
  - Model size vs equivariance: Smaller VoxMol variants (7M, 28M) learn reconstruction equivariance but fail on generation equivariance

- **Failure signatures:**
  - High equivariance error: ||R(̂x₀) − ̂xₙ||₂² >> reconstruction error floor (Equation 4)
  - Generative inconsistency: KL divergence >0.02 for chemical properties between rotated/unrotated seeds (Table 2)
  - Latent space: Low cosine similarity (<0.5) between embeddings of rotated versions of same molecule (Figure 6)
  - Degraded stability: Molecular stability drops >5% when seed is rotated (Table 1)

- **First 3 experiments:**
  1. **Reconstruction equivariance baseline:** Train VoxMol with rotation augmentation on 10% of GEOM-Drugs for 100 epochs. Measure equivariance error (Equation 4) by comparing reconstructions of rotated inputs vs rotated reconstructions of unrotated inputs. Expected: equivariance error < reconstruction error.
  2. **Generation consistency test:** Using a trained model, generate molecules from the same seed rotated by 0°, 90°, 180°, 270°. Compute KL divergence of property distributions (LogP, TPSA, heavy atom count). Compare full VoxMol (111M) vs smaller variant (28M). Expected: 111M shows KL<0.005, 28M shows KL>0.02.
  3. **Latent embedding analysis:** Extract encoder embeddings for molecules under multiple rotations. Compute pairwise cosine similarity. Verify that VoxMol shows low similarity despite high output consistency, while E3NN (if available) shows identical embeddings. This confirms the redundant representation hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can auxiliary training objectives or regularization techniques that align latent embeddings across rotations improve equivariance without requiring large model sizes or datasets?
- **Basis in paper:** [explicit] The authors explicitly hypothesize in the introduction and conclusion that "auxiliary training objectives used to align latent embeddings across rotations could improve robustness without requiring large models or datasets."
- **Why unresolved:** The paper demonstrates that current CNNs learn redundant representations with differing embeddings for rotated inputs, but it does not experimentally validate whether forcing these embeddings to align would improve efficiency or equivariance.
- **What evidence would resolve it:** A comparative study training CNNs with a contrastive loss or similar regularization term on the latent space, measuring if smaller models achieve equivariance comparable to the current large models.

### Open Question 2
- **Question:** Does the observed divergence in latent embeddings for rotated inputs in CNNs negatively impact performance in property-guided generation tasks?
- **Basis in paper:** [inferred] The paper notes that "redundant latent representations... could hinder tasks that rely on meaningful latent structure, such as property prediction or property guided generation," but the experiments primarily focus on denoising and unconditioned generation quality.
- **Why unresolved:** While the authors test property prediction, they do not evaluate conditional generation (e.g., generating molecules with a specific target property), which relies heavily on a smooth and consistent latent space.
- **What evidence would resolve it:** Benchmarks comparing equivariant GNNs and CNNs on conditional generation tasks to see if the CNN's lack of rotation-invariant embeddings leads to mode collapse or inconsistency when conditioning on properties.

### Open Question 3
- **Question:** Is the ability of 3D CNNs to learn approximate equivariance dependent on the specific artifacts or structure of the voxel representation?
- **Basis in paper:** [inferred] The authors note that even models trained *without* rotation augmentation showed lower equivariant error than expected, suggesting "the 3D CNN architecture and the denoising task inherently capture some rotational structure from the sparse voxel inputs."
- **Why unresolved:** It is unclear if this learned robustness is a fundamental property of 3D denoising tasks or if it is an artifact of the aliasing or discrete grid structure inherent to voxelization.
- **What evidence would resolve it:** Repeating the analysis with non-voxel-based 3D CNNs or continuous representations to determine if the "inherent" equivariance persists outside of grid-based inputs.

## Limitations
- E3NN comparison uses significantly smaller model (0.5M vs 111M parameters) due to computational constraints rather than fair architectural comparison
- Redundant representation hypothesis lacks direct experimental validation beyond latent similarity analysis
- Analysis focuses primarily on rotation augmentation; other symmetry transformations (translation, permutation) are not systematically explored

## Confidence

- **High confidence:** CNNs learn reconstruction equivariance through augmentation even with small models and limited data
- **Medium confidence:** Generative equivariance requires larger models and more data, but the threshold is empirically determined rather than theoretically grounded
- **Medium confidence:** Learned equivariance operates through redundant representations rather than true rotational equivalence, though direct evidence is limited

## Next Checks

1. **Architectural scaling experiment:** Train E3NN variants at multiple scales (0.5M, 2M, 10M parameters) to isolate whether performance gaps stem from capacity or architectural differences

2. **Latent representation ablation:** Freeze VoxMol encoder and train separate decoders for different rotations to test whether redundant representations are necessary for equivariance or merely a byproduct of the architecture

3. **Cross-symmetry generalization:** Test whether models trained with rotation augmentation generalize to translation and permutation symmetries, measuring transfer learning efficiency compared to explicitly equivariant architectures