---
ver: rpa2
title: 'Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop
  Simulator, Causal Benchmark and Joint IL-RL Baseline'
arxiv_id: '2504.14709'
source_url: https://arxiv.org/abs/2504.14709
tags:
- learning
- imitation
- driving
- reinforcement
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the "copycat problem" in imitation learning-based
  planners, where policies overfit to initial conditions and fail to generalize to
  diverse scenarios. The authors propose a closed-loop simulator supporting both imitation
  and reinforcement learning, a causal benchmark derived from Waymo Open Dataset to
  rigorously assess this issue, and a joint IL-RL framework (MTR-SAC) to improve policy
  adaptability.
---

# Exposing the Copycat Problem of Imitation-based Planner: A Novel Closed-Loop Simulator, Causal Benchmark and Joint IL-RL Baseline

## Quick Facts
- arXiv ID: 2504.14709
- Source URL: https://arxiv.org/abs/2504.14709
- Reference count: 36
- Key outcome: Pure IL policies drop from 0.51 to 0.31 completion on causal benchmark; MTR-SAC achieves 0.58 completion by integrating IL features with RL.

## Executive Summary
This paper addresses the "copycat problem" in imitation learning-based planners, where policies overfit to initial conditions and fail to generalize to diverse scenarios. The authors propose a closed-loop simulator supporting both imitation and reinforcement learning, a causal benchmark derived from Waymo Open Dataset to rigorously assess this issue, and a joint IL-RL framework (MTR-SAC) to improve policy adaptability. Experiments show that the causal benchmark is more challenging than the original dataset, with completion rates dropping from 0.51 to 0.31. The integrated IL-RL approach significantly outperforms pure IL (0.58 vs. 0.31 completion on test4k) and RL-only methods, demonstrating better generalization and robustness to diverse driving goals.

## Method Summary
The authors address the copycat problem by developing a closed-loop simulator with MPC controller and IDM policy for NPCs, creating a causal benchmark from WOMD through goal augmentation and DFS-based lane-exit detection, and proposing MTR-SAC that concatenates MTR-extracted features with simulator state for SAC training. The pipeline involves training MTR on WOMD, filtering scenarios to those with ego car index, evaluating with MPC in closed-loop, and training MTR-SAC offline with diverse goal scenarios to improve generalization beyond the original dataset's limited goal distribution.

## Key Results
- Completion rate drops from 0.51 (original WOMD) to 0.31 (causal benchmark) for pure IL methods
- MTR-SAC achieves 0.58 completion rate on Test4k, significantly outperforming pure IL (0.31) and RL-only methods (0.04)
- Pure RL (StateSAC) achieves only 0.04 completion, highlighting the importance of IL pretraining

## Why This Works (Mechanism)
The copycat problem arises because imitation learning policies overfit to the specific initial conditions and goals present in the training dataset. By creating a causal benchmark with diverse goals under identical input conditions, the authors expose this limitation. The MTR-SAC framework addresses this by extracting high-level features from the pretrained MTR model and combining them with low-level simulator states for reinforcement learning, allowing the policy to learn goal-directed behavior beyond mere imitation of specific trajectories.

## Foundational Learning
- **MPC controller**: Why needed - enables closed-loop evaluation by following predicted waypoints; Quick check - verify completion rates around 0.15 when MTR uses MPC
- **Causal benchmark generation**: Why needed - exposes copycat problem by varying goals under identical conditions; Quick check - ensure goal diversity causes significant performance drops for pure IL
- **Feature concatenation for RL**: Why needed - combines high-level IL features with low-level simulator states; Quick check - verify MTR-SAC improves completion from ~0.15 to ~0.58 on Test4k

## Architecture Onboarding
- **Component map**: WOMD data -> MTR training -> MPC controller -> Closed-loop simulator -> MTR-SAC training -> Causal benchmark evaluation
- **Critical path**: MTR feature extraction (layer before motion prediction head) -> State concatenation -> SAC agent training -> Goal-directed policy improvement
- **Design tradeoffs**: Pure IL vs joint IL-RL - IL alone suffers from copycat problem but provides strong feature representation; RL alone lacks stability and generalization; joint approach combines strengths
- **Failure signatures**: Large open-loop to closed-loop gap (>0.3 completion drop) indicates copycat problem; RL-only instability (completion <0.1) shows need for IL pretraining; similar performance on both benchmarks indicates insufficient goal diversity
- **First experiments**: 1) Implement MPC controller and verify basic closed-loop functionality; 2) Train MTR and evaluate on original vs causal benchmark; 3) Implement MTR-SAC and verify completion improvement

## Open Questions the Paper Calls Out
None

## Limitations
- Critical implementation details for MPC controller parameters and solver configuration remain unspecified
- Feature extraction layer index and dimensionality in MTR are vaguely defined
- Causal benchmark generation code (DFS lane-exit detection, NMS implementation) lacks sufficient detail
- MLP architectures for SAC networks don't specify hidden dimensions and activation functions

## Confidence
- High confidence: The existence and characterization of the "copycat problem"
- Medium confidence: The effectiveness of MTR-SAC in solving the copycat problem
- Medium confidence: The claim that the causal benchmark is more challenging than the original dataset

## Next Checks
1. Reproduce the MPC-controller baseline with assumed reasonable parameters and verify completion rates around 0.15
2. Validate feature extraction and RL training pipeline by confirming completion rates improve from ~0.15 to ~0.58 on Test4k
3. Benchmark difficulty verification by generating a subset of the causal benchmark and ensuring goal diversity causes significant performance drops for pure IL methods