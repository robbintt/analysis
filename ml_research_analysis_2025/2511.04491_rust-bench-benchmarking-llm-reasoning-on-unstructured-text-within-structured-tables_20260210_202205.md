---
ver: rpa2
title: 'RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured
  Tables'
arxiv_id: '2511.04491'
source_url: https://arxiv.org/abs/2511.04491
tags:
- reasoning
- table
- question
- answer
- tables
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RUST-BENCH, a large-scale dataset designed
  to benchmark large language models (LLMs) on complex tabular reasoning tasks. The
  dataset comprises 7,966 question-answer pairs over 2,031 real-world tables sourced
  from NSF grant records and NBA statistics, emphasizing scale, heterogeneity, domain
  specificity, and multi-hop reasoning.
---

# RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables

## Quick Facts
- **arXiv ID:** 2511.04491
- **Source URL:** https://arxiv.org/abs/2511.04491
- **Reference count:** 40
- **Key outcome:** This paper introduces RUST-BENCH, a large-scale dataset designed to benchmark large language models (LLMs) on complex tabular reasoning tasks. The dataset comprises 7,966 question-answer pairs over 2,031 real-world tables sourced from NSF grant records and NBA statistics, emphasizing scale, heterogeneity, domain specificity, and multi-hop reasoning. A hybrid symbolic-semantic pipeline is used to generate high-quality questions, combining SQL-based symbolic reasoning with LLM-driven semantic generation. Evaluations with 13 state-of-the-art models reveal that even top-performing LLMs struggle significantly on this benchmark, with accuracy dropping sharply as table size increases and multi-hop reasoning becomes more complex. The results highlight persistent weaknesses in current architectures and demonstrate the need for improved mechanisms to handle large, heterogeneous, and domain-specific tabular data.

## Executive Summary
RUST-BENCH is a comprehensive benchmark for evaluating large language models on question answering over semi-structured tables that contain both structured columns and unstructured text. The dataset, built from NSF grant records and NBA statistics, contains 7,966 question-answer pairs across 2,031 tables, designed to test scale, heterogeneity, domain specificity, and multi-hop reasoning capabilities. Using a hybrid symbolic-semantic pipeline, the benchmark generates high-quality questions that require complex reasoning across modalities. Evaluations with 13 state-of-the-art models reveal significant performance degradation as table size increases and reasoning complexity grows, highlighting persistent architectural weaknesses in current LLMs.

## Method Summary
The benchmark uses a hybrid symbolic-semantic pipeline to generate high-quality questions over semi-structured tables. Tables are linearized into pipe-separated format preserving both structured columns and unstructured text fields. The evaluation supports multiple reasoning strategies including Chain-of-Thought (CoT), Program-of-Thoughts (PoT), and hybrid approaches like TableMaster. The primary metric is Exact Match (EM), with secondary metrics including BLEU and LLM-Score using GPT-4o-mini as judge. The benchmark tests 13 state-of-the-art models across different token size bins and reasoning complexity levels, with special attention to unanswerable questions and error analysis.

## Key Results
- Performance degrades significantly as table size increases, dropping from ~45% accuracy (10k-18k tokens) to ~20% (50k-85k tokens)
- Multi-hop reasoning accuracy is substantially lower than single-hop, with error propagation across reasoning steps
- TableMaster (hybrid symbolic-semantic) achieves the best performance at 42-43% EM, outperforming both pure CoT (39-47% EM) and PoT (18-30% EM) approaches
- Even top-performing models struggle with heterogeneous schemas and complex inference across semi-structured data

## Why This Works (Mechanism)

### Mechanism 1: Context-Internal Retrieval Degradation with Scale
As table size increases within context windows, model accuracy degrades due to retrieval and integration challengesâ€”not raw context capacity limits. Larger tables scatter relevant evidence across more tokens, forcing models to perform implicit retrieval within context before reasoning. Critical information is distributed across many rows, compounding attention load. This occurs well within nominal 128k+ context windows. The bottleneck arises from reasoning and attention limitations, not context window length. Evidence shows monotonic decline from ~45% accuracy (10k-18k tokens) to ~20% (50k-85k tokens) across multiple models. If tables are pre-filtered or decomposed into sub-tables before reasoning, degradation should attenuate.

### Mechanism 2: Modality-Specific Reasoning Alignment
Symbolic reasoning excels with explicit schema; semantic reasoning excels with natural language continuity. Semi-structured data maximizes strategy mismatch. Symbolic approaches require syntactic regularity and explicit structures; semantic approaches leverage natural-language flow. Semi-structured tables combine both modalities, forcing mid-reasoning strategy switches that current models handle poorly. Models lack robust mechanisms for dynamic strategy selection when modalities interleave. PoT achieves higher accuracy on structured variants (34-48%) than semi-structured (30-46%); CoT achieves higher accuracy on unstructured variants (42-50%) than semi-structured (39-47%). When adaptive frameworks explicitly select between modalities per query, performance should improve.

### Mechanism 3: Multi-Hop Composition Breakdown
Multi-hop reasoning fails disproportionately when evidence spans both structured fields and unstructured text within the same table. Each hop requires locating relevant evidence, extracting values, and integrating across modalities. Errors cascade rather than being independently recoverable. Error analysis shows 31% logical inconsistencies, 27% misalignment, 22% interpretation errors, 20% extraction failures. Error propagation across hops is the dominant failure mode, not individual step accuracy. Error analysis shows distributed failure modes; 4-hop reasoning requires structured filtering, unstructured extraction, and arithmetic. When intermediate steps are externalized (CoT, Chain-of-Table), performance should improve relative to zero-shot.

## Foundational Learning

- **Concept: Semi-structured data representation**
  - Why needed here: RUST-BENCH tables mix structured columns (dates, amounts) with unstructured text fields (summaries, abstracts). Selecting appropriate reasoning strategies requires understanding which modality dominates each query component.
  - Quick check question: Given a table with columns [date, amount, summary_text], would a SQL-based approach handle all three columns equally well?

- **Concept: Multi-hop reasoning decomposition**
  - Why needed here: 26.18% of questions require multi-hop reasoning across modalities. Each hop may involve different operations (filtering, extraction, arithmetic), and errors compound.
  - Quick check question: For "What was the point difference between the losing team's top scorer and winning team's top scorer in March games at TD Garden?", identify at least 4 reasoning hops.

- **Concept: Context-internal retrieval vs. reasoning**
  - Why needed here: Performance degrades with scale even within large context windows. Understanding whether failures stem from retrieval (finding relevant rows) vs. reasoning (integrating evidence) informs system design choices.
  - Quick check question: Why might a model with a 128k context window still fail on a 40k-token table?

## Architecture Onboarding

- **Component map:** Input Processing -> Reasoning Engine -> Evaluation Layer
- **Critical path:** 1. Table linearization -> 2. Context assembly (table + question) -> 3. Reasoning strategy selection -> 4. Answer generation -> 5. Post-processing for format alignment
- **Design tradeoffs:**
  - PoT vs. CoT: PoT provides verifiable computation but fails on unstructured text (18-30% EM); CoT handles text better (44-53% EM) but lacks execution guarantees
  - Sub-table decomposition (TabSQLify): Reduces context load but may miss cross-row evidence (15.3% EM)
  - Hybrid symbolic-semantic (TableMaster): Best performance (42-43% EM) with higher complexity
- **Failure signatures:**
  - Interpretation errors (22%): Incomplete row filtering, misread question scope
  - Logical inconsistencies (31%): Correct intermediate reasoning, wrong final answer
  - Extraction errors (20%): Missed information in unstructured text fields
  - Misalignment errors (27%): Correct reasoning but wrong output format
- **First 3 experiments:**
  1. Baseline replication: Run GPT-4o-mini with Zero-Shot, Few-Shot, CoT on 100-table subset to confirm performance hierarchy (expected: CoT > Zero-Shot > PoT)
  2. Scale sensitivity: Evaluate same model across token bins (10k-18k, 18k-28k, 28k-35k, 35k-50k, 50k-85k) to replicate monotonic degradation
  3. Heterogeneity analysis: Convert 20 semi-structured tables to (a) fully structured and (b) fully unstructured formats; compare PoT on (a) vs. CoT on (b) to validate modality alignment pattern

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can tabular reasoning systems be adapted to handle multi-table or relational reasoning, moving beyond the single-table focus of current benchmarks?
- Basis in paper: [explicit] The authors state in the Limitations section that the current benchmark "could further incorporate multi-table and relational reasoning" to better reflect real-world database complexity.
- Why unresolved: RUST-BENCH currently limits evaluation to individual tables, whereas many real-world scenarios require joining or comparing data across multiple sources.
- What evidence would resolve it: A benchmark extension including multi-table contexts and corresponding baselines that demonstrate successful cross-table inference.

### Open Question 2
- Question: To what extent does real-world noise (missing cells, typos, schema drift) degrade the reasoning capabilities of LLMs on semi-structured tables?
- Basis in paper: [explicit] The Conclusion notes that future work will "introduce real-world noise... to assess robustness, calibration, and recovery under imperfect data."
- Why unresolved: The current dataset relies on verified, cleaned data, potentially inflating model performance compared to messy, uncurated real-world inputs.
- What evidence would resolve it: Evaluation of current SOTA models on a modified RUST-BENCH test set injected with controlled noise types and error rates.

### Open Question 3
- Question: Does augmenting LLMs with external tools for retrieval, schema induction, and program execution improve accuracy on large, heterogeneous tables?
- Basis in paper: [explicit] The Conclusion proposes pairing LLMs with such tools "aiming for verifiable, scalable reasoning over semi-structured data."
- Why unresolved: The paper's current experiments primarily test models using various prompting strategies (CoT, PoT) and existing reasoning frameworks without integrated external tool-use capabilities.
- What evidence would resolve it: A comparative study showing that tool-augmented agents significantly outperform non-augmented baselines on the "Lost in the Middle" challenges identified in the paper.

## Limitations

- The hybrid symbolic-semantic generation approach may introduce systematic biases toward certain question types that are easier to generate programmatically
- The error analysis categorization lacks inter-annotator reliability measures
- The LLM-Score metric using GPT-4o-mini as judge introduces circular evaluation problems
- The benchmark's focus on exact match may underestimate capabilities of models that provide semantically correct but syntactically different answers

## Confidence

- **High confidence:** The dataset creation methodology and scale (7,966 QA pairs over 2,031 tables) are well-documented and reproducible. The core finding that performance degrades with table size and complexity is consistently demonstrated across multiple models and metrics.
- **Medium confidence:** The error analysis categorization is plausible but not validated through independent annotation. The attribution of failures to specific mechanisms (context-internal retrieval, modality mismatch, error propagation) is supported by patterns but lacks causal validation.
- **Low confidence:** Claims about specific architectural weaknesses being the primary bottleneck are largely theoretical, as the benchmark measures end-to-end performance rather than isolating specific components.

## Next Checks

1. **Error Analysis Replication**: Replicate the error categorization on a held-out subset using independent annotators to establish inter-annotator agreement scores (Cohen's kappa > 0.7 required).

2. **Controlled Modality Experiments**: Systematically convert 50 semi-structured tables to fully structured and fully unstructured versions, then re-run evaluations to confirm the modality alignment patterns with statistical significance testing.

3. **Retrieval vs. Reasoning Isolation**: Implement a retrieval-augmented baseline that pre-filters tables to relevant rows before reasoning, then compare performance degradation curves to determine whether context-internal retrieval or reasoning limitations drive performance drops.