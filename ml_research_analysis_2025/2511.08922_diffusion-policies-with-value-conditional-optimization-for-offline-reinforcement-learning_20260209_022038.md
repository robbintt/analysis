---
ver: rpa2
title: Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement
  Learning
arxiv_id: '2511.08922'
source_url: https://arxiv.org/abs/2511.08922
tags:
- policy
- diffusion
- learning
- offline
- divo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of value overestimation in offline
  reinforcement learning caused by out-of-distribution actions, which limits policy
  performance. Existing diffusion model approaches often apply indiscriminate regularization
  to all actions, leading to excessive conservatism and suboptimal performance.
---

# Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.08922
- Source URL: https://arxiv.org/abs/2511.08922
- Reference count: 40
- Authors: Yunchang Ma; Tenglong Liu; Yixing Lan; Xin Yin; Changxin Zhang; Xinglong Zhang; Xin Xu
- Primary result: State-of-the-art performance on D4RL benchmark with 831.6 average returns on Gym tasks and 486.7 on AntMaze tasks

## Executive Summary
DIVO addresses value overestimation in offline RL by introducing a novel binary-weighted mechanism that leverages advantage values to guide diffusion model training. The method selectively expands high-advantage actions while maintaining alignment with the dataset's distribution, achieving a critical balance between conservatism and explorability. Evaluated on D4RL benchmark, DIVO outperforms existing methods across diverse tasks, delivering significant improvements in average returns for both locomotion tasks and the challenging AntMaze domain with sparse rewards.

## Method Summary
DIVO builds on TD3+BC framework with two key components: (1) Positive Advantage Diffusion Learning (PAD) trains a diffusion behavior policy using a binary-weighted loss based on advantage values, selectively amplifying high-value state-action pairs; (2) Adaptive Diffusion-based Policy Optimization (ADPO) uses high-advantage actions from the diffusion model as target policy, training a separate actor network to maximize Q-value while minimizing distance to this target. The method employs K=5 diffusion steps, with hyperparameters τ=5e-3, γ=0.99/0.995, and α=2.5 (Gym) or {2.5,7.5,20.0} (AntMaze). Training runs for 1M steps with batch size 256 across 5 seeds.

## Key Results
- Achieves state-of-the-art performance on D4RL benchmark with 831.6 average returns on Gym tasks
- Outperforms existing methods in AntMaze domain with sparse rewards, achieving 486.7 average returns
- Demonstrates superior performance across diverse locomotion tasks and challenging navigation scenarios
- Shows robust performance with hyperparameter sensitivity analysis indicating stability across different guidance strengths

## Why This Works (Mechanism)

### Mechanism 1: Advantage-Weighted Diffusion Training
The paper applies a binary weight based on advantage estimates during diffusion training to selectively amplify high-value behaviors while suppressing low-value noise. A Positive Advantage Diffusion learning (PAD) objective weights the standard diffusion loss by $f(s,a) = \eta \cdot \mathbb{1}(Q(s,a) - V(s))$, creating a "selective behavior cloning" effect where the generative model is trained primarily on state-action pairs where Q-value exceeds the baseline Value function ($A > 0$). This addresses the "indiscriminate regularization" problem of existing diffusion approaches.

### Mechanism 2: Decoupled Policy Optimization
DIVO stabilizes training and reduces computational overhead by decoupling the expressive diffusion model from the final policy network. Instead of using the diffusion model as the final policy (requiring backpropagation through time), DIVO uses it to generate a "target action" $\pi_{target}$. A separate, simpler actor network $\pi_\theta$ is then updated to maximize Q-value while minimizing the distance to this target, avoiding the "instability and computational burden" of diffusion backprop.

### Mechanism 3: Dynamic Filtering for Conservatism
The method balances conservatism and explorability by dynamically filtering actions during the policy improvement step rather than applying a global constraint. The ADPO method selects a target action only if the diffusion-generated action has a positive advantage. If the advantage is negative, it defaults to a different action (likely the current policy or a safe baseline). This acts as a dynamic filter that only "steps" toward new behaviors if they promise improvement.

## Foundational Learning

- **Concept: Advantage Function ($A = Q - V$)**
  - Why needed: The core mechanism (PAD) relies on distinguishing "better than average" actions from "worse than average" actions to weight the diffusion loss.
  - Quick check: If $V(s)$ is overestimated, how does that affect the binary mask $f(s,a)$ in the PAD loss?

- **Concept: Forward vs. Reverse Diffusion Process**
  - Why needed: The paper uses a diffusion model to represent the behavior policy. Understanding that the model learns to reverse a noising process is required to grasp why backpropagation is expensive.
  - Quick check: Why does Eq. 8 require iterating from $K$ down to $1$, and how does this relate to the "computational overhead" mentioned in Section III-B?

- **Concept: Distribution Shift & OOD Actions**
  - Why needed: The paper frames its contribution against the problem of "out-of-distribution (OOD) actions" causing "value overestimation." This is the motivation for the "conservatism" the paper tries to balance.
  - Quick check: Why does the paper argue that "indiscriminate regularization" is bad for performance in this context?

## Architecture Onboarding

- **Component map:** Critic ($Q_\phi$) -> Value Network ($V_\psi$) -> Diffusion Model ($\epsilon_\omega$) -> Actor ($\pi_\theta$)
- **Critical path:**
  1. Train Critic/Value networks via TD learning (Eq. 1, 2)
  2. Compute Advantage $A = Q - V$
  3. Train Diffusion: Update $\omega$ using weighted loss (Eq. 9) where weights depend on Advantage
  4. Train Actor: Sample action from Diffusion → Check Advantage → Set Target → Update $\theta$ (Eq. 13)
- **Design tradeoffs:**
  - Binary vs. Soft Weights: The paper uses a binary $\mathbb{1}(\cdot)$ weight, which is aggressive filtering but simpler to compute than soft weighting
  - Expressiveness vs. Efficiency: DIVO trades the expressiveness of a diffusion policy for the efficiency of a Gaussian policy guided by a diffusion behavior model
- **Failure signatures:**
  - Silent Collapse: If Q values fail to distinguish actions, $A \approx 0$ everywhere, potentially reducing the diffusion model to random noise or a full BC clone
  - Filter Stagnation: In sparse rewards, the "Dynamic Filter" might never see $A > 0$, causing the actor to never receive guidance from the diffusion model
- **First 3 experiments:**
  1. Hyperparameter Sensitivity ($\eta, \beta$): Replicate Figure 4 to see if binary weighting holds up across different guidance strengths
  2. Ablation on Binary vs. Soft Weighting: Replace $\mathbb{1}(Q-V)$ with a soft exponential weight to test if sharp distinction matters
  3. Advantage Accuracy Check: Plot correlation between ground-truth return of sampled actions and their computed Advantage $A$

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided text.

## Limitations
- Binary weighting mechanism relies heavily on accurate advantage estimation, which can be unstable in sparse-reward environments like AntMaze
- Decoupled architecture assumes diffusion model can reliably generate high-advantage actions, dependent on quality of advantage signal
- Network architecture details for diffusion model (beyond K=5 steps reference) are underspecified, potentially affecting reproducibility

## Confidence
- **High confidence:** The core mechanism of using advantage-weighted diffusion training (PAD) to selectively amplify high-value behaviors is well-defined and supported by the theoretical framework
- **Medium confidence:** The decoupled policy optimization approach (ADPO) appears sound but its empirical robustness across diverse task distributions requires further validation
- **Medium confidence:** The dynamic filtering mechanism's ability to balance conservatism and explorability depends critically on advantage estimate quality, which varies by environment

## Next Checks
1. **Advantage signal reliability test:** Evaluate the correlation between computed advantage values and actual returns across all D4RL tasks to verify the binary filter is selecting genuinely high-potential actions
2. **Binary vs. soft weighting ablation:** Replace the binary indicator with a smooth exponential weighting function to test whether the sharp distinction claimed in PAD is critical for performance
3. **Diffusion model architecture stress test:** Vary the diffusion steps K (beyond the reported 5) and observe impact on both sample quality and computational efficiency to establish sensitivity to this key hyperparameter