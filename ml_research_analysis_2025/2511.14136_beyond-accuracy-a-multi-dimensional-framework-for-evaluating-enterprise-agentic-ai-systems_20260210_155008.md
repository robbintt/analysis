---
ver: rpa2
title: 'Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic
  AI Systems'
arxiv_id: '2511.14136'
source_url: https://arxiv.org/abs/2511.14136
tags:
- agents
- evaluation
- cost
- tasks
- enterprise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CLEAR, a five-dimensional evaluation framework
  for enterprise agentic AI systems that addresses critical gaps in existing benchmarks
  by incorporating cost, latency, efficacy, assurance, and reliability metrics. Through
  analysis of 12 major benchmarks and evaluation of six state-of-the-art agents across
  300 enterprise tasks, the authors demonstrate that optimizing for accuracy alone
  yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable
  performance.
---

# Beyond Accuracy: A Multi-Dimensional Framework for Evaluating Enterprise Agentic AI Systems

## Quick Facts
- arXiv ID: 2511.14136
- Source URL: https://arxiv.org/abs/2511.14136
- Authors: Sushant Mehta
- Reference count: 8
- Introduces CLEAR framework showing accuracy-only optimization yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance

## Executive Summary
CLEAR introduces a five-dimensional evaluation framework for enterprise agentic AI systems that addresses critical gaps in existing benchmarks by incorporating cost, latency, efficacy, assurance, and reliability metrics. Through analysis of 12 major benchmarks and evaluation of six state-of-the-art agents across 300 enterprise tasks, the framework demonstrates that optimizing for accuracy alone yields agents significantly more expensive than cost-aware alternatives with comparable performance. Expert validation (N=15) shows CLEAR predictions correlate strongly with production success (ρ=0.83) compared to accuracy-only evaluation (ρ=0.41), establishing CLEAR as essential for enterprise deployment decisions.

## Method Summary
The paper evaluates six agent architectures (ReAct-GPT4, ReAct-GPT-o3, Reflexion, Plan-Execute, ToolFormer, Domain-Tuned Llama) on 300 enterprise tasks across six domains using the CLEAR framework. Each task includes natural language descriptions, input contexts, expected outputs, policy documents, and success criteria. Agents are evaluated on Cost-Normalized Accuracy (CNA), Cost-Per-Success (CPS), SLA Compliance Rate (SCR), Policy Adherence Score (PAS), and pass@k reliability metrics. Expert ratings (N=15) validate CLEAR's correlation with production success compared to accuracy-only evaluation. The framework identifies Pareto-optimal agents and exposes reliability degradation under repeated execution.

## Key Results
- Optimizing for accuracy alone yields agents 4.4-10.8x more expensive than cost-aware alternatives with comparable performance
- Agent performance drops from 60% (single run) to 25% (8-run consistency), revealing brittleness masked by accuracy-only metrics
- Domain-tuned smaller models (70B parameters) achieve superior cost-normalized accuracy (260.4 CNA) and reliability (72.8% pass@8) versus general-purpose larger models

## Why This Works (Mechanism)

### Mechanism 1: Multi-Dimensional Cost-Accuracy Trade-offs
Single-metric optimization ignores cost constraints, causing over-engineered solutions where Reflexion's iterative self-refinement adds marginal accuracy at exponential cost. The Pareto frontier reveals that Plan-Execute achieves comparable efficacy (71.9% vs 74.1%) at 4.1x lower cost.

### Mechanism 2: Reliability Degradation Under Repeated Execution
Agents exhibit variance across executions due to temperature sampling and context sensitivity. Pass@k measures probability of k consecutive successes, exposing inconsistency where performance drops from 60% to 25% between single-run and 8-run consistency.

### Mechanism 3: Domain Specialization Efficiency Gains
Task-specific fine-tuning reduces unnecessary reasoning steps and lowers token consumption (31.2K vs 47-89K input tokens). Domain-Tuned models achieve best CNA (260.4) and reliability (72.8% pass@8) through learned domain patterns.

## Foundational Learning

- **Pass@k reliability metric**: Needed because single-run accuracy obscures production brittleness; pass@k (k=3,5,8) measures consistency probability. Quick check: If an agent scores 70% pass@1 but 30% pass@8, would you deploy it for a customer-facing workflow?

- **Pareto frontier optimization**: Needed because multi-dimensional evaluation identifies agents not dominated on any dimension. Quick check: Agent A has 74% efficacy at $5.12/task; Agent B has 72% efficacy at $1.24/task. Which is Pareto-optimal if both have similar reliability?

- **Cost-normalized accuracy (CNA)**: Needed to enable fair comparison across agents with different cost-accuracy profiles. Quick check: Two agents achieve 70% accuracy. Agent X costs $0.27/task; Agent Y costs $2.87/task. Which has higher CNA?

## Architecture Onboarding

- **Component map**: Cost (token consumption, API costs) -> Latency (planning, execution, reflection timing) -> Efficacy (task completion quality) -> Assurance (security, policy compliance) -> Reliability (pass@k consistency)

- **Critical path**: 1) Define domain-specific SLA thresholds (3s customer support, 30s code generation) 2) Establish policy-critical actions and violation categories 3) Run multi-trial evaluation (minimum 8 runs per task) for reliability assessment 4) Calculate CNA and identify Pareto frontier

- **Design tradeoffs**: Reflexion-style self-reflection adds 2-4% efficacy at 3-4x cost, -latency, -reliability; Plan-Execute hierarchical decomposition offers balanced cost-efficacy for multi-step workflows; Domain-tuning provides best CNA and reliability but requires fine-tuning investment.

- **Failure signatures**: High pass@1, low pass@8 indicates brittle agent unsuitable for production; PAS < 0.85 signals policy violations; SLA breach rate > 20% indicates latency issues in reflection-heavy architectures.

- **First 3 experiments**: 1) Run current agent on 20 representative tasks, 8 times each; calculate pass@1 vs pass@8 gap 2) Measure token consumption and latency breakdown for top 3 highest-volume task types 3) Test against 50 adversarial prompts and calculate PAS score

## Open Questions the Paper Calls Out

1. Can adaptive weight learning methods automatically determine optimal dimension weights for different enterprise contexts, and would learned weights outperform default equal-weighting scheme?

2. Does CLEAR's correlation with production success generalize to larger, more diverse expert cohorts and across industries with different regulatory constraints?

3. How should CLEAR framework be extended to evaluate multi-agent coordination systems, and what new dimensions (e.g., inter-agent communication overhead, coordination failure modes) are necessary?

4. Does reliability pass@k threshold (k≥8 for 80% consistency) generalize across all enterprise domains, or should domain-specific thresholds be established?

## Limitations

- Task Suite Representativeness: 300-task enterprise suite composition partially specified; actual task content and complexity distribution unclear
- Domain-Tuned Model Transparency: Performance advantages rely on unspecified fine-tuning procedures, base model version, and training details
- Policy Violation Taxonomy: Assurance metric depends on undefined policy-critical actions and violation categories across domains

## Confidence

- **High Confidence**: Multi-dimensional evaluation necessity, reliability measurement via pass@k, cost-normalized accuracy utility, expert validation showing CLEAR correlation superiority
- **Medium Confidence**: Pareto frontier dominance findings, specific agent performance rankings, domain-tuning effectiveness magnitude, 4.4-10.8x cost differential estimates
- **Low Confidence**: Exact performance thresholds for production readiness, generalizability across enterprise domains, reproducibility of Domain-Tuned model advantages

## Next Checks

1. **Reliability gap quantification**: Evaluate target agent on 20 representative enterprise tasks with 8 runs each; measure pass@1 vs pass@8 degradation; confirm if >15% drop indicates production unsuitability

2. **CNA threshold validation**: Calculate cost-normalized accuracy for current deployment agents; establish minimum CNA thresholds for different task criticality levels

3. **Adversarial policy testing**: Construct 50+ adversarial prompts targeting data leakage, prompt injection, and compliance violations; measure PAS score and identify specific violation patterns that CLEAR flags but accuracy-only evaluation misses