---
ver: rpa2
title: How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction
arxiv_id: '2601.08626'
source_url: https://arxiv.org/abs/2601.08626
tags:
- semantic
- structural
- across
- recovery
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OrderProbe introduces a deterministic benchmark for evaluating
  large language models' structural reconstruction ability using fixed four-character
  expressions in Chinese, Japanese, and Korean. Unlike sentence-level restoration,
  these expressions have unique canonical orders enabling exact-match scoring.
---

# How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction

## Quick Facts
- **arXiv ID:** 2601.08626
- **Source URL:** https://arxiv.org/abs/2601.08626
- **Reference count:** 40
- **Primary result:** Zero-shot exact recovery below 35% reveals clear dissociation between semantic recall and structural planning in LLMs

## Executive Summary
OrderProbe introduces a deterministic benchmark for evaluating large language models' structural reconstruction ability using fixed four-character expressions in Chinese, Japanese, and Korean. Unlike sentence-level restoration, these expressions have unique canonical orders enabling exact-match scoring. The benchmark includes 3,543 curated samples and a diagnostic framework evaluating semantic fidelity, logical validity, consistency, robustness, and information density. Experiments on twelve LLMs show that exact reconstruction remains difficult, with zero-shot recovery frequently below 35%, revealing a clear dissociation between semantic recall and structural planning. The framework enables fine-grained analysis of failure modes, demonstrating that structural robustness is not an automatic byproduct of semantic competence.

## Method Summary
OrderProbe constructs a deterministic benchmark using 3,543 four-character expressions from Chinese, Japanese, and Korean, generating all 23 non-identity permutations for each (81,489 total inputs). Models must reconstruct the exact canonical order and provide a one-line semantic explanation. The evaluation uses six diagnostic metrics: recovery rate (exact match), semantic fidelity (using cross-encoders and embeddings), logical validity (NLI), structural consistency, robustness (harmonic mean of sequential and structural), and information density. The framework tests zero-shot, chain-of-thought, and few-shot prompting conditions across twelve LLMs including Qwen, GPT-4o, DeepSeek, and others.

## Key Results
- Zero-shot exact recovery falls below 35% across models, with best performance at 33.1%
- Clear dissociation between semantic recall (high) and structural planning (low) demonstrated through diagnostic metrics
- Script typology affects difficulty: Chinese/Japanese (logographic) outperform Korean (phonogrammatic) due to preserved semantic anchors
- Chain-of-thought prompting improves recovery for some models (+42.1% for DeepSeek-V3.2) but causes regressions in others (-3.2% for Qwen-3-8B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Exact structural reconstruction from scrambled inputs is not an automatic byproduct of semantic competence in LLMs.
- **Mechanism:** Models appear to process scrambled characters as independent tokens rather than enforcing global ordering constraints. They retrieve memorized semantic entries but fail to reassemble canonical structure from constituent parts.
- **Core assumption:** Four-character expressions have rigid canonical forms that enable deterministic evaluation; failure on this constrained task implies broader structural fragility.
- **Evidence anchors:**
  - [abstract] "Zero-shot recovery frequently falls below 35%... consistent dissociation between semantic recall and structural planning."
  - [section 4.2] "Models can retrieve meanings or provide plausible definitions, but often fail to enforce global ordering constraints."
  - [corpus] Weak direct corpus evidence; related work on positional bias (Sinha et al., 2021; Berglund et al., 2024) supports order-sensitivity limitations.
- **Break condition:** If models were simply retrieving memorized sequences, frequency-stratified analysis would show near-perfect recovery on high-frequency items. The paper reports meaningful failures even in high-frequency bins (Table 7).

### Mechanism 2
- **Claim:** Script typology modulates reconstruction difficulty through local semantic anchor preservation.
- **Mechanism:** Logographic characters (Chinese, Japanese Kanji) retain standalone semantic cues under scrambling, enabling local-to-global reassembly. Phonogrammatic scripts (Korean Hangul) lose syllable-level coherence when scrambled, removing interpretable sub-units.
- **Core assumption:** Reconstruction relies on preserved semantic anchors at the character level; destroying these anchors makes the task fundamentally harder rather than just more confusing.
- **Evidence anchors:**
  - [section 4.4] "Chinese and Japanese exhibit substantially higher recovery than Korean... Logographic characters preserve standalone semantic cues."
  - [section 4.4] "Korean Hangul behaves differently: internal scrambling destroys syllable composition and removes meaningful local cues."
  - [corpus] No direct corpus support for this specific cross-linguistic mechanism.
- **Break condition:** If Korean's low recovery were purely due to tokenization artifacts rather than anchor loss, character-level models would show different patterns than subword models. The paper does not isolate this factor.

### Mechanism 3
- **Claim:** Chain-of-thought prompting improves reconstruction by encouraging explicit constraint checking before generation.
- **Mechanism:** Stepwise reasoning prompts cause models to enumerate and evaluate candidate permutations, reducing impulsive outputs. This helps some models substantially (DeepSeek-V3.2: +42.1% recovery) but causes regressions in others (Qwen-3-8B: -3.2%).
- **Core assumption:** CoT benefits come from structured reasoning rather than simply longer contexts; format drift or unstable decoding can negate benefits.
- **Evidence anchors:**
  - [section 4.2] "CoT prompting improves recovery for many models... improvements are highly model-dependent."
  - [section 4.3] "Few-shot prompting yields the most uniform improvements... CoT gains are less stable and may amplify format drift."
  - [corpus] No corpus papers directly test CoT on structural reconstruction.
- **Break condition:** Gains vanish if CoT is applied to canonical (unscrambled) inputs; Table 11 shows near-ceiling performance regardless of prompting, confirming CoT specifically helps structural reasoning under perturbation.

## Foundational Learning

- **Concept: Permutation group over fixed-length sequences**
  - Why needed here: The paper generates all 23 non-identity permutations of 4-character sequences, creating a complete combinatorial evaluation space. Understanding S₄ symmetry helps interpret why certain permutations are harder (anchor displacement).
  - Quick check question: Why does the paper exclude the identity permutation from P(x)?

- **Concept: Semantic fidelity vs. structural planning dissociation**
  - Why needed here: The core finding is that models can score high on semantic explanation (S_mean_Acc) while failing exact reconstruction. This dissociation invalidates single-metric leaderboards.
  - Quick check question: If a model achieves 0.8 semantic fidelity but 15% recovery, what does that imply about its internal representations?

- **Concept: Harmonic mean for multi-dimensional robustness aggregation**
  - Why needed here: S_Rob combines sequential robustness and structural robustness via harmonic mean, penalizing imbalance. This design choice affects how to interpret composite scores.
  - Quick check question: Why use harmonic mean instead of arithmetic mean for combining S_seq and S_struct?

## Architecture Onboarding

- **Component map:** Multi-source collection → expert filtering (F: C_raw → X) → semantic reference construction (S_x) → permutation generator (P(x) generates 23 variants per item) → model inference → exact-match scoring + diagnostic computation → cross-permutation aggregation

- **Critical path:** Canonical expression selection (must have unique ground truth) → permutation generation (all non-identity) → model inference → exact-match scoring + diagnostic computation → cross-permutation aggregation

- **Design tradeoffs:**
  - Fixed-length expressions enable deterministic scoring but limit generalization to free-form sentences
  - CJK focus provides script typology comparison but excludes alphabetic languages
  - Six-metric framework increases diagnostic power but complicates comparison; no single aggregate score

- **Failure signatures:**
  - **Semantic hallucination:** High S_cons with low S_logic—model outputs stable but factually wrong explanations (e.g., literal composition of "Qing Guo Qing Cheng")
  - **Verbosity inflation:** Low S_Info with moderate S_mean_Acc—knowledge dumping without precision
  - **Permutation sensitivity:** Large gap between MDR and MDA—model collapses on specific reorderings while handling others

- **First 3 experiments:**
  1. **Baseline sweep:** Run all 12 models on full OrderProbe under zero-shot; verify <35% recovery and establish diagnostic signatures per model tier.
  2. **Ablation by syntactic category:** Isolate which structures (parallel vs. verb-object) drive failures; expect parallel structures to be easiest per Figure 5.
  3. **Script typology control:** Compare Chinese/Japanese vs. Korean recovery directly; Korean should show ~5% recovery with high S_cons (generic fallback behavior) per Table 4.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the structural reconstruction limitations observed in logographic CJK scripts persist in alphabetic or morphologically rich languages?
- Basis in paper: [explicit] The authors state in the Limitations section that they "have not conducted detailed experiments on morphologically rich systems" and that "the influence of different script typologies on model performance has not been thoroughly analyzed for alphabetic languages."
- Why unresolved: OrderProbe is restricted to Chinese, Japanese, and Korean, making it unclear if the semantic-structure dissociation is a universal phenomenon or specific to the script typologies tested.
- What evidence would resolve it: Adapting the permutation-based perturbation benchmark to English idioms or morphologically complex languages (e.g., Finnish, Turkish) and comparing recovery rates.

### Open Question 2
- Question: Can LLMs apply structural planning to reconstruct novel combinations of characters, or is performance strictly bound to memorized expressions?
- Basis in paper: [explicit] The authors acknowledge that "correct reconstruction depends on the memorization of these fixed patterns" and note that "future developments may require models to handle novel combinations beyond memorized content."
- Why unresolved: The current benchmark relies on crystallized expressions likely present in pre-training data, conflating structural reasoning with retrieval strength.
- What evidence would resolve it: Testing reconstruction accuracy on synthetic or newly coined four-character expressions that follow valid semantic rules but do not exist in the training corpus.

### Open Question 3
- Question: What specific mechanisms cause Chain-of-Thought (CoT) prompting to improve structural recovery in some models while causing regressions in others?
- Basis in paper: [explicit] Section 4.2 notes that "CoT helps, but gains vary widely" and that "improvements are highly model-dependent," mentioning "format drift and unstable decoding" as potential issues.
- Why unresolved: The paper observes the phenomenon but does not provide a causal explanation for why CoT destabilizes certain model architectures during structural tasks.
- What evidence would resolve it: An ablation study analyzing attention head behavior and decoding stability during CoT reasoning on scrambled inputs compared to standard zero-shot generation.

## Limitations
- The benchmark relies on fixed four-character expressions with unique canonical forms, limiting generalizability to free-form language
- Cross-linguistic comparisons lack direct corpus evidence for the proposed script typology mechanisms
- The six-metric framework increases diagnostic power but complicates model comparison without a single aggregate score
- Chain-of-thought prompting benefits are observed but the underlying mechanisms for model-dependent success/failure remain unexplained

## Confidence
- **High Confidence:** Zero-shot recovery below 35% and dissociation between semantic recall and structural planning are well-supported by experimental results across twelve models
- **Medium Confidence:** Script typology effects (logographic vs. phonogrammatic) are supported by performance gaps but lack direct corpus evidence
- **Low Confidence:** Extrapolation from four-character task to broader structural fragility claims requires validation on free-form sentences

## Next Checks
1. **Frequency-stratified recovery analysis:** Run the full benchmark on high-frequency vs. low-frequency expressions to test whether memorization alone can explain observed failures, particularly examining if high-frequency items show near-perfect recovery.

2. **Character-level vs. subword tokenization ablation:** Compare Korean recovery results between character-level and subword-tokenized models to determine whether low performance stems from anchor loss or tokenization artifacts.

3. **Canonical vs. scrambled input control:** Apply CoT prompting to both canonical (unscrambled) and scrambled inputs across all models to verify that CoT benefits specifically target structural reasoning under perturbation rather than simply providing longer contexts.