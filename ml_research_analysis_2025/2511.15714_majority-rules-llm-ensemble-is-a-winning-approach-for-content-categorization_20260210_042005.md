---
ver: rpa2
title: 'Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization'
arxiv_id: '2511.15714'
source_url: https://arxiv.org/abs/2511.15714
tags:
- ensemble
- categorization
- llms
- performance
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that ensemble-based large language models
  (eLLMs) substantially outperform individual LLMs in hierarchical text categorization,
  achieving up to 65% improvement in F1-score. By combining predictions from multiple
  diverse LLMs under a collective decision-making framework, eLLMs mitigate hallucinations,
  reduce category inflation, and improve classification accuracy on the IAB taxonomy.
---

# Majority Rules: LLM Ensemble is a Winning Approach for Content Categorization

## Quick Facts
- **arXiv ID:** 2511.15714
- **Source URL:** https://arxiv.org/abs/2511.15714
- **Reference count:** 16
- **Primary result:** Ensemble of diverse LLMs achieves up to 65% improvement in F1-score for hierarchical text categorization versus individual models

## Executive Summary
This study demonstrates that ensemble-based large language models (eLLMs) substantially outperform individual LLMs in hierarchical text categorization, achieving up to 65% improvement in F1-score. By combining predictions from multiple diverse LLMs under a collective decision-making framework, eLLMs mitigate hallucinations, reduce category inflation, and improve classification accuracy on the IAB taxonomy. Experiments with 10 LLMs on 8,660 documents show that even small ensembles surpass the best single model, with larger ensembles approaching human-expert performance. The approach offers a scalable, robust solution for taxonomy-based classification, though it requires increased computational cost.

## Method Summary
The method uses zero-shot prompting across 10 diverse LLMs to classify text into IAB 2.2 taxonomy categories (698 categories, 4 levels). The Collective Decision-Making (CDM) framework aggregates predictions using a relevance score that combines vote counts, category depth, and semantic proximity. A consensus threshold (τ=0.65) filters low-agreement predictions, while a Parent Exclusion Rule ensures hierarchical consistency. The system processes 8,660 human-annotated samples, evaluating performance through F1-score, precision, recall, hallucination ratio, and category inflation ratio.

## Key Results
- Ensemble achieves 0.92 F1-score, representing 67% improvement over best single model
- Even small ensembles (3-5 models) surpass individual LLM performance
- Precision improves from 0.69 to 1.00 as consensus threshold increases, demonstrating effective noise filtering
- Larger ensembles approach human-expert classification accuracy on IAB taxonomy

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating diverse LLMs suppresses stochastic errors and hallucinations better than scaling a single model.
- **Mechanism:** The Collective Decision-Making (CDM) framework treats each LLM as an independent voter. By requiring a consensus threshold $\tau$ (e.g., 0.65), idiosyncratic outputs—such as hallucinated categories or "category inflation" generated by only one model—are filtered out. This relies on the statistical principle that independent errors are less likely to coincide across diverse architectures than correct predictions.
- **Core assumption:** Models must have independent error patterns and individual competence greater than random chance (Condorcet Jury Theorem conditions).
- **Evidence anchors:** Table 2 shows precision jumping from 0.69 to 1.00 as the threshold increases; diversity and redundancy suppress individual errors and reduce variance.

### Mechanism 2
- **Claim:** Ensembling expands the effective knowledge universe for sparse taxonomy mapping.
- **Mechanism:** No single LLM perfectly captures the semantic boundaries of a fixed taxonomy (T). Different models (e.g., Claude vs. Llama) encode different prior knowledge. The ensemble aggregates these varied perspectives, allowing the "wisdom of crowds" to cover gaps in any single model's training data, specifically for the IAB 2.2 taxonomy.
- **Core assumption:** The taxonomy categories are semantically reachable by at least a subset of the ensemble members.
- **Evidence anchors:** Majority consensus among models—rather than individual scale—drives superior accuracy; leveraging models built on diverse architectures extends the effective knowledge universe.

### Mechanism 3
- **Claim:** Hierarchical relevance scoring prevents category fragmentation.
- **Mechanism:** The CDM uses a relevance score $R(c)$ combining Popularity ($Q_c$), Importance ($I_c$, depth), and Proximity ($Prox_c$). This ensures that if models disagree on granular Tier-3/4 labels but agree on a Tier-1 parent, the consistency of the hierarchical structure is preserved, preventing the "compression" issues seen in single models.
- **Core assumption:** The taxonomy is strictly hierarchical, and parent-child relationships are valid.
- **Evidence anchors:** Equations (3.7–3.16) formalize the scoring: $R(c) = G(Q_c, I_c, Prox_c)$; Parent Exclusion Rule (PER), which removes a parent if its child is present, ensuring hierarchical consistency.

## Foundational Learning

- **Concept: Zero-Shot Taxonomy Constraints**
  - **Why needed here:** The paper assumes models can map text to a fixed label set (IAB 2.2) without fine-tuning. Understanding that the model is "constrained" to return specific strings (and hallucination occurs when it invents new ones) is vital for interpreting the error reduction results.
  - **Quick check question:** If an LLM returns "Sports > Football" but the taxonomy only has "Sports," is this a hallucination or a hierarchy error?

- **Concept: Precision-Recall Trade-off vs. Threshold**
  - **Why needed here:** The paper demonstrates that tuning the consensus threshold $\tau$ controls the Precision/Recall balance. Understanding this curve is necessary to implement the system for different business needs (e.g., high precision for compliance vs. high recall for discovery).
  - **Quick check question:** If you increase the consensus threshold to 0.90, will Recall increase or decrease, based on Table 2?

- **Concept: Correlated vs. Uncorrelated Errors**
  - **Why needed here:** The mechanism relies on error independence. An engineer must understand that selecting 10 models from the same family (e.g., all GPT-4 variants) might yield lower ensemble gains than 3 models from completely different families, due to correlated training data.
  - **Quick check question:** Why might a "diverse consortium" outperform a "larger consortium" of identical models?

## Architecture Onboarding

- **Component map:** Input Corpus -> 10 parallel LLMs (zero-shot inference) -> CDM Aggregation Engine -> Filter (consensus threshold) -> Output categories
- **Critical path:** The prompt design is the highest leverage point. The system prompt must strictly define the IAB 2.2 taxonomy structure to minimize hallucination before the ensemble even votes.
- **Design tradeoffs:**
  - **Latency vs. Accuracy:** Inference is parallelizable, but waiting for the slowest model (latency) scales with the "weakest link," while cost scales linearly with ensemble size.
  - **Cost:** A 10-LLM ensemble costs ~10x a single model. The paper shows diminishing returns (F1 0.76 -> 0.92) when moving from 3 to 10 models, suggesting a 3–5 model ensemble is the "sweet spot" for cost-efficiency.
- **Failure signatures:**
  - **Stubborn Hallucination:** If 2 out of 3 models hallucinate the same error (perhaps due to shared training bias), the majority rule will cement the error as truth.
  - **Category Inflation:** If $\tau$ is set too low (e.g., 0.50), the ensemble may include low-agreement noise, diluting precision.
- **First 3 experiments:**
  1. **Baseline Diversity Audit:** Run a fixed set of 50 samples through 3 distinct model families. Compare outputs: Are the errors correlated? If they all fail on the same inputs, the ensemble mechanism is broken.
  2. **Threshold Sweep:** Implement a binary search for $\tau$ on a validation set to find the optimal F1 score (the paper found 0.65). Do not assume this value transfers to non-IAB taxonomies.
  3. **Cost-Performance Ablation:** Measure F1 gain per dollar spent. Compare a 3-model ensemble against the 10-model "gold standard" to determine if the marginal F1 gain justifies the 3x cost increase.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can adaptive ensemble composition strategies based on domain specialization outperform fixed-size ensembles?
- **Basis in paper:** Section 9 (Future Work) proposes developing adaptive ensembles that select LLMs based on domain specialization or task complexity.
- **Why unresolved:** The current study relies on static pools of 2–10 models, treating all input domains uniformly rather than adjusting composition dynamically.
- **Evidence:** Experiments comparing classification accuracy and efficiency of dynamic model selection versus fixed pools across diverse content domains.

### Open Question 2
- **Question:** What is the minimal ensemble configuration required to achieve near-maximal accuracy while minimizing inference cost?
- **Basis in paper:** Section 9 identifies the need for "Cost–Performance Optimization" to find the minimal ensemble size that balances accuracy and expense.
- **Why unresolved:** While the paper demonstrates that larger ensembles yield higher F1 scores, the precise "elbow" in the cost-performance curve for industrial deployment remains undefined.
- **Evidence:** A Pareto frontier analysis evaluating F1-scores against computational latency and token costs for varying ensemble sizes.

### Open Question 3
- **Question:** Does implementing weighted voting or confidence calibration significantly improve performance over the standard unweighted majority rule?
- **Basis in paper:** Section 9 suggests implementing "Model Weighting and Fine-Tuning" to assign greater influence to reliable models.
- **Why unresolved:** The current framework treats all LLMs as equal experts using a simple multiplicative relevance score, ignoring variations in individual model reliability or confidence.
- **Evidence:** Comparative evaluation of weighted CDM algorithms against the standard unweighted approach on the 8,660-sample IAB dataset.

## Limitations
- The method's success depends heavily on model diversity, and the paper does not systematically characterize which combinations of architectures yield optimal results
- The IAB 2.2 taxonomy used for evaluation is proprietary, making independent validation difficult
- The computational cost scaling (10x inference cost) may be prohibitive for production deployment, particularly when the marginal F1 gains diminish after 3-5 models

## Confidence
- **High Confidence:** The core ensemble mechanism (CDM framework with consensus threshold) and its ability to reduce hallucinations and category inflation are well-supported by quantitative results
- **Medium Confidence:** The claim that "even small ensembles surpass the best single model" is supported, but the optimal ensemble size and composition remain underspecified
- **Medium Confidence:** The hierarchical relevance scoring mechanism appears sound mathematically, but its practical advantage over simpler majority voting requires further validation on diverse taxonomies

## Next Checks
1. **Diversity Characterization Study:** Systematically test ensembles of varying sizes and compositions (same-family vs. cross-family models) on the same IAB dataset to identify the optimal diversity-efficiency tradeoff and validate the assumption that uncorrelated errors drive performance gains.

2. **Taxonomy Transferability Test:** Apply the exact methodology (prompts, CDM parameters, τ=0.65) to a completely different taxonomy (e.g., open-source ontologies like WordNet or MeSH) to assess whether the 65% improvement generalizes beyond IAB 2.2.

3. **Cost-Benefit Sensitivity Analysis:** Measure the F1 improvement per dollar spent across ensemble sizes, and determine the breakeven point where additional models no longer justify their computational cost, providing practical deployment guidelines.