---
ver: rpa2
title: 'Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute
  Policy Optimization'
arxiv_id: '2509.21871'
source_url: https://arxiv.org/abs/2509.21871
tags:
- aesthetic
- zhang
- wang
- image
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Aes-R1, a comprehensive framework for image
  aesthetic assessment that combines high-quality reasoning data construction with
  reinforcement learning. The method addresses the challenge of subjective aesthetic
  judgment by jointly optimizing absolute score regression and relative ranking order
  through a novel algorithm called Relative-Absolute Policy Optimization (RAPO).
---

# Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization

## Quick Facts
- **arXiv ID:** 2509.21871
- **Source URL:** https://arxiv.org/abs/2509.21871
- **Reference count:** 29
- **Primary result:** Aes-R1 achieves state-of-the-art image aesthetic assessment with 47.9%/34.8% average PLCC/SRCC improvements over the backbone model using only 15K samples.

## Executive Summary
This paper introduces Aes-R1, a comprehensive framework for image aesthetic assessment that combines high-quality reasoning data construction with reinforcement learning. The method addresses the challenge of subjective aesthetic judgment by jointly optimizing absolute score regression and relative ranking order through a novel algorithm called Relative-Absolute Policy Optimization (RAPO). Aes-R1 includes an automatic data pipeline (AesCoT) that generates interpretable aesthetic reasoning data across five dimensions: light & shadow, mood & narrative, composition, color, and exposure. Trained on only 15K samples, Aes-R1 achieves state-of-the-art performance with average PLCC/SRCC improvements of 47.9%/34.8% over the backbone model, outperforming existing methods of similar size. The framework demonstrates strong generalization in out-of-distribution scenarios and provides both accurate aesthetic scores and grounded explanations.

## Method Summary
Aes-R1 consists of two stages: (1) Cold-start supervised fine-tuning on AesCoT dataset generated by GPT-4.1, and (2) reinforcement learning with RAPO algorithm. The AesCoT pipeline generates chain-of-thought critiques along five aesthetic dimensions paired with scores. The RAPO algorithm jointly optimizes relative ranking (pairwise probability) and absolute error rewards through gradient-based policy updates with KL divergence penalty for stability.

## Key Results
- Aes-R1 achieves 0.6337 PLCC and 0.5922 SRCC on AVA dataset, outperforming previous state-of-the-art methods.
- The framework demonstrates strong generalization with consistent performance across TAD66K and FLICKR-AES datasets.
- Aes-R1 generates high-quality interpretable reasoning across five aesthetic dimensions while maintaining accurate score predictions.

## Why This Works (Mechanism)

### Mechanism 1
Cold-start SFT on structured reasoning data activates aesthetic reasoning patterns that pure RL cannot reliably discover. The AesCoT pipeline generates chain-of-thought critiques along five dimensions paired with scores. One epoch of SFT on ~3-10K samples teaches the model to produce explanations before scoring, establishing a reasoning template that RL then refines. The backbone MLLM lacks sufficient aesthetic reasoning patterns in pre-training; without explicit reasoning examples, RL exploration may not discover structured analysis behaviors.

### Mechanism 2
Joint optimization of relative ranking and absolute error rewards aligns both ordinal consistency and score calibration. RAPO computes two rewards per generated output: (1) relative rank reward using pairwise comparison probability based on Gaussian score distributions, and (2) absolute error reward as exponential decay from ground-truth score. The combined gradient signal enforces that higher-scoring images rank above lower-scoring ones while keeping predictions near calibrated values. Human aesthetic judgment has both comparative and absolute components; optimizing only one leads to systematic failures.

### Mechanism 3
Moderate SFT followed by RL from a high-entropy checkpoint maximizes downstream performance by preserving exploration capacity. Token entropy peaks after 1 SFT epoch then declines with continued training. Initializing RAPO from high-entropy checkpoints (0-1 epoch SFT) yields larger RL improvements than from low-entropy checkpoints (10 epoch SFT), because entropy enables exploration of better policies during RL. RL requires policy diversity to discover improved behaviors; excessive SFT narrows the policy distribution, limiting what RL can subsequently learn.

## Foundational Learning

- **Policy gradient methods (REINFORCE-family)**: RAPO optimizes the MLLM via gradient ascent on expected reward; understanding baseline subtraction, advantage estimation, and clipping is essential for debugging training dynamics.
  - *Quick check:* Can you explain why the advantage function is computed as (reward - group_mean) / group_std rather than raw reward?

- **Learning to rank (pairwise/listwise approaches)**: The relative rank reward uses FRank-style pairwise probability; understanding how ranking losses differ from regression losses clarifies why rank-only optimization improves SRCC but not PLCC.
  - *Quick check:* What failure mode would you expect if you used only pairwise ranking loss for score prediction?

- **Chain-of-thought reasoning in vision-language models**: AesCoT's multi-dimensional critique structure relies on the model generating reasoning before the final answer; understanding CoT mechanics helps diagnose cases where reasoning becomes superficial.
  - *Quick check:* How would you detect "reward hacking" where the model generates plausible-looking reasoning that doesn't actually influence the score?

## Architecture Onboarding

- **Component map**: Image-score pairs → GPT-4.1 expert → Multi-dimensional critiques → Filter → AesCoT dataset → SFT (1 epoch) → RAPO (10 epochs) → Aes-R1

- **Critical path**: Data quality filtering (score leakage is most subtle failure mode) → SFT epoch selection (entropy monitoring is diagnostic) → Reward hyperparameters (σ for error reward, γ for rank numerical stability)

- **Design tradeoffs**: More SFT epochs → better format compliance but reduced RL gains; Higher K (samples per prompt) → better advantage estimation but higher compute; Larger σ in error reward → more tolerant of score deviation but weaker calibration signal

- **Failure signatures**: High SRCC, low PLCC → rank reward dominating (check error reward weight); Generic/superficial reasoning → insufficient SFT or data quality issues (check critique diversity); Entropy collapse in RL → learning rate too high or clipping too aggressive

- **First 3 experiments**: 1) Reproduce reward ablation on held-out subset to verify rank/error tradeoff; 2) Plot token entropy vs. SFT epochs to find peak checkpoint; 3) Qualitative analysis: sample 50 images, compare critiques from 0-epoch vs. 1-epoch SFT initialization

## Open Questions the Paper Calls Out

### Open Question 1
What is the precise relationship between SFT duration, model entropy, and final RL performance, and does an optimal SFT duration exist that maximizes RL gains without causing entropy collapse? The paper presents empirical observations for Qwen2.5-VL-7B but lacks a generalizable principle for determining optimal SFT duration a priori.

### Open Question 2
Can RAPO be effectively adapted to assess aesthetics for AI-generated images where ground-truth human preference may be less stable? The framework's reward model is tied to historical human MOSs from photography datasets, but its applicability to evolving aesthetic standards of generative AI is untested.

### Open Question 3
How does RAPO performance and data efficiency scale with larger datasets beyond 15K samples? The paper demonstrates efficacy in a data-efficient regime but does not characterize behavior at scale or whether performance plateaus or continues to improve.

### Open Question 4
Is joint optimization of rank and absolute error rewards superior to alternative multi-objective RL approaches or sequential training strategies? The paper introduces RAPO as novel but does not validate against other methods for combining objectives such as weighted sum of losses or multi-stage training.

## Limitations

- Data pipeline reproducibility is challenging due to unspecified automated filtering logic for reasoning consistency and factual errors in AesCoT construction.
- Reward design assumptions rely on Gaussian pairwise ranking model that may not hold for real-world MOS distributions with different variances.
- Statistical significance of performance improvements is unclear without confidence intervals for PLCC/SRCC metrics on test sets of ~250 images.

## Confidence

**High confidence** - The entropy-SFT relationship is directly supported by Table 3 and entropy measurements; rank-only vs. combined reward comparison shows clear PLCC/SRCC tradeoff patterns.

**Medium confidence** - Cold-start SFT benefits are inferred from comparative results with Aes-R1-Zero, but exact mechanism for why structured reasoning patterns cannot be discovered through RL alone lacks direct ablation.

**Low confidence** - Claims about generalization to out-of-distribution scenarios are based on single-dataset transfer tests without establishing whether performance gains stem from aesthetic reasoning or dataset-specific artifacts.

## Next Checks

1. **Statistical validation** - Compute 95% confidence intervals for PLCC/SRCC improvements across 5 random train/test splits to verify significance of reported gains.

2. **Ablation on σ hyperparameters** - Systematically vary the absolute error reward σ from 0.05 to 0.2 while keeping other components fixed to test sensitivity to this critical hyperparameter.

3. **Cross-dataset calibration** - Evaluate the same model on multiple test sets (AVA, TAD66K, FLICKR-AES) to verify whether performance improvements generalize beyond the training distribution.