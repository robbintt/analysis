---
ver: rpa2
title: 'Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns
  on Theory-of-Mind of Large Language Models'
arxiv_id: '2504.04238'
source_url: https://arxiv.org/abs/2504.04238
tags:
- school
- high
- college
- parameter
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies sparse, low-rank parameter patterns in large
  language models (LLMs) that are critical for Theory-of-Mind (ToM) capabilities.
  Using Fisher information analysis, the authors pinpoint extremely sparse parameter
  sets (as few as 0.001% of total parameters) whose perturbation significantly degrades
  ToM performance while minimally affecting perplexity.
---

# Sensitivity Meets Sparsity: The Impact of Extremely Sparse Parameter Patterns on Theory-of-Mind of Large Language Models

## Quick Facts
- **arXiv ID:** 2504.04238
- **Source URL:** https://arxiv.org/abs/2504.04238
- **Reference count:** 40
- **Primary result:** Sparse parameter patterns (as few as 0.001% of total parameters) critical for ToM identified via Fisher information analysis

## Executive Summary
This paper identifies extremely sparse parameter patterns in large language models that are critical for Theory-of-Mind (ToM) capabilities. Using second-order gradient analysis (Fisher information), the authors pinpoint parameter sets that, when perturbed, significantly degrade ToM performance while minimally affecting general language modeling. The findings reveal that these sensitive parameters are closely linked to the model's positional encoding mechanism, particularly affecting dominant-frequency activations in Rotary Position Embedding (RoPE). The study demonstrates that ToM capabilities emerge through specific sparse parameter interactions with positional encoding, providing insights for enhancing AI alignment and improving human-AI interaction systems.

## Method Summary
The authors compute the diagonal of the empirical Fisher information matrix (approximating the Hessian) on ToM tasks versus general pre-training tasks to identify parameter sensitivity. They generate masks selecting parameters with high ToM-sensitivity but low general-sensitivity, creating extremely sparse parameter patterns (κ ≈ 10⁻⁵). These patterns are then perturbed by setting sensitive weights to the mean of unmasked weights. The method is validated across multiple RoPE-based models (Llama, Qwen) and contrasted with non-RoPE models (Jamba). Evaluation uses constructed ToM datasets, perplexity measures, and attention pattern analysis.

## Key Results
- Extremely sparse parameter patterns (0.001% of total parameters) can significantly degrade ToM performance while preserving perplexity
- ToM-sensitive parameters are concentrated in dimensions corresponding to dominant frequencies in RoPE
- Perturbing these parameters disrupts the geometric relationship between query and key vectors, destabilizing attention sinks
- The effect is architecture-specific, working in RoPE models but not in non-RoPE models like Jamba

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** ToM capabilities rely on an extremely sparse, low-rank subset of parameters isolated via second-order gradient analysis. **Mechanism:** The authors use the diagonal of the Fisher information matrix to estimate parameter sensitivity on ToM tasks versus general tasks, creating masks that select high-ToM-sensitivity, low-general-sensitivity weights. **Core assumption:** The diagonal approximation sufficiently captures critical weight interactions. **Evidence:** Fisher masks isolate 0.001% of parameters that degrade ToM but preserve perplexity. **Break condition:** If off-diagonal Hessian elements contain significant signal, the diagonal approximation fails.

### Mechanism 2
**Claim:** ToM-sensitive parameters regulate dominant-frequency components in RoPE, linking social reasoning to positional encoding. **Mechanism:** RoPE encodes position via rotations at different frequencies; ToM-sensitive weights affect the "slow-rotating" low-frequency components essential for stable long-context processing. **Core assumption:** ToM tasks rely heavily on contextual localization provided by specific frequency bands. **Evidence:** Perturbing sensitive weights disrupts dominant-frequency activations constructed by positional encoding. **Break condition:** If the model doesn't use RoPE or processes information without favoring specific frequency bands, this linkage breaks.

### Mechanism 3
**Claim:** Disrupting ToM-sensitive weights impairs reasoning by destabilizing "attention sinks" through altered geometric relationships. **Mechanism:** The BOS token acts as an attention sink; perturbing sensitive weights makes query and BOS key vectors more orthogonal, causing attention to scatter from the sink to irrelevant tokens. **Core assumption:** Stability of the q-k_BOS relationship is crucial for maintaining coherent attention maps necessary for complex reasoning. **Evidence:** Perturbations significantly impact the angle between query and BOS key vectors. **Break condition:** If the model handles attention normalization differently or doesn't treat BOS as special, geometric disruption may not propagate effectively.

## Foundational Learning

**Concept: Fisher Information Matrix (FIM) & Hessian Approximation**
- **Why needed here:** The core identification method uses the FIM diagonal to estimate loss sensitivity to specific weights without retraining.
- **Quick check question:** Why might ignoring off-diagonal elements of the Hessian fail for highly correlated weights?

**Concept: Rotary Position Embedding (RoPE) & Frequency**
- **Why needed here:** The paper argues ToM relies on specific frequency bands in RoPE's rotation mechanism.
- **Quick check question:** In RoPE, do lower-indexed dimensions rotate faster or slower across token positions, and which does the paper associate with ToM stability?

**Concept: Attention Sinks**
- **Why needed here:** The failure mechanism involves "shifting" where the model pays attention, particularly to the BOS token.
- **Quick check question:** If a model loses its attention sink, does attention become more diffuse or more focused on the correct tokens?

## Architecture Onboarding

**Component map:** Data Inputs (ToM Datasets & General Pre-training Data) -> Sensitivity Analyzer (computes Fisher diagonal) -> Mask Generator (subtracts general mask from ToM mask) -> Target Architecture (RoPE-based Attention Layers) -> Perturbation Module (applies mask)

**Critical path:** Generate Fisher masks -> apply to WQ/WK in RoPE layers -> observe frequency disruption -> verify attention sink shift -> measure ToM performance drop

**Design tradeoffs:**
- **RoPE vs. Non-RoPE:** Mechanism is effective in RoPE models (Llama, Qwen) but behaves differently in non-RoPE models (Jamba)
- **Sparsity (κ):** Too high degrades perplexity; too low might not impact ToM enough. The "sweet spot" is extremely sparse (≈ 10⁻⁵)

**Failure signatures:**
- **Random Perturbation:** If you randomly perturb 0.001% of weights, ToM performance remains stable (control check)
- **Pre-training Mask Only:** If you perturb weights sensitive to general language modeling, perplexity spikes but ToM remains relatively intact
- **Non-RoPE:** In models like Jamba, perturbation actually improves or doesn't degrade ToM performance

**First 3 experiments:**
1. **Verify Sensitivity Isolation:** Compute diagonal Fisher on validation set for RoPE-model. Confirm masking top 10⁻⁵ sensitive weights drops ToM accuracy but keeps Perplexity stable.
2. **Frequency Analysis:** Visualize activation norms of WQ and WK before/after perturbation. Check if dominant frequency bands (low-frequency rotations) show highest deviation.
3. **Attention Sink Check:** Run forward pass with text input. Plot attention map of early layers. Apply ToM mask and re-plot. Check if attention to first token/BOS diminishes or shifts.

## Open Questions the Paper Calls Out

**Open Question 1:** Do non-RoPE architectures (like Mamba-based models) utilize alternative internal mechanisms to represent ToM capabilities without relying on dominant-frequency activations? The authors note that Jamba lacks the frequency-dependent activation structure, suggesting a fundamental difference in how ToM is processed.

**Open Question 2:** Can interventions targeting dominant-frequency activations and attention sinks be used to constructively enhance ToM capabilities, rather than solely degrading them? The paper demonstrates that perturbing these sparse patterns degrades performance, but it remains unclear if amplifying or fine-tuning these specific sparse parameters can improve ToM reasoning beyond the baseline.

**Open Question 3:** Do the identified sparse ToM-sensitive parameter patterns generalize to higher-order ToM reasoning tasks (e.g., second-order beliefs) beyond the first-order false belief tasks tested? The study evaluates ToM using specific datasets, but the methodological setup doesn't verify if identified patterns extend to more complex, recursive social reasoning.

## Limitations
- Architecture dependency: Mechanism appears highly dependent on RoPE-based architectures and may not generalize to ALiBi, absolute positional embeddings, or state-space models
- Diagonal Fisher approximation: Relies on diagonal elements only, potentially missing important collective behaviors from correlated parameters
- Synthetic dataset concerns: DToM-Train is constructed rather than naturally occurring, raising questions about ecological validity
- Sparse masking resolution: Fixed sparsity threshold (κ ≈ 10⁻⁵) without exploration of optimal values across different model sizes and architectures

## Confidence
- **High Confidence:** Empirical observation that extremely sparse parameter perturbations can degrade ToM performance while preserving general language modeling is well-supported
- **Medium Confidence:** Specific mechanism linking ToM-sensitive parameters to RoPE frequency components and attention sinks is plausible but not definitively proven
- **Low Confidence:** Claim that these specific parameter patterns are the primary mechanism for ToM in LLMs, rather than one of many contributing factors, is speculative

## Next Checks
1. **Cross-Architecture Replication:** Apply same sensitivity analysis to ALiBi-based models (like MPT) and state-space models (like Mamba) to determine if RoPE-specific mechanism is truly unique
2. **Controlled Ablation Study:** Systematically vary perturbation method - first perturb only frequency components, then only attention sink-related parameters, then combinations - to isolate which mechanism contributes most to ToM degradation
3. **Natural Dataset Verification:** Test same sensitivity analysis on naturally occurring ToM-rich corpora (like social media conversations or narrative texts) rather than constructed DToM dataset to verify patterns hold in real-world conditions