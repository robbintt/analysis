---
ver: rpa2
title: 'MINR: Implicit Neural Representations with Masked Image Modelling'
arxiv_id: '2507.22404'
source_url: https://arxiv.org/abs/2507.22404
tags:
- masked
- image
- learning
- arxiv
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MINR, a masked implicit neural representations
  framework that integrates masked image modeling with implicit neural representations
  to improve self-supervised learning. The key innovation is using a hypernetwork
  to predict weights for implicit neural representations, enabling continuous function
  learning instead of discrete representations.
---

# MINR: Implicit Neural Representations with Masked Image Modelling

## Quick Facts
- arXiv ID: 2507.22404
- Source URL: https://arxiv.org/abs/2507.22404
- Reference count: 40
- Primary result: Up to 6.4dB PSNR improvement over MAE in in-domain settings using fewer parameters

## Executive Summary
MINR introduces a masked implicit neural representations framework that combines masked image modeling with implicit neural representations for self-supervised learning. The key innovation is using a hypernetwork to predict weights for implicit neural representations, enabling continuous function learning instead of discrete representations. This approach addresses the dependency on masking strategies that limits traditional masked autoencoders and improves performance on out-of-distribution data. MINR leverages transformer-based hypernetworks to modulate MLP weights, with GINR showing particular effectiveness by learning both instance-specific and instance-agnostic patterns.

## Method Summary
MINR integrates masked image modeling with implicit neural representations through a hypernetwork-based architecture. The framework uses a transformer-based hypernetwork (TransINR or GINR) to encode masked images and predict weights for a 5-layer MLP INR decoder that maps coordinates to RGB values. TransINR predicts all MLP weights, while GINR modulates only the second layer as instance-specific while keeping other layers instance-agnostic. The model operates on 182×182 images divided into 14×14 patches with 75% random masking, using L2 reconstruction loss over all pixels. This design enables single-forward-pass reconstruction without per-instance optimization while maintaining instance-specific adaptation.

## Key Results
- MINR achieves up to 6.4dB PSNR improvement over MAE in in-domain settings
- GINR variant shows consistent OOD improvements: 18.041 vs 17.361 (TransINR) on IMG→CEL, 19.509 vs 18.992 on IND→IMG
- MINR uses fewer parameters than MAE while maintaining superior reconstruction quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning continuous coordinate-to-pixel functions reduces sensitivity to masking strategy variations compared to discrete patch prediction.
- Mechanism: Traditional MAE predicts discrete patch tokens directly, creating dependence on which patches are visible during encoding. MINR instead learns fθ: R² → R³ mapping coordinates to RGB values, treating reconstruction as function interpolation rather than patch filling. This decouples the learning objective from specific mask patterns.
- Core assumption: Continuous function representations generalize across mask configurations better than discrete token prediction because they learn underlying image structure rather than patch-to-patch correlations.
- Evidence anchors:
  - [abstract] "MINR learns a continuous function to represent images, enabling more robust and generalizable reconstructions irrespective of masking strategies."
  - [section 3.1] "Our INR approach only leverages implicit information present in the masked images, ensuring adaptability across different masking strategies."
  - [corpus] Weak direct evidence—neighbor papers focus on masking strategies themselves (CoMA, Selective Masking) rather than INR-based alternatives.
- Break condition: If discrete patch representations inherently capture more spatial context than coordinate-based MLPs can encode, continuous functions would underperform on structured images.

### Mechanism 2
- Claim: Hypernetwork-predicted INR weights enable single-forward-pass reconstruction without per-instance optimization.
- Mechanism: Rather than optimizing MLP weights θ(n) per image (computationally prohibitive), a transformer-based hypernetwork encodes the masked image and predicts weights for the INR decoder. This amortizes the optimization across the dataset while maintaining instance-specific adaptation.
- Core assumption: The hypernetwork can learn to predict sufficiently good INR weights from partial (masked) observations to enable accurate reconstruction.
- Evidence anchors:
  - [section 3.2] "TransINR directly maps weight sets, whereas GINR modulates only the second MLP layer as instance-specific, keeping the rest instance-agnostic."
  - [section 3.2] "we utilizes transformer-based hypernetwork architectures to modulate the MLP weights efficiently"
  - [corpus] No direct neighbors use hypernetworks for INR prediction in MIM context—this is a distinct architectural choice.
- Break condition: If masked observations lack sufficient information to predict accurate weights, reconstruction quality degrades substantially compared to per-instance optimization.

### Mechanism 3
- Claim: Partitioning MLP layers into instance-specific and instance-agnostic components improves OOD generalization.
- Mechanism: GINR keeps layers 1, 3, 4, 5 shared across all instances (instance-agnostic ϕ) while only layer 2 weights are predicted per-instance (θ(n)). This forces the network to learn common patterns (edges, textures) in shared layers while adapting instance-specific details in one layer.
- Core assumption: Images within and across domains share low-level structure that can be captured in instance-agnostic layers, with high-level variation concentrated in fewer parameters.
- Evidence anchors:
  - [section 3.2] "partitioning the MLP hidden layers into instance-specific and instance-agnostic layers is effective in learning commonalities across instances"
  - [section 3.2] "This design enables the MLP to learn patterns both common across a dataset and unique per instance, making it ideal for OOD settings."
  - [table 2] GINR shows consistent OOD improvements: 18.041 vs 17.361 (TransINR) on IMG→CEL, 19.509 vs 18.992 on IND→IMG.
  - [corpus] No comparable instance-specific/agnostic decomposition in neighbor papers.
- Break condition: If domain shift primarily affects low-level features rather than high-level semantics, shared layers would transfer poorly.

## Foundational Learning

- Concept: Implicit Neural Representations (INRs)
  - Why needed here: Core representation—MLP that maps coordinates to values, treating images as continuous functions rather than discrete pixel arrays.
  - Quick check question: Can you explain why an MLP with coordinate inputs (x,y) → RGB outputs can represent any resolution image?

- Concept: Masked Image Modeling / Masked Autoencoders
  - Why needed here: Pretext task framework—understanding what MAE does and its masking dependency helps contextualize MINR's improvements.
  - Quick check question: Why does MAE's loss computation only on masked patches create vulnerability to unseen masking strategies at test time?

- Concept: Hypernetworks
  - Why needed here: Architectural backbone—networks that generate weights for another network require understanding weight prediction vs. direct prediction.
  - Quick check question: What is the trade-off between predicting all MLP weights (TransINR) vs. only instance-specific layers (GINR)?

## Architecture Onboarding

- Component map:
  Input pipeline -> Hypernetwork encoder -> Weight predictor -> INR decoder -> RGB output -> L2 loss

- Critical path:
  1. Masked image → hypernetwork encoding
  2. Encoding → weight prediction (full or partial)
  3. Coordinate grid → INR forward pass with predicted weights
  4. RGB output → L2 loss against ground truth

- Design tradeoffs:
  - TransINR vs. GINR: TransINR predicts all weights (~more capacity per instance), GINR shares most layers (~better generalization, fewer instance-specific params)
  - Masking ratio: 75% per paper; lower ratios may reduce learned robustness
  - MLP depth: 5 layers per paper; deeper may overfit, shallower may underfit complex images

- Failure signatures:
  - OOD performance collapse: If shared layers (GINR) don't generalize, OOD results approach or drop below TransINR
  - Blurry reconstructions: If hypernetwork cannot predict sharp weights, outputs lose high-frequency detail
  - Training instability: Weight prediction can be unstable—monitor gradient norms through hypernetwork→INR path

- First 3 experiments:
  1. Replicate ID reconstruction on CelebA: Compare TransINR vs. GINR PSNR against paper's 21.865 and 21.680 values; validates implementation.
  2. Masking strategy robustness test: Train with 75% masking, evaluate at 50%, 60%, 70%, 80%, 90% masking; should show smaller PSNR degradation than MAE baseline per paper's claim of masking independence.
  3. OOD transfer between datasets: Train on Imagenette, evaluate on MIT Indoor67; compare against table 2's 18.045 (GINR) and 17.920 (TransINR) to verify cross-domain generalization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the high reconstruction quality (PSNR) of MINR translate to superior performance on standard downstream tasks like classification and segmentation compared to MAE?
- Basis in paper: [explicit] The conclusion states, "In the future, we plan to leverage the flexibility of MINR... to showcase its performance and applicability across various downstream tasks."
- Why unresolved: The current experiments only evaluate the quality of the mask reconstruction itself, not the utility of the learned representations for other vision tasks.
- What evidence would resolve it: Linear probing or fine-tuning results on standard benchmarks (e.g., ImageNet classification, ADE20K segmentation) using features extracted from the MINR hypernetwork.

### Open Question 2
- Question: Does MINR maintain its parameter efficiency and reconstruction superiority when scaled to massive datasets like ImageNet-1K?
- Basis in paper: [inferred] Experiments are conducted on smaller datasets (Imagenette, CelebA) or specific domains (MIT Indoor67), leaving scalability to large-scale general datasets unverified.
- Why unresolved: It is unclear if the instance-specific modulation mechanism in GINR scales efficiently with the increased data diversity and volume of ImageNet-1K without performance saturation.
- What evidence would resolve it: A comparison of MINR against MAE on the full ImageNet-1K dataset, analyzing both PSNR and computational overhead.

### Open Question 3
- Question: Is the constraint of modulating only the second MLP layer in GINR optimal for all data complexities, or does it limit representational capacity?
- Basis in paper: [inferred] The authors note that using the second layer as instance-specific was "empirically found" to be effective, implying the architectural choice was heuristic rather than theoretically exhaustive.
- Why unresolved: There may exist configurations (e.g., modulating multiple layers or different layers) that yield better results for more complex or textured images.
- What evidence would resolve it: An ablation study comparing GINR performance while varying the depth and number of modulated MLP layers across datasets of varying complexity.

## Limitations

- The paper's reliance on hypernetwork weight prediction introduces technical risks including potential mode collapse or poor generalization, especially for TransINR where all weights must be predicted.
- Architectural choices (MLP depth, hypernetwork size) appear tuned for specific datasets and may not transfer to larger-scale pretraining tasks.
- OOD evaluation tests only three dataset combinations, which may not represent broader domain shift scenarios.

## Confidence

- Mechanism 1 (continuous functions generalize better): **Medium** - The theoretical motivation is sound, but direct experimental comparison against MAE with varying masking strategies is limited to specific percentages rather than systematic exploration.
- Mechanism 2 (hypernetwork efficiency): **High** - This is the core architectural innovation with clear computational advantages over per-instance optimization, and the implementation appears straightforward.
- Mechanism 3 (instance-specific vs. agnostic partitioning): **Medium** - The ablation showing GINR outperforming TransINR in OOD settings is compelling, but the underlying reason could be confounded by other architectural differences.

## Next Checks

1. Test masking strategy robustness by training with 75% masking and evaluating at 50%, 60%, 70%, 80%, and 90% masking ratios to verify the claimed independence from masking strategy.
2. Compare continuous function reconstruction quality against discrete patch prediction baselines across varying image complexities (simple vs. highly textured images) to validate Mechanism 1.
3. Perform ablation studies removing the instance-agnostic layer sharing in GINR to quantify the exact contribution of this architectural choice to OOD performance.