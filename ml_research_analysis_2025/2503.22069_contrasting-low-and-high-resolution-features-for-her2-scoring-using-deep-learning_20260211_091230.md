---
ver: rpa2
title: Contrasting Low and High-Resolution Features for HER2 Scoring using Deep Learning
arxiv_id: '2503.22069'
source_url: https://arxiv.org/abs/2503.22069
tags:
- her2
- classification
- performance
- cancer
- breast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the India Pathology Breast Cancer Dataset
  (IPD-Breast) for automated receptor status classification, focusing on HER2 scoring.
  It evaluates multiple deep learning approaches, including Multiple Instance Learning
  (MIL), end-to-end CNN-based classifiers, and patch-level pipelines, using low-resolution
  whole slide images.
---

# Contrasting Low and High-Resolution Features for HER2 Scoring using Deep Learning

## Quick Facts
- **arXiv ID:** 2503.22069
- **Source URL:** https://arxiv.org/abs/2503.22069
- **Reference count:** 34
- **Primary result:** ConvNeXt-S model achieved 91.79% AUC, 83.52% F1, 83.56% accuracy for 3-way HER2 classification on IPD-Breast dataset

## Executive Summary
This study introduces the India Pathology Breast Cancer Dataset (IPD-Breast) for automated HER2 receptor status classification and evaluates multiple deep learning approaches. The research compares low-resolution whole-slide image processing with high-resolution patch-based methods, finding that end-to-end ConvNeXt networks outperform patch-based pipelines by over 5.35% in F1 score. The ConvNeXt-S model achieved the highest performance metrics for 3-way HER2 classification (0, Low, High), demonstrating that macro-scale processing preserves sufficient diagnostic signal for receptor status determination.

## Method Summary
The study evaluated three deep learning approaches on the IPD-Breast dataset: (1) Multiple Instance Learning using attention mechanisms to highlight diagnostically relevant regions, (2) end-to-end ConvNeXt networks processing low-resolution whole slide images, and (3) patch-based pipelines with annotation-guided labeling. Models were trained using 10-fold stratified cross-validation on 500 patient slides with 3-way HER2 classification. The ConvNeXt-S architecture processed downsampled images (224×224 or 512×512) directly, while patch-based methods extracted 224×224 patches at multiple resolutions with aggregation through random forests, SVMs, or MLPs.

## Key Results
- ConvNeXt-S achieved 91.79% AUC, 83.52% F1, and 83.56% accuracy for 3-way HER2 classification
- End-to-end low-resolution processing outperformed high-resolution patch-based methods by over 5.35% in F1 score
- MIL approaches provided interpretability through attention mechanisms but slightly lower performance than ConvNeXt
- Increasing resolution from 224×224 to 512×512 did not significantly improve performance metrics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Low-resolution whole-slide images processed end-to-end can outperform high-resolution patch-based pipelines for HER2 classification.
- **Mechanism:** Holistic slide-level processing preserves global spatial context and tissue architecture relationships that may be lost when fragmenting slides into patches. ConvNeXt captures staining intensity distributions and cell membrane patterns across the full tissue region, enabling the model to learn that HER2 expression is a population-level property rather than a patch-level one. Aggregation of patch predictions during inference introduces noise and fails to model inter-patch dependencies.
- **Core assumption:** The diagnostic signal for HER2 scoring (IHC staining intensity and completeness of membrane staining) is detectable at macro scales and does not require sub-cellular resolution for the 3-way (0, Low, High) distinction.
- **Evidence anchors:** End-to-end ConvNeXt network utilizing low-resolution IHC images achieved an AUC, F1, and accuracy of 91.79%, 83.52%, and 83.56%, respectively, for 3-way classification, outperforming patch-based methods by over 5.35% in F1 score. A key observation from the experiments is that increasing resolution of IHC slides for training did not result in significant improvements in either patch-level or slide-level performance. Instead, aggregation of patch-level predictions during inference appears to be more impactful, as effective aggregation can better capture the spatial context and heterogeneity within WSIs.

### Mechanism 2
- **Claim:** ConvNeXt's architectural design elements provide better feature extraction for histopathological staining patterns compared to ResNet, DenseNet, and ViT baselines.
- **Mechanism:** Depthwise separable convolutions with 7×7 kernels capture broader spatial context relevant to membrane staining patterns. Group Normalization stabilizes training with small batch sizes common in medical imaging. Residual connections with inverted bottlenecks propagate staining intensity gradients more effectively than standard CNN blocks. The hierarchical structure processes multi-scale information without requiring explicit patch extraction.
- **Core assumption:** HER2 scoring depends more on recognizing patterns of brown IHC staining intensity and distribution across tissue regions than on detecting fine morphological structures.
- **Evidence anchors:** ConvNext-S emerges as the top performer among them, achieving best metrics across binary 3-way, and 4-way classification tasks. Its higher AUC, F1, and accuracy scores compared to ResNet-50 and DenseNet-201 highlight its enhanced capability in feature extraction and classification. ConvNeXt's advanced convolutional structure, including depthwise separable convolutions and residual connections, allowed it to effectively model intricate spatial patterns and staining variations in histopathological images.

### Mechanism 3
- **Claim:** Multiple Instance Learning provides interpretability through attention mechanisms that highlight diagnostically relevant regions, at the cost of slightly lower classification performance.
- **Mechanism:** MIL treats each slide as a "bag" of patch instances, learning to weight patches by diagnostic relevance without requiring patch-level labels. Attention scores identify which tissue regions most influenced the classification, enabling pathologist review of model reasoning. However, the feature extractor (UNI) was pretrained on H&E stains, creating a domain gap for IHC interpretation. The bag-level aggregation may dilute signal from minority cell populations expressing different HER2 levels.
- **Core assumption:** Diagnostically relevant patches can be identified purely from slide-level labels without manual annotation of membrane staining regions.
- **Evidence anchors:** MIL's ability to operate at the slide level helps mitigate issues associated with patch-level approaches, which face challenges such as high computational costs and annotation variability. Approach-1 uses attention mechanisms to emphasize critical regions within WSIs, ensuring that the model focuses on clinically significant areas. UNI captures high-level visual features but faces limitations in IHC applications due to its H&E training background and the specific requirements for HER2 scoring.

## Foundational Learning

- **Concept:** Multiple Instance Learning (MIL) for weakly-supervised classification
  - **Why needed here:** The paper uses MIL to train on slide-level labels without patch-level annotations, which is critical given the labor cost of pixel-level annotation. Understanding bag-instance relationships and attention-based aggregation is required to interpret Approach-1 results and understand when MIL underperforms.
  - **Quick check question:** Given a slide labeled "HER2 High" containing 100 patches, if only 15 patches show strong membrane staining, how should an ideal MIL attention mechanism weight those patches?

- **Concept:** Immunohistochemistry (IHC) scoring principles and ASCO/CAP guidelines
  - **Why needed here:** The 3-way classification scheme (0, Low, High) maps to clinical treatment decisions. Without understanding that HER2 scoring depends on membrane staining intensity and completeness across ≥10% of tumor cells, one cannot evaluate whether model errors are clinically significant or assess the paper's claim about inter-observer variability.
  - **Quick check question:** Why does the paper report that distinguishing HER2 0 from HER2 1+ is particularly challenging, and what clinical implication does this have for the 3-way vs. 4-way classification choice?

- **Concept:** Domain shift in transfer learning for computational pathology
  - **Why needed here:** The paper notes UNI's H&E pretraining limits IHC performance. Understanding feature extractor pretraining domains is essential for selecting or designing models for new staining protocols. The ConvNeXt results suggest end-to-end training may partially overcome domain mismatch.
  - **Quick check question:** If deploying this model to a hospital using a different scanner (e.g., Leica Aperio instead of Hamamatsu NanoZoomer), what validation steps are required before clinical use?

## Architecture Onboarding

- **Component map:**
  WSI Input (40x, 0.23 μm/pixel)
       │
       ├─── [Approach 1: MIL] ───┐
       │     UNI Feature Extractor (ViT, H&E-pretrained)
       │     CLAM-sb / CLAM-mb / DTFD aggregators
       │     Slide-level classifier
       │                         │
       ├─── [Approach 2: End-to-End] ←── Best performer (ConvNeXt-S)
       │     Downsample to 224×224 or 512×512
       │     ConvNeXt-S backbone
       │     Classification head (3-way: 0, Low, High)
       │                         │
       └─── [Approach 3: Patch-Based]
             Patch extraction (224×224 at multiple resolutions)
             Annotation-guided patch labeling (5% overlap threshold)
             ConvNeXt patch classifier
             Aggregation via RF/SVM/MLP using patch score counts

- **Critical path:** Data preprocessing (foreground segmentation via Otsu + watershed) → Resolution selection (low-res 224×224 for ConvNeXt-S) → Model training (10-fold stratified cross-validation, patient-wise splits) → Evaluation on held-out test set. The paper's key result depends on the low-resolution downsampling preserving diagnostic signal.

- **Design tradeoffs:**
  - **Interpretability vs. Performance:** Approach 1 (MIL) provides attention-based region highlighting; Approach 2 (ConvNeXt) achieves +5.35% F1 but offers no intrinsic explainability. Approach 3 provides patch-level granularity but introduces aggregation complexity.
  - **Resolution vs. Compute:** High-resolution patches (23,218×20,529) did not improve performance over low-resolution (1,648×1,471) but increased patch counts from 2,155 to 198,807. Paper recommends moderate resolution as compute-performance balance.
  - **Annotation burden vs. Label quality:** Slide-level labels (Approach 2) require less annotation labor but inherit inter-observer variability (72.4% agreement per cited study). Patch-level labels (Approach 3) require pixel-level annotation but may reduce label noise within homogeneous regions.

- **Failure signatures:**
  - Confusion between HER2 0 and HER2 1+ classes across all approaches (Figure 4b shows performance drop for these classes). Paper attributes this to subtle staining pattern differences and inter-observer variability in ground truth labels.
  - Majority voting in 4-way patch-level classification collapses to near-random performance (F1: 13.88%, Table 6), indicating patch count distributions alone are insufficient for fine-grained classification without learned aggregation.
  - Discrepancy between image features (brown staining intensity/percentage) and assigned labels (Figure 2 misclassified samples), suggesting ground truth noise limits achievable performance.

- **First 3 experiments:**
  1. **Reproduce ConvNeXt-S 3-way classification on IPD-Breast split:** Train ConvNeXt-S on 224×224 low-resolution WSIs with the reported hyperparameters (Adam optimizer, cross-entropy loss, class weights, data augmentation including flips/rotations/affine). Verify that test set AUC approaches 91.79% ± 2.14% on the 10-fold split. Log per-class F1 scores to confirm HER2 0/1+ confusion pattern.
  2. **Ablate resolution sensitivity:** Train ConvNeXt-S on 512×512 inputs and compare against 224×224 results. If performance degrades or improves significantly, investigate whether the paper's low-resolution advantage is architecture-specific or dataset-specific. Extract attention maps from a comparable MIL model (CLAM-sb) at both resolutions to visualize region focus differences.
  3. **Validate ground truth concordance:** Sample 50 misclassified test slides (focusing on HER2 0/1+ confusion cases). Have an independent pathologist re-score these slides blinded to model predictions. Calculate agreement rate between original labels, pathologist re-scores, and model predictions. This addresses the paper's acknowledged limitation that inter-observer variability in ground truth may cap achievable performance.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the model's classification accuracy be improved by establishing ground truth labels based on high-confidence consensus or molecular assays to mitigate the impact of inter-observer variability?
- **Basis in paper:** The authors state, "There is a need to evaluate the concordance of our model with high-confidence ground truth, as we cannot exclude the potential impact of inter-observer variability in our ground truth," noting that confusion between HER2 0 and 1+ arises from label discrepancies.
- **Why unresolved:** The current study relied on standard pathologist annotations which are subject to significant inter-observer variability (mean variability of 72.4%), and techniques like focal loss failed to resolve the resulting label noise.
- **What evidence would resolve it:** A re-evaluation of the misclassified samples (specifically HER2 0 vs. 1+) using a consensus panel of experts or In Situ Hybridization (ISH) data to establish a high-confidence dataset.

### Open Question 2
- **Question:** To what extent does the low-resolution ConvNeXt model generalize to data generated by different scanner hardware and staining protocols?
- **Basis in paper:** The discussion notes that "variations in staining procedures and images generated by different scanners can impact performance" and identifies addressing these limitations as a focus for future efforts.
- **Why unresolved:** The IPD-Breast dataset was retrospectively compiled from a single center (RGCIRC) using a specific scanner (Hamamatsu NanoZoomer), limiting the known generalizability of the model to external domains.
- **What evidence would resolve it:** Performance metrics (AUC, F1) resulting from the deployment of the model on external datasets from different geographical regions and digitization equipment.

### Open Question 3
- **Question:** Does the superior performance of end-to-end low-resolution models over high-resolution patch-based methods hold when explicitly controlling for the "multiple instance" nature of the data?
- **Basis in paper:** The paper concludes that increasing resolution did not improve performance and suggests that "aggregation of patch-level predictions during inference appears to be more impactful."
- **Why unresolved:** The study compared distinct architectures (ConvNeXt vs. CLAM/DTFD) at different resolutions, leaving it unclear if the performance gap is due to the resolution, the model architecture, or the aggregation logic.
- **What evidence would resolve it:** An ablation study applying the same ConvNeXt architecture to both low-resolution whole images and high-resolution patch aggregations to isolate the variable of resolution.

## Limitations
- Ground truth labels may be noisy due to inter-observer variability in HER2 scoring (72.4% agreement reported in literature)
- Limited generalizability to other staining protocols, scanner types, or institutions beyond Hamamatsu NanoZoomer S210
- 3-way classification may not capture clinical distinctions needed for treatment decisions between HER2 0 vs 1+

## Confidence
- **High Confidence:** ConvNeXt-S outperforms patch-based methods for 3-way HER2 classification on IPD-Breast dataset (AUC 91.79%, F1 83.52%)
- **Medium Confidence:** Low-resolution processing preserves sufficient diagnostic signal for HER2 scoring (mechanism assumes staining patterns are macroscopically detectable)
- **Low Confidence:** MIL attention mechanisms reliably identify diagnostically relevant regions without requiring patch-level annotations (domain gap from H&E pretraining may limit effectiveness)

## Next Checks
1. **Ground Truth Validation:** Re-score 50 misclassified test slides with independent pathologist blinded to model predictions to quantify label noise impact
2. **Cross-Institutional Validation:** Test model performance on HER2 IHC slides from different scanners (e.g., Leica Aperio) and staining protocols to assess generalizability
3. **Fine-Grained Classification Test:** Evaluate 4-way classification performance (0, 1+, 2+, 3+) to determine if 3-way results mask clinically important distinctions between HER2 0 and 1+