---
ver: rpa2
title: 'Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure
  Detection'
arxiv_id: '2507.22930'
source_url: https://arxiv.org/abs/2507.22930
tags:
- synthetic
- data
- posts
- dataset
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of protecting privacy in research
  involving personally identifiable information (PII) in social media posts by developing
  a method to generate synthetic datasets that preserve utility while enhancing privacy.
  The authors create a taxonomy of 19 PII categories relevant to vulnerable populations
  and generate synthetic posts using three large language models (LLaMA2-7B, LLaMA3-8B,
  and Zephyr-7B) via sequential instruction prompting.
---

# Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection

## Quick Facts
- **arXiv ID**: 2507.22930
- **Source URL**: https://arxiv.org/abs/2507.22930
- **Reference count**: 37
- **Primary result**: Method generates synthetic PII-labeled Reddit posts preserving privacy while maintaining classification utility (F1 ≈ 0.85)

## Executive Summary
This paper addresses the challenge of protecting privacy in research involving personally identifiable information (PII) in social media posts by developing a method to generate synthetic datasets that preserve utility while enhancing privacy. The authors create a taxonomy of 19 PII categories relevant to vulnerable populations and generate synthetic posts using three large language models (LLaMA2-7B, LLaMA3-8B, and Zephyr-7B) via sequential instruction prompting. They evaluate their synthetic dataset across three metrics: reproducibility equivalence (synthetic data achieves comparable model performance to real data), indistinguishability (humans cannot reliably tell synthetic from original posts), and unlinkability (low text similarity metrics prevent reverse-engineering to original posts).

## Method Summary
The methodology involves collecting Reddit posts, manually annotating PII spans across 19 categories using Doccano, then generating synthetic posts via sequential instruction prompting with three LLMs. The generation process uses two-step prompts: first transforming content by replacing sensitive attributes, then adjusting subreddit context. Synthetic posts are filtered by Meteor score (<0.5) to ensure unlinkability. The resulting dataset is evaluated for utility by training RoBERTa classifiers on both original and synthetic data, and for privacy through human indistinguishability tests and search engine unlinkability checks.

## Key Results
- Synthetic data achieves comparable model performance to real data with F1 scores around 0.85 for classification
- Humans cannot reliably distinguish synthetic from original posts (p-value=0.51 in Turing test)
- Low text similarity metrics (BLEU, ROUGE, Meteor) prevent reverse-engineering to original posts
- Dataset successfully released for non-commercial use while preserving privacy

## Why This Works (Mechanism)

### Mechanism 1
Sequential instruction prompting enables LLMs to transform PII-bearing posts while preserving stylistic and contextual features. The method splits generation into two sequential prompts: first transforming content (replacing sensitive attributes with contextually equivalent alternatives), then adjusting subreddit context. This decomposition addresses the observed limitation where LLMs struggle with complex multi-constraint instructions in a single query.

### Mechanism 2
Low text similarity scores between synthetic and original posts prevent reverse-engineering via search engines. By selecting generation temperatures that minimize cosine similarity and filtering outputs with Meteor scores >0.5, the method ensures synthetic posts do not match indexed web content.

### Mechanism 3
Training classifiers on synthetic data achieves comparable performance to original data because LLMs preserve label-relevant patterns. The generation process maintains PII category proportions and linguistic cues necessary for classification, demonstrated by F1 scores ~0.85 across both conditions.

## Foundational Learning

- **Multi-label token classification (span categorization)**: The task requires identifying which of 19 PII categories apply to each text and localizing the specific spans, not just document-level labels. Quick check: Can you explain how BCEWithLogitsLoss applied at token level differs from standard multi-label document classification?

- **Instruction tuning and prompt engineering for LLMs**: Understanding why sequential prompts outperform single prompts, and how temperature/nucleus sampling affect diversity vs. coherence tradeoffs. Quick check: Why would increasing temperature reduce linkability but potentially hurt classification utility?

- **Text similarity metrics (BLEU, ROUGE, Meteor, cosine similarity)**: Each metric captures different aspects of similarity; the paper uses all four as unlinkability proxies, and understanding their limitations is critical. Quick check: Why might low BLEU score not guarantee semantic unlinkability?

## Architecture Onboarding

- **Component map**: Reddit crawl (PushShift API) → First-person filtering (regex) → Manual annotation (Doccano, 19 categories) → 1,183 PII-positive posts → Sequential prompts → LLM generation → Post-processing → Annotation → Filtering (Meteor <0.5) → RoBERTa classifier training → Human study → Google Search evaluation

- **Critical path**: Annotation quality determines downstream utility—garbage in, garbage out. Temperature selection per LLM directly controls privacy-utility tradeoff. Meteor threshold (0.5) is the final gate before release.

- **Design tradeoffs**: Llama2 is most faithful to original PII distribution but requires multiple generation rounds due to content refusals. Llama3 is more creative/diverse but introduces category bias. Zephyr has best divergence and fewer refusals but moderate category drift. Higher temperature → better unlinkability but potential label noise.

- **Failure signatures**: Llama refusals for sensitive content yield fewer synthetic posts. Empty PII outputs occur in some generated posts. Human detection may occur if participants consistently identify synthetic posts (>55% accuracy with statistical significance).

- **First 3 experiments**:
  1. Baseline replication: Train RoBERTa classifier on original data, establish F1 benchmark. Train identical architecture on each synthetic variant. Compare F1 delta per category.
  2. Temperature sensitivity analysis: Generate synthetic versions at temperatures [0.5, 0.7, 0.9, 1.0, 1.2]. Plot avg cosine similarity, classifier F1, and Meteor score distribution.
  3. Adversarial linkage test: Test semantic retrieval using sentence-transformers. If synthetic posts retrieve originals with >10% success@10, Meteor threshold may be insufficient.

## Open Questions the Paper Calls Out

### Open Question 1
Is the proposed synthetic generation methodology generalizable to other social media platforms and distinct vulnerable populations beyond the r/lgbt community? The current study is limited to a specific subreddit (r/lgbt) and platform (Reddit), leaving validation on other platforms undone.

### Open Question 2
How can the synthetic generation process be systematically adjusted to identify and mitigate inherent biases in the original seed datasets? The authors note that future work will examine how to overcome biases which may exist in the original datasets.

### Open Question 3
What generation strategies are required to improve the representation of under-represented PII categories, such as Degree/Designation and Physical Appearance? The authors plan to extend the dataset to better represent categories with lower occurrence.

### Open Question 4
Does the synthetic dataset maintain unlinkability under more sophisticated adversarial attacks than simple search engine lookup? The unlinkability metric relies exclusively on Google Search results and text similarity, which may not protect against advanced embedding-inversion attacks.

## Limitations
- Unlinkability claims rely on surface-form similarity metrics without testing semantic search engine vulnerability
- Meteor threshold of 0.5 is heuristic without theoretical justification for privacy-utility tradeoff
- Temperature effects on utility across different PII categories are not systematically analyzed
- No evaluation against state-of-the-art privacy attacks beyond search engine lookup

## Confidence

- **High confidence**: Reproducibility equivalence (F1 ≈ 0.85) - Directly supported by empirical classifier evaluation with clear metrics
- **Medium confidence**: Indistinguishability by humans (p=0.51) - Statistically sound but small sample size without stratification analysis
- **Medium confidence**: Unlinkability via text similarity - Appropriate metrics used but assumption about lexical vs semantic search untested

## Next Checks

1. **Semantic retrieval vulnerability test**: Using sentence-transformers to embed both synthetic and original posts, compute top-10 retrieval accuracy. If synthetic posts retrieve their originals with >10% success@10, the current unlinkability threshold is insufficient.

2. **Temperature-utility sensitivity analysis**: For a fixed 50-post sample, generate synthetic versions at temperatures [0.5, 0.7, 0.9, 1.0, 1.2]. Plot classifier F1 vs average cosine similarity vs Meteor score to identify optimal temperature per PII category.

3. **Category-wise performance audit**: Compute per-category precision/recall/F1 for both classifier and span models. Identify which of the 19 PII categories show degraded transfer from original to synthetic data, and correlate with generation temperature and LLM choice.