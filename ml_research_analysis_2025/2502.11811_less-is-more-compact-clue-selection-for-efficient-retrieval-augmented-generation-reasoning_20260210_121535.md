---
ver: rpa2
title: 'Less is More: Compact Clue Selection for Efficient Retrieval-Augmented Generation
  Reasoning'
arxiv_id: '2502.11811'
source_url: https://arxiv.org/abs/2502.11811
tags:
- sentences
- clue
- documents
- generation
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient retrieval-augmented
  generation (RAG) by proposing a compact clue selection mechanism that improves both
  performance and latency. The core idea is to frame LLM-centric retrieval as MinMax
  optimization, extracting sufficient potential clues, reranking them for effectiveness,
  and truncating to the minimal necessary context.
---

# Less is More: Compact Clue Selection for Efficient Retrieval-Augmented Generation Reasoning

## Quick Facts
- **arXiv ID:** 2502.11811
- **Source URL:** https://arxiv.org/abs/2502.11811
- **Reference count:** 40
- **One-line primary result:** CompSelect improves RAG accuracy while reducing context length and latency through compact clue selection

## Executive Summary
This paper addresses the challenge of efficient retrieval-augmented generation (RAG) by proposing a compact clue selection mechanism that improves both performance and latency. The core idea is to frame LLM-centric retrieval as MinMax optimization, extracting sufficient potential clues, reranking them for effectiveness, and truncating to the minimal necessary context. The method, CompSelect, consists of a clue extractor, a reranker, and a truncator, each trained using real LLM feedback. Experiments on three QA datasets demonstrate that CompSelect consistently outperforms baselines in accuracy (SubEM/F1) while reducing context length and system latency. It also shows robustness to unreliable retrieval and strong generalization across tasks. The approach effectively balances informativeness and conciseness, making it suitable for scalable, cost-efficient web-scale RAG applications.

## Method Summary
CompSelect proposes a three-stage pipeline to optimize RAG efficiency: (1) Clue Extraction uses a fine-tuned LLaMA3.2-3B model to generate answer-containing sentences and semantically similar neighbors from top-5 retrieved documents; (2) Clue Reranking employs a Sentence-BERT model trained on pairwise loss to rank extracted clues by their likelihood of leading to correct answers; (3) Adaptive Truncator uses another fine-tuned LLaMA3.2-3B to predict the minimal subset of clues sufficient for answering, trained on iteratively verified subsets. The method is trained end-to-end using real LLM feedback, optimizing for both accuracy (SubEM/F1) and compression ratio while minimizing system latency. Training involves cross-entropy loss for the extractor, pairwise loss for the reranker, and iterative subset verification for the truncator.

## Key Results
- CompSelect consistently outperforms baselines in accuracy (SubEM/F1) across three QA datasets
- Reduces context length by selecting minimal necessary clues while maintaining or improving answer quality
- Demonstrates robust performance even with unreliable or noisy retrieval results
- Shows strong generalization across single-hop (NQ, TriviaQA) and multi-hop (HotpotQA) reasoning tasks

## Why This Works (Mechanism)
The method works by framing RAG as a MinMax optimization problem where the system must balance comprehensiveness (extracting all potentially useful information) with conciseness (truncating to only what's necessary). This approach leverages the fact that LLMs can perform well with focused, relevant context rather than entire documents. By training each component with real LLM feedback, the system learns to extract semantically rich clues, rank them by actual utility, and truncate to the minimal set that enables correct answering. The iterative training process ensures that each stage optimizes for downstream performance rather than proxy metrics.

## Foundational Learning
- **MinMax Optimization:** Balances competing objectives of informativeness vs. conciseness; needed because traditional RAG treats retrieval as a single optimization without considering downstream efficiency
- **Pairwise Ranking Loss:** Trains models to distinguish between helpful and unhelpful clues; needed to learn relative utility rather than absolute scores
- **Iterative Subset Verification:** Systematically tests smaller context subsets to find minimal sufficient information; needed to train the truncator on realistic, task-specific sufficiency criteria
- **KNN-based Semantic Expansion:** Finds sentences similar to answer-containing ones to enrich context; needed to capture relevant information that may not contain exact answer strings
- **Cross-Entropy Fine-tuning:** Adapts LLMs to generate specific output formats (clues); needed to transform general-purpose LLMs into specialized clue extractors
- **Compression Ratio Metric:** Quantifies efficiency gains; needed to measure the trade-off between performance and brevity

## Architecture Onboarding

**Component Map:** Query → Extractor → Reranker → Truncator → Generator

**Critical Path:** The entire pipeline is critical since each stage builds on the previous one. The extractor must generate quality clues, the reranker must correctly prioritize them, and the truncator must preserve only what's necessary.

**Design Tradeoffs:** The system trades preprocessing complexity (three specialized models) for runtime efficiency and better accuracy. This is justified for applications where latency and cost matter more than simple implementation.

**Failure Signatures:** 
- Low SubEM/F1 despite long contexts indicates extractor/reranker failures
- High compression ratio but poor accuracy indicates over-aggressive truncation
- High accuracy but poor compression indicates the truncator isn't learning effectively

**3 First Experiments:**
1. Validate the extractor by checking if generated clues contain answer spans from source documents
2. Test the reranker by verifying that high-ranked clues lead to correct answers more often than low-ranked ones
3. Evaluate the truncator by measuring if minimal subsets it selects actually suffice for correct answering

## Open Questions the Paper Calls Out
None

## Limitations
- Training data generation requires expensive iterative querying of a generator to identify minimal sufficient subsets
- The approach assumes access to a capable generator for training, which may not be available in all deployment scenarios
- Latency improvements may be overstated if preprocessing overhead isn't properly accounted for in end-to-end measurements

## Confidence

**Confidence Mapping:**
- Performance claims: **Medium** - depends on quality of generator-based training data
- Latency improvements: **Medium** - may not account for preprocessing overhead
- Robustness claims: **Low** - limited systematic evaluation of retrieval quality variation

## Next Checks

1. **Reproduce the training data generation pipeline:** Implement the iterative generator-based subset selection to create truncator targets, documenting exactly how "minimal sufficient" subsets are defined and verified. This is critical since the truncator's performance depends entirely on the quality of these targets.

2. **Measure end-to-end system latency:** Compare total system latency (including all preprocessing steps) against baselines, not just generation latency. Calculate the break-even point where CompSelect's preprocessing overhead is justified by downstream efficiency gains.

3. **Test retrieval robustness:** Systematically evaluate CompSelect performance with varying retrieval quality (top-5 vs top-3 vs noisy retrievals) to validate claims about robustness to unreliable retrieval. Include scenarios where no relevant documents are retrieved to assess fallback behavior.