---
ver: rpa2
title: Mixture of Neuron Experts
arxiv_id: '2510.05781'
source_url: https://arxiv.org/abs/2510.05781
tags:
- activation
- layer
- mone
- value
- experts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores parameter sparsity in Mixture-of-Experts (MoE)
  models, finding that the subset of parameters activated by MoE gating is highly
  sparse, with performance degrading only after pruning more than 90% of activated
  parameters. Based on this observation, the authors propose Mixture of Neuron Experts
  (MoNE), which achieves neuron-granular expert selection by applying a simple top-k
  operation within each expert, requiring no additional routing parameters or inter-expert
  communication.
---

# Mixture of Neuron Experts

## Quick Facts
- **arXiv ID:** 2510.05781
- **Source URL:** https://arxiv.org/abs/2510.05781
- **Reference count:** 40
- **Primary result:** MoNE achieves neuron-granular expert selection, matching traditional MoE performance while activating only 50% of parameters.

## Executive Summary
This paper introduces Mixture of Neuron Experts (MoNE), a novel approach to improving parameter efficiency in Mixture-of-Experts (MoE) models. The key insight is that traditional MoEs are "dense" at the neuron level within each expert, activating all neurons even when only a subset is needed for a given input. MoNE addresses this by applying top-k selection at the neuron level within each selected expert, effectively decomposing each expert into smaller "neuron experts" and selecting only the most relevant ones. This approach requires no additional routing parameters or inter-expert communication while achieving significant efficiency gains.

## Method Summary
MoNE modifies the standard MoE architecture by adding neuron-level sparsity within each selected expert. For each input, after the top-K experts are selected via standard routing, MoNE computes gate projections for the selected expert and applies top-K_N selection on the magnitude of these gate outputs. The expert computation then uses only the rows/columns of the weight matrices corresponding to the selected neurons. The paper introduces a Neuron Granular Load Balance Loss (NG-LBL) to prevent neuron expert collapse, ensuring diverse activation across neurons. The method maintains compatibility with existing MoE implementations while achieving significant parameter savings.

## Key Results
- MoNE matches traditional MoE performance while activating only 50% of MoE-layer parameters
- Consistently outperforms traditional MoE when compared at equal numbers of activated parameters
- Improves downstream performance by approximately 1-2% relative to traditional MoE
- Larger performance gains observed at higher activation levels

## Why This Works (Mechanism)
MoNE exploits the inherent sparsity within activated experts by recognizing that a single expert FFN can be decomposed into a mixture of smaller "neuron experts." Traditional MoEs activate all neurons within a selected expert, but many are not needed for a given input. By applying top-k selection at the neuron level, MoNE effectively prunes unused computations while maintaining the same representational capacity. The neuron-granular load balance loss ensures diverse activation across neurons, preventing collapse to a small subset. This approach achieves both efficiency gains and performance improvements by focusing computation on the most relevant parameters for each input.

## Foundational Learning

- **Concept: Feed-Forward Network (FFN) as a Mixture of Experts**
  - Why needed here: MoNE's core insight is that a single expert FFN is mathematically equivalent to a mixture of smaller "neuron experts" (Equation 12). Understanding this decomposition is essential.
  - Quick check question: Given an FFN with weight matrices `W_down` (d_model x d_expert) and `W_up` (d_expert x d_model), can you express its output as a weighted sum of `d_expert` individual rank-1 matrix operations?

- **Concept: Sparse vs. Dense Activation in MoEs**
  - Why needed here: The paper argues that traditional MoEs are still "dense" at the neuron level within an expert. Grasping the difference between token-level sparsity (MoE) and neuron-level sparsity (MoNE) is critical for understanding the efficiency gains.
  - Quick check question: In a standard MoE, after the top-K router selects an expert, what proportion of that expert's parameters are typically involved in the computation for a single token? How does MoNE change this?

- **Concept: Load Balancing Loss in MoEs**
  - Why needed here: Standard MoEs use an auxiliary loss to prevent expert collapse. MoNE introduces a second, finer-grained loss (NG-LBL) to prevent "neuron expert collapse." Distinguishing between expert-level and neuron-level balancing is key.
  - Quick check question: What problem does a standard MoE load balance loss solve? How is the neuron-granular loss in MoNE different in its target?

## Architecture Onboarding

- **Component map:**
  - `Router(x) -> p, I_E`: Standard MoE router. Takes input `x`, produces expert scores `p`, and selects top-K expert indices `I_E`
  - `G_i = SiLU(W_i_gate * x)`: Gate projection for selected expert `i`. Its output `G_i` contains the "neuron expert" weights
  - `I_N = argtopK(Abs(G_i))`: Neuron selector. Identifies the indices of the top-K_N most important neuron experts for expert `i` based on the magnitude of `G_i`
  - `Selected Weights (W~)`: Uses `I_N` to slice out the corresponding rows/columns from `W_up` and `W_down`
  - `Neuron Expert Output`: Computes `W~_down(G[I_N] * W~_up * x)`
  - `Layer Output`: Sums the outputs of all selected neuron experts across all selected experts

- **Critical path:** The performance of MoNE hinges on the efficiency of the `argtopK` operation (Algorithm 1, line 9) on the activation values `G_i`. This must be implemented to be virtually free compared to the subsequent matrix multiplications.

- **Design tradeoffs:**
  - **Expert Count vs. Neuron Selection:** MoNE enables neuron-level selection without the large routing tables and communication overhead of massively multi-expert MoEs (e.g., >256 experts). The tradeoff is less explicit specialization between independent expert modules.
  - **Activation Rate (`K_N`):** A smaller ratio `K_N/d_model` saves computation but may discard useful information. The paper recommends 1/4 (Table 3), but this is a tuneable hyperparameter.
  - **Activation Function:** The paper's experiments (Table 4) suggest `SiLU` or `Sigmoid` are better than `Softmax` for the internal neuron gating, as Softmax can be too concentrated.

- **Failure signatures:**
  - **No Efficiency Gain:** Throughput/memory do not improve. This likely means the `argtopK` implementation is slow or the slicing/indexing operations are not optimized.
  - **Performance Collapse:** Downstream task scores are significantly lower than baseline. This could indicate `K_N` is too aggressive, the load balancing coefficients (`α_aux`, `α_NG`) are too high, or the initial sparsity assumption does not hold for the model/dataset.
  - **Imbalanced Neuron Activation:** Despite NG-LBL, some neurons remain dead. Check `α_NG` and the scale of the neuron gating activations.

- **First 3 experiments:**
  1. **Baseline Sparsity Validation:** On a pre-trained MoE (e.g., from HuggingFace), implement a post-hoc sparsification test. For each token, after an expert is selected, rank its neurons by the magnitude of their gate projection output and zero out the bottom 60-90%. Measure the immediate performance drop on a benchmark. This validates the core assumption of the paper.
  2. **MoNE Ablation on `K_N`:** Train a series of small MoNE models (e.g., 100M-300M params) on a standard dataset. Vary the neuron activation ratio `K_N/d_model` (e.g., 1/2, 1/4, 1/8). Compare downstream task performance to establish the optimal point on the efficiency-performance curve for your setting.
  3. **Ablation on NG-LBL:** Train two MoNE models, one with and one without the Neuron Granular Load Balance Loss. Monitor the variance of neuron activations per expert (as in Figure 6) and compare final downstream performance to quantify the benefit of the auxiliary loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance advantage of MoNE over traditional MoE persist, diminish, or increase when scaling model parameters beyond the 2.81B range?
- Basis: [inferred] The experimental evaluation is limited to relatively small models (925M and 2.81B parameters), while state-of-the-art MoE benefits often emerge at much larger scales (Section 4.1).
- Why unresolved: The distribution of neuron activation sparsity (Figure 2) might change as model capacity increases, potentially affecting the optimal $K_N$ ratio.
- What evidence would resolve it: Pretraining results comparing MoNE and traditional MoE on larger architectures (e.g., 7B or 70B active parameters) on a standard data mixture.

### Open Question 2
- Question: How should the total activated parameter budget be optimally partitioned between the number of selected experts ($K_E$) and the number of selected neurons per expert ($K_N$)?
- Basis: [inferred] The paper fixes the activated parameter count to match baselines and recommends $K_N = 1/4 \cdot d_{model}$ based on empirical results (Table 3), but does not derive a theoretical optimum for this trade-off.
- Why unresolved: It is unclear if the improved performance comes specifically from the neuron-granular selection or simply from the specific allocation of parameters chosen.
- What evidence would resolve it: A comprehensive ablation study varying both $K_E$ and $K_N$ (e.g., Few Experts/Many Neurons vs. Many Experts/Few Neurons) while keeping total FLOPs constant.

### Open Question 3
- Question: Does the top-k sorting operation within the expert introduce training latency or memory bottlenecks when integrated with highly optimized MoE kernels (e.g., Megablocks or Tutel)?
- Basis: [inferred] The paper claims "negligible latency" based on throughput tests with batch size 8 (Table 5), but the $O(d \log d)$ sort operation is fundamentally distinct from standard matrix operations.
- Why unresolved: Standard MoE implementations are heavily optimized for sparse matrix multiplication; introducing a dynamic sorting step inside the expert may disrupt memory access patterns at scale.
- What evidence would resolve it: Detailed profiling of training wall-clock time and memory footprint on large-scale distributed systems (e.g., 128+ GPUs) compared against a fused kernel baseline.

## Limitations

- The paper demonstrates efficiency gains at equal parameter counts rather than equal computational budgets, making it difficult to isolate the benefit of neuron-granular selection
- Performance improvements are primarily validated on language modeling tasks with limited evaluation on diverse downstream applications
- The NG-LBL loss is presented as crucial but the paper does not thoroughly explore sensitivity to different coefficients or alternative balancing strategies

## Confidence

- **High Confidence:** The observation that MoE gating produces sparse activations within experts is well-supported by analysis and experimental validation
- **Medium Confidence:** The claim that MoNE matches traditional MoE performance at 50% parameter activation is supported by experiments, but comparison methodology makes it difficult to isolate neuron-granular selection benefits
- **Low Confidence:** The generalization of MoNE's benefits across diverse tasks, model scales, and datasets is not thoroughly established

## Next Checks

1. **Cross-Architecture Validation:** Test MoNE on multiple MoE architectures beyond LLaMA-3-8B, including different expert counts, model scales, and base model sizes. Compare performance not just at equal parameter counts but also at equal computational budgets to isolate the benefit of neuron-granular selection.

2. **Alternative Balancing Mechanisms:** Implement and compare MoNE with alternative load balancing strategies beyond NG-LBL, such as adaptive threshold-based selection or entropy-based balancing. This would help determine whether NG-LBL is essential or if other approaches could achieve similar results.

3. **Hardware-Agnostic Efficiency Testing:** Measure actual throughput and memory usage of MoNE implementations across different hardware platforms (CPU, GPU, specialized accelerators). This would validate whether the theoretical efficiency gains translate to real-world performance improvements and identify any implementation bottlenecks.