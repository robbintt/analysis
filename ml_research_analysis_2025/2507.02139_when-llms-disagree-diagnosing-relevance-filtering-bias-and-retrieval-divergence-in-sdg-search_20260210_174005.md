---
ver: rpa2
title: 'When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval Divergence
  in SDG Search'
arxiv_id: '2507.02139'
source_url: https://arxiv.org/abs/2507.02139
tags:
- disagreement
- retrieval
- relevance
- each
- filtering
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how classification disagreement between\
  \ two large language models\u2014LLaMA and Qwen\u2014affects document retrieval\
  \ in SDG-focused information retrieval. Using a corpus of scholarly abstracts related\
  \ to SDGs 1, 3, and 7, the authors compare model-specific relevance labels and analyze\
  \ their lexical and semantic differences."
---

# When LLMs Disagree: Diagnosing Relevance Filtering Bias and Retrieval Divergence in SDG Search

## Quick Facts
- arXiv ID: 2507.02139
- Source URL: https://arxiv.org/abs/2507.02139
- Reference count: 28
- Primary result: LLM disagreement in relevance filtering is systematic and impacts retrieval divergence

## Executive Summary
This study investigates how classification disagreement between two large language models—LLaMA and Qwen—affects document retrieval in SDG-focused information retrieval. Using a corpus of scholarly abstracts related to SDGs 1, 3, and 7, the authors compare model-specific relevance labels and analyze their lexical and semantic differences. The research reveals that disagreement is systematic rather than random, with each model filtering distinct lexical subsets and producing divergent top-ranked retrieval results. Logistic regression models can predict disagreement with AUCs above 0.74 using TF-IDF features.

## Method Summary
The study employs a corpus of 34,716 scholarly abstracts related to SDGs 1 (poverty), 3 (health), and 7 (energy). Two LLM families, LLaMA and Qwen, independently classify each document for relevance to these SDGs. The authors then analyze lexical and semantic differences between models' relevance judgments using TF-IDF features, semantic embeddings, and logistic regression models to predict disagreement. Retrieval divergence is measured by comparing top-ranked results from each model, with divergence defined as the percentage of documents that appear in only one model's top-k list.

## Key Results
- LLM disagreement in relevance filtering is systematic, not random, with AUCs > 0.74 for prediction models
- Each model filters distinct lexical subsets, with LLaMA favoring words like "equitable" and Qwen favoring "vaccine"
- Retrieval divergence is significant, with only 40% document overlap in top-20 lists between models

## Why This Works (Mechanism)
The study demonstrates that different LLM architectures develop distinct relevance criteria based on their training and prompting, leading to systematic filtering biases. These biases manifest as lexical preferences and semantic interpretation differences that persist across documents. The systematic nature allows prediction of disagreement using document features, while the divergence in filtered sets directly impacts retrieval performance by changing which documents enter the candidate pool.

## Foundational Learning
**TF-IDF Feature Analysis**: Understanding term importance across documents and models. Why needed: To identify lexical differences in model relevance judgments. Quick check: Compare TF-IDF weight distributions between models for shared relevant documents.

**Logistic Regression for Disagreement Prediction**: Using document features to predict model disagreement. Why needed: To quantify predictability of disagreement patterns. Quick check: Verify AUC scores exceed 0.7 threshold across different feature sets.

**Semantic Embedding Comparison**: Analyzing semantic space differences between models. Why needed: To understand whether differences are lexical or conceptual. Quick check: Compute cosine similarity between model embeddings for shared documents.

## Architecture Onboarding
**Component Map**: Document Corpus -> LLM Relevance Classification (LLaMA + Qwen) -> Feature Extraction (TF-IDF + Embeddings) -> Disagreement Prediction -> Retrieval Divergence Analysis

**Critical Path**: Document Classification → Feature Extraction → Disagreement Prediction → Retrieval Comparison

**Design Tradeoffs**: Single-model vs. ensemble approaches; computational cost of dual classification vs. retrieval quality gains; choice of lexical vs. semantic features for disagreement prediction.

**Failure Signatures**: Random disagreement patterns (would indicate unreliable models); perfect agreement (would suggest redundant filtering); retrieval divergence below 20% (would suggest minimal impact).

**First Experiments**: 1) Test disagreement patterns with different prompt formulations; 2) Compare disagreement rates across temperature settings; 3) Measure impact of ensemble methods on retrieval precision.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to two relatively small model families (LLaMA and Qwen), restricting generalizability
- Focus on only three SDGs and scholarly abstracts limits external validity
- Does not explore whether disagreement patterns change with different prompt formulations or temperature settings
- Practical impact on retrieval quality remains unquantified despite observed statistical significance

## Confidence
- High confidence in systematic nature of LLM disagreement (AUC > 0.74 for prediction models)
- Medium confidence in lexical and semantic differences between models' relevance judgments
- Medium confidence in retrieval divergence implications, though practical impact assessment is limited

## Next Checks
1. Test the disagreement patterns across a broader set of LLM families (GPT-4, Claude, Gemini) and document types (full-text articles, policy reports)
2. Conduct retrieval effectiveness experiments comparing single-model vs. ensemble approaches on downstream tasks
3. Investigate whether ensemble methods can mitigate systematic disagreement while preserving retrieval quality