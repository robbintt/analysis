---
ver: rpa2
title: 'CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training
  for Large Language Models'
arxiv_id: '2509.25996'
source_url: https://arxiv.org/abs/2509.25996
tags:
- training
- sparse
- cast
- sparsity
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CAST, a fully continuous and differentiable
  sparsity-aware training framework for semi-structured (N:M) sparse large language
  models. CAST addresses the limitations of prior methods by jointly optimizing masks
  and weights without relying on sparse forward passes or discontinuous STE approximations.
---

# CAST: Continuous and Differentiable Semi-Structured Sparsity-Aware Training for Large Language Models

## Quick Facts
- **arXiv ID**: 2509.25996
- **Source URL**: https://arxiv.org/abs/2509.25996
- **Reference count**: 40
- **Key outcome**: CAST achieves state-of-the-art 2:4 sparse LLM training with negligible perplexity increase and improved zero-shot accuracy using only 2% of original pretraining tokens.

## Executive Summary
CAST introduces a fully continuous and differentiable framework for semi-structured (N:M) sparse LLM training. Unlike prior methods that rely on sparse forward passes or discontinuous straight-through estimators, CAST maintains dense forward computation while jointly optimizing masks and weights through proportional L1 decay. The framework introduces AdamS, a sparsity-aware optimizer with adaptive decay, learnable weight scaling to compensate for regularization-induced magnitude reduction, and knowledge distillation from dense models. Evaluated across GPT-2, OPT, LLaMA2, and LLaMA3 models ranging from 125M to 13B parameters, CAST achieves state-of-the-art performance with minimal perplexity degradation.

## Method Summary
CAST transforms pretrained dense LLMs into 2:4 semi-structured sparse format by maintaining dense forward passes throughout training while jointly optimizing masks and weights. The method uses AdamS, a sparsity-aware optimizer that applies proportional L1 decay only to masked weights, ensuring uniform convergence to zero. Masks are updated every 10 iterations based on magnitude ranking within each 2:4 group. A learnable weight-scaling module compensates for magnitude reduction from regularization, and knowledge distillation from the dense teacher provides additional supervision. At training end, hard pruning is applied with negligible loss since masked weights have already converged to near-zero.

## Key Results
- Achieves state-of-the-art 2:4 sparse LLM training with only 0.09 perplexity increase on LLaMA2-7B
- Improves zero-shot accuracy by 0.36% while using only 2% of original pretraining tokens
- Establishes empirical scaling laws with R² ≈ 0.99 to predict sparse model performance under different training budgets
- Demonstrates consistent superiority across multiple model families (GPT-2, OPT, LLaMA2, LLaMA3) and sizes (125M to 13B parameters)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Proportional L1 decay enables uniform convergence of masked weights to zero across parameters with varying gradient magnitudes.
- **Mechanism**: AdamS computes decayed gradient as `G̃_t = (1-α_t)G_t + α_tλSign(θ_{t-1})` where α_t increases from 0 to 1, ensuring all masked weights receive sufficient decay regardless of gradient strength.
- **Core assumption**: Important weights maintain magnitude despite decay due to strong gradient signals from their contribution to loss.
- **Evidence anchors**: Abstract mentions "adaptive L1 decay to promote uniform sparsification"; Section IV.A explains proportional strategy provides precise control; related work addresses different sparsity patterns.

### Mechanism 2
- **Claim**: Dense forward passes preserve gradient accuracy and optimization stability, avoiding discontinuities in STE-based methods.
- **Mechanism**: Computing forward passes on full weights provides exact gradients while L1 decay drives masked weights toward zero, making final pruning approximately lossless.
- **Core assumption**: Dense model remains functional during training while masked weights decay to near-zero.
- **Evidence anchors**: Abstract states "without relying on sparse forward passes"; Section V.A shows STE error grows linearly with masked weight magnitude; Figure 5 demonstrates dense-forward stability.

### Mechanism 3
- **Claim**: Learnable per-group weight scaling factors compensate for magnitude reduction from L1 decay while preserving sparsity pattern.
- **Mechanism**: Row-wise scaling factors (initialized to 1) are learned jointly during training and folded into weights at deployment with zero inference overhead.
- **Core assumption**: Systematic magnitude reduction can be compensated by scalar per-group factors without interfering with mask learning.
- **Evidence anchors**: Abstract mentions "learnable weight-scaling module"; Section IV.B explains no inference-time overhead; Table VI ablation shows ~0.37% accuracy drop without scaling.

## Foundational Learning

- **Concept**: N:M Semi-Structured Sparsity
  - **Why needed here**: CAST targets 2:4 sparsity specifically because it is hardware-accelerated on NVIDIA GPUs. Understanding the constraint (each group of 4 weights retains exactly 2 nonzeros) is essential for understanding mask updates.
  - **Quick check question**: Given weights [0.5, -0.3, 0.8, 0.1] in a 2:4 group, which two should be retained?

- **Concept**: L1 vs L2 Regularization for Sparsity
  - **Why needed here**: The paper contrasts L1 (uniform drive toward zero) with L2 (proportional decay that preserves large weights). AdamS uses L1 specifically to avoid oscillation problems with L2 in SR-STE.
  - **Quick check question**: Why would L2 decay suppress large masked weights while letting small weights oscillate?

- **Concept**: Straight-Through Estimator (STE)
  - **Why needed here**: Prior work used STE to approximate gradients through hard masking. CAST is motivated by STE's limitations—discontinuity and gradient error scaling with weight magnitude.
  - **Quick check question**: If a weight is masked (zeroed) in forward pass but receives gradient as if unmasked, what happens when that weight grows large?

## Architecture Onboarding

- **Component map**: Pretrained Dense Model -> AdamS Optimizer (with proportional L1 decay) -> Weight Scaling Module (per-group scalars) -> Knowledge Distillation (KL+CE loss) -> Final Hard Pruning

- **Critical path**:
  1. Initialize from pretrained dense checkpoint
  2. Compute initial masks via magnitude-based 2:4 selection
  3. Forward pass uses scaled dense weights (no masking)
  4. Backward pass computes gradients on dense weights
  5. AdamS applies proportional L1 decay only to masked weights
  6. Every 10 steps, recompute masks based on updated magnitudes
  7. At training end, apply final hard mask and fold scaling into weights

- **Design tradeoffs**:
  - Dense vs sparse forward: Dense preserves gradient accuracy but requires full compute; sparse forward is cheaper but introduces gradient error
  - Mask update frequency (T1): Frequent updates enable better mask exploration but add overhead; paper finds T1=10 sufficient
  - Distillation coefficient (η): Higher η improves perplexity; lower η may improve zero-shot accuracy depending on data-teacher alignment

- **Failure signatures**:
  - Mask oscillation: If L1 decay is too weak or α_t ramps too slowly, masks keep flipping
  - Incomplete sparsification: If training ends before masked weights converge to near-zero, final pruning causes sharp degradation
  - Scaling instability: If weight scaling factors diverge, they may amplify noise in near-zero weights

- **First 3 experiments**:
  1. Baseline reproduction: Run SR-STE on GPT-2 124M with 2:4 sparsity on C4; track flip rate and sparse weight ratio to confirm oscillation behavior
  2. Ablation on decay strategy: Compare additive vs proportional L1 decay on OPT-125M; plot masked weight magnitude distribution at training end
  3. Scaling law validation: Fit the token-only scaling law on LLaMA2-7B using multiple training budgets; verify R² ≈ 0.99 and prediction error < 0.05 perplexity

## Open Questions the Paper Calls Out

- **Question**: Can the continuous optimization framework of CAST be effectively adapted for structured pruning tasks?
- **Basis in paper**: The conclusion states the authors plan to "extend CAST to structured pruning... where efficiency and adaptability are critical."
- **Why unresolved**: CAST relies on fine-grained N:M sparsity to maintain dense forward passes while decaying weights. Structured pruning removes entire blocks or channels, fundamentally altering network topology and gradient flow.
- **What evidence would resolve it**: Successful application of CAST to remove structured blocks (e.g., 4x4 blocks or entire attention heads) with performance metrics matching N:M results on standard benchmarks.

- **Question**: How does the performance of CAST translate to multi-modal or instruction-tuned LLMs?
- **Basis in paper**: The authors explicitly list "multi-modal and instruction-tuned LLMs" as targets for future exploration.
- **Why unresolved**: Current evaluation is restricted to standard decoder-only language models. Multi-modal models involve distinct encoders and fusion layers, while instruction-tuned models may exhibit different sensitivities to L1 regularization.
- **What evidence would resolve it**: Evaluating CAST on models like LLaVA or instruction-tuned variants and measuring performance against dense baselines.

- **Question**: Does the empirical scaling law established for sparse retraining hold for models significantly larger than 13B parameters?
- **Basis in paper**: Experiments are constrained to models ranging from 125M to 13B parameters due to "computational constraints."
- **Why unresolved**: The empirical fit (R² ≈ 0.99) is high for the tested range, but it's unclear if the linear relationship persists for massive models (e.g., 70B+), or if dense forward pass becomes prohibitive.
- **What evidence would resolve it**: Extrapolating derived scaling laws to train a 70B+ parameter model and verifying if predicted token budget yields expected perplexity recovery.

## Limitations
- Training dynamics under extreme sparsity remain untested; proportional L1 decay may require parameter tuning for different sparsity ratios
- Knowledge distillation efficacy depends on data-teacher alignment; optimal coefficient η requires additional hyperparameter searches
- Hardware generalization limited to NVIDIA GPUs supporting 2:4 sparsity; dense forward computation may be impractical for very large models

## Confidence
- **High confidence**: Proportional L1 decay mechanism is well-supported by theoretical analysis and empirical results, with ablation study confirming necessity
- **Medium confidence**: Dense forward pass design is logically sound but lacks direct comparative evidence against other dense-forward sparse training methods
- **Medium confidence**: Weight scaling compensation shows measurable benefits but effectiveness for different sparsity ratios or patterns remains speculative

## Next Checks
1. Fit the token-only scaling law (Equation 17) on LLaMA2-7B using multiple training budgets; verify R² ≈ 0.99 and prediction error < 0.05 perplexity
2. Implement exact magnitude-based 2:4 mask initialization procedure and verify initial mask pattern matches paper's description; track mask evolution over first 100 training steps
3. Compare additive vs proportional L1 decay on OPT-125M; plot masked weight magnitude distribution at training end to confirm proportional decay achieves uniform convergence while additive creates bimodal distributions