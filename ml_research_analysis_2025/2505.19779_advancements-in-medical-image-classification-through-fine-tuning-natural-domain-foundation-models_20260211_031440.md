---
ver: rpa2
title: Advancements in Medical Image Classification through Fine-Tuning Natural Domain
  Foundation Models
arxiv_id: '2505.19779'
source_url: https://arxiv.org/abs/2505.19779
tags:
- medical
- image
- foundation
- arxiv
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates recent foundation models for medical image
  classification, fine-tuning models like DINOv2, MAE, VMamba, CoCa, SAM2, and AIMv2
  on diverse datasets (CBIS-DDSM, ISIC2019, APTOS2019, CHEXPERT). The key finding
  is that AIMv2, DINOv2, and SAM2 outperform other models across all datasets, demonstrating
  that advancements in natural domain training translate effectively to the medical
  domain.
---

# Advancements in Medical Image Classification through Fine-Tuning Natural Domain Foundation Models

## Quick Facts
- arXiv ID: 2505.19779
- Source URL: https://arxiv.org/abs/2505.19779
- Reference count: 0
- Primary result: AIMv2, DINOv2, and SAM2 outperform other foundation models across mammography, dermoscopy, retinal, and chest radiograph classification tasks

## Executive Summary
This study evaluates recent foundation models pre-trained on natural images for medical image classification tasks, fine-tuning models including DINOv2, MAE, VMamba, CoCa, SAM2, and AIMv2 on diverse medical datasets. The key finding is that AIMv2, DINOv2, and SAM2 consistently outperform other models across all datasets, demonstrating that advancements in natural domain training translate effectively to the medical domain. Fine-tuning with multi-layer attention heads consistently improved performance over linear heads, especially in frozen configurations. Despite limited labeled data, these models achieve robust classification results, highlighting their adaptability and potential for enhancing clinical diagnostic tools.

## Method Summary
The study fine-tunes six foundation models (DINOv2, MAE, VMamba, CoCa, SAM2, AIMv2) on four medical datasets using two configurations: frozen backbone (training only the head) and unfrozen (full fine-tuning). Models use ViT backbones of varying sizes (B, L, H). Input images are resized to 224×224, with grayscale datasets using single-channel patch embeddings. Two classification heads are tested: linear (using [CLS] token) and multi-layer attention (using patch embeddings). Training uses AdamW optimizer with learning rate search from 10⁻³ to 10⁻⁵, and standard augmentations including flips, color jitter, and random crops.

## Key Results
- AIMv2 achieved highest performance across both frozen and unfrozen configurations in all four datasets
- Multi-layer attention heads consistently outperformed linear heads, particularly in frozen configurations
- Optimal model scale correlates with dataset size: smaller models (ViT-B) excel on limited datasets while larger models (ViT-L, ViT-H) require more samples
- SAM2, designed for video processing, showed strong performance on static medical images despite lacking a [CLS] token

## Why This Works (Mechanism)

### Mechanism 1
Natural-domain foundation models transfer effectively to medical image classification because their learned visual representations capture hierarchical features (edges, textures, shapes) that remain discriminative across domains. Large-scale pre-training on diverse natural images (130M–400M images) develops general-purpose feature extractors that adapt via gradient updates to domain-specific decision boundaries without requiring full feature learning from scratch. Visual primitives learned from natural images (contrast boundaries, texture patterns, shape hierarchies) share sufficient structure with medical imaging features to enable efficient adaptation.

### Mechanism 2
Multi-layer attention heads outperform linear heads in frozen configurations because they can reweight and combine patch-level features dynamically, compensating for fixed backbone representations. Linear heads use only [CLS] token embeddings—a single compressed representation. Attention heads process all patch embeddings through learned attention weights, enabling selective feature aggregation. When backbone weights are frozen, this flexibility becomes critical for adapting fixed features to new tasks.

### Mechanism 3
Optimal model scale correlates with dataset size—smaller models (ViT-B) perform competitively on limited datasets, while larger models (ViT-L, ViT-H) require more training samples to realize their capacity advantage. Larger models have more parameters and higher representational capacity but also higher variance. With limited data, they risk overfitting. Smaller models have stronger inductive bias, generalizing better from fewer examples.

## Foundational Learning

- **Vision Transformer (ViT) Patch Embeddings**: All evaluated models use ViT architectures. Understanding how images are tokenized into patches and how [CLS] tokens aggregate information is essential for interpreting linear vs. attention head results. *Quick check: Can you explain why SAM2 lacks a [CLS] token and requires average pooling of patch features instead?*

- **Frozen vs. Fine-tuned Transfer Learning**: The paper explicitly compares frozen (feature extractor only) and unfrozen (end-to-end fine-tuning) configurations. Understanding the trade-offs (computational cost, overfitting risk, feature adaptation) is critical for experimental design. *Quick check: When would you choose frozen features over full fine-tuning for a medical dataset with 500 labeled images?*

- **Pre-training Objectives (Self-supervised vs. Contrastive vs. Autoregressive)**: The models compared use different pre-training strategies—DINOv2 (self-supervised distillation), MAE (masked reconstruction), AIMv2 (multimodal autoregressive), CoCa (contrastive captioning). Pre-training objective likely influences transfer quality. *Quick check: Why might MAE (reconstruction-focused) perform differently from DINOv2 (feature-distillation-focused) on medical classification tasks?*

## Architecture Onboarding

- **Component map**: Input Image (224×224) → Patch Embedding Layer → Foundation Model Backbone → [CLS] token OR all patches → Linear head OR Multi-layer Attention → Classification scores → Loss (Cross-Entropy)

- **Critical path**:
  1. Grayscale adaptation: For medical grayscale images (DDSM, CheXpert), modify patch embedding layer to use only first channel weights (do not replicate to 3 channels)
  2. Position embedding interpolation: If pre-trained model uses different input size, apply bicubic interpolation to adjust position embeddings
  3. Head selection: Start with attention head + frozen backbone for rapid iteration; switch to unfrozen + linear for final optimization
  4. Learning rate search: Range 10⁻³ to 10⁻⁵ using AdamW optimizer

- **Design tradeoffs**:
  | Configuration | Training Time | Performance | When to Use |
  |---------------|---------------|-------------|-------------|
  | Frozen + Linear | Longest | Lowest | Resource-constrained inference only |
  | Frozen + Attention | Shortest | High | Quick prototyping, limited compute |
  | Unfrozen + Linear | Medium | Highest (typically) | Full production optimization |
  | Unfrozen + Attention | Medium | Comparable to unfrozen linear | Complex tasks requiring feature reweighting |

- **Failure signatures**:
  - Frozen + Linear underperforming significantly: Backbone features insufficiently task-relevant → switch to attention head or unfreeze
  - Unfrozen training unstable or diverging: Learning rate too high → reduce by 10×, add gradient clipping
  - Large model underperforming small model on small dataset: Overfitting → apply stronger regularization, reduce model size, or increase data augmentation
  - Grayscale images producing degraded features: Channel modification incorrectly applied → verify single-channel weights are extracted correctly

- **First 3 experiments**:
  1. Baseline comparison: Run AIMv2-ViT-L and DINOv2-ViT-L on your target dataset with frozen backbone + attention head. This establishes a strong baseline with minimal training time.
  2. Frozen vs. unfrozen ablation: Take the best-performing model from experiment 1 and compare frozen vs. unfrozen configurations with both linear and attention heads. Document performance gap and training time trade-offs.
  3. Scale sensitivity test: If dataset is small (<5,000 images), compare ViT-B vs. ViT-L variants. If dataset is large (>50,000 images), compare ViT-L vs. ViT-H. This validates whether model scale aligns with dataset size per the paper's findings.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text provided. However, three significant questions arise from the study:

1. **Cross-modal generalization**: Can the best-performing models (AIMv2, DINOv2) effectively transfer to non-radiology medical imaging modalities (e.g., histology slides or ultrasound) beyond structured radiological imaging?

2. **Attention head design**: What is the optimal architecture for the multi-layer attention head, and how much does its design contribute to performance gains versus other factors like backbone selection?

3. **Computational efficiency**: How do the performance gains of larger models justify their computational costs in clinical deployment scenarios, particularly for resource-constrained environments?

## Limitations

- **Domain specificity**: Results may not generalize to pathology slides, ultrasound, or other modalities with significantly different visual characteristics from the four structured medical imaging datasets evaluated.

- **Head architecture ambiguity**: The multi-layer attention head is described as "a pre-trained transformer model" without specific architectural details, making it difficult to assess whether performance gains stem from architectural advantages or implementation specifics.

- **Scale-compute trade-offs**: The paper does not report computational requirements (FLOPs, memory usage) for different configurations, leaving unclear the practical applicability of larger models in resource-constrained clinical environments.

## Confidence

- **High confidence**: The finding that AIMv2, DINOv2, and SAM2 outperform other models across all datasets is well-supported by systematic experiments across multiple architectures and datasets.

- **Medium confidence**: The mechanism explaining why attention heads outperform linear heads in frozen configurations is plausible but relies on indirect evidence without isolating the contribution of patch-level features versus [CLS] token representations.

- **Low confidence**: The scale-dataset size relationship is observed but confounded by task complexity differences between datasets. Without controlling for class numbers and decision boundaries, the optimal capacity findings remain tentative.

## Next Checks

1. **Cross-modal generalization test**: Apply the best-performing models (AIMv2, DINOv2) to a non-radiology medical imaging modality (e.g., histology slides or ultrasound) to verify whether natural domain features transfer beyond structured radiological imaging. Compare against a foundation model pre-trained on medical data.

2. **Attention head ablation study**: Implement variations of the attention head (1-layer vs. 3-layer, different hidden dimensions) while keeping backbone frozen to isolate whether performance gains stem from architectural capacity or specific design choices. Measure both accuracy and parameter efficiency.

3. **Computational efficiency benchmark**: Profile GPU memory usage and inference latency for frozen vs. unfrozen configurations across model scales. Calculate the accuracy-per-FLOP metric to quantify whether performance gains justify computational costs in clinical deployment scenarios.