---
ver: rpa2
title: 'ToMoE: Converting Dense Large Language Models to Mixture-of-Experts through
  Dynamic Structural Pruning'
arxiv_id: '2501.15316'
source_url: https://arxiv.org/abs/2501.15316
tags:
- pruning
- tomoe
- experts
- parameters
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ToMoE converts dense LLMs to MoE models using dynamic structural
  pruning without fine-tuning. It applies top-K and static pruning for attention layers
  and top-1 routing for MLP layers, maintaining fixed computational costs.
---

# ToMoE: Converting Dense Large Language Models to Mixture-of-Experts through Dynamic Structural Pruning

## Quick Facts
- arXiv ID: 2501.15316
- Source URL: https://arxiv.org/abs/2501.15316
- Authors: Shangqian Gao, Ting Hua, Reza Shirkavand, Chi-Heng Lin, Zheng Tang, Zhengao Li, Longge Yuan, Fangyi Li, Zeyu Zhang, Alireza Ganjdanesh, Lou Qian, Xu Jie, Yen-Chang Hsu
- Reference count: 40
- One-line primary result: Converts dense LLMs to MoE models without fine-tuning, achieving better perplexity and accuracy with fewer active parameters than state-of-the-art pruning and MoE construction techniques.

## Executive Summary
ToMoE introduces a dynamic structural pruning approach to convert dense large language models into mixture-of-experts (MoE) models without requiring any weight updates or fine-tuning. The method applies top-K and static pruning for attention layers and top-1 routing for MLP layers, maintaining fixed computational costs while discovering inherent experts within dense models. It outperforms existing pruning and MoE construction methods on diverse models including Phi-2, LLaMA-2/3, and Qwen-2.5 across various tasks, achieving better performance with fewer active parameters and minimal additional parameters.

## Method Summary
ToMoE converts dense models to MoE through dynamic structural pruning by applying top-K and static pruning for attention layers and top-1 routing for MLP layers. The method discovers inherent experts within dense models without weight updates, using self-knowledge distillation where the original dense model serves as teacher. A hypernetwork generates layer-wise expert embeddings, and differentiable Gumbel operators enable joint optimization of routing and expert configuration. The approach maintains fixed computational costs while achieving better performance than state-of-the-art pruning and MoE construction techniques.

## Key Results
- Outperforms state-of-the-art pruning and MoE construction techniques on Phi-2, LLaMA-2/3, and Qwen-2.5 models
- Achieves better perplexity and accuracy with fewer active parameters across WikiText-2, ARC, PIQA, WinoGrande, and HellaSwag tasks
- Requires minimal additional parameters (~0.27% for LLaMA-2 7B) while discovering inherent experts within dense models
- Maintains fixed computational costs through top-1 routing and static attention pruning

## Why This Works (Mechanism)

### Mechanism 1
Converting dense MLP layers to MoE via dynamic structural pruning preserves model capacity better than permanent parameter removal. Each expert is defined by a binary diagonal selection matrix that activates a subset of the original MLP's intermediate dimension, with tokens routed via top-1 selection using a learned router. The union of all experts covers nearly the full original model width, avoiding permanent deletion. This works because dense models contain latent functional specialization that can be carved into experts without weight updates.

### Mechanism 2
Joint optimization of routing and expert configuration outperforms two-stage expert-then-router construction. Expert configurations and routing decisions are learned simultaneously via differentiable Gumbel operators, with a hypernetwork generating layer-wise expert embeddings allowing inter-layer coordination. This works because optimal expert boundaries depend on how tokens will be routed, and vice versa. The simultaneous training captures dependencies between routing decisions and expert definitions that staged approaches miss.

### Mechanism 3
Self-knowledge distillation using frozen original weights guides expert formation without external data requirements. The original dense model serves as teacher, with trainable modules disabled to get teacher logits, then re-enabled for student forward pass. KL divergence between logits replaces language modeling loss. This works because the output distribution of the dense model encodes sufficient signal for learning good routing and expert configurations without needing labeled data.

## Foundational Learning

- **Concept: Straight-Through Gumbel-Softmax Estimator**
  - Why needed here: Enables backpropagation through discrete expert selection (top-1 routing) and binary expert masks. Without this, routing decisions would be non-differentiable.
  - Quick check question: Can you explain why `round(sigmoid(x + g))` during forward pass but using `sigmoid` gradients during backward pass allows learning discrete choices?

- **Concept: Mixture-of-Experts Routing Capacity**
  - Why needed here: Understanding why top-1 routing with load balancing is necessary to prevent expert collapse and maintain inference efficiency with fixed per-token compute.
  - Quick check question: What happens to inference latency if one expert receives 80% of tokens despite having the same width as others?

- **Concept: Rotary Position Embeddings (RoPE) Constraints on Pruning**
  - Why needed here: Explains why Q,K pruning must be static and share the same mask across RoPE subspaces—dynamic masks would misalign position encodings.
  - Quick check question: Why does pruning different head-dimension indices for different tokens break the attention computation when using RoPE?

## Architecture Onboarding

- **Component map:** Input → Router → top-1 expert selection → execute expert's MLP subset via pre-computed mask → output. For MHA: apply static mask to Q,K; apply token-specific top-K to V,O projections.

- **Critical path:** Input passes through router to determine top-1 expert selection, then executes the corresponding MLP subset using pre-computed binary masks. For attention layers, static masks are applied to Q and K matrices while token-specific top-K masks are applied to V and O projections.

- **Design tradeoffs:** More experts (N) increases routing flexibility but adds router parameters and may complicate learning. The paper recommends N ≤ 16. Smaller active ratio p improves inference speed but requires stronger regularization (R_U becomes critical below 50%). Training dataset diversity (WikiText + Alpaca + Code-Alpaca) improves performance over single-domain data.

- **Failure signatures:** Expert collapse occurs when one expert dominates routing—check load balancing loss R_L and increase γ if > 0.2. Low union coverage means R_U remains high—increase β to encourage experts to utilize more of the original model capacity. Inference shape mismatch happens when dynamic Q,K masks cause varying effective attention widths—verify only V,O use token-specific masks.

- **First 3 experiments:**
  1. Run ToMoE on LLaMA-2 7B with p=0.7, N=8 on WikiText only. Verify perplexity < 7.0 and that union coverage exceeds 95% of original parameters.
  2. Compare top-1 vs. top-2 routing on MLP layers while keeping total active parameters constant. Measure impact on ARC-c and PIQA.
  3. Test N∈{4,8,16,24} on Qwen-2.5 7B at p=0.5. Confirm paper finding that N=16 is optimal and N=24 shows no further gain.

## Open Questions the Paper Calls Out
### Open Question 1
Do ToMoE experts develop meaningful semantic specialization across different types of input (e.g., mathematical reasoning vs. natural language), and how does this compare to MoE models trained from scratch? The authors observe that math-related inputs show clearer semantic patterns in expert routing while general text shows syntactic alignment, but further investigation is required to fully understand the precise semantic roles of ToMoE experts.

### Open Question 2
Would fine-tuning the model weights after ToMoE conversion yield further performance improvements, and how would this trade off against the method's efficiency advantage? The paper emphasizes the no-fine-tuning advantage as a key benefit but does not explore whether additional fine-tuning could recover more capacity or improve performance beyond the current state.

### Open Question 3
How does ToMoE perform on encoder-decoder architectures or encoder-only models, rather than the decoder-only architectures tested? The paper states it focuses on decoder-only architectures since most recent LLMs adapt this paradigm, leaving applicability to models like T5 or BERT unknown.

## Limitations
- The paper demonstrates latent expert specialization exists in dense models through empirical observation rather than proven theoretical property
- Claims about superiority of joint optimization over staged methods lack direct comparative evidence
- Self-knowledge distillation approach achieves good results without external data, but comparison against supervised fine-tuning is absent

## Confidence
- **High confidence**: The routing mechanism using ST-Gumbel operators is technically sound and reproducible. The fixed computational cost claim is verifiable through the described masking approach.
- **Medium confidence**: The performance improvements over baselines are well-documented, but the extent to which they stem from the specific ToMoE design versus general MoE benefits remains unclear. The discovery of "inherent experts" is observational rather than mechanistic.
- **Low confidence**: Claims about the superiority of joint optimization over staged methods lack direct comparative evidence. The necessity of the specific regularization terms for different active ratios is demonstrated empirically but not theoretically grounded.

## Next Checks
1. **Ablation of joint vs. staged optimization**: Implement the exact same ToMoE architecture but train expert configurations first (using self-distillation), then freeze experts and train only the router. Compare final performance at identical active parameter ratios to definitively test if joint optimization provides measurable benefits beyond parameter efficiency.

2. **Expert collapse analysis under extreme sparsity**: Systematically reduce active parameter ratios from 50% to 20% in 5% decrements on LLaMA-2 7B. Measure expert utilization variance, union coverage R_U, and task performance to identify precise thresholds where regularization terms become critical and where expert collapse becomes unavoidable.

3. **Transferability of discovered experts**: After training ToMoE on LLaMA-2 7B with WikiText-2, freeze the discovered expert masks and router, then fine-tune only the router on a completely different domain (e.g., medical or legal text). Compare to fine-tuning the original dense model to assess whether the discovered expert structure provides domain-transfer benefits or is task-specific.