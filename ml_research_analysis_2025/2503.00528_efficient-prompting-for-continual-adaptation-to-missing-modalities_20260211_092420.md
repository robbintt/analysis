---
ver: rpa2
title: Efficient Prompting for Continual Adaptation to Missing Modalities
arxiv_id: '2503.00528'
source_url: https://arxiv.org/abs/2503.00528
tags:
- prompts
- missing
- task
- learning
- modality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of continual learning when dealing
  with missing modalities in multimodal data, which commonly occurs in real-world
  applications due to equipment failures or privacy concerns. Existing methods struggle
  with catastrophic forgetting and high computational costs in such scenarios.
---

# Efficient Prompting for Continual Adaptation to Missing Modalities

## Quick Facts
- **arXiv ID:** 2503.00528
- **Source URL:** https://arxiv.org/abs/2503.00528
- **Reference count:** 11
- **Primary result:** A prompt-based continual learning method for multimodal data with missing modalities, achieving higher average accuracy and lower forgetting than state-of-the-art approaches while using only 2-3% of the backbone parameters.

## Executive Summary
This paper tackles the challenge of continual learning when dealing with missing modalities in multimodal data, which commonly occurs in real-world applications due to equipment failures or privacy concerns. Existing methods struggle with catastrophic forgetting and high computational costs in such scenarios. To address this, the authors propose a novel prompt-based approach that introduces three types of prompts—modality-specific, task-aware, and task-specific—to enable the model to learn intra-modality, inter-modality, intra-task, and inter-task features. Additionally, a contrastive task interaction strategy is employed to explicitly learn prompts correlating different modalities. The method is evaluated on three public datasets (CMU-MOSI, IEMOCAP, and CH-SIMS) and consistently outperforms state-of-the-art approaches, achieving higher average accuracy and lower forgetting measure. Notably, the approach is parameter-efficient, requiring only 2-3% of the parameters of the backbone network, making it both effective and scalable.

## Method Summary
The authors propose a prompt-based continual learning method for multimodal data with missing modalities. The approach introduces three types of prompts—modality-specific, task-aware, and task-specific—to mitigate catastrophic forgetting and improve adaptation efficiency. The method is evaluated on three datasets (CMU-MOSI, IEMOCAP, and CH-SIMS) and consistently outperforms state-of-the-art approaches, achieving higher average accuracy and lower forgetting measure. The approach is parameter-efficient, requiring only 2-3% of the parameters of the backbone network.

## Key Results
- The proposed method consistently outperforms state-of-the-art approaches on three datasets (CMU-MOSI, IEMOCAP, and CH-SIMS).
- The method achieves higher average accuracy and lower forgetting measure compared to existing methods.
- The approach is parameter-efficient, requiring only 2-3% of the parameters of the backbone network.

## Why This Works (Mechanism)
The method works by introducing three types of prompts—modality-specific, task-aware, and task-specific—to enable the model to learn intra-modality, inter-modality, intra-task, and inter-task features. The prompts are injected into the Multi-head Self-Attention (MSA) layers of a Multimodal Transformer backbone. The modality-specific prompts capture task-agnostic features, the task-aware prompts capture task-specific features based on missing modalities, and the task-specific prompts capture task-dependent features. A contrastive task interaction strategy is employed to explicitly learn prompts correlating different modalities, further improving performance.

## Foundational Learning
- **Multimodal Transformers:** Neural architectures that process and fuse information from multiple modalities (e.g., audio, video, text). Why needed: They serve as the backbone for processing multimodal data. Quick check: Verify the architecture can handle the input modalities and fusion dimensions.
- **Prompt-based Learning:** A technique that uses learnable prompts to guide the model's behavior without modifying the backbone. Why needed: It allows efficient adaptation to new tasks without catastrophic forgetting. Quick check: Ensure prompts are correctly attached to the MSA layers and updated during training.
- **Continual Learning:** A learning paradigm where the model learns from a sequence of tasks without forgetting previous ones. Why needed: It addresses the challenge of missing modalities in real-world applications. Quick check: Monitor forgetting measure (FM) to ensure previous tasks are not forgotten.

## Architecture Onboarding

**Component Map:**
Input -> Multimodal Transformer Backbone (frozen) -> Modality-specific Prompts -> Task-aware Prompts -> Task-specific Prompts -> Output

**Critical Path:**
The critical path is the flow of information from the input through the frozen Multimodal Transformer backbone, where the three types of prompts are injected into the MSA layers to generate the final output.

**Design Tradeoffs:**
- **Parameter Efficiency vs. Performance:** Using prompts instead of fine-tuning the entire backbone reduces parameters but may limit representational power.
- **Prompt Injection Strategy:** The sequential injection of prompts into different MSA layers allows for hierarchical feature learning but may introduce complexity in optimization.

**Failure Signatures:**
- **High Forgetting Measure (FM):** Indicates that the model is forgetting previous tasks, possibly due to incorrect prompt isolation or backbone updates.
- **Poor Average Accuracy (AA):** Suggests that the prompts are not effectively capturing the necessary features for the current task.

**First Experiments:**
1. Verify that the Multimodal Transformer backbone is completely frozen and only prompts are updated during training.
2. Check the generation of task-aware prompts (Eq. 2) to ensure correct element-wise multiplication with the missing keys.
3. Validate the contrastive loss computation to confirm that positive pairs are correctly formed and used for updating the prompts.

## Open Questions the Paper Calls Out
None

## Limitations
- The exact backbone architecture is unspecified, making it unclear whether reported performance can be matched without careful tuning of pre-trained encoder weights and fusion dimensions.
- The contrastive loss computation is ambiguous in a continual setting without replay buffers, particularly how positive pairs are formed and evaluated for previous tasks.
- The method's scalability to a larger number of tasks or modalities is not thoroughly investigated.

## Confidence
- **High:** Average Accuracy (AA) and Forgetting Measure (FM) trends across datasets.
- **Medium:** Prompt architecture design and parameter efficiency claims.
- **Low:** Exact reproducibility due to unspecified backbone and contrastive sampling.

## Next Checks
1. Implement and train the method with a specified Multimodal Transformer backbone (e.g., Hateful Memes Transformer) to verify if Upperbound performance is achievable.
2. Clarify and implement the contrastive loss computation for a single-task update without replay, ensuring correct positive pair sampling as per Eq. 7.
3. Run an ablation study removing $P_{TA}$ to quantify its contribution to the performance gain, confirming the necessity of the contrastive task interaction strategy.