---
ver: rpa2
title: Automating Violence Detection and Categorization from Ancient Texts
arxiv_id: '2503.08192'
source_url: https://arxiv.org/abs/2503.08192
tags:
- violence
- texts
- data
- violent
- eris
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the use of large language models (LLMs) for
  automating the detection and categorization of violence in ancient texts. The approach
  leverages the ERIS database of manually annotated violent events from Greek, Roman,
  and medieval literature.
---

# Automating Violence Detection and Categorization from Ancient Texts

## Quick Facts
- arXiv ID: 2503.08192
- Source URL: https://arxiv.org/abs/2503.08192
- Reference count: 15
- Primary result: Fine-tuned BERT and RoBERTa achieve F1-scores up to 0.93 for violence detection and 0.86 for categorization in ancient texts

## Executive Summary
This study evaluates large language models for automating violence detection and categorization in ancient texts using the ERIS database of manually annotated violent events from Greek, Roman, and medieval literature. The approach addresses binary classification of violent vs. non-violent passages and multi-class categorization across four dimensions: level of violence, context, motive, and long-term consequences. Fine-tuned BERT and RoBERTa models significantly outperform zero-shot GPT-4o mini, achieving F1-scores up to 0.93 for detection and 0.86 for fine-grained categorization. Data augmentation via paraphrasing improves recall from 0.78 to 0.99, which is critical for supporting manual annotation efforts.

## Method Summary
The methodology uses ERIS database annotations of 3,252 violent passages from ancient texts, paired with non-violent passages extracted from Plutarch biographies in the Perseus corpus. Binary classification models (BERT-large and RoBERTa-large) were fine-tuned on this data with an 80/20 train/test split, augmented by generating three paraphrased versions of each violent passage using GPT-4o mini. For multi-class categorization, separate BERT and RoBERTa models were trained for each of four dimensions (level, context, motive, consequences). Zero-shot GPT-4o mini served as a baseline. Evaluation used precision, recall, and F1-score metrics, with majority and random baselines for comparison.

## Key Results
- Fine-tuned BERT achieved F1-score of 0.93 for violence detection compared to 0.40 for BERT without fine-tuning
- Data augmentation improved BERT recall from 0.78 to 0.99 for minority class detection
- Multi-class categorization achieved F1-scores up to 0.86, with best performance on concrete categories like "Destruction/Devastation" and "Victory"
- Zero-shot GPT-4o mini underperformed fine-tuned models with F1-score of 0.71 for detection

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning transformer-based LLMs on domain-specific historical text substantially improves violence detection performance compared to zero-shot approaches. Pre-trained BERT and RoBERTa capture general linguistic patterns through bidirectional attention over token relationships. Fine-tuning on Plutarch biographies adapts learned representations to archaic syntax, violence-specific vocabulary (e.g., "spear," "slaughter"), and implicit contextual cues that general pre-training does not emphasize. The linguistic regularities in English translations of ancient texts share sufficient structure with modern English for transfer learning to be effective.

### Mechanism 2
LLM-based data augmentation via paraphrasing improves recall for minority classes in imbalanced datasets. Generating multiple paraphrases of violent passages using GPT-4o mini synthetically expands the positive class (461 violent vs 2103 non-violent). This reduces the model's bias toward the majority class, improving recall from 0.78 to 0.99 for BERT. Paraphrased examples preserve semantic content (violence presence, category labels) while providing lexical diversity that improves generalization.

### Mechanism 3
Multi-class categorization performance correlates with class frequency and conceptual concreteness. BERT/RoBERTa classifiers learn category boundaries from token patterns. Well-represented, concrete categories (e.g., "War/Military Campaign" with 181 samples; "Destruction/Devastation") have clearer token-level signatures. Abstract or rare categories (e.g., "Intrapersonal violence" with 17 samples; "Exile" with 6) lack sufficient exemplars and have overlapping features with other classes.

## Foundational Learning

- **Transformer attention and bidirectional context (BERT vs GPT)**: Understanding why BERT/RoBERTa (bidirectional) outperform GPT-4o mini (unidirectional, generative) for classification tasks requires grasping how masked language modeling creates representations that aggregate context from both directions. Can you explain why predicting a masked token using both left and right context produces representations better suited for classification than predicting the next token using only left context?

- **Precision-Recall trade-off in annotation support**: The paper emphasizes recall optimization (0.99 achieved) over precision because human annotators can efficiently reject false positives but struggle to find missed instances. Understanding this trade-off is essential for designing practical annotation assistance systems. In a pre-annotation system for human reviewers, would you optimize for precision or recall? Why does this differ from a fully automated pipeline?

- **Fine-tuning vs zero-shot evaluation**: The paper compares fine-tuned models against zero-shot GPT-4o mini as a baseline. Understanding what each approach assumes about training data availability and task specificity is critical for selecting the right strategy. If you had only 50 labeled violent passages instead of 461, would fine-tuning still be viable? What alternative approaches could you consider?

## Architecture Onboarding

- **Component map**: ERIS database (violent passages + metadata) → Perseus database (full source texts for negative sampling) → preprocessing pipeline (ERIS-Perseus alignment → non-violent extraction → train/test split → augmentation module) → model layer (BERT-large and RoBERTa-large with classification heads) → evaluation layer (precision, recall, F1-score)

- **Critical path**: Data preprocessing (ERIS-Perseus alignment) → fine-tuning BERT/RoBERTa on violence detection → augmentation → multi-dimensional categorization training → evaluation against held-out test set

- **Design tradeoffs**:
  - Fine-tuning vs API: Fine-tuning requires labeled data and infrastructure but yields 0.93 F1; GPT-4o mini API requires no training data but achieves only 0.71 F1
  - Augmentation vs class weighting: Augmentation improved recall for BERT but not RoBERTa; class weighting might be more efficient for larger datasets
  - Single vs separate classifiers: The paper trains separate BERT/RoBERTa models per categorization dimension rather than multi-task learning, trading parameter efficiency for interpretability
  - Binary detection first vs end-to-end: The two-stage approach (detect violence, then categorize) allows independent optimization but may propagate errors

- **Failure signatures**:
  - Zero recall for "as-is" RoBERTa: Without fine-tuning, RoBERTa predicts all non-violent; indicates model confidence calibration is severely misaligned for this domain
  - Low performance on rare classes: "Exile" (6 samples), "Coronation" (12 samples) have F1 < 0.80; class frequency directly limits learning
  - Conceptual confusion in overlapping categories: "Political" vs "Tactical" motives; "Jurisdictional" vs "Military" contexts—suggests taxonomy may need refinement or hierarchical modeling

- **First 3 experiments**:
  1. Reproduce violence detection baseline: Load BERT-large from HuggingFace, fine-tune on the provided Plutarch dataset (461 violent, 2103 non-violent), evaluate on 500-sample held-out test set. Target: F1 ≥ 0.83.
  2. Ablate data augmentation: Train identical models with and without GPT-4o mini paraphrase augmentation. Measure recall improvement and statistical significance using McNemar's test.
  3. Characterize rare-class failure modes: For each categorization dimension, isolate classes with <20 samples. Examine confusion matrices to identify if failures are random or systematically confused with semantically similar categories.

## Open Questions the Paper Calls Out

- Does incorporating surrounding textual context or external knowledge bases (via Retrieval Augmented Generation) improve the classification of abstract violence categories compared to analyzing passages in isolation? The authors suggest that "incorporating surrounding textual context... could further enhance classification performance" and that "exploring advanced techniques like retrieval augmented generation (RAG)" is necessary for identifying abstract categories.

- Can the fine-tuned models maintain high performance when applied to original ancient language texts (Greek/Latin) rather than modern English translations? The paper notes a limitation that experiments were conducted "only operate on translations rather than original texts," acknowledging this "might be a restriction for both text understanding and scaling."

- Can reinforcement learning from human feedback (RLHF) effectively reduce misclassification rates for ambiguous or overlapping violence categories? The authors suggest that "Developing dynamic models that can learn from continuous expert feedback through techniques like reinforcement learning from human feedback (RLHF) could also bridge this gap" regarding abstract or highly contextual categories.

## Limitations

- The paper does not report inter-annotator agreement for the ERIS labels, making it unclear whether poor model performance on rare or overlapping categories reflects true ambiguity or simply insufficient training data.
- All experiments use Plutarch biographies from the Perseus corpus, so claims about scalability across diverse ancient texts remain unproven without testing on texts from other authors, genres, or time periods.
- The data augmentation mechanism assumes paraphrased outputs preserve semantic content and violence intensity, but provides no quantitative validation that GPT-4o mini paraphrases maintain label integrity.

## Confidence

- High confidence: Fine-tuning improves detection (BERT F1 0.40→0.83; RoBERTa F1 0.87→0.93)
- Medium confidence: Augmentation improves recall (BERT recall 0.78→0.99)
- Medium confidence: Rare classes perform poorly due to low sample counts
- Low confidence: Claims about scalability across diverse ancient texts

## Next Checks

1. Validate Annotation Quality: Compute Cohen's kappa or Krippendorff's alpha on a subset of ERIS labels to establish human agreement baseline. Compare model performance on consistently vs. inconsistently labeled passages to assess whether poor performance stems from annotation ambiguity.

2. Test Cross-Domain Transfer: Fine-tune models on Plutarch, then evaluate on non-Plutarch texts (e.g., Caesar's Commentaries, Herodotus). Measure performance drop to quantify domain generalization limits and identify genre-specific linguistic features.

3. Augmentation Semantic Preservation: Sample 50 augmented passages per original. Have independent annotators rate violence level, context, and motive preservation on 5-point scales. Compute inter-rater agreement to verify augmentation maintains semantic integrity.