---
ver: rpa2
title: 'Interpretable graph-based models on multimodal biomedical data integration:
  A technical review and benchmarking'
arxiv_id: '2505.01696'
source_url: https://arxiv.org/abs/2505.01696
tags:
- graph
- each
- feature
- nodes
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents the first technical review and benchmarking
  of interpretable graph-based models for multimodal biomedical data integration.
  Analyzing 26 studies from 2019-2024, we find that most focus on disease classification
  (especially cancer) using static graphs with simple similarity measures, while graph-native
  explainers remain rare.
---

# Interpretable graph-based models on multimodal biomedical data integration: A technical review and benchmarking

## Quick Facts
- arXiv ID: 2505.01696
- Source URL: https://arxiv.org/abs/2505.01696
- Reference count: 40
- Primary result: First technical review and benchmarking of interpretable graph-based models for multimodal biomedical data integration, analyzing 26 studies and benchmarking four XAI methods on Alzheimer's disease data.

## Executive Summary
This study presents the first comprehensive technical review and benchmarking of interpretable graph-based models for multimodal biomedical data integration. Analyzing 26 studies from 2019-2024, the authors find that most focus on disease classification (especially cancer) using static graphs with simple similarity measures, while graph-native explainers remain rare. They categorize interpretability approaches into four families and benchmark four representative methods (sensitivity analysis, gradient saliency, SHAP, and graph masking) on an Alzheimer's disease dataset, showing distinct trade-offs between interpretability depth and computational cost.

## Method Summary
The study systematically reviews 26 papers from 2019-2024 on interpretable graph-based models for multimodal biomedical data integration, categorizing interpretability approaches into feature attribution, graph structure explanation, post-hoc visualization, and inherent interpretability families. For benchmarking, they use the ROSMAP dataset (351 samples) with pre-processed 200 features per modality (mRNA, DNA methylation, microRNA) and train MOGONet (Graph Convolutional Networks + View Correlation Discovery Network) on a 70/30 stratified split using a two-phase training procedure. Four XAI methods are evaluated on the test set and validated using Gene Ontology term enrichment (p ≤ 0.01) and permutation testing.

## Key Results
- Most studies focus on disease classification using static graphs with simple similarity measures (Pearson correlation, Euclidean distance)
- SHAP and sensitivity analysis recover more established disease pathways, while gradient saliency and graph masking highlight unique metabolic and transport signatures
- All four methods outperform random gene sets in permutation tests, with distinct trade-offs between interpretability depth and computational cost
- Graph construction remains the most critical and heuristic-dependent step in the pipeline

## Why This Works (Mechanism)

### Mechanism 1: Relational Fusion via Non-Euclidean Representation
Graph-based models improve diagnostic accuracy by preserving relational structures between heterogeneous modalities through mapping data to a graph where nodes represent entities and edges represent interactions or similarities. Models like GCNs and GATs aggregate neighbor information to capture intra- and inter-modality interactions simultaneously. The core assumption is that static similarity measures accurately reflect biological relevance, though this may be limited if edge definitions rely solely on simple similarity thresholds.

### Mechanism 2: Complementary XAI via Feature Attribution
Different explainability families reveal distinct biological signatures; combining them provides more comprehensive pathological views than any single method. SHAP/Sensitivity methods aggregate feature contributions for global views recovering established pathways, while gradient saliency/graph masking identify local specific dependencies highlighting unique signatures. The assumption is that established disease pathways represent ground truth for validation, though novel signatures require manual verification.

### Mechanism 3: Attention-based Inherent Interpretability
Attention mechanisms allow real-time inherent interpretability without post-hoc computational overhead by learning weights that quantify neighbor importance during training. These attention weights serve directly as explanations for node influence, though the assumption that high attention weights correlate with biological causality requires validation, and simple models may sacrifice accuracy for transparency.

## Foundational Learning

- **Concept: Static vs. Dynamic Graph Construction**
  - Why needed here: 84% of studies use static graphs, understanding limitations versus dynamic topology learning is critical for advancing performance
  - Quick check question: Does removing a modality change the similarity between nodes, potentially breaking the static graph structure? (Answer: Yes)

- **Concept: Message Passing / Aggregation**
  - Why needed here: Core GNN operation involves aggregating features from neighbors; misunderstanding leads to over-smoothing or mixing dissimilar information
  - Quick check question: If a patient node receives messages from dissimilar neighbors due to poor edge construction, what is the likely result? (Answer: Suboptimal performance/confusion)

- **Concept: Shapley Values (SHAP)**
  - Why needed here: Identified as superior for recovering known biology but computationally expensive; understanding the "why" helps justify cost
  - Quick check question: Why might SHAP be prohibitively expensive for a model with 600 features? (Answer: Requires evaluating the model on all possible combinations of features)

## Architecture Onboarding

- **Component map:** Input (Multimodal Data) -> Graph Builder (Node/Edge definition) -> Backbone (GNN Layers) -> Fusion (VCDN/Tensor Fusion) -> Explainer (Post-hoc/Inherent)
- **Critical path:** Graph Construction -> Modality-Specific Processing -> Fusion
- **Design tradeoffs:**
  - Interpretability vs. Compute: SHAP/Masking = Deep bio insight, weeks of compute. Gradient/Sensitivity = Coarse insight, fast execution
  - Static vs. Dynamic: Static = Simple, reproducible. Dynamic = Potentially higher performance, harder to interpret/train
  - Inherent vs. Post-hoc: Inherent (e.g., GAT) = Transparent but potentially lower accuracy. Post-hoc = High accuracy black-box + external explainer
- **Failure signatures:**
  - Modality Elimination Error: Removing features changes graph topology, making performance comparisons unfair
  - Gradient Saturation: Gradient-based saliency fails if model saturates (gradients ≈ 0), leading to misleading explanations
- **First 3 experiments:**
  1. Run 4 XAI methods on MOGONet + ROSMAP to verify recovery of known pathways (e.g., GPCR for AD) and compare compute time
  2. Construct graph using Pearson correlation vs. Learnable Distance to measure performance delta on classification task
  3. Implement GAT model and visualize attention edges; check alignment with known biological interactions

## Open Questions the Paper Calls Out

- **Open Question 1:** Can "graph-in-graph" hierarchical architectures improve diagnostic performance by simultaneously capturing patient-level molecular interactions and population-level relationships? Basis: Authors identify potential to explore graph-in-graph techniques. Evidence needed: Benchmarking hierarchical models against single-level baselines on multimodal datasets.

- **Open Question 2:** Does constructing graph edges using domain-specific knowledge graphs yield more biologically relevant biomarkers than standard static similarity measures? Basis: Paper identifies open area to explore knowledge-graph-based edge construction. Evidence needed: Comparative enrichment analysis showing higher overlap with known disease pathways.

- **Open Question 3:** How can modality importance be accurately evaluated when removing features alters underlying graph topology? Basis: Authors critique modality elimination as conflating feature loss with structural perturbations. Evidence needed: Development of attribution methods that decouple feature importance from topological changes.

## Limitations
- Lack of causal reasoning in current interpretability methods, relying heavily on statistical associations
- Static graph construction (84% of studies) may capture spurious correlations rather than true biological relationships
- Computational expense remains prohibitive for some methods, with SHAP requiring "several weeks" on CPU for large feature sets

## Confidence

- **High confidence:** Static graph construction prevalence (84%), classification task dominance, and general categorization of interpretability families
- **Medium confidence:** Specific biological pathway recovery claims, assuming established pathways represent complete ground truth
- **Low confidence:** Novel biological signatures identified by gradient saliency and graph masking, requiring manual validation through literature review

## Next Checks

1. **Manual validation of "unique" signatures:** Select top 10 genes identified by gradient saliency as "novel metabolic signatures" and verify relevance to Alzheimer's disease through PubMed literature review
2. **Causal reasoning test:** Modify static graph construction to incorporate directional information from biological knowledge graphs and measure impact on classification accuracy and interpretability quality
3. **Cross-dataset generalizability:** Apply same XAI methods to ADNI dataset to verify consistency of identified pathways and signatures across cohorts