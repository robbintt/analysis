---
ver: rpa2
title: 'Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic
  Interpretability -- The Strange Science Part I.ii'
arxiv_id: '2505.01372'
source_url: https://arxiv.org/abs/2505.01372
tags:
- explanation
- explanations
- explanatory
- virtues
- interpretability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Explanatory Virtues Framework, which
  systematically evaluates Mechanistic Interpretability (MI) explanations using criteria
  from philosophy of science. The framework identifies key virtues including Simplicity,
  Unification, Co-Explanation, and Nomological Principles that are often neglected
  in current MI methods.
---

# Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability -- The Strange Science Part I.ii

## Quick Facts
- arXiv ID: 2505.01372
- Source URL: https://arxiv.org/abs/2505.01372
- Authors: Kola Ayonrinde; Louis Jaburi
- Reference count: 37
- This paper introduces the Explanatory Virtues Framework, which systematically evaluates Mechanistic Interpretability (MI) explanations using criteria from philosophy of science.

## Executive Summary
This paper presents a pluralist Explanatory Virtues Framework that systematically evaluates Mechanistic Interpretability explanations by operationalizing philosophical criteria from Bayesian, Kuhnian, Deutschian, and Nomological perspectives. The framework identifies twelve explanatory virtues including Simplicity, Unification, Co-Explanation, and Nomological Principles that are often neglected in current MI methods. Analysis of four MI methods reveals that while they value falsifiability and accuracy, they generally lack unification and nomological principles. The authors argue that adopting these virtues would constitute methodological progress for the field.

## Method Summary
The framework decomposes explanation quality into twelve virtues with mathematical definitions enabling quantitative comparison. Each virtue is computed from an explanation E and background theory B using observational data xT (training) and xI (inference-time). The method involves defining your explanation, computing empirical and theoretical virtues, testing falsifiability and hard-to-varyness, and mapping results to a three-level rubric (✓/●/✗). The approach was validated by analyzing four MI methods (Clustering, SAEs, Causal Circuits, Compact Proofs) against the rubric to identify systematic gaps in current methodologies.

## Key Results
- Current MI methods systematically underperform on Unification and Nomological Principles, scoring "✗" (Not Virtuous) across Clustering, SAEs, and Circuits
- SAEs optimize for Accuracy and Parsimony but lack Hard-to-Varyness and fail to constrain features to causally relevant variables
- Compact Proofs provide a unified evaluation mechanism but face scaling challenges with larger models and superposition noise
- The framework identifies promising research directions including clearer definitions of explanatory simplicity and focusing on unifying explanations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Explanatory Virtues Framework enables systematic comparison between competing mechanistic explanations by operationalizing philosophical criteria into evaluable dimensions.
- Mechanism: The framework decomposes explanation quality into discrete virtues (Precision, Unification, Simplicity, Hard-to-Varyness, Nomologicity) derived from four philosophical accounts. Each virtue has a mathematical definition enabling quantitative comparison—for example, Unification is computed as `Unif(E) = Prec(E) - Power(E)`, capturing how well an explanation connects disparate observations beyond individual predictions.
- Core assumption: Properties that are truth-conducive in general scientific explanation remain truth-conducive when applied to neural network internals.
- Evidence anchors: [abstract] "We introduce a pluralist Explanatory Virtues Framework drawing on four perspectives from the Philosophy of Science" [Section 3, pp. 3-7] Provides mathematical definitions for each virtue and the DAG structure showing their relationships.
- Break condition: If different philosophical accounts yield contradictory rankings of the same explanations, the framework provides no principled weighting scheme for resolving tradeoffs.

### Mechanism 2
- Claim: Compact Proofs provide a unified evaluation mechanism by translating mechanistic explanations into verifiable performance bounds, where better explanations yield tighter bounds with lower verification cost.
- Mechanism: Given a model M and explanation E, a verifier program V(θ, E) produces a lower bound on model performance. Faithful explanations compress the proof space—Gross et al. (2024) showed that causal-mechanistic explanations enable non-vacuous bounds where brute-force proofs fail. The mechanism exploits the trade-off between bound tightness (Accuracy) and proof complexity (Simplicity), pushing the Pareto frontier.
- Core assumption: Explanation quality correlates with proof efficiency—mechanisms that "make sense" should admit shorter formal justifications.
- Evidence anchors: [Section 4.2, p. 10] "Gross et al. (2024) show that faithful mechanistic explanations lead to tighter performance bounds and more efficient (i.e. simpler) proofs."
- Break condition: If scaling to larger models with superposition noise yields only vacuous bounds, the mechanism fails to provide actionable evaluation signals.

### Mechanism 3
- Claim: Current MI methods systematically underperform on Unification and Nomologicity, creating a methodological gap that limits explanatory progress.
- Mechanism: The authors analyze four methods (Clustering, SAEs, Causal Circuits, Compact Proofs) against the rubric. SAEs optimize for Accuracy and Parsimony but lack Hard-to-Varyness (features can be added without justification). Circuits satisfy Falsifiability but don't compose across tasks. The mechanism identifies that methods treat observations in isolation rather than seeking shared explanatory building blocks.
- Core assumption: Scientific fields that adopt nomological principles progress faster than those that remain purely idiographic (cataloguing observations without general laws).
- Evidence anchors: [Table 1, p. 11] Shows Unification and Nomological scoring "✗" (Not Virtuous) across Clustering, SAEs, and Circuits.
- Break condition: If task-specific explanations are fundamentally more useful than unified ones for AI safety applications, then pursuing unification would be misaligned with practical goals.

## Foundational Learning

- Concept: **Theory Choice in Philosophy of Science**
  - Why needed here: The entire framework builds on Kuhn's question—"Given two competing explanatory theories, which should we prefer?"—and extends it to MI.
  - Quick check question: Can you explain why falsifiability alone is insufficient for choosing between two falsifiable but incompatible explanations?

- Concept: **Causal Abstraction / Constructive Abstraction**
  - Why needed here: The paper defines valid MI explanations as requiring Causal-Mechanistic structure, formalized through constructive abstraction mappings between low-level network variables and high-level causal models.
  - Quick check question: Given a neural network layer and a hypothesized high-level variable (e.g., "is thinking about animals"), how would you test whether a constructive abstraction exists?

- Concept: **Kolmogorov Complexity and Description Length**
  - Why needed here: Simplicity is operationalized via K-complexity (shortest program generating the explanation) and Shannon description length—used to distinguish Parsimony, Conciseness, and Complexity as distinct simplicity measures.
  - Quick check question: Why might an explanation with fewer entities (parsimony) still have higher K-complexity than one with more entities?

## Architecture Onboarding

- Component map:
  - Explanatory Virtues Rubric (Table 2): Three-level assessment (✓/●/✗) for 12 virtues across Validity, Bayesian, Kuhnian, Deutschian, and Nomological categories
  - Virtue DAG (Figure 1): Shows dependencies—e.g., Unification depends on Precision and Power; Fruitfulness is starred as critical
  - Mathematical Definitions (Section 3): Canonical formulas for each virtue enabling quantitative comparison
  - Method Evaluation Table (Table 1): Pre-computed assessments for four MI methods as templates

- Critical path:
  1. Define your explanation E and background theory B
  2. Identify training data xT and held-out inference data xI
  3. Compute empirical virtues (Accuracy, Descriptiveness, Co-Explanation) on xT
  4. Compute theoretical virtues (Precision, Power, Unification, Simplicity) via expectations or complexity measures
  5. Test Falsifiability by specifying what observations would contradict E
  6. Test Hard-to-Varyness by checking if small modifications preserve accuracy
  7. Map results to rubric levels and compare against alternatives

- Design tradeoffs:
  - Accuracy vs. Simplicity: The Pareto frontier matters—maximizing one at extreme cost to the other yields poor explanations
  - Unification vs. Precision: Unifying explanations may make weaker predictions per phenomenon to cover more phenomena
  - Nomologicity vs. Causal-Mechanisticity: General laws may abstract away from step-by-step mechanisms

- Failure signatures:
  - **Interpretability Illusion**: High mundane accuracy but low fruitfulness—explanation fits training data but fails on distribution shift or adversarial tests
  - **Ad-hocness**: Easy-to-vary explanations where adding "epicycles" doesn't degrade performance
  - **Cataloguing Trap**: High descriptiveness but zero unification—generating endless feature lists without compositional structure

- First 3 experiments:
  1. **Rubric validation**: Take two existing MI explanations for the same model/behavior, apply the rubric blindly, and check if the ranking aligns with community consensus or downstream utility
  2. **Simplicity measure comparison**: For a fixed SAE explanation, compute Parsimony (L0), Conciseness (description length from MDL-SAE), and K-complexity (approximate via compression). Test which best predicts out-of-distribution faithfulness
  3. **Unification stress test**: Identify a model with known shared subcircuits (e.g., induction heads used across tasks). Evaluate whether your explanation method recovers the shared structure or produces redundant task-specific explanations. Score against the Co-Explanation virtue

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should explanatory simplicity be formally defined in Mechanistic Interpretability, and which measure (Parsimony, Conciseness, or K-Complexity) best captures truth-conduciveness?
- Basis in paper: [explicit] The authors state "appropriately characterising an explanatory Simplicity measure is currently an open question for interpretability" (Section 5) and list "clearly defining explanatory simplicity" as a fruitful research direction.
- Why unresolved: Multiple competing notions of simplicity exist (Parsimony counts entities, Conciseness measures description length, K-Complexity measures shortest generating program), and they may rank explanations differently without clear theoretical justification for preferring one.
- What evidence would resolve it: Empirical studies comparing how different simplicity measures correlate with explanation generalization (fruitfulness) and faithfulness across multiple MI methods and tasks.

### Open Question 2
- Question: How can Compact Proofs be scaled to larger models with superposition noise while maintaining non-vacuous bounds?
- Basis in paper: [explicit] The authors state "At present it is not known how to scale the Compact Proofs methodology to much larger models with additional superposition noise and still get informative, non-vacuous bounds. This remains an open problem and a gold standard for evaluating explanations" (Section 4.2).
- Why unresolved: Larger models have more complex internal representations with superposition, making it computationally expensive to produce tight performance bounds with compact proofs.
- What evidence would resolve it: Development of scalable verification methods that produce non-vacuous bounds on frontier models (e.g., 7B+ parameter LLMs) with demonstrated computational efficiency.

### Open Question 3
- Question: How can SAE explanations be made hard-to-vary and constrained to pick out features causally relevant to downstream behavior?
- Basis in paper: [explicit] The authors state "it still remains an open question as to how to ensure that SAE explanations are truly hard-to-vary and pick out features which are causally relevant to the downstream behaviour of the model" (Section 4.1.2).
- Why unresolved: SAE features activated for reconstruction may have little effect on downstream performance, and expanding feature dictionaries without justification introduces ad-hocness.
- What evidence would resolve it: Methods that constrain SAE training to produce features where small modifications consistently degrade downstream task performance, demonstrating load-bearing causal roles.

## Limitations

- The framework's practical applicability is limited by the lack of standardized benchmarks for most virtues, particularly Hard-to-Varyness and Nomologicity
- The three-level rubric (✓/●/✗) requires subjective judgment calls without clear quantitative thresholds, making cross-study comparisons difficult
- The analysis in Table 1 is largely qualitative and meta-analytic rather than based on systematic empirical evaluation of specific explanations

## Confidence

- **High confidence**: The framework's decomposition of explanation quality into discrete virtues is methodologically sound and addresses genuine gaps in current MI evaluation
- **Medium confidence**: The specific virtue formulations and mathematical definitions provide actionable guidance, though their empirical validity across diverse MI methods needs validation
- **Medium confidence**: The identification of unification and nomological principles as neglected virtues in MI is correct, but whether prioritizing these virtues improves practical outcomes remains unproven

## Next Checks

1. **Empirical rubric validation**: Apply the framework to compare two competing MI explanations for the same phenomenon (e.g., different circuit analyses of induction heads) and verify that the ranking aligns with downstream utility measures like steering success or adversarial robustness

2. **Cross-method comparison standardization**: Replicate Table 1 evaluations for SAE explanations across multiple research groups using standardized metric thresholds, measuring inter-rater agreement and identifying systematic disagreements

3. **Out-of-distribution testing**: For explanations scoring high on Accuracy but low on Unification, test their performance on distribution-shifted data or novel tasks to empirically validate the importance of unification for generalization