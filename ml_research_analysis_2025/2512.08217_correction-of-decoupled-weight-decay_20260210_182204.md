---
ver: rpa2
title: Correction of Decoupled Weight Decay
arxiv_id: '2512.08217'
source_url: https://arxiv.org/abs/2512.08217
tags:
- weight
- decay
- learning
- rate
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper challenges the assumption that decoupled weight decay\
  \ should be proportional to the learning rate (\u03B3) in adaptive optimizers like\
  \ AdamW. The authors argue that the previously proposed orthogonality-based reasoning\
  \ is flawed and derive that decoupled weight decay should instead be proportional\
  \ to \u03B3\xB2 for stable weight norms."
---

# Correction of Decoupled Weight Decay

## Quick Facts
- **arXiv ID:** 2512.08217
- **Source URL:** https://arxiv.org/abs/2512.08217
- **Authors:** Jason Chuan-Chih Chou
- **Reference count:** 40
- **Primary result:** Decoupled weight decay should scale as λ ∝ γ² (not λ ∝ γ) for stable weight norms in adaptive optimizers, contrary to prior assumptions.

## Executive Summary
This paper challenges the long-standing assumption that decoupled weight decay should be proportional to the learning rate in adaptive optimizers like AdamW. Through steady-state analysis, the authors derive that weight decay should instead be proportional to γ² for stable weight norms, where γ is the learning rate. They generalize this correction to the Scion optimizer and propose ScionC, which maintains stable weight and gradient norms while achieving comparable or better performance than the baseline. The work also demonstrates that momentum scheduling can replace cosine learning rate decay for equivalent effective learning rate decay, providing practical implications for training stability.

## Method Summary
The paper derives that for decoupled weight decay to maintain stable weight norms at steady state, the decay coefficient must scale as λ ∝ γ rather than remaining constant, making the effective decay term γλ ∝ γ². This is based on the assumption that parameter updates become independent of the weights at steady state. For the Scion optimizer with normalized updates and momentum α, the effective learning rate becomes γ_eff = γ√((2-α)/α), which better characterizes transfer across momentum values. ScionC implements these corrections with layer-specific LMOs (Sign, Spectral, Bias) and applies the corrected weight decay scaling to all layers except the output layer, where the independence assumption fails due to cross-entropy loss dynamics.

## Key Results
- ScionC achieves comparable or better performance than baseline Scion on ImageNet-1k and 124M Modded-NanoGPT while maintaining stable weight and gradient norms
- Eliminating the perpendicular component of updates has negligible impact on training dynamics, refuting orthogonality-based explanations for weight decay scaling
- Momentum scheduling can replace cosine learning rate decay for equivalent effective learning rate decay, though the weight norm approximation breaks down when α → 1
- The corrected weight decay scaling requires higher maximum values than uncorrected (tested λ ∈ {0.1, 0.2} for AdamC vs. standard values)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decoupled weight decay magnitude should scale as λ ∝ γ (making the effective decay term γλ ∝ γ²) for stable weight norms at steady state, contrary to the standard constant-λ practice.
- **Mechanism:** Under a random walk model where updates become independent of weights at steady state (E[⟨θ_{t-1}, u_t⟩] = 0), the expected squared weight norm is E[||θ_t||²] ≈ γC/(2λ). For this to remain stable when γ changes, λ must scale proportionally to γ.
- **Core assumption:** Updates become independent of weights at steady state regardless of optimizer type.
- **Evidence anchors:**
  - [abstract] "we derive that decoupled weight decay ∝ γ² results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state"
  - [Section 1.2] Derivation showing E[||θ_t||²] = γC/(λ(2-γλ)) ≈ γC/(2λ)
  - [corpus] Related work "Why Gradients Rapidly Increase Near the End of Training" (Defazio 2025) independently proposes similar λ ∝ γ correction (AdamC), providing convergent evidence.
- **Break condition:** If updates remain correlated with weights at steady state (e.g., output layer with cross-entropy loss where E[⟨θ_{t-1}, u_t⟩] ≠ 0), this scaling no longer applies.

### Mechanism 2
- **Claim:** The perpendicular component of parameter updates (u_t⊥) makes negligible contribution to weight norm dynamics in pre-norm transformers, refuting orthogonality-based explanations for weight decay scaling.
- **Mechanism:** A "Renormalized" AdamW variant that explicitly removes u_t⊥'s contribution to weight norm shows virtually identical validation accuracy (77.15% vs 77.45%), weight norms, and gradient norms compared to baseline AdamW.
- **Core assumption:** ViT-S/16 on ImageNet-1k is representative of transformer training dynamics.
- **Evidence anchors:**
  - [abstract] "eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics"
  - [Section 1.1, Figure 1] Shows near-identical curves for weight norm, gradient norm, and accuracy between Renormalized and standard AdamW.
  - [corpus] No direct corpus evidence; this appears to be a novel refutation of Kosson et al. (2024) and Defazio (2025) orthogonality arguments.
- **Break condition:** May not hold for architectures without normalization layers or different layer types not tested.

### Mechanism 3
- **Claim:** For Scion with normalized updates and momentum α, the effective learning rate γ_eff = γ√((2-α)/α) better characterizes the Total Update Contribution (TUC) and transfers optimally across momentum values.
- **Mechanism:** Normalized updates create autocorrelation E[⟨u_{t-k}, u_t⟩] ≈ C²(1-α)^k due to momentum. This modifies the steady-state equation, introducing the momentum-dependent effective learning rate. The corrected weight decay coefficient becomes λ = (2-α)/(2αC²_l) · γ for stability.
- **Core assumption:** Minibatch gradients become independent with time-independent expected L2 norm at steady state.
- **Evidence anchors:**
  - [Section 2.2] Full derivation of E[||θ_t||²] ≈ γ²_eff C²/(2η) where γ_eff = γ√((2-α)/α)
  - [Figure 2] Shows optimal γ_eff clusters within factor of 2 across momentum values α ∈ [0.01, 0.5], while optimal γ varies more widely.
  - [corpus] Weak direct corpus support; Muon optimizer (Jordan et al. 2024b) uses spectral normalization without explicit weight decay, aligning with the theme of normalized updates.
- **Break condition:** When momentum approaches 1.0, the approximation η ≪ α breaks down; when α is very small, the effective learning rate becomes large.

## Foundational Learning

- **Concept:** Decoupled weight decay vs. L2 regularization
  - **Why needed here:** The paper builds on AdamW's decoupled formulation where weight decay is applied directly to weights (θ ← θ - γλθ) rather than added to the loss gradient. Understanding this distinction is essential for following the steady-state derivation.
  - **Quick check question:** In AdamW, does weight decay multiply the learning rate γ, or is it applied independently?

- **Concept:** Steady-state analysis in stochastic optimization
  - **Why needed here:** The core derivation assumes training reaches a statistical steady state where E[||θ_t||²] = E[||θ_{t-1}||²]. The weight decay correction emerges from this equilibrium condition.
  - **Quick check question:** At steady state, why does the expected weight norm stop changing even with ongoing updates?

- **Concept:** Linear Minimization Oracles (LMOs) in Scion
  - **Why needed here:** Scion uses layer-specific LMOs (Sign, Spectral, Bias) that normalize updates to unit norm in the steepest descent direction. This normalization is what creates momentum-dependent update correlations not present in AdamW.
  - **Quick check question:** What does the Spectral LMO return for a matrix weight A?

## Architecture Onboarding

- **Component map:** Sign LMO (embeddings/output) -> Spectral LMO (linear layers) -> Bias LMO (bias terms) -> Momentum buffer m_t -> Learning rate γ and radius ρ_l per layer -> Weight decay λ_t,l

- **Critical path:**
  1. Identify layer types and assign appropriate LMOs (Table 1)
  2. Set target steady-state norm squared C²_l per layer (paper uses C²_l = 1.1875 for non-Sign layers)
  3. Apply λ ∝ γ scaling for all layers except output layer (where E[⟨θ_{t-1}, u_t⟩] ≠ 0)
  4. Optionally schedule C²_t,l (cosine decay) or momentum α_t instead of learning rate

- **Design tradeoffs:**
  - **Constant vs. scheduled C²_l:** ScionC (constant) maintains stable norms but may underperform slightly vs. Scion baseline; ScionC (cosine) matches baseline performance while maintaining norm stability
  - **Momentum scheduling vs. learning rate decay:** Can replace cosine LR decay with momentum scheduling for equivalent γ_eff decay, but weight norm approximation breaks down when α → 1
  - **Higher weight decay needed:** Corrected scaling requires higher maximum λ than uncorrected (paper tested λ ∈ {0.1, 0.2} for AdamC vs. standard values)

- **Failure signatures:**
  - Gradient norm surge near end of training with standard AdamW (Figure 1, 5) → indicates weight decay mismatch with LR decay
  - Output layer weight norm continuously increasing → indicates λ ∝ γ should not be applied to output layer
  - Unstable weight/gradient norms throughout training → indicates C²_l may be set incorrectly

- **First 3 experiments:**
  1. **Baseline comparison:** Train ViT-S/16 on ImageNet-1k for 90 epochs with {AdamW, AdamC, Scion, ScionC (constant)} to validate that corrected weight decay maintains stable norms with comparable accuracy
  2. **Momentum transfer sweep:** Vary momentum α ∈ {0.01, 0.02, 0.1, 0.5} with fixed γ_eff to verify that optimal effective learning rate transfers across momentum values (replicate Figure 2)
  3. **C²_l schedule ablation:** Compare ScionC (constant C²_l) vs. ScionC (cosine decay of C²_l) to determine if performance benefits from terminal weight norm suppression

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the insensitivity of training dynamics to the perpendicular component of the update hold for non-transformer architectures such as CNNs?
- Basis in paper: [explicit] Section 1.1 states, "Although we cannot exclude the possibility that the balancing effects of AdamW are important for training other classes of models, this contradicting evidence... cast[s] doubt on their importance in general."
- Why unresolved: The authors' experiments were limited to pre-norm transformers (ViT, Llama-style), leaving the behavior of CNNs or other architectures unverified.
- What evidence would resolve it: Replicating the "Renormalized" AdamW experiment on architectures like ResNet to observe if the perpendicular component remains negligible.

### Open Question 2
- Question: Can the steady-state weight norm derivation be rigorously extended to the Spectral LMO without relying on heuristic similarity to L2 norms?
- Basis in paper: [explicit] Section 2.2 notes regarding the Spectral LMO: "It is much more difficult to analyze the dynamics of $u_t$... We postulate that the dynamics... would be similar [to L2-based LMOs]."
- Why unresolved: The theoretical justification for ScionC currently relies on a postulate rather than a direct mathematical derivation for the Spectral norm case.
- What evidence would resolve it: A formal derivation of the steady-state norm for the Spectral LMO or empirical proof that the dynamics diverge from the postulated L2 behavior.

### Open Question 3
- Question: Why does AdamC fail to reach steady state in Vision Transformers even after extended training (300 epochs)?
- Basis in paper: [inferred] Section 3.2 notes that "the model trained with AdamC does not seem to be in steady state even after 300 epochs," despite the core theoretical contribution relying on steady-state assumptions.
- Why unresolved: The disconnect between the theoretical assumption (steady state is reached) and the empirical observation (AdamC does not reach it) suggests the scaling law may apply only in specific, unmet training regimes.
- What evidence would resolve it: Analysis of longer training runs or alternative hyperparameters to determine if AdamC eventually reaches the predicted steady state.

### Open Question 4
- Question: Is there an optimal corrected weight decay strategy for the output layer specifically, given the violation of the update-weight independence assumption?
- Basis in paper: [inferred] Appendix D argues that for the output layer, $\mathbb{E}[\langle \theta_{t-1}, u_t \rangle] \neq 0$ due to cross-entropy loss dynamics, suggesting the standard correction should not apply.
- Why unresolved: While the paper suggests avoiding the correction for the output layer, it does not propose a specific alternative scaling rule that accounts for the increasing correlation between weights and updates.
- What evidence would resolve it: A derivation of the steady-state norm specifically for the output layer case or an ablation study testing various decay scalings ($\lambda \propto \gamma^k$) for the final linear layer.

## Limitations

- **Orthogonality assumption failure**: The paper strongly refutes orthogonality-based explanations for weight decay scaling, but the Renormalized AdamW experiment only tests ViT-S/16 on ImageNet-1k. The mechanism may not generalize to architectures without normalization layers or different layer types.
- **Output layer exception**: The derivation assumes E[⟨θ_{t-1}, u_t⟩] → 0 at steady state, but output layers with cross-entropy loss violate this (E[⟨θ_{t-1}, u_t⟩] ≠ 0). The paper applies λ ∝ γ correction to output layers but acknowledges this may be inappropriate.
- **Steady-state approximation**: The λ ∝ γ scaling relies on steady-state assumptions that may break down during early training or when momentum approaches 1.0. The approximation η ≪ α used for Scion's momentum analysis may fail for high momentum values.

## Confidence

- **High confidence**: The core steady-state derivation showing λ ∝ γ² stability is mathematically sound under stated assumptions. The effective learning rate transfer across momentum values (γ_eff = γ√((2-α)/α)) is well-supported by Figure 2.
- **Medium confidence**: The experimental validation on ImageNet-1k and NanoGPT shows comparable performance, but the sample size (single architecture/task pairs) limits generalizability. The Renormalized AdamW experiment provides strong evidence against orthogonality explanations but only tests one model-dataset combination.
- **Low confidence**: The claim that momentum scheduling can fully replace cosine learning rate decay for equivalent effective decay is only partially supported. The paper notes the weight norm approximation breaks down when α → 1, but doesn't fully characterize the limits of this substitution.

## Next Checks

1. **Architecture generalization test**: Validate the Renormalized AdamW findings across multiple architectures (RNNs, CNNs, MLPs) without normalization layers to confirm the perpendicular component's negligible contribution is a general phenomenon.

2. **Output layer ablation**: Train with and without λ ∝ γ scaling on output layers to quantify the performance impact of this exception and test whether separate output layer treatment is necessary.

3. **High momentum regime analysis**: Systematically test momentum values approaching 1.0 to characterize when the steady-state approximations break down and identify safe operating regimes for the corrected scaling.