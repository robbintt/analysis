---
ver: rpa2
title: Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation
  Function
arxiv_id: '2509.01874'
source_url: https://arxiv.org/abs/2509.01874
tags:
- arxiv
- bilinear
- activation
- mnist
- mlps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Signed Quadratic Shrink (SQS), an activation
  function for Gated Linear Units (GLUs) that enables weight-based interpretability
  while maintaining competitive performance with state-of-the-art activation functions.
  The core method idea is to modify the quadratic activation function with a signed
  and shrunk formulation that preserves the bilinear weight structure needed for eigen-decomposition-based
  interpretability.
---

# Preserving Bilinear Weight Spectra with a Signed and Shrunk Quadratic Activation Function

## Quick Facts
- arXiv ID: 2509.01874
- Source URL: https://arxiv.org/abs/2509.01874
- Reference count: 40
- Primary result: SQS-GLUs achieve performance competitive with state-of-the-art activations while enabling weight-based interpretability through preserved spectral structure

## Executive Summary
This paper introduces Signed Quadratic Shrink (SQS), a novel activation function for Gated Linear Units (GLUs) that enables weight-based interpretability while maintaining competitive performance. The key innovation is a signed and shrunk quadratic formulation that preserves the bilinear weight structure needed for eigen-decomposition-based interpretability. SQS achieves performance on par with or better than ReLU, GELU, and SwiGLU on MNIST, Fashion-MNIST, and Tiny Stories datasets, while generating interpretable eigenvectors through eigen-decomposition that reveal meaningful features corresponding to different classes.

## Method Summary
SQS modifies the standard GLU activation by introducing a signed shrink function: σ(x) = (x - c·sgn(x)) / (1 + λ·x·sgn(x)), where c=0.01, λ=0.5, and p=1. This formulation preserves the symmetric interaction matrix structure required for eigen-decomposition while addressing gradient pathologies of raw quadratic activations. The method is evaluated on MNIST, Fashion-MNIST, and Tiny Stories using 2-layer MLPs and 4-layer Transformers with standard training procedures including AdamW optimizer, cosine annealing, and noise injection during training.

## Key Results
- SQS-GLUs achieve lower loss and faster convergence than ReLU-GLUs and Bilinear MLPs on MNIST and Fashion-MNIST
- Final accuracy comparable to GELU and SwiGLU while maintaining spectral structure for interpretability
- Eigenvectors from SQS-GLUs show high cosine similarity (>0.5, up to 0.95) with Bilinear MLP eigenvectors, indicating preserved spectral properties
- Visual inspection reveals eigenvectors correspond to semantically meaningful class features (digits/clothing items)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SQS preserves the symmetric interaction matrix structure required for eigen-decomposition-based interpretability while introducing beneficial non-linearity
- Core assumption: The activation function's modification preserves sufficient spectral structure for interpretability even when not mathematically identical to pure bilinear forms
- Evidence anchors: Abstract claim of competitive performance with interpretability, section 4.1 showing high cosine similarity between eigenvectors, related work confirms weight-based interpretation remains underexplored
- Break condition: If λ > 1.0 or c > 0.1, spectral similarity degrades below meaningful thresholds

### Mechanism 2
- Claim: The signed shrink formulation mitigates gradient pathologies of raw quadratic activations while maintaining directional information
- Core assumption: The quasi-linear regime at extremes provides sufficient gradient flow without destabilizing training
- Evidence anchors: Section 3.1 describes quasi-linear behavior for |x| << 1 and |x| > 10, visual comparison shows bounded behavior vs. unbounded x², no direct corpus confirmation of shrink mechanism benefits
- Break condition: If p > 1, computational cost increases without clear gradient benefit; if λ ≈ 0 and c ≈ 0, reverts to pathological quadratic behavior

### Mechanism 3
- Claim: SQS-GLUs learn eigenfeatures that correspond to semantically meaningful class attributes at rates competitive with state-of-the-art activations
- Core assumption: Eigen-decomposition of per-output interaction matrices yields features that align with human-interpretable concepts
- Evidence anchors: Section 4.1 shows eigenvectors visually correspond to distinct classes, section 4.2.1 demonstrates faster convergence and lower loss, related work supports bilinear structures for interpretable manifolds
- Break condition: On more complex datasets, eigenfeatures may capture statistical regularities rather than semantic concepts

## Foundational Learning

- Concept: **Eigen-decomposition of symmetric matrices**
  - Why needed here: The entire interpretability claim rests on decomposing A_a = ∑λ_i v_i v_i^T to extract eigenvector features
  - Quick check question: Can you explain why only the symmetric component of A_a contributes to g(x)_a = x^T A_a x?

- Concept: **Gated Linear Units (GLUs)**
  - Why needed here: SQS is designed specifically as a GLU activation; understanding the gate structure (Wx) ⊙ σ(Vx) is prerequisite
  - Quick check question: What happens to the gradient through a GLU when σ(Vx) ≈ 0 for many inputs?

- Concept: **Gradient flow through element-wise activations**
  - Why needed here: The design motivation comes from fixing quadratic activation gradient issues; you need to understand why x² vanishes/explodes
  - Quick check question: For f(x) = x², what is df/dx when x = 0.001 versus x = 100?

## Architecture Onboarding

- Component map:
Input x → [Branch 1: Wx] ─────────────────────┐
                                              ├─→ ⊙ (elementwise) → Output
       → [Branch 2: Vx] → [SQS activation] ───┘

- Critical path:
  1. Implement SQS as a numerically stable kernel (handle ||x|| ≈ 0)
  2. Replace standard GLU activation (SwiGLU/GeGLU) with SQS
  3. After training, extract W and V per layer, form interaction matrices A_a per output
  4. Eigen-decompose each A_a to recover interpretable features

- Design tradeoffs:
  - **λ (shrink rate, default 0.5)**: Higher values → more linear behavior, less expressivity; lower → approaches raw quadratic
  - **c (shift, default 0.01)**: Higher → smoother near zero but may blur interpretability; lower → preserves spectral clarity
  - **p (norm exponent, default 1)**: p=1 is fastest; p>1 adds computation without clear benefit in current experiments

- Failure signatures:
  - Training loss diverges → λ too low (insufficient shrink) or learning rate too high for quadratic-like regime
  - Eigenfeatures appear as noise → c too high, or model dimension insufficient for feature separation
  - Slower than baseline → p > 1 or non-optimized implementation; use Triton kernel per Appendix C

- First 3 experiments:
  1. **Sanity check**: Train 2-layer MLP on MNIST with SQS-GLU (λ=0.5, c=0.01, p=1). Verify eigenfeatures visually match digit classes per Figure 2 protocol
  2. **Ablation**: Compare convergence when varying λ ∈ {0.1, 0.5, 1.0} while holding c=0.01. Plot loss curves against ReLU-GLU baseline
  3. **Spectral fidelity**: Compute cosine similarity between SQS-GLU eigenvectors and Bilinear MLP eigenvectors on same checkpoint. Target: >0.5 for top-3 eigenvectors

## Open Questions the Paper Calls Out
None specified in the provided content

## Limitations
- Limited evaluation on simple datasets (MNIST, Fashion-MNIST, Tiny Stories) without testing on more complex benchmarks
- Lack of systematic ablation studies on hyperparameter sensitivity (λ, c, p ranges)
- Interpretability claims rely primarily on visual inspection rather than quantitative metrics

## Confidence
- **High**: The SQS activation function definition and its gradient properties are mathematically sound
- **Medium**: Performance parity with SOTA activations on MNIST/Fashion-MNIST is supported but not on complex benchmarks
- **Low**: Claims about eigenvector interpretability producing semantically meaningful features lack quantitative validation

## Next Checks
1. **Quantitative Interpretability Test**: Compute Fréchet Inception Distance (FID) between reconstructed images from top eigenvectors versus ground truth class images to objectively measure feature quality
2. **Hyperparameter Sensitivity**: Systematically vary λ ∈ {0.1, 0.5, 1.0}, c ∈ {0.001, 0.01, 0.1}, p ∈ {1, 2} and measure impact on both accuracy and eigenvector interpretability metrics
3. **Generalization Stress Test**: Evaluate SQS-GLU performance and interpretability on CIFAR-10/100 to determine if spectral preservation breaks down on more complex image data