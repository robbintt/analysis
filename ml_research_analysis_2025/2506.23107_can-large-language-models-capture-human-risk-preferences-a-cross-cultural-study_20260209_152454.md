---
ver: rpa2
title: Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural
  Study
arxiv_id: '2506.23107'
source_url: https://arxiv.org/abs/2506.23107
tags:
- risk
- human
- data
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the ability of large language models (LLMs)
  to simulate human risk preferences using lottery-based choice tasks. Two models,
  ChatGPT 4o and ChatGPT o1-mini, were prompted with demographic profiles from four
  cultural contexts to predict individual choices.
---

# Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study

## Quick Facts
- arXiv ID: 2506.23107
- Source URL: https://arxiv.org/abs/2506.23107
- Reference count: 40
- Key outcome: Both LLMs exhibited higher risk aversion than humans, with o1-mini aligning more closely to observed behavior; Chinese prompts produced greater deviation than English even for native speakers.

## Executive Summary
This study investigates whether large language models can simulate human risk preferences across cultural contexts using lottery-based choice tasks. Two models, ChatGPT 4o and ChatGPT o1-mini, were prompted with demographic profiles from four regions to predict individual choices, with results analyzed using the Constant Relative Risk Aversion (CRRA) framework. Both models systematically over-estimated risk aversion compared to actual human responses, with o1-mini performing closer to human data than 4o. Language-specific analysis revealed that Chinese prompts produced larger deviations from real responses than English, even for native Chinese speakers. The findings highlight model limitations in replicating human-like risk attitudes and underscore the need for calibrated, culturally aware AI simulations in policy applications.

## Method Summary
The study employed a role-playing approach where LLMs received demographic profiles (age, gender, education, income, city) and predicted binary lottery choices. Four datasets from Sydney, Hong Kong, Dhaka, and Nanjing were used, with participants making choices across 9-10 lottery tasks. For each task, the LLMs generated three choices with majority voting, which were then used to estimate CRRA coefficients. These coefficients were compared to human-derived CRRA values using paired t-tests. The method tested both English and Chinese prompts to examine cross-linguistic performance differences.

## Key Results
- Both ChatGPT 4o and o1-mini exhibited higher risk aversion than human participants across all datasets
- o1-mini's simulated risk estimates were statistically closer to human data than 4o's, except in Dhaka
- Chinese prompts produced larger deviations from human responses than English prompts, even for native Chinese speakers
- English prompts consistently outperformed Chinese prompts in both Hong Kong and Nanjing datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Role-Playing Language Agents (RPLAs) can simulate human risk decisions when prompted with demographic profiles.
- Mechanism: The RPLA architecture receives demographic inputs, constructs a persona via natural language prompts, and generates binary lottery choices. The CRRA utility function U(x) = x^(1-r)/(1-r) is fitted to simulated choices to extract risk parameters.
- Core assumption: Demographic attributes alone sufficiently capture variation in human risk attitudes for simulation purposes.
- Evidence anchors: [abstract] "Demographic inputs were provided to two LLMs... which were tasked with predicting individual choices. Risk preferences were analyzed using the Constant Relative Risk Aversion (CRRA) framework."

### Mechanism 2
- Claim: ChatGPT o1-mini produces risk estimates closer to human data than ChatGPT 4o due to architectural differences.
- Mechanism: o1-mini's reasoning architecture may better calibrate trade-offs between expected values and risk, reducing systematic over-aversion.
- Core assumption: Observed performance differences stem from model architecture rather than prompt sensitivity or random variation.
- Evidence anchors: [abstract] "Both models exhibited higher risk aversion than human participants, with o1-mini aligning more closely to observed behavior."

### Mechanism 3
- Claim: Chinese prompts produce larger deviations from human responses than English prompts for Chinese-speaking populations.
- Mechanism: LLMs predominantly trained on English-language corpora may fail to adequately capture linguistic and cultural nuances embedded in non-English languages.
- Core assumption: Language-specific training data quality, not translation quality, drives the observed performance gap.
- Evidence anchors: [abstract] "Chinese prompts produced greater deviation from real responses than English, even for native Chinese speakers."

## Foundational Learning

- Concept: Constant Relative Risk Aversion (CRRA)
  - Why needed here: CRRA provides the mathematical framework to quantify and compare risk attitudes across humans and LLMs using utility function curvature.
  - Quick check question: Given a CRRA parameter r = 0.5, would an agent prefer a guaranteed $50 or a 50% chance at $100?

- Concept: Role-Playing Language Agent (RPLA) architecture
  - Why needed here: Understanding the four components (profile, memory, planning, action) is essential for debugging simulation failures and designing improved prompts.
  - Quick check question: Which RPLA component would you modify to make an agent consider previous lottery choices when making the next one?

- Concept: Lottery-based choice elicitation
  - Why needed here: The study's validity depends on lottery tasks effectively revealing true risk preferences rather than being confounded by task comprehension.
  - Quick check question: If a participant always chooses the higher-expected-value option regardless of variance, what does this indicate about their revealed risk attitude?

## Architecture Onboarding

- Component map: Profile Module -> Memory Module -> Planning Module -> Action Module -> Estimation Layer
- Critical path: 1. Extract demographic variables from survey data → construct profile prompt; 2. Present lottery task → LLM generates choice; 3. Repeat across all lottery tasks per respondent (3 simulations per task, majority vote); 4. Fit CRRA model to simulated choices → compare to human-fitted CRRA
- Design tradeoffs: Suppressing reasoning output reduces noise but loses interpretability of decision process; majority voting across 3 simulations reduces randomness but increases API costs; English prompts outperform native-language prompts—trade-off between ecological validity and accuracy
- Failure signatures: Systematic rightward shift in CRRA distributions → model over-aversion (4o shows this strongly); Bimodal human distributions but unimodal LLM distributions → failure to capture population heterogeneity; Chinese prompt accuracy worse than English despite native-language data → training corpus imbalance
- First 3 experiments: 1. Replicate with temperature = 0 to isolate deterministic model behavior from sampling variance; 2. Add explicit instruction to "consider your financial constraints" in planning prompt to test whether context improves calibration; 3. Run ablation removing city attribute from profile to quantify whether cultural context improves or degrades prediction accuracy

## Open Questions the Paper Calls Out

- Question: Can personalized calibration of LLM outputs based on sociodemographic and psychological characteristics improve simulated risk decision-making accuracy?
  - Basis in paper: [explicit] "exploring individual-level heterogeneity and implementing personalized calibration of LLM outputs based on respondents' sociodemographic and psychological characteristics may improve the accuracy of simulated decision-making"
  - Why unresolved: The study used only four basic demographic variables without any calibration mechanism or psychological traits.
  - What evidence would resolve it: Comparative experiments with calibrated vs. uncalibrated models, incorporating additional psychological measures.

- Question: Why do Chinese prompts produce greater deviation from actual human responses than English prompts, even for native Chinese speakers?
  - Basis in paper: [explicit] The paper finds Chinese prompts consistently amplified bias, suggesting "LLMs... predominantly trained on English-language corpora, may fail to adequately capture the subtle linguistic and cultural nuances embedded in non-English languages."
  - Why unresolved: The study identifies the effect but does not isolate whether the cause is training data imbalance, translation quality, or cultural context representation.
  - What evidence would resolve it: Experiments with native-language prompt construction (not translation) and models with balanced multilingual training.

- Question: How do LLMs perform in simulating risk preferences across broader decision-making scenarios beyond lottery choice games?
  - Basis in paper: [explicit] "Future work should evaluate the predictive performance of LLMs in a broader range of decision-making scenarios under uncertainty, as well as other types of complex decision-making processes"
  - Why unresolved: This study only used context-free lottery games, which may not generalize to real-world decisions with contextual factors.
  - What evidence would resolve it: Validation studies using domain-specific risky decisions (financial, health, transportation) with real stakes.

## Limitations

- The study assumes demographic profiles alone can fully capture human risk preferences across cultural contexts, which may oversimplify complex decision-making processes.
- The observed systematic over-aversion in both LLMs suggests fundamental model limitations in risk calibration that extend beyond prompt engineering.
- The study assumes CRRA adequately captures all meaningful variation in risk attitudes, potentially missing behavioral nuances like probability weighting or reference-dependent preferences.

## Confidence

- High confidence: Findings about systematic over-aversion across both models and the comparative performance advantage of o1-mini over 4o.
- Medium confidence: Cross-cultural performance differences, given the strong language-specific patterns but unclear causal mechanisms.
- Low confidence: The specific architectural reasons for model differences, as the paper does not empirically test alternative explanations beyond correlation.

## Next Checks

1. Conduct ablation studies varying temperature and seed parameters to isolate deterministic vs. stochastic effects on model choices.
2. Test alternative utility frameworks (e.g., prospect theory with probability weighting) to determine if CRRA limitations drive observed discrepancies.
3. Implement cross-lingual validation with professional translations and back-translation to disentangle language quality effects from model training biases.