---
ver: rpa2
title: 'Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization
  Strategy for LLaMA-based Language Models'
arxiv_id: '2504.21553'
source_url: https://arxiv.org/abs/2504.21553
tags:
- quantization
- spikes
- precision
- outliers
- layers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges assumptions about activation outliers in
  LLMs and proposes a mixed-precision quantization strategy for LLaMA-based models.
  The authors observe that activation spikes in LLaMA architectures are predominantly
  concentrated in specific projection layers, allowing targeted application of higher
  precision (FP16 or FP8) to these layers while quantizing the rest to lower bit-widths.
---

# Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models

## Quick Facts
- **arXiv ID**: 2504.21553
- **Source URL**: https://arxiv.org/abs/2504.21553
- **Reference count**: 21
- **Primary result**: Mixed-precision quantization targeting spike-prone layers improves 8-bit LLaMA quantization perplexity by >30 points over baseline

## Executive Summary
This paper challenges assumptions about activation outliers in LLMs and proposes a mixed-precision quantization strategy for LLaMA-based models. The authors observe that activation spikes in LLaMA architectures are predominantly concentrated in specific projection layers, allowing targeted application of higher precision (FP16 or FP8) to these layers while quantizing the rest to lower bit-widths. Experiments on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in perplexity and zero-shot accuracy, particularly for 8-bit quantization. The approach outperforms general-purpose quantization methods designed to handle outliers across all architecture types, highlighting the benefits of architecture-specific quantization strategies.

## Method Summary
The method identifies specific down_proj (and occasionally out_proj) layers in LLaMA-based architectures where activation spikes concentrate, then applies higher precision (FP16 or FP8) to only these layers while quantizing all other linear layers to INT8 using symmetric uniform per-tensor quantization. The spike locations are determined through profiling activation maxima on calibration data, with LLaMA3-8B requiring FP16 retention for down_proj layers 2 and 32, while other variants have similar but model-specific layer exemptions. The approach specifically targets per-tensor quantization scenarios where outliers cause the most damage, leaving attention matmul, softmax, and RMSNorm operations unquantized.

## Key Results
- LLaMA3-8B achieves 8.24 perplexity in 8-bit per-tensor quantization versus 44.7 with SmoothQuant and 40.5 with naive quantization
- Excluding Beginning-of-Text token from quantization reduces LLaMA3-8B perplexity from 59.9 to 9.02 in 8-bit per-tensor settings
- FP8 quantization of spike-prone layers maintains or slightly improves perplexity versus FP16 exemption
- The method proves effective for 6-bit quantization despite some instability, with results still exceeding FP16 baseline

## Why This Works (Mechanism)

### Mechanism 1: Localized Spike Concentration in Down Projections
Activation spikes in LLaMA architectures concentrate in 2-3 specific down_proj (and occasionally out_proj) layers, typically at early and late model depths. The down projection in the MLP block generates high-magnitude activation spikes (observed up to ~2500 in LLaMA2-7B). These spikes propagate through residual connections to subsequent RMSNorm inputs. Per-tensor quantization scaling factors, derived from max absolute values, become dominated by these few spike-prone layers, causing coarse quantization granularity elsewhere.

### Mechanism 2: Beginning-of-Text Token as Primary Spike Source
In LLaMA models, the Beginning-of-Text (BOT) token is the dominant source of activation spikes; excluding it from quantization substantially reduces perplexity degradation. The BOT token accumulates position-agnostic global context early in the forward pass, interacting with early down_proj layers to produce high-magnitude outputs that propagate residually. Mistral shows weaker BOT-dependency, suggesting training differences affect spike origin.

### Mechanism 3: Mixed-Precision Retention at Spike-Prone Layers Preserves Representational Fidelity
Applying FP16 or FP8 precision to identified spike-prone projections while quantizing all other linear layers to INT8 achieves near-FP16 perplexity and zero-shot accuracy, outperforming general-purpose outlier methods like SmoothQuant in per-tensor settings. By exempting only 2-3 layers from low-bit quantization, the scaling factors for remaining layers are no longer polluted by extreme outlier values, allowing standard per-tensor uniform quantization to operate effectively across the majority of the model.

## Foundational Learning

- **Per-Tensor vs. Per-Token Quantization Scaling**: Why needed here: The paper's key results depend on understanding how per-tensor quantization (one scale per activation tensor) is severely impacted by outliers, while per-token quantization (one scale per token) naturally isolates spikes. The mixed-precision strategy specifically targets per-tensor scenarios where outliers cause the most damage. Quick check: If you quantize a tensor with 99 values near 1.0 and one value at 1000 using per-tensor symmetric quantization to 8 bits, what happens to the 99 values?

- **Activation Outliers vs. Spikes**: Why needed here: The paper distinguishes "spikes" (localized high-magnitude activations tied to specific tokens/layers) from "outliers" (broader statistical deviations). Understanding this distinction is critical for interpreting why general methods like LLM.int8() are outperformed—they address a different outlier characterization. Quick check: A method that detects outliers as values >6σ from mean will flag which differently than a method defining outliers as values >10× the median?

- **Residual Propagation in Transformer Decoders**: Why needed here: Spikes propagate through residual connections, which bypass normalization layers. LLaMA's RMSNorm placement (pre-attention, pre-MLP) versus OPT's post-attention LayerNorm fundamentally changes spike flow. The paper's strategy relies on this architectural property. Quick check: In a residual stream where Add(Norm(x), Sublayer(x)) is the pattern versus Add(x, Norm(Sublayer(x))), how does each affect spike propagation?

## Architecture Onboarding

- **Component map**: Input Token IDs → Embedding → [For each Layer: RMSNorm → Attention (Q/K/V/O projections) → Residual Add; RMSNorm → MLP (gate_proj, up_proj, down_proj) → Residual Add] → Final RMSNorm → LM Head → Logits

- **Critical path**: 1) Profile activation maxima per layer on calibration data (e.g., WikiText2, 512-2048 tokens) 2) Identify down_proj and out_proj layers with max|activation| > threshold 3) Mark identified layers for FP16 retention or FP8 quantization 4) Apply INT8 per-tensor quantization to all other linear layers 5) Validate perplexity on held-out set; if degradation >5% over FP16, re-examine spike layer selection

- **Design tradeoffs**: FP16 exemption vs. FP8 for spike layers (FP16 guarantees no precision loss; FP8 reduces memory 2× but requires hardware support); Static vs. dynamic spike detection (static is deployment-simple; dynamic handles distribution shift but adds overhead); Per-tensor vs. per-token quantization (per-tensor is hardware-efficient but spike-sensitive; per-token is robust but requires per-token scale storage/computation)

- **Failure signatures**: Perplexity explodes (>50 for 8-bit) → spike layers incorrectly identified or not exempted; Zero-shot accuracy drops >10 points → check if out_proj exemption needed (Mistral case); 6-bit quantization unusable (>200 PPL) → current method insufficient; Random mixed precision matches targeted → spike localization hypothesis violated for this model/checkpoint

- **First 3 experiments**: 1) Replicate spike profiling: Run forward pass on WikiText2 subset, record max|activation| per layer for LLaMA2-7B. Verify down_proj at layers 2 and final show highest values. 2) Ablation on layer count: Compare keeping top-1, top-2, top-3 spike layers in FP16. Quantify PPL vs. compute/memory tradeoff. 3) Cross-architecture validation: Apply same profiling to a non-LLaMA model (e.g., Qwen, Gemma). Report whether spike concentration pattern holds or diffuses across layers.

## Open Questions the Paper Calls Out

1. Can combining this targeted mixed-precision approach with weight-scaling methods like SmoothQuant yield greater performance improvements than either method alone? The conclusion states, "combining our approach with other quantization techniques, such as SmoothQuant, could potentially yield even greater performance improvements."

2. What specific mechanisms cause the performance instability observed when applying this mixed-precision strategy to 6-bit quantization? The conclusion lists as future work the need to "investigate the causes of the remaining instability in 6-bit quantization."

3. How effectively does this layer-targeted strategy generalize to non-LLaMA architectures, such as OPT or encoder-decoder models? The conclusion proposes to "explore the application of this method to other model families," while the introduction notes that spikes manifest differently in OPT-like models.

## Limitations

- Spike localization stability may not generalize across all prompt distributions, domains, or fine-tuned variants due to static calibration methodology
- Cross-architecture generalization is unproven, as the spike localization pattern may be architecture-specific to LLaMA and Mistral
- Hardware dependencies on native FP8 support could negate memory bandwidth savings without such acceleration

## Confidence

- **High Confidence**: The observation that per-tensor quantization fails catastrophically when activation spikes are present; experimental results showing significant perplexity improvements (>30 points for LLaMA3-8B)
- **Medium Confidence**: The specific claim that spikes concentrate in 2-3 down_proj layers per model; the BOT token's role as primary spike source
- **Low Confidence**: The assertion that this approach will work well for 6-bit quantization (contradicted by experimental instability); the claim that mixed-precision is superior to general outlier-handling methods like SmoothQuant

## Next Checks

1. Apply the quantization strategy to LLaMA3-8B using calibration data from one domain (e.g., Wikipedia) but evaluate on a substantially different domain (e.g., code, legal documents, scientific papers). Measure whether spike layer identification remains stable and whether perplexity degradation exceeds acceptable thresholds.

2. Implement the same spike profiling and mixed-precision strategy on a non-LLaMA architecture such as Qwen-7B or Gemma-7B. Document whether spikes concentrate in specific down_proj layers or distribute more evenly across the model, and measure resulting quantization performance.

3. Implement a dynamic spike detection mechanism that identifies spike-prone layers per batch rather than using static calibration. Compare perplexity and zero-shot accuracy against the static approach across multiple prompt types and batch sizes to quantify the tradeoff between accuracy and computational overhead.