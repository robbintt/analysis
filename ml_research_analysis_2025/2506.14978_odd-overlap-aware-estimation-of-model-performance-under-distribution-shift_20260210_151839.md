---
ver: rpa2
title: 'ODD: Overlap-aware Estimation of Model Performance under Distribution Shift'
arxiv_id: '2506.14978'
source_url: https://arxiv.org/abs/2506.14978
tags:
- target
- domain
- source
- dis2
- bound
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work improves on Dis2 by proposing an overlap-aware variant
  (ODD) for more accurate performance estimation under distribution shift. The core
  idea is to discount disagreement in the overlapping region between source and target
  domains, removing competition in the Dis2 objective.
---

# ODD: Overlap-aware Estimation of Model Performance under Distribution Shift

## Quick Facts
- arXiv ID: 2506.14978
- Source URL: https://arxiv.org/abs/2506.14978
- Reference count: 31
- Key outcome: Overlap-aware Dis2 variant (ODD) yields tighter bounds with lower MAE while maintaining coverage

## Executive Summary
This work improves on Dis2 by proposing an overlap-aware variant (ODD) for more accurate performance estimation under distribution shift. The core idea is to discount disagreement in the overlapping region between source and target domains, removing competition in the Dis2 objective. This is achieved by training a domain classifier to estimate overlap and weighting target samples accordingly. Experiments show ODD yields tighter bounds than Dis2 on both synthetic and real datasets, with lower mean absolute error while maintaining high coverage. The method also remains reliable even when overlap estimation is imperfect, though improvements are expected with better overlap quantification.

## Method Summary
ODD modifies the Dis2 objective by weighting target samples using domain classifier softmax probabilities, which reduces to zero the disagreement loss contribution for samples likely to be in the overlapping region. The method trains a domain classifier to distinguish source from target samples, uses its softmax probabilities as soft weights for the target disagreement term, and selects critics by maximizing the modified ODD objective. This removes the competing optimization pressures where the critic must simultaneously agree (source objective) and disagree (target objective) with the source classifier on the same samples, yielding tighter bounds while maintaining theoretical validity.

## Key Results
- ODD yields tighter bounds than Dis2 with lower mean absolute error on both synthetic and real datasets
- The method maintains high coverage probability while providing more optimistic (less pessimistic) bounds
- ODD remains reliable even when overlap estimation is imperfect, though occasional bound violations occur

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Discounting disagreement in the overlapping region between source and target domains produces tighter performance bounds under distribution shift.
- Mechanism: ODD modifies the Dis2 objective by weighting target samples using domain classifier softmax probabilities (s(x)₁), which reduces to zero the disagreement loss contribution for samples likely to be in the overlapping region. This eliminates the competing optimization pressures where the critic must simultaneously agree (source objective) and disagree (target objective) with the source classifier on the same samples.
- Core assumption: Assumption 3.5: Target disagreement between ĥ and y* in the overlap region is bounded by source disagreement (ϵᵀ_Dα(ĥ, y*) ≤ ϵˢ_Dα(ĥ, y*)).
- Evidence anchors:
  - [abstract] "discount disagreement in the overlapping region between source and target domains, removing competition in the Dis2 objective"
  - [section 3] "This breakdown of disagreement in the overlapping and non-overlapping portions allows us rethink the Dis2 objective"
- Break condition: When overlap estimation via domain classifier is systematically biased, the soft weights may over-discount or under-discount, causing bound violations.

### Mechanism 2
- Claim: Domain classifier softmax probabilities provide a practical proxy for quantifying domain overlap without explicit density estimation.
- Mechanism: Train a classifier d: X → {0, 1} to distinguish source (label 0) from target (label 1) samples. Use s(x)₁ (probability of target classification) as a soft weight: samples the classifier is uncertain about receive lower weight in the disagreement loss. This converts hard overlap set selection Dα into a smooth discounting scheme amenable to gradient-based optimization.
- Core assumption: Domain classifier uncertainty correlates with overlap density; softmax probabilities are sufficiently calibrated for weighting purposes.
- Evidence anchors:
  - [abstract] "achieved by training a domain classifier to estimate overlap and weighting target samples accordingly"
  - [section 4] "this soft version makes the discounting of overlapping target samples smooth and it works well in practice"
- Break condition: When domain classifier is miscalibrated or when representations are domain-adversarially regularized, s(x)₁ may not reflect true overlap, leading to incorrect discounting.

### Mechanism 3
- Claim: Selecting critics by maximizing ODD rather than Dis2 yields less pessimistic bounds while maintaining validity.
- Mechanism: By removing the competing optimization pressure in the overlap region, the critic ĥ is free to agree with ĥ on overlapping samples while still disagreeing maximally on non-overlapping target samples. This results in a lower discrepancy term Δ(ĥ, ĥ, α) than Δ(ĥ, ĥ), tightening the bound without violating theoretical guarantees (Theorem 3.7).
- Core assumption: Assumption 3.6: The critic maximizing ODD surrogate satisfies Δ(ĥ, y*, α) ≤ Δ(ĥ, ĥ, α).
- Evidence anchors:
  - [abstract] "Experiments show ODD yields tighter bounds than Dis2 on both synthetic and real datasets, with lower mean absolute error while maintaining high coverage"
  - [section 5.1, Figure 4] "ODD consistently improves over Dis2 by choosing a more optimistic ĥ that agrees more with ĥ in the overlapping region"
- Break condition: When Assumption 3.5 is violated, the bound may become invalid; this occurs rarely but is observed in a small fraction of cases.

## Foundational Learning

- Concept: **Disagreement Discrepancy (Dis2)**
  - Why needed here: ODD is built directly on Dis2; understanding that Δ(h, h') = ϵᵀ(h, h') - ϵˢ(h, h') and how it bounds target error via source error plus discrepancy is essential.
  - Quick check question: Given source disagreement 0.1 and target disagreement 0.4, what is the Dis2 discrepancy?

- Concept: **Domain Adaptation Bounds with HΔH Divergence**
  - Why needed here: The paper extends classic domain adaptation theory (Ben-David et al.) using ideal joint hypothesis; familiarity with how divergences bound target risk under shift provides context.
  - Quick check question: Why does the λ term (joint risk of ideal hypothesis) appear in the general Dis2 bound but disappear when y*_S = y*_T?

- Concept: **Domain Classifier Training**
  - Why needed here: ODD relies on training a binary classifier to distinguish source from target samples; understanding standard binary classification, softmax outputs, and calibration is prerequisite.
  - Quick check question: If a domain classifier achieves 50% accuracy, what does this imply about the overlap between source and target distributions?

## Architecture Onboarding

- Component map:
  Source classifier (ĥ) -> Domain classifier (d) -> Critic (ĥ) -> Loss combiner -> Bound computation

- Critical path:
  1. Train domain classifier d on (source∪target, domain labels)
  2. Extract softmax probabilities s(x)₁ for all target samples
  3. Initialize critic ĥ ← ĥ (copy weights)
  4. Optimize ĥ using L̂_Δ(h̄, α) with s(x)₁ as sample weights for target disagreement term
  5. Compute bound: ϵᵀ(ĥ) ≤ ϵˢ(ĥ) + Δ(ĥ, ĥ, α) + concentration term

- Design tradeoffs:
  - Hard vs. soft overlap: Hard indicator 1{x∉D̂_α} is brittle; soft weights s(x)₁ are smoother but depend on calibration
  - Feature space vs. logit space: Features yield higher coverage but looser bounds; logits are tighter but may overestimate
  - With vs. without δ term: Including δ ensures theoretical validity but may be overly conservative; dropping it improves tightness at cost of occasional violations

- Failure signatures:
  - High overlap estimate on dissimilar domains → over-discounting → bound overestimates accuracy (invalid)
  - Domain classifier overfitting → s(x)₁ near 0 or 1 → loss of smooth discounting gradient
  - Using domain-adversarial representations → artificially high overlap → bounds may fail when target error is actually high

- First 3 experiments:
  1. Reproduce synthetic data experiment (Figure 4): Vary overlap factor from 0 to 1; compare Dis2 vs. ODD discrepancy values and bound tightness
  2. Ablate domain classifier architecture: Test linear vs. 2-layer vs. 3-layer MLP on a held-out dev set; measure calibration (ECE) and final MAE
  3. Test on a new dataset shift (e.g., CIFAR-10 → CIFAR-10-C corruption): Compare coverage and MAE of ODD vs. Dis2 vs. ATC baseline across corruption severities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more accurate density estimation methods for overlap quantification reduce ODD's bound violation rate when the overlap is overestimated?
- Basis in paper: [explicit] "Better overlap estimation should be able to mitigate this issue, which we defer to future work" regarding ODD marginally overshooting allowed violation probability.
- Why unresolved: Current method uses domain classifier softmax probabilities as weights, which doesn't guarantee accurate overlap quantification and is sensitive to classifier training hyperparameters.
- What evidence would resolve it: Experiments comparing ODD with alternative overlap estimation methods (e.g., normalizing flows, energy-based models) showing reduced violation rates while maintaining tighter bounds.

### Open Question 2
- Question: How can disagreement and overlap be formally defined for sequence generation tasks to extend ODD to Large Language Models?
- Basis in paper: [explicit] "In the era of Large Language Models (LLMs), where the input/output space is highly complex, we aim to extend our method to develop generic definitions of disagreement and overlap beyond classification settings."
- Why unresolved: Current definitions rely on discrete classification disagreement (argmax mismatch) which doesn't directly transfer to text generation with open-ended outputs.
- What evidence would resolve it: A formal definition of disagreement for sequences, with empirical validation showing ODD-based bounds correlate with actual LLM performance shifts.

### Open Question 3
- Question: Does non-uniform sample complexity analysis reveal conditions where ODD bounds become unreliable?
- Basis in paper: [inferred] The paper notes "Regarding analyzing the variance and how it can in turn affect our discrepancy measure and the final bound with a non-uniform sample complexity, we refer to future work."
- Why unresolved: Current analysis assumes uniform sample complexity; variance in the critic selection process across different sample sizes and overlap degrees is not characterized.
- What evidence would resolve it: Theoretical bounds incorporating non-uniform complexity terms, validated against empirical variance across multiple random seeds and sample sizes.

### Open Question 4
- Question: Under what conditions does Assumption 3.5 (target disagreement bounded by source disagreement in overlap regions) fail?
- Basis in paper: [inferred] Assumption 3.5 is motivated by "intuition" and validated on synthetic data, but the paper acknowledges ODD occasionally overestimates accuracy, potentially due to overlap overestimation violating this assumption.
- Why unresolved: No theoretical characterization of when target disagreement legitimately exceeds source disagreement in overlapping regions (e.g., label shift, spurious correlations).
- What evidence would resolve it: Identification of specific distribution shift types (e.g., label shift) where Assumption 3.5 breaks down, with quantified error bounds.

## Limitations

- Overlap estimation via domain classifier softmax is a practical proxy but lacks theoretical calibration guarantees and may be dataset-dependent
- Assumption 3.5 (target disagreement bounded by source disagreement in overlap) is shown to hold empirically but is not universally guaranteed, leading to rare but observable bound violations
- The method requires careful hyperparameter tuning of the domain classifier and critic optimization to achieve reliable performance

## Confidence

- Mechanism 1 (Overlap-aware Dis2 modification): Medium - Core idea is sound, but empirical validation is limited to specific datasets and overlap regimes
- Mechanism 2 (Domain classifier as overlap estimator): Medium - Practical proxy with no theoretical calibration guarantees; performance depends on classifier quality
- Mechanism 3 (Critic selection via ODD maximization): High - Directly supported by experiments showing lower MAE and maintained coverage

## Next Checks

1. **Calibration Analysis**: Evaluate domain classifier softmax calibration (ECE) across datasets and correlate with bound tightness to quantify proxy quality
2. **Assumption Violation Stress Test**: Systematically generate synthetic shifts where Assumption 3.5 is violated (e.g., target domain with higher disagreement) and measure bound validity
3. **Cross-architecture Generalization**: Test ODD performance when using different source classifier architectures (e.g., ResNet vs. MLP) and training objectives (ERM vs. DA) to assess robustness