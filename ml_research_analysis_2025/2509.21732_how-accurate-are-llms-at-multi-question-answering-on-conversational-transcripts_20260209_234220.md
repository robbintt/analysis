---
ver: rpa2
title: How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?
arxiv_id: '2509.21732'
source_url: https://arxiv.org/abs/2509.21732
tags:
- questions
- llms
- gpt-4o
- question
- navigation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for answering
  multiple yes/no questions based on lengthy conversational transcripts. It explores
  batch prompting as a solution to reduce computational overhead in industrial settings,
  where multiple questions about a single long transcript must be processed efficiently.
---

# How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?

## Quick Facts
- arXiv ID: 2509.21732
- Source URL: https://arxiv.org/abs/2509.21732
- Authors: Xiliang Zhu; Shi Zong; David Rossouw
- Reference count: 6
- Key outcome: Fine-tuned public LLMs with 8B parameters can surpass GPT-4o in multi-question accuracy on conversational transcripts for group sizes N≥30

## Executive Summary
This study evaluates Large Language Models for answering multiple yes/no questions based on lengthy conversational transcripts. The research explores batch prompting as a solution to reduce computational overhead in industrial settings, where multiple questions about a single long transcript must be processed efficiently. Experiments benchmark proprietary models like GPT-4o against fine-tuned public models (Llama3 and Qwen2.5) across varying group sizes (10-50 questions).

## Method Summary
The paper addresses multi-question answering on contact center transcripts by processing N Yes/No questions in a single LLM call with structured JSON output requiring binary judgment plus supporting utterance index. The study uses 2,800 ASR-generated transcripts (token stats: 501/1,153/2,754/6,584 at 25th/50th/75th/95th percentiles) with ground truth generated by GPT-4o using group size N=10. Evaluation metrics include Judgment Accuracy, Navigation F1, Navigation MAE, and JSON Decode Error Rate. Zero-shot inference tests proprietary models (GPT-4o, GPT-4o-mini, Gemini-1.5-flash) and public instruction-tuned models, while fine-tuning uses BFloat-16, lr=3e-5, 3 epochs on 8x A100 GPUs with random K ∈ [5, N] sampling to prevent rigid response patterns.

## Key Results
- GPT-4o achieves the best overall performance but shows accuracy degradation from 0.90 to 0.84 as group size increases from N=10 to N=50
- Fine-tuned Qwen2.5-7B and Llama3.1-8B both surpass GPT-4o in judgment accuracy when N≥30
- Instruction-tuned public models exhibit 12–73% JSON decode error rates, reduced to near-zero after fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning smaller public models on domain-specific multi-question QA can match or exceed proprietary model performance when question group size increases. Fine-tuning exposes the model to the specific distribution of conversational transcripts and multi-question formatting, enabling more efficient parameter utilization for this constrained task compared to general-purpose models.

### Mechanism 2
Variable group size during training prevents models from overfitting to fixed output patterns. Random sampling K∈[5,N] for each training instance prevents the model from learning to generate exactly N responses regardless of input, reducing hallucination and repetition.

### Mechanism 3
Structured JSON output with navigation indices improves production reliability and enables hallucination detection. JSON schema enforcement combined with utterance index requirements creates verifiable outputs that can be parsed programmatically and audited by humans.

## Foundational Learning

- Concept: Batch prompting / multi-question inference
  - Why needed here: Core technique being evaluated—processing multiple questions in single LLM call to reduce latency and cost in production.
  - Quick check question: Can you explain why running 50 separate API calls with identical 2,700+ token context is inefficient compared to one call with 50 questions?

- Concept: Fine-tuning vs. zero-shot inference
  - Why needed here: The paper's central finding depends on understanding when fine-tuned small models become preferable to zero-shot large models.
  - Quick check question: What is the trade-off between using GPT-4o zero-shot vs. fine-tuning Llama3.1-8B for a specific domain task?

- Concept: Mean Absolute Error (MAE) for ordinal predictions
  - Why needed here: Navigation task requires predicting utterance indices; MAE captures how far off predictions are even when not exactly correct.
  - Quick check question: If the correct utterance is index 50 and the model predicts 52, what is the MAE contribution for this single prediction?

## Architecture Onboarding

- Component map: ASR transcript -> N questions -> LLM inference -> JSON parser -> Response extraction
- Critical path: 1) Transcript tokenization and segmentation into M utterances with indices 2) Prompt construction with transcript, N questions, and JSON format example 3) Model inference with single forward pass 4) JSON parsing and validation 5) Response extraction per question
- Design tradeoffs:
  - Group size N vs. accuracy: Larger N reduces API calls but degrades performance (GPT-4o drops from 0.90 to 0.84 accuracy from N=10 to N=50)
  - Proprietary vs. fine-tuned public: Proprietary has lower operational complexity; fine-tuned offers cost control and data privacy
  - Training group size ceiling: Models trained with max N=10 perform well at N=10-20 but may not extrapolate to N=50
- Failure signatures:
  - JSON decode errors: Instruction-tuned models produce malformed JSON; fine-tuning reduces from ~50% error to near-zero
  - Fixed output count: Model always generates exactly N responses even when fewer questions provided (mitigated by variable training)
  - Navigation drift: MAE increases with group size, indicating cumulative attention degradation
- First 3 experiments:
  1. Baseline comparison: Run GPT-4o, GPT-4o-mini, and Gemini-1.5-flash zero-shot on held-out transcripts with N=10, 30, 50 to establish proprietary model degradation curve.
  2. Fine-tuning ablation: Train Llama3.1-8B with max group sizes N=10, 30, 50 and evaluate on matching test sets to identify optimal training configuration.
  3. JSON reliability test: Compare JSON decode error rates between instruction-tuned and fine-tuned variants across all group sizes to quantify production readiness gap.

## Open Questions the Paper Calls Out
None

## Limitations
- Proprietary dataset dependency limits reproducibility and external validation
- Performance advantage measured only within internal Dialpad dataset
- Fine-tuning procedure lacks specific details on batch size and context window limits

## Confidence
- High Confidence: Fine-tuned public LLMs can match or exceed GPT-4o's judgment accuracy for larger question groups (N≥30)
- Medium Confidence: JSON output reliability improvements through fine-tuning are supported but could be more robust
- Low Confidence: Variable group size training mechanism's effectiveness beyond tested range remains uncertain

## Next Checks
1. Apply the same fine-tuning and evaluation pipeline to a public conversational QA dataset (e.g., CoQA or QuAC) converted to Yes/No format with utterance indexing to test generalizability.
2. Systematically vary transcript quality (clean vs. noisy ASR output, varying utterance boundaries) and measure JSON decode error rates for both instruction-tuned and fine-tuned models to quantify robustness.
3. Calculate the total cost (API calls or GPU hours) for processing 10,000 transcripts with varying N values using both proprietary and fine-tuned models, including fine-tuning overhead amortized over usage volume.