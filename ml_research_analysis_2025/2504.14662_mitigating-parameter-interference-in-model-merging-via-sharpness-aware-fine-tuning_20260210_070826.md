---
ver: rpa2
title: Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning
arxiv_id: '2504.14662'
source_url: https://arxiv.org/abs/2504.14662
tags:
- task
- block
- performance
- merging
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of parameter interference in
  model merging, where combining task-specific models leads to performance degradation
  due to conflicting parameters. The authors propose sharpness-aware fine-tuning (SAFT),
  which optimizes for both task performance and reduced parameter interference by
  finding flat minima in the loss landscape.
---

# Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning

## Quick Facts
- arXiv ID: 2504.14662
- Source URL: https://arxiv.org/abs/2504.14662
- Reference count: 40
- Key outcome: SAFT improves merged model performance by up to 30% in normalized accuracy by reducing parameter interference

## Executive Summary
This paper addresses the challenge of parameter interference in model merging, where combining task-specific models leads to performance degradation due to conflicting parameters. The authors propose sharpness-aware fine-tuning (SAFT), which optimizes for both task performance and reduced parameter interference by finding flat minima in the loss landscape. SAFT is inspired by the observation that its objective function aligns with the goals of model merging. Experimental results show that SAFT significantly improves the performance of merged models across various merging methods and fine-tuning approaches, achieving up to 30% improvements in normalized accuracy. The method also demonstrates better weight disentanglement and cross-task linearity, confirming its effectiveness in reducing parameter interference. Theoretical analysis further supports that SAFT induces joint-task loss linearity, ensuring a smaller performance gap between merged and task-specific models.

## Method Summary
The proposed SAFT method is a fine-tuning approach that simultaneously optimizes for task performance and parameter disentanglement during model merging. SAFT incorporates a sharpness-aware component that encourages the optimization to find flat minima in the loss landscape, which correlates with reduced parameter interference. The method works by modifying the fine-tuning objective to include both the standard task loss and a regularization term that promotes parameter orthogonality across tasks. This approach is applied during the fine-tuning phase of merged models, regardless of which merging strategy was used initially. The key insight is that by optimizing for flat minima, SAFT naturally reduces the interference between parameters that were optimized for different tasks.

## Key Results
- SAFT achieves up to 30% improvements in normalized accuracy across various merging methods and fine-tuning approaches
- The method demonstrates better weight disentanglement and cross-task linearity compared to standard fine-tuning
- Theoretical analysis confirms that SAFT induces joint-task loss linearity, reducing the performance gap between merged and task-specific models

## Why This Works (Mechanism)
SAFT works by optimizing for flat minima in the loss landscape during fine-tuning, which inherently reduces parameter interference. When parameters are optimized to lie in flat regions, they become less sensitive to small perturbations, which corresponds to better generalization and reduced interference between task-specific parameters. The sharpness-aware component of SAFT encourages parameters to move toward regions where the loss changes slowly in all directions, creating a more stable and disentangled parameter space. This approach effectively regularizes the fine-tuning process to maintain the benefits of parameter disentanglement achieved during initial task-specific training while still allowing the model to adapt to the merged objective.

## Foundational Learning

Loss landscape analysis: Understanding how the geometry of the loss surface affects optimization and generalization - needed to grasp why flat minima are beneficial for reducing interference; quick check: visualize loss landscapes before and after SAFT fine-tuning.

Parameter interference: The phenomenon where parameters optimized for different tasks conflict with each other during model merging - crucial for understanding the core problem SAFT addresses; quick check: measure parameter correlation changes during merging.

Sharpness-aware optimization: Optimization techniques that explicitly encourage finding flat minima in the loss landscape - forms the theoretical foundation of SAFT; quick check: compare sharpness metrics between SAFT and standard fine-tuning.

Weight disentanglement: The degree to which parameters remain independent across different tasks - a key metric for evaluating the success of interference mitigation; quick check: measure task-specific parameter activation patterns.

Cross-task linearity: The extent to which the loss for combined tasks can be approximated as a linear combination of individual task losses - theoretical property that SAFT aims to achieve; quick check: compute correlation between individual and joint task losses.

## Architecture Onboarding

Component map: Base model (frozen) -> Parameter merging strategy -> SAFT fine-tuning module -> Merged model output

Critical path: Input -> Base model layers -> Merged parameters -> SAFT-regularized optimization -> Output

Design tradeoffs: SAFT trades additional fine-tuning computation for improved merged model performance and stability. The method requires careful tuning of the sharpness-aware regularization strength to balance between maintaining task-specific capabilities and achieving parameter disentanglement.

Failure signatures: Over-regularization leading to underfitting, insufficient fine-tuning time resulting in poor task adaptation, and inappropriate regularization strength causing either excessive interference or loss of task-specific knowledge.

First experiments:
1. Apply SAFT to a simple linear model merging two synthetic tasks with known parameter conflicts to visualize the effect on loss landscape geometry
2. Compare SAFT against standard fine-tuning on a toy classification problem with clearly separable parameter subspaces
3. Measure the evolution of parameter orthogonality during SAFT fine-tuning compared to baseline methods

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis relies on assumptions about loss landscape properties that may not hold uniformly across different model architectures and tasks
- Absolute performance gains vary considerably across different task pairs and merging strategies, suggesting task-dependent effectiveness
- Evaluation primarily focuses on classification tasks, leaving open questions about performance in generative or multimodal scenarios
- Computational overhead introduced by SAFT during fine-tuning is not thoroughly quantified

## Confidence

SAFT's effectiveness in reducing parameter interference: Medium - experimental results are compelling but generalizability across diverse model types requires further validation

Alignment between SAFT objective and model merging goals: High - theoretical connection between flat minima optimization and reduced interference is well-established

Improvement in weight disentanglement and cross-task linearity: Medium - measured through specific metrics but alternative measures could strengthen claims

## Next Checks

1. Conduct experiments on generative models (e.g., GPT variants) and multimodal models to assess SAFT's effectiveness beyond classification tasks, particularly focusing on language generation quality and cross-modal alignment metrics

2. Perform ablation studies varying the computational budget for SAFT to quantify the trade-off between performance gains and additional training overhead, including wall-clock time comparisons with standard fine-tuning

3. Test SAFT's robustness to extreme cases of parameter interference by deliberately constructing task pairs with known conflicting parameter requirements, measuring the method's ability to disentangle these conflicts compared to baseline approaches