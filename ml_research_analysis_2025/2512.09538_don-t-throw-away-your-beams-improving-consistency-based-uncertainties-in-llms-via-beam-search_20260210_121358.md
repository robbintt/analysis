---
ver: rpa2
title: 'Don''t Throw Away Your Beams: Improving Consistency-based Uncertainties in
  LLMs via Beam Search'
arxiv_id: '2512.09538'
source_url: https://arxiv.org/abs/2512.09538
tags:
- beam
- beamsearch
- multinomial
- dissimilarity
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Consistency-based uncertainty quantification (UQ) in large language
  models (LLMs) typically relies on multinomial sampling to generate candidate outputs,
  but this approach suffers from high redundancy and variance, especially for short
  answers. This paper introduces a new family of methods that employ beam search instead
  of multinomial sampling for generating candidates, leveraging the probability-weighted
  outputs from the beam to improve diversity and stability.
---

# Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search

## Quick Facts
- **arXiv ID:** 2512.09538
- **Source URL:** https://arxiv.org/abs/2512.09538
- **Reference count:** 40
- **Primary result:** Beam-weighted estimators using beam search outperform multinomial sampling for consistency-based uncertainty quantification, especially for short QA outputs.

## Executive Summary
This paper addresses the inefficiency of multinomial sampling in consistency-based uncertainty quantification for LLMs, where peaked distributions for short answers lead to high redundancy and variance. The authors propose replacing multinomial sampling with beam search to generate candidate outputs, using probability-weighted aggregation to improve diversity and stability. A theoretical lower bound shows beam search achieves lower error than multinomial sampling when the beam captures sufficient probability mass. Empirical evaluation on six QA datasets with six models demonstrates consistent improvements, with the largest gains for short generations and small sample budgets.

## Method Summary
The method replaces multinomial sampling with beam search (width M=10) to generate candidate outputs {b^(1), ..., b^(M)} with sequence probabilities p(b^(i)|x). Instead of uniform weighting, it uses importance-style weights w_i = p(b^(i)|x) / Σp(b^(j)|x) to concentrate estimation on high-probability hypotheses. The greedy output y* (produced answer) is compared to each beam candidate using NLI-based semantic similarity s(b^(i), y*), and uncertainty is aggregated as Û_D(y*|x) = Σ w_i (1 - s(b^(i), y*)). This probability-weighted dissimilarity estimator theoretically achieves lower MSE than multinomial sampling when beam probability mass m_B > 1 - 1/(2√M).

## Key Results
- Beam search reduces multinomial sampling redundancy from 30-50% for 2-4 token outputs to near zero, improving estimator stability
- Probability-weighted beam estimators consistently outperform uniform weighting and multinomial sampling across six QA datasets
- Theoretical sufficient condition (m_B > 0.842 for M=10) is met in 22.7% of TriviaQA examples overall, up to 30-40% for ≤3 tokens
- Improvements are most pronounced for short generations and at small sample budgets, achieving state-of-the-art UQ performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Beam search reduces sample redundancy in consistency-based UQ by generating guaranteed-distinct candidates, improving estimator stability.
- Mechanism: Multinomial sampling repeatedly draws from the same peaked distribution for short answers (30-50% duplicates for 2-4 token outputs), wasting compute and increasing variance. Beam search deterministically explores the top-M paths, ensuring each candidate is distinct and covers high-probability regions of the output space.
- Core assumption: The model's output distribution for short-form QA is sufficiently peaked that multinomial sampling produces duplicates before exploring alternatives.
- Evidence anchors:
  - [abstract]: "multinomial sampling is prone to producing duplicates due to peaked distributions"
  - [section 2.2, Figure 2]: Shows 30-50% redundancy for short outputs, dropping to ~17% for 8+ tokens.
  - [corpus]: Weak corpus support; neighbors focus on beamforming/decoding efficiency, not UQ redundancy.
- Break condition: If output space is flat or entropy is high (longer generations, open-ended tasks), redundancy decreases and the gap between beam search and multinomial sampling narrows.

### Mechanism 2
- Claim: Probability-weighted beam estimators preserve the model's predictive distribution better than uniform weighting, preventing overrepresentation of low-mass hypotheses.
- Mechanism: Instead of treating beam candidates uniformly, the method uses importance-style weights w_i = p(b(i)|x) / Σp(b(j)|x). This concentrates estimation on the model's actual beliefs, reducing noise from tail hypotheses.
- Core assumption: Beam candidates' autoregressive probabilities are meaningful approximations of their importance under the model distribution.
- Evidence anchors:
  - [section 3.1]: Equations (4)-(5) define the probability-weighted estimator.
  - [section A.2, Table 5]: Ablation shows no clear optimal weight floor ε, but uniform weighting (ε=1) underperforms weighted variants in most cases.
  - [corpus]: No direct corpus support for this specific weighting scheme in UQ.
- Break condition: If probability estimates are miscalibrated or sequence probabilities are dominated by length effects, weighting may introduce bias rather than reduce variance.

### Mechanism 3
- Claim: Beam search achieves lower mean-squared error than multinomial sampling when the beam captures sufficient probability mass, as formalized by a distribution-free sufficient condition.
- Mechanism: The paper proves (Theorem 1) that if beam probability mass m_B > 1 - 1/(2√M), the beam-weighted estimator has lower MSE than the Monte Carlo estimator. For M=10, this threshold is 0.842. In practice, the condition is met more often for short outputs where probability mass concentrates.
- Core assumption: The dissimilarity gap between inside-beam and outside-beam samples |μ_B - μ̄_B| is bounded, and variance is bounded by 1/4 (Popoviciu's inequality).
- Evidence anchors:
  - [section 3.2, Theorem 1]: Formal condition (6) and distribution-free bound (7).
  - [section 3.2, Figure 3]: 22.7% of TriviaQA examples meet m_B > 0.842; up to 30-40% for ≤3 tokens.
  - [corpus]: No corpus support for this specific theoretical contribution.
- Break condition: When m_B is below threshold (longer outputs, diffuse distributions), the guarantee does not hold, and empirical gains may diminish or reverse.

## Foundational Learning

- Concept: Consistency-based Uncertainty Quantification
  - Why needed here: The entire method builds on measuring semantic dissimilarity between candidate outputs to estimate uncertainty. Without understanding how dissimilarity maps to confidence, the weighting and beam adaptations won't make sense.
  - Quick check question: Given two generated answers "Paris" and "Lyon" for "What is France's capital?", would a consistency-based UQ method assign high or low uncertainty? Why?

- Concept: Importance Sampling and Weighting
  - Why needed here: The core innovation is adapting beam outputs using importance weights (Equation 4) to approximate expectations under the full distribution. This is standard Monte Carlo theory applied to a non-sampled setting.
  - Quick check question: If you have samples from distribution q but want expectations under p, what weight would you assign each sample? How does this relate to the beam-weighted estimator?

- Concept: Beam Search Decoding
  - Why needed here: Understanding that beam search maintains top-k partial sequences with exact probabilities is essential. The method exploits the beam as a deterministic approximation to the high-probability output space.
  - Quick check question: How does beam search differ from greedy decoding and multinomial sampling in terms of determinism and probability estimation?

## Architecture Onboarding

- Component map:
  - **Candidate Generator**: Beam search module producing M candidates with sequence probabilities p(b(i)|x)
  - **Similarity Function**: NLI-based semantic similarity s(y', y'') ∈ [0,1] (DeBERTa-large, MNLI fine-tuned)
  - **Weight Calculator**: Normalized probability weights w_i = p(b(i)|x) / Σp(b(j)|x)
  - **Uncertainty Aggregator**: Probability-weighted dissimilarity (Equation 5), or variants (Eccentricity, EigVecDissimilarity, CoCoA)
  - **Reference Output**: Typically greedy decode y*, but can use top-1 beam (Appendix D.1)

- Critical path:
  1. Run beam search (width M=10 default) to get candidates and probabilities
  2. Compute greedy output y* (or use top-1 beam)
  3. Compute semantic similarity s(b(i), y*) for each candidate using NLI model
  4. Normalize beam probabilities to get weights w_i
  5. Aggregate: Û_D(y*|x) = Σ w_i (1 - s(b(i), y*))

- Design tradeoffs:
  - **Beam width M**: Larger M covers more mass but compute scales linearly; M=5-10 sufficient for short QA (Figure 4)
  - **Weight floor ε**: Prevents near-zero weights on tail beams but can introduce uniform-ization; no clear winner in ablations (Table 5)
  - **Similarity function**: NLI vs. cross-encoder (STS); NLI preferred in main experiments, but cross-encoder also works (Table 8)
  - **Reference output**: Greedy vs. top-1 beam; greedy used in main experiments, beam alternative in Appendix D.1

- Failure signatures:
  - **Near-zero uncertainty for M=1**: Beam reduces to greedy; dissimilarity is zero (comparing identical outputs)
  - **No improvement for long outputs**: When probability mass is diffuse, beam coverage drops and redundancy in multinomial sampling decreases (Figure 5)
  - **Negative PRR on some instruct models**: Beam variants can underperform on instruct models (Table 15, some datasets), suggesting overconfidence in aligned models

- First 3 experiments:
  1. **Reproduce redundancy analysis**: On a held-out QA dataset, compare duplicate rates between multinomial sampling and beam search across output length bins. Verify Figure 2 pattern holds.
  2. **Ablate M on single dataset**: Plot PRR vs. M (1-15) for Dissimilarity under both sampling methods. Confirm beam saturates faster and dominates for M≥2 (Figure 4 pattern).
  3. **Check probability mass threshold**: For your domain, compute m_B = Σp(b(i)|x) and plot distribution. Estimate what fraction of examples meet the m_B > 0.842 condition. This predicts where beam will help most.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the effectiveness of beam-weighted estimators generalize to longer-form generation tasks?
- **Basis in paper:** [explicit] The Limitations section states, "experiments are limited to short-form QA datasets, and the generalizability of our findings to longer-form generation remains an open question."
- **Why unresolved:** The method's advantages rely on high duplicate rates in short-form multinomial sampling. Longer texts inherently have fewer duplicates and different probability distributions, potentially altering the trade-offs between beam search and sampling.
- **What evidence would resolve it:** Empirical results on datasets requiring paragraph-length or page-length generation (e.g., summarization, long-form QA).

### Open Question 2
- **Question:** Can beam-weighted uncertainty estimation be adapted for black-box settings where internal model probabilities are inaccessible?
- **Basis in paper:** [explicit] The authors state in Limitations, "the methods could be extended to the black-box settings using empirical probability estimates."
- **Why unresolved:** The current importance-weighting estimator relies on the normalized sequence probabilities p(b^(i)|x), which requires log-probabilities (white-box access). API-only access usually prohibits retrieving these probabilities for generated sequences.
- **What evidence would resolve it:** A methodology approximating sequence probabilities (e.g., via surrogate models or frequency heuristics) that maintains the performance gain over multinomial sampling.

### Open Question 3
- **Question:** How does the gap between dissimilarity inside and outside the beam set (δ) affect the error bounds in practice?
- **Basis in paper:** [inferred] The theoretical analysis notes that the sufficient condition m_B > 1 - 1/(2√M) is frequently not met, yet beam search still outperforms sampling, implying the inside-outside dissimilarity gap δ is small.
- **Why unresolved:** The paper establishes a sufficient condition but suggests the "effective threshold is often lower" due to the gap parameter δ, which is not directly computable during inference.
- **What evidence would resolve it:** Analysis correlating the performance gap between beam and sampling estimators with the actual value of δ on diverse datasets to validate the relaxed condition.

## Limitations
- The theoretical guarantee only applies when beam probability mass exceeds 0.842 (22.7% of TriviaQA examples), with diminishing returns for longer outputs where probability mass is diffuse
- Performance varies across model families, with less improvement or negative results on some instruction-tuned models suggesting limited generalizability
- Method relies on semantic similarity for UQ, inheriting any biases or limitations of the similarity model and may not generalize well to domains where semantic similarity doesn't capture task-relevant uncertainty

## Confidence

- **High confidence:** The empirical demonstration that beam search reduces redundancy in multinomial sampling (Figure 2), and that probability-weighted estimators outperform uniform weighting across multiple datasets and models. The theoretical framework for the MSE advantage is mathematically sound for the specified conditions.

- **Medium confidence:** The claim of "state-of-the-art" UQ performance, as this is relative to the specific baselines and datasets tested. The method's superiority is consistent but the absolute gains vary (PRR improvements of 1-5% in most cases). The generalizability to non-QA tasks or longer-form generation remains untested.

- **Low confidence:** The robustness of the approach across diverse model families and tasks beyond short-form QA. The negative results on instruct models and the dependence on probability mass coverage suggest the method may not be universally applicable.

## Next Checks

1. **Probability Mass Distribution Analysis:** For your target domain, compute the distribution of beam probability mass m_B across examples. If less than 20-30% of examples meet the m_B > 0.842 threshold, expect limited theoretical gains and focus on empirical validation.

2. **Model Family Ablation:** Test the beam-weighted estimator on both base and instruction-tuned versions of your model. The negative results on instruct models suggest the method may be less effective for aligned models with better-calibrated probabilities.

3. **Task Complexity Scaling:** Evaluate the method on progressively longer outputs (e.g., 1-2 tokens → 3-4 → 5-7 → 8+ tokens). The theoretical and empirical results show clear diminishing returns as output length increases, so validate where the break point occurs in your specific application.