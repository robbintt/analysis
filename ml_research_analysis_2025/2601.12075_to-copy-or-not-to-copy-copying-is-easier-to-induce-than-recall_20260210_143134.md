---
ver: rpa2
title: 'To Copy or Not to Copy: Copying Is Easier to Induce Than Recall'
arxiv_id: '2601.12075'
source_url: https://arxiv.org/abs/2601.12075
tags:
- subj
- recall
- copy
- steered
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces an arbitration vector to control the choice
  between parametric knowledge and contextual information in retrieval-augmented language
  models. The vector is computed from the residual-stream centroid difference between
  irrelevant and relevant contexts across 27 relations, and injected as an additive
  intervention at selected layers and token spans.
---

# To Copy or Not to Copy: Copying Is Easier to Induce Than Recall

## Quick Facts
- arXiv ID: 2601.12075
- Source URL: https://arxiv.org/abs/2601.12075
- Reference count: 40
- Primary result: Inducing copying from context requires smaller steering magnitude (α=-3.0) than restoring parametric recall (α=+30.0), with success limited to counterfactual object position for recall restoration

## Executive Summary
This paper introduces an arbitration vector to control the choice between parametric knowledge and contextual information in retrieval-augmented language models. The vector is computed from the residual-stream centroid difference between irrelevant and relevant contexts across 27 relations, and injected as an additive intervention at selected layers and token spans. Experiments on two architectures (decoder-only and encoder/decoder) and two QA benchmarks show that inducing copying from context is an easier, low-resistance process requiring small negative scaling, while restoring parametric recall is more difficult, requiring larger positive scaling and succeeding only at object-token positions with significant perplexity spikes. The study reveals an asymmetry: copying is robust and can be triggered from any control point, while recall requires targeted jamming at the counterfactual object to suppress the copying signal.

## Method Summary
The method extracts an arbitration vector by computing the centroid difference between residual-stream activations when models encounter irrelevant vs. relevant contexts across 27 relations. This vector is then injected as an additive offset at specific layers and token spans during inference to steer the model between copying from context (Recall→Copy) or using parametric knowledge (Copy→Recall). The approach uses 7,642 samples from ParaRel relations with synthetic contexts, applying the vector to Gemma2-2B and T5Gemma-2B models on PopQA and EntityQuestions benchmarks. Interventions are evaluated using EM/F1 metrics, perplexity, and mechanistic analysis of attention patterns and MLP contributions.

## Key Results
- Copying induction requires α=-3.0 scaling while recall restoration requires α=+30.0 scaling, revealing a 10x asymmetry in control resistance
- Recall→Copy intervention works at multiple injection points (subject, object, last token), while Copy→Recall succeeds only at the counterfactual object position
- Attention analysis shows jamming at object position collapses attention toward o′ and redirects to subject, enabling parametric knowledge to re-emerge
- Perplexity spikes (>100× baseline) occur with large scaling magnitudes, indicating representation corruption risk

## Why This Works (Mechanism)

### Mechanism 1: Arbitration Vector Captures Source-Selection Subspace
- The difference vector between irrelevant-context (IC) and relevant-context (RC) activation centroids isolates a directional subspace that encodes the model's arbitration decision between parametric recall and contextual copying. During forward passes, when the model encounters irrelevant context, it maintains representations aligned with internal knowledge retrieval. When it encounters relevant-but-false context, it shifts representations toward copying. The centroid difference extracts this consistent directional offset across 27 relations. Adding scaled versions of this vector shifts the residual stream toward either regime. The subspace is approximately linear and transferable across relations and entities within the tested distribution.

### Mechanism 2: Asymmetric Resistance in Behavioral Control
- Inducing copying (Recall→Copy) requires substantially smaller intervention magnitude (α=-3.0) and tolerates multiple injection points, while restoring recall (Copy→Recall) requires ~10× larger magnitude (α=+30.0) and succeeds only at the counterfactual object token. Copying is the model's "default" response to salient context—it amplifies existing attention pathways (reactivation). Recall suppression must overcome this dominant context-attention signal (jamming), which requires aggressive scaling at precisely the token where context-to-answer attribution is locked. The asymmetry reflects circuit-level competition where context-copying circuits have lower activation thresholds than parametric-recall circuits in the tested models.

### Mechanism 3: Attention Jamming at Counterfactual Object Enables Recall Restoration
- Successful Copy→Recall intervention at the counterfactual object (o′) causes an immediate collapse in attention toward o′ in context, followed by redirected attention toward the query subject, enabling parametric knowledge to re-emerge. At the object position, the steering vector disrupts the attention head(s) responsible for context-to-answer attribution. This "blinds" the model to the misleading context signal. Subsequent layers then amplify the weaker parametric signal, visible as increased MLP contribution to the ground-truth object at later layers. Attention routing to o′ is the critical bottleneck; disrupting it is both necessary and sufficient for recall restoration when the model has parametric knowledge of the true answer.

## Foundational Learning

- **Residual Stream and Additive Interventions**: Understanding how additive offsets shift representations is essential since the arbitration vector is computed from and injected into residual-stream activations. Quick check: If you add vector v to hidden state h at layer ℓ, what happens to all downstream computations that depend on h?
- **Centroid Contrast and Linear Separability**: The method assumes IC and RC conditions produce separable activation clusters whose difference vector captures the control signal. Quick check: Why compute centroids across many samples rather than using a single contrastive pair?
- **Attention Pattern Interpretation**: Mechanistic analysis relies on reading attention probabilities to diagnose whether the model attends to context (copying) or subject (recall). Quick check: If attention from the last token to o′ drops from 0.8 to 0.1 after intervention, what does this suggest about the model's information source?

## Architecture Onboarding

- **Component map**: Extraction -> Centroid Computation -> Arbitration Vector -> Injection -> Behavior Control -> Monitoring
- **Critical path**: 1) Build arbitration dataset with matched IC/RC pairs (7,642 samples across 27 relations) 2) Extract centroids μ^IC and μ^RC at each layer and position; compute v = μ^IC - μ^RC 3) Identify optimal layer-position pairs via grid search (early layers for Copy→Recall; mid layers for Recall→Copy) 4) Apply intervention with scaling sweep; validate on held-out benchmark samples 5) Run mechanistic analysis (attention, MLP, probability trajectories) on single-token subset
- **Design tradeoffs**: Scaling magnitude: α=±30.0 gives strong effect but risks perplexity explosion; α=±3.0 is safer but may fail for recall restoration. Extraction position: Object token most effective for both regimes; subject and last token work only for copying induction. Prompt topology: Query-first vs. context-first affects baseline behavior but intervention effects remain consistent. Relation coverage: Vector computed from 27 relations generalizes to PopQA/PEQ; unknown for out-of-domain relations.
- **Failure signatures**: Perplexity spikes (>100× baseline): Scaling too large; representations corrupted. No behavior change despite intervention: Layer or position wrong; scaling too small. Behavior change without targeted effect (e.g., random outputs): Vector direction not meaningful; re-extract with cleaner dataset separation.
- **First 3 experiments**: 1) Replicate centroid extraction on a single relation (e.g., P30: continent) with 50 IC/RC pairs; verify v produces directional shift at α=±10 2) Sweep layers 0-24 at object position with α∈{-3, -10, +10, +30} on 20 PopQA samples; plot EM(o), EM(o′), and perplexity 3) Intervene at subject vs. object vs. last token with α=+30; observe attention pattern changes to confirm jamming occurs only at object

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the arbitration vector mechanism scale to larger models or architectures explicitly trained for long-context or RAG tasks?
- **Basis in paper**: The authors acknowledge limiting the study to 2B models and standard instruction tuning, leaving larger scales and explicit RAG models unexplored.
- **Why unresolved**: It is unclear if the 10x difference in scaling factors ($\alpha=-3.0$ vs $\alpha=+30.0$) is an artifact of model size or a fundamental property of the architecture.
- **What evidence would resolve it**: Replicating the intervention on larger variants (e.g., 7B–70B) and specialized RAG architectures to compare scaling factors and perplexity trade-offs.

### Open Question 2
- **Question**: Can the arbitration vector successfully suppress context use in realistic RAG pipelines with noisy retrievers, or does it rely on the clean synthetic setup used in the paper?
- **Basis in paper**: The study emulates retrieval via prompt injection and explicitly notes it does not model retriever failures, ranking, or passage selection.
- **Why unresolved**: The intervention was tested on synthetic templates (Archive/Authority) rather than the messy output of dense/sparse retrievers, which may introduce confounding activation noise.
- **What evidence would resolve it**: Evaluating the Copy→Recall steering success on an end-to-end pipeline using standard dense retrieval on benchmarks like Natural Questions with realistic retrieved contexts.

### Open Question 3
- **Question**: Is the arbitration vector independent of the system prompt, or does it encode the specific "copy vs. recall" instructions used during the dataset curation and extraction process?
- **Basis in paper**: The authors note that the explicit system prompt ("You are a context-grounded QA model...") may shape the internal decision boundary and suggest investigating vector robustness across prompts.
- **Why unresolved**: The vector is computed from centroids generated under a specific prompt; if the prompt shifts the activation basis, the vector may lose its steering effect.
- **What evidence would resolve it**: Extracting vectors using diverse system prompts and testing cross-prompt transferability (e.g., training on prompt A and steering with prompt B).

## Limitations

- The arbitration vector's transferability across domains remains unproven, with performance on out-of-domain knowledge or different relation types unknown
- The intervention's success depends heavily on precise token-level alignment between the arbitration vector and target positions, with limited discussion of how multi-token entities or subword tokenization variations might affect effectiveness
- The scaling asymmetry (α=-3.0 vs α=+30.0) appears robust within tested models but may be architecture-specific rather than fundamental

## Confidence

- **High**: The asymmetric scaling requirements between copying induction and recall restoration are consistently observed across both Gemma2-2B and T5Gemma-2B architectures on two independent benchmarks
- **Medium**: The mechanism that attention jamming at the counterfactual object is both necessary and sufficient for recall restoration is supported by attention pattern analysis but could involve additional pathways not captured in the current analysis
- **Low**: The claim that the arbitration vector captures a generalizable "source-selection subspace" across all RAG scenarios is based on limited relation coverage and may not extend to domains with different knowledge-context interaction patterns

## Next Checks

1. **Cross-Domain Generalization Test**: Apply the extracted arbitration vector to a completely different domain (e.g., scientific facts or numerical reasoning) and measure whether the same scaling asymmetry holds. This would validate whether the vector captures domain-general arbitration mechanisms or is overfit to Wikipedia-style relations.

2. **Architecture-Agnostic Validation**: Test the intervention on a third architecture (e.g., LLaMA or Mistral) with different training objectives. If the asymmetric scaling persists, it suggests fundamental circuit-level differences between copying and recall; if not, the effect may be model-specific.

3. **Temporal Stability Analysis**: Track how the arbitration vector and its effectiveness change over multiple training epochs or fine-tuning stages. This would reveal whether the copying-recall asymmetry is a stable architectural feature or emerges from specific training dynamics.