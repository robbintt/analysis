---
ver: rpa2
title: 'Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge
  in LLMs'
arxiv_id: '2505.17512'
source_url: https://arxiv.org/abs/2505.17512
tags:
- uni00000013
- uni00000011
- uni00000019
- undercover
- conceptual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CK-Arena, a game-based benchmark for evaluating
  conceptual knowledge in large language models (LLMs). The benchmark addresses the
  gap in existing evaluations that focus on factual recall rather than conceptual
  reasoning.
---

# Probe by Gaming: A Game-based Benchmark for Assessing Conceptual Knowledge in LLMs

## Quick Facts
- arXiv ID: 2505.17512
- Source URL: https://arxiv.org/abs/2505.17512
- Reference count: 40
- This paper introduces CK-Arena, a game-based benchmark for evaluating conceptual knowledge in large language models (LLMs).

## Executive Summary
CK-Arena addresses a critical gap in LLM evaluation by moving beyond factual recall to assess conceptual reasoning through adversarial gameplay. The benchmark employs the Undercover game format where models must describe and differentiate between related concepts while identifying undercover agents. Through structured metrics for novelty, relevance, and reasonableness, CK-Arena provides a nuanced evaluation of how well models understand conceptual boundaries and relationships. Experiments reveal that conceptual reasoning capabilities do not strictly correlate with model size, challenging conventional assumptions about scaling laws in LLMs.

## Method Summary
CK-Arena is built on the Undercover game where 4 civilian agents share one concept while 2 undercover agents receive a related but distinct concept. Agents take turns describing their concepts, and all players vote to eliminate suspected undercovers. The game uses LLM judges (GPT-4.1 and Claude-3.7) to score statements on novelty, relevance, and reasonableness, with statements below thresholds triggering automatic elimination. The benchmark includes 529 English concept pairs across 11 categories, and performance is measured through win rate, survival rate, and statement-level scores. The evaluation framework combines automated scoring with human review for cases of high judge disagreement.

## Key Results
- DeepSeek-V3 achieved the highest overall performance across all tested LLMs
- Qwen2.5-32B outperformed Qwen2.5-72B, demonstrating that conceptual reasoning does not strictly correlate with model size
- Statement-level evaluation revealed significant variation in how models handle novelty versus relevance tradeoffs
- Human review of high-variance cases showed strong agreement with LLM judges, validating the automated evaluation approach

## Why This Works (Mechanism)

### Mechanism 1: Forced Differentiation via Adversarial Role-Play
- **Claim:** The "Undercover" game structure forces models to demonstrate conceptual understanding of boundaries and commonalities more effectively than static recall tasks.
- **Mechanism:** By assigning "civilians" a concept (e.g., Soccer) and "undercover" agents a related but distinct concept (e.g., Basketball), the model must identify shared semantic features to survive (if undercover) or distinct features to win (if civilian). Success requires navigating the "semantic proximity" of the pair.
- **Core assumption:** Models that rely on surface-level token prediction without relational understanding will fail to maintain strategic ambiguity or specificity, leading to detection.
- **Evidence anchors:**
  - [Abstract]: "CK-Arena challenges models to describe, differentiate, and infer conceptual boundaries... encouraging models to explore commonalities and distinctions."
  - [Section 3.1]: "If the player merely relies on token-based generation without grasping these relationships, they are more likely to expose their undercover role."
  - [Corpus]: The "Decrypto Benchmark" (arXiv:2506.20664) supports the validity of multi-agent games for testing Theory of Mind, though CK-Arena specifically targets conceptual boundaries rather than general reasoning.
- **Break condition:** If the concept pairs are too dissimilar (low semantic proximity), the game becomes a trivial classification task; if too similar, it becomes a guessing game unrelated to reasoning.

### Mechanism 2: Threshold-Based Elimination for Quality Control
- **Claim:** Automated evaluation via LLM-judges using strict thresholds for novelty and reasonableness filters out low-effort descriptions and hallucinations, ensuring data quality.
- **Mechanism:** Statements are scored on a 0-1 scale. If a statement falls below a "reasonableness" or "novelty" threshold, the player is automatically eliminated. This forces the agent to generate high-quality, non-repetitive output.
- **Core assumption:** The "Judge" LLMs (e.g., GPT-4.1, Claude-3.7) are sufficiently capable of evaluating nuance, and their variance is manageable via human review triggers.
- **Evidence anchors:**
  - [Section 3.3]: "Statements falling below the novelty threshold result in automatic elimination... preventing players from making misleading or nonsensical claims."
  - [Section 4]: "A total of 163 statements met this criterion [score variance > 0.04] and were re-evaluated by human reviewers."
- **Break condition:** If the Judge model shares the same biases or blind spots as the Player model, evaluations become circular (e.g., a model rating its own hallucinations as "reasonable").

### Mechanism 3: Strategic Reasoning Decoupled from Knowledge
- **Claim:** Game performance reveals that conceptual reasoning is not strictly correlated with parameter count, suggesting distinct capabilities for "knowledge storage" vs. "knowledge application."
- **Mechanism:** The paper observed that a smaller model (Qwen2.5-32B) outperformed a larger one (Qwen2.5-72B) in win rates. The game requires not just knowing the concept, but strategically deciding how much information to reveal - a capability that does not scale linearly with size.
- **Core assumption:** Winning the game is a valid proxy for "conceptual reasoning," and not just an artifact of the specific game strategy prompt.
- **Evidence anchors:**
  - [Section 4.2]: "Qwen2.5-32B consistently outperforms Qwen2.5-72B... suggesting that its superior performance is... indicative of better adaptability and strategic reasoning."
  - [Corpus]: "Layerwise Recall" (arXiv:2502.10871) discusses how knowledge is geometrically stored; CK-Arena suggests that accessing/using this geometry is a separate skill.
- **Break condition:** If the prompt engineering favors specific phrasing styles inherent to the 32B model's instruction tuning, the result reflects alignment rather than reasoning capability.

## Foundational Learning

- **Concept: Semantic Proximity & Concept Pairs**
  - **Why needed here:** The core dataset relies on pairs that are distinct but share attributes (e.g., "soccer" vs. "basketball"). Understanding how to construct or select these pairs is critical for valid benchmarking.
  - **Quick check question:** Can you explain why pairing "Apple" with "Orange" is a better test of conceptual nuance than pairing "Apple" with "Car"?

- **Concept: Theory of Mind (ToM) in Agents**
  - **Why needed here:** The player agent must infer the intent of other players ("Are they describing my concept or a different one?") to vote correctly.
  - **Quick check question:** How does an agent decide if a vague statement ("It is a sphere") is a safe civilian move or an undercover bluff?

- **Concept: LLM-as-a-Judge Reliability**
  - **Why needed here:** The system relies on LLMs to score "novelty" and "reasonableness." Understanding the limitations and variance of these judges is essential for interpreting results.
  - **Quick check question:** If two judge models disagree significantly (variance > 0.04) on a statement's "reasonableness," what is the standard protocol to resolve it?

## Architecture Onboarding

- **Component map:** Game Master -> Player Agents -> Judge Agents -> Review Layer -> Voting System
- **Critical path:**
  1. Load concept pairs (Section 3.2)
  2. Initialize agents with prompts (Appendix D)
  3. **Loop:** Agent generates statement -> Judge scores statement -> Check elimination thresholds -> Vote
  4. **Exit:** Win condition met (Undercover eliminated or equal numbers)
- **Design tradeoffs:**
  - **Evaluation Cost:** Using two high-end LLMs as judges (plus players) makes the benchmark expensive to run at scale
  - **Static vs. Dynamic:** The concept list is manually curated (529 pairs) for quality, but this limits the "infinite" scalability seen in purely synthetic benchmarks
  - **Model Dependency:** Relying on specific proprietary models (GPT-4.1, Claude-3.7) for judging creates a dependency that may affect reproducibility as those models update
- **Failure signatures:**
  - **Format Collapse:** The model outputs narrative text instead of the required JSON format (Appendix E notes a retry mechanism is needed)
  - **Circular Reasoning:** Models repeating the same descriptions, leading to low Novelty scores and auto-elimination
  - **Identity Confusion:** The model fails to infer its own role or the relationship between concepts, leading to random voting
- **First 3 experiments:**
  1. **Sanity Check (Identity Preservation):** Run a single game with identical models playing all roles to ensure the game loop and judge scoring work without crashes
  2. **Metric Correlation:** Run 50 games and correlate the "Win Rate" with the "Reasonableness" score to see if simply being logical is enough to win, or if "Novelty" is the true differentiator
  3. **Size Scaling Test:** Replicate the Qwen2.5-32B vs. 72B experiment (Section 4.2) using a different model family (e.g., LLaMA variants) to verify if the "smaller model winning" phenomenon is consistent or an artifact of the Qwen training pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can CK-Arena's game-based evaluation paradigm be effectively extended to assess verb-based or abstract concepts, not just nouns?
- **Basis in paper:** [explicit] Appendix B states: "Extending the framework to assess verbs or abstract concepts remains an open challenge."
- **Why unresolved:** Current design assumes concepts with rich, describable attributes suitable for the Undercover game format; verbs and abstract concepts may require fundamentally different interaction mechanics.
- **What evidence would resolve it:** Successful adaptation of game rules and metrics to verb/abstract concept pairs, with pilot experiments showing discriminative power between models.

### Open Question 2
- **Question:** What specific training data characteristics, architectural features, or alignment strategies enable smaller models to sometimes outperform larger ones in conceptual reasoning tasks?
- **Basis in paper:** [inferred] The paper finds Qwen2.5-32B consistently outperforms Qwen2.5-72B in CK-Arena, challenging assumptions about scaling. The authors suggest "model design and data alignment over raw scale" matter but do not isolate causal factors.
- **Why unresolved:** The paper presents the phenomenon but does not conduct ablation studies or analyze training corpora to identify mechanisms.
- **What evidence would resolve it:** Controlled experiments varying specific training data types, architectural components, or alignment procedures while measuring CK-Arena performance.

### Open Question 3
- **Question:** How do cross-linguistic and cultural differences affect LLMs' conceptual reasoning abilities, and does CK-Arena reveal systematic gaps when adapted to non-English languages?
- **Basis in paper:** [explicit] Appendix A states: "Different languages are deeply tied to unique cultural knowledge and conceptual representations, which can reveal cross-linguistic differences in conceptual understanding."
- **Why unresolved:** All current experiments are English-only, limiting understanding of whether conceptual reasoning transfers across languages or is language-specific.
- **What evidence would resolve it:** Multilingual CK-Arena experiments comparing models' performance across languages, with analysis of categories where performance diverges.

### Open Question 4
- **Question:** Can the concept-driven statements generated during CK-Arena gameplay serve as effective training data for improving concept-aware models (Large Concept Models)?
- **Basis in paper:** [explicit] Appendix A states these descriptions "can form a semantic norm, potentially serving as raw data for training concept-aware models, such as Large Concept Models."
- **Why unresolved:** The dataset exists but has not been used for training; it is unclear whether gameplay-generated descriptions contain sufficient quality and structure for knowledge transfer.
- **What evidence would resolve it:** Training experiments using CK-Arena statements to fine-tune or train models, followed by evaluation on independent conceptual reasoning benchmarks.

## Limitations

- Benchmark relies on proprietary LLM judges (GPT-4.1, Claude-3.7) that may change over time, affecting reproducibility
- Manual curation of 529 concept pairs ensures quality but limits scalability and may introduce selection bias
- The game's effectiveness critically depends on semantic proximity of concept pairs, with too-similar or too-dissimilar pairs breaking the evaluation
- Single comparison of Qwen2.5-32B vs 72B may reflect prompt alignment artifacts rather than genuine reasoning capability differences

## Confidence

- **High Confidence:** The benchmark's game mechanics and evaluation framework are well-specified and reproducible with the provided code. The finding that conceptual understanding doesn't strictly correlate with model size is supported by multiple model comparisons.
- **Medium Confidence:** The claim that the game structure uniquely tests conceptual boundaries versus factual recall, as this requires deeper analysis of model behavior beyond win rates.
- **Low Confidence:** The assertion that smaller models demonstrate superior "strategic reasoning" - this may reflect instruction tuning artifacts rather than genuine reasoning capability.

## Next Checks

1. **Threshold Sensitivity Analysis:** Systematically vary the novelty/reasonableness elimination thresholds to determine how robust the game's discrimination power is to these parameters.
2. **Cross-Category Consistency:** Test whether model performance rankings remain consistent across all 11 concept categories, or if certain domains (e.g., sports vs. electronics) disproportionately affect results.
3. **Human Agreement Study:** Have human evaluators score a subset of statements to determine the correlation between LLM judges and human judgment, establishing the reliability of the automated evaluation.