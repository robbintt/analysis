---
ver: rpa2
title: Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent
  Systems
arxiv_id: '2506.02718'
source_url: https://arxiv.org/abs/2506.02718
tags:
- rollout
- agent
- group
- training
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses optimizing multi-agent LLM systems for search
  tasks, where traditional RL methods like MAPPO struggle due to critic instability
  and high computational overhead. To tackle this, the authors propose Multi-Agent
  Heterogeneous Group Policy Optimization (MHGPO), a critic-free algorithm that leverages
  relative reward advantages over heterogeneous rollout groups to guide policy updates.
---

# Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems

## Quick Facts
- **arXiv ID**: 2506.02718
- **Source URL**: https://arxiv.org/abs/2506.02718
- **Reference count**: 17
- **Primary result**: MHGPO achieves up to 49.7% accuracy on multi-agent search tasks, outperforming MAPPO by eliminating critic networks and using relative reward advantages within heterogeneous rollout groups.

## Executive Summary
This paper introduces Multi-Agent Heterogeneous Group Policy Optimization (MHGPO), a critic-free reinforcement learning algorithm designed for LLM-based multi-agent systems. Traditional methods like MAPPO suffer from critic instability and high computational overhead, particularly in heterogeneous multi-agent settings. MHGPO addresses these challenges by estimating advantages via relative rewards computed within heterogeneous rollout groups, eliminating the need for a value function network. The method employs three group sampling strategies—Independent Sampling, Fork-on-first, and Round-robin—to balance exploration and efficiency. Experiments on a three-agent search system using HotpotQA and other datasets demonstrate that MHGPO consistently outperforms MAPPO in both task performance and computational efficiency, achieving up to 49.7% accuracy without requiring warm-up training.

## Method Summary
MHGPO is a critic-free MARL algorithm that optimizes multi-agent LLM systems by computing relative reward advantages within heterogeneous rollout groups. The method uses a shared LLM backbone instantiated across multiple agents (e.g., Rewriter, Reranker, Answerer) with different prompts. During training, multiple rollouts are generated using one of three sampling strategies: Independent Sampling (IS), Fork-on-first (FoF), or Round-robin (RR). Each agent's output is collected with group identifiers, and terminal rewards (e.g., F1 scores) are propagated backward through the agent chain. Advantages are computed as standardized within-group relative rewards, replacing the traditional critic-based advantage estimation. The shared policy is updated using a GRPO-style clipped objective with KL penalty. The approach eliminates the critic network, reducing memory usage by ~45% and avoiding critic-induced instability in heterogeneous settings.

## Key Results
- MHGPO eliminates the critic network, reducing GPU memory usage from 82.2% (MAPPO) to 47.5-57.9% (MHGPO variants).
- The method achieves up to 49.7% accuracy on multi-agent search tasks without warm-up training.
- All three sampling strategies (IS, FoF, RR) outperform MAPPO, with FoF and RR achieving higher ceilings due to end-to-end optimization capabilities.

## Why This Works (Mechanism)

### Mechanism 1: Critic-Free Advantage Estimation via Relative Rewards
- Claim: Eliminating the critic network reduces memory by ~45% and avoids critic-induced instability in heterogeneous multi-agent settings.
- Mechanism: Instead of training a value function V(s) to estimate advantages via GAE, MHGPO computes advantages as standardized within-group relative rewards: Â = (R - mean(group)) / std(group). This replaces learned value estimation with direct comparison across sampled trajectories.
- Core assumption: The group size G=4 is sufficient for stable mean/variance estimation; rewards are scalar and comparable across heterogeneous agent outputs.
- Evidence anchors:
  - [abstract] "MHGPO eliminates the critic network from MARL by estimating advantages via relative rewards computed within heterogeneous rollout groups."
  - [page 5, figure 6] GPU memory usage drops from 82.2% (MAPPO) to 47.5-57.9% (MHGPO variants).
  - [corpus] Related work on GRPO confirms group-based advantage estimation works in discrete action spaces, but continuous control remains unexplored—suggesting mechanism generalization is not universally proven.
- Break condition: If group rewards have high variance or are not meaningfully comparable (e.g., different reward scales across agents), advantage estimates become noisy, potentially destabilizing training.

### Mechanism 2: Backward Reward Propagation for Credit Assignment
- Claim: Propagating terminal rewards backward through agent chains enables end-to-end optimization without per-step reward models.
- Mechanism: For each trajectory, terminal reward R_shared flows backward: each agent's reward aggregates from direct successors via R^shared_{k,i} = Aggr({R^shared_{j,r}}_{j>k}). Agent-specific format penalties are added locally.
- Core assumption: The aggregation operator (averaging in implementation) fairly distributes credit; all downstream contributions are equally weighted.
- Evidence anchors:
  - [page 3, equation 4] Formal definition of backward propagation with aggregation from successors.
  - [page 7, figure 10] Case study shows Rewriter learns to generate fewer, more retrieval-aligned queries by step 160—indicating reward signals propagate meaningfully to early agents.
  - [corpus] No direct corpus validation for this specific propagation scheme in MAS; mechanism remains empirically supported but not theoretically proven.
- Break condition: If agent contributions are highly asymmetric or some agents are irrelevant to final outcomes, uniform aggregation may misallocate credit, slowing or misdirecting learning.

### Mechanism 3: Heterogeneous-to-Homogeneous Group Convergence
- Claim: Heterogeneous groups naturally evolve toward homogeneous groups during training, enabling stable advantage computation even for downstream agents.
- Mechanism: As RL rejection sampling progresses, output diversity within groups decreases (outputs converge toward high-reward modes). Downstream agents eventually receive near-identical inputs, making their groups de facto homogeneous.
- Core assumption: Top-n sampling with trained policy reduces intra-group diversity monotonically.
- Evidence anchors:
  - [page 7, figure 9] Intra-group similarity increases from 44.57%→71.48% (Rewriter), 4.23%→75.98% (Reranker), 34.84%→71.95% (Answerer) over training.
  - [page 7] "This effect arises from the nature of rejection sampling in RL: over time, the diversity of outputs generated through top-n sampling diminishes."
  - [corpus] On the Hidden Objective Biases of Group-based RL notes structural mismatches between reward optimization and training objectives in GRPO—suggesting convergence dynamics may hide subtle biases.
- Break condition: If exploration is insufficient or reward landscape is deceptive, groups may converge to suboptimal modes before discovering better policies.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO) and Actor-Critic Architecture**
  - Why needed here: MHGPO is explicitly designed as a critic-free alternative to MAPPO; understanding PPO's clipped objective, GAE, and critic role clarifies what MHGPO removes and why.
  - Quick check question: Can you explain why the critic network in PPO estimates advantages rather than using raw rewards directly?

- Concept: **Multi-Agent Reinforcement Learning (MARL) with Parameter Sharing**
  - Why needed here: MHGPO uses a shared LLM backbone for all agents; understanding CTDE (centralized training, decentralized execution) and parameter sharing explains the training efficiency gains.
  - Quick check question: Why might parameter sharing cause interference between heterogeneous agent tasks, and how does MHGPO's group structure mitigate this?

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: MHGPO extends GRPO from single-agent to multi-agent settings; the relative advantage computation is directly inherited.
  - Quick check question: In GRPO, why must all samples in a group share the same input? How does MHGPO relax this constraint?

## Architecture Onboarding

- Component map:
  - **Shared LLM Backbone**: Single LLM (Llama3.1-8B in experiments) instantiates all agents via different prompts.
  - **Agent Prompts**: Rewriter (query decomposition), Reranker (document selection), Answerer (final response).
  - **Rollout Sampler**: Implements IS/FoF/RR strategies to generate G trajectories per input.
  - **Reward Module**: Computes F1-based terminal reward + agent-specific format penalties.
  - **Optimizer**: Applies GRPO-style objective with aggregated losses across all agent outputs.

- Critical path:
  1. Sample question → 2. Execute rollout sampling (fork at designated agent) → 3. Collect all agent outputs with group identifiers → 4. Compute terminal rewards → 5. Backward propagate rewards to each agent → 6. Compute relative advantages within groups → 7. Update shared backbone via aggregated policy gradient.

- Design tradeoffs:
  - **IS vs FoF vs RR**: IS optimizes each agent independently (faster convergence, lower ceiling); FoF enables end-to-end optimization (slower, higher ceiling); RR balances both with probabilistic forking.
  - **Group size G**: Larger G improves advantage estimation but increases rollout cost. Paper uses G=4 as default.
  - **Warm-up**: MHGPO explicitly avoids SFT warm-up; tradeoff is potential early instability (mitigated by group-based estimation).

- Failure signatures:
  - **Critic-free instability**: If group variance is too low (collapsed outputs), advantages approach 0/0—check std({R}) before division.
  - **Format collapse**: MAPPO showed format penalties spiking mid-training; monitor agent-specific penalties per step.
  - **Retrieval misalignment**: If Rewriter queries don't match retriever behavior, downstream agents receive garbage—inspect retrieval results at multiple training stages.

- First 3 experiments:
  1. **Baseline replication**: Implement 3-agent MASS with Llama3.1-8B on HotpotQA subset (500 samples), compare unoptimized vs MHGPO-FoF for 50 steps. Verify F1 improvement and memory reduction.
  2. **Ablation on sampling strategies**: Run IS, FoF, RR on same data; plot convergence curves and final F1. Confirm FoF/RR outperform IS on ceiling performance.
  3. **Group size sensitivity**: Test G ∈ {2, 4, 8, 16} with FoF; measure tradeoff between rollout time and final performance. Identify G where marginal gains flatten.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does MHGPO maintain its efficiency and stability advantages when applied to non-sequential multi-agent topologies (e.g., hierarchical or parallel graphs) with significantly more than three agents?
- Basis in paper: [explicit] The conclusion states, "Although our detailed analysis and verification were solely carried out on MASS [Multi-Agent Search System]... Experiments on MAS within more scenarios will be reserved for future work."
- Why unresolved: The empirical validation is restricted to a specific, sequential three-agent pipeline (Rewriter, Reranker, Answerer), leaving scalability to complex, non-sequential architectures unproven.
- What evidence would resolve it: Application of MHGPO to complex benchmarks like multi-agent coding or collaborative gaming environments with >5 agents and non-sequential interaction graphs.

### Open Question 2
- Question: Is the empirical shift from heterogeneous to homogeneous groups during training (induced by rejection sampling) a necessary condition for MHGPO's convergence, or does it introduce a risk of mode collapse?
- Basis in paper: [inferred] The paper observes that "heterogeneous groups naturally evolve into homogeneous ones" and hypothesizes this drives performance gains, but it does not analyze if this reduction in diversity limits the exploration of superior policies.
- Why unresolved: While the paper links increased intra-group similarity to better performance, it does not isolate whether this transition helps convergence or merely reflects the LLM's tendency to narrow outputs, potentially missing the global optimum.
- What evidence would resolve it: An ablation study measuring policy entropy and diversity metrics against final task performance, specifically regulating the "temperature" of sampling to control the rate of homogenization.

### Open Question 3
- Question: How does the performance of MHGPO scale with the group size $G$, and does a larger group size effectively trade off against the elimination of the Critic network in terms of variance reduction?
- Basis in paper: [inferred] The authors set a default group size $G=4$ and mention that "marginally longer rollout durations" occur, but they do not analyze the sensitivity of the relative advantage estimation to this hyperparameter.
- Why unresolved: The reliability of the relative advantage calculation (Equation 5) depends on the statistical properties of the sampled group; it is unclear if small $G$ introduces high variance that a Critic would have smoothed out.
- What evidence would resolve it: A hyperparameter sweep of $G$ (e.g., from 2 to 16) comparing gradient variance and task performance against MAPPO to find the efficiency boundary.

### Open Question 4
- Question: Can the Round-robin (RR) sampling probabilities be adapted dynamically during training rather than fixed heuristically, to better balance the exploration-exploitation trade-off at different agent layers?
- Basis in paper: [inferred] The paper notes that RR "strikes a balance" and assigns static probabilities (0.7, 0.1, 0.2), but the optimal balance likely shifts as different agents (Rewriter vs. Reranker) learn at different rates.
- Why unresolved: Static probabilities assume a constant importance for forking at each agent, which contradicts the observation that agents converge at different speeds and require different exploration levels over time.
- What evidence would resolve it: Implementation of an adaptive scheduling mechanism for fork probabilities $p_i$ based on agent-specific loss curves or validation scores.

## Limitations
- The method's scalability to non-sequential multi-agent topologies with more than three agents remains unproven, as all experiments focus on a specific three-agent pipeline.
- The convergence of heterogeneous groups to homogeneous ones, while observed, lacks theoretical guarantees and may risk mode collapse in deceptive reward landscapes.
- The sensitivity of relative advantage estimation to group size G is not thoroughly analyzed, leaving uncertainty about the variance-reduction tradeoff compared to critic-based methods.

## Confidence
- **High Confidence**: Computational efficiency gains (GPU memory, runtime) and F1/EM improvements vs MAPPO are well-documented and reproducible.
- **Medium Confidence**: The three sampling strategies show distinct tradeoffs, but the optimal choice (FoF vs RR) may be dataset-dependent; limited ablation across tasks weakens generalization claims.
- **Low Confidence**: Claims about natural convergence of heterogeneous groups to homogeneous ones are observational; the mechanism lacks theoretical grounding for diverse reward structures or non-linear agent interactions.

## Next Checks
1. Test MHGPO with >3 agents and non-linear reward dependencies to verify backward propagation stability and group convergence beyond the studied 3-agent chain.
2. Implement a controlled ablation where agent-specific reward scales differ dramatically; measure impact on advantage estimation quality and training stability.
3. Compare MHGPO against MAPPO with critic regularization (e.g., entropy penalties) to isolate whether gains stem from critic removal vs group-based estimation.