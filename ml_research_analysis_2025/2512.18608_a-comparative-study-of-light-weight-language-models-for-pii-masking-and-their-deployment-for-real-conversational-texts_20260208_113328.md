---
ver: rpa2
title: A Comparative Study of Light-weight Language Models for PII Masking and their
  Deployment for Real Conversational Texts
arxiv_id: '2512.18608'
source_url: https://arxiv.org/abs/2512.18608
tags:
- mistral
- masking
- data
- arxiv
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates lightweight transformer models for PII masking,
  comparing T5-small and Mistral-Instruct-v0.3 fine-tuned on the AI4Privacy benchmark.
  Dataset variants with standardized labels show consistent performance improvements
  across architectures.
---

# A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts

## Quick Facts
- arXiv ID: 2512.18608
- Source URL: https://arxiv.org/abs/2512.18608
- Reference count: 32
- Lightweight transformer models achieve PII masking performance comparable to frontier LLMs on formal data, with trade-offs between accuracy and inference latency

## Executive Summary
This study evaluates lightweight transformer models for PII masking, comparing T5-small and Mistral-Instruct-v0.3 fine-tuned on the AI4Privacy benchmark. Dataset variants with standardized labels show consistent performance improvements across architectures. Mistral achieves higher F1 and recall with better robustness to conversational text but suffers from high inference latency (15.6s), while T5 offers faster inference (1.46s) with structured outputs at the cost of some accuracy. Both models reach performance comparable to frontier LLMs on formal data. Real-world deployment on a Discord bot reveals accuracy drops for informal inputs, highlighting the trade-off between robustness and efficiency. Lightweight models demonstrate viable PII masking with privacy benefits over black-box APIs.

## Method Summary
The study fine-tunes T5-small and Mistral-7B-v0.3 on the AI4Privacy PII-masking-200k dataset using LoRA parameter-efficient fine-tuning. Three dataset variants are created: normalized, replaced, and removed, with canonical label mapping reducing 225 raw categories to 24 standardized labels. T5 uses a "mask pii:" prefix prompt with 3 epochs training, while Mistral employs instruction templates with 4-bit quantization and LoRA rank=32. Models are evaluated on span-correct, label-exact, and sequence-level metrics, plus real-world accuracy on Discord messages. Hardware consists of 2× Tesla T4 GPUs with mixed precision training.

## Key Results
- Label standardization consistently improves performance across architectures
- Mistral achieves higher F1 (0.926) and recall (0.915) with better robustness to conversational text
- T5 provides faster inference (1.46s vs 15.6s) with structured outputs
- Both models reach performance comparable to frontier LLMs on formal data
- Real-world deployment accuracy drops for informal inputs (T5: 0.788, Mistral: 0.876 on Discord)

## Why This Works (Mechanism)

### Mechanism 1: Label Standardization Reduces Semantic Redundancy
Canonical mapping of redundant PII labels (e.g., `[GIVEN_NAME_1]`, `[FIRST_NAME]` → `[PERSON_NAME]`) consistently improves detection performance across architectures by reducing the output vocabulary burden, allowing the model to learn more robust entity-type associations rather than memorizing label synonyms.

### Mechanism 2: Decoder-Only Pre-training Confers Robustness to Informal Text
Mistral-7B's extensive web-scale pre-training enables better generalization to noisy, conversational input than T5-small, which overfits to formal training distributions, as decoder-only models internalize diverse syntactic patterns and slang during pre-training that supports PII identification even when input deviates from fine-tuning distribution.

### Mechanism 3: LoRA Fine-tuning Preserves Base Knowledge While Enabling Task Adaptation
Parameter-efficient LoRA adaptation of Mistral-7B achieves SOTA BLEU (0.911) and precision (0.962) while keeping most parameters frozen, as low-rank decomposition matrices inserted into attention projections learn task-specific adjustments without overwriting pre-trained representations, preserving general language understanding.

## Foundational Learning

- **Concept: Sequence-to-Sequence (Seq2Seq) vs. Causal Language Modeling**
  - Why needed here: The paper compares T5 (encoder-decoder, bidirectional context) against Mistral (decoder-only, autoregressive). Understanding how each processes input determines expectations about boundary detection and output controllability.
  - Quick check question: Given an input with PII at the beginning and end, which architecture can attend to both simultaneously during encoding?

- **Concept: Entity-Level vs. Character-Level Evaluation Metrics**
  - Why needed here: The paper reports strict (exact boundary + type), relaxed (any overlap), and SPriV (leakage rate) metrics. Misinterpreting these leads to incorrect performance conclusions.
  - Quick check question: A model correctly identifies "John" as PERSON but masks "John Doe" instead of just "John"—is this a strict or relaxed match?

- **Concept: Precision-Recall Trade-off in Privacy Contexts**
  - Why needed here: The paper notes recall is often prioritized to prevent PII leakage, but over-redaction (low precision) reduces text utility. Deployment requires balancing these.
  - Quick check question: If a system has 0.99 recall but 0.60 precision, what is the primary privacy risk and what is the primary usability cost?

## Architecture Onboarding

- **Component map:** Input Text → Preprocessing (normalization optional) → Fine-tuned Model (T5-small OR Mistral-7B+LoRA) → Post-processing (mask token insertion) → Masked Output
- **Critical path:** 1) Dataset preparation (canonical label mapping + regex-based correction) 2) Model selection based on latency constraints 3) Fine-tuning with appropriate hyperparameters 4) Evaluation on held-out test set + informal text validation 5) Deployment with inference-time monitoring
- **Design tradeoffs:**
  | Dimension | T5-small | Mistral-7B+LoRA |
  |-----------|----------|-----------------|
  | Inference latency | 1.46s | 15.6s |
  | Real-world accuracy | 0.788 | 0.876 |
  | Training time | ~2h | ~12h |
  | GPU memory | Lower | Higher (even with 4-bit quantization) |
  | Output controllability | More structured | More fluent but variable |
  | Robustness to noise | Lower | Higher |
- **Failure signatures:** T5: Accuracy drops sharply on slang, typos, or non-standard syntax (0.97 → 0.788); Mistral (under-tuned): Generation loops, hallucination, over-redaction (Mistral_1: precision 0.647, length ratio 1.42); Both: Performance degrades on PII types underrepresented in training data
- **First 3 experiments:** 1) Fine-tune T5-small on normalized dataset variant; verify inference latency <2s and test-set accuracy >0.95 2) Create held-out set of 100+ informal messages; compare T5 vs. Mistral accuracy 3) Train Mistral with rank ∈ {8, 16, 32, 64}; plot precision/recall vs. rank

## Open Questions the Paper Calls Out

### Open Question 1
Can the robustness of lightweight models on informal text be improved without sacrificing low latency? The paper identifies T5's accuracy drop to 0.788 on Discord messages while maintaining ~1.46s inference time, but doesn't propose methods to close the generalization gap for the faster encoder-decoder architecture.

### Open Question 2
Can decoder-only models be optimized for real-time synchronous use? While the paper evaluates Mistral's accuracy, it doesn't explore model compression techniques to bridge the latency gap (15.6s) for high-stakes, real-time applications.

### Open Question 3
How do these architectures perform in multilingual PII masking contexts? The study scope was restricted to English, leaving the transferability of these fine-tuning strategies to other languages unknown.

## Limitations
- Canonical label mapping from 225 raw categories to 24 standardized labels is only partially specified
- Informal text performance relies on limited Discord deployment sample (76 messages) without systematic variation analysis
- LoRA rank selection appears based on unpublished ablation studies
- Study doesn't address computational cost differences between training approaches

## Confidence

**Confidence Labels for Major Claim Clusters:**
- **High Confidence:** Performance improvements from label standardization, latency measurements, and basic training procedures
- **Medium Confidence:** Comparative architecture performance claims (T5 vs. Mistral) supported by controlled experiments but may not generalize beyond specific dataset and deployment context
- **Low Confidence:** Claims about LoRA rank sufficiency and generalization to unseen PII types lack systematic validation

## Next Checks

1. **Label Mapping Validation:** Implement the complete canonical mapping function by analyzing the raw label distribution in the dataset and verifying that the 24 standardized categories cover all semantic equivalence groups without information loss.

2. **Informal Text Generalization Test:** Create a systematically varied test set of 500+ informal messages spanning different domains (social media, customer service, medical forums) with controlled levels of noise, slang, and typos to quantify architecture-specific robustness.

3. **LoRA Rank Ablation Study:** Train Mistral with rank ∈ {8, 16, 32, 64, 128} and plot precision, recall, and BLEU scores against inference latency and memory usage to identify the optimal trade-off point and validate that rank=32 represents the best balance.