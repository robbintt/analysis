---
ver: rpa2
title: 'iTIMO: An LLM-empowered Synthesis Dataset for Travel Itinerary Modification'
arxiv_id: '2601.10609'
source_url: https://arxiv.org/abs/2601.10609
tags:
- itinerary
- spatial
- categories
- popularity
- before
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study introduces iTIMO, a synthetic dataset for itinerary\
  \ modification tasks, addressing the lack of real-world need-to-modify itinerary\
  \ data. The method uses large language models (LLMs) to perturb real itineraries\
  \ via three operations\u2014REPLACE, ADD, DELETE\u2014guided by intents targeting\
  \ popularity, spatial distance, and category diversity."
---

# iTIMO: An LLM-empowered Synthesis Dataset for Travel Itinerary Modification

## Quick Facts
- arXiv ID: 2601.10609
- Source URL: https://arxiv.org/abs/2601.10609
- Reference count: 40
- Primary result: Synthetic dataset for itinerary modification using LLM-perturbed real itineraries, showing SFT improves small models

## Executive Summary
This paper introduces iTIMO, a synthetic dataset for travel itinerary modification tasks, addressing the scarcity of real-world "need-to-modify" itinerary data. The authors propose a perturbation method using large language models to modify real itineraries through three operations (REPLACE, ADD, DELETE) guided by user intents focused on popularity, spatial distance, and category diversity. The dataset is evaluated through hybrid metrics assessing both perturbation quality and intent alignment. Experiments demonstrate that supervised fine-tuning (SFT) can enhance smaller models to match or outperform larger reasoning models, though retrieval-augmented generation does not consistently improve performance.

## Method Summary
The iTIMO dataset is constructed by perturbing real itineraries using LLM-driven operations (REPLACE, ADD, DELETE) guided by three intent categories: popularity, spatial distance, and category diversity. The perturbation pipeline is evaluated using hybrid metrics that assess both the quality of modifications and their alignment with specified intents. Supervised fine-tuning is applied to smaller models (e.g., 8B parameters), enabling them to achieve performance comparable to or exceeding larger reasoning models. Retrieval-augmented generation is also explored but does not consistently enhance results.

## Key Results
- SFT improves smaller models (e.g., 8B) to match or outperform larger reasoning models
- Retrieval-augmented generation does not consistently enhance performance
- Dataset and benchmarks advance travel recommendation systems toward dynamic, adaptive planning

## Why This Works (Mechanism)
The method leverages LLMs to generate realistic itinerary modifications by perturbing real-world data, ensuring the synthetic dataset reflects plausible user needs. The perturbation operations (REPLACE, ADD, DELETE) are guided by specific intents (popularity, spatial distance, category diversity), which align with common travel planning goals. Hybrid evaluation metrics ensure that modifications are both semantically valid and intent-aligned, making the dataset robust for training and evaluation.

## Foundational Learning
- **Itinerary Modification**: Adjusting travel plans to meet user preferences or constraints. Needed to simulate real-world travel planning scenarios. Quick check: Verify perturbations maintain itinerary coherence.
- **LLM Perturbation**: Using language models to alter data while preserving semantic validity. Needed to generate realistic synthetic data. Quick check: Ensure perturbations align with specified intents.
- **Hybrid Evaluation Metrics**: Combining perturbation quality and intent alignment for comprehensive assessment. Needed to validate dataset utility. Quick check: Confirm metrics balance semantic validity and intent fidelity.
- **Supervised Fine-Tuning (SFT)**: Adapting smaller models to specific tasks using labeled data. Needed to improve model performance on itinerary modification. Quick check: Compare SFT-enhanced models against baseline reasoning models.
- **Retrieval-Augmented Generation**: Integrating retrieval mechanisms with generation to enhance outputs. Needed to explore performance improvements. Quick check: Test consistency of retrieval-augmentation benefits across perturbation types.

## Architecture Onboarding
- **Component Map**: Real itineraries -> LLM perturbation (REPLACE/ADD/DELETE) -> Intent-guided modifications -> Hybrid evaluation -> SFT training -> Model benchmarking
- **Critical Path**: Real itineraries → LLM perturbation → Intent-guided modifications → Hybrid evaluation → SFT training
- **Design Tradeoffs**: Synthetic data generation vs. real-world diversity; perturbation quality vs. intent alignment; model size vs. performance
- **Failure Signatures**: Inconsistent intent alignment; semantic incoherence in perturbations; overfitting to synthetic data
- **First Experiments**:
  1. Validate perturbation quality and intent alignment on a small subset of iTIMO.
  2. Compare SFT-enhanced small models against larger reasoning models on a held-out test set.
  3. Test retrieval-augmented generation consistency across perturbation types.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-generated perturbations may not fully capture real user modification needs, introducing potential bias.
- The claim that retrieval-augmented generation does not consistently enhance performance lacks granular analysis across perturbation types.
- Generalizability to non-synthetic or multimodal travel contexts remains untested.

## Confidence
- **High**: Dataset construction methodology and perturbation pipeline clarity
- **Medium**: SFT effectiveness in improving smaller models, given controlled experimental setup
- **Low**: Claims about retrieval-augmented generation’s inconsistent performance due to lack of detailed analysis

## Next Checks
1. Evaluate iTIMO on real-world user-generated modification queries to assess perturbation realism and intent coverage.
2. Conduct ablation studies isolating the impact of retrieval-augmentation across different model sizes and perturbation types.
3. Test the dataset’s utility on multimodal itinerary tasks (e.g., integrating user reviews or images) to measure robustness beyond text-based scenarios.