---
ver: rpa2
title: Compact and Efficient Neural Networks for Image Recognition Based on Learned
  2D Separable Transform
arxiv_id: '2505.06578'
source_url: https://arxiv.org/abs/2505.06578
tags:
- image
- layer
- used
- neural
- architecture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform

## Quick Facts
- arXiv ID: 2505.06578
- Source URL: https://arxiv.org/abs/2505.06578
- Authors: Maxim Vashkevich; Egor Krivalcevich
- Reference count: 14
- Primary result: MNIST classification accuracy of 98.53% with 15,698 parameters using ResLST-3 architecture

## Executive Summary
This paper introduces Learned Separable Transform (LST) neural networks, which achieve compact and efficient image recognition by decomposing 2D transformations into sequential 1D operations with shared weights. The approach significantly reduces parameter count compared to traditional fully-connected layers while maintaining competitive accuracy on MNIST. A hardware implementation demonstrates the feasibility of efficient FPGA acceleration through weight sharing and in-place computation strategies.

## Method Summary
LST networks process images through a sequence of weight-shared fully-connected layers applied first to all rows, then to all columns, creating 2D separable transformations. This factorization reduces parameters from O(d²ᵢₙ × d²ₒᵤₜ) to O(dᵢₙ × dₒᵤₜ) per transformation stage. The architecture supports multiple LST blocks with optional skip connections (ResLST) and demonstrates effectiveness on MNIST with accuracies ranging from 98.02% to 98.53% while using only 9,474-15,698 parameters. Hardware implementation leverages weight reuse for FPGA efficiency.

## Key Results
- LST2D-1 achieves 98.02% accuracy with only 9,474 parameters
- ResLST-3 achieves 98.53% accuracy with 15,698 parameters
- FPGA implementation uses 6,473 LUTs (36.8%), 680 Flip-Flops (1.9%), and 29 RAMB18 (24.2%)
- 12-bit quantization preserves accuracy without degradation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separable weight sharing reduces parameter count by decomposing 2D transformations into sequential 1D operations.
- Mechanism: A single FC layer processes all rows with shared weights (W₁), producing intermediate representations. A second shared FC layer (W₂) then processes all columns. This factorizes what would require O(d²ᵢₙ × d²ₒᵤₜ) parameters in a traditional FC layer into O(dᵢₙ × dₒᵤₜ) per transformation stage.
- Core assumption: Image structure has separable row and column dependencies that can be learned independently.
- Evidence anchors: [abstract] "LST based on the idea of sharing the weights of one fullyconnected (FC) layer to process all rows of an image. After that, a second shared FC layer is used to process all columns"; [section III] "N = 2·(din+1)·dout" parameters for LST vs O(d²ᵢₙ × d²ₒᵤₜ) for FC
- Break condition: If images have strong non-separable diagonal features or require global spatial reasoning, the row-then-column factorization may fail to capture critical patterns.

### Mechanism 2
- Claim: Weight reuse enables hardware-friendly in-place computation.
- Mechanism: All row transformations share the same ROM-stored weights (W₁), enabling sequential processing with a single physical PE array. Column processing reuses the same architecture by transposing access patterns. This eliminates the need to store multiple weight matrices simultaneously.
- Core assumption: Sequential row/column processing preserves representational capacity while enabling memory reuse.
- Evidence anchors: [section IV] "The suggested architecture adopts the concept of in-place computation. Also, this approach allows to implement only one block for activation function calculation"; [section V.C] "6473 LUTs (36.8%), 680 Flip-Flops (1.9%) and 29 RAMB18 (24.2%)" — substantially low resource usage
- Break condition: If parallel row/column processing is required for latency constraints, the sequential sharing strategy becomes a bottleneck.

### Mechanism 3
- Claim: Learned 2D transforms create irregular spatial encodings that preserve discriminative information.
- Mechanism: The tanh-activated separable transforms generate "irregular chessboard-like patterns" (Fig. 8) that encode spatial structure compactly. Unlike flattened FC layers that destroy 2D geometry, LST maintains spatial relationships through structured transformations.
- Core assumption: Preserving 2D spatial structure through learned transforms improves sample efficiency.
- Evidence anchors: [section V.B] "It can be seen that the LST2D-1 model encoded the image as an irregular chessboard-like pattern"; [Table I] LST2D-1 achieves 98.02% with 9,474 params vs Westby [12] achieving 93.25% with 9,550 params
- Break condition: If downstream tasks require translation invariance or local feature detection (e.g., edge-based recognition), the global row/column transforms may not capture local patterns efficiently.

## Foundational Learning

- Concept: **Separable Transforms / Tensor Decomposition**
  - Why needed here: LST relies on the mathematical fact that 2D operations can be factorized into sequential 1D operations. Without understanding separability, the design appears arbitrary.
  - Quick check question: Can you explain why applying W₁ to rows then W₂ to columns is equivalent to a specific form of 2D transformation, and what constraints this imposes?

- Concept: **Weight Sharing in Neural Networks**
  - Why needed here: The parameter reduction comes entirely from reusing the same weights across spatial locations. Understanding weight sharing distinguishes this from simply using smaller layers.
  - Quick check question: How does weight sharing in LST differ from weight sharing in convolutional layers, and what are the trade-offs?

- Concept: **Fixed-Point Quantization for FPGA**
  - Why needed here: The paper shows 12-bit quantization with no accuracy loss. Understanding quantization is essential for actual hardware deployment.
  - Quick check question: What determines the minimum bit-width needed before accuracy degrades, and why might 5 integer / 7 fractional bits work for MNIST?

## Architecture Onboarding

- Component map: Input Image (28×28) → LST Block: Row FC1 (shared weights) → tanh → Column FC2 (shared weights) → tanh → [Optional: Stack additional LST blocks or add skip connections] → Flatten → FC Output Layer → Softmax → Class Prediction
Hardware mapping: Single PE array reads row weights from ROM, processes all rows, then switches to column weights. One shared tanh approximation block (Eq. 3). Output uses 10 parallel PEs for final classification.

- Critical path:
1. Weight initialization: Glorot (Xavier) initialization is critical — random initialization may fail to converge with such constrained architectures.
2. Sequential row → column processing order must match training and inference.
3. Tanh approximation (Eq. 3) must be used consistently in hardware to match floating-point training results.

- Design tradeoffs:
- **Depth vs Width**: Stacking LST blocks (LST2D-2, ResLST-3) increases accuracy but adds parameters and latency. Single-block LST2D-1 maximizes parameter efficiency.
- **Hidden dimension (dₕ)**: Setting dₕ = dᵢₙ enables skip connections (ResNet-like); dₕ < dᵢₙ reduces parameters further but loses skip-connection capability.
- **Bit-width**: 12-bit shown to preserve accuracy; reducing below this is untested and risky.

- Failure signatures:
- Accuracy drops significantly compared to paper: Check that tanh approximation (Eq. 3) is implemented correctly, not standard library tanh.
- Training fails to converge: Verify Glorot initialization; default PyTorch init may not work well for this architecture.
- Hardware accuracy < software accuracy: Check quantization scheme (5.7 fixed-point) and ensure no overflow in MAC operations.
- Poor performance on non-MNIST data: Expected — LST is validated only on grayscale 28×28 images. Extension to other domains requires architectural changes.

- First 3 experiments:
1. **Reproduce LST2D-1 baseline**: Train single LST block (28×28 → 28×28) + FC on MNIST. Target: ~98% accuracy with ~9.5K parameters. Use Adam, lr=2e-3, weight_decay=1e-5, batch_size=1000, 300 epochs.
2. **Ablate separability**: Replace LST with a standard FC layer of comparable parameter count (or matched MAC operations) to isolate the benefit of separable structure vs. parameter reduction alone.
3. **Quantization sensitivity**: Train LST2D-1 with simulated 8-bit, 10-bit, 12-bit quantization (using quantization-aware training or post-training quantization) to establish minimum bit-width before accuracy degrades. Paper only validates 12-bit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the 2D-centric LST2D approach be effectively adapted for color image recognition using quaternionic neural networks?
- Basis in paper: [explicit] The authors state that extending the approach to color images is "a challenging task" because LST operates on 2D inputs, and suggest quaternionic neural networks as a "possible solution."
- Why unresolved: The current mathematical formulation of LST2D is designed strictly for 2D matrices, whereas color images are inherently 3D data structures.
- What evidence would resolve it: Successful implementation and benchmarking of a Quaternion-Valued LST model on a standard color dataset (e.g., CIFAR-10 or ImageNet).

### Open Question 2
- Question: Does the parameter efficiency and accuracy of LST-based networks generalize to more complex gray-scale datasets such as Fashion MNIST?
- Basis in paper: [explicit] The authors note that "investigation of the performance of the proposed approach in context of other gray-scale image recognition tasks is the subject of future works."
- Why unresolved: The experimental evaluation is restricted to the MNIST dataset, leaving the method's efficacy on datasets with higher texture complexity or feature ambiguity unproven.
- What evidence would resolve it: Comparative analysis of LST2D against FFNNs on Fashion MNIST or medical gray-scale imaging datasets.

### Open Question 3
- Question: What are the actual throughput, latency, and power consumption metrics of the LST-1 hardware implementation?
- Basis in paper: [inferred] The authors explicitly state in Section V.C: "We did not estimate the throughput or other performance metrics for the suggested design," despite claiming "high-performance implementation" in the abstract.
- Why unresolved: While resource usage (LUTs) and accuracy preservation were validated, the temporal efficiency and real-time viability of the FPGA accelerator were not measured.
- What evidence would resolve it: Reporting Frames Per Second (FPS) and energy consumption for the implemented Zybo Z7 design.

## Limitations
- The 2D separable transformation approach is validated only on MNIST (28×28 grayscale digits), limiting generalizability to other image recognition tasks
- No comparison against other lightweight architectures (MobileNet, ShuffleNet, etc.) in terms of accuracy-latency tradeoffs
- FPGA resource measurements are specific to Xilinx Virtex-7; results may not translate directly to other FPGA families or ASIC implementations

## Confidence
- **High confidence**: Parameter reduction mechanism through weight sharing (explicitly specified and mathematically clear)
- **Medium confidence**: Accuracy claims for LST2D-1 (98.02%) and ResLST-3 (98.53%) — reported but not independently verified
- **Low confidence**: Hardware efficiency claims — no baseline comparison to standard FC layers on FPGA, making the true benefit unclear

## Next Checks
1. Implement LST2D-1 and verify 98%+ accuracy on MNIST with ~9.5K parameters under exact paper specifications
2. Compare LST2D-1 against a parameter-matched standard FC layer on both accuracy and hardware resource usage
3. Test LST2D-1 on a second dataset (e.g., Fashion-MNIST) to assess domain transferability beyond MNIST digits