---
ver: rpa2
title: Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy
  Optimization
arxiv_id: '2504.03961'
source_url: https://arxiv.org/abs/2504.03961
tags:
- algorithm
- policy
- movement
- where
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a deep reinforcement learning approach for
  optimizing UAV-based base station positioning in emergency communication scenarios.
  The method employs proximal policy optimization (PPO) with continuous action spaces,
  using reference signals and angle of arrival measurements rather than GPS data to
  determine UAV positioning.
---

# Optimizing UAV Aerial Base Station Flights Using DRL-based Proximal Policy Optimization

## Quick Facts
- arXiv ID: 2504.03961
- Source URL: https://arxiv.org/abs/2504.03961
- Reference count: 24
- Primary result: DRL-based PPO achieves >7 Mbps throughput across multiple UE mobility patterns without GPS data

## Executive Summary
This paper addresses the challenge of optimizing UAV-based base station positioning in emergency communication scenarios where GPS data may be unavailable. The authors propose a deep reinforcement learning approach using proximal policy optimization (PPO) with continuous action spaces. The key innovation is using reference signals and angle of arrival measurements instead of GPS data to determine UAV positioning. The algorithm successfully adapts to various UE mobility patterns including static, linear, circular, and mixed movements, achieving significant performance improvements over static positioning, particularly in complex scenarios where UAV clusters move in perpendicular or opposite directions.

## Method Summary
The authors frame UAV positioning as a reinforcement learning problem where the agent learns to control UAV movement based on signal measurements rather than GPS coordinates. The state representation includes UAV position history, SINR measurements, and AoA statistics (mean and standard deviation). The action space consists of continuous direction and distance commands for UAV movement. The reward function is based on the sum of logarithms of UE rates to ensure fair bandwidth distribution. The PPO algorithm is trained across multiple mobility scenarios using the 3GPP UMa channel model with Rician fading. The approach is evaluated against static positioning baselines and tested for robustness to AoA estimation noise.

## Key Results
- Achieves throughput exceeding 7 Mbps across different UE mobility patterns
- Demonstrates 17.64% improvement in complex scenarios with perpendicular UAV movement
- Shows 11.30% improvement when UAV clusters move in opposite directions
- Maintains high throughput even with significant noise in angle of arrival estimation

## Why This Works (Mechanism)
The success of this approach stems from the PPO algorithm's ability to learn optimal UAV positioning strategies through trial and error using only signal-based observations. By formulating the problem as an RL task with the fair transmission rate as the reward, the agent learns to balance coverage across all UEs rather than maximizing total throughput. The use of AoA statistics (mean and standard deviation) in the state representation provides the agent with directional information about UE locations without requiring precise GPS coordinates. The clipped objective function in PPO ensures stable learning by preventing large policy updates that could degrade performance. The method's robustness to AoA noise demonstrates that the agent has learned meaningful positioning strategies that don't rely on perfect signal measurements.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - **Why needed here:** This is the core algorithm used for the deep reinforcement learning agent. Understanding its on-policy nature and the function of the clipped objective is essential to grasp how the model learns stably.
  - **Quick check question:** How does the "clipped objective" in PPO prevent a policy from degrading during a bad update?

- **Concept: Reinforcement Learning (RL) Formulation (State, Action, Reward)**
  - **Why needed here:** The paper frames the UAV control problem as an RL task. Identifying what the agent observes (state), what it can do (action), and what goal it's given (reward) is the key to understanding its behavior.
  - **Quick check question:** In this paper's model, what are the two components that make up a single action in the continuous action space?

- **Concept: Angle of Arrival (AoA) Estimation**
  - **Why needed here:** The system's state is built on AoA from UEs, not their GPS data. Understanding this as a sensing technique for inferring direction is crucial, as is understanding its vulnerability to noise, which the paper specifically tests.
  - **Quick check question:** The paper uses the mean and standard deviation of the AoA as part of its state. Why would the standard deviation be a useful signal for the agent?

## Architecture Onboarding

- **Component map:** Simulation Environment (3GPP UMa channel model, UE mobility patterns) -> PPO Agent (Actor network: policy outputs direction/distance, Critic network: value function) -> State (UAV position, SINR, AoA stats) -> Reward (normalized fair transmission rate)

- **Critical path:** The most critical element for successful replication is the State Formulation. The choice to use proxies (SINR, AoA stats) instead of ground truth GPS is the paper's central design decision. A flawed implementation of the AoA calculation would invalidate the core experiment.

- **Design tradeoffs:**
  - **Realism vs. Stability:** The paper trades off the simplicity of using known UE locations (GPS) for the realism and robustness of using inferred locations from signal measurements (AoA). This increases training difficulty but improves practical applicability.
  - **Fairness vs. Total Throughput:** The reward function is based on the sum of logarithms of rates ($R_{fair}$). This creates a tradeoff, prioritizing a balanced distribution of bandwidth among all UEs over maximizing the raw total throughput of the cell.

- **Failure signatures:**
  - **Agent fails to converge, reward oscillates wildly:** This indicates the clip parameter ($\epsilon$) may be too high or the learning rate too aggressive, violating the core PPO stability mechanism.
  - **UAV gets stuck in a suboptimal location:** This likely indicates a problem with the reward shaping or state representation. If the reward doesn't clearly signal an improvement for moving towards UEs, the agent won't learn the correct behavior.
  - **Performance collapses with added noise:** This suggests the model has overfit to the perfect AoA conditions of the initial training and is not robust to the noisy estimations described in the paper's evaluation.

- **First 3 experiments:**
  1. **Baseline Replication (Static UEs):** Implement the environment with static UEs and verify the PPO agent can learn to position the UAV to achieve the near-maximum throughput (~8 Mbps). This validates the basic RL loop.
  2. **Ablation on State Representation:** Train two agentsâ€”one with full GPS information (upper bound) and one with only the signal-based state (SINR, AoA) as described in the paper. Compare their performance and convergence speed to measure the cost of using realistic sensing.
  3. **Noise Robustness Test:** Take the trained agent from the baseline experiment and evaluate its performance while injecting increasing levels of Gaussian noise into the AoA estimations. This directly tests the paper's claim of robustness and identifies the breaking point of the policy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the proposed single-agent PPO framework be extended to a multi-agent system to coordinate multiple UAVs for maximizing coverage while avoiding inter-UAV interference and collisions?
- **Basis in paper:** [explicit] The Conclusion explicitly states: "As a future work, we will consider deploying multiple UAVs as aerial BSs to further enhance network coverage and reliability, and explore advanced coordination strategies."
- **Why unresolved:** The current study simplifies the problem by assuming a single UAV ($D=1$) to focus on trajectory optimization and signal handling, leaving the complexities of multi-agent cooperation and interference management unaddressed.
- **What evidence would resolve it:** A modified PPO implementation (e.g., MAPPO) evaluated in a simulation with multiple UAVs, showing metrics for coverage overlap, interference levels, and collision avoidance.

### Open Question 2
- **Question:** How does the inclusion of UAV propulsion energy constraints in the reward function impact the learned trajectory strategies and the operational lifetime of the network?
- **Basis in paper:** [inferred] The paper defines the optimization objective solely as maximizing the total fair transmission rate $R_{fair}$ (Eq. 7) and specifies the action space as distance and direction ($r, \alpha_d$) without penalizing the energy cost of movement.
- **Why unresolved:** While the paper aims for "real-world deployment," it ignores the battery limitations of UAVs. Maximizing throughput might encourage high-energy flight patterns (e.g., constant rapid movement) that would drain the UAV's battery prematurely in a real emergency scenario.
- **What evidence would resolve it:** Comparative results showing the trade-off between throughput ($R_{fair}$) and total energy consumed when an energy penalty term is added to the reward function.

### Open Question 3
- **Question:** Can the PPO agent maintain robust performance in environments with physical obstacles (e.g., urban canyons) that cause abrupt line-of-sight (LoS) blockages, rather than just probabilistic shadowing?
- **Basis in paper:** [inferred] The numerical setup (Section V-A) simulates a 200m x 200m area where UEs reflect off boundaries, relying on probabilistic channel models (UMa) for shadowing ($G_s$) without modeling specific 3D geographical obstacles or buildings.
- **Why unresolved:** Real-world emergency scenarios often involve complex urban geometry. The current state space (SINR, AoA) allows the agent to react to signal loss, but it is unclear if the agent can learn to navigate around physical obstacles to recover the signal without explicit spatial map data.
- **What evidence would resolve it:** Evaluation of the agent in a 3D ray-tracing simulation or a map-based environment with explicit building geometry to test obstacle avoidance and signal recovery capabilities.

## Limitations
- Key hyperparameters (r_max, transmit power, altitude) are not specified, creating ambiguity in the physical constraints
- The exact implementation details of the 3GPP UMa channel model with Rician fading are not fully described
- The paper does not provide information about the neural network optimizer beyond assuming Adam

## Confidence
- **High Confidence**: The core algorithmic approach (PPO with continuous actions) is sound and well-documented. The multi-scenario evaluation methodology is rigorous.
- **Medium Confidence**: The performance claims (>7 Mbps throughput, specific improvement percentages) are reasonable but depend on the unspecified physical parameters.
- **Low Confidence**: The robustness claims under severe noise (STD up to 100) would benefit from additional validation given the sensitivity of AoA estimation.

## Next Checks
1. **Parameter Sensitivity Analysis**: Systematically vary r_max and UAV altitude to determine their impact on throughput and convergence speed.
2. **Generalization Test**: Evaluate the trained policy on mobility patterns not seen during training (e.g., random waypoint, group mobility) to assess true generalization.
3. **Real-world Feasibility Check**: Implement a simplified hardware-in-the-loop simulation using actual AoA estimation from RF measurements to validate the signal-processing assumptions.