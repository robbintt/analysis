---
ver: rpa2
title: 'ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language
  Models'
arxiv_id: '2511.13548'
source_url: https://arxiv.org/abs/2511.13548
tags:
- jailbreak
- forgedan
- prompts
- adversarial
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FORGEDAN introduces an evolutionary framework that enhances automated
  jailbreak generation by integrating multi-level textual perturbations (character,
  word, and sentence), semantic similarity-based fitness evaluation using embedding
  models, and dual-dimensional judgment via an LLM-based classifier. This design improves
  diversity and coherence of adversarial prompts while enabling reliable detection
  of harmful outputs.
---

# ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models

## Quick Facts
- **arXiv ID:** 2511.13548
- **Source URL:** https://arxiv.org/abs/2511.13548
- **Reference count:** 39
- **Primary result:** Introduces an evolutionary framework with multi-level textual perturbations, semantic fitness evaluation, and dual-dimensional judgment to achieve high attack success rates (e.g., 98.27% on Gemma-2-9B) against aligned LLMs.

## Executive Summary
ForgeDAN presents a novel evolutionary framework for automated jailbreak generation that enhances adversarial prompt diversity through multi-level textual perturbations (character, word, and sentence), employs semantic similarity-based fitness evaluation using embedding models, and implements dual-dimensional judgment via LLM-based classifiers. The framework systematically evaluates candidate prompts across multiple dimensions to reliably detect harmful outputs while maintaining attack effectiveness. Evaluated across multiple models and datasets, ForgeDAN consistently achieves higher attack success rates than baseline methods while demonstrating robustness in transfer and real-world scenarios.

## Method Summary
ForgeDAN operates as a genetic algorithm with population size N=10, elite size K=2, and maximum iterations T_max=5. It begins with a seed template and applies 11 plugin-based mutation strategies across character, word, and sentence levels. Fitness is computed as cosine similarity between RoBERTa embeddings of model outputs and harmful reference responses. A dual-dimensional judgment system using two LLM-based classifiers assesses both behavior compliance and content harmfulness. The framework builds upon the garak framework and requires a target LLM, AdvBench dataset, and safety classifiers for implementation.

## Key Results
- Achieves 98.27% attack success rate on Gemma-2-9B, significantly outperforming baseline methods
- Maintains effectiveness across transfer scenarios and real-world datasets (137 samples)
- Ablation studies demonstrate necessity of each module, with ASR dropping from 98.27% to 5.77% when removing semantic fitness on Gemma-2-9B

## Why This Works (Mechanism)

### Mechanism 1
Multi-level textual perturbations increase adversarial prompt diversity while maintaining semantic coherence. Character-level mutations (homoglyphs, swaps), word-level mutations (synonyms, morphology), and sentence-level mutations (restructuring, reordering) expand the search space. A semantic similarity constraint filters invalid candidates before fitness evaluation. Evidence shows removing multi-strategy mutation reduces ASR moderately (e.g., 98.27%→97.88% on Gemma-2-9B).

### Mechanism 2
Embedding-based semantic fitness evaluation more reliably guides evolutionary search than lexical overlap metrics. ForgeDAN computes cosine similarity between RoBERTa embeddings of model output and harmful reference response, capturing semantic equivalence even when tokens differ. Ablation shows removing this module causes catastrophic ASR drop (98.27%→5.77% on Gemma-2-9B, 87.50%→11.31% on Qwen2.5-7B).

### Mechanism 3
Dual-dimensional judgment (behavior compliance + content harmfulness) reduces false positives in jailbreak detection. Two classifiers jointly determine success, with only "comply + harmful" counting as jailbreak. This separates "refused with harmful trace" from true successes. Ablation shows replacing with keyword matching causes severe ASR drops (87.50%→1.92% on Qwen2.5-7B, 58.65%→10.19% on DeepSeek-V3).

## Foundational Learning

- **Evolutionary algorithms (population, mutation, fitness, selection)**: ForgeDAN models jailbreak generation as evolutionary search with population size N=10, elite size K=2, max iterations T_max=5. *Quick check: Can you explain why elite selection preserves high-fitness candidates across generations?*

- **Text embeddings and cosine similarity**: Semantic fitness relies on comparing embedding vectors; understanding how RoBERTa encodes semantic meaning is critical for interpreting fitness scores. *Quick check: Given two outputs "build a bomb" and "construct an explosive device," would Jaccard similarity or cosine similarity of embeddings better capture their equivalence?*

- **LLM safety alignment (SFT, RLHF) and jailbreak taxonomy**: Understanding why models refuse harmful requests informs how perturbations bypass safeguards (e.g., role-playing, obfuscation patterns from DAN prompts). *Quick check: What are three common jailbreak strategies identified in manual red-teaming?*

## Architecture Onboarding

- **Component map:** Adversarial Input Stage (seed template + malicious payload) -> Core Processing Engine (Multi-strategic Mutation Mechanism, Semantic Fitness Measurement, Dual-Dimensional Jailbreak Judgment) -> Attack Results Stage (successful adversarial template)

- **Critical path:** 1) Initialize population via mutation on seed template, 2) For each generation: compute fitness for all candidates, 3) Select highest-fitness candidate for jailbreak judgment, 4) If SUCCESS (comply + harmful), return template; else continue evolution, 5) Elite selection + mutation produces next generation

- **Design tradeoffs:** Mutation diversity vs. semantic drift (more aggressive perturbations risk invalid variants); computational cost vs. ASR (requires model inference + embedding computation + dual classification per candidate); plugin extensibility vs. complexity (modular mutation operators enable adaptation but require semantic validation)

- **Failure signatures:** ASR near 0% indicates semantic fitness module failure; high false positives suggest dual judgment misclassifying "comply + safe" as jailbreak; stagnant fitness across generations indicates mutation strategies producing low-diversity variants

- **First 3 experiments:** 1) Baseline replication on AdvBench subset against Gemma-2-9B to verify ASR approaches ~98%, 2) Ablation sanity check by disabling semantic fitness module to confirm ASR drops sharply (<20% on Gemma-2-9B), 3) Transfer test by generating adversarial templates on one payload and applying to different payload from same category to validate generalization

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How effectively can ForgeDAN be extended to generate adversarial prompts in multi-modal settings (e.g., image-text pairs)?
**Basis in paper:** The Conclusion states that "Future work will explore... extensions to multi-modal adversarial prompting."
**Why unresolved:** The current framework is designed exclusively for textual perturbations across character, word, and sentence levels.
**What evidence would resolve it:** A modified ForgeDAN framework successfully achieving high Attack Success Rates (ASR) against multi-modal models like GPT-4V or LLaVA.

### Open Question 2
**Question:** What are the dynamics of using ForgeDAN in a co-evolutionary setting where defensive models adapt simultaneously?
**Basis in paper:** The Conclusion identifies exploring "co-evolutionary settings" as a specific direction for future research.
**Why unresolved:** The current evaluation treats the target LLM as a static black box; it does not analyze how the evolutionary algorithm adapts when the defense mechanism actively updates to patch discovered vulnerabilities.
**What evidence would resolve it:** A study analyzing the trajectory of ASR over successive generations where the target model undergoes periodic safety re-alignment based on previous attack outputs.

### Open Question 3
**Question:** Does an adaptive mutation strategy outperform the current random sampling approach?
**Basis in paper:** The ablation study discussion notes that simplified mutation occasionally outperformed complex mutation, suggesting "excessive or poorly-calibrated perturbations might sometimes introduce noise," and explicitly calls for "future refinement of adaptive mutation strategies."
**Why unresolved:** The current implementation randomly samples perturbations from a static library, potentially wasting queries on ineffective or noisy mutations.
**What evidence would resolve it:** A comparative analysis showing that a dynamic mechanism (which weights perturbation operators based on historical success rates) converges on successful jailbreaks faster than the existing uniform sampling method.

## Limitations
- Dual-dimensional judgment relies on proprietary LLM-based classifiers whose performance characteristics are not disclosed, limiting reproducibility
- Semantic fitness module assumes reference harmful responses are representative and correctly constructed, but poor reference quality could degrade fitness signals
- Mutation strategy effectiveness depends on target model's preprocessing (e.g., character normalization), which may invalidate certain perturbation types

## Confidence

- **High confidence:** Ablation studies demonstrating necessity of each module are methodologically sound and show clear ASR drops when components are removed
- **Medium confidence:** Transfer and cross-dataset results are promising but tested on relatively small subsets (20 samples for transfer, 137 for real-world)
- **Medium confidence:** Multi-level perturbation approach is theoretically sound but empirical evidence is limited to ForgeDAN's own comparisons

## Next Checks

1. **Classifier calibration test:** Run ForgeDAN with different open-source safety classifiers (e.g., Llama-Guard vs. commercial APIs) to quantify sensitivity of dual judgment to classifier choice
2. **Reference response robustness:** Systematically corrupt or paraphrase AdvBench target strings and measure impact on semantic fitness scores and ASR to assess sensitivity to reference quality
3. **Model family generalization:** Apply ForgeDAN to non-Transformer architectures (e.g., Mamba) or smaller models (<1B parameters) to test claim of broad applicability