---
ver: rpa2
title: 'Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs'
arxiv_id: '2510.03527'
source_url: https://arxiv.org/abs/2510.03527
tags:
- responses
- consensus
- response
- text
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Consensus Graphs (ConGrs) efficiently capture epistemic variation
  in sampled LM responses using lightweight lexical alignment from bioinformatics,
  augmented by minimal LM judging. For long-form biography generation, synthesizing
  from ConGrs improves factual precision by up to 31% and halves unsupported claims
  versus averaging raw responses, while reducing secondary LM usage by over 80% compared
  to claim-level methods.
---

# Sample, Align, Synthesize: Graph-Based Response Synthesis with ConGrs

## Quick Facts
- **arXiv ID:** 2510.03527
- **Source URL:** https://arxiv.org/abs/2510.03527
- **Reference count:** 40
- **Primary result:** Consensus Graphs improve factual precision by up to 31% and halve unsupported claims in biography generation versus averaging raw responses.

## Executive Summary
Consensus Graphs (ConGrs) capture epistemic variation in LM responses using lightweight lexical alignment adapted from bioinformatics (Needleman-Wunsch algorithm). By synthesizing responses from these graphs with a selection threshold, ConGrs improve factual precision up to 31% and reduce unsupported claims by half for biography generation. For refusal-based tasks, ConGr synthesis increases abstention rates by up to 56% and cuts hallucination scores by up to 58%. The method also improves mathematical reasoning accuracy by up to 6 points through guided self-verification that localizes errors via graph-based consensus detection.

## Method Summary
ConGrs are constructed by sampling m=5 responses at temperature 0.9, then applying Needleman-Wunsch alignment to identify anchor spans (consensus nodes) shared across all responses. A secondary LM Judge (GPT-4.1-Mini) clusters semantically equivalent paths between consensus nodes into disagreement nodes. Synthesis occurs via consensus decoding (selecting nodes with weighted degree ≥τ) or guided self-verification (pruning error-prone regions at κ=0.7). The approach trades precision against recall through τ, adapts to entity rarity, and reduces secondary LM usage by over 80% compared to claim-level methods.

## Key Results
- Factual precision improved by up to 31% and unsupported claims halved versus averaging raw responses
- Abstention rates increased by up to 56% and hallucination scores reduced by up to 58% for refusal tasks
- MATH and AIME accuracy improved by up to 6 points over standard self-consistency

## Why This Works (Mechanism)

### Mechanism 1: Anchor Span Extraction via Lexical Alignment
Post-aligned LMs produce "anchor spans" (stable text scaffolding) that can be efficiently identified using bioinformatics sequence alignment, reducing the need for expensive semantic parsing. The system adapts the Needleman-Wunsch algorithm to align multiple LM responses, merging consecutive sequences present in all responses into Consensus Nodes. Core assumption: RLHF-trained models exhibit reduced lexical diversity, resulting in ordered structural overlaps that serve as reliable skeletal metadata.

### Mechanism 2: Hallucination Filtering via Consensus Thresholding
Hallucinations are often low-probability samples; therefore, filtering graph nodes by traversal frequency isolates high-confidence information. Consensus Decoding traverses the graph and selects only nodes with weighted degree ≥τ, effectively taking a "hard intersection" of claims present in a user-defined fraction of samples. Core assumption: "Disagreement nodes" (text varying across responses) are significantly more likely to contain hallucinations than Consensus nodes.

### Mechanism 3: Error Localization for Reasoning
LMs struggle to self-verify because they cannot locate errors; ConGrs localize errors by highlighting high-variability regions in reasoning chains. Guided Self-Verification identifies Consensus Nodes followed by a high number of Disagreement Nodes (branching factor ≥κ) and treats these regions as "error hotspots," prompting the model to verify only these specific sub-paths rather than the whole response. Core assumption: Variability in reasoning steps correlates with error probability, and focused verification is more effective than holistic verification.

## Foundational Learning

- **Concept: Multiple Sequence Alignment (MSA)**
  - **Why needed here:** This bioinformatics technique is the engine of the ConGr construction. You must understand how gap penalties and match scores create a partial order graph from linear text to debug the alignment phase.
  - **Quick check question:** How does the "gap penalty" in Needleman-Wunsch affect the granularity of the consensus nodes when aligning two responses of differing lengths?

- **Concept: Directed Acyclic Graphs (DAGs) for Text**
  - **Why needed here:** The ConGr is a weighted DAG where paths represent responses. Understanding topological sorting is required to implement the synthesis (decoding) step.
  - **Quick check question:** In a ConGr, what does a node with an in-degree of 3 and out-degree of 3 represent versus a node with in-degree 1 and out-degree 5?

- **Concept: Epistemic Uncertainty vs. Aleatoric Uncertainty**
  - **Why needed here:** The paper exploits *epistemic* uncertainty (variation across samples) as a signal. Distinguishing this from noise helps in setting the selection threshold τ.
  - **Quick check question:** Does a high degree of disagreement in the graph indicate high aleatoric noise or a gap in the model's knowledge (hallucination risk)?

## Architecture Onboarding

- **Component map:** Sampler -> Aligner (Lexical) -> Equivalence Classer (Semantic) -> Decoder
- **Critical path:** The Aligner is the bottleneck. If the lexical alignment is poor (e.g., failing to match synonyms), the graph fragments into low-degree nodes, forcing the decoder to abstain or hallucinate.
- **Design tradeoffs:** Threshold τ controls the precision-recall tradeoff. High τ increases precision but lowers recall. The system claims >80% reduction in LM judge calls compared to claim-level methods.
- **Failure signatures:** "Empty Graph" (0 consensus nodes, ~6% occurrence in biography tasks) forces abstention. "Echo Chamber" (high-confidence hallucinations appearing as consensus nodes) occurs when model is consistently wrong.
- **First 3 experiments:**
  1. Alignment Sanity Check: Run pipeline on 5 samples of known biography with T=0. Verify Consensus Nodes cover entire response.
  2. Threshold Sweep: Plot FActScore vs. Supported Facts for τ∈[0.1,0.9] to find Pareto frontier.
  3. Semantic Equivalence Ablation: Replace LM Judge with string-match/Jaccard to quantify performance drop vs. cost savings.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can ConGrs be adapted for highly open-ended tasks where anchor spans are sparse?
- **Basis:** Authors state ConGrs "may offer more limited utility" in settings like creative writing with less shared structure.
- **Why unresolved:** The current architecture relies on consensus nodes which appear infrequently in open-ended generations.
- **What evidence:** Validation on creative tasks (e.g., WritingPrompts) using diversity metrics rather than factuality.

### Open Question 2
- **Question:** Can the consensus threshold τ be automated based on input characteristics?
- **Basis:** The paper notes entity rarity affects optimal τ, and currently "Choosing a selection threshold τ is a matter for the user."
- **Why unresolved:** Manual tuning creates a fixed tradeoff between informativeness and reliability that varies by entity frequency.
- **What evidence:** A dynamic τ selection method that maintains high precision across both rare and frequent entities.

### Open Question 3
- **Question:** Is the secondary LM judge necessary for creating semantic equivalence classes?
- **Basis:** Step 3 of construction relies on an LM judge, though the method aims to reduce reliance compared to claim-level methods.
- **Why unresolved:** Eliminating the secondary LM would further reduce cost and potential "additional hallucinations" introduced by judges.
- **What evidence:** A variant using embedding-based clustering without FActScore degradation.

## Limitations

- The approach breaks when models are consistently wrong across samples (echo chamber effect) or when temperature is too low to generate sufficient variation.
- The exact clustering algorithm for semantic equivalence classes is underspecified, creating potential reproducibility issues.
- Empty consensus nodes occur in ~6% of biography tasks, forcing abstention and limiting reliability for certain inputs.

## Confidence

- **High confidence:** Factual precision improvements (31% gain) and accuracy gains on MATH/AIME (up to 6 points) are directly supported by experimental results.
- **Medium confidence:** >80% reduction in secondary LM usage is based on design goals but lacks detailed cost breakdowns in the paper.
- **Low confidence:** Handling of empty consensus nodes (6% occurrence) and its impact on overall reliability is not fully validated across diverse input distributions.

## Next Checks

1. **Robustness to Consistency:** Test ConGr synthesis on a dataset where a common hallucination appears in >50% of samples to measure false-consensus rate.

2. **Temperature Sensitivity:** Sweep temperature from 0.1 to 1.0 and measure ConGr formation rate and downstream performance to identify optimal range for epistemic variation capture.

3. **Threshold Calibration:** For each task, determine the Pareto-optimal τ by plotting F1-score (factuality × informativeness) against τ to validate the claimed tradeoff between precision and recall.