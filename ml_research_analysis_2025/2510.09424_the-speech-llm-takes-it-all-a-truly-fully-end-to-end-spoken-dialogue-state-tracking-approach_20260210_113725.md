---
ver: rpa2
title: 'The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State
  Tracking Approach'
arxiv_id: '2510.09424'
source_url: https://arxiv.org/abs/2510.09424
tags:
- spoken
- context
- speech
- dialogue
- full
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates context management strategies for end-to-end
  spoken dialogue state tracking using speech-aware large language models. The authors
  compare three approaches: multimodal context (combining written history with spoken
  current turn), full spoken context (using speech embeddings of entire conversation),
  and compressed spoken context (using attention-pooling to compress speech history).'
---

# The Speech-LLM Takes It All: A Truly Fully End-to-End Spoken Dialogue State Tracking Approach

## Quick Facts
- arXiv ID: 2510.09424
- Source URL: https://arxiv.org/abs/2510.09424
- Reference count: 0
- Primary result: Full spoken context achieves 39.32% Joint Goal Accuracy on SpokenWOZ

## Executive Summary
This paper presents a fully end-to-end approach to spoken dialogue state tracking using speech-aware large language models. The authors investigate three context management strategies: multimodal context combining written history with spoken current turn, full spoken context using speech embeddings of entire conversation, and compressed spoken context using attention-pooling to compress speech history. Experiments on the SpokenWOZ corpus demonstrate that full spoken context achieves the highest Joint Goal Accuracy of 39.32%, outperforming prior work and showing reduced error propagation. The compressed spoken context with 10 queries per turn offers a practical trade-off between performance and context size, achieving 36.49% JGA while significantly reducing memory requirements.

## Method Summary
The approach leverages a speech encoder (Wav2Vec2) to extract contextual speech embeddings from dialogue history, which are then combined with the current turn's spoken input and passed to a large language model (LLaVA-1.6) for dialogue state tracking. Three context management strategies are compared: multimodal context that combines written history with spoken current turn, full spoken context using speech embeddings of the entire conversation, and compressed spoken context that applies attention-pooling to compress speech history. The method is evaluated on the SpokenWOZ corpus, demonstrating the effectiveness of speech-aware processing for dialogue state tracking tasks.

## Key Results
- Full spoken context achieves highest JGA of 39.32% on SpokenWOZ, outperforming prior work
- Compressed spoken context with 10 queries per turn achieves 36.49% JGA with reduced context size
- Full spoken context reduces error propagation and improves performance on challenging slots like proper nouns and profile information

## Why This Works (Mechanism)
The approach works by leveraging speech-aware language models that can process acoustic information directly from speech embeddings rather than relying solely on text transcriptions. By maintaining the full spoken context throughout the conversation, the model preserves acoustic cues and speech patterns that are lost in traditional text-based approaches. The attention-pooling mechanism for compressed context allows the model to retain essential information while reducing computational overhead, enabling a practical balance between performance and resource requirements.

## Foundational Learning

**Speech Embeddings**: Vector representations extracted from raw audio using models like Wav2Vec2. *Why needed*: Provides the model with acoustic information beyond text transcription. *Quick check*: Verify embeddings capture speaker characteristics and speech patterns.

**Dialogue State Tracking**: Task of identifying user goals and updating conversation state across turns. *Why needed*: Core component for building conversational AI systems. *Quick check*: Ensure all slot-value pairs are correctly tracked across dialogue turns.

**Attention Pooling**: Mechanism to compress sequence information while retaining important features. *Why needed*: Reduces context size without significant information loss. *Quick check*: Compare performance with different compression ratios.

## Architecture Onboarding

**Component Map**: Speech Encoder (Wav2Vec2) -> Context Manager -> LLM (LLaVA-1.6) -> Dialogue State Output

**Critical Path**: Raw speech input → Wav2Vec2 encoding → Context construction (multimodal/full/compressed) → LLM processing → State tracking output

**Design Tradeoffs**: Full spoken context provides best accuracy but highest computational cost; compressed context offers practical balance; multimodal context simplifies processing but loses acoustic information.

**Failure Signatures**: Performance degradation on proper nouns and profile slots; increased error propagation in longer conversations; sensitivity to speech encoder quality.

**First Experiments**: 
1. Baseline comparison with text-only approaches on SpokenWOZ
2. Ablation study comparing different context sizes for compressed approach
3. Cross-domain evaluation on diverse dialogue datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental evaluation limited to single domain (SpokenWOZ) with modest JGA scores
- Computational overhead of full spoken context not thoroughly explored
- Compressed context approach shows decreased performance that may limit practical use

## Confidence
- High confidence in methodology and experimental setup validity
- Medium confidence in generalizability beyond SpokenWOZ corpus
- Medium confidence in superiority claims given relatively low absolute performance metrics

## Next Checks
1. Evaluate approach on multiple spoken dialogue datasets across different domains
2. Conduct ablation studies quantifying impact of speech encoder quality versus LLM reasoning
3. Measure computational overhead and latency for real-time deployment with varying context sizes