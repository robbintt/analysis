---
ver: rpa2
title: 'Controlled Territory and Conflict Tracking (CONTACT): (Geo-)Mapping Occupied
  Territory from Open Source Intelligence'
arxiv_id: '2504.13730'
source_url: https://arxiv.org/abs/2504.13730
tags:
- territorial
- conflict
- contact
- control
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces CONTACT, a framework for territorial control
  prediction using large language models (LLMs) with minimal supervision. The method
  compares two approaches: SetFit, an embedding-based few-shot classifier, and prompt-tuned
  BLOOMZ-560m, a multilingual generative LLM.'
---

# Controlled Territory and (Geo-)Mapping Occupied Territory from Open Source Intelligence

## Quick Facts
- arXiv ID: 2504.13730
- Source URL: https://arxiv.org/abs/2504.13730
- Reference count: 2
- The paper introduces CONTACT, a framework for territorial control prediction using large language models (LLMs) with minimal supervision.

## Executive Summary
CONTACT is a framework for territorial control prediction using large language models with minimal supervision. The method compares two approaches: SetFit, an embedding-based few-shot classifier, and prompt-tuned BLOOMZ-560m, a multilingual generative LLM. Both are trained on a small hand-labeled dataset of ISIS-related news articles from Syria and Iraq. The BLOOMZ model, fine-tuned using prompt tuning with embedded label definitions, achieved 100% accuracy on a 5-example test set, significantly outperforming the SetFit baseline (40% accuracy). The results demonstrate that prompt-based supervision improves generalization in low-resource settings and that LLMs fine-tuned using few-shot methods can reduce annotation burdens for structured inference from open-source intelligence streams.

## Method Summary
CONTACT uses a small hand-labeled dataset of ISIS-related news articles (20 total, 15 train/5 test) annotated with 5 VIINA-style territorial control labels. Two models are compared: SetFit with sentence-transformer embeddings and one-vs-rest logistic heads, and BLOOMZ-560m with parameter-efficient prompt tuning (8 virtual tokens, frozen base weights). The BLOOMZ model receives prompts with embedded label definitions to disambiguate semantic cues. Training uses minimal supervision (1 epoch for BLOOMZ, 20 iterations for SetFit) with batch size 1.

## Key Results
- BLOOMZ-560m with prompt tuning achieved 100% per-label accuracy on a 5-example test set
- SetFit baseline achieved only 40% per-label accuracy, collapsing to predict only the two most frequent labels
- Embedding label definitions directly in prompts improved generalization in the few-shot multi-label classification task

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Embedding explicit label definitions into prompts improves few-shot multi-label classification in low-resource settings.
- Mechanism: The prompt-tuned BLOOMZ model receives semantic context about each label (e.g., "t_mil - Event is about war/military operations") directly in the input, enabling the model to map implicit textual cues to structured outputs without relying solely on pattern matching from embeddings.
- Core assumption: Instruction-following models can leverage definitional context to disambiguate semantically overlapping labels when training data is scarce.
- Evidence anchors:
  - [abstract] "prompt-based supervision improves generalization in low-resource settings"
  - [Section 5] "By embedding label definitions directly in the prompt, the model was able to contextualize its generation and disambiguate subtle cues associated with territorial control"
  - [corpus] Weak direct corpus support; related work on controlled text generation (CÂ³TG) addresses attribute control but not label definition embedding specifically.
- Break condition: If label definitions are ambiguous, contradictory, or exceed model context window, performance degrades; also sensitive to prompt phrasing variations.

### Mechanism 2
- Claim: Parameter-efficient prompt tuning enables adaptation while preserving base model generalization.
- Mechanism: Only 8 virtual tokens are trained while BLOOMZ-560m base weights remain frozen, allowing task-specific adaptation without catastrophic forgetting or requiring large annotated datasets.
- Core assumption: The frozen pretrained model already possesses sufficient linguistic and semantic knowledge; only a lightweight steering mechanism is needed.
- Evidence anchors:
  - [Section 4.3] "We initialize prompt tuning with 8 virtual tokens... The base model weights remain frozen during training"
  - [Section 5] "parameter-efficient fine-tuning of instruction-following models offers a promising approach for low-resource, OSINT-based territorial inference"
  - [corpus] No direct corpus validation for this specific PEFT configuration in conflict domains.
- Break condition: Fails if base model lacks relevant domain knowledge (e.g., unfamiliar geopolitical contexts) or if virtual token count is insufficient for task complexity.

### Mechanism 3
- Claim: Embedding-based classifiers collapse to frequency-based baselines under extreme few-shot multi-label conditions.
- Mechanism: SetFit uses pooled sentence embeddings with cosine similarity objectives; with only 15 training examples across 5 labels, the model lacks sufficient signal to separate label semantics and defaults to predicting only high-frequency labels.
- Core assumption: Sustained supervision signal is required for embedding-based methods to learn fine-grained label distinctions.
- Evidence anchors:
  - [Section 5] "SetFit model performed poorly, achieving an average per-label accuracy of 40%... it only predicted the two most frequent labels... regardless of content"
  - [Section 5] "known issue in few-shot multi-label classification settings when using pooled sentence embeddings"
  - [corpus] No corpus papers validate this failure mode directly.
- Break condition: Performance may improve with more training data, label-aware contrastive objectives, or task-specific embedding fine-tuning.

## Foundational Learning

- Concept: Prompt Tuning vs. Full Fine-Tuning
  - Why needed here: CONTACT relies on prompt tuning to adapt BLOOMZ with only 8 trainable virtual tokens; understanding this distinction is essential for reproducing results and debugging.
  - Quick check question: Can you explain why freezing base weights might prevent overfitting on a 15-example dataset?

- Concept: Multi-Label Classification Evaluation
  - Why needed here: The task involves 5 binary labels per article; accuracy is computed per-label, not per-document, which affects interpretation.
  - Quick check question: If a model predicts 4 of 5 labels correctly for every article, what is the per-label accuracy?

- Concept: OSINT Data Fragility
  - Why needed here: Articles are removed, geo-restricted, or altered; the pipeline combines live retrieval with Wayback Machine archival recovery.
  - Quick check question: What failure modes might occur when scraping archived news versus live sources?

## Architecture Onboarding

- Component map:
  - wayback_scraper -> Preprocessing -> SetFit branch OR BLOOMZ branch -> Inference

- Critical path:
  1. Construct labeled dataset (currently 20 articles, manual VIINA-style annotation)
  2. Format prompt with embedded label definitions
  3. Train prompt tokens only (1 epoch, lr=3e-2, batch=1)
  4. Evaluate per-label accuracy on held-out test set

- Design tradeoffs:
  - BLOOMZ-560m is smaller/faster but may lack broader multilingual coverage vs. larger models
  - 5-example test set enables rapid iteration but limits statistical confidence
  - Prompt tuning requires careful phrasing; SetFit requires no prompt engineering but fails in extreme few-shot

- Failure signatures:
  - SetFit predicts only t_mil and t_loc regardless of input -> embedding collapse
  - BLOOMZ generates malformed label strings -> prompt formatting or tokenization mismatch
  - Scraper returns empty articles -> archival snapshot unavailable or paywalled

- First 3 experiments:
  1. Replicate BLOOMZ prompt tuning on the released dataset; verify 100% test accuracy is reproducible.
  2. Expand test set to 20+ held-out articles to assess whether 100% generalizes or overfits prompt phrasing.
  3. Ablate label definitions from prompt (use only label names) to quantify the contribution of semantic context.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can newer, factuality-tuned LLMs improve generalization in CONTACT compared to the BLOOMZ-560m baseline?
- Basis in paper: [explicit] The authors state that applying newer models "tuned for factuality" could "boost accuracy and improve generalization across conflict domains."
- Why unresolved: The current study relied on BLOOMZ-560m; newer architectures were not evaluated.
- What evidence would resolve it: Benchmarking CONTACT using state-of-the-art instruction-tuned models on the ISIS dataset and new conflict corpora.

### Open Question 2
- Question: Can the CONTACT framework be extended to accurately extract specific geolocations for active territory tracking?
- Basis in paper: [explicit] The paper notes that "Future versions of CONTACT should include location extraction as a core capability" to actively track controlled territory.
- Why unresolved: The current study focuses on classifying territorial control indicators, not extracting specific location spans for geocoding.
- What evidence would resolve it: Integration of a secondary location-extraction model and evaluation of geocoding accuracy against ground truth.

### Open Question 3
- Question: Does prompt-tuned performance hold on larger datasets that reflect the diversity of real-world reporting?
- Basis in paper: [inferred] The authors caution that the test set of five examples "may not capture the diversity of real-world reporting" and prompt dependencies could affect generalization.
- Why unresolved: The high accuracy (100%) was achieved on a very small sample, raising concerns about overfitting to the specific prompt or data distribution.
- What evidence would resolve it: Evaluation of the prompt-tuned model on a significantly larger, multi-domain test set of OSINT articles.

## Limitations

- The evaluation dataset is extremely small (20 total articles, 5 test examples), limiting statistical reliability and raising overfitting concerns
- The SetFit baseline performed extremely poorly (40% accuracy), suggesting the comparison may not fully represent embedding-based methods' potential in few-shot settings
- The study focuses on classifying territorial control indicators rather than extracting specific geolocations needed for active territory tracking

## Confidence

- **High confidence**: The technical implementation details (model architectures, PEFT configuration, training hyperparameters) appear well-specified and reproducible. The methodology for prompt tuning with embedded label definitions is clearly described.
- **Medium confidence**: The claim that prompt-based supervision improves generalization in low-resource settings is supported by the results but undermined by the small test set size. The mechanism explaining how label definitions improve disambiguation is plausible but not extensively validated.
- **Low confidence**: The absolute performance numbers (100% accuracy for BLOOMZ, 40% for SetFit) given the extreme few-shot conditions and tiny test set. The generalizability of these results to larger datasets or different conflict domains remains unproven.

## Next Checks

1. **Expand test set validation**: Increase the held-out test set from 5 to at least 20 articles and re-evaluate BLOOMZ performance to determine if the 100% accuracy persists. If performance drops significantly, this would indicate overfitting to the small test set or prompt phrasing.

2. **Prompt ablation study**: Remove the label definitions from the prompt (using only label names) and retrain BLOOMZ to quantify the contribution of semantic context. Compare performance against the original prompt to measure the specific benefit of embedding label definitions.

3. **Cross-domain transfer test**: Apply the trained CONTACT model to news articles from a different conflict region (e.g., Yemen or Ukraine) without additional fine-tuning to assess zero-shot generalization. This would validate whether the model learned transferable patterns or simply memorized the Syria-Iraq context.