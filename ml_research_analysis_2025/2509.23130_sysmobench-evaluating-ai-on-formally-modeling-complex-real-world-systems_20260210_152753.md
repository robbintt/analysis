---
ver: rpa2
title: 'SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems'
arxiv_id: '2509.23130'
source_url: https://arxiv.org/abs/2509.23130
tags:
- system
- code
- state
- systems
- server
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "SysMoBench evaluates AI\u2019s ability to formally model complex\
  \ real-world systems using TLA+. It defines four automated metrics: syntax correctness,\
  \ runtime correctness, conformance to system code, and invariant correctness."
---

# SysMoBench: Evaluating AI on Formally Modeling Complex Real-World Systems

## Quick Facts
- arXiv ID: 2509.23130
- Source URL: https://arxiv.org/abs/2509.23130
- Reference count: 40
- Primary result: SysMoBench reveals AI can model small systems with 100% correctness but struggles with complex ones (e.g., 7.69% conformance on Etcd Raft).

## Executive Summary
SysMoBench is a benchmark designed to evaluate AI's ability to formally model complex real-world systems using TLA+. It introduces four automated metrics—syntax correctness, runtime correctness, conformance to system code, and invariant correctness—to systematically assess AI-generated models. Tested on 11 diverse system artifacts, the benchmark reveals that while state-of-the-art LLMs can accurately model small systems like Spinlock, they struggle with larger, complex systems like Etcd Raft. The benchmark highlights the strengths and limitations of different AI agents, particularly the superiority of code translation agents over basic modeling agents for complex systems.

## Method Summary
SysMoBench evaluates AI-generated TLA+ models of real-world systems using four automated metrics: syntax correctness (via SANY), runtime correctness (via TLC), conformance to system code (via LLM-assisted trace mapping), and invariant correctness (via safety/liveness templates). The benchmark tests three types of AI agents—Basic Modeling, Code Translation, and Trace Learning—on 11 diverse system artifacts. Code translation agents outperform others by enforcing structural adherence to source code, while trace learning agents underperform due to overfitting and context limits. The evaluation pipeline stops if syntax fails, ensuring only valid models proceed to runtime and conformance checks.

## Key Results
- AI models small systems (e.g., Spinlock) with 100% correctness but struggles with complex ones (e.g., 7.69% conformance on Etcd Raft).
- Code translation agents outperform basic modeling agents for complex systems, achieving 66.67% runtime correctness on Etcd Raft vs. 25.00%.
- LLMs frequently violate liveness properties (41.9% of invariant violations) due to incorrect fairness assumptions.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Automated conformance evaluation is enabled by LLM-assisted mapping of TLA+ model actions to instrumented system logs.
- **Mechanism:** The system instruments source code to generate execution traces. An LLM is then used to extract constants, variables, and actions from the AI-generated TLA+ model and map them to the specific terminology used in the logs. TLC validates if the trace corresponds to a valid path in the model's state space.
- **Core assumption:** The LLM performing the mapping can reliably understand the semantic equivalence between model actions and code log events despite naming differences (Assumption).
- **Evidence anchors:**
  - [Section 3.2.3] Describes the use of a coding LLM to "extract constants, variables, and actions... and map them to the corresponding elements specified in the task requirement."
  - [Section 4] Notes that LLM-assisted mapping was found to be correct upon inspection and validated against "gold models."
  - [Corpus] Weak; corpus neighbors discuss general verification (e.g., *VeriEquivBench*) but do not describe this specific LLM-trace-mapping mechanism.
- **Break condition:** The mapping LLM hallucinates incorrect associations between model state variables and code events, causing trace validation to fail incorrectly.

### Mechanism 2
- **Claim:** Code translation agents outperform basic modeling agents for complex systems by enforcing structural adherence to the source code.
- **Mechanism:** Unlike basic modeling agents that attempt to abstract behavior directly (often hallucinating logic), the code translation agent translates code statement-by-statement into TLA+ using symbolic control-flow analysis. This anchors the generated model to the implementation details, reducing logical errors in complex systems like Etcd Raft.
- **Core assumption:** LLMs possess stronger code translation capabilities than system abstraction capabilities (inferred from results).
- **Evidence anchors:**
  - [Section 5] States the code translation agent "leverages symbolic control-flow analysis to synthesize a TLA+ model rigorously" and "prevents LLMs from hallucinating logic."
  - [Table 3] Shows Code Translation achieving 66.67% runtime correctness on Etcd Raft vs. 25.00% for Basic Modeling.
  - [Corpus] Weak; neighbors do not contrast translation vs. modeling agents.
- **Break condition:** Source code contains idioms or machine-generated patterns (e.g., PGo systems) that do not map cleanly to TLA+ control flows, causing the translation logic to fail.

### Mechanism 3
- **Claim:** System complexity correlates with a shift in error types from syntax/runtime errors to liveness/fairness violations.
- **Mechanism:** For small systems (e.g., Spinlock), LLMs successfully generate syntactically correct and executable models. For large systems (e.g., Etcd Raft), the increased state space and protocol complexity lead to frequent liveness violations (e.g., fairness assumption errors), even when syntax is correct.
- **Core assumption:** The drop in invariant correctness is driven by the LLM's limited temporal reasoning capability rather than just context window limits.
- **Evidence anchors:**
  - [Figure 4c] Shows 41.9% of invariant violations are liveness properties compared to 8.3% for safety properties.
  - [Section 5] Notes "limited ability of LLMs in temporal reasoning" and that fairness assumption violations are a significant issue.
  - [Corpus] Consistent; *Abductive Vibe Coding* mentions challenges in validating complex AI-generated artifacts.
- **Break condition:** The system size exceeds the LLM's context window, preventing it from even processing the full artifact, resulting in total failure rather than specific liveness errors.

## Foundational Learning

- **Concept:** **TLA+ State Machine Abstraction**
  - **Why needed here:** The benchmark requires generating TLA+ models consisting of variables, initial predicates, and next-state relations. Understanding how to map code to these components is the core task.
  - **Quick check question:** Can you define a `Next` relation in TLA+ that captures the `lock()` and `unlock()` methods of a mutex?

- **Concept:** **Trace Validation**
  - **Why needed here:** This is the primary metric for "Conformance." One must understand how an execution trace (a sequence of states) is checked against a formal specification.
  - **Quick check question:** If a system trace shows event `A` followed by event `B`, what does it mean if the TLA+ model's `Next` relation does not allow `B` after `A`?

- **Concept:** **Fairness Assumptions (WF/SF)**
  - **Why needed here:** The paper highlights that LLMs struggle with liveness properties. Understanding Weak/Strong Fairness is required to fix the 41.9% of liveness violations observed.
  - **Quick check question:** In a Spinlock model, what fairness condition is required to ensure a waiting thread eventually acquires the lock?

## Architecture Onboarding

- **Component map:**
  - **Input:** System Artifacts (Code, Docs, Traces).
  - **Subject:** AI Agents (Basic, Translation, Trace Learning).
  - **Evaluator (SysMoBench):**
    - **SANY:** Syntax Checker.
    - **TLC:** Runtime/State Space Explorer.
    - **Trace Validator:** Checks conformance (requires LLM Mapper).
    - **Invariant Checker:** Validates safety/liveness templates.

- **Critical path:** Syntax Correctness (SANY) → Runtime Correctness (TLC) → Conformance (Trace Validation). If Syntax fails, the pipeline stops.

- **Design tradeoffs:**
  - **LLM-as-Mapper vs. Human Mapping:** The system uses an LLM to map model names to code logs for scalability; this trades off the guarantee of 100% mapping accuracy for automation (validated empirically in Section 4).
  - **Per-Action vs. Full-Model Syntax:** Uses partial scoring to provide feedback on models that aren't fully syntactically valid.

- **Failure signatures:**
  - **PGo Systems (Machine-generated code):** Translation agents struggle here because the code is repetitive and lacks semantic variable names (Section 5).
  - **Large Context (Etcd Raft):** Basic agents fail to maintain context, leading to syntax errors in 50% of models (Table 3b).

- **First 3 experiments:**
  1.  **Baseline Run:** Execute the Basic Modeling Agent on Asterinas Spinlock to verify the "gold standard" 100% syntax/runtime path works.
  2.  **Scalability Test:** Run the Code Translation Agent on Etcd Raft and verify if it achieves higher Runtime Correctness than the Basic Agent (checking Table 3b results).
  3.  **Liveness Analysis:** Inspect the "Fairness" definitions in a generated model for a distributed system to identify why liveness violations occur (referencing Section I.1).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the **maintainability** of AI-generated formal system models be quantified and automated?
- Basis in paper: [explicit] The authors state in Section 7 that they are "exploring ways to measure the maintainability of AI-generated system models."
- Why unresolved: Current automated metrics (syntax, runtime, conformance) capture functional correctness but fail to assess how easily a model can be updated or evolved as the underlying system changes.
- What evidence would resolve it: A defined metric or evaluation protocol that correlates with the human effort required to adapt a generated model when the source code undergoes non-trivial refactoring.

### Open Question 2
- Question: Can AI agents be improved to correctly synthesize **liveness properties** and fairness assumptions?
- Basis in paper: [inferred] Section 5 reports that LLMs violated 41.9% of liveness properties compared to only 8.3% of safety properties, largely due to incorrect fairness assumptions (too broad or too narrow).
- Why unresolved: LLMs demonstrate limited capability in temporal reasoning, struggling to distinguish between weak and strong fairness or to correctly specify "eventual" progress conditions in concurrent systems.
- What evidence would resolve it: An agent architecture or prompting strategy that enables LLMs to achieve high invariant correctness scores on liveness properties (e.g., >90%) for complex systems like Etcd Raft.

### Open Question 3
- Question: How can trace-learning agents effectively generalize from execution logs to valid system models without overfitting?
- Basis in paper: [inferred] The trace learning agent failed to pass runtime checks (Appendix I.2) and Appendix H notes that providing single traces results in overfitting, while large traces exceed context windows.
- Why unresolved: There is a trade-off between providing enough data to capture the full state space and hitting context limits; current models fail to abstract general behavioral rules from specific execution traces.
- What evidence would resolve it: A trace-learning method that achieves high conformance scores on complex distributed systems (like Etcd Raft) using a sample of traces, without hallucinating logic not present in the logs.

### Open Question 4
- Question: What methodologies can effectively integrate **human evaluation** into automated formal specification benchmarks?
- Basis in paper: [explicit] The conclusion (Section 7) explicitly lists "considering ways to include human evaluation as part of SysMoBench" as a future direction.
- Why unresolved: Purely automated metrics cannot capture qualitative aspects like the elegance of an abstraction or the readability of a model, yet human evaluation is slow and hard to standardize across a benchmark.
- What evidence would resolve it: A scalable evaluation protocol where automated metrics show high correlation with expert human judgments of model utility and abstraction quality.

## Limitations

- **Reliance on LLM mapping:** The conformance metric depends on LLM-assisted trace mapping, which introduces an unquantified error rate.
- **Limited scope:** The benchmark is restricted to TLA+ and distributed systems, limiting generalizability to other formalisms or domains.
- **No human guidance:** The "no human guidance" constraint may artificially limit performance, as real-world applications often involve iterative refinement.

## Confidence

- **High Confidence:** The syntax and runtime correctness metrics (SANY/TLC) are deterministic and verifiable. The comparative performance of code translation vs. basic modeling agents is robust.
- **Medium Confidence:** The conformance and invariant correctness metrics depend on LLM mapping and human-annotated ground truth, introducing potential bias.
- **Low Confidence:** The claim about LLMs' inherent limitations in temporal reasoning is inferred from error patterns but not directly measured.

## Next Checks

1. **Precision Audit:** Instrument the LLM mapping component to log all mappings and conduct a manual audit to measure precision/recall of the conformance metric.
2. **Context Window Test:** Systematically vary the size of the input artifact (e.g., truncate Etcd Raft) to quantify the impact of context limits on syntax/runtime errors.
3. **Generalization Probe:** Adapt the benchmark to a different formal specification language (e.g., TLA+ vs. Alloy) to test the framework's portability beyond distributed systems.