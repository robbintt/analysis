---
ver: rpa2
title: 'CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed
  Language Modeling'
arxiv_id: '2505.12587'
source_url: https://arxiv.org/abs/2505.12587
tags:
- language
- switching
- pre-training
- code-mixed
- cmlformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the challenge of modeling code-mixed languages,
  which exhibit frequent language transitions (switching points) that standard multilingual
  models struggle to capture. The authors propose CMLFormer, a dual-decoder Transformer
  architecture with a shared encoder and synchronized decoder cross-attention, designed
  to model linguistic and semantic dynamics of code-mixed text.
---

# CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling

## Quick Facts
- arXiv ID: 2505.12587
- Source URL: https://arxiv.org/abs/2505.12587
- Reference count: 16
- Primary result: Dual-decoder Transformer with synchronized cross-attention improves code-mixed language modeling on Hinglish hate speech detection

## Executive Summary
This work addresses the challenge of modeling code-mixed languages, which exhibit frequent language transitions (switching points) that standard multilingual models struggle to capture. The authors propose CMLFormer, a dual-decoder Transformer architecture with a shared encoder and synchronized decoder cross-attention, designed to model linguistic and semantic dynamics of code-mixed text. They pre-train CMLFormer on an augmented Hinglish corpus with switching point and translation annotations using multiple new objectives aimed at capturing switching behavior, cross-lingual structure, and code-mixing complexity. Experiments show CMLFormer improves F1 score, precision, and accuracy over BERT-based baselines on the HASOC-2021 benchmark, with attention analyses validating its sensitivity to switching points. The results demonstrate the effectiveness of CMLFormer's architecture and multi-task pre-training strategy for modeling code-mixed languages.

## Method Summary
CMLFormer is a dual-decoder Transformer with shared encoder and synchronized decoder cross-attention. The model processes code-mixed input through a shared BERT-base encoder, then uses two separate decoders to generate translations in the base and mixing languages. The key innovation is synchronized cross-attention where each decoder attends to the other decoder's hidden states during decoding, creating mutual dependency that aligns cross-lingual representations. The model is pre-trained on 10K augmented Hinglish samples using six objectives: MLM, BiLTM (back-translation), SPP (switching point prediction), BTSP (back-translation switching points), TLC (token language classification), and CMI (code-mixing index prediction). For fine-tuning, decoders are detached and the encoder is used for downstream classification tasks.

## Key Results
- CMLFormer achieves 0.312 F1 score on HASOC-2021 benchmark, improving over BERT-base (0.130 F1) by +0.182
- Pre-training with BiLTM + SPP objectives yields best performance; adding all six objectives degrades F1 to 0.204
- Attention visualizations show CMLFormer consistently attends to switching points with significantly higher weights than other tokens
- Model demonstrates better precision (0.431 vs 0.190) and accuracy (0.688 vs 0.597) compared to BERT-base baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synchronized dual-decoder cross-attention enables cross-lingual alignment by allowing each decoder to "peek" at the other's hidden states during decoding.
- Mechanism: Each decoder layer computes cross-attention over the other decoder's current-layer hidden states (ob_l = Attention(ab_l, am_l, am_l)), creating mutual dependency that forces both decoders to align their representations of base and mixing language semantics.
- Core assumption: The structural and semantic asymmetries between base and mixing languages benefit from separate decoding pathways that exchange information, rather than monolithic processing.
- Evidence anchors:
  - [abstract]: "dual-decoder Transformer with a shared encoder and synchronized decoder cross-attention, designed to model the linguistic and semantic dynamics of code-mixed text"
  - [section 3.2.1]: "The two decoders are thus fully synchronized as each requires the hidden states of the other in each block to compute its own."
  - [corpus]: Weak direct evidence—related work on code-mixed NER (arXiv:2509.02514) shows fine-tuned models outperform multilingual baselines, but does not test dual-decoder architectures.

### Mechanism 2
- Claim: Switching Point Prediction (SPP) as an auxiliary pre-training objective teaches the encoder to allocate attention to language transitions.
- Mechanism: Binary cross-entropy loss on token-level switch labels forces encoder self-attention layers to develop sensitivity to adjacent-token language changes; this signal propagates to downstream representations.
- Core assumption: Explicit supervision on switching points is necessary because standard MLM does not adequately capture transition dynamics in code-mixed sequences.
- Evidence anchors:
  - [abstract]: "pre-trained on an augmented Hinglish corpus with switching point and translation annotations with multiple new objectives specifically aimed at capturing switching behavior"
  - [section 6.3, Figure 3]: "CMLFormer far exceeds BERTbase's capability at identifying switching points in code-mixed inputs... consistently attend to language transitions, giving it significantly higher weights compared to other tokens."
  - [corpus]: Related work (arXiv:2504.13545) highlights difficulty of code-mixed sentiment analysis, but does not evaluate switching-point-specific mechanisms.

### Mechanism 3
- Claim: Multi-task pre-training with BiLTM + SPP yields better downstream performance than adding more auxiliary objectives (BTSP, TLC, CMI), which introduce conflicting gradients.
- Mechanism: BiLTM enforces three-way alignment (CM → base, CM → mix), SPP enforces local transition sensitivity; adding BTSP/TLC/CMI increases optimization difficulty and may cause representational interference.
- Core assumption: The encoder has limited capacity; too many objectives compete for representational resources, de specializing the model.
- Evidence anchors:
  - [section 6.2, Table 1]: BiLTM-only improves F1 by +0.182; BiLTM+SPP improves F1 by +0.046; adding all six objectives drops F1 to 0.204 (below baseline).
  - [abstract]: "improves F1 score, precision, and accuracy over other approaches on the HASOC-2021 benchmark under selected pre-training setups"
  - [corpus]: No direct corpus evidence on multi-task objective interactions for code-mixed models.

## Foundational Learning

- Concept: **Code-Mixed Language Structure (Switching Points)**
  - Why needed here: Understanding that switching points are token-adjacent language transitions (t_i = 1 if l_i ≠ l_{i-1}) is essential for interpreting SPP labels and attention visualizations.
  - Quick check question: Given the sentence "college mein aaj exam hai," can you identify the switching points and label them as a binary vector?

- Concept: **Encoder-Decoder Cross-Attention**
  - Why needed here: The dual-decoder architecture extends standard encoder-decoder cross-attention with decoder-decoder cross-attention; you must understand Q/K/V mechanics to debug synchronization.
  - Quick check question: In standard transformer decoder cross-attention, what are the queries and what are the keys/values? How does this change in CMLFormer's decoder-decoder cross-attention?

- Concept: **Multi-Task Learning with Shared Representations**
  - Why needed here: CMLFormer jointly optimizes 6 objectives with a shared encoder; understanding gradient interference and task weighting is critical for diagnosing why some objective combinations degrade performance.
  - Quick check question: If two pre-training objectives have gradients pointing in opposite directions for the same encoder parameters, what happens to convergence?

## Architecture Onboarding

- Component map:
  - Code-mixed input C → Shared Encoder (12-layer transformer, 768 hidden, 12 heads) → Hidden states H_enc
  - H_enc → Base Language Decoder (12-layer decoder with encoder-decoder + decoder-decoder cross-attention) → Translation B
  - H_enc → Mixing Language Decoder (symmetric to base decoder) → Translation M
  - H_enc → SPP/TLC/CMI heads → Auxiliary predictions

- Critical path:
  1. Code-mixed input C → Encoder → hidden states H_enc
  2. H_enc → Base Decoder (with cross-attention to Mixing Decoder) → translation B
  3. H_enc → Mixing Decoder (with cross-attention to Base Decoder) → translation M
  4. H_enc → SPP/TLC/CMI heads → auxiliary predictions
  5. **Fine-tuning**: Detach decoders, attach classification head to encoder, fine-tune on downstream task.

- Design tradeoffs:
  - **Synchronous vs. Asynchronous Cross-Attention**: Synchronous (attending to current-layer states) converges faster and more stably than asynchronous (attending to previous-layer states) per Appendix A.3.
  - **Encoder Capacity vs. Decoder Capacity**: Encoder is underparameterized relative to dual decoders + multi-task load; authors hypothesize encoder should be ~2× decoder size for stable optimization.
  - **Number of Pre-training Objectives**: BiLTM + SPP yields best results; adding BTSP/TLC/CMI degrades performance, likely due to gradient conflict and encoder capacity limits.

- Failure signatures:
  - **Stagnating encoder losses during pre-training**: Indicates encoder is overwhelmed by multi-task burden; consider reducing objectives or increasing encoder capacity.
  - **Attention maps show no switching-point sensitivity**: SPP objective may not be receiving gradient signal (check label alignment with subword tokens per Appendix A.6).
  - **Fine-tuning performance drops below baseline**: Pre-training may have overfitted to small dataset (10K samples); check for pre-training loss convergence without generalization.

- First 3 experiments:
  1. **Ablate decoder-decoder cross-attention**: Train with decoders uncoupled (no cross-attention) to quantify contribution of synchronization; expect higher variance and worse alignment per Appendix A.2.1.
  2. **Scale pre-training data**: Pre-train on full 52.9M sentence corpus (vs. 10K subset) to test whether data scarcity is limiting encoder generalization.
  3. **Increase encoder capacity**: Use BERT-large encoder (vs. BERT-base) with same dual-decoder setup to test hypothesis that encoder capacity bottleneck causes multi-task degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will scaling pre-training to the full 52.9M sentence L3Cube-HingCorpus improve downstream generalization and reduce overfitting observed with the 10K sample subset?
- Basis in paper: [explicit] Authors state: "In our future work, we plan to pre-train CMLFormer on the full L3Cube-HingCorpus dataset with 52.9M sentences, along with higher-quality augmentations. Scaling up the dataset would provide more diverse switching patterns and lexical variations, improving the model's ability to generalize."
- Why unresolved: Limited computational resources restricted pre-training to only 10,000 samples. The paper notes concerns about "overfitted pre-training" and potential loss of robust generalization abilities.
- What evidence would resolve it: Pre-training on the full corpus and evaluating on multiple diverse downstream tasks beyond HASOC-2021, comparing generalization gaps between training and test performance.

### Open Question 2
- Question: Does scaling the encoder to approximately 2x the size of each decoder improve multi-task learning stability and downstream performance?
- Basis in paper: [explicit] "We hypothesize that for effective multi-task learning involving a shared encoder and n multiple decoders, the size of the encoder must be approximately n times the size of each decoder."
- Why unresolved: The current encoder was underparameterized relative to the dual decoders and the complexity of pre-training tasks. The paper notes the encoder faces "a greater optimization challenge than each decoder, which is trained for a single task."
- What evidence would resolve it: Ablation experiments with scaled encoder variants (1x, 1.5x, 2x decoder size) measuring convergence rates, loss stability across objectives, and downstream task performance.

### Open Question 3
- Question: Why do additional pre-training objectives (BTSP, TLC, CMI) degrade downstream performance when combined with MLM, BiLTM, and SPP?
- Basis in paper: [explicit] "This decline may be attributed to conflicting learning signals introduced by these additional objectives during pre-training, which could make optimization harder and reduce the model's ability to generalize effectively."
- Why unresolved: The paper identifies the phenomenon but only hypothesizes the cause. The interaction effects between the six pre-training objectives remain unanalyzed.
- What evidence would resolve it: Systematic ablation studies analyzing pairwise and triplet interactions between objectives; gradient conflict analysis during multi-task training; measuring task affinity scores between pre-training objectives.

## Limitations

- **Encoder Capacity Constraint**: The shared encoder is significantly under-parameterized relative to the dual decoders and six auxiliary objectives, potentially limiting the model's ability to learn all tasks effectively simultaneously.
- **Pre-training Data Scale**: CMLFormer was pre-trained on only 10,000 samples from a corpus containing 52.9 million sentences, raising concerns about whether improvements reflect architectural innovations or simply better pre-training on clean, small datasets.
- **Synthetic Data Quality**: Switching point annotations were generated using Gemini 2.0 Flash, introducing potential noise and inconsistencies that directly impact the effectiveness of the SPP objective and attention pattern analysis.

## Confidence

- **High Confidence**: Dual-decoder architecture with synchronized cross-attention is novel and correctly implemented; the mathematical formulation in Equations 1-2 is sound; ablation showing BiLTM+SPP outperforms other combinations is reliable given controlled experimental setup.
- **Medium Confidence**: CMLFormer's improvements over BERT-base on HASOC-2021 are real but may be partially attributable to better pre-training objectives rather than the dual-decoder architecture itself; the attention visualizations in Figure 3 are compelling but could reflect dataset-specific artifacts rather than general switching-point sensitivity.
- **Low Confidence**: Claims about encoder under-parameterization causing multi-objective degradation are speculative without systematic scaling experiments; the optimal combination of pre-training objectives (BiLTM+SPP) may not generalize beyond Hinglish or the specific HASOC-2021 task.

## Next Checks

1. **Scale Pre-training Data**: Pre-train CMLFormer on the full 52.9M sentence L3Cube-HingCorpus to determine whether the current 10K-sample limitation is artificially constraining encoder learning and masking the architecture's true potential.

2. **Encoder Capacity Scaling**: Replace the BERT-base encoder with BERT-large (24 layers, 1024 hidden) while maintaining the same dual-decoder architecture to test whether increased encoder capacity eliminates the observed performance degradation when adding multiple auxiliary objectives.

3. **Cross-Attention Ablation with Data Scaling**: Conduct the decoder-decoder cross-attention ablation (as in Appendix A.2.1) but with pre-training on the full corpus to isolate architectural contributions from data quantity effects.