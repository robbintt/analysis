---
ver: rpa2
title: 'Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback'
arxiv_id: '2602.02369'
source_url: https://arxiv.org/abs/2602.02369
tags:
- memory
- agent
- experience
- experiences
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Live-Evo introduces a memory system for LLM agents that learns\
  \ online from streaming feedback, decoupling stored experiences from task-specific\
  \ usage guidelines. It maintains an Experience Bank and a Meta-Guideline Bank, using\
  \ a four-stage loop\u2014Retrieve, Compile, Act, Update\u2014to adapt guidance over\
  \ time."
---

# Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback

## Quick Facts
- **arXiv ID**: 2602.02369
- **Source URL**: https://arxiv.org/abs/2602.02369
- **Reference count**: 11
- **Primary result**: Improves Brier score by 20.8% and market returns by 12.9% on live Prophet Arena benchmark over 10 weeks

## Executive Summary
Live-Evo introduces a memory system for LLM agents that learns online from streaming feedback, decoupling stored experiences from task-specific usage guidelines. It maintains an Experience Bank and a Meta-Guideline Bank, using a four-stage loop—Retrieve, Compile, Act, Update—to adapt guidance over time. On the live Prophet Arena benchmark over 10 weeks, Live-Evo improves Brier score by 20.8% and market returns by 12.9%, and shows consistent gains on deep-research tasks. Ablations confirm each component is essential for performance. The approach enables agents to filter out stale experiences and reinforce useful ones, leading to progressively better calibration and decision-making under non-stationary conditions.

## Method Summary
Live-Evo implements a dual-bank memory architecture where raw experiences are stored separately from the meta-guidelines that instruct how to use them. The system operates through a four-stage loop: retrieving relevant experiences using weighted similarity search, compiling task-specific guidelines via meta-guidelines, acting through contrastive evaluation (running tasks with and without memory), and updating experience weights based on performance deltas. New experiences are selectively committed only after verification shows they solve specific failures. The method is evaluated on a 10-week live financial forecasting benchmark and deep-research tasks, showing consistent improvements over baselines.

## Key Results
- 20.8% improvement in Brier score on Prophet Arena benchmark over 10 weeks
- 12.9% increase in market returns compared to static memory baselines
- Consistent performance gains across ablation studies confirming each component's necessity

## Why This Works (Mechanism)

### Mechanism 1: Decoupling of Experience and Application
The system maintains two distinct structures: an Experience Bank (storing "what happened") and a Meta-Guideline Bank (storing "how to use it"). When a task arrives, the system retrieves meta-guidelines that instruct the LLM on how to compile a task-specific guideline from raw experiences, rather than directly retrieving similar past cases. This separation allows memory usage to improve over time as the system learns better application strategies.

### Mechanism 2: Contrastive Evaluation for Causal Weighting
The Act stage performs ContrastiveEval, executing each task twice: once with the compiled guideline and once without. The difference in outcome (e.g., Brier score improvement) determines the weight update. If the memory-guided run fails to outperform the baseline, the weights of the retrieved experiences are decayed. This provides a robust signal for filtering misleading or stale memories.

### Mechanism 3: Selective Verification Before Write
The system identifies the worst-performing fraction of tasks and generates candidate experiences from their trajectories, but commits them to the bank only if they yield statistically significant improvement when re-evaluated. This prevents memory bank bloat and noise accumulation by empirically validating new memories before storage.

## Foundational Learning

### Concept: Online vs. Static Continual Learning
Why needed: Live-Evo is designed for live benchmarks where data arrives sequentially and distributions shift. Understanding the difference between folding a static dataset and true streaming is critical to grasping why Live-Evo requires dynamic weight updates.
Quick check: Does your task stream have a fixed endpoint where you can retrain, or must the agent improve continuously without a reset?

### Concept: Brier Score and Calibration
Why needed: The paper uses Brier score (mean squared error of probabilities) rather than accuracy as a primary metric. This measures how calibrated the agent's confidence is, which is essential for the market returns aspect of the evaluation.
Quick check: If an agent predicts an event with 90% confidence and it happens, is that better or worse than predicting it with 51% confidence, assuming both are technically "correct"?

### Concept: Memory as a Policy vs. Memory as a Database
Why needed: Live-Evo treats memory not just as a storage bin but as a mechanism that influences the procedure of solving a task via Meta-Guidelines. This policy approach differs from simply appending retrieved text as context.
Quick check: When you retrieve a memory, do you append it as context (database approach) or use it to generate a new set of instructions (policy approach)?

## Architecture Onboarding

### Component map:
Active Retriever -> Experience Bank + Meta-Guideline Bank -> Compiler -> Executor -> Contrastive Evaluator -> Memory Manager

### Critical path:
The Compile step is the architectural differentiator. Standard RAG pipelines skip this and inject retrieved text directly. In Live-Evo, the `CompileGuideline` function is where the "how to use it" logic executes. If this step fails, the weights update on noise.

### Design tradeoffs:
- **Computational Cost**: The "Contrastive Evaluation" requires solving every task twice (with and without memory), effectively doubling the inference cost per task.
- **Latency vs. Quality**: Strict verification (`min_brier_improvement`) ensures high memory quality but slows down the learning rate (conservative acquisition).
- **Corpus signals**: Related work in the corpus (e.g., ML-Agent, Controlled Self-Evolution) focuses on optimization loops, but evidence on specific "contrastive evaluation" for memory weighting is weak, suggesting this is a specific contribution of Live-Evo.

### Failure signatures:
- **Weight Collapse**: All weights converge to zero if the "memory-free" baseline consistently outperforms the guided version (e.g., due to bad meta-guidelines).
- **Hallucination Loop**: The case study shows "hallucinated" experiences (e.g., suggesting retrieval of speech content for a prediction task) get down-weighted. If the update loop breaks, these low-quality experiences might dominate retrieval.

### First 3 experiments:
1. **Verify the Contrastive Signal**: Run the agent on a small batch where you manually invert the "correct" guideline. Check if the weight update mechanism successfully down-weights the bad experiences.
2. **Ablate the "Compiler"**: Bypass the Meta-Guideline Bank and insert retrieved experiences directly into the prompt. Compare the Brier score degradation to quantify the value of the "Compilation" stage.
3. **Stress Test Verification**: Vary the `min_brier_improvement` threshold (e.g., 0.01 vs. 0.05) on a volatile data stream to find the tipping point between "memory starvation" (too strict) and "memory pollution" (too loose).

## Open Questions the Paper Calls Out
None

## Limitations
- **Model dependency**: Performance gains are tightly coupled to the specific LLM backbone (GPT-4.1-mini) and prompt templates. Generalization to other model families or sizes is untested.
- **Benchmark specificity**: Results are confined to a single financial forecasting task (Prophet Arena). The transferability of the meta-guideline compilation mechanism to domains with different action spaces remains unclear.
- **Verification overhead**: The selective commit mechanism requires re-running tasks with candidate experiences, which doubles the cost for failed cases and may not scale to high-throughput scenarios.

## Confidence
- **High**: Decoupling experience from application improves adaptation to non-stationary data; contrastive evaluation enables causal attribution of performance gains.
- **Medium**: Selective verification prevents memory pollution; meta-guidelines enable compositional generalization across task types.
- **Low**: The system's robustness to catastrophic forgetting in highly dynamic environments; the scalability of the four-stage loop under strict latency constraints.

## Next Checks
1. **Cross-domain transfer**: Apply Live-Evo to a non-financial sequential task (e.g., dialog state tracking) and measure whether meta-guidelines still improve compilation quality.
2. **Memory drift analysis**: Simulate a distribution shift (e.g., flip market regime) and track weight decay curves to confirm stale experiences are down-weighted before harming performance.
3. **Ablation of verification**: Remove the `min_brier_improvement` threshold and quantify the trade-off between learning speed and memory pollution over 10 weeks of Prophet Arena.