---
ver: rpa2
title: 'Fine-Tuning LLMs for Report Summarization: Analysis on Supervised and Unsupervised
  Data'
arxiv_id: '2503.10676'
source_url: https://arxiv.org/abs/2503.10676
tags:
- summaries
- summary
- fine-tuning
- invalid
- topic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies fine-tuning large language models for report
  summarization, particularly for government archives and news articles, under resource
  constraints. The study addresses the challenges of limited compute availability
  (one to two A100 GPUs) and lack of ground-truth summaries for government archives.
---

# Fine-Tuning LLMs for Report Summarization: Analysis on Supervised and Unsupervised Data

## Quick Facts
- arXiv ID: 2503.10676
- Source URL: https://arxiv.org/abs/2503.10676
- Authors: Swati Rallapalli; Shannon Gallagher; Andrew O. Mellinger; Jasmine Ratchford; Anusha Sinha; Tyler Brooks; William R. Nichols; Nick Winski; Bryan Brown
- Reference count: 14
- One-line primary result: Fine-tuning improves LLM summarization quality and reduces invalid summaries under compute constraints.

## Executive Summary
This work studies fine-tuning large language models for report summarization under practical constraints of limited compute (one to two A100 GPUs) and lack of ground-truth summaries for government archives. Two approaches are explored: Knowledge Fine-tuning (KFT) on National Archives documents to reduce invalid summaries, and Format Fine-Tuning (FFT) on news datasets with supervised pairs. Multiple evaluation metrics including ROUGE, BLEU, METEOR, BERTScore, Topic Similarity, and AlignScore are used alongside heuristics to detect invalid summaries. Results show fine-tuning is feasible on modest hardware and improves summary quality in many cases, with KFT reducing invalid summaries by 21 percentage points and FFT achieving up to 138% BLEU improvement.

## Method Summary
The study employs two fine-tuning strategies: Knowledge Fine-Tuning (KFT) using Causal Language Modeling on National Archives documents with Llama-7B and PEFT LoRA adapters, and Format Fine-Tuning (FFT) using Sequence-to-Sequence training on news datasets with T5-Small. KFT trains for 3 epochs on 94,441 sequences with 512-token windows and 64-token overlap, while FFT trains for 10-30 epochs on 2,048 input / 512 output token pairs. Evaluation combines classical metrics (ROUGE, BLEU, METEOR, BERTScore) when references exist, Topic Similarity via LDA for semantic comparison, AlignScore for factual consistency, and heuristics to filter invalid summaries containing code artifacts, repeated tokens, or metadata. All training uses PEFT LoRA (r=16, alpha=32) with DeepSpeed ZeRO on 1-2 A100 GPUs.

## Key Results
- KFT on National Archives documents reduced invalid summary generation by approximately 21 percentage points but did not improve valid summary quality after filtering
- FFT on news datasets consistently improved summary quality, with up to 138% improvement in BLEU and 54% in METEOR metrics
- Topic Similarity and AlignScore trends agreed after invalid summaries were filtered, though AlignScore showed less sensitivity to invalid content due to its chunking and aggregation method

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge Fine-Tuning (KFT) on domain-specific documents reduces invalid summary generation rates.
- Mechanism: Causal language modeling on National Archives documents teaches the model to handle OCR noise and prefer natural language output over code-like artifacts. The model learns domain-specific text patterns, reducing outputs like Python function signatures or metadata strings (e.g., "adobe reader").
- Core assumption: Invalid summaries stem from domain mismatch between pre-training data and target documents, not from fundamental model capacity limitations.
- Evidence anchors:
  - [abstract] "fine-tuning helps improve summary quality and in other cases it helps by reducing the number of invalid or garbage summaries"
  - [section 4.1] "average proportion of invalid summaries goes down from about 36% to 15% with the knowledge fine tuning"
  - [corpus] Related work (Gekhman et al., 2024) suggests fine-tuning on new knowledge can increase hallucination risk, indicating KFT utility may be domain-dependent

### Mechanism 2
- Claim: Format Fine-Tuning (FFT) with supervised summarization pairs consistently improves summary quality metrics.
- Mechanism: Sequence-to-sequence training with (article, summary) pairs teaches the encoder-decoder architecture (T5) the specific input-output mapping for summarization. The model learns to compress information while preserving salient content.
- Core assumption: Ground-truth summaries are available and represent the target summarization style; the model architecture supports seq2seq learning.
- Evidence anchors:
  - [abstract] "FFT on news datasets consistently improved summary quality, with up to 138% improvement in BLEU and 54% in METEOR"
  - [section 4.2.1] "fine-tuning generates summaries equivalent to the reference (ground truth) summaries, and improves mean TS metric by about 10%"
  - [corpus] Baichuan2-Sum paper confirms instruction fine-tuning improves dialogue summarization, supporting task-specific fine-tuning efficacy

### Mechanism 3
- Claim: Multi-metric evaluation with heuristic filtering enables robust quality assessment when ground truth is unavailable.
- Mechanism: Heuristics filter clearly invalid outputs (code snippets, repeated "summary" tokens, metadata artifacts). Topic Similarity (LDA-based cosine similarity) compares summary-to-article topic distribution, reducing length bias. AlignScore evaluates factual consistency but shows lower sensitivity to invalid content due to chunked max-aggregation.
- Core assumption: Invalid summaries have detectable surface patterns; topic distributions capture semantic similarity better than n-gram overlap when reference summaries are absent.
- Evidence anchors:
  - [section 3.3] "TS is more suitable when directly comparing closeness of a generated summary with the original article (as opposed to comparing the generated summary with a reference summary)"
  - [section 4.1] "AlignScore does not improve as much when invalid summaries are removed... could be because of how chunking and aggregating over chunks is done"
  - [corpus] Limited direct corpus evidence on TS vs AlignScore trade-offs; this is a gap requiring further validation

## Foundational Learning

- **Causal Language Modeling vs. Sequence-to-Sequence Modeling**
  - Why needed here: KFT uses Causal LM (predict next token from prefix) for knowledge absorption; FFT uses Seq2Seq (encode input, decode output) for task learning. Choosing the wrong framework will fail.
  - Quick check question: Does your task require learning domain patterns (use Causal LM) or learning input-output transformations (use Seq2Seq)?

- **Parameter-Efficient Fine-Tuning (PEFT/LoRA)**
  - Why needed here: Full fine-tuning of 7B parameters is infeasible on 1-2 GPUs. LoRA freezes base weights and trains low-rank adapters (r=16, alpha=32), reducing memory and enabling on-premise training.
  - Quick check question: Can you explain why LoRA's rank parameter controls the trade-off between adapter capacity and memory footprint?

- **Topic Modeling (LDA) for Semantic Similarity**
  - Why needed here: When reference summaries don't exist, classical metrics (ROUGE, BLEU) can't be applied. LDA reduces documents to topic probability vectors, enabling cosine similarity comparison regardless of length.
  - Quick check question: Why might TS outperform n-gram metrics when comparing a generated summary directly to its source article rather than to a reference summary?

## Architecture Onboarding

- **Component map:**
  - Foundation models: Llama 7B (decoder-only, KFT), T5 Small (encoder-decoder, FFT)
  - Training infrastructure: Hugging Face Transformers + PEFT/LoRA + DeepSpeed ZeRO
  - Hardware: 1-2x NVIDIA A100 80GB GPUs
  - Evaluation pipeline: Heuristics filter → LDA topic modeling (TS) → AlignScore → Classical metrics (when reference available)

- **Critical path:**
  1. Data curation (OCR cleaning, 70% alphanumeric/70% dictionary-word thresholds for NARA)
  2. Tokenization (512-token sequences, 64-token overlap for KFT; 2048 input / 512 output limits for FFT)
  3. Fine-tuning (3 epochs for KFT, 10-30 epochs for FFT)
  4. Summary generation (LangChain "Stuff" method for Llama, HuggingFace pipeline for T5)
  5. Evaluation (heuristic filtering → multi-metric scoring)

- **Design tradeoffs:**
  - Llama 7B vs. T5 Small: Llama chosen for KFT because NARA documents were likely unseen during pre-training; T5 chosen for FFT because encoder-decoder suits summarization. Trade-off: Llama produces more invalid summaries on noisy text.
  - AlignScore vs. Topic Similarity: AlignScore evaluates factual consistency but is less sensitive to invalid summaries (chunked max-aggregation masks errors). TS is more interpretable but requires LDA hyperparameter tuning (K topics, iterations).
  - Heuristic detection: Simple rules (e.g., "summary" appears ≥3 times) are fast but imperfect (FNR=0.18 means some invalid summaries pass).

- **Failure signatures:**
  - High invalid summary rate (>30%): Model may be under-fine-tuned or data is too noisy; increase KFT epochs or improve OCR cleaning.
  - Low TS but high BLEU: Generated summary matches reference style but drifts from source article content; check for reference-summary misalignment.
  - High AlignScore on invalid summaries: Chunking aggregation masks errors; rely more on heuristics and TS for validity detection.
  - Training OOM on single GPU: Reduce batch size, enable DeepSpeed ZeRO stage 2/3, or use smaller LoRA rank.

- **First 3 experiments:**
  1. **Baseline validation:** Generate summaries with foundation Llama 7B and T5 Small on a held-out NARA test set. Compute invalid rate, TS, and AlignScore to establish pre-fine-tuning benchmarks.
  2. **KFT ablation:** Fine-tune Llama 7B on NARA with LoRA (r=16, alpha=32) for 3 epochs. Measure invalid rate reduction and TS change. If invalid rate doesn't drop below 20%, increase epochs or improve data cleaning.
  3. **FFT validation on proxy data:** Fine-tune T5 Small on Kaggle news summary dataset. Verify BLEU improvement (>50%) and TS improvement (>5%) before committing resources to larger datasets like Newsroom. If metrics don't improve, check reference summary quality and prompt formatting ("summarize: " prefix).

## Open Questions the Paper Calls Out
None

## Limitations

- Computational and data constraints limit model size and training data, preventing exploration of full-parameter tuning or larger LoRA ranks
- Evaluation methodology gaps include non-negligible heuristic error rates (FNR=0.18, FPR=0.08) and lack of comprehensive human evaluation on the full dataset
- Domain generalization uncertainty exists regarding whether KFT improvements transfer to other specialized domains beyond government archives

## Confidence

**High Confidence**: Fine-tuning feasibility on modest hardware (1-2 A100 GPUs) with LoRA and DeepSpeed is well-supported. FFT achieving up to 138% BLEU and 54% METEOR improvements is robust and reproducible.

**Medium Confidence**: KFT reducing invalid summaries by 21 percentage points is supported by data, but the mechanism (domain knowledge vs. noise filtering) remains somewhat speculative. AlignScore's reduced sensitivity to invalid summaries due to chunking is plausible but requires further validation.

**Low Confidence**: The assertion that Topic Similarity is "more suitable" than n-gram metrics for comparing generated summaries to source articles lacks strong corpus evidence. Heuristic definitions for invalid summary detection are somewhat ad-hoc and may not generalize well.

## Next Checks

1. **Cross-domain KFT validation**: Apply the same KFT approach to a different specialized domain (e.g., medical reports or legal documents) to test whether knowledge fine-tuning generalizes beyond government archives. Measure both invalid summary reduction and content quality improvements.

2. **Heuristic robustness testing**: Systematically evaluate the invalid summary heuristics across multiple model architectures and domains to quantify false positive and false negative rates. Test whether more sophisticated detection methods (e.g., small classifier trained on invalid vs. valid summaries) would improve filtering accuracy.

3. **Human evaluation correlation study**: Conduct comprehensive human evaluation on a stratified sample of summaries (valid, invalid, borderline cases) to establish ground truth for summary quality. Compare human judgments against all automated metrics to identify which metrics best predict human preferences and where metric disagreements occur.