---
ver: rpa2
title: Learning to Select In-Context Demonstration Preferred by Large Language Model
arxiv_id: '2505.19966'
source_url: https://arxiv.org/abs/2505.19966
tags:
- arxiv
- demonstrations
- learning
- demonstration
- genicl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GENICL, a generative preference learning
  framework for optimizing demonstration selection in in-context learning (ICL). The
  key idea is to reformulate ICL as a generative Bayesian optimization problem using
  a latent variable to bridge demonstration selection and LLM inference.
---

# Learning to Select In-Context Demonstration Preferred by Large Language Model

## Quick Facts
- arXiv ID: 2505.19966
- Source URL: https://arxiv.org/abs/2505.19966
- Reference count: 24
- Introduces GENICL, a generative preference learning framework for optimizing demonstration selection in in-context learning

## Executive Summary
This paper presents GENICL, a novel framework for optimizing demonstration selection in in-context learning (ICL) by leveraging LLM feedback through preference learning. The key innovation is reformulating ICL as a generative Bayesian optimization problem using a latent variable to bridge demonstration selection and LLM inference. The framework directly optimizes demonstration selection based on relative effectiveness, outperforming existing approaches on 19 datasets across 11 task categories.

## Method Summary
GENICL introduces a generative preference learning framework that reformulates ICL as a Bayesian optimization problem. The method uses a latent variable to connect demonstration selection with LLM inference, allowing the framework to optimize for demonstrations that the LLM prefers. This is achieved through a preference learning approach that evaluates the relative effectiveness of different demonstrations, enabling the selection of the most effective examples for a given task.

## Key Results
- GENICL achieves 63.9% accuracy on the PAWS dataset, outperforming EPR (57.7%) and LLM-R (57.5%)
- Significant improvements over baselines on 19 datasets across 11 task categories
- Demonstrates strong generalizability across different LLM sizes and tasks
- Selected demonstrations are reusable across different models

## Why This Works (Mechanism)
GENICL works by reformulating ICL as a generative Bayesian optimization problem, using a latent variable to bridge demonstration selection and LLM inference. This approach allows the framework to directly optimize for demonstrations that the LLM prefers, based on relative effectiveness. The preference learning component evaluates the relative effectiveness of different demonstrations, enabling the selection of the most effective examples for a given task.

## Foundational Learning
1. **Bayesian Optimization** - Needed for efficiently searching the space of possible demonstrations; Quick check: Verify that the optimization converges to better demonstrations over iterations
2. **Preference Learning** - Needed to evaluate the relative effectiveness of demonstrations based on LLM feedback; Quick check: Ensure the preference model accurately captures LLM preferences
3. **In-Context Learning (ICL)** - Needed as the target application for demonstration selection; Quick check: Confirm that selected demonstrations improve ICL performance on held-out tasks

## Architecture Onboarding

**Component Map:**
LLM Inference -> Demonstration Selection -> Preference Learning -> Bayesian Optimization

**Critical Path:**
LLM inference produces outputs for candidate demonstrations, which are evaluated by the preference learning model. The results feed into Bayesian optimization to select better demonstrations in subsequent iterations.

**Design Tradeoffs:**
- Direct preference learning vs. reward modeling: Direct preference learning is simpler but may be less sample-efficient
- Latent variable formulation vs. explicit demonstration scoring: The latent variable approach is more principled but adds complexity

**Failure Signatures:**
- Poor initial demonstration pool leading to suboptimal final selections
- Preference learning model not capturing true LLM preferences
- Bayesian optimization converging to local optima

**First Experiments:**
1. Test on a simple few-shot classification task to verify basic functionality
2. Compare performance with and without preference learning to isolate its impact
3. Evaluate scalability by testing on datasets of increasing size

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to extremely large datasets remains uncertain due to multiple inference passes required
- Potential biases introduced by LLM feedback in the preference learning component
- Effectiveness of the latent variable formulation needs further validation across diverse task types

## Confidence
- High confidence in framework's ability to improve ICL performance on tested datasets
- Medium confidence in generalizability claims to different LLM sizes
- Low confidence in framework's scalability to extremely large datasets and diverse task types

## Next Checks
1. Test GENICL on significantly larger datasets (e.g., 10x current test sets) to evaluate scalability and computational efficiency
2. Evaluate framework performance on a broader range of task types beyond the 11 categories tested
3. Conduct ablation studies to isolate impact of latent variable formulation and preference learning components on overall performance