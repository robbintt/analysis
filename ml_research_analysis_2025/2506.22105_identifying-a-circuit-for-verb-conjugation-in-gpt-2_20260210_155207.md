---
ver: rpa2
title: Identifying a Circuit for Verb Conjugation in GPT-2
arxiv_id: '2506.22105'
source_url: https://arxiv.org/abs/2506.22105
tags:
- circuit
- heads
- plural
- verb
- irregular
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study identifies a compact subnetwork (or "circuit") responsible
  for subject-verb agreement in GPT-2 Small, demonstrating that only 12 attention
  heads spanning 7 layers are needed to achieve near-model performance on the base
  task. The method employs a greedy iterative circuit search combined with path patching
  to isolate and verify the contribution of each attention head.
---

# Identifying a Circuit for Verb Conjugation in GPT-2

## Quick Facts
- arXiv ID: 2506.22105
- Source URL: https://arxiv.org/abs/2506.22105
- Authors: David Demitri Africa
- Reference count: 33
- The study identifies a compact subnetwork (or "circuit") responsible for subject-verb agreement in GPT-2 Small, demonstrating that only 12 attention heads spanning 7 layers are needed to achieve near-model performance on the base task.

## Executive Summary
This study identifies a sparse circuit in GPT-2 Small responsible for subject-verb agreement, demonstrating that only 12 attention heads across 7 layers are sufficient to achieve near-model performance on basic conjugation tasks. The research employs a greedy iterative search method combined with path patching to isolate and verify the contribution of each attention head. The findings suggest that GPT-2 relies on compact circuits for basic syntactic processing, though more distributed mechanisms are required for complex linguistic features.

## Method Summary
The study uses a greedy iterative circuit search combined with path patching to identify and verify attention heads contributing to subject-verb agreement. The method systematically tests attention heads by removing them and measuring performance degradation, then reconstructs the circuit by adding heads that restore performance. This approach allows for precise identification of which components are necessary versus sufficient for the task.

## Key Results
- Only 12 attention heads spanning 7 layers are needed for near-model performance on basic subject-verb agreement
- Performance degrades significantly in more complex settings, requiring 125 heads for 80% accuracy versus the full model's 100%
- The identified circuit demonstrates that GPT-2 relies on sparse mechanisms for basic syntactic processing

## Why This Works (Mechanism)
The mechanism works by identifying a minimal set of attention heads that can capture the essential patterns for subject-verb agreement. These heads likely encode syntactic relationships and number agreement through selective attention patterns that propagate information about subject plurality to the appropriate verb forms. The path patching approach isolates the causal contribution of each head by testing whether its removal degrades performance and whether adding it back restores functionality.

## Foundational Learning

**Attention Mechanisms** - Why needed: To understand how specific heads contribute to syntactic processing. Quick check: Verify that attention weights show meaningful patterns when subjects and verbs are processed.

**Path Patching** - Why needed: To isolate causal contributions of individual components. Quick check: Confirm that patching a head restores performance to levels matching the full model.

**Greedy Iterative Search** - Why needed: To efficiently identify minimal sufficient circuits. Quick check: Ensure that adding heads in different orders converges to similar circuit sizes.

## Architecture Onboarding

**Component Map:** Input -> Token Embeddings -> Attention Heads (12 across 7 layers) -> MLP Layers -> Output

**Critical Path:** Token embeddings flow through selected attention heads in specific layers, which capture syntactic relationships and propagate number agreement information to produce correct verb forms.

**Design Tradeoffs:** Sparse circuits offer efficiency but may lack robustness to complex linguistic phenomena. The tradeoff between circuit compactness and task complexity suggests different architectural strategies for different language processing requirements.

**Failure Signatures:** Performance degradation when circuits are incomplete or when linguistic complexity exceeds the circuit's capacity, as evidenced by the need for 125 heads in complex settings versus 12 for basic tasks.

**First Experiments:**
1. Test circuit robustness by applying it to sentences with nested clauses or pronoun resolution
2. Apply the methodology to GPT-2 Medium to assess if circuit sparsity scales
3. Evaluate the impact of fine-tuning on circuit functionality and sparsity

## Open Questions the Paper Calls Out
Major uncertainties include the robustness of the circuit identification method across different datasets and the generalizability of the findings to larger models or other transformer architectures. The study focuses on a specific, controlled setup with singular/plural subject-verb agreement, which may not capture the full complexity of linguistic phenomena.

## Limitations
- The study focuses on a specific, controlled setup with singular/plural subject-verb agreement
- Results may not generalize to other linguistic phenomena or transformer architectures
- The performance degradation in complex settings suggests identified circuits may not be sufficient for all syntactic tasks

## Confidence

**High:** Core finding that a small, sparse circuit can handle basic subject-verb agreement in GPT-2 Small. Methodology is well-defined and results are reproducible within study constraints.

**Medium:** Claim that more complex settings require substantially more heads. Study does not explore full range of linguistic complexity or alternative model architectures.

**Low:** Claims about generalizability of these circuits to other tasks or models without further validation.

## Next Checks

1. Test the circuit identification method on a more diverse set of syntactic tasks (e.g., nested clauses, pronoun resolution) to assess robustness.

2. Apply the methodology to larger models (e.g., GPT-2 Medium or GPT-3) to determine if the sparsity of circuits holds or if they become more distributed.

3. Evaluate the impact of fine-tuning on the identified circuits to see if their sparsity and functionality are preserved or altered.