---
ver: rpa2
title: 'Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging'
arxiv_id: '2507.15576'
source_url: https://arxiv.org/abs/2507.15576
tags:
- image
- classification
- phase
- intensity
- vlms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Vision-Language Models (VLMs) with In-Context
  Learning (ICL) for classifying Terahertz (THz) images in security applications.
  THz imaging is promising for non-invasive material analysis, but is challenging
  due to scarce annotations, low resolution, and visual ambiguity.
---

# Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging

## Quick Facts
- arXiv ID: 2507.15576
- Source URL: https://arxiv.org/abs/2507.15576
- Authors: Nicolas Poggi; Shashank Agnihotri; Margret Keuper
- Reference count: 40
- Primary result: Mistral-Small-3.1-24B accuracy improves from 0.4950 (zero-shot) to 0.7193 (one-shot) with ICL on 1,400 THz frames.

## Executive Summary
This paper explores Vision-Language Models (VLMs) with In-Context Learning (ICL) for classifying Terahertz (THz) images in security applications. THz imaging enables non-invasive material analysis but suffers from scarce annotations, low resolution, and visual ambiguity. The proposed ICL framework adapts general-purpose VLMs to THz data without fine-tuning using modality-aligned prompting. Evaluated on 1,400 frames with two open-weight VLMs, results show Mistral improves accuracy from 0.4950 to 0.7193 and F1-score from 0.3825 to 0.4126, while Qwen shows increased recall but decreased accuracy, highlighting model-specific volatility in few-shot adaptation.

## Method Summary
The study applies two open-weight VLMs (Mistral-Small-3.1-24B and Qwen2.5-VL-7B) to classify THz images for C4 explosive detection. The approach uses In-Context Learning with modality-aligned prompting: task instruction plus optional demonstration (26×26 pixel crop from Frame 663 labeled "YES C4") plus query frame. Data preprocessing includes Hamming window, FFT, and fftshift to generate intensity (|D|²) and phase (arg(D)) heatmaps. Inference runs via Hugging Face, parsing outputs for "Yes C4"/"No C4" classifications. Evaluation metrics include Accuracy, Precision, Recall, F1-score, and frame-level Prediction Changes.

## Key Results
- Mistral accuracy improves from 0.4950 (zero-shot) to 0.7193 (one-shot) with ICL
- Qwen recall improves to 0.9661 but accuracy drops with one-shot ICL
- Frame-level analysis: Mistral improves on 408 frames, declines on 94; Qwen improves on 131, declines on 394
- ICL increases stability for Mistral while introducing volatility for Qwen

## Why This Works (Mechanism)

### Mechanism 1: In-Context Learning as Task Adaptation
- Claim: Prepending demonstration examples shifts VLM classification behavior without parameter updates
- Mechanism: Models condition on demonstration image-text pairs as prior context, applying learned associations to queries via frozen attention patterns
- Core assumption: VLMs have sufficient pre-trained cross-modal alignment to transfer visual features from demonstration to query
- Evidence: Mistral improves on 408 frames vs 94 declines with one-shot ICL; PictSure confirms ICL effectiveness for few-shot image classification depends on embedding quality

### Mechanism 2: Modality-Aligned Prompting for Cross-Modal Grounding
- Claim: Maintaining positional consistency between visual and textual elements improves demonstration-to-query transfer
- Mechanism: Structured prompts [instruction] → [demonstration image + label] → [query image] encourage visual feature attention
- Core assumption: VLMs maintain positional attention bindings across multimodal sequences
- Evidence: Weak corpus support; "What do vision-language models see in the context?" investigates ICL mechanisms but doesn't address positional alignment

### Mechanism 3: Conservative vs. Over-Detection Trade-offs Under ICL
- Claim: ICL induces model-specific shifts in precision-recall balance
- Mechanism: Positive-class demonstration creates directional bias, with model architecture determining whether this manifests as refinement or over-sensitivity
- Core assumption: Demonstration's positive label creates bias
- Evidence: Mistral precision improves (0.2409→0.3187), recall drops slightly; Qwen recall surges (0.5000→0.9661), accuracy drops; BiasICL shows demonstration composition affects subgroup performance

## Foundational Learning

- **Terahertz Imaging Physics (Intensity vs. Phase)**
  - Why needed: Input data consists of dual plots showing reflected signal strength (intensity) and wave propagation delay (phase)
  - Quick check: Can you explain why metallic plate and metal-coated C4 objects produce different intensity and phase signatures across 1,400 frequency frames?

- **In-Context Learning (ICL) Paradigm**
  - Why needed: Core adaptation method requiring distinction between zero-shot (instruction-only) and few-shot (instruction + demonstrations)
  - Quick check: What happens if you prepend a mislabeled demonstration?

- **Precision-Recall Trade-offs in Imbalanced Detection**
  - Why needed: Dataset has class imbalance and models exhibit different precision-recall shifts under ICL
  - Quick check: If Qwen's recall improves to 0.9661 but accuracy drops, what does this tell you about false positive rates?

## Architecture Onboarding

- **Component map**: Raw THz .mat files → FFT + Hamming window → Intensity and Phase extraction → Softmax-normalized 2D heatmaps → Prompt assembly → VLM inference → Natural language classification → Metric computation

- **Critical path**: Demonstration crop selection (Frame 663) → Prompt template construction → Inference on all 1,400 frames → Parse model outputs for "Yes C4"/"No C4"

- **Design tradeoffs**: Open-weight vs proprietary VLMs (accessibility vs context window limits), zero-shot vs one-shot (simplicity vs accuracy), 26×26 crop size (signal retention vs noise exclusion)

- **Failure signatures**: Mistral baseline underperformance (0.4950 zero-shot accuracy), Qwen volatility (394 frame declines), aliasing artifacts in THz data

- **First 3 experiments**:
  1. Ablate demonstration selection: Test crops from different frequency ranges (low: 50-200, mid: 300-700, high: 1000-1200)
  2. Add negative demonstrations: Include "No C4" example to test precision-recall balance restoration
  3. Test on held-out frames: Reserve frames 1200-1400 for validation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent can fine-tuned or proprietary VLMs with larger context windows improve upon ICL-based models in THz imaging?
- Basis: Limitations section states "proprietary models with larger context windows or dedicated fine-tuning may offer superior performance"
- Resolution: Comparative evaluation against fine-tuned variants or proprietary models (GPT-4o, Gemini) on same dataset

### Open Question 2
- Question: Does signal-domain preprocessing to remove aliasing artifacts enhance VLM semantic feature extraction?
- Basis: Limitations note THz spectra contain aliasing artifacts that may "hinder the model's ability to extract semantically meaningful features"
- Resolution: Ablation study comparing performance on raw data vs pre-processed dataset

### Open Question 3
- Question: How does scaling the number of in-context examples affect prediction volatility and sensitivity-specificity trade-offs?
- Basis: Results show divergent behaviors where ICL stabilizes Mistral but increases volatility in Qwen
- Resolution: k-shot analysis (0 to 5 shots) tracking F1-scores and Prediction Changes Metric trajectories

## Limitations
- Dataset limited to 1,400 frames, creating potential overfitting concerns
- Model-specific volatility, particularly Qwen's 394 frame-level declines with ICL
- Arbitrary demonstration crop selection without frequency-range ablation studies
- No negative demonstrations tested to address precision-recall imbalance
- Context window constraints limit demonstration count in open-weight models

## Confidence
- **High confidence**: ICL improves Mistral accuracy from 0.4950 to 0.7193 and F1 from 0.3825 to 0.4126 in one-shot setting
- **Medium confidence**: Qwen's recall improvement to 0.9661 with ICL indicates precision-recall trade-off
- **Low confidence**: Modality-aligned prompting maintains positional consistency across multimodal sequences

## Next Checks
1. Ablate demonstration frequency ranges: Test 26×26 crops from low (50-200), mid (300-700), and high (1000-1200) frequency frames
2. Add negative demonstrations: Include "No C4" labeled demonstration to test precision-recall balance restoration
3. Validate on held-out frames: Reserve frames 1200-1400 as validation set to assess generalization