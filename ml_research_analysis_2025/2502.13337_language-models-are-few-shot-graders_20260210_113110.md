---
ver: rpa2
title: Language Models are Few-Shot Graders
arxiv_id: '2502.13337'
source_url: https://arxiv.org/abs/2502.13337
tags:
- grading
- rubric
- gpt-4o
- https
- examples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an automatic short answer grading (ASAG) pipeline
  using large language models (LLMs). The approach uses few-shot learning by incorporating
  instructor-graded examples into prompts, combined with retrieval-augmented generation
  (RAG) for example selection.
---

# Language Models are Few-Shot Graders

## Quick Facts
- arXiv ID: 2502.13337
- Source URL: https://arxiv.org/abs/2502.13337
- Reference count: 40
- Primary result: LLM-based ASAG pipeline achieves state-of-the-art accuracy with GPT-4o, using few-shot learning and RAG-based example selection

## Executive Summary
This paper presents an automatic short answer grading (ASAG) pipeline using large language models with few-shot learning. The approach incorporates instructor-graded examples into prompts, combined with retrieval-augmented generation (RAG) for example selection, and achieves state-of-the-art performance across multiple datasets. The system demonstrates grading accuracy comparable to human graders while substantially reducing grading workload, with GPT-4o providing the best balance of accuracy and cost-effectiveness.

## Method Summary
The method uses prompt engineering with four components: general grading instructions, question text, five graded examples (selected randomly or via RAG), and the target student response. The pipeline employs OpenAI's GPT models (GPT-4, GPT-4o, o1-preview) to generate JSON-formatted grades (0-100) and feedback. RAG-based selection retrieves contextually similar graded examples using embedding similarity, while rubric integration decomposes evaluation into structured binary decisions per criterion. The system parses JSON output with fallback re-grading for malformed responses.

## Key Results
- GPT-4o achieves best accuracy/cost balance (~$0.01/submission, 1-2s latency)
- Including graded examples significantly improves grading accuracy (t = 10.84, p < 0.001 for GPT-4)
- RAG-based example selection outperforms random selection (t = 5.21, p < 0.001 for GPT-4)
- Integrating grading rubrics enhanced accuracy on 2 of 4 questions tested

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing instructor-graded examples in prompts improves grading accuracy through few-shot in-context learning.
- Mechanism: Graded examples establish an implicit grading standard within the model's context window, allowing pattern-matching against human judgments without weight updates.
- Core assumption: Model's learned representations can transfer grading patterns from examples to new responses without explicit training.
- Evidence anchors: Significant improvements across all models when using graded examples; fine-tuning paper validates few-shot prompting as standard ASAG intervention.

### Mechanism 2
- Claim: RAG-based example selection outperforms random selection by retrieving contextually similar graded responses.
- Mechanism: Embedding-based similarity search identifies examples sharing lexical and semantic features with the response being graded, grounding judgment in comparable cases.
- Core assumption: Semantic similarity correlates with grading difficulty and criteria applicability.
- Evidence anchors: RAG selection significantly more effective than random across all models; SteLLA paper independently validates RAG for structured grading systems.

### Mechanism 3
- Claim: Rubrics improve accuracy by converting open-ended quality assessment into structured binary decisions per criterion.
- Mechanism: Decomposes evaluation into explicit criterion-level checks, reducing ambiguity and forcing independent reasoning through each requirement before aggregation.
- Core assumption: Well-defined rubric criteria make binary decisions less ambiguous than holistic scoring.
- Evidence anchors: Rubric integration significantly improved accuracy for 2 of 4 questions; ongoing research suggests potential for structured grading systems.

## Foundational Learning

- Concept: **Few-shot in-context learning**
  - Why needed here: The entire pipeline depends on providing examples in prompts to guide model behavior without fine-tuning.
  - Quick check question: Can you explain why few-shot learning doesn't require model weight updates?

- Concept: **Embedding similarity and retrieval**
  - Why needed here: RAG-based selection requires understanding how text is converted to vectors and how distance metrics identify similar examples.
  - Quick check question: Why might Euclidean distance in embedding space capture semantic similarity?

- Concept: **Prompt engineering for structured output**
  - Why needed here: The system requires JSON-formatted responses; understanding how to constrain LLM output format is critical for automation.
  - Quick check question: What happens if the model returns malformed JSON, and how does this system handle it?

## Architecture Onboarding

- Component map: Prompt constructor -> Example selector (Random/RAG) -> LLM endpoint -> Output parser -> Rubric processor (optional)
- Critical path: 1) Instructor grades initial submissions (creates example pool) 2) Embeddings generated for all graded submissions 3) For each new submission: retrieve 5 similar examples via RAG 4) Construct prompt with examples + target response 5) Call LLM API, parse JSON response 6) If parsing fails, re-grade
- Design tradeoffs: GPT-4o offers best accuracy/cost balance; o1-preview has higher variance despite accuracy; RAG improves accuracy but requires embedding storage; example count affects token costs
- Failure signatures: High variance in o1-preview; JSON parsing failures (<1%); rubric shows inconsistent gains (significant on 2/4 questions only)
- First 3 experiments: 1) Baseline validation: Run None condition on held-out subset 2) Ablation on example count: Test 0, 3, 5, 10 examples with RAG 3) Domain transfer test: Apply pipeline to your own course data

## Open Questions the Paper Calls Out
- The quality and pedagogical effectiveness of LLM-generated feedback compared to human feedback, and its impact on student learning outcomes.
- The inconsistent rubric effectiveness across different questions (significant for P3 and P4, negligible for P1 and P2).
- Reducing high variance in o1-preview's grading errors to make chain-of-thought models practical for classroom use.

## Limitations
- Embedding model for RAG selection is not specified, critical for reproducing similarity-based retrieval.
- Data splitting protocol between example pool and test set is unclear.
- API parameters (temperature, top_p) not specified, though defaults are likely used.

## Confidence
- **High confidence**: Few-shot learning with graded examples significantly improves accuracy (t = 10.84, p < 0.001 for GPT-4)
- **Medium confidence**: RAG-based selection outperforms random selection (t = 5.21, p < 0.001 for GPT-4), depends on embedding model choice
- **Medium confidence**: Rubric integration shows inconsistent improvements (significant on 2 of 4 questions only)

## Next Checks
1. Reproduce baseline: Run the "None" condition (no examples) on a held-out subset to verify RMSE values match the paper's benchmarks.
2. Ablation study: Test 0, 3, 5, and 10 examples with RAG selection to identify the optimal example count for your specific domain.
3. Domain transfer validation: Apply the pipeline to your own course data with instructor-graded examples and measure correlation with human grades before deployment.