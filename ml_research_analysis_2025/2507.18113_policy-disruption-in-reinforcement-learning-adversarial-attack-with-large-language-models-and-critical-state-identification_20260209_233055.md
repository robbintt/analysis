---
ver: rpa2
title: Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language
  Models and Critical State Identification
arxiv_id: '2507.18113'
source_url: https://arxiv.org/abs/2507.18113
tags:
- reward
- adversarial
- self
- policy
- float
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARCS, a novel adversarial attack framework
  that disrupts reinforcement learning agents by guiding their behavior toward suboptimal
  actions without modifying the environment. The core innovation is a reward iteration
  optimization method that uses large language models to generate adversarial rewards
  tailored to the specific vulnerabilities of the target policy, combined with a critical
  state identification mechanism that pinpoints decision points where suboptimal actions
  have the greatest impact.
---

# Policy Disruption in Reinforcement Learning:Adversarial Attack with Large Language Models and Critical State Identification

## Quick Facts
- arXiv ID: 2507.18113
- Source URL: https://arxiv.org/abs/2507.18113
- Reference count: 40
- Primary result: Novel ARCS framework disrupts RL agents with 0.91 attack success rate using LLM-guided adversarial rewards and critical state identification

## Executive Summary
This paper introduces ARCS, a novel adversarial attack framework that disrupts reinforcement learning agents by guiding their behavior toward suboptimal actions without modifying the environment. The core innovation is a reward iteration optimization method that uses large language models to generate adversarial rewards tailored to the specific vulnerabilities of the target policy, combined with a critical state identification mechanism that pinpoints decision points where suboptimal actions have the greatest impact. Experimental results across six environments, including MuJoCo and autonomous driving tasks, demonstrate that ARCS achieves attack success rates of up to 0.91, significantly outperforming existing methods. Ablation studies confirm that both the LLM-generated rewards and critical state fine-tuning contribute substantially to the effectiveness of the attacks.

## Method Summary
ARCS is a two-stage black-box adversarial attack framework. First, it uses large language models to generate adversarial rewards through an iterative process: the Reward Generator LLM creates candidate rewards based on task descriptions, multiple attacker policies are trained in parallel using these rewards, and the Reward Evaluator LLM selects the best performer based on empirical win rates. This process repeats for several rounds until an effective adversarial reward is obtained. Second, ARCS identifies critical states where perturbing the victim's actions causes maximal performance degradation. An auxiliary binary policy learns to select these states, and the attacker receives additional fine-tuning rewards at these points, concentrating the attack's impact on the most consequential decision points. The final attacker policy is trained using a composite reward combining the adversarial reward and the critical state fine-tuning signal.

## Key Results
- Achieves attack success rates up to 0.91 across six environments
- Critical state identification causes 15-35% higher failure rates than random perturbation
- Outperforms existing methods by leveraging victim-specific adversarial rewards
- Ablation studies confirm both LLM rewards and critical state fine-tuning are essential components

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-guided reward generation produces victim-specific adversarial objectives that outperform generic attack rewards.
- Mechanism: The Reward Generator LLM receives structured task descriptions and training feedback to produce reward functions with multiple components (dense shaping, sparse signals, terminal bonuses). The Reward Evaluator LLM selects the best candidate based on empirical win rates, creating an iterative refinement loop.
- Core assumption: LLMs can synthesize domain knowledge from environment descriptions to identify exploitable behavioral patterns in victim policies.
- Evidence anchors:
  - [abstract] "leverages large language models (LLMs) to generate adversarial rewards explicitly tailored to the vulnerabilities of the target agent"
  - [Section 3.3] "Through several rounds of generation, training, evaluation, and refinement, an effective adversarial reward function is ultimately obtained."
  - [corpus] Limited direct evidence; one related paper uses LLMs for reward design in cyber defense but in a defensive rather than adversarial context.
- Break condition: If the victim's policy structure is fundamentally incompatible with the reward features the LLM can express (e.g., novel architectures not represented in training data), adaptation may fail.

### Mechanism 2
- Claim: Critical state identification concentrates attack learning on decision points where victim errors cause maximal performance degradation.
- Mechanism: An auxiliary binary policy πM learns to select states where replacing the victim's action with a random action causes significant return drops. The attacker receives additional fine-tuning rewards (R_ft) at these states, amplifying behavioral deviation signals.
- Core assumption: Victim policies have non-uniform state sensitivity—some states are more consequential than others.
- Evidence anchors:
  - [abstract] "critical state identification algorithm is designed to pinpoint the target agent's most vulnerable states, where suboptimal behavior from the victim leads to significant degradation"
  - [Table 2] Perturbation at critical states causes 15-35% higher failure rates than random perturbation across all environments.
  - [corpus] Related work on adversarial attacks in RL from policy distribution perspectives suggests state-sensitive attacks exist, but critical state identification as a separate module is novel.
- Break condition: If victim performance is uniformly degraded across all states (no differential sensitivity), or if the transition model and victim policy estimator are highly inaccurate.

### Mechanism 3
- Claim: Trust region optimization guarantees that identified critical states produce non-increasing victim returns when actions are perturbed.
- Mechanism: Theorem 1 proves minimizing M(π) = L_π_old(π) + C·max KL(π_old||π) guarantees η(π) ≤ η(π_old). Theorem 2 reformulates the constrained optimization (C_2 ≤ N ≤ C_1 perturbed states) into an unconstrained dual form amenable to PPO optimization.
- Core assumption: The local approximation of victim return degradation remains valid within the trust region.
- Evidence anchors:
  - [Section 3.3, Theorem 1] "Minimizing M(π) guarantees non-increasing expected return: η(π) ≤ η(π_old)."
  - [Appendix A.1-A.2] Complete proofs for both theorems.
  - [corpus] Theoretical grounding aligns with standard TRPO guarantees; no corpus contradiction.
- Break condition: If the discount factor is very high (near 1.0) causing long-horizon dependency, or if the advantage estimates are highly noisy.

## Foundational Learning

- **Concept: Proximal Policy Optimization (PPO)**
  - Why needed here: The attacker policy, auxiliary critical-state selector, and dual variables all use PPO for stable policy updates.
  - Quick check question: Can you explain why PPO's clipped objective prevents destructively large policy updates compared to vanilla policy gradients?

- **Concept: Trust Region Methods and KL Divergence**
  - Why needed here: Theoretical guarantees for critical state identification rely on bounding policy change via KL constraints.
  - Quick check question: What does it mean geometrically when two policies have small KL divergence, and why does this relate to performance guarantees?

- **Concept: Potential-Based Reward Shaping**
  - Why needed here: The LLM-generated rewards use potential functions (e.g., phi1, phi2 in Sumo-Human) to provide dense learning signals without altering optimal policies.
  - Quick check question: If you add a reward term γΦ(s') - Φ(s) to an MDP, why does this not change the optimal policy?

## Architecture Onboarding

- **Component map:**
  [Task Description] → [Reward Generator LLM] → [Candidate Rewards (4)]
                                                     ↓
                      [Parallel PPO Training] ← [Candidate Rewards]
                              ↓
  [Training Stats] → [Reward Evaluator LLM] → [Best Reward Selection]
                              ↓
              [Adversarial Policy Pre-training] → [Trained Attacker π_A]
                              ↓
  [Transition Model P̃] + [Victim Estimator π̃_O] → [Critical State Selector π_M]
                              ↓
          [Fine-tuning Reward R_ft = (-ΔO_o + ΔA_o)(1-π_M)] → [Final Attacker]

- **Critical path:** LLM reward generation → PPO training → Evaluator selection → Critical state identification → Fine-tuning. The reward iteration typically converges in 4 rounds (32 API calls), making LLM cost manageable.

- **Design tradeoffs:**
  - LLM reward complexity vs. training stability: More reward components provide richer signals but risk unstable gradient magnitudes (addressed via component scaling and nonlinear transforms).
  - Critical state constraint bounds (C_1=40, C_2=20): Tighter bounds reduce attack coverage but improve focus; looser bounds increase search space.
  - Fine-tuning weight λ=0.3: Higher values prioritize critical-state exploitation over original adversarial reward.

- **Failure signatures:**
  - LLM generates rewards with dominating components (check reward component logs for imbalance).
  - Critical state selector identifies too few/many states (verify N ∈ [20, 40]).
  - Victim estimator π̃_O has high prediction error (monitor supervised loss l_π).
  - Win rate plateaus early in reward iteration (suggests LLM not receiving useful feedback).

- **First 3 experiments:**
  1. **Sanity check**: Run AR without critical state fine-tuning in Sumo-Human against a fixed victim. Verify win rate exceeds 0.5 (baseline1 achieves ~0.37). If not, inspect LLM-generated reward components for meaningful gradients.
  2. **Ablation validation**: Compare random state perturbation vs. critical state perturbation on a pre-trained victim. Expected: critical states show ≥15% higher failure rate (per Table 2). If gap is smaller, check auxiliary policy π_M training convergence.
  3. **Reward iteration convergence**: Log win rates across 4 LLM iterations. Expected: monotonic improvement with diminishing returns. If win rate oscillates, examine Evaluator prompts for inconsistent selection criteria.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the ARCS framework scale to complex, n-agent scenarios involving more than two agents?
- **Basis in paper:** [Explicit] The conclusion states, "Future work will explore extending ARCS to more complex multi-agent scenarios... for enhanced robustness and adaptability."
- **Why unresolved:** The current experimental evaluation is strictly limited to two-player adversarial settings (one attacker vs. one victim), leaving the dynamics of multi-agent cooperation or competition unexplored.
- **What evidence would resolve it:** Empirical results from environments with $n > 2$ agents showing ARCS's ability to disrupt multi-agent coordination or withstand coalition formation.

### Open Question 2
- **Question:** Can meta-learning replace the iterative LLM optimization process to improve the speed and generalizability of adversarial reward generation?
- **Basis in paper:** [Explicit] The authors identify "integrating advanced techniques such as meta-learning and multi-task learning" as a specific direction for future work to enhance robustness.
- **Why unresolved:** The current method relies on an iterative, API-heavy process where an LLM generates and evaluates candidates, which may be inefficient compared to a learned optimizer.
- **What evidence would resolve it:** A comparative analysis showing a meta-learned reward generator achieving comparable attack success rates to the LLM-based method with fewer environment interactions or lower computational cost.

### Open Question 3
- **Question:** Does ARCS remain effective when the victim policy is non-stationary and adapts its behavior during the attack?
- **Basis in paper:** [Inferred] Section 3.1 explicitly defines the victim as following a "fixed policy $\pi_O$."
- **Why unresolved:** The theoretical guarantees and critical state identification rely on the victim's behavioral patterns remaining static; a victim that adapts or retrains online could invalidate the identified critical states.
- **What evidence would resolve it:** Experiments evaluating ARCS against a victim agent that simultaneously updates its policy via online reinforcement learning, measuring if the attack success rate degrades.

## Limitations

- No victim policy availability: The six pre-trained victim policies are essential for reproducing ARCS but are not provided, creating a fundamental reproducibility barrier.
- Autonomous driving environment ambiguity: The specific simulators or implementations for HighwayMerge, HighwayExit, and HighwayCutIn are not identified, requiring significant implementation effort to recreate.
- LLM reward generation reliability: The effectiveness depends heavily on the quality of LLM-generated rewards, which may vary significantly across different model versions or prompt engineering choices.

## Confidence

- **High confidence**: The theoretical framework for critical state identification (Theorems 1-2) and the general two-stage attack methodology are well-specified and internally consistent.
- **Medium confidence**: The experimental results are persuasive, but the lack of victim policy details and specific environment implementations means independent validation is currently impossible.
- **Low confidence**: The reproducibility of LLM reward iteration across different model versions or without access to the specific prompts and evaluator criteria used.

## Next Checks

1. **Theoretical validation**: Implement a simplified version of Theorems 1-2 in a 1D MDP with known optimal policy to verify that minimizing M(π) guarantees η(π) ≤ η(π_old) and that the dual reformulation correctly handles the perturbed state constraint.
2. **Critical state identification ablation**: In a controlled environment (e.g., a simple gridworld), compare critical state perturbation vs. random perturbation on a fixed victim policy to verify the claimed 15-35% performance gap when the auxiliary models are correctly trained.
3. **Reward iteration convergence**: Implement the full LLM reward generation loop with a simplified evaluator (e.g., based on simple heuristics rather than LLM) to verify that win rates improve monotonically across iterations and that the process converges within 4 rounds.