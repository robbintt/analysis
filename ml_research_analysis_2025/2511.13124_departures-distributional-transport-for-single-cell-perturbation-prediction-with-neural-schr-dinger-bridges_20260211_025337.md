---
ver: rpa2
title: "Departures: Distributional Transport for Single-Cell Perturbation Prediction\
  \ with Neural Schr\xF6dinger Bridges"
arxiv_id: '2511.13124'
source_url: https://arxiv.org/abs/2511.13124
tags:
- perturbation
- gene
- single-cell
- bridge
- cell
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Departures presents a Schr\xF6dinger Bridge-based framework to\
  \ predict single-cell perturbation outcomes by directly aligning control and perturbed\
  \ cell distributions without requiring bidirectional modeling or latent space alignment.\
  \ The method uses Minibatch Optimal Transport for efficient sample pairing and jointly\
  \ learns two bridge models\u2014one for discrete gene activation states and another\
  \ for continuous expression dynamics\u2014to capture biological fidelity and single-cell\
  \ heterogeneity."
---

# Departures: Distributional Transport for Single-Cell Perturbation Prediction with Neural Schrödinger Bridges

## Quick Facts
- arXiv ID: 2511.13124
- Source URL: https://arxiv.org/abs/2511.13124
- Reference count: 8
- Primary result: State-of-the-art single-cell perturbation prediction using Minibatch OT-based Schrödinger Bridges

## Executive Summary
Departures presents a novel framework for predicting single-cell perturbation outcomes by directly aligning control and perturbed cell distributions using Schrödinger Bridges. The method avoids the ill-posedness of bidirectional modeling by leveraging Minibatch Optimal Transport for efficient sample pairing. By jointly learning discrete gene activation states and continuous expression dynamics, the model captures biological fidelity and single-cell heterogeneity while achieving state-of-the-art performance on both genetic and drug perturbation datasets.

## Method Summary
Departures predicts single-cell perturbation outcomes by modeling the distributional transport between control and perturbed populations using Schrödinger Bridges. The method uses Minibatch Optimal Transport to pair control and perturbed samples efficiently, avoiding the need for bidirectional inference. It learns two bridge models: one for discrete gene activation states using Continuous-Time Markov Chains, and another for continuous expression dynamics using stochastic differential equations. The final prediction combines both components via Hadamard product, with joint training preventing mode collapse in sparse single-cell data.

## Key Results
- Achieves state-of-the-art performance on E-distance and EMD metrics for perturbation prediction
- Outperforms existing approaches on both genetic (Adamson dataset) and drug (sci-Plex3 dataset) perturbation tasks
- Ablation studies confirm discrete modeling and OT-based pairing are essential for robust prediction
- Effectively models gene activation states with high Pearson correlation on validation sets

## Why This Works (Mechanism)

### Mechanism 1
Minibatch Optimal Transport pairing provides a scalable approximation to Schrödinger Bridge source-target coupling without requiring ill-posed backward process modeling. Instead of Iterative Markovian Fitting which alternates between forward and backward models, Departures computes entropically-regularized OT plans within each minibatch using the Sinkhorn algorithm. This directly yields a joint distribution over pairs that guides bridge matching, eliminating the need for bidirectional inference. The local OT pairings computed within minibatches approximate the global optimal coupling that would emerge from iterative SB refinement, assuming minibatch samples are sufficiently representative of marginal distributions.

### Mechanism 2
Joint training of discrete gene activation and continuous expression bridges prevents mode collapse in sparse single-cell data while capturing biological fidelity. Gene expression data is sparse (most genes have zero expression), so the discrete bridge models gene activation states (0/1) via Continuous-Time Markov Chains, learning transition rates. The continuous bridge models expression values but critically, its loss is masked to only compute over genes with non-zero expression at the endpoint. Final prediction combines both: endpoint = continuous prediction ⊙ discrete activation. This separable modeling assumes gene activation and expression level changes can be modeled independently then combined multiplicatively.

### Mechanism 3
Predicting the endpoint x_T rather than velocity field v_t stabilizes optimization and reduces numerical error in bridge matching. Standard bridge matching regresses velocity v_t = (x_T - x_t)/(T-t), but Departures parameterizes the network to predict x_T directly, then transforms to velocity. This reduces compounding numerical errors during training and provides more stable gradients. For inference, Euler-Maruyama discretization samples the SDE using the learned drift. This endpoint prediction provides a well-conditioned learning objective even when intermediate trajectory states are stochastic.

## Foundational Learning

- **Schrödinger Bridge Problem**
  - Why needed: Core mathematical framework seeking a stochastic process between two marginal distributions that stays close to a reference process (entropy-regularized OT). Understanding this clarifies why Minibatch-OT approximates SB.
  - Quick check: Given two Gaussian distributions, can you sketch why the optimal transport map differs from the Schrödinger Bridge solution when entropy regularization is applied?

- **Doob's h-transform and Diffusion Bridges**
  - Why needed: The method constructs conditioned diffusion processes using h-transforms. The Brownian Bridge is the reference process when f_t=0 and σ_t=σ.
  - Quick check: For a Brownian Bridge from x₀=0 to x_T=1 over time T=1, what is the mean and variance at t=0.5?

- **Continuous-Time Markov Chains on Discrete Spaces**
  - Why needed: The discrete gene activation model uses CTMC dynamics characterized by transition rates u_t.
  - Quick check: Given transition rate u_t(a,b) from state a to b, how do you derive the probability of transitioning within small time h?

## Architecture Onboarding

- Component map:
  Input: (x_0, ct, P) → [Gene Regulatory Network Encoder] → conditioning embeddings
                                ↓
                    ┌───────────────────────────────────────┐
                    │         Minibatch OT Pairing          │
                    │  (Sinkhorn on control-perturbed batch) │
                    └───────────────────────────────────────┘
                                ↓
         ┌──────────────────────────────────────────────────────┐
         │                                                      │
  [Continuous Bridge Network x_θ]              [Discrete Bridge Network d_θ]
  - Predicts endpoint x_T                      - Predicts activation d_T
  - Loss masked by d_T (Eq. 9)                 - Cross-entropy loss (Eq. 14)
  - SDE sampling via Eq. 16                    - CTMC sampling via Eq. 17
         │                                                      │
         └────────────────────┬─────────────────────────────────┘
                              ↓
                    [Hadamard Product: x̃_T = x̂_T ⊙ d̂_T]

- Critical path:
  1. **OT Pairing Quality**: At epoch start, compute Sinkhorn OT between control and perturbed samples. Incorrect pairing propagates noise through both bridges.
  2. **Discrete Model Convergence**: Must converge sufficiently before continuous model can rely on its masks. Monitor PCC on validation set.
  3. **Noise Scale σ**: Set to 0.2 per paper. Too high → excessive stochasticity dominates drift; too low → insufficient exploration.

- Design tradeoffs:
  - **Euclidean vs Cosine cost for OT**: Ablation (Fig. 6) shows Euclidean outperforms cosine for this task, but the paper doesn't explain why. Test both on your data.
  - **Number of time steps (inference)**: Paper uses 50 steps. Fewer steps → faster but coarser approximation. More steps → marginal gains with linear cost increase.
  - **Shared vs separate conditioning**: Both bridges receive same (ct, P) conditioning. Ablation doesn't test separate conditioning, which could allow specialized representations.

- Failure signatures:
  - **Mode collapse in expression**: Discrete model PCC drops below 0.8 → continuous model receives unreliable masks → check discrete loss convergence.
  - **Unrealistic negative predictions**: If x̂_T has negative values, final product with binary mask still shows them → add non-negativity constraint or post-hoc clipping.
  - **OT pairing degeneracy**: If Sinkhorn produces near-uniform couplings (all pairs equally likely), minibatches may lack sufficient structure → increase batch size or check data preprocessing.

- First 3 experiments:
  1. **Baseline reproduction on Adamson**: Replicate Table 1 results for Departures and top 2 baselines (GRAPE, GEARS) on held-out genes. Verify E-distance and EMD metrics match reported values ±10%.
  2. **Ablation: Discrete model contribution**: Train without discrete model (w/o Discrete variant). Quantify performance gap on genes with high activation shift (>20% cells change state).
  3. **Pairing strategy comparison**: Compare Minibatch-OT vs random pairing vs full-batch OT (if computationally feasible) on a subset of 10 perturbations. Measure both downstream metrics and OT plan quality (Sinkhorn divergence from uniform).

## Open Questions the Paper Calls Out
None explicitly called out in the paper.

## Limitations
- The method's reliance on Minibatch-OT approximation lacks theoretical bounds on the approximation error compared to global Schrödinger Bridge solutions
- Separable modeling of gene activation and expression may introduce bias for genes with gradual regulation patterns rather than binary on/off states
- Limited validation on combinatorial perturbation scenarios where sample sparsity could destabilize OT pairing quality

## Confidence

- **High confidence**: E-distance and EMD metric improvements over baselines; discrete model's contribution to performance (confirmed by ablation); OT pairing avoiding bidirectional inference complexity
- **Medium confidence**: Minibatch OT as scalable SB approximation; joint training preventing mode collapse; endpoint prediction improving optimization stability
- **Low confidence**: Theoretical guarantees of OT pairing quality; separability assumption between activation and expression; numerical stability across diverse perturbation types

## Next Checks

1. **OT pairing sensitivity**: Systematically vary batch size and measure downstream performance degradation to quantify pairing quality dependence on minibatch representativeness

2. **Long-range perturbation testing**: Evaluate performance on perturbations with 48+ hour time horizons to stress-test endpoint prediction stability and numerical error accumulation

3. **Coupled vs separable modeling**: Implement a hybrid discrete-continuous model that learns coupled activation-expression dynamics and compare against the separable approach on genes with known gradual regulation patterns