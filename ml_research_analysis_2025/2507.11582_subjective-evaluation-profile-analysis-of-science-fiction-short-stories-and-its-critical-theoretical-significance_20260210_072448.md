---
ver: rpa2
title: Subjective Evaluation Profile Analysis of Science Fiction Short Stories and
  its Critical-Theoretical Significance
arxiv_id: '2507.11582'
source_url: https://arxiv.org/abs/2507.11582
tags:
- evaluation
- literary
- profiles
- story
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated six LLMs' literary assessment patterns using
  10 science fiction stories across 7 sessions, revealing statistically significant
  evaluation diversity. Principal component analysis extracted four dimensions explaining
  76.32% of variance, while clustering identified five distinct evaluation profiles.
---

# Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance

## Quick Facts
- arXiv ID: 2507.11582
- Source URL: https://arxiv.org/abs/2507.11582
- Authors: Kazuyoshi Otsuka
- Reference count: 11
- Primary result: LLMs exhibit distinct evaluation profiles analogous to human critical schools when assessing science fiction stories

## Executive Summary
This study reveals that large language models demonstrate statistically significant diversity in their literary assessment patterns, challenging assumptions about AI evaluation neutrality. Using principal component analysis and clustering methods on 10 science fiction stories evaluated across seven sessions, the research identified four dimensions explaining 76.32% of variance and five distinct evaluation profiles. Evaluation consistency varied dramatically between models (α = 1.00 to 0.35), with up to 4.5-fold variance differences between stories. TF-IDF analysis revealed model-specific evaluation vocabularies, suggesting internalized aesthetic frameworks rather than uniform judgment.

The findings provide quantitative evidence that LLM literary assessment follows probabilistic rather than deterministic processes, with each model exhibiting individual "evaluation profiles" comparable to human critical schools. This methodological framework demonstrates that AI evaluation of creative works is inherently subjective, establishing a new paradigm for understanding how different models approach literary criticism and aesthetic judgment.

## Method Summary
The study evaluated six different LLMs using 10 science fiction short stories across seven separate evaluation sessions. Each story received multiple assessments from each model, generating a comprehensive dataset of literary evaluations. Principal component analysis extracted four primary dimensions from the evaluation data, accounting for 76.32% of total variance. K-means clustering identified five distinct evaluation profiles among the models. The research employed statistical measures including Cronbach's alpha to assess evaluation consistency and TF-IDF analysis to examine vocabulary patterns in model assessments. GPT-4o served as both an evaluator and validation reference throughout the study.

## Key Results
- Principal component analysis identified four dimensions explaining 76.32% of evaluation variance across six LLMs
- Clustering analysis revealed five distinct evaluation profiles with statistical significance
- Evaluation consistency varied widely between models (α = 1.00 to 0.35), with up to 4.5-fold variance differences between stories
- TF-IDF analysis demonstrated model-specific evaluation vocabularies suggesting internalized aesthetic frameworks

## Why This Works (Mechanism)
The mechanism underlying these findings relates to how different LLMs develop unique evaluation patterns through their training processes and architectural differences. Each model internalizes distinct aesthetic frameworks based on its training data composition, attention mechanisms, and optimization objectives. These internalized frameworks manifest as probabilistic evaluation patterns rather than deterministic judgments, creating the observed diversity in literary assessment. The variation in evaluation consistency reflects fundamental differences in how each model processes subjective criteria like narrative quality and thematic depth.

## Foundational Learning

**Principal Component Analysis** - Statistical technique for dimensionality reduction that identifies underlying patterns in high-dimensional data. Why needed: To extract meaningful evaluation dimensions from complex literary assessment data. Quick check: Verify eigenvalues explain sufficient variance (76.32% achieved).

**K-means Clustering** - Unsupervised machine learning algorithm that groups similar data points into clusters. Why needed: To identify distinct evaluation profile groups among different LLMs. Quick check: Validate cluster stability across multiple runs and distance metrics.

**Cronbach's Alpha** - Statistical measure of internal consistency or reliability. Why needed: To quantify evaluation consistency across multiple assessments by the same model. Quick check: Values range from 0.35 to 1.00, indicating significant consistency variation.

**TF-IDF Analysis** - Text mining technique that reflects word importance in documents relative to corpus. Why needed: To identify model-specific vocabulary patterns in literary evaluations. Quick check: Compare vocabulary overlap between models to assess uniqueness.

## Architecture Onboarding

Component Map: Story Corpus -> Multiple LLM Evaluations -> PCA Analysis -> Clustering Analysis -> Profile Validation

Critical Path: Data Collection (10 stories × 6 models × 7 sessions) -> Statistical Analysis (PCA + Clustering) -> Vocabulary Analysis (TF-IDF) -> Profile Characterization

Design Tradeoffs: Using multiple evaluation sessions increases reliability but requires more computational resources. Single genre focus (science fiction) provides consistency but limits generalizability. Using GPT-4o as validation reference provides standardization but introduces potential bias.

Failure Signatures: Inconsistent clustering results across runs suggest unstable evaluation patterns. Low explained variance in PCA indicates poor dimension extraction. High vocabulary overlap between models suggests lack of distinct evaluation profiles.

First Experiments:
1. Replicate PCA analysis with randomized story order to test result stability
2. Conduct clustering with different k values to validate optimal cluster count
3. Perform vocabulary overlap analysis between individual model pairs

## Open Questions the Paper Calls Out
None

## Limitations
- Limited to six specific LLMs and 10 science fiction stories, raising generalizability concerns
- GPT-4o used as both evaluator and validation reference, potentially introducing circularity
- No control for temperature or sampling parameters that could affect evaluation consistency

## Confidence

High: Existence of statistically significant evaluation diversity across models
Medium: Characterization of evaluation profiles as analogous to human critical schools
Low: Claims about "internalized aesthetic frameworks" based solely on TF-IDF vocabulary differences

## Next Checks

1. Replicate the study using a larger, more diverse corpus of literary works across multiple genres to test generalizability of evaluation profile findings
2. Conduct comparative analysis between LLM evaluation profiles and human literary critic assessments to validate the "critical school" analogy
3. Implement blinded evaluation sessions with standardized rubrics and controlled sampling parameters to isolate true model-specific evaluation patterns from technical artifacts