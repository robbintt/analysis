---
ver: rpa2
title: 'The Era of Real-World Human Interaction: RL from User Conversations'
arxiv_id: '2509.25137'
source_url: https://arxiv.org/abs/2509.25137
tags:
- user
- feedback
- rlhi
- human
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces RLHI, a method for training language models
  by learning directly from real user conversations rather than relying on pre-annotated
  expert feedback. RLHI leverages two complementary approaches: (1) User-Guided Rewrites,
  where unsatisfactory model outputs are revised based on users'' natural-language
  follow-up feedback, and (2) User-Based Rewards, which ranks candidate responses
  using a reward model conditioned on each user''s long-term interaction history (persona).'
---

# The Era of Real-World Human Interaction: RL from User Conversations

## Quick Facts
- arXiv ID: 2509.25137
- Source URL: https://arxiv.org/abs/2509.25137
- Reference count: 35
- Primary result: RLHI improves personalization and instruction-following over base models, achieving 72.6% win rate for User-Guided Rewrites and 77.9% length-controlled win rate on AlpacaEval 2.0

## Executive Summary
The paper introduces RLHI, a method for training language models by learning directly from real user conversations rather than relying on pre-annotated expert feedback. RLHI leverages two complementary approaches: (1) User-Guided Rewrites, where unsatisfactory model outputs are revised based on users' natural-language follow-up feedback, and (2) User-Based Rewards, which ranks candidate responses using a reward model conditioned on each user's long-term interaction history (persona). These methods link long-term user personas to turn-level preferences via persona-conditioned preference optimization. Trained on real user conversations, both RLHI variants outperform strong baselines in personalization and instruction-following, with User-Guided Rewrites achieving a 72.6% win rate over the base model in human studies, and User-Based Rewards attaining a 77.9% length-controlled win rate on AlpacaEval 2.0. Similar feedback also improves reasoning performance, raising accuracy from 26.5 to 31.8 across four benchmarks.

## Method Summary
RLHI trains language models using organic user interactions through two complementary methods. User-Guided Rewrites detects when users provide natural-language feedback to revise unsatisfactory responses, then generates rewrites based on this feedback to create preference pairs for training. User-Based Rewards ranks multiple candidate responses per user request using a reward model conditioned on the user's persona (extracted from long-term conversation history) to form preference pairs. Both methods use persona-conditioned Direct Preference Optimization (DPO) to train the model, with quality filtering applied to ensure training signal reliability. The approach is trained on WildChat-1M and evaluated on personalization, instruction-following, and reasoning tasks.

## Key Results
- User-Guided Rewrites achieves 72.6% win rate over base model in human studies
- User-Based Rewards attains 77.9% length-controlled win rate on AlpacaEval 2.0
- Reasoning accuracy improves from 26.5 to 31.8 across four benchmarks
- Quality filtering critical: improves performance from +2.5 to +23.4 points
- User diversity matters: 1268 diverse users outperform 10 users with equivalent data

## Why This Works (Mechanism)

### Mechanism 1: Leveraging Natural Follow-up as Preference Signal
The method classifies user follow-up messages to identify "re-attempts with feedback" (26.51% of messages). When detected, the model uses this natural-language feedback (e.g., "make it more detailed," "add statistics") to generate a rewrite of the original unsatisfactory response. This creates a preference pair (rewrite=chosen, original=rejected) grounded in authentic user intent, which is then used for DPO training. Core assumption: user follow-up feedback reliably indicates how the initial response fell short and provides actionable guidance for improvement.

### Mechanism 2: Persona-Conditioned Reward Modeling
A natural-language persona summary is distilled from each user's conversation history (e.g., "prefers answers with numbers and statistics"). This persona conditions a reward model when scoring candidate responses, allowing the model to learn that "good" is user-specific—a response favored by one user may be disfavored by another. Core assumption: user preferences are stable enough to be captured as a persona and that this summary provides useful signal for evaluating new responses.

### Mechanism 3: RL over SFT for Relative Preference Learning
DPO outperforms supervised fine-tuning for learning from interaction data because it leverages both chosen and rejected examples. SFT only imitates positive examples. DPO explicitly optimizes the policy to increase likelihood of chosen responses while decreasing likelihood of rejected ones, conditioned on the same prompt and persona. This relative signal is more information-rich. Core assumption: the preference pairs are of high quality—rejected responses are genuinely worse than chosen ones.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO)**
  - Why needed here: Core training objective for both RLHI variants. Understanding how DPO optimizes relative preferences (chosen vs. rejected) is essential.
  - Quick check question: Given a prompt, a chosen response, and a rejected response, what quantity does DPO optimize, and how does it differ from the loss used in supervised fine-tuning?

- **Concept: Persona-Conditioned Optimization**
  - Why needed here: This is the key innovation linking long-term user history to turn-level preferences. Implementation requires modifying the standard DPO data format to include persona context.
  - Quick check question: How would you structure a training batch to include persona information alongside the standard (prompt, chosen, rejected) tuple?

- **Concept: Quality Filtering for Noisy Data**
  - Why needed here: The paper demonstrates this is not optional—without filtering, gains are marginal (+2.5 vs +23.4). Understanding why specific filters work (length as proxy for prompt quality, reward gap as indicator of ambiguity) is critical.
  - Quick check question: Why might a large reward gap between chosen and rejected responses indicate a problematic training example rather than a clear preference signal?

## Architecture Onboarding

- **Component map:** Conversation Ingestion -> Preprocessing Pipeline (user classifier, persona summarizer, candidate generator, rewrite generator, reward model scorer, quality filters) -> Training Loop (persona-conditioned DPO) -> Inference (context-only vs persona-guided)
- **Critical path:** The preference pair generation pipeline. A single failure here (misclassified feedback, poor rewrite, weak filtering) cascades into corrupted training signal.
- **Design tradeoffs:**
  - Rewrites vs. Rewards: Rewrites are higher-precision (explicit feedback) but lower-recall (requires follow-up). Rewards are higher-recall (works on any request) but lower-precision (relies on inferred persona).
  - Personalization vs. Instruction-following: Persona-guided inference can improve personalization (+7.9 points) while slightly hurting instruction-following (-1.0 points, Table 2).
  - User diversity vs. data volume: Figure 4 shows 1268 diverse users outperform 10 users with equivalent total data.
- **Failure signatures:**
  - Reward hacking: Model generates persona-flattering but unhelpful responses (e.g., always adding statistics regardless of context).
  - Noise amplification: Training on unfiltered data leads to instability or degradation.
  - Catastrophic forgetting: Over-specialization to training users hurts general capability.
- **First 3 experiments:**
  1. Reproduce the quality filtering ablation: Train three models—(a) no filtering, (b) length filter only, (c) full filtering. Evaluate on WildChat UserEval to confirm the +21-point gap.
  2. Measure the personalization-instruction tradeoff: Run a trained model with and without persona-guided inference. Plot personalization vs. instruction-following scores to characterize the Pareto frontier.
  3. Validate diversity scaling: Create matched-size datasets from (a) 10 high-activity users and (b) 1000+ diverse users. Train and compare to reproduce Figure 4's finding.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can RLHI be implemented effectively in a fully online, continual learning loop where the model updates from its own generated responses and user feedback in real-time?
  - Basis in paper: The Conclusion states, "we believe using RLHI within an online learning loop, where a continually updating deployed model learns from its organic interactions, would bring major gains compared to the fixed training data setup in our experiments."
  - Why unresolved: The experiments in this paper rely on offline training using the static WildChat dataset derived from ChatGPT interactions, rather than a live deployment where the model encounters its own generated outputs and evolving user dynamics.
  - What evidence would resolve it: A comparative study measuring performance stability, sample efficiency, and catastrophic forgetting risks in a live deployment setting versus the offline results reported in the paper.

- **Open Question 2:** How can the RLHI framework be adapted to preserve user privacy while maintaining the benefits of conditioning on long-term user personas?
  - Basis in paper: The Conclusion identifies "privacy-preserving personalization" as a key future opportunity.
  - Why unresolved: The current methodology explicitly relies on accessing, summarizing, and conditioning on extensive conversation histories ("User Persona") to generate personalized rewards and rewrites, which poses a direct privacy risk.
  - What evidence would resolve it: Experiments integrating differential privacy or federated learning techniques into the persona-conditioning mechanism that demonstrate a specific trade-off curve between privacy guarantees and personalization win rates.

- **Open Question 3:** Is it possible to reduce the dependency on external reward models (e.g., Athene-RM-8B) for quality filtering without degrading the performance of User-Guided Rewrites?
  - Basis in paper: Section 4.2 states, "without filtering high-quality signals using reward models, RLHI with User-Guided Rewrites achieves only marginal gains... underscoring the critical role of quality control in leveraging human interaction."
  - Why unresolved: The paper relies heavily on a separate, pre-trained reward model to filter "noisy" human interaction data. This reliance suggests the method may not be fully scalable or "organic" if it requires strong external supervision to clean the data.
  - What evidence would resolve it: An ablation study showing that a model can learn intrinsic quality filtering or self-correction mechanisms from raw interaction data that matches the performance of the current reward-model-filtered approach.

- **Open Question 4:** To what extent does the performance of RLHI generalize to multimodal interactions (e.g., image and audio) compared to the text-only focus of the current study?
  - Basis in paper: The Conclusion lists "broader modality and task coverage" as a direction for future work.
  - Why unresolved: The architecture and evaluation are strictly text-based (Llama-3.1-8B-Instruct on WildChat), and it is unclear if the "User-Guided Rewrite" mechanism transfers effectively to non-textual feedback loops where "revisions" are harder to define.
  - What evidence would resolve it: Application of RLHI to a multimodal dataset where user feedback includes non-textual signals (e.g., selecting regions of an image), measuring alignment improvements similar to the text-based AlpacaEval results.

## Limitations
- Data Representativeness and Bias: RLHI relies on WildChat-1M, which may not represent all user populations, with 16.1% of users having no clear expertise preference and 14.1% having no informativeness preference
- Evaluation Constraints: Human study sample size (80 conversations) and demographic (predominantly technical, under 35) limit generalizability; only win rates reported without effect sizes or statistical significance
- Implementation Complexity: Multi-stage pipeline creates numerous failure points; quality filtering parameters appear empirically chosen rather than theoretically justified

## Confidence
- **High Confidence:** RLHI improves personalization and instruction-following over base models; User-Guided Rewrites and User-Based Rewards outperform SFT on respective tasks; quality filtering significantly impacts performance (+23.4 vs +2.5 points); user diversity matters more than total data volume
- **Medium Confidence:** 72.6% win rate for User-Guided Rewrites in human studies; 77.9% length-controlled win rate on AlpacaEval 2.0 for User-Based Rewards; reasoning performance improvements (26.5→31.8) on four benchmarks; the persona-instruction tradeoff characterization
- **Low Confidence:** The specific impact of persona-guided inference vs context-only inference; generalization to non-English conversations; performance on adversarial or inconsistent feedback scenarios

## Next Checks
1. **Ablation on Quality Filtering Parameters:** Systematically vary the filtering thresholds (length, reward, gap) and measure the impact on both training stability and final performance. This validates whether the chosen parameters are robust or overfit to the specific dataset.
2. **Cross-Demographic Human Evaluation:** Conduct human studies with users outside the technical, under-35 demographic. Test whether personalization improvements hold across different user groups and whether the persona extraction process generalizes.
3. **Adversarial Feedback Testing:** Create synthetic conversations with inconsistent, malicious, or contradictory user feedback. Evaluate whether RLHI degrades performance or learns to game the persona signal, and whether the quality filters adequately protect against such scenarios.