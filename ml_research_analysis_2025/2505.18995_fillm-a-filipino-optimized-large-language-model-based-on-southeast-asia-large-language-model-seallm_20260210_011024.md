---
ver: rpa2
title: FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia
  Large Language Model (SEALLM)
arxiv_id: '2505.18995'
source_url: https://arxiv.org/abs/2505.18995
tags:
- fillm
- language
- filipino
- performance
- calamancy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces FiLLM, a Filipino-optimized large language
  model based on SeaLLM-7B 2.5, fine-tuned using LoRA to improve efficiency and task-specific
  performance. The model was evaluated on NLP tasks including Named Entity Recognition
  (NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text Summarization
  using Filipino datasets.
---

# FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)

## Quick Facts
- **arXiv ID**: 2505.18995
- **Source URL**: https://arxiv.org/abs/2505.18995
- **Reference count**: 0
- **Primary result**: FiLLM achieved F1-scores of 0.89 in NER and POS tagging but 0.73 in dependency parsing

## Executive Summary
This study introduces FiLLM, a Filipino-optimized large language model built on SeaLLM-7B 2.5 using LoRA fine-tuning. The model was evaluated on key NLP tasks including Named Entity Recognition (NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text Summarization using Filipino datasets. FiLLM demonstrated strong performance in NER and POS tagging tasks with F1-scores of 0.89, but showed limitations in handling complex syntax with lower dependency parsing performance. When compared to CalamanCy, FiLLM performed slightly worse overall, though statistical analysis confirmed significant differences between the models.

## Method Summary
FiLLM was developed by fine-tuning the SeaLLM-7B 2.5 base model using Low-Rank Adaptation (LoRA) to improve efficiency and task-specific performance. The fine-tuning process utilized Filipino language datasets across multiple NLP tasks including NER, POS tagging, dependency parsing, and text summarization. The model was then evaluated against these tasks using standard metrics, with particular focus on F1-scores for each task. Comparative analysis was conducted against CalamanCy, another Filipino language model, to benchmark performance across the evaluated tasks.

## Key Results
- Achieved F1-score of 0.89 in Named Entity Recognition (NER)
- Achieved F1-score of 0.89 in Part-of-Speech (POS) tagging
- Showed lower performance in Dependency Parsing with F1-score of 0.73

## Why This Works (Mechanism)
Assumption: The strong performance in NER and POS tagging tasks likely stems from the model's ability to effectively learn and recognize Filipino language patterns and word-level features through LoRA fine-tuning on domain-specific datasets. The lower performance in dependency parsing may indicate that while the model can identify individual words and their basic categories, it struggles with the more complex task of understanding grammatical relationships and sentence structure, which requires deeper syntactic understanding.

## Foundational Learning
- **LoRA fine-tuning**: Low-Rank Adaptation technique that freezes original model weights while adding small trainable adapters, enabling efficient task-specific optimization
- **F1-score metric**: Harmonic mean of precision and recall used to evaluate model performance across different NLP tasks
- **Dependency parsing**: NLP task focused on analyzing grammatical structure and relationships between words in sentences
- **Named Entity Recognition**: Task of identifying and classifying named entities (persons, organizations, locations) in text
- **Part-of-Speech tagging**: Process of marking up words in text as corresponding to particular parts of speech
- **Statistical significance testing**: Method to determine whether observed differences between models are meaningful or due to chance

## Architecture Onboarding

**Component Map**
Base model (SeaLLM-7B 2.5) -> LoRA adapters -> Task-specific fine-tuning -> Evaluation metrics

**Critical Path**
Base model selection → LoRA implementation → Fine-tuning on Filipino datasets → Performance evaluation → Comparative analysis

**Design Tradeoffs**
The use of LoRA fine-tuning provides computational efficiency and faster training times but may limit the model's ability to capture deep linguistic patterns compared to full fine-tuning approaches. This tradeoff allows for more accessible deployment but potentially at the cost of maximum performance optimization.

**Failure Signatures**
Lower performance in dependency parsing suggests difficulties with complex syntactic structures and grammatical relationships. The statistical significance between FiLLM and CalamanCy indicates model-specific limitations in handling certain linguistic patterns, particularly in tasks requiring deep syntactic understanding.

**First Experiments**
1. Evaluate FiLLM on additional Filipino language datasets beyond the current evaluation set
2. Test the model's performance on zero-shot and few-shot learning scenarios
3. Conduct ablation studies comparing LoRA fine-tuning with full fine-tuning approaches

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions, but implicit questions remain about the model's performance on broader linguistic tasks, its ability to handle different Filipino dialects, and its potential for zero-shot learning capabilities.

## Limitations
- Lower performance in dependency parsing (F1-score 0.73) suggests limitations in handling complex syntax
- Comparison with CalamanCy shows slightly worse overall performance despite statistical significance
- Evaluation is limited to specific NLP tasks and may not reflect broader language understanding capabilities

## Confidence
- **High Confidence**: Strong performance in NER and POS tagging tasks (F1-scores of 0.89)
- **Medium Confidence**: Statistical significance findings between FiLLM and CalamanCy
- **Low Confidence**: Overall performance in complex syntax handling and dependency parsing

## Next Checks
1. Conduct comprehensive error analysis on dependency parsing failures to identify specific syntactic patterns that challenge the model
2. Evaluate FiLLM on additional Filipino NLP benchmarks beyond the current dataset to assess generalization capabilities
3. Perform ablation studies comparing different fine-tuning approaches (full fine-tuning vs. LoRA) to quantify the impact on performance and efficiency trade-offs