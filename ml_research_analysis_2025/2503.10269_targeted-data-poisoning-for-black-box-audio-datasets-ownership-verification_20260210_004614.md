---
ver: rpa2
title: Targeted Data Poisoning for Black-Box Audio Datasets Ownership Verification
arxiv_id: '2503.10269'
source_url: https://arxiv.org/abs/2503.10269
tags:
- data
- dataset
- audio
- keys
- poisoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to protect audio datasets from unauthorized
  use by adapting the data taggants approach from image datasets. The core idea is
  to subtly poison a small fraction (1%) of the dataset such that models trained on
  the poisoned data exhibit a predictable behavior on specially crafted "key" samples.
---

# Targeted Data Poisoning for Black-Box Audio Datasets Ownership Verification

## Quick Facts
- arXiv ID: 2503.10269
- Source URL: https://arxiv.org/abs/2503.10269
- Authors: Wassim Bouaziz; El-Mahdi El-Mhamri; Nicolas Usunier
- Reference count: 31
- Primary result: Novel audio dataset ownership verification method using targeted data poisoning with 1% contamination rate achieves p-values as low as 10⁻²⁵ while maintaining SNR > 55 dB and < 0.5% validation accuracy drop

## Executive Summary
This paper introduces a method to protect audio datasets from unauthorized use by adapting the data taggants approach from image datasets. The core idea is to subtly poison a small fraction (1%) of the dataset such that models trained on the poisoned data exhibit a predictable behavior on specially crafted "key" samples. Detection is performed by checking if a black-box model's top-k predictions on these keys match the expected labels, using statistical testing to ensure low false positive rates. Experiments on SpeechCommands and ESC50 datasets with transformer models show high detection accuracy while preserving model performance and maintaining stealthiness. The method is robust to common data augmentations and different initialization, making it a practical solution for dataset ownership verification in audio applications.

## Method Summary
The method crafts targeted data poisoning by aligning gradients between poisoned training samples and synthetic out-of-distribution "key" samples. A surrogate model generates perturbations that cause models trained on poisoned data to predict specific labels for the keys. Keys are synthesized as random matrices in the spectral domain, converted to audio via Griffin-Lim reconstruction, and assigned random labels. The poisoning process uses gradient alignment optimization with signed Adam, clipping perturbations to maintain imperceptibility. Detection queries only the black-box model's top-k predictions on keys and applies binomial hypothesis testing to determine if predictions significantly deviate from chance level.

## Key Results
- Detection confidence: p-values as low as 10⁻²⁵ with K=10 keys and k=1-10
- Stealthiness: SNR > 55 dB across all experiments, maintaining audio quality
- Performance preservation: Validation accuracy drop < 0.5% compared to clean training
- Robustness: Effective against common data augmentations (frequency/time masking, mixup)
- Cross-initialization: Works across different random initializations of same architecture

## Why This Works (Mechanism)

### Mechanism 1: Gradient Alignment for Cross-Modal Poisoning
Perturbing training samples to align their loss gradients with those of secret "key" samples causes models to memorize key-label associations during normal training. The method optimizes perturbations δ on training audio samples so that ∇θL(f(x(key), θA), y(key)) aligns with ∑∇θL(f(x+δ, θA), y). When Bob's model trains on perturbed data via standard SGD, the aligned gradients push the model toward predicting y(key) for x(key)—without Bob knowing the keys exist. Core assumption: Gradient direction correlation between Alice's surrogate model and Bob's independently initialized model transfers sufficiently to induce the target behavior.

### Mechanism 2: Out-of-Distribution Key Design for Statistical Guarantees
Synthesizing keys as out-of-distribution (OOD) audio samples ensures benign models perform at chance level, enabling provable false positive rate bounds. Keys are generated as random d×d matrices in the spectral domain, resized to mel-spectrogram dimensions, then converted to audio via Griffin-Lim reconstruction. Since these synthetic spectrograms don't correspond to real audio, they fall outside the training distribution, making their predictions under H₀ uniform random. Core assumption: OOD keys do not accidentally correlate with any natural audio patterns that benign models might have learned.

### Mechanism 3: Black-Box Top-k Statistical Detection
Querying only top-k predictions on K keys provides sufficient signal for high-confidence detection while remaining practical in restricted-access scenarios. Alice queries Bob's model API for top-k predictions on each key, computes top-k accuracy, and applies binomial test. The p-value directly bounds false positive probability—no model internals needed. Core assumption: Adversary does not know the keys or deliberately evade by modifying predictions on those specific inputs.

## Foundational Learning

- Concept: Gradient-based data poisoning / gradient matching
  - Why needed here: The entire poisoning mechanism depends on understanding how to craft input perturbations that align gradient directions. Without this, you cannot induce targeted behavior in models you don't train yourself.
  - Quick check question: Given a surrogate model f(·, θ), can you explain why minimizing cos(∇θL_key, ∇θL_poisoned) would cause training on poisoned samples to affect predictions on keys?

- Concept: Griffin-Lim algorithm for phase reconstruction
  - Why needed here: Keys are generated as mel-spectrograms but must be converted to waveforms. Griffin-Lim iteratively estimates phase from magnitude spectrograms, enabling the OOD audio synthesis pipeline.
  - Quick check question: Why can't you directly invert a mel-spectrogram to audio, and what does Griffin-Lim approximate to solve this?

- Concept: Binomial hypothesis testing and p-value interpretation
  - Why needed here: Detection relies on bounding false positive rates via binomial test. Understanding what p=10⁻²⁵ means—and what it doesn't (e.g., not probability of H₀ being true)—is essential for correct interpretation.
  - Quick check question: If a benign model has top-1 accuracy 2/10 on 10 keys for a 10-class problem, what is the approximate p-value under H₀?

## Architecture Onboarding

- Component map: Key Generator -> Poisoning Optimizer -> Protected Dataset -> Detector
- Critical path: Key generation quality → gradient alignment optimization convergence → perturbation stealthiness (SNR >55dB) → detection statistical power. Failure at any stage breaks the chain.
- Design tradeoffs:
  - Key dimension d: Higher d (128) gives better detection (p∼10⁻²⁵) but may increase risk of resembling natural audio; lower d (16) fails (FNR >0)
  - Poisoning rate ε: 1% chosen for stealth; higher rates improve detection but risk detection by Bob
  - Interpolation method: Bilinear outperforms nearest neighbor for uniform distribution keys
  - k in top-k: Lower k gives stronger p-values but requires higher key accuracy
- Failure signatures:
  - High FNR (>0): Check d parameter—d=16 with Bernoulli+nearest fails; increase to d≥32
  - Detectable artifacts: SNR >55dB but human listening reveals artifacts; paper notes need for perceptual loss term
  - Validation accuracy drop: Not observed in experiments (<0.5% difference), but monitor if perturbation amplitude or poisoning rate increased
  - Cross-architecture failure: Paper only tests same-architecture (AST); different architectures may require re-tuning
- First 3 experiments:
  1. Reproduce key generation ablation: Generate keys with d∈{16,32,64,128}, uniform vs Bernoulli, nearest vs bilinear. Measure top-k accuracy and p-values on AST model trained on poisoned SpeechCommands. Confirm d=128, uniform, bilinear gives p<10⁻²⁰.
  2. Test cross-architecture robustness: Train a different audio model (e.g., CNN-based like AudioResNet) on the same poisoned dataset. Query with same keys. If detection fails, analyze gradient alignment between AST surrogate and target architecture.
  3. Stealthiness audit: Compute SNR of perturbations, conduct human listening tests on poisoned vs clean samples. Identify which acoustic features reveal artifacts. Test whether adding perceptual loss (e.g., STFT magnitude difference) improves imperceptibility while maintaining detection.

## Open Questions the Paper Calls Out

- Can the integration of an audio-specific perceptual loss function eliminate perceivable artifacts while maintaining the effectiveness of the data poisoning?
  - The authors state future work should introduce an audio perceptual loss to enforce imperceptibility, noting qualitative analysis revealed perceptible artifacts despite high SNR.

- Does the targeted data poisoning scheme transfer effectively when the suspect model (Bob) uses a different architecture than the surrogate model (Alice)?
  - Experiments only consider Bob's model having the same architecture as Alice's model but with different initialization. Cross-architecture transferability remains untested.

- Is the method robust against automated data sanitization or filtering techniques designed to remove outliers?
  - The paper only evaluates robustness against data augmentations, not data cleaning. It's unknown if unsupervised outlier detection would flag poisoned samples as anomalies and filter them out before training.

## Limitations

- Cross-architecture transferability remains unproven beyond same-architecture settings with different initializations
- Stealthiness claims rely on SNR measurements without comprehensive human perceptual evaluation
- No analysis of adaptive adversary scenarios where Bob knows about the verification scheme

## Confidence

**High confidence** in the core detection mechanism: The statistical framework using binomial testing on top-k predictions is well-established, and the reported p-values (down to 10⁻²⁵) demonstrate strong empirical evidence for reliable detection when conditions are met.

**Medium confidence** in gradient-based poisoning effectiveness: While the adaptation of gradient matching from image to audio datasets shows promising results, the limited scope to single architecture and lack of cross-model validation create uncertainty about generalizability.

**Medium confidence** in stealthiness claims: The SNR >55 dB measurements provide quantitative evidence, but human perceptual evaluation is limited to brief mentions without systematic assessment, leaving questions about detectability in real-world scenarios.

**Low confidence** in adaptive adversary robustness: The paper does not address scenarios where Bob knows about the verification scheme and actively tries to evade detection, which represents a significant gap in practical applicability.

## Next Checks

1. **Cross-architecture gradient alignment study**: Train the poisoning surrogate model (AST) and target models with different architectures (e.g., CNN-based AudioResNet, CRNN) on the same poisoned dataset. Quantify how gradient alignment quality correlates with detection success across architectures to establish transferability bounds.

2. **Comprehensive stealthiness evaluation**: Conduct systematic human listening tests with poisoned vs clean audio samples across different audio classes and poisoning rates. Measure detection rates at varying SNR levels and identify which acoustic features (e.g., spectral artifacts, phase distortions) most commonly reveal perturbations.

3. **Adaptive adversary stress testing**: Design scenarios where Bob implements query logging, frequency analysis to detect OOD key patterns, or adversarial fine-tuning to reduce key prediction accuracy. Measure how detection performance degrades under these conditions and identify potential countermeasures.