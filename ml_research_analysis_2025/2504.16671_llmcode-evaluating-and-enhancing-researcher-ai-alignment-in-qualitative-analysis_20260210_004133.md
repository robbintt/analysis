---
ver: rpa2
title: 'LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative Analysis'
arxiv_id: '2504.16671'
source_url: https://arxiv.org/abs/2504.16671
tags:
- coding
- qualitative
- examples
- research
- participants
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "LLMCode introduces a systematic approach to evaluate and align\
  \ LLM-driven qualitative coding within research for design (RfD). The tool integrates\
  \ two metrics\u2014Intersection over Union (IoU) and Modified Hausdorff Distance\
  \ (MHD)\u2014to measure alignment between human and LLM-generated insights."
---

# LLMCode: Evaluating and Enhancing Researcher-AI Alignment in Qualitative Analysis

## Quick Facts
- arXiv ID: 2504.16671
- Source URL: https://arxiv.org/abs/2504.16671
- Authors: Joel Oksanen; Andrés Lucero; Perttu Hämäläinen
- Reference count: 40
- Primary result: Introduces a tool using IoU and MHD metrics to align human and LLM-driven qualitative coding, showing strong deductive alignment but challenges in deeper interpretation.

## Executive Summary
LLMCode is a tool designed to evaluate and improve alignment between human researchers and LLMs in qualitative coding for design research. It uses two metrics—Intersection over Union (IoU) for surface-level text overlap and Modified Hausdorff Distance (MHD) for semantic code similarity—to guide iterative refinement of few-shot examples. Across two studies with 26 designers, the system demonstrated effective alignment in deductive coding tasks but struggled with deeper interpretive reasoning. The tool enables a reciprocal dynamic where humans and AI influence each other's annotations, though it highlights ongoing challenges in preserving interpretive depth while enabling scalable collaboration. LLMCode is open-source and available for further development.

## Method Summary
The method involves manually coding a subset of texts to establish a codebook and validation set, then selecting few-shot examples to guide LLM annotation of remaining texts. The LLM generates annotations, which are compared to human work using IoU (character-level highlight overlap) and MHD (semantic code distance via embeddings). Users iteratively refine examples based on alignment metrics, aiming to improve human-LLM agreement. The process is tested across two studies with designers analyzing Reddit posts about board games, evaluating both performance trends with example quantity and the effectiveness of iterative refinement versus random selection.

## Key Results
- IoU and MHD metrics effectively capture alignment between human and LLM annotations, with IoU measuring surface overlap and MHD assessing semantic similarity.
- Iterative refinement of few-shot examples using alignment metrics outperformed random selection, particularly after the first iteration.
- The tool showed strong performance in deductive coding but struggled with inductive tasks requiring deeper interpretive reasoning or novel code generation.
- Users experienced a reciprocal dynamic, sometimes adopting AI-suggested codes or revising their own perspectives based on model outputs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing more few-shot examples improves surface-level alignment between LLM and human annotations.
- Mechanism: In-context learning allows the LLM to recognize patterns in human-annotated examples and apply similar annotation patterns to new texts, increasing IoU (highlight overlap) and decreasing MHD (semantic code distance).
- Core assumption: The LLM can generalize annotation patterns from examples to unseen texts with similar characteristics.
- Evidence anchors:
  - [abstract] "The system showed strong performance in deductive coding but struggled with deeper interpretive reasoning."
  - [section 5.1.1] Figure 2 shows increasing IoU trend with more examples; Figure 3 shows decreasing MHD; exponential growth curve fitted to data.
  - [corpus] DeTAILS paper confirms similar findings: LLM support works best when integrated into iterative workflows with human oversight.
- Break condition: Performance plateaus when codebook saturates; model struggles when test texts diverge semantically from example themes (Pearson correlation = 0.53 between example-text dissimilarity and performance decline).

### Mechanism 2
- Claim: Iterative refinement of few-shot examples using alignment metrics produces better human-LLM alignment than random example selection.
- Mechanism: The IoU and MHD metrics enable users to identify misalignments by sorting texts by quality scores, then curating examples that address specific failure modes.
- Core assumption: Users can correctly diagnose why the model misaligned and select corrective examples.
- Evidence anchors:
  - [abstract] "Users iteratively refined model outputs using the metrics, demonstrating a reciprocal dynamic."
  - [section 5.2] Figure 6 shows human iteration outperforms random baseline by third iteration, particularly for IoU.
  - [corpus] IDEAlign paper similarly finds structured comparison between LLM and human annotations improves alignment in interpretive tasks.
- Break condition: Users struggle when they cannot articulate *why* an example is representative; first iteration often matches random baseline due to lack of prior model behavior insight.

### Mechanism 3
- Claim: LLM suggestions can reshape human coding perspectives, creating bi-directional influence.
- Mechanism: When users observe divergent AI annotations, they sometimes adopt new codes or revise their interpretive framework, particularly when AI surface novel thematic distinctions.
- Core assumption: Users trust AI suggestions enough to reconsider their own annotations, but not so much that they abandon critical judgment.
- Evidence anchors:
  - [abstract] "A reciprocal dynamic where humans both shaped and adapted to AI suggestions."
  - [section 5.3.1] Participant P25 initially removed a code because AI didn't annotate it, then re-adopted it when AI later suggested "Participant Background"; P23 reflected on whether "surprise" and "story" codes were meaningfully distinct from "novelty" and "lore."
  - [corpus] Bridging Human Interpretation paper highlights risks of LLM-mediated interpretation shifting researcher stance.
- Break condition: Over-trust leads to adopting AI's "vague" or context-inappropriate codes; under-trust leads to dismissing useful alternative perspectives.

## Foundational Learning

- Concept: **Inductive vs. Deductive Coding**
  - Why needed here: LLMCode handles both, but inductive coding (codes emerge during analysis) is where the model struggles most; understanding this distinction is critical for interpreting MHD scores.
  - Quick check question: Can you explain why the paper finds deductive coding alignment easier to achieve than inductive?

- Concept: **In-Context Learning (Few-Shot Prompting)**
  - Why needed here: The entire alignment workflow depends on users understanding that examples are submitted as a static batch, not a conversational history—this was a major source of user confusion.
  - Quick check question: How does in-context learning differ from chat-based interaction in terms of how "corrections" are processed?

- Concept: **Embedding-Based Semantic Similarity**
  - Why needed here: MHD uses embedding distances to compare code semantics; interpreting MHD requires understanding that scores reflect model-specific embedding space properties, not absolute truth.
  - Quick check question: Why might two codes with different wording still have low MHD (high semantic similarity)?

## Architecture Onboarding

- Component map:
  Manual Coding Interface -> Example Selector -> LLM Coding Engine -> Alignment Metrics Module -> Comparison View

- Critical path:
  1. Manually code subset of texts (establish codebook and validation set).
  2. Select few-shot examples from coded texts.
  3. Run LLM on remaining texts; review comparison view sorted by lowest alignment.
  4. Iterate: adjust examples, prompt instructions, or human annotations.
  5. After convergence, apply final configuration to full corpus.

- Design tradeoffs:
  - **Validation vs. test set**: Paper used validation set only due to time constraints; recommends separate test set for rigorous work to detect overfitting.
  - **Static examples vs. conversational feedback**: Static batch approach confuses users accustomed to chat; future versions could support "corrected annotation" examples.
  - **Automation depth**: Full automation risks homogenizing perspectives; tool preserves interpretive depth by requiring human example curation.

- Failure signatures:
  - **Hallucinated text changes**: Model sometimes "corrects" typos despite instructions; LLMCode uses approximate string matching to recover.
  - **Vague or overlapping codes**: AI generates codes like "improvements" that lack specificity; IoU may be high but MHD reflects semantic drift.
  - **Missed meme/humor context**: Model fails to recognize when niche humor shifts text meaning entirely.
  - **Overfitting to validation set**: High alignment scores on iterated texts don't guarantee performance on held-out data.

- First 3 experiments:
  1. **Baseline IoU/MHD with random examples**: Code 20 texts manually; select 5 examples randomly; measure alignment on 15 remaining texts. Repeat with 10, 15 examples to reproduce Figure 2/3 curves.
  2. **Iterative vs. random example selection**: After first pass, use comparison view to identify worst-aligned texts; manually select corrective examples; measure improvement over 3 iterations against random baseline (reproduce Figure 6).
  3. **Out-of-distribution test**: Train examples on texts from one code cluster; test on texts from different cluster; measure MHD degradation (reproduce Figure 5 correlation analysis).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLMs move beyond surface-level pattern recognition to genuinely emulate a researcher's inductive reasoning and emergent understanding?
- Basis in paper: [explicit] The paper concludes that while LLMs capture surface patterns, "The challenge of true interpretive alignment—where an LLM would spontaneously identify, refine, or propose new codes that mirror a researcher's emergent understanding—remains largely unmet" (Section 6.1).
- Why unresolved: The study found that model performance was correlated with the representativeness of examples, and it struggled to extrapolate to novel themes or codes not present in the few-shot examples, suggesting limitations in deeper interpretive capabilities.
- What evidence would resolve it: Evidence of an LLM successfully identifying and proposing valid, novel codes for texts that diverge significantly from the provided few-shot examples, matching the nuanced quality of human expert analysis.

### Open Question 2
- Question: How can interface designs bridge the gap between users' conversational mental models and the static requirements of few-shot in-context learning?
- Basis in paper: [explicit] Section 6.2.1 identifies a "core challenge" where users apply chat-based mental models (iterative correction) to a system that requires static example selection, leading to confusion about how to teach the model.
- Why unresolved: The authors note that the static nature of the examples confused participants, who wished to provide feedback on the "reasoning" behind selections (e.g., "You've misunderstood the text"), a capability the current system lacks.
- What evidence would resolve it: User studies testing alternative interfaces (e.g., those allowing "corrected" versions of AI output as examples) that demonstrate users can successfully teach the model their perspective with lower cognitive load and confusion.

### Open Question 3
- Question: To what extent does the "reciprocal influence" of AI suggestions on human analysis affect the overall trustworthiness and interpretive depth of research?
- Basis in paper: [inferred] While the paper observes that users adapt their coding based on AI output (Section 5.3.1), it explicitly calls for future work to investigate "appropriate reliance" and ensure adoption does not "compromise the reflexive and context-rich nature of design inquiry" (Section 6.2.2).
- Why unresolved: The study observed users adopting AI codes (sometimes based on "leaps of faith"), but it did not measure whether these AI-induced changes improved or diminished the objective quality or validity of the final insights.
- What evidence would resolve it: Comparative studies measuring the quality, novelty, and validity of insights generated by human-only analysis versus human-AI reciprocal analysis, specifically assessing if AI suggestions narrow or broaden the diversity of thought.

### Open Question 4
- Question: How well do alignment metrics (IoU and MHD) optimized during iterative validation generalize to unseen test data?
- Basis in paper: [inferred] Section 6.3 notes that the iterative approach is susceptible to overfitting and that Study 2 did not employ a separate test set, meaning "these scores may not fully reflect the true human-AI alignment."
- Why unresolved: Due to time constraints and participant fatigue, the study relied on validation set metrics to demonstrate tool utility, leaving the generalizability of these improvements to new, unannotated data unverified.
- What evidence would resolve it: An experiment where alignment metrics are tracked on a held-out test set after iterative refinement, demonstrating that improved scores on the validation set correlate with improved performance on unseen data.

## Limitations
- Evaluation lacked a true test set, relying on internal validation due to time and dataset constraints.
- MHD metric depends on a specific embedding space, which may not fully capture nuanced thematic distinctions.
- Tool favors deductive over inductive coding, limiting utility for exploratory research.
- Participant pool (26 designers) may not reflect diversity of qualitative researchers in other domains.

## Confidence
- **High confidence**: IoU metric performance in surface-level alignment; iterative refinement mechanism effectiveness; bi-directional influence between human and AI coding.
- **Medium confidence**: MHD-based semantic alignment; generalizability across research domains; tool’s ability to preserve interpretive depth.
- **Low confidence**: Full automation potential; long-term researcher-AI alignment stability; impact on research outcomes.

## Next Checks
1. Test generalizability: Apply LLMCode to a different qualitative dataset (e.g., interviews or open-ended survey responses) and compare alignment performance to the original study.
2. Measure interpretive depth: Conduct a post-hoc analysis to determine whether LLM-suggested codes introduce novel insights or merely replicate existing themes.
3. Evaluate overfitting risk: Split the dataset into training, validation, and test sets to assess whether high alignment scores on iterated examples translate to held-out data.