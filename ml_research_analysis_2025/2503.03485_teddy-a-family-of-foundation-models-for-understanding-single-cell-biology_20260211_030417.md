---
ver: rpa2
title: 'TEDDY: A Family Of Foundation Models For Understanding Single Cell Biology'
arxiv_id: '2503.03485'
source_url: https://arxiv.org/abs/2503.03485
tags:
- data
- disease
- cell
- teddy
- gene
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TEDDY, a family of transformer-based foundation
  models for single-cell RNA sequencing (scRNA-seq) data. The authors pre-trained
  models ranging from 10M to 400M parameters on a 116M cell dataset from CELL XGENE,
  using both self-supervised gene modeling and supervised biological annotation tasks.
---

# TEDDY: A Family Of Foundation Models For Understanding Single Cell Biology

## Quick Facts
- **arXiv ID:** 2503.03485
- **Source URL:** https://arxiv.org/abs/2503.03485
- **Reference count:** 20
- **Primary result:** TEDDY-G models outperform existing foundation models on held-out donor disease classification (accuracy 0.72±0.04)

## Executive Summary
This paper introduces TEDDY, a family of transformer-based foundation models for single-cell RNA sequencing (scRNA-seq) data. The authors pre-trained models ranging from 10M to 400M parameters on a 116M cell dataset from CELL XGENE, using both self-supervised gene modeling and supervised biological annotation tasks. Two variants, TEDDY-G (rank-value encoding) and TEDDY-X (binned expression), were developed. The models were evaluated on two disease classification tasks: identifying held-out disease states and distinguishing healthy from diseased cells. Results showed that TEDDY-G models outperformed existing foundation models on held-out donors (accuracy 0.72±0.04) and provided consistent improvements over task-specific machine learning methods. Performance improved predictably with model scale and data volume, though improvements plateaued for larger models. Supervision via biological annotations further enhanced downstream performance.

## Method Summary
TEDDY models are transformer-based foundation models pre-trained on 116M single-cell RNA-seq profiles from CELL XGENE. The models use rank-value encoding (TEDDY-G) or binned expression (TEDDY-X) to handle the sparsity of scRNA-seq data. Pre-training combines masked language modeling (predicting masked gene identities or expression ranks) with supervised classification of biological annotations (disease, tissue, cell type, sex). The models range from 10M to 400M parameters and are evaluated on two disease classification tasks using held-out donors and unseen diseases. Training uses AdamW optimizer with weight decay 0.1, linear warmup, and batch size 256.

## Key Results
- TEDDY-G (rank-value encoding) outperformed existing foundation models on held-out donor disease classification (accuracy 0.72±0.04)
- Performance improved predictably with model scale and data volume, though improvements plateaued for larger models
- Supervision via biological annotations enhanced downstream performance on disease classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Encoding gene expression as a ranked list (TEDDY-G) enables the model to learn gene co-manifestation patterns robust to batch effects and absolute count noise.
- **Mechanism:** By sorting genes by expression level and training the model to predict the gene identity at a masked position (rather than the count value), the architecture learns which genes function together regardless of the specific sequencing depth or donor-specific expression magnitude.
- **Core assumption:** The relative ranking of genes within a cell carries sufficient signal to distinguish biological states (e.g., disease vs. healthy) without needing exact expression counts.
- **Evidence anchors:**
  - [abstract] "TEDDY-G (rank-value encoding)... outperformed existing foundation models."
  - [section 3] "This rank-value encoding scheme ranks genes expressed in a cell based on their (normalized) expression values... pre-training task then involves predicting the gene expressed at a particular position."
  - [corpus] Contextual; related work like Geneformer validates rank-encoding, but specific scaling limits at 400M parameters are unique to this paper.
- **Break condition:** If downstream tasks require distinguishing subtle expression changes where ranks are identical (e.g., two genes switching ranks slightly but having massive expression differences critical to a pathway), this mechanism provides no signal.

### Mechanism 2
- **Claim:** Supervising pre-training with biological annotations forces the model to organize cell embeddings according to known biological ontologies, improving generalization.
- **Mechanism:** The model minimizes a joint loss: standard gene masking plus a classification loss predicting metadata (disease, tissue, cell type, sex) from special tokens. This explicitly maps the high-dimensional gene space to lower-dimensional biological concepts, aligning the learned manifold with known disease biology.
- **Core assumption:** The metadata labels (e.g., coarse "cancer" or "immune" labels) are accurate and consistent across the 116M cells, and these labels correlate with the gene programs relevant to the downstream target task.
- **Evidence anchors:**
  - [abstract] "leveraged the availability of large-scale biological annotations as a form of supervision during pre-training."
  - [section 3.2] "We leverage the available annotations as supervisory signals... encourages models to learn embeddings that... align with high-level biological properties."
  - [corpus] Weak; corpus neighbors focus on multimodal/text integration rather than ontology-based supervision signals.
- **Break condition:** If annotations are noisy (e.g., mislabeled cell types in the source corpus) or if the downstream task involves a novel disease mechanism uncoupled from the coarse ontologies (e.g., "cancer"), the supervision may bias the model toward spurious correlations.

### Mechanism 3
- **Claim:** Scaling data volume and model parameters improves generalization to unseen donors, but with diminishing returns.
- **Mechanism:** A larger transformer (up to 400M params) and larger corpus (116M cells) increase the network's capacity to memorize rare gene-program interactions and reduce overfitting to specific donor batches, allowing the model to identify the "disease signal" separate from the "donor noise."
- **Core assumption:** The underlying biology of disease is consistent across donors, and the "noise" of donor variability can be averaged out or ignored given sufficient model capacity.
- **Evidence anchors:**
  - [abstract] "Performance improved predictably with model scale and data volume, though improvements plateaued for larger models."
  - [section 5.1] "We observe that both model variants improve in performance with the number of parameters... but improvements plateau."
  - [corpus] Consistent with general scaling laws, but specific plateauing at 400M suggests data quality or architectural bottlenecks.
- **Break condition:** If the testing donor population has a confounding variable (e.g., a specific medication or comorbidity) absent in the training distribution, scaling will not resolve the distribution shift.

## Foundational Learning

- **Concept: Single-Cell RNA-seq (scRNA-seq) Sparsity**
  - **Why needed here:** scRNA-seq data is extremely sparse (most genes have zero counts). Understanding that "zero" can mean "not expressed" or "not captured" (dropout) is critical for interpreting why the authors use rank-value encoding or imputation-like objectives.
  - **Quick check question:** Can you explain why a cell might have a count of zero for a gene that is actually highly expressed?

- **Concept: Transformer Self-Attention & Context Windows**
  - **Why needed here:** The model uses a context length of 2048 genes (tokens). You must understand that the model's "view" of a cell is limited to the top 2048 expressed genes, effectively ignoring low-expressed genes in a single cell.
  - **Quick check question:** If a critical disease marker is the 3000th most expressed gene in a cell, will TEDDY-G see it during processing?

- **Concept: Fine-tuning vs. Linear Probing**
  - **Why needed here:** The evaluation distinguishes between updating model weights (fine-tuning) and training a classifier on frozen embeddings (linear probing). The success of linear probing (Section 5.4) indicates the quality of the pre-trained representation manifold.
  - **Quick check question:** If linear probing works well but fine-tuning fails, what does that suggest about the pre-trained features?

## Architecture Onboarding

- **Component map:** Normalized expression matrix -> Tokenizer (Rank-based for -G, Bin-based for -X) -> Special Tokens (`<disease>`, `<tissue>`, etc.) -> Transformer Encoder (12-24 layers, 512-1024 hidden dim) -> Parallel outputs for Masked Gene Prediction and Annotation Classification

- **Critical path:**
  1. Data Cleaning: Filter cells with <225 genes or >10% mitochondrial reads (Section 3.1)
  2. Normalization: Non-zero median scaling (Section 3.4)
  3. Tokenization: Sort genes by expression; take top 2048
  4. Forward Pass: Mask 15% of genes; compute loss (MLM + CLS)
  5. Backprop: Use AdamW with weight decay 0.1 (critical for stability)

- **Design tradeoffs:**
  - **TEDDY-G vs. TEDDY-X:** -G (Rank) is better for classification tasks but loses absolute magnitude info. -X (Binned) preserves magnitude info but performs worse on the specific donor generalization task reported here.
  - **Supervision:** Adding annotation loss improves downstream performance but requires clean metadata, which may not exist for all custom datasets.

- **Failure signatures:**
  - **Loss Spikes/Divergence:** Addressed by high weight decay (0.1) and warmup (10k steps) (Section 3.4)
  - **Data Contamination:** Performance on "Held-out Donors" is the only valid metric; high accuracy on "Held-out Diseases" is noisy/modest. If validation loss drops but held-out donor accuracy is random, the model has overfit to study-specific artifacts.

- **First 3 experiments:**
  1. **Sanity Check (Linear Probe):** Extract embeddings from the frozen TEDDY-G 70M model for a small labeled dataset; train a Logistic Regression to verify embeddings separate healthy/diseased cells better than random.
  2. **Ablation (Supervision):** Compare TEDDY-G 70M with vs. without the annotation classification head on the held-out donor task to quantify the value of the supervised signal.
  3. **Scaling Test:** Run inference on a single cell batch using the 10M, 70M, and 400M variants to benchmark inference latency vs. accuracy gains on your specific hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the limited performance on the "held-out diseases" task stem from model generalization failure or noise in the benchmark datasets?
- **Basis in paper:** [Explicit] The authors state in the Discussion that "Carefully disentangling these factors is important for future work," referring to whether the limited effectiveness is due to the "failure of the models to adequately generalize" or "noise from experimental and data collection artifacts."
- **Why unresolved:** The evaluation relies on donor-level labels rather than cell-level ground truth, making it difficult to isolate the source of the error.
- **Evidence to resolve:** Systematic analysis of benchmark datasets to quantify experimental noise or the acquisition of cell-level ground truth labels for disease states.

### Open Question 2
- **Question:** How does the weighting of the supervised annotation loss ($\ell_{CLS}$) relative to the gene modeling loss ($\ell_{MLM}$) impact downstream performance?
- **Basis in paper:** [Explicit] The authors note that while preliminary experiments showed weighting did not substantially affect performance, "A more careful exploration is part of our planned future work."
- **Why unresolved:** The current implementation uses a simple summation of losses, leaving the optimal balance between learning biological ontologies and raw gene representations unexplored.
- **Evidence to resolve:** A hyperparameter sweep over the loss weighting coefficient during pre-training, evaluated on a diverse set of downstream tasks.

### Open Question 3
- **Question:** What is the benefit of jointly training on multi-species data (human and mouse) at scale for foundation models?
- **Basis in paper:** [Explicit] Appendix A.3 states, "A more large scale exploration of using multi-species data is needed to better understand the benefit of jointly training on multi-species data."
- **Why unresolved:** Preliminary experiments on a 4M cell subset showed similar test losses for joint vs. human-only models, but the effects at the full 116M cell scale remain unknown.
- **Evidence to resolve:** Comparative training of large-scale models on human-only versus joint corpora, followed by evaluation on cross-species conservation tasks.

## Limitations

- **Data Contamination Risk:** The evaluation on "held-out donors" and "held-out diseases" may be compromised by unintentional inclusion of test cells in the pre-training corpus, as both derive from the same CELL XGENE database.
- **Batch Effect Generalization:** The paper does not test cross-platform generalization (e.g., from 10x Genomics to Smart-seq2) or cross-species transfer beyond the mixed human-mouse pre-training.
- **Model Scale Diminishing Returns:** The reported plateauing of performance beyond 70M parameters suggests either data quality limitations or architectural bottlenecks, but the paper does not investigate whether larger models would benefit from more diverse data sources or different architectural choices.

## Confidence

**High Confidence:** Claims about TEDDY-G outperforming existing foundation models on held-out donors are supported by explicit benchmark results and the rank-value encoding mechanism is well-justified by the need to handle scRNA-seq sparsity and batch effects.

**Medium Confidence:** Claims about supervision improving downstream performance are plausible but rely on the assumption that the 43 coarse biological annotations are accurate and relevant to disease biology. The paper does not investigate the impact of annotation noise or ontology misalignment.

**Low Confidence:** Claims about TEDDY-G being superior for drug discovery applications are speculative. The paper only evaluates classification tasks, not generative or perturbation prediction tasks that would be relevant for drug discovery workflows.

## Next Checks

1. **Data Contamination Audit:** Request from authors the exact list of dataset IDs excluded from pre-training to verify that no cells from the "held-out diseases" datasets appear in the 116M training corpus. Re-run evaluation if contamination is found.

2. **Cross-Platform Transfer:** Fine-tune TEDDY-G on a dataset from a different sequencing platform (e.g., Smart-seq2) or technology (e.g., spatial transcriptomics) to test whether rank-value encoding generalizes beyond 10x Genomics data.

3. **Architecture Scaling Analysis:** Train TEDDY-G with 70M parameters on progressively larger subsets of the 116M cells (10M, 50M, 116M) to determine whether the plateau is due to data limitations or architectural constraints, and whether different architectures (e.g., with sparse attention) would scale better.