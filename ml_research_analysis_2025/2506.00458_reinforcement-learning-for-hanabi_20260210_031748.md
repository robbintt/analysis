---
ver: rpa2
title: Reinforcement Learning for Hanabi
arxiv_id: '2506.00458'
source_url: https://arxiv.org/abs/2506.00458
tags:
- learning
- agents
- agent
- sarsa
- card
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigated the performance of tabular and deep reinforcement
  learning algorithms in the cooperative card game Hanabi, where players have incomplete
  information about the environment. Four tabular methods (Q-learning, SARSA, n-step
  SARSA, Expected SARSA) and their deep learning counterparts were evaluated through
  1,000-game experiments against both same-type and different-type agents.
---

# Reinforcement Learning for Hanabi

## Quick Facts
- arXiv ID: 2506.00458
- Source URL: https://arxiv.org/abs/2506.00458
- Reference count: 1
- Key outcome: Deep reinforcement learning agents outperformed tabular versions by 80% in cooperative Hanabi, with Expected SARSA achieving highest scores among tabular methods

## Executive Summary
This study systematically compares tabular and deep reinforcement learning algorithms in the cooperative card game Hanabi, where players have incomplete information about their own cards. Four tabular methods (Q-learning, SARSA, n-step SARSA, Expected SARSA) and their deep learning counterparts were evaluated through 1,000-game experiments. The research found that Expected SARSA achieved the highest scores among tabular agents, while deep Q-learning and 2-step SARSA with four hidden layers performed best among deep learning agents. Deep learning agents showed 80% improvement in scores compared to tabular versions (p=0.0038), though with 8-12x longer training times.

## Method Summary
The study implemented four tabular temporal difference algorithms (Q-learning, SARSA, n-step SARSA with n∈{1,2,8}, Expected SARSA) and deep learning counterparts using four-layer MLPs with ReLU activations and softmax outputs. Training used the Hanabi environment from the IAI project with a 20-action, 12-reward-reason framework. Agents played 1,000 games per evaluation pairing, with Expected SARSA using an increasing epsilon parameter. Deep networks used Adam optimizer (β1=0.900, β2=0.999, ε=1e-07, momentum=0.990) with learning rate 0.01. The primary metric was average score per game, supplemented by turns, plays, discards, and hints.

## Key Results
- Expected SARSA achieved the highest scores among tabular methods, leveraging hybrid on/off-policy learning dynamics
- Deep Q-learning and 2-step SARSA with four hidden layers performed best among deep learning agents
- Deep learning agents showed 80% improvement in scores compared to tabular versions (p=0.0038)
- 2-step SARSA demonstrated predictive advantage through multi-step returns in the sparse-reward Hanabi environment

## Why This Works (Mechanism)

### Mechanism 1
Expected SARSA outperforms other tabular methods in cooperative, partially observable settings due to its hybrid on/off-policy learning dynamics. Expected SARSA computes the expected value over all actions weighted by their probabilities, rather than selecting the maximum (Q-learning) or the actual next action (SARSA). This reduces variance in value estimates and allows smoother adaptation when partner behavior is uncertain. Early training approximates on-policy SARSA to moderate loss; later training shifts toward off-policy Q-learning as the policy refines.

### Mechanism 2
Neural network function approximation replaces Expected SARSA's expected-reward advantage, causing Q-learning and n-step SARSA to dominate in deep variants. The neural network learns a continuous approximation of Q(s,a), generalizing across similar states. This generalization effectively performs the "expectation" that Expected SARSA explicitly computes in tabular form. Q-learning's greedy maximization then exploits the smoother value landscape without the computational overhead of explicit expectation calculation.

### Mechanism 3
N-step lookahead provides predictive advantage in deep settings by bootstrapping from multi-step returns. 2-step SARSA accumulates rewards over two transitions before bootstrapping, reducing bias from single-step estimation while maintaining acceptable variance. In Hanabi's sparse-reward structure, this allows the agent to associate actions with delayed outcomes more effectively.

## Foundational Learning

- **Temporal Difference (TD) Learning**: All four algorithms are TD methods that bootstrap value estimates from subsequent states rather than waiting for episode completion. Quick check: Can you explain why TD methods have lower variance than Monte Carlo methods but higher bias?

- **On-policy vs. Off-policy Learning**: The paper leverages Expected SARSA's ability to interpolate between on-policy (SARSA) and off-policy (Q-learning) behavior based on ε annealing. Quick check: What is the difference between the behavior policy and target policy in off-policy learning?

- **Function Approximation in RL**: The core comparison between tabular and deep methods hinges on whether neural networks can effectively approximate Q-tables in a partially observable, multi-agent environment. Quick check: Why does function approximation introduce generalization but also potential instability compared to tabular storage?

## Architecture Onboarding

- **Component map**: Hanabi game engine (adapted from Kantack et al. IAI project) → Reward calculator (20 actions × 12 reasons) → Agent selection → State encoder → Q-network (1-4 hidden layers, ReLU, Softmax output) → Action selector (ε-greedy) → Experience buffer → Loss computation (MSE) → Adam optimizer

- **Critical path**: Verify reward matrix dimensions (20×12) and weighting scheme before training → Initialize Q-network with 4 hidden layers, learning rate 0.01 → Run ablation if adapting to different game variants or state representations

- **Design tradeoffs**: Tabular: Faster training (8-12x), interpretable Q-tables, but limited generalization and state-space scalability; Deep: Higher scores (80% improvement, p=0.0038), better generalization, but 8-12x longer training without GPU, 3-4x with GPU; Network depth: 1-2 layers insufficient for policy complexity; 4 layers optimal but increases compute

- **Failure signatures**: Scores plateau below 10: Check reward signal propagation, learning rate may be too high; Excessive discards, few hints: Agent may be risk-averse; adjust exploration or reward shaping; Training instability with deep agents: Reduce learning rate, check gradient clipping, verify state normalization

- **First 3 experiments**: Replicate tabular Expected SARSA vs. Q-learning baseline (1,000 games) to validate environment setup against reported scores; Ablate network depth (1, 2, 3, 4 hidden layers) with Q-learning agent to reproduce learning rate × depth interaction from Figure 1; Cross-play deep Q-learning vs. deep 2-step SARSA to verify the paper's finding that these two deep methods achieve highest scores

## Open Questions the Paper Calls Out

- **Is the 80% performance improvement of deep reinforcement learning agents over tabular methods justified by the 8-12x increase in computational training time?**: The discussion section explicitly states this efficiency question, noting that wall time is a proxy for complexity and no formal cost-benefit analysis was performed. Evidence needed: A comprehensive efficiency analysis (e.g., score vs. FLOPs) or comparison of performance gains per unit of compute time.

- **How do tabular agents perform when playing directly against deep reinforcement learning agents?**: The authors state in Future Work that additional experiments are needed for cross-type matchups. Evidence needed: Running the established experimental protocol using heterogeneous pairs (e.g., a Tabular Expected SARSA agent playing against a Deep Q-learning agent).

- **Can inverse reinforcement learning (IRL) or imitation learning agents effectively generalize to the diverse strategies and idiosyncrasies of human players?**: The authors identify a need to study IRL and imitation learning effects, hypothesizing that such agents will need to generalize to different human strategies. Evidence needed: Training agents via imitation learning on human gameplay datasets and evaluating their performance when partnered with human players of varying skill levels.

## Limitations

- The study is limited to two-player Hanabi, which may not generalize to larger player counts or different cooperative game structures
- Ablation study focused on learning rate and network depth but did not explore alternative state representations or reward shaping schemes
- The 1,000-game evaluation provides statistical power but may not capture long-term policy convergence dynamics

## Confidence

- **High Confidence**: Expected SARSA achieves highest scores among tabular methods (supported by p=0.0038 statistical significance, consistent evaluation protocol)
- **Medium Confidence**: Deep learning agents outperform tabular versions by 80% (statistically significant but limited to this specific game implementation)
- **Medium Confidence**: 2-step SARSA and deep Q-learning perform best among deep variants (based on ablation results but not extensively cross-validated)

## Next Checks

1. **Cross-game validation**: Test the same algorithm comparison framework on other cooperative partially observable games (e.g., Bridge, Pandemic) to assess generalizability beyond Hanabi
2. **State representation ablation**: Systematically vary state encoding methods (card encoding, hint history, common knowledge tracking) to quantify impact on tabular vs deep performance gaps
3. **Partner policy stability**: Evaluate Expected SARSA performance against slowly vs rapidly changing partner policies to test the mechanism hypothesis about adaptive learning dynamics