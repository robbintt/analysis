---
ver: rpa2
title: 'One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression'
arxiv_id: '2508.13836'
source_url: https://arxiv.org/abs/2508.13836
tags:
- pruning
- iterative
- one-shot
- network
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study provides the first systematic and comprehensive comparison
  of one-shot and iterative pruning regimes for model compression. It finds that one-shot
  pruning is more effective at lower pruning ratios (< 80%) while iterative pruning
  excels at higher ratios and with transformer architectures.
---

# One Shot vs. Iterative: Rethinking Pruning Strategies for Model Compression

## Quick Facts
- arXiv ID: 2508.13836
- Source URL: https://arxiv.org/abs/2508.13836
- Reference count: 40
- This study provides the first systematic and comprehensive comparison of one-shot and iterative pruning regimes for model compression.

## Executive Summary
This study systematically compares one-shot and iterative pruning strategies for neural network compression, introducing a geometric scheduler and hybrid "few-shot" approach. The authors find that one-shot pruning outperforms iterative methods at lower pruning ratios (<80%) while iterative geometric scheduling excels at higher ratios, particularly for transformer architectures. Experiments across CNNs and transformers on vision and NLP tasks demonstrate that one-shot pruning reduces retraining time while iterative pruning is preferable for second-derivative pruning criteria. The results provide clear guidelines for selecting pruning strategies based on specific performance goals and computational constraints.

## Method Summary
The study evaluates three main pruning regimes: one-shot (single prune + retrain), iterative constant (fixed % of initial weights per step), and iterative geometric (fixed % of remaining weights per step). A hybrid "few-shot" approach combines large initial pruning (60-80% of target) with geometric refinement. Models are fine-tuned using SGD with learning rate set to 1/10 of original, momentum=0.9, weight decay=0.0005, and batch size=512. Patience-based early stopping is applied. Experiments cover CIFAR-10, CIFAR-100, ImageNet1K for vision tasks and TinyStories dataset for NLP, using ResNet-18, EfficientNet V2-S, ViT-S/16, and TinyStories-33M models. Pruning criteria include magnitude, Hessian-based, and Taylor expansion methods.

## Key Results
- One-shot pruning consistently outperforms iterative pruning at pruning rates below 80%, particularly for CNNs
- Iterative geometric scheduling outperforms constant scheduling at high sparsity ratios (>88%)
- Hybrid pruning approach achieves best performance across nearly all pruning rates by combining strengths of both regimes
- Transformer architectures show significantly greater sensitivity to one-shot pruning compared to CNNs, degrading even at 10-20% sparsity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** One-shot pruning can outperform iterative pruning at moderate sparsity (<80%) when paired with adaptive retraining duration.
- **Mechanism:** Single-cycle weight removal minimizes disruption to learned representations. The network undergoes one perturbation event followed by extended fine-tuning, avoiding cumulative drift from repeated small perturbations that may not allow sufficient recovery between steps.
- **Core assumption:** Importance rankings computed from the trained network remain valid for identifying redundant parameters at moderate sparsity levels.
- **Evidence anchors:**
  - [abstract] "one-shot pruning is more effective at lower pruning ratios (< 80%) while iterative pruning excels at higher ratios and with transformer architectures"
  - [section 5.2] "one-shot pruning consistently outperforms iterative pruning, particularly at pruning rates below 80%"
  - [corpus] Related work on one-shot pruning (PGB, Pruning as Regularization) supports efficiency but notes steep decline at extreme sparsity
- **Break condition:** Performance degrades sharply beyond ~80% sparsity or when applied to transformer architectures sensitive to abrupt structural changes.

### Mechanism 2
- **Claim:** Iterative geometric scheduling (fixed percentage of remaining weights per step) outperforms constant scheduling and is preferred at high sparsity ratios.
- **Mechanism:** Pruning progressively smaller fractions allows the network to recalibrate importance rankings after each perturbation. This prevents error propagation from early pruning decisions based on stale sensitivity estimates.
- **Core assumption:** Weight importance is context-dependent and should be recalculated as network structure evolves.
- **Evidence anchors:**
  - [section 3] "Geometric: A fixed percentage p of the remaining weights is pruned at each step... as the pruning process progresses, fewer weights are pruned at each iteration"
  - [figure 2 caption] "'Geometric' outperforms 'Constant' in most high-sparsity scenarios"
  - [appendix B] "iterative pruning enables recalculating the Hessian at each step, ensuring a more precise sensitivity evaluation of the weights retained"
  - [corpus] "Beyond One-Way Pruning" observes both methods decline at extreme sparsity, suggesting geometric delays but may not eliminate degradation
- **Break condition:** Computational budget constraints may make many-step iterative approaches impractical at extreme sparsity (>95%).

### Mechanism 3
- **Claim:** A hybrid few-shot approach (large initial prune + geometric refinement) can outperform pure regimes across pruning ratios.
- **Mechanism:** Exploits one-shot efficiency for removing clearly redundant parameters while preserving iterative precision for fine-grained refinement. Reduces unnecessary early cycles while maintaining accuracy at high sparsity.
- **Core assumption:** A significant portion of parameters are trivially redundant (identifiable in one pass); remaining parameters require careful iterative refinement.
- **Evidence anchors:**
  - [abstract] "proposes a hybrid 'few-shot' approach combining strengths of both regimes"
  - [section 5.5] "Hybrid pruning leverages the strengths of both one-shot and iterative approaches: it removes the majority of weights in the initial iteration, reducing redundant cycles"
  - [figure 6] Shows hybrid approach achieving best performance across nearly all pruning rates
  - [corpus] Insufficient corpus evidence for hybrid approaches specifically
- **Break condition:** Requires careful tuning of initial ratio (pk) and step size (pi); suboptimal choices underperform simpler regimes.

## Foundational Learning

- **Concept: Weight importance criteria (magnitude, Hessian, Taylor)**
  - **Why needed here:** Criterion choice interacts with regime selection—expensive criteria (Hessian) computed once favor one-shot; criteria benefiting from recalculation favor iterative.
  - **Quick check question:** Why would Hessian-based pruning prefer iterative over one-shot at high sparsity?

- **Concept: Retraining dynamics and early stopping**
  - **Why needed here:** Retraining duration significantly impacts outcomes. Patience-based stopping adapts to actual recovery rather than arbitrary epoch counts.
  - **Quick check question:** How does patience-based retraining prevent over-training compared to fixed epochs?

- **Concept: Structured vs. unstructured pruning**
  - **Why needed here:** Structured pruning (channels/filters) creates regular sparsity but is harder at high ratios; unstructured allows fine-grained removal but requires specialized hardware.
  - **Quick check question:** Why might structured pruning require different scheduling than unstructured at the same target sparsity?

## Architecture Onboarding

- **Component map:** Initial training → Criterion computation → Regime scheduler (one-shot/iterative-constant/iterative-geometric/hybrid) → Pruning execution → Patience-based fine-tuning → Evaluation

- **Critical path:**
  1. Select criterion (magnitude: fast/simple; Hessian: precise but expensive)
  2. Determine target sparsity p
  3. Choose regime: p < 80% + CNN → one-shot; p > 80% or transformer → iterative geometric; mixed → hybrid (pk=60-80% of target, then geometric)
  4. Configure patience (longer for one-shot, shorter per-step for iterative)

- **Design tradeoffs:**
  - Computation vs. accuracy: Iterative requires more criterion computations but may achieve better high-sparsity accuracy
  - Simplicity vs. flexibility: One-shot easier to implement; hybrid requires tuning two parameters
  - Architecture sensitivity: Transformers degrade earlier with one-shot; CNNs more tolerant

- **Failure signatures:**
  - Accuracy cliff at high sparsity with one-shot on transformers → switch to iterative geometric
  - Slow/no convergence with constant scheduling → use geometric
  - Over-training (plateau then degradation) → reduce patience or add LR decay
  - Layer collapse in structured pruning → enforce minimum channels per layer

- **First 3 experiments:**
  1. Reproduce regime comparison: Magnitude-based one-shot vs. iterative-geometric on ResNet-18/CIFAR-10 at 70%, 80%, 90% sparsity with patience=50
  2. Scheduler ablation: Iterative-constant vs. iterative-geometric at 88% sparsity with fixed total retraining budget
  3. Hybrid validation: Hybrid (pk=0.6, pi=0.05) for 88% target on ViT/CIFAR-100 to test transformer advantage

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the optimal split between the initial one-shot pruning ratio ($p_k$) and the subsequent iterative pruning ratio ($p_i$) be theoretically determined or automated for the hybrid "few-shot" regime?
- **Basis:** [inferred] The paper introduces a hybrid approach in Section 5.5 but relies on empirical heuristics (e.g., "60–80% of the target pruning rate") and manual benchmarking to configure the scheduler, rather than providing a theoretical derivation for the optimal transition point.
- **Why unresolved:** The authors provide general guidelines for the hybrid parameters but acknowledge that finding the exact balance requires testing multiple configurations (Tables 3 and 4), suggesting a lack of a unified rule for different architectures.
- **What evidence would resolve it:** An algorithmic formulation that dynamically sets $p_k$ and $p_i$ based on network architecture and target sparsity, demonstrating consistent superiority over heuristic settings without requiring manual search.

### Open Question 2
- **Question:** What underlying mechanisms cause Large Language Models (LLMs) to be significantly more sensitive to one-shot pruning at low ratios compared to Vision models?
- **Basis:** [inferred] Section 5.2.1 notes that unlike vision tasks, NLP models show performance degradation even with only 10–20% of parameters removed via one-shot pruning, a distinct behavioral divergence the paper observes but does not fully explain.
- **Why unresolved:** The paper identifies the symptom (sensitivity) and the benefit of iterative pruning for perplexity, but it does not isolate whether this is due to weight interdependence, layer structure, or specific data modality characteristics.
- **What evidence would resolve it:** A layer-wise analysis comparing weight sensitivity and loss landscape geometry in LLMs versus CNNs during the initial pruning steps to identify structural causes for the fragility of one-shot pruning in NLP.

### Open Question 3
- **Question:** How does the choice of pruning regime interact with emerging or complex pruning criteria (e.g., regularization-based or learnable pruning) beyond the standard magnitude and second-derivative methods tested?
- **Basis:** [explicit] The Conclusion explicitly states: "Future research should further investigate the impact of pruning strategies under different pruning criteria... and refining techniques for more effective pruning regimes."
- **Why unresolved:** The study limited its scope primarily to magnitude, Taylor expansion, and Hessian-based criteria, leaving the interaction between iterative/geometric regimes and other diverse pruning methodologies unexplored.
- **What evidence would resolve it:** A follow-up study applying the paper's patience-based and geometric scheduling protocols to criteria like SNIP, GraSP, or learned importance scores to verify if the superior performance of geometric scheduling holds across all criteria types.

## Limitations
- Findings primarily validated on vision and small NLP tasks; generalizability to larger language models and other domains remains untested
- Hybrid approach requires careful tuning of two parameters (initial prune ratio and step size), complicating practical deployment
- Study focuses on magnitude-based pruning as primary criterion, with limited exploration of how findings extend to more sophisticated criteria like Hessian or Taylor-based methods

## Confidence
- Core claims about one-shot vs. iterative regime performance at different sparsity levels: Medium
- Geometric scheduling superiority at high sparsity: Medium
- Hybrid approach effectiveness: Medium
- Transformer architecture sensitivity to one-shot pruning: Medium

## Next Checks
1. Test the one-shot vs. iterative boundary at 80% sparsity across additional architectures (e.g., ConvNeXt, Swin Transformer) to verify the claimed transition point.
2. Evaluate Hessian-based pruning under both one-shot and iterative regimes to determine if computational cost justifies iterative recalculation at high sparsity.
3. Conduct ablation studies on hybrid pruning parameters (pk, pi) to establish guidelines for parameter selection and identify when the hybrid approach fails to outperform pure regimes.