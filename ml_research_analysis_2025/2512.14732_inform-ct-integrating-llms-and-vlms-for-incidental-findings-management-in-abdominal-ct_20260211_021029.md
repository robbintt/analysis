---
ver: rpa2
title: 'INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in
  Abdominal CT'
arxiv_id: '2512.14732'
source_url: https://arxiv.org/abs/2512.14732
tags:
- findings
- incidental
- abdominal
- guidelines
- scans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents INFORM-CT, a framework that integrates large
  language models (LLMs) and vision-language models (VLMs) for automated detection
  and classification of incidental findings in abdominal CT scans. The method uses
  a planner-executor architecture where the planner (a ReAct agent) generates Python
  scripts from parsed medical guidelines, and the executor runs these scripts using
  base functions such as organ segmentation, tumor detection, and attribute labeling.
---

# INFORM-CT: INtegrating LLMs and VLMs FOR Incidental Findings Management in Abdominal CT

## Quick Facts
- arXiv ID: 2512.14732
- Source URL: https://arxiv.org/abs/2512.14732
- Reference count: 24
- Multi-organ incidental findings detection: INFORM-CT integrates LLMs and VLMs for automated detection and classification of incidental findings in abdominal CT scans, achieving significantly higher accuracy and F1 scores compared to pure VLM baselines.

## Executive Summary
This paper introduces INFORM-CT, a framework that combines large language models (LLMs) and vision-language models (VLMs) to automate the detection and classification of incidental findings in abdominal CT scans. The system uses a planner-executor architecture where the planner (a ReAct agent) generates Python scripts from parsed medical guidelines, and the executor runs these scripts using base functions such as organ segmentation, tumor detection, and attribute labeling. Evaluated on a multi-organ benchmark using ACR guidelines, INFORM-CT demonstrates significantly higher accuracy (63.09% for liver) and weighted F1 scores compared to a pure VLM baseline (12.5% for liver), showing its effectiveness in producing guideline-adherent recommendations.

## Method Summary
INFORM-CT uses a planner-executor agent framework to detect and classify incidental findings in abdominal CT scans. The planner (Claude 3.5 ReAct agent) generates Python scripts from parsed ACR guideline decision trees (converted to JSON by GPT-4o), orchestrating base functions including organ/tumor segmentation (TotalSegmentor/nnUNet), attribute labeling (MERLIN VLM), and measurement routines (diameter, Hounsfield Units, border thickness). The executor runs the generated scripts, performing multi-turn iterative refinement based on execution feedback. The system was evaluated on liver (168 scans, venous phase), pancreas (188 scans, venous phase), and kidney (98 scans, arterial phase) from an internal CT dataset, with ground truth extracted from radiology reports via GPT-4o.

## Key Results
- INFORM-CT achieves 63.09% accuracy for liver recommendations, significantly outperforming the pure VLM baseline MERLIN at 12.5%
- Weighted F1 scores are substantially higher for INFORM-CT across all three organs compared to the baseline
- Ablation study confirms segmentation components are critical, with accuracy dropping to 20.45% when segmentation is replaced by VLM labeling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured code generation from parsed clinical guidelines enables guideline-adherent reasoning that pure VLMs cannot match.
- Mechanism: The planner converts parsed decision trees into executable Python scripts that orchestrate multiple base functions in sequences matching clinical logic paths. This explicit program structure forces adherence to guideline conditionals that VLMs struggle to apply consistently.
- Core assumption: The LLM can accurately translate guideline logic into syntactically and semantically correct code; parsing from PDF to JSON preserves decision-tree structure.
- Evidence anchors:
  - [abstract]: "The planner, based on LLM, generates Python scripts using predefined base functions, while the executor runs these scripts to perform the necessary checks and detections"
  - [section 2.2]: "The challenge lies in the complex structure of the decision trees from Section 2.1 and the variety of visual subroutines involved"
- Break condition: If guideline PDFs contain ambiguous logic that cannot be deterministically parsed, or if the LLM fails to generate executable code for novel guideline structures, the mechanism degrades.

### Mechanism 2
- Claim: Segmentation-based spatial grounding is the primary driver of performance gains over pure VLM approaches.
- Mechanism: Explicit organ and tumor segmentation models provide binary masks enabling precise measurements (diameter, HU intensity, border thickness). These quantitative features feed into conditional logic that VLMs cannot reliably extract from raw voxels.
- Core assumption: Segmentation models generalize adequately to the target population; lesions are detectable in the selected contrast phases.
- Evidence anchors:
  - [section 3.2.3 / Table 2]: Ablation shows accuracy drops from 63.09% to 20.45% when segmentation is replaced by VLM labeling
  - [section 2.2.1]: "These include multiple different segmentation models that cover a wide range of tasks, including organ and tumor segmentation in the abdomen"
- Break condition: If segmentation models fail on atypical anatomy, small lesions, or out-of-distribution contrast protocols, downstream measurements and recommendations become unreliable.

### Mechanism 3
- Claim: Multi-turn iterative refinement with execution feedback improves code correctness beyond single-pass generation.
- Mechanism: The planner executes generated code, captures errors, and feeds feedback to the LLM for regeneration. A STOP criterion evaluates syntactic and semantic validity, enabling self-correction.
- Core assumption: Execution errors provide meaningful signal for correction; the feedback loop converges within practical iteration limits.
- Evidence anchors:
  - [section 2.2.2]: "The generation process works in an interactive manner... If the STOP criterion is not met, another call to the LLM is made to regenerate the code based on the feedback"
  - [abstract]: Framework is "fully automatic end-to-end"
- Break condition: If errors are ambiguous or if the LLM fails to map feedback to corrections, iteration may not converge or may introduce new errors.

## Foundational Learning

- Concept: **ReAct agent architecture**
  - Why needed here: The planner uses reasoning-acting loops to generate code; understanding how thought-action-observation cycles work is prerequisite to debugging planner behavior.
  - Quick check question: Can you trace how a ReAct agent would decompose "measure liver lesion diameter and compare to 1cm threshold" into reasoning steps and function calls?

- Concept: **Medical imaging phases (venous vs arterial)**
  - Why needed here: The paper restricts liver/pancreas to venous phase and kidney to arterial phase for lesion detectability; incorrect phase selection invalidates segmentation.
  - Quick check question: Why would a lesion be more detectable in venous phase for liver but arterial phase for kidney?

- Concept: **Hounsfield Units (HU) and CT intensity measurement**
  - Why needed here: Base functions measure gray-level intensity in HU; understanding HU ranges for different tissues is necessary to validate labeler outputs and intensity-based conditionals.
  - Quick check question: What HU range would you expect for a fluid-filled cyst versus a solid tumor?

## Architecture Onboarding

- Component map:
  - Guideline Parser (GPT-4o + LangChain) -> JSON decision tree
  - Planner (Claude 3.5 ReAct agent) -> JSON + base function API -> Python script
  - Executor -> Runs generated script, invokes base functions
  - Base Functions -> Segmentation (nnUNet/TotalSegmentor), measurement (diameter, HU, border), labeling (MERLIN VLM)
  - Evaluation -> Matches predicted path against report-extracted ground truth

- Critical path:
  1. PDF guideline parsing accuracy -> 2. Code generation correctness -> 3. Segmentation quality -> 4. Measurement precision -> 5. Final recommendation accuracy
  - Breaks at step 3 dominate failures (ablation confirms segmentation criticality).

- Design tradeoffs:
  - Generalizability vs. specificity: Using general abdominal segmentation models vs. organ-specific fine-tuned models
  - Interpretability vs. end-to-end learning: Explicit code paths are interpretable but require more engineering than black-box VLMs
  - Phase restriction vs. coverage: Limiting to specific contrast phases improves segmentation but reduces applicable scans

- Failure signatures:
  - Low accuracy on small lesions (<1cm): Segmentation models may miss or poorly delineate
  - Incorrect recommendations for edge-case attribute combinations: VLM labeler may misclassify fine-grained features
  - Code execution errors on novel guideline structures: Planner may generate invalid function calls

- First 3 experiments:
  1. **Validate segmentation pipeline independently**: Run organ and tumor segmentation on held-out scans, compute Dice scores against manual annotations; establish upper bound on downstream performance.
  2. **Ablate individual base functions**: Systematically replace each (segmentation, measurement, labeling) with VLM equivalents to quantify contribution; confirm segmentation dominance.
  3. **Test parser-planner on a new guideline PDF**: Parse a kidney or pancreatic guideline not used in development, generate code, and evaluate on a small scan set; assess generalization to unseen decision trees.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating a vision-language model (VLM) specifically trained for local, fine-grained feature detection improve the accuracy of attribute labeling and final recommendations compared to the current global approach?
- Basis in paper: [explicit] The Discussion section states, "we expect that a VLM capable of better labeling fine details against local scan regions will further improve recommendation performance."
- Why unresolved: The current implementation uses MERLIN, which may lack the resolution or specific training to distinguish subtle clinical attributes necessary for complex guideline adherence.
- What evidence would resolve it: A comparative study replacing the current labeler with a region-aware VLM, showing statistically significant improvement in attribute classification metrics.

### Open Question 2
- Question: Does the system's performance hold when evaluated against manual radiologist annotations rather than LLM-extracted "ground truth" labels?
- Basis in paper: [inferred] Section 3.1.1 describes extracting "correct" recommendations using GPT-4o to match reports to decision trees, introducing a potential source of label noise or circularity.
- Why unresolved: Using an LLM to generate ground truth may mask errors or hallucinations that align with the LLM planner's logic but differ from actual clinical validity.
- What evidence would resolve it: Re-evaluating the benchmark using a dataset where the "correct" decision paths have been manually validated by clinical experts.

### Open Question 3
- Question: How does the system's performance degrade when applied to organs lacking high-quality public segmentation models?
- Basis in paper: [explicit] Section 3.2.1 notes, "We were limited in this implementation by the variety of available strong segmentation models for abdominal organ lesions," and the ablation study shows a drop from 63% to 20% without segmentation.
- Why unresolved: It is unclear if the framework is viable for organs where segmentation models are less mature, or if the system can rely more heavily on VLMs in the absence of segmentation masks.
- What evidence would resolve it: Extending the benchmark to organs beyond the liver, pancreas, and kidney (which have established nnUNet models) and reporting the change in accuracy.

## Limitations
- Internal CT dataset and corresponding radiology reports are not publicly available, preventing direct replication of the reported performance numbers
- The efficacy of the guideline parser and code-generation planner is untested on decision trees beyond the three ACR guidelines examined
- Segmentation model generalization to atypical anatomy or out-of-distribution contrast phases remains a critical vulnerability

## Confidence
- **High Confidence**: Segmentation-based grounding is the primary performance driver (confirmed by ablation study)
- **Medium Confidence**: LLM-to-code translation reliably produces executable scripts matching guideline logic (supported by end-to-end results but not independently validated)
- **Low Confidence**: Generalizability to unseen guidelines and broader clinical scenarios (no direct evidence beyond the three evaluated guidelines)

## Next Checks
1. **Independent Segmentation Validation**: Evaluate organ and lesion segmentation Dice scores on an external annotated dataset; establish baseline for downstream accuracy
2. **Planner-Generalization Test**: Parse a new ACR guideline (e.g., for thyroid nodules) not used in development; generate code and assess recommendation accuracy on a small scan set
3. **VLM-Attribute Error Analysis**: Systematically misclassify fine-grained attributes (e.g., HU, border thickness) using MERLIN on held-out regions; measure impact on final recommendation accuracy