---
ver: rpa2
title: Sparse Bayesian Message Passing under Structural Uncertainty
arxiv_id: '2601.01207'
source_url: https://arxiv.org/abs/2601.01207
tags:
- signed
- structural
- graph
- spam
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes SpaM, a sparse Bayesian message passing network
  for semi-supervised node classification under structural uncertainty. SpaM explicitly
  models a posterior distribution over signed adjacency matrices, allowing each edge
  to be positive, negative, or absent, which naturally captures both edge noise and
  heterophily.
---

# Sparse Bayesian Message Passing under Structural Uncertainty

## Quick Facts
- arXiv ID: 2601.01207
- Source URL: https://arxiv.org/abs/2601.01207
- Authors: Yoonhyuk Choi, Jiho Choi, Chanran Kim, Yumin Lee, Hawon Shin, Yeowon Jeon, Minjeong Kim, Jiwoo Kang
- Reference count: 36
- Primary result: SpaM achieves state-of-the-art performance on node classification under structural uncertainty and heterophily

## Executive Summary
This paper introduces SpaM, a sparse Bayesian message passing network designed to handle semi-supervised node classification under structural uncertainty. The key innovation is modeling a posterior distribution over signed adjacency matrices, allowing edges to be positive, negative, or absent. This approach naturally captures both edge noise and heterophily, which are common challenges in real-world graph data. SpaM's sparse signed message passing layer uses local sparse coding to select informative neighbors while aggregating positive and negative relations through separate channels, providing both accuracy and robustness.

## Method Summary
SpaM explicitly models uncertainty in graph structure by maintaining a posterior distribution over signed adjacency matrices. Each edge can be positive, negative, or absent, capturing both noise and heterophily. The sparse signed message passing layer performs local sparse coding to select informative neighbors, then aggregates messages through separate positive and negative channels. Theoretical analysis shows SpaM approximates an ideal Bayesian predictor under structural uncertainty, with the sparse signed layer acting as a robust MAP estimator. This design allows SpaM to maintain accuracy even when graph structure is corrupted or relationships are heterophilic.

## Key Results
- SpaM consistently outperforms strong baseline models on nine heterophilic benchmarks and three large-scale graphs
- The model shows particular strength under structural noise and low homophily conditions
- Ablation studies confirm the contributions of structural posterior modeling, sparse coding, and signed aggregation to improved accuracy and robustness

## Why This Works (Mechanism)
SpaM's effectiveness stems from its principled Bayesian treatment of structural uncertainty. By maintaining a posterior over signed adjacency matrices, it captures the uncertainty inherent in real-world graphs where edges may be noisy or relationships may be heterophilic. The sparse signed message passing layer's local sparse coding mechanism selectively aggregates information from the most relevant neighbors, while separate positive and negative channels preserve the directionality of relationships. This allows SpaM to extract meaningful signals even when traditional message passing would fail due to noise or heterophily.

## Foundational Learning
- **Bayesian inference under uncertainty**: Needed to model the posterior over adjacency matrices; quick check: understanding how priors and likelihoods combine in the graph context
- **Heterophily vs homophily**: Essential for recognizing when traditional GNNs fail; quick check: identifying scenarios where neighbors have different labels
- **Sparse coding in message passing**: Critical for neighbor selection; quick check: understanding how sparsity improves robustness to noise
- **Signed graph neural networks**: Important for handling both positive and negative relationships; quick check: recognizing how separate channels preserve relationship direction
- **Structural uncertainty in graphs**: Fundamental to the problem SpaM addresses; quick check: identifying scenarios where edge existence or sign is uncertain
- **Approximate Bayesian computation**: Relevant for the theoretical analysis; quick check: understanding MAP estimation in the graph context

## Architecture Onboarding

**Component map:** Input features → Structural posterior module → Sparse signed message passing layer → Separate positive/negative aggregation → Output layer

**Critical path:** The sparse signed message passing layer is the core innovation, combining sparse coding for neighbor selection with signed aggregation to handle both homophilic and heterophilic relationships.

**Design tradeoffs:** The model trades computational complexity (maintaining a posterior distribution) for robustness to structural uncertainty. The sparse coding adds overhead but improves noise resilience. Separate positive/negative channels increase expressiveness but require careful normalization.

**Failure signatures:** The model may struggle with extremely sparse graphs where few neighbors are available for sparse coding, or with highly dynamic graphs where the posterior distribution cannot adapt quickly enough to structural changes.

**First experiments:**
1. Verify the structural posterior module correctly captures edge uncertainty on synthetic graphs with known noise patterns
2. Test the sparse coding mechanism's ability to select informative neighbors under varying levels of noise
3. Validate the signed aggregation's handling of heterophilic relationships on controlled synthetic datasets

## Open Questions the Paper Calls Out
The paper acknowledges that its theoretical guarantees rely on approximations that may not hold in all real-world scenarios. The computational complexity of maintaining a full posterior over adjacency matrices, even with sparse approximations, could become prohibitive for very large-scale graphs. Additionally, while the experiments focus on node classification, questions remain about SpaM's performance on other graph learning tasks like link prediction or graph-level classification.

## Limitations
- Theoretical guarantees rely on approximations that may not hold in all scenarios
- Computational complexity could become prohibitive for very large-scale graphs
- Experiments focus primarily on node classification, leaving open questions about other graph learning tasks
- The model's performance under extreme structural uncertainty has not been fully characterized

## Confidence
- **High Confidence**: The core contribution of modeling structural uncertainty through signed posteriors is well-justified and technically sound
- **Medium Confidence**: Empirical results showing consistent improvements across benchmarks, though the extent of superiority varies by dataset
- **Medium Confidence**: Theoretical analysis connecting SpaM to Bayesian predictors, with some approximations acknowledged but not fully quantified

## Next Checks
1. **Scalability Test**: Evaluate SpaM's performance and runtime on graphs with millions of nodes to assess practical scalability limits
2. **Ablation under Extreme Noise**: Systematically vary the level of edge corruption to determine the breaking point of SpaM's robustness guarantees
3. **Cross-Task Evaluation**: Test SpaM on non-node-classification tasks (e.g., link prediction, graph classification) to assess generalizability beyond the current experimental scope