---
ver: rpa2
title: 'EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage
  Maximization'
arxiv_id: '2508.09662'
source_url: https://arxiv.org/abs/2508.09662
tags:
- evaluation
- data
- subset
- effieval
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces EffiEval, a training-free approach for efficient
  model evaluation that addresses computational challenges posed by large language
  model (LLM) benchmarks. The method adaptively selects representative subsets of
  evaluation data by maximizing capability coverage using the Model Utility Index
  (MUI), which measures neuron activation patterns.
---

# EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization

## Quick Facts
- arXiv ID: 2508.09662
- Source URL: https://arxiv.org/abs/2508.09662
- Authors: Yaoning Wang; Jiahao Ying; Yixin Cao; Yubo Ma; Yugang Jiang
- Reference count: 22
- Primary result: Training-free method achieving Kendall's τ > 0.9 using only 5% of original data, outperforming baselines

## Executive Summary
EffiEval introduces a training-free approach for efficient LLM evaluation by adaptively selecting representative subsets of evaluation data. The method maximizes capability coverage using the Model Utility Index (MUI), which measures neuron activation patterns without requiring model performance data during selection. Unlike prior methods that may introduce bias by selecting "hard" or discriminative samples, EffiEval remains performance-agnostic while achieving strong generalization across datasets and model families. Experiments on four benchmarks with 17 diverse models demonstrate that EffiEval consistently outperforms random selection, K-Means clustering, and state-of-the-art baselines in ranking consistency while maintaining high coverage of model capabilities.

## Method Summary
EffiEval is a training-free evaluation method that selects representative subsets by maximizing coverage of activated model neurons, measured via the Model Utility Index (MUI). The approach computes neuron activation patterns for each sample using an indicator model, then applies a greedy algorithm to select samples that maximize the union of covered neurons. This creates a subset that exercises the broadest capability space of the model while remaining performance-agnostic during selection. The method allows users to balance efficiency and representativeness based on coverage thresholds, achieving Kendall's τ > 0.9 with only 5% of original data.

## Key Results
- Achieves Kendall's τ > 0.9 using only 5% of original data, and > 0.95 with 10%
- Consistently outperforms random selection, K-Means clustering, and state-of-the-art baselines
- Maintains high coverage of model capabilities across diverse model families
- Generalizes across datasets (GSM8K, ARC, Hellaswag, MMLU) with 17 evaluation models

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Selecting evaluation samples that maximize the union of activated neurons in a model effectively proxies the coverage of that model's diverse capabilities.
- **Mechanism:** Different capabilities activate distinct sets of neurons. By formulating selection as a Maximum Coverage Problem on these activated neuron sets, EffiEval identifies a subset that exercises the broadest "capability space" of the model.
- **Core assumption:** Capabilities in LLMs are functionally localized or correlated with specific neuron activation patterns.
- **Break condition:** If model capabilities are distributed diffusely across all parameters without distinct activation signatures.

### Mechanism 2
- **Claim:** The selection process remains unbiased regarding model performance because the "effort" measured by MUI is statistically independent of whether the answer is correct.
- **Mechanism:** EffiEval avoids bias by relying on activation magnitude rather than correctness, validated using a Mann-Whitney U test.
- **Core assumption:** Neuron activation density reflects "effort" or "capability engagement" rather than just "confidence" or "memorization" of the answer.
- **Break condition:** If incorrect answers consistently trigger significantly different neuron patterns than correct ones.

### Mechanism 3
- **Claim:** A subset derived from one model generalizes to evaluate others because models share similar latent distributions of capability diversity.
- **Mechanism:** While MUI is model-specific, the resulting data subset captures broad semantic domains. The paper argues that capability diversity is correlated across model families.
- **Core assumption:** Architectural differences between models do not fundamentally alter the semantic mapping of "diverse" data samples.
- **Break condition:** When evaluating models with radically different architectures where capability encodings do not align.

## Foundational Learning

- **Concept:** Model Utility Index (MUI)
  - **Why needed here:** This replaces human labeling by quantifying "effort" via the norm of activated neuron weights, not output logits.
  - **Quick check question:** If I feed a sample into a model, how do I calculate a scalar "utility" score from the hidden states?

- **Concept:** Maximum Coverage Problem (MCP)
  - **Why needed here:** The selection algorithm aims to maximize the union of covered "elements" (neurons) rather than using random or clustering approaches.
  - **Quick check question:** Why does the greedy algorithm only guarantee a $(1-1/e)$ approximation rather than a perfect maximum?

- **Concept:** Rank Correlation (Kendall's $\tau$)
  - **Why needed here:** Success is measured by preserving the relative ranking of models, not absolute accuracy.
  - **Quick check question:** Why would high Kendall's $\tau$ be more valuable to a model developer than low Mean Absolute Error?

## Architecture Onboarding

- **Component map:** Indicator Model -> MUI Calculator -> Greedy Selector -> Evaluator

- **Critical path:**
  1. Pre-compute activated neuron sets for every sample in the full benchmark using the Indicator Model
  2. Run Greedy Selection to determine the subset
  3. Evaluate target models on the selected subset

- **Design tradeoffs:**
  - Indicator Choice: Smaller models are faster but might miss capabilities only present in larger models
  - Threshold: Layer-wise top-k% balances noise inclusion vs. capability detection
  - Subset Size: Fixed k vs. Adaptive Coverage based on desired coverage threshold

- **Failure signatures:**
  - Low Correlation on Specific Datasets: May indicate insufficient coverage for benchmarks with highly entangled capabilities
  - High MAE with High Correlation: Expected behavior as rankings, not absolute scores, are preserved

- **First 3 experiments:**
  1. Calculate MUI for 10 random samples from GSM8K on a local LLaMA model to verify complex questions yield higher MUI
  2. Run Algorithm 1 on a toy dataset and plot the marginal gain curve to observe diminishing returns
  3. Select a subset using Model A, evaluate Model B on it, and compare rankings against Model B's full-dataset performance

## Open Questions the Paper Calls Out

1. **Redundancy-aware evaluation strategies:** Future work should explore more reasonable and effective evaluation strategies that account for redundancy, diversity, and the actual capabilities being tested, as maximizing neuron coverage may fail to reflect the original data distribution when significant redundancy exists.

2. **Difficulty-aware sample selection:** The method currently optimizes for coverage but lacks a mechanism to filter for sample difficulty, leading to ceiling effects where models cannot be distinguished. Integrating difficulty estimation metrics alongside MUI could ensure the selected subset maintains discriminative power.

3. **Architectural transferability bounds:** While generalizability claims are supported, evidence is limited to transformer-based models of similar scale. It remains unclear if a single indicator model can robustly capture neuron activation patterns necessary to evaluate structurally distinct models without introducing bias.

## Limitations

- **Evaluation context dependence:** The neuron-coverage approach may break down for models with fundamentally different architectures or when benchmark questions have entangled capabilities.
- **Scalability constraints:** The one-time cost of computing MUI for the entire benchmark remains substantial, with no address of computational requirements for extremely large benchmarks.
- **Transferability bounds:** Generalizability works for transformer-based models within similar size ranges but may degrade for models with substantially different parameter counts or training objectives.

## Confidence

**High Confidence (Mechanism 1):** Correlation results (τ > 0.9 with 5% data) are well-supported by experimental data across four diverse benchmarks with clearly specified greedy selection algorithm.

**Medium Confidence (Mechanism 2):** Performance-agnostic selection is supported by statistical tests, but the independence assumption between MUI and correctness may not hold for all model architectures or benchmark characteristics.

**Medium Confidence (Mechanism 3):** Generalizability results are promising (r_K > 0.94 across model families), but evidence is limited to transformer-based models of similar scale, potentially not extending to radically different architectures.

## Next Checks

1. **Architecture Transfer Test:** Evaluate EffiEval-selected subsets on non-transformer architectures (Mamba, RWKV) to test generalizability bounds and identify architecture-specific limitations.

2. **Safety Impact Analysis:** Test whether the method handles safety-related samples consistently, particularly for benchmarks where models may refuse to answer or where neuron activation patterns differ significantly for safety-related content.

3. **Scale Sensitivity Study:** Conduct experiments varying the indicator model size (1B vs 7B vs 70B) to quantify the trade-off between computational efficiency and coverage quality, identifying the minimum viable indicator size for different benchmark complexities.