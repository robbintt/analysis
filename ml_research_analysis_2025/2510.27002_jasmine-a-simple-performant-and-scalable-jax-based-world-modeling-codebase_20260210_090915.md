---
ver: rpa2
title: 'Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase'
arxiv_id: '2510.27002'
source_url: https://arxiv.org/abs/2510.27002
tags:
- jasmine
- world
- training
- modeling
- frames
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Jasmine is a JAX-based world modeling codebase that achieves an
  order-of-magnitude faster training speed compared to prior implementations for the
  CoinRun case study. The codebase provides fully reproducible training with support
  for diverse sharding configurations and includes performance optimizations across
  data loading, training, and checkpointing.
---

# Jasmine: A Simple, Performant and Scalable JAX-based World Modeling Codebase

## Quick Facts
- arXiv ID: 2510.27002
- Source URL: https://arxiv.org/abs/2510.27002
- Authors: Mihir Mahajan; Alfred Nguyen; Franz Srambical; Stefan Bauer
- Reference count: 40
- Key outcome: Order-of-magnitude faster training speed (under 9 hours vs. over 100 hours) for CoinRun world model training

## Executive Summary
Jasmine is a JAX-based codebase that enables training world models for interactive environment generation with significantly improved efficiency. The codebase achieves this through optimized infrastructure including ArrayRecord data format, Grain data loading with prefetching, mixed-precision training with bfloat16, and FlashAttention. A critical architectural modification—prepending latent actions rather than adding them to video embeddings—enables faithful reproduction of the CoinRun environment. Jasmine provides fully reproducible training with support for diverse sharding configurations and includes performance optimizations across data loading, training, and checkpointing.

## Method Summary
Jasmine implements a three-component Genie architecture for world modeling: a VQ-VAE tokenizer, a latent action model (LAM), and a MaskGIT-based dynamics model, all using ST-Transformer backbones. The codebase introduces key modifications including prepending latent actions to video embeddings instead of adding them, and employs efficient infrastructure optimizations such as ArrayRecord format with Grain data loading, mixed-precision training with bfloat16, and FlashAttention via cuDNN SDPA. The training pipeline supports both co-training and sequential training modes, with warmup-stable-decay learning rate schedules and comprehensive sharding configurations through Shardy.

## Key Results
- Achieves under 9 hours of training on a single GPU versus over 100 hours reported in prior work for CoinRun environment
- Enables faithful reproduction of CoinRun environment with improved rollout quality through latent action prepending modification
- Provides order-of-magnitude speedup through infrastructure optimizations including ArrayRecord format, Grain dataloader, and mixed-precision training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimized data loading accelerates training time.
- Mechanism: Using ArrayRecord format and Grain with prefetching reduces I/O bottlenecks, enabling faster data access during training. This allows the accelerators to spend more time computing and less time waiting for data.
- Core assumption: The dataset is preprocessed into the ArrayRecord format with appropriate chunking (100 records per file, 160 frames per record).
- Evidence anchors:
  - [abstract] "...efficient data loading with ArrayRecord format..."
  - [section] "A substantial portion of our speedup compared to Willi et al. (2024) arises from our data loader design... We use Grain for data loading with prefetching enabled and preprocess datasets into ArrayRecords..."
  - [corpus] No direct corpus evidence for ArrayRecord's specific impact; neighbors like Octax focus on JAX environments but not this data format.
- Break condition: Data loading is no longer the bottleneck (e.g., extremely slow model or compute-bound tasks) or I/O system is fundamentally different.

### Mechanism 2
- Claim: Prepended latent actions improve rollout quality for the CoinRun environment.
- Mechanism: Instead of adding latent actions to video embeddings, prepending them likely allows the dynamics model to more distinctly attend to and process the action information before integrating it with the visual context, leading to better temporal consistency during autoregressive generation.
- Core assumption: The user wants to reproduce the CoinRun environment faithfully.
- Evidence anchors:
  - [abstract] "Jasmine's modifications to the original Genie architecture, specifically prepending latent actions instead of adding them to video embeddings, enable faithful reproduction of the CoinRun environment with improved rollout quality."
  - [section] Figure 4 shows improved visual quality with prepending vs. adding. "However, a minimal modification... yields autoregressive generations that faithfully simulate the CoinRun environment."
  - [corpus] No direct corpus evidence for this specific architectural modification.
- Break condition: Applying this change to a different dataset or architecture without validation may not yield improvements or could degrade performance.

### Mechanism 3
- Claim: Parallelism and mixed-precision training improve scalability and throughput.
- Mechanism: Utilizing JAX and XLA allows scaling from single hosts to hundreds of accelerators with minimal code changes via Shardy. Mixed precision (bfloat16) and FlashAttention reduce memory usage and computational cost, increasing throughput.
- Core assumption: The hardware supports bfloat16 and the necessary CUDA/cuDNN versions for FlashAttention.
- Evidence anchors:
  - [abstract] "...highly optimized infrastructure combining scalable training pipelines... mixed-precision training, FlashAttention..."
  - [section] "Jasmine further leverages FlashAttention via cuDNN SDPA... and mixed precision training with bfloat16." Table 2 shows throughput changes with these features ablated.
  - [corpus] Weak signals; neighbor Instella mentions fully open language models but not specific technical details relevant here.
- Break condition: Extremely small models or batch sizes where XLA heuristics favor full precision kernels (as mentioned in Section B), or hardware lacking support for these features.

## Foundational Learning

- **VQ-VAE (Vector Quantized Variational Autoencoder):**
  - Why needed here: Used by the video tokenizer and the Latent Action Model (LAM) to compress visual information and action information into discrete latent codes.
  - Quick check question: Can you explain how the codebook is used in VQ-VAE to discretize continuous latent vectors?

- **ST-Transformer (Spatial-Temporal Transformer):**
  - Why needed here: Used as the backbone for all modules (tokenizer, LAM, dynamics model) to efficiently process video sequences by separating spatial (intra-frame) and temporal (inter-frame) attention.
  - Quick check question: How does an ST-Transformer reduce attention complexity compared to a standard full-attention Transformer for video data?

- **MaskGIT (Masked Generative Image Transformer):**
  - Why needed here: Used by the dynamics model as the training objective, masking input tokens during training to predict missing parts of the video sequence, similar to BERT.
  - Quick check question: What is the key difference between the MaskGIT masking strategy and standard BERT masking?

## Architecture Onboarding

- **Component map:** Raw video/interaction data -> ArrayRecord files -> Grain DataLoader -> Prefetching -> Tokenizer/LAM inputs -> ST-Transformer processing -> Discrete latent codes -> Dynamics Model -> Predicted next frame tokens

- **Critical path:**
  1. Setup JAX/accelerator environment
  2. Preprocess dataset to ArrayRecord format (100 records/file, 160 frames/record)
  3. Initialize Tokenizer, LAM, and Dynamics Model using NNX
  4. Define training step (forward pass, loss, backward pass, update) with JIT compilation
  5. Launch training with desired sharding configuration (Shardy)

- **Design tradeoffs:**
  - Co-training vs. Sequential Training: Co-training LAM and Dynamics saves total time but is more complex to implement and debug; sequential training is simpler but slower
  - FlashAttention: Enabled by default for memory savings but may not always be faster than XLA-compiled kernels at small scales
  - Prepending vs. Adding Actions: Prepended actions improve CoinRun quality but might not be optimal for all domains

- **Failure signatures:**
  - Deteriorating rollouts: May indicate using "add" instead of "prepend" for latent actions or issues with MaskGIT extension to video
  - Low throughput: Check data loader configuration (records/file), mixed precision settings, and batch size
  - High loss variance: Check batched masking logic (use per-sample masking probabilities)

- **First 3 experiments:**
  1. Reproduce CoinRun: Run `jasmine-base` config on the CoinRun dataset to verify installation and baseline performance
  2. Ablate Data Loading: Compare training throughput with default settings vs. using a non-optimized data loader (e.g., lower records/file)
  3. Ablate Action Conditioning: Train dynamics model with "add" vs. "prepend" latent actions and compare rollout metrics (PSNR/SSIM)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do compute and data requirements scale with environment complexity for training agents inside world models?
- Basis in paper: [explicit] "Jasmine provides the foundational infrastructure for future empirical investigation of how compute and data requirements scale with environment complexity for downstream agent training."
- Why unresolved: The paper focuses on infrastructure and CoinRun reproduction; systematic scaling studies across environments of varying complexity are not conducted.
- What evidence would resolve it: Training curves across diverse environments (e.g., CoinRun, Atari, Doom) with varying model sizes and dataset scales, reporting downstream agent performance.

### Open Question 2
- Question: What is the optimal method for extending MaskGIT from images to videos?
- Basis in paper: [explicit] "We hypothesize that this discrepancy between Bruce et al. (2024) and our work stems from an ambiguity in extending MaskGIT to videos."
- Why unresolved: Multiple approaches exist (uniform masking probability across sequence, per-frame probabilities, leaving k frames unmasked), but no systematic comparison is provided.
- What evidence would resolve it: Controlled ablations comparing different video masking strategies on identical architectures and datasets.

### Open Question 3
- Question: Can fully causal architectures match MaskGIT performance with extended training?
- Basis in paper: [explicit] "Our results indicate that the fully causal baseline in particular may benefit from longer training."
- Why unresolved: The causal baseline underperforms at 200k steps but the loss curve suggests potential improvement; experiments are truncated.
- What evidence would resolve it: Training the causal baseline for 400k+ steps with hyperparameters tuned specifically for this architecture.

### Open Question 4
- Question: Would custom Pallas kernels improve mixed-precision throughput at small batch sizes?
- Basis in paper: [inferred] The paper notes XLA operates on heuristics and "posit that writing optimized Pallas kernels for key operations in the model forward pass will result in mixed precision outperforming full precision."
- Why unresolved: No Pallas kernels were implemented; current throughput measurements rely on default XLA compilation.
- What evidence would resolve it: Implementing optimized Pallas kernels for attention and feedforward operations, then benchmarking against current throughput.

## Limitations

- Performance improvements rely heavily on infrastructure optimizations (ArrayRecord, Grain, FlashAttention) that may not transfer directly to different hardware configurations or datasets
- The critical architectural modification of prepending latent actions is shown to work for CoinRun but hasn't been extensively validated across multiple environments
- The WSD learning rate schedule's specific parameters and co-training loss weighting scheme are underspecified, which could affect reproducibility

## Confidence

**High Confidence:** Claims about infrastructure optimizations (ArrayRecord, Grain, FlashAttention, mixed precision) improving throughput are directly supported by Table 2 ablation studies and documented implementation details.

**Medium Confidence:** The architectural claim that prepending latent actions (rather than adding them) is critical for faithful CoinRun reproduction is supported by Figure 4 comparisons, but this hasn't been extensively validated across diverse environments or architectures.

**Low Confidence:** Claims about the generality of these optimizations for other world modeling tasks or datasets are not empirically validated beyond the CoinRun case study.

## Next Checks

1. **Cross-environment validation:** Test the action-prepending architecture modification on at least two additional environments (e.g., Atari games, Procgen) to verify its generality beyond CoinRun.

2. **Infrastructure transferability:** Benchmark Jasmine's performance on different hardware configurations (CPU-only, TPU pods) to assess the portability of the ArrayRecord and Grain optimizations.

3. **Learning rate schedule validation:** Experiment with alternative learning rate schedules (cosine annealing, step decay) to determine whether the WSD schedule is critical for performance or if other schedules achieve comparable results.