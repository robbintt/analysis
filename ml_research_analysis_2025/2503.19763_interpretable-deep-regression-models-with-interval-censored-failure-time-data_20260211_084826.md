---
ver: rpa2
title: Interpretable Deep Regression Models with Interval-Censored Failure Time Data
arxiv_id: '2503.19763'
source_url: https://arxiv.org/abs/2503.19763
tags:
- data
- deep
- linear
- proposed
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a deep learning framework for analyzing interval-censored
  failure time data using partially linear transformation models. The method models
  key covariate effects parametrically while using deep neural networks to flexibly
  capture nonlinear effects of nuisance covariates, addressing the curse of dimensionality.
---

# Interpretable Deep Regression Models with Interval-Censored Failure Time Data

## Quick Facts
- **arXiv ID**: 2503.19763
- **Source URL**: https://arxiv.org/abs/2503.19763
- **Reference count**: 40
- **Primary result**: A deep learning framework for interval-censored failure time data using partially linear transformation models achieves superior estimation and prediction accuracy while maintaining interpretability.

## Executive Summary
This paper proposes a novel deep learning framework for analyzing interval-censored failure time data using partially linear transformation models. The method separates covariates into key parametric components and flexible nonparametric components modeled by deep neural networks, addressing the curse of dimensionality while preserving interpretability. The authors develop an EM algorithm with stochastic gradient descent for tractable parameter estimation and establish theoretical guarantees including asymptotic properties and minimax-optimal convergence rates.

## Method Summary
The method uses a partially linear transformation model where key covariates X have interpretable linear effects (β) while nuisance covariates W are modeled by a deep neural network (ϕ). The baseline hazard function is approximated using monotone splines. An EM algorithm with data augmentation and Poisson latent variables enables tractable likelihood maximization. The M-step uses stochastic gradient descent for DNN updates and closed-form updates for spline coefficients, while the E-step computes expected values of latent variables. The framework handles left, interval, and right censoring within a unified likelihood framework.

## Key Results
- Superior estimation and prediction accuracy compared to state-of-the-art methods in simulation studies
- Novel insights and improved predictive performance when applied to Alzheimer's disease progression data
- Theoretically established asymptotic properties and minimax-optimal convergence rates for the neural network estimator

## Why This Works (Mechanism)

### Mechanism 1
Separating covariates into parametric (X) and nonparametric (W) components preserves statistical interpretability for key variables while absorbing high-dimensional noise. The partially linear structure βᵀX + ϕ(W) provides direct inference through β while the DNN ϕ acts as a nuisance sink to capture complex interactions without polluting interpretable coefficients. This assumes true linearity of key covariate effects.

### Mechanism 2
Sparse DNN architectures mitigate the curse of dimensionality by utilizing composite function structure rather than exponentially many parameters. Enforcing sparsity through L1 penalty and dropout, combined with bounded weights, allows convergence rates dependent on intrinsic dimension rather than ambient dimension. This assumes the true function lies within Hölder smoothness class.

### Mechanism 3
An EM algorithm with Poisson data augmentation makes the intractable likelihood of interval-censored data solvable via SGD. Latent Poisson variables construct a complete-data likelihood, with the E-step calculating expectations and the M-step using SGD to update the DNN and closed-form updates for spline coefficients. This assumes conditional independence of failure time and observation times given covariates.

## Foundational Learning

- **Concept: Interval-Censored Data**
  - **Why needed here:** Standard right-censoring methods fail when we only know T ∈ (L, R]; specialized likelihood construction is required.
  - **Quick check question:** If a patient visits at t=2 (negative) and t=5 (positive), what is the contribution to the likelihood?

- **Concept: Sieve Maximum Likelihood Estimation**
  - **Why needed here:** Estimates infinite-dimensional functions by approximating them with finite-parameter spaces (sieves) like monotone splines and DNNs.
  - **Quick check question:** Why is "monotonicity" required for the spline approximation of the cumulative hazard Λ(t)?

- **Concept: Partially Linear Models (PLM)**
  - **Why needed here:** The structural core assumes additivity between the linear part (X) and the nonlinear part (W).
  - **Quick check question:** How does the identifiability condition E[ϕ(W)]=0 ensure the intercept doesn't get absorbed into the nonparametric function?

## Architecture Onboarding

- **Component map:** Input Layer (X to Linear Head; W to DNN Head) -> DNN Head (q hidden layers, SeLU, Dropout, Sparsity) -> Spline Head (I-splines for Λ(t)) -> Fusion (Linear + DNN -> Log-relative risk -> Interact with Spline via G) -> Optimization (EM Loop -> E-step -> M-step SGD/Global)

- **Critical path:**
  1. Initialize β, γ ≈ 0, DNN with Glorot uniform
  2. E-Step: Compute expected counts E(Z_{il}), E(Y_{il}) using current parameters
  3. M-Step (SGD): Train DNN ϕ(W) to minimize loss based on expected counts
  4. M-Step (Global): Update β and γ using one-step optimization
  5. Constraint: Force γ ≥ 0 to ensure monotonicity of hazard

- **Design tradeoffs:**
  - SeLU vs ReLU: SeLU keeps negative neurons active and ensures self-normalizing properties; ReLU may cause dead neurons
  - Dropout Rate: High dropout prevents overfitting but may slow convergence (suggested 0.1)
  - Transformation Function G: Choice of r (e.g., r=0 for PH, r=1 for PO) must be tuned on validation set

- **Failure signatures:**
  - Non-monotone Hazard: If SGD steps are too aggressive, spline coefficients γ might oscillate
  - Divergent EM: If E[ϕ(W)] ≠ 0, parameters may drift due to unidentifiability
  - Overfitting: Low training loss but high Integrated Brier Score (IBS) on test data

- **First 3 experiments:**
  1. Baseline Simulation (Case 1): Verify β estimates match ground truth and DNN doesn't introduce spurious noise
  2. Hyperparameter Scan: Vary knots (p_n) and DNN layers (q) to find minimax convergence rate elbow
  3. ADNI Data Validation: Confirm "Age" significance and compare IBS against "Spline-Trans" baseline

## Open Questions the Paper Calls Out

### Open Question 1
Can the framework be extended to multivariate interval-censored data where multiple correlated event times exist for the same individual? The current methodology assumes univariate failure times, and the theoretical properties are established for a single event per subject. A proposed joint likelihood formulation for multiple events and a proof of consistency for the resulting multivariate estimator would resolve this.

### Open Question 2
How can the method be generalized to account for complex features like left truncation, competing risks, or biased sampling designs like case-cohort studies? The current sieve maximum likelihood estimation and EM algorithm are derived specifically for standard interval censoring without these structural complexities. Derivation of a modified objective function or weighted estimation procedure that remains consistent under these complex sampling mechanisms would resolve this.

### Open Question 3
Is there a theoretically grounded method for selecting neural network hyperparameters (depth, width) to replace empirical grid search? Supplementary Material notes that hyperparameters are tuned via "grid search" and fixed to "save the computational burden," implying the current selection is heuristic. A theoretical derivation linking optimal network sparsity to sample size or simulations validating an adaptive selection algorithm would resolve this.

## Limitations
- The EM algorithm relies on strong regularity conditions that may not hold in real-world datasets with extreme sparsity or heavy censoring
- Theoretical minimax rate analysis assumes bounded weights and smooth functions that may deviate in practical DNN implementations
- Monotone spline approximation for the baseline hazard requires careful knot placement that could induce bias in small samples

## Confidence
- **High confidence**: Partially linear transformation model structure and EM-SGD estimation framework are well-defined and implementable
- **Medium confidence**: Asymptotic properties and minimax convergence rates depend on idealized assumptions that may not fully transfer to finite-sample performance
- **Low confidence**: Real-world performance on extremely high-dimensional W (p >> n) or highly irregular censoring patterns remains unverified

## Next Checks
1. **Simulation stress test**: Generate data with non-linear effects in X to assess model bias when the linear assumption is violated
2. **Knot sensitivity analysis**: Systematically vary the number and placement of spline knots to quantify their impact on hazard estimation stability
3. **Censoring robustness check**: Create datasets with increasing proportions of interval-censored observations to measure degradation in prediction accuracy