---
ver: rpa2
title: 'Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts'
arxiv_id: '2509.21743'
source_url: https://arxiv.org/abs/2509.21743
tags:
- reasoning
- arxiv
- tokens
- efficient
- thought
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new framework called Retrieval-of-Thought
  (RoT) to improve the efficiency of large reasoning models by reusing prior reasoning
  steps. RoT builds a structured thought graph that stores individual reasoning steps
  as nodes, then uses metadata filtering and reward-guided traversal to dynamically
  assemble problem-specific templates at inference time.
---

# Retrieval-of-Thought: Efficient Reasoning via Reusing Thoughts

## Quick Facts
- **arXiv ID:** 2509.21743
- **Source URL:** https://arxiv.org/abs/2509.21743
- **Reference count:** 40
- **Key outcome:** RoT reuses prior reasoning steps to reduce tokens by up to 40%, latency by up to 82%, and cost by 59% on math benchmarks while maintaining accuracy.

## Executive Summary
Retrieval-of-Thought (RoT) introduces a framework for efficient reasoning in large language models by reusing prior reasoning steps stored as a structured thought graph. At inference, RoT retrieves relevant reasoning paths, applies metadata filtering and reward-guided traversal, and dynamically assembles problem-specific templates to guide the LLM. This approach consistently achieves accuracy comparable to or higher than baselines like Chain-of-Thought, while significantly reducing computational overhead.

## Method Summary
RoT constructs a thought graph where each node represents an individual reasoning step, annotated with metadata and associated rewards. During inference, relevant thoughts are retrieved based on problem features, filtered by metadata, and traversed using a reward model to select the most promising reasoning paths. These paths are assembled into a template and used to guide the LLM's response. The system is evaluated on mathematical reasoning benchmarks (AIME and AMC), demonstrating substantial efficiency gains over traditional and retrieval-augmented baselines.

## Key Results
- RoT+TI achieves comparable or higher accuracy than Chain-of-Thought and retrieval-augmented baselines on AIME and AMC.
- Reduces output tokens by up to 40%, inference latency by up to 82%, and cost by 59%.
- Reduces path switching by up to 81.8%, indicating more stable reasoning trajectories.

## Why This Works (Mechanism)
RoT works by structuring prior reasoning into a reusable graph, enabling dynamic template assembly at inference. Metadata filtering ensures retrieved thoughts are relevant, while reward-guided traversal selects the most promising reasoning paths. This reuse reduces redundant reasoning steps and stabilizes the inference process, leading to both accuracy and efficiency gains.

## Foundational Learning
- **Thought Graph:** A structured representation of reasoning steps as nodes, enabling reuse and retrieval. Needed for organizing and accessing prior reasoning efficiently. Quick check: Verify nodes store reasoning steps, metadata, and rewards.
- **Metadata Filtering:** Selection of relevant thoughts based on problem features. Needed to ensure retrieved reasoning is applicable. Quick check: Confirm filtering criteria match problem context.
- **Reward-Guided Traversal:** Use of a reward model to select promising reasoning paths. Needed to prioritize effective reasoning sequences. Quick check: Validate reward model aligns with task success.
- **Template Assembly:** Dynamic construction of problem-specific reasoning templates from retrieved paths. Needed to guide LLM inference efficiently. Quick check: Ensure templates are coherent and relevant.
- **Inference-Time Reasoning:** Generating responses during inference using retrieved and assembled reasoning. Needed to avoid redundant computation. Quick check: Compare efficiency and accuracy with standard prompting.

## Architecture Onboarding

**Component Map:** Retrieval System -> Metadata Filter -> Reward Model -> Template Assembler -> LLM

**Critical Path:** Problem features → Retrieve thoughts → Metadata filter → Reward-guided traversal → Template assembly → LLM inference

**Design Tradeoffs:** 
- Trade accuracy for efficiency by reusing prior reasoning.
- Balance retrieval relevance and reward signal strength.
- Manage thought graph size and diversity for scalability.

**Failure Signatures:** 
- Irrelevant or noisy thoughts degrade reasoning quality.
- Over-reliance on template assembly may miss novel problem aspects.
- Poor reward model calibration leads to suboptimal path selection.

**First Experiments:**
1. Ablate metadata filtering to measure impact on retrieval relevance and accuracy.
2. Disable reward-guided traversal to assess the value of path selection.
3. Evaluate RoT on non-mathematical reasoning tasks to test generalizability.

## Open Questions the Paper Calls Out
The paper does not explicitly call out additional open questions beyond its own limitations and uncertainties.

## Limitations
- Relies on proprietary models (o1-preview, o1-mini) and closed-source retrieval systems, limiting reproducibility.
- Lacks hyperparameter sensitivity and ablation studies for key components.
- Evaluation is limited to mathematical reasoning benchmarks; benefits for other domains are unclear.
- Cost savings are estimated, not directly measured; scalability to large datasets is unexplored.

## Confidence
- **Efficiency gains (token reduction, latency, cost): High** - Controlled experiments and clear baselines support reported improvements.
- **Robustness and generalizability: Medium** - Limited cross-task evaluation and absence of ablation studies reduce confidence.
- **Scalability and deployment: Low** - No scaling studies or exploration of large-scale deployment challenges.

## Next Checks
1. Perform ablation studies to quantify the contribution of metadata filtering, reward-guided traversal, and template assembly to RoT's efficiency gains.
2. Evaluate RoT on non-mathematical reasoning tasks (e.g., commonsense QA, code generation) to assess generalizability.
3. Conduct a scalability analysis to determine the impact of thought graph size and diversity on performance and efficiency as the number of stored thoughts grows.