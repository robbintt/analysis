---
ver: rpa2
title: A systematic review of trial-matching pipelines using large language models
arxiv_id: '2509.19327'
source_url: https://arxiv.org/abs/2509.19327
tags:
- data
- trial
- patient
- clinical
- matching
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic review analyzed 31 studies from 2020-2025 examining
  large language models (LLMs) for clinical trial matching. Key findings showed GPT-4
  consistently outperformed other models in patient-to-trial matching tasks, with
  studies reporting accuracy rates between 85-96% for eligibility assessment.
---

# A systematic review of trial-matching pipelines using large language models

## Quick Facts
- **arXiv ID**: 2509.19327
- **Source URL**: https://arxiv.org/abs/2509.19327
- **Reference count**: 0
- **Primary result**: GPT-4 consistently outperformed other models in patient-to-trial matching tasks with accuracy rates between 85-96%

## Executive Summary
This systematic review analyzed 31 studies from 2020-2025 examining large language models (LLMs) for clinical trial matching. The review identified four main pipeline components: data acquisition/pre-processing, enrichment/retrieval, matching, and output generation. GPT-4 consistently showed superior performance across multiple studies, with accuracy rates ranging from 85-96% for eligibility assessment. However, most studies relied on synthetic data rather than real patient data, raising questions about real-world applicability.

The review highlighted promising approaches including retrieval-augmented generation and fine-tuning smaller open-source models for privacy concerns. Despite strong performance metrics, challenges remain in data standardization, cost optimization, and bias mitigation. The authors conclude that standardized metrics and real-world validation are essential for broader clinical implementation of LLM-based trial-matching systems.

## Method Summary
The authors conducted a systematic review of peer-reviewed literature from 2020-2025 focusing on LLMs for clinical trial matching. They analyzed 31 studies that employed various models including GPT-4, GPT-3.5, and open-source alternatives. The review examined pipeline components, data types (synthetic vs. real patient data), evaluation metrics, and implementation challenges. Studies were assessed for their methodological rigor, data sources, and reported performance outcomes.

## Key Results
- GPT-4 consistently outperformed other models with accuracy rates between 85-96% for eligibility assessment
- Four main pipeline components identified: data acquisition/pre-processing, enrichment/retrieval, matching, and output generation
- Most studies used synthetic data (16) versus real patient data (14), limiting generalizability

## Why This Works (Mechanism)
The success of LLMs in trial matching stems from their ability to process unstructured clinical text and extract relevant eligibility criteria. GPT-4's transformer architecture enables understanding of complex medical terminology and relationships between patient characteristics and trial requirements. The models can handle ambiguous language in trial descriptions and map it to structured patient data. Retrieval-augmented approaches further enhance performance by incorporating domain-specific knowledge bases and structured trial databases.

## Foundational Learning
**Clinical Trial Eligibility Criteria**: Medical and demographic requirements for patient participation in trials; needed for accurate matching, check by reviewing trial databases like clinicaltrials.gov
**Natural Language Processing**: Techniques for processing unstructured clinical text; needed for extracting meaningful information, check by testing on medical documents
**Patient Data Standardization**: Converting heterogeneous medical records into consistent formats; needed for reliable model inputs, check by validating across multiple EMR systems
**Bias Detection in Medical AI**: Methods for identifying and mitigating algorithmic bias; needed for equitable patient matching, check by analyzing demographic performance gaps
**Cost-Benefit Analysis in Healthcare AI**: Evaluating operational expenses versus clinical outcomes; needed for sustainable implementation, check by calculating per-patient matching costs
**Retrieval-Augmented Generation**: Combining LLMs with external knowledge bases; needed for improved accuracy, check by comparing with baseline LLM performance

## Architecture Onboarding
**Component Map**: Data Acquisition -> Preprocessing -> Enrichment/Retrieval -> Matching -> Output Generation
**Critical Path**: Patient data preprocessing and matching algorithm are the most critical components, as errors here propagate through the entire pipeline
**Design Tradeoffs**: GPT-4 offers superior accuracy but high costs versus open-source models that are cheaper but require fine-tuning; synthetic data is readily available but may not capture real-world complexity
**Failure Signatures**: Poor matching accuracy when trial descriptions use non-standard terminology; bias toward overrepresented demographics in training data; high costs limiting scalability
**First Experiments**: 1) Compare GPT-4 matching accuracy against traditional rule-based systems on identical patient-trial pairs, 2) Test synthetic data performance against real patient cohorts, 3) Evaluate cost per successful match across different LLM models

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the broader implementation of LLM-based trial matching. These include the need for standardized evaluation metrics across studies, the challenge of bias mitigation in diverse patient populations, and the sustainability of high-cost models like GPT-4 for large-scale clinical use. The authors also question how well synthetic data performance translates to real-world scenarios and what optimization strategies can reduce operational expenses while maintaining accuracy.

## Limitations
- Most studies relied on synthetic data rather than real patient data, limiting real-world applicability
- Significant heterogeneity in evaluation metrics across studies makes direct comparisons difficult
- Cost optimization strategies remain under-explored despite high operational expenses of leading models
- Limited long-term validation data on model performance and stability in clinical settings

## Confidence
- **High confidence**: The identification of four pipeline components and the overall trend toward LLMs improving matching accuracy
- **Medium confidence**: GPT-4 performance superiority claims, given limited real-world validation
- **Low confidence**: Cost-benefit analyses and long-term sustainability of LLM-based solutions

## Next Checks
1. Conduct prospective validation using real patient cohorts across multiple clinical trial sites to verify synthetic data performance claims
2. Implement standardized evaluation frameworks comparing LLMs against traditional matching algorithms using identical datasets and metrics
3. Perform cost-effectiveness analysis incorporating operational expenses, maintenance requirements, and scalability considerations for different clinical settings