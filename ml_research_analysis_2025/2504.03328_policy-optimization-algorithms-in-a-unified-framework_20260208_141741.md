---
ver: rpa2
title: Policy Optimization Algorithms in a Unified Framework
arxiv_id: '2504.03328'
source_url: https://arxiv.org/abs/2504.03328
tags:
- policy
- algorithms
- optimization
- gradient
- average
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified framework for understanding and implementing
  policy optimization algorithms in reinforcement learning by leveraging generalized
  ergodicity theory and perturbation analysis. The framework addresses the common
  confusion between discounted and average reward setups, which often leads to incorrect
  implementations.
---

# Policy Optimization Algorithms in a Unified Framework

## Quick Facts
- arXiv ID: 2504.03328
- Source URL: https://arxiv.org/abs/2504.03328
- Reference count: 13
- Primary result: Unified framework using generalized ergodicity and perturbation analysis resolves confusion between discounted and average reward setups, identifies common implementation errors, and provides correct algorithm derivations.

## Executive Summary
This paper addresses the persistent confusion and implementation errors in policy optimization algorithms caused by the coexistence of different reward setups (discounted, total, average) in reinforcement learning. The authors introduce a unified framework based on generalized ergodicity theory that converts all time-based MDP formulations into a single space-based formulation, enabling clearer derivations and implementations. By combining this with perturbation analysis, the framework systematically derives fundamental policy optimization algorithms and provides insights into approximate methods like TRPO and PPO. The paper demonstrates how common implementation errors arise from mismatched visitation measures and value functions, and shows through LQR case studies how slight algorithmic variations affect outcomes.

## Method Summary
The method leverages generalized ergodicity theory to unify different MDP reward setups into a single space-based formulation, where time averages are converted to space averages using a generalized measure νπ•. This unified notation allows for cleaner derivations and implementation rules. Perturbation analysis is then applied to this unified framework to systematically derive exact algorithms (policy iteration) and approximate methods (policy gradient, natural policy gradient, TRPO, PPO) by analyzing small policy perturbations. The approach identifies common implementation errors as mismatches between the visitation measure and value function across different reward setups, and provides explicit conversion rules from space-based formulas to implementable time-based algorithms.

## Key Results
- Generalized ergodicity theory successfully unifies discounted, total, and average reward setups into a single space-based formulation
- Common implementation errors stem from mismatching visitation measures (νπ•) and value functions (Qπ•) across reward setups
- LQR case studies demonstrate that incorrect implementations converge to suboptimal policies compared to correct ones
- The unified framework provides clear guidelines for implementing policy optimization algorithms across all reward setups

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalized ergodicity theory provides a unified framework to convert multiple time-based MDP formulations into a single space-based formulation
- Mechanism: The framework extends classical ergodicity by defining a generalized "averaging" measure νπ• that equates time averages to space averages for all reward setups, allowing researchers to work with random variables instead of stochastic processes
- Core assumption: The MDP and policy satisfy appropriate ergodicity conditions (e.g., unichain property, finite expected total reward for discounted case)
- Evidence anchors: Section 3 establishes unified notation J•(π|s0) = ∫s∈S νπ•(ds) E[R(s,a)] for all setups; weak corpus evidence for this specific unification
- Break condition: If the Markov chain is not ergodic (e.g., not unichain) or expected total reward is infinite for discounted case, the time-space conversion fails

### Mechanism 2
- Claim: Perturbation analysis systematically derives fundamental policy optimization algorithms from the unified performance difference formula
- Mechanism: Treating policy differences as perturbations, the framework derives exact optimization (policy iteration) and first-order approximations (policy gradient), extending to second-order approximations (natural policy gradient, TRPO)
- Core assumption: For approximate methods, the new policy π' remains close to the old policy π
- Evidence anchors: Section 4 uses perturbation analysis to derive policy iteration (Eq. 7), policy gradient (Eq. 8), and approximate methods; weak corpus evidence for this unified derivation approach
- Break condition: If policy updates are too large, linear and quadratic approximations break down, causing algorithms to diverge or perform poorly

### Mechanism 3
- Claim: The framework reveals common implementation errors stem from mismatching visitation measures and value functions across reward setups
- Mechanism: Incorrect implementations often use average-reward visitation measure (νπµ) with discounted value function (Qπγ), creating biased estimators that the framework explicitly identifies through consistent notation
- Core assumption: Implementers are misled by simplified notation that fails to distinguish between different visitation measures
- Evidence anchors: Section 5.1 details this mechanism, showing "typical incorrect policy gradient" formula leads to biased implementation; weak corpus evidence on this specific error mechanism
- Break condition: If practitioners ensure consistent matching of visitation measure and value function (e.g., using γk weighting in discounted case), the identified errors are avoided

## Foundational Learning

Concept: **Markov Decision Process (MDP) and Reward Setups**
- Why needed here: The entire paper unifies different MDP reward setups (discounted, total, average). Understanding these is prerequisite to grasping the framework's purpose.
- Quick check question: What is the key mathematical difference between a discounted reward objective Jγ and a long-run average reward objective Jµ?

Concept: **Ergodicity Theory (Classical)**
- Why needed here: The paper introduces "generalized" ergodicity. Understanding classical ergodicity (time averages = space averages) is prerequisite to grasping the extension.
- Quick check question: In a system governed by classical ergodicity, what is the relationship between the long-term average of a value observed over time and the average of that value over the state space's steady-state distribution?

Concept: **Policy Gradient Methods**
- Why needed here: The framework re-derives and analyzes standard algorithms like Policy Gradient, Natural Policy Gradient, TRPO, and PPO. Basic concept of updating policy based on performance gradient is required.
- Quick check question: The policy gradient theorem provides a formula for the gradient of expected reward with respect to policy parameters. What are the two main components typically multiplied together in this formula?

## Architecture Onboarding

- Component map:
  1. Generalized Ergodicity Module: Core theoretical component defining unified measure νπ• and rules for converting between time-based and space-based formulations
  2. Perturbation Analysis Engine: Derivation methodology starting from unified performance difference formula to generate update rules for various algorithms
  3. Implementation Translators: Practical rules for converting space-based formulas to concrete time-based algorithms and preventing common errors
  4. Diagnostic Comparator: Component allowing practitioners to check existing implementations against the unified framework

- Critical path:
  1. Understand the problem: confusion and implementation errors in existing policy optimization due to different setups (Section 1)
  2. Master the core theoretical component: generalized ergodicity theory unifying setups into space-based form (Section 3, especially Eq. 1-3)
  3. Follow the derivation engine: Use perturbation analysis on unified formula to derive algorithms from exact (PI) to approximate (PG, NPG, TRPO, PPO) (Section 4)
  4. Apply the translator: Use time-space conversion rules (Table 2) to write correct implementations and diagnostic comparator to fix flawed ones (Section 5)

- Design tradeoffs:
  - Theoretical Elegance vs. Implementation Complexity: Space-based formulation is mathematically cleaner but not directly implementable; translator adds conversion layer back to time domain
  - Exact vs. Approximate Optimization: Framework shows tradeoff between exact policy iteration (computationally expensive) and approximate methods like PG/NPG/TRPO/PPO (cheaper but require small policy changes)
  - Precision vs. Speed in Implementation: Framework identifies incorrect implementations often arise from neglecting discount factor weighting (γk), correcting this adds overhead but ensures unbiased gradients

- Failure signatures:
  - Hybrid Gradient Error: Implementing discounted PG using average-reward visitation measure (e.g., omitting γk weighting) - Section 5.1
  - Convergence to Suboptimal Policy: Incorrect implementations may still converge but to different, often suboptimal, policy compared to correct one - Figure 3
  - Confusion in Undiscounted Cases: Misapplying formulas when γ = 1 or in average-reward limit, leading to unstable or incorrect learning
  - Failure on Slow Dynamics: Slower system dynamics require larger discount factor to approximate average reward effectively - Figure 4

- First 3 experiments:
  1. Re-implement Policy Gradient for a Discounted MDP: Implement incorrect version (without γk weighting) and correct version (with γk weighting) in simple environment, compare learning curves and final performance
  2. Derive TRPO Update from the Framework: Use perturbation analysis method to derive TRPO update rule from approximate performance difference and policy curvature constraint, compare to standard TRPO
  3. Analyze Failure on Different MDPs: Implement correct average-reward policy gradient, test on ergodic vs. non-ergodic MDPs, observe algorithm failure on non-ergodic case

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why does Proximal Policy Optimization (PPO) consistently outperform Trust Region Policy Optimization (TRPO) in practice?
- **Basis in paper:** [explicit] Remark 4.2 notes that "PPO also surpasses TRPO in performance, a phenomenon not yet fully understood"
- **Why unresolved:** Paper provides hypothesis that TRPO's linear and quadratic approximations reduce effectiveness compared to PPO's direct optimization, but does not conclusively prove this is the primary cause
- **What evidence would resolve it:** Theoretical analysis or ablation study isolating error from TRPO's quadratic constraint approximation versus PPO's clipping mechanism

### Open Question 2
- **Question:** How can the PPO-Clip algorithm be effectively adapted for deterministic policies?
- **Basis in paper:** [inferred] Section 4.2 notes probability ratio becomes problematic for deterministic policies; authors formulate theoretical constraint but state it's "more challenging to implement" without providing solution
- **Why unresolved:** Paper defines theoretical objective for deterministic policies but leaves practical algorithm design and implementation details as open challenge
- **What evidence would resolve it:** Defined algorithm for deterministic PPO-Clip and empirical validation showing stability and convergence on standard continuous control benchmarks

### Open Question 3
- **Question:** What is the theoretical relationship between convergence speed of system dynamics and required discount factor (γ) when approximating average reward setups?
- **Basis in paper:** [inferred] Section 6.2 observes numerically that "slower systems require larger discount factor to approximate average reward setup effectively," but provides no theoretical bounds
- **Why unresolved:** Observation derived from specific LQR numerical examples rather than generalized theoretical proof
- **What evidence would resolve it:** Theoretical derivation quantifying approximation error between discounted and average reward objectives as function of system matrix eigenvalues

## Limitations
- Framework extension to continuous state-action spaces relies on assumptions that may not hold in practice
- Perturbation analysis assumes small policy changes, which may not apply to aggressive learning rates
- LQR validation, while rigorous, represents a restricted class of problems and doesn't address computational complexity implications

## Confidence

- Theoretical framework claims: **High confidence** - Mathematical derivations are consistent and LQR case studies demonstrate correctness
- Generality to non-linear problems: **Medium confidence** - Validation limited to linear quadratic case
- Framework reducing misuse in practice: **Low confidence** - No empirical evidence from practitioners or real-world codebases

## Next Checks

1. **Implementation Survey**: Survey open-source RL repositories to identify actual policy gradient implementations using "incorrect hybrid" formulation and measure empirical performance gap compared to correct implementations

2. **Non-Linear Testbed**: Apply unified framework to non-linear control problem (e.g., pendulum swing-up) and compare convergence properties of correct vs. incorrect implementations to test framework generalizability

3. **Convergence Robustness**: Systematically vary policy update magnitude in LQR experiments to identify threshold where perturbation-based approximations (PG, NPG, TRPO) break down, validating small-change assumption