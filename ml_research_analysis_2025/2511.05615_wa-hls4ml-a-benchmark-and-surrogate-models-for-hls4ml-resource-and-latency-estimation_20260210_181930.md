---
ver: rpa2
title: 'wa-hls4ml: A Benchmark and Surrogate Models for hls4ml Resource and Latency
  Estimation'
arxiv_id: '2511.05615'
source_url: https://arxiv.org/abs/2511.05615
tags:
- resource
- dataset
- latency
- hls4ml
- surrogate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: wa-hls4ml provides a large dataset of over 680,000 synthesized
  ML models and a benchmark for FPGA resource/latency prediction. It introduces graph
  neural network (GNN) and transformer-based surrogate models that predict LUT, FF,
  DSP, BRAM, cycles, and II.
---

# wa-hls4ml: A Benchmark and Surrogate Models for hls4ml Resource and Latency Estimation

## Quick Facts
- **arXiv ID**: 2511.05615
- **Source URL**: https://arxiv.org/abs/2511.05615
- **Reference count**: 40
- **Primary result**: wa-hls4ml introduces GNN and transformer surrogate models for predicting FPGA resource usage and latency from hls4ml-synthesized neural networks, with SMAPE of 19.5% (GNN) and 14.1% (transformer) on synthetic test data.

## Executive Summary
wa-hls4ml provides a comprehensive benchmark dataset of over 680,000 synthesized machine learning models and introduces two novel surrogate models—graph neural networks (GNN) and transformers—for predicting FPGA resource utilization (LUT, FF, DSP, BRAM) and latency metrics (clock cycles, II) without full synthesis. The dataset spans diverse neural network architectures including fully connected, Conv1D, and Conv2D networks with systematic variation in precision, reuse factors, and hls4ml strategies. While achieving strong performance on synthetic test data, the models show significant degradation on realistic scientific models, highlighting a distribution gap that represents both a limitation and an opportunity for future research.

## Method Summary
The approach involves generating synthetic neural network architectures with varying hyperparameters, synthesizing them through hls4ml to obtain ground-truth resource usage, and training surrogate models to predict these metrics directly from architectural specifications. The GNN model treats each layer as a graph node with 18-dimensional features and uses GATv2Conv layers with attention mechanisms to learn inter-layer relationships. The transformer model treats layers as sequence tokens with self-attention to capture global dependencies. Both models predict six targets (BRAM, DSP, FF, LUT, Cycles, II) using log-scaled regression with MSE loss and AdamW optimization.

## Key Results
- GNN achieves 19.5% SMAPE and transformer achieves 14.1% SMAPE on synthetic test data
- Transformer outperforms GNN on FF (2.9% SMAPE) and LUT (2.9% SMAPE) predictions
- GNN shows better performance on DSP and Cycles with R² of 0.89 for both
- Both models demonstrate significant performance degradation on exemplar scientific models, with negative R² values indicating predictions worse than mean baseline

## Why This Works (Mechanism)

### Mechanism 1: Graph Neural Network Attention Learning
Graph neural networks predict hardware resources by modeling neural network layers as graph nodes and learning which layer connections most impact resource usage through multi-head attention. Each layer becomes an 18-dimensional feature vector, and GATv2Conv layers dynamically assign importance weights to edges based on learned compatibility. Multi-strategy pooling aggregates node embeddings into graph-level predictions. This works because resource usage patterns emerge from local layer interactions and global configuration rather than aggregate statistics alone. Evidence shows GNN achieves R²=0.89 for DSP and Cycles on synthetic test data. The mechanism breaks when architectures have complex skip connections not well-represented in training data, as shown by dramatic SMAPE degradation on exemplar models.

### Mechanism 2: Transformer Layer Sequence Modeling
Transformers estimate hardware metrics by treating neural network layers as tokens in a sequence, using self-attention to capture global layer relationships. Each layer's feature vector becomes a token projected to 512-dimensional embedding with positional encodings preserving layer order. A [CLS] token aggregates sequence information, and two encoder blocks with 8-head self-attention process the sequence. The [CLS] output maps to six hardware predictions. This works because layer position and sequential context matter for resource estimation beyond per-layer statistics. Evidence shows transformer achieves best SMAPE for FF (2.9%) and LUT (2.9%) on test set, with R²=0.95 for Cycles and II. The mechanism breaks with fixed maximum sequence length (51 layers), limiting applicability to deeper networks where attention dilution may occur.

### Mechanism 3: Large-Scale Synthesized Dataset Generalization
Large-scale synthesized datasets enable surrogate models to generalize across architecture types by covering diverse layer configurations, precisions, and hls4ml strategies. The 683,176 samples systematically vary layers (2-7), neurons (8-128), precision (2-16 bits), reuse factor (1-32,795), strategies, and I/O types. Full synthesis chain preservation enables regression from architecture specification to ground-truth resources. This works because synthetic randomly-generated architectures sufficiently cover the design space that realistic architectures will interpolate within training distribution. Evidence shows systematic coverage of FCNs (608,679), Conv1D (31,278), and Conv2D (43,219) architectures. The mechanism breaks when exemplar models show dramatically worse performance than synthetic test set, indicating synthetic data doesn't cover realistic architecture distributions.

## Foundational Learning

- **FPGA Resource Types (LUT, FF, DSP, BRAM)**: These four resource categories are the primary targets of prediction; understanding what they represent is essential for interpreting predictions and knowing which matter for your workload. Quick check: If your model has many large matrix multiplications with 16-bit weights, which resource is most likely to be the bottleneck?

- **hls4ml Reuse Factor and Strategies**: Reuse factor and strategy (resource vs. latency) are core input features that dramatically affect both resources and latency; predictions are conditional on these choices. Quick check: A reuse factor of 1 vs. 1024 will have opposite effects on DSP usage vs. latency—which increases and which decreases?

- **Graph Attention Networks (GATv2)**: The GNN surrogate uses GATv2Conv layers; understanding attention coefficients helps debug why certain layer connections drive predictions. Quick check: In a 3-layer network, if the GNN over-predicts DSP, which attention weights would you inspect to understand why?

## Architecture Onboarding

- **Component map**: JSON → 18-dim feature vectors per layer → normalization → GNN graph construction (sequential layers + self-loops) OR transformer token embedding → model forward pass → denormalization → 6 hardware predictions

- **Critical path**: Input JSON → feature extraction → normalization → graph construction (GNN) or token embedding (transformer) → model forward pass → denormalization → resource predictions. For GNN, graph construction is the key step.

- **Design tradeoffs**: GNN handles arbitrary graph topologies but requires graph construction overhead; best for DSP/Cycles (R²=0.89/0.89). Transformer simpler to implement but limited to 51 layers; best for FF/LUT (SMAPE 2.9%). MLP baseline fastest inference but poorest accuracy (DSP R²=0.03). Over-prediction bias may be preferable for design safety.

- **Failure signatures**: Negative R² on exemplar models indicates predictions worse than mean baseline—sign of out-of-distribution inputs. SMAPE >50% on specific architectures (Quarks, CookieBox) suggests those patterns absent from training. Wide IQR in RPE box plots indicates prediction instability.

- **First 3 experiments**: 
  1. Train MLP on 478K training samples, evaluate SMAPE on test set. Expected: DSP SMAPE ~105%, Cycles SMAPE ~32%.
  2. Compare GNN with 3 vs. 5 GATv2Conv layers on Conv2D subset. Expected: Deeper GNN should improve R² for complex architectures.
  3. Train on synthetic data only, evaluate on exemplar set. Then add 10% exemplar data to training. Expected: Significant SMAPE reduction on exemplars.

## Open Questions the Paper Calls Out

1. Can the general-purpose HLS estimation approach by Wu et al. [38] achieve prediction accuracy comparable to domain-specific models when retrained on the wa-hls4ml dataset? The authors state this as an interesting future study, noting the current comparison is unfair because Wu et al. was trained on generic C/C++ applications while wa-hls4ml targets neural networks.

2. Does modifying the training loss function to penalize underestimation more heavily improve the safety and utility of resource estimates for hardware budgeting? The authors mention exploring techniques such as preferring overestimation when calculating the loss as a specific avenue for future model refinement, given current models sometimes underpredict resources.

3. To what extent does including complex topologies with skip connections in the training dataset reduce the performance gap between synthetic test data and realistic scientific models? The authors explicitly list intricate architectures with features like skip connections as a dataset expansion target, given the current synthetic dataset primarily covers simple feed-forward structures.

## Limitations
- Significant distribution gap between synthetic training data and real-world exemplar models leads to poor generalization
- Architectural constraints limit applicability: transformer capped at 51 layers, models struggle with complex topologies
- Resource-intensive dataset generation requiring 1000+ CPU cores for months may limit community adoption

## Confidence
- **High Confidence**: Core mechanism of using GNNs and transformers for hardware estimation is well-established; dataset generation pipeline and training methodology are clearly described
- **Medium Confidence**: Reported SMAPE and R² values are based on synthetic test data where models likely perform well due to distribution alignment; exemplar performance shows significant degradation
- **Low Confidence**: Claim that work "advances codesign research for edge ML accelerators" is supported by dataset creation but not fully validated by exemplar performance

## Next Checks
1. Train the GNN on a combined dataset of synthetic and exemplar samples (80/20 split) and evaluate performance on a held-out exemplar set to measure whether including real architectures improves generalization.
2. Generate synthetic architectures with 50+ layers and skip connections, then evaluate both GNN and transformer performance to determine practical depth limits and architectural constraints.
3. Apply the best-performing model to predict resources for a real physics analysis network from CMS or LHCb collaborations and compare predictions against actual synthesis results, measuring prediction error and design safety implications.