---
ver: rpa2
title: Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models
arxiv_id: '2504.07807'
source_url: https://arxiv.org/abs/2504.07807
tags:
- pruning
- answer
- expert
- experts
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Cluster-driven Expert Pruning (C-Prune), a
  two-stage framework for adaptive task-specific compression of MoE LLMs. C-Prune
  addresses intra-layer expert homogeneity and inter-layer similarity patterns in
  MoE models through layer-wise expert clustering followed by global cluster pruning
  using parameter similarity metrics.
---

# Cluster-Driven Expert Pruning for Mixture-of-Experts Large Language Models

## Quick Facts
- arXiv ID: 2504.07807
- Source URL: https://arxiv.org/abs/2504.07807
- Reference count: 25
- This paper proposes C-Prune, achieving 20% parameter reduction while maintaining performance on DeepSeek-V2-Lite and Qwen1.5-MoE models

## Executive Summary
This paper addresses the challenge of compressing Mixture-of-Experts (MoE) large language models through a two-stage framework called Cluster-driven Expert Pruning (C-Prune). The method identifies and eliminates redundant experts by first grouping functionally similar experts within each layer through parameter similarity metrics, then applying global pruning across layers using unified importance scoring. Experiments demonstrate that C-Prune can reduce parameters by 20% (from 15.7B to 13.0B for DeepSeek) while maintaining strong performance across MMLU, GSM8K, and HumanEval benchmarks, outperforming existing pruning methods.

## Method Summary
C-Prune operates through a two-stage process: layer-wise expert clustering and global cluster pruning. The framework first computes expert embeddings by averaging outputs over calibration data, then constructs affinity matrices using scaled cosine similarity. Hierarchical agglomerative clustering groups similar experts within each layer with adaptive thresholds. After layer-wise clustering, a unified importance scoring mechanism evaluates clusters across all layers simultaneously, capturing cross-layer homogeneity patterns. The method employs parameterized expert merging using attention-style aggregation and updates routing weights with exploration noise to maintain load balance. This approach eliminates both intra-layer redundancy through clustering and inter-layer similarity through global pruning.

## Key Results
- Achieves 20% parameter reduction (15.7B to 13.0B for DeepSeek-V2-Lite) while maintaining performance
- Outperforms existing pruning methods with 44.94 average MMLU score vs 32.03 for Group&Merge
- Shows particular effectiveness in technical domains like computer science and mathematics
- Hierarchical clustering provides ~10% performance improvement over K-means

## Why This Works (Mechanism)

### Mechanism 1: Layer-wise Expert Clustering for Intra-layer Redundancy Elimination
Grouping functionally similar experts within each MoE layer using parameter similarity metrics reduces intra-layer homogeneity without requiring gradient-based optimization. For each expert, compute a characteristic embedding by averaging expert outputs over calibration data. Construct an expert affinity matrix using scaled cosine similarity, then apply hierarchical agglomerative clustering with adaptive layer-specific thresholds. Merge clusters iteratively until target cluster count is reached. The core assumption is that expert parameter similarity correlates with functional similarity, and redundant experts can be merged without catastrophic knowledge loss.

### Mechanism 2: Global Cluster Pruning for Inter-layer Consistency
A unified importance scoring mechanism that evaluates clusters across all layers simultaneously captures cross-layer homogeneity patterns that layer-isolated pruning misses. After layer-wise clustering, compute global importance scores that account for cross-layer expert similarity patterns. Prune clusters based on their contribution to function preservation while maintaining diversity constraints. The core assumption is that deeper layers contain progressively more homogeneous experts, so global pruning can aggressively target deeper-layer redundancy.

### Mechanism 3: Parameterized Expert Merging with Routing Adaptation
Weighted merging of clustered experts using attention-style aggregation preserves more functional capacity than naive averaging, while routing weight redistribution maintains load balance. For merged cluster C_k, compute merged parameters as weighted sum where weights are derived from attention-style aggregation. Update routing weights by averaging original weights with exploration noise. The core assumption is that attention-weighted parameter interpolation approximates the functional ensemble of original experts, and routing exploration prevents premature expert specialization collapse.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing Dynamics**: Understanding how top-k routing, load balancing, and expert selection interact is essential for diagnosing post-pruning failures. Quick check: Given a 64-expert MoE layer with top-2 routing, if two similar experts are merged, what happens to the routing probabilities for tokens that previously selected both?

- **Hierarchical Agglomerative Clustering**: C-Prune uses hierarchical clustering (validated with 0.449 vs 0.405 for K-means) to build expert groups. Understanding linkage criteria and stopping conditions determines merge quality. Quick check: When merging experts with different activation frequencies, should the affinity matrix weight frequency, parameter similarity, or both?

- **Calibration Data Distribution**: Expert embeddings are computed as E_x∼D[f_i(x)], so the choice of calibration distribution D directly determines which experts appear redundant. Quick check: If you prune a math-focused MoE using general-purpose calibration data, what failure mode would you expect on GSM8K tasks?

## Architecture Onboarding

- **Component map**: Input (calibration data D) → Expert Embedding Module [Equation 4] → Affinity Matrix Construction [Equation 7] → Layer-wise Clustering [Equations 5-6, per-layer] → Global Importance Scoring [Equation 3] → Cluster Pruning Decision [Equation 11] → Parameterized Merging [Equation 12] → Routing Weight Adaptation [Equation 13] → Pruned MoE Model

- **Critical path**: Expert embedding quality → clustering accuracy → merge effectiveness. Errors in embedding computation (insufficient calibration samples, distribution mismatch) propagate through all subsequent stages.

- **Design tradeoffs**: Higher pruning rate (0.2 vs 0.1) provides more compression but GSM8K drops from ~40 to ~25. Layer-wise vs Global pruning ratio: Technical subjects prefer higher layer-wise ratios; economics prefers higher global ratios. Hierarchical vs K-means clustering: 10% average performance gain but higher computational cost.

- **Failure signatures**: Random pruning baseline: 64% MMLU drop (Table 1) — indicates routing collapse or critical expert removal. Task-agnostic pruning on specialized domains: 15-20% performance gaps vs task-specific. Aggressive global pruning on technical tasks: Math college scores drop from 0.36 to 0.29.

- **First 3 experiments**: 1) Baseline validation on single domain: Run C-Prune on DeepSeek-V2-Lite with math calibration data only. Measure MMLU-math before/after pruning. 2) Ablation of clustering method: Replace hierarchical clustering with K-means. Expect ~10% performance degradation. 3) Cross-domain transfer test: Prune using CS calibration data, evaluate on GSM8K.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does C-Prune maintain its effectiveness on trillion-parameter scale MoE models with significantly more experts?
- Basis in paper: Due to computational constraints, the authors could not validate the method on larger-scale MoE models to demonstrate real-world scalability.
- Why unresolved: Experiments were limited to DeepSeek-V2-Lite (15.7B) and Qwen1.5-MoE-A2.7B (14.3B), leaving scalability to models like GPT-4 scale untested.
- What evidence would resolve it: Applying C-Prune to models with 100B+ parameters and demonstrating consistent pruning efficiency without disproportionate performance degradation.

### Open Question 2
- Question: Can C-Prune be combined with quantization or knowledge distillation for compound efficiency gains?
- Basis in paper: Section 5.4 mentions potential integration with complementary optimization techniques such as quantization and knowledge distillation to further enhance deployment efficiency.
- Why unresolved: The paper only evaluates C-Prune in isolation; interactions with other compression methods remain unexplored.
- What evidence would resolve it: Experiments applying 4-bit quantization or teacher-student distillation after C-Prune, measuring cumulative parameter reduction and performance retention.

### Open Question 3
- Question: What mechanisms determine the optimal layer-wise vs. global pruning ratio configuration for specific domains?
- Basis in paper: Section 5.5 shows Mathematics performs best with 12/6 layerwise/global experts while Computer Science prefers 12/18, but no systematic method for determining these configurations is provided.
- Why unresolved: Current configurations appear empirically discovered; the underlying principles for domain-optimal distributions remain unclear.
- What evidence would resolve it: A systematic study correlating domain characteristics (e.g., reasoning depth, knowledge modularity) with optimal expert distribution patterns.

## Limitations
- Computational overhead of pre-processing phase may be prohibitive for very large MoE models
- Calibration data sensitivity could lead to suboptimal pruning if distribution doesn't match target tasks
- Routing weight adaptation with exploration noise might introduce instability in long-sequence inference

## Confidence

**High Confidence Claims:**
- C-Prune outperforms random pruning (Table 1: 44.94 vs 32.03 MMLU average)
- Hierarchical clustering provides ~10% performance improvement over K-means (Table 3)
- Layer-wise expert clustering effectively reduces intra-layer homogeneity

**Medium Confidence Claims:**
- Global cluster pruning captures cross-layer homogeneity patterns
- The 20% parameter reduction maintains task performance across diverse benchmarks
- Task-specific calibration data yields better results than generic data

**Low Confidence Claims:**
- The framework generalizes to MoE architectures beyond DeepSeek-V2-Lite and Qwen1.5
- The routing weight adaptation mechanism doesn't introduce long-sequence degradation
- The computational overhead is acceptable for practical deployment

## Next Checks

1. **Calibration Data Ablation Study** - Run C-Prune with three different calibration sets: (a) general-purpose data, (b) task-specific data (e.g., GSM8K for math), and (c) mixed-domain data. Measure MMLU and GSM8K performance for each configuration to quantify calibration sensitivity and determine optimal calibration strategies.

2. **Long-sequence Inference Stability Test** - Evaluate pruned models on tasks requiring processing of long contexts (e.g., multi-turn dialogue or document summarization). Measure routing consistency, attention stability, and performance degradation compared to the original model to validate the routing weight adaptation mechanism.

3. **Scalability Validation** - Apply C-Prune to larger MoE architectures (e.g., 128 experts per layer) and measure both computational overhead and performance retention. Document how clustering runtime, memory usage, and pruning effectiveness scale with model size to assess practical deployment constraints.