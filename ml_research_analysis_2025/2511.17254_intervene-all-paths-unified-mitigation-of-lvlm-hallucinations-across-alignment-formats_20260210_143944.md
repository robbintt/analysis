---
ver: rpa2
title: 'Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment
  Formats'
arxiv_id: '2511.17254'
source_url: https://arxiv.org/abs/2511.17254
tags:
- heads
- image
- attention
- arxiv
- pope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a comprehensive intervention framework for\
  \ mitigating hallucinations in Large Vision-Language Models (LVLMs) by targeting\
  \ both image-to-text and text-to-text causal pathways. The method identifies critical\
  \ attention heads using two novel metrics\u2014Log Probability Increase for text-to-text\
  \ heads and image attention concentration for image-to-text heads\u2014then applies\
  \ adaptive scaling to suppress hallucination-promoting heads and enhance grounding-promoting\
  \ heads."
---

# Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats

## Quick Facts
- arXiv ID: 2511.17254
- Source URL: https://arxiv.org/abs/2511.17254
- Reference count: 40
- Intervenes on specific attention heads to mitigate hallucinations in LVLMs across discriminative and generative formats

## Executive Summary
This paper introduces a training-free intervention framework that mitigates hallucinations in Large Vision-Language Models by targeting specific attention heads across both image-to-text and text-to-text causal pathways. The method identifies critical heads using novel metrics—Log Probability Increase for text-to-text heads and image attention concentration for image-to-text heads—then applies adaptive scaling to suppress hallucination-promoting heads and enhance grounding-promoting heads. Experiments demonstrate consistent hallucination reduction across multiple benchmarks, with improvements of 2.1–5.2% F1 on POPE, 2.6% on CHAIR, and up to 18.3 points reduction in hallucination metrics.

## Method Summary
The approach operates through three key steps: First, it identifies hallucination-promoting text-to-text (T2T) heads using Log Probability Increase scores that measure each head's differential contribution to correct versus hallucinated tokens. Second, it identifies image-to-text (I2T) heads by measuring attention concentration on semantically aligned image regions. Third, during inference, it applies adaptive scaling—suppressing negative T2T heads (γ− = 0.0) and enhancing positive heads (γ+ = 2.0)—to modify the model's attention outputs without retraining. The method is format-adaptive, using different head selection thresholds for discriminative versus generative tasks.

## Key Results
- 2.1–5.2% F1 improvement on POPE across COCO, A-OKVQA, and GQA splits
- 2.6% F1 improvement on CHAIR benchmark
- Up to 18.3 points reduction in hallucination metrics on CHAIR
- Effective across discriminative (POPE) and generative (CHAIR) formats
- Consistently outperforms baselines including PAI and GFA

## Why This Works (Mechanism)

### Mechanism 1: Differential Head Contribution (T2T)
Hallucinations are concentrated in specific attention heads that disproportionately increase log-probability of incorrect tokens. By calculating Log Probability Increase (LPI) scores comparing head contributions to correct versus hallucinated tokens, these "hallucination promoters" can be isolated and suppressed. This assumes hallucinations are mechanistically localized rather than distributed globally across the network.

### Mechanism 2: Visual Grounding Density (I2T)
Effective visual grounding requires attention heads that focus on semantically relevant image regions while ignoring background noise. Heads are identified by measuring the ratio of attention on aligned regions versus total image attention, with intervention applied to enhance heads showing strong semantic focus. This assumes attention density on aligned regions reliably proxies for grounding quality.

### Mechanism 3: Format-Dependent Pathway Utilization
LVLMs adaptively select different causal pathways based on question format—image-to-input for discriminative tasks like POPE, image-to-output for generative tasks like CHAIR. Unified intervention requires adapting to these format-specific pathway dependencies rather than applying a one-size-fits-all approach.

## Foundational Learning

- **Multi-Head Attention (MHA) Granularity**: Understanding that attention is a sum of independent heads is crucial, as the method intervenes at the head level (l,n) rather than layer or neuron level. Quick check: If you zero out one attention head in a standard Transformer, does the model crash? (Answer: No, output is still valid).

- **Causal Tracing in Transformers**: The distinction between "Image-to-Input-Text" and "Image-to-Output-Text" requires understanding the causal mask that limits token t to only see tokens <t. Quick check: Can output token at step t attend to input text token at step t+1? (Answer: No, strict causality).

- **Training-Free Intervention**: This method modifies activations during inference without requiring backward passes or gradient updates, contrasting with fine-tuning approaches. Quick check: Does this method require a backward pass or gradient updates? (Answer: No, it modifies the forward pass only).

## Architecture Onboarding

- **Component map**: Image (Vision Encoder) + Text (Embeddings) -> Probing (forward pass with calibration data) -> Scorers (LPI Calculator + Attention Density Calculator) -> Intervention (multiplier λ applied to head outputs)

- **Critical path**: 1) Probe: Run vanilla model on calibration data to identify head sets Z+ and Z-. 2) Inject: During inference, modify hidden state update with scaled head outputs. 3) Scale: Apply γ+ ≈ 2.0 to grounding heads and γ− ≈ 0.0 to hallucination heads.

- **Design tradeoffs**: Hyperparameters (ξ, ζ) balance between selecting enough heads to capture hallucination sources without suppressing useful linguistic priors. Static head identification assumes roles don't shift drastically across samples.

- **Failure signatures**: Excessive suppression (γ− ≈ 0) may cause generic/empty responses rather than detailed descriptions. Mis-identification of heads can cause correlation drops when applied to wrong formats.

- **First 3 experiments**: 1) Path Ablation: Run AllPath with T2T-only and I2T-only interventions to verify both contribute to final F1. 2) Correlation Check: Visualize head rankings for POPE vs CHAIR to confirm heads are not transferable across formats. 3) Qualitative Inspection: Visualize attention maps of top-scoring heads to ensure they ground objects rather than finding image noise.

## Open Questions the Paper Calls Out

- **Training strategies**: The paper does not explore training strategies that could systematically improve the model's internal utilization of causal pathways (image-to-text vs. text-to-text).

- **Pathway mapping**: The authors do not investigate what causal pathways are most effective for different types of questions, leaving the optimal mapping between question types and intervention configurations unknown.

- **Subtle hallucinations**: The analysis is constrained by existing benchmarks which may not capture a broader range of unrecognized or subtle hallucinations beyond object existence.

## Limitations

- Head identification generalizability is limited, with strong performance on COCO-based datasets but minimal cross-dataset validation on non-COCO domains
- While described as "training-free," the method requires calibration data to identify heads, with unclear requirements for annotations, data volume, and cross-architecture transferability
- The static intervention assumption may not hold for highly diverse inputs or novel scenarios where head roles could shift

## Confidence

**High Confidence**: Hallucinations are mechanistically localized to specific attention heads, validated by ablation studies showing independent contributions from T2T and I2T interventions.

**Medium Confidence**: Specific head identification metrics (LPI, attention concentration) are empirically validated but require further testing across different LVLM architectures and datasets.

**Low Confidence**: The claim of being "unified" across all alignment formats is limited by requiring separate head identification procedures for different formats.

## Next Checks

1. **Cross-dataset robustness test**: Apply head sets identified from COCO calibration to completely different visual domains (medical imaging, satellite imagery, or abstract art) to verify generalizability.

2. **Architectural transfer validation**: Apply head sets from LLaVA-v1.5-7B to other attention-based LVLMs without re-identification to test architecture invariance.

3. **Scaling factor sensitivity analysis**: Systematically vary γ+ and γ− across a wider range to identify optimal values that maximize the F1-to-hallucination reduction ratio.