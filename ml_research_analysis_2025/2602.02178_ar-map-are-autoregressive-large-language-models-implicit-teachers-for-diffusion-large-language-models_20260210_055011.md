---
ver: rpa2
title: 'AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion
  Large Language Models?'
arxiv_id: '2602.02178'
source_url: https://arxiv.org/abs/2602.02178
tags:
- arxiv
- diffusion
- language
- ar-map
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AR-MAP, a transfer learning framework that
  leverages preference-aligned autoregressive large language models (AR-LLMs) as implicit
  teachers for aligning diffusion large language models (DLLMs). The method exploits
  the shared architectural structure between AR-LLMs and DLLMs by computing task vectors
  from fine-tuned weight differences and scaling them according to reward modeling
  fit.
---

# AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?

## Quick Facts
- arXiv ID: 2602.02178
- Source URL: https://arxiv.org/abs/2602.02178
- Reference count: 40
- Key outcome: AR-MAP achieves 69.08% average score across 6 preference alignment tasks by transferring alignment knowledge from AR-LLMs to DLLMs via scaled weight addition.

## Executive Summary
AR-MAP introduces a transfer learning framework that leverages preference-aligned autoregressive large language models (AR-LLMs) as implicit teachers for aligning diffusion large language models (DLLMs). The method exploits the shared architectural structure between AR-LLMs and DLLMs by computing task vectors from fine-tuned weight differences and scaling them according to reward modeling fit. This approach circumvents the high variance and computational overhead of direct DLLM alignment. Comprehensive experiments across six diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods.

## Method Summary
AR-MAP transfers preference alignment knowledge from AR-LLMs to DLLMs through a weight scaling approach. First, an AR-LLM is trained with direct preference optimization (DPO) on preference pairs. The task vector τ_pref is extracted as the weight difference between the aligned and base AR-LLM. For a target DLLM, the method adds a scaled version of τ_pref to the DLLM weights: W_final = W_DLLM + γ·τ_pref. The scaling factor γ is determined through a reward-based search algorithm that maximizes pairwise reward accuracy on a validation batch. This exploits the shared architectural structure between AR-LLMs and DLLMs while avoiding the high variance of direct DLLM alignment.

## Key Results
- AR-MAP achieves an average score of 69.08% across six preference alignment tasks and three model combinations
- The method outperforms or matches existing DLLM-specific alignment methods like VRPO and d2-stepMerge
- Reward-based scaling search identifies optimal γ values that vary significantly across tasks (ranging from 1-5)
- Spectral analysis reveals that τ_diffusion magnitudes are approximately 10× larger than τ_pref, necessitating aggressive scaling

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DLLMs can absorb preference alignment knowledge from homologous AR-LLMs through weight transfer.
- Mechanism: Task vectors (τ_pref = W_aligned_AR - W_AR) computed from AR-LLM DPO training are added to DLLM weights via scaled addition: W_aligned_DLLM ≈ W_AR + τ_diffusion + γ·τ_pref. This exploits shared architecture between AR and diffusion models.
- Core assumption: AR-LLMs and DLLMs derived from the same base share a common parameter space where alignment knowledge is additive.
- Evidence anchors:
  - [abstract] "DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms."
  - [section 3.1, Table 1] Shows identical architecture parameters (head num, hidden size, layers) between AR-LLMs and their DLLM counterparts obtained via continuous pre-training.
  - [corpus] Neighbor papers confirm shared architectural premises in DLLM research (DMark, Diffusion Forcing) but do not test cross-paradigm weight transfer.
- Break condition: Architectural divergence between AR base and DLLM (different layer counts, attention configurations) would likely invalidate direct weight mapping.

### Mechanism 2
- Claim: Preference task vectors require aggressive scaling (γ > 1) to overcome spectral shadowing by diffusion adaptation weights.
- Mechanism: SVD analysis reveals ∥τ_diffusion∥₂ ≫ ∥τ_pref∥₂, causing alignment signals to be "trapped" in an ε-neighborhood of diffusion weights. Scaling factor γ amplifies the signal-to-noise ratio. Weyl's inequality provides theoretical bounds.
- Core assumption: The magnitude disparity between diffusion and preference vectors is systematic, not noise.
- Evidence anchors:
  - [section 3.2, Figure 2] "Singular value spectrum in one MLP layer" shows ~10× magnitude difference between diffusion and preference max singular values.
  - [section 3.2, Proposition 3.1] Formal derivation of the spectral shadowing bound: ∥τ_diffusion + γτ_pref∥₂ ≤ (1 + γε)∥τ_diffusion∥₂.
  - [corpus] No direct corpus evidence for spectral analysis in cross-paradigm transfer; this appears novel to AR-MAP.
- Break condition: If preference and diffusion vectors had comparable spectral norms, scaling would cause over-amplification and degradation.

### Mechanism 3
- Claim: Batch reward accuracy maximization identifies the optimal global scaling factor without test-set leakage.
- Mechanism: The implicit reward r_γ(x,y) = log(π_γ(y|x)/π_ref(y|x)) is computed for preference pairs (y_w, y_l). The scaling factor ̂γ maximizes pairwise discrimination accuracy: ̂γ = argmax_γ (1/|B|) Σ I[r_γ(x, y_w) > r_γ(x, y_l)].
- Core assumption: Reward accuracy on a training batch correlates with downstream task performance.
- Evidence anchors:
  - [section 3.3, Algorithm 1] Two-phase search: coarse search (γ += 2 until accuracy drops), then fine-grained check.
  - [section 4.3, Figure 5] Shows accuracy curves peaking at different γ values across models/tasks (e.g., SDAR peaks at γ≈3 for helpfulness).
  - [corpus] Corpus papers on DLLM alignment (VRPO, d2-stepMerge referenced in intro) use ELBO-based approaches, not reward-based scaling search.
- Break condition: Non-monotonic reward accuracy landscapes could cause premature stopping; batch sampling bias could misidentify optimal γ.

## Foundational Learning

- Concept: **Task Vectors / Model Arithmetic**
  - Why needed here: AR-MAP builds directly on the intuition that weight differences encode task-specific knowledge transferable via linear operations.
  - Quick check question: Given base model W₀ and fine-tuned W₁, what does (W₁ - W₀) represent, and how would you combine two task vectors?

- Concept: **Evidence Lower Bound (ELBO) in Diffusion Models**
  - Why needed here: The paper's motivation stems from ELBO's high variance in DLLMs; understanding why direct DLLM alignment is hard contextualizes the AR-teacher workaround.
  - Quick check question: Why does ELBO estimation in DLLMs require "doubly stochastic" approximation, and what failure mode does this introduce?

- Concept: **Singular Value Decomposition for Weight Analysis**
  - Why needed here: AR-MAP uses SVD to diagnose the spectral gap between τ_diffusion and τ_pref, motivating the scaling law.
  - Quick check question: If a weight matrix has singular values [10, 1, 0.1], what does the condition number tell you about its sensitivity to perturbations?

## Architecture Onboarding

- Component map: AR-LLM Training Module -> Task Vector Extraction -> DLLM Base -> Scaling Search -> Merge Step
- Critical path:
  1. Verify AR-LLM and DLLM share architecture (heads, hidden dim, layers)
  2. Train AR-LLM with DPO on preference data
  3. Extract τ_pref (LoRA or full weight delta)
  4. Run reward-based search (Algorithm 1) on 4096-sample batch
  5. Apply merge with discovered ̂γ
- Design tradeoffs:
  - **LoRA vs. full-weight transfer**: Paper uses LoRA (rank=16) for efficiency; full-weight may capture more alignment signal but increases merge complexity.
  - **Global vs. layer-wise scaling**: Paper uses single global γ; per-layer adaptive scaling (mentioned in Future Work) could improve but adds hyperparameter search cost.
  - **Batch size for search**: 4096 samples used; smaller batches may be noisier, larger increases compute.
- Failure signatures:
  - **Overscaling collapse**: γ > 4-5 causes reasoning tasks (GSM8K, MATH) to catastrophically degrade (Table 3 shows SDAR collapses at γ > 2 for math).
  - **Architecture mismatch**: Attempting transfer between non-homologous models (e.g., Llama → Dream) will fail due to shape mismatches.
  - **Reward accuracy plateau**: If accuracy doesn't peak within search range, may indicate insufficient scaling or incompatible task vectors.
- First 3 experiments:
  1. **Sanity check**: Verify homologous architecture by comparing parameter shapes between AR-LLM and target DLLM. Mismatch → stop.
  2. **Baseline scaling**: Apply τ_pref with γ=1 (no scaling). Expect suboptimal performance per spectral shadowing analysis.
  3. **Search validation**: Run Algorithm 1 on held-out batch subset; compare discovered ̂γ against grid search over γ ∈ {1,2,3,4,5}. Convergence confirms search reliability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-grained, parameter-wise adaptive scaling strategies improve alignment transfer compared to the uniform global scaling factor γ used in AR-MAP?
- Basis in paper: [explicit] Appendix A states: "using a uniform scaling factor γ across all parameters may not capture the nuanced absorption needs of distinct model components. Future work will focus on developing fine-grained adaptive merging strategies."
- Why unresolved: The current method uses a single scalar for all weights, but Table 3 shows different tasks (e.g., math vs. open-ended generation) have vastly different optimal γ values, suggesting parameter-level heterogeneity.
- What evidence would resolve it: Experiments with layer-wise or parameter-group-specific scaling factors demonstrating performance gains over uniform scaling.

### Open Question 2
- Question: Does AR-MAP generalize to DLLMs that are not derived via continuous pre-training from AR-LLMs (e.g., models trained from scratch like LLaDA)?
- Basis in paper: [inferred] The paper only evaluates on Dream-7B and SDAR-8B, both obtained via CPT from Qwen models. Table 1 shows architectural compatibility, but no experiments on non-homologous DLLMs.
- Why unresolved: The weight transfer relies on shared architectural structure between AR and DLLM weights. Models trained from scratch lack this shared initialization, potentially breaking the task vector assumption.
- What evidence would resolve it: Applying AR-MAP to LLaDA or other scratch-trained DLLMs and measuring transfer effectiveness.

### Open Question 3
- Question: Can the weight transfer paradigm extend beyond preference alignment to other capabilities such as reasoning, domain knowledge, or multilingual skills?
- Basis in paper: [inferred] The framework is demonstrated only for preference alignment tasks (helpfulness, truthfulness, math). The theoretical motivation (Equation 6) suggests generality, but this remains untested.
- Why unresolved: The spectral shadowing analysis (Figure 2) is specific to preference task vectors. Different capability types may have different spectral properties affecting transfer requirements.
- What evidence would resolve it: Experiments transferring task vectors for diverse capabilities (e.g., code generation, factual knowledge) from AR-LLMs to DLLMs.

## Limitations
- The method assumes architectural homology between AR-LLMs and DLLMs, limiting generalizability to arbitrary architecture pairs
- The reward-based scaling search may converge to local optima depending on batch composition and landscape topology
- The paper demonstrates performance gains but lacks interpretability of what alignment knowledge is actually transferred

## Confidence
- **High confidence**: The architectural homology verification (shared parameters between homologous models) and the reward-based scaling search algorithm are well-specified and reproducible.
- **Medium confidence**: The SVD-based spectral shadowing analysis and its implications for scaling factors are theoretically grounded but rely on specific empirical observations that may not generalize.
- **Medium confidence**: The aggregate performance improvement (69.08% average score) is robust across tasks, but individual task results show sensitivity to scaling factors and model combinations.

## Next Checks
1. **Architectural generalization test**: Apply AR-MAP to a pair of AR-LLM and DLLM with deliberately modified architectural parameters (e.g., different layer counts or attention configurations) to quantify the method's sensitivity to architectural homology.
2. **Layer-wise scaling analysis**: Replace the global scaling factor γ with per-layer scaling to determine whether the spectral shadowing effect varies across network depths and whether localized adaptation improves performance.
3. **Knowledge attribution study**: Conduct ablation experiments to isolate which components of τ_pref (attention vs. MLP weights, different rank values in LoRA) contribute most to performance gains, providing interpretability of the transferred alignment knowledge.