---
ver: rpa2
title: 'Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and
  Proprietary Models'
arxiv_id: '2502.15155'
source_url: https://arxiv.org/abs/2502.15155
tags:
- speech
- extreme
- llama
- hate
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses extreme speech classification by evaluating
  Large Language Models (LLMs) on the Indian subset of the Xtreme Speech Dataset.
  The authors compare zero-shot inference, fine-tuning, preference optimization, and
  ensembling approaches using open-source Llama models (1B to 70B parameters) and
  closed-source GPT models.
---

# Extreme Speech Classification in the Era of LLMs: Exploring Open-Source and Proprietary Models

## Quick Facts
- arXiv ID: 2502.15155
- Source URL: https://arxiv.org/abs/2502.15155
- Reference count: 36
- Primary result: Fine-tuned Llama models (even 3B) outperform previous SOTA methods and approach GPT performance

## Executive Summary
This paper evaluates Large Language Models for extreme speech classification using the Indian subset of the Xtreme Speech Dataset. The authors compare zero-shot inference, fine-tuning, preference optimization, and ensembling across open-source Llama models (1B to 70B) and closed-source GPT models. Fine-tuning significantly improves performance across all models, with smaller fine-tuned Llama models outperforming previous state-of-the-art methods. While GPT models slightly outperform Llama in zero-shot settings, this gap disappears after fine-tuning. The study demonstrates that open-source models can match or approach closed-source performance when properly fine-tuned for domain-specific tasks.

## Method Summary
The study uses the Indian subset of the Xtreme Speech Dataset (4,933 samples) split into train (64%), DPO/ensemble buffer (16%), and test (20%) with stratified sampling. Zero-shot inference uses Chain-of-Thought prompting with Llama models (4-bit quantized) and GPT-4o API. Supervised Fine-Tuning (SFT) employs label-only training on Llama models. DPO is tested on SFT-ed Llama 3.1 8B using the held-out buffer. Ensembling combines fine-tuned models via F1-weighted and probability-weighted voting. All models classify extreme speech into three categories: derogatory (0), exclusionary (1), and dangerous (2).

## Key Results
- Fine-tuning boosts F1-macro from ~52 to ~72 across all models
- Fine-tuned Llama 3.1 3B (71.65 F1) matches or exceeds zero-shot GPT-4o-mini (52.20 F1)
- Larger models show better zero-shot performance but converge after fine-tuning
- Exclusionary speech remains the hardest category (F1: 10.17-56.06)
- DPO and ensembling provide no additional performance gains over SFT alone

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning with domain-specific data significantly improves extreme speech classification by enabling cultural and contextual adaptation. Domain-specific fine-tuning exposes the model to labeled examples that encode sociocultural patterns, allowing the model to recalibrate its decision boundaries for the target context (Indian extreme speech categories). The pre-trained representations provide a foundation, while supervised fine-tuning aligns the output layer with task-specific label semantics. This works because the pre-trained LLM has sufficient representational capacity to capture the relevant linguistic and cultural patterns after exposure to domain examples. Break condition: If the domain-specific training data is too small, mislabeled, or lacks diversity, fine-tuning may overfit or fail to generalize to test examples.

### Mechanism 2
Chain-of-thought prompting (generating justifications before labels) improves zero-shot classification performance by decomposing reasoning. Intermediate reasoning steps allow the model to explicitly surface contextual interpretations before committing to a label, reducing premature classification errors. This aligns with the observation that zero-shot inference benefits from justification generation. This works because the model has sufficient reasoning capacity in its pre-trained weights to generate meaningful justifications without supervision. Break condition: If the prompt design elicits unfaithful or hallucinated justifications, the intermediate reasoning may mislead rather than guide the final classification.

### Mechanism 3
Label-only fine-tuning outperforms justification-included fine-tuning because the loss function is directly optimized for classification accuracy when constrained to label outputs. When justifications are included during SFT, the model must distribute capacity across generating coherent reasoning and correct labels. The cross-entropy loss is computed over all tokens, diluting the gradient signal for the classification decision. Label-only training concentrates the loss on the label tokens, directly minimizing classification error. This works because the model's pre-trained reasoning capabilities are sufficient; explicit justification during training adds noise rather than signal. Break condition: For tasks where interpretability is critical for deployment, label-only training sacrifices explainability even if accuracy improves.

## Foundational Learning

- **Concept:** Zero-shot vs. fine-tuned inference paradigms
  - Why needed here: The paper directly compares these approaches, showing that zero-shot GPT-4o outperforms zero-shot Llama, but fine-tuned Llama closes the gap. Understanding when each is appropriate is critical for system design.
  - Quick check question: Given a limited labeling budget and a proprietary model API cost constraint, would you prioritize zero-shot prompting with CoT or invest in fine-tuning a smaller open-source model?

- **Concept:** Direct Preference Optimization (DPO) vs. Supervised Fine-Tuning (SFT)
  - Why needed here: The paper tested DPO and found no gains over SFT for classification. Understanding why prevents wasted experimentation effort.
  - Quick check question: Why might DPO, designed for aligning generative outputs with human preferences, fail to improve discriminative classification tasks?

- **Concept:** Ensemble diversity requirements
  - Why needed here: The paper's ensemble experiments failed because fine-tuned models exhibited homogeneous error patterns. This illustrates a common failure mode in ensemble design.
  - Quick check question: If you fine-tune multiple models on the same dataset with the same objective, why would you expect their error patterns to be similar?

## Architecture Onboarding

- **Component map:**
  Raw text from social media -> prompt template (with or without CoT instruction) -> Llama family (1B–70B parameters, 4-bit quantized) or GPT-4o API -> token generation constrained to class labels (0/1/2) -> SFT on labeled examples -> optional DPO post-hoc -> optional ensemble inference -> F1-macro evaluation

- **Critical path:**
  1. Dataset preparation: Split into train (64%), DPO/ensemble buffer (16%), test (20%) with stratified sampling
  2. Zero-shot baseline: Run inference with CoT prompting on test set
  3. SFT: Fine-tune with label-only objective on training set
  4. Evaluate on test set using F1-macro per class

- **Design tradeoffs:**
  - Open-source (Llama) vs. closed-source (GPT): Open-source enables fine-tuning and full control; closed-source offers stronger zero-shot but no weight access
  - Model size vs. inference cost: Larger models (70B) perform better zero-shot but require significant compute; after fine-tuning, smaller models (1B–3B) achieve comparable performance
  - Justification generation: Improves zero-shot but degrades SFT—must choose based on deployment context

- **Failure signatures:**
  - Zero-shot exclusionary speech detection consistently underperforms across all models (F1: 10.17–36.61), indicating the category is inherently ambiguous or underspecified
  - DPO yields no improvement or slight degradation—loss function mismatch with discriminative task
  - Ensembling provides no gain—error pattern homogeneity among fine-tuned models

- **First 3 experiments:**
  1. Establish zero-shot baselines with CoT prompting across model sizes; record per-class F1 to identify systematic weaknesses (exclusionary speech expected to be lowest)
  2. Fine-tune smallest viable Llama model (3.2 3B) with label-only SFT; compare against zero-shot GPT-4o-mini benchmarks (F1-macro ~52.20 vs fine-tuned ~71.65)
  3. Analyze confusion matrix on the held-out ensemble buffer before attempting ensemble methods; if error patterns are highly correlated across models, skip ensemble development

## Open Questions the Paper Calls Out

### Open Question 1
Can larger models (e.g., Llama 3.1 405B) coupled with Odds Ratio Preference Optimization (ORPO) surpass the performance of current fine-tuned benchmarks? The study was restricted to models up to 70B parameters and used SFT/DPO; the specific combination of massive scale and ORPO remains untested for this task. Benchmarking Llama 3.1 405B fine-tuned with ORPO on the Indian Xtreme Speech dataset against the current Llama 3.1 8B baseline would resolve this.

### Open Question 2
Why does the inclusion of justifications (Chain-of-Thought) degrade performance during Supervised Fine-Tuning (SFT) despite improving zero-shot inference? Section 4.2 notes that the SFT variant with justifications "underperformed compared to the label-only approach," speculating that the loss function fails to optimize for classification accuracy when reasoning is included. An ablation study modifying the SFT loss function to weight the classification token higher than the reasoning tokens, or using a multi-task objective, would resolve this.

### Open Question 3
Can diversity-inducing techniques enable ensembling of LLMs to outperform individual fine-tuned models for extreme speech detection? Section 5.3 explains that ensembling failed because fine-tuned models exhibited "homogeneity" in error patterns, limiting the ensemble's ability to correct mistakes. Training ensemble members on distinct data partitions or using adversarial training to force diverse error distributions, then measuring ensemble F1-macro scores, would resolve this.

## Limitations
- The study was limited to models up to 70B parameters due to computational constraints
- DPO and ensembling techniques showed no performance gains over standard SFT
- Exclusionary speech classification remains challenging across all approaches with F1 scores below 60%

## Confidence

**High Confidence:**
- Fine-tuning significantly improves performance across all model sizes and types
- GPT models slightly outperform Llama in zero-shot settings but converge after fine-tuning
- Label-only SFT outperforms SFT with justifications

**Medium Confidence:**
- Smaller fine-tuned models can match zero-shot GPT performance
- Exclusionary speech is inherently harder to classify

**Low Confidence:**
- DPO and ensembling would never work for this task with any modifications
- Larger models (405B) would definitively outperform current benchmarks

## Next Checks
1. Verify that the Indian subset filtering and stratified splits are correctly implemented before running experiments
2. Test label extraction from zero-shot CoT outputs with multiple prompt variations to ensure consistent formatting
3. Run ablation on SFT loss function by weighting classification tokens higher when justifications are included