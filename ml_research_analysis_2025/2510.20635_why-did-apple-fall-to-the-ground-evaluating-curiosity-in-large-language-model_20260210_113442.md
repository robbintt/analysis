---
ver: rpa2
title: 'Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model'
arxiv_id: '2510.20635'
source_url: https://arxiv.org/abs/2510.20635
tags:
- curiosity
- llms
- xiao
- learning
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper evaluates the curiosity of large language models (LLMs)
  by designing a systematic assessment framework based on human curiosity theories.
  It combines questionnaire studies, behavioral experiments, and curiosity-driven
  learning evaluations across three dimensions: information seeking, thrill seeking,
  and social curiosity.'
---

# Why Did Apple Fall To The Ground: Evaluating Curiosity In Large Language Model

## Quick Facts
- **arXiv ID**: 2510.20635
- **Source URL**: https://arxiv.org/abs/2510.20635
- **Authors**: Haoyu Wang; Sihang Jiang; Yuyan Chen; Yitong Wang; Yanghua Xiao
- **Reference count**: 40
- **Primary result**: This paper evaluates the curiosity of large language models (LLMs) by designing a systematic assessment framework based on human curiosity theories. It combines questionnaire studies, behavioral experiments, and curiosity-driven learning evaluations across three dimensions: information seeking, thrill seeking, and social curiosity. Results show that LLMs generally exhibit stronger curiosity than humans, especially in knowledge acquisition, but tend to be more conservative when facing uncertainty. Furthermore, curiosity-driven questioning and thinking significantly enhance LLMs' reasoning and active learning abilities.

## Executive Summary
This paper presents a comprehensive framework for evaluating curiosity in large language models by integrating human curiosity theories with systematic assessment methods. The researchers developed a multi-dimensional evaluation approach that combines questionnaire studies, behavioral experiments, and learning tasks to measure information seeking, thrill seeking, and social curiosity in LLMs. The study reveals that while LLMs demonstrate stronger curiosity than humans in knowledge acquisition, they show more conservative behavior when dealing with uncertainty. The research also demonstrates that curiosity-driven questioning and thinking significantly improve LLMs' reasoning capabilities and active learning performance.

## Method Summary
The researchers designed a systematic assessment framework for evaluating LLM curiosity based on established human curiosity theories. They employed three complementary methodologies: questionnaire studies to capture subjective curiosity patterns, behavioral experiments to observe curiosity-driven actions, and curiosity-driven learning evaluations to measure the impact of curiosity on knowledge acquisition. The framework specifically examines three dimensions of curiosity - information seeking, thrill seeking, and social curiosity - providing a comprehensive view of how LLMs engage with novel information and uncertain situations. The evaluation was conducted across multiple LLM architectures to ensure broad applicability of the findings.

## Key Results
- LLMs generally exhibit stronger curiosity than humans, particularly in knowledge acquisition tasks
- LLMs demonstrate conservative behavior when facing uncertainty, suggesting limitations in handling ambiguous situations
- Curiosity-driven questioning and thinking significantly enhance LLMs' reasoning abilities and active learning performance

## Why This Works (Mechanism)
The evaluation framework works by translating human curiosity theories into measurable LLM behaviors through systematic assessment protocols. By combining subjective questionnaire data with objective behavioral observations and learning task performance, the framework captures both the qualitative and quantitative aspects of curiosity. The three-dimensional approach (information seeking, thrill seeking, social curiosity) provides a comprehensive mapping of curiosity manifestations in LLMs, while the integration of learning evaluations demonstrates the practical impact of curiosity on model performance.

## Foundational Learning
- **Human curiosity theories**: Understanding established psychological frameworks for measuring curiosity is essential for developing valid LLM evaluation metrics. Quick check: Review major curiosity theories (Berlyne's optimal arousal, Loewenstein's information-gap theory, Kashdan's multidimensional curiosity framework).
- **Curiosity dimensions**: Knowledge of the three primary curiosity dimensions (information seeking, thrill seeking, social curiosity) is needed to design comprehensive assessment protocols. Quick check: Map each dimension to specific LLM behavioral indicators.
- **Active learning principles**: Understanding how curiosity drives knowledge acquisition helps interpret the relationship between curiosity and learning performance. Quick check: Examine the connection between curiosity-induced questioning and knowledge retention.
- **Behavioral assessment methods**: Familiarity with experimental design for measuring curiosity behaviors in AI systems is crucial for proper evaluation. Quick check: Review protocols for measuring information-seeking behaviors in LLMs.
- **Uncertainty handling in LLMs**: Understanding how models process ambiguous or uncertain information is key to interpreting conservative curiosity responses. Quick check: Analyze model confidence scores in uncertain scenarios.
- **Questionnaire design for AI**: Knowledge of how to adapt human assessment tools for AI evaluation ensures valid comparative analysis. Quick check: Validate questionnaire items against LLM response patterns.

## Architecture Onboarding

**Component Map**: Questionnaire Studies -> Behavioral Experiments -> Learning Evaluations -> Curiosity Assessment Framework

**Critical Path**: Human curiosity theory adaptation → Questionnaire design → Behavioral experiment setup → Learning task implementation → Comparative analysis → Framework validation

**Design Tradeoffs**: The framework prioritizes comprehensive dimension coverage over single-dimension depth, accepting increased complexity for more holistic curiosity assessment. The use of multiple evaluation methods balances subjective and objective measurements but requires careful normalization of results across different data types.

**Failure Signatures**: 
- Inconsistent curiosity patterns across different dimensions may indicate framework calibration issues
- Poor correlation between questionnaire responses and behavioral observations suggests measurement validity problems
- Lack of improvement in learning tasks despite high curiosity scores indicates potential misalignment between curiosity measurement and learning outcomes

**First Experiments**:
1. Conduct initial questionnaire validation with a small LLM subset to verify question interpretability
2. Run pilot behavioral experiments with controlled uncertainty levels to establish baseline responses
3. Implement curiosity-driven learning tasks with simplified reasoning problems to test performance impact

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that LLMs exhibit stronger curiosity than humans relies on specific experimental conditions that may not generalize across all domains or task types
- Conservative behavior when facing uncertainty requires clarification on what constitutes uncertainty in the evaluation metrics
- The causal relationship between curiosity induction and performance improvement needs further isolation from confounding factors such as prompt engineering effects

## Confidence
- **LLMs exhibit stronger curiosity than humans**: Medium confidence - depends on experimental conditions and domain specificity
- **Conservative behavior in uncertainty**: Medium confidence - requires clarification of uncertainty metrics and model training biases
- **Curiosity-driven reasoning enhancement**: Medium confidence - needs isolation from confounding factors like prompt engineering

## Next Checks
1. Test the framework across diverse LLM architectures beyond the current selection to verify generalizability of curiosity patterns
2. Implement longitudinal studies to assess whether curiosity-driven learning effects persist across multiple training epochs rather than single-task evaluations
3. Conduct cross-cultural validation of the human baseline measurements to ensure the comparative framework accounts for cultural variations in curiosity expression