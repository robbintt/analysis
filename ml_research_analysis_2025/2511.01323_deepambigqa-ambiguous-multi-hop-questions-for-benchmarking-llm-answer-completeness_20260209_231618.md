---
ver: rpa2
title: 'DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness'
arxiv_id: '2511.01323'
source_url: https://arxiv.org/abs/2511.01323
tags:
- question
- reasoning
- answer
- query
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEEPAMBIGQA addresses the challenge of evaluating large language
  models on complex, ambiguous multi-hop questions that require both entity disambiguation
  and comprehensive evidence retrieval. The authors introduce DEEPAMBIGQAGEN, an automatic
  pipeline that synthesizes such questions using knowledge graphs aligned with text
  corpora, generating structured reasoning plans that can be executed to produce verifiable
  answers.
---

# DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness

## Quick Facts
- **arXiv ID:** 2511.01323
- **Source URL:** https://arxiv.org/abs/2511.01323
- **Reference count:** 40
- **Key outcome:** Introduces a benchmark and automatic generation pipeline for ambiguous multi-hop questions, revealing that even state-of-the-art models struggle with answer completeness, achieving only 0.13 EM on ambiguous questions versus 0.21 on non-ambiguous ones.

## Executive Summary
DEEPAMBIGQA addresses the challenge of evaluating large language models on complex, ambiguous multi-hop questions that require both entity disambiguation and comprehensive evidence retrieval. The authors introduce DEEPAMBIGQAGEN, an automatic pipeline that synthesizes such questions using knowledge graphs aligned with text corpora, generating structured reasoning plans that can be executed to produce verifiable answers. DEEPAMBIGQA, the resulting dataset, contains 3,600 questions—half with explicit name ambiguity leading to multiple valid reasoning paths—and requires at least two reasoning steps, with some involving up to eight. Experiments show that even state-of-the-art models like GPT-5 struggle with answer completeness, achieving only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous ones, highlighting the need for more robust QA systems focused on information gathering and answer completeness.

## Method Summary
The paper introduces DeepAmbigQAGen, an automated pipeline for generating ambiguous multi-hop questions. It uses Wikipedia text and Wikidata as source data, constructing executable reasoning plans through 7 operation types (Atomic, Join, Filter, Union, Difference, Intersection, GroupBy). These plans are then translated into natural language questions. The evaluation uses composite metrics (Precision, Recall, Exact Match) over entity answer sets, aggregating across all reasoning branches for ambiguous questions. The approach leverages LLMs (specifically GPT-5 variants) for entity selection and question translation, with heuristic filtering and validation steps.

## Key Results
- GPT-5 achieves only 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous ones
- Precision remains high (typically above 70%) while recall and exact match are much lower
- Adding query expansion and evidence extraction modules provides only modest improvements
- 71% of GPT-5's failures stem from incomplete information extraction
- Ambiguous questions require on average 3.3 reasoning branches to resolve

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Grounding question generation in executable Knowledge Graph (KG) operations creates verifiable multi-hop reasoning paths that text-only generation often misses.
- **Mechanism:** DeepAmbigQAGen constructs questions by first building a structured "execution plan" (sequences of Atomic, Join, Filter operations) using Wikidata. It only translates this plan into natural language *after* the logical structure is verified. This inverse process (Logic $\to$ Text rather than Text $\to$ Logic) ensures the resulting question requires genuine reasoning steps (e.g., joins across entities) rather than simple pattern matching.
- **Core assumption:** The translation from the formal execution plan to natural language preserves the logical dependencies and does not "leak" the answer or oversimplify the constraints.
- **Evidence anchors:**
  - [Section 3.2] "DEEPAMBIGQAGEN utilizes the seed data and composes structured execution plans... incrementally builds executable plans that represent typical multi-step reasoning."
  - [Abstract] "generating natural and verifiable questions that systematically embed name ambiguity and multi-step reasoning."
  - [Corpus] Related work like **PGDA-KGQA** supports the efficacy of combining KGs with LLMs for QA generation, though DeepAmbigQA focuses specifically on ambiguity.
- **Break condition:** If the "LLM-based Entity Selection" fails to identify entities that yield non-empty answer sets, the synthesis pipeline must discard the candidate question to maintain data quality.

### Mechanism 2
- **Claim:** Explicitly modeling "name ambiguity" as multiple distinct execution branches forces models to perform exhaustive retrieval rather than settling for a single plausible path.
- **Mechanism:** The pipeline identifies names mapping to multiple entities (e.g., "Heat" $\to$ {1995 film, 1986 film}). It generates a single question but retains a set of distinct execution plans. To be scored as "correct," a system must resolve the ambiguity by executing *all* valid branches and aggregating the results.
- **Core assumption:** The ambiguity is "explicit" (name collision) rather than "implicit" (vague intent), allowing for discrete ground-truth enumeration.
- **Evidence anchors:**
  - [Section 3.2] "For ambiguous names, the same plan can be re-executed with each possible entity to obtain corresponding answers."
  - [Section 4.2] "Ambiguous questions... Average # reasoning branches: 3.3."
  - [Corpus] **DTKG** emphasizes KG-verified reasoning, aligning with the need for structure to handle complexity, but DeepAmbigQA uniquely focuses on the branching factor of entity names.
- **Break condition:** If the model hallucinates a merge of entities (treating distinct entities as one) or ignores branches with fewer search results (popularity bias), exact match scores drop drastically.

### Mechanism 3
- **Claim:** The benchmark reveals a "Completeness Gap" by penalizing partial retrieval, which standard exact-match metrics often miss if they only check for "any" valid answer.
- **Mechanism:** The evaluation uses a composite of Precision, Recall, and Exact Match (EM) over sets of entities. While LLMs maintain high precision (finding correct entities), they fail on recall (finding *all* entities). The low scores of GPT-5 (0.13 EM on ambiguous questions) suggest that current systems optimize for sufficiency ("good enough answer") rather than exhaustiveness.
- **Core assumption:** The ground truth answer sets derived from Wikidata are complete and the "valid" interpretations cover the user's likely intent.
- **Evidence anchors:**
  - [Section 5.2] "Precision remains high (typically above 70%), recall and exact match are much lower... revealing limitations in comprehensive information retrieval."
  - [Section 5.3] "Adding modules such as query expansion... brings only modest improvements and cannot fully address these challenging questions."
- **Break condition:** If a retrieval system fetches the entire corpus for every query, recall would technically be high, but the reasoning model would fail due to context window limits and noise.

## Foundational Learning

- **Concept:** **Multi-hop Entity Join (in KGs)**
  - **Why needed here:** The core difficulty of the dataset is not just finding a fact, but traversing a relation (e.g., "film $\to$ cast") and applying a condition (e.g., "cast $\to$ awards"). You must understand how to chain these lookups.
  - **Quick check question:** If querying "Directors of films starring X," do you retrieve the film first or the director first?

- **Concept:** **Entity Disambiguation (ED)**
  - **Why needed here:** 50% of the dataset relies on the system realizing that a string like "The Island" refers to multiple distinct nodes in the graph, not a single fuzzy concept.
  - **Quick check question:** Does the phrase "Apple" refer to a fruit or a company? How would a search query change based on that distinction?

- **Concept:** **Set-based Answer Evaluation**
  - **Why needed here:** Unlike boolean (right/wrong) or scalar (similarity) metrics, this task requires set operations (Union, Intersection) on the answer list. A model that finds 3 out of 5 correct answers gets partial credit for Precision but fails Exact Match.
  - **Quick check question:** If the answer is {A, B, C} and the model outputs {A, B}, is the Precision 1.0 or 0.66?

## Architecture Onboarding

- **Component map:** Wikipedia (Text Corpus) + Wikidata (Structured Graph) -> DeepAmbigQAGen (builds plans $\to$ translates to questions) -> LLM-based QA System (e.g., GPT-5 with Search Tools) -> Metric calculator (P/R/EM) checking against KG-derived ground truth

- **Critical path:** The most sensitive component is the **Query Synthesis (Step 2)**. It determines the difficulty. If the "Filter" or "Join" operations are too simple, the benchmark fails to stress the model. If they are too complex (too many hops), the generated natural language question may become incoherent.

- **Design tradeoffs:** The authors chose **Automated Generation** over **Human Annotation**. This allows for scale (3,600 questions) and verifiable logic, but may result in questions that sound "synthetic" or lack the nuanced ambiguity of real user queries (which might involve temporal or intent ambiguity, not just name collisions).

- **Failure signatures:**
  - **High Precision, Low Recall:** Model answers correctly but incompletely (e.g., lists 1 actor instead of 5)
  - **Branch Drop:** Model answers one interpretation of an ambiguous query perfectly but ignores the other 2 branches
  - **Hallucinated Constraints:** Model invents a filter condition not present in the question to reduce the search space (e.g., assuming "recent" when not specified)

- **First 3 experiments:**
  1. **Retrieval Ablation:** Run the evaluation with *perfect* retrieval (provide the ground-truth Wikipedia passages directly). If EM is still low, the bottleneck is reasoning, not search.
  2. **Branch Analysis:** Isolate the "Ambiguous" subset and measure performance *per branch*. Do models favor more popular entities (e.g., the 1995 "Heat" over the 1986 one)?
  3. **Module Integration:** Implement the "Query Expansion" module described in 5.3. Does explicit disambiguation ("Heat (1995 film)" vs "Heat (1986 film)") actually improve recall, or does it confuse the retrieval system?

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What architectural or algorithmic improvements can enable LLM-based QA systems to achieve exhaustive retrieval and answer completeness, particularly for questions requiring disambiguation across multiple reasoning paths?
- **Basis in paper:** [explicit] The paper concludes that "enhancing retrieval information coverage remains the dominant challenge" and that current models suffer from "incomplete information extraction" (71% of GPT-5's failures). Query expansion and evidence extraction provide only "modest improvements."
- **Why unresolved:** The paper demonstrates the problem but does not propose or test a solution that fully closes the gap between high precision and low recall/exact match.
- **What evidence would resolve it:** A new system or training methodology that achieves significantly higher recall and exact match on the DeepAmbigQA benchmark (e.g., EM > 0.5) by explicitly targeting information gathering completeness.

### Open Question 2
- **Question:** To what extent does the performance gap on ambiguous vs. non-ambiguous questions in DeepAmbigQA stem from failures in coreference/entity resolution versus failures in multi-branch reasoning integration?
- **Basis in paper:** [inferred] The paper shows a performance drop on ambiguous questions (GPT-5 EM: 0.13 vs. 0.21) but does not isolate the root cause among disambiguation, branch enumeration, or answer aggregation.
- **Why unresolved:** The error analysis collapses multiple causes under "incomplete information extraction," which could include missed branches or poor integration.
- **What evidence would resolve it:** An ablation study or error analysis that separately quantifies errors due to (1) failing to identify ambiguous entities, (2) failing to explore all valid branches, and (3) failing to correctly merge branch-level answers.

### Open Question 3
- **Question:** Can the DeepAmbigQAGen pipeline be generalized to create benchmarks for other types of ambiguity (e.g., temporal, intent-based) or for domains outside Wikipedia/Wikidata?
- **Basis in paper:** [explicit] The limitations section states: "it does not yet cover all potential real-world ambiguities (e.g., temporal or intention ambiguities). Future extensions can broaden the operation types..."
- **Why unresolved:** The current pipeline and operation set are designed for name-entity ambiguity in a KG context.
- **What evidence would resolve it:** A modified generation pipeline successfully producing a high-quality dataset for temporal or intent ambiguity, validated through human evaluation similar to Section 4.3.

### Open Question 4
- **Question:** Why does providing a structured KG query tool (WikiSPARQL) degrade LLM performance, and how can this tool be made more effective for complex, multi-hop QA?
- **Basis in paper:** [explicit] Section 5.3 reports a performance drop when adding a KG tool, noting failures in "generating correct and executable SPARQL queries" (Table 5). The paper concludes that "simply integrating them remains unreliable."
- **Why unresolved:** The paper identifies the problem (incorrect SPARQL generation) but does not explore solutions like better in-context examples, specialized training, or a more robust API abstraction.
- **What evidence would resolve it:** A refined implementation of the KG tool interface (e.g., with a program translator, schema-aware prompting, or error correction) that leads to a measurable improvement in precision and recall compared to the baseline retrieval system.

## Limitations

- The synthetic approach may not fully capture nuanced, context-dependent ambiguity found in real user queries (e.g., temporal or intent ambiguity beyond simple name collisions)
- Reliance on specific model versions (GPT-5, GPT-5-mini, GPT-5-nano) creates reproducibility challenges as these appear to be either internal identifiers or speculative designations
- The exact heuristic filtering rules and SPARQL templates remain partially unspecified, limiting faithful reproduction

## Confidence

- **High confidence**: The claim that LLMs struggle with answer completeness on multi-hop ambiguous questions is well-supported by the 0.13 EM score on ambiguous questions versus 0.21 on non-ambiguous ones. The methodology of using KG-derived ground truth and composite metrics (P/R/EM) is sound.
- **Medium confidence**: The assertion that current systems "optimize for sufficiency rather than exhaustiveness" is reasonable but could benefit from additional experiments isolating reasoning versus retrieval capabilities.
- **Medium confidence**: The effectiveness of the automated generation pipeline is demonstrated, but the exact heuristic filtering rules and SPARQL templates remain partially unspecified, limiting faithful reproduction.

## Next Checks

1. **Retrieval vs. Reasoning Isolation**: Run evaluations with perfect retrieval (ground-truth passages provided) to determine if low EM scores stem from reasoning limitations or retrieval failures.
2. **Branch Popularity Analysis**: Analyze whether models systematically favor more popular entities when resolving ambiguous questions (e.g., 1995 "Heat" over 1986 "Heat").
3. **Module Integration Testing**: Implement and test the "query expansion" module mentioned in Section 5.3 to verify if explicit disambiguation improves recall or introduces additional confusion.