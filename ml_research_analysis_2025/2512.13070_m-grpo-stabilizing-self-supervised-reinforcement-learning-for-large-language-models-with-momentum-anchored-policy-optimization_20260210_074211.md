---
ver: rpa2
title: 'M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language
  Models with Momentum-Anchored Policy Optimization'
arxiv_id: '2512.13070'
source_url: https://arxiv.org/abs/2512.13070
tags:
- policy
- training
- m-grpo
- learning
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a critical instability in self-supervised
  reinforcement learning for large language models, where policies collapse during
  long-horizon training. The authors propose M-GRPO, a momentum-anchored policy optimization
  framework that uses a slowly evolving momentum model to provide stable training
  targets, combined with an IQR-based trajectory entropy filter to prevent premature
  convergence.
---

# M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization

## Quick Facts
- arXiv ID: 2512.13070
- Source URL: https://arxiv.org/abs/2512.13070
- Authors: Bizhe Bai; Hongming Wu; Peng Ye; Tao Chen
- Reference count: 6
- Primary result: M-GRPO prevents policy collapse in self-supervised RL for LLMs, achieving 79.75% accuracy on MATH500 compared to 47.50% for baseline

## Executive Summary
This paper addresses a critical instability in self-supervised reinforcement learning for large language models, where policies tend to collapse during extended training periods. The authors introduce M-GRPO (Momentum-anchored GRPO), a novel framework that stabilizes training by maintaining a slowly evolving momentum model that provides consistent optimization targets. By combining this with an IQR-based trajectory entropy filter, M-GRPO prevents premature convergence while maintaining exploration capabilities. The approach demonstrates significant improvements over standard GRPO across multiple reasoning benchmarks, with particularly striking gains on mathematical problem-solving tasks.

## Method Summary
M-GRPO extends the standard GRPO algorithm by introducing a momentum-anchored policy optimization framework. The core innovation involves maintaining a slowly evolving momentum model that serves as a stable reference point during training, preventing the policy from collapsing into degenerate solutions. The framework incorporates an Interquartile Range (IQR)-based trajectory entropy filter that selectively retains diverse trajectories, ensuring continued exploration throughout the training process. This dual approach of stable momentum anchoring and intelligent trajectory filtering addresses the key challenge of maintaining policy diversity while providing consistent learning signals during long-horizon self-supervised reinforcement learning.

## Key Results
- M-GRPO prevents policy collapse during extended training periods observed in standard GRPO
- Achieves 79.75% accuracy on MATH500 benchmark compared to 47.50% for baseline GRPO
- Demonstrates superior stability and effectiveness across multiple reasoning benchmarks
- Shows consistent performance improvements without compromising training efficiency

## Why This Works (Mechanism)
M-GRPO works by addressing the fundamental instability in self-supervised RL where policies tend to collapse toward degenerate solutions during long training horizons. The momentum-anchored approach provides a stable optimization target that evolves slowly, preventing the policy from making abrupt changes that could lead to collapse. The IQR-based trajectory entropy filter ensures that only diverse and informative trajectories are retained for training, preventing premature convergence while maintaining exploration. This combination creates a stable training environment where the policy can continuously improve without falling into local optima or collapsing into trivial solutions.

## Foundational Learning
- **Self-Supervised Reinforcement Learning**: Learning without explicit reward signals from external sources, using intrinsic motivation or generated rewards. Why needed: Enables LLM training without costly human annotations. Quick check: Can the model generate coherent sequences without external feedback?
- **Policy Collapse**: The phenomenon where RL policies degenerate into trivial or repetitive behaviors. Why needed: Understanding this failure mode is crucial for stable training. Quick check: Does the policy diversity decrease over training steps?
- **Momentum Anchoring**: Using a slowly updating reference model to provide stable optimization targets. Why needed: Prevents abrupt policy changes that lead to instability. Quick check: Is the momentum model updating at a significantly slower rate than the policy?
- **Trajectory Entropy Filtering**: Selecting trajectories based on their information content or diversity. Why needed: Maintains exploration while focusing on valuable experiences. Quick check: Does the entropy distribution show sustained diversity throughout training?
- **Interquartile Range (IQR) Filtering**: Statistical method for identifying and removing outliers in trajectory selection. Why needed: Ensures robust trajectory selection without being affected by extreme values. Quick check: Are the retained trajectories within the middle 50% of the entropy distribution?
- **GRPO (Group Relative Policy Optimization)**: A variant of policy optimization that considers group-level performance. Why needed: Provides the baseline framework that M-GRPO extends. Quick check: Does the baseline GRPO implementation match the standard formulation?

## Architecture Onboarding

**Component Map**: Policy Network -> Momentum Model -> Trajectory Filter -> GRPO Optimizer -> Updated Policy Network

**Critical Path**: During each training iteration, the current policy generates trajectories, which are filtered by the IQR-based entropy filter. The filtered trajectories are then used to update the momentum model slowly, which in turn provides stable targets for the GRPO optimizer to update the policy network. This creates a feedback loop where the momentum model acts as a stabilizing anchor.

**Design Tradeoffs**: The slow update rate of the momentum model provides stability but may reduce adaptation speed to new information. The IQR filtering ensures diversity but may discard potentially valuable extreme cases. The framework balances stability against adaptability, with the momentum update rate and entropy thresholds as key hyperparameters that control this tradeoff.

**Failure Signatures**: Policy collapse manifests as repetitive or trivial outputs, loss of diversity in generated text, and plateauing performance on reasoning tasks. Without the momentum anchor, policies may show erratic behavior or sudden performance drops. Without proper entropy filtering, training may converge prematurely to suboptimal solutions with limited generalization capability.

**3 First Experiments**: 1) Train M-GRPO vs standard GRPO on a simple text generation task and monitor policy diversity metrics over training time. 2) Test the effect of different momentum update rates on stability and final performance. 3) Evaluate the impact of IQR threshold selection on trajectory diversity and convergence speed.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to significantly larger model sizes beyond the 1.5B parameter experiments remains uncertain
- Stability improvements demonstrated on reasoning benchmarks may not generalize to other task domains
- Computational overhead from maintaining and updating the momentum model could impact practical deployment

## Confidence

**Policy collapse prevention**: High
**Performance improvements on MATH500**: High
**Momentum model stability benefits**: Medium
**Generalization across task domains**: Low
**Computational efficiency claims**: Medium

## Next Checks
1. Evaluate M-GRPO performance and stability when scaling to models with 7B+ parameters across diverse task domains
2. Conduct ablation studies isolating the individual contributions of momentum anchoring versus trajectory entropy filtering
3. Measure and report the computational overhead introduced by M-GRPO's momentum model updates compared to standard GRPO