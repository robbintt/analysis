---
ver: rpa2
title: 'The threat of analytic flexibility in using large language models to simulate
  human data: A call to attention'
arxiv_id: '2509.13397'
source_url: https://arxiv.org/abs/2509.13397
tags:
- silicon
- data
- samples
- human
- scale
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models are increasingly used to simulate human survey
  responses, promising efficiency and broader research reach. This study examined
  how analytic decisions affect the quality of these synthetic datasets.
---

# The threat of analytic flexibility in using large language models to simulate human data: A call to attention

## Quick Facts
- arXiv ID: 2509.13397
- Source URL: https://arxiv.org/abs/2509.13397
- Reference count: 0
- Primary result: No single LLM configuration reliably simulates human survey responses across all validity dimensions; analytic flexibility creates unpredictable variations in silicon sample quality.

## Executive Summary
Large language models are increasingly used to simulate human survey responses, promising efficiency and broader research reach. This study examined how analytic decisions affect the quality of these synthetic datasets. Researchers generated 252 configurations varying in model choice, temperature/reasoning effort, demographic detail, and scale presentation format, then compared outputs to human data across three features: participant ranking, response distributions, and scale correlations. Results showed wide variation in performance: no single configuration excelled across all features, and accuracy on one dimension did not predict accuracy on others. This indicates that no "one-size-fits-all" approach exists and highlights the risk of analytic flexibility distorting simulated data quality. The findings underscore the need for careful, transparent methodological choices and suggest using specification curves and preregistration to mitigate this threat. Without such rigor, silicon samples may yield misleading results and undermine research validity.

## Method Summary
The study compared 252 LLM configurations against human survey data from 85 participants who completed Belief in a Just World and Gut Feelings scales. Configurations varied across 9 models (including ChatGPT 5-mini, 4o, 3.5-turbo, and Llama 3.3), temperature/reasoning settings, demographic conditioning (none, age-gender, or extensive), and scale presentation strategies (all-in-one, scale-by-scale, item-by-item). Each configuration generated synthetic participant data, which was then evaluated on three validity metrics: correlation of participant rankings between human and LLM scores, Wasserstein distance for response distribution matching, and accuracy of between-scale correlations compared to ground truth. The analysis focused on identifying which configuration choices most affected each validity dimension and whether any configuration performed consistently well across all metrics.

## Key Results
- Configurations showed substantial variation in their capacity to estimate participant rankings, response distributions, and between-scale correlations.
- No single configuration performed well across all three validity dimensions; high performance on one feature did not predict high performance on others.
- The observed range of between-scale correlations spanned from -0.27 to 0.36 for BJW scale and -0.25 to 0.32 for Gut Feelings scale, indicating significant configuration-dependent variation.
- Wasserstein distance accuracy on one scale did not predict accuracy on another scale, suggesting configuration effects are context-dependent.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Small changes in analytic configuration produce large, unpredictable variations in silicon sample output quality.
- Mechanism: LLMs are sensitive to multiple interacting hyperparameters and prompt design choices. Each decision point (model, temperature, demographic conditioning, scale presentation) creates a branching path where outputs diverge from ground truth in non-linear ways. The paper tested 252 configurations and found correlation estimates between scales ranged from -0.26 to 0.71.
- Core assumption: The observed variation reflects genuine sensitivity to configuration choices rather than random noise alone.
- Evidence anchors:
  - [abstract] "Configurations (N = 252) varied substantially in their capacity to estimate (i) rank ordering of participants, (ii) response distributions, and (iii) between-scale correlations."
  - [Page 5-6] "The observed range of correlations was -0.27 < r < 0.36 for the BJW scale and -0.25 < r < 0.32 for the Gut Feelings scale."
  - [corpus] Neighbor paper "ChatGPT is not A Man but Das Man" corroborates structural inconsistency in silicon samples, finding "failure in structural consistency" across LLM simulations.
- Break condition: If configuration effects are consistent across data features and scales, the flexibility threat would be manageable through simple optimization.

### Mechanism 2
- Claim: Performance on one validity dimension does not predict performance on others.
- Mechanism: The data features being evaluated (ranking, distribution, correlation) capture distinct aspects of correspondence with human data. A configuration that preserves rank-ordering may distort distributions or between-variable relationships. The paper found correlations between performance on different features were generally small (r ~ 0.00 to 0.40).
- Core assumption: The three evaluation metrics (participant ranking, response distributions, scale correlations) represent partially independent validity criteria.
- Evidence anchors:
  - [abstract] "Most critically, configurations were not consistent in quality: those that performed well on one dimension often performed poorly on another."
  - [Page 7, Table 2] Cross-feature correlations showed weak relationships; distribution approximation between scales showed no significant correlation (r = 0.12).
  - [corpus] Limited direct evidence on cross-dimensional consistency in neighbor papers; this appears to be a novel contribution of this work.
- Break condition: If a universal configuration existed that optimized all features simultaneously, the multi-dimensional validity problem would dissolve.

### Mechanism 3
- Claim: Specification curve analysis and preregistration can partially mitigate analytic flexibility risks.
- Mechanism: Specification curves visualize heterogeneity across configurations, revealing which analytic decisions most affect outcomes. Preregistration documents configuration choices before piloting, preventing post-hoc selection of favorable results. These tools surface hidden degrees of freedom rather than eliminating them.
- Core assumption: Transparency about configuration space reduces selective reporting and encourages systematic exploration.
- Evidence anchors:
  - [Page 8] "One concrete tool which has emerged to combat analytic flexibility in other contexts is the specification curve... it would benefit researchers using silicon samples to use a specification curve to visualise the heterogeneity."
  - [Page 8-9] "Preregistration could be useful to delineate between initial testing of configurations vs. the point where researchers decide on a canonical configuration."
  - [corpus] No neighbor papers directly test specification curve efficacy for silicon samples; this is proposed but not validated.
- Break condition: If configuration effects are entirely context-dependent and ungeneralizable, even transparent documentation provides limited protection.

## Foundational Learning

- Concept: **Analytic flexibility (garden of forking paths)**
  - Why needed here: The central thesis is that silicon sampling introduces many defensible-but-consequential decision points. Without understanding this concept, engineers may treat LLM outputs as stable rather than configuration-dependent.
  - Quick check question: Can you list at least 5 distinct decision points that affect silicon sample output?

- Concept: **Specification curve analysis**
  - Why needed here: The paper recommends this as the primary diagnostic tool for understanding configuration effects. Understanding how to read and construct specification curves is essential for valid silicon sampling workflows.
  - Quick check question: What does a specification curve show, and what would a flat curve versus a highly variable curve indicate?

- Concept: **Wasserstein distance for distribution comparison**
  - Why needed here: One of the three evaluation metrics. Understanding this metric is necessary to interpret the distribution-accuracy results and to implement similar validation pipelines.
  - Quick check question: Why might Wasserstein distance be preferred over simpler metrics like mean difference for comparing response distributions?

## Architecture Onboarding

- Component map:
  Configuration generator -> Silicon sample producer -> Validation layer -> Specification curve visualizer

- Critical path:
  1. Define configuration space (which parameters to vary, which to fix)
  2. Generate silicon samples for all configurations with complete data
  3. Compute three validity metrics per configuration
  4. Visualize specification curves to identify high-variance decision points
  5. Document chosen configuration with justification in preregistration

- Design tradeoffs:
  - Exhaustive configuration testing vs. computational cost (252 configurations required substantial API calls)
  - Strict output validation vs. data loss (excluding refusals reduced sample size; paper required â‰¥50% complete data per configuration)
  - Generalizability vs. specificity: findings from two scales may not transfer to other constructs

- Failure signatures:
  - Near-zero variance in outputs (temperature=0 with no demographics produced identical scores, breaking correlation calculations)
  - High refusal rates (some configurations returned incomplete data for >50% of simulated participants)
  - Cross-scale inconsistency (Wasserstein distance accuracy on one scale did not predict accuracy on another)

- First 3 experiments:
  1. Replicate the specification curve analysis on a different pair of psychological scales to test generalizability of configuration effects.
  2. Systematically vary one parameter (e.g., temperature) while holding others constant to isolate its effect size on each validity metric.
  3. Test whether preregistered configuration choices (made before seeing any outputs) produce different final selections than unconstrained exploration.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do unexamined hyperparameters, such as top-k and top-p constraints, impact the fidelity of silicon samples compared to the widely studied temperature parameter?
- Basis in paper: [explicit] The author states that while temperature has received attention, "other LLM hyperparameters such as top-k... or top-p... have not" been examined in the context of silicon sampling.
- Why unresolved: The present study restricted its hyperparameter analysis to temperature and reasoning effort, leaving other sampling settings unexplored.
- What evidence would resolve it: A specification curve analysis comparing silicon samples generated with varying top-k and top-p settings against human ground-truth data across multiple scales.

### Open Question 2
- Question: Does providing supplementary context, such as psychometric norms or precise technical definitions of constructs, improve the alignment between LLM outputs and human response distributions?
- Basis in paper: [explicit] The paper lists "Scale information and context provided" as a dimension of analytic flexibility, noting there are "unexamined questions relating to the degree of information provided."
- Why unresolved: The experimental design varied demographic information and scale presentation but did not test conditions where models were provided with population distributions or technical definitions.
- What evidence would resolve it: Comparative studies of silicon samples generated with standard items versus those enriched with normative data and construct definitions, measuring Wasserstein distance to human data.

### Open Question 3
- Question: Can distinct classes of analytic strategies be identified that perform reliably well for specific types of research contexts or data features?
- Basis in paper: [explicit] The author concludes that because no single configuration works for all features, future work should "aim to identify classes of strategies that perform reliably well in particular types of situations."
- Why unresolved: The current results showed high heterogeneity and inconsistency across features, but the study did not attempt to map specific configurations to specific contexts.
- What evidence would resolve it: A meta-analytic approach or multi-study design that clusters configuration performance by task type (e.g., correlation estimation vs. distribution matching) to find stable strategy patterns.

### Open Question 4
- Question: Do the patterns of analytic flexibility and configuration inconsistency observed in this study generalize to diverse psychological scales and larger participant samples?
- Basis in paper: [explicit] The author notes the scope was narrow and that "a more comprehensive evaluation along these three axes is certainly needed for future research," adding results may not generalize to other scales.
- Why unresolved: The findings rely on a small sample (N=85) and only two specific scales (BJW and Gut Feelings), limiting the generalizability of the observed flexibility effects.
- What evidence would resolve it: Replication of the 252-configuration specification curve analysis across a broader battery of psychological measures and larger human datasets.

## Limitations
- The study examined only two psychological scales with a small sample (N=85), limiting generalizability to other constructs and larger populations.
- Unknown exact prompt templates and model identifiers prevent faithful reproduction of the methodology.
- The computational burden of testing 252 configurations may prevent comprehensive exploration in resource-constrained settings.

## Confidence
- **High confidence**: The core finding that analytic flexibility exists in silicon sampling is well-supported by the observed variation across configurations and corroborated by the neighbor paper on structural inconsistency.
- **Medium confidence**: The claim that performance dimensions are independent is supported by weak cross-feature correlations but requires validation with additional scales.
- **Medium confidence**: Specification curves and preregistration are recommended as mitigation strategies, but their efficacy in this specific context remains theoretical rather than empirically demonstrated.

## Next Checks
1. Replicate the specification curve analysis using a different pair of psychological scales to test whether configuration effects generalize beyond the BJW and Gut Feelings scales.
2. Systematically vary temperature while holding other parameters constant to quantify its isolated effect size on each validity metric, confirming whether it's the primary driver of variation.
3. Conduct an experiment comparing preregistered configuration choices (made before seeing any outputs) against post-hoc optimization to determine whether preregistration meaningfully constrains analytic flexibility.