---
ver: rpa2
title: 'IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)'
arxiv_id: '2506.08844'
source_url: https://arxiv.org/abs/2506.08844
tags:
- e-06
- e-05
- e-07
- data
- imputation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IMAGIC-500, a large-scale synthetic benchmark
  dataset for missing data imputation in structured socioeconomic surveys. Derived
  from the World Bank's Synthetic Data for an Imaginary Country, IMAGIC-500 contains
  500k individual records with 19 mixed-type features reflecting real-world household
  survey complexity.
---

# IMAGIC-500: IMputation benchmark on A Generative Imaginary Country (500k samples)

## Quick Facts
- arXiv ID: 2506.08844
- Source URL: https://arxiv.org/abs/2506.08844
- Reference count: 40
- Primary result: IMAGIC-500 introduces a 500k-sample synthetic benchmark for missing data imputation in structured socioeconomic surveys, evaluating 14 methods across MCAR, MAR, and MNAR mechanisms.

## Executive Summary
IMAGIC-500 is a large-scale synthetic benchmark dataset for evaluating missing data imputation methods in structured socioeconomic surveys. Derived from the World Bank's Synthetic Data for an Imaginary Country, it contains 500,000 individual records with 19 mixed-type features reflecting real-world household survey complexity. The benchmark evaluates 14 imputation methods across three missingness mechanisms (MCAR, MAR, MNAR) and five missingness ratios (10%-50%), using multiple metrics including RMSE, F1 score, runtime, and downstream classification performance.

The study reveals that traditional methods like MissForest remain highly effective, particularly at low missingness levels, while deep learning approaches excel under high missingness scenarios. The benchmark demonstrates that no single imputation method dominates across all evaluation metrics and missingness conditions, highlighting the importance of multi-metric evaluation in imputation research. The authors provide an open resource for reproducible research in missing data imputation.

## Method Summary
The IMAGIC-500 benchmark evaluates 14 imputation methods across synthetic data generated from the World Bank's Synthetic Data for an Imaginary Country. The dataset contains 500,000 records with 19 mixed-type features representing household survey data. Three missingness mechanisms are simulated: MCAR (Missing Completely at Random), MAR (Missing at Random), and MNAR (Missing Not at Random). Five missingness ratios (10%, 20%, 30%, 40%, and 50%) are tested for each mechanism. Methods are evaluated using RMSE and F1 score for imputation accuracy, runtime for computational efficiency, and downstream classification performance on income prediction tasks.

## Key Results
- Traditional methods like MissForest remain highly effective at low missingness levels (<30%), outperforming many deep learning approaches
- Deep learning methods show superior performance under high missingness (>40%), particularly for MAR and MNAR mechanisms
- No single imputation method dominates across all evaluation metrics and missingness conditions, demonstrating the need for multi-metric evaluation
- Runtime efficiency varies significantly across methods, with some deep learning approaches requiring substantially more computational resources

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its use of synthetic data that mirrors real-world socioeconomic survey complexity while allowing controlled experimentation with missingness mechanisms. By evaluating methods across multiple missingness ratios and mechanisms, the study captures the diverse challenges present in real-world imputation scenarios. The multi-metric evaluation approach ensures that methods are assessed not just on imputation accuracy but also on computational efficiency and downstream task performance, providing a comprehensive understanding of method strengths and weaknesses.

## Foundational Learning
- Missing data mechanisms (MCAR, MAR, MNAR): Understanding these is crucial because different imputation methods perform differently under each mechanism, affecting method selection for real-world applications.
- Mixed-type feature handling: Essential for socioeconomic data which contains both numerical and categorical variables; methods must handle this complexity effectively.
- Multi-metric evaluation: Needed to capture different aspects of imputation quality (accuracy, efficiency, downstream impact) that single metrics cannot fully represent.
- Imputation method taxonomy: Traditional vs. deep learning approaches have distinct strengths and weaknesses that must be understood for appropriate method selection.
- Synthetic data generation: Critical for creating controlled benchmarks while maintaining realistic data characteristics for meaningful method evaluation.

## Architecture Onboarding
**Component map**: Data generation -> Missingness simulation -> Imputation methods -> Evaluation metrics -> Results aggregation

**Critical path**: Synthetic data generation → Missingness mechanism application → Imputation method execution → Multi-metric evaluation → Performance comparison

**Design tradeoffs**: 
- Synthetic vs. real data: Synthetic data enables controlled experiments but may not capture all real-world complexities
- Method selection: Balancing traditional methods (proven reliability) with deep learning approaches (potential for complex patterns)
- Metric selection: Multiple metrics provide comprehensive evaluation but increase computational complexity

**Failure signatures**: 
- Methods performing well on one metric but poorly on others indicate overfitting to specific evaluation criteria
- High variance across missingness ratios suggests sensitivity to missing data patterns
- Poor downstream task performance despite good imputation accuracy indicates loss of important data relationships

**First 3 experiments to run**:
1. Replicate benchmark results on a subset of methods to verify computational setup and evaluation pipeline
2. Test a single imputation method across all missingness mechanisms and ratios to understand its behavior profile
3. Compare traditional vs. deep learning methods at the 10% missingness level to validate the study's findings on low-missingness performance

## Open Questions the Paper Calls Out
None explicitly stated in the provided information.

## Limitations
- The synthetic nature of IMAGIC-500 may not fully capture the complexity and noise present in actual socioeconomic surveys
- Absence of certain modern techniques like transformer-based approaches limits the comprehensiveness of the comparison
- Reliance on RMSE and F1 score as primary metrics may not fully capture imputation quality nuances for all downstream tasks

## Confidence
- High confidence in traditional methods' effectiveness at low missingness, supported by extensive benchmarking across multiple metrics
- Medium confidence in deep learning superiority under high missingness, as conclusions are based on synthetic data that may not perfectly translate to real-world scenarios
- High confidence in the importance of multi-metric evaluation, demonstrated by the study's findings that different methods excel under different evaluation criteria

## Next Checks
1. Validate the benchmark results on real-world socioeconomic datasets to assess the transferability of findings from synthetic to actual survey data.
2. Expand the imputation method comparison to include cutting-edge transformer-based approaches and evaluate their performance against the methods tested in IMAGIC-500.
3. Conduct a more diverse set of downstream task evaluations beyond classification to comprehensively assess the impact of imputation quality on various data analysis pipelines.