---
ver: rpa2
title: 'AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation
  and Historical Revisiting'
arxiv_id: '2511.09478'
source_url: https://arxiv.org/abs/2511.09478
tags:
- difficulty
- reasoning
- arxiv
- training
- adacurl
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the problem of gradient starvation and policy
  degradation in reinforcement learning-based reasoning for large language models,
  caused by training on mixed-difficulty samples. The core method, AdaCuRL, integrates
  a coarse-to-fine difficulty estimation strategy with adaptive curriculum scheduling
  to dynamically align data difficulty with model capability.
---

# AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting

## Quick Facts
- arXiv ID: 2511.09478
- Source URL: https://arxiv.org/abs/2511.09478
- Reference count: 29
- Qwen2.5-VL-3B achieves 43.81% average accuracy on mathematical reasoning

## Executive Summary
This work addresses the problem of gradient starvation and policy degradation in reinforcement learning-based reasoning for large language models, caused by training on mixed-difficulty samples. The core method, AdaCuRL, integrates a coarse-to-fine difficulty estimation strategy with adaptive curriculum scheduling to dynamically align data difficulty with model capability. It also employs a data revisitation mechanism to mitigate catastrophic forgetting and uses adaptive reference and sparse KL strategies to prevent policy degradation. Experiments on both multimodal and language models show that AdaCuRL consistently outperforms baseline methods, achieving significant improvements in reasoning benchmarks.

## Method Summary
AdaCuRL introduces a novel curriculum learning framework for reinforcement learning-based reasoning in large language models. The method employs a coarse-to-fine difficulty estimation strategy to assess sample complexity, followed by adaptive curriculum scheduling that dynamically adjusts training data difficulty based on the model's current capability. A historical revisiting mechanism prevents catastrophic forgetting by periodically re-exposing the model to previously encountered samples. Additionally, adaptive reference and sparse KL divergence strategies are implemented to mitigate policy degradation during training. The framework is evaluated across both multimodal and language models, demonstrating consistent performance improvements over existing RL-based reasoning approaches.

## Key Results
- Qwen2.5-VL-3B achieves 43.81% average accuracy on mathematical reasoning benchmarks
- Qwen2.5-Math-1.5B achieves 32.32% accuracy on reasoning tasks
- AdaCuRL consistently outperforms baseline methods across multiple reasoning benchmarks

## Why This Works (Mechanism)
The effectiveness of AdaCuRL stems from its ability to address gradient starvation and policy degradation through curriculum-based learning. By dynamically aligning data difficulty with model capability, the method prevents overwhelming the model with overly complex samples while ensuring sufficient challenge for continued learning. The historical revisiting mechanism addresses catastrophic forgetting by maintaining knowledge of previously learned concepts. The adaptive reference and sparse KL strategies help stabilize policy updates, preventing degradation that commonly occurs in RL-based reasoning approaches.

## Foundational Learning
- Curriculum learning theory: why needed - provides systematic progression from simple to complex tasks; quick check - verify monotonic difficulty progression in data ordering
- Reinforcement learning fundamentals: why needed - forms the basis for policy optimization in reasoning tasks; quick check - ensure proper reward shaping and exploration-exploitation balance
- Catastrophic forgetting mitigation: why needed - prevents loss of previously acquired knowledge during training; quick check - measure retention of early-learned concepts over training epochs
- KL divergence regularization: why needed - stabilizes policy updates and prevents divergence; quick check - monitor KL divergence values during training to ensure they remain within acceptable bounds

## Architecture Onboarding
Component map: Data sampling -> Difficulty estimation -> Curriculum scheduling -> Policy update -> Historical revisiting -> KL regularization

Critical path: The primary training loop follows: sample selection based on difficulty estimation → adaptive curriculum scheduling → policy update with KL regularization → historical data revisitation for catastrophic forgetting prevention.

Design tradeoffs: The coarse-to-fine difficulty estimation balances computational efficiency against accuracy in difficulty assessment. The adaptive scheduling mechanism trades off exploration of difficult samples against exploitation of currently learnable content. The historical revisiting frequency represents a tradeoff between computational cost and forgetting prevention.

Failure signatures: Performance plateaus may indicate ineffective difficulty estimation or curriculum scheduling. Policy degradation manifests as decreasing reward signals despite continued training. Catastrophic forgetting appears as performance drops on previously mastered tasks when training on new samples.

First experiments:
1. Validate difficulty estimation accuracy by comparing predicted vs. actual sample difficulty rankings
2. Test curriculum scheduling effectiveness by measuring learning curves with vs. without adaptive scheduling
3. Evaluate historical revisiting impact by comparing performance with and without the revisitation mechanism

## Open Questions the Paper Calls Out
None

## Limitations
- Limited discussion of cross-domain generalization capabilities
- Heavy reliance on single benchmark family raises overfitting concerns
- Precise implementation details of coarse-to-fine difficulty estimation remain underspecified
- Historical revisiting mechanism lacks thorough validation against simpler alternatives

## Confidence
- Performance improvements on evaluated benchmarks: High
- Effectiveness of adaptive curriculum scheduling: Medium
- Robustness to different reasoning domains: Low
- Scalability to larger models and more diverse tasks: Low

## Next Checks
1. Evaluate AdaCuRL on a broader set of reasoning benchmarks, including cross-domain and multi-step reasoning tasks, to assess generalization.
2. Compare the historical revisiting mechanism against simpler experience replay baselines to isolate its contribution to performance gains.
3. Conduct ablation studies to quantify the impact of each component (difficulty estimation, curriculum scheduling, KL divergence strategies) on overall performance.