---
ver: rpa2
title: Message passing-based inference in an autoregressive active inference agent
arxiv_id: '2509.25482'
source_url: https://arxiv.org/abs/2509.25482
tags:
- inference
- agent
- message
- free
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel active inference agent implemented
  as a message passing procedure on a factor graph, specifically designed for autoregressive
  models with continuous-valued observations and bounded continuous-valued actions.
  The agent learns system dynamics online through Bayesian filtering and plans actions
  by minimizing expected free energy over a finite horizon.
---

# Message passing-based inference in an autoregressive active inference agent

## Quick Facts
- **arXiv ID:** 2509.25482
- **Source URL:** https://arxiv.org/abs/2509.25482
- **Reference count:** 23
- **Key outcome:** A message passing-based active inference agent that achieves lower free energy than MPC while demonstrating cautious, uncertainty-aware control in robot navigation

## Executive Summary
This paper presents MARX-EFE, a novel active inference agent that implements inference as a message passing procedure on a factor graph specifically designed for autoregressive models with continuous observations and bounded actions. The agent learns system dynamics online through Bayesian filtering while planning actions by minimizing expected free energy over a finite horizon. The core innovation distributes inference across a planning graph where each node solves a one-step ahead EFE minimization problem. Experiments on a robot navigation task show MARX-EFE achieves lower free energy than a model-predictive control baseline, arriving at the goal later but with superior precision due to its cautious, uncertainty-aware control strategy.

## Method Summary
The MARX-EFE agent operates through three interconnected modules: Bayesian system identification that updates model parameter beliefs online using Matrix Normal Wishart distributions, a planning loop constructed as a factor graph unrolled for horizon H=3, and action selection that minimizes expected free energy through backward message passing. The agent maintains autoregressive buffers for observations and actions, uses a backward pass of predictive likelihoods to generate intermediate goal priors, and employs Laplace approximations to handle the fusion of future predictions with current goals. The EFE minimization balances epistemic value (reducing uncertainty) with goal-seeking behavior, producing actions that start small when uncertainty is high and increase as the model becomes more confident.

## Key Results
- MARX-EFE achieves lower free energy than MARX-MPC, indicating superior model learning
- While arriving at the goal later, MARX-EFE demonstrates better precision in parking at the target position
- The agent's cautious control strategy effectively balances exploration and exploitation, with actions scaling with model confidence

## Why This Works (Mechanism)
The message passing approach distributes the computationally intensive EFE minimization across multiple nodes in a planning graph, transforming a single high-dimensional optimization problem into a sequence of tractable one-step problems. Each node solves for actions that minimize immediate expected free energy while incorporating predictions from future nodes, creating a coordinated planning strategy. The backward pass propagates predictive likelihoods to form intermediate sub-goals, allowing earlier time steps to optimize based on refined future expectations. This decomposition enables online learning and planning in continuous domains while maintaining the theoretical guarantees of active inference.

## Foundational Learning
- **Matrix Normal Wishart distributions:** Needed for Bayesian online learning of autoregressive model parameters; quick check: verify parameter update rules maintain distribution properties
- **Expected Free Energy decomposition:** Combines epistemic value (information gain) and pragmatic value (goal achievement); quick check: confirm EFE gradients balance exploration/exploitation appropriately
- **Laplace approximation for Student-t products:** Required to fuse predictive distributions with goal priors in backward message passing; quick check: monitor Hessian conditioning during iterations
- **Factor graph message passing:** Enables distributed inference across planning horizon; quick check: verify message flow preserves probability semantics
- **Constrained optimization for bounded actions:** Ensures feasible control inputs; quick check: test constraint satisfaction across optimization iterations
- **Autoregressive buffer management:** Maintains temporal context for predictions; quick check: validate buffer updates preserve temporal dependencies

## Architecture Onboarding

**Component Map:** Bayesian Filter -> Planning Graph -> Action Optimizer -> Environment -> Observations/Rewards -> Bayesian Filter

**Critical Path:** Online Bayesian filtering updates model beliefs → Planning graph construction with backward message passing → Constrained EFE minimization → Action execution → Environment response → Observation update

**Design Tradeoffs:** The use of Dirac delta approximations for action posteriors enables computational tractability but discards uncertainty information that could improve robustness. Laplace approximations provide analytic convenience but may fail when predictive distributions are highly non-Gaussian. The autoregressive buffer approach maintains temporal dependencies but increases computational complexity with buffer length.

**Failure Signatures:** "Caution" deadlock occurs when uncertainty dominates the EFE, producing vanishingly small actions that never reduce uncertainty sufficiently to reach goals. Numerical instability manifests as ill-conditioned Hessian matrices during Laplace approximation, causing backward message passing failures. Poor model learning appears as persistently high free energy despite planning iterations.

**3 First Experiments:** 1) Test Bayesian filter convergence on synthetic linear systems with known dynamics to verify parameter recovery. 2) Validate single-step EFE minimization on simple control tasks to ensure action quality. 3) Verify backward message passing stability on horizon-1 and horizon-2 planning graphs before scaling to full horizon.

## Open Questions the Paper Calls Out

**Open Question 1:** Can the joint posterior predictive distribution over the planning horizon be marginalized analytically to avoid variational and Laplace approximation errors? The paper identifies that translating the autoregressive structure into nested coefficient blocks makes analytic marginalization difficult, and a solution would eliminate errors in Equations 31, 34, and 40.

**Open Question 2:** Does the Expected Free Energy (EFE) functional provide distinct behavioral advantages over standard free energy minimization in this specific factor graph formulation? Due to translation invariance, the EFE yields the same solution as standard free energy, raising questions about whether the EFE complexity is necessary except in non-translation invariant scenarios.

**Open Question 3:** Does collapsing the variational action posterior to a Dirac delta distribution degrade the agent's ability to manage uncertainty during planning? While necessary for computational tractability, reducing distributions to point estimates may discard variance information that could improve robustness, though this impact remains unquantified.

## Limitations
- Constrained optimization algorithm for action selection is unspecified, affecting reproducibility
- Laplace approximation procedure requires iterative solvers with unspecified initialization and convergence criteria
- Linear Gaussian system assumption in experiments limits validation to narrow test cases
- "Caution" behavior may lead to practical deadlocks where uncertainty never decreases sufficiently

## Confidence
- **High Confidence:** Core message passing framework and theoretical foundations are sound
- **Medium Confidence:** Bayesian filtering implementation is clearly specified but numerical stability is uncertain
- **Low Confidence:** Practical performance implications of unspecified optimization algorithms and Laplace approximation procedure

## Next Checks
1. Implement and compare multiple constrained optimization algorithms (IPOPT, SQP, gradient-based) for action selection to identify optimal solver for bounded constraints
2. Characterize backward message passing stability by varying goal covariance and control precision parameters, measuring numerical failures across horizon lengths
3. Test framework on nonlinear/non-Gaussian systems (pendulum, cart-pole) to evaluate computational feasibility and EFE effectiveness beyond linear Gaussian assumptions