---
ver: rpa2
title: 'Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim
  Verification'
arxiv_id: '2505.12348'
source_url: https://arxiv.org/abs/2505.12348
tags:
- claim
- reasoning
- reasoning-cv
- arxiv
- verification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel CoT-Verify paradigm for knowledge-assisted
  claim verification, replacing the traditional Decompose-Then-Verify approach. Instead
  of decomposing claims into sub-claims, the method uses a fine-tuned LLM to generate
  a Chain-of-Thought (CoT) reasoning path for the original claim.
---

# Reasoning-CV: Fine-tuning Powerful Reasoning LLMs for Knowledge-Assisted Claim Verification

## Quick Facts
- arXiv ID: 2505.12348
- Source URL: https://arxiv.org/abs/2505.12348
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on claim verification using only an 8B LLM, outperforming GPT-4o+CoT and o1-preview

## Executive Summary
This paper introduces Reasoning-CV, a novel approach for knowledge-assisted claim verification that replaces traditional claim decomposition with end-to-end Chain-of-Thought (CoT) reasoning paths. The method fine-tunes an 8B pre-trained LLM through two stages: supervised fine-tuning (SFT) to learn from GPT-4o-generated reasoning paths, followed by self-improvement direct preference optimization (DPO) to refine reasoning quality. The approach demonstrates superior performance on both in-domain and out-of-domain claim verification benchmarks while reducing errors from claim decomposition and showing strong generalization across domains.

## Method Summary
Reasoning-CV fine-tunes an 8B LLM using a two-stage approach: first, SFT learns from GPT-4o-generated CoT reasoning paths conditioned on ground-truth veracity labels, then self-improvement DPO refines judgment thresholds through conditional generation of reasoning paths for each possible veracity option. The method uses only an 8B model with LoRA fine-tuning (rank=64) and achieves state-of-the-art results on multiple claim verification benchmarks including FEVEROUS, HOVER, Healthver, and out-of-domain datasets like LLM-AggreFact, SciFact, and VitaminC.

## Key Results
- Achieves 86.1 macro-F1 on FEVEROUS with NEI labels using only an 8B model
- Outperforms GPT-4o+CoT (83.1) and o1-preview (85.8) on FEVEROUS
- Shows strong generalization across in-domain and out-of-domain datasets
- Reduces errors from claim decomposition by using end-to-end CoT reasoning

## Why This Works (Mechanism)
The method works by eliminating claim decomposition errors through end-to-end CoT reasoning paths while using a two-stage fine-tuning approach to both learn from high-quality GPT-4o demonstrations and self-improve judgment thresholds. The conditional generation in Stage 2 creates informative preference pairs for DPO by contrasting reasoning paths for different veracity options, allowing the model to better calibrate acceptance thresholds and improve consistency in reasoning.

## Foundational Learning
- **Claim Verification vs. Fact-Checking**: Understanding the specific task scope where evidence is provided/retrievable is critical, as the method assumes available evidence and uses "gold evidence" vs. "open book" settings. Quick check: Can you define these settings and which datasets support "NEI" labels?
- **Direct Preference Optimization (DPO) for Reasoning Alignment**: The self-improvement stage uses DPO, not standard RLHF, to calibrate judgment thresholds through conditional generation of preference pairs. Quick check: How does the paper construct the preference dataset and why condition on specific veracity labels?
- **LoRA Fine-Tuning with Long Contexts**: Implementation uses LoRA (rank=64) on an H100 GPU for long claim-evidence inputs (thousands of tokens), making full fine-tuning impractical. Quick check: What are the learning rates for the two stages and why might RL-based fine-tuning fail with the same hardware?

## Architecture Onboarding
- **Component map**: Claim (C) + Evidence (E) -> Stage 1 (SFT Module) -> Stage 2 (Self-Improvement DPO Module) -> Final veracity judgment
- **Critical path**: 1) Data Preparation: Combine 55K training samples from FEVEROUS, HOVER, Healthver. 2) SFT Data Generation: Query GPT-4o for CoT paths conditioned on ground truth, filter for correct judgments (~50K paths). 3) SFT: Fine-tune 8B LLaMA for 3 epochs. 4) DPO Round 1: Generate conditional paths, build preference pairs, run DPO for 3 epochs. 5) DPO Round 2 (Optional): Repeat for further refinement.
- **Design tradeoffs**: End-to-End vs. Decomposition simplifies pipeline but requires high-quality CoT training data; Two-Stage Fine-Tuning establishes base capability then refines thresholds; Conditional Generation for DPO improves contrast but adds prompt complexity; Model Size uses only 8B for efficiency.
- **Failure signatures**: Stage 1 Failure if GPT-4o distillation data is noisy; Stage 2 Failure if conditional paths are too similar for effective DPO; Overfitting with too many DPO rounds harming out-of-domain performance; Context Length Issues with very long evidence exceeding model context.
- **First 3 experiments**: 1) SFT Ablation: Remove ground-truth conditioning in Stage 1 to validate distillation quality mechanism. 2) DPO Ablation: Remove conditional generation in Stage 2 to show value of contrastive preference pairs. 3) Model Size Scaling: Apply to 3B model and compare against GPT-4o and Minicheck-7B to verify transferability.

## Open Questions the Paper Calls Out
- **Open Question 1**: Can the Stage 2 conditional generation mechanism be generalized to other tasks with enumerable answer sets beyond claim verification? The paper notes this as future work since it's only validated on claim verification.
- **Open Question 2**: How can the framework integrate retrieval capabilities to assist reasoning in open-book settings? Current work separates retrieval from verification, treating them as distinct stages.
- **Open Question 3**: How can self-improvement DPO be optimized to prevent performance degradation in out-of-domain generalization? Iterative preference optimization appears to overfit training distribution, sacrificing robustness.

## Limitations
- Methodologically dependent on GPT-4o for high-quality CoT path generation, with limited analysis of retained path quality distribution
- DPO implementation details like batch size, beta/temperature coefficient, and gradient accumulation are unspecified
- Significant performance drop when moving from in-domain to out-of-domain settings suggests limited generalizability

## Confidence
- **High Confidence**: Core methodology (CoT-Verify paradigm, two-stage fine-tuning) and main experimental results are well-documented and reproducible
- **Medium Confidence**: Performance claims relative to black-box models (GPT-4o+CoT, o1-preview) are compelling but should be interpreted with caution
- **Medium Confidence**: Efficiency claims (using only 8B model) are supported but computational requirements for fine-tuning stages are not fully characterized

## Next Checks
1. **DPO Hyperparameter Sensitivity**: Systematically vary DPO learning rate, batch size, and number of training epochs to determine optimal settings and assess robustness
2. **Cross-Dataset Transfer Analysis**: Conduct detailed error analysis on out-of-domain datasets to identify specific failure modes and test whether additional DPO rounds can improve generalization
3. **Model Size Scaling Study**: Implement the full Reasoning-CV pipeline on both 3B and 13B parameter models to validate scalability claims and determine minimum effective model size for different claim complexity levels