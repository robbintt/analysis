---
ver: rpa2
title: 'NovoMolGen: Rethinking Molecular Language Model Pretraining'
arxiv_id: '2508.13408'
source_url: https://arxiv.org/abs/2508.13408
tags:
- molecular
- molecules
- chemical
- generation
- atomwise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: NovoMolGen is a family of transformer-based molecular language
  models pretrained on 1.5 billion molecules to improve de novo molecular design.
  The study systematically investigates how molecular representation (SMILES, SELFIES,
  SAFE, DeepSMILES), tokenization (atomwise vs BPE), model size, and dataset scale
  impact generation performance.
---

# NovoMolGen: Rethinking Molecular Language Model Pretraining

## Quick Facts
- **arXiv ID**: 2508.13408
- **Source URL**: https://arxiv.org/abs/2508.13408
- **Reference count**: 40
- **Primary result**: NovoMolGen family of transformer-based molecular language models pretrained on 1.5B molecules, demonstrating early saturation of pretraining benefits and achieving state-of-the-art de novo molecular design performance.

## Executive Summary
NovoMolGen is a family of transformer-based molecular language models designed for de novo molecular generation and goal-directed optimization. Pretrained on 1.5 billion molecules from ZINC-22, the study systematically investigates how molecular representation, tokenization strategy, model size, and dataset scale impact generation performance. The research reveals that performance on downstream tasks saturates remarkably early during pretraining, with weak correlation between pretraining metrics and actual downstream success. SMILES with Byte Pair Encoding tokenization consistently delivers the most robust performance across diverse molecular generation tasks.

## Method Summary
NovoMolGen employs a Llama-style decoder-only transformer architecture trained on 1.5B molecules from ZINC-22 using next-token prediction. The study evaluates multiple molecular representations (SMILES, SELFIES, SAFE, DeepSMILES) with two tokenization strategies (atomwise vs BPE). Models are pretrained for 75K steps with global batch size 19200, then fine-tuned using REINVENT-style reinforcement learning with experience replay buffers. Performance is assessed through distribution metrics (validity, novelty, FCD) and goal-directed benchmarks (PMO, molecular docking).

## Key Results
- Early saturation of performance on downstream tasks, with minimal gains from larger models or extended pretraining
- Weak correlation between pretraining metrics and downstream task success (r=0.376 for FCD vs PMO)
- SMILES with BPE tokenization delivers most consistent performance across all evaluated tasks
- NovoMolGen establishes new state-of-the-art results, outperforming prior Mol-LLMs and specialized generative models in both unconstrained and goal-directed molecular generation

## Why This Works (Mechanism)

### Mechanism 1: Early Chemical Syntax Saturation
Performance on downstream generation tasks saturates rapidly during pretraining because chemical libraries like ZINC primarily encode chemical syntax and synthetic accessibility rules rather than functional semantics. The model masters these grammatical constraints quickly, after which further self-supervision refines distributional matching without necessarily improving goal-directed capabilities. The pretraining dataset contains limited functional signals compared to the complexity of biological interactions.

### Mechanism 2: Substructure-Compression via BPE
Byte Pair Encoding tokenization allows the model to process chemical "words" (substructures) rather than raw atoms, improving efficiency without sacrificing validity. BPE merges frequent co-occurring atoms into single tokens, reducing sequence length and effective path length for the attention mechanism. This allows the model to capture common chemical motifs more efficiently than atomwise tokenization.

### Mechanism 3: Anchored Reinforcement Learning
The model optimizes for specific properties by balancing exploration against a "prior" of chemical validity through a KL-style constraint. The fine-tuning uses a cost function combining the agent's likelihood, a fixed prior's likelihood, and the task reward. This prevents the model from drifting into chemically invalid regions while maximizing rewards, specifically focused on top-k promising candidates.

## Foundational Learning

- **Concept: SMILES vs. SELFIES Representations**
  - **Why needed here**: The paper benchmarks these string formats. SMILES is a direct graph-to-text mapping but can generate invalid strings; SELFIES is a grammar that guarantees validity.
  - **Quick check question**: Can you explain why a standard RNN/Transformer might generate invalid SMILES but valid SELFIES, and why the paper still favors SMILES+BPE?

- **Concept: Autoregressive Decoding (Next-Token Prediction)**
  - **Why needed here**: NovoMolGen uses a decoder-only architecture. Understanding that P(molecule) = ∏ P(token_i | tokens_<i>) is essential to grasp why tokenization strategy matters.
  - **Quick check question**: How does the "left-to-right" nature of the model impact the generation of ring closures in SMILES strings?

- **Concept: Goal-Directed Optimization (PMO Benchmark)**
  - **Why needed here**: The paper evaluates performance not just on generating random molecules, but on optimizing specific properties (e.g., QED, docking scores).
  - **Quick check question**: Why is "sample efficiency" (number of oracle calls) the critical metric in the PMO benchmark?

## Architecture Onboarding

- **Component map**: ZINC-22 → Deduplication → Representation Converter (SMILES/SELFIES) → Tokenizer (Atomwise/BPE) → Llama-style Decoder (FlashAttention) → Pretraining (Next-Token Prediction) → REINVENT-style RL Fine-Tuning → Scorer (Oracle) → Experience Replay Buffer → Agent Update

- **Critical path**: The Tokenizer training → Pretraining stability (Loss convergence) → RL Hyperparameter search. Note that the paper finds early checkpoints (e.g., step 75k) often suffice, so mapping checkpoint selection to downstream performance is critical.

- **Design tradeoffs**:
  - Model Size: 300M vs. 32M. The paper suggests 32M is a "practical and competitive option" with lower compute, as scaling yields inconsistent gains.
  - Tokenizer: Atomwise (simple, granular) vs. BPE (efficient, captures motifs). BPE is recommended for consistency.
  - Metric Reliance: Do not rely on FCD (Fréchet ChemNet Distance) as a proxy for downstream optimization capability; the correlation is weak.

- **Failure signatures**:
  - High FCD, Low PMO: The model mimics the training distribution well (low FCD) but cannot optimize for specific goals.
  - Syntax Collapse: During RL fine-tuning, the agent drifts too far from the prior, generating invalid strings (if using SMILES) or gibberish.
  - Overtraining: Training past the "saturation point" consumes resources without improving functional generation.

- **First 3 experiments**:
  1. Tokenizer Ablation: Train two 32M models (Atomwise vs. BPE) on a 10M subset of ZINC. Compare validation loss and the validity of generated SMILES.
  2. Saturation Check: Evaluate a 32M model at 10%, 50%, and 100% of training. Plot FCD vs. PMO score on a single task (e.g., QED optimization) to verify the "early saturation" claim.
  3. RL Stability Test: Run the fine-tuning pipeline with and without the "Experience Replay Buffer" on a specific docking task to measure sample efficiency.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can pretraining objectives be redesigned to prioritize functional semantics over chemical syntax?
- Basis in paper: The authors conclude that "future progress... requires a paradigm shift away from objectives focused solely on chemical validity" and suggests incorporating "objectives related to protein-ligand interactions, physicochemical properties, or experimental bioactivity."
- Why unresolved: Current large-scale datasets (e.g., ZINC) are curated for synthetic accessibility, providing a weak learning signal that forces models to learn only syntax rather than functional utility.
- What evidence would resolve it: Demonstrating that a model pretrained with property-aware or bioactivity-aware objectives significantly outperforms syntax-only models on goal-directed benchmarks like PMO.

### Open Question 2
- Question: What pretraining evaluation metrics reliably predict downstream goal-directed generation performance?
- Basis in paper: The study reveals a "weak correlation between performance metrics measured during pretraining and actual downstream performance," specifically noting that FCD is a poor predictor of PMO scores (correlation r=0.376).
- Why unresolved: Distribution-based metrics like FCD measure chemical similarity but fail to capture a model's capacity for functional optimization or navigating complex reward landscapes.
- What evidence would resolve it: Identification of new proxy metrics that show a strong, statistically significant correlation with final PMO benchmark scores across diverse molecular representations.

### Open Question 3
- Question: Can reinforcement learning be effectively introduced during the pretraining phase rather than only during fine-tuning?
- Basis in paper: The authors propose "introducing reinforcement learning at earlier stages" to provide a 'fitness' signal analogous to natural selection, aligning generation with functional goals sooner.
- Why unresolved: Current architectures separate pretraining (self-supervised) and fine-tuning (RL), potentially limiting the model's initial exploration of high-value chemical space.
- What evidence would resolve it: A training framework that successfully interleaves next-token prediction with RL rewards from the start, showing faster convergence and higher final rewards on downstream tasks compared to the standard two-stage pipeline.

## Limitations

- **Data functional relevance uncertainty**: The early saturation claim relies on assumptions about ZINC-22 primarily encoding chemical syntax rather than functional semantics, which is not directly verified through functional annotation analysis.

- **BPE generalizability limits**: The consistent superiority of SMILES+BPE may be partially attributed to specific tokenizer configuration (vocab size, dropout rate, training corpus) rather than fundamental advantages.

- **RL fine-tuning reproducibility concerns**: The experience replay buffer and top-k sampling approach sensitivity to hyperparameters and seed selection is not fully characterized across diverse molecular optimization landscapes.

## Confidence

- **Early Pretraining Saturation**: Medium - Supported by empirical observations but relies on unverified assumptions about data functional content
- **SMILES+BPE Dominance**: High - Consistently demonstrated across multiple downstream tasks with clear performance margins
- **RL Fine-Tuning Stability**: Medium - Methodology is sound but sensitivity to hyperparameters not fully characterized
- **Model Size Irrelevance**: Medium - Supported by direct comparisons but limited model size range examined

## Next Checks

1. **Data Functional Diversity Analysis**: Conduct comprehensive analysis of functional annotations (bioactivity, target interactions) in ZINC-22 subset to directly test whether dataset is dominated by synthetic accessibility signals versus functional chemical diversity.

2. **Tokenizer Robustness Evaluation**: Systematically vary BPE vocabulary size (100-2000) and training corpus size (10M-500M molecules) to establish sensitivity of SMILES+BPE performance to tokenizer configuration, including tests on molecules with rare substructures.

3. **Cross-Domain Transfer Assessment**: Evaluate NovoMolGen models pretrained on ZINC-22 for tasks involving molecules from different chemical spaces (natural products, synthetic intermediates, drug-like vs. lead-like) to test generalizability of pretraining saturation and BPE advantages across diverse molecular domains.