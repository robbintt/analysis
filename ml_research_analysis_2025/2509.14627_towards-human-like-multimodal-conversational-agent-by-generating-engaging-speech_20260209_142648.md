---
ver: rpa2
title: Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech
arxiv_id: '2509.14627'
source_url: https://arxiv.org/abs/2509.14627
tags:
- speech
- speaker
- text
- audio
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating natural, engaging
  speech responses in human-like multimodal conversational agents by incorporating
  paralinguistic information such as tone, pace, and pitch. The authors introduce
  the MultiSensory Conversation (MSenC) dataset, containing about 31,000 utterances
  from daily conversation YouTube videos, and develop a multimodal LLM-based model
  that generates both text responses and voice descriptions to enable contextually
  appropriate speech synthesis.
---

# Towards Human-like Multimodal Conversational Agent by Generating Engaging Speech

## Quick Facts
- arXiv ID: 2509.14627
- Source URL: https://arxiv.org/abs/2509.14627
- Reference count: 0
- Key outcome: A multimodal LLM-based conversational agent generates text responses plus voice descriptions, improving speech naturalness and emotional continuity versus text-only baselines.

## Executive Summary
This paper tackles the challenge of building conversational agents that produce natural, engaging speech by integrating visual, audio, and textual context. The authors introduce the MultiSensory Conversation (MSenC) dataset of ~31,000 YouTube utterances, and develop a multimodal model that generates both text and voice descriptions (capturing paralinguistic features like tone and pace). These descriptions are then synthesized into speech using Parler-TTS. Experiments show that incorporating audio and video modalities improves text generation quality and that the model outperforms baselines in emotional continuity and user-rated naturalness.

## Method Summary
The approach uses CLIP-ViT for visual feature extraction and WavLM for audio features, both compressed via Q-Former architectures into fixed-size embeddings. These are projected into the LLM's embedding space and concatenated for generation. A Mistral-7B backbone is fine-tuned with LoRA to produce text responses and natural language voice descriptions. The model is trained with cross-entropy loss on concatenated (response || description) sequences. Inference involves synthesizing speech from the voice description using Parler-TTS. The dataset preprocessing pipeline includes ASR (Whisper), scene detection (PySceneDetect), speaker diarization (WeSpeaker + HDB-SCAN), and voice description annotation (DataSpeech/Parler-TTS).

## Key Results
- Incorporating visual and audio modalities improves text generation metrics (BLEU@1: 15.11 vs 12.30; METEOR: 6.89 vs 5.81) over text-only baselines.
- The model achieves 15.10% emotional continuity accuracy, outperforming baselines (11.20%–13.72%).
- User studies rate the model higher on emotional expression, suitability, and naturalness than StyleTTS2, HierSpeech++, and Parler-TTS.

## Why This Works (Mechanism)

### Mechanism 1: Multimodal Feature Compression via Q-Former
- **Claim:** Q-Former compression of video/audio into fixed-size embeddings improves response quality over text-only baselines.
- **Mechanism:** Q-Former (from BLIP-2/Video-LLaMA) compresses video (3 fps) and full audio into fixed-size feature vectors, projected into LLM embedding space for cross-modal attention.
- **Core assumption:** Paralinguistic and visual context provide complementary emotional/situational information beyond text.
- **Evidence anchors:** Abstract states multimodal use improves engagement; section 4.2.1 shows METEOR improves from 5.81 to 6.89 and BLEU@1 from 12.30 to 15.11.
- **Break condition:** If audio/video features are corrupted (e.g., background music, poor lighting), performance may degrade below text-only baseline.

### Mechanism 2: Paralinguistic Description as Bridging Representation
- **Claim:** Generating natural language voice descriptions preserves paralinguistic intent across text-to-speech handoff.
- **Mechanism:** LLM outputs text response + voice description (e.g., "A female voice speaks quite slowly with a very low pitch"), fed to Parler-TTS for synthesis.
- **Core assumption:** Natural language suffices to capture paralinguistic dimensions that matter for engagement; fine-grained acoustic details are secondary.
- **Evidence anchors:** Abstract notes descriptions cover paralinguistic information; section 2.2.2 details annotation and conversion to natural language.
- **Break condition:** If Parler-TTS cannot faithfully render descriptions—especially conflicting attributes—output may sound unnatural or inconsistent.

### Mechanism 3: Instruction Tuning for Speaker-Aware Generation
- **Claim:** Providing speaker IDs and instructions to generate voice descriptions improves conversational role consistency and emotional continuity.
- **Mechanism:** Utterances tagged with speaker ID; instruction prompt includes conversation history with speaker labels and directive to generate [text (voice description)].
- **Core assumption:** Emotional continuity (matching prior utterance's emotion) correlates with perceived empathy and engagement.
- **Evidence anchors:** Section 3.2 describes speaker labeling and description generation; section 4.3.2 shows emotion classification accuracy of 15.10% vs 11.20%–13.72% for baselines.
- **Break condition:** If conversation history exceeds ~10 multimodal turns (beyond 800-token limit), older context is truncated—potentially breaking long-range emotional arcs or speaker consistency.

## Foundational Learning

- **Concept: Q-Former / Query Transformer**
  - **Why needed here:** Core mechanism for compressing multimodal inputs into LLM-compatible representations.
  - **Quick check question:** Given a 5-second video clip sampled at 3 fps with corresponding audio, can you trace how many query tokens are produced and what dimension they have after projection?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA/Adapters)**
  - **Why needed here:** The paper fine-tunes a 7B-parameter LLM using PEFT.
  - **Quick check question:** If LoRA rank is set too low, what symptom would you expect in the generated voice descriptions—repetition, generic outputs, or modality fusion failure?

- **Concept: Cross-Entropy Loss for Sequence Generation**
  - **Why needed here:** Training objective concatenates text response and voice description, computing cross-entropy jointly.
  - **Quick check question:** If voice descriptions are consistently lower quality than text responses during training, what modification to the loss weighting might help?

## Architecture Onboarding

- **Component map:** [Video Frames] → CLIP-ViT → Video Q-Former → Linear Projection → [Concat] → [LLM (Mistral-7B)] → [Text Response]; [Audio Waveform] → WavLM → Audio Q-Former → Linear Projection → [Concat] → [LLM] → [Voice Description] → [Parler-TTS] → [Speech Output]

- **Critical path:**
  1. Multimodal feature extraction (CLIP-ViT, WavLM)—if these fail, downstream has no signal
  2. Q-Former compression—determines how much temporal/contextual info survives
  3. LLM generation of voice description—this is the paralinguistic bottleneck
  4. Parler-TTS synthesis—final output quality depends on TTS capability

- **Design tradeoffs:**
  - Video sampling rate (3 fps): Redundancy reduction vs. missing rapid gestures.
  - Audio unsampled: Full audio preserved for prosody, but increases compute.
  - Natural language descriptions vs. discrete speech tokens: Descriptions are interpretable and controllable but lose fine acoustic detail.
  - Max history length (~10 turns): Prevents context explosion but truncates long conversations.

- **Failure signatures:**
  - Modality fusion failure: Model ignores audio/video cues, generates generic text-only responses.
  - Voice description incoherence: TTS produces robotic or conflicting speech (e.g., high-pitched monotone).
  - Speaker ID confusion: Model responds as wrong speaker or loses turn-taking structure.
  - Emotional drift: Emotion classification accuracy drops below baseline.

- **First 3 experiments:**
  1. Ablation by modality: Replicate Table 1 on a subset of MSenC. Run Text-only, Text+Audio, Text+Video, Text+Audio+Video. Confirm multimodal fusion improves METEOR and BLEU.
  2. Description quality audit: Sample 50 generated voice descriptions. Manually rate whether they match ground-truth audio characteristics (pitch, pace, expressiveness).
  3. Emotional continuity stress test: Evaluate emotion classification accuracy on conversations with 5+ turns. Compare against baseline TTS systems (StyleTTS2, HierSpeech++).

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating disfluencies, overlaps, and non-speech vocalizations—currently excluded from MSenC—further improve naturalness and human-likeness of conversational agents?
  - **Basis in paper:** [explicit] The paper states the dataset ensures "no overlaps, disfluencies, and non-speech vocalizations," yet these are natural features of human conversation that may contribute to perceived authenticity.
  - **Why unresolved:** The authors intentionally excluded these elements to ensure clear, high-quality training data, leaving open whether their inclusion would enhance or degrade model performance.
  - **What evidence would resolve it:** Training separate models on datasets with and without these features, followed by comparative user studies on naturalness ratings.

- **Open Question 2:** How would the model perform in real-time conversational settings where inference latency impacts timing and rhythm of responses?
  - **Basis in paper:** [inferred] The paper provides no analysis of computational efficiency or real-time feasibility, yet temporal appropriateness is critical for engaging conversation.
  - **Why unresolved:** The architecture uses multiple components (CLIP-ViT, WavLM, Q-Former, Mistral-7B, Parler-TTS) that may introduce cumulative latency unsuitable for interactive deployment.
  - **What evidence would resolve it:** Latency measurements across the full pipeline and user studies comparing response timing against human conversational norms.

- **Open Question 3:** Can the model maintain consistent voice identity and personality across extended multi-session conversations?
  - **Basis in paper:** [explicit] The authors note: "Our model cannot replicate a speaker's exact voice from historical recordings," acknowledging voice identity limitations.
  - **Why unresolved:** The model uses a single voice during inference, but whether it maintains coherent personality traits, speaking patterns, and paralinguistic style across different conversation sessions remains unexplored.
  - **What evidence would resolve it:** Longitudinal user studies measuring perceived consistency in agent personality and voice characteristics across multiple conversation sessions.

- **Open Question 4:** Does the 15.10% emotional continuity accuracy represent meaningful empathetic engagement or merely surface-level emotion matching?
  - **Basis in paper:** [inferred] The paper assumes emotional continuity equals empathy, stating "if an utterance's emotion aligns with the previous one, it is considered empathetic," without validating this assumption.
  - **Why unresolved:** Emotion classification accuracy may not capture nuanced empathetic responses that require understanding emotional context rather than simple matching.
  - **What evidence would resolve it:** Human evaluation comparing emotion-matched responses against responses rated for genuine empathetic understanding.

## Limitations

- The paper does not validate the Q-Former's ability to compress temporal visual and audio streams without significant information loss.
- The sufficiency of natural language voice descriptions to capture "engaging" paralinguistic information is weakly supported; no comparison against direct speech token generation.
- Speaker clustering accuracy of 95.49% is reported on 602 samples, but performance on noisier, real-world data is not addressed.

## Confidence

- **High confidence:** Dataset construction pipeline (MSenC) and basic multimodal feature extraction (CLIP-ViT, WavLM) are reproducible and well-documented. Reported text generation metrics (BLEU, METEOR) are standard and verifiable.
- **Medium confidence:** Instruction tuning approach for speaker-aware generation is plausible, given reported 15.10% emotion continuity accuracy versus 11.20%–13.72% baselines. However, ablation studies do not isolate contribution of instruction tuning from other factors.
- **Low confidence:** Claim that voice descriptions are sufficient to capture "engaging" paralinguistic information is weakly supported. User study sample size (n=12) is too small for robust generalization.

## Next Checks

1. **Q-Former compression fidelity:** Sample 50 video/audio pairs from MSenC. Extract Q-Former features and reconstruct pseudo-frames/audio via nearest-neighbor retrieval from training set. Measure reconstruction loss (MSE for video, spectral distance for audio). If loss exceeds 0.1, compression is discarding critical paralinguistic cues.

2. **Voice description ambiguity audit:** Generate 100 voice descriptions using the fine-tuned model. For each, compute the number of mutually exclusive attributes (e.g., "high pitch" + "monotone" + "expressive"). If >20% of descriptions contain contradictions, the LLM's paralinguistic generation is unreliable, and TTS quality will suffer.

3. **Long-context emotional drift:** Evaluate emotion continuity on conversations with 15+ turns (longer than training distribution). Compare against a text-only baseline and a strong TTS system (e.g., StyleTTS2 with emotion control). If accuracy drops by >5%, the 800-token input limit is truncating critical emotional context.