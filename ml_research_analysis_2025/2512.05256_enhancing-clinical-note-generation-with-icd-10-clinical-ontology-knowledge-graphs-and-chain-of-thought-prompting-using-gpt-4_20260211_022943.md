---
ver: rpa2
title: Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge
  Graphs, and Chain-of-Thought Prompting Using GPT-4
arxiv_id: '2512.05256'
source_url: https://arxiv.org/abs/2512.05256
tags:
- clinical
- prompting
- text
- codes
- notes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of physician burnout caused
  by the administrative burden of clinical note documentation in electronic health
  records (EHRs). The authors propose enhancing clinical note generation by leveraging
  large language models (LLMs) with Chain-of-Thought (CoT) prompting, integrating
  ICD-10 codes, clinical ontology knowledge graphs, and semantic search results.
---

# Enhancing Clinical Note Generation with ICD-10, Clinical Ontology Knowledge Graphs, and Chain-of-Thought Prompting Using GPT-4

## Quick Facts
- **arXiv ID:** 2512.05256
- **Source URL:** https://arxiv.org/abs/2512.05256
- **Reference count:** 21
- **Primary result:** CoT prompting with ICD code-based semantic search outperforms standard one-shot prompts in generating clinically aligned notes

## Executive Summary
This study addresses physician burnout from clinical note documentation by enhancing LLM-based note generation with Chain-of-Thought prompting, ICD-10 codes, clinical ontology knowledge graphs, and semantic search. The authors test their approach on six clinical cases from the CodiEsp dataset using GPT-4, demonstrating that CoT prompting—particularly when combined with ICD code-based semantic search—achieves lower cosine distance scores than standard one-shot prompts, indicating improved semantic alignment with ground-truth clinical cases. While SNOMED CT knowledge graph integration increases output variability without improving precision, the results validate that LLMs can effectively reason about ICD codes to generate clinical notes using instruction prompting.

## Method Summary
The method integrates ICD-10 codes, patient demographics, and semantic search to generate clinical notes. CodiEsp dataset (750 cases) is embedded using OpenAI text-embedding-3-small, and top-10 similar cases are retrieved based on ICD code queries. Chain-of-Thought prompts incorporate these retrieved cases as reasoning exemplars, while SNOMED CT OWL expressions provide structured ontology knowledge. GPT-4 generates clinical notes, which are evaluated against ground truth using BERT-large-cased embeddings and cosine distance. The study compares one-shot, CoT, and CoT+KG approaches across six test cases with 100 generations each.

## Key Results
- CoT prompting with ICD code-based semantic search achieves lower cosine distance than one-shot prompts across all six test cases
- KDE plots show leftward shift for CoT semantic search, indicating better semantic alignment with ground truth
- SNOMED CT knowledge graph integration increases output variance without improving precision
- ICD code queries yield higher relatedness scores (0.7-0.85) than text reference queries (0.4-0.65)

## Why This Works (Mechanism)

### Mechanism 1
Chain-of-Thought prompting with ICD code-based semantic search improves semantic alignment of generated clinical notes with ground truth. CoT prompts retrieve top-10 similar clinical cases via embedding-based semantic search using ICD codes as queries. These retrieved cases serve as reasoning exemplars that guide the LLM's generation pattern, providing contextual scaffolding for how clinical information should be structured and expressed. The LLM generalizes from similar case examples to synthesize new clinical narratives that maintain semantic consistency with ground truth documentation styles. [abstract, section 4.1, corpus]

### Mechanism 2
SNOMED CT knowledge graph prompts add structured domain knowledge but increase output variability without improving precision. OWL expressions from SNOMED CT provide formal ontological relationships that are injected as natural language prompt context, intended to enrich clinical reasoning with definitional constraints. The LLM parses and applies ontological axioms presented as text to constrain clinical narrative generation. [section 3.2, section 4.2, corpus]

### Mechanism 3
ICD code queries yield higher semantic relatedness scores than text reference queries, making them more effective for retrieval-augmented prompting. ICD codes provide standardized, semantically dense representations that map more precisely to clinical case embeddings. Text references are broader and noisier, yielding lower top-10 relatedness scores. The embedding model captures meaningful semantic similarity between ICD codes and clinical case content. [section 4.4, table 3, corpus]

## Foundational Learning

- **Cosine Distance vs. Cosine Similarity**
  - Why needed here: The paper uses cosine distance (1 - cosine similarity) as its primary evaluation metric. Lower distance = better semantic alignment. Understanding this distinction is critical for interpreting KDE plots and box plots.
  - Quick check question: If Model A has cosine distance 0.35 and Model B has 0.42, which produces more semantically similar notes to ground truth?

- **Semantic Search with Embedding-Based Retrieval**
  - Why needed here: The core innovation uses vector embeddings to find similar clinical cases. Understanding how text-embedding-3-small maps ICD codes and clinical text to dense vectors enables debugging retrieval quality.
  - Quick check question: Why might an ICD code query return higher relatedness scores than a text reference query for the same clinical concept?

- **Chain-of-Thought Prompting**
  - Why needed here: CoT provides intermediate reasoning steps (retrieved cases) that guide generation. Without understanding CoT's role in decomposing reasoning, the prompt design appears arbitrary.
  - Quick check question: What is the difference between a one-shot prompt and a CoT prompt with 10 retrieved examples?

## Architecture Onboarding

- **Component map:** CodiEsp Dataset -> Semantic Search Module -> Knowledge Graph Module -> Prompt Constructor -> Generation Engine -> Evaluation Pipeline
- **Critical path:** ICD codes + patient info -> semantic search query -> top-10 cases -> CoT prompt assembly -> GPT-4 -> generated note -> BERT embedding -> cosine distance vs. ground truth
- **Design tradeoffs:**
  - CoT-only vs. CoT+KG: CoT alone yields tighter distributions and lower distance; CoT+KG increases variance without precision gains
  - ICD vs. text reference queries: ICD queries have higher relatedness (0.7-0.85) but require structured input; text references (0.4-0.65) are more accessible but noisier
  - Sample size: 6 clinical cases with 100 API calls each provides distributional insights but limits generalizability
- **Failure signatures:**
  - Ambiguous ICD codes: R52 (Pain, unspecified) produces vague outputs; prefer specific codes (K08.89)
  - Low relatedness scores: Text reference queries with top-10 scores <0.5 may underperform baseline
  - KG-induced drift: Ontology prompts without semantic search grounding cause token distribution shifts without semantic improvement
- **First 3 experiments:**
  1. Reproduce baseline vs. CoT-ICD comparison: Run 100 generations each for one-shot and CoT prompts on Clinical Case A (2 ICD codes) using identical GPT-4 parameters; verify leftward KDE shift
  2. Ablate semantic search: Test CoT prompts with 0, 5, and 10 retrieved cases to identify the retrieval quantity threshold for semantic improvement
  3. Validate ICD vs. text reference: For Clinical Case D (9 ICD codes, 9 text references), run paired comparisons to confirm ICD query superiority; examine edge cases where text references may excel

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating human-in-the-loop evaluation significantly alter the assessment of clinical note accuracy compared to automatic metrics like cosine distance?
  - Basis in paper: The authors state, "Designing a human-in-the-loop evaluation phase to complement our current quantitative analyses will be one of our future directions," noting that different doctors write EHRs differently.
  - Why unresolved: The study relied exclusively on cosine distance, which measures semantic similarity but may not capture clinical relevance, safety, or factual precision.
  - What evidence would resolve it: Results from structured expert review or validation protocols comparing model outputs against ground truth for factual accuracy and clinical utility.

- **Open Question 2:** Can clinical ontology knowledge graphs be integrated into prompts in a way that reduces output variability and surpasses the performance of Chain-of-Thought (CoT) semantic search?
  - Basis in paper: The authors identify as a key focus, "How to effectively incorporate clinical ontology KG into prompting and how to optimally extract and align model outputs with respect to the KG."
  - Why unresolved: The current CoT KG approach resulted in the highest cosine distances and greatest variance, failing to improve precision over CoT semantic search due to increased output diversity.
  - What evidence would resolve it: A prompting strategy using natural language descriptions or graph-based representations of KGs that achieves lower cosine distance and variance than semantic search alone.

- **Open Question 3:** Does combining Retrieval-Augmented Generation (RAG) with Chain-of-Thought prompting minimize hallucinations while maintaining high semantic alignment in clinical note generation?
  - Basis in paper: The authors propose, "Use RAG in conjunction with CoT prompting," suggesting it can minimize hallucinations by feeding relevant facts into the model.
  - Why unresolved: The current study utilized semantic search on a static dataset (CodiEsp) but did not implement a dynamic RAG architecture to verify factual consistency.
  - What evidence would resolve it: Comparative error analysis showing a reduction in factually incorrect or fabricated clinical details when RAG is added to the CoT pipeline.

## Limitations
- Small sample size (6 clinical cases) limits generalizability despite 100 generations per case
- Exclusive reliance on GPT-4 without testing other LLMs may produce GPT-4-specific results
- Absence of clinical expert evaluation means semantic similarity may not translate to clinical accuracy or safety
- SNOMED CT integration methodology lacks full transparency regarding OWL-to-text mapping process

## Confidence
- **High confidence**: CoT prompting with ICD code-based semantic search outperforms standard one-shot prompting
- **Medium confidence**: SNOMED CT knowledge graph integration increases output variability without precision gains
- **Medium confidence**: ICD code queries yield higher semantic relatedness scores than text reference queries

## Next Checks
1. **Clinical Expert Validation**: Have board-certified physicians evaluate a subset of generated notes for clinical accuracy, coherence, and completeness beyond semantic similarity metrics
2. **Model Ablation Study**: Test the prompting framework with different LLMs (e.g., Claude, Gemini) to determine if results are GPT-4-specific or represent a general prompting methodology
3. **Larger Scale Testing**: Expand evaluation to include all 250 test cases from CodiEsp with stratified sampling across different clinical domains to assess generalizability and identify domain-specific performance variations