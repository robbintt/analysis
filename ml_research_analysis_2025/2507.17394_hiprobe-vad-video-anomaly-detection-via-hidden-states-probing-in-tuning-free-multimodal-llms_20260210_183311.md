---
ver: rpa2
title: 'HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free
  Multimodal LLMs'
arxiv_id: '2507.17394'
source_url: https://arxiv.org/abs/2507.17394
tags:
- anomaly
- video
- layer
- detection
- ieee
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HiProbe-VAD introduces a tuning-free framework for video anomaly
  detection by leveraging intermediate hidden states of pre-trained multimodal large
  language models (MLLMs). The method identifies and extracts information-rich representations
  from optimal intermediate layers using a Dynamic Layer Saliency Probing (DLSP) mechanism,
  which are then processed by a lightweight anomaly scorer and temporal localization
  module.
---

# HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs

## Quick Facts
- arXiv ID: 2507.17394
- Source URL: https://arxiv.org/abs/2507.17394
- Authors: Zhaolin Cai; Fan Li; Ziwei Zheng; Yanjun Qin
- Reference count: 40
- One-line primary result: Introduces a tuning-free VAD framework using intermediate hidden states of pre-trained MLLMs, achieving state-of-the-art performance among tuning-free methods

## Executive Summary
HiProbe-VAD addresses video anomaly detection by leveraging intermediate hidden states from pre-trained multimodal large language models (MLLMs). The method identifies information-rich representations in optimal intermediate layers using Dynamic Layer Saliency Probing (DLSP), then employs lightweight linear probing for detection. This approach achieves strong performance on UCF-Crime and XD-Violence datasets while maintaining the "tuning-free" property for the backbone model. The framework demonstrates cross-model generalization capabilities across different MLLM architectures without requiring fine-tuning.

## Method Summary
HiProbe-VAD extracts hidden states from intermediate layers of pre-trained MLLMs and uses DLSP to identify the most informative layer based on anomaly sensitivity, class separability, and information concentration metrics. The selected hidden states are processed by a lightweight logistic regression classifier for anomaly detection. The method employs temporal localization through Gaussian smoothing and adaptive thresholding, with an optional explanation generator using the MLLM's auto-regressive capabilities. The approach requires only a small labeled calibration set (~1%) for DLSP and scorer training, preserving the tuning-free nature of the MLLM backbone.

## Key Results
- Outperforms existing tuning-free and many traditional approaches on UCF-Crime and XD-Violence datasets
- Achieves state-of-the-art results among tuning-free methods with significant AUC improvements
- Demonstrates strong cross-model generalization across different MLLM architectures (InternVL2.5, Qwen2.5-VL, LLaVA-OneVision)
- Shows that intermediate layers provide more discriminative features for anomaly detection than output layers

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Layer Information-Rich Phenomenon
The paper claims pre-trained MLLMs encode features distinguishing normal from anomalous video content more effectively in intermediate layers than in the final output layer. As input traverses MLLM layers, early layers capture fine-grained visual features while deeper layers abstract this into semantic concepts for text generation. The paper argues an "optimal equilibrium" exists in intermediate layers where the model retains visual fidelity necessary for anomaly spotting before it's "compressed" or "over-abstracted" for language generation in final layers. Core assumption: MLLM pre-training on large-scale image-text data instills generalizable understanding of "normal" visual patterns, causing anomalies to naturally manifest as outliers in latent space of intermediate layers.

### Mechanism 2: Dynamic Layer Saliency Probing (DLSP)
A static layer choice is suboptimal; dynamically identifying the most informative layer maximizes detection performance. DLSP assesses hidden state quality across layers using composite saliency score derived from Anomaly Sensitivity (KL Divergence), Class Separability (LDR), and Information Concentration (Entropy). It aggregates these metrics (after normalization) to select layer index l* maximizing separation between normal and abnormal feature distributions. Core assumption: Optimal layer for anomaly detection is consistent within specific model architecture but requires data-driven search using small validation set rather than heuristic.

### Mechanism 3: Lightweight Linear Probing
If hidden states are sufficiently separable, simple linear classifier is adequate for detection, preserving "tuning-free" nature of backbone. Logistic Regression classifier is trained on hidden states extracted from DLSP-identified optimal layer. This "probing" approach treats massive MLLM as frozen feature extractor, avoiding computational cost of backpropagation through transformer blocks. Core assumption: Representations in optimal intermediate layer are approximately linearly separable (as suggested by high Silhouette scores), negating need for complex non-linear classifiers.

## Foundational Learning

- **Hidden States & Layer-wise Probing in Transformers**: Why needed: Core innovation relies on extracting and analyzing h_l (output of l-th transformer layer) rather than final text output. Understanding that different layers encode different levels of abstraction is essential. Quick check: Why does paper prefer Layer 20 over Layer 0 or Layer 31 in InternVL2.5? (Answer: Layer 20 balances visual detail and semantic abstraction; deeper layers lose detail for text generation).

- **Statistical Separability Metrics (LDR & Silhouette Score)**: Why needed: DLSP mechanism relies on metrics like Local Discriminant Ratio (LDR) and Silhouette Score to quantify how "good" layer is at separating normal/anomaly data. Quick check: What does higher Silhouette Score indicate in context of HiProbe-VAD? (Answer: Better defined clusters of normal vs. anomalous samples, implying easier classification).

- **Weakly-Supervised vs. Tuning-Free VAD**: Why needed: Paper positions itself against weakly-supervised methods (which require video-level labels for training) by claiming "tuning-free" backbone, though it uses small labeled set for DLSP/Scorer. Quick check: Is HiProbe-VAD completely zero-shot? (Answer: No, requires "few-shot" subset (~1%) of labeled data to train DLSP and logistic regression scorer).

## Architecture Onboarding

- **Component map**: Input (Video -> Frames -> Vision Encoder) -> Feature Extraction (MLLM Backbone processes frames) -> DLSP Module (calculates KL, LDR, Entropy for each layer on calibration set -> Selects l*) -> Anomaly Scorer (Logistic Regression takes h_l* as input -> Outputs Anomaly Probability) -> Temporal Localization (Gaussian smoothing + Adaptive Thresholding) -> Explanation Generator (Auto-regression on detected frames).

- **Critical path**: DLSP module's selection of layer l* is most critical step. If wrong layer is probed (e.g., final layer), logistic regression scorer receives features with low separability, causing entire pipeline to fail or underperform.

- **Design tradeoffs**: Scorer Complexity (uses Logistic Regression for speed/simplicity over SVMs or Neural Heads, reducing overfitting risk on small probe data but may limit capacity for complex distributions); Sampling Rate (K=8 balances accuracy and latency); Adaptive Threshold (uses μ_A + 0.2σ_A, trading simplicity for robustness against dataset shifts).

- **Failure signatures**: High False Positives (if threshold κ is too low or MLLM sensitive to "normal" motion); Low Separability (Random Guessing if DLSP selects layer with low Silhouette Score); Text-Visual Mismatch (explanation generator might hallucinate if visual features from anomaly segment are ambiguous).

- **First 3 experiments**: 1) Layer-wise Analysis (Run small subset through MLLM, extract all layer hidden states, plot KL Divergence/LDR vs. Layer index to verify "peak" exists for specific model choice); 2) DLSP vs. Last-Layer Baseline (Compare AUC of scorer trained on final layer hidden states vs. DLSP-selected layer); 3) Cross-Dataset Zero-Shot (Train DLSP/Scorer on UCF-Crime and test immediately on XD-Violence to verify cross-dataset generalization claims).

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but the limitations section suggests several areas for future research, particularly regarding the requirement for a small labeled calibration set and the consistency of the intermediate layer phenomenon across different types of anomalies.

## Limitations

- **Tuning-Free Misnomer**: The method requires approximately 1% labeled data for DLSP and scorer training, making it "tuning-free" only for the MLLM backbone, not the entire system.
- **Layer Dependency**: The optimal layer selection may be dataset-dependent, potentially limiting cross-dataset applicability despite claims of strong generalization.
- **Prompt Sensitivity**: The prompt template for MLLM input is only partially specified, which could affect reproducibility of the hidden state patterns and overall performance.

## Confidence

- **High Confidence**: Experimental results showing superior AUC/Average Precision scores compared to traditional methods on both UCF-Crime and XD-Violence datasets.
- **Medium Confidence**: Generalizability claims across different MLLM architectures (Qwen2.5-VL, LLaVA-OneVision) beyond primary InternVL2.5 experiments.
- **Medium Confidence**: Effectiveness of DLSP mechanism in consistently identifying optimal layers across diverse anomaly types and video content.
- **Low Confidence**: Complete "tuning-free" nature given requirement for labeled calibration data for DLSP and scorer training.

## Next Checks

1. **Prompt Template Sensitivity**: Systematically vary the MLLM prompt template (e.g., different scene description formats) and measure impact on layer separability metrics and final detection performance to quantify sensitivity to prompt design.

2. **Calibration Set Size Scaling**: Evaluate performance degradation as calibration set size decreases below 1%, testing whether method truly requires minimal labeled data or becomes unstable with very few examples.

3. **Temporal Resolution Analysis**: Conduct ablation studies with different sampling rates (K=4, K=16) and segment lengths to quantify trade-off between detection accuracy and computational efficiency, and identify optimal temporal granularity for various anomaly types.