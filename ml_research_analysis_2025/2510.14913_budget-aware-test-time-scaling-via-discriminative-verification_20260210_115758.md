---
ver: rpa2
title: Budget-aware Test-time Scaling via Discriminative Verification
arxiv_id: '2510.14913'
source_url: https://arxiv.org/abs/2510.14913
tags:
- verification
- discriminative
- arxiv
- generative
- solutions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores discriminative verification as a more compute-efficient
  alternative to generative verification for test-time scaling of large language models
  on reasoning tasks. While generative verifiers achieve strong performance, they
  are computationally expensive due to costly chain-of-thought generation for each
  candidate solution.
---

# Budget-aware Test-time Scaling via Discriminative Verification

## Quick Facts
- **arXiv ID:** 2510.14913
- **Source URL:** https://arxiv.org/abs/2510.14913
- **Reference count:** 40
- **Primary result:** Hybrid discriminative verification achieves up to 15.3% higher accuracy than generative verification while using only 2% of the compute overhead

## Executive Summary
This paper explores discriminative verification as a more compute-efficient alternative to generative verification for test-time scaling of large language models on reasoning tasks. While generative verifiers achieve strong performance, they are computationally expensive due to costly chain-of-thought generation for each candidate solution. The authors propose hybrid discriminative verification methods—weighted self-consistency (WSC) and pessimistic verification (PV)—that combine a lightweight discriminative verifier's scalar scores with consensus signals from self-consistency. Their empirical analysis shows that these hybrid methods consistently outperform both vanilla self-consistency and best-of-N selection across challenging benchmarks like AIME2025, LiveBench Math, and GPQA. Under practical compute budgets, hybrid discriminative verification achieves up to 15.3% higher accuracy than state-of-the-art generative verification while using only 2% of the compute overhead. The approach scales effectively with solver model size and reasoning budget, making it a practical and efficient alternative for real-world applications where inference costs must be constrained.

## Method Summary
The method involves training a discriminative verifier to score candidate solutions without generating reasoning chains. The verifier is trained on a dataset of 32k NuminaMath problems with responses from 10 LLMs, where reasoning content is stripped between specific tags. The model uses a Bradley-Terry pairwise ranking loss with L2 regularization. For inference, N candidate solutions are generated by the solver, then scored by the discriminative verifier. Two hybrid aggregation methods are proposed: Weighted Self-Consistency (WSC) which sums verifier scores by answer cluster, and Pessimistic Verification (PV) which adds a penalty term proportional to ln(N)/(n_a+1) that suppresses low-support answers. These hybrid methods combine consensus signals from self-consistency with the verifier's scalar scores to achieve both accuracy and efficiency.

## Key Results
- Hybrid discriminative verification achieves up to 15.3% higher accuracy than state-of-the-art generative verification on AIME2025, LiveBench Math, and GPQA benchmarks
- The approach uses only 2% of the compute overhead compared to generative verification methods
- Under practical compute budgets, discriminative methods outperform generative verification because scaling the number of candidate solutions produces greater returns than scaling verifications
- WSC and PV consistently outperform vanilla self-consistency and best-of-N selection across all evaluated benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid methods (WSC, PV) recover discriminative verifier reliability by combining consensus signals with scalar scores
- Mechanism: WSC aggregates verifier scores by answer cluster; PV adds a penalty term proportional to ln(N)/(n_a+1) that suppresses low-support answers even if they score highly. This guards against long-tail vulnerability where BoN selects confident-but-incorrect outliers
- Core assumption: Verifier assigns higher scores to correct solutions on average, and majority consensus correlates with correctness
- Evidence anchors:
  - [abstract] "while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism"
  - [Section 2.2] "To guard against the long-tail of high-scoring but incorrect responses, hybrid discriminative verification methods combine the consensus signal from SC with the verifier's signal from BoN"
  - [corpus] Weak direct corpus support for this specific hybrid mechanism; related work on PRMs focuses on generative verification, not discriminative hybrids
- Break condition: If verifier is miscalibrated (assigns higher scores to incorrect solutions), hybrid methods may amplify errors

### Mechanism 2
- Claim: Discriminative verifiers achieve orders-of-magnitude lower latency than generative verifiers by avoiding autoregressive decoding
- Mechanism: Discriminative verifiers replace the LM head with a scalar value head, requiring only prefill (single forward pass). Generative verifiers must decode full CoT rationales token-by-token, incurring memory-bandwidth and KV-cache overhead
- Core assumption: Verification task can be reduced to scalar scoring without explicit reasoning traces
- Evidence anchors:
  - [abstract] "achieves up to 15.3% higher accuracy... while using only 2% of the compute overhead"
  - [Section 3.1.2, Table 2] Verifying 32 solutions discriminatively takes 1.66s vs 3423.7s for generative (M=2)—over 2000× faster
  - [corpus] Related work (Process Reward Models That Think) focuses on step-wise generative verification, confirming discriminative vs. generative as an active design axis
- Break condition: If verification requires explicit reasoning chains (e.g., catching logical fallacies), scalar scores may be insufficient

### Mechanism 3
- Claim: Under fixed compute budgets, allocating FLOPs to more candidate samples outperforms allocating to stronger verification
- Mechanism: Pass@N (upper bound) rises with N, but verification cannot recover answers absent from candidate pool. Discriminative methods spend ~98% of budget on sampling; generative methods split budget, reducing N for same compute
- Core assumption: Solver produces at least one correct solution within the sampled N; verifier is better than random at ranking
- Evidence anchors:
  - [Section 3.1.1] "under practical FLOP budgets, hybrid discriminative verification techniques outperform generative verification... scaling the number of candidate solutions produces greater returns than scaling verifications"
  - [Section 3.1.1, Figure 3] Hybrid discriminative verification optimal for compute budgets < 2.2×10^16 FLOPs when M=2
  - [corpus] Budget-Aware Tool-Use paper similarly finds optimal compute allocation depends on task structure and budget constraints
- Break condition: For easy tasks where N saturates quickly, additional samples provide diminishing returns and stronger verification may dominate

## Foundational Learning

- Concept: **Prefill vs. Decode Compute in Transformers**
  - Why needed here: The efficiency claim hinges on discriminative verifiers avoiding decode-phase compute
  - Quick check question: For a 1.5B discriminative verifier processing 32 solutions of length 8K tokens, what fraction of total FLOPs is prefill vs. decode?

- Concept: **Bradley-Terry Ranking Loss**
  - Why needed here: The verifier is trained to maximize P(correct > incorrect) across all pairs in a batch
  - Quick check question: Given a batch with 3 correct and 5 incorrect solutions, how many pairwise comparisons does the loss compute?

- Concept: **Self-Consistency as Baseline**
  - Why needed here: SC provides the consensus signal hybrid methods build upon; understanding its failure modes (majority-wrong scenarios) motivates PV's penalty term
  - Quick check question: For N=32 with answer distribution {a: 15, b: 12, c: 5}, what does SC select and what cluster size penalty would PV assign to each?

## Architecture Onboarding

- Component map:
  - Solver (e.g., DeepSeek-R1-Distill-Qwen-32B) -> Generates N candidate solutions with CoT reasoning
  - Discriminative Verifier (1.5B scalar-head model) -> Outputs r(s_i) ∈ ℝ for each solution
  - Aggregation Module -> Implements WSC (sum scores by answer) or PV (mean score - α·penalty)

- Critical path: Sample N solutions → Strip reasoning content → Verifier forward pass → Group by answer → Apply aggregation rule → Return a*

- Design tradeoffs:
  - N vs. M: Under fixed budget, increasing N (more samples) outperforms increasing M (verifications per sample) until Pass@N saturates
  - α in PV: α=0.5 balances consensus vs. verifier; lower α trusts verifier more (appropriate for stronger verifiers)
  - Include vs. exclude reasoning content: Paper finds excluding reasoning traces improves verifier accuracy (reduces noise)

- Failure signatures:
  - BoN degrades at high N: verifier over-ranks confident incorrect solutions → use WSC/PV instead
  - PV underperforms SC: verifier is anti-correlated with correctness → check training data quality
  - WSC matches SC exactly: verifier scores are uniform → check verifier head initialization and loss convergence

- First 3 experiments:
  1. Replicate SC@32 baseline on validation set to establish pass@N upper bound and SC accuracy
  2. Train 1.5B discriminative verifier on NuminaMath subset, monitoring score margin (correct vs. incorrect) as in Figure 2
  3. Compare BoN@32, WSC@32, PV@32 at α∈{0.1, 0.5, 1.0} against SC@32; verify hybrid methods recover or exceed SC when BoN fails

## Open Questions the Paper Calls Out
None

## Limitations
- The empirical advantage relies critically on the assumption that the discriminative verifier is moderately accurate but not perfectly calibrated; severe misalignment could amplify errors
- Performance on tasks with subjective or multi-modal correct answers remains untested, as the analysis focuses on mathematical reasoning where majority consensus correlates strongly with correctness
- The compute-efficiency claim depends on relative FLOPs of prefill vs. decode operations, which may vary with model architecture and hardware implementation

## Confidence
- **High Confidence:** The fundamental latency advantage of discriminative over generative verification (2000× faster in the reported measurements) and the general trend that under fixed compute budgets, increasing candidate samples yields better returns than increasing verification strength
- **Medium Confidence:** The specific accuracy gains of WSC and PV over vanilla SC and BoN, as these depend on the quality of the trained verifier and may not generalize to all solver-verifier pairs or reasoning domains
- **Low Confidence:** The exact magnitude of the 15.3% accuracy improvement and 2% compute overhead claim, as these are benchmark-specific and sensitive to the choice of N, M, and α parameters

## Next Checks
1. **Verifier Calibration Test:** Evaluate the discriminative verifier's score distribution on a held-out validation set to confirm that correct answers receive higher scores than incorrect ones on average, and measure calibration error
2. **Cross-Domain Generalization:** Apply the hybrid discriminative verification pipeline to a non-mathematical reasoning benchmark (e.g., commonsense QA or code generation) to test robustness beyond the reported domains
3. **Parameter Sensitivity Analysis:** Systematically vary N (number of candidates), M (number of verifications per candidate), and α (penalty weight in PV) to map the Pareto frontier of accuracy vs. compute cost and identify the optimal operating point for different budget regimes