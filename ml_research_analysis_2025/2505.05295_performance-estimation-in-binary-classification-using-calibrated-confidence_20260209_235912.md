---
ver: rpa2
title: Performance Estimation in Binary Classification Using Calibrated Confidence
arxiv_id: '2505.05295'
source_url: https://arxiv.org/abs/2505.05295
tags:
- distribution
- confidence
- metrics
- shift
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CBPE (Confidence-based Performance Estimation),
  a novel method for estimating binary classification metrics without access to ground
  truth labels. CBPE treats confusion matrix elements as random variables and leverages
  calibrated confidence scores to estimate their distributions, enabling estimation
  of any binary classification metric.
---

# Performance Estimation in Binary Classification Using Calibrated Confidence

## Quick Facts
- arXiv ID: 2505.05295
- Source URL: https://arxiv.org/abs/2505.05295
- Reference count: 40
- Primary result: CBPE achieves MAE <1% for accuracy/precision and <3% for recall/F1 on TableShift benchmark under distribution shift

## Executive Summary
This paper presents CBPE (Confidence-based Performance Estimation), a novel method for estimating binary classification metrics without access to ground truth labels. CBPE treats confusion matrix elements as random variables and leverages calibrated confidence scores to estimate their distributions, enabling estimation of any binary classification metric. The method provides full probability distributions for metrics including accuracy, precision, recall, and F1, allowing uncertainty quantification through valid confidence intervals.

Theoretical guarantees show CBPE produces unbiased and consistent estimates when the model is calibrated and confidence-consistent. Empirical results demonstrate strong performance: on the TableShift benchmark with 8 datasets, CBPE achieved mean absolute errors typically below 1% for accuracy and precision, and below 3% for recall and F1, even under distribution shift. A synthetic experiment with controlled covariate shift confirmed CBPE's effectiveness across different feature dimensions, with estimation errors remaining small relative to performance changes.

## Method Summary
CBPE estimates binary classification performance by treating confusion matrix elements as Poisson binomial random variables. For positive predictions, the confidence score S directly represents the probability that the prediction is correct, forming the TP distribution. Similarly, 1-S for negative predictions forms the TN distribution. These distributions are combined to derive other confusion matrix elements, and any metric defined as a function of these elements can be estimated as a full probability distribution. The method uses FFT-based algorithms for efficient Poisson binomial computation and provides shortcut formulas for point estimates when monitoring window size exceeds 500 samples.

## Key Results
- On TableShift benchmark with 8 datasets, CBPE achieved MAE <1% for accuracy and precision, and <3% for recall and F1 under distribution shift
- Synthetic experiment confirmed estimation errors remain small relative to performance changes across different feature dimensions (100-1000 features)
- Strong correlation (ρ~0.96 for accuracy) between calibration error and estimation error, validating the calibration requirement
- Method provides valid confidence intervals through Highest Density Intervals computed from full probability distributions

## Why This Works (Mechanism)

### Mechanism 1: Confidence-to-Probability Mapping via Calibration
When a model's confidence scores are calibrated, they can be interpreted as the probability that each prediction is correct, enabling label-free performance estimation. For positive predictions, the confidence score S directly represents P(Y=1|S=s). For negative predictions, 1-S represents P(Y=0|S=s). This transforms each prediction into a Bernoulli trial with known probability. Core assumption: The model is calibrated (P(Y=1|S=s) = s for all s) and confidence-consistent (predictions are deterministic given a confidence score).

### Mechanism 2: Poisson Binomial Distribution for Confusion Matrix Elements
Treating confusion matrix elements as Poisson binomial random variables enables full probability distributions rather than point estimates alone. Each positive prediction contributes confidence S_i as a heterogeneous probability parameter to the TP count distribution. The sum follows a Poisson binomial. Similarly, 1-S_i for negative predictions gives TN distribution. This captures estimation uncertainty naturally. Core assumption: Predictions are independent.

### Mechanism 3: Metric Distribution Derivation via Joint Probability Propagation
Any metric defined as a function of confusion matrix elements can be estimated as a full probability distribution by iterating over the joint distribution of component random variables. For recall, iterate over joint of X_TP and X_FN, computing X_recall = X_TP/(X_TP + X_FN) for each combination weighted by probability. Similar approach for F1. Core assumption: For recall and F1 approximations, monitoring window size is sufficiently large for asymptotic convergence.

## Foundational Learning

- **Confidence Calibration (Expected Calibration Error)**: Why needed: CBPE's theoretical guarantees require the monitored model to be calibrated. Uncalibrated models produce biased estimates proportional to their calibration error. Quick check: Given a model that outputs confidence 0.8 on 100 predictions, approximately how many should be correct if the model is perfectly calibrated?

- **Poisson Binomial Distribution**: Why needed: Core probabilistic machinery for representing uncertainty in confusion matrix estimates when each prediction has different correctness probability. Quick check: How does the Poisson binomial differ from a standard binomial distribution, and why does this matter when confidence scores vary across predictions?

- **Confusion Matrix and Derived Metrics**: Why needed: CBPE estimates the confusion matrix first, then derives metrics. Understanding the formulas is essential for interpreting shortcut estimators. Quick check: Why can accuracy and precision expectations be computed directly from confidence scores, while recall and F1 require iteration (or approximations)?

## Architecture Onboarding

- **Component map**: Input layer (predictions Ŷ_i and calibrated confidence scores S_i) -> Split routing (separate positive predictions I+ from negative predictions I-) -> Poisson binomial computation (distributions for X_TP and X_TN) -> Derived variables (X_FP = n+ - X_TP, X_FN = n- - X_TN) -> Metric calculator (shortcut formulas or Algorithms 1-2) -> Confidence bands (Algorithm 3 for HDI)

- **Critical path**: 1) Verify model is calibrated on reference data (compute ACE with M=20 bins) 2) Apply calibration mapping if needed (isotonic regression) 3) For each monitoring window, collect confidence scores 4) If window size ≥500 and only point estimates needed → use shortcuts 5) If uncertainty quantification needed or window small → compute full distributions via FFT-based Poisson binomial then Algorithms 1-2

- **Design tradeoffs**: Full distribution vs. shortcuts: Full gives confidence intervals; shortcuts are O(n) vs O(n²). Window size vs. precision: Larger windows reduce variance but delay detection; error drops ~1/√n. Calibration method choice: Paper uses isotonic regression; ACE with adaptive bins (M=20) is robust choice.

- **Failure signatures**: High ACE with large estimation errors: Model uncalibrated → recalibrate or reject estimates. Recall/F1 variance high despite good calibration: Window too small → increase or accept uncertainty. Estimates diverge after distribution shift: Likely concept shift → method fundamentally cannot handle this.

- **First 3 experiments**: 1) Calibration health check: On held-out labeled data, compute ACE and compare CBPE estimates vs. true metrics. Target: MAE <1% for accuracy/precision when ACE <1%. 2) Window size sensitivity: Vary monitoring window from 50 to 1000 samples. Verify recall/F1 shortcut error drops as predicted by Figure 1 (should be <0.001 at n=100). 3) Distribution shift robustness test: Use TableShift dataset split (ID vs OoD). Compare estimation error vs. calibration error change. Expect strong correlation (ρ~0.96 for accuracy) between ACE increase and estimation degradation.

## Open Questions the Paper Calls Out

- **How can confidence-based estimators be adapted to provide valid estimates under concept shift (p_s(y|x) ≠ p_t(y|x))?** The Discussion section states that estimators "struggle under concept shift" and calls these issues "important topics for future research." Concept shift breaks the stochastic dependency between features and labels, rendering static confidence scores obsolete and the estimation problem fundamentally unidentifiable.

- **What methods can ensure a model remains calibrated under covariate and label shifts to preserve the validity of CBPE?** The authors note that "calibration error typically increases with covariate shift... and label shift" and explicitly state that "methods for remaining calibrated under these shifts are needed." CBPE's theoretical guarantees assume perfect calibration; however, standard models often lose calibration when the data distribution changes, leading to biased estimates.

- **How can the specific components of dataset shift (covariate, concept, label) be quantified from a finite sample using principled metrics?** The Discussion states that "no principled metrics exist for even approximating the shift from a finite sample in any of the components." Current benchmarks use ad hoc metrics that may measure correlated phenomena rather than distinct shift components, making it difficult to diagnose the cause of performance degradation.

## Limitations
- Method fundamentally cannot handle concept shift where p(y|x) changes, as this breaks the identifiability of performance estimation without labels
- Small monitoring windows (<100 samples) produce imprecise recall/F1 estimates due to the 1/√n approximation error rate
- High calibration error causes biased estimates proportional to the calibration error (correlation ρ~0.96 for accuracy)

## Confidence

- **CBPE produces unbiased and consistent estimates**: High confidence (supported by Theorem 1 and empirical results across 8 datasets with MAE <1% for accuracy/precision)
- **Calibration is the key requirement**: High confidence (direct theoretical link established; calibration error directly predicts estimation error)
- **Method fails under concept shift**: High confidence (explicitly stated limitation with theoretical justification)
- **Poisson binomial distribution enables uncertainty quantification**: Medium confidence (novel application; core mathematical machinery sound but limited direct corpus validation)

## Next Checks
1. **Calibration error sensitivity test**: Systematically vary calibration quality (ACE from 0.1% to 5%) on synthetic data and measure corresponding estimation error degradation to validate the claimed 0.96 correlation coefficient.

2. **Concept shift failure mode**: Design experiment where p(y|x) changes after distribution shift (e.g., flip labels with probability p). Verify CBPE estimates diverge from true values while maintaining calibration on marginal distributions.

3. **Small window precision**: For n=50, 100, 200 monitoring windows, compute exact recall/F1 distributions (not approximations) and compare to shortcut estimates to verify the 1/√n error rate empirically.