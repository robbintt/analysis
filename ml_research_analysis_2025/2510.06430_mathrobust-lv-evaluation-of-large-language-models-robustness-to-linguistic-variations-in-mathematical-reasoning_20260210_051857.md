---
ver: rpa2
title: 'MathRobust-LV: Evaluation of Large Language Models'' Robustness to Linguistic
  Variations in Mathematical Reasoning'
arxiv_id: '2510.06430'
source_url: https://arxiv.org/abs/2510.06430
tags:
- math
- reasoning
- arxiv
- variation
- robustness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MathRobust-LV, a test set and evaluation
  methodology to measure the robustness of large language models (LLMs) to linguistic
  variations in mathematical reasoning. The authors generate variations of high school-level
  math problems by changing surface details like names, contexts, and variables while
  preserving numerical structure and answers, mirroring how instructors rephrase problems
  across assessments.
---

# MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning

## Quick Facts
- **arXiv ID**: 2510.06430
- **Source URL**: https://arxiv.org/abs/2510.06430
- **Reference count**: 12
- **Primary result**: 34 models show 2-11% accuracy drops when linguistic variations are applied to mathematical problems

## Executive Summary
This paper introduces MathRobust-LV, a test set and evaluation methodology to measure the robustness of large language models (LLMs) to linguistic variations in mathematical reasoning. The authors generate variations of high school-level math problems by changing surface details like names, contexts, and variables while preserving numerical structure and answers, mirroring how instructors rephrase problems across assessments. Evaluating 34 open and closed-source models ranging from 0.6B to 70B+ parameters, they find that most models experience accuracy drops of 2-11% when moving from baseline to variant problems, with smaller models showing the largest degradation (9-11%). Even strong models show measurable performance declines, while frontier models like GPT-5 and Gemini-2.5-pro remain comparatively stable.

## Method Summary
The authors created MathRobust-LV by generating linguistic variations of high school-level math problems, systematically changing surface details like names, contexts, and variables while preserving numerical structure and answers. They evaluated 34 models ranging from 0.6B to 70B+ parameters using zero-shot prompting, comparing performance on original problems versus their linguistically varied counterparts. The evaluation captures how models handle the same mathematical reasoning task presented with different linguistic framing, simulating real educational scenarios where instructors rephrase problems.

## Key Results
- Most models experience accuracy drops of 2-11% when moving from baseline to variant problems
- Smaller models show the largest degradation (9-11%) compared to larger models
- Frontier models like GPT-5 and Gemini-2.5-pro remain comparatively stable with minimal performance decline
- The results reveal that linguistic robustness is a fundamental challenge in mathematical reasoning

## Why This Works (Mechanism)
Linguistic variations expose whether LLMs rely on surface-level pattern matching or genuine mathematical reasoning. When problems are rephrased with different names, contexts, and variable structures, models that depend on memorizing specific problem templates fail, while those with deeper reasoning capabilities maintain performance. The systematic perturbations reveal whether models understand the underlying mathematical structure or merely associate specific word patterns with solution procedures.

## Foundational Learning
- **Linguistic variation in education**: Understanding how instructors rephrase problems across assessments is crucial for evaluating real-world LLM deployment in educational settings. Quick check: Compare textbook problem variations across different editions or publishers.
- **Mathematical reasoning vs. pattern matching**: Distinguishing between genuine reasoning and surface-level pattern recognition is essential for assessing model capabilities. Quick check: Test models on structurally identical problems with different surface features.
- **Model size-performance relationship**: The correlation between parameter count and robustness to linguistic variation indicates scalability limits. Quick check: Plot accuracy degradation against model size across the evaluated range.
- **Zero-shot prompting limitations**: The evaluation method reveals how models perform without task-specific fine-tuning or few-shot examples. Quick check: Compare zero-shot vs. few-shot performance on variant problems.

## Architecture Onboarding
**Component Map**: Problem Generation -> Model Evaluation -> Performance Analysis -> Robustness Assessment
**Critical Path**: Generate linguistic variations → Prompt models zero-shot → Compare baseline vs. variant accuracy → Analyze degradation patterns
**Design Tradeoffs**: Synthetic variations vs. natural problem diversity; controlled perturbations vs. real-world complexity; zero-shot evaluation vs. few-shot instruction
**Failure Signatures**: Accuracy drops of 2-11% indicate sensitivity to linguistic variation; larger drops for smaller models suggest scaling benefits for robustness
**First Experiments**:
1. Generate 10 linguistic variations for a single problem type and test across model families
2. Compare performance on baseline problems with and without chain-of-thought prompting
3. Evaluate a subset of models on real-world textbook problem variations

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic linguistic variations may not capture full complexity of real-world problem rephrasing
- Performance drops could partly reflect sensitivity to specific transformation patterns rather than genuine reasoning deficits
- Zero-shot evaluation without exploring few-shot or chain-of-thought prompting strategies may underestimate model capabilities

## Confidence
- **High Confidence**: Core finding that linguistic variations impact mathematical reasoning performance across model families
- **Medium Confidence**: Claim that "linguistic robustness is a fundamental challenge" requires more nuanced interpretation
- **Medium Confidence**: Assertion that "strong models show measurable performance declines" should be contextualized by modest absolute changes

## Next Checks
1. **Real-world validation**: Test MathRobust-LV patterns against human-generated problem variants from actual textbooks or assessments
2. **Cross-lingual robustness**: Evaluate whether sensitivity to linguistic variation extends to problems in languages other than English
3. **Prompting strategy impact**: Investigate whether different prompting strategies can reduce performance gap between baseline and variant problems