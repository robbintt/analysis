---
ver: rpa2
title: 'MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing
  and Incorporating Outlier Scores'
arxiv_id: '2504.16786'
source_url: https://arxiv.org/abs/2504.16786
tags:
- compression
- token
- mooscomp
- outlier
- compressor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MOOSComp enhances a BERT-based long-context compressor by mitigating
  over-smoothing and incorporating outlier scores. It introduces an inter-class cosine
  similarity loss to improve token classification accuracy and uses outlier scores
  to preserve rare but task-critical tokens, thereby improving generalization.
---

# MOOSComp: Improving Lightweight Long-Context Compressor via Mitigating Over-Smoothing and Incorporating Outlier Scores

## Quick Facts
- arXiv ID: 2504.16786
- Source URL: https://arxiv.org/abs/2504.16786
- Reference count: 40
- Primary result: Achieves 3.3x speedup at 4x compression ratio on resource-constrained mobile devices while outperforming state-of-the-art task-agnostic hard prompt methods.

## Executive Summary
MOOSComp addresses the challenge of compressing long contexts for efficient LLM inference by improving a BERT-based token classification compressor. The method introduces two key innovations: an inter-class cosine similarity loss to combat over-smoothing in deep Transformer representations, and an outlier score mechanism to preserve rare but task-critical tokens. By balancing classifier probability with outlier detection, MOOSComp achieves superior generalization across four long-context benchmarks including understanding and reasoning tasks, demonstrating significant speedups while maintaining or improving downstream task performance.

## Method Summary
MOOSComp trains a binary token classifier (preserve/discard) using xlm-roberta-large as the encoder, with a combined loss function that includes both cross-entropy and an inter-class cosine similarity term to reduce over-smoothing. During compression, tokens are scored using a weighted combination of classifier probability and normalized outlier scores computed separately for each predicted class. The method uses 20% held-out data per task to tune the α hyperparameter balancing these two components, achieving effective task-agnostic compression without requiring task-specific training data.

## Key Results
- Consistently outperforms state-of-the-art task-agnostic hard prompt methods across four benchmarks
- Achieves 3.3x speedup at 4x compression ratio on resource-constrained mobile devices
- Demonstrates improved generalization by preserving rare but critical tokens through outlier scoring
- Shows effectiveness on diverse long-context understanding and reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding an inter-class cosine similarity loss during training improves token classification accuracy by increasing discriminability between "preserve" and "discard" token representations.
- Mechanism: The loss term directly penalizes high cosine similarity between token representations of different classes in the final BERT layer (Eq. 2: LCS = average pairwise cosine similarity across class boundaries). This counteracts the natural tendency of deep Transformer layers to produce homogeneous representations.
- Core assumption: The BERT encoder exhibits over-smoothing, where token representations converge toward similarity as depth increases, impairing downstream classification.
- Evidence anchors:
  - [abstract] "In the training phase, we add an inter-class cosine similarity loss term to penalize excessively similar token representations, thereby improving the token classification accuracy."
  - [section 3.2, Figure 2] Empirical analysis shows inter-class cosine similarity starts ~0.2 at layer 0 but approaches 1.0 at the final layer for both xlm-roberta-large and the fine-tuned llmlingua-2 model.
  - [corpus] Weak corpus connection; related compression papers (ATACompressor, ACoRN) focus on task-adaptive or noise-robust compression but do not address over-smoothing in BERT-based classifiers.
- Break condition: If the base encoder does not suffer from representation collapse (e.g., shallow models, or architectures with built-in anti-smoothing mechanisms), this loss may provide minimal benefit while adding optimization complexity.

### Mechanism 2
- Claim: Integrating Z-score-based outlier scores with classifier probabilities preserves rare but task-critical tokens that would otherwise be discarded in task-agnostic compression.
- Mechanism: During compression, tokens are split by predicted class (threshold 0.5). Within each group, Z-scores are computed against the group mean/std, L2-normalized, then min-max scaled to [0,1]. Final retention metric: m = α × p_preserve + (1−α) × s_norm (Eq. 7).
- Core assumption: Tokens with representations distant from their class centroid are more likely to be task-relevant, even if the classifier assigns them moderate preservation probabilities.
- Evidence anchors:
  - [abstract] "During the compression phase, we introduce outlier scores to preserve rare but critical tokens that are prone to be discarded in task-agnostic compression."
  - [section 4.3, Table 7] Ablation shows MOOSComp w/o snorm underperforms full MOOSComp on LongBench (e.g., SingleDoc 2K: 32.6 vs 34.9).
  - [corpus] Noise-Robust Abstractive Compression (FMR=0.56) addresses irrelevant/noisy context but does not use outlier detection for token preservation—different problem framing.
- Break condition: If task-critical tokens are statistically common (not outliers) or if the domain has uniform token importance (e.g., code where structural tokens matter more than rarity), outlier scores may add noise without benefit. Section 4.2 notes code completion did not benefit from outlier scores.

### Mechanism 3
- Claim: Computing outlier scores separately per predicted class is more effective than global computation because the inter-class loss creates distinct class-conditional distributions.
- Mechanism: Rather than computing Z-scores over all tokens jointly, the method partitions tokens into Î′_p (p_preserve ≥ 0.5) and Î′_d (p_preserve < 0.5), then computes separate mean/variance for normalization within each partition.
- Core assumption: The inter-class cosine similarity loss creates sufficiently separated representation distributions per class; computing a single global distribution would conflate these and produce misleading outlier scores.
- Evidence anchors:
  - [section 3.4] "Since we use inter-class cosine similarity loss during training to increase the inter-class distance, it would be inappropriate to estimate a single distribution over all the token representations."
  - [section 4.3, Table 7] MOOSComp w/ snorm variant (single distribution) achieves 33.0 SingleDoc 2K vs 34.9 for full MOOSComp; KNN-based alternatives also underperform.
  - [corpus] No direct corpus evidence; related work does not address partitioned outlier scoring.
- Break condition: If class representations overlap significantly despite the loss term, or if one class has very few tokens (highly imbalanced), per-class statistics may be unstable.

## Foundational Learning

- **Over-smoothing in Transformers**
  - Why needed here: The paper's core diagnosis is that BERT layers progressively collapse token representations toward similarity, impairing classification. Understanding this phenomenon (rank collapse of attention matrices, loss of token distinctiveness) is prerequisite to appreciating why the inter-class loss helps.
  - Quick check question: If you computed pairwise cosine similarity between all token representations at layer 0 vs layer 24 of a BERT model, what pattern would you expect in a model suffering from over-smoothing?

- **Token classification / Sequence labeling**
  - Why needed here: MOOSComp frames compression as binary token classification (preserve/discard). Understanding how classifiers map token representations to class probabilities, and how cross-entropy loss operates per-token, is essential.
  - Quick check question: Given a sequence of n tokens with binary labels, how would you compute the cross-entropy loss for this task?

- **Z-score outlier detection**
  - Why needed here: The outlier score computation uses Z-score normalization (distance from mean in units of standard deviation) followed by L2 norm and min-max scaling. This is a simple parametric outlier detection method.
  - Quick check question: For a token representation h ∈ R^d, its Z-score z = (h − μ)/σ produces a d-dimensional vector. How does taking the L2 norm of z aggregate information across dimensions, and what does a high value indicate?

## Architecture Onboarding

- **Component map**: [Input prompt + GPT-generated labels] → [xlm-roberta-large encoder] → [linear classifier] → [Cross-entropy loss + Inter-class cosine similarity loss (weighted by β)]
- **Compression**: [Input prompt] → [xlm-roberta-large encoder] → [Classifier probabilities] → [Per-class Z-score outlier computation] → [Weighted combination (α)] → [Top-̂n token selection by compression ratio]

- **Critical path**:
  1. During training, the inter-class loss must successfully reduce last-layer similarity (verify with Figure 2-style analysis on your data).
  2. During compression, the α hyperparameter must be tuned per task/domain (paper uses 20% held-out data for selection).
  3. The compressor outputs a ranked token list; the compression ratio determines the cutoff threshold.

- **Design tradeoffs**:
  - β (loss weight): Higher values reduce over-smoothing more aggressively but may destabilize training. Paper selected β=0.001 based on validation accuracy.
  - α (compression metric weight): Higher values trust the classifier more; lower values prioritize outlier tokens. Paper found optimal α varies by task (0.5 for MeetingBank, 0.7–0.8 for LongBench/GSM8K).
  - Compression ratio (1/τ): Higher ratios reduce tokens more aggressively, speeding up inference but risking information loss. Paper tested 3x–5x.

- **Failure signatures**:
  - Code completion tasks show minimal benefit from outlier scores (Section 4.2)—structural token patterns may not align with "rarity = importance."
  - If validation accuracy plateaus or degrades as β increases, the loss may be too dominant relative to cross-entropy.
  - If compressed prompts lose critical named entities or numerical values, α may be too high (over-trusting classifier that wasn't trained on this domain).

- **First 3 experiments**:
  1. **Reproduce the over-smoothing analysis (Figure 2)** on your target BERT variant: compute inter-class cosine similarity per layer on a sample dataset. If last-layer similarity is already low (<0.7), the inter-class loss may not help.
  2. **Ablation study on your domain**: Compare MOOSComp vs. MOOSComp w/o LCS vs. MOOSComp w/o snorm on a held-out validation set. This isolates which mechanism drives gains for your specific task.
  3. **α sensitivity sweep**: On a small validation set (e.g., 100–200 examples), test α ∈ {0.3, 0.5, 0.7, 0.9} at fixed compression ratio. Plot task metric vs. α to identify the optimal balance for your domain before full evaluation.

## Open Questions the Paper Calls Out

- **Question**: How can the outlier score mechanism be adapted to effectively identify task-critical tokens in inherently structured domains like code, where statistical outliers may not correlate with logical importance?
- **Basis in paper**: [explicit] The authors state in Section 4.2 that "the proposed outlier detection mechanism does not benefit the code completion task," suspecting this failure is due to the "unique inherently structural nature of code texts."
- **Why unresolved**: The current Z-score method relies on distributional rarity in embedding space, which likely fails to capture the syntactic logic required for code completion.
- **What evidence would resolve it**: Demonstrating a modified outlier metric (e.g., one accounting for syntax trees or token frequency in code) that improves compression performance on the LongBench code task.

## Limitations

- **Dataset dependency**: MOOSComp's performance gains are demonstrated primarily on MeetingBank (summarization) and LongBench (mixed tasks). The method's effectiveness on non-summarization domains like code completion is limited (Section 4.2), suggesting potential domain sensitivity that is not fully characterized across diverse long-context tasks.
- **Hyperparameter sensitivity**: The optimal α (compression metric weight) varies significantly by task (0.5 for MeetingBank, 0.7–0.8 for others), and β (loss weight) is fixed at 0.001 without sensitivity analysis. This suggests performance may degrade if these values are not carefully tuned per domain, but the paper does not provide a systematic tuning protocol or general guidance.
- **GPT-4 label generation**: The binary "preserve/discard" labels are derived from GPT-4 compressed outputs, which introduces model-specific biases. If GPT-4's compression preferences differ from human or task-specific preferences, the classifier may learn suboptimal token importance patterns that do not generalize.

## Confidence

- **High confidence**: The inter-class cosine similarity loss effectively reduces over-smoothing in BERT representations (verified via Figure 2 analysis). The partitioned outlier scoring approach is mathematically sound given the training loss design.
- **Medium confidence**: MOOSComp outperforms state-of-the-art task-agnostic hard prompt methods on the tested benchmarks. The 3.3x speedup at 4x compression ratio is achievable on resource-constrained mobile devices as claimed.
- **Low confidence**: MOOSComp's effectiveness generalizes across all long-context tasks beyond summarization and reasoning benchmarks. The method will consistently outperform alternatives when deployed on diverse, unseen domains without careful hyperparameter tuning.

## Next Checks

1. **Domain transferability test**: Apply MOOSComp to a non-summarization, non-reasoning long-context task (e.g., document retrieval, code generation, or dialogue understanding) and measure whether it maintains the same relative performance gains. Compare against domain-specific compressors if available.
2. **Hyperparameter robustness analysis**: Perform a grid search over α ∈ [0.3, 0.9] and β ∈ [0.0001, 0.01] on a small validation set from your target domain. Plot task performance vs. hyperparameters to identify the stability region and determine if performance is robust or highly sensitive.
3. **Compression ratio vs. latency tradeoff**: At fixed α and β, measure task performance and actual latency as compression ratio varies from 2x to 6x on your target hardware. Verify whether the claimed 3.3x speedup at 4x compression is consistently achievable or if the relationship is non-linear due to hardware-specific factors.