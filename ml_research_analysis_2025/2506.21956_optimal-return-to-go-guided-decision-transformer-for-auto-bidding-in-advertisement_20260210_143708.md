---
ver: rpa2
title: Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement
arxiv_id: '2506.21956'
source_url: https://arxiv.org/abs/2506.21956
tags:
- training
- transformer
- decision
- dataset
- trajectories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses challenges in auto-bidding for online advertising
  by proposing a novel approach using the Decision Transformer framework. The key
  problem is that traditional Decision Transformers require a preset return-to-go
  (RTG) value and are restricted by mixed-quality trajectories in the training data.
---

# Optimal Return-to-Go Guided Decision Transformer for Auto-Bidding in Advertisement

## Quick Facts
- arXiv ID: 2506.21956
- Source URL: https://arxiv.org/abs/2506.21956
- Reference count: 18
- Primary result: R* DT achieves 36.97 average score, 41.89 conversions, and 97.97% budget consumption on dataset 1, outperforming baseline auto-bidding methods.

## Executive Summary
This paper addresses key challenges in auto-bidding for online advertising by proposing a novel approach using the Decision Transformer framework. The authors identify two critical issues: the need for preset return-to-go (RTG) values and the limitation of mixed-quality trajectories in training data. Their solution, R* Decision Transformer, introduces a three-stage process that eliminates manual RTG specification, estimates upper-bound RTG values using quantile regression, and iteratively generates high-quality trajectories via a simulator to augment training data. Experiments on a public bidding dataset demonstrate significant performance improvements over baseline methods.

## Method Summary
The R* DT framework extends the standard Decision Transformer by introducing RTG prediction and iterative data augmentation. The method operates in three stages: first, R DT learns to predict both actions and RTG values from state sequences using MSE loss; second, R^ DT estimates the highest achievable RTG for each state using quantile regression loss to target upper bounds; finally, R* DT generates trajectories using the suboptimal policy plus noise, evaluates them with a simulator, and iteratively augments the training dataset with high-RTG trajectories. This approach progressively improves the policy by enhancing the quality of training data through simulator-based trajectory generation and selection.

## Key Results
- R* DT achieves 36.97 average score on dataset 1 compared to baseline methods
- The approach delivers 41.89 average conversions while maintaining 97.97% budget consumption
- Iterative data augmentation shows consistent improvement across 5 iterations with diminishing returns after iteration 4-5

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Predicting RTG from state eliminates the need for manually specified return targets that may be inaccurate.
- Mechanism: R DT extends the traditional Decision Transformer to jointly predict both action and RTG from historical sequences using MSE loss on observed RTG values.
- Core assumption: The training data contains sufficient state-RTG pairs to learn a meaningful mapping.
- Evidence anchors: [abstract] "R DT stores actions based on state and RTG value" and [Section 2.1] "The predicted RTG R_t(s_t) is learned using the MSE loss."
- Break condition: If state representation is insufficiently informative, the learned mapping will produce noisy predictions that degrade action quality.

### Mechanism 2
- Claim: Quantile regression loss enables estimation of the upper-bound RTG achievable from a given state.
- Mechanism: R^ DT uses asymmetric loss function derived from quantile regression, with λ approaching 0 to target the maximum observed RTG.
- Core assumption: High-RTG trajectories in training set represent near-optimal behavior.
- Evidence anchors: [abstract] "We forecast the highest value (within the training set) of RTG for a given state" and [Section 2.2] "As λ approaches 0, Ŕ_t(s_t) effectively approximates the maximum R_t."
- Break condition: If high-RTG trajectories are sparse or training set lacks coverage of high-reward regions, estimated upper bound will be unreliable.

### Mechanism 3
- Claim: Iteratively generating and selecting high-reward trajectories via simulator improves training data quality.
- Mechanism: Starting with R^ DT's suboptimal policy, the model generates candidate trajectories using stochastic noise and evaluates them with a simulator that models auction outcomes.
- Core assumption: The simulator accurately reflects true auction dynamics and generated trajectories transfer to real deployment.
- Evidence anchors: [abstract] "we generate trajectories and select those with high rewards (using a simulator) to augment our training dataset" and [Section 3.3, Table 2] ablation shows R* DT(1) through R* DT(5) improving from score 34.34 to 36.97.
- Break condition: If simulator diverges from real auction dynamics, generated trajectories may be high-reward in simulation but low-reward in deployment.

## Foundational Learning

- Concept: **Decision Transformer (DT) architecture and sequence modeling**
  - Why needed here: R* DT builds directly on DT; understanding how transformers process (state, RTG, action) sequences as tokens is prerequisite to comprehending the modified input structure and dual prediction heads.
  - Quick check question: Can you explain why DT treats reinforcement learning as a sequence modeling problem, and what role the causal attention mask plays?

- Concept: **Return-to-Go (RTG) in offline RL**
  - Why needed here: The entire paper centers on eliminating preset RTG and predicting optimal RTG; without understanding RTG as cumulative future reward and its role as a conditioning signal, the mechanism will be opaque.
  - Quick check question: In standard DT, how does the RTG value at inference time affect action selection, and what happens if an unrealistic RTG is specified?

- Concept: **Quantile regression and expectile regression**
  - Why needed here: The R^ DT loss function is derived from quantile regression; understanding asymmetric loss functions and how they target distribution percentiles is necessary to grasp why this estimates an upper bound.
  - Quick check question: For a quantile regression loss with λ=0.1, what percentile of the target distribution is the model being trained to predict?

## Architecture Onboarding

- Component map:
  - Preprocessed dataset -> R DT backbone (Transformer with dual heads) -> R^ DT layer (quantile loss) -> Simulator module -> Iterative training loop (generate, filter, augment, retrain)

- Critical path:
  1. Preprocess offline bidding dataset into (state, action, reward) trajectories; compute RTG for each timestep.
  2. Train R DT with MSE loss on both action and RTG prediction until validation loss stabilizes.
  3. Switch to quantile regression loss (R^ DT), tune λ; train until RTG predictions plateau at upper quantile.
  4. Build/configure simulator using historical opponent eCPM distributions; validate simulator fidelity against held-out trajectories.
  5. Run iterative generation: for each iteration k, generate trajectories, compute RTG via simulator, select top trajectories, merge into T^(k), retrain R^ DT.
  6. Monitor RTG distribution shift; stop when improvement saturates.

- Design tradeoffs:
  - λ selection: Lower λ pushes toward maximum RTG but risks overfitting to rare trajectories; higher λ is more conservative but may underestimate achievable returns.
  - Simulator fidelity vs. speed: High-fidelity simulator improves trajectory quality but slows iteration; low-fidelity may introduce distribution shift.
  - Number of iterations: Table 2 shows diminishing returns after iteration 4-5 on dataset 1; over-iteration may overfit to simulator artifacts.
  - Noise scale ε_t: Controls exploration during generation; too low → insufficient diversity; too high → degenerate trajectories.

- Failure signatures:
  - RTG prediction collapse: If RTG head predicts constant values regardless of state, check learning rate balance between action and RTG heads.
  - Simulator exploitation: If generated trajectories achieve high RTG in simulation but low score at test time, simulator is misaligned—validate against held-out real trajectories.
  - No improvement across iterations: If Ŕ^(k) does not increase, either λ is too conservative or training data lacks exploitable structure—inspect RTG distribution per iteration.
  - Budget consumption divergence: If budget ratio oscillates wildly across datasets, policy is overfitting to specific CPA constraints—check regularization.

- First 3 experiments:
  1. Baseline replication: Train standard DT with oracle RTG, R DT (MSE RTG prediction), and R^ DT (quantile RTG) on dataset 1; compare scores to validate each component's contribution.
  2. Ablation on λ: Sweep λ ∈ {0.01, 0.05, 0.1, 0.2, 0.5}; plot predicted RTG distribution vs. actual training RTG distribution; identify where predictions exceed data support.
  3. Simulator validation: Generate 1000 trajectories using R^ DT; compute RTG via simulator; compare simulator-predicted outcomes vs. actual outcomes on held-out test logs; report correlation coefficient to quantify simulator fidelity before running iterative training.

## Open Questions the Paper Calls Out

- **Open Question 1**: How robust is the R* DT framework when deployed in non-stationary real-world auction environments where opponent strategies or user conversion probabilities drift significantly from the historical logs used to construct the simulator?
- **Open Question 2**: Does the iterative augmentation of synthetic trajectories risk compounding errors from the simulator or the suboptimal policy, potentially leading to a distribution shift that degrades performance?
- **Open Question 3**: How sensitive is the convergence of the R* DT policy to the choice of the quantile parameter λ in the R^ loss function?

## Limitations
- The simulator fidelity is critical but not explicitly validated—the method assumes the simulator accurately models real auction dynamics.
- Key hyperparameters (λ for quantile loss, noise scale εt, iteration stopping criteria) are not specified, creating ambiguity about reproducibility.
- Convergence guarantees for R*_t are limited—the paper shows monotonic improvement but does not prove convergence to the true optimal RTG.

## Confidence
- **High confidence**: The three-mechanism framework is internally consistent and the mathematical formulations are sound. The MSE and quantile regression losses are correctly specified.
- **Medium confidence**: The experimental results on the Tianchi dataset demonstrate clear performance improvements, but the absence of ablation on λ and noise parameters reduces confidence in optimal hyperparameter selection.
- **Low confidence**: The claim that R*_DT converges toward "optimal" policy is weakly supported—the paper shows monotonic improvement but does not prove optimality or compare against ground-truth optimal policies.

## Next Checks
1. **Simulator fidelity test**: Generate 1000 trajectories using R^_DT with εt noise; compute RTG via simulator and via actual auction outcomes on held-out test logs; report Pearson correlation coefficient and RMSE. Target: correlation > 0.8 to ensure simulator is not the primary source of performance gains.

2. **λ sensitivity analysis**: Train R^_DT with λ ∈ {0.01, 0.05, 0.1, 0.2, 0.5} on dataset 1; plot predicted RTG distribution vs. actual training RTG distribution per λ; identify where predictions exceed data support (overfitting) or remain too conservative.

3. **Real-world deployment validation**: Deploy R*_DT on a subset of real ad inventory (not simulator); compare conversions, CPA adherence, and budget consumption against standard DT and R^_DT baselines over 7 days; report statistical significance (p < 0.05) to confirm simulator-independent performance gains.