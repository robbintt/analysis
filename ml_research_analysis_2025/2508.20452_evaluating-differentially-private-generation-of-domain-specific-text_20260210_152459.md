---
ver: rpa2
title: Evaluating Differentially Private Generation of Domain-Specific Text
arxiv_id: '2508.20452'
source_url: https://arxiv.org/abs/2508.20452
tags:
- data
- text
- privacy
- synthetic
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a benchmark for evaluating differentially\
  \ private synthetic text generation across domain-specific datasets. The benchmark\
  \ addresses key challenges like prior exposure and representativeness by using gated-access,\
  \ domain-specific corpora (medical, clinical, legal, and financial) and realistic\
  \ privacy budgets (\u03B5 \u2208 {0.5, 1, 2, 4})."
---

# Evaluating Differentially Private Generation of Domain-Specific Text

## Quick Facts
- arXiv ID: 2508.20452
- Source URL: https://arxiv.org/abs/2508.20452
- Reference count: 40
- Key outcome: Introduces benchmark for differentially private synthetic text generation across domain-specific datasets, showing 15-28% utility retention under privacy constraints

## Executive Summary
This paper presents a comprehensive benchmark for evaluating differentially private synthetic text generation in domain-specific contexts. The benchmark addresses critical challenges in prior work, including prior exposure and representativeness, by utilizing gated-access domain-specific corpora from medical, clinical, legal, and financial domains. The evaluation employs realistic privacy budgets (ε ∈ {0.5, 1, 2, 4}) and assesses two state-of-the-art methods—DP-Gen (DP-SGD) and AUG-PE—using both utility metrics (downstream classification F1 scores) and fidelity measures (MAUVE, entity overlap, text length divergence).

The results reveal substantial performance degradation when applying differential privacy, with synthetic data retaining only 15-28% of real data utility even without privacy guarantees. Domain-specific datasets, particularly gated-access ones, present significant challenges that highlight the limitations of current approaches for realistic privacy-preserving data sharing scenarios. The findings underscore the need for improved methods that can better balance privacy requirements with utility preservation in domain-specific text generation applications.

## Method Summary
The paper introduces a benchmark that evaluates differentially private synthetic text generation across four domain-specific datasets: medical, clinical, legal, and financial. The evaluation addresses prior limitations in the field by using gated-access datasets that better reflect real-world privacy constraints and data availability. Two state-of-the-art methods are evaluated: DP-Gen, which uses DP-SGD for training, and AUG-PE, an augmentation-based approach. The privacy budget range (ε ∈ {0.5, 1, 2, 4}) represents realistic constraints for practical applications. Utility is measured through downstream classification F1 scores, while fidelity is assessed using MAUVE, entity overlap, and text length divergence metrics. The benchmark specifically addresses challenges related to prior exposure and representativeness that have plagued previous evaluations in this space.

## Key Results
- Synthetic data retains only 15-28% of real data utility even without privacy guarantees
- Substantial performance degradation observed under realistic privacy budgets (ε ∈ {0.5, 1, 2, 4})
- Domain-specific gated-access datasets pose significant challenges for current differentially private methods
- Both DP-Gen (DP-SGD) and AUG-PE methods show limitations in preserving utility while maintaining privacy

## Why This Works (Mechanism)
The benchmark works by creating controlled evaluation conditions that better reflect real-world privacy constraints through gated-access domain-specific datasets. By limiting prior exposure to training data and using representative samples from specific domains, the evaluation methodology provides more realistic assessment of differentially private methods' performance. The use of downstream task performance as a utility metric, combined with fidelity measures, creates a comprehensive evaluation framework that captures both functional utility and data quality aspects.

## Foundational Learning
- Differential Privacy (DP): A mathematical framework for preserving individual privacy in data analysis and machine learning
  - Why needed: Provides theoretical guarantees for privacy protection while enabling data utility
  - Quick check: Verify that privacy budget (ε) values are correctly implemented and calibrated

- DP-SGD (Differentially Private Stochastic Gradient Descent): A training algorithm that adds noise to gradients to achieve differential privacy
  - Why needed: Enables training machine learning models with provable privacy guarantees
  - Quick check: Confirm that noise calibration follows theoretical privacy accounting

- MAUVE metric: A measure for comparing generated text distributions to reference distributions
  - Why needed: Provides quantitative assessment of text generation quality beyond simple n-gram overlap
  - Quick check: Validate that MAUVE scores are computed correctly across all evaluation runs

- Entity overlap: Measure of how well synthetic data preserves named entities from real data
  - Why needed: Ensures that domain-specific terminology and important entities are preserved in synthetic data
  - Quick check: Verify entity recognition and matching algorithms are consistent across datasets

- Text length divergence: Measure of distributional differences in text lengths between real and synthetic data
  - Why needed: Captures stylistic and structural properties of domain-specific text
  - Quick check: Confirm that length distributions are properly normalized and compared

## Architecture Onboarding
Component map: Dataset Preprocessing -> DP Training -> Synthetic Generation -> Evaluation Pipeline
Critical path: Gated-access dataset acquisition -> Privacy parameter configuration -> Model training with DP guarantees -> Multi-metric evaluation
Design tradeoffs: Privacy-utility tradeoff necessitates careful calibration of noise parameters; gated-access datasets limit sample size but improve representativeness
Failure signatures: Over-privacy (ε too low) leads to random synthetic output; under-privacy (ε too high) risks privacy leakage; dataset bias affects downstream task performance
First experiments:
1. Baseline evaluation without privacy constraints to establish utility ceiling
2. Incremental privacy budget testing (ε = 0.5 → 4) to map privacy-utility curve
3. Cross-domain transferability testing to assess method generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to domain-specific gated-access datasets, excluding general-purpose text corpora
- Privacy budget range (ε ∈ {0.5, 1, 2, 4}) may not capture full spectrum of practical scenarios, particularly stricter (ε < 0.5) or more relaxed (ε > 4) requirements
- Evaluation methodology focuses on downstream classification performance, potentially overlooking other quality aspects like semantic coherence, stylistic consistency, or factual accuracy

## Confidence
- High confidence in benchmark methodology and dataset curation: The gated-access approach and controlled evaluation environment provide robust foundations for comparative analysis
- Medium confidence in performance degradation claims: While the observed utility drops are substantial, they are specific to the evaluated metrics and datasets, and may not generalize across all domain-specific applications
- Medium confidence in privacy-utility tradeoff analysis: The findings are consistent with theoretical expectations but limited by the specific algorithms and parameter settings evaluated

## Next Checks
1. Replicate experiments using additional domain-specific datasets beyond the four gated-access corpora to assess generalizability across different text domains and privacy requirements
2. Extend evaluation to include qualitative assessments of synthetic text quality through human evaluation, complementing the quantitative metrics to provide a more comprehensive view of utility degradation
3. Test alternative differentially private mechanisms and training approaches beyond DP-Gen and AUG-PE to determine whether the observed performance limitations are inherent to the privacy constraint or specific to the evaluated methods