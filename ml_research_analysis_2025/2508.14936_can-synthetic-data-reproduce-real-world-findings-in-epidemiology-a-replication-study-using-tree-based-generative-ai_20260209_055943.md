---
ver: rpa2
title: Can synthetic data reproduce real-world findings in epidemiology? A replication
  study using tree-based generative AI
arxiv_id: '2508.14936'
source_url: https://arxiv.org/abs/2508.14936
tags:
- data
- synthetic
- original
- dataset
- mean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether synthetic data can reproduce key
  epidemiological findings across six diverse studies, including blood pressure, anthropometry,
  myocardial infarction, accelerometry, loneliness, and diabetes. Using adversarial
  random forests (ARF), synthetic data was generated and compared to original data
  results for various statistical analyses, including linear, logistic, and Cox regressions.
---

# Can synthetic data reproduce real-world findings in epidemiology? A replication study using tree-based generative AI

## Quick Facts
- arXiv ID: 2508.14936
- Source URL: https://arxiv.org/abs/2508.14936
- Reference count: 40
- Primary result: ARF-generated synthetic data closely replicated original epidemiological findings across six diverse studies.

## Executive Summary
This study evaluates whether synthetic data generated by adversarial random forests (ARF) can reproduce key epidemiological findings across six diverse studies. Using ARF to generate synthetic data from original datasets and comparing statistical analyses (linear, logistic, Cox regressions), the researchers found that synthetic data closely replicated original results even for datasets with low sample size-to-dimensionality ratios. The study demonstrates ARF's practical utility for rapid prototyping and data sharing in epidemiology, while highlighting the importance of task-specific data preparation and pre-deriving variables to improve synthesis quality.

## Method Summary
The researchers used ARF implemented in the R `arf` package to generate synthetic data from six epidemiological datasets (NAKO, BSR-U45, GFHS). They trained 20 ARF models per dataset and sampled synthetic data 5 times per model, creating 100 synthetic datasets total. The method preserves missing value patterns by generating corresponding missingness rather than imputing. They compared synthetic vs. original results using medians and percentile-based 95% CIs across replicates for regression coefficients, means, proportions, and other statistical measures. Both full-dataset and task-specific approaches were tested, with task-specific synthesis showing superior results.

## Key Results
- Synthetic data reproduced original regression coefficients and confidence intervals across all six studies with high fidelity
- Task-specific synthesis with pre-derived variables consistently outperformed full-dataset synthesis
- ARF handled datasets with challenging sample size-to-dimensionality ratios (down to 4.7) while maintaining result accuracy
- Preserving missingness patterns was critical for replicating analyses dependent on systematic missing data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Limiting datasets to analysis-relevant variables and pre-deriving features improves synthetic data quality and result stability.
- Mechanism: Reducing dimensionality simplifies the joint distribution the generative model must learn. Pre-deriving variables before synthesis, rather than synthesizing raw variables and deriving them afterwards, removes the burden of learning complex derivation logic from the model, leading to more accurate statistical replication.
- Core assumption: A smaller, task-specific variable set captures the essential dependencies required for the target statistical analysis, and pre-computed derivations are more reliable than post-synthesis derivations.
- Evidence anchors: [abstract] "Reducing dimensionality and pre-deriving variables further improved synthesis quality and stability." [section] (Section 3.2) "The effect of the reduced dimensionality and complexity on the synthesis quality was apparent... task-specific dataset synthesis results were consistently closer."
- Break condition: Overly aggressive dimensionality reduction that removes critical interaction variables required for the downstream analysis.

### Mechanism 2
- Claim: Adversarial Random Forests (ARF) generate synthetic tabular data by learning a mixture distribution based on local variable independence.
- Mechanism: ARF iteratively trains a random forest classifier to distinguish between original data and a permuted (independent) version. This process partitions the data manifold into regions where original and permuted entries are indistinguishable. Within each region, it assumes variable independence and estimates univariate densities, which are then combined into a global mixture distribution for sampling.
- Core assumption: The complex joint distribution of the original data can be approximated by a mixture of simpler, locally independent distributions within the learned partitions.
- Evidence anchors: [abstract] "Using adversarial random forests (ARF), synthetic data was generated..." [section] (Section 2.2.1) "It iteratively learns data dependencies... This is utilised to assume variable independence for the original data locally and perform variable-wise univariate density estimation... combined in a global mixture distribution."
- Break condition: Data exhibiting strong, non-local dependencies that cannot be adequately captured by local partitions.

### Mechanism 3
- Claim: Preserving missing value patterns in synthetic data is essential for replicating analyses on datasets with systematic missingness.
- Mechanism: Instead of imputing missing values during synthesis, ARF can be configured to generate corresponding missing entries. This preserves the structure and potential informativeness of the missingness (e.g., missing not at random), ensuring that analyses dependent on this structure align with original findings.
- Core assumption: Missing data patterns in the original epidemiological datasets are not completely random (MCAR) and carry structural information that influences statistical outcomes.
- Evidence anchors: [section] (Section 2.2.2 & 4) "we let ARF generate corresponding missingness... to prevent divergent analysis results caused by systematic missingness patterns." and "If missingness is not introduced during synthesis, results may deviate from the original ones in the presence of systematic missingness patterns."
- Break condition: The method is unnecessary if data is missing completely at random, where imputation would suffice.

## Foundational Learning

**Joint Distribution Modeling**
- Why needed here: The core task of any generative model is to learn the joint probability distribution *P(X₁, X₂, ..., Xₙ)* of the original data. ARF approximates this using a mixture of simpler distributions. Understanding this goal clarifies why reducing dimensionality (simpler P) helps.
- Quick check question: How does assuming local variable independence simplify the estimation of a high-dimensional joint distribution?

**Statistical Replication vs. Distributional Similarity**
- Why needed here: The paper's primary metric for quality is *utility*—can the synthetic data reproduce the *same statistical findings* (e.g., regression coefficients, p-values) as the original? This is a more direct and practical metric than generic distributional similarity scores (like KL-divergence).
- Quick check question: Why is replicating the confidence interval of a logistic regression coefficient a stronger test of utility than matching marginal histograms?

**Sample Size-to-Dimensionality Ratio**
- Why needed here: This ratio is a key indicator of how difficult a dataset is to model. The paper notes that even with a low ratio, ARF performed well, but stability (e.g., confidence interval width) was still affected.
- Quick check question: What is the likely effect on the confidence intervals of synthetic data results when the original dataset has a low sample size-to-dimensionality ratio?

## Architecture Onboarding

**Component map:** Original Dataset → Data Pre-processor → ARF Generator → Analysis Replicator → Evaluator

**Critical path:** Task-Specific Data Preparation → ARF Training → Synthetic Data Generation → Replicated Analysis → Evaluation. The most critical step for success is the initial data preparation, as it directly impacts the model's difficulty.

**Design tradeoffs:**
- Full vs. Task-Specific Synthesis: A full synthesis requires no prior knowledge of analyses but may be lower quality. A task-specific synthesis yields higher fidelity but requires knowing the target analysis in advance.
- Missing Value Handling: Imputation is simpler but can distort patterns; generating missingness is more faithful but complex to implement.
- Model Choice (ARF vs. Deep Learning): ARF is fast, interpretable, and runs on CPU. Deep learning methods (GANs, VAEs) may offer higher capacity for extremely complex data but are computationally expensive and harder to tune.

**Failure signatures:**
- Systematic Bias: Synthetic regression coefficients consistently biased in one direction.
- Widened Confidence Intervals: Synthetic 95% CIs that are consistently wider than the originals, indicating model uncertainty.
- Subgroup Collapse: Accurate overall results but wildly inaccurate results for small or rare subgroups (e.g., the hypertension variable in the myocardial infarction study).
- Derived Variable Drift: Discrepancies in variables derived post-synthesis (e.g., BMI) compared to pre-derived versions.

**First 3 experiments:**
1. **Baseline Replication:** Select one study with a high sample size and low dimensionality (e.g., Schikowski et al. on blood pressure). Run ARF on the full dataset without special preprocessing and attempt to replicate the main analysis. Document baseline fidelity.
2. **Task-Specific Improvement:** Take the same study from Experiment 1, create a task-specific dataset with only the variables used in the analysis, and pre-derive any necessary features (e.g., age groups). Re-run the synthesis and replication. Quantify the improvement in result fidelity and stability.
3. **Stress Test on Low Sample-Size Data:** Select a study with a challenging ratio (e.g., Breau et al. or Wienbergen et al.). Apply both full and task-specific synthesis. Pay close attention to the replication of subgroup analyses and variables with few positive cases (e.g., the hypertension variable) to identify failure modes.

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: Does ARF-generated synthetic data provide robust privacy guarantees against inference attacks in an epidemiological context?
- Basis in paper: [explicit] The authors state, "As we did not assess privacy protection in this study, future work should include dedicated evaluations—–especially since this remains a non-trivial task and an active area of research."
- Why unresolved: While the study confirms statistical utility, it explicitly notes that synthetic data does not inherently provide privacy guarantees, yet no privacy metrics were calculated.
- What evidence would resolve it: An evaluation of membership inference and attribute disclosure risks on the generated synthetic datasets.

**Open Question 2**
- Question: How effective is ARF synthetic data for specific downstream applications like data balancing and missing data augmentation?
- Basis in paper: [explicit] The authors note, "The specific use of synthetic ARF data for tasks such as data balancing or augmentation remains to be explored."
- Why unresolved: The study focused on replicating existing statistical analyses rather than testing whether the synthetic data improves the performance of predictive models in imbalanced scenarios.
- What evidence would resolve it: Experiments measuring model performance (e.g., sensitivity/specificity) when training on ARF-augmented datasets compared to original data.

**Open Question 3**
- Question: How does ARF compare to other generative methods (e.g., GANs, diffusion models) in reproducing epidemiological findings?
- Basis in paper: [explicit] The authors acknowledge that "a comparison with other synthetic data approaches was beyond the scope of this work but would help to better contextualize ARF's relative performance."
- Why unresolved: Without comparative benchmarks, it is unclear if ARF's superior computational speed and ease of use come at the cost of statistical fidelity compared to more complex deep learning methods.
- What evidence would resolve it: A comparative study replicating the same epidemiological analyses using various state-of-the-art tabular data synthesizers.

## Limitations

- The study relies on proprietary epidemiological datasets (NAKO, BSR-U45, GFHS) that are not publicly available, making independent verification difficult.
- The paper demonstrates strong utility but does not extensively validate privacy guarantees or compare against alternative synthetic data methods beyond GANs and CT-GANs in a single case.
- The assumption that local variable independence within ARF partitions adequately captures complex epidemiological dependencies remains an untested limitation for highly non-linear relationships.

## Confidence

- **High Confidence**: ARF can reproduce key epidemiological findings (regression coefficients, descriptive statistics) across diverse study designs and variable types when applied to task-specific datasets with pre-derived variables.
- **Medium Confidence**: The improvement in synthetic data quality from dimensionality reduction and pre-derivation is significant, though the exact magnitude depends on dataset characteristics and target analyses.
- **Medium Confidence**: ARF handles missing data patterns better than imputation-based approaches when missingness is systematic, but the privacy implications of preserving these patterns require further investigation.

## Next Checks

1. **Independent Dataset Replication**: Apply the ARF methodology to a publicly available epidemiological dataset (e.g., NHANES or UK Biobank public samples) with documented analyses to verify reproducibility claims outside the original data ecosystem.
2. **Privacy Risk Assessment**: Conduct formal privacy analysis (e.g., membership inference attacks) on the synthetic datasets to quantify actual disclosure risk, particularly for rare subgroups or small samples where synthetic data may be most identifiable.
3. **Cross-Method Comparison**: Benchmark ARF against modern deep learning approaches (e.g., CTAB-GAN, PrivBayes) on identical epidemiological datasets to assess relative performance in both utility (statistical replication) and privacy preservation.