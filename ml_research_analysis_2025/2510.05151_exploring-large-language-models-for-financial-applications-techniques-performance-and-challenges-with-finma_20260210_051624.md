---
ver: rpa2
title: 'Exploring Large Language Models for Financial Applications: Techniques, Performance,
  and Challenges with FinMA'
arxiv_id: '2510.05151'
source_url: https://arxiv.org/abs/2510.05151
tags:
- financial
- tasks
- language
- performance
- finma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FinMA, a financial large language model, demonstrates strong performance
  in sentiment analysis and classification but struggles with numerical reasoning,
  entity recognition, and summarization. Built on LLaMA architecture and fine-tuned
  on the Financial Instruction Tuning (FIT) dataset, FinMA excels in zero-shot sentiment
  analysis (93.9% F1 on FPB) and news headline classification (97.5% average F1).
---

# Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA

## Quick Facts
- arXiv ID: 2510.05151
- Source URL: https://arxiv.org/abs/2510.05151
- Authors: Prudence Djagba; Abdelkader Y. Saley
- Reference count: 0
- Primary result: FinMA demonstrates strong sentiment analysis (93.9% F1) but struggles with numerical reasoning (7.4% EM) and entity recognition (64.1% F1)

## Executive Summary
FinMA is a financial large language model built on LLaMA architecture and fine-tuned on the Financial Instruction Tuning (FIT) dataset. It achieves strong performance in zero-shot sentiment analysis and news headline classification but exhibits significant weaknesses in numerical reasoning, named entity recognition, and text summarization. The model's limitations stem from its general-purpose architecture and the relatively small size of the financial training dataset. Future work will focus on specialized fine-tuning, retrieval-augmented generation, and multimodal integration to address these shortcomings.

## Method Summary
FinMA-7B-full was fine-tuned from LLaMA-7B using the FIT dataset (136k instruction samples) with AdamW optimizer (lr=8e-6, warmup=5%). Training ran for 3 epochs on 8×A100 40GB GPUs with batch size 32 and max sequence length 2048. The model was evaluated on the FLARE benchmark across six financial NLP tasks using zero-shot and few-shot settings. Inference uses float16 precision with automatic device mapping. The evaluation pipeline leverages Hugging Face datasets and the PIXIU framework.

## Key Results
- Zero-shot sentiment analysis achieves 93.9% F1 on FPB dataset
- News headline classification reaches 97.5% average F1 across all classes
- Named entity recognition drops from 69.0% (zero-shot) to 64.1% F1 (5-shot)
- Question answering scores only 7.4% Exact Match on FinQA
- Text summarization achieves minimal 2.8% ROUGE-1 score

## Why This Works (Mechanism)

### Mechanism 1: Instruction Tuning for Semantic Alignment
FinMA achieves high performance in sentiment analysis and classification because instruction tuning aligns the model's latent semantic knowledge with financial domain labels. The FIT dataset reformulates raw text into (instruction, input, response) triplets, adjusting LLaMA weights to map financial jargon to categorical outputs via next-token prediction. This works when the base LLaMA model possesses sufficient general linguistic competence and primarily needs domain-specific alignment.

### Mechanism 2: Architecture-Induced Numerical Fragility
The model's poor performance in numerical reasoning stems from LLaMA's token-probabilistic nature combined with data sparsity. As a decoder-only architecture treating numbers as text tokens, FinMA attempts to predict the next token in calculation sequences probabilistically rather than algorithmically. This fundamental limitation persists because the FIT dataset focuses on textual reasoning over numerical calculations.

### Mechanism 3: Negative Transfer in Few-Shot Extraction
FinMA's NER performance degrades in few-shot settings because the generative model struggles to align precise span boundaries with variable prompt examples. Unlike discriminative models that classify tokens, FinMA generates text, and few-shot examples may confuse the model between replicating format versus extraction logic, leading to boundary errors.

## Foundational Learning

- **Concept: Decoder-only Autoregressive Modeling**
  - Why needed here: FinMA predicts the next token based solely on previous context (causal masking), explaining its fluency in generation but struggles with bidirectional context tasks like entity recognition.
  - Quick check question: Why would a model trained to predict "the next word" struggle to identify the *start* and *end* of a specific entity in a sentence simultaneously?

- **Concept: Instruction Tuning (Supervised Fine-Tuning)**
  - Why needed here: The FIT dataset converts raw financial data into instruction-response pairs, aligning a base model to follow commands rather than just completing text.
  - Quick check question: How does formatting a dataset as `(Instruction: Analyze sentiment. Input: [Text]. Response: [Label])` differ from standard masked language modeling used in BERT?

- **Concept: Hallucination vs. Calculation Error**
  - Why needed here: Distinguishing between a model "guessing" a plausible-looking number (hallucination) and failing a logic step is critical for debugging financial LLMs.
  - Quick check question: In the context of FinQA, if the model outputs "15.2" when the answer is "15.4", is this likely a calculation error or a token probability issue?

## Architecture Onboarding

- **Component map:** LLaMA-7B/30B (Transformer, RoPE embeddings, SwiGLU activation) -> Financial Instruction Tuning (FIT) Dataset (136k samples, JSON format) -> FLARE Benchmark (Evaluation suite covering SA, NER, QA, SMP)

- **Critical path:**
  1. Environment: Setup PyTorch + HuggingFace (8×A100 or equivalent for full fine-tuning; T4/Kaggle viable for inference only)
  2. Data Prep: Convert raw financial text to FIT JSON format (id, conversations, text, label)
  3. Training: Execute full fine-tuning (AdamW, LR 8e-6) to produce FinMA weights
  4. Inference: Run zero-shot prompts against the FLARE benchmark

- **Design tradeoffs:**
  - Full Fine-tuning vs. LoRA: Full fine-tuning provides deep adaptation but requires high compute costs (128 GPUs for 30B); LoRA reduces memory but may capture less domain nuance
  - Generative vs. Discriminative: LLaMA allows flexible instruction following but sacrifices precision on span-based tasks (NER) compared to discriminative models like FinBERT

- **Failure signatures:**
  - Span Slippage (NER): Model identifies entity type correctly but generates incorrect start/end indices
  - Numerical Drift (QA): Answers are semantically plausible but mathematically wrong due to token probability
  - Instruction Overfit: Model ignores specific questions and recites general financial knowledge

- **First 3 experiments:**
  1. Sentiment Baseline Reproduction: Run zero-shot inference on FPB dataset to verify 93.9% F1 score
  2. Few-Shot NER Stress Test: Compare zero-shot vs 5-shot performance on FIN Agreements to observe boundary errors
  3. Prompt Engineering for Math: Attempt to improve FinQA scores by appending "Think step-by-step" to prompts

## Open Questions the Paper Calls Out

### Open Question 1
Can retrieval-augmented generation (RAG) effectively reduce hallucinations and improve factual consistency in FinMA's financial predictions? The paper proposes considering RAG implementation to reduce hallucinations and enhance factual consistency, noting LLaMA's 57% TruthfulQA score indicates potential misinformation risks.

### Open Question 2
Would fine-tuning FinMA on specialized financial entity datasets (e.g., FINER-139) improve NER performance to competitive levels with GPT-4? Authors propose fine-tuning on specialized financial datasets like FINER-139 for named entity recognition, noting current 64.1% F1 falls far below GPT-4's 83%.

### Open Question 3
Can hybrid multimodal architectures that integrate text and time-series data improve stock movement prediction accuracy beyond the current 50.5–51.2% range? Authors identify developing hybrid models to integrate text and time-series data as a research direction, noting LLaMA's text-only architecture struggles with FIT's multimodal SMP datasets.

### Open Question 4
Would incorporating numerical reasoning datasets (e.g., GSM8K) into training improve FinMA's question answering performance from 7.4% to competitive levels? Authors state training on numerical datasets like GSM8K can strengthen question answering capabilities, attributing poor FinQA performance to LLaMA's weak mathematical reasoning (10.6% on MATH benchmark).

## Limitations

- Performance constrained by limited FIT dataset size (136k samples) that may not comprehensively cover financial reasoning diversity
- Decoder-only architecture fundamentally limits bidirectional context tasks like NER, creating an architectural ceiling
- Evaluation focuses on English-only datasets, leaving multilingual financial reasoning capabilities unexplored
- Paper does not address potential domain shift between FIT training data and real-world financial applications

## Confidence

- **High Confidence:** Zero-shot sentiment analysis and headline classification performance (93.9% F1 on FPB, 97.5% average F1) - supported by direct benchmark results
- **Medium Confidence:** NER performance degradation in few-shot settings - documented but lacks strong supporting evidence for negative transfer mechanism
- **Low Confidence:** Numerical reasoning and question answering results (7.4% Exact Match) - attributed to architectural limitations without ablation studies

## Next Checks

1. **Ablation Study on Architecture:** Implement BERT-based FinBERT model fine-tuned on FIT dataset and compare its NER and numerical reasoning performance against FinMA to isolate whether architecture or training data is the limiting factor.

2. **Temporal Domain Shift Test:** Evaluate FinMA's performance on financial news headlines from different years (2019 vs 2023) to quantify how temporal changes in financial terminology affect zero-shot capabilities.

3. **Multimodal Integration Experiment:** Extend FinMA with a simple image feature extractor (using CLIP embeddings) for tabular data visualization tasks, testing whether proposed multimodal extension could address numerical reasoning limitations.