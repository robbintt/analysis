---
ver: rpa2
title: Temporal Latent Variable Structural Causal Model for Causal Discovery under
  External Interferences
arxiv_id: '2511.10031'
source_url: https://arxiv.org/abs/2511.10031
tags:
- causal
- variables
- latent
- data
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles causal discovery in time series data where external
  interferences complicate the identification of true causal relationships. The authors
  address this by introducing a Temporal Latent Variable Structural Causal Model that
  incorporates latent variables to represent unobserved external factors.
---

# Temporal Latent Variable Structural Causal Model for Causal Discovery under External Interferences

## Quick Facts
- arXiv ID: 2511.10031
- Source URL: https://arxiv.org/abs/2511.10031
- Reference count: 39
- Outperforms existing methods including PCMCI, VAR-LiNGAM, SVAR-FCI, CLH-NV, LPCMCI, and DYNOTEARS

## Executive Summary
This paper introduces a novel approach to causal discovery in time series data where external interferences complicate causal identification. The authors propose a Temporal Latent Variable Structural Causal Model that explicitly models unobserved external factors through latent variables. The key innovation is decomposing the causal matrix into binary adjacency and continuous weight components, enabling simultaneous handling of causal presence/absence and strength. The model is estimated using variational inference with informative priors when available, demonstrating superior performance over existing methods across synthetic and real-world datasets including fMRI and financial time series.

## Method Summary
The Temporal Latent Variable Structural Causal Model incorporates latent variables to represent external interferences that affect observed time series variables. The model decomposes the causal matrix into two parts: an adjacency matrix indicating the presence/absence of causal relationships (binary) and a weight matrix representing the strength of these relationships (continuous). This decomposition is estimated through variational inference, with informative priors incorporated when prior knowledge about external interference exists. The model builds on vector autoregressive processes but extends them to handle latent confounders representing external factors.

## Key Results
- Outperforms existing approaches (PCMCI, VAR-LiNGAM, SVAR-FCI, CLH-NV, LPCMCI, DYNOTEARS) across multiple metrics
- Maintains high precision while effectively filtering out spurious correlations caused by external interferences
- Demonstrates superior performance on both synthetic data and real-world applications (fMRI and financial time series)

## Why This Works (Mechanism)
The model works by explicitly modeling unobserved external factors as latent variables, which addresses a fundamental limitation of existing causal discovery methods that assume all relevant variables are observed. By decomposing the causal matrix into adjacency (binary) and weight (continuous) components, the model can simultaneously determine whether causal relationships exist and quantify their strength. The variational inference approach allows for efficient estimation while incorporating prior knowledge about external interferences when available. This dual decomposition strategy enables the model to distinguish between true causal relationships and spurious correlations induced by external factors.

## Foundational Learning
- **Latent Variable Structural Causal Models**: Used to represent unobserved external factors that influence observed variables; needed because real-world systems often have unmeasured confounders
- **Variational Inference**: Enables efficient posterior approximation in complex latent variable models; needed because exact inference is intractable for high-dimensional time series
- **Vector Autoregressive Processes**: Provides the temporal dependency framework; needed to model how variables evolve over time and influence each other across time lags
- **Causal Matrix Decomposition**: Separates structure (adjacency) from magnitude (weights); needed to handle both presence/absence and strength of causal relationships
- **Informative Priors**: Incorporates domain knowledge about external interferences; needed to guide estimation when data alone is insufficient
- **Causal Discovery Metrics**: Precision, Recall, and F1 scores used to evaluate performance; needed to quantify trade-offs between false positives and false negatives

## Architecture Onboarding

**Component Map:**
Latent Variables (Z) -> Observed Variables (X) -> Variational Inference Engine -> Adjacency Matrix + Weight Matrix

**Critical Path:**
Time series data → Vector autoregressive model with latent variables → Variational inference optimization → Causal matrix decomposition → Adjacency + weight estimation

**Design Tradeoffs:**
- Linear vs non-linear causal relationships (current model assumes linearity)
- Computational complexity vs estimation accuracy (variational inference vs exact methods)
- Prior knowledge incorporation vs data-driven discovery (informative priors vs uninformative)

**Failure Signatures:**
- Poor performance when external interferences are highly correlated with observed variables
- Degradation when latent variables cannot fully capture complex interference patterns
- Scalability issues with high-dimensional systems

**3 First Experiments:**
1. Apply to synthetic data with known causal structure and varying levels of external interference
2. Test on benchmark time series datasets (e.g., financial markets) with ground truth available
3. Ablation study removing informative priors to quantify their contribution

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the Temporal Latent Variable Structural Causal Model be generalized to accommodate non-linear causal relationships between variables?
- Basis in paper: [explicit] The conclusion states: "In further work, we will focus on generalizing the model to accommodate non-linear scenarios, to make it more applicable."
- Why unresolved: The current model relies on linear vector autoregressive processes (Eq. 3), and the identification theory depends on linear structural assumptions.
- What evidence would resolve it: A theoretical extension proving identifiability under non-linear functional forms, coupled with empirical validation on datasets with known non-linear causal mechanisms.

### Open Question 2
- Question: How does the model perform with time lags greater than 1, and what theoretical guarantees hold in multi-lag settings?
- Basis in paper: [explicit] The paper notes: "we just consider the causal effect with a time lag 1 in the above model, but note that our method can be extended to the case with a time lag that is more than 1."
- Why unresolved: All experiments use lag-1 settings; no theoretical or empirical validation is provided for higher-order lags.
- What evidence would resolve it: Experiments on synthetic data with known multi-lag causal structures, showing F1 scores comparable to lag-1 performance.

### Open Question 3
- Question: What is the impact of relaxing the assumption that latent variables only influence themselves (diagonal A_ZZ), and can the model handle correlated latent confounders?
- Basis in paper: [inferred] The model assumes A_ZZ is diagonal, meaning "each Z_i is an autoregressive time series that is only influenced by itself and independent of other latent variables" (page 7). Real-world latent factors may be interrelated.
- Why unresolved: This simplifying assumption is not tested; correlated latent variables could introduce additional spurious correlations among observed variables.
- What evidence would resolve it: Sensitivity experiments varying the density of off-diagonal elements in A_ZZ, measuring degradation in causal discovery accuracy.

## Limitations
- The assumption that latent variables can fully capture external interferences may not hold in highly complex real-world systems with non-stationary or non-linear interference patterns
- Computational complexity increases significantly with system dimensionality, potentially limiting scalability
- The requirement for informative priors about external interference may not be practical in many domains where such knowledge is unavailable

## Confidence
- **High confidence**: The mathematical framework and decomposition approach are well-established and rigorously presented
- **Medium confidence**: The superiority claims over existing methods, while supported by experiments, may be influenced by specific experimental conditions
- **Low confidence**: The model's performance on highly dynamic systems with rapidly changing interference patterns remains untested

## Next Checks
1. Test model robustness on synthetic data with time-varying interference patterns to assess adaptability to dynamic external factors
2. Evaluate computational scalability by applying the model to datasets with 100+ variables to determine practical limits
3. Conduct ablation studies removing informative priors to quantify their actual contribution to performance gains