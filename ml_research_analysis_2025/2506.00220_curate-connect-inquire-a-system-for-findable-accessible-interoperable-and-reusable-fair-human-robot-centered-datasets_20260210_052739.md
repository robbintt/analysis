---
ver: rpa2
title: 'Curate, Connect, Inquire: A System for Findable Accessible Interoperable and
  Reusable (FAIR) Human-Robot Centered Datasets'
arxiv_id: '2506.00220'
source_url: https://arxiv.org/abs/2506.00220
tags:
- data
- datasets
- robotics
- system
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a system for curating, publishing, and accessing
  human-robot interaction (HRI) datasets using FAIR principles and a ChatGPT-based
  conversational interface. The system addresses the challenge of scattered, inconsistently
  described, and hard-to-access robotics datasets by implementing a structured data
  model, a data report template, a knowledge graph, and a chatbot interface.
---

# Curate, Connect, Inquire: A System for Findable Accessible Interoperable and Reusable (FAIR) Human-Robot Centered Datasets

## Quick Facts
- arXiv ID: 2506.00220
- Source URL: https://arxiv.org/abs/2506.00220
- Authors: Xingru Zhou; Sadanand Modak; Yao-Cheng Chan; Zhiyun Deng; Luis Sentis; Maria Esteva
- Reference count: 40
- One-line primary result: A ChatGPT-based system for HRI datasets achieves high evaluation scores (4.65-4.9/5) across information retrieval, answer stability, factual accuracy, and comparison capability.

## Executive Summary
This paper presents a system for curating, publishing, and accessing human-robot interaction (HRI) datasets using FAIR principles and a ChatGPT-based conversational interface. The system addresses the challenge of scattered, inconsistently described, and hard-to-access robotics datasets by implementing a structured data model, a data report template, a knowledge graph, and a chatbot interface. Expert evaluation showed the chatbot achieved high performance across four dimensions: Information Retrieval (4.65/5), Answer Stability (4.9), Factual Accuracy (4.9), and Comparison Capability (4.9). A pilot user session confirmed the system's effectiveness in helping researchers discover and explore datasets intuitively. The system improves dataset findability, accessibility, interoperability, and reuse, demonstrating the value of structured curation and metadata standardization in HRI research.

## Method Summary
The system curates HRI datasets using a data report template aligned with a robotics data model, then publishes them to a Dataverse repository with DDI metadata. Python scripts parse JSON metadata and structured reports to extract key elements (robot models, sensors, experiment conditions) via keyword detection. These elements are mapped to a Neo4j knowledge graph with defined node relationships (e.g., usesModel). A RAG pipeline connects the graph and supplementary documents to ChatGPT, enabling natural language queries for dataset retrieval and comparison. The system was evaluated using seven curated datasets from Texas Robotics labs.

## Key Results
- Chatbot evaluation achieved high scores: Information Retrieval (4.65/5), Answer Stability (4.9), Factual Accuracy (4.9), and Comparison Capability (4.9).
- Expert feedback confirmed the system's effectiveness for dataset discovery and exploration.
- Structured templates and knowledge graphs enable reliable automated retrieval and comparison of HRI datasets.

## Why This Works (Mechanism)

### Mechanism 1
Standardizing heterogeneous robotics data via a structured template and uniform data model creates consistent semantic signals required for reliable automated retrieval. The Data Report Template forces explicit declaration of variables often implicit in lab notes, which are parsed into machine-actionable JSON for downstream processing.

### Mechanism 2
Translating flat metadata into a property graph (Neo4j) enables relational reasoning necessary for dataset comparison and filtering. Metadata maps to nodes and relationships (e.g., RobotModel -> usesModel -> Dataset), converting keyword matching into graph traversal for structural lookups.

### Mechanism 3
Grounding the LLM in a RAG pipeline using curated documentation and knowledge graph improves factual accuracy over generic model training. Instead of relying on pre-trained weights, the system retrieves specific context to generate answers, with evaluation showing this grounding drives high stability and accuracy scores.

## Foundational Learning

- **Concept: FAIR Principles (Findable, Accessible, Interoperable, Reusable)**
  - Why needed here: The paper frames its entire architecture around rectifying the lack of FAIR data in robotics. You cannot understand the design tradeoffs without understanding that the goal is persistent IDs (Findable) and standardized metadata (Interoperable).
  - Quick check question: Does hosting a dataset on a personal GitHub page satisfy the "Findable" and "Accessible" criteria of FAIR? (Answer: No, due to lack of PIDs and long-term preservation guarantees).

- **Concept: Knowledge Graphs & Semantic Triples (Subject-Predicate-Object)**
  - Why needed here: The system moves from flat files to a Neo4j graph. Understanding that data is stored as relationships (e.g., Dataset_A *usesSensor* LiDAR) is required to debug why a query succeeds or fails.
  - Quick check question: In this system, how would the relationship "Boston Dynamics Spot" be stored differently than in a standard SQL row?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: The chatbot's "intelligence" is not innate; it is borrowed from the retrieved documents. Understanding RAG explains why the chatbot fails on vague questions (retrieval failure) but succeeds on specific ones (context injection).
  - Quick check question: Why does the chatbot need the "Data Report" PDF specifically to answer questions about experimental conditions?

## Architecture Onboarding

- **Component map:** Dataverse Repository + Human-created Data Report (PDF) -> Python scripts parse JSON metadata + PDFs -> Map to Data Model -> Neo4j Knowledge Graph + TACC Storage (Raw datasets) -> ChatGPT + RAG Pipeline
- **Critical path:** Data Curation. The architecture is brittle at the entry point. If the human researcher fills out the Data Report Template poorly, the Python parser fails or creates misleading graph nodes, and the LLM subsequently hallucinates.
- **Design tradeoffs:**
  - Standardization vs. Flexibility: The uniform data model forces structure (good for LLM) but risks excluding non-standard HRI experimental designs (bad for novel research).
  - Repository vs. High-Performance Storage: Using Dataverse for metadata/search but TACC storage for massive files ensures preservation but adds complexity to access scripts (users need scripts to download large blobs).
- **Failure signatures:**
  - "Vague Query Failure": Chatbot returns generic or incorrect answers when dataset names are not explicitly mentioned.
  - "Metadata Drift": New datasets containing sensor types not yet in the Data Model append to an "emerging patterns" list rather than the main graph, potentially making them invisible to graph-based queries.
  - "Hallucination on General Knowledge": If RAG retrieval fails, ChatGPT may revert to its pre-trained knowledge about robots, which may be outdated or incorrect for specific experimental setups.
- **First 3 experiments:**
  1. Probe the Context Boundary: Ask the chatbot a question about a specific dataset that requires combining information from the Data Report (PDF) and the JSON metadata. Verify if it successfully synthesizes both sources or ignores one.
  2. Stress Test the Data Model: Attempt to curate a mock dataset that involves a non-standard interaction (e.g., brain-computer interface control). Determine where the current Data Report Template fails to capture the necessary metadata.
  3. Comparison Robustness: Ask the chatbot to "Compare all datasets regarding privacy handling." (A query requiring synthesizing info across multiple nodes). Check if it invents policies for datasets where none are specified.

## Open Questions the Paper Calls Out

- To what extent does the conversational interface's performance degrade when applied to datasets with lower curation quality? (Basis: Future Work section states intent to conduct comparative studies using chatbots trained on datasets of varied curation quality. Why unresolved: Current evaluation relied exclusively on seven high-quality, manually curated datasets, leaving the system's robustness to "messy" data unknown.)

- Can the system effectively execute comparative queries that are vague or lack specific dataset names? (Basis: Results section notes Comparison Capability "relies on precise queries" and yields poor results on general questions. Why unresolved: Evaluation focused on targeted queries; system's ability to infer user intent for comparison without explicit dataset references remains limited.)

- What scalable strategies are required to automate metadata standardization across disparate external repositories? (Basis: Future Work identifies "automating integration across disparate repositories" as a challenge. Why unresolved: Current system relies on specific data report template and local repository JSON structure, which are not standard across different archives.)

## Limitations
- Evaluation relied on small expert sample (4 participants) and single pilot user group (5 members), limiting generalizability.
- System performance on novel, non-standard HRI datasets (e.g., soft robotics, brain-computer interfaces) remains untested.
- RAG pipeline's effectiveness depends on precise metadata extraction; paper does not detail error handling for malformed data reports.

## Confidence
- **High confidence**: Chatbot's high evaluation scores (4.65-4.9/5) and expert feedback confirm system's utility for dataset discovery and comparison within tested scope.
- **Medium confidence**: Claim that structured templates and knowledge graphs improve interoperability is supported by design logic and partial evidence, but lacks direct validation for novel dataset types.
- **Low confidence**: System's scalability to thousands of datasets and ability to handle edge cases (e.g., missing metadata fields) are speculative.

## Next Checks
1. Test the chatbot with a mock dataset involving non-standard sensors (e.g., EEG) to assess Data Model flexibility.
2. Measure retrieval accuracy when metadata fields are intentionally omitted or mislabeled.
3. Evaluate user experience with ambiguous queries (e.g., "Which datasets are best for studying human trust?") to quantify hallucination risk.