---
ver: rpa2
title: 'NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and
  Results'
arxiv_id: '2504.10685'
source_url: https://arxiv.org/abs/2504.10685
tags:
- detection
- object
- few-shot
- team
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the results of the 1st NTIRE 2025 Challenge
  on Cross-Domain Few-Shot Object Detection (CD-FSOD), which aims to advance object
  detection under domain shifts with limited labeled data. The challenge attracted
  152 registered participants, with 42 teams submitting results and 13 making final
  submissions.
---

# NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results

## Quick Facts
- arXiv ID: 2504.10685
- Source URL: https://arxiv.org/abs/2504.10685
- Reference count: 40
- Challenge attracted 152 registered participants, with 42 teams submitting results and 13 making final submissions

## Executive Summary
This paper presents the results of the 1st NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection (CD-FSOD), which aims to advance object detection under domain shifts with limited labeled data. The challenge introduced open-source CD-FSOD as a new setting, allowing participants to leverage foundation models and diverse knowledge sources. Participants approached the task from diverse perspectives, proposing novel models that achieved new state-of-the-art results under both open-source and closed-source settings. The top-performing teams significantly surpassed baseline scores, demonstrating the effectiveness of proposed methods and the significance of this new task setting.

## Method Summary
The challenge evaluated cross-domain few-shot object detection methods across two settings: open-source (allowing foundation model usage) and closed-source (with stricter constraints). Teams developed novel approaches to address domain adaptation challenges when limited labeled data is available in target domains. The evaluation framework measured performance across domain shifts, with participants submitting results for both competition tracks. The open-source setting enabled leveraging pre-trained foundation models, while the closed-source setting required more self-contained solutions.

## Key Results
- Open-source track: Top three teams achieved scores of 231.01, 215.92, and 215.48 respectively
- Closed-source track: Top team achieved a score of 125.90
- 152 registered participants, 42 teams submitted results, 13 made final submissions
- Results demonstrate significant improvement over baseline methods

## Why This Works (Mechanism)
The paper reports challenge results rather than proposing new hypotheses, so the effectiveness stems from the diverse approaches participants took to address cross-domain few-shot detection. Teams leveraged various techniques including domain adaptation strategies, meta-learning approaches, and foundation model integration. The open-source setting allowed for leveraging pre-trained knowledge, while the closed-source setting forced development of more robust domain adaptation techniques that don't rely on external knowledge sources.

## Foundational Learning
- Cross-domain adaptation: Understanding how to transfer knowledge between different visual domains; needed to handle domain shift between source and target datasets; quick check: verify feature distributions align across domains
- Few-shot learning: Techniques for learning from very limited labeled examples; essential when target domain has minimal annotations; quick check: measure performance degradation as shot count decreases
- Object detection fundamentals: Core concepts of bounding box regression and classification; foundation for any detection approach; quick check: validate basic detection performance on source domain
- Domain generalization: Methods to improve robustness across unseen domains; important for real-world deployment; quick check: test on held-out domain variants

## Architecture Onboarding
Component map: Input images -> Feature extractor -> Domain adapter -> Detection head -> Output predictions

Critical path: Domain adaptation happens in the feature extractor stage, where domain-invariant representations are learned. The detection head then operates on these adapted features to produce bounding boxes and class predictions. Model optimization focuses on balancing domain alignment with detection performance.

Design tradeoffs: Open-source setting allows foundation model usage for better initialization but may introduce domain bias. Closed-source setting requires more robust adaptation techniques but limits available knowledge. Few-shot constraint necessitates efficient parameter sharing between domains.

Failure signatures: Performance degradation typically occurs when domain shift is large or when few-shot samples don't capture target domain variability. Models may overfit to source domain characteristics or fail to generalize from limited target examples.

First experiments: 1) Baseline detection performance on source domain, 2) Cross-domain performance without adaptation, 3) Adaptation effectiveness measured by domain alignment metrics

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but the significant performance gap between open-source (231.01) and closed-source (125.90) settings raises questions about the practical applicability of open-source approaches that rely heavily on foundation models. The challenge results suggest opportunities for developing more robust closed-source solutions that can achieve performance closer to open-source approaches without external knowledge dependencies.

## Limitations
- The 13 final submissions may not represent a fully comprehensive sample of all challenge approaches
- Performance claims lack independent verification without access to challenge datasets and evaluation code
- The significant performance gap between open and closed-source settings may not reflect realistic application scenarios

## Confidence
- Challenge organization and participation statistics: High confidence - based on verifiable submission records
- State-of-the-art performance claims: Medium confidence - results are consistent internally but lack independent verification
- Significance of open-source CD-FSOD setting: Medium confidence - novel setting but comparative fairness to closed-source unclear

## Next Checks
1. Obtain and run evaluation code on held-out data to verify reported score differences between open and closed-source settings
2. Analyze top-performing methods for dataset-specific optimizations that may not generalize across domains
3. Conduct ablation studies on foundation model usage to quantify its contribution versus domain adaptation techniques