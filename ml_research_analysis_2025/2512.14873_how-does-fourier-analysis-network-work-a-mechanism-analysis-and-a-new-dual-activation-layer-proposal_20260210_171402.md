---
ver: rpa2
title: How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation
  Layer Proposal
arxiv_id: '2512.14873'
source_url: https://arxiv.org/abs/2512.14873
tags:
- relu
- sine
- activation
- training
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the mechanism behind Fourier Analysis Networks
  (FAN), which combine ReLU, sine, and cosine activations to improve neural network
  performance. Through systematic experiments on synthetic and real-world datasets
  (noisy sinusoidal signal classification, MNIST, ECG-ID), the authors demonstrate
  that only the sine activation contributes positively to performance, while cosine
  is detrimental.
---

# How Does Fourier Analysis Network Work? A Mechanism Analysis and a New Dual-Activation Layer Proposal

## Quick Facts
- **arXiv ID:** 2512.14873
- **Source URL:** https://arxiv.org/abs/2512.14873
- **Reference count:** 26
- **Key result:** Only sine activation contributes positively to neural network performance; cosine is detrimental. Improvement stems from non-zero derivative near x=0, not periodicity.

## Executive Summary
This paper systematically investigates Fourier Analysis Networks (FAN), which combine ReLU, sine, and cosine activations. Through controlled experiments on synthetic and real-world datasets, the authors demonstrate that the performance gains attributed to FAN arise not from spectral properties but from improved gradient stability. Only the sine activation provides benefits, while cosine degrades performance. The improvement stems from sine's non-zero derivative near x=0, which mitigates the dying ReLU problem by maintaining gradient flow. Based on these insights, the authors propose the Dual-Activation Layer (DAL), which mixes ReLU with sine (or similar functions like tanh) to accelerate convergence across all tested tasks while maintaining or improving final accuracy.

## Method Summary
The study uses paired experimental designs where identical architectures are trained with different activation functions, starting from the same weight initialization. The primary evaluation compares standard ReLU, FAN (ReLU:sine:cosine), and variants with only one or two activation functions. DAL is implemented as a single layer that applies two functions (f, g) in parallel to the same input, then combines outputs in ratio a:b. The authors test across three datasets: synthetic 1D signal classification (noisy sinusoids vs pure noise), MNIST digit classification, and ECG-ID biometric recognition (90 classes). Training uses Adam optimizer with binary/categorical crossentropy loss, N=50-100 repeated runs per experiment, and convergence speed as the primary metric.

## Key Results
- Only sine activation contributes positively to performance; cosine is detrimental and should be excluded from FAN
- Performance improvement stems from sine's non-zero derivative near x=0, not its periodic properties
- DAL consistently accelerates convergence across all tasks, with final accuracy improvements only on complex tasks (ECG-ID)
- DAL reduces dead neurons by 54% on average compared to ReLU-only networks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sine activation improves performance through its non-zero derivative near x=0, not through periodic properties.
- **Mechanism:** Sine maintains non-zero gradients when inputs cluster near zero (μ≈0, σ∈[0.2,0.9]), enabling continuous weight updates. This contrasts with ReLU's hard zero derivative for x≤0.
- **Core assumption:** Most activation inputs cluster near zero during training; local behavior in [-1.5, +1.5] dominates dynamics.
- **Evidence anchors:** Experiments show non-periodic functions (tanh, TSine, linear) with similar local shapes near zero produce comparable improvements to sine.
- **Break condition:** If inputs shift substantially away from zero, the local-derivative benefit may diminish.

### Mechanism 2
- **Claim:** Cosine activations degrade performance and should be excluded.
- **Mechanism:** Cosine has zero derivative at the origin (cos'(0)=0), creating a local gradient bottleneck where most inputs cluster, counteracting sine's benefit.
- **Core assumption:** Gradient behavior at x=0 is critical; functions with zero derivative at origin provide no training signal when inputs cluster near zero.
- **Evidence anchors:** (ReLU, cos, 6:2) achieves 76.08% vs ReLU-only's 79.06% — statistically worse performance.
- **Break condition:** If input distributions rarely fall near zero, cosine's zero-derivative-at-origin problem becomes less relevant.

### Mechanism 3
- **Claim:** DAL works primarily by reducing dead/near-dead neurons, improving gradient propagation.
- **Mechanism:** DAL provides redundant gradient pathways: when ReLU outputs zero, the sine branch carries gradients backward, keeping upstream weights updating and preventing cascade failure in deep networks.
- **Core assumption:** Dead neurons significantly impair training; networks benefit from redundant gradient pathways.
- **Evidence anchors:** Dense1 layer: 36.95% dead neurons (ReLU-only) vs 16.80% (DAL); with batch normalization: 46.67% vs 13.34%.
- **Break condition:** If a network already has robust gradient pathways (deep residual connections, extensive normalization), DAL's marginal benefit may be smaller.

## Foundational Learning

- **Concept: Dying ReLU Problem**
  - **Why needed here:** DAL's primary mechanism is dead-neuron mitigation; understanding this phenomenon is essential to interpret experimental results.
  - **Quick check question:** Given a ReLU neuron that has received negative inputs for 100 consecutive batches, what is its gradient contribution to the previous layer?

- **Concept: Gradient Flow Through Activations**
  - **Why needed here:** The paper reframes FAN's benefit from "spectral representation" to "gradient stability" — you need to understand how activation derivatives affect backpropagation.
  - **Quick check question:** Why does cos(x) have zero derivative at x=0, and what happens to weight updates for neurons whose activations consistently receive near-zero inputs when using cosine?

- **Concept: Local vs Global Activation Properties**
  - **Why needed here:** The paper's key insight is that local behavior near x=0 matters more than periodicity. This challenges intuitive assumptions about why periodic activations might help.
  - **Quick check question:** If 90% of activation inputs fall in [-1.5, 1.5], which matters more for training: the function's behavior in this interval, or its behavior at x=10π?

## Architecture Onboarding

- **Component map:**
  - Standard activation layer -> DAL(f, g, a:b) layer where f=ReLU (or GELU/Swish), g=sine (or tanh), ratio a:b typically 6:2 to 7:1

- **Critical path:**
  1. Identify all activation layers in your architecture (conv layers, dense layers — exclude output softmax/sigmoid)
  2. Replace each with DAL(f, g, a:b) where f is your base activation, g is sine or tanh
  3. Start with ratio 7:1 or 6:2; tune if needed
  4. Expect faster convergence, not necessarily higher final accuracy (unless problem is complex and underfitting)

- **Design tradeoffs:**
  - **Ratio a:b:** Higher proportion of g increases gradient stability but may reduce ReLU's sparse activation efficiency. Optimal ratio is task-dependent (paper used 6:2 to 1:1).
  - **Choice of g:** Sine and tanh produce near-identical results; tanh avoids computing trig functions if that's a concern.
  - **Batch normalization:** DAL provides independent benefit but has reduced marginal gain when BN is present. Consider DAL especially in architectures without extensive normalization.

- **Failure signatures:**
  - **No convergence improvement:** Check if inputs to activations are already well-normalized (μ≈0, bounded σ); DAL's benefit is largest when dead neurons are a problem.
  - **Worse performance:** Likely using too high a proportion of g, or using cosine (verify you excluded it).
  - **Training instability:** Sine's unbounded derivatives at large |x| could cause issues if inputs aren't reasonably bounded.

- **First 3 experiments:**
  1. **Baseline comparison:** Train your architecture with DAL(ReLU, sine, 7:1) vs ReLU-only for 5-10 epochs. Plot training loss curves. Expect DAL to reach lower loss faster.
  2. **Ablation on ratio:** Test ratios 8:1, 7:1, 6:2, 1:1 on a validation set. Expect diminishing returns beyond 6:2 for simple tasks.
  3. **Dead neuron audit:** After training both variants, log percentage of neurons with zero (or near-zero) average activation across training samples. DAL should show substantially fewer dead neurons, especially in deeper layers.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal mixing ratio a:b for the Dual-Activation Layer across different tasks and architectures, and can it be determined adaptively during training?
- **Basis in paper:** "We emphasize, however, that the optimal ratio a:b is task- and architecture-dependent. The 6:2 ratio was not optimized, and different proportions may yield better results."
- **Why unresolved:** Authors manually tested limited ratio combinations without systematic optimization or proposing an automated selection method.
- **What evidence would resolve it:** Systematic sweep across ratio combinations on diverse benchmark tasks, or a meta-learning approach that adaptively adjusts ratios during training.

### Open Question 2
- **Question:** Why does the cosine activation consistently degrade performance when combined with ReLU, and what properties of alternative functions make them detrimental versus beneficial?
- **Basis in paper:** Experiments show cosine degrades performance, but mechanism is not fully explained beyond noting derivative properties.
- **Why unresolved:** Paper demonstrates cosine's detrimental effect empirically but lacks theoretical analysis of why cosine behaves differently from sine despite their mathematical relationship.
- **What evidence would resolve it:** Systematic analysis of activation function properties (symmetry, derivative at zero, gradient flow characteristics) correlated with performance across multiple function families.

### Open Question 3
- **Question:** Under what conditions does DAL improve final accuracy versus only accelerating convergence, and can these conditions be predicted a priori?
- **Basis in paper:** DAL accelerates convergence universally but only improves final accuracy on ECG-ID (complex task), not on MNIST or synthetic tasks where models converge to similar final accuracy.
- **Why unresolved:** Relationship between task complexity, network depth, and DAL's benefit to final accuracy remains unclear from limited experiments.
- **What evidence would resolve it:** Controlled experiments varying task complexity, network depth, and dead-neuron prevalence systematically to identify predictive factors for final accuracy gains.

### Open Question 4
- **Question:** How does DAL interact with other gradient-enhancing techniques (residual connections, advanced optimizers, layer normalization) in very deep architectures?
- **Basis in paper:** "In very deep or complex network architectures, the DAL can be effectively employed in conjunction with other gradient flow enhancement techniques... to provide a synergistic effect."
- **Why unresolved:** Only batch normalization interaction was tested; other combinations remain unexplored.
- **What evidence would resolve it:** Experiments combining DAL with ResNet-style skip connections, AdamW/Adamax optimizers, and layer normalization on standardized deep network benchmarks.

## Limitations

- The study focuses exclusively on convergence speed rather than final accuracy improvements, leaving open questions about DAL's impact on generalization.
- The proposed mechanism (mitigating dead neurons) may not fully explain performance in architectures with built-in gradient stability like ResNets.
- The optimal activation ratio (a:b) appears task-dependent but lacks systematic tuning guidelines.

## Confidence

- **High Confidence:** The core finding that cosine activations degrade performance while sine improves convergence speed is strongly supported by paired experiments across multiple datasets. The mechanism linking sine's non-zero derivative near x=0 to improved gradient flow is mathematically sound and empirically validated.
- **Medium Confidence:** The claim that DAL's primary benefit comes from dead-neuron mitigation is well-supported in standard CNNs but may not generalize to architectures with residual connections or extensive normalization layers. The specific ratio recommendations (6:2 to 1:1) are empirically derived but lack theoretical justification.
- **Low Confidence:** The paper's assertion that DAL provides independent benefits beyond batch normalization is based on limited ablation studies and may not hold across diverse architectural families.

## Next Checks

1. Test DAL in residual network architectures (ResNet-18/34) to assess whether dead-neuron mitigation remains the dominant mechanism when skip connections provide alternative gradient paths.
2. Conduct systematic hyperparameter sweeps for the a:b ratio across task complexities to establish data-driven guidelines rather than empirical rules-of-thumb.
3. Evaluate DAL's impact on final test accuracy and generalization bounds (not just convergence speed) using proper train/validation/test splits on real-world datasets.