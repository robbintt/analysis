---
ver: rpa2
title: 'TREX: Tokenizer Regression for Optimal Data Mixture'
arxiv_id: '2601.13588'
source_url: https://arxiv.org/abs/2601.13588
tags:
- data
- compression
- mixture
- tokenizer
- trex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TREX, a regression-based framework for finding
  optimal data mixtures when training multilingual tokenizers. Instead of relying
  on heuristics or expensive search, TREX trains small proxy tokenizers on randomly
  sampled mixtures, collects compression statistics, and learns to predict compression
  performance from data mixtures.
---

# TREX: Tokenizer Regression for Optimal Data Mixture

## Quick Facts
- arXiv ID: 2601.13588
- Source URL: https://arxiv.org/abs/2601.13588
- Reference count: 36
- Primary result: Regression-based framework predicting optimal tokenizer training data mixtures with high accuracy

## Executive Summary
TREX introduces a regression-based framework for finding optimal data mixtures when training multilingual tokenizers. Instead of relying on heuristics or expensive search, TREX trains small proxy tokenizers on randomly sampled mixtures, collects compression statistics, and learns to predict compression performance from data mixtures. This allows efficient search of the mixture space before large-scale tokenizer training. Experiments show TREX achieves high predictive accuracy and discovers mixtures that improve compression by up to 12% over baselines.

## Method Summary
TREX operates by first training a set of small proxy tokenizers on randomly sampled data mixtures from the available corpus. For each trained proxy, compression statistics are collected across multiple evaluation datasets. These statistics, along with the corresponding mixture proportions, form the training data for a regression model that learns to predict compression performance from mixture composition. During the search phase, this regression model can efficiently evaluate numerous potential mixtures without the need to train additional tokenizers, enabling rapid identification of high-performing data mixtures for full-scale tokenizer training.

## Key Results
- TREX achieves high predictive accuracy with MAPE below 2% and Spearman correlation exceeding 0.97
- Discovered mixtures improve compression by up to 12% over established baselines including LLaMA3 and GPT-4o
- Reduces language model training time by approximately 1,000 hours through optimized tokenizer performance

## Why This Works (Mechanism)
TREX works by creating a learnable mapping between data mixture compositions and their resulting compression performance. The framework leverages the observation that small proxy tokenizers trained on representative mixtures can provide reliable indicators of how larger tokenizers would perform on the same data. By training a regression model on this proxy data, TREX can predict the performance of any potential mixture without the computational expense of actually training a full tokenizer for each candidate. This approach transforms an intractable search problem into a tractable regression problem, enabling efficient exploration of the mixture space.

## Foundational Learning

**Data Mixture Optimization**: Understanding how different proportions of training data affect model performance is crucial for efficient multilingual tokenization. Why needed: Different languages and domains require different vocabulary allocations for optimal compression. Quick check: Verify that varying mixture proportions produces measurable differences in compression ratios.

**Proxy Model Training**: Using small-scale models to predict performance of larger models is a common efficiency technique. Why needed: Full tokenizer training is computationally expensive, making exhaustive search impractical. Quick check: Confirm that proxy model performance correlates strongly with full model performance.

**Compression Statistics**: Measuring how well a tokenizer compresses text is the key evaluation metric. Why needed: Compression ratio directly impacts downstream language model efficiency and quality. Quick check: Validate that compression improvements translate to meaningful performance gains in actual language models.

## Architecture Onboarding

**Component Map**: Data corpus -> Random mixture sampling -> Proxy tokenizer training -> Compression measurement -> Regression model training -> Mixture search and prediction

**Critical Path**: The most computationally intensive step is training the proxy tokenizers, as each requires full training cycles. The regression model training is relatively lightweight in comparison. The search phase is computationally negligible once the regression model is trained.

**Design Tradeoffs**: TREX trades off some precision in predictions (due to proxy model limitations) for massive gains in search efficiency. The framework could potentially be extended to predict other tokenizer quality metrics beyond compression, but this would require additional proxy training data collection.

**Failure Signatures**: Poor predictive accuracy likely indicates that the proxy tokenizers are not representative of full-scale tokenizers, or that the compression statistics collected are not sufficiently discriminative. Overfitting in the regression model suggests insufficient diversity in the training mixtures.

**3 First Experiments**:
1. Train proxy tokenizers on a small set of randomly sampled mixtures and verify that compression statistics vary meaningfully with mixture composition
2. Train a simple regression model on proxy data and evaluate its predictive accuracy on held-out mixtures
3. Use the trained regression model to search for high-performing mixtures and validate their actual compression performance with full tokenizer training

## Open Questions the Paper Calls Out
None

## Limitations
- Predictive accuracy metrics are evaluated on the same data distribution used for training, leaving open questions about true out-of-distribution generalization
- The 12% compression improvement stems from optimizing specifically for compression performance, which may not translate directly to downstream language modeling quality
- Computational savings are based on proxy experiments and extrapolation rather than direct measurement on full-scale training runs

## Confidence

**Major Claims Confidence:**
- Predictive accuracy metrics (MAPE < 2%, Spearman Ï > 0.97): High
- 12% compression improvement over baselines: Medium (optimization objective may not reflect all use cases)
- 1,000-hour training time reduction: Medium (based on extrapolation from proxy experiments)

## Next Checks
1. Evaluate TREX-predicted mixtures on actual language model pretraining to verify claimed training time reductions, measuring both convergence speed and final model quality across multiple downstream tasks
2. Test the framework's generalization by applying TREX to datasets from entirely different domains (e.g., code, scientific literature) than those used in the original experiments, measuring both predictive accuracy and compression improvements
3. Conduct ablation studies comparing TREX's random sampling approach against more sophisticated search strategies (Bayesian optimization, genetic algorithms) to quantify the trade-off between computational efficiency and mixture quality