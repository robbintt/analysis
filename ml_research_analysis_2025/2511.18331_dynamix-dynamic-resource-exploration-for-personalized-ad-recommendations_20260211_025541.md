---
ver: rpa2
title: 'DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations'
arxiv_id: '2511.18331'
source_url: https://arxiv.org/abs/2511.18331
tags:
- feature
- users
- dynamic
- user
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamix introduces a self-supervised user-segmentation approach
  that dynamically allocates resources in ad-recommendation systems by removing or
  boosting features based on user engagement patterns. The method leverages dwell-time
  correlations with conversion events to classify users as active or passive segments
  in real-time, enabling targeted feature pruning for passive users and selective
  feature boosting for active users.
---

# DynamiX: Dynamic Resource eXploration for Personalized Ad-Recommendations

## Quick Facts
- arXiv ID: 2511.18331
- Source URL: https://arxiv.org/abs/2511.18331
- Authors: Sohini Roychowdhury; Adam Holeman; Mohammad Amin; Feng Wei; Bhaskar Mehta; Srihari Reddy
- Reference count: 13
- Dynamix achieves 1.15% training QPS, 1.8% inference QPS gains via dynamic feature removal; 0.033 NE gains and 4.2% inference QPS improvement via selective feature boosting

## Executive Summary
DynamiX introduces a self-supervised user-segmentation approach that dynamically allocates resources in ad-recommendation systems by removing or boosting features based on user engagement patterns. The method leverages dwell-time correlations with conversion events to classify users as active or passive segments in real-time, enabling targeted feature pruning for passive users and selective feature boosting for active users. Experimental results demonstrate that dynamic feature removal increases training and inference throughput by 1.15% and 1.8% respectively, while dynamic feature boosting delivers 0.033 NE gains and 4.2% inference QPS improvement over baseline models. This approach achieves significant cost efficiency and performance improvements in personalized sequence-based recommendation systems without sacrificing ad-relevance accuracy.

## Method Summary
DynamiX computes per-user dwell-time correlations with conversion events to segment users into active and passive groups in real-time. The system uses a 60-second logging buffer around conversion events and a forecast horizon to calculate correlation statistics, then thresholds these to assign user segments. Dynamic feature removal prunes low-importance attributes from active users' ad-impression EBFs, improving throughput with minimal NE impact. Dynamic feature boosting selectively enriches EBF attributes for active users while limiting passive users to organic-impression features only, improving both NE and inference QPS. The approach assumes ~66.67% active users and binary segmentation, processing 40+ billion user samples over one month.

## Key Results
- Dynamic feature removal increases training QPS by 1.15% and inference QPS by 1.8% for active users
- Selective feature boosting for active users delivers 0.033 NE gains and 4.2% inference QPS improvement
- Binary segmentation achieves 14.3% inference QPS improvement over uniform EBF boosting across all users

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dwell-time patterns before conversion events are predictive of user intent and can segment users in real-time.
- Mechanism: The system computes a windowed correlation statistic (Corr^u_s) comparing a user's average ad dwell-time near conversion events vs. far from them. Users with |Corr^u_s| > ε are classified as "active" (their ad-view history predicts commercial intent); others are "passive." This segmentation updates multiple times per day per user based on browsing behavior.
- Core assumption: Log-dwell-times are normally distributed conditioned on conversion events (log(D^u_t) | C^u_[t,t+s] ~ N(μ^u_C, σ^u_C)), and σ^u_{Ct=0} ≈ σ^u_{Ct=1} for all users.
- Evidence anchors:
  - [abstract]: "leverages correlations between dwell-times and ad-conversion events" for user segmentation
  - [section 3.1]: Figure 3 shows empirical histogram evidence that dwell-time increases before conversion events; Equations (2)-(7) derive the logistic sigmoid relationship
  - [section 3.2]: Equation (8) defines Corr^u_s(D, C); Equation (9) defines threshold-based segmentation U^u_s
  - [corpus]: No directly comparable dwell-time→conversion correlation mechanism found; closest is Davoudi et al. (2019) on dwell-time prediction for news, but not for user segmentation
- Break condition: If dwell-time logging is unreliable (paper notes outliers from milliseconds to hours) or conversion/impression timestamps drift >60 seconds, correlation estimates become noisy and segmentation degrades.

### Mechanism 2
- Claim: Pruning low-importance feature attributes from active-user segments improves training and inference throughput without degrading CTR prediction.
- Mechanism: After segmentation, low-importance attributes (ranked by feature importance) are removed from ad-impression EBFs for active users. Since active users have predictable ad-intent correlations, the model maintains accuracy with fewer features.
- Core assumption: Active users' conversion behavior is sufficiently captured by remaining high-importance features; passive users are already noisy and less sensitive to feature changes.
- Evidence anchors:
  - [abstract]: "dynamic feature removal increases training and inference throughput by 1.15% and 1.8%"
  - [section 4.1, Table 2]: Attribute removal for active users yields +1.1% training QPS, +1.8% inference QPS with NE gain of 0.006 (within noise range)
  - [section 4.1]: "pruning less significant feature attributes from active user-segments result in minimal change in averaged-CTR... while significantly speeding up training and inference"
  - [corpus]: Weak direct evidence; P-CAFE (arXiv 2508.08646) discusses personalized feature selection in EHR but not real-time ad systems
- Break condition: If feature importance rankings are stale or if active-user fraction shifts dramatically (paper assumes ~66.67% active), throughput gains diminish.

### Mechanism 3
- Claim: Selectively boosting features for active users while limiting passive users to organic-impression features improves both NE and inference QPS.
- Mechanism: Dynamic user-segmentation routes enriched EBF attributes (ad-impressions, organic-impressions, new-page-impressions) to active users; passive users receive only organic-impression additions. This reduces compute on low-predictability traffic.
- Core assumption: Passive users contribute noise and do not benefit from additional EBF features; active users drive CTR gains.
- Evidence anchors:
  - [abstract]: "dynamic feature boosting provides 0.033 NE gains while boosting inference QPS by 4.2%"
  - [section 4.2, Table 3]: Selective boosting for active users yields NE gain 0.033, inference QPS +4.2%; homogeneous boosting causes -10.1% inference QPS regression
  - [section 5]: "14.3% inference QPS improvement over uniform EBF boosting across all users"
  - [corpus]: No directly comparable dynamic boosting for ad-recommendation found; DYNAMIX (arXiv 2510.08522) addresses batch-size optimization via RL, not feature boosting
- Break condition: If threshold ε is misconfigured, passive users may be incorrectly enriched (compute waste) or active users under-served (CTR loss).

## Foundational Learning

- **Event-Based Features (EBFs)**:
  - Why needed here: EBFs are the core input—timestamped sequences of user-ad engagements (impressions, dwell-time, conversions) that Dynamix filters and boosts per-user.
  - Quick check question: Can you list three attributes of an ad-impression EBF and explain why dwell-time matters for conversion prediction?

- **Self-Supervised Learning from Conversion History**:
  - Why needed here: Dynamix uses past conversion behavior as proxy labels to compute dwell-time→conversion correlations without explicit supervision.
  - Quick check question: How does Equation (7) estimate w_MLE, and why is this considered self-supervised rather than supervised?

- **Normalized Entropy (NE) as Evaluation Metric**:
  - Why needed here: NE quantifies CTR prediction quality; gains in NE directly measure ad-relevance improvement.
  - Quick check question: Given Equation (10), what does a positive NE gain indicate about model predictions vs. baseline?

## Architecture Onboarding

- **Component map**: EBF Preprocessing Layer -> Correlation Computation Module -> Segmentation Engine -> Feature Router -> Recommendation Model

- **Critical path**: EBF ingestion → dwell-time denoising → Corr^u_s computation → segmentation → feature routing → model inference. Delays or errors in correlation computation directly impact segmentation accuracy.

- **Design tradeoffs**:
  - Higher ε → fewer active users → more compute savings but potentially missed CTR gains from borderline-active users.
  - Lower ε → more users receive boosted features → higher compute cost; risk of noise from misclassified passive users.
  - Binary segmentation (active/passive) is simpler but may miss nuance; multi-level thresholding could improve but risks instability.

- **Failure signatures**:
  - Inference QPS regression >5%: Check if passive-user feature removal was misconfigured or if ε was too low.
  - NE gain near zero despite boosting: Verify dwell-time logging quality; check for timestamp drift between impression and conversion logs.
  - Unstable segmentation (users flipping frequently): Increase window size s or smooth Corr^u_s over multiple windows.

- **First 3 experiments**:
  1. **Baseline correlation check**: Compute Corr^u_s for a sample of users; verify distribution matches Figure 3 (positive mean, user-level variance). Confirm logging buffer compensates for timestamp drift.
  2. **Ablation on ε**: Run segmentation with ε values {0.1, 0.2, 0.3}; measure active-user fraction and NE gain. Identify threshold where NE gains stabilize.
  3. **Feature removal pilot**: Remove lowest-importance ad-impression attribute for active users only; measure training QPS, inference QPS, and NE gain. Compare against Table 2 baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does extending the segmentation beyond binary classes (active/passive) to multi-level user groups introduce instability that negates resource optimization gains?
- Basis in paper: [explicit] The authors state that "Additional user-group partitioning may lead to more noisy and unstable resource explorations that remain to explored in future works."
- Why unresolved: The current work validates only a binary segmentation approach, leaving the trade-offs of finer granularity untested.
- What evidence would resolve it: Experiments comparing binary vs. multi-class segmentation schemes, measuring variance in inference QPS and Normalized Entropy (NE) stability.

### Open Question 2
- Question: Does the simplifying assumption of equal conditional variances for dwell-time distributions ($\sigma_{C_t=0} = \sigma_{C_t=1}$) introduce systematic bias in user-segmentation boundaries?
- Basis in paper: [inferred] The paper assumes equal variance in Equation (6) to derive the logistic probability but provides no ablation to verify if this holds for the skewed empirical distributions shown in Figure 3.
- Why unresolved: Real-world dwell-time data is often heteroscedastic, and violating this assumption could misclassify users near the threshold $\epsilon$.
- What evidence would resolve it: Ablation studies comparing the current Maximum Likelihood Estimator against a model with unconstrained variances to measure changes in segmentation accuracy.

### Open Question 3
- Question: Why does dynamic feature removal for passive users fail to yield inference QPS gains, and does the overhead of dynamic masking offset computational savings for low-intent users?
- Basis in paper: [inferred] Table 2 shows that attribute removal for passive users results in a -0.37% inference QPS regression, contradicting the efficiency gains seen for active users.
- Why unresolved: The paper explains the efficiency for active users but does not clarify why the logic failed to improve throughput for the passive segment.
- What evidence would resolve it: Profiling the latency of the self-supervised selection logic relative to the compute savings from feature pruning for low-activity sequences.

## Limitations
- The core dwell-time→conversion correlation mechanism lacks strong external validation; no directly comparable approach found in the literature
- Feature selection details are underspecified: the paper does not clarify how low-importance attributes are identified for pruning or which specific attributes are added during boosting
- User segmentation is binary, which may oversimplify heterogeneous user behaviors and risk misclassifying borderline users

## Confidence
- **High confidence**: QPS improvements from dynamic feature removal and inference speedup are well-supported by ablation tables and align with standard feature-pruning theory
- **Medium confidence**: NE gains from selective boosting are plausible given the empirical results, but depend on correct dwell-time logging and correlation estimation
- **Low confidence**: The self-supervised segmentation mechanism is innovative but lacks direct comparative literature; its robustness across different ad domains is uncertain

## Next Checks
1. **Correlation stability test**: Recompute Corr^u_s across multiple time windows per user; verify that |Corr^u_s| > ε holds consistently over days and matches the positive mean pattern in Figure 3
2. **Segmentation sensitivity analysis**: Vary ε systematically (e.g., {0.1, 0.2, 0.3}) and measure changes in active-user fraction, NE gain, and inference QPS to identify stable operating points
3. **Dwell-time logging audit**: Check timestamp ordering and buffer application for all impression-conversion pairs; quantify fraction of samples with >60-second drift and assess impact on correlation estimates