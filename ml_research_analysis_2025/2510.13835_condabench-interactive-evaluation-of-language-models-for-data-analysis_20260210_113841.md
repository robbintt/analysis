---
ver: rpa2
title: 'ConDABench: Interactive Evaluation of Language Models for Data Analysis'
arxiv_id: '2510.13835'
source_url: https://arxiv.org/abs/2510.13835
tags:
- data
- code
- answer
- user
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ConDABench is a framework for evaluating conversational data analysis
  (ConDA) assistants on realistic, interactive tasks. It introduces a modular, multi-agent
  pipeline that generates benchmarks from real-world data analysis articles by extracting
  query-answer pairs and reverse-engineering the underlying code, enabling automated
  simulation of user interactions via a User Proxy agent.
---

# ConDABench: Interactive Evaluation of Language Models for Data Analysis

## Quick Facts
- arXiv ID: 2510.13835
- Source URL: https://arxiv.org/abs/2510.13835
- Reference count: 40
- Primary result: Framework for evaluating conversational data analysis assistants on realistic interactive tasks

## Executive Summary
ConDABench introduces a modular, multi-agent framework for evaluating conversational data analysis (ConDA) assistants on realistic, interactive tasks. The framework extracts query-answer pairs from real-world data analysis articles, reverse-engineers the underlying code, and uses this code-grounded approach to simulate user interactions via a User Proxy agent. Experiments on 1,420 tasks show that while newer models achieve higher accuracy, they often rely on shorter interactions rather than sustained, long-form engagement, revealing important tradeoffs between correctness and conversation quality.

## Method Summary
ConDABench uses a multi-agent pipeline: Curator extracts query-answer pairs from 338 real-world data analysis articles; Code Generator and Audited Reviewer iteratively produce supporting Python code that matches ground-truth answers; User Proxy agent simulates user conversations using the code for consistent parameter responses; Evaluator grades responses for correctness while Rubric Grader assesses conversation quality. The framework supports diverse query types (open-ended, projection, QA) and evaluates both correctness and conversation quality using a code-grounded evaluation harness, tested with multiple assistant frameworks.

## Key Results
- Code-grounded evaluation achieves Pearson correlation of 0.748 with human judgments for correctness
- Conversation quality metric shows F1-score of 0.75 when trained on 120 human annotations
- Models optimized for accuracy often achieve it through shorter conversations rather than sustained engagement
- 59.2% of extracted tasks successfully pass code generation validation and become benchmark tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reverse-engineered code provides a verifiable ground truth that both validates benchmark quality and enables consistent interactive simulation.
- Mechanism: Code Generator produces candidate Python programs from (query, data) pairs; Audited Reviewer iteratively checks outputs against ground-truth answers without leaking the answer. Convergent programs become task artifacts that can answer future clarification requests deterministically.
- Core assumption: Articles' stated results are derivable from their accompanying datasets via some (unknown) analysis code.
- Break condition: Code fails to converge when article claims are unsupported by data, data changed post-publication, or reasoning relies on external knowledge.

### Mechanism 2
- Claim: Code-grounded User Proxy enables automated, consistent multi-turn evaluation without hard-coding responses.
- Mechanism: User Proxy receives system's utterance, classifies it (answer/clarification/confirmation), then uses supporting code to generate grounded replies (e.g., parameter values like thresholds) without revealing code or answer.
- Core assumption: Generated code correctly encodes latent reasoning needed to answer any natural clarification assistant might ask.
- Break condition: Code omits assumptions needed by assistant, or Proxy proactively leaks guidance.

### Mechanism 3
- Claim: Separate correctness and conversation-quality metrics capture distinct axes of performance and reveal tradeoffs between efficiency and collaboration.
- Mechanism: Evaluator LLM judges semantic match across modalities (text, numbers, plots). Rubric-based grader rates SAT/DSAT conversational behaviors; regressor trained on human annotations aggregates rubrics into quality label.
- Core assumption: LLM-based evaluators can approximate human judgment for both answer correctness and conversational quality, with measurable alignment.
- Break condition: Judge-contestant circularity or evaluator bias.

## Foundational Learning

- Concept: Conversational evaluation under ambiguity
  - Why needed here: Real data-analysis queries are underspecified (e.g., "low-budget" without threshold). Static benchmarks ignore this; ConDABench makes it first-class via User Proxy.
  - Quick check question: Can you sketch how a static QA benchmark would evaluate a model that asks a necessary clarification question?

- Concept: Multi-agent pipelines with audited feedback
  - Why needed here: Code Generator-Reviewer loop requires structured feedback that avoids answer leakage while remaining useful.
  - Quick check question: Given incorrect intermediate result, write one "leaky" reviewer feedback and one "audited" version that steers without revealing answer.

- Concept: Dual metrics and tradeoff analysis
  - Why needed here: High accuracy does not imply good collaboration. ConDABench shows newer models often achieve correctness via fewer turns, not sustained engagement.
  - Quick check question: If Model A has 80% accuracy with 1.1 turns and Model B has 75% accuracy with 3.0 turns, which is "better"? What additional information do you need?

## Architecture Onboarding

- Component map: Curator -> Code Generator + Audited Reviewer -> User Proxy -> Evaluator + Rubric Grader
- Critical path: Curation -> Code Generation -> Proxy Construction -> Evaluation Harness. Code generation is validation chokepoint; non-convergent pairs are discarded.
- Design tradeoffs: Assistants API yields higher correctness but lower conversation quality (verbose, repetitive). Tool Calling/InfiAgent-Conv yield concise responses but struggle with deep/open-ended tasks. Reasoning effort (low/medium/high) trades off conciseness vs. accuracy.
- Failure signatures: (1) Code overfits to curated answer (e.g., drops rows to match incorrect article claim); (2) Proxy leaks parameters proactively; (3) Models repeat questions or generate unnecessary code (high D.1/D.3 rubrics); (4) Temporal/contextual misinterpretation in long conversations.
- First 3 experiments:
  1. Reproduce Curator -> Code Generator pipeline on small article+dataset subset; inspect convergence rate and failure modes.
  2. Run simple (q,a,d,c) task through User Proxy with candidate DA assistant; verify Proxy answers parameter queries correctly without leaking c.
  3. Compare reasoning model (e.g., o3) and non-reasoning model (e.g., GPT-4.1) on 10 shallow + 10 deep tasks; log Score, ConvQ, and average conversation length to validate Pareto tradeoff observation.

## Open Questions the Paper Calls Out

- How can conversational data analysis benchmarks be adapted to evaluate agents on tasks involving dynamic intent shifts or purely exploratory queries?
  - Basis: Current framework "does not consider cases where users change their intent mid-conversation or pose exploratory queries, such as 'Give me some insights about this data.'"
  - Why unresolved: User Proxy is grounded in static code derived to support fixed answer, structurally incapable of simulating user who modifies goals dynamically.

- What training or architectural modifications are necessary to improve LLMs' ability to sustain long-form engagement without sacrificing task efficiency?
  - Basis: While newer models are more accurate, they are "not necessarily better at solving tasks that require sustained, long-form engagement" and often rely on shorter interactions.
  - Why unresolved: Current models optimize for "Pareto frontier" of solving problems quickly with minimal interaction, rather than acting as collaborative partners.

- How can code generation pipeline be improved to prevent "over-fitting" to provided answer artifacts?
  - Basis: Code generator produces incorrect code that explicitly filters correct data to match potentially erroneous answer in text.
  - Why unresolved: Code Generator optimizes strictly for matching provided answer, which can lead to "gaming" validation step via hard-coded filters.

## Limitations
- 40.8% of tasks fail to converge during code generation, potentially biasing benchmark toward easier, more deterministic analyses
- User Proxy effectiveness depends on supporting code capturing all latent assumptions needed for consistent clarification responses
- ConvQ metric, while showing good human alignment (F1=0.75), was trained on 120 human annotations that may not capture full diversity of conversational quality patterns

## Confidence
- Code-grounded evaluation mechanism: High confidence
- Multi-agent architecture effectiveness: Medium confidence
- Tradeoff between correctness and conversation quality: High confidence
- Generalizability to real-world data analysis: Medium confidence

## Next Checks
1. Implement ablation study comparing performance with and without Audited Reviewer feedback loop to quantify impact on code generation quality
2. Conduct human evaluation study specifically targeting User Proxy consistency by having multiple annotators verify parameter responses across different assistants
3. Expand ConvQ training dataset with annotations for 300 additional tasks spanning all reasoning effort categories to test metric robustness across task complexity levels