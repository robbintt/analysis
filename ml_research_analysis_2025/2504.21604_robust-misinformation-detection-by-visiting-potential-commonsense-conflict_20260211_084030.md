---
ver: rpa2
title: Robust Misinformation Detection by Visiting Potential Commonsense Conflict
arxiv_id: '2504.21604'
source_url: https://arxiv.org/abs/2504.21604
tags:
- commonsense
- md-pcc
- fake
- wang
- misinformation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting misinformation online
  by leveraging commonsense knowledge to identify potential conflicts in articles.
  The proposed method, MD-PCC, uses a plug-and-play augmentation approach that constructs
  commonsense expressions to highlight potential conflicts inferred from the difference
  between extracted commonsense triplets and golden ones from the COMET tool.
---

# Robust Misinformation Detection by Visiting Potential Commonsense Conflict

## Quick Facts
- arXiv ID: 2504.21604
- Source URL: https://arxiv.org/abs/2504.21604
- Reference count: 24
- Primary result: MD-PCC achieves +0.92 to +2.75 Macro F1 improvements over baselines on misinformation detection benchmarks

## Executive Summary
This paper addresses misinformation detection by leveraging commonsense knowledge to identify potential conflicts in articles. The proposed method, MD-PCC, constructs commonsense expressions highlighting conflicts between extracted triplets and golden ones from COMET, then uses these as augmentations to train existing detectors. Experiments across four public datasets and a new commonsense-oriented CoMis dataset demonstrate consistent improvements over baselines, with in-context learning and conjunction words identified as particularly important components.

## Method Summary
MD-PCC is a plug-and-play augmentation approach that enhances misinformation detection by constructing commonsense expressions to highlight potential conflicts. The method extracts commonsense triplets (subject, relation, object) from articles using an LLM with in-context learning, generates golden objects using COMET, and computes conflict scores based on the difference between extracted and golden objects. High-conflict triplets are expressed with "However ... instead of ..." while low-conflict ones use "And ...". These expressions are concatenated to original articles to train any existing misinformation detection method. The approach is tested across four public benchmarks and a new CoMis dataset where fake articles are specifically designed around commonsense conflicts.

## Key Results
- MD-PCC consistently outperforms existing baselines across four public datasets and CoMis
- Macro F1 score improvements range from +0.92 to +2.75
- In-context learning and conjunction words are identified as critical components through ablation studies
- The method successfully transfers between English and Chinese datasets

## Why This Works (Mechanism)

### Mechanism 1
Fake articles are more likely to contain statements conflicting with established commonsense knowledge compared to real articles. MD-PCC extracts commonsense triplets and measures conflict via the difference between extracted and golden objects using perplexity-based scoring. When articles involve topics outside ATOMIC/ConceptNet coverage, or when misinformation relies on factual errors rather than commonsense violations, the method may fail.

### Mechanism 2
In-context learning improves triplet extraction quality, which directly impacts detection performance. Few-shot examples guide the T5-based triplet extractor to correctly identify subject-relation-object structures, with filtering based on generation log-probabilities. When in-context examples are poorly matched to target domain, or when K (number of examples) is too low, extraction quality degrades.

### Mechanism 3
Explicit adversarial conjunctions ("However") in commonsense expressions help detectors learn the correlation between conflict signals and veracity labels. Template-based expressions use "However ... instead of ..." for high-conflict triplets and "And ..." for low-conflict ones. When articles have no extractable triplets meeting the conflict threshold, or when templates produce incoherent expressions, the method may confuse rather than help detectors.

## Foundational Learning

- **Commonsense Knowledge Bases (ATOMIC, ConceptNet)**: Required to understand relation types and interpret triplet extraction and conflict scoring. Quick check: Can you name three commonsense relation types used in ATOMIC and explain what xEffect captures?

- **In-Context Learning (ICL) with Large Language Models**: Critical for the triplet extractor's quality through few-shot prompting. Quick check: What happens to extraction quality if in-context examples are from a different domain than the target articles?

- **Perplexity-based Confidence Filtering**: Used to filter extracted triplets using generation log-probabilities. Quick check: Given a generated triplet with perplexity 2.5 vs 15.0, which would pass the filter if ε=0.8?

## Architecture Onboarding

- **Component map**: Article → [Triplet Extractor (T5 + ICL)] → Extracted Triplets (s, r, o) → [Confidence Filter] → Filtered Triplets → [COMET] → Golden Objects ô for each (s, r) → [Conflict Scorer] → Conflict Scores c → [Template Filler] → Commonsense Expression e → [Concatenation] → Augmented Article x̂ = x ⊕ e → [Any MD Detector Fθ] → Prediction

- **Critical path**: Triplet extraction → Conflict scoring → Expression construction. If extraction fails or produces noisy triplets, downstream components degrade. Ablation confirms ICL removal is most damaging.

- **Design tradeoffs**: Template rigidity vs expressiveness ensures consistency but may produce awkward phrasing; computational overhead adds ~6 hours per 10K articles with frozen parameters reducing memory to ~7GB; conflict threshold μ=0.6 balances detection sensitivity against false positives.

- **Failure signatures**: No triplets extracted → No expression generated → Falls back to original article only; low conflict scores across all triplets → "And" expressions added regardless of veracity; template produces incoherent text → May confuse detector rather than help.

- **First 3 experiments**: 1) Reproduce ablation on GossipCop with and without ICL to verify macro F1 drop of ~1.4 points; 2) Test sensitivity to conflict threshold μ ∈ {0.4, 0.6, 0.8} on CoMis; 3) Zero-shot domain transfer by training on GossipCop and testing on PolitiFact to assess cross-domain generalization.

## Open Questions the Paper Calls Out

- **Open Question 1**: To what extent does MD-PCC degrade when applied to misinformation relying on contextual or political inconsistencies rather than physical commonsense conflicts? The paper's evaluation relies heavily on datasets where physical commonsense is a strong signal, with efficacy on nuanced political misinformation remaining less validated.

- **Open Question 2**: How robust is MD-PCC to the "hallucination" or accuracy limits of the COMET model used to generate golden objects? The method treats COMET-generated objects as "golden" knowledge without analyzing error propagation from the tool's own inaccuracies.

- **Open Question 3**: Does explicit inclusion of adversarial conjunction words cause detectors to learn superficial linguistic shortcuts rather than semantic conflict? The ablation study notes conjunction words are "more important" than extracted objects because detectors can "effectively learn the pattern between conjunctions and veracity labels."

## Limitations
- Effectiveness depends heavily on quality of extracted triplets and coverage of commonsense knowledge bases
- May fail when articles involve topics outside ATOMIC/ConceptNet scope
- Conflict scoring assumes differences between extracted and golden objects indicate misinformation, which may not hold for nuanced cases

## Confidence
- **High Confidence**: Ablation results showing in-context learning improves triplet extraction quality
- **Medium Confidence**: Generalizability of conjunction-based augmentation strategy across domains
- **Medium Confidence**: Claim that fake articles are more likely to contain commonsense conflicts

## Next Checks
1. **Zero-shot domain transfer**: Train MD-PCC on one dataset and test on a completely different domain to assess cross-domain robustness
2. **Adversarial perturbation analysis**: Systematically introduce commonsense violations into real articles and measure detection accuracy degradation
3. **Template quality audit**: Manually evaluate 100 randomly selected augmented expressions for coherence and relevance to determine false positive rates