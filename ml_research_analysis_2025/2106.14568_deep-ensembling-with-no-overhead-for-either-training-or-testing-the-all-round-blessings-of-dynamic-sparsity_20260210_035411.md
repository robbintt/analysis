---
ver: rpa2
title: 'Deep Ensembling with No Overhead for either Training or Testing: The All-Round
  Blessings of Dynamic Sparsity'
arxiv_id: '2106.14568'
source_url: https://arxiv.org/abs/2106.14568
tags:
- ensemble
- training
- sparse
- edst
- dense
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FreeTickets, a novel ensemble learning framework
  that leverages dynamic sparse training to create efficient and accurate ensembles
  of sparse subnetworks. FreeTickets addresses the computational and memory limitations
  of traditional deep ensembles by directly training sparse subnetworks from scratch
  and extracting diverse yet accurate subnetworks during this efficient, sparse-to-sparse
  training.
---

# Deep Ensembling with No Overhead for either Training or Testing: The All-Round Blessings of Dynamic Sparsity

## Quick Facts
- arXiv ID: 2106.14568
- Source URL: https://arxiv.org/abs/2106.14568
- Reference count: 40
- One-line primary result: FreeTickets achieves efficient deep ensembling by training sparse subnetworks with dynamic sparsity, yielding ensembles that outperform dense baselines in accuracy, uncertainty estimation, and out-of-distribution robustness while using significantly fewer FLOPs.

## Executive Summary
This paper introduces FreeTickets, a novel ensemble learning framework that leverages dynamic sparse training to create efficient and accurate ensembles of sparse subnetworks. FreeTickets addresses the computational and memory limitations of traditional deep ensembles by directly training sparse subnetworks from scratch and extracting diverse yet accurate subnetworks during this efficient, sparse-to-sparse training. The framework is instantiated through two methods: Dynamic Sparse Training Ensemble (DST Ensemble) and Efficient Dynamic Sparse Training Ensemble (EDST Ensemble). DST Ensemble independently trains multiple sparse networks with dynamic sparsity, while EDST Ensemble yields many diverse subnetworks in a single training run. Both methods significantly outperform dense baselines in terms of predictive accuracy, uncertainty estimation, out-of-distribution robustness, and efficiency for both training and inference. For example, on ImageNet, FreeTickets with ResNet50 outperforms the naive deep ensemble using only around 1/5 of the training FLOPs required by the latter. The diversity of the subnetworks is analyzed, confirming the effectiveness of the methods in inducing model diversity. The results suggest that sparse neural networks enjoy favorable properties beyond efficiency, opening new research directions.

## Method Summary
FreeTickets introduces two ensemble methods built on dynamic sparse training. DST Ensemble trains M independent sparse networks using RigL with ERK initialization, periodic prune-and-grow cycles (rate p=0.5, interval ΔT=1000-4000), and SGD optimization. EDST Ensemble achieves the same diversity in a single training run by combining an initial exploration phase with sequential refinement phases, each followed by a forced basin escape using aggressive pruning (rate q=0.8) and learning rate reset. Both methods train from scratch rather than pruning pretrained models, with ensemble predictions formed by averaging softmax probabilities across all M subnetworks. The approach maintains high accuracy while dramatically reducing computational overhead compared to dense ensembles.

## Key Results
- On CIFAR-10/100, FreeTickets outperforms naive dense ensembles using 10× fewer parameters and 100× fewer FLOPs
- On ImageNet, FreeTickets with ResNet50 outperforms dense ensembles while using only ~1/5 of the training FLOPs
- FreeTickets achieves superior uncertainty estimation (lower NLL/ECE) and out-of-distribution robustness compared to single dense models and traditional ensembles
- Analysis confirms subnetworks have diverse topologies and prediction patterns, validating the diversity mechanism

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Diversity from topology exploration enhances ensemble performance.
- Mechanism: Dynamic Sparse Training (DST) uses periodic prune-and-grow cycles to explore the parameter space, allowing sparse networks to converge to different connectivities (topologies). This creates structural diversity among ensemble members, which translates to functional diversity (different prediction errors). Averaging predictions from these structurally diverse models reduces variance, improving generalization, robustness, and uncertainty estimation beyond what random initialization alone achieves in dense ensembles.
- Core assumption: The prune-and-grow exploration strategy in DST leads to subnetworks that are both accurate and diverse in their error patterns.
- Evidence anchors:
  - [abstract] "extract diverse yet accurate subnetworks during this efficient, sparse-to-sparse training."
  - [section 5.1] "each DST run converges to different sparse connectivities, promoting even higher diversity over the naive dense Ensemble."
  - [corpus] Related work like "NeuroTrails" also uses dynamic sparse heads for ensembling, supporting this mechanism.
- Break condition: The exploration rate `p` is too small or the update interval `ΔT` is too large, leading to subnetworks that are topologically similar and fail to provide diverse predictions.

### Mechanism 2
- Claim: An ensemble of cheap, sparse models can outperform a single, expensive dense model.
- Mechanism: Individually, a sparse network may underperform a dense one. However, FreeTickets demonstrates that the collective performance of an ensemble of diverse sparse models can compensate for their reduced individual capacity. The efficiency gain from sparsity (fewer parameters, fewer FLOPs) allows for training many more models, and their diversity ensures the ensemble's variance is low, leading to a superior accuracy-to-compute ratio.
- Core assumption: The performance boost from ensembling diverse models outweighs the performance drop from using sparse models.
- Evidence anchors:
  - [abstract] "Despite being an ensemble method, FreeTickets has even fewer parameters and training FLOPs than a single dense model."
  - [section 4, Table 1-3] Shows DST/EDST Ensemble outperforming the Single Dense Model and often the Dense Ensemble on accuracy, NLL, and ECE.
  - [corpus] Papers like "Structured Basis Function Networks" explore multi-hypothesis ensembles, aligning with the principle that managed diversity improves predictive uncertainty.
- Break condition: The individual sparse models are too inaccurate (e.g., sparsity is set too high) such that their combined predictions remain worse than a single dense baseline.

### Mechanism 3
- Claim: A single training run can yield multiple diverse subnetworks via forced exploration phases.
- Mechanism: The EDST Ensemble method creates M diverse models in one run. It begins with an "exploration phase" (large learning rate, DST) to broadly search the parameter space. It then enters M sequential "refinement phases." In each phase, the model is trained to convergence and saved. To ensure diversity, it then *forces* an escape from the current local optimum by applying a large global exploration rate `q` (pruning a huge fraction of weights) and increasing the learning rate before starting the next refinement phase.
- Core assumption: The forced basin escape via aggressive pruning and a learning rate increase reliably leads to new, performant local optima without catastrophic forgetting or collapse.
- Evidence anchors:
  - [abstract] "EDST Ensemble yields many diverse subnetworks in a single training run."
  - [section 3.2.2] "force the model to escape the current basin by significantly changing a large fraction of the sparse connectivity... The training procedure... is one end-to-end training run consisting of one exploration phase followed by M consecutive refinement phases."
  - [corpus] Corpus signals are weak for directly corroborating the specific EDST phased mechanism, though related works discuss dynamic sparsity for ensembling.
- Break condition: The forced escape is too weak (`q` is small), causing subsequent subnetworks to re-converge to similar solutions, or it's too strong, destroying learned structure and degrading performance.

## Foundational Learning

- **Ensemble Learning & Diversity.**
  - Why needed here: The paper's core hypothesis is that diversity among models is key to a strong ensemble. Understanding that a good ensemble requires members to make different mistakes is essential to grasp why DST's topological diversity is valuable.
  - Quick check question: Why is an ensemble of five identical models no better than a single model?

- **Sparsity & Pruning.**
  - Why needed here: FreeTickets is built on sparse neural networks, where most weights are zero. Grasping that these networks are computationally cheaper (fewer FLOPs, less memory) is the foundation for understanding the paper's efficiency claims.
  - Quick check question: If a network has 90% sparsity, approximately what fraction of its weights are non-zero?

- **Dynamic Sparse Training (DST).**
  - Why needed here: This is the enabling technology. Unlike static pruning, DST changes the network's structure during training, which is the active process that generates the necessary diversity for the ensemble.
  - Quick check question: In DST, what two opposing actions are periodically taken on the network's weights?

## Architecture Onboarding

- **Component map:**
  ERK Initialization -> Sparse Network -> DST Parameter Exploration (Prune & Grow) -> SGD Optimization -> Save Model (DST Ensemble) OR Save Model + Forced Escape (EDST Ensemble) -> Ensemble Aggregation

- **Critical path:**
  1. Initialize: Create a sparse network using the ERK distribution.
  2. Generate Subnetworks (DST Ensemble): Run M independent training jobs. In each, loop over epochs and perform prune-and-grow every `ΔT` steps. Save each final model.
  3. Generate Subnetworks (EDST Ensemble): Run one job. First, run an exploration phase. Then, loop M times: train, save the model, then force a basin escape via aggressive pruning (`q`) and a learning rate reset.
  4. Ensemble: Load all M saved subnetworks. For any input, get a prediction from each and return their average.

- **Design tradeoffs:**
  - **DST Ensemble vs. EDST Ensemble:** DST Ensemble is simpler and achieves higher diversity (and often accuracy) with multiple independent runs. EDST Ensemble is far more computationally efficient (single run) but may produce slightly less diverse subnetworks.
  - **Sparsity (S):** Higher sparsity (e.g., 0.9) increases efficiency and allows for more ensemble members but reduces individual model accuracy. The paper's sweet spot is 0.8-0.9.
  - **Exploration Rate (p):** A higher rate (e.g., 0.5) increases diversity but may disrupt learning if too high. The paper uses `p=0.5` for CIFAR/ImageNet.
  - **Global Exploration Rate (q - EDST):** A higher rate (e.g., 0.8) forces greater diversity between refinement phases but risks destroying learned structure.

- **Failure signatures:**
  - **Ensemble accuracy is no better than a single dense model.** The individual sparse models are likely too inaccurate. Reduce the sparsity level `S`.
  - **EDST Ensemble members have similar predictions.** The forced basin escape is ineffective. Increase the global exploration rate `q`.
  - **Training becomes unstable or loss diverges (especially in EDST).** The exploration rate `p` or `q` may be too aggressive. Try reducing them.

- **First 3 experiments:**
  1. **Validate Core Claim (DST Ensemble):** Train a single dense Wide ResNet-28-10 on CIFAR-10. Separately, train M=3 sparse networks using DST Ensemble (S=0.8). Compare the final ensemble accuracy against the single dense model.
  2. **Validate Efficiency (EDST Ensemble):** Implement EDST Ensemble with a fixed FLOP budget (e.g., equivalent to 0.6x a dense run). Compare its accuracy to a traditional dense ensemble trained for the same total FLOP budget.
  3. **Ablation on Forced Diversity:** Run EDST Ensemble once with the standard high global exploration rate `q=0.8` and once with a low rate `q=0.1`. Measure the diversity (e.g., KL divergence) between the resulting subnetworks to prove the mechanism.

## Open Questions the Paper Calls Out
None

## Limitations
- Lack of systematic ablation studies for key hyperparameters, particularly the forced escape mechanism in EDST Ensemble
- Limited exploration of how sensitive results are to specific DST hyperparameters (p, ΔT) and ERK initialization
- Diversity analysis shows structural differences but doesn't directly prove these differences translate to improved ensemble performance beyond random initialization

## Confidence

- **High confidence:** The efficiency claims regarding FLOPs and parameters are well-supported by the methodology and results
- **Medium confidence:** The claim that topological diversity from DST improves ensemble performance beyond random initialization
- **Medium confidence:** The effectiveness of the EDST Ensemble's forced escape mechanism

## Next Checks

1. **Ablation study on EDST's forced escape:** Run EDST Ensemble with multiple q values (0.1, 0.5, 0.8) and measure both diversity metrics and final ensemble performance to quantify the mechanism's contribution.

2. **Random initialization baseline:** Train M dense networks with different random seeds and compare their diversity and ensemble performance against DST Ensemble to determine if topological exploration provides unique benefits.

3. **Hyperparameter sensitivity analysis:** Systematically vary p (0.3, 0.5, 0.7) and ΔT (500, 1000, 2000) in DST Ensemble to identify which parameters most affect both individual model accuracy and ensemble diversity.