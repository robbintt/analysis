---
ver: rpa2
title: 'Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation Framework'
arxiv_id: '2504.01908'
source_url: https://arxiv.org/abs/2504.01908
tags:
- data
- synthetic
- training
- samples
- framework
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces a holdout-based benchmarking framework for
  synthetic tabular data evaluation, addressing the challenge of simultaneously measuring
  fidelity and privacy. The core method employs a three-pronged approach: (1) accuracy
  metrics using total variation distance on discretized univariate, bivariate, and
  sequential coherence distributions; (2) embedding-based centroid similarity using
  cosine distance and discriminative model AUC; and (3) novelty assessment through
  nearest-neighbor distances.'
---

# Benchmarking Synthetic Tabular Data: A Multi-Dimensional Evaluation Framework

## Quick Facts
- arXiv ID: 2504.01908
- Source URL: https://arxiv.org/abs/2504.01908
- Reference count: 39
- Primary result: Introduces holdout-based benchmarking framework for synthetic tabular data evaluation measuring fidelity and privacy simultaneously

## Executive Summary
This paper presents a comprehensive framework for evaluating synthetic tabular data quality by benchmarking against holdout samples. The method addresses the critical challenge of simultaneously measuring fidelity (how well synthetic data matches the original distribution) and novelty (ensuring privacy preservation). By comparing synthetic-to-training similarity against holdout-to-training similarity, the framework identifies when synthetic data has learned underlying patterns versus when it has memorized specific training records. The approach supports mixed data types and sequential data with contextual information, providing interpretable metrics for data scientists and practitioners.

## Method Summary
The framework employs a three-pronged approach: (1) accuracy metrics using total variation distance on discretized univariate, bivariate, and sequential coherence distributions; (2) embedding-based centroid similarity using cosine distance and discriminative model AUC; and (3) novelty assessment through nearest-neighbor distances. Numerical and datetime columns are discretized into deciles based on training data quantiles, while categorical columns retain only the top-10 most frequent categories. Each row is serialized to a string and embedded using all-MiniLM-L6-v2 (384-dimensional space). The framework requires training, synthetic, and holdout DataFrames, with optional context tables for sequential data.

## Key Results
- Synthetic data quality can be effectively benchmarked against holdout samples, with optimal synthetic data achieving fidelity scores approaching the holdout reference
- The framework demonstrates that synthetic data can maintain sufficient novelty to preserve privacy while matching training data as closely as holdout samples
- Open-source implementation enables standardized, reproducible evaluation across different synthesis techniques
- For the UCI Adult Census dataset, the method successfully distinguishes between generators with different fidelity-privacy trade-offs

## Why This Works (Mechanism)

### Mechanism 1: Holdout-Based Reference Benchmarking
Using an unseen holdout sample as a reference point enables simultaneous assessment of both fidelity and novelty in synthetic data. The framework splits original data into training and holdout sets. Synthetic data is generated from training data only. Quality is measured by comparing synthetic-to-training similarity against holdout-to-training similarity. If synthetic data matches training data approximately as well as holdout data does (neither better nor worse), it has learned the underlying distribution without memorizing specific records. Core assumption: holdout samples represent valid novel samples from the same underlying distribution.

### Mechanism 2: Discretized Total Variation Distance for Multi-Scale Accuracy
Discretizing continuous and categorical columns into normalized 10-bin frequency vectors enables consistent, interpretable accuracy comparison across mixed data types. Numerical and datetime columns are discretized into deciles based on training data quantiles. Categorical columns retain only top-10 most frequent categories. For each feature, normalized frequency vectors are constructed for both training and synthetic data. Accuracy = 100% - (0.5 × L1 norm of difference). This is computed for univariate distributions, bivariate contingency tables (capturing pairwise dependencies), and coherence tables (capturing sequential transitions).

### Mechanism 3: Embedding-Space Distance Analysis for Novelty Detection
Mapping tabular records to a pre-trained language model embedding space enables high-dimensional similarity and novelty assessment that captures complex joint distributions. Each row is serialized to a string (value_col1;value_col2;...), embedded using all-MiniLM-L6-v2 (384-dimensional space). Centroids are computed for training/synthetic/holdout groups. Cosine similarity between centroids measures distributional alignment. A discriminative model trained to distinguish synthetic from real records provides AUC-based distinguishability scores. Distance to closest record (DCR) from synthetic to training samples measures novelty.

## Foundational Learning

- **Concept: Total Variation Distance (TVD)**
  - Why needed here: All accuracy metrics are computed as (100% - TVD). Understanding that TVD measures the maximum difference between two probability distributions (ranging 0-1) explains why discretization into normalized bins is necessary for consistent comparison.
  - Quick check question: If two distributions have TVD of 0.15, what is the accuracy score? (Answer: 85%)

- **Concept: Holdout Methodology in Generative Models**
  - Why needed here: Unlike supervised learning where holdout directly evaluates predictive generalization, here holdout serves as a "novelty reference"—defining what statistical similarity looks like for samples drawn from the same distribution but not used in training.
  - Quick check question: Why should synthetic data not be more similar to training data than holdout data is? (Answer: Excessive similarity suggests memorization/overfitting, not learning the true distribution)

- **Concept: Embedding Space Geometric Properties**
  - Why needed here: The framework uses cosine similarity for centroid comparison and L2 distance for nearest-neighbor calculations. Understanding that cosine similarity captures directional alignment (invariant to magnitude) while L2 distance captures absolute proximity is necessary to interpret these metrics correctly.
  - Quick check question: If two centroids have cosine similarity of 0.95 but large L2 distance, what does this indicate? (Answer: The distributions share similar directional patterns in embedding space but may differ in scale or have shifted means)

## Architecture Onboarding

- **Component map:** Input Layer (three pandas DataFrames) -> Discretization Module (10-bin frequency vectors) -> Accuracy Engine (univariate → bivariate → coherence TVD) -> Embedding Module (all-MiniLM-L6-v2 serialization) -> Similarity Engine (centroid cosine + discriminative AUC) -> Distance Engine (DCR distributions) -> Report Generator (HTML visualization)

- **Critical path:**
  1. Data validation and discretization (training bin boundaries must be computed first, then applied to synthetic and holdout)
  2. Accuracy computation (univariate → bivariate → coherence for sequential)
  3. Embedding generation (most computationally intensive; batches recommended)
  4. Similarity and distance calculations (depend on embeddings)
  5. Report assembly and metric aggregation

- **Design tradeoffs:**
  - **Decile discretization vs. continuous metrics:** Discretization enables interpretability and cross-type consistency but loses fine-grained distributional detail. Assumption: 10 bins is sufficient for most quality assessment needs.
  - **Fixed top-10 categories vs. full cardinality:** Truncating categorical tails improves computational efficiency and visualization but may miss rare-but-important categories. Assumption: Tail categories contribute minimally to overall distributional fidelity.
  - **Pre-trained LM embedding vs. custom encoder:** Using all-MiniLM-L6-v2 provides zero-training embeddings but may not capture domain-specific semantics. Trade-off: convenience vs. specialization.
  - **Holdout requirement vs. small-data scenarios:** Framework assumes sufficient data for meaningful train/holdout split. For small datasets, this requirement may be prohibitive.

- **Failure signatures:**
  - **Accuracy >> holdout reference:** Synthetic data is overfitting/memorizing training data. Check DCR share—if significantly >50%, privacy risk is elevated.
  - **Accuracy << holdout reference:** Generator failed to learn distribution. Check individual column accuracies to identify systematic failures.
  - **High discriminator AUC (>0.9):** Synthetic data has systematic artifacts making it easily distinguishable from real data.
  - **DCR share significantly >50%:** Synthetic samples cluster closer to training than expected; possible privacy leakage.
  - **Coherence accuracy near 50% (for sequential data):** Temporal/sequential dependencies not captured; model treating time steps as independent.

- **First 3 experiments:**
  1. **Baseline validation:** Run framework on a holdout sample vs. training (should produce accuracy ~theoretical max, DCR share ~50%, discriminator AUC ~0.5). This validates the reference metrics are correctly calibrated.
  2. **Degradation test:** Apply perturbation methods (e.g., flip K% as shown in paper Figure 7) to create synthetic datasets with known quality degradation. Verify metrics capture the expected fidelity-privacy trade-off curve.
  3. **Generator comparison:** Generate synthetic data using multiple synthesizers (e.g., mostly default, synthpop, gretel as referenced). Compare metric profiles to identify each generator's characteristic strengths/weaknesses.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of embedding model affect the reliability of similarity and distance-based novelty assessments across different data domains?
- Basis in paper: [explicit] "While the choice of language model is flexible, we specifically opted for all-MiniLML6v2 as it is a lightweight, compute-efficient universal model."
- Why unresolved: The paper acknowledges flexibility in model choice but provides no empirical comparison or guidance on how different embedding models impact the centroid similarity and DCR metrics.
- What evidence would resolve it: Systematic comparison of multiple embedding models (e.g., different transformer architectures, domain-specific models) on benchmark datasets, measuring correlation between model choice and metric outcomes.

### Open Question 2
- Question: Does discretizing numerical values into deciles and limiting categorical columns to top-10 categories introduce systematic biases in accuracy assessments for high-cardinality or skewed distributions?
- Basis in paper: [explicit] "For categorical columns, only the ten most frequent categories are retained, while the less common ones are excluded."
- Why unresolved: The discretization strategy discards information from tail distributions, potentially masking fidelity issues in underrepresented subpopulations—a stated goal of synthetic data.
- What evidence would resolve it: Experiments comparing discretized vs. full-distribution accuracy metrics on datasets with known long-tail properties, correlating with downstream task performance.

### Open Question 3
- Question: Can the coherence metric, based on pairs of successive sequence elements, adequately capture longer-range temporal dependencies in sequential data?
- Basis in paper: [explicit] "For each data subject, we randomly sample two successive sequence elements (time steps) from their sequential data."
- Why unresolved: Random sampling of adjacent pairs may fail to detect violations of higher-order Markov dependencies or multi-step patterns critical in behavioral and time-series applications.
- What evidence would resolve it: Evaluation on synthetic sequential data with injected long-range dependency violations, measuring detection sensitivity compared to alternative sequential fidelity metrics.

## Limitations
- Framework assumes sufficient data for meaningful train/holdout splits; small datasets may produce unreliable reference metrics
- Discretization into deciles and top-10 categories may mask fidelity issues in tail distributions and underrepresented subpopulations
- Embedding space assumptions are not validated—the all-MiniLM-L6-v2 model was trained on general text, not tabular data

## Confidence
- **High Confidence:** Holdout methodology for benchmarking fidelity vs. novelty is well-grounded in machine learning theory; discretized TVD approach for multi-scale accuracy is transparent and reproducible
- **Medium Confidence:** Embedding-based novelty assessment is mechanistically sound but lacks validation that MiniLM embeddings capture privacy-relevant distances in tabular data
- **Low Confidence:** Theoretical maximum accuracy values and their calibration across different data distributions are not explicitly specified

## Next Checks
1. **Reference Calibration Test:** Generate synthetic data from a known distribution (e.g., Gaussian mixtures) and verify that metrics correctly identify when synthetic data matches or deviates from the reference distribution
2. **Cardinality Stress Test:** Evaluate framework performance on categorical columns with varying cardinality (2 vs. 100+ categories) to validate the top-10 truncation assumption
3. **Embedding Space Validation:** Compare MiniLM-based novelty detection against alternative embedding approaches (e.g., autoencoder-trained on tabular data) to assess sensitivity to embedding choice