---
ver: rpa2
title: 'Large Language Models for the Summarization of Czech Documents: From History
  to the Present'
arxiv_id: '2511.18848'
source_url: https://arxiv.org/abs/2511.18848
tags:
- czech
- summarization
- dataset
- language
- summaries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of Czech text summarization,
  particularly for historical documents, due to the language's linguistic complexity
  and lack of high-quality annotated datasets. The authors leverage advanced Large
  Language Models (LLMs), specifically Mistral 7B and mT5, to establish new state-of-the-art
  results on the SumeCzech dataset for modern Czech summarization.
---

# Large Language Models for the Summarization of Czech Documents: From History to the Present

## Quick Facts
- arXiv ID: 2511.18848
- Source URL: https://arxiv.org/abs/2511.18848
- Reference count: 40
- Key outcome: Establishes new state-of-the-art Czech summarization results using Mistral 7B and mT5, introduces Posel od Čerchova historical dataset

## Executive Summary
This study addresses the challenge of Czech text summarization, particularly for historical documents, due to the language's linguistic complexity and lack of high-quality annotated datasets. The authors leverage advanced Large Language Models (LLMs), specifically Mistral 7B and mT5, to establish new state-of-the-art results on the SumeCzech dataset for modern Czech summarization. Additionally, they propose a novel translation-based approach (TST) that translates Czech texts into English, summarizes them using an English-language model, and translates the summaries back into Czech. The authors also introduce a new dataset, Posel od Čerchova, derived from 19th-century Czech publications, for historical text summarization. Experimental results show that the Mistral-based model (M7B-SC) achieves new performance benchmarks on SumeCzech, while the translation-based approach proves highly competitive. These contributions advance Czech summarization research and provide valuable resources for processing historical texts in digital humanities and cultural heritage preservation.

## Method Summary
The authors fine-tune multilingual LLMs (Mistral 7B and mT5) on Czech summarization data using QLoRA for parameter-efficient adaptation. They also develop a translation-based summarization pipeline (TST) that leverages English summarization capabilities through translation. For historical texts, they introduce the Posel od Čerchova dataset and fine-tune models specifically for 19th-century Czech. Evaluation uses ROUGE-raw metrics to assess summary quality.

## Key Results
- Mistral 7B fine-tuned on SumeCzech achieves ROUGE raw-1 F1 of 21.2, outperforming all baselines
- Translation-based approach (TST) achieves competitive F1 scores (19.9 on POC-P, 17.5 on POC-I)
- Historical model (M7B-POC) shows more coherent grammar but more factual errors compared to TST
- mT5 struggles with historical texts, producing irrelevant or hallucinated content

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning multilingual LLMs on Czech summarization data may enable state-of-the-art performance by adapting pre-trained multilingual representations to morphologically rich, medium-resource languages. Mistral 7B's pre-training on diverse multilingual corpora provides latent Czech language understanding; QLoRA fine-tuning adapts attention patterns to Czech morphology while preserving cross-lingual semantic knowledge. Grouped-Query Attention (GQA) and Sliding Window Attention (SWA) enable efficient processing of longer Czech documents.

### Mechanism 2
The Translation-Summarization-Translation (TST) pipeline may achieve competitive performance by leveraging stronger English summarization capabilities while mitigating translation error through high-quality neural machine translation. ALMA-R translation model provides Czech↔English translation that exceeds GPT-4 quality; English Mistral 7B summarizes using richer English-specific training; back-translation recovers Czech linguistic patterns. This bypasses limited Czech summarization training data.

### Mechanism 3
Domain-specific fine-tuning on historical Czech data may enable summarization of diachronically variable 19th-century texts by learning archaic vocabulary, spelling variations, and evolving syntactic patterns. LLM-generated summaries with manual correction provide training signal that bridges modern and historical Czech; fine-tuning adapts models to handle orthographic variation and different journalistic conventions from the 1880s.

## Foundational Learning

- **Concept**: Transformer Attention Mechanisms (Self-Attention, GQA, SWA)
  - Why needed here: Mistral 7B uses Grouped-Query Attention for inference speed and Sliding Window Attention for long-document efficiency—critical for summarizing 400+ word Czech articles
  - Quick check question: Given a sequence of length n and window size w, how does SWA change attention complexity from standard O(n²)?

- **Concept**: Parameter-Efficient Fine-Tuning (QLoRA)
  - Why needed here: Paper fine-tunes 7B parameter Mistral on single A40 GPU using 4-bit quantization + LoRA adapters; understanding this is essential for reproducing results on limited hardware
  - Quick check question: If a model has d=4096 hidden dimensions and LoRA rank r=8, how many trainable parameters does each adapter matrix add compared to full fine-tuning?

- **Concept**: ROUGE Evaluation Metrics
  - Why needed here: Paper uses ROUGE-raw (no preprocessing) to evaluate Czech summaries; differs from standard ROUGE which assumes English-like tokenization
  - Quick check question: What is the difference between ROUGE-1, ROUGE-2, and ROUGE-L in terms of what n-grams they measure?

## Architecture Onboarding

- **Component map**: SumeCzech/POC data → Preprocessing → Mistral 7B/mT5/TST pipeline → QLoRA/AdamW training → Inference → ROUGE-raw evaluation

- **Critical path**: Data preparation → Model selection (Mistral vs mT5 vs TST) → Fine-tuning (QLoRA config or AdamW) → Inference → ROUGE-raw evaluation

- **Design tradeoffs**:
  - M7B-SC: Best ROUGE scores (21.2 F1), requires 7B params + GPU memory, produces coherent but occasionally factually inaccurate summaries
  - mT5-SC: Second-best (19.2 F1), smaller footprint (580M params), faster inference, but struggles with historical texts (irrelevant outputs)
  - TST: Competitive F1 (17.5-19.9), no Czech fine-tuning needed, slower (3 inference calls), fewer factual errors but more translation artifacts
  - M7B-POC vs TST for historical: M7B-POC more coherent grammar but more factual errors; TST less coherent but fewer factual errors

- **Failure signatures**:
  - mT5 on historical data: Produces completely irrelevant or extractive summaries
  - M7B-POC: Grammatical archaisms + factual errors
  - TST: Translation artifacts, stylistic mismatches, some incoherence in back-translation

- **First 3 experiments**:
  1. Reproduce M7B-SC baseline: Load pre-quantized Mistral 7B, configure QLoRA (r=8, alpha=16), fine-tune on SumeCzech train split (86.5%), evaluate ROUGE-raw on test split—target F1 ≈ 21.2
  2. Compare TST pipeline: Load ALMA-R 13B for translation, English Mistral for summarization, run end-to-end on POC-P subset (106 pages)—compare F1 against M7B-POC baseline
  3. Ablate training data size: Fine-tune Mistral on 10%, 25%, 50%, 100% of SumeCzech training data to characterize data efficiency curve and minimum viable training set size

## Open Questions the Paper Calls Out

### Open Question 1
Can hybrid architectures combining direct Czech summarization models with the TST pipeline improve the trade-off between coherence and factual accuracy? The paper identifies that M7B-POC produced more coherent summaries while TST produced fewer factual errors, but does not propose or test any hybrid approach to address this trade-off.

### Open Question 2
How robust are the current approaches to different historical time periods beyond the 19th century? The Posel od Čerchova dataset covers only 19th-century publications, and the paper does not evaluate whether models generalize to earlier or later historical periods with different diachronic variation patterns.

### Open Question 3
What is the impact of synthetic reference summaries (generated by GPT-4/Claude) on evaluation reliability and model training? The paper uses LLM-generated summaries with manual correction for the historical dataset but does not analyze how this affects ROUGE scores or whether it introduces systematic biases.

### Open Question 4
Can the Posel od Čerchova dataset be scaled to support effective fine-tuning rather than just evaluation? With only 432 pages and 100 article-level summaries, the paper acknowledges the dataset as a foundation but does not explore methods to expand it or leverage data augmentation techniques for low-resource historical summarization.

## Limitations

- **Data representativeness**: The Posel od Čerchova dataset covers only 432 pages from a single historical periodical, limiting generalization to other Czech historical domains.
- **Translation pipeline reliability**: The TST approach depends heavily on the quality of the ALMA-R translation model, with no independent verification of its quality claims.
- **Evaluation methodology**: ROUGE-raw scores provide surface-level measures but may not capture factual accuracy or coherence; systematic quantification of factual errors is lacking.

## Confidence

**High Confidence**: Mistral 7B achieves state-of-the-art results on SumeCzech dataset; Multilingual LLMs demonstrate effectiveness for morphologically rich languages; Translation-based approaches can achieve competitive performance

**Medium Confidence**: Fine-tuned models consistently outperform baseline approaches; Historical text summarization benefits from domain-specific adaptation; Quality of LLM-generated historical summaries with manual correction

**Low Confidence**: Systematic evaluation of factual accuracy across models; Long-term stability of translation-based pipeline with historical texts; Generalizability of results to other medium-resource languages

## Next Checks

1. **Factual accuracy benchmarking**: Implement a systematic factual error evaluation protocol comparing M7B-POC and TST outputs on historical texts, using expert annotators to identify and categorize factual discrepancies, hallucination types, and translation artifacts.

2. **Translation pipeline stress testing**: Conduct controlled experiments varying translation quality (using different translation models with known quality scores) to quantify the impact of translation error on final summarization quality, particularly for historical Czech terms and idioms.

3. **Historical generalization study**: Test trained models on multiple historical Czech datasets from different periods and sources (beyond Posel od Čerchova) to evaluate whether domain-specific fine-tuning generalizes across diachronic variations in Czech language use.