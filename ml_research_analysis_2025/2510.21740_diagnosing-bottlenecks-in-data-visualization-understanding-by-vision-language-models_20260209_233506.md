---
ver: rpa2
title: Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language
  Models
arxiv_id: '2510.21740'
source_url: https://arxiv.org/abs/2510.21740
tags:
- data
- point
- points
- tasks
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FUGU, a benchmark for evaluating vision-language
  models' ability to understand data visualizations. The authors identify that current
  VLMs struggle with extracting data point coordinates from scatter plots, which creates
  a bottleneck for downstream reasoning.
---

# Diagnosing Bottlenecks in Data Visualization Understanding by Vision-Language Models

## Quick Facts
- arXiv ID: 2510.21740
- Source URL: https://arxiv.org/abs/2510.21740
- Reference count: 40
- Primary result: Current VLMs struggle with extracting data point coordinates from scatter plots, creating a bottleneck for downstream reasoning.

## Executive Summary
This paper introduces FUGU, a benchmark for evaluating vision-language models' ability to understand data visualizations. The authors identify that current VLMs struggle with extracting data point coordinates from scatter plots, which creates a bottleneck for downstream reasoning. Using activation patching and linear probes, they show that while visual encoders successfully represent coordinate information, the language modules often fail to access it. Providing ground-truth coordinates improves performance on simple tasks but not complex ones requiring statistical reasoning. Fine-tuning on FUGU does not achieve ceiling performance. These findings highlight fundamental architectural limitations in current VLMs for data visualization understanding.

## Method Summary
The paper evaluates three VLMs (LLaMA-3.2 11B, LLaVA-OneVision 7B, InternVL3 14B) on FUGU, a benchmark of 768 procedurally generated scatter plots with tasks requiring coordinate extraction and reasoning. The evaluation uses activation patching to identify causal information flow, linear probes to assess representational content, and ground-truth coordinate provision to test bottleneck isolation. Fine-tuning experiments test whether additional training data can overcome identified limitations.

## Key Results
- Vision encoders achieve 100% linear probe accuracy for coordinate extraction, but LLM layers show sharp accuracy drops
- Providing ground-truth coordinates improves simple task performance but consistently harms ensemble task performance
- Fine-tuning on FUGU does not achieve ceiling performance, suggesting architectural rather than data limitations
- InternVL3 maintains near-ceiling coordinate extraction accuracy even with many data points, unlike other models

## Why This Works (Mechanism)

### Mechanism 1
Vision encoders successfully represent coordinate information, but the vision-language handoff fails to preserve this information linearly. Linear probes achieve 100% test accuracy for (x,y) coordinates in vision encoder layers, but accuracy drops sharply in LLM layers—indicating the adapter/connection components perform non-linear transformations that degrade extractable spatial information.

### Mechanism 2
Coordinate extraction serves as a serial bottleneck—errors propagate through reasoning chains and cannot be recovered downstream. Models like LLaMA-3.2 and InternVL3 explicitly list coordinates in chain-of-thought before arithmetic operations. Incorrect coordinate extraction propagates errors through Euclidean distance formulas, mean calculations, etc. Providing ground-truth coordinates at inference time recovers performance on simple tasks.

### Mechanism 3
The coordinate-listing strategy that helps simple tasks harms complex ensemble reasoning tasks. For tasks requiring statistical relationships across 16-128 points, providing explicit coordinates overwhelms context windows or distracts from global pattern recognition. Models do not spontaneously list coordinates for these tasks, and forced listing degrades performance.

## Foundational Learning

- Concept: **Vision-Language Adapters/Projectors**
  - Why needed here: The paper identifies the handoff between vision encoder and LLM as the primary bottleneck. Understanding how visual features are projected into LLM embedding space is essential for diagnosing where information is lost.
  - Quick check question: Can you explain the difference between cross-attention adapters (LLaMA-3.2) and decoder-only token injection (LLaVA, InternVL)?

- Concept: **Activation Patching / Causal Tracing**
  - Why needed here: The paper uses activation patching to isolate which representations causally determine model outputs, distinguishing correlation from causation in information flow.
  - Quick check question: If patching "dot token" activations from source to target image changes the output prediction, what does that prove about those activations?

- Concept: **Linear Probing for Representational Analysis**
  - Why needed here: Linear probes determine whether information is *linearly decodable* from representations, which provides a lower bound on information availability. High probe accuracy suggests information is present; low accuracy proves it's not linearly accessible.
  - Quick check question: If a linear probe fails to extract coordinate information from LLM layers, can we conclude the LLM cannot access this information? Why or why not?

## Architecture Onboarding

- Component map: Image → Vision Encoder (layer 0: localized dot information) → Vision Encoder (deeper layers: distributed representation) → **Adapter (information loss occurs here)** → LLM layers (downstream reasoning)
- Critical path: Image → Vision Encoder → Adapter → LLM layers
- Design tradeoffs:
  - Cross-attention (LLaMA-3.2) vs. token injection (LLaVA/InternVL): Cross-attention may preserve more fine-grained spatial information but introduces more parameters; token injection is simpler but may dilute spatial precision
  - Multi-crop (InternVL) vs. single-crop: Multi-crop provides higher resolution for coordinate extraction but increases compute
- Failure signatures:
  - Coordinate extraction errors that scale with number of data points
  - Sharp drop in linear probe accuracy from vision encoder to LLM layers
  - Ground-truth coordinates improve simple tasks but harm ensemble tasks
  - Fine-tuning fails to reach ceiling performance (suggests architectural constraint, not data scarcity)
- First 3 experiments:
  1. Probe the adapter layer directly: Train linear probes on adapter outputs (before LLM input) to pinpoint whether information loss occurs in projection or in LLM processing
  2. Intervention scaling test: For models using token injection, vary the number of visual tokens and measure coordinate extraction accuracy to test whether token budget constrains spatial precision
  3. Cross-architecture probe comparison: Compare linear probe accuracy across LLaMA-3.2 (cross-attention) vs. LLaVA (token injection) at equivalent layer depths to isolate adapter architecture effects from LLM processing effects

## Open Questions the Paper Calls Out

### Open Question 1
Why does providing ground-truth coordinates systematically degrade performance on ensemble tasks (correlation, clustering, function fitting) that require reasoning over many data points? The authors identify this counterintuitive finding but do not investigate the mechanism causing performance degradation when models are given correct coordinate information.

### Open Question 2
What architectural modifications to the vision-language adapter would enable effective transfer of spatial information that is linearly decodable in the vision encoder but inaccessible to the language model? The paper identifies the vision-language handoff as the primary bottleneck but does not propose or test architectural solutions.

### Open Question 3
What architectural or training differences enable InternVL3 to maintain near-ceiling coordinate extraction accuracy as data point density increases, while LLaMA-3.2 and LLaVA-OneVision degrade substantially? The authors describe architectural differences but do not isolate which specific design choices account for InternVL3's superior coordinate extraction.

## Limitations
- Procedurally generated scatter plots with strict constraints may not generalize to real-world visualizations with continuous values, overlapping points, or varied styling conventions
- The paper does not isolate whether the vision-language bottleneck occurs in the adapter projection or in LLM processing
- The exact mechanism causing ground-truth coordinates to harm ensemble task performance remains unexplained

## Confidence
- High Confidence: Vision encoders successfully represent coordinate information that VLMs cannot access
- Medium Confidence: The vision-language handoff is the primary bottleneck for coordinate extraction
- Medium Confidence: Coordinate extraction serves as a serial bottleneck that propagates errors through reasoning chains

## Next Checks
- Probe the adapter layer directly to isolate whether information loss occurs in projection or in LLM processing
- Test whether the harm from coordinate provision on ensemble tasks reflects context window limitations or architectural constraints
- Validate that the coordinate extraction bottleneck generalizes beyond procedurally generated data to real-world visualizations