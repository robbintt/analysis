---
ver: rpa2
title: 'R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts'
arxiv_id: '2502.20395'
source_url: https://arxiv.org/abs/2502.20395
tags:
- r2-t2
- routing
- multimodal
- arxiv
- test-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts\
  \ improves routing in multimodal MoE models by dynamically adjusting routing weights\
  \ during inference based on reference samples. The method uses neighborhood-based\
  \ optimization\u2014specifically, gradient descent over similar successful tasks\u2014\
  to refine routing without retraining."
---

# R2-T2: Re-Routing in Test-Time for Multimodal Mixture-of-Experts

## Quick Facts
- arXiv ID: 2502.20395
- Source URL: https://arxiv.org/abs/2502.20395
- Authors: Zhongyang Li; Ziyue Li; Tianyi Zhou
- Reference count: 38
- Primary result: Achieves up to 6.9% accuracy gains on multimodal benchmarks by dynamically adjusting routing weights during inference without retraining

## Executive Summary
R2-T2 introduces a test-time optimization method for multimodal Mixture-of-Experts (MoE) models that dynamically adjusts routing weights based on reference samples. The approach uses Neighborhood Gradient Descent (NGD) to optimize routing weights by minimizing a surrogate loss derived from correctly-predicted neighbors, eliminating the need for ground truth labels at test time. Tested on MoAI and MoVA models, R2-T2 consistently improves performance across 8 benchmarks, achieving significant accuracy gains while matching or exceeding larger VLMs.

## Method Summary
R2-T2 optimizes routing weights in multimodal MoE models through test-time adaptation using Neighborhood Gradient Descent. For each test sample, the method retrieves k=5 nearest neighbors from a reference set of correctly-predicted samples using NV-Embed-V2 embeddings. It then runs 10 steps of gradient descent on the routing weight vector, minimizing a kernel-weighted surrogate loss that aggregates neighbor outputs. The optimization operates only on routing weights without modifying base model parameters, achieving accuracy improvements by correcting the base router's systematic biases and over-reliance on certain experts.

## Key Results
- Achieves up to 6.9% accuracy gains on MME-P benchmark for MoVA-7B model
- Outperforms baseline MoAI-7B and MoVA-7B models across all 8 evaluated benchmarks
- Matches or exceeds larger VLMs despite using smaller 7B parameter models
- Captures 70-80% of oracle performance improvement without requiring ground truth labels
- Corrects expert over-reliance patterns, shifting from ILANG to more suitable experts like IAUX and LAUX

## Why This Works (Mechanism)

### Mechanism 1: Neighborhood Surrogate Loss Optimization
R2-T2 improves routing by minimizing a surrogate loss derived from correctly-predicted neighbors rather than requiring ground truth labels at test time. Given test sample x, retrieve k nearest neighbors from a reference set of correctly-predicted samples. Compute a weighted surrogate loss L(r) = Σ K(x_i, x) × ℓ[f(x_i, r), y_i] / Σ K(x_i, x), then apply gradient descent on routing weights r for 10 steps. This transfers successful routing patterns from similar solved tasks.

### Mechanism 2: kNN Retrieval with Gaussian Kernel Weighting
Using k=5 nearest neighbors with Gaussian kernel similarity weighting yields optimal routing adjustments, outperforming ε-ball and other kernels. Embed test question using NV-Embed-V2, retrieve 5 most similar reference samples, weight their contributions by Gaussian kernel K(x_i, x) ∝ exp(-||E(x_i) - E(x)||² / 2σ²). Gaussian outperformed linear kernel by 4.4% in ablation.

### Mechanism 3: Expert Over-Reliance Correction
R2-T2 corrects the base router's over-reliance on language-aligned experts (ILANG) by shifting weight toward specialized experts (IAUX, LAUX, LIMG) based on task requirements. The pretrained router favors ILANG (language-aligned visual features) disproportionately. NGD progressively reduces ILANG dominance through case studies showing transitions from ILANG→IAUX for spatial/counting tasks, ILANG→LAUX for object-specific recognition, and ILANG→LIMG for fine-grained visual details.

## Foundational Learning

- **Mixture-of-Experts Routing**: Why needed here: R2-T2 operates entirely on routing weight vectors (softmax outputs over experts). You must understand that MoE inference cost depends on which experts are activated, and routing quality directly determines output quality. Quick check question: Given routing weights r = [0.1, 0.6, 0.3] for 3 experts, which expert contributes most to the final output, and what happens if you change r to [0.3, 0.1, 0.6]?

- **Test-Time Adaptation vs. Training-Time Optimization**: Why needed here: R2-T2 updates routing weights without modifying any model parameters. This is fundamentally different from fine-tuning—you're optimizing inference behavior using only reference data. Quick check question: If you run R2-T2 on 1000 test samples, how many model parameters change? (Answer: zero—only routing weights are modified per-sample at inference time.)

- **Kernel Methods and kNN**: Why needed here: The method relies on kernel-weighted neighbor aggregation. Understanding how Gaussian kernels decay with distance and why k=5 balances signal vs. noise is essential for debugging poor retrieval. Quick check question: With Gaussian kernel K(d) = exp(-d²/2), if neighbor distances are [0.1, 0.3, 0.5, 1.0, 2.0], which neighbor dominates the weighted average?

## Architecture Onboarding

- **Component map**: Reference Set -> NV-Embed-V2 -> kNN Retrieval -> NGD Optimizer -> Router -> Experts -> Final Prediction
- **Critical path**: 1) Pre-compute embeddings and store reference samples with their correct routing weights 2) At inference, embed test question and retrieve k=5 neighbors 3) Initialize r from base router, then run 10 NGD steps minimizing surrogate loss 4) Pass optimized routing weights to MoE forward pass for final prediction
- **Design tradeoffs**: kNN vs. ε-ball: kNN provides consistent neighbor count (predictable compute), but may include dissimilar samples in sparse regions. ε-ball adapts to density but risks too few/many neighbors. NGD steps: 5 steps underperforms (76.6%), 10 steps optimal (80.7%), 20+ steps show diminishing returns with higher compute cost.
- **Failure signatures**: Stagnant accuracy after NGD: Check if retrieved neighbors are actually similar—embedding model may be failing. Try NV-Embed-V2 if using smaller embeddings. Performance degradation on specific task types: Reference set may lack that task type. Verify reference coverage includes the benchmark's task categories.
- **First 3 experiments**: 1) Sanity check: Run oracle baseline (using ground truth labels for gradient descent) on a small benchmark subset. You should see ~90% of oracle gains captured by NGD without labels. 2) Ablation on k: Test k∈{3,5,10,20} on a single benchmark (e.g., MMBench). Confirm k=5 is optimal before running full evaluation. 3) Expert transition visualization: Log top-1 expert before and after R2-T2 for 100 samples. Verify ILANG dominance decreases and transitions match task requirements (e.g., spatial tasks → IAUX).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational overhead of R2-T2 be minimized to enable real-time deployment in resource-constrained settings?
- Basis in paper: Table 4 shows that R2-T2 (kNN, NGD) requires 67.5T FLOPs per case, compared to 9.9T FLOPs for the base model, representing a nearly 7x increase in computational cost.
- Why unresolved: While the method is "efficient" relative to retraining, the multi-step gradient descent over neighbors creates a significant inference latency bottleneck not addressed by the current optimizations.
- What evidence would resolve it: A modified R2-T2 strategy or caching mechanism that achieves comparable accuracy gains while maintaining FLOPs within 2x of the base model inference cost.

### Open Question 2
- Question: Can R2-T2 be effectively applied to Large Language Model (LLM) Mixture-of-Experts architectures where experts are feed-forward networks rather than vision encoders?
- Basis in paper: The scope is limited to "multimodal MoE" models (MoAI, MoVA) where experts are distinct vision encoders or modality mixers (Section 4.1).
- Why unresolved: The "re-routing" logic relies on the assumption that experts provide distinct semantic features (e.g., text vs. spatial); it is unclear if this transfers to LLM experts which are often semi-randomly initialized or homogeneous.
- What evidence would resolve it: Experiments applying R2-T2 to standard decoder-only MoE LLMs (e.g., Mixtral or DeepSeek-MoE) to observe if similar accuracy gains occur on text-only benchmarks.

### Open Question 3
- Question: What factors contribute to the remaining 20-30% gap between R2-T2 and the Oracle performance upper bound?
- Basis in paper: The authors note in Section 4.2 that their method captures "70-80% of the potential improvement," implying a persistent performance deficit compared to the theoretical maximum.
- Why unresolved: The paper does not analyze the specific conditions or sample types where the neighborhood-based surrogate loss fails to approximate the true oracle gradient.
- What evidence would resolve it: A detailed failure analysis identifying whether the remaining gap is caused by insufficient reference neighbors, embedding space misalignment, or limitations of the local optimization landscape.

## Limitations
- The assumption that semantically similar tasks share optimal routing configurations remains unproven for diverse, real-world multimodal problems
- Bandwidth selection for Gaussian kernel is unspecified, creating a critical hyperparameter that significantly impacts retrieval quality and downstream performance
- Reference set coverage and quality directly determine R2-T2 effectiveness—if the reference lacks diversity or correct routing patterns for specific task types, performance degrades substantially

## Confidence
- High confidence in the mechanism of gradient-based routing weight optimization
- Medium confidence in the transferability assumption (neighborhood-based routing patterns work across similar tasks)
- Medium confidence in embedding model effectiveness (NV-Embed-V2 retrieval works well in practice)

## Next Checks
1. **Task boundary transfer validation**: Design an experiment where reference set contains strong routing patterns for Task A but not Task B, despite semantic similarity. Measure whether R2-T2 successfully transfers or fails appropriately.
2. **Bandwidth sensitivity analysis**: Systematically vary Gaussian kernel bandwidth σ across several orders of magnitude. Plot performance vs. bandwidth to identify optimal range and confirm robustness to this critical hyperparameter.
3. **Reference set coverage ablation**: Construct reference sets with varying levels of task type diversity (e.g., only VQA tasks vs. balanced across all 8 benchmark types). Measure how performance scales with reference coverage to quantify the method's dependency on comprehensive reference data.