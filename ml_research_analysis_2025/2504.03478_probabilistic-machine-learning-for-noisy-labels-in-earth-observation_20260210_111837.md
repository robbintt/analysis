---
ver: rpa2
title: Probabilistic Machine Learning for Noisy Labels in Earth Observation
arxiv_id: '2504.03478'
source_url: https://arxiv.org/abs/2504.03478
tags:
- uncertainty
- noise
- learning
- label
- samples
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of label noise in Earth Observation
  (EO) by leveraging probabilistic machine learning to model input-dependent noise
  and quantify uncertainty. The authors propose a probabilistic framework that extends
  standard deep learning models by placing a Gaussian distribution over the logits,
  enabling the estimation of heteroscedastic aleatoric uncertainty.
---

# Probabilistic Machine Learning for Noisy Labels in Earth Observation

## Quick Facts
- arXiv ID: 2504.03478
- Source URL: https://arxiv.org/abs/2504.03478
- Reference count: 40
- Primary result: Uncertainty-aware probabilistic models consistently outperform deterministic approaches across Earth Observation tasks by modeling heteroscedastic label noise and quantifying uncertainty.

## Executive Summary
This study addresses the critical challenge of label noise in Earth Observation (EO) by introducing a probabilistic machine learning framework that models input-dependent noise and quantifies uncertainty. The authors extend standard deep learning models by placing a Gaussian distribution over network logits, enabling the estimation of heteroscedastic aleatoric uncertainty. Applied to four diverse EO tasks—land use classification, landslide segmentation, volcanic activity detection, and wildfire danger forecasting—the approach demonstrates consistent performance improvements over deterministic baselines while providing reliable uncertainty estimates that successfully identify mislabeled or ambiguous samples.

## Method Summary
The method places a Gaussian distribution over network logits, outputting both mean and variance predictions per class. During training, noise is injected using the reparameterization trick (u = f + σ·μ where μ ~ N(0,1)), and predictions are made via tempered softmax with Monte Carlo sampling (S=1000). A tunable temperature parameter τ controls the distribution's peakiness and is optimized per dataset. The framework is evaluated across four EO tasks using ResNet-50, U-Net++, and LSTM architectures, with uncertainty measured as the variance of MC samples. The approach specifically targets aleatoric (data) uncertainty from label noise, excluding epistemic (model) uncertainty.

## Key Results
- Probabilistic models achieved higher F1 scores than deterministic baselines across most EO datasets
- Uncertainty estimates successfully identified misclassified samples, with error rates decreasing as uncertain samples were removed
- Optimal temperature values varied significantly across tasks (0.2 to 5.0), highlighting the need for dataset-specific tuning
- The framework maintained robustness to label noise while providing interpretable uncertainty quantification

## Why This Works (Mechanism)

### Mechanism 1: Heteroscedastic Noise Modeling via Logit Distribution
The network outputs mean and variance logits, creating a soft loss for high-variance (noisy) samples while maintaining strong gradients for clean samples. This works when noise varies significantly between samples, but offers no benefit for uniform noise distributions.

### Mechanism 2: Temperature Scaling for Tractable Inference
A tunable temperature parameter τ in the tempered softmax approximation optimizes the bias-variance trade-off. Different optimal values (0.2, 0.9, 2, 5) across tasks show the importance of validation-based tuning for handling specific noise characteristics.

### Mechanism 3: Uncertainty as a Label Quality Proxy
The variance of Monte Carlo samples serves as a reliable proxy for aleatoric uncertainty, successfully isolating mislabeled or ambiguous samples. The model learns to assign high variance to difficult samples rather than memorizing with high confidence.

## Foundational Learning

- **Aleatoric vs. Epistemic Uncertainty**: The method focuses only on aleatoric (data) uncertainty from label noise, not epistemic (model) uncertainty. Quick check: If training data doubles, should aleatoric uncertainty for a noisy sample decrease? (Answer: No)

- **The Reparameterization Trick**: Essential for backpropagation through sampling. Quick check: How to differentiate loss w.r.t. σ when involving random samples? (Answer: Move randomness to μ, treated as constant noise input)

- **Heteroscedasticity**: Core assumption that noise varies per sample. Quick check: In constant annotation quality datasets, would per-sample variance prediction improve results? (Answer: Likely not, may introduce instability)

## Architecture Onboarding

- **Component map**: Input -> Backbone Encoder (ResNet-50, U-Net++, LSTM) -> Probabilistic Head (Mean + Variance outputs) -> Sampling Module (Reparameterization + Tempered Softmax) -> Loss (Cross-Entropy)

- **Critical path**: 1) Implement head outputting twice channels (Classes + Variances), 2) Apply reparameterization trick with MC sampling, 3) Tune temperature τ on validation set via grid search

- **Design tradeoffs**: Inference requires 1000 forward passes (high computational cost), risk of variance collapse if learning rate/initialization off, significant operational overhead not fully addressed

- **Failure signatures**: Poor performance at default τ=1.0, flat uncertainty density plots showing no separation between correct/incorrect samples, gradient instability from exploding σ values

- **First 3 experiments**: 1) Verify probabilistic model matches deterministic baseline at τ=1, 2) Grid search τ (0.1-5.0) and plot F1 vs τ, 3) Run discard test sorting by uncertainty and plot accuracy improvement

## Open Questions the Paper Calls Out

- **Joint Epistemic Uncertainty Estimation**: Can the framework be extended to estimate both aleatoric and epistemic uncertainty? The paper explicitly notes this as a key limitation and future work direction.

- **Softmax as Uncertainty Proxy**: Do softmax probabilities serve as a sufficient proxy for data uncertainty in wildfire forecasting? The paper observes a correlation but requires quantitative validation.

- **Segmentation AUPRC Degradation**: Can the approach be refined to prevent AUPRC degradation in segmentation tasks while maintaining F1 improvements? The method reduces confidence in ambiguous regions, negatively impacting ranking-based metrics.

## Limitations

- Highly sensitive to temperature parameter tuning requiring extensive hyperparameter optimization per dataset
- Substantial computational overhead from 1000 Monte Carlo samples impacts real-time deployment feasibility
- Performance validation limited to four specific EO tasks with particular noise characteristics
- Framework designed specifically for heteroscedastic noise, unclear performance on homoscedastic distributions

## Confidence

**High Confidence**: Probabilistic framework models heteroscedastic aleatoric uncertainty (experimental results across four datasets), uncertainty correlates with correctness (discard tests), performance improvements over baselines (F1/AUPRC metrics).

**Medium Confidence**: Temperature parameter critical for performance (Table II/III showing variation), method isolates mislabeled samples (MF/DI metrics).

**Low Confidence**: Framework generalizes to arbitrary EO noise distributions (limited to four tasks), computational overhead acceptable for practical deployment (not deeply addressed).

## Next Checks

1. **Cross-Dataset Validation**: Train on one dataset and evaluate on another to test generalization of temperature parameter and uncertainty estimates.

2. **Noise Type Ablation**: Systematically vary injected label noise types (heteroscedastic vs. homoscedastic, symmetric vs. asymmetric) to quantify benefits over simpler robust methods.

3. **Inference Efficiency Analysis**: Measure latency with 1000 MC samples vs. deterministic baselines and evaluate performance-uncertainty trade-offs with reduced sample counts (S=100, S=10) for operational use.