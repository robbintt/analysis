---
ver: rpa2
title: Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?
arxiv_id: '2601.19410'
source_url: https://arxiv.org/abs/2601.19410
tags:
- translation
- context
- llms
- linguistics
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  improve automatic post-editing (APE) by leveraging document-level context. It compares
  proprietary and open-weight LLMs in a systematic evaluation of APE quality, contextual
  behavior, robustness, and efficiency.
---

# Do LLMs Truly Benefit from Longer Context in Automatic Post-Editing?

## Quick Facts
- **arXiv ID:** 2601.19410
- **Source URL:** https://arxiv.org/abs/2601.19410
- **Reference count:** 27
- **Primary result:** While proprietary LLMs achieve near-human APE quality at sentence level, they largely fail to exploit document context for contextual error correction, while open-weight models suffer from data poisoning and hallucination when given long contexts.

## Executive Summary
This study systematically evaluates whether document-level context improves automatic post-editing (APE) quality for large language models. The authors compare proprietary (GPT-4o, GPT-4o-mini) and open-weight (LLaMA3-8B, Qwen2.5-32B) models on English-to-Korean APE tasks using the WMT24++ dataset. Results show that while proprietary models achieve near-human APE quality even with simple one-shot prompting, they largely ignore document context. Open-weight models, conversely, suffer from data poisoning effects and increased hallucination when given full document context. The findings suggest that current LLM architectures and prompting approaches are insufficient for leveraging long-context information in APE tasks, despite strong baseline performance at the sentence level.

## Method Summary
The study compares APEseg (sentence-level only) against APEdoc (full document context) approaches across four LLMs on English-to-Korean translation post-editing. Using WMT24++ dataset with 7,974 segments across 59 documents, models were prompted with one-shot examples and document context placed at prompt end. Evaluation used TER, COMET, BLEU, chrF++, and human relative ranking (886 segments, 3 translators). Models tested included GPT-4o, GPT-4o-mini, LLaMA3-8B, and Qwen2.5-32B with temperature=0.2, top_p=0.9. Document length buckets were analyzed for TER differences, and hallucination detection used TER>100 threshold.

## Key Results
- Proprietary LLMs achieve near-human APE quality (APEdoc and APEseg within CD=0.19 in human evaluation) without requiring document context
- Open-weight models show increasing TER with longer documents when given context, while proprietary models show decreasing edits
- LLaMA3-8B produces TER>100 (hallucination/complete rewrites) in 50%+ of cases with document context versus <5% without
- COMET scores fail to detect hallucinations, remaining ~0.78 on severely drifted outputs

## Why This Works (Mechanism)

### Mechanism 1
Proprietary LLMs achieve near-human APE quality at sentence-level without requiring document context. Their strong instruction-following learned during alignment produces conservative, adequacy-preserving edits that act as a protective filter against unnecessary modifications. Core assumption: instruction-tuning quality correlates with edit conservativeness. Evidence: human evaluation shows APEseg and APEdoc are statistically indistinguishable (CD=0.19), and proprietary models maintain stable performance across context conditions.

### Mechanism 2
Naive document-level prompting fails to improve APE because models cannot selectively attend to relevant context within full documents. Placing full document context at prompt end creates a "data poisoning" effect—irrelevant segments compete for attention, diluting focus on target sentence. Core assumption: lack of improvement stems from attention dilution rather than inherent unimportance of document context. Evidence: open-weight models show increasing TER with longer documents while GPT-4o-mini shows decreasing edits; related work on data poisoning cited.

### Mechanism 3
Open-weight models suffer from hallucination and over-editing when given document context due to weaker instruction adherence and context discrimination. Smaller or less-aligned models exhibit "contextual drift"—borrowing fluent but irrelevant phrases from elsewhere in document. Core assumption: hallucination rates correlate with model scale and alignment quality. Evidence: LLaMA3-8B produces TER>100 in 50%+ of cases with document context; hallucinated fragments are "borrowed from elsewhere in the document but absent in corresponding source text."

## Foundational Learning

- **Concept: Automatic Post-Editing (APE) vs. Machine Translation**
  - Why needed here: APE refines existing MT output rather than generating from scratch; success measured by minimal but sufficient edits
  - Quick check question: Given "I have a cat" → "Tengo un perro" (wrong: perro=dog), should an APE system output "Tengo una gata" or rewrite entire sentence?

- **Concept: Translation Edit Rate (TER) and COMET as complementary metrics**
  - Why needed here: TER measures edit quantity (lower=better), while COMET captures semantic similarity; high TER + stable COMET indicates paraphrastic edits
  - Quick check question: If APEdoc produces TER=96.44 but COMET=0.62, what does this suggest about edit nature?

- **Concept: Data poisoning in long-context prompting**
  - Why needed here: Irrelevant context can degrade performance even with sufficient context window capacity; this is attention allocation problem
  - Quick check question: Why might placing document context at end of prompt (vs. beginning) reduce verbatim copying?

## Architecture Onboarding

- Component map: Source Document + MT Output → [Prompt Constructor] ← ICL example placement → [APEseg path] or [APEdoc path] → [LLM Post-Editor] → [Evaluation Layer] → TER, COMET, Human Ranking

- Critical path: Prompt design → context placement → model selection → output filtering for hallucinations (TER>100 detection)

- Design tradeoffs:
  - APEseg vs. APEdoc: APEseg is 60x cheaper with statistically equivalent quality for proprietary models
  - Proprietary vs. Open-weight: Proprietary offers stability; open-weight offers cost savings but requires output validation
  - Automatic vs. Human evaluation: COMET fails to detect hallucinations (scores ~0.78 on severely drifted outputs); human evaluation remains necessary

- Failure signatures:
  - TER > 100 with COMET > 0.7: Likely hallucination with borrowed document content
  - APEdoc TER substantially higher than APEseg: Context poisoning in open-weight models
  - Output identical to draft: Model ignoring task (common in overly long prompts)

- First 3 experiments:
  1. Baseline parity check: Run APEseg on 100 segments with GPT-4o-mini; verify human ranking within CD=0.19 of human PE
  2. Context poisoning test: Compare APEdoc on short (N<150) vs. long (N>500) documents with LLaMA3-8B; measure TER increase and hallucination rate
  3. Selective context ablation: Instead of full document, provide only 3 preceding + 3 following segments; compare against naive APEdoc

## Open Questions the Paper Calls Out
None

## Limitations
- Findings hinge on assumption that document-level context is relevant for APE, but lack of improvement may stem from prompting approach limitations rather than inherent irrelevance
- Human evaluation sample (886 segments) represents subset of full dataset, and relative ranking methodology may not fully capture practical impact of paraphrastic edits
- "Data poisoning" effect could potentially be mitigated through selective context retrieval or relevance-aware conditioning, which was not explored

## Confidence
- **High Confidence**: Proprietary LLMs achieving near-human APE quality at sentence level (supported by human evaluation showing APEseg and APEdoc within critical difference CD=0.19)
- **Medium Confidence**: Document context fails to improve APE due to attention dilution rather than irrelevance (supported by qualitative evidence of distraction effects, but lacks systematic exploration of alternative context presentation methods)
- **Low Confidence**: Open-weight models' performance degradation is primarily due to hallucination and over-editing when given document context (evidence shows correlation with model scale and alignment quality, but causal mechanisms need further validation)

## Next Checks
1. **Selective Context Retrieval Test**: Instead of full document context, provide only the 3 preceding and 3 following segments for each target sentence. Compare performance against both APEseg and full APEdoc approaches to determine if reduced context improves open-weight model stability while preserving any potential document-level benefits.

2. **Context Placement Variation**: Systematically vary the position of document context within prompts (beginning, middle, end, interspersed) across multiple model runs. Measure changes in edit patterns, hallucination rates, and TER scores to isolate whether end-placement is optimal or if alternative arrangements could mitigate data poisoning effects.

3. **Multi-Document Cross-Reference Analysis**: Extract cases where open-weight models produce TER>100 outputs and perform detailed analysis of whether borrowed content appears in the source document or represents pure hallucination. This would validate the hypothesis that contextual drift specifically involves document-extracted but source-unrelated phrases versus general model hallucination tendencies.