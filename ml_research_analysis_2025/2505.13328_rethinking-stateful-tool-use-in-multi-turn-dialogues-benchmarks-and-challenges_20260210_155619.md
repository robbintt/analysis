---
ver: rpa2
title: 'Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges'
arxiv_id: '2505.13328'
source_url: https://arxiv.org/abs/2505.13328
tags:
- tool
- dialogue
- arguments
- date
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating Language Models
  (LMs) as Language Agents (LAs) in multi-turn dialogues, focusing on stateful tool
  use. Existing benchmarks primarily assess stateless, single-turn interactions, overlooking
  the inherent complexities of stateful tool use across an entire lifecycle.
---

# Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges

## Quick Facts
- **arXiv ID**: 2505.13328
- **Source URL**: https://arxiv.org/abs/2505.13328
- **Reference count**: 20
- **Primary result**: Even state-of-the-art LLMs struggle with stateful tool use over long horizons in multi-turn dialogues

## Executive Summary
This paper addresses a critical gap in language model evaluation by introducing DialogTool, a benchmark specifically designed to assess Language Agents' (LAs) ability to use tools in stateful, multi-turn dialogues. Unlike existing benchmarks that focus on stateless, single-turn interactions, DialogTool evaluates the entire lifecycle of tool use including creation, utilization, and role-consistent response generation. The authors build VirtualMobile, an embodied virtual mobile environment, to simulate API calls and test the robustness of created APIs. Their comprehensive evaluation across 13 distinct open- and closed-source LLMs reveals that even the most advanced models face significant challenges when handling stateful tool interactions across multiple turns, highlighting the complexity of this problem domain.

## Method Summary
The authors propose a comprehensive framework for evaluating stateful tool use in multi-turn dialogues. They create DialogTool, a dataset covering six key tasks across three stages: tool creation, tool utilization (tool awareness, tool selection, tool execution), and role-consistent response (response generation and role play). To support this evaluation, they developed VirtualMobile, an embodied virtual mobile environment that simulates API calls and assesses the robustness of created APIs. The evaluation framework tests language models across these various stages of the tool lifecycle, measuring performance on both individual task completion and end-to-end dialogue coherence. The benchmark is designed to capture the complexities that arise when tools maintain state across multiple conversational turns, requiring models to track tool availability, understand when to use tools, and generate contextually appropriate responses.

## Key Results
- Even state-of-the-art LLMs show significant performance degradation when handling stateful tool use across multiple dialogue turns
- Models struggle particularly with tool selection and execution in long-horizon interactions
- The evaluation reveals that current language agents cannot reliably maintain tool state awareness throughout extended conversations
- Performance varies substantially across different stages of the tool lifecycle, with tool creation and response generation being relatively stronger than tool utilization phases

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its holistic approach to evaluating the complete tool lifecycle rather than isolated interactions. By requiring models to handle tool creation, selection, execution, and response generation within the same conversational context, the benchmark captures the real-world complexities that arise when tools maintain state. The VirtualMobile environment adds authenticity by simulating realistic API interactions and testing API robustness, providing a controlled yet challenging setting that exposes weaknesses in current language agents' ability to manage stateful tool use.

## Foundational Learning
- **Stateful vs Stateless Tool Use**: Understanding the difference between tools that maintain state across interactions versus one-time operations - needed to appreciate why multi-turn dialogues are more challenging than single-turn evaluations
- **Tool Lifecycle Management**: The complete process from tool creation through utilization to response generation - needed to understand the benchmark's comprehensive scope
- **Embodied Virtual Environments**: Simulated environments that replicate real-world tool interactions - needed to grasp how VirtualMobile tests API robustness
- **Multi-turn Dialogue Context**: Maintaining conversational coherence and tool awareness across multiple exchanges - needed to understand the core challenge being addressed
- **API Call Simulation**: Testing how well models can interact with and create functional APIs - needed to appreciate the practical validation aspect of the evaluation
- **Role Play Consistency**: Maintaining consistent character or agent roles while using tools - needed to understand one of the benchmark's evaluation dimensions

## Architecture Onboarding

**Component Map**: DialogTool Dataset -> VirtualMobile Environment -> LLM Evaluation Pipeline -> Performance Metrics

**Critical Path**: Tool Creation -> Tool Awareness -> Tool Selection -> Tool Execution -> Response Generation -> Role Play Consistency

**Design Tradeoffs**: The benchmark prioritizes comprehensive lifecycle coverage over task simplicity, choosing to evaluate end-to-end tool use rather than isolated capabilities. This makes the benchmark more challenging but also more representative of real-world scenarios.

**Failure Signatures**: Common failure modes include losing track of available tools across turns, selecting inappropriate tools for given tasks, generating incorrect API calls, and failing to maintain role consistency while using tools.

**First Experiments**:
1. Evaluate a simple baseline model on single-turn tool use tasks within the VirtualMobile environment
2. Test a model's ability to track tool state across exactly two conversational turns
3. Measure performance on tool creation tasks in isolation before integrating with utilization phases

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The scope of tasks within DialogTool may not fully capture the diversity and complexity of real-world multi-turn dialogues
- The reliance on a simulated environment (VirtualMobile) may not accurately reflect challenges of real-world tool interactions
- The evaluation covers only 13 open- and closed-source LLMs, potentially missing important model variations
- The benchmark focuses on mobile-specific tools, which may limit generalizability to other tool domains

## Confidence
- Claims about state-of-the-art model struggles: Medium
- Methodology validity for stateful tool evaluation: Medium
- Generalizability to real-world scenarios: Low
- Dataset comprehensiveness: Medium
- VirtualMobile environment fidelity: Medium

## Next Checks
1. Expand DialogTool dataset to include a wider variety of tasks and real-world scenarios to enhance generalizability
2. Validate findings using real-world tool interaction environments to assess robustness and applicability of results
3. Evaluate a broader range of language models, including those not currently in the study, to provide more comprehensive assessment of stateful tool use challenges across different model architectures