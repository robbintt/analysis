---
ver: rpa2
title: Look-ahead Reasoning with a Learned Model in Imperfect Information Games
arxiv_id: '2510.05048'
source_url: https://arxiv.org/abs/2510.05048
tags:
- information
- game
- games
- each
- public
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LAMIR, a method for learning abstract models
  of imperfect information games directly from gameplay experience. LAMIR combines
  a learned model of game dynamics with a clustering-based abstraction to enable tractable
  look-ahead reasoning in large games.
---

# Look-ahead Reasoning with a Learned Model in Imperfect Information Games

## Quick Facts
- arXiv ID: 2510.05048
- Source URL: https://arxiv.org/abs/2510.05048
- Reference count: 28
- Key outcome: LAMIR achieves 60-80% win rates against strong baselines in games with over 10^18 states by learning tractable abstractions for look-ahead reasoning

## Executive Summary
LAMIR introduces a method for learning abstract models of imperfect information games directly from gameplay experience. It combines a learned model of game dynamics with clustering-based abstraction to enable tractable look-ahead reasoning in large games. The learned abstraction limits subgame size to manageable information sets, making theoretically principled search methods applicable even in domains where previous approaches couldn't scale. Experiments show LAMIR learns exact underlying game structures with sufficient capacity and learns valuable abstractions that improve game playing performance even with limited capacity.

## Method Summary
LAMIR learns a latent dynamics model that enables look-ahead reasoning without explicit game rules by jointly training representation, dynamics, and legal action functions. The method uses soft clustering of information sets within public states to create tractable abstractions while preserving strategic equivalence. For depth-limited solving, LAMIR employs multi-valued states with learned transformations to approximate relevant strategic variations without explicit belief state computation. The approach trains via trajectory sampling with ε-on-policy exploration, updates policy via RNaD with V-trace correction, and uses CFR+ at test time for equilibrium computation in the abstracted game tree.

## Key Results
- In small games (II-Goofspiel 5), LAMIR learns exact underlying game structure with sufficient capacity, achieving near-zero exploitability
- In large games with over 10^18 consistent states, LAMIR achieves 60-80% win rate against strong baselines, significantly outperforming model-free policy gradient methods
- LAMIR's learned abstractions provide better strategic understanding than model-free approaches, particularly in domains with massive state spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LAMIR learns a latent dynamics model that enables look-ahead reasoning without explicit game rules by jointly training representation, dynamics, and legal action functions.
- Mechanism: The representation function Λ^I_θ encodes information sets into fixed-size latent states. The dynamics function Υ_θ unrolls forward from latent states using actual actions to predict next states, rewards, and termination. Gradients flow through a combined loss that enforces consistency between predicted and actual latent representations, rewards, and legal actions.
- Core assumption: Information set structure can be captured in fixed-dimensional latent space while preserving strategic distinctions necessary for equilibrium computation.
- Evidence anchors: [Section 3] Combined loss function over trajectory; [Section 6.1] Exact model learning with sufficient capacity; [corpus] Weak direct support, related to PSRO learning.

### Mechanism 2
- Claim: Soft clustering of information sets within public states creates tractable abstractions while preserving strategic equivalence.
- Mechanism: For each public state, the model predicts L abstract information set centers. Real information sets are assigned to nearest abstract cluster using κ (clustering property function). The loss L^A_θ pulls cluster centers toward real information sets with similar κ values.
- Core assumption: Information sets with similar κ values (legal actions, strategies, or histories) have similar optimal strategies and can be merged without catastrophic strategic loss.
- Evidence anchors: [Section 4] Hypothesis on grouping similar information sets; [Figure 2] Near-zero exploitability with L=30 in small game; [Appendix B.1] Tabular K-means validation.

### Mechanism 3
- Claim: Multi-valued states with learned transformations enable depth-limited solving without explicit belief state computation.
- Mechanism: Instead of computing values for all possible belief distributions, LAMIR learns T "transformation directions" representing characteristic strategy deviations. The value function v^χ_θ predicts outcomes for each transformation pair at leaf nodes.
- Core assumption: A small number of learned transformations can approximate the relevant strategic variations in the value function.
- Evidence anchors: [Section 5] Transformation computation from strategy differences; [Table 1] 60-80% win rate in large games; [corpus] No direct precedent for multi-valued states.

## Foundational Learning

- Concept: Counterfactual Regret Minimization (CFR)
  - Why needed here: LAMIR uses CFR+ at test time to compute strategies in the abstracted game tree.
  - Quick check question: Can you explain why CFR guarantees convergence to Nash equilibrium in perfect-recall games but not necessarily in imperfect-recall abstractions?

- Concept: Information Sets and Public States
  - Why needed here: LAMIR's core innovation is reasoning over public states rather than just the acting player's information set.
  - Quick check question: In a simplified poker game where you hold a King and the public board shows Q-7-2, what is your information set versus the public state?

- Concept: Abstraction in Extensive-Form Games
  - Why needed here: LAMIR automatically learns information-set abstractions, a topic extensively studied in poker AI.
  - Quick check question: Why might an imperfect-recall abstraction violate CFR's convergence guarantees, and what conditions restore them?

## Architecture Onboarding

- Component map:
  Trajectory Sampler → [Λ^I_θ: Info Set Encoder] → Latent States (s^i)
                        [Λ^i_θ: Public State Encoder] → Abstract Infosets (s^i_1...L)
                        [κ^θ: Clustering Property] → Clustering Features
                        ↓
                    [Υ_θ: Dynamics Function] → Next States, Rewards, Termination
                        [Γ_θ: Legal Actions] → Action Masks
                        ↓
                    [π_θ: Policy Network] → Base Strategy (trained via RNaD)
                        [τ_θ: Transformations] → Strategy Directions (T learned vectors)
                        [v_θ: Value Function] → Leaf Values (T×T payoff matrix)

- Critical path:
  1. Training: Sample trajectories with ε-on-policy exploration (ε=0.5)
  2. Update policy via RNaD (NeuRD loss, V-trace for off-policy correction)
  3. Compute strategy change δ^t, assign to nearest transformation via soft clustering
  4. Update abstraction (Λ^i_θ, κ^θ) and dynamics (Υ_θ) with combined loss
  5. Update value function v^θ using V-trace targets from abstract states
  6. Test time: Construct depth-D subgame, run CFR+ with transformation-based leaf values

- Design tradeoffs:
  - **Abstraction size L**: Larger L → better strategic fidelity but slower CFR (O(L^2|A|^(2D)) nodes). Paper uses L=20 for large games.
  - **Clustering property κ**: Action history → exact model with sufficient L; strategy → better strategic clustering but requires stable π^θ.
  - **Depth limit D**: Deeper → better lookahead but exponential node growth. Paper uses D=1 in all experiments, relying on strong value function.
  - **Transformations T**: More transformations → better value approximation but quadratic growth in value function output (T^2). Paper uses T=10.

- Failure signatures:
  - **Cluster collapse**: Multiple abstract information sets converge to same location. Fixed via repulsion loss and noise injection.
  - **Dynamics inconsistency**: Model predicts multiple next states for same abstract infoset, leaking information. Fixed via two-stage prediction.
  - **Poor extrapolation**: Strategy trained on abstracted states fails in real game if abstraction merges strategically distinct states.
  - **Training instability**: RNaD policy changes rapidly, making κ^θ unstable when using strategy features. Fixed via hard clustering threshold and regularization schedule.

- First 3 experiments:
  1. **Small-game validation (II-Goofspiel 5)**: Train with L=30, κ=action_history, verify exploitability approaches 0. Target: <0.1 mbb/g exploitability.
  2. **Ablation study**: Compare κ∈{legal_actions, strategy, action_history} with L∈{5,10,20,30} in Oshi-Zumo 3,5. Expected: action_history best with high L, strategy better with low L.
  3. **Head-to-head vs RNaD**: Train LAMIR and RNaD for 3M episodes in II-Goofspiel 13, play 100K matches. Target: >55% win rate (paper achieves ~60% with κ=legal_actions).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can stochastic chance events be effectively integrated into LAMIR's learned model and abstraction framework?
- Basis in paper: The authors state, "integrating chance nodes into our framework... is a key area for future work," noting the current limitation to games without chance.
- Why unresolved: The current dynamics network is deterministic and cannot represent the probability distributions required for chance nodes.
- What evidence would resolve it: A modification of the dynamics function and clustering mechanism that successfully models stochastic games like Poker.

### Open Question 2
- Question: Can the clustering property function κ be learned end-to-end rather than using pre-defined proxies?
- Basis in paper: The authors note, "Ideally, κ would also be learned during the training process," rather than relying on simple proxies like legal actions.
- Why unresolved: Currently, κ relies on hand-selected features which may not capture all strategic nuances necessary for optimal abstraction.
- What evidence would resolve it: Demonstration that a learned κ results in lower exploitability or higher win rates compared to hand-crafted clustering properties.

### Open Question 3
- Question: How can action abstraction be integrated with LAMIR to handle large or continuous action spaces?
- Basis in paper: The paper states LAMIR "does not inherently abstract action spaces" and that "integrating them... presents another direction for future improvement."
- Why unresolved: While information set abstraction is handled, the "sheer number of actions can remain a bottleneck" in complex games.
- What evidence would resolve it: An extension of LAMIR that jointly learns state and action abstractions, maintaining performance in high-action games.

### Open Question 4
- Question: Do Counterfactual Regret Minimization (CFR) convergence guarantees hold for LAMIR abstractions in non-A-loss recall games?
- Basis in paper: The authors note that for games like Dark Chess or Stratego, which do not satisfy A-loss recall conditions, "theoretical convergence guarantees... are not assured."
- Why unresolved: The empirical evaluation was restricted to games where the abstraction satisfies A-loss recall.
- What evidence would resolve it: Theoretical analysis or empirical verification of CFR convergence in LAMIR abstractions of non-A-loss recall games.

## Limitations
- The learned abstraction may not capture all crucial strategic nuances, particularly in complex games where similar information sets have different optimal strategies.
- LAMIR doesn't inherently abstract action spaces, which can remain a bottleneck in games with large or continuous action spaces.
- Theoretical convergence guarantees of CFR only hold for games satisfying A-loss recall, which may not cover all interesting domains like Dark Chess or Stratego.

## Confidence
- **High confidence** in the core mechanism: The combination of learned dynamics models with abstraction clustering is theoretically sound and well-supported by small-game experiments.
- **Medium confidence** in large-game performance: The 60-80% win rates against RNaD are promising, but results depend heavily on specific hyperparameter choices that may not generalize.
- **Medium confidence** in the multi-valued state approach: While novel and elegant, this mechanism hasn't been extensively validated against alternatives.

## Next Checks
1. **Abstraction fidelity test**: Systematically vary L from 5 to 50 in II-Goofspiel 10 and measure exploitability vs. win rate against RNaD to quantify the abstraction quality tradeoff.

2. **Out-of-distribution generalization**: Train LAMIR on II-Goofspiel 10, then test on II-Goofspiel 15 (same rules, larger deck) to measure performance degradation compared to RNaD.

3. **Depth sensitivity analysis**: Re-run the large-game experiments with depth=2 and depth=3 (accepting exponential runtime increase) to identify whether the strong value function compensates for shallow lookahead or if deeper search would yield significant improvements.