---
ver: rpa2
title: 'The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed
  Text'
arxiv_id: '2506.05209'
source_url: https://arxiv.org/abs/2506.05209
tags:
- license
- data
- text
- arxiv
- apache
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Common Pile v0.1, an 8TB dataset of openly
  licensed text curated from 30 diverse sources including research papers, code, books,
  government documents, and more. The authors demonstrate the dataset's utility by
  training two 7B parameter models, Comma v0.1-1T and Comma v0.1-2T, on 1 and 2 trillion
  tokens respectively.
---

# The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text

## Quick Facts
- arXiv ID: 2506.05209
- Source URL: https://arxiv.org/abs/2506.05209
- Reference count: 40
- One-line primary result: 7B-parameter models trained on 8TB of openly licensed text achieve competitive performance to models trained on unlicensed data.

## Executive Summary
The Common Pile v0.1 is an 8TB dataset of text sourced exclusively from openly licensed and public domain materials. The authors construct the dataset from 30 diverse sources including academic papers, code repositories, government documents, and books, then apply rigorous filtering for language, quality, and deduplication. Two 7B-parameter models (Comma v0.1-1T and Comma v0.1-2T) are trained on 1 and 2 trillion tokens respectively, achieving competitive performance compared to models trained on unlicensed data with similar computational budgets.

## Method Summary
The authors curated 30 openly licensed text sources into an 8TB corpus, then applied preprocessing including language identification (English-only), toxicity filtering, PII redaction, and global fuzzy deduplication (>90% 20-gram overlap). They trained small 1.7B proxy models on each source to estimate quality, then heuristically reweighted the mixture to prioritize high-performing sources. The final dataset was used to train Llama-style 7B decoder models with a two-stage training procedure (cosine learning rate schedule followed by linear cool-down on a high-quality subset).

## Key Results
- Comma v0.1-1T and Comma v0.1-2T models achieve competitive performance to Llama 1 and 2 7B on benchmarks including ARC, MMLU, and HumanEval.
- Both models outperform those trained on other openly licensed datasets (OLC, Common Corpus, KL3M) on the same benchmarks.
- The dataset demonstrates that high-quality, licensed text can approximate the utility of unlicensed web-scraped data when properly curated and deduplicated.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating diverse, openly licensed domains (code, academic, legal) can approximate the utility of unlicensed web-scraped data if rigorous deduplication and quality filtering are applied.
- **Mechanism:** The "Common Pile" aggregates 30 distinct sources (e.g., GitHub, ArXiv, PubMed) into an 8TB corpus. The pipeline filters non-English text, removes toxicity, and applies global fuzzy deduplication. This reduces noise and repetition, concentrating the "information density" of the licensed data so a model can learn efficiently from fewer unique tokens than a noisy web scrape.
- **Core assumption:** High-quality, specialized text (like textbooks and code) provides a stronger learning signal per token than general web text, compensating for the lack of "blog" or "social media" data found in Common Crawl.
- **Evidence anchors:**
  - [abstract] "Both models attain competitive performance to LLMs trained on unlicensed text... such as Llama 1 and 2 7B."
  - [section 4.1] "We apply language identification... remove documents with pervasive OCR errors... perform global document-level fuzzy deduplication."
  - [corpus] Neighbor paper "The German Commons" validates this approach for non-English languages, suggesting the method generalizes across linguistic contexts.
- **Break condition:** Performance on commonsense reasoning tasks (like HellaSwag) may lag if the licensed corpus lacks the informal, conversational data typically found in unlicensed web scrapes.

### Mechanism 2
- **Claim:** Heuristic reweighting of data sources based on small-scale proxy evaluations improves final model performance more effectively than training on raw data proportions.
- **Mechanism:** The authors train small 1.7B parameter models on 28B tokens for each individual source to estimate quality. They then up-weight high-performing sources (e.g., code, wikis) and down-weight low-performing or repetitive ones (e.g., patents) before the full 1T/2T training run. This curriculum prioritizes "learnable" data.
- **Core assumption:** The quality rankings of small models (1.7B params) predict the optimal data mixture for larger models (7B params).
- **Evidence anchors:**
  - [section 4.2] "We heuristically set mixing weights to up- and down-weight high- and low-performance sources respectively."
  - [section 4.3] "The Comma dataset-based model outperforms the models trained OLC, Common Corpus, and KL3M."
  - [corpus] Evidence regarding the optimality of this specific heuristic vs. automated methods like MixMin is limited in the broader corpus; the paper notes MixMin did not improve over heuristics.
- **Break condition:** If the small-scale proxy models fail to capture the complexity of the full training run, the resulting mixture may overfit to specific benchmark-friendly domains (e.g., MMLU) at the expense of generalization.

### Mechanism 3
- **Claim:** Fine-grained license laundering detection and source validation are mechanistic prerequisites for creating a legally compliant dataset that retains utility.
- **Mechanism:** The pipeline rejects sources with "license laundering" (incorrect metadata), excludes CC-NC/ND licenses, and manually verifies top domains. By strictly adhering to the "Open Definition," the dataset ensures that the model's weights are not contaminated by data with legal encumbrances, which allows for unrestricted release.
- **Core assumption:** A "clean" license chain exists and is verifiable for the majority of high-value internet text.
- **Evidence anchors:**
  - [section 2.1] "A common pitfall is 'license laundering,' where a copyrighted work is redistributed... with an incorrect license."
  - [section 3] "We manually curated a set of over 2,000 YouTube channels... to avoid license laundering."
  - [corpus] "Towards Best Practices for Open Datasets" highlights the fragility of this approach, noting the rapid decline of the AI data commons and increasing restrictions (Consent in Crisis), implying this mechanism requires constant maintenance.
- **Break condition:** The dataset integrity breaks if upstream providers (e.g., GitHub, YouTube) change their Terms of Service or if license metadata is systematically incorrect in a way that bypasses regex/manual checks.

## Foundational Learning

- **Concept: Next-Token Prediction (Pretraining)**
  - **Why needed here:** This is the fundamental objective function used to train the Comma models. Understanding that the model learns statistical correlations from the 8TB text corpus to predict subsequent tokens is essential to grasping why data quality and diversity matter.
  - **Quick check question:** How does the model learn world knowledge from the Common Pile without explicit labels?

- **Concept: Data Deduplication (Exact vs. Fuzzy)**
  - **Why needed here:** The paper emphasizes global fuzzy deduplication (90% 20-gram overlap) to prevent the model from memorizing repeated text (reducing overfitting) and improving efficiency.
  - **Quick check question:** Why is removing duplicate documents critical before training, rather than just letting the model see data multiple times?

- **Concept: License Compatibility (Open Definition)**
  - **Why needed here:** The paper strictly excludes CC-NC (Non-Commercial) and CC-ND (No Derivatives). One must understand that "open" in this context specifically allows for modification and commercial use, which is narrower than just "free to read."
  - **Quick check question:** Why does the Common Pile exclude a dataset like "OpenAlex" despite it being publicly accessible?

## Architecture Onboarding

- **Component map:** Collection (30 sources) -> Preprocessing (Dolma toolkit) -> Deduplication (Bloom filters) -> Mixing (Heuristic reweighting) -> Training (Lingua framework) -> Comma v0.1-1T/2T models

- **Critical path:** The Preprocessing and Mixing stage is the most fragile. Incorrect language filtering or over-aggressive toxicity removal could delete valuable niche data, while poor mixing ratios (e.g., too much code) could degrade non-code reasoning capabilities.

- **Design tradeoffs:**
  - **Breadth vs. Purity:** The authors exclude high-volume sources (like OpenAlex) due to license ambiguity, sacrificing data scale for legal certainty.
  - **Quality vs. Retention:** Aggressive filtering reduces the raw 8TB collection significantly, potentially discarding "messy" data that might aid robustness.

- **Failure signatures:**
  - **Benchmark Skew:** High performance on MMLU/Knowledge tasks but poor performance on HellaSwag/PIQA (commonsense). This indicates a lack of informal/conversational web data.
  - **Repetition Artifacts:** If training exceeds the effective token budget (e.g., the 2T model run), performance gains diminish or quality degrades due to repeated epochs on the same small pool of licensed text.

- **First 3 experiments:**
  1. **Replicate Ablation:** Train a 1.7B model on a subset of Common Pile vs. FineWeb/OSCAR to verify the "quality per token" claim before scaling up.
  2. **Mixture Sensitivity:** Ablate the heuristic mixing weights (e.g., remove code or up-weight legal text) to observe the effect on specific downstream benchmarks (Humaneval vs. LegalBench).
  3. **Contamination Check:** Scan the trained model outputs for memorized copyrighted text that might have bypassed the "license laundering" filters (membership inference).

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the performance gap between openly licensed models and those trained on unlicensed data be closed by further scaling the Common Pile?
- **Basis in paper:** [explicit] The authors note that "Qwen3 8B’s superior performance... confirms the benefit of larger training budgets and motivates future efforts on scaling up the Common Pile."
- **Why unresolved:** The current Comma models (1T–2T tokens) were compared against Qwen3 (36T tokens), leaving the "upper bound" of the open-license data regime unknown.
- **What evidence would resolve it:** Training larger models on an expanded Common Pile (e.g., >10T tokens) and comparing performance against compute-matched baselines.

### Open Question 2
- **Question:** To what extent does performance at the 2-trillion-token scale suffer from the current data repetition strategy?
- **Basis in paper:** [explicit] The authors state the Comma v0.1-2T results are "likely not a best-case" due to "excessive repetition" (up to 16 passes for some sources) caused by simply repeating the 1T mixture.
- **Why unresolved:** It is unclear if the model's diminishing returns on certain benchmarks are due to data quality limits or the suboptimal repetition of the specific 1T mixture.
- **What evidence would resolve it:** Training a 2T model using a unique, curated 2T-token mixture rather than repeating the 1T mixture, and comparing downstream performance.

### Open Question 3
- **Question:** Is the observed underperformance on commonsense reasoning tasks an inherent limitation of openly licensed data domains?
- **Basis in paper:** [inferred] The paper reports "significantly worse performance" on HellaSwag and PIQA, noting these tasks rely on domains like personal blogs and hobbies that are "poorly represented in the Common Pile."
- **Why unresolved:** It is not established whether high-quality, openly licensed sources for these specific informal domains exist or if the license constraint fundamentally caps performance on such tasks.
- **What evidence would resolve it:** Identifying and integrating new openly licensed sources of informal/hobbyist text, followed by re-evaluation on HellaSwag and PIQA.

## Limitations

- **License Chain Integrity:** The verification process relies heavily on manual checks of top domains and regex patterns, creating uncertainty about whether systematic or novel license laundering attempts could evade these filters.
- **Data Mixture Generalization:** The heuristic mixing weights derived from 1.7B proxy models may not perfectly transfer to 7B models, and the paper does not compare against automated methods like MixMin beyond noting it didn't improve over heuristics.
- **Benchmark Representativeness:** The models show competitive performance on academic benchmarks but the paper notes potential weaknesses on commonsense reasoning (HellaSwag), suggesting the licensed corpus may lack informal web data.

## Confidence

- **High Confidence:** The dataset construction pipeline (language filtering, deduplication, license screening) is technically detailed and reproducible. The claim that Comma v0.1-1T/2T achieve competitive benchmark performance vs Llama 2-7B is well-supported by the presented results.
- **Medium Confidence:** The assertion that open licensing is a scalable alternative to unlicensed data for LLM pretraining is supported by this single dataset release but requires broader replication across languages and domains to be fully validated.
- **Low Confidence:** The long-term sustainability of the "open data commons" is questioned by the neighbor paper "Towards Best Practices for Open Datasets," which notes increasing restrictions on data access.

## Next Checks

1. **License Contamination Audit:** Perform membership inference testing on the trained models to detect potential memorization of copyrighted text that may have bypassed license filters, particularly focusing on code repositories and academic papers where license metadata is complex.
2. **Cross-Domain Generalization Test:** Evaluate the Comma models on real-world, non-benchmark tasks (e.g., open-domain question answering on current events, code generation for unseen APIs) to verify that benchmark performance translates to practical utility beyond academic metrics.
3. **Alternative Mixing Method Comparison:** Re-train the 7B model using automated data mixture methods (e.g., MixMin or recent learned-mixture approaches) to determine if the heuristic mixing weights are truly optimal or if data-driven approaches could yield further performance gains.