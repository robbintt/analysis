---
ver: rpa2
title: 'CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity
  and Low-Precision Networks'
arxiv_id: '2512.12981'
source_url: https://arxiv.org/abs/2512.12981
tags:
- pruning
- codeq
- quantization
- dead-zone
- sparsity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoDeQ, a fully differentiable method for
  joint pruning-quantization in neural networks. The core idea leverages the observation
  that the dead-zone of a scalar quantizer naturally implements magnitude pruning.
---

# CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks

## Quick Facts
- **arXiv ID**: 2512.12981
- **Source URL**: https://arxiv.org/abs/2512.12981
- **Reference count**: 23
- **Primary result**: Achieves ~91.8% ImageNet accuracy with ResNet-18 at ~5% of full precision BOPs through joint pruning-quantization

## Executive Summary
CoDeQ introduces a fully differentiable method for joint model pruning and quantization by leveraging the observation that the dead-zone of a scalar quantizer naturally implements magnitude pruning. The method parameterizes and learns the dead-zone width via backpropagation alongside quantization parameters, enabling simultaneous determination of sparsity patterns and quantized weights in a single end-to-end optimization. By eliminating auxiliary procedures and using only a single global hyperparameter for sparsity control, CoDeQ achieves highly sparse, low-precision models while maintaining accuracy close to full precision across both fixed and mixed-precision regimes.

## Method Summary
CoDeQ implements joint pruning-quantization through a dead-zone quantizer that replaces standard quantization in the forward pass. The dead-zone width d is parameterized as d(θ_dz) = 2·max(|w|)·(1 - tanh(|θ_dz|)) and learned via gradient descent, with L2 regularization on θ_dz expanding the dead-zone to induce sparsity. A pruning-aware scale factor s̃ = (max(|w|) - d/2) / (Q_b - 1/2) preserves quantization precision for non-zero weights. The method is architecture-agnostic, requires minimal hyperparameters, and uses Straight-Through Estimator to handle non-differentiable operations.

## Key Results
- Achieves ~91.8% ImageNet accuracy with ResNet-18 at ~5% of full precision BOPs
- Maintains ~94% CIFAR-10 accuracy with ResNet-20 at ~3% relative BOPs
- Eliminates need for separate pruning and quantization procedures while matching or exceeding state-of-the-art accuracy-compression trade-offs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The dead-zone of a scalar quantizer is mathematically equivalent to magnitude pruning.
- **Mechanism**: In uniform symmetric quantization, the zero bin maps inputs within [-d/2, d/2] to zero, identical to magnitude pruning with threshold τ = d/2. By learning d, CoDeQ directly controls sparsity through quantization.
- **Core assumption**: Magnitude correlates with weight importance (common heuristic).
- **Evidence anchors**: [abstract] formal equivalence claim; [section 4.1] Eq. (9) showing mathematical equivalence; [corpus] QST uses tied threshold-step size.
- **Break condition**: If magnitude doesn't correlate with importance, useful weights get pruned and accuracy degrades sharply.

### Mechanism 2
- **Claim**: Regularizing learnable dead-zone parameter produces sparsity through gradient descent.
- **Mechanism**: θ_dz is regularized via L2 loss, pushing |θ_dz| → 0, which expands d via (1 - tanh) mapping. Larger d → more weights in dead-zone → more sparsity. All gradients flow through STE.
- **Core assumption**: STE provides accurate gradient signals despite being a biased estimator.
- **Evidence anchors**: [section 5.2] explicit d(θ_dz) formula; [section 5.3] L_CoDeQ includes λ_dz||θ_dz||²_2; [section 6.3 ablation] shows λ_dz directly controls sparsity.
- **Break condition**: If λ_dz too high, over-pruning occurs; if too low, sparsity never emerges.

### Mechanism 3
- **Claim**: Pruning-aware scale factor preserves quantization precision for non-zero weights.
- **Mechanism**: Standard absmax scale wastes quantization levels on dead-zone region. Pruning-aware scale subtracts dead-zone from dynamic range before computing step size, ensuring all 2Q_b non-zero levels are used for remaining weights.
- **Core assumption**: Scale factor can be computed per-layer without gradient tracking.
- **Evidence anchors**: [section 4.2] derivation in Eq. (12-13); [figure 2] visual comparison; [corpus] no direct comparison.
- **Break condition**: If entire layer pruned, division by zero occurs; paper adds ε = 10⁻⁸ safeguard.

## Foundational Learning

- **Concept: Straight-Through Estimator (STE)**
  - **Why needed**: Quantizer contains rounding, ReLU, clipping—non-differentiable or zero-gradient almost everywhere. STE bypasses this by replacing ∂F/∂w with identity in backward pass.
  - **Quick check**: Can you explain why applying STE to ReLU (vs. letting zero-gradient propagate) preserves learning signal for weights currently in dead-zone?

- **Concept: Quantization-Aware Training (QAT)**
  - **Why needed**: CoDeQ is a QAT method—forward pass uses quantized weights, simulating low-precision inference during training. Essential distinction from post-training quantization.
  - **Quick check**: In QAT, why must we simulate quantization in forward pass rather than applying only at deployment?

- **Concept: L2 Regularization as Sparsity Induction**
  - **Why needed**: CoDeQ uses ||θ_dz||²_2 to expand dead-zones. This is weight decay on dead-zone parameter, but effect is inverted (smaller θ_dz → larger d).
  - **Quick check**: Why does regularizing θ_dz toward zero increase sparsity, while regularizing weights toward zero also increases sparsity but through different path?

## Architecture Onboarding

- **Component map**: Weights -> Dead-zone quantizer (with STE) -> θ_dz (learnable dead-zone) -> θ_bit (optional, mixed-precision) -> Scale factor s̃ (computed) -> Regularization terms
- **Critical path**:
  1. Initialize θ_dz ≈ 3 (minimal dead-zone)
  2. Forward: Compute d from θ_dz, compute s̃, apply dead-zone quantizer
  3. Backward: STE gradients flow to w and θ_dz
  4. Update: θ_dz shrinks via L2 regularization → d expands → sparsity increases
  5. Convergence: θ_dz stabilizes at layer-specific values
- **Design tradeoffs**:
  - Fixed-bit vs. mixed-precision: MP requires tuning λ_bit, uses non-power-of-two widths; fixed-bit (e.g., 4-bit) is hardware-friendly and performs nearly as well
  - Layer-wise vs. finer granularity: Paper uses layer-wise; channel/block-wise could improve accuracy but adds complexity
  - Unstructured vs. structured sparsity: CoDeQ produces unstructured—harder to accelerate than structured
- **Failure signatures**:
  - Over-pruning: Accuracy collapses (>5% drop) → λ_dz too high; reduce and re-run
  - No sparsity: Final sparsity <10% → λ_dz too low or θ_dz learning rate too small
  - Layer fully pruned: Division-by-zero warnings → add ε safeguard or cap d < 2·max(|w|)
  - Gradient explosion: Check STE implementation; ensure indicator function not applied to dead-zone quantizer
- **First 3 experiments**:
  1. **Sanity check on CIFAR-10/ResNet-20**: Use paper hyperparameters (λ_dz=0.01, λ_bit=0.01 for MP). Verify ~91.8% accuracy with ~2.6-3.0% relative BOPs. Compare fixed 4-bit vs. MP.
  2. **Ablation on λ_dz**: Train same model with λ_dz ∈ {0.001, 0.005, 0.01, 0.02, 0.05}. Plot final sparsity vs. accuracy. Identify knee point for target compression.
  3. **Architecture transfer test**: Apply to TinyViT on CIFAR-10 with identical hyperparameters. If sparsity patterns differ from CNNs, validates architecture-agnostic claims; if accuracy degrades substantially, may need architecture-specific tuning.

## Open Questions the Paper Calls Out
- **Can CoDeQ be extended to structured sparsity patterns** (e.g., channel-wise, block-wise) to improve hardware exploitability? Current formulation produces unstructured sparsity, which is harder to exploit efficiently on existing hardware than structured pruning.
- **Would finer quantization granularity** (channel- or block-wise) improve accuracy-compression trade-offs? Current implementation uses layer-wise quantization; finer granularities could further improve accuracy.
- **Can learned step-size quantization be integrated** with dead-zone quantizer to replace absmax scale factor? Current method relies on classical absmax scale rather than learning scales.
- **How does interaction between dead-zone regularization and standard weight decay** affect optimal sparsity-accuracy trade-offs? Both mechanisms can induce sparsity via different pathways, but combined effect on final model quality is not characterized.

## Limitations
- Relies on magnitude as proxy for weight importance, which lacks rigorous theoretical justification and may fail for certain architectures where magnitude doesn't correlate with functional importance
- Produces unstructured sparsity, challenging to accelerate on current hardware compared to structured alternatives
- Assumes adequate gradient flow through STE approximations, but long-term impact of biased gradient estimates on convergence remains understudied

## Confidence
- **High confidence**: Mathematical equivalence between dead-zone quantization and magnitude pruning is rigorously established; experimental results demonstrating significant BOP reduction while maintaining accuracy are reproducible and internally consistent
- **Medium confidence**: Regularization-based sparsity induction mechanism is well-founded but depends on STE gradient accuracy with inherent approximation errors; pruning-aware scale factor design is theoretically sound but lacks direct empirical validation
- **Medium confidence**: Claims of architecture-agnostic performance require further validation beyond two architectures tested, particularly for transformers and vision transformers

## Next Checks
1. **Importance correlation study**: Systematically evaluate correlation between magnitude and weight importance across different architectures using ablation studies with different pruning criteria
2. **STE gradient accuracy analysis**: Compare training dynamics and final accuracy between CoDeQ and exact gradient variant for non-dead-zone weights to quantify STE approximation impact
3. **Hardware acceleration evaluation**: Measure actual inference speedup and energy consumption on hardware supporting unstructured sparsity to validate BOP reduction translates to practical performance gains