---
ver: rpa2
title: 'LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with
  Spectral Adversarial Perturbations'
arxiv_id: '2506.13344'
source_url: https://arxiv.org/abs/2506.13344
tags:
- data
- graph
- lapddpm
- scrna-seq
- spectral
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LapDDPM, a conditional generative model for
  single-cell RNA sequencing (scRNA-seq) data that combines graph-based representations
  with a score-based diffusion model. The model constructs a k-NN graph from gene
  expression data, enriches it with Laplacian Positional Encodings, and employs a
  novel spectral adversarial perturbation mechanism on graph edge weights to enhance
  robustness.
---

# LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation with Spectral Adversarial Perturbations

## Quick Facts
- arXiv ID: 2506.13344
- Source URL: https://arxiv.org/abs/2506.13344
- Authors: Lorenzo Bini; Stephane Marchand-Maillet
- Reference count: 22
- Key outcome: LapDDPM achieves lower RBF-kernel MMD and 2-Wasserstein distances than baselines (e.g., 0.41±0.15 vs 0.94±0.05 MMD for scVI on PBMC3K), demonstrating high fidelity in capturing scRNA-seq distributions.

## Executive Summary
LapDDPM introduces a conditional generative model for single-cell RNA sequencing (scRNA-seq) data that leverages graph-based representations and a score-based diffusion model. By constructing a k-NN graph from gene expression data and enriching it with Laplacian Positional Encodings (LPEs), the model captures structural cellular relationships. A novel spectral adversarial perturbation mechanism on graph edge weights enhances robustness, while a conditional score-based diffusion model learns the complex distribution in a learned latent space. Experimental results on multiple scRNA-seq datasets demonstrate superior performance compared to baselines, indicating high fidelity in capturing data distributions and enabling cell-type-specific generation.

## Method Summary
LapDDPM preprocesses scRNA-seq count matrices through gene filtering, normalization, and PCA. It constructs a k-NN graph on PCA features and computes LPEs from the normalized Laplacian. A Chebyshev GNN encoder maps gene expression augmented with LPEs (and optionally perturbed edge weights) to a latent space. A conditional score-based diffusion model (ScoreNet) learns to denoise latent vectors, conditioned on cell type labels. The decoded latent vectors are reconstructed as gene counts using a Poisson distribution. Training involves a weighted sum of diffusion loss, KL divergence, and Poisson NLL reconstruction, with spectral adversarial perturbations applied to graph edge weights for robustness.

## Key Results
- LapDDPM outperforms baselines (scVI, scGen, SCORT, Dyngen) on PBMC3K with MMD of 0.41±0.15 and 2-Wasserstein distance of 0.41±0.08.
- On Dentate Gyrus dataset, LapDDPM achieves MMD of 0.24±0.03 and 2-Wasserstein distance of 0.30±0.02.
- Ablation studies show LPEs and spectral adversarial perturbations significantly improve performance (e.g., MMD: 0.60→0.41 without LPEs on PBMC3K).

## Why This Works (Mechanism)

### Mechanism 1: Graph-Augmented Latent Representation via Spectral Positional Encodings
LPEs from the normalized graph Laplacian provide the GNN encoder with explicit structural information about cellular relationships in the k-NN graph. This allows the model to distinguish cells based on their structural roles, mapping gene expression features into a more informative latent space. Ablation studies show performance degradation when LPEs are removed.

### Mechanism 2: Conditional Generation via Classifier-Free Guidance in Latent Space
The encoder reduces dimensionality and sparsity, mapping data to a latent space amenable to diffusion modeling. A conditional score-based diffusion model learns the conditional distribution p(z|c) via cell type labels, enabling guided sampling. The Poisson NLL loss effectively reconstructs count data from decoded log-rates.

### Mechanism 3: Robustness Enhancement via Spectral Adversarial Perturbations on Graph Structure
The LaplacianPerturb module modifies edge weights using perturbations proportional to the product of principal eigenvector components (v_i v_j). This targets the graph's dominant spectral modes, forcing the encoder to learn robust representations invariant to structural noise and enhancing stability against real-world noise in cellular networks.

## Foundational Learning

- **Single-Cell RNA Sequencing (scRNA-seq) Data Characteristics**
  - Why needed: LapDDPM is designed for high-dimensional, sparse, and non-Gaussian scRNA-seq data. Understanding these properties is crucial to see why a graph-based latent diffusion approach is proposed.
  - Quick check: How does the model address the issue of data sparsity during the reconstruction phase?

- **Score-Based Diffusion Models (SDEs)**
  - Why needed: The core generative engine is a conditional score-based diffusion model. Understanding the forward and reverse processes defined by SDEs is essential for implementing and tuning the ScoreNet.
  - Quick check: In the reverse denoising process, what does the ScoreNet predict?

- **Spectral Graph Theory & Laplacian Positional Encodings**
  - Why needed: The model's innovations rely on spectral graph methods. Understanding the graph Laplacian, its eigenvectors, and how they serve as positional encodings is necessary to grasp the encoder's design.
  - Quick check: From which matrix are the Laplacian Positional Encodings (LPEs) derived?

## Architecture Onboarding

- **Component map:** Preprocessed counts -> PCA -> k-NN graph + LPEs -> Chebyshev GNN encoder -> Latent space z -> ScoreNet (conditional diffusion) -> Sampled z_0 -> Feature decoder -> Poisson distribution for counts

- **Critical path:**
  1. Data Ingestion: Preprocess scRNA-seq matrix.
  2. Graph & LPEs: Build k-NN graph, compute LPEs.
  3. Perturbation: Generate adversarial edge weights.
  4. Encoding: GNN maps (genes, LPEs, perturbed graph) to latent z.
  5. Diffusion Training: Forward diffuse z to z_t, train ScoreNet to predict noise.
  6. Generation: Sample noise z_T -> Reverse diffuse conditioned on label c -> z_0 -> Decode to synthetic counts.

- **Design tradeoffs:**
  - Latent vs. Pixel-Space Diffusion: Operating in latent space reduces computational cost but relies on a well-trained encoder/decoder.
  - Perturbation Granularity: Perturbing edge weights provides nuanced structural attack versus coarser node feature perturbations but is more complex to implement.
  - Graph Construction: Using k-NN graph makes the model scalable but discards potential long-range relationships.

- **Failure signatures:**
  - Training Divergence: KL weight too low or perturbation strength too high. Latent space may become unstructured.
  - Poor Reconstruction: Decoder's Poisson NLL loss plateaus high. Latent space may not be informative enough.
  - Mode Collapse: Conditional dropout rate is too high or too low, or ScoreNet fails to learn meaningful scores.
  - OOM: k-NN graph is too dense or latent dimension too high for the GPU.

- **First 3 experiments:**
  1. Train LapDDPM on PBMC3K without LaplacianPerturb module. Compare MMD and 2-Wasserstein distance to full model.
  2. Vary perturbation strength epsilon on PBMC3K. Plot MMD vs. epsilon to find optimal balance.
  3. Generate samples with and without cell type labels. Compute per-type MMD for conditional case to verify conditioning effectiveness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LapDDPM be extended to generate multi-modal single-cell data, such as paired gene expression and chromatin accessibility, while maintaining high fidelity?
- Basis: Conclusion states extending to multi-modal data would significantly broaden utility.
- Why unresolved: Current architecture is designed for uni-modal generation and would require modifications for distinct but correlated data modalities.
- Evidence: A modified LapDDPM model successfully trained on a multi-modal dataset (e.g., 10x Multiome) that generates realistic paired profiles, evaluated by correlation metrics between modalities.

### Open Question 2
- Question: Can the conditioning mechanism be adapted to handle continuous variables (e.g., cell states, time, or dosage) rather than discrete labels to enable more sophisticated in-silico perturbation studies?
- Basis: Conclusion suggests investigating more granular or continuous conditioning mechanisms.
- Why unresolved: Current model uses embedding layer for discrete labels, which does not inherently support continuous interpolation or regression-based conditioning.
- Evidence: Demonstration of the model generating a smooth trajectory of gene expression changes in response to a continuous input variable (e.g., increasing drug concentration).

### Open Question 3
- Question: What specific distributed training paradigms or sub-sampling strategies are required to maintain LapDDPM's linear complexity and performance on datasets significantly larger than the Human Lung Cell Atlas (~0.5M cells)?
- Basis: Conclusion notes exploring optimizations for extremely large-scale datasets, potentially involving sub-sampling or distributed training.
- Why unresolved: Practical memory overhead limits application to extremely large-scale datasets without specific engineering solutions.
- Evidence: A scalability analysis showing training time and memory usage on a dataset with >1M cells, comparing standard versus distributed/sub-sampled implementations.

### Open Question 4
- Question: Is robustness training based solely on perturbations aligned with the graph's principal eigenvector sufficient to defend against adversarial attacks targeting higher-order spectral components or local graph topology?
- Basis: Methodology explicitly restricts spectral adversarial perturbation to the principal eigenvector, leaving robustness to variations in non-dominant spectral modes unverified.
- Why unresolved: Focusing on dominant spectral mode may create specific robustness profile that fails to generalize to structural noise or attacks altering local connectivity.
- Evidence: An ablation study or robustness test where the model is attacked using perturbations derived from 2nd through k-th eigenvectors, comparing performance degradation against current principal-vector defense.

## Limitations
- Spectral adversarial perturbation mechanism lacks direct experimental validation beyond ablation studies and may be dataset-specific.
- Model relies on k-NN graph construction assuming biologically meaningful neighborhood structures, which may not hold for all cell types.
- Scalability to extremely large datasets (>1 million cells) is not demonstrated despite theoretical linear complexity.

## Confidence
- Graph-augmented latent representation performance: **Medium** (strong ablation evidence but no comparison to alternative graph augmentations)
- Conditional generation fidelity: **High** (superior quantitative metrics across multiple datasets)
- Spectral adversarial perturbation robustness gains: **Low-Medium** (mechanism described clearly but limited ablation context)

## Next Checks
1. Test LapDDPM on a larger, more heterogeneous dataset (e.g., >500k cells) to evaluate scalability and performance consistency.
2. Perform a systematic sensitivity analysis of the spectral perturbation hyperparameters (epsilon, power iterations) to identify optimal ranges and failure points.
3. Compare LapDDPM against a baseline that uses the same latent diffusion framework but with a different graph augmentation strategy (e.g., GAT-based features instead of LPEs) to isolate the benefit of the specific spectral approach.