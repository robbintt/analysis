---
ver: rpa2
title: An Agentic AI System for Multi-Framework Communication Coding
arxiv_id: '2512.08659'
source_url: https://arxiv.org/abs/2512.08659
tags:
- agent
- annotation
- patient
- page
- mosaic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "MOSAIC is an agentic AI system for multi-framework clinical communication\
  \ coding that uses LangGraph to coordinate four specialized agents\u2014Plan, Update,\
  \ Annotation, and Verification\u2014for scalable, reliable annotation of patient-provider\
  \ interactions. The system employs codebook-guided retrieval-augmented generation\
  \ (RAG), dynamic few-shot prompting, and continuous learning from human adjudication\
  \ feedback."
---

# An Agentic AI System for Multi-Framework Communication Coding

## Quick Facts
- **arXiv ID:** 2512.08659
- **Source URL:** https://arxiv.org/abs/2512.08659
- **Reference count:** 0
- **Primary result:** MOSAIC achieves 92.8% F1 on multi-framework clinical communication coding

## Executive Summary
MOSAIC is an agentic AI system for multi-framework clinical communication coding that uses LangGraph to coordinate four specialized agents—Plan, Update, Annotation, and Verification—for scalable, reliable annotation of patient-provider interactions. The system employs codebook-guided retrieval-augmented generation (RAG), dynamic few-shot prompting, and continuous learning from human adjudication feedback. Evaluated on 50 transcripts spanning rheumatology and OB/GYN domains, MOSAIC achieved an overall F1 score of 92.8%, with highest performance in rheumatology (F1=96.2%) and strongest results for Patient Behavior annotations (F1=95.1%). Ablation studies showed that the full system outperforms single-agent baselines by 7.9% in F1. The framework enables efficient, reproducible, and transparent coding of clinical conversations at scale.

## Method Summary
MOSAIC is a LangGraph-based multi-agent system that orchestrates four specialized agents for clinical communication coding: Plan (routing), Update (codebook versioning), Annotation (labeling via specialized sub-agents), and Verification (consistency checks). The system uses codebook-guided RAG with MiniLM embeddings, FAISS vector store, MMR retrieval, and MedCPT reranking to ground annotations in explicit coding rules. Dynamic few-shot prompting from an adjudication-informed example library enables continuous improvement without model retraining. The system was evaluated on 50 transcripts from rheumatology and OB/GYN domains using GPT-4o with temperature 0.3.

## Key Results
- Overall F1 score of 92.8% across five codebooks (WISER, Global, Intervention/5As, Patient Behavior, Bias)
- Highest performance in rheumatology domain (F1=96.2%) and Patient Behavior annotations (F1=95.1%)
- Full system outperforms single-agent baselines by 7.9% in F1 (93.0% vs. 85.9%)
- Dynamic few-shot prompting improves performance over multi-agent without optimization (89.5% vs. 85.7% F1)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-agent orchestration with specialized roles improves annotation quality over monolithic single-pass approaches.
- **Mechanism:** LangGraph coordinates four agents—Plan (routing), Update (codebook versioning), Annotation (labeling via specialized sub-agents), and Verification (consistency checks)—with conditional branching and state management. The Plan Agent dispatches to framework-specific sub-agents (WISER, Bias, Patient Behavior, etc.), reducing cognitive load per agent and enabling parallel processing of overlapping frameworks.
- **Core assumption:** Specialization reduces error propagation; explicit verification catches inconsistencies that single-pass models miss.
- **Evidence anchors:** Ablation shows single-agent baseline F1=85.9% vs. full MOSAIC F1=93.0%; multi-agent without optimization performed at 85.7%.

### Mechanism 2
- **Claim:** Codebook-guided RAG with domain-specific reranking grounds annotations in explicit coding rules, reducing hallucination.
- **Mechanism:** Codebooks are chunked via sliding window, embedded with MiniLM-L6-v2, stored in FAISS, retrieved via MMR for diversity, reranked with MedCPT (trained on PubMed logs), then filtered to retain only chunks with valid annotation tags. This ensures prompts contain rule definitions rather than generic context.
- **Core assumption:** Biomedical reranking models improve retrieval relevance for clinical text over generic embeddings.
- **Evidence anchors:** RAG aligns transcripts with codebooks... reducing hallucinations and enforcing label alignment.

### Mechanism 3
- **Claim:** Dynamic few-shot prompting from an adjudication-informed example library enables continuous improvement without model retraining.
- **Mechanism:** The Verification Agent compares predictions to gold labels, identifies mismatches, and stores sentence-context-label triplets in the Example Library. Correct matches and contrastive errors are re-embedded; retrieval is precision-weighted to reduce overannotation. The feedback loop updates prompts, promotes high-value examples, and prunes ineffective ones.
- **Core assumption:** Example quality correlates with downstream F1; contrastive examples improve boundary decisions.
- **Evidence anchors:** Dynamic prompting configuration achieved F1=89.5% vs. multi-agent without optimization at 85.7%.

## Foundational Learning

- **Concept: LangGraph state machines**
  - **Why needed here:** Understanding how nodes (agents), edges (routing), and state (transcript, codebooks, flags) interact is prerequisite to debugging workflow failures.
  - **Quick check question:** If the "codebook update" flag is False, which nodes does a transcript skip?

- **Concept: RAG retrieval metrics (MMR, reranking)**
  - **Why needed here:** The annotation quality depends on retrieval relevance; tuning MMR's λ parameter and reranker thresholds affects which codebook rules surface.
  - **Quick check question:** Why does MMR alone risk returning redundant chunks, and how does MedCPT address this?

- **Concept: Precision-recall tradeoffs in imbalanced labeling**
  - **Why needed here:** Dataset has ~95% "None" labels; weighted F1 can mask poor performance on rare codes (e.g., Attentive:4 F1=0 per eTable 5).
  - **Quick check question:** If a code appears in 2 of 15,000 turns, what happens to per-label F1 if the model never predicts it?

## Architecture Onboarding

- **Component map:** Preprocessing: ADS → transcript segmentation → fixed-length batching → Agentic Core: Plan Agent → [Update Agent] → Annotation Sub-Agents → Verification Agent → Retrieval: Codebook RAG → Example Library RAG → Interface: Gradio UI

- **Critical path:** Plan Agent routing → Annotation Agent inference → Verification Agent flagging. Latency accumulates at LLM calls per sub-agent; parallelization bounded by API rate limits.

- **Design tradeoffs:**
  - Temperature 0.0: deterministic but rigid (F1=90.2%) vs. 0.3: nuanced but may omit content (F1=92.8%)
  - Single generic prompt vs. task-specific templates: latter improves F1 but increases maintenance
  - Text-only input: fast but misses prosodic cues critical for Global and Bias codes (F1=83.1%, 94.8% respectively)

- **Failure signatures:**
  - Plan Agent misroutes on ambiguous prompts ("tone," "sentiment") → wrong sub-agent or no routing
  - Verification flags low-confidence spans but no human review → errors persist in Example Library
  - Chunking splits conversational boundaries → semantic coherence lost, recall drops

- **First 3 experiments:**
  1. **Ablation by agent:** Disable Verification Agent; measure F1 delta on held-out transcripts to quantify quality gate contribution.
  2. **Retrieval depth sweep:** Vary top-k (3, 5, 10, 20) in Codebook RAG; plot F1 vs. latency to find optimal retrieval budget.
  3. **Temperature calibration by codebook:** Run temperature grid (0.0, 0.3, 0.5, 0.7) separately for WISER vs. Global; test if structured codes benefit from lower temperature than relational codes.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does integrating multimodal inputs (e.g., tone, rhythm) significantly improve annotation performance for text-dependent codebooks like Global and Bias?
- **Basis in paper:** "Future iterations will integrate audio features and human-in-the-loop verification to better capture empathy, communication flow, and bias-related behaviors."
- **Why unresolved:** The current text-only system underperforms on codes relying on paralinguistic features (e.g., Attentive vs. Concerned), which are listed as a primary limitation.
- **What evidence would resolve it:** A comparative evaluation of MOSAIC's F1 scores on Global and Bias codebooks using text-only versus audio-augmented inputs.

### Open Question 2
- **Question:** Can reinforcement learning (RL) techniques enhance multi-agent coordination and prompt adaptation better than the current static orchestration?
- **Basis in paper:** "We will... explore reinforcement learning (e.g., joint training, reward shaping) to enhance agent coordination and prompt adaptation."
- **Why unresolved:** The current system uses fixed LangGraph orchestration and heuristic feedback loops; it is unknown if dynamic RL-based routing would yield higher efficiency or accuracy.
- **What evidence would resolve it:** Benchmarking experiments comparing the current Plan Agent's routing efficiency and final annotation accuracy against an RL-optimized agent framework.

### Open Question 3
- **Question:** Does MOSAIC generalize effectively to clinical specialties beyond rheumatology and OB/GYN, specifically for underrepresented codes like SDOH and weight?
- **Basis in paper:** "We aim to generalize MOSAIC to additional specialties, languages, and underrepresented codes (e.g., SDOH, weight)."
- **Why unresolved:** The current evaluation was limited to two specific domains, and codebooks like SDOH lacked sufficient gold-standard labels for testing.
- **What evidence would resolve it:** Reporting F1 scores and reliability metrics for MOSAIC when applied to new clinical domains (e.g., oncology, primary care) and previously untested codebooks.

## Limitations

- Limited generalizability due to evaluation on only 50 transcripts from two clinical domains (rheumatology and OB/GYN)
- Missing methodological details including prompt templates and Example Library construction methodology
- Severe class imbalance with ~95% "None" labels potentially masking poor performance on rare codes
- Text-only input misses prosodic cues critical for certain codebooks (Global, Bias)

## Confidence

- **High Confidence:** Multi-agent architecture with specialized roles and explicit verification improves performance over single-agent baselines (F1 93.0% vs. 85.9%)
- **Medium Confidence:** Dynamic few-shot prompting from adjudication-informed example library contributes to performance gains (F1 89.5% vs. 85.7%)
- **Medium Confidence:** Codebook-guided RAG with domain-specific reranking reduces hallucination, but MedCPT impact not empirically validated

## Next Checks

1. **Ablation of Verification Agent:** Disable the Verification Agent and rerun annotation on held-out transcripts to quantify the quality gate's contribution to overall F1.

2. **Per-Class Performance Audit:** Compute and report per-class precision, recall, and F1 scores, with special attention to rare codes (e.g., Attentive:4, Bias codes). Visualize confusion matrices to identify systematic misclassifications.

3. **Prompt Template Transparency:** Reconstruct and publicly release the full prompt templates used for each sub-agent. Validate that prompts accurately encode codebook rules by having independent annotators map prompts to codebook excerpts.