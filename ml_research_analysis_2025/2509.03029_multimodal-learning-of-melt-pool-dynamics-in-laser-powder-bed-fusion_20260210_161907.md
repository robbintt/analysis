---
ver: rpa2
title: Multimodal learning of melt pool dynamics in laser powder bed fusion
arxiv_id: '2509.03029'
source_url: https://arxiv.org/abs/2509.03029
tags:
- pool
- melt
- data
- absorptivity
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of predicting melt pool dynamics
  in laser powder bed fusion (LPBF) using a cost-effective approach. The authors propose
  a multimodal learning framework that combines high-fidelity X-ray imaging data with
  low-fidelity absorptivity data.
---

# Multimodal learning of melt pool dynamics in laser powder bed fusion

## Quick Facts
- arXiv ID: 2509.03029
- Source URL: https://arxiv.org/abs/2509.03029
- Reference count: 40
- Key outcome: Multimodal CNN-RNN model achieves R² scores of 0.9241 (melt pool) and 0.9842 (keyhole) for LPBF dynamics prediction, with transfer learning model using only absorptivity data achieving R² scores of 0.9336 and 0.9360 respectively

## Executive Summary
This paper presents a multimodal learning framework for predicting melt pool dynamics in laser powder bed fusion (LPBF) using a combination of high-fidelity X-ray imaging data and low-fidelity absorptivity signals. The approach integrates convolutional neural networks for spatial feature extraction from X-ray images with recurrent neural networks for temporal feature extraction from absorptivity data, using early fusion during training. The framework is further leveraged for transfer learning, enabling accurate predictions using only the cost-effective absorptivity modality. Results demonstrate that the multimodal model achieves superior accuracy compared to single-modality models, with the transfer learning model maintaining high predictive performance while eliminating the need for expensive X-ray sensors.

## Method Summary
The authors propose a multimodal learning framework that combines CNN-extracted spatial features from X-ray images with RNN-extracted temporal features from absorptivity signals. The model uses early fusion by concatenating CNN and RNN outputs before the final regression head. A transfer learning approach is then applied where the multimodal model serves as a teacher to fine-tune a unimodal RNN student that predicts melt pool dynamics using only absorptivity data. The framework is evaluated on NIST AM-Bench 2022 data with 148 annotated samples, using R² and MAE as metrics. The multimodal model achieves R² scores of 0.9241 for melt pool and 0.9842 for keyhole predictions, while the transfer learning model achieves R² scores of 0.9336 and 0.9360 respectively.

## Key Results
- Multimodal CNN-RNN model achieves R² = 0.9241 (melt pool) and R² = 0.9842 (keyhole) predictions
- Transfer learning model using only absorptivity data achieves R² = 0.9336 (melt pool) and R² = 0.9360 (keyhole)
- Single-modality RNN using absorptivity alone achieves R² = 0.8695, demonstrating significant improvement from multimodal training
- Early fusion strategy outperforms single-modality approaches by effectively combining spatial and temporal information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early fusion of spatial and temporal modalities creates a richer joint representation than either modality alone
- Mechanism: CNN-extracted spatial features from X-ray images capture subsurface geometry (melt pool/keyhole boundaries), while RNN-extracted temporal features from absorptivity signals encode laser-material interaction dynamics. Concatenation before the final regression head allows the network to learn cross-modal correlations during backpropagation
- Core assumption: The two modalities contain complementary predictive information that can be jointly optimized via gradient descent
- Evidence anchors:
  - [abstract] "integrates convolutional neural networks (CNNs) for spatial feature extraction from X-ray data with recurrent neural networks (RNNs) for temporal feature extraction from absorptivity signals, using an early fusion strategy"
  - [section 2.3.3] "The outputs of the CNN and RNN branches were then fused by concatenation, enabling the model to learn melt pool features from image and absorptivity data"
  - [corpus] Related work on multimodal anomaly detection in LPBF (FMR=0.467) similarly exploits complementary sensor signals

### Mechanism 2
- Claim: Knowledge distillation transfers multimodal representations to a unimodal student model for inference-only deployment
- Mechanism: The trained CNN-RNN teacher generates pseudo-labels that encode X-ray-derived spatial understanding. The student RNN, trained on these pseudo-labels alongside ground truth, learns to approximate the teacher's multimodal predictions using only absorptivity input
- Core assumption: Absorptivity signals contain sufficient latent information about melt pool geometry, but require multimodal supervision to extract it
- Evidence anchors:
  - [abstract] "The model is further used as a transfer learning model to fine-tune the RNN model that can predict melt pool dynamics only with absorptivity"
  - [section 2.2.4] "the student model relied solely on the temporal absorptivity sequences but learned to approximate the teacher's outputs through distillation"
  - [corpus] No direct corpus evidence for distillation in LPBF; mechanism is extrapolated from general knowledge distillation literature

### Mechanism 3
- Claim: Noisy low-fidelity signals become predictive when regularized by high-fidelity supervision during training
- Mechanism: Absorptivity data alone yields R²=0.87 (melt pool) due to noise. When jointly trained with X-ray features, the RNN branch receives gradient signals conditioned on spatial ground truth, effectively denoising its internal representation. At inference, this learned structure persists
- Core assumption: Noise in absorptivity is random rather than systematic; signal correlates with true melt pool dynamics
- Evidence anchors:
  - [section 3.2] "RNN achieves... R² score of 0.8695... The performance reflects the low-fidelity and noisy nature of the absorptivity data"
  - [section 3.5] "transfer CNN–RNN... achieved R² scores of 0.9336 (melt-pool)... substantially better than the absorptivity-only RNN"
  - [corpus] Physics-guided denoiser networks (FMR=0.453) similarly use high-fidelity supervision to improve noisy sensor data utility

## Foundational Learning

- Concept: **Convolutional Neural Networks for spatial feature extraction**
  - Why needed here: X-ray images contain spatial patterns (melt pool boundaries, keyhole geometry) that require translation-invariant feature learning
  - Quick check question: Can you explain why max-pooling helps CNNs recognize features regardless of position in the image?

- Concept: **Long Short-Term Memory (LSTM) networks for temporal sequences**
  - Why needed here: Absorptivity signals are time-series with temporal dependencies that feed-forward networks cannot capture
  - Quick check question: What problem do LSTM gates (forget, input, output) solve compared to vanilla RNNs?

- Concept: **Early vs. Late Fusion strategies**
  - Why needed here: Fusion timing determines whether modalities share representation learning (early) or are combined at decision level (late)
  - Quick check question: Why might early fusion outperform late fusion when modalities contain correlated predictive information?

## Architecture Onboarding

- Component map:
  - CNN branch: Input (128×128 grayscale) → Conv2D(32) → MaxPool → Conv2D(32) → MaxPool → Flatten → Dense(64)
  - RNN branch: Absorptivity scalar → Dense(32)
  - Fusion: Concatenate(CNN_out, RNN_out) → Dense(64, ReLU) → Linear(1)
  - Transfer model: LSTM(32) → Dense(64) → Linear(1)

- Critical path: Data preprocessing (7° rotation, normalization, co-registration) → CNN/RNN feature extraction → concatenation → regression loss

- Design tradeoffs:
  - Larger CNN (4 blocks) vs. smaller (2 blocks): Paper uses 4 for single-modality, 2 for multimodal—suggests fusion reduces need for deep spatial encoder
  - Sequence length (T=1 vs T=5): Single-modality uses T=1; transfer uses T=5—longer context may help when X-ray unavailable
  - Assumption: Tradeoffs are dataset-dependent; paper provides limited ablation

- Failure signatures:
  - Low R² on test set despite high training R² → overfitting; check dropout, data augmentation
  - Transfer model underperforms RNN-only → teacher quality insufficient; verify teacher convergence first
  - Predictions lag ground truth → temporal misalignment; check co-registration timestamps

- First 3 experiments:
  1. Replicate single-modality CNN on X-ray data; verify R²≈0.96 to confirm preprocessing pipeline
  2. Train multimodal CNN-RNN with early fusion; compare melt pool R² against paper's 0.9241
  3. Train transfer RNN student using teacher predictions; confirm R² improvement over absorptivity-only baseline (target: 0.93 vs 0.87)

## Open Questions the Paper Calls Out
- **Question 1:** Can the transfer learning framework maintain predictive accuracy when applied to different machines, materials, and environmental conditions?
  - Basis in paper: [explicit] The authors state that "process variability across machines, materials, and environmental conditions introduces domain shifts that the current model may not fully handle" and suggest future work explore domain adaptation
  - Why unresolved: The current study relied on a controlled dataset (N=148) specific to Ti-6Al-4V, limiting generalizability to broader industrial settings
  - What evidence would resolve it: Validation of the model performance across distinct AM hardware platforms and varying metal alloys without retraining from scratch

- **Question 2:** How does the integration of additional modalities, such as acoustic emission or ultrasonic monitoring, impact the fidelity of the multimodal fusion model?
  - Basis in paper: [explicit] The discussion notes the framework is "extensible to other sensor combinations" like acoustic or ultrasonic data, but this capability was not tested
  - Why unresolved: The experiments were limited to fusing X-ray imaging and absorptivity data only
  - What evidence would resolve it: Quantitative results (e.g., R² scores) from an extended architecture that ingests and fuses acoustic or thermal streams with the existing data

- **Question 3:** Does the trained model effectively predict melt pool dynamics during continuous laser scanning and complex part geometries?
  - Basis in paper: [inferred] The methodology specifies the use of "spot laser" data (stationary laser interaction), whereas industrial LPBF involves moving laser tracks and complex thermal histories
  - Why unresolved: The dynamics of a stationary spot laser differ significantly from those of a moving scan vector, introducing untested physical variables
  - What evidence would resolve it: Successful application of the transfer learning model on time-series data derived from moving laser paths and layer-by-layer builds

## Limitations
- Small dataset size (148 annotated samples) limits generalizability despite high R² scores achieved
- Temporal co-registration between X-ray images and absorptivity signals is not explicitly detailed, creating potential alignment uncertainty
- Distillation methodology lacks specificity regarding loss function formulation and hyperparameter tuning for transfer learning phase

## Confidence
- **High Confidence:** Multimodal architecture design and its superiority over single-modality baselines (R² improvements from 0.87 to 0.93+ are well-documented and replicable)
- **Medium Confidence:** Transfer learning effectiveness, as the mechanism is theoretically sound but depends on quality of pseudo-labels and temporal alignment assumptions
- **Medium Confidence:** Early fusion benefits, as the paper demonstrates improved performance but doesn't provide ablation studies comparing early vs. late fusion strategies

## Next Checks
1. **Temporal Alignment Verification:** Reconstruct the co-registration pipeline between X-ray frames and absorptivity timestamps using the NIST AM-Bench 2022 dataset. Test prediction stability under ±5ms misalignment to quantify sensitivity.

2. **Distillation Loss Function Audit:** Implement and compare multiple distillation formulations (MSE-only vs. weighted sum with ground truth) on a held-out validation set to determine optimal training objective for the transfer model.

3. **Generalization Test:** Apply the trained multimodal model to a separate LPBF dataset with different material parameters (e.g., Ti-6Al-4V vs. stainless steel) to assess cross-material predictive capability beyond the original experimental conditions.