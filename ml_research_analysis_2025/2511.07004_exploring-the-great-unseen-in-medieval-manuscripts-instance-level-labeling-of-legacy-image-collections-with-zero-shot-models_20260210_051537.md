---
ver: rpa2
title: 'Exploring the "Great Unseen" in Medieval Manuscripts: Instance-Level Labeling
  of Legacy Image Collections with Zero-Shot Models'
arxiv_id: '2511.07004'
source_url: https://arxiv.org/abs/2511.07004
tags:
- visual
- medieval
- segmentation
- image
- annotation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We aim to extend the scope of iconographic database work to include
  previously overlooked visual content in medieval manuscripts, such as decorative
  patterns, marginalia, and non-narrative forms. By using zero-shot segmentation models
  like SAM2 alongside multi-modal models like RAM++ and Grounded-SAM, we create a
  semi-automatic annotation system that allows instance-level segmentation and labeling.
---

# Exploring the "Great Unseen" in Medieval Manuscripts: Instance-Level Labeling of Legacy Image Collections with Zero-Shot Models

## Quick Facts
- arXiv ID: 2511.07004
- Source URL: https://arxiv.org/abs/2511.07004
- Reference count: 0
- Extends iconographic database work to previously overlooked medieval visual content

## Executive Summary
This paper presents a semi-automatic system for instance-level annotation of medieval manuscript images, targeting previously overlooked visual content such as decorative patterns, marginalia, and non-narrative forms. By leveraging zero-shot segmentation models (SAM2), multi-modal models (RAM++ and Grounded-SAM), and legacy annotations from French manuscript collections, the authors create a pipeline that enables pixel-level masks and labels for individual visual elements. The system supports batch labeling through nearest-neighbor search and iterative fine-tuning, facilitating distant viewing and large-scale analysis of medieval manuscript imagery.

## Method Summary
The system converts legacy image-level annotations from Mandragore and Initiale collections into instance-level masks using a combination of zero-shot and multi-modal models. SAM2 generates segmentation proposals across entire folios, which users refine through point and box prompts. RAM++ provides open-set image-level tags while Grounded-SAM enables text-prompted instance segmentation. A custom interface supports human validation and drag-and-drop label assignment from legacy vocabularies. The resulting instance-level annotations enable nearest-neighbor batch labeling and iterative fine-tuning for domain-specific models.

## Key Results
- Extends iconographic database work to include decorative patterns, marginalia, and non-narrative forms
- Creates semi-automatic annotation system using SAM2, RAM++, and Grounded-SAM
- Enables instance-level segmentation and labeling for richer downstream computer vision applications
- Facilitates distant viewing through batch labeling via nearest-neighbor search

## Why This Works (Mechanism)

### Mechanism 1
Zero-shot segmentation models can generate usable instance masks for medieval manuscripts without domain-specific training data. SAM2 produces segmentation proposals across entire folios, which users filter and refine via point prompts (positive/negative) and box prompts to isolate objects of interest. This bypasses the need for pre-labeled medieval masks, though it assumes visual features from modern natural images transfer sufficiently to medieval iconography and decorative elements.

### Mechanism 2
Multi-modal models can propose candidate labels that, when combined with human validation, extend legacy vocabularies to new visual categories. RAM++ generates open-set image-level tags while Grounded-SAM grounds text prompts to bounding boxes then segments via SAM2. Users accept, reject, or extend these tags via drag-and-drop in a custom interface. This mechanism assumes tag proposals from models trained on modern data include enough generic categories to be useful, even if domain-specific terms are missed.

### Mechanism 3
Instance-level annotations enable nearest-neighbor batch labeling, which compounds productivity across the corpus. Once segments are masked and labeled, their visual embeddings support nearest-neighbor search over unlabeled segments, allowing users to apply labels in batches rather than one-by-one, then iteratively fine-tune domain-specific models. This assumes visual similarity in embedding space correlates with semantic equivalence for medieval visual elements.

## Foundational Learning

- **Zero-shot vs. fine-tuned models**: The system relies on SAM2, RAM++, and Grounded-SAM operating without medieval-specific training. Understanding the difference between zero-shot inference and domain adaptation is critical. Quick check: Can you explain why a model might detect "crown" but fail on "crozier" in a medieval image?

- **Instance segmentation vs. image-level labeling**: Legacy datasets lack spatial coordinates; the pipeline upgrades these to pixel-level masks for downstream CV tasks. Quick check: What is the minimum annotation format required to train an instance segmentation model?

- **Human-in-the-loop annotation systems**: All segmentations undergo human validation; the interface design determines throughput. Quick check: In a semi-automatic pipeline, where does the human intervene and what is left fully automatic?

## Architecture Onboarding

- **Component map**: Input (medieval manuscript images + legacy annotations) -> SAM2 (zero-shot segmentation) -> RAM++ (image-level tags) -> Grounded-SAM (text-prompted segmentation) -> Annotation interface (human validation) -> Embedding index (nearest-neighbor search) -> Output (instance-level masks + labels)

- **Critical path**: 1) Load image and legacy annotations, 2) Run SAM2 automatic mask generation or prompt-guided segmentation, 3) Apply RAM++ for tags or Grounded-SAM for text-prompted segmentation, 4) Human validates/corrects masks and assigns/confirms labels, 5) Index validated segments for nearest-neighbor retrieval, 6) Use batch labeling to propagate labels to similar unlabeled segments, 7) Fine-tune domain-specific models on accumulated annotations

- **Design tradeoffs**: Automatic vs. promptable segmentation (full coverage vs. precision), legacy vocabulary vs. extended vocabulary (consistency vs. inclusivity), flat vs. hierarchical labeling (simplicity vs. nuance)

- **Failure signatures**: SAM2 oversegments parchment texture/ink bleed as objects, RAM++ proposes anachronistic modern tags, Grounded-SAM fails on crowded scenes with multiple overlapping figures, nearest-neighbor batch labeling propagates incorrect labels due to visual similarity without semantic equivalence

- **First 3 experiments**: 1) Baseline SAM2 mask quality assessment: Run automatic segmentation on 50 manuscript pages; measure precision/recall against human-annotated ground truth for 5 target categories, 2) RAM++ tag relevance audit: Run RAM++ on 100 images with existing legacy labels; calculate overlap between proposed tags and legacy vocabulary, 3) Nearest-neighbor label propagation test: Manually annotate 200 instances across 5 categories; retrieve top-10 nearest neighbors for each; measure label accuracy

## Open Questions the Paper Calls Out

1. How should hierarchies of visual forms and interpretive actions be formally defined to construct a computational grammar of medieval iconography? The authors state they haven't decided how to define such hierarchies and describe their work as a first step to constructing a grammar of low-level objects.

2. To what extent can zero-shot and multi-modal models be fine-tuned to overcome their reliance on modern training data when identifying medieval-specific vocabulary? The paper notes models suffer from a lack of knowledge about domain-specific vocabulary suitable for medieval manuscripts due to modern training data.

3. How effective is nearest-neighbor search in facilitating reliable batch labeling across heterogeneous collections of medieval manuscripts? The paper presents the pipeline for this functionality but does not provide analysis of the precision or time-efficiency gains of the batch labeling component specifically.

## Limitations

- Zero-shot models trained on modern natural images may not reliably detect medieval-specific elements like croziers, rotuli, and mitres
- Nearest-neighbor batch labeling may have insufficient precision when intra-class visual variance is high or inter-class variance is low
- Semantic alignment between model-proposed tags and medieval iconographic vocabularies remains unvalidated

## Confidence

- **High Confidence**: Technical feasibility of combining SAM2, RAM++, and Grounded-SAM; value of extending legacy datasets to instance-level; general concept of human-in-the-loop annotation systems
- **Medium Confidence**: Visual similarity correlates with semantic equivalence for medieval elements; effectiveness of nearest-neighbor batch labeling; adequacy of generic tags for medieval annotation tasks
- **Low Confidence**: Actual quality of zero-shot segmentation on medieval manuscripts without domain adaptation; semantic alignment between model tags and medieval vocabularies; system throughput compared to manual annotation

## Next Checks

1. **SAM2 Mask Quality Benchmark**: Evaluate automatic segmentation on 50 manuscript folios against human-annotated ground truth for 5 target categories (historiated initials, marginalia, decorated initials, human figures, decorative borders). Measure precision, recall, and annotation correction time.

2. **RAM++ Domain Gap Analysis**: Run RAM++ on 100 images with known legacy labels. Calculate tag overlap and semantic distance between proposed and legacy vocabularies. Test whether adding medieval-specific terms to the model's prompt improves recall.

3. **Batch Labeling Precision Test**: Manually annotate 200 instances across 5 categories. Use extracted mask embeddings to retrieve top-10 nearest neighbors per instance. Measure label accuracy and compare manual correction effort against one-by-one annotation.