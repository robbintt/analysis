---
ver: rpa2
title: 'An Artificial Intelligence-Based Framework for Predicting Emergency Department
  Overcrowding: Development and Evaluation Study'
arxiv_id: '2504.18578'
source_url: https://arxiv.org/abs/2504.18578
tags:
- waiting
- data
- time
- hourly
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study develops machine learning models to forecast emergency
  department (ED) waiting room occupancy, addressing ED overcrowding through predictive
  modeling. Using data from a southeastern U.S.
---

# An Artificial Intelligence-Based Framework for Predicting Emergency Department Overcrowding: Development and Evaluation Study

## Quick Facts
- **arXiv ID:** 2504.18578
- **Source URL:** https://arxiv.org/abs/2504.18578
- **Reference count:** 40
- **Primary result:** ML models forecast ED waiting room occupancy up to 24 hours ahead with MAE 2.00-4.19

## Executive Summary
This study develops and evaluates machine learning models to predict emergency department waiting room occupancy, addressing the critical issue of ED overcrowding. The framework uses data from a southeastern U.S. hospital (2019-2023) to forecast patient counts at both six-hour and 24-hour horizons. Eleven algorithms were trained on 16 feature sets incorporating ED data, weather, and event information. The models demonstrate strong predictive performance, with TSiTPlus achieving the best hourly predictions and XCMPlus excelling in daily forecasts. These tools enable proactive resource allocation and improved ED flow management.

## Method Summary
The study employed a comprehensive machine learning approach using data from a southeastern U.S. hospital system spanning 2019-2023. Eleven different algorithms were trained on 16 distinct feature sets combining internal ED operational data, weather information, and local event calendars. The models were evaluated for two prediction horizons: hourly forecasts six hours ahead and daily forecasts 24 hours ahead. Performance was assessed using multiple metrics including MAE and MSE, with particular attention to extreme case analysis during high-volume periods. The methodology included rigorous validation across different prediction scenarios to ensure model reliability under various operational conditions.

## Key Results
- TSiTPlus achieved best hourly prediction performance with MAE of 4.19 and MSE of 29.32
- XCMPlus delivered optimal daily forecasting with MAE of 2.00 and MSE of 6.64
- Models maintained reliable performance during extreme high-volume periods

## Why This Works (Mechanism)
The framework leverages machine learning's ability to identify complex patterns in ED utilization by combining multiple data streams. Internal ED data captures operational patterns and patient flow dynamics, while external factors like weather and local events provide contextual influences on ED demand. The temporal aspect is addressed through both short-term (hourly) and medium-term (daily) forecasting, allowing for different operational planning horizons. The ensemble approach of testing multiple algorithms ensures robust model selection, while the feature engineering process identifies the most predictive combinations of variables.

## Foundational Learning
- **Feature Engineering**: Combining internal ED data with external factors (weather, events) provides comprehensive predictive signals - needed to capture all demand drivers, quick check: correlation analysis of features with target
- **Multi-horizon Forecasting**: Separate models for hourly vs daily predictions address different operational needs - needed for appropriate planning granularity, quick check: compare performance across horizons
- **Algorithm Diversity**: Testing multiple ML approaches ensures optimal model selection - needed to find best fit for complex ED patterns, quick check: cross-validation performance comparison
- **Extreme Case Analysis**: Validating performance during high-volume periods ensures reliability when most needed - needed for operational trust, quick check: stress-test model with peak load data
- **Multi-metric Evaluation**: Using MAE, MSE, and other metrics provides complete performance picture - needed for balanced assessment, quick check: compare across all evaluation metrics

## Architecture Onboarding
- **Component Map**: Data Preprocessing -> Feature Engineering -> Model Training -> Validation -> Deployment
- **Critical Path**: Raw data collection → feature extraction → model training → performance evaluation → operational implementation
- **Design Tradeoffs**: Accuracy vs interpretability (complex models may be less transparent), breadth vs depth (comprehensive features vs focused predictors), computational cost vs real-time feasibility
- **Failure Signatures**: Poor performance during unusual events, overfitting to training period patterns, sensitivity to missing data, degradation during rapid operational changes
- **First Experiments**: 1) Validate model performance on external hospital data, 2) Implement in live ED operations for A/B testing, 3) Test model stability across different seasons and operational conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Single hospital system data limits generalizability to other geographic regions and healthcare systems
- COVID-19 period included in training may bias models toward pandemic-era patterns
- Focus on waiting room counts doesn't address patient boarding times or inpatient bed availability
- Limited clinical validation of feature importance findings

## Confidence
- **High Confidence**: Model performance metrics for the specific hospital system and prediction tasks
- **Medium Confidence**: Generalizability to other hospital settings and healthcare systems
- **Medium Confidence**: Model robustness during non-pandemic conditions

## Next Checks
1. External validation study using data from multiple hospital systems across different regions to assess model generalizability and identify necessary adaptations for different operational contexts
2. Prospective clinical trial implementing the models in real-time ED operations to evaluate actual impact on resource allocation, wait times, and patient flow metrics
3. Extended testing during a full calendar year of typical (non-pandemic) conditions to confirm model stability across seasonal variations and identify any temporal dependencies not captured in the training data