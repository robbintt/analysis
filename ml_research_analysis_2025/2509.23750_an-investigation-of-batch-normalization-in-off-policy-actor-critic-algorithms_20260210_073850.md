---
ver: rpa2
title: An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms
arxiv_id: '2509.23750'
source_url: https://arxiv.org/abs/2509.23750
tags:
- critic-i
- training
- learning
- mode
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Batch Normalization (BN) faces limited adoption in deep reinforcement
  learning (DRL) due to non-i.i.d. data and shifting distributions.
---

# An Investigation of Batch Normalization in Off-Policy Actor-Critic Algorithms

## Quick Facts
- **arXiv ID:** 2509.23750
- **Source URL:** https://arxiv.org/abs/2509.23750
- **Reference count:** 40
- **Primary result:** Mode-Aware Batch Normalization (MA-BN) accelerates training, broadens learning rate range, enhances exploration, and reduces optimization difficulty in off-policy actor-critic algorithms.

## Executive Summary
Batch Normalization (BN) faces limited adoption in deep reinforcement learning due to non-i.i.d. data and shifting distributions. This work systematically investigates BN mode selection in off-policy actor-critic algorithms. We identify that training mode BN in policy evaluation can cause instability due to distribution mismatch, while evaluation mode BN can yield more stable Q-value estimates. Based on these insights, we propose Mode-Aware Batch Normalization (MA-BN), which leverages evaluation mode BN in policy updates for stable learning. Empirical results show MA-BN accelerates training, broadens the effective learning rate range, enhances exploration, and reduces optimization difficulty compared to no normalization or Layer Normalization across DMC and MuJoCo tasks.

## Method Summary
The paper proposes Mode-Aware Batch Normalization (MA-BN) for off-policy actor-critic algorithms, specifically using DRQv2 as the base algorithm. MA-BN configures BN layers differently across network components: the Critic uses ETT mode (Critic-I=Eval, Critic-II=Train, Critic-III=Train), while the Actor uses TT mode (Actor-I=Train, Actor-II=Train). The key insight is using evaluation mode BN in the Critic's policy update (Critic-I) to prevent distribution mismatch between on-policy actions and replay buffer statistics, while using training mode BN in the Actor to introduce beneficial stochasticity for exploration.

## Key Results
- MA-BN accelerates training convergence compared to no normalization and Layer Normalization baselines
- MA-BN broadens the effective learning rate range, enabling stable learning at higher learning rates
- MA-BN enhances exploration, as measured by higher coverage in state-action space
- MA-BN reduces optimization difficulty, enabling comparable performance with memory-efficient optimizers like SGD

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Setting Batch Normalization (BN) to evaluation mode during the Critic's policy update stabilizes learning by preventing distribution mismatch between on-policy actions and replay buffer statistics.
- **Mechanism:** In off-policy actor-critic algorithms, the Critic is trained on data from the replay buffer. If BN uses training mode during the Actor update, it calculates normalization statistics based on the current on-policy actions, which diverge from the buffer distribution. Evaluation mode forces the use of running statistics, which better represent the buffer distribution, yielding more accurate Q-value estimates for the Actor.
- **Core assumption:** The divergence is caused primarily by the distribution shift between the current policy and the replay buffer.
- **Evidence anchors:** [Page 5, text]: "The failure of Critic-I T is attributed to inaccurate Q-value estimation... effectively masking the distributional shift of the updated actor." [Page 5, Figure 3]: Shows that mixing buffer data into the batch rescues the "Training mode" performance, validating the mismatch hypothesis.

### Mechanism 2
- **Claim:** Using training mode in the Actor network introduces beneficial stochasticity that enhances exploration.
- **Mechanism:** In training mode, BN statistics fluctuate with each mini-batch. This variance introduces noise into the Actor's forward pass, effectively creating a stochastic policy even from a deterministic network. This noise helps the agent explore the state-action space more broadly than a purely deterministic policy.
- **Core assumption:** The noise introduced by batch variance is roughly isotropic or beneficial.
- **Evidence anchors:** [Page 7, text]: "TT mode yields strong results in most cases while maximizing stochasticity..." [Page 8, Figure 7]: Shows "Coverage MA-BN" is significantly higher than "Coverage Origin," linking BN stochasticity to physical exploration.

### Mechanism 3
- **Claim:** BN smooths the loss landscape, broadening the effective learning rate range and enabling the use of memory-efficient optimizers like SGD.
- **Mechanism:** BN rescales activations and gradients, preventing them from vanishing or exploding. This makes the optimization surface smoother, allowing the algorithm to tolerate higher learning rates without divergence. This condition also reduces the reliance on adaptive optimizers, allowing for SGD which uses less memory.
- **Core assumption:** The primary bottleneck for using SGD or high LRs in DRL is the roughness of the loss landscape.
- **Evidence anchors:** [Page 8, Figure 8 (Left)]: Shows MA-BN maintains high reward at higher learning rates where the baseline fails. [Page 8, Figure 8 (Right)]: Shows MA-BN enables SGD to perform comparably to Adam-based baselines.

## Foundational Learning

- **Concept:** Train vs. Eval Modes in Batch Norm
  - **Why needed here:** The entire contribution of the paper rests on distinguishing when to compute statistics from the current batch (Train) versus using a running average (Eval).
  - **Quick check question:** Does "Eval" mode mean "don't update network weights"? (Answer: No, it refers to using running stats for normalization while weights still update).

- **Concept:** Off-Policy Actor-Critic Updates
  - **Why needed here:** The paper breaks the algorithm into distinct steps (Critic-I, Critic-II, etc.). You must understand that Critic-I uses current policy actions while Critic-II uses buffer actions to grasp the distribution shift problem.
  - **Quick check question:** In Step Critic-I, where does the action input come from? (Answer: The current Actor policy π_φ(s_t)).

- **Concept:** The i.i.d. Assumption in RL
  - **Why needed here:** Standard BN assumes independent and identically distributed data. RL data is correlated and non-stationary. Understanding this violation explains why BN is historically difficult in RL.
  - **Quick check question:** Why does a large replay buffer help BN stability? (Answer: It dilutes the temporal correlation and provides a more stable distribution for running statistics).

## Architecture Onboarding

- **Component map:**
  - **Critic Network:** Configured as ETT (Eval-Train-Train)
    - *Critic-I (Input from Actor):* Eval Mode (Critical for stability)
    - *Critic-II (Training Q-function):* Train Mode (Standard)
    - *Critic-III (Target Q-value):* Train Mode (Default for simplicity)
  - **Actor Network:** Configured as TT (Train-Train)
    - *Actor-I (Update):* Train Mode (For exploration/stochasticity)
    - *Actor-II (Target gen):* Train Mode

- **Critical path:**
  1. Identify the specific forward pass in your code corresponding to "Critic-I" (computing Q-values for the Actor gradient).
  2. Force the Critic's BN layers to `eval()` specifically during this pass, while keeping `requires_grad=True` for the weights.
  3. Ensure the Actor remains in `train()` mode during its own training step.

- **Design tradeoffs:**
  - **Stability vs. Exploration:** Enforcing Eval mode on the Critic stabilizes Q-estimates while Train mode on the Actor adds noise for exploration.
  - **Simplicity vs. Precision:** The paper defaults to "TT" for the Target Critic, but notes that soft-updating BN stats might extract slightly more performance at the cost of complexity.

- **Failure signatures:**
  - **Critic-I in Train Mode:** Characterized by high variance in Q-bias and eventual performance collapse or "freezing" at low rewards.
  - **Stale Eval Stats:** If using Eval mode without proper warm-up or updates, the running stats may drift from the true data distribution, causing divergence.

- **First 3 experiments:**
  1. **Ablation on Critic-I Mode:** Run DRQv2 on a simple DMC task (e.g., Hopper) with Critic-I set to "Train" vs. "Eval". Confirm the "Train" curve diverges or stagnates.
  2. **Buffer Mixing Test:** Implement the "buffer mixing" strategy for Critic-I in Train mode. Verify that increasing the mixing ratio stabilizes the training, confirming the distribution mismatch hypothesis.
  3. **Learning Rate Sweep:** Compare MA-BN vs. Baseline (No BN) across a grid of learning rates (e.g., 1e-4 to 1e-3) to validate the claim of a broader optimization sweet spot.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the specific theoretical mechanisms beyond distribution mismatch that cause the Training mode to fail in Step Critic-I?
- **Basis in paper:** [explicit] The authors state in Section 3.1 that experiments varying batch and buffer sizes suggest the failure of Critic-I T "is not primarily caused by fluctuations in batch statistics, indicating the presence of additional underlying factors that warrant further investigation."
- **Why unresolved:** While the authors identify distribution mismatch as a factor, the empirical results regarding buffer mixing ratios and variance adjustments do not fully account for the observed instability, leaving the complete set of failure causes undefined.
- **What evidence would resolve it:** A theoretical analysis or ablation study isolating the interaction between the actor's gradient updates and the critic's batch-dependent normalization noise.

### Open Question 2
- **Question:** Can a unified update mechanism be designed to eliminate the residual misalignment between target network parameters and Batch Normalization statistics?
- **Basis in paper:** [inferred] Section 3.2 notes that while synchronizing BN stats with target network parameters improves performance, they "do not completely eliminate the mismatch" due to different update scales and dynamics.
- **Why unresolved:** The paper empirically tests soft updates and direct copying but concludes that a residual misalignment persists, suggesting the current update strategies are suboptimal for target critics using Batch Normalization.
- **What evidence would resolve it:** Derivation of a theoretical bound on the misalignment or a new update rule that empirically achieves zero-latency alignment in the target network.

### Open Question 3
- **Question:** How does the Mode-Aware Batch Normalization (MA-BN) framework generalize to on-policy actor-critic algorithms?
- **Basis in paper:** [inferred] The title and methodology explicitly restrict the scope to "Off-Policy Actor-Critic Algorithms," leaving the applicability to on-policy methods unexplored.
- **Why unresolved:** The theoretical justification for MA-BN relies heavily on the distributional shift between the replay buffer and the current policy (Theorem 1), a dynamic that differs fundamentally in on-policy settings.
- **What evidence would resolve it:** Empirical evaluation of MA-BN in standard on-policy benchmarks (e.g., PPO, TRPO) to determine if the "TT" and "ETT" mode configurations retain their benefits without the off-policy replay buffer context.

## Limitations
- The paper does not specify exact network architectures, hyperparameters, or BN initialization details, making exact reproduction challenging
- The mechanism for exploration through BN noise assumes the stochasticity is beneficial, but the paper doesn't analyze the directional bias of this noise
- The stability claim for evaluation mode BN relies on the assumption that running statistics accurately track the replay buffer distribution, which may break down with rapid policy changes

## Confidence
- **High Confidence:** The core claim that Critic-I in training mode causes distribution mismatch and instability, leading to poor performance
- **Medium Confidence:** The broader claims about MA-BN accelerating training, broadening the learning rate range, and enhancing exploration across diverse tasks
- **Low Confidence:** The assertion that BN enables SGD to match Adam performance without providing direct SGD vs Adam comparison under MA-BN

## Next Checks
1. **Ablation on Critic-I Mode:** Run DRQv2 on hopper hop with Critic-I set to "Train" vs. "Eval" mode. Verify that training mode causes divergence or stagnation while evaluation mode maintains stable learning curves.

2. **Buffer Mixing Test:** Implement the buffer mixing strategy (mixing replay buffer samples into Critic-I batch) for training mode BN. Confirm that increasing mixing ratio stabilizes training, validating the distribution mismatch hypothesis.

3. **Learning Rate Sweep:** Compare MA-BN vs. baseline (no BN) across a grid of learning rates (1e-4 to 1e-3). Verify that MA-BN maintains high reward at higher learning rates where baseline fails, demonstrating the broader optimization sweet spot.