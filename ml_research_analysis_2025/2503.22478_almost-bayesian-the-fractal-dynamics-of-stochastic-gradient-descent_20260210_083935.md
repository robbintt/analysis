---
ver: rpa2
title: 'Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent'
arxiv_id: '2503.22478'
source_url: https://arxiv.org/abs/2503.22478
tags:
- learning
- diffusion
- dimension
- which
- fractal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper connects the dynamics of stochastic gradient descent
  (SGD) in deep learning to Bayesian statistics by showing that SGD behaves like diffusion
  on a fractal landscape. The fractal dimension of the loss landscape, which determines
  how "accessible" different regions are, can be described using the local learning
  coefficient from singular learning theory.
---

# Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent

## Quick Facts
- **arXiv ID**: 2503.22478
- **Source URL**: https://arxiv.org/abs/2503.22478
- **Reference count**: 28
- **Primary result**: Shows SGD dynamics on deep learning loss landscapes can be modeled as fractal diffusion, connecting singular learning theory to empirical training behavior.

## Executive Summary
This paper establishes a theoretical bridge between stochastic gradient descent (SGD) and Bayesian statistics by modeling the dynamics as diffusion on a fractal landscape. The authors show that the loss landscape's fractal dimension, quantified by the local learning coefficient from singular learning theory, fundamentally constrains where the model weights can move during training. This results in subdiffusive behavior that follows a fractional Fokker-Planck equation rather than standard diffusion.

The theory explains why SGD behaves like a "modified" Bayesian sampler, where the probability of finding solutions depends not just on minimizing loss but also on the accessibility of different regions of the landscape. Experiments on MNIST with various architectures confirm that the spectral dimension governing particle movement is bounded above by the local learning coefficient, and that solutions concentrate in regions with higher effective diffusion coefficients.

## Method Summary
The authors train fully connected networks on a 10,000-image MNIST subset using SGD (lr=0.001, batch=256, no weight decay) across multiple seeds and architectures. They compute the local learning coefficient (LLC) every 100 training steps using the devinterp estimator, taking the average of the last 10 estimates as the final value. Weight displacement is tracked over time, and the spectral dimension is estimated via linear regression of log-displacement against log-time. The main validation checks whether the spectral dimension is bounded above by the LLC and whether diffusion exponents concentrate among higher values.

## Key Results
- Weight trajectories follow power-law scaling R(t) ~ t^(1/ν) with ν > 2, confirming subdiffusive behavior
- Spectral dimension d_s is bounded above by the local learning coefficient λ(w) at each time point
- Diffusion exponents concentrate among higher values, indicating solutions preferentially occupy accessible regions
- LLC correlates with generalization performance across different architectures

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The Local Learning Coefficient (LLC) from singular learning theory serves as the fractal dimension of the loss landscape, determining the volume of accessible low-loss states.
- **Mechanism**: The paper posits that the volume of parameters V(ε) with loss less than ε scales as ε^λ(w*), which aligns with the definition of a mass fractal dimension. This geometry dictates how "porous" the loss landscape is, controlling where a diffusing particle (the model weights) can move.
- **Core assumption**: The "Near Stability Hypothesis" (Hypothesis 3.1) assumes that at any point during training, the model is sufficiently close to a metastable state so that the local geometry is effectively captured by the LLC.
- **Evidence anchors**:
  - [abstract] Mentions the fractal dimension is "captured by the local learning coefficient."
  - [section 3.1] Equations 4 and 5 explicitly derive the relationship between the LLC and the mass fractal dimension.
  - [corpus] Related work [68863] supports the general premise that SGD dynamics are influenced by landscape geometry (avoiding sharp minima), though it does not explicitly verify the fractal link.
- **Break condition**: If the loss landscape is dominated by non-degenerate local minima (smooth geometry), the fractal dimension approximation fails, potentially leading to logarithmic displacement scaling rather than power-law.

### Mechanism 2
- **Claim**: SGD follows a subdiffusive process better modeled by a fractional Fokker-Planck equation (FFPE) than a standard one.
- **Mechanism**: Standard diffusion assumes uncorrelated steps. The paper introduces a Caputo fractional derivative D^α_t into the Fokker-Planck equation to model "memory" effects and trapping within the fractal geometry (pores). This results in weight displacement R(t) scaling as a power law t^(1/ν) (subdiffusive) rather than t^(1/2).
- **Core assumption**: Assumption: The system operates in the "large batch size regime" where gradient noise is Gaussian-like rather than heavy-tailed (Lévy flights).
- **Evidence anchors**:
  - [abstract] "The authors model SGD as a fractional Fokker-Planck equation where the fractional derivative accounts for subdiffusive behavior."
  - [section 3] Equation 2 defines the FFPE; Section 3.1 and Figure 1 confirm the power-law scaling of weight displacement.
  - [corpus] Paper [90046] provides context on PDE perspectives for SGD, though specific fractional dynamics are not cited as a primary confirmation in the corpus.
- **Break condition**: If the batch size is too small or learning rate too high, noise becomes heavy-tailed, inducing "jumps" (Lévy flights) that violate the subdiffusive assumption (Appendix E).

### Mechanism 3
- **Claim**: SGD acts as a "modified" Bayesian sampler where the probability of a solution is scaled by an effective diffusion coefficient dependent on the fractal dimension.
- **Mechanism**: By solving for the steady-state of the FFPE, the authors show the distribution p_s(w) ∝ e^(-γL_m[w]/D_ξ). This implies SGD finds solutions that maximize not just the likelihood (minimize loss), but also the effective diffusion coefficient D_ξ (accessibility), linking optimization dynamics to Bayesian posteriors.
- **Core assumption**: Assumption: The effective diffusion coefficient D_ξ can be approximated as a scalar constant over a local region W (homogenization).
- **Evidence anchors**:
  - [abstract] "SGD can be regarded as a modified Bayesian sampler which accounts for accessibility constraints."
  - [section 3.2] Lemma 3.1 and Corollary 3.1 derive the steady-state distribution and its relation to the Bayesian posterior.
  - [corpus] General links between SGD and Bayesian sampling are discussed in [68863] and [90046], supporting the high-level connection.
- **Break condition**: If the spectral dimension d_s varies significantly over time (non-asymptotic regime) or if the homogenization scale ξ is poorly chosen, the scalar approximation of the diffusion tensor fails.

## Foundational Learning

- **Concept**: **Singular Learning Theory (SLT)**
  - **Why needed here**: The paper relies on the LLC (λ) derived from SLT to quantify the "complexity" or degeneracy of the loss landscape. Unlike regular BIC, SLT handles the non-identifiable, singular nature of neural networks.
  - **Quick check question**: Why does the standard Bayesian Information Criterion (BIC) fail for deep learning models according to SLT? (Answer: BIC assumes a non-degenerate Hessian/regular model, which DNNs are not).

- **Concept**: **Anomalous Diffusion (Subdiffusion)**
  - **Why needed here**: To understand why standard Brownian motion models of SGD are insufficient. Subdiffusion implies the "particle" (weights) gets trapped or slowed down by the geometry, resulting in slower spread over time.
  - **Quick check question**: How does the mean squared displacement (MSD) scale with time in a subdiffusive process compared to normal diffusion? (Answer: MSD scales as t^α with α < 1, versus t for normal diffusion).

- **Concept**: **Fokker-Planck Equation**
  - **Why needed here**: This is the baseline mathematical tool for describing the time evolution of probability density for a particle in a potential. The paper modifies this with fractional derivatives.
  - **Quick check question**: In the context of SGD, what do the "potential" V(w) and "diffusion coefficient" D represent in the Fokker-Planck equation? (Answer: V(w) is the loss landscape; D relates to the noise scale/gradient variance).

## Architecture Onboarding

- **Component map**: Training data X_m -> Model parameters w -> LLC Estimator (devinterp) -> Local learning coefficient λ(w) -> Weight displacement tracker -> Spectral dimension analyzer

- **Critical path**:
  1. **Verify Subdiffusion**: Confirm weight trajectories follow a power law (t^(1/ν)) rather than linear or logarithmic scaling (Figure 1).
  2. **Calibrate LLC Estimator**: Select correct "characteristic length scale" ξ (epsilon ball radius) to ensure LLC is positive and stable.
  3. **Check Boundedness**: Validate d_s ≤ λ(w(t)) (Lemma 3.2) to confirm the model is obeying the predicted fractal constraints.

- **Design tradeoffs**:
  - **Scale ξ**: Choosing the scale for homogenization (averaging out fluctuations). Too small = high variance; too large = loss of local geometric detail (Appendix C).
  - **Batch Size**: Small batches may induce Lévy flights (jumps) which break the subdiffusive assumption but might be useful for "grokking" (Appendix E).

- **Failure signatures**:
  - **Negative LLC**: Indicates the model is not near a metastable state/minima (Section 3.1).
  - **Linear Diffusion**: If R(t) ~ t, the fractal theory does not apply (landscape is not singular/porous in the predicted way).
  - **Violation of Inequality**: If d_s > λ(w), the theoretical bound is broken, suggesting estimation error or invalid regime.

- **First 3 experiments**:
  1. **Reproduce Scaling Law**: Train a fully connected network on MNIST; plot log(displacement) vs log(time). Verify the slope is not 0.5 (normal diffusion) but typically ≤ 0.5 (subdiffusive).
  2. **LLC vs. Generalization**: Calculate the LLC for models of varying widths/depths. Plot LLC against test error to verify the correlation predicted by the "Bayesian Antecedent Hypothesis" (Figure 5a).
  3. **Diffusion Concentration**: Run 50 seeds of the same architecture. Histogram the diffusion exponent ν(w) = d_s / 2λ(w). Verify that solutions concentrate in regions where this exponent is larger (Figure 4).

## Open Questions the Paper Calls Out
- How does the fractal diffusion framework explain the necessity of adaptive optimizers (e.g., AdamW) for training large models?
- Can the introduction of a spatial fractional derivative (or fractional Laplacian) in the Fokker-Planck equation fully account for the sudden large jumps in weight space observed during grokking?
- How does the fractal diffusion theory extend to regimes where extreme hyperparameter choices cause noise to dominate the system dynamics?

## Limitations
- The theory assumes the "Near Stability Hypothesis" holds throughout training, but this is difficult to verify empirically without expensive landscape characterization.
- The homogenization scale ξ for computing the LLC is presented as a tunable parameter but lacks a principled selection criterion.
- The theory focuses on asymptotic dynamics (long training time), while practical neural network training often involves transient phases that may violate these assumptions.

## Confidence
- **High confidence**: The empirical observation of subdiffusive weight trajectories (R(t) ~ t^(1/ν) with ν > 2) is directly verifiable and well-supported by the MNIST experiments.
- **Medium confidence**: The theoretical connection between LLC and fractal dimension is mathematically sound within the singular learning theory framework, though the practical approximation of LLC via devinterp introduces uncertainty.
- **Low confidence**: The specific quantitative predictions about concentration of solutions in high-diffusion regions (Figure 4) rely on unstated hyperparameters in the LLC estimation process.

## Next Checks
1. **Scale sensitivity analysis**: Systematically vary the homogenization scale ξ and measure its effect on LLC estimates and the ds ≤ λ(w) bound to establish robustness.
2. **Architecture generalization test**: Validate the theory on CIFAR-10 with convolutional networks to check if the fractal dynamics persist beyond simple fully-connected architectures.
3. **Heavy-tail regime verification**: Intentionally train with small batch sizes to induce Lévy flights and confirm the theory's prediction that these violate the subdiffusive assumption.