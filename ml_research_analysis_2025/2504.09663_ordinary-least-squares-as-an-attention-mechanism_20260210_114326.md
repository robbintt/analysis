---
ver: rpa2
title: Ordinary Least Squares as an Attention Mechanism
arxiv_id: '2504.09663'
source_url: https://arxiv.org/abs/2504.09663
tags:
- attention
- regression
- where
- space
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that ordinary least squares (OLS) predictions
  can be reformulated as a restricted attention module, akin to those in large language
  models. This equivalence emerges when OLS is framed as a similarity-based estimator
  in a transformed orthonormal space, where the optimal encoding-decoding operations
  correspond to the inverse covariance matrix of predictors.
---

# Ordinary Least Squares as an Attention Mechanism

## Quick Facts
- arXiv ID: 2504.09663
- Source URL: https://arxiv.org/abs/2504.09663
- Authors: Philippe Goulet Coulombe
- Reference count: 40
- Key outcome: OLS predictions reformulated as a restricted attention module equivalent to similarity-based estimation in transformed orthonormal space

## Executive Summary
This paper establishes a theoretical equivalence between ordinary least squares regression and restricted attention mechanisms by reformulating OLS as a similarity-based estimator in transformed orthonormal space. The work bridges classical statistics and modern deep learning by showing that OLS can be interpreted as an information retrieval process where optimal encoding-decoding operations correspond to inverse covariance matrices of predictors. The proposed Attention Regression framework is validated across six diverse data-generating processes, demonstrating competitive performance with standard machine learning methods while maintaining a simpler architectural structure.

## Method Summary
The framework reinterprets OLS through an attention lens by transforming predictors into an orthonormal space where the regression problem becomes equivalent to computing weighted inner products between training and test vectors. The key insight is that OLS coefficients can be decomposed into encoding and decoding matrices that operate on transformed predictor space, with the inverse covariance matrix serving as the optimal transformation. This perspective shifts from traditional coefficient estimation to learning optimal mappings for similarity-based comparisons, effectively turning regression into a restricted attention mechanism that retrieves relevant information through inner product computations.

## Key Results
- Attention Regression performs competitively with Random Forest and Gradient Boosting across six diverse data-generating processes
- The framework matches or exceeds performance of standard ML methods while maintaining simpler architecture than Multi-Layer Perceptrons
- Empirical validation demonstrates the practical viability of the theoretical OLS-attention equivalence

## Why This Works (Mechanism)
The equivalence emerges from viewing OLS as a similarity-based estimator in transformed space. When predictors are transformed into an orthonormal basis, the regression problem reduces to computing inner products between transformed training and test vectors. The optimal transformation matrix corresponds to the inverse covariance matrix of predictors, which serves as both encoding and decoding operation. This allows OLS to be reframed as computing weighted similarities between observations, where the weights are determined by the inverse covariance structure, effectively turning regression into an attention mechanism that retrieves relevant information through these similarity computations.

## Foundational Learning
1. **Orthonormal transformations in regression** - Why needed: To establish the mathematical equivalence between OLS and attention mechanisms; Quick check: Verify that transformed predictors maintain variance properties and enable inner product interpretation
2. **Covariance matrix inversion in statistical learning** - Why needed: The inverse covariance matrix serves as the critical transformation between OLS and attention; Quick check: Confirm that the inverse covariance correctly captures predictor relationships in transformed space
3. **Attention mechanisms in deep learning** - Why needed: To understand how OLS maps to restricted attention modules; Quick check: Validate that the similarity computations match standard attention formulations
4. **Inner product similarity measures** - Why needed: Forms the basis for the information retrieval interpretation; Quick check: Ensure that inner products in transformed space preserve meaningful similarity relationships

## Architecture Onboarding

**Component Map:** Predictors -> Orthonormal Transformation -> Inner Product Computation -> Weighted Sum -> Prediction

**Critical Path:** The transformation step using inverse covariance matrix is critical, as it determines the optimal encoding-decoding operations that enable the attention interpretation.

**Design Tradeoffs:** The framework trades computational complexity (matrix inversion) for interpretability and theoretical insight, potentially sacrificing scalability for conceptual clarity and the ability to leverage attention-based architectures.

**Failure Signatures:** Poor performance when predictor correlations are unstable or when the orthonormal transformation fails to capture meaningful relationships; computational bottlenecks with high-dimensional data due to matrix inversion requirements.

**First Experiments:**
1. Test the framework on a simple linear dataset with known correlation structure to verify the attention interpretation
2. Compare computational complexity against standard OLS and attention mechanisms on synthetic data
3. Evaluate sensitivity to predictor scaling and correlation structure through controlled experiments

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Theoretical equivalence holds under specific conditions (orthonormal transformation) that may not generalize to complex real-world scenarios
- No clear demonstration of practical advantages over standard regression or attention methods
- Computational costs and scalability implications remain unexplored
- The information retrieval interpretation requires more rigorous justification beyond mathematical framework

## Confidence

| Claim | Confidence |
|-------|------------|
| Theoretical equivalence between OLS and attention mechanisms | Medium |
| Empirical performance claims | Medium |
| Practical implications and advantages of the framework | Low |

## Next Checks
1. Conduct scalability experiments to evaluate computational complexity and performance as dataset size and dimensionality increase, comparing against both standard OLS and attention mechanisms.
2. Test the framework on real-world datasets with known non-linear relationships and high-dimensional features to assess robustness beyond the synthetic data-generating processes used.
3. Perform ablation studies to isolate the contribution of the attention mechanism components versus the underlying OLS structure, determining whether the attention reformulation provides genuine advantages beyond standard regression approaches.