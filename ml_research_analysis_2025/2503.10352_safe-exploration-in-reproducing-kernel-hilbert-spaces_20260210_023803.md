---
ver: rpa2
title: Safe exploration in reproducing kernel Hilbert spaces
arxiv_id: '2503.10352'
source_url: https://arxiv.org/abs/2503.10352
tags:
- rkhs
- norm
- algorithm
- probability
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a safe Bayesian optimization algorithm that
  learns to estimate the RKHS norm from data, removing the need for a priori guessing.
  The algorithm uses a neural network and random RKHS functions to compute an upper
  bound on the RKHS norm with statistical guarantees, then integrates this bound into
  confidence intervals for safe exploration.
---

# Safe exploration in reproducing kernel Hilbert spaces

## Quick Facts
- arXiv ID: 2503.10352
- Source URL: https://arxiv.org/abs/2503.10352
- Reference count: 40
- Primary result: Safe Bayesian optimization algorithm that learns RKHS norm from data, achieving improved safety and scalability compared to SafeOpt

## Executive Summary
This paper addresses a fundamental limitation in safe Bayesian optimization (BO): the need to know the RKHS norm of the objective function a priori. The authors propose an algorithm that learns this norm from data using random RKHS functions and neural networks, enabling statistically guaranteed safe exploration without prior knowledge. The method is extended with a local notion of RKHS norm to improve scalability and exploration in high dimensions.

## Method Summary
The algorithm combines Gaussian process posterior estimates with confidence intervals that incorporate learned RKHS norm bounds. It uses random RKHS functions that interpolate observed data points to compute an upper bound on the true RKHS norm with statistical guarantees. This bound is then integrated into the confidence intervals to ensure safety. The method is further extended with local RKHS norms computed over adaptive cubes, enabling more aggressive exploration and better scalability to higher dimensions.

## Key Results
- Algorithm achieves 100% safety on synthetic and real-world control tasks
- Local RKHS approach enables safe exploration in up to 8 dimensions
- Estimated RKHS norms provide over-approximations with violation probability γ×κ (e.g., 0.1% for γ=0.1, κ=0.01)
- Outperforms SafeOpt in both safety and objective value across benchmark problems

## Why This Works (Mechanism)

### Mechanism 1
Random RKHS functions provide a statistically guaranteed over-estimate of the true RKHS norm with probability at least 1-γ and confidence 1-κ. Generate m random RKHS functions that interpolate observed data points with σ-sub-Gaussian noise. The first t coefficients are determined by interpolation; remaining coefficients and center points are sampled i.i.d. from uniform distributions. Sort by RKHS norm and select Bt from the (m-r)th function, where r is determined by the PAC condition (1-γ)^(m-1)(1+γ(m-1)) ≤ κ. The scenario approach (Theorem 2.1, Campi & Garatti 2011) guarantees this satisfies the chance constraint. Core assumption: Assumption 2—the random RKHS functions ρt,j and ground truth f are i.i.d. samples from the same (potentially unknown) probability space.

### Mechanism 2
Over-estimated RKHS norms replace known bounds in confidence intervals while retaining theoretical guarantees. Construct confidence intervals Qt(a) = μt(a) ± βtσt(a) where βt incorporates the estimated Bt. The proof constructs a product probability space combining: (i) epistemic uncertainty from random RKHS generation (measure P1) and (ii) aleatoric uncertainty from measurement noise (measure P2). By independence, P[(Bt, Et)] = P1[Bt] × P2[Et] = (1-γ)(1-δ). Core assumption: Assumption 1—kernel is symmetric, positive definite, continuous; action sequence is adapted to filtration Ft-1; noise is σ-sub-Gaussian.

### Mechanism 3
Local RKHS norms enable more aggressive exploration and better scalability than global norms. Define N local cubes of widths (1,...,N)·Δ around each sample. Compute local RKHS norms ||fc||k restricted to each cube Ac. Since local norms can be substantially smaller than global norms in smooth regions, confidence intervals become tighter. Discretization is performed separately per cube, reducing per-cube dimensionality and enabling exploration in higher dimensions (up to 8D in experiments). Core assumption: Each local function fc: Ac → R defined by restriction remains in the RKHS with finite norm.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: The entire theoretical framework assumes f ∈ Hk with bounded norm ||f||k. Understanding that this encodes smoothness through the kernel is essential.
  - Quick check question: Can you explain why ||f||k = √(Σαsαtk(xs,xt)) quantifies function complexity?

- Concept: Scenario approach for chance-constrained optimization
  - Why needed here: The PAC guarantees rely on sampling-and-discarding from Campi & Garatti 2011. Without this, the norm estimation has no theoretical backing.
  - Quick check question: Given m samples and violation probability γ, what's the minimum r such that the solution violates at most r constraints?

- Concept: Filtration and supermartingale theory
  - Why needed here: Corollary 1 lifts single-iteration guarantees to all iterations via supermartingale stopping time arguments (Abbasi-Yadkori 2011).
  - Quick check question: Why does Bt ≤ Bt-1 ensure the sequence {Bt} forms a supermartingale?

## Architecture Onboarding

- Component map:
Algorithm 4 (Localized Safe BO) -> Algorithm 2 (Sample Acquisition) -> GP posterior (μt, σt) from equations (2-3) -> Algorithm 3 (RKHS Norm Estimation) -> RNN branch (estimates ||f||k from sequences) + Random RKHS functions (m functions via pre-RKHS approach) -> Confidence intervals Qt via equation (4) -> Safe set St via equation (6) + Local cubes: Ct = {0,...,t·N} with adaptive sub-domains

- Critical path:
1. Initialize with safe set S0 (required input—must contain at least one safe point)
2. For each local cube c ∈ Ct: compute local domain Ac, local samples, local Bt via Algorithm 3
3. Select at+1 = argmax uncertainty across all cubes
4. Execute experiment, update GP, repeat

- Design tradeoffs:
| Parameter | Low Value | High Value |
|-----------|-----------|------------|
| m (random functions) | Tighter bounds, weaker guarantees (κ ↑) | Stronger guarantees, slower computation |
| γ (violation prob.) | Larger r, tighter Bt | Smaller r, looser Bt |
| N (local cubes) | More global, less scalable | More local, better exploration |
| Δ (cube width) | Aggressive local exploration | Conservative, misses local patterns |

- Failure signatures:
1. Empty safe set: Bt too large → all points classified unsafe → premature stopping (Figure 8)
2. Safety violations: Bt underestimated → confidence intervals don't contain f → unsafe experiments (Figure 1, bottom)
3. Exploration stall: N=0 (no locality) → coarse discretization in high dimensions → empty safe sets
4. Assumption 2 violation: Ground truth has fundamentally different structure than random functions → guarantees void (Section 6 acknowledges this limitation)

- First 3 experiments:
1. Reproduce Figure 5 toy example: Generate synthetic f ∈ Hk with known ||f||k = 5, 1000 random center points. Compare Algorithm 4 vs SafeOpt with B≡25 (conservative) and B≡1 (unsafe). Verify: (a) your implementation finds optimum safely, (b) SafeOpt exhibits predicted failure modes. Hyperparameters: γ=0.1, κ=0.01, m=1000, σ=0.01, N=5, Δ=0.1.
2. Validate PAC guarantees (Figure 4): Generate 200 RKHS functions with norms uniformly in [1,10]. For each, run Algorithm 3 for 50 iterations and track Bt/||f||k ratio. Expected: only ~2/200 underestimations (matching γ×κ = 0.001 theoretical rate). Check: Does the ratio decrease with more samples as Figure 2 suggests?
3. Ablation on Assumption 2 sensitivity: Generate ground truth f with non-uniform coefficient decay (e.g., exponential vs polynomial). Compare estimation accuracy vs uniform-coefficient f. This tests the practical robustness of Assumption 2. Report: ratio Bt/||f||k and safety violation count for each decay pattern.

## Open Questions the Paper Calls Out
- Investigating regret bounds: The paper proves safety with high probability but does not analyze convergence rate or cumulative regret relative to optimal safe policy.
- Disentangling constraints from reward function: Currently, safety is defined as a threshold on the reward function, whereas many problems require separate, independent safety constraints.
- Relaxing the assumption that ground truth and random RKHS functions share the same probability space: This strong assumption may not hold for real-world functions with structured patterns.

## Limitations
- Assumption 2 (ground truth and random RKHS functions from same distribution) is strong and may not hold for structured real-world functions
- Local RKHS approach lacks theoretical guarantees and may fail for functions with discontinuities at cube boundaries
- No theoretical regret bounds provided for the algorithm

## Confidence
- High confidence: Theorem 2 (PAC guarantees for random RKHS norm estimation) and Corollary 1 (safety preservation across all iterations)
- Medium confidence: Algorithm 3 implementation details and computational complexity claims
- Low confidence: Assumption 2 practical validity and local RKHS theoretical guarantees

## Next Checks
1. Assumption 2 robustness test: Generate ground truth functions with non-uniform coefficient decay (exponential vs polynomial) and measure estimation accuracy and safety violations compared to uniform-coefficient functions.
2. Local RKHS boundary sensitivity: Test the local approach on functions with sharp transitions or discontinuities at cube boundaries to quantify failure modes when local smoothness assumptions break.
3. Scalability verification: Implement the algorithm for 8D problems and measure wall-clock time per iteration to confirm the claimed O(n²N) complexity where n is the number of samples per cube.