---
ver: rpa2
title: Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning
arxiv_id: '2505.02576'
source_url: https://arxiv.org/abs/2505.02576
tags:
- problem
- solution
- word
- each
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Recursive Decomposition with Dependencies
  (RDD), a divide-and-conquer framework for solving reasoning tasks with large language
  models. RDD recursively decomposes problems into sub-tasks, supports sub-task dependencies
  to form a directed acyclic graph, and includes an error recovery mechanism.
---

# Recursive Decomposition with Dependencies for Generic Divide-and-Conquer Reasoning

## Quick Facts
- **arXiv ID:** 2505.02576
- **Source URL:** https://arxiv.org/abs/2505.02576
- **Reference count:** 40
- **Primary result:** Recursive Decomposition with Dependencies (RDD) outperforms Chain-of-Thought and Least-to-Most prompting with self-consistency on two synthetic benchmarks with six difficulty levels each, while reducing computational overhead.

## Executive Summary
This paper introduces Recursive Decomposition with Dependencies (RDD), a divide-and-conquer framework for solving reasoning tasks with large language models. RDD recursively decomposes problems into sub-tasks, supports sub-task dependencies to form a directed acyclic graph, and includes an error recovery mechanism. Unlike prior methods, it requires minimal supervision, can be applied generically to new tasks without task-specific examples, and reduces computational overhead. Experiments on two benchmarks with six difficulty levels each show that RDD outperforms state-of-the-art baselines like Chain-of-Thought and Least-to-Most prompting with self-consistency in a compute-matched setting, especially as task complexity increases. Additionally, RDD reduces execution time and context window usage compared to baselines.

## Method Summary
The RDD framework recursively decomposes problems into sub-tasks with dependencies, forming a directed acyclic graph. It uses a scheduler (BFS for decomposition, DFS for solving) to manage execution, a parser to extract sub-problem IDs and dependencies, and a prompt constructor that varies based on phase (Decompose, Unit-Solve, Merge). The framework operates with minimal supervision, using generic meta-task demonstrations instead of task-specific examples. It includes an error recovery mechanism in the merge step that allows the model to fix mistakes or solve the parent problem directly if sub-solutions are erroneous.

## Key Results
- RDD outperforms Chain-of-Thought and Least-to-Most prompting with self-consistency on synthetic benchmarks as task complexity increases
- The framework reduces computational overhead, execution time, and context window usage compared to baselines
- RDD shows a clear transition point where it becomes superior to direct solving (n* ≈ 20-50 depending on task)
- Error recovery mechanism successfully handles formatting errors in sub-solutions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Recursive decomposition improves accuracy on complex tasks by reducing the effective difficulty of individual LLM calls.
- **Mechanism:** The framework estimates a "transition point" (n*) where direct solving accuracy drops below the accuracy of the decomposed path. By splitting a problem of difficulty n into sub-problems of difficulty n_i (where n_i << n), the model operates in a higher-accuracy regime for each step.
- **Core assumption:** The LLM is capable of generating a valid decomposition and aggregation strategy for the specific problem class.
- **Evidence anchors:**
  - [Section 2.1]: Defines the transition point n* where φ_RDD(c_0, n_0) ≥ φ_u(c_0, n_0).
  - [Section 3.1]: Shows RDD underperforms CoT for n_0 < 20 but outperforms it significantly for n_0 ≥ 50.
  - [Corpus]: The CARD framework similarly argues that decomposition must be adapted to problem complexity to be effective.

### Mechanism 2
- **Claim:** Isolating sub-problems into separate context windows mitigates the "lost in the middle" phenomenon and quadratic token scaling.
- **Mechanism:** RDD restricts the prompt context to only the current sub-problem and generic examples, explicitly excluding ancestor history. This forces the decomposition step to create self-contained sub-tasks.
- **Core assumption:** Sub-problems can be formulated as self-contained units without requiring the full historical trace of the root problem.
- **Evidence anchors:**
  - [Section 2.2]: "When prompting for a decomposition... we do not include the history of ancestor problem descriptions."
  - [Section 3.5]: Hypothesis 4 is confirmed; RDD reduces average tokens per generation.
  - [Corpus]: The ToM paper corroborates that splitting contexts (MapReduce style) aids long-context reasoning.

### Mechanism 3
- **Claim:** The merging step acts as an error-recovery layer, allowing the system to "heal" from malformed sub-solutions.
- **Mechanism:** If sub-solutions are detected as erroneous, the merge prompt instructs the model to fix mistakes or solve the parent directly. This effectively elevates the merge step to a "supervisor" role.
- **Core assumption:** The error in a sub-solution is detectable by the LLM given the parent problem description.
- **Evidence anchors:**
  - [Section 3.4]: "If you find any mistakes in the sub-solutions, you can fix the mistakes while you merge."
  - [Appendix H]: Provides an example where a malformed sub-problem is bypassed by the root merge step solving the problem directly.

## Foundational Learning

- **Concept:** Directed Acyclic Graphs (DAGs)
  - **Why needed here:** RDD moves beyond tree-based decomposition to DAGs to handle dependencies (e.g., Sub-task B requires the output of Sub-task A).
  - **Quick check question:** If Task A depends on Task B, and Task B depends on Task A, can RDD solve this? (Answer: No, this is a cycle, violating the DAG constraint).

- **Concept:** Breadth-First Search (BFS) vs. Depth-First Search (DFS)
  - **Why needed here:** The paper specifies a hybrid scheduling strategy: BFS for decomposition (expanding the graph) and DFS for solving (executing dependencies).
  - **Quick check question:** Which traversal method is used to ensure all dependencies of a sub-task are resolved before attempting to solve the sub-task itself?

- **Concept:** Compute-Matched Evaluation
  - **Why needed here:** To fairly evaluate RDD against baselines like CoT+Self-Consistency, the paper matches computational resources (tokens/calls).
  - **Quick check question:** Why is comparing a single RDD run against a single CoT run insufficient for proving efficiency gains in this context?

## Architecture Onboarding

- **Component map:**
  - User Input → Decompose (Check if Unit) → Schedule (Build DAG) → Unit-Solve (Leaf nodes) → Embed (Substitute {ID} with solutions) → Merge (Aggregate results)

- **Critical path:** User Input → **Decompose** (Check if Unit) → **Schedule** (Build DAG) → **Unit-Solve** (Leaf nodes) → **Embed** (Substitute {ID} with solutions) → **Merge** (Aggregate results)

- **Design tradeoffs:**
  - **Generic vs. Specific Examples:** The paper uses generic examples to prove zero-shot transfer, but performance degrades compared to task-specific few-shotting.
  - **Parallelism vs. Dependencies:** Independent sub-problems can be solved in parallel, but introducing dependencies forces sequential execution.

- **Failure signatures:**
  - **Infinite Recursion:** The model fails to identify a "unit problem" and keeps decomposing indefinitely.
  - **Context Starvation:** The decomposition step strips necessary context, causing the unit-solver to fail.
  - **Error Propagation:** A silent hallucination in a leaf node propagates up the DAG.

- **First 3 experiments:**
  1. **Find the Transition Point:** Run RDD vs. CoT on a simple list manipulation task with increasing list sizes (5, 10, 20, 50 items) to identify the specific difficulty n* where RDD becomes superior.
  2. **Dependency Stress Test:** Implement the "Length Reversal" task (Map → Reverse). Verify that the parser correctly substitutes {P-1} with the output of the mapping step.
  3. **Ablation on Error Recovery:** Run a task prone to formatting errors with and without the "If you find any mistakes..." instruction in the merge prompt.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantifiable speedup when implementing RDD with parallel execution of independent sub-problems?
- Basis in paper: [explicit] The conclusion explicitly states that future work should "quantify the speedup achieved by a parallelized implementation of RDD."
- Why unresolved: While the paper theoretically claims parallelizability, the empirical results report total execution time for a sequential execution flow.
- What evidence would resolve it: A comparative benchmark measuring wall-clock time between the current sequential scheduler and a parallelized implementation.

### Open Question 2
- Question: How can the performance transition point (where RDD outperforms baselines) be predicted for novel problem classes?
- Basis in paper: [inferred] The paper identifies a "transition point" n* in difficulty where RDD becomes superior to Chain-of-Thought, but also notes in the Limitations that CoT wins before this point.
- Why unresolved: The transition points are observed empirically post-hoc for specific tasks but no method is provided to predict them a priori.
- What evidence would resolve it: A theoretical model or heuristic that accurately estimates the difficulty threshold n* for a new task before running the full evaluation.

### Open Question 3
- Question: Can alternative implementations of the unit-problem classifier improve the reliability of the stopping criteria?
- Basis in paper: [explicit] The conclusion suggests exploring "alternative implementations of the unit-problem classifier" as a direction for future work.
- Why unresolved: The current implementation relies on the same LLM used for solving, potentially compounding errors if the model fails to recognize a unit problem.
- What evidence would resolve it: An ablation study comparing the LLM-based classifier against external heuristics or smaller, fine-tuned classifiers for detecting unit problems.

## Limitations
- Parser robustness for malformed LLM outputs is not specified, which could significantly impact performance
- Error recovery mechanism primarily handles obvious formatting errors rather than subtle logical flaws
- Scalability to real-world tasks with complex, implicit dependencies remains unproven
- The extent of true generality across radically different domains without task-specific adaptation is uncertain

## Confidence

**High Confidence:**
- The RDD framework architecture and its components are clearly specified
- Experimental results showing RDD outperforming baselines on synthetic benchmarks are reproducible
- Core claim that recursive decomposition reduces effective task difficulty for complex problems (n_0 ≥ 50) is well-supported

**Medium Confidence:**
- Theoretical justification for the transition point n* is mathematically sound but practical identification may vary across domains
- Claim that RDD mitigates "lost in the middle" phenomenon is supported but improvement extent depends on task context reliance

**Low Confidence:**
- Scalability to real-world, open-domain tasks with complex, implicit dependencies is unproven
- Robustness of error recovery mechanism against subtle, propagating logical errors is not thoroughly validated

## Next Checks

1. **Parser Robustness Test:** Implement and stress-test the parser with malformed LLM outputs, including missing brackets, incorrect dependency syntax, and nested sub-problems. Measure the failure rate and develop fallback strategies for parsing errors.

2. **Silent Error Propagation Analysis:** Design a task where sub-problems can have subtle logical errors (e.g., off-by-one mistakes in arithmetic). Run RDD with and without the error recovery mechanism and analyze whether errors propagate to the final answer. Quantify the "silent error" rate.

3. **Real-World Transfer Benchmark:** Apply RDD to a non-synthetic, real-world reasoning task (e.g., multi-step scientific problem-solving or complex code generation) that requires both decomposition and handling of implicit dependencies. Compare its performance and efficiency against standard prompting methods to validate its claimed generality.