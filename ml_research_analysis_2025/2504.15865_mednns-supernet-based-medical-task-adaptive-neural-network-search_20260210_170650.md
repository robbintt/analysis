---
ver: rpa2
title: 'MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search'
arxiv_id: '2504.15865'
source_url: https://arxiv.org/abs/2504.15865
tags:
- medical
- dataset
- datasets
- neural
- meta-space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedNNS addresses the challenge of adapting deep learning models
  to medical imaging tasks, where different tasks require specialized architectures
  and weight initialization significantly impacts convergence and performance. The
  framework constructs a meta-space encoding datasets and models based on their joint
  performance, leveraging Supernetworks to expand the model zoo by 51x compared to
  previous state-of-the-art methods.
---

# MedNNS: Supernet-based Medical Task-Adaptive Neural Network Search

## Quick Facts
- arXiv ID: 2504.15865
- Source URL: https://arxiv.org/abs/2504.15865
- Authors: Lotfi Abdelkrim Mecharbat; Ibrahim Almakky; Martin Takac; Mohammad Yaqub
- Reference count: 28
- Primary result: MedNNS achieves 1.7% average accuracy improvement over ImageNet pre-trained models while converging faster

## Executive Summary
MedNNS introduces a framework for adapting deep learning models to medical imaging tasks by jointly optimizing architecture and weight initialization. The method constructs a meta-space that encodes relationships between datasets and models based on their joint performance, leveraging Supernetworks to create a massive model zoo (720k+ pairs) that is 51x larger than previous state-of-the-art methods. By incorporating rank loss and Fréchet Inception Distance (FID) loss, MedNNS captures inter-model and inter-dataset relationships more accurately than previous approaches, resulting in significantly better model retrieval performance.

## Method Summary
MedNNS constructs a meta-space for model retrieval by training Supernetworks on medical datasets and extracting subnetworks to build a massive model zoo. The framework uses three encoders (model, dataset, and performance) trained with a composite loss function combining performance prediction, rank preservation, and FID-based dataset similarity. During inference, a new dataset is encoded into this space and the closest model embedding is retrieved. The method employs a two-stage training scheme for Supernetworks and uses three retrieval protocols (T1, T5, T10) to balance speed and accuracy.

## Key Results
- Achieved 1.7% average accuracy improvement over ImageNet pre-trained models
- Converged substantially faster than both ImageNet pre-trained models and state-of-the-art NAS methods
- Constructed model zoo 51x larger than previous state-of-the-art methods (720k+ pairs vs previous benchmarks)

## Why This Works (Mechanism)
MedNNS works by creating a joint embedding space where both datasets and models are represented as vectors. The key innovation is the composite loss function that aligns models with datasets they perform well on (performance loss), preserves ranking relationships between models (rank loss), and captures dataset similarity through FID loss. This multi-signal approach creates a more informative meta-space than previous methods that relied solely on performance metrics. The Supernetwork architecture enables efficient generation of diverse model architectures with inherited weights, while the rank loss ensures the meta-space preserves the true performance hierarchy among models.

## Foundational Learning
- **Supernetworks & Weight Sharing**
  - Why needed here: MedNNS relies on a Supernetwork to create a massive model zoo (720k+ pairs). Understanding that one large network can be trained to contain many smaller, valid subnetworks (via masks) is critical to grasping the method's efficiency.
  - Quick check question: How does extracting a subnetwork from a Supernetwork differ from training a standalone network from scratch?

- **Meta-Learning Spaces (Metric Learning)**
  - Why needed here: The core of MedNNS is the construction of a shared embedding space for both datasets and models. Knowledge of contrastive or metric learning (how to pull related items closer and push unrelated items apart) is required to understand the function of the loss terms.
  - Quick check question: In a meta-space, what does the distance between a dataset embedding and a model embedding represent?

- **Fréchet Inception Distance (FID)**
  - Why needed here: FID is used as a key signal for dataset similarity. Understanding that FID compares the statistical distribution of features (typically from an Inception network) between two datasets is necessary to interpret the paper's claims on why certain source datasets are better for transfer.
  - Quick check question: A lower FID score between two datasets indicates what about their relationship? How is this used in MedNNS?

## Architecture Onboarding
- **Component map:**
  Supernetwork Trainer -> Model Zoo -> Encoders (Em, Ed) -> Meta-Space Optimizer -> Inference Retriever

- **Critical path:**
  1. Train Supernetworks on base medical datasets
  2. Extract subnetworks to build the zoo
  3. Train the encoders and performance predictor using the composite loss function (L_perf + L_rank + L_FID)
  4. For a new dataset, generate its embedding and perform nearest-neighbor search in meta-space

- **Design tradeoffs:**
  - Speed vs. Performance (T1, T5, T10): T1 is fastest but T10 (select top 10, train 1 epoch, pick best) yields higher accuracy at higher computational cost
  - Generalization vs. Specificity: Powerful for datasets with similar counterparts in zoo, fails when new dataset is too dissimilar from anything in zoo

- **Failure signatures:**
  - Negative Transfer: Poor results if new dataset's FID to selected source dataset is misleadingly low or source task features aren't genuinely reusable
  - Rank Corruption: If Supernetwork's subnetwork performance rankings don't match standalone training, meta-space will be built on incorrect signals
  - Domain Mismatch: TissueMNIST case where dataset is outlier in feature space

- **First 3 experiments:**
  1. Reproduce Baseline Ablation: Train meta-space using only contrastive loss vs proposed rank+FID loss on single dataset to verify performance gain
  2. Sensitivity Analysis on T_k: Implement MedNNS-T5 and MedNNS-T10 protocols to quantify trade-off between training k models for 1 epoch and accuracy improvement
  3. Out-of-Distribution Test: Query meta-space with dataset known to be very different from training zoo to observe degradation and validate FID-based alignment hypothesis

## Open Questions the Paper Calls Out
- Can expanding the diversity of datasets in the meta-space effectively resolve generalization failures observed in target datasets with high feature distribution dissimilarity (e.g., TissueMNIST)?
- How can hardware constraints (e.g., latency, memory) be integrated into the MedNNS search process without degrading the joint optimization of architecture and weight initialization?
- Does the reliance on estimated performance $\hat{P}$ (from Supernetwork weight sharing) introduce systematic bias in the meta-space ranking compared to true performance $P$?

## Limitations
- Effectiveness is contingent on having source datasets in meta-space that are sufficiently similar to target dataset
- Method fails when new dataset is too dissimilar from anything in the model zoo (TissueMNIST failure case)
- Missing critical hyperparameter details (MLP encoder architectures, β and σ values) make precise reproduction challenging

## Confidence
- **High** confidence in core architectural contribution and empirical improvement claims (1.7% accuracy gain)
- **Medium** confidence in convergence speed claims due to relative nature of training time comparisons
- **Low** confidence in ability to reproduce exact numerical results due to missing hyperparameter details

## Next Checks
1. Ablation on Loss Components: Train meta-space optimizer using only performance loss versus full composite loss on subset of MedMNIST datasets to quantify contribution of rank and FID losses
2. Out-of-Distribution Robustness Test: Construct synthetic dataset with very high FID score to all real medical datasets and measure performance degradation
3. Sensitivity Analysis on T_k Protocols: Benchmark all three retrieval protocols (T1, T5, T10) on single dataset to measure trade-off between computational cost and accuracy improvement