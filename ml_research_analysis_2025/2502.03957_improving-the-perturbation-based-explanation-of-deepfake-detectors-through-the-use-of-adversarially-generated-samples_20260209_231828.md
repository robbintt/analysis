---
ver: rpa2
title: Improving the Perturbation-Based Explanation of Deepfake Detectors Through
  the Use of Adversarially-Generated Samples
arxiv_id: '2502.03957'
source_url: https://arxiv.org/abs/2502.03957
tags:
- explanation
- image
- deepfake
- methods
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach to improve the explanations
  produced by perturbation-based methods for deepfake detectors. The core idea is
  to use adversarially-generated samples of input deepfake images to form perturbation
  masks for inferring the importance of different input features.
---

# Improving the Perturbation-Based Explanation of Deepfake Detectors Through the Use of Adversarially-Generated Samples

## Quick Facts
- arXiv ID: 2502.03957
- Source URL: https://arxiv.org/abs/2502.03957
- Authors: Konstantinos Tsigos; Evlampios Apostolidis; Vasileios Mezaris
- Reference count: 40
- This paper introduces a new approach to improve the explanations produced by perturbation-based methods for deepfake detectors by using adversarially-generated samples to form perturbation masks

## Executive Summary
This paper addresses the challenge of interpreting deepfake detection models by improving perturbation-based explanation methods. The core innovation is using adversarially-generated samples - created specifically to fool the detector into classifying deepfakes as real - as the basis for generating perturbation masks. These masks are then used by state-of-the-art explanation methods (LIME, SHAP, SOBOL, and RISE) to identify which regions of deepfake images are most important for detection. The approach is evaluated on the FaceForensics++ dataset using a state-of-the-art deepfake detection model, demonstrating improved explanation quality through both quantitative metrics and qualitative visual analysis.

## Method Summary
The paper proposes a novel approach to improve perturbation-based explanations for deepfake detectors by generating adversarial samples that can fool the detector into misclassifying deepfakes as real. Using Natural Evolution Strategies, the method generates adversarial perturbations that maximize the likelihood of flipping the detector's decision. These adversarial samples are then used to create perturbation masks for four state-of-the-art explanation methods (LIME, SHAP, SOBOL, and RISE). The masks are designed to highlight regions of deepfake images that are most important for the detector's decision-making process. The modified explanation methods are evaluated on the FaceForensics++ dataset using a state-of-the-art deepfake detection model, with quantitative assessments showing improved performance across multiple metrics, particularly for the modified LIME method which achieved a 10.5% average drop in detection accuracy.

## Key Results
- The proposed perturbation approach mostly improves the performance of explanation methods, with modified LIME achieving the best results
- The method leads to a 10.5% average drop in detection accuracy for the modified LIME approach
- Qualitative analysis demonstrates that modified explanation methods more accurately demarcate manipulated image regions

## Why This Works (Mechanism)
The paper does not explicitly provide a mechanism section explaining why the approach works.

## Foundational Learning

### Natural Evolution Strategies
**Why needed:** Required for generating adversarial samples that can fool the deepfake detector without relying on gradient information
**Quick check:** Verify that NES can effectively explore the input space to find adversarial perturbations that flip detection decisions

### Perturbation-based explanation methods
**Why needed:** These methods form the foundation for understanding which image regions the detector focuses on
**Quick check:** Confirm that LIME, SHAP, SOBOL, and RISE can effectively use the generated adversarial perturbations as mask bases

### Adversarial sample generation for image classification
**Why needed:** Critical for creating perturbations that specifically target the detector's weaknesses
**Quick check:** Validate that generated adversarial samples successfully fool the detector while remaining perceptually similar to originals

## Architecture Onboarding

### Component Map
Deepfake Detector -> Adversarial Generator (NES) -> Perturbation Mask Generator -> Explanation Methods (LIME, SHAP, SOBOL, RISE) -> Evaluation Metrics

### Critical Path
The most critical path is: Deepfake Detector output → Adversarial sample generation (NES) → Perturbation mask creation → Explanation method application → Evaluation of explanation quality

### Design Tradeoffs
The use of NES for adversarial generation trades computational efficiency for gradient-free operation, which may be necessary for black-box detectors but limits scalability compared to gradient-based methods.

### Failure Signatures
If adversarial samples fail to fool the detector, the perturbation masks will not effectively highlight important regions, resulting in poor explanation quality and low detection accuracy drops.

### 3 First Experiments
1. Test adversarial sample generation success rate on a small subset of the FaceForensics++ dataset
2. Validate that perturbation masks generated from successful adversarial samples improve explanation localization compared to random masks
3. Compare explanation quality metrics between modified and unmodified versions of each explanation method

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation relies heavily on quantitative metrics computed through random sampling, which may not fully capture explanation stability across different data distributions
- The study focuses exclusively on the FaceForensics++ dataset and a single deepfake detection model, limiting generalizability
- Natural Evolution Strategies may not be the most efficient approach compared to gradient-based methods, potentially limiting scalability

## Confidence
**Medium** - The quantitative improvements in explanation metrics are demonstrated, but the practical significance for real-world interpretability remains unclear
**High** - The technical implementation of adversarial perturbation generation is clearly described and follows established evolutionary strategies
**Medium** - The comparative advantage over existing perturbation methods is unclear due to limited ablation studies and lack of gradient-based comparisons

## Next Checks
1. Conduct stability analysis by evaluating explanation consistency across multiple runs with different random seeds and data subsets
2. Test the approach on additional deepfake datasets (e.g., DFDC, Celeb-DF) and detection models to assess generalizability
3. Compare computational efficiency and explanation quality against gradient-based adversarial perturbation methods to establish practical advantages