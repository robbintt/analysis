---
ver: rpa2
title: Complex Ontology Matching with Large Language Model Embeddings
arxiv_id: '2502.13619'
source_url: https://arxiv.org/abs/2502.13619
tags:
- embeddings
- similarity
- embedding
- matching
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper integrates large language models (LLMs) into complex
  ontology matching to improve expressiveness beyond simple entity correspondences.
  The approach uses SPARQL queries to guide alignment, identifying instance sub-graphs
  in source and target knowledge graphs and matching them based on semantic similarity.
---

# Complex Ontology Matching with Large Language Model Embeddings

## Quick Facts
- arXiv ID: 2502.13619
- Source URL: https://arxiv.org/abs/2502.13619
- Reference count: 40
- Primary result: 45% F-measure improvement over baseline using GritLM-7B embeddings

## Executive Summary
This paper presents an approach for complex ontology matching that leverages large language model (LLM) embeddings to improve semantic expressiveness beyond simple entity correspondences. The method uses SPARQL queries to guide alignment, identifying instance sub-graphs in source and target knowledge graphs and matching them based on semantic similarity. Three architectural modifications are proposed: label embedding similarity, embeddings of SPARQL queries, and subgraph embeddings. The approach is evaluated on the OAEI Conference benchmark using 15 different embedding models, showing significant improvements in both precision and F-measure compared to state-of-the-art complex matchers.

## Method Summary
The approach extends the CANARD architecture by integrating LLM embeddings for complex ontology matching. It processes ontologies through a pipeline that begins with pre-computing embeddings for all ontology labels using models like GritLM-7B, then executes SPARQL queries to identify subgraphs around common instances (or potential anchors via instance embeddings when explicit links are missing). The matching occurs through cosine similarity between query embeddings and subgraph label embeddings, with three settings tested: Label Embedding Similarity (LES), Embeddings of SPARQL Query (ESQ), and Subgraph Embeddings (SE). The system requires significant computational resources (24GB VRAM for large models) but produces expressive correspondences without requiring training or reference alignments.

## Key Results
- 45% F-measure improvement over baseline CANARD approach
- GritLM-7B achieved best performance: 0.68 precision and 0.68 F-measure in ESQ setting
- ESQ architecture outperformed LES and SE across most evaluation metrics
- The approach outperformed state-of-the-art complex matchers in both precision and F-measure

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing lexical string metrics with LLM-based embeddings better captures semantic equivalence in complex correspondences.
- **Mechanism:** The architecture replaces Levenshtein distance with cosine similarity between vector embeddings, allowing identification of synonyms and conceptually similar entities that lexical matching would miss.
- **Core assumption:** Semantic richness of LLM embeddings provides more robust similarity signals than surface-level string editing.
- **Evidence anchors:** 45% F-measure increase over baseline; LES replaces Levenshtein to filter false positives like "Review" vs "Reviewer"; GenOM and KROMA corpus support LLM integration trends.

### Mechanism 2
- **Claim:** Aggregating query embeddings before comparison (ESQ) creates more stable semantic centroids than comparing individual labels (LES).
- **Mechanism:** Averaging query labels into a single vector condenses user's knowledge need into unified semantic representation, reducing sparse or conflicting signals.
- **Core assumption:** Averaging token embeddings preserves collective semantic intent better than treating query components as isolated features.
- **Evidence anchors:** ESQ offers more contextual representation than LES; ESQ with GritLM-7B achieved highest precision (0.68) and F-measure (0.68); corpus provides weak direct evidence for aggregation strategies.

### Mechanism 3
- **Claim:** Using embeddings for instance linking (IE) improves precision by finding semantic anchors where explicit `owl:sameAs` links are missing.
- **Mechanism:** Computes similarity matrix between source and target instances, allowing system to anchor matching process even when data is heterogeneous.
- **Core assumption:** Sufficient lexical or semantic overlap in instance labels for LLM to identify highest similarity candidates above threshold.
- **Evidence anchors:** Cross-cosine similarity finds corresponding instances when standard predicates absent; IE improves precision while reducing query-based metrics in some models; corpus papers support LLM use for alignment tasks.

## Foundational Learning

- **Concept: Complex Ontology Matching vs. Simple Matching**
  - **Why needed here:** Paper targets "expressive correspondences" (e.g., relations involving logical constructors), not just 1:1 entity matching.
  - **Quick check question:** Can you explain the difference between matching `Author ≡ Writer` (simple) and `Accepted_Paper ≡ Paper ⊓ ∃ hasDecision.Acceptance` (complex)?

- **Concept: SPARQL Query as Alignment Need**
  - **Why needed here:** System is query-driven; uses SPARQL results to define "surroundings" or subgraphs to be matched, rather than matching whole graph blindly.
  - **Quick check question:** How does limiting matching scope to SPARQL query results (the "alignment need") affect system's ability to generalize?

- **Concept: Embedding Aggregation (Pooling)**
  - **Why needed here:** Paper compares architectures based on when and how embeddings are aggregated (LES vs. ESQ vs. SE).
  - **Quick check question:** Why does paper suggest averaging embeddings (ESQ) performs better than creating complex subgraph embeddings (SE) for this specific task?

## Architecture Onboarding

- **Component map:** Input Ontologies + SPARQL Queries -> Encoder (LLM) -> Tokenizer -> Last Hidden State -> Mean Pooling -> Matcher (CANARD Core with IE, LES, ESQ) -> Output Complex Alignment

- **Critical path:**
  1. Pre-computation: Encode all ontology labels and cache them (requires significant RAM/VRAM, e.g., 24GB cited)
  2. Linking (IE): Execute SPARQL on Source -> Find anchors in Target via Embedding Similarity
  3. Subgraph Extraction: Retrieve triples/paths around anchors
  4. Similarity Calculation: Compare Query Embeddings (LES/ESQ) against Subgraph Embeddings

- **Design tradeoffs:**
  - LES vs. ESQ: LES (fine-grained) vs. ESQ (contextual). ESQ favored in results for higher precision
  - IE Threshold: High threshold = High Precision/Low Recall (risk of dropping valid links). Low threshold = Noise
  - Runtime vs. Accuracy: IE step drastically increases runtime (up to 2h+ in experiments) but necessary when explicit links are missing

- **Failure signatures:**
  - SE Performance Drop: Using Subgraph Embeddings (SE) yields lower F-measure due to noise from unweighted aggregation of complex structures
  - IE Stalls: If IE step runs for hours, similarity threshold is likely too low, resulting in exhaustive comparisons between source and target instances

- **First 3 experiments:**
  1. Baseline Validation: Replicate "LES" setting using smaller model (BERT-Int or Stella-base) to verify embedding pipeline functioning before scaling to 7B models
  2. Ablation on IE: Run matcher with explicit `owl:sameAs` links (skipping IE) vs. using IE to quantify precision/recall trade-off specific to your dataset
  3. Aggregation Comparison: Directly compare ESQ vs. SE on single ontology pair to confirm paper's finding that context aggregation (ESQ) outperforms structural aggregation (SE)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a purely T-Box (schema-based) strategy be developed to enable complex matching without relying on instance data (ABox) or user-provided SPARQL queries?
- **Basis in paper:** Conclusion states "The first direction for extension involves devising a purely T-Box strategy," acknowledging reliance on user needs expressed as SPARQL queries is a limitation.
- **Why unresolved:** Current approach is fundamentally ABox-based, requiring execution of SPARQL queries on instance data to identify subgraphs, which limits applicability when instance data is sparse or unavailable.
- **What evidence would resolve it:** Modified version of pipeline that successfully generates expressive correspondences using only schema elements (concepts, properties) without accessing or querying the ABox.

### Open Question 2
- **Question:** Can weighted aggregation techniques, such as Graph Neural Networks (GNNs), prevent semantic information loss observed when averaging embeddings for subgraph matching?
- **Basis in paper:** Conclusion suggests "exploring improved aggregation techniques for subgraphs may yield superior results." SE setting degraded performance because "combining embeddings without weight transformation... can cause embeddings to lose semantic information."
- **Why unresolved:** SE setting using simple averaging resulted in "poorest results" compared to label-based settings, indicating current aggregation methods introduce too much noise.
- **What evidence would resolve it:** Evaluation of SE setting utilizing attention mechanisms or GNNs for embedding aggregation, demonstrating F-measure exceeding current best-performing LES and ESQ settings.

### Open Question 3
- **Question:** Does fine-tuning Large Language Models or optimizing prompts used for entity embedding generation significantly improve performance over pre-trained models?
- **Basis in paper:** Conclusion lists "fine-tuning LLMs and delving deeper into prompts guiding creation of entity embeddings" as direction for future work.
- **Why unresolved:** Experiments utilized off-the-shelf pre-trained models from MTEB benchmark to ensure generalization, but potential benefits of domain-specific adaptation remain untested.
- **What evidence would resolve it:** Comparative analysis showing LLM fine-tuned on ontology structures or matching tasks outperforms best general-purpose models (e.g., GritLM-7B) identified in paper.

### Open Question 4
- **Question:** Can approach be extended to handle n-ary queries (variables ≥ 3) and SPARQL modifiers like CONSTRUCT or ASK?
- **Basis in paper:** Section 2 explicitly lists this as limitation: "Queries for approach are limited to unary and binary questions... approach does not deal with transformation functions or filters inside SPARQL queries."
- **Why unresolved:** Current architectural modifications (LES, ESQ, SE) were designed and evaluated strictly for unary and binary SELECT queries, leaving more complex query structures unsupported.
- **What evidence would resolve it:** Generalized matching algorithm capable of processing n-ary queries and evaluating performance on benchmark datasets containing higher-arity reference alignments.

## Limitations

- **Resource Intensive:** GritLM-7B requires ~24GB VRAM, limiting accessibility for many researchers
- **Query Dependency:** Approach fundamentally relies on user-provided SPARQL queries, making it unsuitable for automated matching scenarios
- **Threshold Optimization:** Best results appear to use post-hoc threshold optimization (0.5-1.0), raising questions about real-world deployment performance

## Confidence

- Claim 1 (LLM embeddings improve semantic matching): High - supported by consistent F-measure improvements across settings
- Claim 2 (ESQ architecture superiority): Medium - optimal in reported results but threshold-dependent
- Claim 3 (IE improves precision): Low - shows mixed results with precision/recall trade-offs

## Next Checks

1. Replicate baseline LES setting with smaller model (BERT-Int/Stella-base) to verify embedding pipeline functionality
2. Conduct ablation study comparing explicit owl:sameAs links vs. IE step to quantify precision/recall trade-offs
3. Test ESQ vs. SE settings on a single ontology pair to confirm context aggregation superiority for your specific use case