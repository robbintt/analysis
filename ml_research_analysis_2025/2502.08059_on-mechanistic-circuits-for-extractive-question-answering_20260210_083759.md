---
ver: rpa2
title: On Mechanistic Circuits for Extractive Question-Answering
arxiv_id: '2502.08059'
source_url: https://arxiv.org/abs/2502.08059
tags:
- context
- attention
- circuit
- language
- attribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extracts mechanistic circuits for the real-world task
  of extractive question-answering (QA) in large language models, providing insights
  into how models use context versus parametric memory. The authors design probe datasets
  and use causal mediation analysis to identify context-faithfulness and memory-faithfulness
  circuits.
---

# On Mechanistic Circuits for Extractive Question-Answering

## Quick Facts
- arXiv ID: 2502.08059
- Source URL: https://arxiv.org/abs/2502.08059
- Reference count: 40
- Primary result: Extracted mechanistic circuits reveal distinct pathways for context vs. parametric memory in extractive QA, enabling fast single-head attribution and model steering.

## Executive Summary
This paper extracts mechanistic circuits for the real-world task of extractive question-answering (QA) in large language models, providing insights into how models use context versus parametric memory. The authors design probe datasets and use causal mediation analysis to identify context-faithfulness and memory-faithfulness circuits. They find that a small set of attention heads in the context circuit performs reliable data attribution by default, enabling a fast attribution method called ATTNATTRIB. This method achieves state-of-the-art attribution accuracy across various extractive QA benchmarks using just one attention head in a single forward pass. Additionally, they show that using attributions as an extra signal during prompting improves extractive QA accuracy by up to 9% by steering models toward context faithfulness.

## Method Summary
The paper employs causal mediation analysis with activation patching to identify mechanistic circuits for extractive QA. They construct probe datasets with contexts containing answer tokens replaced by semantically similar (Dcopy) or unrelated tokens (Dmemory) to isolate context-faithfulness and memory-faithfulness behaviors. For each component (attention heads, MLPs), they measure the effect on answer probability when patching activations between clean and corrupted inputs. Components are greedily selected based on their causal impact until a threshold is met. The context-faithfulness circuit is then used to identify low-entropy attention heads that naturally perform data attribution, enabling the ATTNATTRIB method. Finally, they demonstrate that using these attributions as prompt augmentation can steer models toward context faithfulness.

## Key Results
- Identified distinct computational circuits for context-faithfulness (4-5 attention layers) versus memory-faithfulness (>15 attention layers) in extractive QA
- ATTNATTRIB achieves state-of-the-art attribution accuracy using a single attention head in one forward pass
- Incorporating attribution signals as prompt augmentation improves extractive QA accuracy by up to 9%
- Context-faithfulness circuit components are minimally overlapping with memory-faithfulness components

## Why This Works (Mechanism)

### Mechanism 1: Distinct Computational Circuits for Context vs. Parametric Memory
Language models use minimally overlapping subgraphs when answering from external context versus internal parametric memory. Causal mediation analysis reveals that context-faithfulness requires only 4–5 attention layers (~10 heads) for high metric scores (>0.95), whereas memory-faithfulness requires >15 layers (>30 heads). MLPs show minimal contribution to context-faithfulness in Vicuna/Phi-3 but are critical for memory retrieval.

### Mechanism 2: Low-Entropy Attention Heads Perform Attribution by Default
A sparse set of attention heads in the context-faithfulness circuit inherently attend to answer spans with low entropy, enabling single-head attribution. These heads exhibit peaky attention distributions concentrated on context tokens containing answers, with entropy inversely correlating with attribution accuracy.

### Mechanism 3: Attribution Signal Enables Steering Toward Context Faithfulness
Even when models answer from parametric memory, attribution heads maintain attention on perturbed answer tokens; using this signal as prompt augmentation improves context-faithfulness. When upweighting attention at answer token spans (scaling factor β=10) or mean-ablating memory-circuit MLPs, models switch from memory to context answers (92% and 68% success rates respectively).

## Foundational Learning

- **Causal Mediation Analysis (Activation Patching)**: The core methodology for identifying which components causally affect outputs by measuring probability changes when activations are swapped between clean and corrupted inputs.
  - Why needed here: Enables isolation of context-faithfulness versus memory-faithfulness pathways
  - Quick check question: When patching activation from a corrupted model (low answer probability) to a clean model, what does a high "score" (1 - P_answer) indicate about that component's importance?

- **Transformer Residual Stream Decomposition**: Understanding that logits decompose additively into contributions from attention heads and MLPs enables isolating individual component effects.
  - Why needed here: Allows attribution of output changes to specific attention heads
  - Quick check question: Why does the residual stream architecture allow attribution of output changes to specific attention heads?

- **Attention Entropy as Attribution Signal**: ATTNATTRIB relies on identifying low-entropy heads that concentrate attention on answer-bearing spans.
  - Why needed here: Low entropy indicates peaky attention distributions focused on answer tokens
  - Quick check question: What does high entropy in attention distribution suggest about the head's usefulness for attribution?

## Architecture Onboarding

- **Component map**: (Question, Context) → Tokenizer → Transformer Layers → Attention heads (32 heads/layer for 8B models) + MLP → Residual stream → Logits

- **Critical path**:
  1. Construct probe dataset (Dcopy with semantically similar answer replacements, Dmemory with distant replacements)
  2. Run hierarchical patching: Hierarchy-0 (direct effects on logits) → Hierarchy-1 (effects on Hierarchy-0 nodes)
  3. Identify low-entropy attention heads from context-faithfulness circuit
  4. Deploy ATTNATTRIB: extract max-attention span per generated token

- **Design tradeoffs**:
  - Single-head vs. multi-head attribution: Paper demonstrates single head suffices, but multi-hop QA shows degraded performance
  - Hierarchy-0 only vs. higher-order circuits: Hierarchy-1 showed limited practical utility (metric 0.71 for Llama-3-8B)
  - White-box requirement: Method requires parameter access; cannot apply to black-box APIs

- **Failure signatures**:
  - Random circuit yields answer probability ~0.04–0.08 (vs. extracted circuit)
  - Vicuna attribution degrades with longer contexts (2048 token limit)
  - Multi-hop reasoning questions: F1 drops to 0.47–0.51 (vs. single-hop 0.57–0.59)
  - Binary Yes/No questions: attribution F1 = 0.14 without supporting tokens

- **First 3 experiments**:
  1. Circuit validation: Ablate extracted context-faithfulness circuit on held-out datasets (NQ-Swap, Natural-Questions, HotPotRA); expect >70% accuracy drop vs. random circuit ablation.
  2. Attribution benchmarking: Compare ATTNATTRIB against self-attribution, iterative prompting, sentence similarity, and gradient methods on synthetic and NQ-Swap datasets; measure exact match accuracy.
  3. Steering evaluation: Augment prompts with attribution-derived signals; measure extractive QA accuracy improvement against baseline prompting and Context-aware Contrastive Decoding; expect 5–9% gains.

## Open Questions the Paper Calls Out

### Open Question 1
Do second-order circuit components (hierarchy-1 effects) provide additional utility for data attribution or model steering beyond hierarchy-0 components? The authors extracted hierarchy-1 components achieving a metric score of 0.71 for Llama-3-8B but found no specific utility for their applications. Demonstrating that second-order components enable improved attribution accuracy, steering capabilities, or mechanistic insights that hierarchy-0 components cannot provide would resolve this question.

### Open Question 2
Can probe datasets designed specifically for multi-hop questions yield circuits that substantially improve attribution performance on multi-hop QA tasks? ATTNATTRIB showed degraded but reasonable performance on multi-hop questions using a 0-hop probe dataset, but this hypothesis remains untested. Comparing attribution F1-scores on multi-hop QA using circuits extracted from multi-hop probe datasets versus 0-hop probe datasets would resolve this question.

### Open Question 3
Can circuit-based attribution methods be adapted for black-box LLMs without requiring white-box parameter access? The paper's approach requires internal attention head access, but deployment scenarios increasingly use API-only access to proprietary models. Developing and validating methods that approximate attention patterns or circuit behavior using only input-output query access would resolve this question.

### Open Question 4
How robust are the extracted context-faithfulness circuits across fundamentally different extractive QA domains (e.g., legal documents, medical records, code)? The probe dataset uses factual questions from the Known dataset with synthetically generated contexts, and validation tested on NQ-Swap, Natural-Questions, and HotPotRA—all general-domain datasets. Extracting circuits using domain-specific probe datasets and measuring overlap in circuit components; testing ATTNATTRIB attribution accuracy on specialized domain benchmarks would resolve this question.

## Limitations

- **Probe Dataset Dependency**: Methodology relies heavily on the assumption that Dcopy and Dmemory probe datasets successfully isolate context-faithfulness and memory-faithfulness behaviors.
- **Circuit Transferability**: Approach requires white-box access and may not transfer to other architectures, smaller models, or black-box APIs.
- **Attribution Reliability**: ATTNATTRIB's success depends on the presence of low-entropy attention heads and degrades on longer contexts, multi-hop reasoning, and binary questions.

## Confidence

- **High Confidence**: Distinct computational circuits exist for context-faithfulness versus memory-faithfulness (well-supported by causal mediation analysis showing different layer requirements).
- **Medium Confidence**: ATTNATTRIB achieves state-of-the-art attribution accuracy across various benchmarks (supported by comparisons to baseline methods but limited to synthetic and NQ-Swap datasets).
- **Low Confidence**: Incorporating attributions as prompt signals improves extractive QA accuracy by up to 9% (demonstrated but lacks comparison to alternative steering methods).

## Next Checks

1. **Cross-Dataset Generalization Test**: Apply extracted circuits and ATTNATTRIB to a diverse set of real-world extractive QA datasets (including those with mixed context/memory answering, longer contexts, and multi-hop reasoning) to validate robustness beyond synthetic and NQ-Swap datasets.

2. **Black-Box Transferability Evaluation**: Test whether ATTNATTRIB can be approximated using only attention outputs (without parameter access) on models like GPT-4 or Claude, or whether the method requires architectural modifications for black-box deployment.

3. **Steering Component Ablation**: Conduct systematic ablation studies of the steering mechanism by testing different values of the scaling factor β, mean-ablating only specific MLP layers, and comparing against alternative prompting strategies to isolate which components contribute most to the 9% accuracy improvement.