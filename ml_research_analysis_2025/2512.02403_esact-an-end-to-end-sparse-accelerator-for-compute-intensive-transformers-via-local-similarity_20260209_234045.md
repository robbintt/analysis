---
ver: rpa2
title: 'ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers
  via Local Similarity'
arxiv_id: '2512.02403'
source_url: https://arxiv.org/abs/2512.02403
tags:
- similarity
- attention
- sparsity
- prediction
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ESACT, an accelerator for compute-intensive
  Transformers that leverages local similarity for end-to-end sparsity across all
  components. The core method, SPLS, uses HybridLog Quantization (HLog) to predict
  local attention sparsity before QK generation, achieving efficient sparsity in QKV
  generation, attention computation, and FFNs.
---

# ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity

## Quick Facts
- arXiv ID: 2512.02403
- Source URL: https://arxiv.org/abs/2512.02403
- Reference count: 40
- Primary result: Achieves 3.27 TOPS/W energy efficiency with 51.7% computation reduction in transformers

## Executive Summary
ESACT introduces a novel end-to-end sparse accelerator for compute-intensive transformers that leverages local similarity patterns to achieve significant efficiency gains. The system employs SPLS (Sparsity Prediction via Local Similarity) with HybridLog Quantization to predict attention sparsity before QK generation, enabling efficient processing across all transformer components. Through three hardware innovations including a bit-level prediction unit and progressive generation scheme, ESACT demonstrates substantial improvements in energy efficiency while maintaining minimal accuracy loss across 26 benchmarks.

## Method Summary
The core innovation centers on SPLS, which uses HybridLog Quantization (HLog) to predict local attention sparsity patterns before the QK generation phase. This prediction enables end-to-end sparsity exploitation across QKV generation, attention computation, and FFN layers. The architecture incorporates a bit-level prediction unit for efficient quantization and low-power prediction, a progressive generation scheme that overlaps prediction with QKV generation, and a dynamic allocation strategy to handle irregular sparsity patterns. This approach achieves 51.7% total computation reduction with less than 1% accuracy loss while maintaining high energy efficiency.

## Key Results
- Achieves 3.27 TOPS/W end-to-end energy efficiency
- Improves attention-level energy efficiency by 2.95x over SpAtten and 2.26x over Sanger
- Reduces total computation by 51.7% with less than 1% accuracy loss across 26 benchmarks

## Why This Works (Mechanism)
The mechanism exploits local similarity patterns in transformer attention mechanisms through predictive sparsity. By using HybridLog Quantization to identify sparse regions before QK generation, the system can avoid unnecessary computations throughout the pipeline. The bit-level prediction unit provides energy-efficient quantization and prediction, while the progressive generation scheme enables overlapping of prediction and computation phases. The dynamic allocation strategy effectively manages irregular sparsity patterns that emerge from the predictive approach.

## Foundational Learning

**HybridLog Quantization (HLog)**: A quantization method that combines logarithmic and linear quantization schemes. Why needed: Provides efficient representation of both small and large values in attention weights. Quick check: Verify that the quantization error remains below the 1% accuracy threshold across all benchmarks.

**Local Similarity Patterns**: Spatial correlations in attention matrices that exhibit predictable sparsity. Why needed: Enables early prediction of which computations can be skipped. Quick check: Measure pattern consistency across different transformer architectures and sequence lengths.

**Bit-level Prediction Unit**: Hardware component that performs quantization and prediction at the bit level. Why needed: Reduces power consumption compared to traditional arithmetic units. Quick check: Compare power consumption against baseline integer arithmetic units.

**Progressive Generation Scheme**: Pipeline optimization that overlaps prediction with QKV generation. Why needed: Hides prediction latency and improves throughput. Quick check: Measure pipeline stall rates under various sparsity patterns.

**Dynamic Allocation Strategy**: Runtime memory management for irregular sparsity patterns. Why needed: Handles fragmentation and variable sparsity density. Quick check: Monitor memory fragmentation under worst-case sparsity distributions.

## Architecture Onboarding

**Component Map**: Input -> Bit-level Prediction Unit -> Progressive Generation -> Dynamic Allocation -> QKV/Attention/FFN Computation -> Output

**Critical Path**: The longest path is typically through the Progressive Generation scheme where prediction must complete before QKV generation can proceed efficiently.

**Design Tradeoffs**: Early prediction enables higher sparsity but requires accurate prediction models; bit-level operations reduce power but may limit numerical precision; dynamic allocation handles irregularity but adds complexity.

**Failure Signatures**: 
- Prediction errors leading to incorrect sparsity patterns and accuracy degradation
- Pipeline stalls when prediction cannot keep pace with generation
- Memory fragmentation causing allocation failures or performance degradation

**First Experiments**:
1. Benchmark SPLS accuracy across different transformer architectures (ViT, BERT, GPT variants)
2. Measure power consumption of bit-level prediction unit versus traditional arithmetic units
3. Stress-test dynamic allocation with adversarial sparsity patterns to identify fragmentation limits

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Generalization uncertainty across diverse model architectures beyond the 26 tested benchmarks
- Effectiveness of HLog prediction may vary with different attention mechanisms and sequence lengths
- Lack of detailed circuit-level power analysis for bit-level prediction unit

## Confidence

**High**: Core architectural contributions (bit-level prediction unit, progressive generation, dynamic allocation) are well-defined and technically sound based on the presented methodology.

**Medium**: Claimed energy efficiency improvements of 2.95x and 2.26x over SpAtten and Sanger respectively are based on the provided experimental setup, but independent verification would strengthen these claims.

**Low**: Generalizability of SPLS to emerging transformer variants and larger-scale deployments beyond the tested benchmarks remains uncertain.

## Next Checks

1. Conduct ablation studies testing SPLS across different transformer architectures (ViT, BERT variants, GPT-style models) to assess pattern robustness
2. Implement power measurement at the circuit level for the bit-level prediction unit to verify energy efficiency claims
3. Stress-test the dynamic allocation strategy with adversarial sparsity patterns to identify potential performance bottlenecks