---
ver: rpa2
title: 'SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced
  Safety in LLM-based Robotic Task Planning'
arxiv_id: '2503.06892'
source_url: https://arxiv.org/abs/2503.06892
tags:
- task
- safety
- tasks
- safeplan
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SafePlan, a multi-component framework that
  combines formal logic and chain-of-thought reasoning to enhance safety in LLM-based
  robotic task planning. The core idea is to use a Prompt Sanity Check COT Reasoner
  to filter unsafe natural language prompts, followed by Invariant COT Reasoners that
  generate and verify preconditions, postconditions, and invariants for task plans
  and code.
---

# SafePlan: Leveraging Formal Logic and Chain-of-Thought Reasoning for Enhanced Safety in LLM-based Robotic Task Planning

## Quick Facts
- arXiv ID: 2503.06892
- Source URL: https://arxiv.org/abs/2503.06892
- Authors: Ike Obi; Vishnunandan L. N. Venkatesh; Weizheng Wang; Ruiqi Wang; Dayoon Suh; Temitope I. Amosa; Wonse Jo; Byung-Cheol Min
- Reference count: 21
- Primary result: 90.5% reduction in harmful task acceptance while maintaining reasonable acceptance of safe tasks

## Executive Summary
SafePlan is a multi-component framework designed to enhance safety in LLM-based robotic task planning by combining formal logic verification with chain-of-thought reasoning. The framework employs a Prompt Sanity Check COT Reasoner to filter unsafe natural language prompts, followed by Invariant COT Reasoners that generate and verify preconditions, postconditions, and invariants for task plans and code. The system was evaluated using a benchmark of 621 expert-curated task prompts and AI2-THOR simulations, demonstrating significant improvements in safety performance compared to baseline models.

## Method Summary
SafePlan integrates formal logic verification with chain-of-thought reasoning to create a robust safety framework for LLM-based robotic task planning. The approach uses multiple COT Reasoners to systematically check prompts, generate task plans, and verify safety invariants before execution. The framework was tested on 621 expert-curated task prompts within the AI2-THOR simulation environment, comparing its performance against baseline models in terms of harmful task acceptance and safe task execution.

## Key Results
- Achieved 90.5% reduction in harmful task acceptance compared to baseline models
- Maintained reasonable acceptance rates for safe tasks while improving safety metrics
- Demonstrated significant performance improvements over baseline models in AI2-THOR simulations

## Why This Works (Mechanism)
SafePlan works by systematically applying formal logic verification through chain-of-thought reasoning at multiple stages of the task planning pipeline. The Prompt Sanity Check COT Reasoner filters unsafe prompts before they reach the planning stage, while Invariant COT Reasoners generate and verify safety constraints for both task plans and generated code. This multi-layered approach ensures that safety considerations are embedded throughout the planning and execution process rather than being checked only at the final stage.

## Foundational Learning
- **Chain-of-Thought Reasoning**: Sequential logical reasoning process needed for complex safety verification
  - Why needed: Enables systematic safety checks that simple pattern matching cannot achieve
  - Quick check: Can trace reasoning steps and identify logical gaps

- **Formal Logic Verification**: Mathematical framework for proving safety properties
  - Why needed: Provides rigorous foundation for safety guarantees beyond heuristic approaches
  - Quick check: Can express and verify safety invariants formally

- **Invariant Generation**: Automatic creation of safety constraints for robotic tasks
  - Why needed: Ensures safety properties hold throughout task execution
  - Quick check: Can generate preconditions, postconditions, and invariants from task descriptions

- **Multi-stage Safety Filtering**: Layered approach to safety verification
  - Why needed: Prevents unsafe tasks from propagating through the planning pipeline
  - Quick check: Can demonstrate progressive filtering of harmful tasks

## Architecture Onboarding

**Component Map**: Prompt Input -> Prompt Sanity Check COT Reasoner -> Task Planning COT Reasoner -> Invariant COT Reasoners -> Code Generation -> Code Verification COT Reasoner -> Execution

**Critical Path**: The most critical path is Prompt Input -> Prompt Sanity Check COT Reasoner, as this initial filtering stage prevents harmful tasks from entering the system. The prompt sanity check must operate with high accuracy to maintain overall system safety.

**Design Tradeoffs**: The framework trades computational overhead for enhanced safety through multiple COT Reasoner stages. While this approach provides robust safety verification, it increases latency compared to single-pass verification methods. The use of chain-of-thought reasoning rather than formal theorem proving provides flexibility but may introduce gaps in absolute safety guarantees.

**Failure Signatures**: 
- High false positive rates in prompt filtering indicating overly conservative safety checks
- Inconsistent invariant generation across similar task types
- Failure to catch safety violations in edge cases or complex scenarios
- Performance degradation when handling ambiguous or context-dependent safety requirements

**First 3 Experiments**:
1. Test prompt filtering accuracy with a diverse set of harmful and safe task prompts
2. Validate invariant generation consistency across different task types and scenarios
3. Measure performance overhead introduced by multi-stage safety verification

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to AI2-THOR simulation environment with 621 expert-curated task prompts
- Relies on chain-of-thought reasoning rather than formal theorem proving for logic verification
- Performance on ambiguous or context-dependent safety scenarios not thoroughly evaluated
- Limited testing of long-term operation and diverse user populations

## Confidence
- "90.5% reduction in harmful task acceptance" - High confidence
- "Maintains reasonable acceptance of safe tasks" - Medium confidence
- "Enhanced safety in LLM-based robotic task planning" - Medium confidence

## Next Checks
1. Deploy SafePlan in physical robot testbeds with real-world task scenarios to validate simulation-to-reality transfer of safety improvements.

2. Conduct adversarial testing with expert safety reviewers to identify edge cases where the formal logic verification might fail or produce false positives/negatives.

3. Perform longitudinal testing to assess whether the safety benefits degrade over extended operation periods or with diverse user populations and task types.