---
ver: rpa2
title: 'Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math'
arxiv_id: '2510.26143'
source_url: https://arxiv.org/abs/2510.26143
tags:
- reasoning
- zhang
- wang
- arxiv
- math
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reasoning Curriculum, a two-stage reinforcement
  learning curriculum designed to improve general reasoning in large language models
  (LLMs). The method first elicits reasoning skills through math-focused RL, then
  adapts and refines these skills across diverse domains using joint RL.
---

# Reasoning Curriculum: Bootstrapping Broad LLM Reasoning from Math
## Quick Facts
- arXiv ID: 2510.26143
- Source URL: https://arxiv.org/abs/2510.26143
- Authors: Bo Pang; Deqian Kong; Silvio Savarese; Caiming Xiong; Yingbo Zhou
- Reference count: 12
- Two-stage RL curriculum improves general reasoning in LLMs, showing consistent gains across math, STEM, code, logic, and tabular tasks

## Executive Summary
This paper introduces Reasoning Curriculum, a two-stage reinforcement learning approach that improves general reasoning capabilities in large language models. The method first develops reasoning skills through math-focused RL training, then refines these skills across diverse domains using joint RL. Evaluated on Qwen3-4B and Llama-3.1-8B models, the curriculum demonstrates consistent performance gains across multiple reasoning benchmarks including math, STEM, code, logic, simulation, and tabular tasks. The approach is backbone-agnostic and requires only standard verifiable rewards, providing a simple yet effective recipe for enhancing general reasoning in LLMs.

## Method Summary
The Reasoning Curriculum employs a two-stage reinforcement learning approach. Stage 1 focuses on eliciting reasoning skills through math-specific RL training, using verifiable rewards from math problems. Stage 2 adapts and refines these developed skills across diverse domains through joint RL training on multiple task types. The curriculum is designed to be backbone-agnostic, working with any model that can utilize standard verifiable rewards. For Llama-3.1-8B, the authors added difficulty curricula within the math stage to achieve consistent improvements. The method includes ablation studies confirming the necessity of both stages for optimal performance.

## Key Results
- Qwen3-4B models with Reasoning Curriculum outperform similarly sized baselines and match or exceed 32B models on many tasks
- Consistent performance gains across math, STEM, code, logic, simulation, and tabular reasoning tasks
- Cognitive-skill analysis shows increased use of advanced reasoning behaviors like verification and backtracking
- Both stages of the curriculum are necessary, as confirmed by ablation studies

## Why This Works (Mechanism)
The curriculum leverages the structured nature of mathematical reasoning to bootstrap general reasoning capabilities. By first developing robust reasoning skills in the constrained domain of math problems with clear verifiable rewards, the model learns fundamental reasoning patterns that transfer to broader contexts. The two-stage approach allows for initial skill acquisition in a controlled environment before adaptation to more diverse and complex reasoning scenarios. The math-focused stage provides clear feedback signals through verifiable rewards, enabling the model to develop reliable reasoning patterns that can then be generalized and refined across different domains in the second stage.

## Foundational Learning
- **Mathematical reasoning fundamentals** - Why needed: Math provides structured problems with clear right/wrong answers, creating reliable learning signals for developing reasoning patterns
  Quick check: Can the model solve basic to advanced math problems with verifiable correctness
- **Reinforcement learning with verifiable rewards** - Why needed: Standard RL requires clear reward signals, which math problems naturally provide through correct/incorrect answers
  Quick check: Does the reward signal properly reflect solution quality across different math problem types
- **Domain transfer learning** - Why needed: Skills developed in math must generalize to other reasoning domains like code, logic, and STEM
  Quick check: Can reasoning patterns learned from math successfully apply to non-math reasoning tasks

## Architecture Onboarding
- **Component map**: Math RL Stage -> Joint RL Stage -> Domain Adaptation -> Enhanced Reasoning Capabilities
- **Critical path**: Verifiable math problems → RL training with reward signals → Skill transfer to diverse domains → Performance evaluation across multiple benchmarks
- **Design tradeoffs**: Focused skill development vs. immediate broad training; structured math problems vs. diverse real-world scenarios; computational efficiency vs. comprehensive skill coverage
- **Failure signatures**: Overfitting to math-specific patterns; poor transfer to non-math domains; reward hacking in math stage; loss of reasoning capabilities during domain adaptation
- **3 first experiments**: 1) Baseline math performance without curriculum, 2) Single-stage RL on mixed tasks, 3) Math-only RL without domain adaptation stage

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Experimental scope limited to 4B and 8B parameter models, leaving uncertainty about effectiveness on larger models
- Evaluation relies entirely on multiple-choice and structured reasoning tasks, with limited assessment of open-ended problem-solving
- Transfer to highly abstract or creative reasoning domains remains untested despite strong performance on math and STEM tasks
- Long-term stability of reasoning improvements and optimal curriculum hyperparameters require further investigation

## Confidence
- **High**: Performance improvements on math and STEM tasks, curriculum design validity, skill transfer mechanism
- **Medium**: Generalizability to larger models, effectiveness across diverse real-world domains, necessity of both curriculum stages
- **Low**: Long-term stability of reasoning improvements, optimal curriculum hyperparameters, transfer to abstract reasoning tasks

## Next Checks
1. Test curriculum effectiveness on 30B+ parameter models to assess scalability and potential performance saturation points
2. Evaluate reasoning improvements on open-ended problem-solving tasks and real-world application domains beyond multiple-choice formats
3. Conduct longitudinal studies measuring reasoning stability and potential degradation over extended training periods and varied task distributions