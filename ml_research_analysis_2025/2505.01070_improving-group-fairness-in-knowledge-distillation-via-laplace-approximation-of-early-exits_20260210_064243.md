---
ver: rpa2
title: Improving Group Fairness in Knowledge Distillation via Laplace Approximation
  of Early Exits
arxiv_id: '2505.01070'
source_url: https://arxiv.org/abs/2505.01070
tags:
- student
- teacher
- auxiliary
- layer
- laplace
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses group fairness in knowledge distillation by
  improving uncertainty estimation in early-exit neural networks. The authors propose
  using Laplace approximation to obtain well-calibrated uncertainty estimates from
  intermediate layers, replacing the margin-based approach in the DEDIER model.
---

# Improving Group Fairness in Knowledge Distillation via Laplace Approximation of Early Exits

## Quick Facts
- arXiv ID: 2505.01070
- Source URL: https://arxiv.org/abs/2505.01070
- Authors: Edvin Fasth; Sagar Singh
- Reference count: 2
- Primary result: Laplace approximation improves worst-group accuracy in KD by ~0.5% over margin-based uncertainty estimation

## Executive Summary
This paper addresses group fairness in knowledge distillation by improving uncertainty estimation in early-exit neural networks. The authors propose using Laplace approximation to obtain well-calibrated uncertainty estimates from intermediate layers, replacing the margin-based approach in the DEDIER model. Their method approximates the posterior distribution of auxiliary network weights using Bayesian updates, providing more robust identification of difficult instances. Experiments on the MultiNLI dataset show that this approach yields slightly better accuracy than the original DEDIER model, particularly when the auxiliary network is placed on earlier layers (layer 3 vs. layer 6).

## Method Summary
The method trains a student network (DistilBERT) using knowledge distillation from a teacher (BERT-base), with an auxiliary network placed on an early layer (typically layer 3). The auxiliary network is trained each batch to predict labels from the student's intermediate features. Laplace approximation computes a Gaussian posterior over the auxiliary weights, which is used to estimate predictive uncertainty via Monte Carlo sampling. Instance weights are derived from the entropy of this predictive distribution and applied to the distillation loss. This reweights difficult or ambiguous instances, theoretically reducing reliance on spurious correlations learned in early layers.

## Key Results
- Laplace approximation yields ~0.5% accuracy improvement over margin-based DEDIER on MultiNLI
- Earlier auxiliary placement (layer 3) outperforms later placement (layer 6) for fairness
- Lower confidence margins observed on worst-group predictions indicate increased uncertainty calibration
- Method is computationally efficient compared to alternatives like MC dropout

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Laplace approximation provides better-calibrated uncertainty estimates than margin-based confidence for identifying difficult instances.
- Mechanism: The method approximates the posterior distribution over auxiliary network weights using a Gaussian (Laplace) approximation, capturing both aleatoric and epistemic uncertainty through the covariance structure Σϕ. This yields entropy-based uncertainty scores rather than raw confidence margins.
- Core assumption: Difficult or spuriously-correlated instances will exhibit higher predictive uncertainty when assessed at early layers before the model has learned robust representations.
- Evidence anchors:
  - [abstract]: "Laplace approximation offers a more robust identification of difficult or ambiguous instances compared to margin-based approaches"
  - [Section 2]: "This method tends to yield high uncertainty for difficult instances, attributed to potential model misspecification"
  - [corpus]: Related work on uncertainty propagation in KD (arxiv:2601.18909) supports that collapsing uncertainty to point estimates distorts learning, though direct comparison to margin-based methods is limited
- Break condition: If early-layer representations are already well-calibrated for the target task, Laplace-based uncertainty may not provide additional signal over simpler margin methods.

### Mechanism 2
- Claim: Reweighting distillation loss using uncertainty estimates reduces reliance on spurious surface-level features.
- Mechanism: Weight computation w = exp(β · H(p)^α) assigns higher weights to high-entropy (uncertain) predictions. This upweights instances where the student's early layers are uncertain, forcing the distillation process to focus teacher guidance on challenging examples rather than letting the student rely on simple feature shortcuts.
- Core assumption: Spurious correlations are learned early and produce overconfident incorrect predictions; uncertainty-based reweighting counteracts this by emphasizing harder cases.
- Evidence anchors:
  - [Section 1]: "student models tend to learn simpler, surface-level features in their early layers. This discrepancy can increase errors in groups where labels spuriously correlate"
  - [Section 4.1.2]: "confidence margins are generally lower than those reported in the original paper...using the Laplace transformation"
  - [corpus]: No direct corpus evidence on uncertainty-weighted KD for fairness; this is a gap requiring further validation
- Break condition: If the teacher model itself has learned spurious correlations, reweighting may amplify biased signals rather than correct them.

### Mechanism 3
- Claim: Placing the auxiliary network on earlier layers (layer 3 vs. layer 6) provides stronger fairness improvements.
- Mechanism: Earlier layers capture more fundamental representations; intervening at layer 3 influences feature processing more directly before the model has committed to potentially biased representations.
- Core assumption: Fairness interventions are most effective when applied at layers where feature representations are still malleable and before final classification decisions.
- Evidence anchors:
  - [Section 4.1.1]: "Using a single-layer auxiliary network on the third of six layers was found to be more effective than placing the auxiliary network on the final layer"
  - [Section 4.1.1]: "earlier layers tend to capture more fundamental and informative representations"
  - [corpus]: Early-exit distillation work (ERDE, SAFE-KD) confirms early-layer intervention points but does not directly address layer depth vs. fairness
- Break condition: Too-early layers may lack task-relevant information, making uncertainty estimates uninformative.

## Foundational Learning

- Concept: **Knowledge Distillation (KD) Fundamentals**
  - Why needed here: The entire method builds on transferring knowledge from teacher to student via softened probability distributions. Without understanding temperature scaling, alpha weighting between CE and KD losses, and the role of soft targets, the reweighting mechanism won't make sense.
  - Quick check question: Can you explain why temperature > 1 is used during distillation and how it affects the soft target distribution?

- Concept: **Laplace Approximation in Bayesian Neural Networks**
  - Why needed here: The core innovation replaces margin-based confidence with Laplace-approximated posteriors. Understanding how this Gaussian approximation captures weight uncertainty and propagates it through predictions is essential.
  - Quick check question: Given a trained linear classifier with weights W, how would you compute the Laplace approximation to the posterior p(W|D), and what does the covariance matrix represent?

- Concept: **Group Fairness and Spurious Correlations**
  - Why needed here: The paper's metric is worst-group accuracy, not overall accuracy. Understanding how models exploit shortcuts (e.g., negation words correlating with contradiction labels) is critical to interpreting why uncertainty-based reweighting might help.
  - Quick check question: In MultiNLI, what constitutes a "group" for fairness evaluation, and why might a model achieve high average accuracy while failing on specific subgroups?

## Architecture Onboarding

- Component map:
  - BERT-base-uncased (12 layers, 768 hidden) -> DistilBERT-base-uncased (6 layers, 768 hidden) -> Single linear auxiliary classifier on layer 3 or 6

- Critical path:
  1. Forward pass through student to get features ϕ at early exit layer
  2. Train auxiliary classifier on current batch (cross-entropy)
  3. Compute Laplace covariance and sample predictive distribution
  4. Calculate entropy-based weight w for each sample
  5. Apply weighted KD loss + unweighted CE loss
  6. Backpropagate to update student only (auxiliary is retrained each batch)

- Design tradeoffs:
  - **Auxiliary complexity**: Paper uses single layer for speed; deeper auxiliary could capture more nuance but adds overhead
  - **Exit layer depth**: Layer 3 showed better results than layer 6, but optimal depth likely depends on student architecture
  - **Reweighting frequency**: Current approach reweights every batch; could decay weights over training as student improves
  - **Numerical stability**: Ridge term added to Σϕ for positive-definiteness; tuning this affects uncertainty calibration

- Failure signatures:
  - **Exploding weights**: If entropy estimates are miscalibrated, some samples may receive extreme weights, destabilizing training
  - **No improvement over baseline**: Check if auxiliary network is actually learning (accuracy > random) before trusting its uncertainty
  - **Worse worst-group accuracy**: Teacher may have inherited biases; reweighting won't fix a biased teacher
  - **NaN in covariance**: Feature collapse or insufficient batch diversity; increase batch size or add stronger regularization

- First 3 experiments:
  1. **Reproduce baseline comparison**: Train standard DEDIER (margin-based) vs. Laplace version on MultiNLI with identical hyperparameters; verify ~0.5% accuracy improvement and lower confidence margins on worst-group predictions
  2. **Ablate exit layer**: Test auxiliary on layers 2, 3, 4, 5, 6 to find optimal intervention point for your specific student architecture
  3. **Dataset transfer**: Validate on a second dataset with known spurious correlations (e.g., Waterbirds or CelebA) to confirm mechanism generalizes beyond NLP

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Laplace approximation-based reweighting method generalize to computer vision benchmarks such as Waterbirds or CelebA, or is it specific to NLP tasks like MultiNLI?
- Basis in paper: [explicit] Section 4.2.2 states, "The current experiments are limited to the MultiNLI dataset... Suitable datasets include Waterbirds, Civil Comments, and CelebA."
- Why unresolved: The authors validate their approach solely on the MultiNLI textual dataset, leaving the efficacy of Laplace-based uncertainty reweighting on image data unproven.
- What evidence would resolve it: Experimental results showing worst-group accuracy improvements on the explicitly listed image datasets (Waterbirds, CelebA) compared to the standard DEDIER baseline.

### Open Question 2
- Question: Would aggregating uncertainty estimates from multiple auxiliary layers (ensembled by computational cost) outperform the single-layer auxiliary network used in this study?
- Basis in paper: [explicit] Section 4.2.1 proposes evaluating "a more sophisticated auxiliary architecture" that combines predictions from multiple layers as described in Eq. 7.
- Why unresolved: The implemented auxiliary network was a simple, single-layer model; the potential benefit of combining estimates across layers, as suggested by Meronen et al. (2024), remains untested in this fairness context.
- What evidence would resolve it: Ablation studies comparing the current single-layer performance against a multi-layer ensemble auxiliary network using the weighted sum formula provided in the paper.

### Open Question 3
- Question: Do the observed lower confidence margins indicate genuinely improved model calibration, or do they merely signal weaker class separation that could harm reliability?
- Basis in paper: [inferred] Section 4.1.2 notes an ambiguity in the results: "While lower confidence margins can sometimes indicate increased model uncertainty... they can also signal weaker separation between correct and incorrect classes."
- Why unresolved: The paper reports lower margins but does not include specific calibration metrics (like Expected Calibration Error) or confusion matrix analysis to distinguish between beneficial uncertainty and detrimental loss of discriminative power.
- What evidence would resolve it: Reporting calibration metrics (e.g., ECE) and analyzing the separation between correct and incorrect logits to confirm if reduced confidence correlates with better robustness.

### Open Question 4
- Question: To what extent are the performance gains attributable to the Laplace approximation itself versus the modified training schedule where auxiliary reweighting occurs every layer?
- Basis in paper: [inferred] Section 3.2 notes a "major difference" where "auxiliary reweighting is done in every layer" (batch) rather than every $L$ epochs as in the original DEDIER.
- Why unresolved: The authors conflate the change in uncertainty estimation method (Laplace) with a change in update frequency (every batch), making it unclear which modification drives the accuracy improvements.
- What evidence would resolve it: An ablation study that applies the "every layer" update schedule to the original margin-based DEDIER model to isolate the impact of the schedule from the Laplace method.

## Limitations

- Limited evaluation scope: Only tested on MultiNLI negation vs. non-negation groups, not on datasets with more severe or diverse spurious correlations
- Missing hyperparameter specifications: Key values for α, β, ridge regularization, and MC sample count are not provided, preventing exact reproduction
- Unclear attribution of improvements: The paper doesn't distinguish whether gains come from Laplace approximation specifically or from the changed update frequency

## Confidence

- **High Confidence**: The core mechanism of using Laplace approximation for uncertainty estimation in early-exit auxiliary networks is technically sound and well-grounded in Bayesian methods. The observation that earlier layers (layer 3) perform better than later layers (layer 6) is consistent with early-exit literature.
- **Medium Confidence**: The claim that Laplace-based uncertainty provides better identification of difficult instances than margin-based approaches is supported by confidence margin comparisons, but lacks direct statistical testing or comparison to alternative uncertainty methods. The improvement in worst-group accuracy is modest (~0.5%) and requires replication to confirm significance.
- **Low Confidence**: The assertion that this method effectively addresses group fairness through reweighting spurious correlations is primarily theoretical at this stage, with limited empirical validation across diverse fairness scenarios or comparison to other fairness-aware KD methods.

## Next Checks

1. **Ablation on Uncertainty Estimation**: Replace the Laplace approximation with simpler uncertainty estimates (e.g., prediction entropy from the student's early exit, or MC dropout) and compare worst-group accuracy. This would determine whether the complexity of Laplace approximation is necessary or if simpler methods suffice.

2. **Cross-Dataset Fairness Evaluation**: Apply the method to Waterbirds (spurious correlation between background and bird type) and CelebA (attribute prediction with demographic subgroups). This would test whether the approach generalizes beyond linguistic negation patterns to other types of spurious correlations.

3. **Teacher Bias Investigation**: Train the teacher model with explicit fairness regularization (e.g., worst-group loss) and compare student performance when using this fairer teacher versus the standard teacher. This would determine whether the method can compensate for teacher bias or if fair teachers are still necessary.