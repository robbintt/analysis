---
ver: rpa2
title: Non-convergence to the optimal risk for Adam and stochastic gradient descent
  optimization in the training of deep neural networks
arxiv_id: '2503.01660'
source_url: https://arxiv.org/abs/2503.01660
tags:
- satisfy
- holds
- corollary
- every
- assume
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proves that stochastic gradient descent (SGD) and Adam
  methods do not converge to the optimal true risk in training deep neural networks.
  The authors show that for a wide range of activation functions, loss functions,
  and network architectures, there is a strictly positive probability that the true
  risk of the optimization process does not converge to the optimal value.
---

# Non-convergence to the optimal risk for Adam and stochastic gradient descent optimization in the training of deep neural networks

## Quick Facts
- arXiv ID: 2503.01660
- Source URL: https://arxiv.org/abs/2503.01660
- Reference count: 40
- This work proves that SGD and Adam methods do not converge to the optimal true risk in training deep neural networks, with failure probability approaching 1 for very deep architectures.

## Executive Summary
This theoretical paper demonstrates that stochastic gradient descent (SGD) and adaptive optimization methods like Adam can fail to converge to the optimal true risk when training deep neural networks. The key mechanism is the formation of "inactive neurons" - neurons that output constant values due to gradient saturation in flat regions of activation functions like ReLU. Once inactive, these neurons remain permanently inactive, potentially preventing the network from achieving optimal risk. The authors prove that for very deep networks, this non-convergence occurs with high probability, establishing theoretical lower bounds on the failure probability that increase with network depth.

## Method Summary
The paper uses abstract mathematical analysis to establish lower bounds on the probability that SGD and Adam optimization processes fail to converge to the optimal true risk. The theoretical framework analyzes fully-connected feedforward deep neural networks with activation functions having flat regions (like ReLU), random initialization, and various popular optimizers. The proofs rely on establishing the "invariance of inactive neurons" - showing that once a neuron's pre-activation falls into a flat region of the activation function, its upstream weights receive zero gradient and remain permanently inactive. The analysis connects this mechanism to suboptimal risk through demonstrating that constant network outputs cannot achieve optimal risk for non-constant target functions.

## Key Results
- SGD and Adam optimization processes have strictly positive probability of not converging to the optimal true risk for deep neural networks with ReLU-like activations.
- For very deep networks, the probability of non-convergence approaches 1 as depth increases.
- Inactive neurons (those outputting constant values) are the key mechanism preventing convergence to optimal risk.
- The theoretical results apply to a wide range of popular optimizers including standard SGD, momentum SGD, Nesterov accelerated SGD, Adagrad, RMSprop, Adadelta, Adam, Adamax, Nadam, Nadamax, and AMSGrad.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** SGD and Adam fail to converge to optimal true risk because they can become trapped in regions where neurons are "inactive."
- **Mechanism:** If a neuron's activation falls within a region where the activation function has zero derivative (e.g., negative region for ReLU), the gradient with respect to upstream weights becomes zero. Once inactive, neurons remain inactive for all future training steps, permanently removing computational capacity.
- **Core assumption:** Activation function $A$ has a non-empty interval where derivative is zero, and initialization is random.
- **Evidence anchors:** Lemma 2.2 establishes "Invariance of inactive neurons during training procedure"; abstract states inactive neurons prevent convergence to optimal risk.
- **Break condition:** Strictly increasing activation functions with no flat regions (e.g., smooth sigmoids without saturation) do not create this inactive neuron mechanism.

### Mechanism 2
- **Claim:** Probability of failing to converge to optimal risk increases as network depth increases.
- **Mechanism:** As network depth grows, probability that at least one layer contains inactive configurations increases. The paper provides lower bounds showing this probability approaches 1 for very deep networks.
- **Core assumption:** Network architecture is very deep (depth $L \to \infty$ or sufficient depth per Corollary 4.14).
- **Evidence anchors:** Theorem 1.3 states for very deep ANNs, probability of non-convergence converges to 1; Corollary 4.14 proves $\liminf_{k \to \infty} P(\dots) = 1$ for sufficiently deep architectures.
- **Break condition:** Shallow networks may still suffer non-convergence but with strictly positive probability rather than high probability guarantees for deep architectures.

### Mechanism 3
- **Claim:** Optimization converges to strictly suboptimal risk because resulting network function becomes constant, failing to model non-constant target data.
- **Mechanism:** When neurons in hidden layers are inactive, network's output function reduces to a constant. If true data distribution requires non-constant mapping, risk of this constant function is strictly higher than optimal risk.
- **Core assumption:** Conditional expectation of output given input is not constant ($P(E[Y|X] = E[Y]) < 1$).
- **Evidence anchors:** Corollary 3.7 demonstrates for non-constant targets, infimum risk of constant realization functions is strictly greater than global infimum.
- **Break condition:** If target function is actually constant, the "suboptimal" risk of constant network realization would be optimal.

## Foundational Learning

- **Concept:** True Risk vs. Empirical Risk
  - **Why needed here:** The paper proves non-convergence to the *true* risk (population risk), distinguishing from empirical risk often minimized in practice.
  - **Quick check question:** Does low training loss (empirical risk) guarantee low error on new data (true risk) according to this theory?

- **Concept:** Activation Saturation (Flat Regions)
  - **Why needed here:** Non-convergence mechanism relies entirely on existence of "flat regions" in activation function where gradient is zero.
  - **Quick check question:** Which common activation function creates a "dead" (gradient zero) state for all negative inputs?

- **Concept:** Convergence in Probability
  - **Why needed here:** Main result is phrased as failure to converge *in probability* to optimal value, implying specific mathematical definition of limit.
  - **Quick check question:** If a process does not converge in probability to value $v$, can it still converge to value slightly worse than $v$?

## Architecture Onboarding

- **Component map:** Inputs $X$ and $Y$ -> Fully-connected feedforward DNN with $L$ layers -> ReLU or clipping activation -> SGD/Adam optimizer -> True Risk $L(\theta)$ output

- **Critical path:**
  1. **Initialization:** Weights and biases are randomized
  2. **Forward Pass:** Compute neuron activations
  3. **Inactivity Check:** If pre-activation is in "flat" zone (e.g., negative for ReLU), neuron outputs constant
  4. **Gradient Step:** If inactive, gradients are zero; neuron "dies" and stops learning
  5. **Risk Evaluation:** If network becomes constant, converges to suboptimal risk floor

- **Design tradeoffs:**
  - **Depth vs. Safety:** Increasing depth ($L$) increases representational power but theoretically increases probability of non-convergence to optimal risk
  - **Initialization Variance:** Standard normal init used; wide variances might increase chance of falling into saturation zones initially

- **Failure signatures:**
  - **Dead Neurons:** High percentage of neurons outputting constant values across dataset
  - **Stagnation:** Training loss stabilizes at value significantly higher than theoretical minimum, gradient norms for specific layers vanish completely

- **First 3 experiments:**
  1. **Inactive Neuron Audit:** Train deep ReLU network and plot percentage of "dead" neurons (zero activation for all training data) over epochs to verify invariance lemma
  2. **Depth vs. Non-convergence:** Run optimization on fixed dataset while scaling depth $L$, measuring frequency of runs that stall at suboptimal risk compared to shallow baseline
  3. **Activation Sensitivity:** Swap ReLU for strictly increasing smooth activation (e.g., GELU or Softplus with very small curvature) to test if "high probability" non-convergence disappears or reduces

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does SGD optimization process converge to specific suboptimal critical point (local minimum), or merely remain in suboptimal region without converging?
- **Basis:** Paper proves risk does not converge to optimal value; abstract suggests it "may very well converge to strictly suboptimal" value, but main theorems only establish non-convergence to infimum, not convergence to specific limit.
- **Why unresolved:** Theoretical analysis focuses on lower bounds for non-convergence probability rather than characterizing limiting distribution or criticality of limit points.
- **What evidence would resolve it:** Rigorous proof showing iterates $\Theta_n$ converge to critical point $\Theta^*$ such that $\nabla L(\Theta^*) = 0$ (even if $L(\Theta^*) > \inf L$).

### Open Question 2
- **Question:** Do architectural modifications like skip connections (ResNets) or batch normalization circumvent "inactive neuron" trap identified in fully-connected feedforward networks?
- **Basis:** Paper's scope restricted to "arbitrary fully-connected feedforward DNNs"; non-convergence relies on inactive neurons preventing gradient flow in specific layers.
- **Why unresolved:** Skip connections provide identity pathways for gradients that may bypass inactive neurons, potentially invalidating invariance property established in Lemma 2.3.
- **What evidence would resolve it:** Extension of non-convergence analysis to ResNet architectures, demonstrating whether probability of stuck inactive neurons remains strictly positive or tends to zero.

### Open Question 3
- **Question:** How can one verify assumption that target function "cannot be represented exactly" by neural network to reconcile this work with convergence results in prior literature?
- **Basis:** Literature review states while prior works show convergence to optimal risk if target function is not representable, "it is not yet clear how this condition can be verified for concrete supervised learning problem."
- **Why unresolved:** This condition is theoretical assumption in convergence theorems that is difficult to check for real-world datasets where target relationship is unknown.
- **What evidence would resolve it:** Constructive theoretical criterion or algorithm that determines whether given data distribution's target function falls within specific function class of neural network.

## Limitations

- The paper provides theoretical probability bounds without concrete numerical examples demonstrating the non-convergence phenomenon in practice.
- Assumption that target function is non-constant may not hold in many real-world regression tasks.
- Theoretical bounds use worst-case analysis that may not reflect typical training trajectories.

## Confidence

- **High Confidence:** Proof of neuron inactivity invariance (Lemma 2.2) is mathematically rigorous and well-established.
- **Medium Confidence:** Claim that inactive neurons cause strictly suboptimal risk is theoretically sound but requires empirical validation in practical settings.
- **Low Confidence:** Quantitative predictions about non-convergence probability scaling with depth lack empirical verification.

## Next Checks

1. **Empirical Probability Validation:** Train multiple deep ReLU networks on synthetic non-constant regression tasks and measure actual frequency of convergence to suboptimal risk floors.

2. **Activation Function Sensitivity:** Systematically compare ReLU, leaky ReLU, and smooth activations (GELU, Softplus) to test relationship between gradient saturation and non-convergence probability.

3. **Depth Scaling Experiment:** Vary network depth while monitoring both fraction of inactive neurons and gap between achieved and theoretical optimal risk to empirically validate depth-dependent probability bounds.