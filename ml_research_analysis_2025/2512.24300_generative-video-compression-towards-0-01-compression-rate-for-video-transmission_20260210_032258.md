---
ver: rpa2
title: 'Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission'
arxiv_id: '2512.24300'
source_url: https://arxiv.org/abs/2512.24300
tags:
- video
- compression
- generative
- communication
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Generative Video Compression (GVC), a new
  video compression framework that achieves extreme compression rates (as low as 0.02%)
  by leveraging generative video models. Instead of traditional pixel-level compression,
  GVC encodes video into compact latent representations and reconstructs it using
  generative priors, trading computation for compression.
---

# Generative Video Compression: Towards 0.01% Compression Rate for Video Transmission

## Quick Facts
- arXiv ID: 2512.24300
- Source URL: https://arxiv.org/abs/2512.24300
- Authors: Xiangyu Chen; Jixiang Luo; Jingyu Xu; Fangqiu Yi; Chi Zhang; Xuelong Li
- Reference count: 4
- One-line primary result: Achieves extreme compression rates (as low as 0.02%) by shifting reconstruction burden to receiver-side generative inference, requiring up to 6x less bandwidth than HEVC.

## Executive Summary
This paper introduces Generative Video Compression (GVC), a new video compression framework that achieves extreme compression rates (as low as 0.02%) by leveraging generative video models. Instead of traditional pixel-level compression, GVC encodes video into compact latent representations and reconstructs it using generative priors, trading computation for compression. The framework is built on the AI Flow paradigm and prioritizes task-oriented communication over fidelity. Experimental results on the MCL-JCV dataset show that GVC achieves competitive perceptual quality (LPIPS=0.180) at just 0.008 bpp, requiring up to 6x less bandwidth than HEVC. It also maintains strong downstream task performance (VOS J&F=79.28%) at 0.0175 bpp.

## Method Summary
GVC encodes video into compact latent tokens (discrete/continuous) and transmits them at extremely low bitrates (~0.008 bpp). A diffusion-based generative model at the receiver reconstructs the video using these tokens as conditions, leveraging learned priors to synthesize details not present in the transmitted signal. The framework consists of a Neural Encoder that compresses raw video into compressed tokens (keyframes + semantic descriptors + continuous features), a Transmitter that applies residual coding to tokens, and a Generative Decoder that is a diffusion-based video model conditioned on the tokens. To enable practical deployment on consumer-grade GPUs with ~2s latency, the system applies model compression techniques including distillation and quantization, often compensating for reduced decoder capacity by transmitting higher-dimensional latent features.

## Key Results
- Achieves extreme compression rates (0.02%) by shifting reconstruction burden to receiver-side generative inference
- Maintains competitive perceptual quality (LPIPS=0.180) at just 0.008 bpp, requiring up to 6x less bandwidth than HEVC
- Preserves strong downstream task performance (VOS J&F=79.28%) at 0.0175 bpp while reducing bandwidth requirements

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Extreme compression rates (0.02%) are achievable by shifting the reconstruction burden from transmission bandwidth to receiver-side generative inference.
- **Mechanism:** The framework encodes video into compact latent tokens (discrete/continuous) and transmits them at ~0.008 bpp. A diffusion-based generative model at the receiver reconstructs the video using these tokens as conditions, leveraging learned priors to synthesize details not present in the transmitted signal.
- **Core assumption:** The generative model's priors are sufficiently robust to hallucinate plausible details that satisfy human perception (LPIPS) or downstream task requirements without pixel-perfect ground truth.
- **Evidence anchors:** [abstract] "Encodes video into extremely compact representations and delegates content reconstruction to the receiver." [section 2.2] "GVC shifts the burden of reconstruction to the decoder... using computation and prior knowledge... to synthesize realistic frames from minimal inputs." [corpus] Supported by "Generative AI Meets 6G and Beyond" which identifies generative diffusion models as key enablers for reconstructing semantic content from minimal data.
- **Break condition:** Fails if the content is highly out-of-distribution (e.g., rare medical imaging) where the model's generative priors produce hallucinations rather than accurate reconstructions.

### Mechanism 2
- **Claim:** Optimizing for the "Effectiveness Problem" (Shannon Level C) allows the system to discard non-semantic data, preserving utility for downstream tasks (like VOS) at bitrates where fidelity-based codecs fail.
- **Mechanism:** The Neural Encoder is designed to preserve high-level semantic information and motion dynamics rather than pixel distortion. By measuring success via task metrics (e.g., Video Object Segmentation) instead of pixel-error (PSNR), the system allocates bits solely to semantic consistency.
- **Core assumption:** The receiver's goal is semantic understanding or perceptual satisfaction, not bit-perfect reproduction.
- **Evidence anchors:** [section 1] "Level C focuses on the effectiveness problem... GVC prioritizes whether the transmitted information meets perceptual expectations." [table 2] Shows GVC maintaining 79.28% VOS J&F score at low bitrate, whereas HEVC drops significantly. [corpus] "Few-shot Semantic Encoding" validates that semantic-level transmission is effective for video surveillance tasks, aligning with the GVC task-oriented approach.
- **Break condition:** Fails in scenarios requiring precise pixel-level audit trails (e.g., legal evidence or fine-grained medical diagnosis) where semantic equivalence is insufficient.

### Mechanism 3
- **Claim:** Practical deployment is enabled by a bi-directional trade-off: increasing transmitted latent richness to reduce decoder computational load.
- **Mechanism:** To avoid the latency of large generative models (14B parameters), the system sends "richer" latent features. This reduces the generative effort required at the decoder, allowing for smaller, distilled models that run on consumer GPUs (~2s latency).
- **Core assumption:** The system has some surplus bandwidth available to transmit the richer latents required to support the smaller, faster decoder.
- **Evidence anchors:** [section 2.3] "We apply model compression techniques... and employ distillation... often compensate... by transmitting higher-dimensional... features." [table 3] Demonstrates inference on consumer-grade RTX 4090 GPUs with ~2s latency for 480p video. [corpus] Weak direct evidence in provided neighbors for this specific trade-off strategy, though "Lightweight High-Fidelity... Compression" touches on efficiency needs.
- **Break condition:** Fails if the available bandwidth is too low to support the "richer" latents needed for the miniaturized decoder, forcing a return to the heavy model and high latency.

## Foundational Learning

- **Concept:** **Diffusion Models for Video Synthesis**
  - **Why needed here:** GVC relies on a diffusion-based decoder to reverse noise into video frames conditioned on sparse tokens. Without understanding how diffusion iteratively denoises data, the decoder's role in "generative compression" is opaque.
  - **Quick check question:** How does conditioning a diffusion model on a latent vector alter the denoising trajectory compared to unconditional generation?

- **Concept:** **Shannon-Weaver Levels (A, B, C)**
  - **Why needed here:** The paper explicitly frames GVC as a shift from Level A (Technical/Fidelity) to Level C (Effectiveness/Task). Understanding this distinction is critical to grasping why LPIPS is prioritized over PSNR.
  - **Quick check question:** In the context of GVC, does a high PSNR score guarantee success at Level C? (Answer: No, task performance is the metric).

- **Concept:** **Latent Space Representations**
  - **Why needed here:** The encoder does not output pixels but "compact representations" (tokens/features). Understanding the difference between pixel space and latent space is required to visualize the compression mechanism.
  - **Quick check question:** Why does manipulating a compact latent vector result in semantic changes to the reconstructed video while saving bandwidth?

## Architecture Onboarding

- **Component map:** Neural Encoder -> Transmitter (Residual coding) -> Bitstream -> Generative Decoder (Diffusion model) -> Reconstructed Video

- **Critical path:** The bottleneck is the **Generative Decoder**. While the Encoder runs in ~1.5s (1080p on 4090), the Decoder can take 21.5s on the same hardware for full models. The system is constrained by the iterative sampling steps of the diffusion process.

- **Design tradeoffs:**
  - **Extreme Compression vs. Latency:** To get <0.01% compression, you need a massive decoder and high latency. To get ~2s latency, you must relax compression slightly (send richer latents) and use a miniaturized model.
  - **Perception vs. Task:** The system optimizes for LPIPS/VOS. It may lose fine-grained texture details deemed irrelevant to the task.

- **Failure signatures:**
  - **Semantic Drift:** The reconstructed video looks "real" (good LPIPS) but contains objects or actions not present in the original (hallucination).
  - **Flickering/Temporal Instability:** The generative model fails to maintain consistency across frames, creating jitter in static scenes.

- **First 3 experiments:**
  1. **Baseline Fidelity vs. Rate:** Compare GVC vs. HEVC on MCL-JCV at fixed bitrates (0.008 bpp) using LPIPS to confirm the "6x bandwidth" claim in [Figure 2].
  2. **Task Preservation Test:** Run Video Object Segmentation (VOS) on reconstructed outputs to measure the semantic gap between Original → Reconstructed vs. HEVC → Reconstructed [Table 2].
  3. **Latency Profiling:** Measure end-to-end latency on a consumer GPU (RTX 4090) switching between the "miniaturized" model (rich latents) and the full model to verify the trade-off curve in [Table 3].

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the theoretical 0.01% compression rate be consistently achieved across diverse video content without critical loss of semantic detail?
- **Basis in paper:** [explicit] The abstract asks, "Whether a video can be compressed at an extreme compression rate as low as 0.01%?", while noting current results achieve 0.02% only "at some cases."
- **Why unresolved:** The paper demonstrates progress toward this target but does not confirm 0.01% as a reliable average rate, leaving the absolute lower bound for stable reconstruction undefined.
- **What evidence would resolve it:** Empirical results consistently hitting 0.01% compression across standard benchmarks (e.g., MCL-JCV) without significant degradation in LPIPS or downstream task accuracy.

### Open Question 2
- **Question:** How can the computational overhead be further reduced to support real-time, interactive applications?
- **Basis in paper:** [inferred] While the paper claims practicality via "consumer-grade GPUs with around 2-second latency," this latency renders the method unsuitable for real-time uses like video conferencing.
- **Why unresolved:** The proposed "compression-computation trade-off" currently optimizes for deployment feasibility rather than the speed required for synchronous communication.
- **What evidence would resolve it:** Demonstration of sub-200ms inference latency on edge devices while maintaining the extreme compression rates described.

### Open Question 3
- **Question:** To what extent does the reliance on generative priors introduce hallucinations that alter semantic fidelity in high-stakes scenarios?
- **Basis in paper:** [inferred] The methodology shifts from "pixel-level fidelity" to "perception-centric" reconstruction, relying on the decoder to synthesize content.
- **Why unresolved:** While downstream task performance (VOS) is measured, the paper does not quantify the rate of semantic errors (e.g., generating objects that were not present) caused by the generative model filling in missing data.
- **What evidence would resolve it:** A failure case analysis or new metric specifically evaluating semantic consistency between the original and reconstructed video frames.

## Limitations

- The framework's dependence on generative priors introduces potential semantic drift where hallucinated content may appear realistic but diverge from the original.
- The trade-off between compression ratio and computational burden is theoretically sound but the practical boundaries where this becomes ineffective are not quantified.
- The paper does not address potential security implications of generative reconstruction or the impact of network variability on the compression-computation trade-off.

## Confidence

**High Confidence:** The core theoretical framework connecting generative models to extreme compression rates is well-founded, supported by established diffusion model literature and Shannon-Weaver Level C communication theory. The bidirectional trade-off between transmitted latent richness and decoder computational load is conceptually sound and practically demonstrated.

**Medium Confidence:** The specific quantitative claims (6x bandwidth reduction, 2-second latency on RTX 4090, 79.28% VOS score at 0.0175 bpp) are supported by the experimental results but cannot be fully verified without access to the complete implementation. The generalizability of these results to different video content, resolutions, and downstream tasks remains uncertain.

**Low Confidence:** The practical deployment scenarios and real-world constraints are described at a high level without detailed analysis of edge cases, error handling, or scalability beyond the tested configurations.

## Next Checks

1. **Independent Implementation Verification:** Reconstruct the neural encoder and residual coding pipeline using publicly available diffusion video models. Validate whether 0.008 bpp achieves comparable LPIPS scores (0.180) on MCL-JCV without access to the original model weights.

2. **Cross-Task Generalization Test:** Apply the GVC framework to alternative downstream tasks (action recognition, video captioning) on standard benchmarks. Measure task performance degradation relative to the original video to quantify the semantic preservation limits beyond VOS.

3. **Resource-Constrained Deployment Analysis:** Profile the complete system (encoder + decoder) on progressively lower-end GPUs (RTX 3060, integrated graphics) and quantify the compression-latency trade-off curve. Identify the minimum hardware requirements where the framework remains competitive with traditional codecs.