---
ver: rpa2
title: 'TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards'
arxiv_id: '2507.18618'
source_url: https://arxiv.org/abs/2507.18618
tags:
- prompt
- textual
- reward
- prompts
- rewards
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TRPrompt introduces a novel query-dependent prompt optimization
  framework that uses textual rewards instead of numerical ones to guide prompt model
  training. The method trains a prompt model via supervised fine-tuning on synthetic
  datasets pairing prompts with natural language feedback, iteratively refining prompts
  based on their effectiveness in guiding target LLMs to correct answers.
---

# TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards

## Quick Facts
- arXiv ID: 2507.18618
- Source URL: https://arxiv.org/abs/2507.18618
- Reference count: 30
- State-of-the-art performance on mathematical reasoning datasets (GSMHard +1%, MATH +2% accuracy)

## Executive Summary
TRPrompt introduces a novel framework for query-dependent prompt optimization that uses natural language feedback instead of numerical rewards. The method trains a prompt model via supervised fine-tuning on synthetic datasets pairing prompts with textual critiques, iteratively refining prompts based on their effectiveness in guiding target LLMs to correct answers. Experiments on GSM8K, GSMHard, and MATH demonstrate state-of-the-art performance, achieving +1% accuracy on GSMHard and +2% on MATH compared to existing methods.

## Method Summary
TRPrompt is a query-dependent prompt optimization framework that replaces sparse numerical rewards with detailed textual feedback. The method uses a three-component system: a target LLM to be guided, a prompt model that generates task-specific prompts, and a textual reward model that provides natural language critiques. Through an iterative process of prompt generation, evaluation, and fine-tuning, the framework progressively improves its ability to generate effective prompts. The method includes an optimal reward search component that continuously refines the textual reward signal to maximize target model performance.

## Key Results
- Achieves +1% accuracy on GSMHard compared to state-of-the-art baselines
- Demonstrates +2% accuracy improvement on MATH dataset
- Shows superior generalization when transferring across datasets
- Excels on more challenging reasoning tasks where numerical rewards are sparse

## Why This Works (Mechanism)

### Mechanism 1: High-Resolution Learning Signal from Textual Feedback
Replacing scalar rewards with detailed textual critiques improves prompt model training for complex reasoning tasks. A textual reward model generates natural language feedback (e.g., "This prompt failed because it didn't guide the model to decompose the equation") rather than a binary success/failure signal. This rich signal is then used as a conditioning input during supervised fine-tuning of the prompt model, allowing it to internalize the qualities of an effective prompt.

### Mechanism 2: Iterative Self-Improvement Loop
A closed-loop process of prompt generation, evaluation, and fine-tuning allows the prompt model to progressively improve by learning from its own mistakes. The framework iterates over three steps: (1) the current prompt model generates prompts, (2) a reward model evaluates them, creating a synthetic training dataset of prompt-reward pairs, and (3) the prompt model is fine-tuned on this fresh data. In the next iteration, the newly fine-tuned model generates better prompts, which in turn yield more instructive rewards.

### Mechanism 3: Dual-Objective Optimization via Optimal Reward Search
The prompt optimization problem requires not only a capable prompt model but also an optimal textual reward to condition it. The process is framed as two interlinked objectives: (1) optimizing the prompt model to generate prompts for a given reward, and (2) finding the optimal reward that maximizes the target model's accuracy when used to condition the prompt model. The paper uses a train-free optimizer (TextGrad) to search for this optimal reward after each round of prompt model fine-tuning.

## Foundational Learning

- **Concept: Supervised Fine-Tuning (SFT)**
  - Why needed here: This is the core method for training the prompt model. The model learns to associate a textual reward with a high-quality prompt, enabling it to generate better prompts when conditioned on a good reward.
  - Quick check question: How does the training objective in TRPrompt differ from a standard language modeling task? (Answer: The loss is calculated on the prompt, conditioned on the question and the textual reward).

- **Concept: Query-Dependent Prompting**
  - Why needed here: This is the core task. It moves beyond finding a single "best" prompt for an entire dataset, instead generating a unique prompt tailored to each specific question.
  - Quick check question: Why is a query-dependent approach potentially more powerful than a task-level one? (Answer: A general prompt might be suboptimal for specific, hard instances, whereas a query-specific prompt can provide tailored guidance).

- **Concept: Prompt Optimization Landscape**
  - The search space for prompts is vast and discrete. Numerical rewards (e.g., accuracy) are sparse, offering little guidance. Textual rewards provide a dense, high-dimensional signal, making the optimization landscape more navigable. Understanding this difference is key to understanding the paper's central claim.
  - Why needed here: A new engineer must grasp that the innovation lies in changing the training signal, not just the model architecture.
  - Quick check question: Why is a binary reward (correct/incorrect) considered a weak signal for training? (Answer: It provides no information on why a prompt failed or how to improve it, making exploration inefficient).

## Architecture Onboarding

- **Component map:**
Target LLM <- Prompt Model <- Textual Reward Model

- **Critical path:**
1. Synthetic Data Generation: The current prompt model generates prompts for a set of questions. The reward model generates critiques.
2. Prompt Model Fine-Tuning: The prompt model is trained via SFT on the new pairs.
3. Optimal Reward Update: A search process (TextGrad) is used to find an updated optimal textual reward that maximizes accuracy from the newly fine-tuned prompt model. This entire loop repeats for K iterations.

- **Design tradeoffs:**
  - Reward Signal Richness vs. Computational Cost: Textual rewards are richer but require a costly LLM call to generate and process. The Optimal Reward Search is identified as the main computational bottleneck.
  - Self-Improvement (Same-Model) vs. Specialized Models: The authors choose to use the same model for all components to demonstrate self-improvement, but performance might differ with more powerful or specialized models.
  - Query-Dependent vs. Task-Level: Query-dependent optimization is more powerful but requires running the prompt model for every input, increasing inference cost compared to a single task-level prompt.

- **Failure signatures:**
  - Stagnation on easy datasets: If the target model already performs well, the training set will be dominated by positive feedback, providing a weak learning signal. Rebalancing the data or creating more nuanced negative feedback would be needed.
  - Performance degradation without optimal reward update: As shown in the ablation study, if the optimal reward is static, the prompt model overfits and performance declines.
  - High compute cost & slow training: The non-parallelizable optimal reward search step is a known bottleneck.

- **First 3 experiments:**
  1. Reproduce Core Ablation: Run the full TRPrompt pipeline for 4 iterations on a small dataset like GSMHard, and compare it to an ablation that removes the "Optimal Reward Update" step (only runs SFT). Confirm that the full pipeline's performance increases while the ablation's decreases.
  2. Vary Reward Signal: Instead of natural language feedback, use a simple binary correct/incorrect signal as the reward. Compare the prompt model's learning curve and final accuracy to the full TRPrompt method to quantify the impact of the high-resolution signal.
  3. Test Generalization: Train a prompt model on GSM8K and evaluate its zero-shot performance on the MATH dataset. Compare this cross-dataset transfer accuracy against a baseline model (e.g., QPO) to validate the generalization claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational bottleneck of the Optimal Reward Search step be mitigated to improve scalability?
- Basis in paper: Section 6 notes that "The Optimal Reward Search step involving Textgrad is computationally expensive and difficult to parallelize," identifying it as the main bottleneck that limits scalability.
- Why unresolved: The current implementation relies on a sequential, non-parallelizable optimization process (Textgrad) that accounts for approximately 70% of the total training time.
- What evidence would resolve it: A modified framework incorporating a parallelizable or more efficient search algorithm that reduces training time without compromising prompt quality.

### Open Question 2
- Question: Can TRPrompt effectively generalize to creative domains where numerical rewards are difficult to define?
- Basis in paper: Section 6 states, "Our framework can be extended to tasks where numerical rewards are difficult or unnatural to define, such as creative writing or poetry."
- Why unresolved: The current experiments are restricted to mathematical reasoning tasks with binary correctness metrics, leaving the framework's efficacy on subjective or open-ended tasks unproven.
- What evidence would resolve it: Evaluations on creative writing benchmarks demonstrating that textual rewards provide superior alignment compared to standard numerical metrics.

### Open Question 3
- Question: How can the training signal be refined to improve performance on datasets where the model already exhibits high accuracy?
- Basis in paper: Section 5.1 hypothesizes that "limited performance gains on GSM8K are partly due to an unbalanced training set dominated by positive feedback."
- Why unresolved: When most prompts yield correct answers, the resulting positive textual rewards lack the contrast necessary to provide a strong learning signal, leading to stagnation.
- What evidence would resolve it: Experiments utilizing data rebalancing or negative sampling strategies that result in significant accuracy gains on "easier" datasets like GSM8K.

## Limitations
- The iterative pipeline, particularly the TextGrad-based optimal reward search, is computationally expensive and non-parallelizable, limiting scalability.
- The method's success depends on having a reasonably capable textual reward model and prompt model from the start, with unclear robustness if initial components are weak.
- The framework relies entirely on synthetic data generated by the model itself, with potential quality issues and risk of generating misleading feedback.

## Confidence

**High Confidence**: The core mechanism of using high-resolution textual feedback instead of sparse numerical rewards for prompt optimization is well-supported by ablation studies and illustrative examples.

**Medium Confidence**: The claim of state-of-the-art performance and superior generalization across datasets is supported by results, but comparison is limited to few baselines and scalability concerns are acknowledged but not empirically tested.

**Low Confidence**: The paper does not provide sufficient evidence to fully support the claim of true "self-improvement" - it's unclear if improvements are due to genuine learning from mistakes or overfitting to synthetic data distribution.

## Next Checks
1. **Stress Test the Bootstrapping Assumption**: Run the pipeline with a degraded initial prompt model (e.g., one that generates random or very short prompts) and a reward model with lower capability. Measure if the self-improvement loop can still converge to good performance, or if it fails to bootstrap.

2. **Analyze Synthetic Data Quality Over Iterations**: For each iteration, sample and manually inspect the generated prompts, answers, and textual rewards. Quantify the proportion of high-quality vs. misleading feedback. This will help identify if the model is learning from genuine critique or noise.

3. **Benchmark Against a Broader Set of Baselines**: Re-run the experiments comparing TRPrompt against a wider range of recent prompt optimization methods, including those that use different reward signals or optimization strategies. This will provide a more robust assessment of its state-of-the-art claim.