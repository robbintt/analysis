---
ver: rpa2
title: Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse
arxiv_id: '2512.07400'
source_url: https://arxiv.org/abs/2512.07400
tags:
- task
- buffer
- replay
- feature
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a fundamental asymmetry in continual learning
  with experience replay: small replay buffers effectively preserve feature-space
  separability (deep forgetting) but fail to maintain classifier alignment (shallow
  forgetting), requiring much larger buffers for full performance recovery. The authors
  formalize this as the replay efficiency gap and explain it through an extension
  of Neural Collapse theory to the sequential setting.'
---

# Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse

## Quick Facts
- arXiv ID: 2512.07400
- Source URL: https://arxiv.org/abs/2512.07400
- Authors: Giulia Lanzillotta; Damiano Meier; Thomas Hofmann
- Reference count: 40
- One-line primary result: Minimal replay buffers preserve feature-space separability but fail to maintain classifier alignment, creating a fundamental asymmetry in continual learning efficiency.

## Executive Summary
This paper identifies a fundamental asymmetry in continual learning with experience replay: small replay buffers effectively preserve feature-space separability (deep forgetting) but fail to maintain classifier alignment (shallow forgetting), requiring much larger buffers for full performance recovery. The authors formalize this as the replay efficiency gap and explain it through an extension of Neural Collapse theory to the sequential setting. They show that without replay, past-task features drift to out-of-distribution subspaces, while minimal replay anchors features in the active subspace but leaves the classifier underdetermined due to statistical divergence between buffer and population distributions. The work provides theoretical bounds on linear separability and suggests that addressing the geometric artifacts induced by small buffers—rather than simply increasing buffer size—could improve continual learning efficiency.

## Method Summary
The study analyzes continual learning with experience replay across three protocols: class-incremental learning (CIL), domain-incremental learning (DIL), and task-incremental learning (TIL). Experiments use CIFAR100 (10 tasks, 10 classes/task), Tiny-ImageNet (10 tasks, 200 classes total), and CUB200 with pretrained ResNet50. The core method involves training on sequential tasks with task-balanced buffer sampling, then measuring both network accuracy (shallow forgetting) and linear probe accuracy on frozen features (deep forgetting). The analysis extends Neural Collapse theory to the sequential setting, deriving bounds on signal-to-noise ratio and feature drift, and characterizing how small buffers induce classifier under-determination through rank-deficient covariance and inflated class means.

## Key Results
- Small replay buffers (1-5%) effectively preserve feature-space separability of past tasks, preventing deep forgetting
- The same small buffers cause significant classifier misalignment, creating shallow forgetting that requires much larger buffers (>50%) to resolve
- Buffer-induced under-determination arises from Neural Collapse artifacts: rank-deficient covariances and inflated class means
- Weight decay plays a dual role in forgetting dynamics, both erasing residual past-task signal and preventing feature-norm explosion

## Why This Works (Mechanism)

### Mechanism 1: Feature Geometry Anchoring via Replay
Minimal replay buffers preserve feature-space separability by preventing drift to out-of-distribution subspaces. Past-task features without replay drift into the orthogonal complement S⊥ of the active subspace S spanned by current-task class means. Any non-zero replay introduces a mixture component within S, guaranteeing non-vanishing signal-to-noise ratio (SNR) asymptotically. Formally, with replay mixing coefficient π > 0, SNR(c1, c2) ∈ Θ(r²) as t → ∞, where r² = π²/(1-π)².

### Mechanism 2: Classifier Under-Determination from Buffer-Induced Collapse
Small replay buffers cause classifier misalignment because buffer statistics diverge from population statistics, leaving optimization under-determined. Neural Collapse on small buffers induces "strong collapse"—within-class variance vanishes (NC1), buffer covariance becomes rank-deficient (~K−1), and buffer means inflate radially. The classifier can achieve zero training error along many buffer-optimal decision boundaries that fail to generalize to population boundaries.

### Mechanism 3: Weight Decay's Dual Effect on OOD Signal
Weight decay both erases residual past-task signal in S⊥ and prevents feature-norm explosion, creating a trade-off in separability preservation. With weight decay λ > 0, S⊥ components decay as (1 − ηλ)^(t−t₀), reducing the numerator of SNR. Simultaneously, λ constrains centered class-mean norm βₜ from growing unboundedly, preventing denominator explosion.

## Foundational Learning

### Concept: Neural Collapse (NC1–NC4)
- Why needed here: Central to the paper's theory; defines asymptotic feature geometry in terminal phase of training
- Quick check question: Can you explain why within-class variability vanishing (NC1) leads to rank-deficient covariance for small buffers?

### Concept: Out-of-Distribution (OOD) detection via orthogonality
- Why needed here: Links forgetting to OOD behavior; forgotten features become orthogonal to active subspace
- Quick check question: Why does ID/OOD orthogonality imply maximal predictive uncertainty under NC?

### Concept: Signal-to-Noise Ratio (SNR) as linear separability proxy
- Why needed here: Quantifies deep forgetting; SNR lower-bounds Mahalanobis distance and thus linear separability
- Quick check question: How does SNR decompose across active subspace S and orthogonal complement S⊥?

## Architecture Onboarding

### Component map:
Feature extractor ϕ(x) (ResNet/ViT backbone) → feature space → Classifier head W_h (linear) → Replay buffer B (task-balanced) → Training loop (SGD with weight decay λ, learning rate η)

### Critical path:
1. Train on task sequence with buffer sampling
2. Monitor NC metrics (NC1 variance, NC2 ETF angles, NC3 alignment) to confirm stabilization
3. Measure deep forgetting via linear probe on frozen features; shallow forgetting via output accuracy
4. Compute SNR, mean/covariance gaps to diagnose shallow vs. deep forgetting

### Design tradeoffs:
- Buffer size vs. efficiency: Small buffers preserve features but misalign classifiers; large buffers align classifiers but increase memory
- Weight decay λ: Higher λ accelerates S⊥ erasure but prevents norm explosion; too high may over-constrain features
- Head structure: Single-head (CIL/DIL) maintains global NC; multi-head (TIL) reduces rank and avoids Minority Collapse but lacks cross-task alignment

### Failure signatures:
- Shallow forgetting >> deep forgetting: Likely buffer-induced under-determination (check covariance rank, mean inflation)
- Deep forgetting >> shallow forgetting: May indicate feature drift to S⊥ (check S⊥ norm evolution, weight decay settings)
- NC metrics not converging: Terminal phase not reached; increase training iterations or reduce learning rate

### First 3 experiments:
1. Ablate buffer size (0%, 1%, 5%, 10%, 100%) on Cifar100/CIL; plot deep vs. shallow forgetting curves and SNR to confirm efficiency gap
2. Vary weight decay λ (0, 1e-5, 1e-4, 1e-3) with fixed buffer 1%; measure βₜ growth and S⊥ decay to validate dual-effect mechanism
3. Compare single-head (CIL) vs. multi-head (TIL) with matched buffer; analyze rank of centered mean matrix and cross-task alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we design continual learning algorithms that explicitly correct the statistical artifacts of small replay buffers (specifically covariance deficiency and mean norm inflation) to bridge the replay efficiency gap without increasing memory size?
- Basis in paper: The abstract states that the work "challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay." Section 4.3 further elaborates that "to bridge the gap... one must explicitly counteract the effects of Neural Collapse."
- Why unresolved: The paper identifies the geometric mechanism causing the gap (the under-determined classifier) but does not propose or test a specific algorithmic correction for these artifacts
- What evidence would resolve it: An algorithm that normalizes feature norms or regularizes covariance rank on small buffers, demonstrating that shallow forgetting can be reduced to deep forgetting levels without increasing buffer capacity

### Open Question 2
- Question: Is the Neural Collapse (NC) structure inherently beneficial or detrimental to the stability of past-task knowledge in continual learning?
- Basis in paper: The final discussion raises this broad inquiry: "Is the Neural Collapse structure beneficial or detrimental in this context? Our results suggest that while NC enhances feature organization, it also exacerbates the mismatch between replay and true distributions."
- Why unresolved: The paper presents a dual-edged view where NC aids feature separability (deep retention) but creates geometric rigidity that harms classifier alignment (shallow forgetting) on small buffers
- What evidence would resolve it: Experiments manipulating the degree of Neural Collapse (e.g., via explicit ETF regularization or anti-collapse loss terms) to correlate the strength of NC properties with the magnitude of the replay efficiency gap

### Open Question 3
- Question: How does classifier head initialization influence the growth of class feature norms ($\beta_t$) across sequential tasks, and does stabilizing this growth improve forgetting metrics?
- Basis in paper: Section 4.2.2 notes an observed drift in feature norms and states: "We hypothesize this is an artifact of classifier head initialization in sequential settings... we leave a comprehensive investigation of this finding to future research."
- Why unresolved: While the paper validates that norm-matching initialization suppresses growth, it found negligible impact on final accuracy, leaving the causal link between initialization, norm growth, and forgetting performance unclear
- What evidence would resolve it: A systematic ablation across different initialization scales correlating the rate of $\beta_t$ growth with Signal-to-Noise Ratio (SNR) decay and final shallow forgetting rates

### Open Question 4
- Question: How do the geometric dynamics of deep and shallow forgetting differ during the transient phase of training (before the Terminal Phase of Training)?
- Basis in paper: The limitations section states the analysis adopts an "asymptotic perspective, thereby neglecting the transient dynamics of early training, which are likely central to the onset of forgetting."
- Why unresolved: The theoretical bounds rely on the convergence of Neural Collapse, yet practical continual learning often involves early stopping or shorter task durations where these asymptotic assumptions do not hold
- What evidence would resolve it: An analysis of subspace drift ($S_t$ vs $S_\perp$) and classifier alignment trajectories from initialization to the onset of NC, specifically looking for the emergence of the efficiency gap in pre-asymptotic regimes

## Limitations
- Theoretical analysis assumes Neural Collapse properties stabilize during training, which may not hold in practice for deeper architectures or shorter training schedules
- Bounds rely on asymptotic conditions (t → ∞) that may not be reached in finite-task settings
- Buffer-induced classifier under-determination mechanism assumes i.i.d. sampling from population, but task boundaries and class ordering effects are not fully characterized
- Extension of NC theory to sequential settings requires empirical validation beyond CIFAR100/Tiny-ImageNet benchmarks

## Confidence

### Confidence Labels for Major Claim Clusters
- **High Confidence**: The existence of the replay efficiency gap (asymmetry between deep and shallow forgetting) is well-supported by experimental evidence across multiple architectures and datasets
- **Medium Confidence**: The mechanism linking minimal replay to feature-space anchoring (SNR preservation) is theoretically sound but relies on NC stabilization assumptions that may not always hold
- **Medium Confidence**: The buffer-induced classifier under-determination mechanism is mechanistically plausible but lacks direct empirical validation beyond the observed performance degradation

## Next Checks

### Three Concrete Next Validation Checks
1. **NC Stabilization Verification**: Empirically measure NC1–NC3 metrics during training across different architectures (ResNet, ViT) to verify the assumption that NC properties stabilize by the terminal phase. Track how quickly S locks and whether this varies with buffer size

2. **Buffer Sampling Bias Analysis**: Systematically vary buffer sampling strategies (task-balanced vs. reservoir vs. class-balanced) and measure the resulting rank-deficiency in buffer covariance matrices and classifier performance. Quantify the statistical divergence between buffer and population distributions

3. **Cross-Task Alignment in Multi-Head Settings**: Extend the analysis to TIL with cross-task classifier alignment. Measure whether small buffers in multi-head settings still induce under-determination when cross-task boundaries are explicitly modeled, and whether this mitigates the efficiency gap