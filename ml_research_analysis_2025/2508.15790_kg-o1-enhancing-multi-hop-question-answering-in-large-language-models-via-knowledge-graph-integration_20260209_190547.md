---
ver: rpa2
title: 'KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via
  Knowledge Graph Integration'
arxiv_id: '2508.15790'
source_url: https://arxiv.org/abs/2508.15790
tags:
- reasoning
- performance
- arxiv
- multi-hop
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: KG-o1 improves multi-hop reasoning in LLMs by integrating knowledge
  graphs to guide explicit reasoning paths. The approach constructs complex subgraphs
  from KBs, generates logical reasoning paths, and uses these to create training data
  for supervised fine-tuning and direct preference optimization.
---

# KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration

## Quick Facts
- **arXiv ID:** 2508.15790
- **Source URL:** https://arxiv.org/abs/2508.15790
- **Reference count:** 18
- **Primary result:** KG-o1 achieves up to 69.36% exact match on KG-MHQA benchmark

## Executive Summary
KG-o1 is a framework that enhances multi-hop reasoning in large language models by integrating knowledge graph paths into the training process. The approach constructs complex subgraphs from the FB15k knowledge base, generates logical reasoning paths, and uses these to create training data for supervised fine-tuning and direct preference optimization. KG-o1 models outperform existing LRMs on four multi-hop reasoning datasets, with the Qwen2.5-14B-based KG-o1 achieving strong performance while demonstrating generalization capabilities beyond the training data distribution.

## Method Summary
KG-o1 integrates knowledge graph reasoning into LLM training through a multi-stage pipeline. First, it constructs complex subgraphs from the FB15k knowledge base and generates logical reasoning paths using entity clustering and placeholder substitution. These paths guide ChatGPT-4o to create multi-hop question-answer pairs with detailed "brainstorming" reasoning traces. The resulting KG-MHQA dataset is used for supervised fine-tuning of base LLMs, followed by Self-improved Adaptive Direct Preference Optimization (DPO) using the model's own outputs to generate preference pairs. The training uses LlamaFactory with DeepSpeed ZeRO-3 on A800 80GB GPUs, with specific hyperparameters for SFT and DPO stages.

## Key Results
- KG-o1 models outperform existing LRMs on four multi-hop reasoning datasets
- Qwen2.5-14B-based KG-o1 achieves 69.36% exact match on KG-MHQA benchmark
- KG-o1 demonstrates strong generalization to public datasets (HotpotQA, 2WikiMultiHopQA, MINTQA)
- The framework successfully reduces reasoning drift by grounding "slow thinking" in verifiable KG paths

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Injecting explicit Knowledge Graph (KG) paths into training data reduces reasoning drift by grounding "slow thinking" in a priori logic.
- **Mechanism:** The framework extracts logical paths from KG subgraphs (FB15k) and uses them to guide a teacher model (ChatGPT-4o) to generate "brainstorming" reasoning traces. This ensures the reasoning steps follow a verifiable logical chain rather than hallucinated connections.
- **Core assumption:** The structure of the KG paths accurately represents the necessary cognitive steps for solving the target multi-hop questions.
- **Evidence anchors:** Mentions that LLM CoTs "often deviate from real or a priori reasoning paths," which KGs explicitly represent; describes the "iterative procedure" where reasoning is refined using knowledge retrieved from the KG.

### Mechanism 2
- **Claim:** Supervised Fine-Tuning (SFT) on extensive "brainstorming" transcripts enables smaller models to internalize "System 2" (slow) reasoning behaviors.
- **Mechanism:** The paper creates a "KG-MHQA SFT dataset" where the output format mimics a protracted exploration of the problem before concluding. SFT forces the student model to mimic this high-compute behavior during inference.
- **Core assumption:** The quality of the teacher model's (ChatGPT-4o) "brainstorming" logic is high enough that imitating it improves reasoning, rather than just increasing output length.
- **Evidence anchors:** States the goal to "train LLMs to imitate long-term reasoning" via the dataset; Figure 3 analysis notes that "reasoning ability improvement is substantially greater" in KG-o1 compared to base models.

### Mechanism 3
- **Claim:** Adaptive rejection sampling stabilizes Direct Preference Optimization (DPO) by dynamically generating negative examples based on the model's specific failure modes.
- **Mechanism:** The "Self-improved Adaptive DPO" strategy uses the SFT model itself to generate candidates. If the model answers correctly, it serves as a positive sample against the original SFT data (negative). If incorrect, the original SFT data is positive, and the model's output is negative.
- **Core assumption:** The base SFT model is capable enough to occasionally generate correct answers that are "better" or "distinct" from the distilled training data.
- **Evidence anchors:** "We implement a dual path contrastive data construction protocol using the correctness of the answers as the gold standard"; Table 4 shows "SFT + SADPO" outperforming "SFT + DPO" and "SFT + GRPO."

## Foundational Learning

- **Concept: Knowledge Graph (KG) Subgraph Extraction**
  - **Why needed here:** The system relies on selecting specific "triplet units" from FB15k to build reasoning paths. Without understanding how entities expand into subgraphs, you cannot debug the data generation pipeline.
  - **Quick check question:** How does the system decide which relation types to include when expanding an initial entity into a subgraph?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** The final stage of training uses DPO to align the model. You need to distinguish between standard DPO (using fixed human/Mistral data) and this paper's "Self-improved Adaptive" approach.
  - **Quick check question:** In the "Self-improved Adaptive DPO" phase, which model generates the *negative* response when the SFT model answers incorrectly?

- **Concept: Chain-of-Thought (CoT) vs. Long-term Thinking**
  - **Why needed here:** The paper explicitly contrasts standard CoT (short, direct) with their "o1-style" brainstorming (iterative, exploratory). Understanding this distinction is vital for evaluating model outputs.
  - **Quick check question:** What structural token does the model use to separate the "Think" (brainstorming) phase from the final "Output"?

## Architecture Onboarding

- **Component map:** FB15k KG -> Subgraph Selection & Logical Path Generation -> ChatGPT-4o (teacher) -> KG-MHQA SFT dataset -> Qwen2.5/Llama 3.1 (student) -> Self-improved Adaptive DPO
- **Critical path:** The **Logical Path Generation** (Methodology, Page 3). If the conversion of clustered entities into logical triplets fails or creates ambiguous identifiers, the subsequent question generation will be nonsensical.
- **Design tradeoffs:**
  - **Distillation Source:** Uses ChatGPT-4o to generate "slow thinking" traces. This ensures high-quality reasoning syntax but ties the student's "thought process" to OpenAI's logic style, potentially limiting the student model's native reasoning capabilities.
  - **Inference Cost:** Figure 4(b) shows KG-o1 models use significantly more tokens than base models. The tradeoff is higher latency/cost for higher accuracy.
- **Failure signatures:**
  - **Knowledge Errors (Figure 4a):** The model reasons correctly but selects the wrong entity. This suggests the KG context was insufficient or the model's internal knowledge is conflicting.
  - **Logic Errors:** The model fails to connect multiple facts (e.g., merging unrelated documents). Check the "Entity Clustering" logic in the data prep stage.
- **First 3 experiments:**
  1. **Verify Data Integrity:** Sample 5 logical paths from the `KG-MHQA` train split and manually verify that the "Brainstorming Session" text strictly follows the entity relations in the path.
  2. **SFT-Only Baseline:** Train the 7B model on the SFT data *without* the DPO stage to measure the isolated impact of the "brainstorming" data on Exact Match (EM).
  3. **Ablate the "Brainstorming":** Create a minimal dataset where the "Think" section is replaced with standard CoT (no multiple sessions). Train and compare against the full KG-o1 to quantify the value of the extended reasoning format.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the alignment process be refined to evaluate specific segments of the reasoning chain rather than treating the thinking process as a monolithic whole? The Conclusion states that the current approach "primarily treats the thinking process as a whole for contrastive learning," and identifies "more granular alignment" as a necessary direction for future exploration.

- **Open Question 2:** Can the KG-o1 framework be successfully adapted to handle multi-hop questions requiring comparative or boolean answers? The authors acknowledge in the Conclusion that the current data construction focused solely on "free-text answers, overlooking other answer formats such as comparisons or yes/no judgments."

- **Open Question 3:** To what extent is KG-o1's performance dependent on the proprietary teacher model (ChatGPT-4o) used for data distillation? The Methodology section details the use of ChatGPT-4o to generate both the complex questions and the "brainstorming" reasoning sessions. The paper does not analyze if the reasoning quality is inherent to the KG or the specific teacher's style.

## Limitations
- The approach relies heavily on ChatGPT-4o for generating training data, creating uncertainty about reproducibility with open-source models
- The extended "brainstorming" format significantly increases inference cost without comprehensive analysis of the computational tradeoff
- The superiority of Self-improved Adaptive DPO over standard methods lacks comparison to alternative preference optimization approaches

## Confidence

- **High Confidence:** The multi-hop reasoning improvements on KG-MHQA test set (69.36% EM for Qwen2.5-14B) are well-documented and reproducible with the provided datasets.
- **Medium Confidence:** Claims about reduced "reasoning drift" and improved generalization to public datasets are supported by results but lack ablation studies isolating the KG integration component.
- **Low Confidence:** The superiority of "Self-improved Adaptive DPO" over standard DPO is based on a single ablation study without comparison to other preference optimization variants.

## Next Checks

1. **Test generalization robustness:** Evaluate KG-o1 models on multi-hop questions from domains absent in FB15k (e.g., biomedical or legal domains) to assess whether KG integration teaches reasoning patterns or just FB15k-specific associations.

2. **Analyze reasoning path quality:** Sample 50 KG-o1 outputs and categorize failures as knowledge errors vs. logic errors. This would validate whether the "reasoning drift" reduction claim holds across failure modes.

3. **Benchmark inference efficiency:** Measure the actual inference time increase from the "brainstorming" format and calculate the accuracy-per-token ratio compared to standard CoT approaches to quantify the computational tradeoff.