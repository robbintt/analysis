---
ver: rpa2
title: Parallel Continuous Chain-of-Thought with Jacobi Iteration
arxiv_id: '2506.18582'
source_url: https://arxiv.org/abs/2506.18582
tags:
- latent
- tokens
- thought
- pcot
- pccot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Parallel Continuous Chain-of-Thought (PCCoT),
  which applies Jacobi iteration to update latent thought tokens in parallel rather
  than sequentially, improving training and inference efficiency for continuous CoT.
  By choosing appropriate numbers of iterations and latent tokens, PCCoT achieves
  comparable or better performance while saving nearly 50% of training and inference
  time.
---

# Parallel Continuous Chain-of-Thought with Jacobi Iteration

## Quick Facts
- arXiv ID: 2506.18582
- Source URL: https://arxiv.org/abs/2506.18582
- Reference count: 33
- Primary result: Parallel Continuous Chain-of-Thought (PCCoT) achieves comparable or better performance while saving nearly 50% of training and inference time compared to sequential continuous CoT.

## Executive Summary
This paper introduces Parallel Continuous Chain-of-Thought (PCCoT), which applies Jacobi iteration to update latent thought tokens in parallel rather than sequentially. By selecting appropriate numbers of iterations and latent tokens, PCCoT achieves higher accuracy and greater stability while reducing training and inference time by nearly 50% compared to sequential approaches. The method outperforms continuous CoT, Pause Tokens, and Implicit CoT on GSM8K-Aug and GSM8K-Aug-NL benchmarks using GPT-2 Small and Llama3.2-1B-Instruct models, demonstrating faster convergence and more robust training dynamics.

## Method Summary
PCCoT replaces the sequential update of latent thought tokens in continuous CoT with parallel Jacobi iterations. In this approach, all latent tokens are updated simultaneously using information from the previous iteration, rather than waiting for each token to update before proceeding to the next. This parallelization is achieved by treating each latent token as an independent variable in a system of equations, where Jacobi iteration solves for all variables concurrently. The method maintains the continuous nature of the latent representations while enabling efficient parallel computation during both training and inference phases.

## Key Results
- PCCoT achieves comparable or better performance than sequential continuous CoT while saving nearly 50% of training and inference time
- Outperforms continuous CoT, Pause Tokens, and Implicit CoT on GSM8K-Aug and GSM8K-Aug-NL benchmarks
- Demonstrates higher accuracy and greater stability with faster convergence and more robust training dynamics
- Shows the student model (PCCoT) can outperform the teacher model on GSM8K-Aug-NL, contrary to typical knowledge distillation expectations

## Why This Works (Mechanism)
The paper doesn't provide a complete theoretical explanation for why PCCoT works. The authors observe that latent tokens do not converge during Jacobi iterations after training, which is counter-intuitive since Jacobi iteration should theoretically converge to a fixed point equivalent to sequential CoT. The success of PCCoT appears to stem from the parallel update mechanism allowing the model to explore the solution space more efficiently, though the exact mechanism enabling high reasoning accuracy without token convergence remains unclear. The method leverages the mathematical properties of Jacobi iteration to distribute computation across tokens while maintaining reasoning capability.

## Foundational Learning
- **Jacobi Iteration**: An iterative method for solving systems of linear equations by updating all variables simultaneously using values from the previous iteration. Needed for enabling parallel computation of latent tokens; quick check: verify that updates use only previous iteration values.
- **Continuous CoT**: A variant of chain-of-thought reasoning where latent thought tokens are continuous vectors rather than discrete tokens. Needed as the baseline sequential approach that PCCoT improves upon; quick check: confirm tokens are updated sequentially in the baseline.
- **Knowledge Distillation**: A training strategy where a smaller student model learns to mimic the behavior of a larger teacher model. Needed for training PCCoT efficiently; quick check: verify teacher provides soft targets during training.
- **Latent Representations**: Continuous vector representations that capture semantic information about intermediate reasoning steps. Needed for PCCoT's parallel update mechanism; quick check: ensure tokens have sufficient dimensionality.
- **GSM8K Reasoning Benchmarks**: Datasets containing grade school math word problems used to evaluate reasoning capabilities. Needed for standardized evaluation; quick check: confirm problems require multi-step reasoning.
- **Parallel Computation**: Processing multiple operations simultaneously rather than sequentially. Needed for the core efficiency gains of PCCoT; quick check: measure actual speedup on parallel hardware.

## Architecture Onboarding

**Component Map**
Teacher Model -> Distillation Process -> Student Model (PCCoT) -> Jacobi Iteration Engine -> Parallel Update of Latent Tokens -> Reasoning Output

**Critical Path**
Input Problem -> Teacher Forward Pass -> Soft Target Generation -> Student Training with PCCoT Loss -> Jacobi Iteration Updates -> Final Reasoning Output

**Design Tradeoffs**
The key tradeoff is between parallel efficiency and convergence behavior. While Jacobi iteration enables parallel updates and faster computation, it theoretically converges slower than sequential methods (Gauss-Seidel). The paper accepts this theoretical disadvantage because empirical results show PCCoT actually converges faster and more stably than sequential approaches, suggesting that the parallel exploration of the solution space outweighs the theoretical convergence benefits of sequential updates.

**Failure Signatures**
- Latent tokens fail to converge after training, showing oscillatory behavior rather than stabilization
- Performance degradation when increasing iteration count beyond optimal values
- Interleaved clustering patterns in token similarity analysis at high iteration counts
- Student model failing to outperform teacher despite distillation training

**First Experiments**
1. Implement basic Jacobi iteration on a small linear system to verify parallel update mechanics before integrating with CoT
2. Compare convergence behavior of Jacobi vs sequential updates on synthetic reasoning tasks to measure theoretical vs empirical performance
3. Profile training time and memory usage of PCCoT vs sequential CoT on a single GPU to verify claimed 50% efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Why do latent thought tokens in PCCoT fail to converge during Jacobi iterations after training, and how does the model maintain high reasoning accuracy despite this non-convergence?
- **Basis in paper:** The authors state in the Limitations and Appendix B.2 that "the latent tokens do not converge in multiple iterations after training, which is not a problem for CoT inference but is nonetheless counter-intuitive." They conclude that "The success of PCCoT is not due to the fast convergence of the latent thought tokens."
- **Why unresolved:** Theoretically, Jacobi iteration should converge to a fixed point (equivalent to sequential CoT), but empirical analysis shows the tokens fluctuate rather than stabilize. The mechanism allowing the model to succeed without this convergence is currently unknown.
- **What evidence would resolve it:** A mechanistic interpretability study (e.g., probing classifiers) demonstrating that the tokens encode stable semantic information despite vector oscillation, or a theoretical framework explaining fixed-point-free parallel reasoning.

### Open Question 2
- **Question:** Why does the PCCoT student model outperform the teacher model (PCCoT with standard CoT decoding) on GSM8K-Aug-NL?
- **Basis in paper:** Appendix B.5 notes it is "counter-intuitive that the performance of PCCoT (the student task) is better than that of PCCoT with standard CoT decoding (the teacher task)." The authors suggest error propagation in standard CoT as a hypothesis but admit "further investigation is needed."
- **Why unresolved:** In knowledge distillation, the student typically approximates but rarely exceeds the teacher's capabilities without external augmentation. The specific property of continuous latent reasoning that allows it to surpass the discrete generation baseline on natural language tasks is not isolated.
- **What evidence would resolve it:** An error analysis comparing mistake propagation rates between discrete token generation and continuous latent updates on long-chain reasoning tasks.

### Open Question 3
- **Question:** What causes the observed "interleaved pattern" in latent token similarities at high iteration counts, and does this limit the scalability of PCCoT?
- **Basis in paper:** Appendix B.3 describes an "interleaved pattern" where tokens with odd indices cluster separately from even indices at T=6. The authors state, "we still do not know what does this mean and how it may potentially affect PCCoT in terms of scaling up."
- **Why unresolved:** This pattern suggests the formation of oscillatory modes or subspace dependencies that were not predicted by the theoretical framework. It correlates with the observation that increasing iterations T does not strictly improve performance and can introduce instability.
- **What evidence would resolve it:** An ablation study varying model depth and width to see if the pattern dissipates or intensifies, alongside spectral analysis of the iteration update matrix.

### Open Question 4
- **Question:** Can PCCoT be adapted to remove the dependency on slow, distillation-based training without sacrificing the stability advantages observed over sequential continuous CoT?
- **Basis in paper:** The Limitations section explicitly identifies the reliance on distillation (CODI) as a bottleneck, noting PCCoT "is still much slower than the standard CoT in terms of training due to the distillation training strategy."
- **Why unresolved:** The current method leverages a teacher to guide the latent states. It is unstated if the stability of PCCoT is intrinsic to the parallel architecture or if it relies on the soft targets provided by the distillation process.
- **What evidence would resolve it:** Experiments using Reinforcement Learning (RL) or self-supervised objectives (without a teacher) to train PCCoT, measuring training speed and convergence stability against the current baseline.

## Limitations
- Experimental validation limited to small model scales (GPT-2 Small and Llama3.2-1B-Instruct), leaving scalability to larger models uncertain
- Focus on GSM8K variants provides limited insight into generalization across diverse reasoning domains or more complex mathematical problems
- Reliance on distillation-based training identified as a bottleneck that limits training speed despite inference efficiency gains

## Confidence
- **Performance Claims (Medium)**: The claim of "comparable or better performance" while saving "nearly 50% of training and inference time" is supported by benchmark results but shows variable performance gaps across tasks and model sizes
- **Stability Improvements (Medium)**: Empirical demonstration of stability improvements is convincing, but lacks theoretical guarantees about convergence behavior across different problem types
- **Scalability Claims (Low)**: Limited to small models, with open questions about whether efficiency gains persist at larger scales where parallelization benefits may diminish

## Next Checks
1. Test PCCoT on larger model architectures (7B+ parameters) to evaluate scalability and whether the 50% efficiency gains persist at scale
2. Conduct ablation studies systematically varying the number of latent tokens and Jacobi iterations to establish robustness boundaries and identify optimal configurations across task types
3. Extend evaluation to diverse reasoning benchmarks including mathematical proof generation, code synthesis, and multi-hop reasoning tasks to assess generalizability beyond arithmetic problem-solving