---
ver: rpa2
title: Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition
arxiv_id: '2602.01967'
source_url: https://arxiv.org/abs/2602.01967
tags:
- accent
- speech
- expert
- training
- routing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of automatic speech recognition
  (ASR) for accented speech, where most models struggle with non-native accents due
  to limited training data diversity. To tackle this, the authors propose MOE-CTC,
  a Mixture-of-Experts (MoE) architecture with intermediate Connectionist Temporal
  Classification (CTC) supervision.
---

# Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition

## Quick Facts
- arXiv ID: 2602.01967
- Source URL: https://arxiv.org/abs/2602.01967
- Authors: Wonjun Lee; Hyounghun Kim; Gary Geunbae Lee
- Reference count: 22
- Key outcome: MOE-CTC achieves up to 29.3% relative WER reduction over FastConformer baselines on MCV-ACCENT, with largest gains on unseen accents (27.8% relative WER reduction)

## Executive Summary
This paper addresses the challenge of automatic speech recognition (ASR) for accented speech by proposing MOE-CTC, a Mixture-of-Experts (MoE) architecture with intermediate Connectionist Temporal Classification (CTC) supervision. The method uses accent-aware routing during training to encourage expert specialization for specific accents, then transitions to accent-agnostic inference without accent labels. Each expert is equipped with its own CTC head to align routing decisions with transcription quality, and a routing-augmented loss stabilizes optimization. Experiments on the MCV-ACCENT benchmark demonstrate consistent gains across seen and unseen accents in both low- and high-resource conditions.

## Method Summary
MOE-CTC extends FastConformer with 3 MoE modules containing 5 experts each, where each expert has its own CTC head. The model employs a two-stage training strategy: Stage 1 uses accent-aware routing with bias terms and accent classification loss to encourage expert specialization, while Stage 2 removes accent-specific signals for generalization. The routing-augmented loss couples routing probabilities to expert CTC losses, encouraging selection of experts that yield better transcriptions. At inference, the model operates without accent labels, relying on learned routing patterns.

## Key Results
- MOE-CTC achieves 29.3% relative WER reduction over strong FastConformer baselines
- Largest improvements observed on unseen accents (up to 27.8% relative WER reduction)
- Consistent gains across both low- and high-resource conditions
- Accent-agnostic fine-tuning improves performance on both seen (5.8→5.5 WER) and unseen accents (14.2→12.5 WER)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Accent-aware routing during training followed by accent-agnostic fine-tuning enables expert specialization while preserving label-free inference.
- **Mechanism:** During Stage 1, a biasing term α·1[j=a_i] is added to routing logits to guide each accent toward its designated expert, coupled with an auxiliary accent classification loss. Stage 2 removes these signals, allowing the router to autonomously select experts based on learned acoustic patterns.
- **Core assumption:** Experts that learned accent-specific patterns during supervised routing will retain useful specialization when routing becomes unsupervised.
- **Evidence anchors:**
  - [Section 4.2] "We define the accent-biased logit: L̃ᵢ,ⱼ = Lᵢ,ⱼ + α·1[j=aᵢ]"
  - [Section 4.4] "The second stage promotes generalization by removing these accent-specific signals, allowing the router to autonomously select experts."
  - [Table 4] Accent-agnostic fine-tuning reduces WER from 5.8→5.5 (seen) and 14.2→12.5 (unseen) for MOE-CTC.
  - [corpus] Neighbor papers on accent/dialect ASR (e.g., Zero-Shot Context-Aware ASR for Diverse Arabic Varieties) similarly address label-free inference but use different approaches (context-aware decoding vs. learned routing).
- **Break condition:** If Stage 1 training is too short or α is too weak, experts may not develop sufficient specialization; if too long, the router may overfit to accent labels and fail to generalize.

### Mechanism 2
- **Claim:** Equipping each expert with its own CTC head aligns routing decisions with transcription quality rather than just accent discrimination.
- **Mechanism:** Each expert outputs through a dedicated CTC head, producing expert-specific losses L_CTC^(ℓ,j). The router is trained to minimize a weighted combination of these losses, creating feedback that preferentially routes to experts yielding better transcriptions.
- **Core assumption:** Transcription quality is a meaningful proxy for whether an expert has learned useful representations for a given input.
- **Evidence anchors:**
  - [Abstract] "Each expert is equipped with its own CTC head to align routing with transcription quality."
  - [Section 4.3] "Expert-level CTC supervision directly links routing to transcription quality, encouraging experts to specialize in improving ASR accuracy rather than only modeling accent distinctions."
  - [Table 1] MOE-CTC achieves 6.5% additional WERR over ACCENT-MOE on seen accents and 7.5% on unseen.
  - [corpus] No direct corpus comparison; related accent-ASR papers do not use expert-level CTC supervision.
- **Break condition:** If CTC heads are shared globally instead of per-expert, specialization degrades (Table 11 shows +8.0% WER degradation with global sharing).

### Mechanism 3
- **Claim:** The routing-augmented loss L_local stabilizes optimization by coupling routing weights to expert performance.
- **Mechanism:** L_local = Σₗ Σⱼ gⱼ^(ℓ) · L_CTC^(ℓ,j) weights each expert's CTC loss by its routing probability. Minimizing this encourages the router to assign higher weights to lower-loss experts.
- **Core assumption:** Gradient flow through routing weights can meaningfully adjust expert selection behavior.
- **Evidence anchors:**
  - [Section 4.3] "By minimizing this loss, the router is encouraged to assign higher weights to experts that yield lower CTC loss."
  - [Equation 5] Formal definition of L_local combining routing probabilities with expert CTC losses.
  - [corpus] No corpus evidence; this specific loss formulation appears novel to this work.
- **Break condition:** If β coefficient is too high, the model may collapse to always selecting the globally best expert; if too low, routing-quality coupling is weak.

## Foundational Learning

- **Concept: Connectionist Temporal Classification (CTC)**
  - **Why needed here:** MOE-CTC extends intermediate CTC supervision to individual experts; understanding CTC's marginalization over alignments explains why auxiliary heads provide stable gradient signals.
  - **Quick check question:** Why does CTC not require frame-level alignments, and how does this property enable auxiliary supervision at intermediate layers?

- **Concept: Sequence-level vs. Token-level MoE Routing**
  - **Why needed here:** The paper uses sequence-level routing (entire utterance to experts) rather than token-level; this matches accent being an utterance-level attribute.
  - **Quick check question:** For a 10-second audio clip with 500 frames, how does sequence-level routing differ computationally from token-level routing?

- **Concept: Top-K Routing with Renormalization**
  - **Why needed here:** The model uses K=2, activating only 2 of 5 experts per utterance; understanding sparsity tradeoffs is critical for efficiency.
  - **Quick check question:** If K=2 and expert probabilities are [0.5, 0.3, 0.15, 0.04, 0.01], what are the renormalized weights for the selected experts?

## Architecture Onboarding

- **Component map:**
  FastConformer encoder backbone (12-18 layers depending on size) -> 3 MoE modules at layers 4, 8, 12 -> Each MoE: 5 experts + router -> Each expert: 2-layer FFN with ReLU + dedicated CTC head + projection back to hidden dim -> Router: mean-pools utterance → linear layer → softmax → top-K selection

- **Critical path:**
  1. Audio → FastConformer layers 1-3
  2. Layer 4 output → MoE module → expert CTC heads compute L_local → residual addition
  3. Repeat at layers 8, 12
  4. Final encoder output → global CTC head → transcription
  5. Loss: L = L_CTC + β·L_local + γ·L_accent (Stage 1) or L = L_CTC + β·L_local (Stage 2)

- **Design tradeoffs:**
  - More experts (8 vs 5): +1.9% WERR on unseen but +7% parameters (Table 10)
  - Layer-wise CTC head sharing vs. full separation: -5% parameters but +2-4% WER (Table 11)
  - K=2 routing: balances compute and expert diversity; K=1 may miss complementary patterns

- **Failure signatures:**
  - Router collapse: one expert receives >90% of samples (check routing distribution entropy)
  - Stage 1 overfitting: validation WER improves but test WER on unseen accents degrades
  - CTC head divergence: expert CTC losses vary wildly (>2x difference) indicates unstable specialization

- **First 3 experiments:**
  1. **Ablation on α (bias strength):** Train with α ∈ {0, 1, 2, 4} to verify accent-aware routing contribution; expect U-shaped curve where too little/too much bias hurts generalization.
  2. **Expert routing visualization:** After Stage 2, plot confusion matrix of (true accent, top-1 expert) to verify adaptive redistribution (e.g., Canadian→US, Scottish→England patterns as in Figure 2).
  3. **Oracle routing upper bound:** Provide ground-truth accent labels at inference with α=∞; gap between oracle and learned routing indicates room for router improvement (Table 5 shows 5.5→5.3 WER gap).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the reliance on explicit accent labels during Stage 1 training be replaced by unsupervised clustering without degrading performance?
- Basis in paper: [explicit] The "Limitations" section states the model "relies on accent labels during early training, limiting applicability in unsupervised settings."
- Why unresolved: The accent-aware routing bias (Eq. 3) and auxiliary classification loss (Eq. 4) currently require ground-truth accent indices ($a_i$) to initialize expert specialization.
- What evidence would resolve it: Experiments replacing ground-truth labels in Stage 1 with pseudo-labels derived from acoustic clustering (e.g., x-vector clustering) and comparing the resulting WER to the supervised baseline.

### Open Question 2
- Question: How can the architecture be adapted to handle intra-utterance accent variation or code-switching?
- Basis in paper: [explicit] The "Limitations" section notes that sequence-level routing "assumes discrete accent boundaries" and "may not generalize to mixed or code-switched speech."
- Why unresolved: The routing network uses mean-pooling over the entire sequence (Eq. 2), forcing a single expert assignment per utterance rather than allowing segment-level adaptation.
- What evidence would resolve it: Evaluation on code-switched benchmarks using a modified, token-level routing mechanism compared against the current sequence-level approach.

### Open Question 3
- Question: Does the expert specialization strategy transfer effectively to multilingual speech recognition?
- Basis in paper: [explicit] The "Limitations" section identifies "broader multilingual evaluation" as remaining future work.
- Why unresolved: It is unknown if the routing mechanism will successfully disentangle language identity from accent identity, or if the fixed number of experts (5) is sufficient for cross-lingual diversity.
- What evidence would resolve it: Application of MOE-CTC to multilingual benchmarks (e.g., FLEURS or ML-Superb) with analysis of expert activation patterns across different languages.

## Limitations

**Architecture Complexity and Inference Overhead**
The MOE-CTC design introduces significant complexity: 5 experts per MoE module, 3 MoE layers, and separate CTC heads for each expert. While the paper reports 22% computational overhead for K=2 routing (Table 9), this assumes efficient sparse kernel implementations. Real-world deployment may face challenges with irregular memory access patterns and reduced parallelism compared to dense models.

**Generalization to Truly Unseen Accents**
The experimental validation relies on MCV-ACCENT, which defines "unseen" accents as those absent from the fine-tuning set but present in the broader corpus. This is a limited form of generalization - the model has still encountered these accents during pre-training or Stage 1 training. The paper does not test performance on accents completely absent from all training data, which would represent a more stringent zero-shot generalization test.

**Hyperparameter Sensitivity and Scalability**
The results depend heavily on specific design choices: N=5 experts, K=2 routing, α=1.0 accent bias, and particular layer placement (4,8,12). While ablation studies examine some variations (Table 10-11), the sensitivity to these choices across different dataset sizes, accent distributions, or language families remains unexplored.

## Confidence

- **High Confidence**: The core observation that expert-level CTC supervision improves ASR performance on accented speech is well-supported by controlled ablations (Tables 1, 11) and has clear theoretical grounding in the alignment of routing with transcription quality.

- **Medium Confidence**: The claim of effective generalization to unseen accents rests on experimental evidence but faces limitations in test distribution coverage. The accent-agnostic fine-tuning mechanism appears effective within the MCV-ACCENT experimental setup, but broader applicability requires validation.

- **Low Confidence**: The assertion that the routing-augmented loss L_local is essential for stable optimization lacks direct ablation evidence in the paper. While the mechanism is described and theoretically plausible, no experiment isolates its contribution from other components.

## Next Checks

1. **Zero-Shot Generalization Test**: Evaluate MOE-CTC on accents completely absent from all training stages (no pre-training, no Stage 1 exposure). Compare against a simple fine-tuned baseline to quantify true generalization capability beyond the current "unseen" definition.

2. **Routing Behavior Analysis**: Track expert selection distributions across training stages and accent types. Specifically, verify that Stage 1 accent-aware routing produces the expected accent-expert alignment, and that Stage 2 fine-tuning results in the adaptive redistribution patterns described in Figure 2. Use entropy metrics to detect router collapse.

3. **Efficiency-Agnostic Performance**: Measure actual inference throughput and memory usage on target hardware (GPU/CPU), comparing against theoretical computational overhead. Include end-to-end latency measurements including routing decision time, and verify that the reported 22% overhead translates to practical deployment constraints.