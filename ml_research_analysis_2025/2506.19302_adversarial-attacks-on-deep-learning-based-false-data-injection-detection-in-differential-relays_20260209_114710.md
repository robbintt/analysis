---
ver: rpa2
title: Adversarial Attacks on Deep Learning-Based False Data Injection Detection in
  Differential Relays
arxiv_id: '2506.19302'
source_url: https://arxiv.org/abs/2506.19302
tags:
- adversarial
- lcdr
- fdia
- attacks
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of deep learning-based false
  data injection attack (FDIA) detection systems in line current differential relays
  (LCDRs) to adversarial attacks. The authors propose a novel adversarial attack framework
  using the Fast Gradient Sign Method (FGSM) to craft subtle perturbations to LCDR
  remote measurements, leading to misclassification of FDIA as legitimate faults and
  triggering the LCDR to trip.
---

# Adversarial Attacks on Deep Learning-Based False Data Injection Detection in Differential Relays

## Quick Facts
- arXiv ID: 2506.19302
- Source URL: https://arxiv.org/abs/2506.19302
- Reference count: 40
- Primary result: All tested deep learning models show high vulnerability to FGSM-based adversarial attacks on LCDR FDIA detection, with some exceeding 99.7% attack success rate

## Executive Summary
This paper investigates the vulnerability of deep learning-based false data injection attack (FDIA) detection systems in line current differential relays (LCDRs) to adversarial attacks. The authors demonstrate that adversarial attacks using Fast Gradient Sign Method (FGSM) can craft subtle perturbations to LCDR remote measurements, causing deep learning models to misclassify FDIAs as legitimate faults and trigger unnecessary trips. They evaluate multiple deep learning architectures (MLP, CNN, LSTM, ResNet) and find all exhibit varying degrees of vulnerability, with attack success rates exceeding 99.7% in some cases. The paper proposes adversarial training as a defense mechanism, showing it significantly improves model robustness against adversarial FDIAs while maintaining fault detection accuracy.

## Method Summary
The study uses PSCAD simulations to generate a dataset of 48,600 samples (23,040 FDIA + 25,560 fault) with 3-phase local and remote current measurements sampled at 1 kHz over 4 power cycles. Four deep learning models (MLP, CNN, LSTM, ResNet) are trained on 80% of the data for binary classification of FDIAs versus legitimate faults. The FGSM attack generates adversarial samples by computing gradient-based perturbations applied only to remote measurements, with amplitude scaling and clipping to maintain stealthiness. Adversarial training augments the training set with FGSM-generated adversarial samples and retrains models for 10 additional epochs to improve robustness.

## Key Results
- All deep learning models show high vulnerability to FGSM-based adversarial attacks, with MLP achieving 97.44% fooling rate at ε=0.5
- Adversarial training significantly improves robustness across all models, though some adversarial samples still evade detection
- ResNet demonstrates the highest resilience among tested models with 9.78% fooling rate after adversarial training
- Increasing FGSM iterations from 5 to 50 provides only marginal improvement in attack success rates

## Why This Works (Mechanism)

### Mechanism 1: FGSM Gradient-Based Perturbation Crafting
Small, gradient-aligned perturbations to LCDR remote measurements cause DL models to misclassify FDIAs as legitimate faults. FGSM computes δx = ε · sign(∇xJ(θ, x, y)) · a, moving inputs in the direction of maximum loss increase. The amplitude factor a scales perturbations proportionally to input magnitude, maintaining subtlety. This works because the attacker has (or can approximate) model parameters θ and access to the loss gradient with respect to inputs.

### Mechanism 2: Remote-Measurement-Only Attack Constraint
Constraining perturbations to remote measurements only is sufficient for high success rates while reflecting realistic attacker capabilities. Local measurements traverse copper wires directly to the relay (hard to intercept), while remote measurements traverse longer distances via fiber, wireless, or IEC 61850 networks with multiple interception points. This constraint reflects the practical difficulty of compromising local measurement channels.

### Mechanism 3: Adversarial Training as Proactive Defense
Adversarial training improves model robustness by augmenting training data with adversarial samples. Algorithm 3 generates adversarial samples using FGSM, labels them correctly as FDIAs, and augments the training set. The model then learns to map adversarial regions to correct labels, effectively expanding the "FDIA" decision region to cover perturbation neighborhoods. This works when adversarial samples seen during training are representative of attack-time perturbations.

## Foundational Learning

- **Concept**: Line Current Differential Relay (LCDR) Operating Principle
  - **Why needed here**: Understanding what makes an LCDR trip is essential to grasp why FDIAs work and what the dual success criterion (misclassify AND trip) requires.
  - **Quick check question**: Given local current i1 and remote current i2, under what condition does the LCDR issue a trip signal? What does a differential current id significantly greater than zero indicate?

- **Concept**: Fast Gradient Sign Method (FGSM) and Gradient-Based Attacks
  - **Why needed here**: The paper's attack framework is built on FGSM; understanding how gradients relate to decision boundaries explains both attack success and defense mechanisms.
  - **Quick check question**: Why does the sign of the gradient ∇xJ indicate the direction of maximum loss increase? How does ε control the attack's stealthiness vs. effectiveness tradeoff?

- **Concept**: Transferability of Adversarial Examples
  - **Why needed here**: The threat model assumes attackers may not have exact model parameters; understanding transferability informs both attack feasibility and defense limitations.
  - **Quick check question**: If an adversarial sample is crafted against a substitute model f', how likely is it to fool the target model f? What factors affect this transferability?

## Architecture Onboarding

- **Component map**: Power System (PSCAD simulation) → Current Transformers → Local i1 (copper wire, secure) → LCDR Fault Detection Module → Trip Decision Logic → Circuit Breaker. Remote i2 (communication network, vulnerable) → [Attacker interception point] → Manipulated i2 (if FDIA) → LCDR Fault Detection Module

- **Critical path**: Remote measurement i2 → Communication channel (attack surface) → FDIA detection module → If misclassified as fault → Unnecessary line trip → Potential cascading failure

- **Design tradeoffs**:
  - Model complexity vs. robustness: Deeper models (ResNet) show higher adversarial resilience but higher inference time
  - Perturbation magnitude ε vs. stealthiness: Higher ε increases FR but may trigger anomaly detection
  - Adversarial training epochs vs. clean accuracy: More epochs improve robustness but risk overfitting
  - Detection speed vs. accuracy: Observation window of 4 cycles plus inference must stay within protection speed requirements

- **Failure signatures**:
  - Model-dependent: MLP shows >97% FR (near-complete failure); ResNet maintains ~90% detection under attack
  - ε-dependent: All models show increasing FR with ε; MLP reaches asymptotic failure at ε≥0.5
  - Iteration-dependent: Increasing FGSM iterations from 5 to 50 only marginally improves FR for MLP (+2.56%)
  - Post-adversarial-training: Significant improvement but not 100% robust; some adversarial samples still evade detection

- **First 3 experiments**:
  1. Baseline vulnerability assessment: Train each model on Dtrain, test on D'test with FGSM attacks (ε=0.5, 5 iterations). Record FR, accuracy, precision, recall, F1.
  2. Perturbation magnitude sweep: Vary ε from 0.1 to 0.7 in 0.1 increments for each model. Plot FR vs. ε curve.
  3. Adversarial training effectiveness: Apply Algorithm 3 to augment Dtrain with adversarial samples. Retrain all models for 10 additional epochs. Re-evaluate on D'test.

## Open Questions the Paper Calls Out
- Evaluating the proposed scheme under various types of synchronization errors, communication delays and protocol-specific vulnerabilities
- Exploring the effects of other adversarial attack algorithms beyond FGSM
- Investigating the effects of noisy communication channels on LCDR performance

## Limitations
- Assumes attackers can only manipulate remote measurements, which may not hold in all real-world deployments
- FGSM attack assumes white-box access to model gradients, though transferability is not extensively tested
- Adversarial training effectiveness is shown only against FGSM, not other attack methods

## Confidence
- **High Confidence**: Attack mechanism (FGSM perturbation crafting), adversarial training methodology, dataset availability, and general vulnerability trends across models
- **Medium Confidence**: Specific numerical results (FR percentages, timing measurements) due to potential differences in model architectures and hyperparameters
- **Low Confidence**: Transferability claims, real-world applicability of remote-only measurement constraint, and adversarial training robustness against unknown attack methods

## Next Checks
1. Implement model architectures from referenced tsai library to verify exact numerical results and timing measurements
2. Test adversarial sample transferability by crafting attacks against one model and evaluating against others to assess black-box attack feasibility
3. Evaluate adversarial training robustness against alternative attacks (PGD, Carlini-Wagner) to assess defense generalization beyond FGSM