---
ver: rpa2
title: 'SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation'
arxiv_id: '2507.19033'
source_url: https://arxiv.org/abs/2507.19033
tags:
- code
- retrieval
- generation
- selfracg
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SelfRACG, a retrieval-augmented code generation
  framework that enables large language models (LLMs) to self-express their information
  needs during generation. The core method involves an Information Need Expression
  (INE) module using Layer-wise Low-Rank Adaptation (L-LoRA) to extract retrieval
  representations from the LLM's hidden states, and a two-stage Information Need-Guided
  (ING) training strategy to align retrieval with the LLM's generation preferences.
---

# SelfRACG: Enabling LLMs to Self-Express and Retrieve for Code Generation

## Quick Facts
- arXiv ID: 2507.19033
- Source URL: https://arxiv.org/abs/2507.19033
- Reference count: 8
- Key outcome: SelfRACG improves code completion performance over vanilla RACG with external retrievers while using less than 1/8 of the training cost of the strongest baseline

## Executive Summary
SelfRACG introduces a retrieval-augmented code generation framework that enables LLMs to self-express their information needs during generation. The core innovation is an Information Need Expression (INE) module using Layer-wise Low-Rank Adaptation (L-LoRA) to extract retrieval representations from the LLM's hidden states, eliminating the need for external query formulation. A two-stage Information Need-Guided (ING) training strategy aligns retrieval with the LLM's generation preferences, significantly improving performance on two benchmarks (RepoEval and CrossCodeEval) across various code LLMs while reducing training costs substantially.

## Method Summary
SelfRACG enables LLMs to retrieve relevant code fragments from a repository without external retrievers by extracting retrieval representations directly from the model's hidden states using L-LoRA. The framework employs a two-stage training approach: Stage 1 learns general code logic patterns from GitHub code pairs, while Stage 2 aligns retrieval with the LLM's generation preferences using self-synthesized data. The L-LoRA module adds parallel retrieval-aware attention projections at each transformer layer, which are mean-pooled to produce the final retrieval embedding. This embedding is used to retrieve relevant fragments via MIPS, which are then provided to the LLM as context for generation.

## Key Results
- SelfRACG achieves 1.1-2.7% improvement in Exact Match and 3.1-4.2% improvement in Edit Similarity on code completion benchmarks
- The framework uses less than 1/8 of the training cost compared to the strongest baseline (GritLM)
- Retrieval performance shows MRR@10 improvements from 0.316 to 0.358 when adding Stage 2 preference alignment

## Why This Works (Mechanism)

### Mechanism 1: Hidden-State-Derived Information Need Expression
The next-token hidden states in an LLM encode what the model needs to generate subsequent content, enabling retrieval without external query formulation. L-LoRA adds parallel retrieval-aware attention projections at each transformer layer to compute layer-wise retrieval representations, which are mean-pooled to produce the final retrieval embedding. This works under the assumption that hidden states already encapsulate information needs for future content generation.

### Mechanism 2: Two-Stage Preference-Aligned Training
Separating foundational retrieval learning from preference alignment enables efficient training that aligns retrieval with the LLM's generation behavior. Stage 1 uses GitHub code pairs with in-batch negatives to learn general patterns, while Stage 2 synthesizes candidates via the LLM itself using the model's immediate generation as the positive sample. This approach assumes the LLM's own generations represent its "preference."

### Mechanism 3: Task Decoupling via Parallel Attention
Running retrieval-aware attention parallel to self-attention preserves generation capability while enabling retrieval. Unlike vanilla LoRA which modifies the main attention pathway, L-LoRA's projections feed a separate attention computation that doesn't backprop to the original self-attention output. This works under the assumption that retrieval and generation tasks are sufficiently separable.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed: SelfRACG's L-LoRA builds directly on LoRA's weight decomposition. Understanding rank, alpha scaling, and the trade-off between parameter efficiency and expressiveness is essential.
  - Quick check: Can you explain why LoRA's low-rank decomposition enables efficient fine-tuning compared to full parameter updates?

- **Concept: Contrastive Learning for Retrieval**
  - Why needed: Both ING stages use InfoNCE-style contrastive loss. Understanding positive/negative sampling, temperature scaling, and batch composition effects is critical.
  - Quick check: How does in-batch negative sampling affect contrastive learning, and why might smaller batches hurt retrieval performance?

- **Concept: Repository-Level Code Completion Context**
  - Why needed: The "content gap" problem—consecutive fragments diverging in logic—is central to SelfRACG's motivation. Understanding cross-file dependencies and logical progression in code is needed to interpret results.
  - Quick check: Why would BM25 or embedding similarity fail when retrieving context for code that logically follows but has different surface content?

## Architecture Onboarding

- **Component map**: Input Code → LLM Backbone (frozen) → Self-Attention → Hidden States → Generation; L-LoRA Projections → Retrieval-Aware Attention → Layer-wise r^(l) → Mean Pool → r (query embedding) → MIPS against indexed corpus → Retrieved fragments + Input → Generation

- **Critical path**: INE module extraction (L-LoRA forward pass) → embedding generation → vector search → context augmentation → code generation. The retrieval representation quality at the first step gates everything downstream.

- **Design tradeoffs**:
  - L-LoRA rank=16, alpha=32: balances expressiveness vs overhead
  - Mean pooling vs weighted layer aggregation: paper uses simple mean
  - Two-stage vs end-to-end training: staged approach is more data-efficient but requires synthetic data generation overhead

- **Failure signatures**:
  - Low Recall@1 but high Recall@10: retrieval representation captures general domain but not precise need
  - Generation degradation after training: L-LoRA may be leaking gradients into backbone
  - High MRR but low generation EM: retrieved context is relevant but model fails to use it

- **First 3 experiments**:
  1. Validate L-LoRA isolation: Run generation benchmarks on base LLM with L-LoRA enabled but no retrieval
  2. Ablate each training stage: Train with only Stage 1 vs only Stage 2 vs full ING
  3. Corpus scale sensitivity: Index subsets of the GitHub corpus and measure retrieval performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the SelfRACG framework be effectively generalized to general text-based RAG scenarios beyond the code generation domain?
- Basis in paper: The authors explicitly state in Appendix A that the approach has been evaluated exclusively on RACG and they "plan to extend it to general retrieval-augmented generation scenarios."
- Why unresolved: The current ING training strategy relies on code-specific logical progression. Natural language reasoning may exhibit different patterns of "information need" that the current hidden-state extraction method may not capture as effectively.
- What evidence would resolve it: Experimental results applying SelfRACG to standard text-based RAG benchmarks demonstrating performance parity or improvement over vanilla RAG baselines.

### Open Question 2
- Question: Does the performance of the Information Need Expression (INE) module scale effectively to Large Language Models with significantly larger parameter counts (e.g., 70B+)?
- Basis in paper: Appendix A notes a limitation where "due to limited computational resources, our method has been validated only on LLMs with up to 8B parameters."
- Why unresolved: While the paper shows positive trends for 1.5B to 8B models, it is unverified if L-LoRA remains sufficient to capture complex information needs in much larger models.
- What evidence would resolve it: Benchmarks of SelfRACG on models with 70B or more parameters, showing that L-LoRA representations successfully align with the generation preferences of larger architectures.

### Open Question 3
- Question: Can the Layer-wise Low-Rank Adaptation (L-LoRA) mechanism be adapted for general text embedding tasks outside of retrieval?
- Basis in paper: Appendix A states, "the potential of L-LoRA extends beyond retrieval tasks. We will explore its application to various embedding tasks, enabling low-cost embedding capabilities for every LLM."
- Why unresolved: The L-LoRA module is currently trained via contrastive learning specifically to align with generation preferences. It is unclear if this mechanism transfers effectively to tasks requiring pure semantic understanding.
- What evidence would resolve it: Evaluation of L-LoRA equipped models on the Massive Text Embedding Benchmark (MTEB) for tasks like pair classification or clustering.

## Limitations
- The framework's effectiveness for significantly larger models (70B+) and non-code domains remains untested
- The self-synthesis approach may create self-reinforcing loops where the model learns to retrieve context that aligns with its existing biases
- The theoretical mechanism explaining why hidden states encode retrieval needs lacks direct empirical validation

## Confidence

- **High confidence**: Retrieval performance improvements (MRR@10, Recall@K metrics) are directly measurable and consistently show SelfRACG outperforming baselines across multiple experiments and model sizes
- **Medium confidence**: Code generation quality improvements (EM, ES, Pass@1) are demonstrated, but these results are downstream of retrieval quality and depend on how effectively retrieved context is integrated into generation
- **Low confidence**: The theoretical mechanism explaining why hidden states encode retrieval needs is weakly supported—the paper provides implementation details and performance evidence but limited ablation or analysis of the information flow

## Next Checks

1. **Hidden-state interpretability analysis**: Apply feature attribution or probing techniques to L-LoRA's retrieval representations to determine what semantic aspects of code they capture. Compare these representations against traditional embedding methods to understand the unique information encoded in hidden states.

2. **Cross-domain generalization test**: Evaluate SelfRACG on non-code retrieval tasks (e.g., natural language document retrieval) to test whether the hidden-state-as-query approach generalizes beyond code. This would validate whether the mechanism is domain-specific or a more general principle.

3. **Long-term preference stability evaluation**: Train SelfRACG for multiple rounds of self-synthesis (Stage 2 trained on Stage 2 outputs iteratively) and monitor whether retrieval performance plateaus, degrades, or continues improving. This would reveal whether self-reinforcing preference alignment creates stable or pathological retrieval patterns.