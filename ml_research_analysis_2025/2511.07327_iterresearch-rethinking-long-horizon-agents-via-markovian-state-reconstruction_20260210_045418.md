---
ver: rpa2
title: 'IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction'
arxiv_id: '2511.07327'
source_url: https://arxiv.org/abs/2511.07327
tags:
- arxiv
- tool
- research
- report
- paradigm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the context suffocation and noise contamination
  problems in long-horizon research agents that accumulate all information linearly.
  The authors propose IterResearch, which reformulates deep research as a Markov Decision
  Process with iterative workspace reconstruction, maintaining an evolving report
  as memory and periodically synthesizing insights to preserve consistent reasoning
  capacity.
---

# IterResearch: Rethinking Long-Horizon Agents via Markovian State Reconstruction

## Quick Facts
- arXiv ID: 2511.07327
- Source URL: https://arxiv.org/abs/2511.07327
- Reference count: 40
- Key outcome: +14.5pp average improvement across six benchmarks with up to 19.2pp gains on long-horizon tasks

## Executive Summary
IterResearch addresses the context suffocation and noise contamination problems in long-horizon research agents by reformulating deep research as a Markov Decision Process with iterative workspace reconstruction. The method maintains an evolving report as compressed memory and periodically synthesizes insights to preserve consistent reasoning capacity. By reconstructing a bounded workspace at each round containing only the question, current report, and immediate context, IterResearch achieves O(1) workspace complexity regardless of exploration depth, enabling sustained reasoning capacity at arbitrary exploration depths. Experiments demonstrate significant performance gains, with IterResearch achieving 42.5% accuracy on 2048-interaction tasks compared to just 3.5% for mono-contextual baselines.

## Method Summary
IterResearch formalizes deep research as a Markov Decision Process where each state transition discards full trajectory history and reconstructs a bounded workspace containing only the question, evolving report, and immediate context from the previous step. The method employs Efficiency-Aware Policy Optimization (EAPO) that adapts geometric reward discounting to incentivize efficient exploration while maintaining accuracy. Training involves two stages: supervised fine-tuning on 30K filtered QA pairs and 110K synthesized trajectories, followed by reinforcement learning with geometric discounting (γ=0.995) and adaptive downsampling. The policy network generates structured decisions including thinking, report updates, and actions, enabling iterative deep research with sustained reasoning capacity at arbitrary exploration depths.

## Key Results
- Average +14.5pp improvement across six benchmarks (HLE, BrowseComp, BrowseComp-zh, GAIA, Xbench-DeepSearch, SEAL-0)
- Scales to 2048 interactions with performance improving from 3.5% to 42.5% on BrowseComp
- Serves as effective prompting strategy, improving frontier models by up to 19.2pp on long-horizon tasks
- Reduces average interactions by 5.7% while maintaining or improving accuracy compared to standard GSPO

## Why This Works (Mechanism)

### Mechanism 1: Iterative Workspace Reconstruction
- Claim: Iterative workspace reconstruction maintains bounded context complexity regardless of exploration depth, enabling sustained reasoning capacity.
- Mechanism: Each state transition discards full trajectory history and reconstructs a workspace containing only the question, evolving report, and immediate context, maintaining O(1) workspace size vs. O(t) linear growth in mono-contextual approaches.
- Core assumption: The evolving report can adequately compress task-relevant information from discarded history without critical information loss.
- Evidence anchors: Abstract states "reconstructs a bounded workspace at each round containing only the question, current report, and immediate context, enabling sustained reasoning capacity at arbitrary exploration depths." Section 3.1.2 explicitly compares O(t) vs O(1) growth. WebResearcher (FMR=0.49) similarly uses iterative deep-research with bounded memory.

### Mechanism 2: Evolving Report Memory
- Claim: Evolving report memory provides noise filtering through selective retention driven by the agent's learned synthesis behavior.
- Mechanism: The report is naturally generated by the LLM as part of its structured decision output, creating natural breakpoints for filtering noise through periodic synthesis.
- Core assumption: The agent learns effective synthesis behavior during training that balances information preservation with noise exclusion.
- Evidence anchors: Section 3.1.2 states "irrelevant information or errors from early rounds cannot directly propagate to future decisions—they must first pass through the agent's synthesis." Section 4.2 confirms "our iterative paradigm provides natural breakpoints for filtering noise through periodic synthesis."

### Mechanism 3: Geometric Reward Discounting
- Claim: Geometric reward discounting with γ ∈ (0,1) creates implicit efficiency pressure that incentivizes concise exploration without explicit length penalties.
- Mechanism: Rewards are computed as r_t = γ^{T-t} · R_T, where earlier steps in shorter trajectories receive proportionally higher rewards. For γ=0.995, step t=3 in a 5-step trajectory receives ~0.99 reward vs. ~0.918 in a 20-step trajectory.
- Core assumption: Efficient trajectories (shorter with equivalent correctness) represent genuinely superior research strategies rather than superficial shortcuts.
- Evidence anchors: Section 3.2.1 defines the discounted reward formula. Section 4.3 shows "EAPO reduces average interactions by 5.7% while maintaining or improving accuracy" compared to standard GSPO.

## Foundational Learning

- Concept: Markov Decision Processes (MDPs) and state independence
  - Why needed here: The paper formalizes deep research as an MDP where future states depend only on current state, not full history. Understanding this is essential to grasp why workspace reconstruction preserves decision-theoretic optimality.
  - Quick check question: In an MDP, if the optimal action at step t depends only on state s_t, why can we discard s_{0:t-1} without losing optimality?

- Concept: Context window limitations in Transformers
  - Why needed here: The mechanism explicitly addresses "context suffocation"—as context fills, available reasoning space shrinks. Understanding quadratic attention complexity O(n²) explains why bounded workspace enables computational scalability.
  - Quick check question: Why does attention computation scale quadratically with sequence length, and how does maintaining constant workspace size affect this cost?

- Concept: Reinforcement learning with sparse rewards and policy gradients
  - Why needed here: EAPO builds on Group Sequence Policy Optimization (GSPO) with discounted terminal rewards. Understanding baseline policy gradient methods clarifies how efficiency incentives emerge from the discount factor.
  - Quick check question: In sparse reward settings where only terminal reward R_T is available, how does geometric discounting create per-step learning signals?

## Architecture Onboarding

- Component map:
  State constructor -> Policy network -> Tool environment -> Report synthesizer -> Reward computer -> Adaptive downsampler

- Critical path:
  1. Question q initializes empty report M_0 = ∅
  2. At each round t: policy receives s_t → generates Think_t, M_{t+1}, a_t
  3. Environment executes a_t → returns TR_t
  4. Workspace reconstructs: s_{t+1} = (q, M_{t+1}, {a_t, TR_t})
  5. Loop until a_t = answer or t reaches T_max

- Design tradeoffs:
  - Report size bounds: Larger reports preserve more history but consume context; bounded by learned synthesis behavior
  - Discount factor γ: Lower values enforce stronger efficiency but risk premature termination; paper uses 0.995
  - Training horizon T_max: Constrained to 32 during training (efficiency pressure) but can extrapolate to 2048+ at inference

- Failure signatures:
  - Context suffocation persists: Report synthesis is not effectively compressing—check synthesis quality in M_t
  - Excessive interactions without progress: Efficiency incentive too weak (γ too high) or synthesis failing to guide focused exploration
  - Early termination with wrong answers: γ too low or reward signal not adequately distinguishing correctness
  - Cross-paradigm transfer fails: Trajectories may not generalize if trained agent overfits to iterative structure

- First 3 experiments:
  1. Baseline comparison on BrowseComp: Implement mono-contextual ReAct baseline with identical backbone, measure accuracy degradation as max_turns increases from 32→256→512 to confirm context suffocation hypothesis.
  2. Report synthesis ablation: Replace learned report synthesis with fixed-length summarization or oracle summaries to isolate the contribution of learned compression vs. generic summarization.
  3. Efficiency incentive sensitivity: Train with γ ∈ {0.99, 0.995, 0.999} and measure average trajectory length vs. accuracy tradeoff to validate that 0.995 balances efficiency and thoroughness as claimed.

## Open Questions the Paper Calls Out

- Question: What are the practical limits of interaction scaling beyond 2048 interactions, and does performance continue to improve or eventually plateau?
- Basis in paper: The paper claims "theoretically extensible to infinite depths" but only empirically validates up to 2048 interactions, noting this is "unprecedented" and "currently infeasible for mono-contextual agents."
- Why unresolved: The experiments only tested up to 2048 turns; the theoretical claim of unbounded scaling remains unverified beyond this point, and it is unclear whether performance gains continue linearly or encounter diminishing returns.
- What evidence would resolve it: Experiments extending to 4096+ interactions on BrowseComp or similar benchmarks, with analysis of whether accuracy continues to scale and whether report synthesis remains effective at extreme depths.

## Limitations
- The effectiveness of report synthesis as a compression mechanism is assumed rather than empirically validated against simpler approaches
- The 30K QA pair dataset source and filtering criteria remain unspecified, creating potential reproducibility barriers
- The geometric discount factor γ=0.995 may be suboptimal for different task distributions and is not systematically ablated

## Confidence

- **High confidence**: Iterative workspace reconstruction provides computational benefits (O(1) vs O(t) context growth) and empirically improves performance across benchmarks. The efficiency incentive mechanism through geometric discounting is mathematically sound and demonstrates measurable impact on trajectory length.
- **Medium confidence**: Report synthesis effectively filters noise through selective retention. The zone of proximal development selection (20-60% success rates) appropriately balances challenge and learnability. Cross-paradigm knowledge transfer generalizes beyond iterative structures.
- **Low confidence**: The specific choice of γ=0.995 represents optimal efficiency-correctness tradeoff. Learned report synthesis significantly outperforms generic summarization approaches. The method scales robustly to 2048+ interactions without performance degradation.

## Next Checks

1. **Report synthesis ablation**: Replace learned report synthesis with fixed-length summarization or oracle summaries to isolate the contribution of learned compression versus generic summarization techniques.
2. **Efficiency incentive sensitivity**: Systematically vary γ across {0.99, 0.995, 0.999} to measure accuracy-efficiency tradeoffs and validate the claimed optimal balance.
3. **Context compression analysis**: Monitor report token growth across rounds and implement content analysis to verify that critical information from early rounds is preserved in compressed form.