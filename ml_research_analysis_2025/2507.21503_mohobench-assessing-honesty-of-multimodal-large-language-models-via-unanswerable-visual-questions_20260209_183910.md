---
ver: rpa2
title: 'MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable
  Visual Questions'
arxiv_id: '2507.21503'
source_url: https://arxiv.org/abs/2507.21503
tags:
- question
- image
- honesty
- visual
- refusal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work presents the first systematic investigation of honesty\
  \ in multimodal large language models through unanswerable visual questions. The\
  \ authors define four types of unanswerable visual questions\u2014context dependent,\
  \ false premises, subjective or philosophical, and vague description\u2014and construct\
  \ MoHoBench, a high-quality dataset of over 12,000 examples."
---

# MoHoBench: Assessing Honesty of Multimodal Large Language Models via Unanswerable Visual Questions

## Quick Facts
- arXiv ID: 2507.21503
- Source URL: https://arxiv.org/abs/2507.21503
- Authors: Yanxu Zhu; Shitong Duan; Xiangxu Zhang; Jitao Sang; Peng Zhang; Tun Lu; Xiao Zhou; Jing Yao; Xiaoyuan Yi; Xing Xie
- Reference count: 40
- Most MLLMs fail to appropriately refuse unanswerable visual questions (average 21.3% refusal rate)

## Executive Summary
This work presents the first systematic investigation of honesty in multimodal large language models through unanswerable visual questions. The authors define four types of unanswerable visual questions—context dependent, false premises, subjective or philosophical, and vague description—and construct MoHoBench, a high-quality dataset of over 12,000 examples. They benchmark 28 MLLMs and find that most models fail to appropriately refuse to answer when necessary, with an average refusal rate of only 21.3%. The results show that model size does not guarantee better honesty performance, and that honesty is deeply influenced by visual information. The authors also conduct visual corruption experiments revealing that image quality affects honesty behavior, and implement alignment methods (SFT, DPO, SimPO, ORPO) that significantly improve refusal rates, with some models reaching over 95% refusal accuracy.

## Method Summary
The authors construct MoHoBench by generating unanswerable visual questions using MLLMs with in-context learning on COCO 2014 validation images and HaloQuest datasets, then filtering for quality through multiple MLLM evaluations and human verification. They evaluate 28 MLLMs using a three-step LLM-as-a-judge pipeline (o3-mini for refusal classification, GPT-4o for rationality and helpfulness scoring). For alignment, they implement SFT, DPO, SimPO, and ORPO on Qwen2.5-VL-7B and InternVL2.5 models using preference pairs between honest and dishonest responses, balanced with RLHF-V data at 1:1 ratio.

## Key Results
- Most MLLMs show low honesty performance with average refusal rate of 21.3% on unanswerable visual questions
- Model size does not correlate with honesty performance (e.g., Qwen2.5-VL-72B performs worse than smaller variants)
- SFT alignment achieves up to 98.86% refusal rate on Qwen-7B, though sometimes at cost to refusal rationality
- Visual corruption experiments show that image quality directly modulates honesty behavior (noise decreases refusal rates, contrast adjustment increases them)
- Subjective/Philosophical category consistently shows lowest refusal rates (<5%) across all models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal honesty requires joint visual-linguistic reasoning to recognize when available information is insufficient for reliable answers.
- Mechanism: The model must integrate visual perception (what can be extracted from the image) with linguistic reasoning (what the question demands), then compare these to determine if a grounded answer is possible. When visual cues are missing, contradictory, or insufficient, the model should trigger a refusal pathway rather than generate speculative content.
- Core assumption: Honesty behavior emerges from the interaction between visual encoding fidelity and language model uncertainty calibration, not from either modality alone.
- Evidence anchors:
  - [abstract] "MMLMs' honesty is not solely a language modeling issue, but is deeply influenced by visual information, necessitating the development of dedicated methods for multimodal honesty alignment."
  - [Section 5.1] Visual corruption experiments show additive noise decreases refusal rates while contrast adjustment increases them, suggesting visual perception quality directly modulates honesty behavior.
  - [corpus] Related work on unanswerable questions in visually rich documents (arXiv:2511.11468) similarly finds VLLMs struggle to detect unanswerable questions, confirming cross-modality integration challenge.

### Mechanism 2
- Claim: Category-specific honesty patterns emerge because different unanswerable question types require distinct reasoning pathways.
- Mechanism: The four categories (Context Dependent, False Premises, Subjective/Philosophical, Vague Description) each demand different detection strategies. False Premises require contradiction detection between text and image; Context Dependent requires recognizing missing external knowledge; Subjective questions require meta-cognitive awareness of objective limitations; Vague questions require ambiguity resolution failure detection.
- Core assumption: Models develop uneven capabilities across these reasoning types based on training data distribution and alignment procedures.
- Evidence anchors:
  - [Section 4.2] "Refusals are most frequently associated with the Context Dependent and False Premises categories... Subjective or Philosophical category consistently shows the lowest refusal rates across models, typically below 5%."
  - [Section 5.1] Under contrast corruption, only Subjective/Philosophical questions show decreased refusal rates, suggesting stronger reliance on language modality for this category.
  - [corpus] Weak corpus evidence—no directly comparable category-specific analysis found in neighbor papers.

### Mechanism 3
- Claim: Preference-based alignment methods can successfully induce honest refusal behavior by reshaping the model's response distribution toward uncertainty expression.
- Mechanism: Methods like DPO, SimPO, and ORPO create preference pairs where refusal responses (with appropriate justifications) are preferred over speculative answers. Through contrastive learning, the model learns to associate unanswerable questions with refusal behavior while maintaining helpfulness on answerable queries.
- Core assumption: The alignment signal generalizes across question types and doesn't simply memorize specific patterns from training data.
- Evidence anchors:
  - [Section 5.2] SFT achieves up to 98.86% refusal rate on Qwen-7B (vs. 28.92% baseline), with DPO and ORPO also showing dramatic improvements (82-97% range).
  - [Table 3] Alignment improves refusal rates but sometimes at cost to refusal rationality scores (e.g., SFT drops from 6.99 to 3.10), suggesting tradeoff between refusal frequency and explanation quality.
  - [corpus] RLHF-V dataset (Yu et al., 2024) is cited as source for balancing aligned samples, indicating preference learning is established approach for MLLM alignment.

## Foundational Learning

- Concept: HHH Principle (Helpfulness, Honesty, Harmlessness)
  - Why needed here: MoHoBench operationalizes the "Honesty" component of this alignment framework, which concerns knowledge boundary recognition rather than factual accuracy (which overlaps with hallucination research).
  - Quick check question: Can you explain why a model might be honest but still produce hallucinated content?

- Concept: Unanswerable vs. Hallucination distinction
  - Why needed here: The paper explicitly separates honesty (awareness of inability to answer reliably) from hallucination (factual accuracy of generated content), which have different evaluation metrics (refusal rate vs. accuracy).
  - Quick check question: If a model refuses to answer a question but the refusal reason is wrong, is this a hallucination problem or an honesty problem?

- Concept: Cross-modal grounding
  - Why needed here: MLLM honesty requires determining whether visual evidence sufficiently grounds the textual answer, which is fundamentally a cross-modal reasoning problem absent in text-only LLM honesty work.
  - Quick check question: What visual features would a model need to extract to determine if "What color is the car?" is answerable vs. unanswerable from a given image?

## Architecture Onboarding

- Component map:
  Image sources (COCO, HaloQuest) → MLLM-based question generation via ICL → Hard sample filtering (3+ models must fail to refuse) → Category validation (o1) → Human verification → MoHoBench dataset
  → o3-mini evaluation (refusal classification) → GPT-4o evaluation (rationality/helpfulness) → Benchmark results
  → Preference pair construction → SFT/DPO/SimPO/ORPO training → Aligned models → Evaluation

- Critical path:
  1. Understand the four unanswerable category definitions thoroughly (Table 4 in appendix shows generation prompts)
  2. Implement LLM-as-a-Judge evaluation pipeline with prompts from Tables 8-10
  3. Validate evaluation quality via human annotation (paper reports 91.43% agreement)

- Design tradeoffs:
  - Refusal rate vs. Rationality: SFT achieves highest refusal rates but lowest rationality scores, suggesting quantity-quality tension
  - Data balance: Pure refusal training causes over-refusal; mixing with RLHF-V at 1:1 maintains general capability
  - Evaluation model choice: o3-mini for binary refusal decision vs. GPT-4o for nuanced scoring reflects cost/accuracy tradeoff

- Failure signatures:
  - Low refusal rate with high helpfulness score → model is confidently wrong (speculative answering)
  - High refusal rate with low rationality score → model learned to refuse without understanding why
  - Category-specific refusal imbalance → model has systematic blind spots (especially Subjective/Philosophical)
  - Performance degradation under visual corruption → visual perception bottleneck rather than honest reasoning

- First 3 experiments:
  1. Replicate baseline evaluation on 3-5 representative MLLMs (e.g., Qwen2.5-VL-7B, InternVL2.5-8B, LLaMA-3.2-11B-Vision) to validate evaluation pipeline against reported refusal rates.
  2. Implement single-category SFT alignment (e.g., only False Premises) to test whether improvements transfer across categories or remain category-specific.
  3. Conduct controlled visual corruption experiment varying Gaussian noise σ parameter to map the relationship between perceptual degradation and refusal rate changes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can dedicated training objectives or loss functions be developed to specifically optimize honesty alignment in MLLMs beyond standard SFT or DPO?
- Basis in paper: [explicit] The authors state in the Limitations section that they applied existing techniques (SFT, DPO, SimPO, ORPO) and that "investigating dedicated training objectives or loss functions for honesty alignment remains an open and valuable direction."
- Why unresolved: Current alignment methods show trade-offs; for instance, SFT significantly boosts refusal rates but often degrades refusal rationality or general helpfulness (Table 3), indicating standard methods are sub-optimal for this specific behavior.
- What evidence would resolve it: A novel alignment strategy that achieves a higher Balanced Performance Index (BPI) than current baselines by improving refusal rates without sacrificing reasoning capability or helpfulness.

### Open Question 2
- Question: Can the taxonomy of unanswerable visual questions be expanded to capture more diverse and nuanced forms of uncertainty present in real-world scenarios?
- Basis in paper: [explicit] The authors note in the Limitations that their definition "includes four representative categories, but it may not exhaust all possible types of unanswerability in real-world scenarios."
- Why unresolved: The current four categories (Context Dependent, False Premises, etc.) provide a foundational framework, but complex visual inputs may contain hybrid or novel forms of ambiguity not covered by this taxonomy.
- What evidence would resolve it: A dataset extension identifying new distinct categories of unanswerable questions where current MLLMs fail in unique ways, validated by human evaluation.

### Open Question 3
- Question: How can cross-modal integration mechanisms be improved to ensure consistent honesty behavior when visual input quality is degraded?
- Basis in paper: [explicit] The analysis concludes that "honesty behavior in MLLMs is influenced by both visual and linguistic modalities," and explicitly calls for "future work [to] focus on improving cross-modal integration... to ensure more consistent honesty."
- Why unresolved: Visual corruption experiments reveal that models tend to become overconfident (refusal rates drop) when noise is added, suggesting the visual modality's influence on honesty is unstable.
- What evidence would resolve it: An architecture or training method that maintains stable refusal rates across varying levels of visual corruption (e.g., Gaussian noise, contrast reduction).

## Limitations
- Evaluation relies heavily on LLM-as-a-judge methods rather than extensive human evaluation, raising concerns about potential evaluator bias
- Alignment experiments were conducted on only two model families (Qwen2.5-VL and InternVL2.5) with limited parameter sizes, leaving uncertainty about scalability to frontier models
- Dataset construction process may have introduced category-specific biases through the multi-stage filtering and validation pipeline

## Confidence

- **High Confidence**: The core finding that most MLLMs fail to appropriately refuse unanswerable questions (average 21.3% refusal rate) is well-supported by comprehensive benchmarking across 28 models. The observation that visual information significantly impacts honesty behavior is strongly evidenced by the corruption experiments.
- **Medium Confidence**: The effectiveness of alignment methods (SFT achieving 98.86% refusal on Qwen-7B) is demonstrated but limited to specific model architectures. The tradeoff between refusal rate and rationality scores is observed but not fully characterized across the parameter space.
- **Low Confidence**: Claims about category-specific reasoning patterns (particularly the Subjective/Philosophical category showing lowest refusal rates) are based on observed correlations without establishing causal mechanisms. The assertion that honesty is "deeply influenced by visual information" beyond simple perception quality lacks mechanistic detail.

## Next Checks
1. **Cross-evaluator validation**: Run the same evaluation pipeline using Claude-3.5-Sonnet and Gemini-1.5-Pro as judges to assess consistency of refusal classifications and rationality scores compared to the reported o3-mini and GPT-4o results.

2. **Generalization to frontier models**: Benchmark the four largest commercially available MLLMs (GPT-4o, Claude-3.5-Sonnet, Gemini-1.5-Pro, and Llama-3.2-11B-Vision) on MoHoBench to test whether scale and architecture improvements overcome the observed honesty limitations.

3. **Temporal stability assessment**: Evaluate model versions before and after recent capability updates to determine whether improvements in general MLLM performance translate to better honesty behavior, controlling for potential data contamination from MoHoBench-like examples in training data.