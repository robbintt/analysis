---
ver: rpa2
title: Error Taxonomy-Guided Prompt Optimization
arxiv_id: '2602.00997'
source_url: https://arxiv.org/abs/2602.00997
tags:
- error
- prompt
- answer
- each
- guidance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Error Taxonomy-Guided Prompt Optimization (ETGPO) improves large
  language model performance by building a global error taxonomy from failed traces
  and generating targeted guidance for the most prevalent error categories. Unlike
  iterative bottom-up methods, ETGPO takes a top-down approach, filtering out long-tail
  errors to focus on high-impact failure modes.
---

# Error Taxonomy-Guided Prompt Optimization

## Quick Facts
- arXiv ID: 2602.00997
- Source URL: https://arxiv.org/abs/2602.00997
- Reference count: 40
- Key outcome: ETGPO improves LLM performance by building a global error taxonomy from failed traces and generating targeted guidance for the most prevalent error categories.

## Executive Summary
Error Taxonomy-Guided Prompt Optimization (ETGPO) is a top-down approach to automatic prompt optimization that improves large language model performance by building a global error taxonomy from failed traces and generating targeted guidance for the most prevalent error categories. Unlike iterative bottom-up methods, ETGPO takes a top-down approach, filtering out long-tail errors to focus on high-impact failure modes. Across six benchmarks in mathematics, question answering, and logical reasoning, ETGPO matches or exceeds state-of-the-art methods while using roughly one third of the optimization-phase token usage.

## Method Summary
ETGPO is a four-step pipeline that runs the backbone LLM on a validation set K times to collect failed traces, batch-processes these traces into a clustered taxonomy of error categories via an optimizer LLM, filters categories with at least two problems and selects the top G by failure count, then generates detailed guidance text for each selected category and appends it to the base prompt. The method uses fixed hyperparameters (K=5, G=10, B=6) derived from AIME optimization and employs detailed guidance style with explicit wrong/correct examples to maximize performance gains.

## Key Results
- ETGPO achieves 2-7 percentage point accuracy improvements on individual tasks across six benchmarks
- ETGPO uses roughly one-third the optimization-phase token usage compared to state-of-the-art methods
- ETGPO matches or exceeds performance of iterative bottom-up optimization approaches

## Why This Works (Mechanism)

### Mechanism 1: Statistical Filtering of Long-Tail Noise
ETGPO improves generalization by prioritizing systematic failure modes over idiosyncratic, single-instance errors. The system aggregates failed traces across K runs and filters out error categories tied to single problems, forcing the optimization to ignore "long-tail" noise and focus on high-prevalence error patterns that are likely to recur on unseen data.

### Mechanism 2: In-Context Contrastive Guidance
Performance gains are driven by providing the backbone LLM with detailed "Wrong vs. Correct" examples for specific error types, rather than generic instructions. The Guidance Generation step constructs a prompt containing a Description, Actionable Advice, and explicit WRONG/CORRECT example pairs, acting as contrastive in-context learning.

### Mechanism 3: Efficient Search Space Reduction (Top-Down)
ETGPO reduces token usage by replacing iterative exploration with a single synthesis pass over the error landscape. Instead of evolving prompts over multiple generations, ETGPO builds the taxonomy once and generates guidance once, treating prompt optimization as a report-generation task rather than a reinforcement learning task.

## Foundational Learning

- **Automatic Prompt Optimization (APO)**: The goal is to modify the "system prompt" (discrete text) to maximize a reward (accuracy) without touching model weights. Quick check: Can you distinguish between "weight tuning" (fine-tuning) and "prompt optimization" (ETGPO)?
- **Backbone vs. Optimizer LLMs**: The architecture splits labor. The "Backbone" takes the test; the "Optimizer" grades it and writes the study guide. Quick check: In ETGPO, which model generates the error taxonomy—the model solving the math problem or a separate instructor model?
- **Error Taxonomy**: This is the core data structure—not just a list of logs, but a clustered hierarchy of failure modes with prevalence stats. Quick check: If the taxonomy contains 50 categories but you only select the top G=10, what happens to the guidance for the other 40 categories?

## Architecture Onboarding

- **Component map**: Error Collector -> Taxonomy Engine -> Selector -> Guidance Generator -> Prompt Constructor
- **Critical path**: The Taxonomy Creation Prompts (Appendix B, Figures 4-7). If the Optimizer LLM fails to create "self-contained" categories, the guidance will not generalize.
- **Design tradeoffs**: Detail vs. Context Window (detailed guidance fills context window), Cost vs. Coverage (increasing K increases cost but captures more errors)
- **Failure signatures**: "Category Drift" (creating new categories for every batch), "Abstention Loops" (guidance teaches model to refuse to answer)
- **First 3 experiments**: 1) Implement Chain-of-Thought vs. ETGPO on AIME to verify 2-7% lift, 2) Compare "Short Guidance" vs. "Detailed Guidance" on FOLIO to confirm importance of examples, 3) Vary G on FOLIO to test if G=5 is saturation point

## Open Questions the Paper Calls Out

### Open Question 1
Does per-dataset hyperparameter tuning (K, G) yield significant performance improvements over the fixed AIME-derived defaults? The authors state tuning could improve performance but use fixed values to reduce computational cost, leaving the potential gains from dataset-specific optimization unquantified.

### Open Question 2
Can ETGPO be effectively combined with retrieval-augmented generation (RAG) to improve performance on knowledge-intensive tasks where it currently shows modest gains? The authors note gains on MMLU-Pro are modest because questions "often require more specific domain knowledge," but the interaction between error-guided reasoning prompts and external retrieval remains untested.

### Open Question 3
How does ETGPO performance degrade in saturated regimes where the backbone model's initial accuracy is already very high? The paper observes marginal gains on AR-LSAT with DeepSeek-V3.1, speculating that "execution logs provide little additional signal" when accuracy approaches 90%, but it's unclear if the technique reaches a hard ceiling when failure examples are too sparse.

## Limitations

- The taxonomy's representativeness depends heavily on validation set quality and size (90-160 problems reported without statistical power analysis)
- The approach assumes guidance-based interventions are sufficient for correction, which may not hold for knowledge-intensive tasks where the model lacks underlying factual knowledge
- The single-pass design trades off exploration depth—complex, interacting errors requiring multi-step iterative refinement may be missed entirely

## Confidence

- **High confidence**: The efficiency claim (one-third token usage) is well-supported by direct comparisons in Table 2 against GEPA and other baselines
- **Medium confidence**: The 2-7 percentage point accuracy improvements are reported across benchmarks but individual task results vary significantly (2-7 point range suggests inconsistency)
- **Low confidence**: The assumption that high-frequency errors represent systematic capability gaps rather than dataset artifacts—this is not empirically validated with external datasets

## Next Checks

1. **Cross-dataset validation**: Test whether guidance optimized on one dataset's error taxonomy transfers to improve performance on held-out datasets from different domains to verify systematic vs. dataset-specific error patterns
2. **Guidance specificity ablation**: Systematically vary the detail level in guidance (removing wrong/correct examples vs. removing actionable advice) across all error categories to quantify which guidance components drive performance gains
3. **Error taxonomy stability**: Run the taxonomy generation with different random seeds and K values (e.g., K=3 vs K=10) to measure variance in selected error categories and guidance stability—this validates whether the top-down approach consistently identifies the same high-impact errors