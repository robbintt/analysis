---
ver: rpa2
title: Do LLMs Surpass Encoders for Biomedical NER?
arxiv_id: '2504.00664'
source_url: https://arxiv.org/abs/2504.00664
tags:
- llms
- entities
- entity
- dataset
- biomedical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether decoder-based Large Language Models
  (LLMs) can outperform encoder models for biomedical Named Entity Recognition (NER)
  while maintaining per-token BIO tagging. The authors systematically compare three
  encoder models (BERT-(un)cased, BiomedBERT, DeBERTa-v3) and two LLMs (Mistral, Llama)
  using five biomedical NER datasets with varying entity lengths.
---

# Do LLMs Surpass Encoders for Biomedical NER?

## Quick Facts
- arXiv ID: 2504.00664
- Source URL: https://arxiv.org/abs/2504.00664
- Reference count: 39
- Primary result: LLMs achieve 2-8% higher F1 scores than encoders on biomedical NER but require 40-220x more inference time and 2 GPUs versus 1

## Executive Summary
This study systematically compares encoder models (BERT-(un)cased, BiomedBERT, DeBERTa-v3) against decoder-based Large Language Models (Mistral, Llama) for biomedical Named Entity Recognition using identical BIO tagging schemes and tokenization. The authors find LLMs outperform encoders by 2-8% F1 across most datasets, with particularly strong gains (up to 20 points) on longer entities (≥3 tokens). However, LLMs require 40-220x more inference time and two H100 GPUs versus one for encoders, making them computationally prohibitive for many applications. The modest performance advantage for most datasets suggests encoder models remain preferable when real-time feedback is needed or when performance differences are small, while LLMs show promise for high-stakes applications where their superior handling of longer entities justifies the computational cost.

## Method Summary
The study evaluates three encoder models (BERT variants and DeBERTa-v3) and two LLMs (Mistral-7B, Llama-7B/8B) on five biomedical NER datasets (BC5CDR, ChemProt, GAD, BioRED, Reddit-Impacts) using the same BIO tagging scheme for fair comparison. All models are fine-tuned with QLoRA (4-bit quantization, LoRA adapters with rank=128) to approximately 340M trainable parameters, controlling for model size. The LLM approach generates sequential BIO tags through autoregressive decoding rather than traditional span generation, enabling direct comparison with encoder token classification. Performance is measured with strict and relaxed F1 scores, and inference time is benchmarked on H100 GPUs.

## Key Results
- LLMs outperform encoders by 2-8% F1 overall across most datasets, with larger gains (up to 20 points) for entities ≥3 tokens
- BC5CDR shows encoder parity/slight advantage (80.6% singletons), while Reddit-Impacts (56.25% long entities) shows ~8 F1 point overall gain
- Inference time: LLMs require 40-220x more time and 2 H100 GPUs versus 1 for encoders
- LLMs show ~4 point lower F1 on length-1 entities compared to length≥3 in some datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-token BIO tagging enables fair comparison between encoder and decoder architectures by preserving positional information typically lost in span-only generation.
- Mechanism: LLMs are prompted to generate sequential BIO tags (B-/I-/O- prefixed entity type) for each input token, converting a generative task into a structured sequence labeling format compatible with standard NER evaluation.
- Core assumption: The BIO prompting strategy does not significantly degrade LLM capabilities compared to their natural span-generation mode.
- Evidence anchors:
  - [abstract] "employing the same BIO entity tagging scheme (that retains positional information)"
  - [section I] "Outputting answer spans (just text strings) can lead to loss of repeated entity mentions and can adversely effect downstream RE components"
  - [corpus] "Assessment of Generative Named Entity Recognition in the Era of Large Language Models" evaluates generative NER paradigms, suggesting this remains an open design question.
- Break condition: If BIO tag sequences exceed model context windows, or if tag tokenization (e.g., "B-DiseaseOrPhenotypicFeature" splitting into multiple tokens) substantially increases generation overhead and error rates.

### Mechanism 2
- Claim: Larger model capacity in LLMs (7-8B parameters) provides measurable gains on longer entities (≥3 tokens) compared to encoders (~335-435M parameters).
- Mechanism: Greater parameter count and pretraining scale may capture more complex multi-token entity patterns and longer-range dependencies, particularly beneficial for entities like "peripheral blood lymphocytes" that require consistent boundary detection across multiple tokens.
- Core assumption: The performance difference is attributable to model scale rather than architectural differences (decoder vs. encoder attention patterns).
- Evidence anchors:
  - [abstract] "larger gains for longer entities (≥ 3 tokens)"
  - [section III, Table III] Reddit-Impacts (56.25% long entities) shows ~8 F1 point overall gain and ~20 point gain for length≥3 entities; BC5CDR (9.82% long entities) shows encoder parity/slight advantage
  - [corpus] Weak direct evidence; neighbor papers focus on efficiency and domain adaptation, not scale-entity length interactions.
- Break condition: If dataset entity length distributions heavily favor singletons (e.g., >70% length-1 as in BioRED/BC5CDR), scale advantages diminish substantially.

### Mechanism 3
- Claim: QLoRA fine-tuning with matched trainable parameter counts (~340M) enables controlled comparison while preventing overfitting that occurs with full-parameter fine-tuning.
- Mechanism: 4-bit quantization with LoRA adapters (r=128, α=256, dropout=0.05) reduces effective trainable parameters to approximately encoder model sizes; full 7B/8B fine-tuning degraded performance due to overfitting.
- Core assumption: QLoRA configuration approximates what full fine-tuning would achieve with proper regularization.
- Evidence anchors:
  - [section II-B] "using the full 7B/8B LLMs without any QLoRA adaptation made the performances worse. This could be potentially due to extreme overfitting"
  - [section II-B] Mistral effective trainable size 341M; Llama 353M—comparable to encoder model sizes
  - [corpus] UniversalNER (mentioned in paper) uses knowledge distillation from LLMs to smaller models, suggesting parameter-efficient transfer is an active research direction.
- Break condition: If hyperparameter sweeps were feasible for LLMs (authors note they were not tractable), optimal configurations might differ substantially.

## Foundational Learning

- **BIO Tagging Scheme**: Begin-Inside-Outside token labeling where B-X marks entity start, I-X continues it, O marks non-entity tokens.
  - Why needed here: The entire experimental design hinges on understanding why preserving per-token positions matters for downstream tasks and fair evaluation.
  - Quick check question: Given "aspirin treats headache," what BIO tags would you assign if both are drug/disease entities?

- **Encoder vs. Decoder Attention**: Encoders use bidirectional self-attention (see all positions simultaneously); decoders use causal autoregressive attention (generate left-to-right).
  - Why needed here: Explains why encoders are naturally suited to token classification and 40-220x faster—parallel vs. sequential output generation.
  - Quick check question: Why must a decoder generate "B-Drug I-Drug O B-Disease" token-by-token rather than all at once?

- **Parameter-Efficient Fine-Tuning (LoRA/QLoRA)**: Low-rank adaptation freezes base model weights and trains small adapter matrices; quantization reduces memory footprint.
  - Why needed here: Understanding why 7B models with ~340M trainable params are compared against 335M full encoders.
  - Quick check question: If LoRA rank r=128 and hidden dimension is 4096, how many trainable parameters does one adapter matrix add?

## Architecture Onboarding

- **Component map**: Input → Tokenizer (model-specific) → Token alignment layer (maps predictions back to dataset tokenization) → Encoder path: Transformer encoder → Classification head → BIO tags (parallel) OR LLM path: Prompt template + input → Quantized base model → LoRA adapters → Autoregressive BIO generation (sequential) → Output → Entity extraction → Strict/relaxed evaluation

- **Critical path**: Entity boundary detection for multi-token spans—this is where LLMs show consistent advantage and where evaluation (strict vs. relaxed) most affects reported metrics.

- **Design tradeoffs**:
  - Performance vs. latency: 2-8 F1 point gain for 40-220x slower inference
  - Hardware: 1 H100 for encoders vs. 2 H100s for LLMs
  - BIO tag naming: Long entity type names (e.g., "DiseaseOrPhenotypicFeature") increase LLM generation overhead due to multi-token tag representation

- **Failure signatures**:
  - Singleton precision drops: LLMs show ~4 point lower F1 on length-1 entities vs. length≥3 in some datasets
  - Small dataset unreliability: Reddit-Impacts (80 test entities) shows large variance; interpret with caution
  - Tag tokenization overhead: BioRED's long entity names caused 220x (not just 40x) slowdown

- **First 3 experiments**:
  1. **Baseline replication**: Run BiomedBERT-large on BC5CDR (high singleton proportion) with BIO tagging; verify ~88-89 F1 strict match before attempting LLM comparison.
  2. **Entity length stratification**: On a dataset with known long-entity distribution (e.g., ChemProt 34% length≥3), compute F1 separately for length-1, length-2, and length≥3 to diagnose where gains originate.
  3. **Inference budget calibration**: Measure average inference time per sample for encoder vs. LLM on identical hardware; calculate throughput ratio to determine if your deployment constraints (e.g., real-time feedback <100ms) rule out LLMs regardless of accuracy gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance generalization uncertainty: Results rely heavily on Reddit-Impacts and GAD, while BC5CDR (80.6% singletons) shows encoder parity, suggesting advantages may be dataset-dependent
- Hardware requirement gap: 40-220x inference slowdown and 2x GPU requirement represent fundamental scalability barriers not fully evaluated under practical deployment constraints
- Tag tokenization overhead: BIO tag generation introduces compounding inefficiency as long entity type names tokenize into multiple subwords, increasing both generation time and error surface

## Confidence
- **High confidence**: Experimental methodology is sound with controlled tokenization, consistent BIO tagging, matched parameter counts via QLoRA, and five dataset evaluations providing robust comparative framework
- **Medium confidence**: Claim that LLMs "outperform encoders" in overall F-scores may be dominated by Reddit-Impacts results, with BC5CDR's singleton distribution potentially masking specific conditions where advantages manifest
- **Low confidence**: Attribution of longer entity performance to parameter scale assumes no architectural advantages from decoder attention patterns, without controlled ablation studies

## Next Checks
1. **Entity length distribution stratification**: Compute F1 scores separately for length-1, length-2, and length≥3 entities across all datasets to reveal whether LLM advantages concentrate in longer entities or are obscured by BC5CDR's singleton dominance

2. **Inference constraint evaluation**: Measure LLM performance degradation under batch processing and sequence length truncation to determine whether the 40-220x cost is an absolute barrier or can be mitigated through practical engineering

3. **Architectural ablation study**: Train a 7B encoder model with QLoRA on the same datasets and compare against 7B LLM performance to isolate whether parameter count or decoder architecture drives observed differences