---
ver: rpa2
title: Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training
arxiv_id: '2510.25803'
source_url: https://arxiv.org/abs/2510.25803
tags:
- uni00000013
- uni00000014
- datasets
- uni00000048
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of heterogeneous PDE datasets
  causing interference in neural operator pre-training and the high computational
  cost of dense models. To solve this, it proposes MoE-POT, a sparse Mixture-of-Experts
  architecture that uses a router-gating network to dynamically select 4 routed experts
  from 16, plus 2 shared experts, for efficient specialization and generalization.
---

# Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training

## Quick Facts
- arXiv ID: 2510.25803
- Source URL: https://arxiv.org/abs/2510.25803
- Reference count: 40
- Sparse MoE architecture achieves 40% lower zero-shot error than dense models with 33% fewer activated parameters on 6 public PDE datasets

## Executive Summary
This paper addresses the challenge of heterogeneous PDE datasets causing interference in neural operator pre-training and the high computational cost of dense models. The authors propose MoE-POT, a sparse Mixture-of-Experts architecture that uses a router-gating network to dynamically select 4 routed experts from 16, plus 2 shared experts, for efficient specialization and generalization. Pre-trained models with 90M activated parameters achieve up to 40% lower zero-shot error than dense models with 120M activated parameters on 6 public PDE datasets. The router network also achieves 98% accuracy in classifying PDE types, validating the interpretability and effectiveness of the MoE design.

## Method Summary
MoE-POT extends the Diffusion-based Pre-Trained Operator (DPOT) by replacing dense feed-forward networks with a Mixture-of-Experts (MoE) layer. The architecture includes 16 routed experts and 2 shared experts, with a router network selecting the top-4 experts for each input. The model is pre-trained using auto-regressive next-frame prediction on 6 heterogeneous PDE datasets, with noise injection for denoising pre-training. The objective function combines L2 reconstruction loss with a load balancing loss to ensure expert utilization. Fine-tuning is performed with the router network frozen to preserve pre-trained knowledge.

## Key Results
- MoE-POT-S (90M activated parameters) achieves 40% lower zero-shot error than DPOT-M (120M activated parameters) on 6 public PDE datasets
- Router network achieves 98% accuracy in classifying input PDE types
- 33% reduction in activated parameters while maintaining or improving performance across all tested datasets

## Why This Works (Mechanism)

### Mechanism 1
Sparse routing mitigates "negative transfer" caused by conflicting physical dynamics in heterogeneous PDE datasets. The model replaces dense feed-forward networks with a Mixture-of-Experts (MoE) layer. A router network activates a subset of experts (e.g., 4 out of 16) for specific inputs. This isolation allows different experts to specialize in distinct equation types (e.g., Navier-Stokes vs. Shallow Water) without their gradients interfering during backpropagation. Core assumption: The features required to solve different PDE families are sufficiently distinct that parameter separation yields higher fidelity than parameter sharing.

### Mechanism 2
Integrating "shared experts" ensures the retention of universal physical laws (e.g., conservation laws) across all PDE types. While routed experts specialize, 2 "shared" experts are always active for every input. These experts are forced to process all data distributions, ostensibly learning common differential operators or boundary condition handling that applies broadly. Core assumption: There exists a common substrate of physical dynamics across the pre-training dataset that benefits all tasks.

### Mechanism 3
The router network functions as an implicit PDE classifier, enabling the model to "identify" the physics before solving it. The router weights are optimized to minimize prediction error. The paper demonstrates that these weights correlate strongly with the specific PDE dataset (98% accuracy), suggesting the model learns a preliminary "equation detection" step that gates the appropriate solver pathways. Core assumption: The temporal/spatial tokens of different PDEs form distinguishable clusters in the feature space learned by the router.

## Foundational Learning

- **Neural Operators (Fourier Neural Operators - FNO)**
  - Why needed here: The base architecture (POT) relies on spectral convolution layers (Fourier layers) to learn mappings between infinite-dimensional function spaces.
  - Quick check question: Can you explain why learning in the frequency domain (Fourier space) is advantageous for solving PDEs compared to standard CNNs?

- **Auto-regressive Rollout**
  - Why needed here: The model predicts $u_{t+1}$ from $u_{t}$. Understanding error accumulation (propagation) is critical, as the paper highlights noise injection to mitigate this.
  - Quick check question: What is "exposure bias" in auto-regressive models, and how does noise injection (as used in DPOT/MoE-POT) help?

- **Load Balancing in MoE**
  - Why needed here: Without explicit regularization, routers tend to collapse, selecting the same few "best" experts for all inputs, leaving others untrained.
  - Quick check question: How does the "coefficient of variation" loss term (Eq. 12) mathematically force the router to utilize different experts?

## Architecture Onboarding

- **Component map:** Data Input → Patchify → Router Gating (Select Top-4) → Expert Compute (Weighted Sum) → Aggregation → Fourier Layer → Output Projection
- **Critical path:** Data Input → Patchify → **Router Gating** (Select Top-4) → **Expert Compute** (Weighted Sum) → Aggregation
- **Design tradeoffs:**
  - CNN vs. MLP Experts: Paper uses CNNs to preserve spatial info; MLPs caused instability
  - Shared vs. Routed: 2 Shared experts provide stability/common knowledge; 16 Routed provide capacity
  - Top-K Selection: $K=4$ is a balance between capacity and compute cost
- **Failure signatures:**
  - Routing collapse: One expert dominates all inputs (check auxiliary loss)
  - Dense-Style Degradation: Performance matches or is worse than dense DPOT (indicates dataset heterogeneity was insufficient to warrant MoE or load balancing failed)
  - OOD Failure: Router assigns random weights to Out-of-Distribution physics (check specialized fine-tuning)
- **First 3 experiments:**
  1. Replicate Figure 2 (Left): Train a small FNO on mixed datasets vs. single datasets to quantify the "negative transfer" baseline
  2. Router Probing: Run inference on the validation set, extract the router's softmax output, and run a simple K-means clustering to see if PDE types naturally separate (validating Section 5.4)
  3. Inference Cost Benchmark: Measure wall-clock time and memory for MoE-POT-S (90M activated) vs. DPOT-M (122M activated) to verify the claimed 33% compute reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Noise injection mechanism lacks specific parameterization details (epsilon scale), making reproducibility challenging
- Choice of CNN architecture for experts is generically described as "convolutional subnetworks" without specifying layer configurations
- Paper assumes equal weighting for all datasets during pre-training without justification for datasets with vastly different complexity or sample sizes

## Confidence
- **High Confidence:** General MoE architecture design and performance improvements on standard PDE datasets are measurable and reproducible
- **Medium Confidence:** Interpretation of shared experts as capturing universal physical laws is plausible but not empirically validated beyond performance numbers
- **Low Confidence:** Specific noise injection parameters and their impact on training stability are unclear without additional experimentation

## Next Checks
1. Evaluate the router network on a held-out PDE dataset not seen during pre-training to verify the 98% classification accuracy generalizes beyond the training distribution
2. Conduct ablation studies varying the number of shared experts (0, 1, 2, 4) and routed experts (8, 12, 16, 20) to empirically determine the optimal configuration for different levels of dataset heterogeneity
3. Systematically vary the noise injection scale (epsilon) across an order of magnitude (0.001 to 0.1) to quantify its impact on pre-training stability and final model performance