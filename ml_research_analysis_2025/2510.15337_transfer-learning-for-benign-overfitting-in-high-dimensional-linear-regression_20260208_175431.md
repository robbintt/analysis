---
ver: rpa2
title: Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression
arxiv_id: '2510.15337'
source_url: https://arxiv.org/abs/2510.15337
tags:
- transfer
- learning
- where
- shift
- lemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges transfer learning and benign overfitting in
  high-dimensional linear regression by proposing a two-step Transfer Minimum-Norm
  Interpolator (TM) that first pre-trains on source data then fine-tunes on target
  data while minimizing distance to the pre-trained model. The authors characterize
  the non-asymptotic excess risk of TM, showing it can outperform target-only MNI
  under certain conditions including model and covariate shifts.
---

# Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression

## Quick Facts
- arXiv ID: 2510.15337
- Source URL: https://arxiv.org/abs/2510.15337
- Reference count: 40
- Key outcome: This paper bridges transfer learning and benign overfitting in high-dimensional linear regression by proposing a two-step Transfer Minimum-Norm Interpolator (TM) that first pre-trains on source data then fine-tunes on target data while minimizing distance to the pre-trained model. The authors characterize the non-asymptotic excess risk of TM, showing it can outperform target-only MNI under certain conditions including model and covariate shifts. They identify "free-lunch" covariate shifts where leveraging heterogeneous data yields knowledge transfer benefits at limited cost. A data-driven procedure detects informative sources and an ensemble method aggregates multiple informative Transfer MNIs with adaptive weights. Finite-sample experiments demonstrate robustness to distribution shifts and superior performance over transfer baselines including pooled-MNI and SGD-based transfer, confirming empirical advantage under overparameterization. The work reveals that benign overfitting mechanisms can be enhanced through carefully designed transfer learning schemes that balance bias reduction against variance inflation.

## Executive Summary
This paper proposes a novel transfer learning framework for minimum-$\ell_2$-norm interpolators (MNI) in high-dimensional linear regression, where benign overfitting occurs when $n < p$. The Transfer MNI (TM) method pre-trains on source data then fine-tunes on target data while retaining target-learned signal and transferring source knowledge only into the null space. The authors provide non-asymptotic excess risk bounds characterizing when TM achieves positive transfer, identifying conditions on model shift (SSR), covariate shift, and sample sizes. They discover "free-lunch" covariate shifts where upscaling source covariance eigenvalues reduces variance inflation without affecting bias. A cross-validation-based source detection algorithm identifies informative sources, and an ensemble method (WTM) aggregates multiple informative Transfer MNIs. Experiments validate theoretical predictions and demonstrate robustness to distribution shifts.

## Method Summary
The method proposes a two-step Transfer Minimum-Norm Interpolator (TM) for high-dimensional linear regression. First, pre-train on source data to obtain $\hat{\beta}_M^{(q)}$. Second, fine-tune on target data by interpolating target observations while minimizing distance to the pre-trained model, yielding $\hat{\beta}_{TM}^{(q)} = \hat{\beta}_M^{(0)} + (I_p - H^{(0)})\hat{\beta}_M^{(q)}$ where $H^{(0)}$ is the projection matrix onto the target design span. The framework includes a cross-validation-based source detection algorithm to identify informative sources and a weighted ensemble method (WTM) that aggregates multiple informative Transfer MNIs with weights inversely proportional to their cross-validation losses. The approach is evaluated against target-only MNI, pooled-MNI, and SGD-based transfer baselines.

## Key Results
- TM can achieve positive transfer when model shift SSR$_q < 1$ and source sample size is below the optimal threshold $n_q^* = p - 1 - \sqrt{p(p-1)/(\text{SNR}_q(1-\text{SSR}_q))}$
- "Free-lunch" covariate shifts exist where uniform upscaling of source covariance eigenvalues ($\alpha > 1$) reduces variance inflation by factor $1/\alpha$ while preserving bias bounds
- Cross-validation reliably detects informative sources, and WTM ensemble outperforms naive averaging and individual Transfer MNIs
- TM shows superior robustness to distribution shifts compared to pooled-MNI and SGD-based transfer methods in finite-sample experiments

## Why This Works (Mechanism)

### Mechanism 1: Retain-Plus-Transfer Decomposition
The Transfer MNI (TM) preserves target-learned information in the row space while injecting source knowledge only into the null space. The TM estimate decomposes as $\hat{\beta}_{TM}^{(q)} = \hat{\beta}_M^{(0)} + (I_p - H^{(0)})\hat{\beta}_M^{(q)}$. The projection $(I_p - H^{(0)})$ ensures source knowledge transfers exclusively to directions orthogonal to target training data, avoiding interference with target-learned signal in span $\mathcal{S}_0$. This mechanism relies on target and source designs having full row rank ($n_q < p$) so null spaces are non-trivial.

### Mechanism 2: Bias Reduction vs. Variance Inflation Tradeoff
Positive transfer requires model shift small enough that bias reduction outweighs inevitable variance inflation from adding source noise. Lemma 1 shows TM variance includes an inflation term $V^{(q)}_\uparrow > 0$ almost surely from source noise projected onto target null space. Corollary 1 quantifies: positive transfer iff $\text{SSR}_q < 1$ and $\text{SNR}_q(1 - \text{SSR}_q) > p/(p-n_q-1)$. This tradeoff depends on sub-Gaussian covariates with bounded spectral ratios $\|\Sigma^{(0)}(\Sigma^{(q)})^{-1}\| = O(1)$.

### Mechanism 3: Free-Lunch Covariate Shift
Uniform upscaling of source covariance eigenvalues ($\Lambda^{(q)} = \alpha \Lambda^{(0)}$, $\alpha > 1$) with partial eigenvector alignment reduces variance inflation without affecting bias convergence rate. Corollary 2 shows: (A) partial alignment of top $\tau^*$ eigenvectors preserves bias bounds; (B) full alignment reduces exact variance inflation by factor $1/\alpha$. This occurs because upscaling increases "effective regularization" by spreading noise over larger-magnitude directions.

## Foundational Learning

- **Concept: Minimum-norm interpolators and implicit regularization**
  - Why needed here: The entire approach builds on MNI properties—understanding why interpolation generalizes in overparameterized regimes is prerequisite to grasping how transfer can enhance it.
  - Quick check question: Given design matrix $X \in \mathbb{R}^{n \times p}$ with $n < p$, write the closed-form MNI solution and explain why it has minimum $\ell_2$-norm among all interpolators.

- **Concept: Effective ranks and benign overfitting conditions**
  - Why needed here: Theorem 2's bounds depend on effective ranks $r_k(\Sigma)$ and $R_k(\Sigma)$; understanding Definition 1 is essential to interpret when TM is "benign."
  - Quick check question: For covariance with eigenvalues $\lambda_j \asymp j^{-\gamma}$, compute $r_k(\Sigma)$ and state what conditions on $\gamma$ and $(n, p)$ ensure benign overfitting per Definition 2.

- **Concept: Model shift vs. covariate shift in transfer learning**
  - Why needed here: The paper analyzes both independently and jointly; distinguishing $\delta^{(q)} = \beta^{(q)} - \beta^{(0)}$ (model) from $\Sigma^{(q)} \neq \Sigma^{(0)}$ (covariate) is foundational.
  - Quick check question: For target model $\beta^{(0)}$ and source model $\beta^{(1)} = 2\beta^{(0)}$, compute SSR$_1$. If $\Sigma^{(1)} = 3\Sigma^{(0)}$, what type of shift is this and does Corollary 2 apply?

## Architecture Onboarding

- **Component map:** Single-source TM (Section 3) -> Pre-train source MNI $\hat{\beta}_M^{(q)}$ -> Compute target MNI $\hat{\beta}_M^{(0)}$ and projection $(I_p - H^{(0)})$ -> Return $\hat{\beta}_{TM}^{(q)} = \hat{\beta}_M^{(0)} + (I_p - H^{(0)})\hat{\beta}_M^{(q)}$ -> Informative source detector (Section 4, Algorithm 1) -> K-fold CV on target data -> Compare TM vs. target-only CV losses -> Detect sources where $\hat{L}(\hat{\beta}_{TM}^{(q)}) - \hat{L}(\hat{\beta}_M^{(0)}) \leq D_0^{(0)}$ -> WTM ensemble (Algorithm 1) -> Collect detected sources $\hat{\mathcal{I}}$ -> Weight by inverse CV loss $w_i = [\hat{L}(\hat{\beta}_{TM}^{(i)})]^{-1}$ -> Return $\hat{\beta}_{WTM} = \sum_{i \in \hat{\mathcal{I}}} w_i \hat{\beta}_{TM}^{(i)}$

- **Critical path:**
  1. Verify $n_q < p$ for all sources and target (interpolation requires overparameterization)
  2. Compute target projection $H^{(0)} = X^{(0)\top}(X^{(0)}X^{(0)\top})^{-1}X^{(0)}$ efficiently via SVD
  3. Run CV detection before committing to ensemble—detecting non-informative sources early prevents negative transfer contamination

- **Design tradeoffs:**
  - **K-fold CV (default K=5):** Higher K improves detection accuracy but increases computation; lower K risks false positives/negatives in source selection.
  - **Detection threshold $\varepsilon_0$:** Smaller $\varepsilon_0$ is more conservative (fewer sources, less variance inflation risk) but may miss beneficial sources; larger $\varepsilon_0$ is aggressive.
  - **Source sample size $n_q$:** Corollary 1 shows optimal $n_q^*$ exists—transferring more samples helps only up to $n_q^*$, then hurts.

- **Failure signatures:**
  - TM excess risk exceeds target-only baseline: Likely $\text{SSR}_q$ too large or $n_q > n_q^*$; check model alignment
  - WTM performs worse than best single TM: CV detection may have included non-informative source; increase $\varepsilon_0$
  - Variance dominates even with "benign" covariances: Effective rank conditions may be violated; verify $r_{k_q^*}(\Sigma^{(q)}) \geq b_q n_q$

- **First 3 experiments:**
  1. **Reproduce isotropic model shift results (Figure 2):** Set $\Sigma^{(0)} = \Sigma^{(q)} = I_p$, vary SSR ∈ {0.1, 0.4, 0.7}, compute optimal $n_q^*$ per Corollary 1, compare TM vs. target-only vs. pooled-MNI excess risk across $p \in \{300, \ldots, 1000\}$
  2. **Validate free-lunch covariate shift (Corollary 2):** Use spiked covariance $\Sigma^{(0)}$ with eigenvalues $\lambda_j^{(0)} \asymp j^{-\gamma}$ for $j > s$; set $\Sigma^{(q)} = \alpha \Sigma^{(0)}$ with $\alpha \in \{1, 4, 8\}$; verify variance inflation decreases by $1/\alpha$ while bias bound unchanged
  3. **Test source detection robustness:** Generate Q=3 sources with SSR = (0, 0.3, 0.6); run Algorithm 1 with K=5, $\varepsilon_0 = 0.5$; verify $\hat{\mathcal{I}}$ excludes the high-SSR source and WTM outperforms naive ensemble of all three

## Open Questions the Paper Calls Out

### Open Question 1
Does cross-validation consistently identify the true set of informative sources for Transfer MNI under benign overfitting regimes? The authors note that establishing the consistency of informative source detection via cross-validation remains an open problem, as existing transferability detection consistency results rely on explicit regularization, whereas benign overfitting involves implicit regularization only.

### Open Question 2
Can the free-lunch covariate shift result for Transfer MNI be extended to general covariance structures without requiring τ*-alignment of leading eigenvectors? While Remark 2 relaxes the τ*-alignment condition to a bound on eigenvector misalignment, the analysis remains limited to specific spectral structures, and whether similar benefits emerge under arbitrary covariate shifts is unknown.

### Open Question 3
How does the minimum-RKHS-norm Transfer MNI behave under model and covariate shifts, particularly regarding generalization gains and benign overfitting? The extension to nonlinear, infinite-dimensional regression in a reproducing kernel Hilbert space introduces kernel-dependent factors that govern benign overfitting differently from linear settings, and the interplay with distribution shifts is unexplored.

## Limitations

- The theoretical guarantees assume fixed designs, but experiments use random Gaussian designs, creating a gap between theory and practice that requires careful justification.
- The free-lunch covariate shift mechanism (Corollary 2) lacks direct empirical validation in the paper—the claims about variance reduction by factor 1/α are theoretically derived but not experimentally verified with different α values.
- The detection threshold ε₀ = 1/2 in Algorithm 1 is chosen heuristically without sensitivity analysis, and the robustness of source detection to this hyperparameter choice remains unclear.

## Confidence

- **High confidence:** The decomposition-based transfer mechanism (Mechanism 1) is mathematically sound and the variance-bias tradeoff conditions (Mechanism 2) are rigorously derived with clear break conditions.
- **Medium confidence:** The free-lunch covariate shift claims are theoretically elegant but lack direct empirical support. The ensemble weighting scheme is reasonable but the CV-based detection threshold choice needs more validation.
- **Low confidence:** The effective rank conditions (rₖ(Σ) ≥ bₖn) are critical for benign overfitting but their verification in practical settings is challenging, and the paper doesn't provide clear diagnostics for when these conditions fail.

## Next Checks

1. **Validate free-lunch mechanism:** Run controlled experiments varying α ∈ {1, 4, 8} for uniform covariance scaling with partial eigenvector alignment. Measure actual variance reduction vs. theoretical 1/α prediction.

2. **Test detection threshold sensitivity:** Run Algorithm 1 with ε₀ ∈ {0.3, 0.5, 0.7} on synthetic data with known informative/non-informative sources. Evaluate detection accuracy and ensemble performance degradation.

3. **Check effective rank conditions empirically:** For real or synthetic data with different spectral decays (γ ∈ {0.5, 1.0, 1.5}), compute empirical effective ranks and verify they meet theoretical requirements for benign overfitting.