---
ver: rpa2
title: A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing
  Detection
arxiv_id: '2510.21679'
source_url: https://arxiv.org/abs/2510.21679
tags:
- dataset
- videos
- video
- labels
- framing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multimodal benchmark dataset for detecting
  framing in oil and gas advertising, with the potential to identify greenwashing.
  The dataset includes expert-annotated video ads from Facebook and YouTube across
  20 countries, covering 13 framing types.
---

# A Multimodal Benchmark for Framing of Oil & Gas Advertising and Potential Greenwashing Detection

## Quick Facts
- arXiv ID: 2510.21679
- Source URL: https://arxiv.org/abs/2510.21679
- Reference count: 40
- Models detect environmental framing at 79% F1 but struggle with green innovation framing at 46% F1

## Executive Summary
This paper introduces a multimodal benchmark dataset for detecting strategic framing in oil and gas advertising, with the potential to identify greenwashing. The dataset includes expert-annotated video ads from Facebook and YouTube across 20 countries, covering 13 framing types. Experiments show that while GPT-4.1 can detect environmental messages with 79% F1 score, identifying framing around green innovation remains challenging at 46% F1 score. The dataset is designed to evaluate vision-language models on real-world strategic framings and supports cross-domain, entity-level, and temporal analysis.

## Method Summary
The researchers created a dataset of 706 video advertisements from Facebook and YouTube spanning 20 countries and 13 framing types. Videos were downloaded, transcribed using Whisper-1, and segmented by speech breaks. Frames were extracted at transcript midpoints and paired with corresponding transcript segments. Frame-transcript pairs were fed to vision-language models with annotation guidelines, either zero-shot or with entity-aware 1-shot examples retrieved via CLIP embedding similarity. Performance was evaluated using micro-averaged F1 scores across multi-label predictions.

## Key Results
- GPT-4.1 achieves 79% F1 on environmental framing detection but only 46% F1 on green innovation framing
- Multimodal analysis is critical: 37% of Facebook videos lack transcripts and ~30% of all videos contain no spoken language
- Entity-aware 1-shot prompting improves open-weight models significantly (Qwen2.5-VL 7B: 37.3%→59.2% on YouTube) but provides minimal benefit for GPT-4.1
- Geographic performance varies: DeepSeekVL2 outperforms GPT-4o-mini on Asian content while GPT-4o-mini excels on North American/European content

## Why This Works (Mechanism)

### Mechanism 1
Entity-aware 1-shot prompting improves VLM framing detection by retrieving examples from the same corporate entity with similar visual-transcript content. Entity Restriction (ER) limits candidate examples to videos from the same company, ensuring stylistic consistency. Embedding-based Search (ES) then uses CLIP embeddings of frames and transcripts to find the most similar training video via cosine similarity. The retrieved example provides in-context guidance for the target video's framing patterns. Core assumption: Framing strategies are consistent within entities and similar visual-transcript content indicates similar framing types. Evidence anchors: Qwen2.5-VL 32B with ES+ER achieved 66.2% (YouTube) vs 64.0% without ES; mixed ER effects across models in Table 8. Break condition: When few training videos exist per entity, ER may limit retrieval quality.

### Mechanism 2
Multimodal frame-transcript pairing enables detection of framing that relies on implicit visual cues absent from spoken text. Whisper-1 segments transcripts by speech breaks; mean timestamps map to corresponding video frames. VLMs receive paired (frame, transcript) inputs, allowing cross-modal reasoning. This captures framing conveyed purely through imagery (e.g., wind turbines implying environmental commitment without verbal claims). Core assumption: Framing information distributes across visual and textual modalities; aligning them temporally preserves semantic coherence. Evidence anchors: 37% of FACEBOOK videos lack transcripts, highlighting visual modality necessity; transcript ablation dropped FACEBOOK performance from 70.5% to 60.6% (Qwen2.5-VL 32B). Break condition: Automatic transcription errors may misalign frames; very short transcripts (<3 words) are excluded as noisy.

### Mechanism 3
Distant labels transferred from text annotations to video content remain sufficiently valid for benchmarking when video-text framing correlates strongly. FACEBOOK labels derived from ad text (not video) are mapped to associated videos. A validation study found 83% F1 agreement between original text-based labels and manual video-only re-annotation on 20 random samples. Core assumption: Text-based framing annotations generalize to video content within the same ad campaign. Evidence anchors: Manual validation achieved 83% F1 between distant and video-only labels. Break condition: Videos with framing primarily in imagery (not accompanying text) may be mislabeled. Low-resource labels like 'PB' showed consistently poor F1 across models, potentially indicating annotation noise.

## Foundational Learning

- **Framing Theory (Entman)**
  - Why needed here: The paper operationalizes Entman's definition—"select some aspects of perceived reality to promote problem definition, causal interpretation, moral evaluation"—into 13 discrete labels. Without understanding framing as strategic selection, the annotation schema appears arbitrary.
  - Quick check question: Can you explain why "showing wind turbines without mentioning emissions reductions" qualifies as environmental framing under Entman's definition?

- **Vision-Language Model (VLM) Context Windows**
  - Why needed here: Frame count (NFrame) varies by model—10 for GPT-4.1, 3 for InternVL2—due to context length and computational constraints. Understanding token budgets explains why longer videos require sampling strategies.
  - Quick check question: Why might a 32B parameter model (Qwen2.5-VL) outperform smaller models on longer videos despite identical NFrame settings?

- **Multi-label Classification Evaluation**
  - Why needed here: Videos receive multiple labels (e.g., "Community and Life" + "Environment"). Micro-averaged F1 treats each label prediction independently, which differs from macro-averaging or subset accuracy. Misunderstanding this leads to incorrect performance interpretation.
  - Quick check question: If a video has gold labels [A, B] and predictions [A, C], what are precision and recall for this instance under micro-averaging?

## Architecture Onboarding

- **Component map**: Data Layer -> Preprocessing -> Prompt Constructor -> VLM Inference -> Evaluation
- **Critical path**: Input video URL → download/validate → Extract frames at transcript segment midpoints (or uniform if no transcript) → Construct prompt: annotation guidelines + frame-transcript pairs + 1-shot example (if enabled) → VLM generates JSON label list → Compare against gold labels → compute F1
- **Design tradeoffs**:
  - NFrame selection: Higher values capture more content but increase token costs and may exceed context windows. Paper uses 3–10 based on model capacity.
  - Entity Restriction: Ensures entity-specific framing consistency but fails when entities have sparse training data. Table 8 shows ER helped Qwen2.5-VL 32B but hurt DeepSeekVL2 on YouTube.
  - Zero-shot vs. 1-shot: 1-shot improved open-weight models significantly (Qwen2.5-VL 7B: 37.3%→59.2% on YouTube) but GPT-4.1 zero-shot outperformed its 1-shot (71.0% vs 69.3%), suggesting strong models may not need examples.
- **Failure signatures**:
  - Low F1 on implicit framings: 'Green Innovation' at 46% (GPT-4.1 best) suggests models struggle with subjective impressions vs. concrete claims
  - Geographic bias: Figure 5 shows GPT-4o-mini performs better on North American/European content; DeepSeekVL2 outperforms on Asian content
  - Over-labeling on difficult categories: Figure 6 shows higher recall than precision for 'Green Innovation' and 'Patriotism'—models predict these liberally while annotators are conservative
  - Short video underperformance: Figure 4 shows 0–60 sec videos scored lowest (counterintuitive); authors suggest short videos are more contextualized/vague
- **First 3 experiments**:
  1. Zero-shot baseline across all 6 VLMs: Establish model capabilities without examples; compare GPT-4.1 vs. open-weight alternatives on both domains
  2. 1-shot with full ES+ER pipeline: Validate entity-aware retrieval contribution; expect largest gains on entities with >10 training videos
  3. Transcript ablation on FACEBOOK subset: Quantify multimodal necessity; expect >10% F1 drop given 37% of FACEBOOK videos lack transcripts

## Open Questions the Paper Calls Out

### Open Question 1
Can the task be extended to granularly distinguish between implicit and explicit messaging, and identify the specific modality (visual vs. spoken vs. captions) used to convey the frame? The authors attempted to annotate "implicit vs. explicit" distinctions during dataset construction but abandoned it due to a lack of inter-annotator agreement. What evidence would resolve it: A refined annotation schema with high agreement scores for these attributes and a model capable of disambiguating the source of framing with high accuracy.

### Open Question 2
What techniques can improve VLM performance on high-subtlety, low-frequency framing categories such as "Green Innovation," where models currently struggle (46% F1)? Models tend to over-label difficult categories (high recall, low precision) or fail to capture the subjective, futuristic nature of "Green Innovation" compared to clearer visual cues like "Community." What evidence would resolve it: A model architecture or prompting strategy that achieves a balanced precision-recall trade-off and raises the F1 score for "Green Innovation" to levels comparable with "Environment" or "Community."

### Open Question 3
How can VLMs be improved to handle framing detection in short-form video content (e.g., TikTok, Reels) where contextual cues are sparser? The limitations section notes the study excludes "short-form video content... despite its significant and increasing prevalence," and error analysis showed shorter videos (0-60s) in the current dataset were more challenging. What evidence would resolve it: Benchmark results from a dataset of short-form O&G videos showing that VLMs can maintain performance stability as video length decreases.

## Limitations
- The dataset contains only 706 videos, limiting statistical power for rare framing types
- The 'distant' label transfer from text to video content introduces potential bias, despite 83% F1 validation
- Model performance drops significantly on implicit framings like green innovation (46% F1), suggesting the benchmark may be more suitable for explicit messaging detection than nuanced strategic communication

## Confidence
- **High Confidence**: Multimodal necessity for framing detection (37% FACEBOOK videos lack transcripts, Figure 4 shows transcript ablation drops performance)
- **Medium Confidence**: Entity-aware retrieval improves performance (mixed results in Table 8, some models hurt by ER)
- **Medium Confidence**: 1-shot prompting benefits open-weight models (Qwen2.5-VL 7B: 37.3%→59.2% on YouTube, but GPT-4.1 shows minimal gains)
- **Low Confidence**: Generalizability to unseen framing types (limited evaluation on rare categories, PB consistently underperforms)

## Next Checks
1. **Cross-cultural validation**: Test the benchmark's 13 framing types on oil and gas advertising from additional regions (Africa, South America) to assess cultural framing detection gaps
2. **Temporal consistency**: Re-annotate 50 videos from different years to measure label stability over time and detect framing evolution patterns
3. **Human expert comparison**: Have three framing experts independently annotate 100 random videos to establish upper bounds on achievable F1 scores and identify annotation ambiguities