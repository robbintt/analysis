---
ver: rpa2
title: Confidence is Not Competence
arxiv_id: '2510.24772'
source_url: https://arxiv.org/abs/2510.24772
tags:
- belief
- state
- solved
- assessment
- unsolved
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study demonstrates that large language models exhibit a fundamental
  decoupling between their internal assessment of solvability and their actual problem-solving
  competence. Researchers developed a confound-resistant dataset curation protocol
  and employed linear probes to identify a robust, linearly decodable "solvability
  belief" state that generalizes across model families and reasoning domains.
---

# Confidence is Not Competence

## Quick Facts
- **arXiv ID:** 2510.24772
- **Source URL:** https://arxiv.org/abs/2510.24772
- **Reference count:** 40
- **Primary result:** Large language models exhibit a fundamental decoupling between internal solvability assessment and actual problem-solving competence.

## Executive Summary
This study reveals that large language models maintain a robust internal "solvability belief" that is linearly decodable but causally inert with respect to task performance. Researchers developed a confound-resistant dataset curation protocol and employed linear probes to identify this belief state, which generalizes across model families and reasoning domains. The key finding is that forcefully steering this belief state left final task accuracy statistically unchanged, indicating that internal confidence does not translate to reliable performance. The authors propose a "Two Brains" architecture where a high-dimensional assessment system feeds into a low-dimensional execution system, explaining why interventions at the belief level fail to improve competence.

## Method Summary
The researchers curated a confound-resistant dataset of 846 math problems (423 solved, 423 unsolved) with length control and topic balancing. They trained linear probes (Logistic Regression, C=0.8) on hidden states from the last input token across all layers of Gemma 3 4B, Llama 3.1 8B, and Mistral Small 24B models. The derived belief vector was used for causal steering interventions at the layer of peak probe accuracy. Participation Ratio analysis via PCA tracked dimensionality collapse from high-dimensional assessment (~33-44) to low-dimensional execution (~16-18) spaces. Validation extended to Knights & Knaves, OpenR1 Coding, and QwQ Planning tasks.

## Key Results
- Linear probes decoded solvability belief with 70-75% accuracy, matching non-linear probe performance
- Causal steering experiments showed belief flips (~0.9) but zero effect on task accuracy (p > 0.9)
- Principal component analysis revealed geometric collapse: assessment manifolds (PR ~33-44) vs. execution traces (PR ~16-18)
- Results generalized across three model families and three reasoning domains

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A model's internal "solvability belief" is a linearly decodable state that exists prior to generation.
- **Mechanism:** The model encodes an assessment of task difficulty along a specific direction (d_solv) in the residual stream. Because this signal is linearly separable, simple classifiers (Logistic Regression) can decode it as effectively as complex non-linear ones.
- **Core assumption:** The decoded signal reflects a genuine "belief" about solvability rather than a superficial heuristic like prompt length.
- **Evidence anchors:**
  - [abstract] "A simple linear probe decodes the internal 'solvability belief' of a model..."
  - [Section 3.2] "The powerful non-linear probes offer no significant improvement over the simple linear one."
- **Break condition:** If non-linear probes significantly outperform linear probes, the "simple axis" assumption fails.

### Mechanism 2
- **Claim:** Manipulating the internal belief state (confidence) has no causal effect on final task competence (accuracy).
- **Mechanism:** Forcefully steering the activation state along the belief vector (h + alpha * d_solv) successfully flips the internal "confidence" reading but fails to alter the output trajectory.
- **Core assumption:** The steering vector derived from the probe weights accurately represents the functional "belief" direction.
- **Evidence anchors:**
  - [abstract] "Causal intervention experiments showed that forcefully steering this belief state left final task accuracy statistically unchanged..."
  - [Section 4.2] Table 2 shows ~0.9 belief flip but 0.0% performance change (p > 0.9).
- **Break condition:** If steering the belief vector *does* change accuracy, the two systems are coupled.

### Mechanism 3
- **Claim:** The decoupling is caused by a geometric "collapse" from a high-dimensional assessment space to a low-dimensional execution space.
- **Mechanism:** The "Assessment Brain" operates on a high-dimensional manifold (Participation Ratio ~33-44) to evaluate complexity. Upon generation, the system "collapses" into a low-dimensional "Execution Brain" (PR ~16-18). Linear interventions in the high-dimensional assessment space are geometrically insufficient to alter the constrained dynamics of the low-dimensional execution path.
- **Core assumption:** The Participation Ratio accurately reflects the functional complexity and constraints of the cognitive systems.
- **Evidence anchors:**
  - [abstract] "...belief state resides on a high-dimensional manifold... while execution traces occupy a much lower-dimensional space..."
  - [Section 5.1] "The 'Assessment' system... has a high effective dimensionality... The 'Execution' system... is more than twice as simple."
- **Break condition:** If execution traces exhibit high dimensionality similar to assessment, the geometric explanation for causal inertness fails.

## Foundational Learning

- **Concept:** **Linear Probes & Logistic Regression**
  - **Why needed here:** To understand how the authors extracted the "solvability belief." You must grasp that if a linear classifier can separate "solved" vs. "unsolved" activations, the concept is encoded linearly in the vector space.
  - **Quick check question:** If a linear probe fails but a 2-layer MLP succeeds, is the belief state linearly decodable? (Answer: No).

- **Concept:** **Participation Ratio (Effective Dimensionality)**
  - **Why needed here:** This metric quantifies the "geometric complexity." It explains *why* the assessment system is harder to control (many degrees of freedom) than the execution system (few degrees of freedom).
  - **Quick check question:** Does a higher Participation Ratio indicate a more constrained or more complex manifold? (Answer: More complex).

- **Concept:** **Activation Steering (Vector Addition)**
  - **Why needed here:** To replicate the causal intervention. You need to understand that `h' = h + alpha * vector` modifies the internal state without changing model weights.
  - **Quick check question:** In this paper, did steering the activation vector change the model's weights? (Answer: No).

## Architecture Onboarding

- **Component map:** Assessment Brain (last input token, high-dim) -> Collapse Event (first generated token) -> Execution Brain (CoT traces, low-dim)

- **Critical path:** To intervene successfully, you must target the **Execution dynamics** (low-dim), not the **Assessment state** (high-dim). The paper argues against steering high-level representations for reliability.

- **Design tradeoffs:**
  - **Pros:** The Assessment state can be used for early stopping or resource allocation (predicting failure early).
  - **Cons:** You cannot use the Assessment state to "fix" reasoning errors or improve reliability.

- **Failure signatures:**
  - **High Confidence, Low Competence:** The model predicts it will solve the problem (Probe > 0.9) but the execution trace falls off the low-dimensional "competence" manifold.
  - **Inert Steering:** You apply a steering vector, the probe output flips, but the final answer remains identical.

- **First 3 experiments:**
  1. **Probe Extraction:** Train a Logistic Regression classifier on hidden states (last token) from "Solved" vs. "Unsolved" math problems. Verify accuracy > 70%.
  2. **Causal Test:** Use the probe weights as a steering vector on a test set. Compare "Steered" accuracy vs. "Baseline" accuracy to confirm statistical inertness (p > 0.05).
  3. **Dimensionality Check:** Run PCA on the assessment states vs. execution traces. Calculate the Participation Ratio to verify the "collapse" from ~40 dimensions to ~16.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the "Two Brains" architecture persist in open-ended or creative domains where a single "correct" execution manifold may not exist?
- **Basis in paper:** [explicit] The authors explicitly ask whether the model generalizes to tasks like poetry or dialogue, proposing competing hypotheses that the execution manifold might be higher-dimensional or that the phases might interleave (Section C.1).
- **Why unresolved:** The study limited its scope to verifiable reasoning tasks (math, code, logic) to ensure ground-truth competence could be measured, leaving creative domains unexplored.
- **What evidence would resolve it:** Measurements of Participation Ratios (PR) for execution traces in creative tasks; observation of interleaved assessment/execution phases rather than a sharp collapse.

### Open Question 2
- **Question:** Can interventions targeting the low-dimensional execution dynamics successfully improve task competence where steering the high-dimensional belief state failed?
- **Basis in paper:** [explicit] The conclusion argues that "future reliability work should target execution dynamics rather than assessment states," implying that the efficacy of execution-level interventions remains untested (Section 6).
- **Why unresolved:** The paper successfully demonstrated that steering the *Assessment* brain is causally inert, but did not attempt to steer the *Execution* brain to verify if it is a more effective lever.
- **What evidence would resolve it:** Experiments applying activation steering or causal interventions specifically within the low-dimensional execution subspace (identified by the Execution Basis) to observe changes in final task accuracy.

### Open Question 3
- **Question:** How does the geometric separation between assessment and execution manifolds scale with model size beyond 24 billion parameters?
- **Basis in paper:** [explicit] The authors note that while results are consistent across 4Bâ€“24B models, it is a "critical open question" whether the geometric separation sharpens or the execution manifold grows in dimensionality in frontier-scale models (Section C.1).
- **Why unresolved:** Compute constraints limited the study to smaller instruct-tuned models; frontier models may develop different internal geometric properties due to capacity.
- **What evidence would resolve it:** Replication of the Participation Ratio analysis and subspace fitting experiments on models exceeding 100B parameters.

## Limitations

- **Limited model scale:** Results are based on 4B-24B parameter models; geometric properties may differ in frontier-scale models
- **Task domain restriction:** Only verifiable reasoning tasks (math, code, logic) were studied; creative domains remain unexplored
- **Steering parameter uncertainty:** Exact steering strength (alpha) and layer indices for intervention were not fully specified

## Confidence

- **High confidence:** Linear probe accuracy (70-75%) matches non-linear performance
- **High confidence:** Causal steering shows no effect on task accuracy (p > 0.9)
- **Medium confidence:** Dimensionality collapse measurements due to unspecified steering parameters
- **Low confidence:** Generalization to creative domains without empirical validation

## Next Checks

1. **Dataset curation validation:** Verify confound-resistant filtering by testing length distributions and topic balancing with two-sample t-tests (p > 0.4)
2. **Probe replication:** Train Logistic Regression probe on held-out data and confirm 70-75% accuracy with linear probe matching non-linear performance
3. **Dimensionality analysis:** Run PCA on assessment states and execution traces, calculate Participation Ratio to confirm geometric collapse from ~40 to ~16 dimensions