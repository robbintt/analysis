---
ver: rpa2
title: A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering
  Drawings Using Vision Language Model
arxiv_id: '2510.21862'
source_url: https://arxiv.org/abs/2510.21862
tags:
- drawings
- detection
- stage
- engineering
- layout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a three-stage hybrid framework to automate
  interpretation of multi-view engineering drawings using vision language models (VLMs).
  Stage 1 applies YOLOv11-det for layout segmentation, isolating views, title blocks,
  and notes.
---

# A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model

## Quick Facts
- arXiv ID: 2510.21862
- Source URL: https://arxiv.org/abs/2510.21862
- Reference count: 11
- Primary result: Three-stage hybrid framework using YOLOv11 and Donut VLMs achieves F1 scores of 0.923 (measures), 0.965 (GD&T), and 0.672 (alphabetical) for automated interpretation of multi-view engineering drawings.

## Executive Summary
This paper presents a three-stage hybrid framework that automates the interpretation of multi-view engineering drawings into structured JSON data. The system employs YOLOv11 for layout segmentation and annotation localization, followed by specialized Donut-based Vision Language Models (VLMs) for semantic parsing of numerical and alphabetical content. The framework addresses the challenge of extracting structured information from complex 2D drawings, achieving strong performance on technical annotations while identifying areas for improvement in textual content extraction. The approach enables scalable automated information extraction suitable for direct integration into CAD and manufacturing databases.

## Method Summary
The framework consists of three sequential stages: (1) YOLOv11-det for layout segmentation, isolating views, title blocks, and notes from the drawing; (2) YOLOv11-obb for orientation-aware localization of technical annotations including measures, GD&T frames, and surface roughness symbols; and (3) two specialized Donut-based VLMs for semantic parsingâ€”a Numerical VLM fine-tuned for quantitative specifications and an Alphabetical VLM operating in zero-shot mode for textual content. The system was trained on two domain-specific datasets: 1,000 drawings for layout detection and 1,406 for annotation localization and VLM training, with outputs merged into unified JSON format.

## Key Results
- Numerical VLM achieved F1 scores of 0.923 (measures), 0.965 (GD&T), and 1.0 (surface roughness)
- Alphabetical VLM achieved overall F1 of 0.672 with high hallucination rates (0.478) in title blocks
- Surface roughness detection accuracy (0.54) significantly lower than other classes due to class imbalance
- Layout detection accuracy: Views 0.96, Title Block 0.99, Notes 0.98
- Annotation detection accuracy: Measures 0.95, GD&Ts 0.97, Roughness 0.54

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition improves performance on complex multi-view documents by isolating semantic regions before semantic parsing. The framework uses YOLOv11-det to segment layouts, preventing region overlap and degraded localization that occurs when processing dense, multi-view drawings in a single pass.

Core assumption: Engineering drawing layouts follow consistent enough structure for reliable segregation before semantic parsing.

Evidence anchors:
- [abstract] "Stage 1 applies YOLOv11-det for layout segmentation, isolating views, title blocks, and notes."
- [section 4 intro] "Processing one view at a time minimizes confusion that might occur if the detector operated on the entire drawing..."
- [corpus] Neighbor papers corroborate the shift toward hybrid frameworks for complex drawings.

Break condition: Failure of Stage 1 detector to separate overlapping views cascades localization errors into subsequent stages.

### Mechanism 2
Orientation-aware object detection (OBB) accurately localizes rotated technical annotations that standard axis-aligned bounding boxes miss. Stage 2 employs YOLOv11-obb to predict object angles and anisotropic dimensions, addressing rotated GD&T frames and symbols that traditional detectors often fail to capture.

Core assumption: Annotations possess distinct visual features even when rotated, allowing orientation-invariant learning from training data.

Evidence anchors:
- [abstract] "Stage 2 uses YOLOv11-obb for orientation-aware localization of annotations..."
- [section 4.2] "Oriented box approach ensures robust localization of rotated elements, which traditional axis-aligned detectors... often miss."
- [corpus] Limited direct evidence in neighbors regarding OBB specifically.

Break condition: Dense clusters of symbols with significant bounding box overlap may require non-maximum suppression tuning.

### Mechanism 3
Specializing VLMs for numerical/symbolic vs. textual content with schema-constrained fine-tuning yields higher fidelity extraction than generic OCR. The Numerical VLM fine-tuned to map visual input directly to schema-defined JSON minimizes output variability, while the zero-shot Alphabetical VLM handles free-form text.

Core assumption: Numerical and symbolic data follow stricter, more repeatable schemas than free-form textual fields.

Evidence anchors:
- [section 4.3] "Numerical VLM... benefited from a schema-constrained learning setup that minimized output variability..."
- [table 1] Numerical VLM Overall F1 = 0.963 vs. Alphabetical VLM Overall F1 = 0.672.
- [section 5] "Fine-tuned Numerical VLM performed consistently... Conversely, Alphabetical VLM exhibited variability..."

Break condition: High hallucination rates (0.478) in zero-shot Alphabetical VLM may render it unreliable for automated database entry without human verification.

## Foundational Learning

- Concept: YOLO Object Detection (Standard vs. OBB)
  - Why needed here: Framework relies on YOLOv11-det (axis-aligned) for layout and YOLOv11-obb (oriented) for rotated symbols. Understanding bounding box regression differences is vital for debugging localization failures.
  - Quick check question: Can you explain why an axis-aligned bounding box would fail to tightly enclose a 45-degree rotated dimension line, and how an OBB solves this?

- Concept: OCR-free Document Understanding (Donut)
  - Why needed here: Stage 3 uses Donut-based VLMs to bypass traditional OCR. Understanding encoder-decoder structure (Vision Encoder -> Text Decoder) is necessary to grasp how it maps image patches directly to JSON strings.
  - Quick check question: How does a Donut model handle a document without an intermediate optical character recognition step, and what is the role of the JSON prompt during inference?

- Concept: Schema-Constrained Fine-Tuning
  - Why needed here: Performance gap between Numerical and Alphabetical VLMs attributed to explicit schema supervision. Recognizing how JSON structure regularizes model output helps in designing training data.
  - Quick check question: Why might forcing a model to generate output conforming to a strict JSON schema reduce hallucination for quantitative data compared to free-form text generation?

## Architecture Onboarding

- Component map:
  Input -> YOLOv11-det (Layout Segmenter) -> Crops of Views, Title Blocks, Notes -> YOLOv11-obb (Annotation Detector) -> Crops of Measures, GD&T, Roughness -> Donut (Numerical) & Donut (Alphabetical) -> Semantic JSON patches -> Aggregator -> Final JSON

- Critical path: Stage 1 Layout Detection. If YOLOv11-det fails to identify a "View" region, no downstream annotation localization or numerical parsing occurs for that view.

- Design tradeoffs:
  - Modularity vs. Latency: Three-stage pipeline is robust to complex layouts but introduces sequential inference latency compared to single-pass models.
  - Fine-tuning vs. Zero-shot: Authors traded high performance on numerical data for lower performance on title blocks to save annotation effort, accepting ~40% hallucination rate in textual fields.

- Failure signatures:
  - Class Imbalance: Surface Roughness detection accuracy (0.54) significantly lower than Measures/GD&T due to limited training samples (152 instances).
  - Textual Hallucination: High hallucination rates (0.478) in Title Blocks suggest zero-shot Alphabetical VLM invents fields when layout is dense or non-standard.
  - View Confusion: Closely spaced views might be merged into single bounding box if layout detector's confidence threshold is not tuned.

- First 3 experiments:
  1. Layout Validation: Run YOLOv11-det on 50 diverse drawings to verify correct distinction between adjacent orthographic views without merging.
  2. OBB Orientation Test: Pass cropped view with rotated GD&T symbols through YOLOv11-obb and visualize OBBs to confirm angle prediction is working.
  3. Schema Stress Test: Feed Numerical VLM degraded or handwritten measure crop to see if it degrades gracefully or outputs invalid JSON.

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating advanced VLMs (e.g., GPT-4o) significantly improve extraction accuracy for categorical and free-form textual fields compared to the current zero-shot Alphabetical VLM?

Basis in paper: [explicit] Authors state plans to "integrate advanced VLMs such as GPT-4o... aiming to enhance performance on categorical and free-form textual fields" in future work.

Why unresolved: Current Alphabetical VLM (zero-shot Donut) underperformed with overall F1 of 0.672 and high hallucination rate of 0.478 for Title Blocks.

What evidence would resolve it: Comparative F1 scores and hallucination rates between baseline Donut model and advanced VLMs on same Title Block and Notes dataset.

### Open Question 2
To what extent does synthetic data augmentation improve detection accuracy for underrepresented annotation classes such as Surface Roughness?

Basis in paper: [explicit] Conclusion identifies "synthetic data augmentation for underrepresented annotations" as future research focus to address class imbalance.

Why unresolved: Surface Roughness class suffered from low detection accuracy (0.54) due to only 152 annotated instances compared to thousands of Measures.

What evidence would resolve it: Marked increase in detection accuracy for Surface Roughness symbols after training on synthetically balanced dataset.

### Open Question 3
Can integration of VLM-based verification modules effectively identify and correct inconsistencies in generated structured JSON outputs?

Basis in paper: [explicit] Paper proposes future "integration of VLM-based verification modules to cross check generated outputs for consistency."

Why unresolved: While framework generates structured JSON, it currently lacks internal mechanism to validate semantic consistency or correctness of extracted data.

What evidence would resolve it: Reduction in logical errors and hallucination rates in final output when verification module is active versus baseline pipeline.

## Limitations

- Dataset Generalization: Framework performance relies on domain-specific datasets that may not capture full diversity of engineering drawing standards across industries, with significant performance gaps suggesting potential class imbalance issues.
- Alphabetical VLM Reliability: High hallucination rate (0.478) in zero-shot Alphabetical VLM represents critical limitation for production deployment, requiring substantial human verification.
- Layout Assumption Dependency: Hierarchical approach assumes consistent drawing layouts where views, title blocks, and notes can be reliably separated before semantic parsing.

## Confidence

**High Confidence Claims:**
- Three-stage hybrid architecture successfully processes engineering drawings into structured JSON output
- YOLOv11-obb orientation-aware detection provides measurable benefits for rotated technical annotations
- Schema-constrained fine-tuning significantly improves numerical data extraction compared to zero-shot approaches

**Medium Confidence Claims:**
- Framework's superior performance compared to alternative approaches (limited comparisons in provided text)
- Claim that hierarchical decomposition prevents "region overlap and degraded localization" - theoretically sound but not empirically validated against single-pass alternatives
- Assertion that modularity enables easier adaptation to new symbol types - plausible but no adaptation experiments presented

**Low Confidence Claims:**
- Framework's ability to handle drawings from diverse manufacturing domains beyond training data
- Claims about latency and computational efficiency in production environments (no benchmarking data provided)
- Long-term maintainability as drawing standards evolve

## Next Checks

1. **Cross-Domain Robustness Test:** Evaluate framework on engineering drawings from at least three different manufacturing industries (aerospace, automotive, consumer products) to quantify performance degradation and identify domain-specific failure modes.

2. **Single-Pass Comparison:** Implement and benchmark single-stage end-to-end model against three-stage pipeline on same dataset to empirically validate claimed benefits of hierarchical decomposition in preventing region overlap issues.

3. **Alphabetical VLM Enhancement:** Conduct ablation study comparing current zero-shot Alphabetical VLM against versions with schema-constrained fine-tuning and template-aware prompting to determine if hallucination rates can be reduced below 20% while maintaining reasonable recall.