---
ver: rpa2
title: 'Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation
  Control'
arxiv_id: '2501.03847'
source_url: https://arxiv.org/abs/2501.03847
tags:
- video
- control
- arxiv
- videos
- tracking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Diffusion as Shader (DaS), a novel 3D-aware
  video diffusion method that supports multiple video control tasks within a unified
  architecture. Unlike prior methods limited to 2D control signals, DaS leverages
  3D tracking videos as control inputs, making the video diffusion process inherently
  3D-aware.
---

# Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control

## Quick Facts
- arXiv ID: 2501.03847
- Source URL: https://arxiv.org/abs/2501.03847
- Reference count: 21
- Primary result: 3D tracking videos enable versatile video control tasks within a unified diffusion framework

## Executive Summary
Diffusion as Shader (DaS) introduces a novel 3D-aware video diffusion method that uses 3D tracking videos as control signals, enabling versatile video generation control tasks within a unified architecture. Unlike prior 2D control methods, DaS leverages 3D trajectories extracted from monocular video to provide superior temporal consistency and explicit geometric control. The method achieves strong performance across mesh-to-video generation, camera control, motion transfer, and object manipulation tasks with just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos.

## Method Summary
DaS processes both an input image and a 3D tracking video through a shared VAE encoder, then uses a ControlNet-style architecture where the first 18 blocks of a frozen denoising DiT are copied as a trainable condition branch. The 3D tracking video is encoded as dense RGB colors derived from normalized 3D coordinates (with reciprocal z), then injected into the frozen denoising DiT via zero-initialized linear layers at each corresponding block. This design enables the model to learn how to interpret 3D geometric control signals while preserving the base model's generative quality. The method supports multiple video control tasks by simply manipulating the 3D tracking video input, achieving strong performance with minimal training data.

## Key Results
- Achieves rotation errors of 5.97 degrees and translation errors of 27.85 degrees for small camera movements
- Motion transfer performance: text-video alignment score of 32.6 and temporal consistency score of 0.971
- Tracking videos outperform depth-only conditioning (FVD 551.3 vs 645.1)
- 3D point density of 4900 provides optimal balance between quality and computational cost

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 3D tracking videos provide superior temporal consistency compared to depth map sequences by establishing cross-frame correspondences.
- Mechanism: Each 3D point is assigned a color based on its normalized first-frame coordinates (x, y, 1/z). These colors remain fixed across all frames, creating persistent "color anchors" that bind the same 3D point to consistent appearance throughout the video, even when regions temporarily exit and re-enter the frame.
- Core assumption: The underlying 3D point trajectories accurately represent scene geometry and motion, and the diffusion model can learn to interpret color-encoded spatial coordinates.
- Evidence anchors:
  - [abstract] "A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos."
  - [Page 4, Section 3.3] "These colors remain the same for different timesteps t. Then, to get a specific t-th frame of the tracking video, we project these 3D points onto the t-th camera to render this frame."
  - [Page 10, Table 3] Ablation shows tracking videos (FVD 551.3) outperform depth-only conditioning (FVD 645.1).
  - [corpus] Related work TrajectoryCrafter (arXiv 2503.05638) similarly finds explicit trajectory representations improve control, supporting the general principle that structured motion signals outperform implicit embeddings.

### Mechanism 2
- Claim: Isolating control signal processing to a trainable "condition DiT" branch while freezing the base denoising model preserves generative quality while enabling efficient adaptation.
- Mechanism: The architecture creates a trainable copy of the first 18 DiT blocks (of 42 total). Zero-initialized linear layers project condition features into the frozen denoising DiT at each corresponding block. During training, gradients flow only through the condition branch, learning to modulate the frozen backbone without disrupting its learned priors.
- Core assumption: The pretrained video diffusion model has sufficient generative capacity, and the control signal provides complementary rather than redundant information.
- Evidence anchors:
  - [Page 4, Section 3.3] "We copy the first 18 blocks as the condition DiT. In the condition DiT, we extract the output feature of each DiT block, process it with a zero-initialized linear layer, and add the feature to the corresponding feature map of the denoising DiT."
  - [abstract] "With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities."
  - [corpus] Corpus evidence for ControlNet-style architectural patterns in video diffusion is limited; the cited related papers focus on camera embeddings rather than parallel conditioning branches.

### Mechanism 3
- Claim: Normalizing 3D coordinates to RGB color space creates a dense, differentiable spatial encoding that transformer attention can learn to decode.
- Mechanism: 3D point coordinates (x, y, z) are normalized to [0,1]³ and directly mapped to RGB values. The z-coordinate uses its reciprocal (1/z) to emphasize near-field detail. This transforms sparse 3D point clouds into dense video tensors compatible with standard VAE encoding, allowing the DiT to learn an implicit "rendering" function from coordinate-color mappings to realistic appearance.
- Core assumption: The coordinate-to-color mapping is learnable by the diffusion model, and the VAE latent space preserves sufficient spatial information for reconstruction.
- Evidence anchors:
  - [Page 4, Section 3.3] "The colors of these points are determined by their coordinates in the first frame, where we normalize the coordinates into [0, 1]³ and convert the coordinates into RGB colors {cᵢ}. Note we adopt the reciprocal of z-coordinate in the normalization."
  - [Page 6, Table 3] Increasing tracking point density from 900 to 4900 improves PSNR (18.52→19.27) and SSIM (0.586→0.658), suggesting the model benefits from denser spatial encoding.

## Foundational Learning

- Concept: Latent Video Diffusion with VAE
  - Why needed here: DaS operates on compressed latent representations (T/4 × H/8 × W/8 × 16) rather than pixel space, requiring understanding of encoder-decoder pipelines and how control signals propagate through latent bottlenecks.
  - Quick check question: Can you explain why the 3D tracking video is encoded through the same VAE as the target video, and what information might be lost in the 8× spatial compression?

- Concept: 3D Point Tracking and SpatialTracker
  - Why needed here: The method relies on extracting dense 3D trajectories from monocular video using SpatialTracker. Understanding the limitations of monocular depth estimation and multi-view triangulation is critical for diagnosing tracking failures.
  - Quick check question: Why might 3D tracking fail on textureless regions, and how would this manifest in DaS outputs?

- Concept: ControlNet-style Conditional Injection
  - Why needed here: The zero-initialized linear layers and parallel branch architecture directly borrow from ControlNet. Understanding why zero initialization prevents early-training disruption helps diagnose slow convergence.
  - Quick check question: If you observe that the first 1000 training steps produce outputs nearly identical to the base model, is this expected behavior or a bug?

## Architecture Onboarding

- Component map:
  Input image and 3D tracking video → VAE encoder → Condition DiT (18 blocks, trainable) → zero-initialized linear projections → Denoising DiT (42 blocks, frozen) → VAE decoder → Generated video

- Critical path:
  1. Construct 3D tracking video from task-specific source (depth + camera path, animated mesh, or SpatialTracker output)
  2. Encode both tracking video and zero-padded input image through shared VAE encoder
  3. Process tracking latent through condition DiT, inject features at each of first 18 blocks
  4. Run 50-step DDIM sampling with CFG scale 7.0
  5. Decode to pixel space

- Design tradeoffs:
  - **Condition DiT depth (18 vs 42 blocks)**: Shallower condition branch reduces training cost but may limit control precision. Paper does not ablate this choice.
  - **Tracking point density (4900 default)**: Higher density improves quality marginally but increases SpatialTracker runtime significantly (Table 3 shows diminishing returns above 4900).
  - **Frozen vs. full fine-tuning**: Freezing backbone preserves generative quality but may limit adaptation to out-of-distribution control signals.

- Failure signatures:
  - **Scene transitions**: When tracking video geometry contradicts input image, model generates a "cut" to a new compatible scene (Figure 11, top)
  - **Uncontrolled regions**: Areas without tracking coverage may generate hallucinated content (Figure 11, bottom)
  - **Temporal drift**: If 3D tracking is inaccurate, appearance consistency degrades despite the color-anchor mechanism

- First 3 experiments:
  1. **Validate base I2V quality**: Generate videos using all-zero (black) tracking videos to confirm frozen backbone produces reasonable outputs without control signal interference.
  2. **Camera control stress test**: Input a static image with a large spiral camera trajectory tracking video. Measure rotation/translation error using the paper's SIFT-based pose estimation protocol to verify your implementation matches reported 10.40° rotation error on large movements.
  3. **Ablate tracking density**: Run motion transfer on 5 validation videos with tracking point counts [900, 2500, 4900, 8100]. Plot FVD vs. density to verify the 4900-point sweet spot generalizes to your data distribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a generative model be developed to synthesize 3D tracking videos autonomously, removing the dependency on pre-existing animated meshes or source videos?
- Basis in paper: [explicit] Page 10 states, "we currently rely on provided animated meshes or existing videos to get high-quality 3D tracking videos and a promising direction is to learn to generate these 3D tracking videos with a new diffusion model."
- Why unresolved: The current DaS pipeline requires explicit external inputs (meshes or tracker outputs) to derive the 3D control signals; it cannot yet synthesize the control signal itself from abstract inputs like text.
- What evidence would resolve it: A new diffusion model capable of generating valid 3D tracking videos that successfully guide DaS without relying on source geometry.

### Open Question 2
- Question: How can the model be improved to handle structural incompatibility between the input image and the 3D tracking video?
- Basis in paper: [inferred] Page 10 notes a failure case where "the input image should be compatible with the 3D tracking videos. Otherwise, the generated videos would be implausible."
- Why unresolved: The model currently hallucinates a scene transition to a "compatible new scene" rather than resolving the geometric conflict between the static image and the dynamic tracking signal.
- What evidence would resolve it: Qualitative results showing stable video generation where the model adjusts the tracking or image content to maintain scene integrity without a transition.

### Open Question 3
- Question: How can temporal consistency be guaranteed in regions where 3D tracking points are sparse or absent?
- Basis in paper: [inferred] Page 10 identifies that "for regions without 3D tracking points, the tracking video fails to constrain these regions and DaS may generate some uncontrolled content."
- Why unresolved: The method relies on "color anchors" from tracking points for consistency; areas with low point density lack these explicit constraints, leading to artifacts.
- What evidence would resolve it: Improved temporal consistency metrics (e.g., CLIP Temp-Con) or qualitative examples in sparse regions achieved via densification or implicit propagation mechanisms.

## Limitations
- The method fundamentally depends on accurate 3D tracking, which fails on textureless regions, occlusions, and low-texture surfaces
- Structural incompatibility between input image and tracking video causes implausible scene transitions rather than reconciliation
- Color-encoding scheme assumes consistent visibility across frames; points exiting/re-entering may not reconstruct perfectly

## Confidence
- **High confidence**: Architectural innovation of using 3D tracking videos, training procedure with frozen backbone and trainable condition branch, camera control metrics (5.97° rotation, 27.85° translation)
- **Medium confidence**: Temporal consistency improvements and video quality metrics (FVD, PSNR, SSIM) depend heavily on 3D tracking quality
- **Low confidence**: Text-video alignment score of 32.6 for motion transfer lacks detailed methodology on CLIP-based computation

## Next Checks
1. **Geometric consistency validation**: Generate videos using tracking videos with artificially corrupted 3D point trajectories (e.g., random perturbations to 10% of points) and measure degradation in temporal consistency and appearance coherence to quantify the method's sensitivity to tracking accuracy.
2. **Out-of-distribution robustness**: Test DaS on videos with extreme camera motions (>90° rotation) or novel object types not present in the training mix (e.g., marine animals, complex machinery) to evaluate generalization limits and identify failure modes.
3. **Control signal ablation**: Compare DaS against variants using alternative control signals (depth maps only, optical flow fields, or simple camera embeddings) on the same validation sets to isolate the contribution of the 3D tracking video representation versus the conditioning architecture.