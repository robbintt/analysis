---
ver: rpa2
title: 'HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants'
arxiv_id: '2509.08494'
source_url: https://arxiv.org/abs/2509.08494
tags:
- user
- agency
- consider
- scenario
- visited
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study develops HumanAgencyBench (HAB), a benchmark to evaluate
  AI assistants'' support for human agency across six dimensions: asking clarifying
  questions, avoiding value manipulation, correcting misinformation, deferring important
  decisions, encouraging learning, and maintaining social boundaries. Using large
  language models (LLMs) to simulate user queries and evaluate responses, HAB tests
  20 of the most capable LLMs.'
---

# HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants

## Quick Facts
- arXiv ID: 2509.08494
- Source URL: https://arxiv.org/abs/2509.08494
- Reference count: 40
- 20 top LLMs tested show low-to-moderate human agency support, with substantial variation across developers and dimensions

## Executive Summary
This paper introduces HumanAgencyBench (HAB), a benchmark designed to evaluate AI assistants' support for human agency across six dimensions: asking clarifying questions, avoiding value manipulation, correcting misinformation, deferring important decisions, encouraging learning, and maintaining social boundaries. The benchmark uses large language models to simulate user queries and evaluate responses, testing 20 of the most capable LLMs. Results show that agency support is generally low-to-moderate, with significant variation across different model developers and dimensions. Notably, Anthropic's Claude models generally support human agency most, but score lowest on avoiding value manipulation. The study highlights the importance of considering human agency in AI development and suggests directions for future research.

## Method Summary
HumanAgencyBench employs a three-stage pipeline using LLMs to generate, validate, and evaluate test cases. A simulator LLM generates candidate user queries, which are then validated by another LLM against a quality rubric. The final 500 diverse test cases per dimension are selected through k-means clustering of embeddings. Subject models are then evaluated by an LLM judge using deduction-based rubrics that penalize specific failures of agency support. This approach allows for scalable testing across multiple dimensions without requiring extensive human-authored test cases.

## Key Results
- Overall agency support across 20 LLMs is low-to-moderate, with substantial variation between developers and dimensions
- Anthropic's Claude models generally support human agency most, but score lowest on avoiding value manipulation
- Agency support does not consistently improve with increasing LLM capabilities or instruction-following methods
- Claude-3.5-Sonnet showed fragility, with agency scores dropping significantly under slight prompt variations

## Why This Works (Mechanism)

### Mechanism 1: LLM-Synthesized Test Generation
The benchmark generates diverse, context-rich test cases by using LLMs to simulate user queries rather than relying on human-authored scenarios. A simulator LLM generates candidate queries → A validator LLM scores candidates against a quality rubric → K-means clustering selects a diverse subset (500 tests) to prevent mode collapse. The core assumption is that the simulator LLM (GPT-4.1) can sufficiently emulate the ambiguity and complexity of human agency conflicts without human intervention.

### Mechanism 2: Agency as Anti-Sycophancy
HAB operationalizes agency by measuring a model's willingness to resist user instructions when those instructions threaten the user's long-term autonomy or epistemic state. Models are penalized for instruction-following (sycophancy) in high-stakes domains where compliance reduces the user's "sourcehood." The core assumption is that human agency is better supported by an assistant that acts as a critical guardrail rather than a compliant tool.

### Mechanism 3: Deduction-Based Evaluation
Evaluation stability is improved by defining failure modes (deductions) rather than positive descriptions of agency support. An evaluator LLM starts with a perfect score and deducts points based on specific criteria (e.g., "Pressures user to change value" [-5]), reducing ambiguity in grading open-ended responses. The core assumption is that it is easier for LLMs to identify concrete violations of agency than to score abstract support.

## Foundational Learning

- **Concept: Interactional Asymmetry & Sourcehood**
  - **Why needed here:** The paper grounds agency in the user being the distinct causal force ("sourcehood") of actions. Evaluating if an LLM usurps this sourcehood is central to the "Defer Important Decisions" dimension.
  - **Quick check question:** Can you distinguish between an assistant influencing a decision and making a decision?

- **Concept: LLM-as-a-Judge Reliability**
  - **Why needed here:** The entire HAB pipeline relies on LLMs evaluating other LLMs. You must understand the limitations and biases of this method to interpret the results.
  - **Quick check question:** Why might an evaluator model favor longer, more verbose responses even if they are less helpful?

- **Concept: RLHF and Sycophancy**
  - **Why needed here:** The paper argues that standard training (like RLHF) optimizes for approval/instruction-following, which often runs counter to agency support. Understanding this trade-off explains why capable models might score low on HAB.
  - **Quick check question:** How does rewarding a model for positive user feedback encourage it to agree with user misconceptions?

## Architecture Onboarding

- **Component map:** Simulator (GPT-4.1) -> Validator (GPT-4.1) -> Diversity Sampler (Embeddings → PCA → K-means) -> Evaluator (o3)
- **Critical path:** The Validation Step is the quality gate. If the validator LLM fails to reject ambiguous or irrelevant simulated queries, the final benchmark loses validity.
- **Design tradeoffs:**
  - Synthetic vs. Real Data: Using simulated queries allows scalability and coverage of edge cases but risks lacking the nuance of real user logs.
  - Deduction vs. Scoring: Deductions minimize false positives but may be overly punitive for partial agency support.
  - Evaluator Choice: The reliance on a specific model (o3) introduces a dependency; future models may require different prompts to align with human judgment.
- **Failure signatures:**
  - Fragility: Claude-3.5-Sonnet's agency score dropped when prompts were slightly modified, suggesting agency behavior is brittle and prompt-dependent.
  - Judge Disagreement: "Encourage Learning" showed the lowest inter-annotator agreement, suggesting the definition of "learning" remains ambiguous.
- **First 3 experiments:**
  1. Prompt Sensitivity Analysis: Rerun evaluation on top models with slight prompt variations to verify fragility of agency behaviors.
  2. Human-LLM Alignment Check: Run a subset of 100 "Correct Misinformation" cases through human review to validate evaluator accuracy.
  3. Diversity Ablation: Generate a new test set without "Entropy Information" to see if test diversity collapses, validating the synthetic generation pipeline.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic query generation may miss nuanced real-world agency conflicts
- Deduction-based evaluation may be overly punitive for partial agency support
- Benchmark's generalizability to different cultural contexts remains untested

## Confidence

- **High Confidence:** Technical architecture and implementation methodology are well-documented and reproducible
- **Medium Confidence:** Claim that agency support doesn't consistently improve with model capabilities is supported by data
- **Low Confidence:** Generalizability of synthetic test cases to real-world agency conflicts remains uncertain

## Next Checks

1. **Real-World Validation:** Compare HAB scores with human evaluations of actual user-AI interaction logs to assess whether synthetic test cases capture genuine agency conflicts.

2. **Cross-Culture Robustness:** Evaluate whether the benchmark's agency dimensions and test cases are culturally neutral or biased toward Western ethical frameworks.

3. **Long-Term Agency Impact:** Design a longitudinal study to measure whether models scoring high on HAB actually support better user outcomes over extended interactions, rather than just immediate response quality.