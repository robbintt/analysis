---
ver: rpa2
title: Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians?
  Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative
  Autonomous Vehicles
arxiv_id: '2504.10873'
source_url: https://arxiv.org/abs/2504.10873
tags:
- gestures
- gesture
- traffic
- captions
- pedestrian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates vision-language models' ability to interpret
  dynamic pedestrian gestures for autonomous driving. Two custom datasets (ATG and
  ITGI) are created and annotated with natural language descriptions of body posture
  and gesture intent.
---

# Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians? Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative Autonomous Vehicles

## Quick Facts
- arXiv ID: 2504.10873
- Source URL: https://arxiv.org/abs/2504.10873
- Reference count: 40
- Primary result: Current vision-language models show limited ability to interpret dynamic pedestrian gestures for autonomous driving

## Executive Summary
This study evaluates vision-language models' (VLMs) capability to interpret dynamic pedestrian gestures for autonomous driving applications. The research introduces two custom datasets - ATG and ITGI - annotated with natural language descriptions of body posture and gesture intent. Three evaluation methods assess model performance: embedded similarity, classification, and pose reconstruction. Results demonstrate that existing VLMs struggle significantly with gesture understanding, achieving performance far below human expert levels. The study emphasizes the need for further research in this domain and provides public access to datasets and code for continued development.

## Method Summary
The study employs a multi-faceted evaluation approach to assess VLM performance on dynamic gesture interpretation. Researchers created two custom datasets - ATG (Act to Gesture) and ITGI (Image to Gesture Intent) - containing annotated video sequences of pedestrian gestures. Three complementary evaluation methods were implemented: embedded similarity measures the semantic alignment between predicted and ground truth descriptions, classification tasks evaluate intent recognition accuracy using F1 scores, and pose reconstruction assesses the models' ability to map textual descriptions back to accurate body configurations. The ATG dataset focuses on video sequences with temporal dynamics, while ITGI provides static images with corresponding gesture intent annotations.

## Key Results
- Average sentence similarity scores remain below 0.59, indicating poor semantic alignment
- Classification F1 scores achieve only 0.14-0.39, compared to 0.70 expert human baseline
- Pose reconstruction accuracy is unreliable, with models failing to consistently map text to accurate body configurations

## Why This Works (Mechanism)
The study's methodology works because it combines multiple complementary evaluation approaches that target different aspects of VLM performance. The embedded similarity metric captures semantic understanding, classification tasks measure intent recognition capability, and pose reconstruction evaluates the models' ability to translate between modalities. By using both video sequences (ATG) and static images (ITGI), the research tests VLMs across different temporal contexts. The human expert baseline provides a meaningful comparison point for assessing current VLM limitations in real-world applicability.

## Foundational Learning
- **Vision-Language Model Architecture**: Understanding how VLMs process and integrate visual and textual information is crucial for interpreting results. Quick check: Review VLM transformer architecture focusing on cross-attention mechanisms.
- **Gesture Recognition Fundamentals**: Knowledge of human gesture patterns and their variations helps contextualize model performance limitations. Quick check: Study common pedestrian gestures used in traffic communication.
- **Natural Language Processing Metrics**: Familiarity with similarity measures and classification metrics is needed to interpret evaluation results. Quick check: Review cosine similarity and F1 score calculations.
- **Computer Vision Pose Estimation**: Understanding body pose representation is essential for the reconstruction task. Quick check: Examine keypoint-based pose estimation techniques.
- **Dataset Annotation Standards**: Knowledge of annotation quality and consistency requirements for reliable evaluation. Quick check: Review best practices for gesture dataset annotation.
- **Autonomous Vehicle Communication Protocols**: Understanding nonverbal communication needs in traffic scenarios. Quick check: Study current standards for pedestrian-driver interaction.

## Architecture Onboarding

**Component Map**: Data Collection -> Annotation Pipeline -> VLM Models -> Evaluation Metrics -> Performance Analysis

**Critical Path**: ATG/ITGI Dataset Creation → Model Training/Inference → Multi-method Evaluation → Performance Comparison with Human Baseline

**Design Tradeoffs**: The study balances dataset size against annotation quality, choosing controlled environments over real-world complexity. Static images (ITGI) offer easier annotation but miss temporal dynamics compared to video sequences (ATG).

**Failure Signatures**: VLMs show consistent failure in capturing gesture intent, with similarity scores plateauing below 0.59 regardless of model architecture. Classification tasks reveal systematic confusion between related gesture categories, while reconstruction failures indicate fundamental limitations in cross-modal mapping.

**3 First Experiments**:
1. Test VLMs on a subset of ATG data with varying temporal resolutions to assess sensitivity to gesture timing.
2. Evaluate model performance on ITGI data with different background complexity levels to measure environmental robustness.
3. Compare VLM performance across different annotation styles (detailed vs. concise descriptions) to understand sensitivity to natural language variation.

## Open Questions the Paper Calls Out
The paper emphasizes that current VLMs lack the accuracy and robustness required for trustworthy autonomous driving applications. It calls for further research to improve gesture interpretation capabilities, particularly in real-world conditions with varying lighting, occlusion, and background complexity. The study also highlights the need for larger, more diverse datasets that capture cultural variations in pedestrian gestures and communication patterns.

## Limitations
- Performance metrics fall well below human expert levels, indicating current VLMs are not ready for deployment
- Controlled datasets may not fully represent the complexity and variability of real-world traffic scenarios
- The study does not address cultural diversity in gesture interpretation, which could impact model generalizability

## Confidence

**High confidence**: The evaluation methodology is sound, with multiple complementary assessment approaches (similarity, classification, reconstruction) providing consistent evidence of VLM limitations.

**Medium confidence**: The comparison between VLM performance and human expert baseline (F1=0.70) is meaningful, though the study does not detail the human evaluation methodology or sample size.

**Low confidence**: The generalizability of results to real-world autonomous driving scenarios, as the study uses controlled datasets rather than live traffic data.

## Next Checks
1. Test the same VLM models on live traffic camera footage to assess performance in real-world conditions with varying lighting, occlusion, and background complexity.
2. Conduct human-in-the-loop studies to evaluate whether VLM-assisted systems improve safety outcomes compared to baseline autonomous driving systems without gesture interpretation.
3. Expand the dataset to include diverse cultural contexts and pedestrian demographics to assess model performance across different populations and gesture conventions.