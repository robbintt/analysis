---
ver: rpa2
title: 'Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire
  Systems: Input-level Perturbation Strategies and Medical Constraint Validation'
arxiv_id: '2508.05677'
source_url: https://arxiv.org/abs/2508.05677
tags:
- attack
- adversarial
- attacks
- medical
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the vulnerability of reinforcement learning
  (RL)-based medical questionnaire systems to adversarial attacks. The author implemented
  six major white-box attack methods (FGSM, PGD, C&W, BIM, DeepFool, AutoAttack) with
  varying perturbation levels (epsilon values) on the AdaptiveFS framework, which
  uses the NHIS dataset to predict 4-year mortality.
---

# Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation

## Quick Facts
- **arXiv ID**: 2508.05677
- **Source URL**: https://arxiv.org/abs/2508.05677
- **Reference count**: 40
- **Key outcome**: Six white-box attack methods (FGSM, PGD, C&W, BIM, DeepFool, AutoAttack) successfully compromised RL-based medical questionnaire systems, with success rates ranging from 33.08% to 64.70% when perturbations satisfied 247 medical constraints ensuring clinical plausibility

## Executive Summary
This paper investigates the vulnerability of reinforcement learning-based medical questionnaire systems to adversarial attacks, focusing on input-level perturbations that maintain clinical plausibility. The author implements six major white-box attack methods on the AdaptiveFS framework, which uses the NHIS dataset to predict 4-year mortality through an RL-based adaptive questioning process. A comprehensive medical constraint framework with 247 rules ensures that adversarial examples remain clinically plausible while achieving significant attack success rates, demonstrating critical security risks for deploying such systems in clinical settings.

## Method Summary
The study evaluates adversarial attacks on an RL-based medical questionnaire system (AdaptiveFS) trained on NHIS dataset (2005-2011) for 4-year mortality prediction. The framework combines a DQN for question selection and a Guesser network for diagnosis. Six white-box attack methods are applied with varying perturbation levels (epsilon values), and a medical constraint framework ensures generated adversarial examples remain clinically plausible through physiological bounds, feature correlations, and conditional rules.

## Key Results
- Attack success rates ranged from 33.08% (FGSM) to 64.70% (AutoAttack) across 1,000 test samples
- Medical constraint framework achieved 97.6% success rate in generating clinically plausible adversarial examples
- AutoAttack demonstrated highest effectiveness (64.70% average ASR) but required significantly more computation time (47 seconds vs 0.055 seconds for FGSM)

## Why This Works (Mechanism)

### Mechanism 1: Gradient-Based Perturbation Exploitation
Gradient-based attacks exploit model gradients to craft perturbations maximizing classification loss within epsilon-bounded neighborhoods. The method computes gradients with respect to patient feature vectors, applies signed gradients scaled by epsilon, and projects onto medical constraint space. Iterative methods (PGD, BIM) repeat this with smaller steps, achieving higher success rates through gradual optimization. This assumes white-box access to model architecture, parameters, and gradients, with normalized feature vectors making uniform epsilon values meaningful across different medical features.

### Mechanism 2: Medical Constraint Satisfaction for Clinical Plausibility
The 247-rule constraint framework enables generation of adversarial examples satisfying physiological bounds, feature correlations, and conditional medical rules. Three-layer validation checks physiological bounds (e.g., age-adjusted BP limits), preserves feature correlations via Pearson/Cramér's V thresholds, and enforces conditional rules (e.g., diabetic ⇒ elevated glucose). A CSP solver iteratively projects violations back to feasible space, achieving 97.6% plausibility. This assumes medical knowledge can be encoded as hard constraints and violations are detectable and correctable through projection.

### Mechanism 3: Ensemble Attack Superiority via Method Diversity
AutoAttack achieves highest success rate by combining APGD-CE, APGD-DLR, FAB, and Square Attack, exploiting different vulnerability dimensions. The ensemble probes multiple loss landscapes and decision boundary geometries simultaneously, with APGD variants targeting different loss functions, FAB finding minimal norm perturbations, and Square Attack using random search. This diversity overcomes single-method limitations, assuming model vulnerabilities are method-specific and combining orthogonal attack strategies increases probability of finding successful perturbations.

## Foundational Learning

- **Markov Decision Process (MDP) Formulation**: The RL questionnaire system is defined as MDP (S, A, P, R, γ) where state = patient responses + question mask, actions = ask question or diagnose. Understanding this is prerequisite to grasping how perturbations affect sequential decision-making. Quick check: Given state sₜ = [xₜ, mₜ] ∈ ℝ²ᵈ, what does mₜ represent and how does it change after each action?

- **White-Box vs. Black-Box Threat Models**: The paper evaluates white-box attacks assuming full gradient access. Distinguishing threat models determines which attack vectors are realistic for clinical deployment scenarios. Quick check: Why might FGSM (white-box) be impractical in real hospital settings, and what corpus-evidenced alternative exists?

- **Constraint Satisfaction Problem (CSP) Solvers**: The medical constraint framework uses CSP to ensure adversarial examples remain clinically plausible. Algorithm 2's projection and consistency propagation steps require CSP fundamentals. Quick check: In Algorithm 2, what happens when the CSP solver makes "no progress"—what optimization fallback is triggered?

## Architecture Onboarding

- **Component map**: Patient Input (x, m) → DQN Network (128→128 ReLU) → Q-values (question selection) ↘ Guesser Network (256→256→128 PReLU) → Diagnosis (2-class softmax)
- **Critical path**: Input perturbation → Guesser misclassification (primary attack surface); DQN question selection (secondary—perturbations can alter questioning policy)
- **Design tradeoffs**: Attack effectiveness vs. computational cost (AutoAttack 64.70% ASR, 47s vs. FGSM 33.06% ASR, 0.055s); perturbation magnitude vs. clinical detectability (ε=2.0 allows strong attacks but risks detection; ε=0.1-0.3 more realistic for measurement noise); constraint strictness vs. attack success (247 rules filter implausible examples but 2.4% rejected or manually resolved)
- **Failure signatures**: Low ASR with high perturbation indicates model may be robust or constraints too restrictive; constraint violations spike suggests rule set incomplete for feature interactions; BIM normality violation indicates implementation inconsistency
- **First 3 experiments**: 1) Baseline FGSM attack (ε=0.3) on 100 correctly classified samples to establish minimum vulnerability threshold; verify medical constraint pass rate >95%. 2) PGD vs. AutoAttack comparison (ε=1.0) on same sample set; measure ASR difference and computational cost ratio to validate ensemble advantage claim. 3) Ablation on constraint categories: Disable conditional constraints only, run C&W attacks; quantify ASR increase to assess which constraint type contributes most to plausibility filtering

## Open Questions the Paper Calls Out

### Open Question 1
How robust are these adversarial attack strategies when applied to multi-task diagnosis systems utilizing real-world Electronic Health Records (EHR) rather than population health surveys? The study relies on the NHIS dataset (a population survey) and a single-task focus (mortality), which may not represent the complexity of clinical settings or multi-task systems. The experiments were computationally limited to the top-50 features of the NHIS dataset and a binary mortality prediction task.

### Open Question 2
Can these adversarial examples bypass real-time input sanitization and confidence-threshold detection mechanisms likely to be present in deployed clinical environments? While the study ensured medical plausibility via constraints, it "did not take other detection mechanisms that might be deployed in real-life clinical systems into account." The threat model assumed direct input access without active security layers opposing the attack.

### Open Question 3
How effectively do these white-box attacks transfer to black-box settings where attackers lack model gradients and architecture details? The study highlights the "White-box Assumption" as a key limitation, noting that model parameters would "hardly be publicly available in real-life settings." The research focused on white-box methods to establish worst-case vulnerability bounds rather than practical attack feasibility.

## Limitations
- The 247-rule medical constraint framework is incompletely specified, with only illustrative examples provided rather than complete definitions
- The study assumes white-box access to model parameters and gradients, which is unrealistic for deployed clinical systems
- Clinical realism of maximum ε=2.0 perturbations is questionable, as such large changes could be easily detected by clinicians

## Confidence
- **High confidence**: Gradient-based attack mechanisms (FGSM, PGD, BIM) exploiting model gradients—well-established in adversarial ML literature with clear mathematical formulation in the paper
- **Medium confidence**: Ensemble attack superiority claim—supported by ASR comparisons but dependent on specific implementation details not fully disclosed
- **Low confidence**: Medical constraint validation framework—247 rules claimed but only examples provided; 97.6% plausibility rate cannot be independently verified without complete rule set

## Next Checks
1. **Constraint Rule Completeness Audit**: Request or reconstruct the full 247-rule medical constraint set from the authors; test on edge cases (e.g., pregnant male, BMI=55, diabetic without glucose elevation) to identify gaps in physiological bounds and conditional logic
2. **Transferability Assessment**: Apply successful adversarial examples from white-box attacks to a separately trained AdaptiveFS model with different initialization; measure ASR drop to evaluate whether vulnerabilities are model-specific or fundamental to the RL questionnaire architecture
3. **Clinical Detection Simulation**: Have medical domain experts review ε=0.3-1.0 adversarial examples flagged as "clinically plausible"; quantify false negative rate where experts identify implausible combinations that constraints missed, establishing practical security boundaries