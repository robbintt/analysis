---
ver: rpa2
title: 'Auto prompting without training labels: An LLM cascade for product quality
  assessment in e-commerce catalogs'
arxiv_id: '2510.23941'
source_url: https://arxiv.org/abs/2510.23941
tags:
- prompt
- product
- cascade
- mixtral
- baseline
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an LLM cascade that automatically generates
  and refines prompts for e-commerce product quality assessment across tens of thousands
  of product-category-attribute pairs. Starting from a small set of human-crafted
  instructions, the cascade iteratively optimizes prompts by incorporating catalog-specific
  metadata and domain knowledge, eliminating the need for training labels or model
  fine-tuning.
---

# Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs

## Quick Facts
- **arXiv ID**: 2510.23941
- **Source URL**: https://arxiv.org/abs/2510.23941
- **Reference count**: 18
- **Primary result**: LLM cascade generates domain-specific prompts for e-commerce quality assessment, achieving 8-10% F1 improvements over CoT baselines without training labels.

## Executive Summary
This paper introduces an LLM cascade that automatically generates and refines prompts for e-commerce product quality assessment across tens of thousands of product-category-attribute pairs. Starting from a small set of human-crafted instructions, the cascade iteratively optimizes prompts by incorporating catalog-specific metadata and domain knowledge, eliminating the need for training labels or model fine-tuning. Evaluated across correctness and applicability tasks in five languages, the method achieves 8-10% improvements in precision and recall over chain-of-thought prompting, while reducing expert effort from 5.1 hours to 3 minutes per attribute—a 99% reduction. The cascade generalizes effectively across multiple LLMs and quality tasks, consistently maintaining performance gains without task-specific supervision.

## Method Summary
The method employs a two-iteration LLM cascade that transforms a small set of human-crafted instructions into thousands of domain-specific prompts. In iteration 1, the cascade generates intermediate instructions for sampled product categories using the seed instructions as few-shot examples. Iteration 2 uses these intermediate instructions as few-shot examples to produce final product-category-attribute (PC-SA) specific instructions. The cascade incorporates structured catalog metadata (PC/SA definitions, valid values) as explicit context during instruction generation. Generated instructions are then inserted into a structured prompt template with a dedicated "Instruction" slot, which is combined with product data and chain-of-thought rules for downstream classification tasks.

## Key Results
- Achieves 8-10% F1 improvements over chain-of-thought prompting across correctness and applicability tasks
- Reduces expert effort from 5.1 hours to 3 minutes per attribute (99% reduction)
- Maintains performance gains across five languages with 8-10% F1 improvements
- Optimal performance at T=2 iterations with M=6 few-shot examples
- Cross-task instruction transfer shows modest gains from correctness to applicability tasks

## Why This Works (Mechanism)

### Mechanism 1: Iterative Instruction Refinement Through Few-Shot Transfer
- Claim: A 2-iteration cascade can transform a small set of human-crafted instructions into thousands of domain-specific prompts without labeled data.
- Mechanism: Iteration 1 uses M=6 manually crafted instructions as few-shot examples with PC/SA definitions to generate intermediate instructions for sampled product categories. Iteration 2 uses these automatically-generated instructions as few-shot examples to produce final PC-SA-specific instructions, refining from generic definitions to contextualized ones.
- Core assumption: The LLM can transfer instruction-writing patterns from one PC-SA pair to another via few-shot learning, maintaining semantic coherence across iterations.
- Evidence anchors:
  - [abstract] "Starting from a seed of human-crafted prompts, the cascade progressively optimizes instructions to meet catalog-specific requirements."
  - [Section 3.4] Formalizes mapping π′(d(pa), d(sb), ip1sb, ..., ipKsb) = i′pasb, reducing manual effort from O(K×|S|) to O(M).
  - [Section 5] F1 scores peak at T=2 iterations across all tested LLMs; performance is robust to varying M.
  - [corpus] Neighbor papers on auto-prompting suggest iterative prompt optimization is a growing pattern.

### Mechanism 2: Metadata-Grounded Domain Knowledge Injection
- Claim: Structured catalog metadata enables zero-shot domain adaptation without fine-tuning.
- Mechanism: The cascade uses definition functions dP(p) and dS(s) as explicit context in the prompt template. The LLM integrates this metadata during instruction generation, producing instructions that encode catalog-specific constraints.
- Core assumption: Catalog metadata accurately captures domain semantics; LLMs can reliably synthesize this into actionable instructions.
- Evidence anchors:
  - [abstract] "bridges the gap between general language understanding and domain-specific knowledge at scale"
  - [Section 3.4] "incorporating detailed metadata, such as PC and SA definitions, valid-values etc."
  - [Table 4] Qualitative example shows instruction resolving "base material" ambiguity by referencing ground-contact semantics derived from metadata.

### Mechanism 3: Structured Prompt Architecture with Instruction Slot
- Claim: A fixed prompt template with a dedicated "Instruction" slot enables consistent reasoning structure while allowing PC-SA-specific adaptation.
- Mechanism: The template separates concerns: introduction (persona), product data, rules (CoT steps), test value, instruction (auto-generated), and output format. The instruction slot provides disambiguating context that shifts evidence retrieval and reasoning patterns.
- Core assumption: The structured template constrains reasoning variability sufficiently that instruction quality becomes the dominant performance factor.
- Evidence anchors:
  - [Section A.2-A.3] Figures 5-6 show template structure; Figure 6 adds the instruction slot to Figure 5's CoT template.
  - [Section 5] Qualitative results demonstrate reasoning shifts attributable to instruction injection.

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: The paper builds on CoT as a baseline and augments it with auto-generated instructions. Understanding CoT's strengths (reasoning steps) and limitations (lacks domain granularity) is essential to see what the cascade adds.
  - Quick check question: Can you explain why zero-shot CoT struggles with thousands of heterogeneous PC-SA pairs?

- Concept: **Few-Shot In-Context Learning**
  - Why needed here: The cascade uses M=6 few-shot examples per iteration to transfer instruction-writing patterns. Without grasping few-shot learning, the mechanism of iterative knowledge transfer is unclear.
  - Quick check question: How does the cascade reduce manual effort from O(K×|S|) to O(M) while preserving instruction quality?

- Concept: **Product-Category-Attribute (PC-SA) Taxonomy**
  - Why needed here: The method is structured around PC-SA pairs; understanding hierarchical product taxonomies and structured vs. unstructured attributes is prerequisite to interpreting the correctness and applicability tasks.
  - Quick check question: What are the two quality assessment tasks, and how do they differ in their classification objectives?

## Architecture Onboarding

- Component map: Seed Instructions -> Iteration 1 (generate intermediate instructions) -> Iteration 2 (generate final PC-SA instructions) -> Prompt Template Engine -> Quality Classification Module

- Critical path:
  1. **Seed Instruction Curation**: Manually create M=6 instructions per SA for sampled PCs (one-time effort, ~3 min/PC-SA).
  2. **Iteration 1**: For each SA, generate intermediate instructions for M sampled PCs using seed instructions as few-shot examples.
  3. **Iteration 2**: For each target PC-SA pair, generate final instructions using iteration-1 outputs as few-shot examples.
  4. **Offline Instruction Storage**: Cache generated instructions indexed by PC-SA for reuse.
  5. **Inference**: Retrieve instruction for PC-SA, assemble prompt, run classification LLM, return decision + rationale.

- Design tradeoffs:
  - **T vs. M**: Higher T (>2) risks overfitting; higher M (>6) shows diminishing returns. Paper settles on T=2, M=6 for robustness across LLMs.
  - **Instruction LLM vs. Classification LLM**: Using a stronger LLM (Claude 3.5 Sonnet) for instruction generation improves downstream performance even on smaller classification LLMs (Mixtral 8x7B), but adds cost.
  - **Task Generalization**: Instructions generated for correctness task transfer to applicability task with modest gains, suggesting some cross-task robustness.

- Failure signatures:
  1. **Iteration Overfit**: Performance degrades at T>2; instructions may become repetitive or overly specific to synthetic patterns.
  2. **Metadata Gaps**: If PC/SA definitions are missing or vague, generated instructions may be generic and ineffective.
  3. **Cross-Language Drift**: Multilingual results show smaller gains vs. English; instructions may not fully capture language-specific nuances.
  4. **Instruction-Rule Conflict**: If auto-generated instruction contradicts CoT rules, LLM may produce inconsistent reasoning.

- First 3 experiments:
  1. **Baseline Replication**: Implement vanilla and CoT prompting on a sample of 100 PC-SA pairs from your catalog; measure precision/recall/F1 to establish baselines.
  2. **Iteration Tuning on Held-Out Set**: Implement the cascade with T=1,2,3 and M=4,6,8 on a tuning dataset. Plot F1 vs. T and M to verify T=2, M=6 is optimal.
  3. **Cross-Task Transfer Test**: Generate instructions for correctness task and evaluate on applicability task. Measure F1 lift vs. CoT to validate cross-task generalization claim.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can intrinsic, quantitative metrics be developed to directly evaluate auto-generated instruction quality (clarity, consistency, edge-case handling) without relying on downstream task performance?
- Basis in paper: [explicit] Authors explicitly state: "This work also underscores the need for quantitative metrics to evaluate instruction quality directly, measuring clarity, consistency, and handling of edge cases."
- Why unresolved: Current evaluation is purely extrinsic (downstream F1 scores). No methodology exists to measure instruction quality independently of task outcomes.
- What evidence would resolve it: Development of validated metrics that correlate with downstream performance but can be computed directly on generated instructions.

### Open Question 2
- Question: Does the auto-prompt cascade generalize effectively to other e-commerce tasks such as attribute value generation and product categorization?
- Basis in paper: [explicit] Authors state: "the framework's effectiveness on other e-commerce tasks, such as attribute value generation or product categorization, remains an open question. Extending and validating the auto prompt cascade for these different tasks is a key direction for future work."
- Why unresolved: The study only evaluated correctness and applicability tasks. Generation and categorization require different output structures and reasoning patterns.
- What evidence would resolve it: Empirical evaluation of cascade-generated prompts on attribute generation and categorization benchmarks with comparable performance gains.

### Open Question 3
- Question: How robust is the cascade to variation in the number and quality of initial human-crafted seed instructions?
- Basis in paper: [inferred] The method relies on only M=6 seed instructions, but experiments only used high-quality expert-crafted seeds. The sensitivity to seed quantity or lower-quality seeds is unexplored.
- Why unresolved: Authors note seed instructions require "domain experts" but don't analyze performance degradation with fewer or noisier seeds.
- What evidence would resolve it: Ablation study varying M (1-20) and injecting synthetic noise into seed instructions, measuring downstream F1 changes.

### Open Question 4
- Question: Why does performance plateau at T=2 iterations, and does this generalize across different catalog sizes or domain complexity?
- Basis in paper: [inferred] Figure 3 shows F1 peaks at T=2 across all models, but the paper provides only a qualitative explanation. The convergence behavior for larger or more diverse catalogs is unknown.
- Why unresolved: The hyperparameter tuning used fixed datasets; whether T=2 is universally optimal or dataset-dependent remains unclear.
- What evidence would resolve it: Systematic study of iteration convergence across catalogs with varying PC-SA complexity and size distributions.

## Limitations

- **Metadata Dependency**: The method's effectiveness heavily depends on the quality and completeness of catalog metadata (PC/SA definitions), which may be incomplete or ambiguous in real-world scenarios.
- **Cross-Language Performance**: Multilingual results show smaller performance gains (8-10% vs. potentially higher in English), suggesting limitations in cross-language generalization and cultural/linguistic adaptation.
- **Iteration Overfitting**: Performance degrades at T>2 iterations, indicating a potential overfitting risk where the cascade may generate repetitive or overly specific instructions that don't generalize well.

## Confidence

- **High Confidence**: The core mechanism of iterative instruction refinement and the 8-10% F1 improvement over CoT baselines are well-supported by the empirical results and consistent across multiple LLMs.
- **Medium Confidence**: The scalability claim (99% reduction in expert effort) is based on controlled experiments but may vary with catalog complexity and instruction quality requirements.
- **Low Confidence**: The cross-language generalization claims are supported by limited multilingual data (5 languages) with smaller effect sizes, suggesting potential cultural or linguistic limitations not fully explored.

## Next Checks

1. **Metadata Quality Impact Test**: Systematically vary the completeness and specificity of PC/SA definitions to measure the sensitivity of generated instruction quality and downstream classification performance.

2. **Iteration Sensitivity Analysis**: Implement the cascade with T=1,2,3 and M=4,6,8 on a held-out dataset to verify the claimed optimal parameters and identify overfitting thresholds for different catalog domains.

3. **Cross-Catalog Generalization**: Apply the cascade to a different e-commerce domain (e.g., fashion vs. electronics) with distinct attribute structures to test the robustness of instruction transferability across product categories.