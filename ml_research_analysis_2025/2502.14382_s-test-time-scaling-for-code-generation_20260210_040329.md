---
ver: rpa2
title: 'S*: Test Time Scaling for Code Generation'
arxiv_id: '2502.14382'
source_url: https://arxiv.org/abs/2502.14382
tags:
- arxiv
- code
- scaling
- performance
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces S, the first hybrid test-time scaling framework
  for code generation that substantially improves both coverage and selection accuracy.
  S extends the existing parallel scaling paradigm with sequential scaling through
  iterative debugging and incorporates adaptive input synthesis, a novel mechanism
  that synthesizes distinguishing test inputs to differentiate candidates and identify
  correct solutions via execution results.
---

# S*: Test Time Scaling for Code Generation

## Quick Facts
- arXiv ID: 2502.14382
- Source URL: https://arxiv.org/abs/2502.14382
- Reference count: 21
- Key outcome: S* is the first hybrid test-time scaling framework for code generation that substantially improves both coverage and selection accuracy through parallel and sequential scaling, enabling smaller models to outperform larger ones.

## Executive Summary
This paper introduces S*, a hybrid test-time scaling framework for code generation that combines parallel candidate generation with sequential iterative debugging. The key innovation is adaptive input synthesis, which generates distinguishing test inputs to differentiate between candidate solutions and identify correct ones through execution. S* demonstrates consistent improvements across benchmarks, enabling a 3B model to outperform GPT-4o mini and achieving 86.7% on LiveCodeBench with DeepSeek-R1-Distill-Qwen-32B, approaching o1-high performance.

## Method Summary
S* extends existing parallel test-time scaling paradigms by incorporating sequential scaling through iterative debugging. The framework generates multiple candidate solutions in parallel, then uses adaptive input synthesis to create test cases that can distinguish between correct and incorrect solutions. This adaptive mechanism synthesizes inputs that expose differences in candidate outputs, allowing the system to identify correct solutions through execution results. The hybrid approach combines the coverage benefits of parallel scaling with the accuracy improvements of sequential refinement.

## Key Results
- 3B model outperforms GPT-4o mini using S* framework
- GPT-4o mini surpasses o1-preview by 3.7% on LiveCodeBench
- DeepSeek-R1-Distill-Qwen-32B achieves 86.7% on LiveCodeBench, approaching o1-high at 88.5%
- Consistent improvements across both LiveCodeBench and CodeContests benchmarks

## Why This Works (Mechanism)
S* works by leveraging the complementary strengths of parallel and sequential scaling. Parallel scaling generates diverse candidate solutions to maximize coverage, while sequential scaling through iterative debugging refines and validates these candidates. The adaptive input synthesis mechanism is crucial - it creates test inputs specifically designed to differentiate between candidate solutions by exposing their differences in behavior. This allows the system to identify correct solutions even when they produce similar outputs on standard test cases.

## Foundational Learning
- Test-time scaling concepts: The framework extends beyond training-time improvements to leverage computational resources during inference, enabling smaller models to achieve competitive performance through clever use of additional compute.
- Adaptive input synthesis: A novel mechanism that generates distinguishing test inputs on-the-fly to differentiate between candidate solutions, moving beyond static test suites.
- Parallel vs sequential scaling: Understanding when to generate multiple candidates versus iteratively refining a single solution, with S* combining both approaches for optimal results.

## Architecture Onboarding

**Component Map**: Problem Description -> Candidate Generation -> Adaptive Input Synthesis -> Execution & Validation -> Final Selection

**Critical Path**: The most time-critical path is Adaptive Input Synthesis -> Execution & Validation, as each iteration requires running multiple candidate solutions against synthesized test cases to determine correctness.

**Design Tradeoffs**: The framework trades increased computational overhead during inference (generating multiple candidates and running iterative debugging) for improved accuracy and the ability to use smaller, more efficient models. This contrasts with approaches that rely solely on larger models or static test suites.

**Failure Signatures**: Performance degradation occurs when problems lack clear distinguishing test cases, when candidates produce similar outputs across all synthesized inputs, or when the iterative debugging loop fails to converge to a correct solution within computational limits.

**3 First Experiments**:
1. Compare S* performance with only parallel scaling versus only sequential scaling to quantify the hybrid benefit
2. Test adaptive input synthesis on problems with known ambiguous outputs to evaluate its discrimination capability
3. Measure the number of iterations required for different problem complexities to understand scaling behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation primarily on LiveCodeBench and CodeContests benchmarks, limiting generalizability
- Computational overhead of sequential scaling not thoroughly characterized
- No ablation studies isolating contributions of individual components
- Limited discussion of performance on domain-specific or complex code generation tasks

## Confidence
- High: Overall effectiveness of S* in improving code generation performance
- Medium: Specific performance numbers given limited benchmark scope
- Medium: Adaptive input synthesis contribution due to lack of detailed ablation studies

## Next Checks
1. Conduct ablation studies isolating the contribution of adaptive input synthesis versus parallel candidate generation to quantify each component's impact
2. Evaluate S* on additional code generation benchmarks (e.g., HumanEval, MBPP) to assess generalizability beyond LiveCodeBench and CodeContests
3. Measure and report wall-clock time and computational overhead for the sequential scaling component to enable practical deployment assessment