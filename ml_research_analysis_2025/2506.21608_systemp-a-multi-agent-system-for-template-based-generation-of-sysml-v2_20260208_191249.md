---
ver: rpa2
title: 'SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2'
arxiv_id: '2506.21608'
source_url: https://arxiv.org/abs/2506.21608
tags:
- sysml
- system
- generation
- language
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SysTemp, a multi-agent system designed to
  generate SysML v2 models from natural language specifications. The system uses a
  template-based approach where a TemplateGeneratorAgent creates a syntactically correct
  skeleton, and a WriterAgent iteratively completes it while a ParserAgent checks
  for syntax errors.
---

# SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2

## Quick Facts
- arXiv ID: 2506.21608
- Source URL: https://arxiv.org/abs/2506.21608
- Authors: Yasmine Bouamra; Bruno Yun; Alexandre Poisson; Frédéric Armetta
- Reference count: 35
- The paper introduces SysTemp, a multi-agent system designed to generate SysML v2 models from natural language specifications using a template-based approach that significantly reduces syntax errors.

## Executive Summary
SysTemp is a multi-agent system that generates SysML v2 models from natural language specifications through a template-based approach. The system uses four specialized agents: SpecificationGeneratorAgent extracts requirements into a structured dictionary, TemplateGeneratorAgent creates a syntactically correct skeleton using Jinja2 templates, WriterAgent completes the skeleton with content, and ParserAgent validates syntax and provides feedback. An ablation study demonstrates that using the template generator increases error-free convergence from 20% to 80% across five bicycle-related scenarios tested with GPT-4 Turbo and Claude 3.5 Sonnet.

## Method Summary
SysTemp employs a four-agent pipeline where SpecificationGeneratorAgent first extracts structured requirements from natural language using few-shot examples. The TemplateGeneratorAgent then applies expert-defined Jinja2 templates to create a syntactically valid SysML v2 skeleton. WriterAgent completes this skeleton with content, while ParserAgent validates the output using a Java-based OMG-compliant parser. WriterAgent and ParserAgent iterate in a feedback loop until the model is error-free or a maximum iteration limit is reached. The system was evaluated on five bicycle scenarios, with an ablation study comparing performance with and without the template generator.

## Key Results
- Template-based generation increased error-free convergence from 20% to 80% across five bicycle scenarios
- GPT-4 Turbo and Claude 3.5 Sonnet showed similar performance improvements with the template
- Iterative parser-feedback loops reduced syntax errors across iterations in both template and non-template conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Template-based skeleton generation substantially reduces syntax errors in SysML v2 output.
- Mechanism: The TemplateGeneratorAgent applies expert-defined rules via Jinja2 templates to produce a syntactically valid skeleton, constraining the LLM's output space to structurally correct patterns before content completion.
- Core assumption: SysML v2 syntax errors primarily stem from structural malformation rather than content-level mistakes.
- Evidence anchors:
  - [abstract]: "80% of scenarios converging to error-free models versus 20% without the template"
  - [section 5, Figure 6]: Ablation study shows near-systematic convergence with template, only 1/5 without
- Break condition: If syntax errors are primarily semantic or context-dependent rather than structural, template scaffolding may not address the root cause.

### Mechanism 2
- Claim: Iterative parser-feedback loops enable progressive error correction beyond single-pass generation.
- Mechanism: The ParserAgent uses a Java-based OMG-compliant parser to detect syntax errors and return structured error reports, which the WriterAgent incorporates to revise the model through cycling.
- Core assumption: The parser provides actionable, locatable error descriptions; the LLM can correctly interpret and apply corrections.
- Evidence anchors:
  - [section 3.1.4]: ParserAgent "identifies potential syntax or structural errors and returns a detailed report E specifying the errors and their corresponding locations"
  - [section 5]: Both GPT-4 and Claude 3.5 show error reduction across iterations
- Break condition: If parser error messages are ambiguous or if LLM error-correction capability plateaus before full convergence.

### Mechanism 3
- Claim: Structured intermediate representation (Python dictionary) reduces information loss between natural language and formal model.
- Mechanism: The SpecificationGeneratorAgent uses few-shot prompting (k=3 examples) to extract requirements into a typed dictionary with keys for packages, attributes, constraints, and requirements.
- Core assumption: The natural language input contains extractable, well-defined requirements; few-shot examples adequately cover the input distribution.
- Evidence anchors:
  - [section 3.1.1]: Formal definition shows Dictq = f(concat(PRGA, Ex, DqNL))
  - [appendix C]: Five scenarios demonstrate structured extraction from varied bicycle specifications
- Break condition: If requirements are ambiguous, contradictory, or outside the few-shot example distribution.

## Foundational Learning

- Concept: SysML v2 concrete syntax and OMG grammar
  - Why needed here: The entire pipeline targets SysML v2 textual notation; understanding package structure, requirement definitions, and attribute syntax is essential to interpret agent outputs and debug failures.
  - Quick check question: Can you identify syntax errors in a sample SysML v2 requirement block (e.g., missing semicolon, misplaced brace)?

- Concept: Multi-agent orchestration and role specialization
  - Why needed here: SysTemp relies on distinct agent roles (extraction, templating, writing, parsing) with handoffs; understanding coordination patterns is necessary to modify or extend the pipeline.
  - Quick check question: What is the data flow between SpecificationGeneratorAgent and TemplateGeneratorAgent, and what format must the handoff take?

- Concept: Template-based code generation with Jinja2
  - Why needed here: The TemplateGeneratorAgent uses Jinja2 templates with expert-defined rules; modifying or creating new templates requires understanding placeholder substitution and control flow.
  - Quick check question: Given a new requirement type (e.g., "actions"), how would you extend the templating rules in Appendix B?

## Architecture Onboarding

- Component map: SpecificationGeneratorAgent -> TemplateGeneratorAgent -> WriterAgent -> ParserAgent -> WriterAgent (iterative loop)
- Critical path:
  1. User provides natural language description
  2. SpecificationGeneratorAgent extracts structured dictionary
  3. TemplateGeneratorAgent produces skeleton via template rules
  4. WriterAgent completes skeleton with content
  5. ParserAgent validates syntax; if errors, return to WriterAgent
  6. Iterate until error-free or max iterations (5 in evaluation)

- Design tradeoffs:
  - Template rigidity vs. flexibility: Templates improve syntax correctness but constrain expressiveness; may struggle with novel requirement types not covered in rules
  - LLM choice: Paper tested only GPT-4 Turbo and Claude 3.5 Sonnet; open-source models significantly underperform (per preliminary experiments)
  - Few-shot example count: k=3 used; fewer may reduce extraction quality, more increases prompt length

- Failure signatures:
  - Non-convergence: Error count stabilizes above zero after 5 iterations (observed in 1/5 template cases, 4/5 non-template)
  - Semantic drift: Syntactically correct model but values/constraints misaligned with user intent (paper explicitly notes this as unresolved)
  - Extraction errors: Missing or malformed dictionary fields cause template tool failures

- First 3 experiments:
  1. Replicate ablation study: Run same 5 bicycle scenarios with/without TemplateGeneratorAgent using GPT-4 Turbo; verify 80% vs. 20% convergence rates.
  2. Error taxonomy analysis: For non-converging cases, classify error types (structural vs. content) to validate template mechanism assumptions.
  3. Template extension test: Add a new requirement type (e.g., "interfaces") to templating rules and evaluate whether new scenarios converge without prompt changes.

## Open Questions the Paper Calls Out

- Question: How can the semantic correctness of generated model values be validated beyond syntactic compliance?
  - Basis in paper: [explicit] The authors state that "the semantics of the generated models require further investigation to ensure the correct interpretation of concepts," as the current ParserAgent only identifies syntax errors.
  - Why unresolved: The current system lacks a mechanism to verify if the generated attributes and constraints make logical sense (e.g., physical feasibility) or align with the user's deeper intent.
  - Evidence to resolve it: Integration of an ontology-based validation agent or a formal verification step that flags semantically inconsistent values (e.g., impossible weights) would address this.

- Question: What standardized benchmarks are needed to effectively evaluate the quality of SysML v2 code generation?
  - Basis in paper: [explicit] The authors note that assessing quality is constrained by the "absence of standard benchmarks," making it difficult to compare results or measure progress reliably.
  - Why unresolved: Without a diverse, standardized corpus of SysML v2 models and evaluation metrics, performance remains subjective and difficult to quantify across different approaches.
  - Evidence to resolve it: The development and adoption of a public benchmark dataset containing varied natural language specifications and their corresponding "gold standard" SysML v2 models.

- Question: Does the SysTemp architecture generalize to complex, heterogeneous engineering domains outside of bicycle components?
  - Basis in paper: [inferred] The evaluation was strictly limited to "five use cases" involving bicycles (frames, tires, forks), suggesting potential overfitting or limited proof of robustness for broader systems engineering applications.
  - Why unresolved: It is unclear if the template-based approach can handle the scale, complexity, and different syntax patterns required for industries like aerospace or automotive without extensive manual re-engineering of rules.
  - Evidence to resolve it: Successful generation of error-free models for dissimilar complex systems (e.g., an avionics system or a manufacturing plant) using the identical pipeline presented.

## Limitations

- The evaluation is constrained to five bicycle scenarios, limiting generalizability to complex industrial systems.
- The study does not address semantic correctness—models may be syntactically valid but semantically misaligned with user intent.
- Template rules are manually crafted and may not scale to diverse requirement types without extensive re-engineering.

## Confidence

- High confidence in the template mechanism's effectiveness: The ablation study provides clear quantitative evidence (80% vs 20% convergence) across two different LLMs.
- Medium confidence in iterative refinement benefits: While the study shows error reduction across iterations, the mechanism depends heavily on parser error quality and LLM correction capability, which vary with implementation.
- Medium confidence in the overall pipeline architecture: The four-agent design is novel, but performance on open-source models was explicitly excluded, limiting accessibility and reproducibility.

## Next Checks

1. Conduct a semantic correctness study: Evaluate whether SysTemp-generated models preserve user intent by having domain experts validate model semantics against original requirements.
2. Test template scalability: Apply the system to five non-bicycle scenarios with different requirement types (e.g., aerospace, automotive) to assess template rule coverage and necessary extensions.
3. Open-source model comparison: Replicate the ablation study using GPT-3.5 Turbo and Llama 3.1 to quantify the performance gap and identify potential improvements for smaller models.