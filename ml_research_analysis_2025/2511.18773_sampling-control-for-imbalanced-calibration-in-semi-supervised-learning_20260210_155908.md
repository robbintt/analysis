---
ver: rpa2
title: Sampling Control for Imbalanced Calibration in Semi-Supervised Learning
arxiv_id: '2511.18773'
source_url: https://arxiv.org/abs/2511.18773
tags:
- data
- classes
- sampling
- learning
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles class imbalance in semi-supervised learning
  by proposing a decoupled sampling control framework (SC-SSL) that addresses both
  feature-level and logits-level bias through an expansive classifier and optimization
  bias correction. The core idea is to adaptively adjust sampling probabilities during
  training to mitigate imbalance effects and use bias terms from the linear classifier
  to calibrate logits during inference.
---

# Sampling Control for Imbalanced Calibration in Semi-Supervised Learning

## Quick Facts
- arXiv ID: 2511.18773
- Source URL: https://arxiv.org/abs/2511.18773
- Authors: Senmao Tian; Xiang Wei; Shunli Zhang
- Reference count: 40
- This paper tackles class imbalance in semi-supervised learning by proposing a decoupled sampling control framework (SC-SSL) that addresses both feature-level and logits-level bias through an expansive classifier and optimization bias correction.

## Executive Summary
This paper addresses class imbalance in semi-supervised learning by proposing a decoupled sampling control framework (SC-SSL) that tackles both feature-level and logits-level bias. The method introduces an expansive classifier with adaptive sampling probabilities and uses an optimization bias vector for post-hoc logit calibration. Extensive experiments across multiple benchmarks show consistent state-of-the-art performance with accuracy improvements up to 3.1% over prior methods under various distribution settings.

## Method Summary
SC-SSL implements a decoupled sampling control framework that addresses class imbalance in semi-supervised learning through three key mechanisms: an expansive classifier for controlled feature-level sampling, three-factor decoupled sampling control, and optimization bias vector calibration. The method uses three classifiers sharing a backbone: base (random sampling), output (balanced sampling), and expansive (skewed toward tail classes). During training, sampling probabilities are adaptively adjusted based on data distribution, logit adjustment, and confidence thresholds. At inference, logits are calibrated by subtracting the optimization bias vector extracted from the balanced classifier.

## Key Results
- Consistent state-of-the-art performance across CIFAR10/100-LT, STL10-LT, and ImageNet-127 benchmarks
- Accuracy improvements up to 3.1% over prior methods under various distribution settings and label-unlabeled ratios
- Superior balance in learning representations for minority classes demonstrated through extensive ablation studies

## Why This Works (Mechanism)

### Mechanism 1: Expansive Classifier for Controlled Feature-Level Sampling
The expansive classifier ($F_e$) shares the backbone but operates with distinct sampling thresholds ($\rho_e$) initialized lower for non-head classes. This increases the probability of sampling pseudo-labels for minority classes during consistency regularization, even when noisy. The expansion-separation assumption provides theoretical grounding: under sufficient feature expansion ($c > 3$) and low perturbation sensitivity, even noisy pseudo-labels can improve feature learning for non-head classes.

### Mechanism 2: Decoupled Sampling via Three Control Factors ($\gamma_u$, $\Delta p$, $\rho$)
The three factors allow: (1) $\gamma_u$ to characterize imbalance magnitude; (2) $\Delta p$ (logit adjustment via balanced softmax) to counteract classifier bias from labeled data; (3) $\rho$ to dynamically filter pseudo-labels by confidence. Theorem 0.1 analytically derives pseudo-label sampling probability under binary classification with Gaussian class-conditionals.

### Mechanism 3: Optimization Bias Vector for Post-Hoc Logit Calibration
Three classifiers are trained: base (random sampling), expansive (skewed toward tail classes), output (balanced sampling). The output classifier's bias term ($b_{opt}$) excludes data imbalance effects, isolating optimization dynamics. At inference: $\tilde{F}(B(x)) = F_b(B(x)) - b_{opt}$. Additionally, $b_{opt}$ serves as a prior to estimate unlabeled data distribution via KL divergence matching.

## Foundational Learning

- **Concept: Semi-Supervised Learning with Pseudo-Labels and Consistency Regularization**
  - Why needed here: SC-SSL builds on FixMatch, which uses weak augmentation for pseudo-label generation and strong augmentation for consistency loss. Understanding this dual-path structure is essential.
  - Quick check question: Can you explain why FixMatch uses different augmentations for pseudo-label generation vs. consistency training?

- **Concept: Long-Tailed Distributions and Imbalance Ratios**
  - Why needed here: The paper assumes labeled data follows a long-tailed distribution ($N_1 > N_2 > ... > N_K$) with imbalance ratio $\gamma_l = N_1/N_K$. Non-head (tail) classes have fewer samples.
  - Quick check question: Given $\gamma_l = 100$ and 10 classes with exponential decay, how many samples are in class 10 if class 1 has 5000?

- **Concept: Expansion-Separation Assumption for Self-Training**
  - Why needed here: The theoretical justification for using noisy pseudo-labels rests on this assumption. Understanding $(a, c)$-expansion and its connection to generalization bounds is critical.
  - Quick check question: What does the expansion factor $c > 3$ imply about pseudo-label noise tolerance?

## Architecture Onboarding

- **Component map**: Backbone $B(\cdot)$ -> Base classifier $F_b$ (random sampling) -> Output classifier $F_b$ (balanced sampling) -> Expansive classifier $F_e$ (skewed sampling)

- **Critical path**: 
  1. Estimate unlabeled distribution via anchor matching (KL divergence) â†’ determine expansion factor $c$
  2. Initialize class-specific thresholds $\rho_b$ and $\rho_e$ based on $c$ (Eq. 9-10)
  3. Training loop: compute supervised losses for both classifiers, generate pseudo-labels via $F_e$, apply dynamic threshold updates (Eq. 8)
  4. Extract $b_{opt}$ from output classifier after training
  5. Inference: forward pass through $F_b$, subtract $b_{opt}$

- **Design tradeoffs**:
  - Head/Non-head binary split (vs. finer partitions): Simpler but may miss intermediate class dynamics
  - Fixed expansion factors per distribution type (vs. learned): Reduces complexity but assumes known distribution families
  - Bias-only calibration (vs. weight calibration): Cleaner decomposition but ignores weight-feature interactions

- **Failure signatures**:
  - Non-head class accuracy plateaus: $\rho$ thresholds may be too high; check $b_{opt}$ values
  - Chaotic unlabeled distribution: Default to uniform ($c=5$) rather than anchor matching
  - Training instability: Reduce $\alpha$ (threshold update step size) or increase $\nu$ (bias threshold)

- **First 3 experiments**:
  1. Reproduce CIFAR10-LT with consistent distribution ($\gamma_l=\gamma_u=100$, N=1500, M=3000) to validate baseline performance
  2. Ablate each control factor ($\gamma_u$, $\Delta p$, $\rho$) independently following Table 6 setup to confirm sensitivity
  3. Visualize bias term distribution across classes under random vs. balanced vs. expansive sampling to verify optimization bias decomposition

## Open Questions the Paper Calls Out

### Open Question 1
How robust is the sampling control mechanism when class-conditional feature distributions deviate from the Gaussian assumption used in Theorem 0.1? The paper validates final performance empirically but does not analyze theoretical error bounds when normality assumption is violated.

### Open Question 2
Can the expansion factor $c$ be estimated dynamically during training rather than being mapped to fixed anchor distributions? The paper states $c$ is dataset-independent, limiting adaptability to novel or "chaotic" data distributions.

### Open Question 3
Is the linear classifier's bias term ($b_{opt}$) a sufficient proxy for optimization bias when the expansive classifier fails to fully balance feature representations? The inference calibration assumes feature-level imbalance is mitigated, potentially overlooking interactions between weight updates and bias adjustments.

## Limitations
- The sampling control framework relies on Gaussian class-conditional assumptions that may not hold for complex, multi-modal data distributions
- The expansion-separation assumption (c > 3) is theoretically grounded but lacks extensive empirical validation across diverse SSL settings
- The optimization bias vector calibration assumes clean decomposition of bias terms, potentially overlooking interactions between weight updates and bias adjustments

## Confidence
- **High Confidence**: Performance improvements on benchmark datasets are well-supported by experimental results showing consistent state-of-the-art performance
- **Medium Confidence**: The three-factor sampling control mechanism is theoretically justified but relies on assumptions that may not generalize to all SSL scenarios
- **Medium Confidence**: The optimization bias vector approach is innovative but assumes the bias term cleanly isolates optimization effects from data imbalance

## Next Checks
1. **Robustness to Non-Gaussian Distributions**: Test SC-SSL performance on multi-modal unlabeled distributions to validate the sampling control mechanism beyond theoretical assumptions
2. **Bias Decomposition Validation**: Conduct ablation studies comparing bias-only calibration vs. combined weight-bias calibration to quantify the impact of ignoring weight-feature interactions
3. **Real-World Application Testing**: Evaluate SC-SSL on imbalanced SSL problems in domain-specific applications where distribution assumptions may differ significantly from benchmark datasets