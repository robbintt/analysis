---
ver: rpa2
title: 'SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size'
arxiv_id: '2510.03275'
source_url: https://arxiv.org/abs/2510.03275
tags:
- quantization
- weight
- matrix
- weights
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SDQ-LLM introduces a novel Sigma-Delta quantization framework for
  1-bit LLM compression that provides continuous precision control through adjustable
  Over-Sampling Ratio (OSR). The method combines upsampling with Sigma-Delta quantizers
  to binarize or ternarize weights, Hadamard matrix smoothing to reduce quantization
  errors, and MultiOSR layer- and linear-wise allocation based on weight variance.
---

# SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size

## Quick Facts
- arXiv ID: 2510.03275
- Source URL: https://arxiv.org/abs/2510.03275
- Authors: Junhao Xia; Ming Zhao; Limin Xiao; Xiujun Zhang
- Reference count: 29
- Key outcome: SDQ-LLM introduces a novel Sigma-Delta quantization framework for 1-bit LLM compression that provides continuous precision control through adjustable Over-Sampling Ratio (OSR). The method combines upsampling with Sigma-Delta quantizers to binarize or ternarize weights, Hadamard matrix smoothing to reduce quantization errors, and MultiOSR layer- and linear-wise allocation based on weight variance. Experiments on OPT and LLaMA models show SDQ-LLM achieves better perplexity and downstream task performance than existing 1-bit methods while maintaining faster quantization times. The continuous OSR parameter enables flexible trade-offs between model size and accuracy, achieving compression ratios as low as 14.81% with competitive performance.

## Executive Summary
SDQ-LLM presents a novel Sigma-Delta quantization approach for extreme LLM compression that enables continuous precision control through the Over-Sampling Ratio (OSR) parameter. The method combines Hadamard matrix smoothing to reduce quantization sensitivity, Sigma-Delta noise shaping to push quantization error to high frequencies where activations naturally suppress it, and MultiOSR layer-wise allocation based on weight variance. Tested on OPT and LLaMA models, SDQ-LLM achieves state-of-the-art performance among 1-bit methods with compression ratios as low as 14.81% while maintaining competitive perplexity and zero-shot accuracy on 6 downstream tasks.

## Method Summary
SDQ-LLM is a post-training quantization method that compresses LLM weights to 1-bit or 1.58-bit representations using Sigma-Delta modulation. The core approach applies a Hadamard transform to smooth weight outliers, then uses an upsampling-quantization-feedback loop where weights are upsampled by OSR factor and iteratively quantized through an integrator. The resulting quantization noise is shaped toward high frequencies, which activations suppress during inference via Parseval's theorem. A ternary quantizer (−1, 0, +1) with scaling factor α = mean(|W|) and threshold β ≈ 0.5α is used. Block-wise error compensation with inverse Hessian ensures output consistency. MultiOSR allocates precision based on weight variance, with higher OSR assigned to low-variance, high-sensitivity weights. The method supports flexible trade-offs between compression ratio and accuracy through continuous OSR adjustment.

## Key Results
- Achieves WikiText2 perplexity of 38.24 on OPT-1.3B with OSR=2 (compression ratio 29.63%)
- Improves over RTN-2bit and GPTQ-2bit on zero-shot tasks while using fewer bits
- Hadamard smoothing reduces WikiText2 PPL from 2434.87 to 20.13 on LLaMA3-8B
- MultiOSR provides incremental improvement (20.13 → 17.02 PPL) when combined with Hadamard
- Quantization time is 2× faster than GPTQ-2bit on OPT-1.3B

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Sigma-Delta quantization with noise shaping enables accurate 1-bit weight representation by pushing quantization error to frequencies that activations naturally suppress.
- **Mechanism:** Weights are upsampled by OSR factor, then iteratively quantized through an integrator feedback loop. The noise transfer function (1 - z⁻¹) acts as a high-pass filter, concentrating quantization noise at high frequencies. During inference, when computing A × W^T, activations function as a low-pass filter via Parseval's theorem, suppressing the shaped high-frequency noise.
- **Core assumption:** Weight matrices and activations have spectral properties where activation energy concentrates in low frequencies, making them compatible with noise-shaped quantization.
- **Evidence anchors:**
  - [Section 3.2]: "Since H_e is a high-pass filter, the quantizer shapes the noise toward higher frequencies... the quantized weights b effectively pass through a low-pass filter formed by A"
  - [Section 3.2]: Equations 7-11 define the first-order Σ-Δ quantizer with noise shaping
  - [Corpus]: No direct corpus validation of this specific noise-shaping mechanism for LLM weights exists; related work (ICQuant, PTQ1.61) addresses outliers through different approaches
- **Break condition:** If weight distributions have significant high-frequency energy components correlated with task-relevant features, the noise shaping could degrade accuracy rather than preserve it.

### Mechanism 2
- **Claim:** Hadamard matrix transformation smooths weight outliers, concentrating energy in low-to-mid frequencies and reducing quantization sensitivity.
- **Mechanism:** Multiplying weight matrices by orthogonal Hadamard matrix H redistributes outlier values across the matrix. The transformation shifts energy from uniform frequency distribution to concentrated low-frequency bands, matching the Σ-Δ quantizer's in-band precision region.
- **Core assumption:** Weight outliers in LLMs can be diffused without destroying semantically important structure; the smoothed representation remains functionally equivalent after quantization.
- **Evidence anchors:**
  - [Section 3.3]: "After applying the Hadamard transformation, the weight matrix becomes smoother in the time domain, which is reflected in the frequency domain as a concentration of energy in the low-to-mid-frequency range"
  - [Figure 3 description]: Shows time/frequency domain distributions before and after Hadamard transformation
  - [Table 2]: Ablation shows Hadamard reduces WikiText2 PPL from 2434.87 to 20.13 on LLaMA3-8B (without MultiOSR)
  - [Corpus]: Quarot (cited in paper) uses similar rotation approach; Assumption: the outlier-diffusion principle transfers across methods
- **Break condition:** If model performance depends critically on specific outlier weight values (e.g., rare but important knowledge), smoothing may erase irreplaceable information.

### Mechanism 3
- **Claim:** Weight variance inversely correlates with quantization sensitivity, enabling data-driven OSR allocation across layers and linear modules.
- **Mechanism:** MultiOSR ranks components by weight variance, assigning higher OSR (more precision) to low-variance modules under the hypothesis that low-variance weights have higher information density per parameter. OSR is distributed at layer-level first, then refined at linear-module level (q_proj, k_proj, v_proj, etc.) proportionally to both variance and parameter count.
- **Core assumption:** Weight variance is a reliable proxy for quantization sensitivity across all LLM architectures and layers; the relationship is monotonic and predictable.
- **Evidence anchors:**
  - [Section 3.4]: "Low-variance weights, despite lower entropy, have higher information density and greater quantization sensitivity (small errors cause large noise)"
  - [Table 2]: MultiOSR provides incremental improvement (20.13 → 17.02 PPL when combined with Hadamard)
  - [Corpus]: Layer-Wise Quantization work cited validates layer-level sensitivity variation; no corpus evidence directly validates linear-wise variance-based allocation
- **Break condition:** If variance-sensitivity relationship is non-monotonic or architecture-dependent, fixed allocation formulas may misallocate precision, over-compressing sensitive layers.

## Foundational Learning

- **Concept:** Sigma-Delta Modulation (oversampling + noise shaping)
  - **Why needed here:** Core technique underlying the entire quantization approach; understanding the integrator feedback loop and noise transfer function is essential for debugging quantization quality.
  - **Quick check question:** Given a 2× OSR and ternary quantizer, explain why the effective bit-width is 1.58 × 2 / 16 = 19.75% compression ratio.

- **Concept:** Parseval's Theorem (time-frequency domain equivalence)
  - **Why needed here:** Justifies why high-frequency noise shaped by Σ-Δ is suppressed during matrix multiplication with activations.
  - **Quick check question:** If activations A have low-pass spectral characteristics, how does this affect the quantization noise E during A × W_q^T computation?

- **Concept:** Hadamard Matrix Properties (orthogonality, recursive construction)
  - **Why needed here:** Explains why this specific transform diffuses outliers while preserving matrix rank and invertibility.
  - **Quick check question:** Why must the Hadamard transform be inverted (H^T applied) after quantization to maintain output alignment?

## Architecture Onboarding

- **Component map:**
  Input Weights W → Hadamard Transform (H × W) → Resample(upsample by OSR) → Sigma-Delta Quantizer (integrator + ternary quantization) → Resample(downsample to original size) → Hadamard Inverse (× H^T) → GPTQ-style block compensation → Quantized weights W_q
  During inference: Activation A → Resample(upsample by OSR) → A × W_q^T

- **Critical path:**
  1. Hadamard smoothing (without this, PPL degrades catastrophically per Table 2)
  2. Sigma-Delta quantizer integrator initialization (i₀ = 0) and feedback loop
  3. OSR configuration (controls compression-accuracy trade-off)
  4. Block-wise error compensation using inverse Hessian (inherited from GPTQ)

- **Design tradeoffs:**
  | Decision | Options | Impact |
  |----------|---------|--------|
  | OSR value | 1.0 – 4.0+ | Higher OSR = better accuracy, larger model size; concave PPL curve suggests diminishing returns |
  | Quantizer type | Binary (-1,+1) vs Ternary (-1,0,+1) | Ternary provides better accuracy at 1.58 bits vs 1 bit |
  | MultiOSR | Disabled / Layer-wise / Full | Linear-wise adds ~3 PPL improvement on LLaMA3-8B |
  | Block size | 128 (default) | Larger blocks may reduce compensation granularity |

- **Failure signatures:**
  - PPL > 1000: Hadamard smoothing likely disabled or misapplied
  - PPL degradation on specific model sizes: OSR too low for that architecture's weight variance profile
  - Inference dimension mismatch: Activation upsampling OSR doesn't match weight OSR
  - Slow quantization: Check Cholesky decomposition of Hessian for numerical instability

- **First 3 experiments:**
  1. **Baseline validation:** Quantize OPT-1.3B with OSR=2, ternary quantizer, Hadamard enabled. Verify WikiText2 PPL ≈ 38.24 (Table 1). If PPL > 50, debug Hadamard application order.
  2. **OSR sweep:** Run OSR from 1.0 to 3.0 in 0.25 increments on OPT-6.7B. Plot PPL vs compression ratio. Confirm concave curve shape (Figure 6). Identify knee point for memory-accuracy trade-off.
  3. **Ablation configuration:** Test 4 configurations on LLaMA3-8B: (no-Hadamard/no-MultiOSR), (Hadamard/no-MultiOSR), (no-Hadamard/MultiOSR), (Hadamard/MultiOSR). Replicate Table 2. If Hadamard-only PPL ≠ ~20.13, verify H^T is applied correctly after quantization.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are the theoretical limits of performance retention for SDQ-LLM under aggressive compression settings (e.g., OSR < 1.5)?
- **Basis in paper:** [explicit] The conclusion states there are "significant challenges" and "considerable room for exploration regarding the trade-off between size and precision at extremely low OSR, as well as the limits of performance retention under such aggressive compression."
- **Why unresolved:** The experiments primarily test OSR values of 2.0 and above, leaving the behavior and potential failure modes at much lower ratios (higher compression) unexplored.
- **What evidence would resolve it:** Empirical results showing perplexity and zero-shot accuracy degradation curves as OSR approaches 1.0.

### Open Question 2
- **Question:** Does the computational overhead of real-time activation upsampling negate the theoretical latency gains from replacing multiplication with addition?
- **Basis in paper:** [inferred] The method claims to enhance inference efficiency by replacing multiplication with addition (Section 1), yet Figure 5 shows that activations must be upsampled via FFT during inference.
- **Why unresolved:** The paper benchmarks quantization time but does not measure actual inference latency, leaving the net efficiency gain of the additional resampling step unverified.
- **What evidence would resolve it:** A comparison of end-to-end inference throughput and latency between SDQ-LLM and baseline floating-point models on standard hardware.

### Open Question 3
- **Question:** Is weight variance the theoretically optimal metric for fine-grained OSR allocation (MultiOSR), or do higher-order statistics offer better results?
- **Basis in paper:** [inferred] Section 3.4 notes the correlation between variance and sensitivity is based on a "possible explanation" and "preliminary investigation," suggesting the heuristic is not rigorously proven.
- **Why unresolved:** While effective, the allocation strategy relies on an empirical observation about information density, potentially missing other factors like activation magnitudes or outlier distribution.
- **What evidence would resolve it:** Ablation studies comparing variance-based OSR allocation against strategies based on Hessian sensitivity or activation-aware metrics.

## Limitations

- The noise-shaping mechanism's effectiveness depends on weight distributions having compatible spectral properties with activation low-pass characteristics, which may not hold for all LLM architectures
- The Hadamard transformation's impact on semantically critical outlier information is not experimentally validated, raising concerns about information loss
- MultiOSR variance-based allocation formula is not fully specified, making faithful reproduction challenging

## Confidence

- **High confidence**: The Hadamard smoothing mechanism's effectiveness (validated by PPL improvements in Table 2), the basic Sigma-Delta quantization implementation, and the continuous OSR parameter's impact on compression-accuracy trade-offs.
- **Medium confidence**: The MultiOSR variance-based allocation method's optimality (only incremental improvements shown), the noise-shaping mechanism's universal applicability across LLM architectures, and the claim that ternary quantization provides superior accuracy over binary.
- **Low confidence**: The universal validity of the noise-shaping principle for all LLM weight distributions, the Hadamard transform's preservation of semantically critical outlier information, and the absence of evaluation on multilingual or code-generation tasks.

## Next Checks

1. **Spectral validation experiment**: Compute and compare the frequency spectra of weight matrices before and after Hadamard transformation, then measure the actual frequency content of quantization noise during inference. Verify that the shaped noise concentrates in high frequencies that activations suppress.

2. **MultiOSR allocation verification**: Implement the variance-to-OSR mapping explicitly and test whether manual, oracle-based OSR allocation (using a small validation set to optimize per-layer OSR) outperforms the automatic variance-based method.

3. **Outlier sensitivity analysis**: Create synthetic weight matrices with known critical outlier values, apply SDQ-LLM quantization, and measure whether model performance degrades when those specific outliers are diffused by the Hadamard transform. Compare against baseline quantization methods that preserve outliers.