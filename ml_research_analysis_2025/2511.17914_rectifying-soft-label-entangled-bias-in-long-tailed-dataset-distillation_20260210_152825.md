---
ver: rpa2
title: Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation
arxiv_id: '2511.17914'
source_url: https://arxiv.org/abs/2511.17914
tags:
- dataset
- distillation
- distilled
- soft
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses dataset distillation performance degradation
  under long-tailed class distributions by analyzing soft label bias. The authors
  derive an imbalance-aware generalization bound showing that biased soft labels from
  imbalanced datasets limit distilled model performance.
---

# Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation

## Quick Facts
- arXiv ID: 2511.17914
- Source URL: https://arxiv.org/abs/2511.17914
- Reference count: 40
- Primary result: ADSA improves tail-class accuracy by up to 11.8% on ImageNet-1k-LT with IPC=50

## Executive Summary
This work addresses the performance degradation of dataset distillation under long-tailed class distributions by identifying and correcting soft-label bias. The authors develop an imbalance-aware generalization bound showing that biased soft labels from imbalanced datasets limit distilled model performance. Through perturbation experiments, they identify two bias sources: the distillation model and distilled images. They propose ADSA, an Adaptive Soft-label Alignment module that calibrates soft label distributions by minimizing class-wise confidence variance across distilled images. ADSA acts as a post-hoc module requiring no training modification.

## Method Summary
The paper introduces ADSA (Adaptive Soft-label Alignment), a lightweight post-hoc module that corrects soft-label bias in long-tailed dataset distillation. ADSA works by analyzing the distilled images as a validation set, computing class-wise average confidence statistics, and finding an optimal calibration strength parameter Ï„ that minimizes the variance of these confidences across classes. This calibration is applied via logit adjustment, effectively aligning the soft label distribution with the balanced test prior. The method is designed to be plug-and-play, requiring no modification to existing dataset distillation pipelines.

## Key Results
- ADSA improves tail-class accuracy by up to 11.8% on ImageNet-1k-LT with IPC=50
- Overall accuracy reaches 41.4% when applied to ImageNet-1k-LT
- Method consistently improves performance across multiple dataset distillation techniques and imbalance settings
- ADSA requires no training modification and adds minimal computational overhead

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Performance degradation in long-tailed dataset distillation stems from two entangled bias sources: the distillation model and the synthetic images, rather than just data scarcity.
- **Mechanism:** The distillation model, trained on imbalanced data, produces over-confident predictions for head classes (model bias). Simultaneously, the synthetic images generated via inversion exhibit a feature distribution shift, causing even a balanced model to mispredict on tail classes (image bias). These combine to distort the soft labels $p_{obs}(y|x)$.
- **Core assumption:** The bias can be decomposed linearly or quasi-linearly as $p_{obs} \approx p_{target} + \epsilon_T + \epsilon_I$ (Eq. 8), and the distilled images are sufficiently representative to act as a diagnostic set.
- **Break condition:** If the synthetic images are so low-quality that they lack semantic meaning for tail classes, no label calibration can recover the lost information.

### Mechanism 2
- **Claim:** Optimizing a calibration strength parameter $\tau$ to minimize class-wise confidence variance effectively disentangles and removes the bias.
- **Mechanism:** ADSA functions as a post-hoc calibrator. It adjusts logits via $f_y(x) - \tau \log \pi_y$. By treating the distilled set as a validation set, it searches for $\tau$ that flattens the confidence distribution across classes (minimizing the variance of $\bar{y}$), thereby forcing the posterior $p_{dd}(y|x)$ to align with the balanced test prior $p_{te}(y|x)$.
- **Core assumption:** The distilled images, despite their distribution shift, still retain sufficient class-relative structure for the "hold-out" confidence statistics to be meaningful.
- **Break condition:** If the imbalance factor (IF) is extreme and the IPC (Images Per Class) is very low (e.g., 1), the variance estimate may become too noisy for stable $\tau$ optimization.

### Mechanism 3
- **Claim:** The calibration generalizes because it tightens the imbalance-aware upper bound on test loss.
- **Mechanism:** Theoretically, the test error is bounded by a term involving $D_{KL}(p_{te}(y|x) \| p_{dd}(y|x))$. By enforcing uniform confidence across classes (matching the balanced test prior), the method directly minimizes this KL divergence, tightening the bound described in Theorem 3.1.
- **Core assumption:** The class-conditional distributions are consistent between training and testing ($p_{tr}(x|y) = p_{te}(x|y)$), a standard assumption in long-tailed recognition.
- **Break condition:** If the test set is not balanced or does not share the conditional feature distribution of the training set, the bound relaxation fails, and calibration may over-correct.

## Foundational Learning

- **Concept:** Dataset Distillation (Squeeze-Revover-Relabel)
  - **Why needed here:** The paper modifies the "Relabel" phase of this specific pipeline. You must understand that "synthetic images" are learnable tensors optimized to mimic a larger dataset, distinct from standard data augmentation.
  - **Quick check question:** Can you distinguish between the "distillation model" (teacher) and the "evaluation model" (student) in the SRe2L pipeline?

- **Concept:** Logit Adjustment / Calibration
  - **Why needed here:** The core mathematical operation of ADSA ($f_y(x) - \tau \log \pi_y$). You need to know how logits relate to softmax probabilities and how class priors affect the decision boundary.
  - **Quick check question:** If $\pi_y$ is small for a tail class, does adding $-\tau \log \pi_y$ increase or decrease the effective logit for that class?

- **Concept:** Long-Tailed Distribution (Head vs. Tail)
  - **Why needed here:** The mechanism relies on the "imbalance factor" (IF). You must understand that models tend to be overconfident on "head" (frequent) classes and underconfident on "tail" (rare) classes.
  - **Quick check question:** In a dataset with IF=100, how many times more samples does the head class have compared to the tail class?

## Architecture Onboarding

- **Component map:** Distilled Images -> ADSA Module -> Calibrated Soft Labels -> Evaluation Model
- **Critical path:** The **Statistics Collector** is the bottleneck. The calibration is purely statistical; if the batch of distilled images does not represent the full class spectrum, the variance calculation fails.
- **Design tradeoffs:**
  - *Post-hoc vs. In-loop:* ADSA is designed as a lightweight post-processing module. This avoids complex re-training of the distillation model but assumes the bias is correctable at the label level.
  - *IPC vs. Stability:* With low IPC (e.g., 1), per-class statistics are noisy. The paper suggests ADSA is robust, but visually inspecting the confidence distributions is recommended before trusting the auto-selected $\tau$.
- **Failure signatures:**
  - **High Entropy on Head Classes:** If $\tau$ is too high, the model might become underconfident on head classes, reducing overall accuracy.
  - **No Improvement:** If the distillation model is already perfectly balanced (or uses strong resampling), ADSA may yield marginal gains (Table 15 shows some cases where gains are small).
- **First 3 experiments:**
  1. **Perturbation Reproduction (Sanity Check):** Implement Config 1 vs Config 4 from Section 3.2 on CIFAR-100-LT to verify that image bias and model bias exist independently.
  2. **Tau Sensitivity Analysis:** On ImageNet-LT (IPC=50), plot the relationship between $\tau$ and tail-class accuracy to verify that the variance-minimization heuristic actually lands near the optimal point.
  3. **Cross-Method Integration:** Apply ADSA to a non-distribution-matching method (like MTT or DREAM, as in Table 3) to confirm the "plug-and-play" capability without modifying the base distillation loss.

## Open Questions the Paper Calls Out
None

## Limitations
- Data Quality Dependency: ADSA's effectiveness hinges on the distilled images being sufficiently representative of all classes.
- Distribution Shift Assumptions: The method assumes the test set is balanced and shares the same class-conditional feature distribution as training.
- Statistical Stability: With very low IPC (e.g., 1 image per class), the class-wise confidence statistics become unreliable.

## Confidence

**High Confidence (8/10):**
- The existence of dual bias sources (model and image) is well-supported by controlled perturbation experiments.
- The post-hoc calibration framework (ADSA) is mathematically sound and demonstrably effective across multiple base methods.
- The plug-and-play nature and lightweight computational overhead are verified empirically.

**Medium Confidence (6/10):**
- The theoretical bound tightening claim lacks external validation and depends heavily on the assumption of balanced test distributions.
- The generalization across extreme imbalance scenarios (very low IPC) is inferred but not extensively tested.

**Low Confidence (4/10):**
- The claim that variance minimization always finds the optimal calibration strength needs more rigorous sensitivity analysis, especially for different imbalance factors.

## Next Checks

1. **Extreme Imbalance Stress Test:** Apply ADSA to ImageNet-LT with IPC=1 and IF=200. Measure whether the variance-minimization still produces meaningful calibration or if the statistics become too noisy to be useful.

2. **Cross-Distribution Generalization:** Train and evaluate on a naturally long-tailed test set (not artificially balanced). Compare ADSA's performance against a fixed, hand-tuned calibration to see if automatic variance minimization adapts appropriately or over-corrects.

3. **Bias Decomposition Validation:** Design an ablation where you independently control model bias (via reweighting) and image bias (via feature regularization during distillation). Apply ADSA to each scenario to quantify which bias source contributes more to the performance gain.