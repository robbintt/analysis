---
ver: rpa2
title: 'StatsMerging: Statistics-Guided Model Merging via Task-Specific Teacher Distillation'
arxiv_id: '2506.04567'
source_url: https://arxiv.org/abs/2506.04567
tags:
- merging
- statsmerging
- tasks
- task
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StatsMerging introduces a lightweight, learning-based model merging
  approach guided by weight distribution statistics, specifically leveraging singular
  values from SVD to capture task-specific weight distributions. It employs a lightweight
  StatsMergeLearner to predict merging coefficients based on statistical features
  extracted from pre-trained models, avoiding the need for ground-truth labels or
  test samples.
---

# StatsMerging: Statistics-Guided Model Merging via Task-Specific Teacher Distillation

## Quick Facts
- arXiv ID: 2506.04567
- Source URL: https://arxiv.org/abs/2506.04567
- Reference count: 20
- Key outcome: Achieves 94.5% average accuracy on merging models from eight tasks, surpassing WEMoE by 5.1%

## Executive Summary
StatsMerging introduces a lightweight, learning-based model merging approach guided by weight distribution statistics, specifically leveraging singular values from SVD to capture task-specific weight distributions. It employs a lightweight StatsMergeLearner to predict merging coefficients based on statistical features extracted from pre-trained models, avoiding the need for ground-truth labels or test samples. The method introduces Task-Specific Teacher Distillation to merge vision models with heterogeneous architectures. Experiments across eight tasks show that StatsMerging outperforms state-of-the-art techniques, achieving 94.5% average accuracy on merging models from eight tasks, surpassing WEMoE by a margin of 5.1%. The approach also demonstrates improved generalization to unseen tasks and robustness to image quality variations.

## Method Summary
StatsMerging extracts weight statistics (mean, variance, L2 norm, and top-3 singular values via SVD) from fine-tuned models, then uses a 2-layer MLP (StatsMergeLearner) to predict merging coefficients. The method employs Task-Specific Teacher Distillation where each pre-trained model acts as a teacher, generating pseudo-labels on validation data. The StatsMergeLearner is trained to predict coefficients that produce merged model outputs matching these pseudo-labels. For heterogeneous architectures, models are first distilled into a unified backbone before merging. The final merged model is created by combining individual models using the predicted coefficients, either layer-wise or task-wise.

## Key Results
- Achieves 94.5% average accuracy on merging models from eight tasks, outperforming WEMoE by 5.1%
- Demonstrates improved generalization to unseen tasks compared to baseline methods
- Shows robustness to image quality variations including Motion Blur, Impulse Noise, and Gaussian Noise
- Layer-wise merging outperforms task-wise merging while requiring more complex coefficient management

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Statistical features derived from weight matrices, specifically singular values, serve as a proxy for task importance, enabling a lightweight learner to predict optimal merging coefficients without gradient descent on the full model.
- **Mechanism:** Rather than using raw weights (high dimension) or validation loss (computationally expensive), the system extracts a compact feature vector $S_k = [\mu, \sigma^2, m, \sigma'_r]$ via SVD. A Multilayer Perceptron (StatsMergeLearner) maps these static statistics to a scalar or vector coefficient $\lambda$. This effectively compresses the "state" of a fine-tuned model into a learnable input.
- **Core assumption:** The magnitude and distribution of weights—captured via singular values—correlate strongly with the relative contribution a model should make to a unified multi-task model.
- **Evidence anchors:**
  - [abstract] "It uniquely leverages singular values from singular value decomposition (SVD) to capture task-specific weight distributions..."
  - [section 3.2] "We hypothesize that singular values encapsulate essential information regarding the weight distribution, which can guide the allocation of weights..."
  - [corpus] While specific SVD-merging links are novel here, related work confirms "Model Merging via Multi-Teacher Knowledge Distillation" relies on similar high-level transfer constraints [arXiv:2512.21288].
- **Break condition:** If models have identical weight distributions (e.g., fine-tuned on nearly identical data) or if SVD rank $r=3$ is insufficient to capture the feature space nuance, the learner receives uninformative inputs.

### Mechanism 2
- **Claim:** Merging coefficients can be optimized via distillation from task-specific teachers, removing the need for ground-truth human labels.
- **Mechanism:** Each pre-trained model acts as a "Teacher" generating pseudo-labels on a validation set. The StatsMergeLearner predicts coefficients $\lambda$, which are used to merge models. The merged model's predictions are compared against the Teacher's pseudo-labels using Cross-Entropy loss. Gradients update the *StatsMergeLearner*, not the merged model weights directly.
- **Core assumption:** The "Teacher" models are sufficiently accurate on their specific tasks that their pseudo-labels provide a reliable supervisory signal for the merged model.
- **Evidence anchors:**
  - [abstract] "...merging learning paradigm that avoids costly ground-truth labels by task-specific teacher distillation."
  - [section 3.3] "Our key intuition is that each pre-trained model $\theta_k$ is already good at its own task... therefore we regard it as the Task-Specific Teacher."
  - [corpus] Supports general trend of using distillation for transfer [arXiv:2509.08814].
- **Break condition:** If the validation set contains out-of-distribution samples for a specific teacher, the generated pseudo-labels will be noisy, potentially destabilizing the coefficient learning.

### Mechanism 3
- **Claim:** Heterogeneous architectures (e.g., CNNs and ViTs) can be merged by first distilling them into a unified "target" architecture.
- **Mechanism:** Before merging, if a model $M_i$ has a different architecture than the target, it is distilled into the target architecture using standard KD (KL divergence on logits). Once all models reside in the same weight space (architecture), the StatsMerging coefficient prediction proceeds as usual.
- **Core assumption:** The distillation process preserves enough of the original model's functional capabilities such that the subsequent weight merging adds value.
- **Evidence anchors:**
  - [abstract] "It introduces Task-Specific Teacher Distillation for merging vision models with heterogeneous architectures..."
  - [section 4.2] "...we first distill them into a single backbone before applying our merging method."
  - [corpus] Weak direct evidence in neighbors; this appears to be a specific contribution of this paper.
- **Break condition:** If the target architecture lacks the capacity (e.g., too few parameters) to model the knowledge of the source teacher, the pre-merge distillation fails, rendering the merge ineffective.

## Foundational Learning

- **Concept: Singular Value Decomposition (SVD)**
  - **Why needed here:** The paper relies on low-rank approximation ($\Sigma_k$) to capture the "dominant properties" of weight matrices. Understanding that singular values represent the energy/magnitude of directions in the weight space is crucial for interpreting the input features to the learner.
  - **Quick check question:** Can you explain why the top $r$ singular values are chosen as features rather than random weight samples?

- **Concept: Task Arithmetic**
  - **Why needed here:** The method builds upon the "Task Vector" paradigm ($\theta_{MTL} = \theta_{pre} + \sum \lambda_k T_k$). StatsMerging essentially learns the $\lambda$ scaling factors for this arithmetic operation.
  - **Quick check question:** How does adding a "task vector" to a pre-trained model differ from traditional fine-tuning?

- **Concept: Knowledge Distillation (Teacher-Student)**
  - **Why needed here:** The training loop for the merging coefficients relies entirely on the "Task-Specific Teacher Distillation" concept. You must understand how soft/hard labels from a teacher guide a student (the merged model) without ground truth.
  - **Quick check question:** In this context, is the *merged model* the student, or is the *StatsMergeLearner* the student? (Trick: The merged model generates the logits, but the SML learns the parameters).

## Architecture Onboarding

- **Component map:** Pre-trained weights $\theta_{pre}$ and Fine-tuned weights $\theta_k$ -> Stats Extractor (computes mean, variance, norm, SVD) -> StatsMergeLearner (2-layer MLP) -> Merger (applies $\theta_{merged} = \sum \lambda_k \theta_k$) -> Trainer (compares predictions vs Teacher pseudo-labels)
- **Critical path:** The SVD extraction ($S_k$) $\to$ SML forward pass ($\lambda$) $\to$ Model Merging $\to$ Loss Calculation. The SVD step is static; the SML is the only learnable component.
- **Design tradeoffs:**
  - **Hard vs. Soft Labels:** The paper (Table 5/6) surprisingly finds Hard Pseudo Labels (Cross Entropy) outperform Soft Pseudo Labels (KL-Div) for this specific merging task, hypothesizing that noisy inter-class relationships in aggregated datasets hurt soft labels.
  - **Granularity:** Layer-wise merging outperforms Task-wise merging (Table 2) but requires managing $L \times K$ coefficients instead of just $K$.
- **Failure signatures:**
  - **Uniform Coefficients:** If the SML outputs constant values regardless of input stats, the gradient signal from the distillation loss is likely too weak or the learning rate is too low.
  - **Performance Collapse on Unseen Tasks:** If the learner overfits to the validation set of the "seen" tasks, generalization (Table 3) will degrade significantly.
- **First 3 experiments:**
  1. **Ablate Statistics:** Run the merger using only the SVD features ($\sigma'$) vs. only the magnitude ($m$) to verify the contribution of singular values (replicate Table 6).
  2. **Loss Function Check:** Train the SML using KL-Divergence (Soft Labels) vs. Cross-Entropy (Hard Labels) on a subset of tasks to confirm the paper's finding that Hard Labels work better for this architecture.
  3. **Heterogeneity Test:** Attempt to merge a ResNet and a ViT using the provided distillation pre-step to verify the architecture-agnostic claim.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the weight statistics-guided merging approach be effectively extended to non-classification vision tasks?
- **Basis in paper:** [explicit] Section B.5 states the current work focuses on vision-based classification tasks, explicitly leaving extensions to object detection, super-resolution, and video restoration for future work.
- **Why unresolved:** The methodology and experiments are currently validated exclusively on classification benchmarks (e.g., SUN397, Stanford Cars), which have distinct loss landscapes compared to detection or generation tasks.
- **Evidence:** Experimental results showing that StatsMerging successfully merges models trained on object detection (e.g., COCO) or restoration tasks without significant performance degradation relative to individual models.

### Open Question 2
- **Question:** Does the reliance on singular values generalize to merging Large Language Models (LLMs) and multi-modal models?
- **Basis in paper:** [explicit] Section B.5 identifies expanding the approach to language tasks, specifically LLMs, and multi-modal learning as promising directions for further research.
- **Why unresolved:** The paper validates the method using Vision Transformers (ViTs) and CNNs. It is unknown if SVD-based weight statistics capture task importance in the embedding spaces of LLMs as effectively as they do in vision models.
- **Evidence:** Successful application of StatsMergeLearner to merge distinct LLMs (e.g., instruction-tuned variants) or multi-modal models, demonstrating improved perplexity or benchmark accuracy over baseline merging methods.

### Open Question 3
- **Question:** Can the performance drop associated with soft pseudo labels (KL-Divergence) be mitigated by addressing inter-class relationships?
- **Basis in paper:** [explicit] Section B.2 hypothesizes that the observed performance drop when using KL-Divergence loss is due to "noisy inter-class relationships within the aggregated dataset," suggesting this for future research.
- **Why unresolved:** The authors utilized hard pseudo labels (Cross-Entropy) to avoid this issue but did not propose a mechanism to clean or weight the soft labels to fix the underlying noise problem.
- **Evidence:** A modified distillation loss or re-weighting scheme that accounts for inter-class noise, resulting in soft-label training outperforming the current hard-label baseline.

## Limitations
- The core approach relies on the assumption that singular values extracted from weight matrices contain sufficient information to predict optimal merging coefficients, which is not rigorously validated
- The method's reliance on pseudo-labels from task-specific teachers introduces a potential failure mode if teachers are not sufficiently accurate or if validation data is out-of-distribution
- The heterogeneous architecture merging claims are promising but the pre-merging distillation step may not preserve all task-specific knowledge, particularly for architectures with fundamentally different inductive biases

## Confidence

- **High confidence**: The experimental results showing StatsMerging outperforming baselines (94.5% Avg Acc vs WEMoE's 89.4%) are well-documented and reproducible. The layer-wise vs task-wise merging comparison is also clearly demonstrated.
- **Medium confidence**: The claim that singular values serve as effective task importance indicators is supported by results but lacks theoretical justification. The superiority of hard pseudo labels over soft labels is demonstrated but not deeply explained.
- **Low confidence**: The robustness claims to unseen tasks and image corruptions need more extensive validation, as these are only briefly evaluated.

## Next Checks

1. **Ablation of Statistical Features**: Replicate Table 6 by running experiments with only SVD features vs only magnitude features to quantify the specific contribution of singular values to merging performance.
2. **Loss Function Validation**: Train the SML using KL-Divergence (soft labels) vs Cross-Entropy (hard labels) on a subset of tasks to confirm the paper's finding that hard labels work better for this architecture.
3. **Heterogeneous Architecture Test**: Attempt to merge a ResNet and a ViT using the provided distillation pre-step to verify the architecture-agnostic claim and assess knowledge preservation through the distillation process.