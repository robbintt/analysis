---
ver: rpa2
title: 'Trade-offs in Financial AI: Explainability in a Trilemma with Accuracy and
  Compliance'
arxiv_id: '2602.01368'
source_url: https://arxiv.org/abs/2602.01368
tags:
- financial
- accuracy
- explainability
- systems
- compliance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study examines how finance professionals navigate competing\
  \ priorities when deploying AI systems, moving beyond the simplistic accuracy\u2013\
  explainability trade-off. Through semi-structured interviews with twenty finance\
  \ professionals, it reveals that explainability is shaped by a trilemma: accuracy\
  \ and compliance act as non-negotiable prerequisites, while ease of understanding\
  \ determines usability and adoption."
---

# Trade-offs in Financial AI: Explainability in a Trilemma with Accuracy and Compliance

## Quick Facts
- arXiv ID: 2602.01368
- Source URL: https://arxiv.org/abs/2602.01368
- Reference count: 8
- One-line primary result: Finance professionals treat accuracy and compliance as non-negotiable prerequisites, with ease of understanding determining AI adoption

## Executive Summary
This study examines how finance professionals navigate competing priorities when deploying AI systems, moving beyond the simplistic accuracyâ€“explainability trade-off. Through semi-structured interviews with twenty finance professionals, it reveals that explainability is shaped by a trilemma: accuracy and compliance act as non-negotiable prerequisites, while ease of understanding determines usability and adoption. Cost and speed are operational constraints, not core criteria. Findings challenge the dominant framing of explainability as merely a sacrifice of accuracy, highlighting the regulatory and organizational realities of AI in finance. The study recommends designing explanation tools with simultaneous attention to all three dimensions and involving compliance and business users early in model development.

## Method Summary
The study conducted twenty semi-structured interviews (30-60 minutes each) with finance professionals across banks, AI vendors, consultancies, EMIs, and development banks. Participants ranked five factors (accuracy, ease of understanding, compliance, cost, speed) and provided justifications. Analysis used dual independent coding (holistic deductive plus Gioia inductive) merged into shared themes. The purposive and snowball sampling approach focused on professionals with at least six months of direct AI interaction in finance.

## Key Results
- Accuracy and compliance function as binary "hygiene factors" that must be satisfied before any other considerations
- Ease of understanding drives adoption rates and requires explicit translation layers between technical metrics and business users
- Speed and cost act as feasibility constraints but become hard blocks only during live operational deployment

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: In financial AI deployment, accuracy and compliance function as non-negotiable "hygiene factors" rather than trade-off variables.
- **Mechanism**: The selection process acts as a binary filter. If a system fails to meet the accuracy threshold or regulatory compliance standards (e.g., auditability), it is rejected as a liability regardless of its operational efficiency or raw explainability scores.
- **Core assumption**: Financial institutions prioritize risk avoidance (regulatory/legal liability) over operational efficiency when the former is threatened.
- **Evidence anchors**:
  - [abstract]: "Findings reveal... accuracy and compliance act as non-negotiable prerequisites... without them, an AI system is viewed as a liability."
  - [section 4.2]: Participants described compliance as an "unbreachable barrier to entry," where lack of explainability results in zero approval.
  - [corpus]: "Agentic AI for Financial Crime Compliance" supports the criticality of aligning AI with regulatory expectations to avoid opacity risks.
- **Break condition**: If the deployment context moves to a jurisdiction with weak regulatory regimes or purely internal/experimental use cases with no client impact.

### Mechanism 2
- **Claim**: "Ease of understanding" drives adoption rates and is distinct from technical "explainability," requiring an explicit translation layer between model outputs and business users.
- **Mechanism**: Technical XAI metrics (e.g., SHAP values) are converted into role-specific narratives (e.g., "loan denied due to low balance"). Without this translation, business users cannot utilize the model for decision-making or defense, stalling adoption.
- **Core assumption**: Business users (operators/decision-ratifiers) lack the technical bandwidth to interpret raw model diagnostics and require simplified "human-readable" logic.
- **Evidence anchors**:
  - [section 4.3]: P6 noted that business units "would not understand" SHAP plots, requiring a "translation layer" into narrative categories.
  - [abstract]: "Ease of understanding determines usability and adoption."
  - [corpus]: "Unlocking the Black Box" corroborates the need for frameworks that balance predictability with regulator-required explainability.
- **Break condition**: If the end-user is a data scientist or if the AI operates fully autonomously without human intervention.

### Mechanism 3
- **Claim**: Speed and cost act as feasibility constraints rather than primary optimization targets, but latency becomes a hard block during live operational deployment.
- **Mechanism**: Development phases tolerate high latency/cost (e.g., batch processing), but strict operational KPIs enforce a latency ceiling at deployment. If explanation generation slows the workflow below this ceiling, users abandon the tool.
- **Core assumption**: User tolerance for latency is strictly tied to the specific workflow's temporal constraints (e.g., real-time support vs. overnight risk reports).
- **Evidence anchors**:
  - [section 4.4.1]: P1 warned that if a model "would take one hour... they would rather type," prioritizing manual speed over AI assistance.
  - [section 4.4.2]: Cost is deprioritized during "hype cycles" but becomes a constraint during "cost-minded" reality phases.
  - [corpus]: "Comparing Open-Source and Commercial LLMs..." highlights design trade-offs involving domain-specific efficiency.
- **Break condition**: If computation moves to asynchronous background processes or business value justifies the wait time.

## Foundational Learning

- **Concept: Hygiene Factors vs. Motivators**
  - **Why needed here**: The paper reframes compliance and accuracy not as features to be "traded" but as baseline prerequisites (hygiene factors). Understanding this prevents engineers from fruitlessly optimizing other features when the baseline is unmet.
  - **Quick check question**: Is the feature you are optimizing a "motivator" (increases adoption) or a "hygiene factor" (prevents rejection)?

- **Concept: The Accuracy-Explainability Frontier**
  - **Why needed here**: The paper situates the problem on this frontier, noting that moving toward higher accuracy often moves the system toward "black box" territory. This is the core tension the trilemma attempts to resolve.
  - **Quick check question**: Does increasing model complexity significantly shift the system into an unexplainable region for your specific data type?

- **Concept: Translation Layers in XAI**
  - **Why needed here**: The study highlights that raw explainability metrics (SHAP/LIME) fail to drive adoption because they are not "human-readable."
  - **Quick check question**: Can a non-technical stakeholder explain the model's decision using the output you currently provide, or do they need a separate translation step?

## Architecture Onboarding

- **Component map**: Prerequisite Check -> Core Model -> XAI Engine -> Translation Layer -> Constraint Layer
- **Critical path**: Verify Accuracy -> Verify Compliance (Auditability) -> Generate Explanation -> Translate for Business User -> Check Latency against KPIs
- **Design tradeoffs**:
  - *Complexity vs. Auditability*: Complex models (Accuracy++) require heavy translation layers (Compliance/Ease--)
  - *Speed vs. Fidelity*: Exact explanation calculation (High Fidelity) increases latency (Speed--), potentially violating operational KPIs
- **Failure signatures**:
  - **The "Unapprovable Black Box"**: High accuracy, but compliance rejects it because the decision logic is opaque
  - **The "Overconfident Translator"**: The LLM-based explanation layer produces fluent text that contradicts the actual model logic (hallucination)
  - **The "Shelved Prototype"**: Fully compliant and accurate, but the interface is too slow for customer support staff to use
- **First 3 experiments**:
  1. **Translation A/B Test**: Measure user trust/comprehension when presenting raw SHAP plots vs. narrative explanations for the same predictions
  2. **Latency Profiling**: Measure the computational overhead of generating explanations (e.g., LIME vs. SHAP) to identify if they breach the operational "speed" constraints defined by business users
  3. **Compliance Mapping**: Trace a specific regulatory requirement (e.g., "Right to Contest") through the system to verify the generated explanation is "defensible" (Section 4.1)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do the trilemma prioritization patterns (accuracy and compliance as prerequisites, ease of understanding as adoption gateway) hold across different regulatory environments outside the EU?
- **Basis in paper**: [explicit] "Future work should therefore extend this study to more diverse regions... in order to examine whether prioritization patterns differ across regulatory environments and professional demographics."
- **Why unresolved**: Sample was skewed toward European institutions operating under stricter regulatory frameworks (GDPR, EU AI Act); only one non-European case (Asia) was discussed, where weaker regulation enabled alternative credit scoring models.
- **What evidence would resolve it**: Comparative studies with finance professionals in jurisdictions with varying regulatory stringency (e.g., US, UK, Singapore, emerging markets) using the same ranking methodology.

### Open Question 2
- **Question**: What specific explanation formats and translation mechanisms effectively bridge technical XAI outputs (e.g., SHAP values) and business-user mental models?
- **Basis in paper**: [explicit] Findings point to "the need for systematic, multi-method follow-up studies, especially a deeper investigation into how automated decisions are explained to achieve greater ease of understanding by various audiences."
- **Why unresolved**: Participants reported that standard XAI tools like SHAP are not "intuitive enough" for business users, requiring manual "translation layers," but effectiveness of these translations was not evaluated.
- **What evidence would resolve it**: Controlled experiments comparing comprehension, decision quality, and trust across different explanation formats (SHAP plots, narrative translations, LLM-generated explanations) with actual finance professionals.

### Open Question 3
- **Question**: Does the "hygiene factor" conceptualization of compliance translate into measurable differences in model selection, documentation practices, and regulatory outcomes?
- **Basis in paper**: [inferred] Compliance is repeatedly described as a "given" and "hygiene factor" that participants ranked low because it is assumed satisfied; however, self-reported rankings cannot verify whether compliance is actually achieved uniformly in practice.
- **Why unresolved**: The study relies on self-reported perceptions; actual compliance practices, audit outcomes, and model governance documentation were not examined.
- **What evidence would resolve it**: Document analysis of model governance records, combined with interviews examining how "compliance by design" approaches are implemented and audited in practice.

## Limitations
- Findings based on 20 interviews primarily within EU regulatory contexts, potentially limiting generalizability to jurisdictions with different regulatory frameworks
- Interview guide was not fully specified in the paper, and the complete codebook merging process remains undocumented
- Purposive and snowball sampling may have introduced selection bias toward participants with specific perspectives on AI deployment challenges

## Confidence
- **High Confidence**: The identification of accuracy and compliance as prerequisite "hygiene factors" that must be satisfied before other considerations; the distinction between technical explainability metrics and business-user comprehension requirements
- **Medium Confidence**: The characterization of speed and cost as operational constraints rather than primary optimization targets; the specific framing of the trilemma structure
- **Low Confidence**: The relative weightings between ease of understanding versus other factors across different financial sub-sectors; the universality of the translation layer concept across all AI deployment contexts

## Next Checks
1. **Geographic Replication Test**: Conduct interviews with finance professionals in regulatory environments outside the EU (e.g., US, Singapore, or emerging markets) to test if the "compliance as hygiene factor" finding holds when regulatory frameworks differ
2. **Technical Translation Validation**: Implement A/B testing of raw XAI outputs versus narrative explanations with actual business users to measure comprehension, trust, and adoption rates
3. **Latency Constraint Mapping**: Profile explanation generation methods (SHAP, LIME, counterfactuals) to empirically determine if computational overhead breaches the operational speed thresholds identified by participants