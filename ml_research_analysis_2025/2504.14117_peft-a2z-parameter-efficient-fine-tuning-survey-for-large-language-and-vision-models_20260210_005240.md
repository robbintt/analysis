---
ver: rpa2
title: 'PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision
  Models'
arxiv_id: '2504.14117'
source_url: https://arxiv.org/abs/2504.14117
tags:
- arxiv
- language
- preprint
- fine-tuning
- peft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey comprehensively reviews Parameter-Efficient Fine-Tuning
  (PEFT) methods for adapting large-scale pre-trained models across natural language
  processing, computer vision, and multimodal learning domains. It introduces a structured
  taxonomy grouping PEFT techniques into additive, selective, reparameterized, hybrid,
  and unified frameworks, providing clear design comparisons.
---

# PEFT A2Z: Parameter-Efficient Fine-Tuning Survey for Large Language and Vision Models

## Quick Facts
- arXiv ID: 2504.14117
- Source URL: https://arxiv.org/abs/2504.14117
- Reference count: 40
- Modern PEFT approaches achieve performance close to or exceeding full fine-tuning while reducing trainable parameters by up to 95%

## Executive Summary
This survey comprehensively reviews Parameter-Efficient Fine-Tuning (PEFT) methods for adapting large-scale pre-trained models across natural language processing, computer vision, and multimodal learning domains. It introduces a structured taxonomy grouping PEFT techniques into additive, selective, reparameterized, hybrid, and unified frameworks, providing clear design comparisons. The paper evaluates representative methods such as LoRA, adapters, prompt tuning, and their variants on benchmark tasks including GLUE, commonsense reasoning, and visual recognition. Results demonstrate that modern PEFT approaches achieve performance close to or exceeding full fine-tuning while reducing trainable parameters by up to 95%, enabling scalable deployment on resource-constrained hardware. The survey also identifies open challenges and outlines future research directions to enhance efficiency, generalization, and theoretical understanding of PEFT.

## Method Summary
The paper systematically surveys PEFT methods by introducing a structured taxonomy that categorizes approaches into five main frameworks: additive (adapters), selective (bitfit), reparameterized (LoRA), hybrid, and unified methods. It evaluates representative techniques on standard benchmarks including GLUE for natural language understanding and various reasoning tasks, using pre-trained models like RoBERTaBase/Large, BLOOM 7B, GPT-J 6B, and LLaMA-2 variants. The evaluation compares parameter efficiency, performance metrics (accuracy, F1, MCC, Pearson/Spearman correlation), and computational overhead across different PEFT methods and model sizes, demonstrating that modern approaches can achieve near-full fine-tuning performance while training only 5-20% of parameters.

## Key Results
- PEFT methods achieve performance comparable to full fine-tuning while reducing trainable parameters by up to 95%
- Modern PEFT approaches like LoRA, adapters, and prompt tuning show consistent gains across NLP, computer vision, and multimodal tasks
- The survey identifies key challenges including overfitting, catastrophic forgetting, and the need for theoretical understanding of parameter sensitivity

## Why This Works (Mechanism)

### Mechanism 1: Low-Rank Subspace Adaptation
- **Claim:** Effective adaptation of large models does not require modifying the full dense parameter space; changes can be confined to a low-dimensional subspace with minimal performance loss.
- **Mechanism:** Instead of updating the full pre-trained weight matrix $W$, PEFT methods like LoRA inject trainable low-rank decomposition matrices $B$ and $A$ such that the update $\Delta W \approx BA$. The model output becomes $Wx + BAx$. This assumes the adaptation dynamics for a downstream task reside in a manifold of significantly lower dimensionality than the full parameter count.
- **Core assumption:** The pre-trained model possesses a low "intrinsic dimension" for specific downstream tasks, meaning a small subspace of parameters dictates performance shifts.
- **Evidence anchors:**
  - [abstract] "modern PEFT approaches achieve performance close to or exceeding full fine-tuning while reducing trainable parameters by up to 95%."
  - [section 5.6.1] "The foundation of reparameterized PEFT lies in low-rank decomposition, where the parameter update matrix $\Delta W \dots$ is approximated as the product of two low-rank matrices."
  - [corpus] Paper 12740 ("Parameter-Efficient Fine-Tuning for Foundation Models") supports the view that PEFT minimizes computational complexity while striving for optimal downstream performance via dimensionality reduction.
- **Break condition:** If the downstream task requires a significant domain shift or capabilities not present in the pre-trained distribution (e.g., a new language unseen during pre-training), the low-rank assumption may fail to capture the necessary complexity, leading to underfitting compared to full fine-tuning.

### Mechanism 2: Modular Residual Scaling
- **Claim:** Adapting a model can be achieved by injecting small, isolated neural modules that learn task-specific features without disrupting the frozen pre-trained representations.
- **Mechanism:** Additive methods (Adapters) insert bottleneck layers into the transformer blocks. These modules process the input $h_{in}$ and output a scaled residual: $h_{out} = h_{in} + \alpha h_{adapter}$. The bottleneck design (projection down $\rightarrow$ non-linearity $\rightarrow$ projection up) ensures parameter efficiency, while the residual connection preserves the original feature hierarchy.
- **Core assumption:** Task-specific knowledge is distinct and can be compressed into a compact representation that supplements the general pre-trained features without overwriting them.
- **Evidence anchors:**
  - [abstract] "PEFT has emerged as a promising solution that allows adapting large models... by updating only a small portion of parameters."
  - [section 5.1.1] Describes serial adapters where "task-specific features are added without disrupting the foundational knowledge... The final output is computed as: $h_{out} = h_{in} + h_{up}$."
  - [corpus] Paper 7196 ("Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models") discusses trade-offs in PVMs, implicitly supporting the mechanism of modular adaptation for scalability.
- **Break condition:** If the adapter bottleneck size is too small (limiting capacity) or if the specific architecture placement (serial vs. parallel) blocks gradient flow to the frozen backbone (though gradients are typically blocked to backbone), the model may fail to adapt to complex tasks.

### Mechanism 3: Implicit Regularization via Constrained Optimization
- **Claim:** Limiting the number of trainable parameters acts as a form of regularization that mitigates catastrophic forgetting and overfitting, particularly in low-data regimes.
- **Mechanism:** By freezing the vast majority of parameters ($\theta_f$) and optimizing only a subset ($\theta_s$), the optimization space is drastically constrained. This prevents the model from drifting too far from the pre-trained initialization in the high-dimensional space, preserving general knowledge while fitting the specific task.
- **Core assumption:** The pre-trained weights are already near a good solution for the downstream task, and significant deviation (allowed by full fine-tuning) is unnecessary or harmful.
- **Evidence anchors:**
  - [abstract] "highlight key issues, such as overfitting, catastrophic forgetting, and parameter inefficiency."
  - [section 3.11] "PEFT methods address overfitting by updating a small, structured subset of parameters... This implicit regularization reduces variance without adding significant bias."
  - [corpus] Paper 98883 ("Parameter-Efficient Continual Fine-Tuning") supports the link between PEFT and mitigating forgetting (continual learning challenges).
- **Break condition:** In high-data regimes where extensive domain adaptation is required, this constraint might limit the model's ability to learn new patterns, causing it to underperform relative to full fine-tuning.

## Foundational Learning
- **Concept: Transformer Architecture (Self-Attention & FFN)**
  - **Why needed here:** The paper details how PEFT modules (adapters, LoRA) are specifically inserted into Attention ($Q, K, V$) and Feed-Forward Network (FFN) layers. Understanding the data flow (residuals, layer norm) is required to grasp where and how parameters are frozen or modified.
  - **Quick check question:** Can you identify where the "down-projection" and "up-projection" occur in a Serial Adapter relative to the Multi-Head Self-Attention block?
- **Concept: Intrinsic Dimensionality**
  - **Why needed here:** This is the theoretical justification for why LoRA works (Section 5.6.1). It posits that the "solution space" for fine-tuning is actually much smaller than the model size suggests.
  - **Quick check question:** If a model has 7 billion parameters, but the intrinsic dimension for a specific task is $r=8$, how many effective parameters are you actually optimizing in a LoRA setup?
- **Concept: Overfitting vs. Catastrophic Forgetting**
  - **Why needed here:** These are the two primary failure modes PEFT is designed to solve (Section 3.7 & 3.11). Differentiating them is key: Overfitting happens with small data; Forgetting happens when updating large models on new tasks.
  - **Quick check question:** Does updating all parameters in a 7B model with 1,000 samples likely cause overfitting, forgetting, or both?

## Architecture Onboarding
- **Component map:**
  - Frozen Backbone: The pre-trained Transformer (LLM/VLM) where weights are immutable
  - PEFT Wrapper/Modules: Injected layers (Adapters) or Matrices (LoRA $A, B$) wrapping the Linear layers
  - Gating/Scaling: Mechanisms (like $\alpha$ in parallel adapters) to control the influence of the new weights
- **Critical path:**
  1. **Initialization:** Clone pre-trained weights (frozen). Initialize PEFT modules (e.g., LoRA $A$ with Gaussian noise, $B$ with zeros)
  2. **Forward Pass:** Input $x$ passes through frozen $W$ and trainable PEFT path $f(x)$. Results are merged ($Wx + \Delta Wx$)
  3. **Backward Pass:** Gradients flow through PEFT modules. Frozen backbone accumulates no gradients (saves memory)
  4. **Merge/Export:** For inference, PEFT weights can be merged ($W_{new} = W + \Delta W$) to remove architectural overhead
- **Design tradeoffs:**
  - **LoRA:** Best for minimizing memory; merges into weights for zero-latency inference. *Risk:* Underfitting if rank is too low
  - **Adapters:** Good for modularity (swapping adapters per task). *Risk:* Introduces inference latency (extra layers)
  - **Prompt Tuning:** Least parameters, but consumes context window length
- **Failure signatures:**
  - **Underfitting:** PEFT performance lags significantly behind Full FT. *Fix:* Increase rank ($r$) or adapter size; check if the target modules include all critical layers (e.g., $q\_proj$, $v\_proj$)
  - **Training Instability:** Loss spikes. *Fix:* Check initialization (LoRA $B$ should be 0); lower learning rate (PEFT usually requires higher LR than pre-training but lower than scratch)
- **First 3 experiments:**
  1. **LoRA Rank Search:** Run a sweep on a standard benchmark (e.g., GLUE/RTE) with Rank $r \in \{2, 4, 8, 16\}$ to identify the "knee" of the performance curve
  2. **Target Module Ablation:** Compare training only Attention weights ($W_q, W_v$) vs. training both Attention and FFN ($W_{up}, W_{down}$)
  3. **Inference Latency Check:** Measure tokens/sec for Frozen + LoRA (merged) vs. Frozen + Adapter (unmerged) to quantify the latency overhead of non-merged methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can theoretical frameworks be developed to analytically quantify the influence of small trainable parameter subsets on overall model adaptation?
- Basis in paper: [explicit] Section 11.1 states that most PEFT methods rely on empirical success rather than analytical rigor, lacking "principled understanding of parameter sensitivity."
- Why unresolved: Current designs are driven by heuristics, resulting in inconsistent performance across different tasks and architectures without a fundamental mathematical explanation for efficiency.
- Evidence: Mathematical proofs or information-theoretic metrics (e.g., mutual information between adapted modules and outputs) that predict adaptation success rates before training.

### Open Question 2
- Question: How can layer-wise sensitivity analysis be utilized to determine optimal, non-uniform placement strategies for adapter modules?
- Basis in paper: [explicit] Section 11.2 argues that uniform insertion of modules is suboptimal because "not all layers contribute equally to downstream task performance."
- Why unresolved: There is a lack of automated tools to identify layers yielding the highest performance-to-parameter ratio or to dynamically activate modules based on input complexity.
- Evidence: Comparative studies showing that sensitivity-guided placement (using Fisher Information or Jacobian analysis) outperforms standard uniform insertion on benchmarks like GLUE or VTAB.

### Open Question 3
- Question: What specific PEFT strategies are required to adapt non-transformer architectures, such as Graph Neural Networks (GNNs), without disrupting structural integrity?
- Basis in paper: [explicit] Section 11.4 notes that fine-tuning strategies for non-transformer backbones "remains largely uncharted" despite the prevalence of these architectures in specific domains.
- Why unresolved: Most existing PEFT techniques are explicitly designed for the self-attention mechanisms of Transformers and do not account for the unique structural constraints of other architectures.
- Evidence: Successful application of PEFT variants on GNNs or Recurrent Neural Networks (RNNs) demonstrating parameter efficiency comparable to Transformer-based LoRA or Adapters.

## Limitations
- Implementation details: The survey reports performance metrics across diverse PEFT methods but does not provide complete hyperparameter configurations (learning rates, LoRA ranks, adapter bottleneck sizes) necessary for exact reproduction
- Hardware variability: Reported parameter reductions (up to 95%) and inference speed comparisons may vary significantly across different hardware setups (GPU vs CPU inference, memory bandwidth)
- Task generalization: While results show PEFT works well on standard benchmarks, the survey does not extensively address performance degradation on tasks requiring significant domain shifts or novel capabilities

## Confidence
- **High confidence**: The theoretical framework and taxonomy of PEFT methods (LoRA, adapters, prompt tuning) are well-established and consistently validated across the literature
- **Medium confidence**: Empirical results showing 95% parameter reduction with minimal performance loss are supported by multiple studies, but exact figures depend on unreported hyperparameters
- **Medium confidence**: Claims about PEFT mitigating overfitting and catastrophic forgetting are supported by theoretical reasoning, though real-world effectiveness varies by data regime and task complexity

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically vary LoRA ranks (r âˆˆ {2, 4, 8, 16}) and adapter bottleneck sizes on a standard benchmark (GLUE RTE) to identify performance thresholds and confirm reported parameter counts
2. **Cross-Domain Transfer**: Test PEFT methods on a dataset significantly different from pre-training distribution (e.g., biomedical text with clinicalBERT) to evaluate low-rank subspace adaptation limitations
3. **Memory-Footprint Verification**: Measure actual GPU memory usage during training for Frozen+LoRA (merged) vs Frozen+Adapter (unmerged) configurations to validate claimed 95% parameter reduction efficiency gains