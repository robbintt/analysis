---
ver: rpa2
title: Replication and Exploration of Generative Retrieval over Dynamic Corpora
arxiv_id: '2504.17519'
source_url: https://arxiv.org/abs/2504.17519
tags:
- retrieval
- documents
- docids
- docid
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the performance of generative retrieval (GR)
  methods in dynamic corpora, where new documents are continuously added. The authors
  systematically evaluate existing GR models, finding that models using text-based
  docids generalize better to unseen documents than those using numeric-based docids,
  which tend to overfit to the initial corpus.
---

# Replication and Exploration of Generative Retrieval over Dynamic Corpora

## Quick Facts
- arXiv ID: 2504.17519
- Source URL: https://arxiv.org/abs/2504.17519
- Reference count: 39
- Key outcome: Text-based docids generalize better to unseen documents than numeric-based docids in dynamic corpora; MDGR achieves competitive performance balancing efficiency and generalization

## Executive Summary
This paper investigates how generative retrieval (GR) models perform when new documents are continuously added to a corpus. The authors systematically compare different docid designs, finding that text-based identifiers (n-grams, titles, URLs) significantly outperform numeric clustering/quantization approaches in dynamic settings. They identify three key advantages of text-based docids—semantic alignment with pretrained language models, finer granularity, and higher lexical diversity—that enable better generalization to unseen documents. Based on these insights, they propose MDGR, a multi-docid approach that combines the efficiency of numeric methods with the generalization of text-based designs.

## Method Summary
The study evaluates GR models in dynamic corpus scenarios where documents are incrementally added. The MDGR framework uses T5-base backbone with three training objectives: synthetic queries→docids, document chunks→docids, and real queries→docids. Documents are chunked using sliding windows (256 tokens, stride 128), and docids are generated through product quantization (size=1024, length=4). The method employs constrained docid expansion where new documents reuse docids from the initial corpus, and uses a scoring function combining coverage count and beam search ranking. Experiments use NQ and MS-MARCO datasets split into initial corpus (50%) and five incremental sets (10% each).

## Key Results
- Text-based docids (n-gram) achieve Hit@10 ~0.73 on new documents vs ~0.32 for numeric docids
- Numeric-based docids exhibit significant retrieval bias toward initial documents (IDBI scores >0.5)
- MDGR balances efficiency and generalization with Hit@10 ~0.63 on new documents
- Removing constrained docid expansion causes MDGR performance to drop from 0.633 to 0.356 on D₅

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Text-based docids generalize better to unseen documents because they maintain semantic alignment with the language model's pretraining distribution.
- Mechanism: N-grams, titles, and URLs are composed of tokens that appear in the model's pretraining corpus. When generating docids for new documents, the model can leverage familiar token distributions rather than needing to map to an arbitrary numeric space that was only seen during fine-tuning on the initial corpus.
- Core assumption: The language model's pretraining distribution provides transferable semantic knowledge that aids generation of docids for documents not seen during GR training.
- Evidence anchors:
  - [abstract] "We identify three critical advantages of text-based docids in dynamic corpora: (i) Semantic alignment with language models' pretrained knowledge..."
  - [section 5.1] "Text-based docids inherently preserve distributional alignment with the underlying language model... In contrast, numeric-based docids... form a distinct symbolic system that lacks grounding in the model's pretraining data."
  - [corpus] Related work on few-shot indexing (arXiv:2408.02152) suggests leveraging pretrained knowledge reduces training costs, providing indirect support but not direct validation of this specific alignment mechanism.
- Break condition: If text-based docids show no performance advantage over numeric docids on corpora whose vocabulary distribution diverges significantly from pretraining data, the semantic alignment hypothesis would be weakened.

### Mechanism 2
- Claim: Finer-grained docid designs enable better generalization by allowing more flexible query-document matching at sub-document levels.
- Mechanism: N-gram docids can represent any substring within a document, creating multiple entry points for retrieval. This contrasts with document-level identifiers (single title/URL) that require the model to learn abstract semantic mappings. Chunk-level granularity in the proposed MDGR similarly allows partial semantic matching.
- Core assumption: Query-document relevance can be captured through partial content overlap, and models can learn to generate relevant substrings without memorizing full document representations.
- Evidence anchors:
  - [section 5.2] "Figure 2 demonstrates that the n-gram docid approach achieves the highest Hit@10 performance, significantly outperforming the document-level (title), paragraph-level, and sentence-level baselines."
  - [section 6] "removing the constrained docid expansion strategy leads to significant deterioration... when disabling multi-docid indexing (single-docid per document), the performance drops significantly"
  - [corpus] CAT-ID² (arXiv:2511.01461) explores hierarchical semantic identifiers, supporting granularity importance, but does not isolate the mechanism in dynamic settings.
- Break condition: If fine-grained docids underperform coarse-grained designs on corpora where documents are highly homogeneous (minimal internal variation), the granularity advantage may be corpus-dependent.

### Mechanism 3
- Claim: High lexical diversity in docid design reduces overfitting to initial document sets by forcing the model to learn generative patterns rather than memorize specific mappings.
- Mechanism: N-gram docids utilize effective vocabulary sizes 10-100× larger than title/URL docids and 100-1000× larger than numeric docids. This expanded space ensures individual docids occupy small fractions of capacity, encouraging the model to learn substring generation patterns applicable to novel combinations.
- Core assumption: Models learn different generalization strategies depending on the sparsity and diversity of the identifier space they are trained on.
- Evidence anchors:
  - [section 5.2, Table 5] "n-gram docids use effective vocabulary sizes that are 10-100× larger than Title or URL docids and 100-1000× larger than numeric-based docids."
  - [section 5.1, Figure 1] "numeric-based docids (e.g., GenRET, Ultron-PQ) exhibit significant retrieval bias toward initial documents... While text-based docids demonstrate better semantic alignment, they still exhibit retrieval bias, particularly for titles and URLs."
  - [corpus] No direct corpus evidence addresses lexical diversity as an isolated mechanism; related work focuses on architecture rather than vocabulary design.
- Break condition: If increasing numeric docid vocabulary size alone (without other design changes) fails to reduce initial-document bias, then lexical diversity alone is insufficient.

## Foundational Learning

- Concept: **Generative Retrieval (GR) paradigm**
  - Why needed here: The paper assumes familiarity with GR's core idea—encoding corpus information into model parameters and autoregressively generating docids. Without this, the comparison between numeric vs. text-based approaches and the dynamic corpus challenge will be unclear.
  - Quick check question: Can you explain why GR differs fundamentally from "index-then-retrieve" pipelines like BM25 or dense retrieval?

- Concept: **Docid design tradeoffs**
  - Why needed here: The entire paper centers on how different docid representations (numeric clustering/quantization vs. text-based n-grams/titles/URLs) affect generalization. Understanding what docids are and why their design matters is prerequisite.
  - Quick check question: What are two methods for generating numeric docids, and what is one text-based alternative?

- Concept: **Constrained decoding with prefix trees**
  - Why needed here: The paper mentions prefix trees (T_D) that enforce valid docid generation. Understanding constrained decoding is necessary to grasp how new documents are indexed and retrieved without retraining.
  - Quick check question: Why must GR models restrict token generation at each decoding step during inference?

## Architecture Onboarding

- Component map: Documents -> Indexer -> Prefix Tree (T_D) <- Retriever (T5-base) <- Constrained Beam Search -> Documents

- Critical path:
  1. Train GR model on initial corpus D_0 with query-docid pairs
  2. When new documents D_new arrive, indexer generates docids using frozen encoder/quantizer (numeric) or extracts text identifiers (text-based)
  3. Update prefix tree T_D with new valid docid prefixes
  4. At inference, constrained beam search generates candidate docids; retrieve documents containing those docids

- Design tradeoffs:
  - **Numeric docids**: High efficiency, compact storage, fast decoding (~216-269ms latency) but severe generalization drop on new documents (Hit@10 can fall from 0.87 to 0.32)
  - **Text-based docids (n-gram)**: Strong generalization (Hit@10 ~0.73 on new docs) but higher memory (2200MB vs 865MB) and slower inference (~619-778ms)
  - **MDGR**: Attempts to balance both—moderate memory (886MB), moderate latency (241-320ms), competitive generalization (Hit@10 ~0.63 on new docs)

- Failure signatures:
  - Numeric docids generate docids only seen during initial training (high IDBI scores indicate bias toward D_0)
  - URL/title docids fail on new documents with novel metadata patterns (Hit@10 drops from 0.76 to 0.17 for URLs)
  - Excessive docid vocabulary size (k=8192) creates sparse mappings, degrading all performance

- First 3 experiments:
  1. **Replicate the forgetting metric (F_n)** on your target corpus: Train on D_0, incrementally add D_1 through D_5, measure Hit@10 degradation on original queries. Confirm whether your numeric docid setup exhibits the reported bias.
  2. **Ablate docid granularity**: Compare document-level (title), paragraph-level, sentence-level, and n-gram docids on your domain. Identify which granularity suits your document structure.
  3. **Pilot MDGR with constrained docid expansion**: Implement the frozen-codebook constraint for new documents and measure whether generalization improves without retraining. Verify the inference scoring function's sensitivity to β.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid docid strategies or pretraining-enhanced generalization mechanisms be developed to fully bridge the performance gap between static and dynamic retrieval without sacrificing efficiency?
- Basis in paper: [explicit] The conclusion explicitly states, "Future work may explore hybrid docid strategies or pretraining-enhanced generalization mechanisms to further bridge the gap between static and dynamic retrieval performance."
- Why unresolved: While the proposed MDGR combines numeric efficiency with text-based generalization, it still does not match the absolute best performance of pure text-based methods in all dynamic scenarios.
- What evidence would resolve it: A novel hybrid model that maintains the low latency/memory of numeric methods while matching or exceeding the Hit@10 of text-based methods like SEAL on dynamic benchmarks.

### Open Question 2
- Question: How does the semantic alignment and lexical diversity of docids interact with parameter updates (continual learning) to affect the "initial document bias" in dynamic corpora?
- Basis in paper: [inferred] The paper focuses on "Index adaptation" (frozen parameters) to avoid computational costs but notes that "Model adaptation" (retraining/continual learning) is a distinct paradigm. The interaction between the identified docid attributes and parameter updates is not tested.
- Why unresolved: The study isolates the impact of docid design on frozen models; it remains unclear if text-based docids still outperform numeric ones when the model is allowed to fine-tune on new documents.
- What evidence would resolve it: Experiments evaluating the IDBI (Initial Document Bias Index) of both numeric and text-based models under a continual learning regime (e.g., using DSI++ adapters).

### Open Question 3
- Question: Is the "constrained docid expansion" strategy optimal for numeric-based identifiers, or does it limit the representational capacity necessary for future corpus shifts?
- Basis in paper: [inferred] The proposed MDGR framework uses a constrained expansion where new documents must reuse docids from the initial set to maintain semantic familiarity. This heuristic prevents the generation of "unfamiliar" numeric sequences but may constrain the index.
- Why unresolved: The paper demonstrates this constraint improves performance over unconstrained numeric indexing, but does not explore if this creates a ceiling for representing semantically novel documents that diverge from the initial cluster centroids.
- What evidence would resolve it: An analysis of retrieval accuracy on documents that are semantically distant from the initial corpus $D_0$, comparing constrained expansion against dynamic codebook updating methods.

## Limitations

- The analysis relies on synthetic document additions rather than real-world continuous streams, potentially missing temporal dynamics
- Memory and latency comparisons conflate multiple design choices, making it difficult to isolate which factors drive efficiency differences
- MDGR's substantial implementation complexity raises questions about practical deployment overhead

## Confidence

- **High confidence**: The superiority of text-based docids over numeric docids for generalization to new documents is well-supported by multiple ablation studies and consistent across both NQ and MS-MARCO datasets
- **Medium confidence**: The three identified advantages of text-based docids are logically coherent and partially validated, but the mechanisms are not fully isolated
- **Low confidence**: The exact contribution of each proposed mechanism to MDGR's performance is unclear, as the design combines multiple innovations simultaneously

## Next Checks

1. **Temporal validation**: Test the dynamic corpus scenario using chronologically ordered document additions from real-world datasets (e.g., Wikipedia edit history or news corpora) rather than random splits to verify that the observed generalization patterns hold under temporal dynamics

2. **Domain generalization**: Evaluate all docid designs across diverse domains (scientific literature, e-commerce products, social media) to determine whether the identified advantages of text-based docids are domain-agnostic or corpus-specific

3. **Mechanism isolation**: Conduct controlled experiments that vary only one design parameter at a time (e.g., vocabulary size, granularity level, semantic alignment through pretraining distribution) while keeping others fixed to quantify the individual contribution of each proposed mechanism