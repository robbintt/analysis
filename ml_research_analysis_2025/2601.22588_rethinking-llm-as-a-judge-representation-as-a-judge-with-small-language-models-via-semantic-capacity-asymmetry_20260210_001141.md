---
ver: rpa2
title: 'Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models
  via Semantic Capacity Asymmetry'
arxiv_id: '2601.22588'
source_url: https://arxiv.org/abs/2601.22588
tags:
- probing
- evaluation
- arxiv
- score
- small
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a new paradigm, Representation-as-a-Judge, to
  address the high cost and opacity of large language model (LLM) evaluation. The
  key insight is that small language models (LMs) encode evaluative signals in their
  internal representations even when their generative outputs are weak, a phenomenon
  formalized as the Semantic Capacity Asymmetry Hypothesis.
---

# Rethinking LLM-as-a-Judge: Representation-as-a-Judge with Small Language Models via Semantic Capacity Asymmetry

## Quick Facts
- arXiv ID: 2601.22588
- Source URL: https://arxiv.org/abs/2601.22588
- Reference count: 40
- Primary result: Probing-based framework achieves comparable performance to LLM-as-a-Judge while being significantly more efficient

## Executive Summary
This paper introduces Representation-as-a-Judge, a paradigm that leverages small language models' internal representations to predict evaluation scores, addressing the high cost and opacity of traditional LLM-as-a-Judge approaches. The key insight is the Semantic Capacity Asymmetry Hypothesis: small LMs encode rich evaluative signals in their hidden states despite weak generative ability. The proposed INSPECTOR framework uses linear probes on these representations to predict evaluation scores, achieving performance comparable to full LLM judges while being more efficient and interpretable.

## Method Summary
The framework trains small language models to encode evaluative signals in their representations, then uses probing classifiers to predict evaluation scores from these hidden states. Specifically, M_large generates gold-standard scores for M_med responses, which are then used to train probes on M_small representations. The approach leverages the Semantic Capacity Asymmetry Hypothesis, which posits that small LMs encode sufficient evaluative information in intermediate layers for assessment tasks, even when their generative outputs are unreliable.

## Key Results
- INSPECTOR achieves 80-90% F1 for binary classification and 50-60% F1 for multiclass classification on reasoning benchmarks
- Probing performance closely approximates full LLM-as-a-Judge evaluation while being significantly more efficient
- Probing classifiers effectively filter high-quality data for supervised fine-tuning, achieving performance comparable to LLM-based filtering

## Why This Works (Mechanism)

### Mechanism 1: Evaluative Signals Reside in Intermediate Representations
Small language models encode sufficient evaluative information in their hidden states to support quality assessment, even when their generative outputs are unreliable. The model's internal representations compress semantic features needed for evaluation (e.g., logical coherence, factual accuracy) into structured subspaces accessible via linear probing, bypassing the higher-capacity demands of autoregressive decoding.

### Mechanism 2: Layer-Specific Feature Subspaces Capture Evaluative Dimensions
Different evaluation aspects (logicality, fluency, factuality) exhibit peak predictive signal at distinct layer depths. Early layers encode surface features (fluency), while deeper layers accumulate abstract semantic relationships (logicality, factuality). PCA-based projections reveal these structured subspaces more clearly than raw embeddings.

### Mechanism 3: Linear Probes Suffice Due to Signal Compression
Simple logistic regression classifiers on dimensionality-reduced embeddings achieve high fidelity because evaluative signals are already linearly separable in the representation space. The probing framework (PCA → 50 dimensions + statistics) preserves the discriminative subspace while discarding noise, allowing low-capacity classifiers to generalize from limited labeled data.

## Foundational Learning

- **Concept: Representation Probing**
  - Why needed here: The entire method depends on extracting task-relevant information from hidden states; without understanding how probes work, you cannot diagnose why they succeed or fail.
  - Quick check question: Can you explain why a linear probe's performance reflects information encoded in the representation rather than the probe's learning capacity?

- **Concept: Layer-wise Feature Evolution in Transformers**
  - Why needed here: Selecting optimal layers for probing requires understanding how semantic abstraction progresses through the network.
  - Quick check question: Which layers would you probe first for syntactic vs. semantic tasks, and why?

- **Concept: Imbalanced Multi-class Classification**
  - Why needed here: Evaluation scores (1–5) are inherently imbalanced; the paper uses downsampling but practitioners must understand alternatives (cost-sensitive learning, SMOTE).
  - Quick check question: Given a 5-class problem with class frequencies [50, 200, 400, 150, 30], what strategies could you use beyond random undersampling?

## Architecture Onboarding

- **Component map:**
M_large (DeepSeek-V3) → generates gold evaluation scores (1–5) across 5 aspects
↓
M_med (Llama-3-8B) → generates candidate responses to benchmark questions
↓
M_small (Qwen3-1.7B, Llama-3.2-1B) → frozen, forward pass on (prompt, response) pairs
↓
Feature Extractor → extracts per-layer pooled vectors + attention statistics
↓
PCA (d=50) + Scaler → dimensionality reduction (inside CV pipeline)
↓
Probe Classifier (Logistic Regression / Linear SVM) → predicts evaluation score
↓
Deployment → binary filter for data curation (threshold τ=4)

- **Critical path:**
1. Label quality is paramount: The probe can only be as good as the M_large annotations. Use multiple annotators or consensus mechanisms if possible.
2. Layer selection must be data-driven: Do not assume "deeper is better"—run cross-validated layer-wise diagnostics (as in Figure 6) before final training.
3. Binary classification is the practical output: Multiclass performance (~50–60% F1) is insufficient for most applications; use binary filtering instead.

- **Design tradeoffs:**
- Mean vs. last-token pooling: Mean pooling aggregates global information (better for coherence judgments); last-token captures instruction-following signals. Test both.
- Binary vs. multiclass targets: Binary is robust (80–90% F1) but loses granularity; multiclass preserves detail but requires more data.
- Cross-dataset transfer: Out-of-distribution probing (Appendix G) shows binary transfers reasonably (35–62% F1), multiclass does not (10–25%). Train on domain-matched data.

- **Failure signatures:**
- Probe overfits to training distribution (high train accuracy, low test accuracy) → increase regularization, reduce PCA dimensions, or gather more diverse data.
- Layer selection is unstable across CV folds → features are not sufficiently discriminative; reconsider evaluation aspect definitions.
- Binary probe predicts everything as "high quality" → class imbalance not properly addressed; verify downsampling or use class weights.

- **First 3 experiments:**
1. Layer-wise diagnostic: For each aspect, plot probing accuracy across all layers using mean-pooled PCA features. Identify peak layers before building multi-layer probes.
2. Pooling ablation: Compare mean, last, min, max, and concat pooling on a held-out validation set. Confirm mean pooling dominance on your specific task.
3. OOD generalization test: Train on one benchmark (e.g., GSM8K), test on another (e.g., MATH). If binary F1 drops below 50%, your probe is capturing dataset-specific artifacts rather than general evaluative features.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability across domains remains untested, particularly for creative or domain-specific tasks
- Label quality dependence creates vulnerability to systematic biases in M_large annotations
- Computational cost trade-offs may not favor probing for small-scale or real-time applications

## Confidence
- **High Confidence**: Semantic Capacity Asymmetry Hypothesis validity for mathematical reasoning tasks; binary classification performance (80-90% F1); layer-wise feature evolution patterns in transformer representations.
- **Medium Confidence**: Cross-dataset generalization for binary classification; PCA-based dimensionality reduction effectiveness; superiority of mean pooling over alternatives.
- **Low Confidence**: Multiclass classification reliability (50-60% F1); effectiveness on non-mathematical domains; probe robustness to adversarial or ambiguous inputs.

## Next Checks
1. **Domain Transfer Experiment**: Train probes on GSM8K, MATH, and GPQA, then evaluate on unrelated domains (e.g., medical QA, legal reasoning). Compare binary vs. multiclass performance degradation to quantify domain-specific vs. general evaluative features.

2. **Human-in-the-Loop Validation**: Select 100 high-disagreement cases where probe predictions diverge from M_large scores. Have human experts annotate these samples and calculate inter-annotator agreement to determine whether probes capture systematic M_large errors or genuine evaluative patterns.

3. **Real-time Efficiency Benchmark**: Implement both direct M_large evaluation and Representation-as-a-Judge pipelines on identical hardware. Measure end-to-end latency, memory usage, and throughput across varying batch sizes to identify break-even points where probing becomes cost-effective.