---
ver: rpa2
title: Prediction Loss Guided Decision-Focused Learning
arxiv_id: '2509.08359'
source_url: https://arxiv.org/abs/2509.08359
tags:
- loss
- pred
- optimization
- decision
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Prediction Loss Guided Decision-Focused Learning
  (PLG-DFL), a method that improves decision-focused learning by dynamically incorporating
  prediction loss gradients. The approach addresses the flat-and-sharp loss landscape
  issues in vanilla DFL by guiding the decision loss gradient using the prediction
  loss gradient, with a decaying parameter controlling their relative influence.
---

# Prediction Loss Guided Decision-Focused Learning

## Quick Facts
- arXiv ID: 2509.08359
- Source URL: https://arxiv.org/abs/2509.08359
- Authors: Haeun Jeon; Hyunglip Bae; Chanyeong Kim; Yongjae Lee; Woo Chang Kim
- Reference count: 27
- Primary result: PLG-DFL improves decision-focused learning by dynamically incorporating prediction loss gradients, achieving lower normalized test regret with more stable training across three stochastic optimization tasks.

## Executive Summary
This paper introduces Prediction Loss Guided Decision-Focused Learning (PLG-DFL), a method that improves decision-focused learning by dynamically incorporating prediction loss gradients. The approach addresses the flat-and-sharp loss landscape issues in vanilla DFL by guiding the decision loss gradient using the prediction loss gradient, with a decaying parameter controlling their relative influence. The method is universally applicable, requires no additional training, and can be combined with any DFL solver. Theoretical analysis guarantees convergence to Pareto stationary points. Experiments on three stochastic optimization problems (knapsack, budget allocation, portfolio optimization) show that PLG-DFL consistently outperforms baselines, achieving lower normalized test regret with more stable training across all tasks.

## Method Summary
PLG-DFL introduces a gradient perturbation mechanism that constructs update directions by bisecting prediction loss and decision loss gradients. The method computes unit norm directions for both gradients, then creates a merged gradient using the geometric mean of their norms. A sigmoid-like decaying parameter α = (1 + e^(t-c))^(-κ) controls the relative influence of prediction loss guidance, starting with κ=0 (pure bisection) and optionally transitioning to κ=1 (decision-focused) after inflection point c=50. This approach ensures the update direction never conflicts with the decision gradient while incorporating prediction loss stability, particularly valuable during early training when predictive models are uncalibrated.

## Key Results
- PLG-DFL achieves lower normalized test regret than baselines across all three benchmark tasks (knapsack, budget allocation, portfolio optimization)
- The method demonstrates more stable training curves compared to vanilla DFL
- Performance is consistent across different κ values (κ=0 and κ=1), with κ=0 providing theoretical convergence guarantees to Pareto stationary points
- The approach requires no additional training or architectural changes, making it universally applicable to any differentiable solver

## Why This Works (Mechanism)

### Mechanism 1: Gradient Perturbation via Angle Bisection
Bisecting the angle between prediction loss and decision loss gradients creates an update direction that never conflicts with the decision gradient while incorporating prediction loss guidance. The method computes unit norm directions u_pred and u_dec, then constructs a merged gradient g = m · (α·u_pred + u_dec) / ||α·u_pred + u_dec||, where m is the geometric mean of gradient norms. When κ=0, α=1 throughout training, ensuring the update always bisects both gradients. Core assumption: Both loss landscapes share a common desirable minimum where ŷ = y (perfect prediction).

### Mechanism 2: Adaptive Decay with Sigmoid-Like Parameter
A sigmoid-decaying weight (controlled by inflection point c and steepness κ) allows early training to benefit from stable prediction gradients while gradually shifting focus to decision quality. α = (1 + e^(t-c))^(-κ) where t is the current epoch. When κ=1, α smoothly decreases from 1 to 0 as training progresses past inflection point c, shifting the merged gradient from balanced bisection toward pure decision loss gradient. Core assumption: Early training benefits more from prediction loss stability; later training requires decision-focused refinement.

### Mechanism 3: Geometric Mean Norm Balancing
Using the geometric mean of gradient norms (m = √(||∇L_pred|| · ||∇L_dec||)) as the merged gradient magnitude prevents the larger prediction loss gradient from dominating updates. Instead of weighting by raw magnitudes (where ||∇L_pred|| is typically 10-1000x larger than ||∇L_dec||), the geometric mean provides balanced influence while preserving relative scale information. Core assumption: Both gradient directions carry useful information regardless of their original magnitudes.

## Foundational Learning

- **Concept**: Decision-Focused Learning (DFL) vs. Prediction-Focused Learning (PFL)
  - *Why needed*: PLG-DFL combines both paradigms; understanding their tradeoffs is essential for grasping why gradient perturbation helps.
  - *Quick check*: In a portfolio optimization task, would training solely on MSE of predicted returns guarantee optimal portfolio weights? Why or why not?

- **Concept**: Gradient Conflict in Multi-Task Learning
  - *Why needed*: The paper frames gradient perturbation as a multi-task learning problem; recognizing when gradients conflict (negative cosine similarity) explains the need for bisection.
  - *Quick check*: If ∇L_pred and ∇L_dec have cosine similarity of -0.8, what happens if you naively sum them with equal weights?

- **Concept**: Pareto Stationarity
  - *Why needed*: The theoretical guarantee of convergence to Pareto stationary points (not necessarily global optima) defines what success looks like mathematically.
  - *Quick check*: At a Pareto stationary point, can you find a direction that simultaneously reduces both L_pred and L_dec?

## Architecture Onboarding

- **Component map**: x → M_θ → ŷ → solver → a*(ŷ) → L_dec; x → M_θ → ŷ → L_pred; ∇L_pred and ∇L_dec → gradient merger → g → θ update

- **Critical path**:
  1. Forward pass: x → M_θ → ŷ → solver → a*(ŷ) → L_dec
  2. Backward pass: Compute ∇L_pred (standard backprop) and ∇L_dec (requires differentiable solver)
  3. Gradient merge: Compute u_pred, u_dec, m, α(t), then g
  4. Update: θ ← θ - η · g

- **Design tradeoffs**:
  - **κ=0 vs. κ=1**: κ=0 guarantees Pareto stationarity but may not fully optimize decision loss; κ=1 adapts toward decision focus but loses theoretical guarantee
  - **Inflection point c**: Earlier c → faster shift to decision loss but risk of unstable early predictions; later c → more stable initialization but slower decision optimization
  - **Solver choice**: Exact gradients (KKT) vs. approximations (perturbation) affects gradient quality and computational cost

- **Failure signatures**:
  - **Gradient magnitude collapse**: If ||∇L_dec|| → 0 (flat region), geometric mean m → 0, causing near-zero updates; monitor gradient norm ratios
  - **Consistent conflict**: If cosine similarity remains strongly negative throughout training, bisection may oscillate without progress
  - **Solver non-differentiability**: If ∂a*/∂ŷ is undefined or poorly approximated, ∇L_dec will be noisy or incorrect

- **First 3 experiments**:
  1. **Reproduce budget allocation baseline**: Implement PFL, DFL, and PLG-DFL (κ=0, κ=1) on the 0 fake targets setting; verify normalized test regret matches Table 1 (ours: 0.102±0.073 for κ=0)
  2. **Ablate inflection point c**: Test c ∈ {10, 25, 50, 100, 200} on weighted knapsack; plot training curves to identify where prediction loss stabilization ends and decision loss refinement should begin
  3. **Stress test gradient conflict**: Construct a synthetic problem where ∇L_pred and ∇L_dec have intentionally high conflict (cosine similarity < -0.5); compare PLG-DFL against PCGrad, MGDA, and DCGD to verify robustness

## Open Questions the Paper Calls Out

- **Question**: Can PLG-DFL be effectively extended to Decision-Focused Learning contexts that rely on approximate surrogate losses rather than exact decision loss gradients?
  - *Basis*: The authors state in the Conclusion that the current formulation assumes access to exact gradients and that "extending our method to [surrogate loss] contexts remains an open direction."
  - *Why unresolved*: The method currently depends on specific geometric properties of exact gradients to resolve conflicts; it is unclear if surrogate approximations preserve these properties sufficiently for the perturbation mechanism to work.
  - *What evidence would resolve it*: Empirical validation of PLG-DFL integrated with surrogate-based DFL methods (like SPO+ or NCE) showing maintained or improved performance on standard benchmarks.

- **Question**: How does PLG-DFL behave in edge cases where both the prediction loss gradient and the decision loss gradient lead to catastrophically poor results?
  - *Basis*: The Conclusion notes that while the method works when one gradient is good, "it is unclear how it behaves when both gradients lead to catastrophically poor results."
  - *Why unresolved*: The guiding mechanism assumes at least one gradient provides a useful signal for the other; if both are misleading, the bisection method might amplify errors rather than correct them.
  - *What evidence would resolve it*: Theoretical analysis or experiments on synthetic datasets specifically constructed to induce misleading gradients in both loss functions.

- **Question**: Does the theoretical guarantee of convergence to a Pareto stationary point hold for the decaying parameter setting (κ > 0), or is it restricted to the static bisection case (κ = 0)?
  - *Basis*: Theorem 3.4 explicitly provides a convergence proof for κ = 0, but the paper relies on empirical results to justify the decaying κ = 1 setting, leaving a theoretical gap for the dynamic schedule.
  - *Why unresolved*: The proof relies on the update direction bisecting the angle between gradients, a property strictly held only when κ = 0.
  - *What evidence would resolve it*: A formal extension of Theorem 3.4 that accounts for the time-varying weight α or a counter-example showing divergence for κ > 0.

## Limitations
- Effectiveness depends critically on prediction and decision loss gradients sharing a common optimum; high conflict (cosine similarity < -0.5) may limit benefits
- Choice of inflection point c = 50 is heuristic and may not generalize across different problem scales or optimization complexities
- Geometric mean norm balancing could cause gradient vanishing when either loss landscape becomes flat

## Confidence
- **High Confidence**: The theoretical guarantee of Pareto stationary point convergence (Theorem 1) and the empirical performance improvements across all three benchmark tasks
- **Medium Confidence**: The specific choice of sigmoid decay parameters (c = 50, κ ∈ {0, 1}) and their claimed optimal values, as these were not systematically validated across diverse problem instances
- **Low Confidence**: The universality claim that PLG-DFL works "with any differentiable solver" - the method's performance may vary significantly depending on solver quality and the accuracy of computed gradients

## Next Checks
1. **Conflict Stress Test**: Construct synthetic problems with intentionally high gradient conflict (cosine similarity < -0.5) and compare PLG-DFL against gradient conflict resolution methods like PCGrad, MGDA, and DCGD to quantify robustness

2. **Parameter Sensitivity Analysis**: Systematically vary inflection point c across {10, 25, 50, 100, 200} and κ across {0, 0.5, 1, 2} on weighted knapsack to identify optimal parameter ranges and their relationship to problem complexity

3. **Solver Dependency Study**: Compare PLG-DFL performance using different differentiable solvers (KKT vs. perturbation vs. cvxpylayers) on the same benchmark tasks to assess sensitivity to solver quality and gradient approximation accuracy