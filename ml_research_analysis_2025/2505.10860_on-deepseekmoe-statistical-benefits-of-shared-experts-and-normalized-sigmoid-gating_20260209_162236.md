---
ver: rpa2
title: 'On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid
  Gating'
arxiv_id: '2505.10860'
source_url: https://arxiv.org/abs/2505.10860
tags:
- experts
- expert
- gating
- have
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive theoretical study of DeepSeekMoE,
  focusing on two unique features: the shared expert strategy and normalized sigmoid
  gating. The authors analyze expert estimation convergence rates, revealing that
  shared experts in DeepSeekMoE exhibit significantly faster convergence compared
  to routed experts and standard MoE models.'
---

# On DeepSeekMoE: Statistical Benefits of Shared Experts and Normalized Sigmoid Gating

## Quick Facts
- arXiv ID: 2505.10860
- Source URL: https://arxiv.org/abs/2505.10860
- Authors: Huy Nguyen; Thong T. Doan; Quang Pham; Nghi D. Q. Bui; Nhat Ho; Alessandro Rinaldo
- Reference count: 40
- Primary result: Shared experts and normalized sigmoid gating in DeepSeekMoE provide faster convergence and better sample efficiency than standard MoE architectures

## Executive Summary
This paper presents a comprehensive theoretical and empirical analysis of DeepSeekMoE, focusing on two key innovations: shared experts that are always active and normalized sigmoid gating. The authors prove that shared experts achieve significantly faster convergence rates (O(ε⁻⁴)) compared to routed experts, while normalized sigmoid gating provides superior sample efficiency over traditional softmax gating. These theoretical findings are validated through extensive experiments on synthetic regression tasks, language modeling, and vision-language modeling, demonstrating improved convergence speed, routing stability, and overall model performance.

## Method Summary
The paper analyzes DeepSeekMoE's architecture theoretically, proving faster convergence rates for shared experts and normalized sigmoid gating compared to standard MoE components. The theoretical analysis uses Gaussian DeepSeekMoE models and examines parameter estimation through convergence rates. Empirical validation includes synthetic regression experiments with EM algorithms, language modeling on SlimPajama datasets, and vision-language modeling on LLaVA-1.5. The implementation requires custom EM updates for gating parameters and careful attention to router stability metrics like Jain's Fairness Index and Router Change Rate.

## Key Results
- Shared experts achieve O(ε⁻⁴) sample complexity, significantly faster than routed experts' O(ε⁻ʳ₂,ⱼ) rates
- Normalized sigmoid gating demonstrates O(n⁻¹/²) parametric convergence rates, superior to softmax's polynomial equation solvability limitations
- Router analysis shows normalized sigmoid gating improves stability with lower change rates and higher fairness indices

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Shared experts significantly reduce sample complexity for learning common knowledge compared to routed experts.
- **Mechanism:** By decoupling universal knowledge into always-active experts, the model avoids slow convergence rates associated with polynomial equation solvability in standard MoE parameter estimation.
- **Core assumption:** Input data contains distinct common features benefiting from dense processing and specialized features benefiting from sparse processing.
- **Evidence anchors:** [abstract] "Shared experts... exhibit significantly faster convergence"; [section 3.2] "Sample complexity... O(ε⁻⁴) for shared experts vs O(ε⁻ʳ₂,ⱼ) for routed experts."
- **Break condition:** If data has no shared structure, fixed capacity for shared experts becomes wasted compute.

### Mechanism 2
- **Claim:** Normalized sigmoid gating improves sample efficiency of routed experts over softmax gating.
- **Mechanism:** Softmax creates inter-dependent expert weights complicating gradient flow, while sigmoid gating scores experts independently before normalization, decoupling parameters and enabling O(n⁻¹/²) convergence.
- **Core assumption:** The "dense regime" applies where gating weights are not input-independent.
- **Evidence anchors:** [abstract] "Normalized sigmoid gating demonstrates superior sample efficiency"; [section 4.2] "Convergence rates... O(n⁻¹/²)... substantially faster than softmax."
- **Break condition:** If over-specified gating parameters vanish (sparse regime), sigmoid offers no clear advantage.

### Mechanism 3
- **Claim:** Combination of shared experts and sigmoid gating stabilizes routing decisions and improves expert utilization.
- **Mechanism:** Shared experts absorb common variance reducing router burden, while sigmoid gating lowers router change rates promoting consistent expert specialization.
- **Core assumption:** Stable routing correlates with better convergence and performance.
- **Evidence anchors:** [section 5.4.2] "Models employing normalized sigmoid gating have significantly lower change rates"; [section 5.4.3] "Models... maintain higher fairness index."
- **Break condition:** In non-stationary environments requiring constant routing adaptation, this stability might hinder adaptation speed.

## Foundational Learning

### Concept: Sample Complexity (O(n⁻¹/²) vs O(n⁻¹/⁴))
- **Why needed here:** Core contribution proves speedup; understanding exponential difference in data requirements is key to valuing architectural changes.
- **Quick check question:** Does faster convergence rate mean model learns faster or requires fewer samples for same error?

### Concept: Identifiability in Mixture Models
- **Why needed here:** Theoretical bottlenecks hinge on "strong identifiability" (GELU/FFN) vs "weak identifiability" (linear experts), explaining specific activation function choices.
- **Quick check question:** Why do linear experts violate "strong identifiability" condition used to prove fast convergence?

### Concept: Softmax vs. Normalized Sigmoid
- **Why needed here:** Understanding mathematical difference (sum-to-one constraint vs independent scoring) is crucial for correct gating function implementation.
- **Quick check question:** In normalized sigmoid gating, does Expert A's score depend directly on Expert B's input embedding?

## Architecture Onboarding

### Component map:
Input embedding x → Shared Expert Path (FFN_shared(x), always active) + Router computes logits z = Wx → Normalized Sigmoid scores s = sigmoid(z) / sum(sigmoid(z)) → Top-K selection I → Output: y = y_shared + sum(s_i · FFN_i(x) for i in I)

### Critical path:
Implementation of Normalized Sigmoid Router requires replacing standard softmax with sigmoid followed by normalization over selected experts.

### Design tradeoffs:
- Shared vs. Routed Ratio: Increasing shared experts improves generalization/convergence speed but reduces specialized capacity for rare tokens
- Sigmoid vs. Softmax: Sigmoid improves sample efficiency but requires careful implementation for numerical stability and proper normalization

### Failure signatures:
- Router Saturation (Low Fairness): Jain's Fairness Index drops in later layers, routed experts collapsing
- High Router Change Rate: Router fluctuates wildly between steps, sigmoid gating misconfigured or learning rate too high

### First 3 experiments:
1. Synthetic Convergence Test: Train small MoE on synthetic data with Softmax vs Sigmoid, plot loss to verify theoretical O(n⁻¹/²) vs O(n⁻¹/⁴) gap
2. Router Saturation Analysis: Log "Router Saturation" metric during training of standard model vs DeepSeekMoE to see if sigmoid gates saturate earlier
3. Ablation Study: Remove shared experts from DeepSeekMoE architecture to quantify isolated performance drop on downstream tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How many shared experts should be employed to achieve optimal performance given fixed computational budget?
- Basis in paper: [explicit] Discussion section identifies this as "open problem of model selection," noting tradeoff between generalization and redundancy
- Why unresolved: Paper proves shared experts improve sample efficiency but doesn't determine optimal quantity; too many risks losing sparsity benefits
- What evidence would resolve it: Scaling law involving data heterogeneity, model scale, and routing sparsity through extensive experimentation

### Open Question 2
- Question: Do theoretical convergence benefits persist for discrete distributions used in language modeling?
- Basis in paper: [inferred] Sections 3 and 4 use "Gaussian DeepSeekMoE model" with continuous conditional densities, while LLMs operate on discrete token distributions
- Why unresolved: Convergence proofs rely on Gaussian likelihood properties and Taylor expansions specific to continuous density functions
- What evidence would resolve it: Theoretical extension to discrete mixture models or empirical validation confirming rate alignment with Gaussian analysis

### Open Question 3
- Question: How does domain heterogeneity in training data influence convergence rates of shared versus routed experts?
- Basis in paper: [inferred] Discussion mentions optimal configuration depends on "data heterogeneity," yet theoretical results assume i.i.d. samples
- Why unresolved: Current analysis doesn't account for how data distribution shifts alter sample efficiency or expert identifiability
- What evidence would resolve it: Theoretical bounds or experiments measuring estimation error on datasets with controlled domain shifts

## Limitations

- Theoretical analysis assumes specific expert function families (GELU/FFN) and idealized conditions that may not fully translate to practical deep networks
- EM algorithm for synthetic experiments requires custom numerical solutions for gating parameters without closed-form updates, introducing implementation-dependent variability
- Paper doesn't fully address tradeoffs in dense regime when routing decisions must frequently adapt to non-stationary distributions

## Confidence

- **High Confidence:** Theoretical sample complexity bounds for shared experts (O(ε⁻⁴) vs O(ε⁻ʳ₂,ⱼ) for routed experts) are well-established under stated assumptions; synthetic experiments provide clear empirical validation
- **Medium Confidence:** Sample efficiency improvements from normalized sigmoid gating over softmax are theoretically sound but depend on specific data distribution and implementation details
- **Medium Confidence:** Routing stability improvements from combined architecture are demonstrated empirically, but causal relationship to downstream performance could benefit from additional ablation studies

## Next Checks

1. Implementation Robustness Test: Reproduce EM algorithm convergence on synthetic data with varying initialization schemes and verify theoretical O(n⁻⁰·⁵⁵) rate is consistently observed across different random seeds

2. Real-world Scalability Analysis: Evaluate DeepSeekMoE architecture on larger-scale language modeling tasks (beyond 679M parameters) to verify convergence and stability benefits scale proportionally with model size

3. Non-stationary Distribution Test: Assess model performance when input data distribution shifts over time, measuring how quickly routing decisions adapt and whether stability benefits become detrimental in dynamic environments