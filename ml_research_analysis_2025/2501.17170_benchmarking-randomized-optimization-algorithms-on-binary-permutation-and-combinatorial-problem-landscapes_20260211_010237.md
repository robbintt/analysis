---
ver: rpa2
title: Benchmarking Randomized Optimization Algorithms on Binary, Permutation, and
  Combinatorial Problem Landscapes
arxiv_id: '2501.17170'
source_url: https://arxiv.org/abs/2501.17170
tags:
- mimic
- fitness
- problem
- optimization
- population
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically evaluates four randomized optimization\
  \ algorithms\u2014Randomized Hill Climbing (RHC), Simulated Annealing (SA), Genetic\
  \ Algorithms (GA), and MIMIC\u2014across binary, permutation, and combinatorial\
  \ problems. Using benchmark fitness functions, the performance of each algorithm\
  \ was analyzed based on solution quality, convergence speed, computational cost,\
  \ and robustness."
---

# Benchmarking Randomized Optimization Algorithms on Binary, Permutation, and Combinatorial Problem Landscapes

## Quick Facts
- arXiv ID: 2501.17170
- Source URL: https://arxiv.org/abs/2501.17170
- Reference count: 40
- Primary result: GA offers balanced performance across problem types, while MIMIC achieves highest solution quality at significantly higher computational cost.

## Executive Summary
This study systematically evaluates four randomized optimization algorithms—Randomized Hill Climbing (RHC), Simulated Annealing (SA), Genetic Algorithms (GA), and MIMIC—across binary, permutation, and combinatorial problems. Using benchmark fitness functions, the performance of each algorithm was analyzed based on solution quality, convergence speed, computational cost, and robustness. Results show that GA and MIMIC excel in producing high-quality solutions, with MIMIC achieving the highest fitness scores but at a significantly higher computational cost, particularly in permutation and combinatorial problems. GA offers a balanced approach, delivering strong performance with moderate computational overhead, making it suitable for scenarios where a compromise between solution quality and time efficiency is needed. RHC and SA struggled with solution quality and efficiency, particularly in complex problem landscapes, with RHC being the least effective due to its inability to escape local optima.

## Method Summary
The study evaluates RHC, SA, GA, and MIMIC on benchmark problems from the `mlrose` Python package, including binary problems (OneMax, FlipFlop, FourPeaks, SixPeaks, ContinuousPeaks), permutation problems (TSP, N-Queens), and a combinatorial problem (Knapsack). Each algorithm was tested with specific hyperparameter grids (e.g., SA cooling constants, GA and MIMIC population sizes, RHC restarts) across 5 independent runs. Performance was measured using average fitness score, wall-clock time, and convergence behavior (function evaluations). The analysis focused on comparing solution quality against computational efficiency to guide algorithm selection based on problem type and accuracy requirements.

## Key Results
- MIMIC achieved the highest fitness scores but incurred the highest computational cost, especially for permutation and combinatorial problems.
- GA delivered strong performance with moderate computational overhead, making it a robust choice across problem types.
- RHC and SA were less effective, with RHC particularly struggling due to local optima trapping and SA requiring high function evaluations.
- The study provides practical guidance for selecting algorithms based on problem structure, accuracy needs, and computational constraints.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Probabilistic modeling of variable dependencies (MIMIC) can yield higher solution quality in structured problems compared to operators that do not explicitly model dependencies.
- Mechanism: MIMIC estimates mutual information between variables to build a dependency tree, then samples new solutions from this distribution. This captures problem structure, allowing focused exploration of promising regions.
- Core assumption: The optimization problem exhibits dependencies between decision variables that, if captured, guide search more efficiently than independent perturbation.
- Evidence anchors:
  - [abstract] "MIMIC and GA excel in producing high-quality solutions for binary and combinatorial problems, their computational demands vary significantly."
  - [section] Section III.D describes MIMIC's probabilistic model building and mutual information estimation (Equation 6).
  - [corpus] Limited direct corpus support; related work focuses on unsupervised learning for combinatorial optimization, not MIMIC specifically.
- Break condition: When variables are nearly independent or the dependency structure is too weak to inform sampling, the overhead of model building outweighs benefits.

### Mechanism 2
- Claim: Population-based recombination with selection pressure (GA) balances exploration and exploitation, achieving strong solution quality with moderate computational cost.
- Mechanism: GA maintains a population, selects fit parents, applies crossover to combine partial solutions, and mutates to introduce variation. Selection pressure exploits good solutions; crossover and mutation explore new regions.
- Core assumption: High-quality solution components can be recombined to form better solutions (building block hypothesis), and the problem landscape permits gradual improvement.
- Evidence anchors:
  - [abstract] "GA offers a balanced approach, delivering strong performance with moderate computational overhead."
  - [section] Section III.C details selection, crossover (Equation 5), and mutation; Section V results show GA achieving 85–90% of optimum across problem groups.
  - [corpus] Weak direct evidence; neighboring papers discuss Bayesian optimization and GNNs for combinatorial problems, not GA specifically.
- Break condition: When optimal solutions require highly interdependent components that crossover disrupts, or when fitness landscapes are deceptive.

### Mechanism 3
- Claim: Single-point local search with probabilistic acceptance of worse solutions (SA) enables escape from local optima but can introduce instability and high function evaluation counts.
- Mechanism: SA accepts improving moves unconditionally and worse moves with probability decreasing over time (temperature schedule). Early high temperature promotes exploration; later low temperature exploits local improvements.
- Core assumption: A well-tuned cooling schedule allows sufficient exploration before convergence; the problem benefits from occasional uphill moves.
- Evidence anchors:
  - [abstract] "RHC and SA, while computationally less expensive, demonstrate limited performance in complex problem landscapes."
  - [section] Section III.B (Equation 3) defines acceptance probability; Section V shows SA requiring highest feval in binary/permutation problems with variable fitness.
  - [corpus] Limited corpus support; related work mentions simulated annealing efficiency in thermal conductance optimization (reference [26]).
- Break condition: Poorly tuned cooling schedules (too fast or too slow), or problems where the energy barrier between local and global optima is too large.

## Foundational Learning

- Concept: Exploration–Exploitation Trade-off
  - Why needed here: The paper's core comparison hinges on how each algorithm balances searching broadly (exploration) versus refining known good solutions (exploitation).
  - Quick check question: Can you explain why RHC fails on rugged landscapes while SA sometimes succeeds?

- Concept: Fitness Landscapes and Local Optima
  - Why needed here: Binary, permutation, and combinatorial problems have different landscape structures (smooth vs. rugged) that determine algorithm suitability.
  - Quick check question: What landscape feature causes RHC to achieve only 2.2 average fitness on FourPeaks (Table IV)?

- Concept: Computational Complexity in Randomized Search
  - Why needed here: The paper explicitly trades off solution quality against wall-clock time and function evaluations; understanding complexity guides algorithm selection.
  - Quick check question: Why does MIMIC achieve high fitness but incur higher wall-clock time (Figure 10)?

## Architecture Onboarding

- Component map:
  - RHC: Single solution state, neighbor generator, fitness comparator, restart controller.
  - SA: Single solution state, neighbor generator, temperature schedule, probabilistic acceptance module.
  - GA: Population store, selection operator, crossover operator, mutation operator, replacement strategy.
  - MIMIC: Population store, mutual information estimator, dependency tree builder, probabilistic sampler.

- Critical path:
  1. Define problem representation (binary string, permutation, or knapsack item selection).
  2. Configure algorithm hyperparameters per Table I (restarts, cooling schedule, population size).
  3. Run optimization loop for fixed iterations or until convergence.
  4. Log fitness, feval, and wall-clock time across multiple runs (5 recommended).

- Design tradeoffs:
  - MIMIC: Highest solution quality vs. highest computational cost; suited for accuracy-critical tasks.
  - GA: Balanced quality and speed; robust default for most problem types.
  - SA: Moderate quality, high feval count; useful when problem structure matches annealing intuition.
  - RHC: Lowest cost, poorest quality; only for resource-constrained or simple landscapes.

- Failure signatures:
  - RHC stagnates at low fitness with few feval (local optima trap).
  - SA shows erratic convergence or requires extreme feval (poor cooling schedule).
  - GA population converges prematurely (insufficient diversity, low mutation rate).
  - MIMIC plateaus early in combinatorial problems (model fails to capture complex dependencies).

- First 3 experiments:
  1. Replicate OneMax benchmark with GA and MIMIC (population sizes 100, 200, 300); verify both achieve max fitness of 50.
  2. Run FourPeaks with RHC (restarts 0, 5, 10) and SA (ExpConst 0.001, 0.005, 0.01); compare fitness progression to Figures 3a and 3b.
  3. Execute Knapsack with GA and MIMIC across population sizes; observe plateau behavior in MIMIC vs. steady GA improvement (Figures 8c and 8d).

## Open Questions the Paper Calls Out

- **Question**: How do different coding schemes influence the convergence speed and solution quality of the evaluated randomized optimization algorithms?
  - Basis in paper: [explicit] The Conclusion section explicitly identifies exploring the effect of different coding schemes as a specific avenue for future work.
  - Why unresolved: This study used standard representations for binary, permutation, and combinatorial problems but did not vary the encoding methods to test their impact on algorithmic efficiency.
  - What evidence would resolve it: Comparative experiments using alternative encoding strategies on the same benchmark problems, measuring changes in function evaluations and wall-clock time.

- **Question**: Can MIMIC be optimized to reduce its computational overhead while maintaining its superior solution quality?
  - Basis in paper: [explicit] The Conclusion notes that "ongoing research aims to develop more efficient algorithms that maintain high performance while reducing computational overhead."
  - Why unresolved: While MIMIC achieved the highest fitness scores, the results show it incurred significantly higher computational costs (wall-clock time) compared to GA, particularly in permutation problems.
  - What evidence would resolve it: A study demonstrating a modified MIMIC implementation or hybrid approach that achieves statistically similar fitness to the original MIMIC but with significantly lower time complexity.

- **Question**: How do these optimization algorithms compare in terms of resilience and robustness when applied to dynamic planning and scheduling environments?
  - Basis in paper: [explicit] The Conclusion references preliminary work on "algorithmic resilience" and indicates that a specific study on this topic is a distinct research direction.
  - Why unresolved: The current study focused on static benchmark fitness functions and did not evaluate performance in dynamic environments or measure resilience to disruptions.
  - What evidence would resolve it: Benchmarking the algorithms on dynamic scheduling problems where system constraints change over time, measuring recovery speed and solution stability.

## Limitations

- The study relies on synthetic benchmark problems, and the transferability of results to complex industrial optimization problems remains uncertain.
- The choice of hyperparameter grids may not fully explore the performance landscape of each algorithm.
- The specific instance configurations (e.g., TSP city coordinates, Knapsack item weights) are not fully specified in the text, requiring access to external repositories.

## Confidence

- **High Confidence**: GA's balanced performance across problem types; MIMIC's high solution quality at computational cost; RHC's poor performance on complex landscapes.
- **Medium Confidence**: SA's performance variability; the specific ranking of algorithms on permutation problems.
- **Low Confidence**: Direct comparison of wall-clock time across problems of different scales; claims about real-world applicability without validation on practical instances.

## Next Checks

1. Apply the best-performing algorithms (GA, MIMIC) to a real-world combinatorial problem (e.g., vehicle routing) and compare solution quality and runtime against the benchmark results.
2. Conduct a finer-grained grid search around the reported optimal parameters to assess the robustness of algorithm rankings to hyperparameter selection.
3. Implement and test additional algorithms (e.g., Bayesian optimization, reinforcement learning-based methods) on the same benchmark suite to validate the relative performance of RHC, SA, GA, and MIMIC.