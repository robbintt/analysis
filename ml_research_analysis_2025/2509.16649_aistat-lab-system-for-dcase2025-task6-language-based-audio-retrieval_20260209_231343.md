---
ver: rpa2
title: 'AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval'
arxiv_id: '2509.16649'
source_url: https://arxiv.org/abs/2509.16649
tags:
- audio
- classi
- cation
- arxiv
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a dual-encoder approach for language-based
  audio retrieval, using separate encoders for audio and text modalities aligned via
  contrastive learning. The system incorporates distillation loss from ensemble soft
  labels, LLM-based data augmentation (back-translation and caption mixing), and an
  auxiliary clustering-based classification task.
---

# AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval

## Quick Facts
- arXiv ID: 2509.16649
- Source URL: https://arxiv.org/abs/2509.16649
- Reference count: 0
- Primary result: Dual-encoder ensemble achieved mAP@16 of 48.83 on Clotho development test

## Executive Summary
This paper presents a dual-encoder approach for language-based audio retrieval that combines contrastive learning with distillation from ensemble soft labels, LLM-based data augmentation, and an auxiliary clustering-based classification task. The system uses separate encoders for audio and text modalities aligned via contrastive learning, with three different audio models (PaSST, EAT, BEATs) combined into an ensemble. The approach achieved state-of-the-art performance on the Clotho development test split with mAP@16 of 48.83, demonstrating the effectiveness of soft label distillation and multi-stage training.

## Method Summary
The system employs a dual-encoder architecture with audio and text encoders aligned through contrastive learning. The training pipeline consists of three stages: initial pretraining on ClothoV2.1, AudioCaps, and WavCaps datasets using supervised contrastive loss; finetuning on Clotho with contrastive loss plus distillation loss from an ensemble of pretrained models; and re-finetuning with additional auxiliary classification loss based on semantically derived clusters. The method incorporates LLM-based data augmentation including back-translation and caption mixing, generating 50,000 new audio-caption pairs. Three audio models (PaSST, EAT, BEATs) were combined into an ensemble, with PaSST consistently outperforming other models.

## Key Results
- Best single system (PaSST) achieved mAP@16 of 46.62 on Clotho development test
- Ensemble of three models (PaSST, EAT, BEATs) achieved mAP@16 of 48.83
- PaSST consistently outperformed EAT and BEATs across all evaluation metrics
- Distillation loss and LLM-based augmentation contributed to performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distillation loss from ensemble soft labels improves retrieval performance over standard binary supervision by capturing nuanced, many-to-many relationships between audio and text.
- Mechanism: Standard contrastive learning treats audio-text pairs as binary matches. This method uses an ensemble of pretrained models to generate soft probability distributions over all pairs in a batch (averaging their similarity scores). The student model is then trained to match these soft targets via an additional cross-entropy loss term, forcing it to learn a more structured embedding space where semantically related (but not identical) pairs have higher similarity than random pairs.
- Core assumption: The ensemble's averaged similarity scores provide a richer and more accurate supervisory signal than binary ground truth, reflecting true semantic relatedness between audio and captions.
- Evidence anchors: [abstract] "...we implemented a distillation approach..." [section 2.2] "This method uses soft correspondence probabilities from an ensemble of pretrained models to capture nuanced audio-text relationships, improving generalization."
- Break condition: Performance degrades if the teacher ensemble is poorly aligned, yielding noisy or uniform soft labels that provide no clear learning signal.

### Mechanism 2
- Claim: An auxiliary classification task based on semantically derived clusters enhances audio-text alignment by encouraging representations to map to a shared topic space.
- Mechanism: Captions are clustered using BERTopic (UMAP + HDBSCAN) to assign pseudo-labels. Classification heads are added to both audio and text encoders to predict these cluster labels. The model is trained with an additional loss term (位2=0.05) for this task. This forces the audio encoder to learn features predictive of text-derived semantic topics, creating an implicit alignment pressure supplementary to contrastive learning.
- Core assumption: The clustering meaningfully captures semantic structure relevant to retrieval, and audio features contain information predictive of these text-derived topics.
- Evidence anchors: [abstract] "...we incorporated clustering to introduce an auxiliary classification task..." [section 2.3] "This setup encourages the audio encoder to learn representations that are aligned with the semantic clusters of the captions..."
- Break condition: Fails if clusters are too coarse (providing no new information) or too noisy (causing the model to overfit to incorrect pseudo-labels).

### Mechanism 3
- Claim: LLM-based data augmentation (back-translation and caption mixing) improves generalization by increasing linguistic and acoustic diversity in the training set.
- Mechanism: Back-translation generates paraphrased captions with varied linguistic expressions. LLM mix creates novel multi-event audio clips and uses an LLM to generate a corresponding composite caption. These techniques expose the model to a wider range of language and acoustic combinations, reducing overfitting to the original dataset's limited patterns.
- Core assumption: The LLM-generated paraphrases and composite captions are semantically faithful and accurately describe the audio content.
- Evidence anchors: [abstract] "...leveraged large language models (LLMs) for effective data augmentation techniques, including back-translation and LLM mix." [section 2.4] "By doing so, back-translation generates captions that retain the same semantic meaning... With LLM mix, we created 50,000 new audio-caption pairs..."
- Break condition: Fails if back-translation introduces semantic drift or if LLM mix generates hallucinatory captions that misdescribe the audio, teaching incorrect associations.

## Foundational Learning

- Concept: **Contrastive Learning (InfoNCE Loss)**
  - Why needed here: This is the core loss function used to align audio and text in a joint embedding space. Without understanding this, the primary training objective is unclear.
  - Quick check question: In a batch of N audio-text pairs, how are the "positive" and "negative" pairs defined for a single anchor audio clip?

- Concept: **Dual-Encoder Architecture**
  - Why needed here: The system uses separate encoders for audio and text. This structure is fundamental to how the embeddings are generated, aligned, and used for retrieval.
  - Quick check question: Why is a dual-encoder architecture generally preferred over a cross-encoder for large-scale retrieval tasks?

- Concept: **Knowledge Distillation**
  - Why needed here: A key contribution is using distillation loss from an ensemble. Understanding how soft targets transfer knowledge from a teacher to a student model is necessary to grasp this mechanism.
  - Quick check question: How do soft labels from a teacher model provide a different learning signal than hard, binary labels?

## Architecture Onboarding

- Component map:
  - Inputs: Audio (log-mel spectrograms), Text (captions)
  - Encoders: Audio (PaSST [32kHz], EAT [16kHz], BEATs [16kHz]), Text (RoBERTa-large)
  - Projection/Output: Embeddings for audio and text
  - Loss Functions: Supervised Contrastive, Distillation, Auxiliary Classification
  - Data Augmentation: LLM-based (back-translation, mix)

- Critical path: The training pipeline has three sequential stages:
  1. Initial Pretraining: Train dual-encoder on Clotho+AudioCaps+WavCaps with contrastive loss only
  2. Finetuning: Train on Clotho with contrastive + distillation loss (from ensemble of Stage 1 models)
  3. Re-finetuning: Train on Clotho with contrastive + distillation + auxiliary classification loss (using clusters derived from Stage 2 or e5 embeddings)

- Design tradeoffs:
  - Model Choice: PaSST performed best but requires 32kHz audio. EAT/BEATs use 16kHz, which may be more resource-efficient
  - Ensemble Cost: The ensemble yields top performance (mAP@16 48.83) but requires running multiple models, increasing complexity and inference time
  - Loss Balancing: Distillation (位=1.0) is weighted equally to contrastive loss, while clustering (位2=0.05) is a light regularizer

- Failure signatures:
  - No gain over baseline (SID1): Check pretrained weights, contrastive loss implementation (temperature, batch size), or data pipeline
  - Distillation fails (SID2 vs SID1): Ensure teacher ensemble is well-pretrained and soft labels are calculated correctly
  - Augmentation fails (SID3 vs SID2): Inspect generated captions for semantic drift or errors
  - Clustering hurts performance: Pseudo-labels may be too noisy; try different clustering parameters or reduce 位2

- First 3 experiments:
  1. Run System ID 1 (Baseline) to validate the dual-encoder pipeline with standard contrastive loss on Clotho
  2. Run System ID 2 (Distillation) using an ensemble of baseline models as teachers to measure the impact of soft labels
  3. Run System ID 3 (Augmentation) adding LLM-based data augmentation to the distillation setup to quantify gains from increased data diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the auxiliary cluster-based classification task degrade performance for the EAT and BEATs models while improving PaSST?
- Basis in paper: [inferred] Table 2 shows that adding clustering (SID 5 vs. SID 3) improves PaSST (46.41 to 46.50 mAP@16) but causes a noticeable drop in performance for EAT (46.05 to 45.34) and BEATs.
- Why unresolved: The paper does not analyze the interaction between the clustering pseudo-labels and different model architectures.
- What evidence would resolve it: An ablation study visualizing the cluster separability within the embedding spaces of PaSST versus EAT/BEATs.

### Open Question 2
- Question: To what extent does the "LLM mix" augmentation introduce semantic noise that limits retrieval performance?
- Basis in paper: [inferred] Section 2.4 describes synthesizing captions for mixed audio using GPT-4o without verifying acoustic alignment, and results in Table 2 (SID 2 vs. SID 3) show mixed or marginal gains.
- Why unresolved: The method relies on the LLM's "hallucination" to describe overlapping sounds, which may create text embeddings that do not match the actual audio content.
- What evidence would resolve it: A human evaluation of the retrieval precision for the 50,000 synthetic samples compared to the original data.

### Open Question 3
- Question: Is the distillation loss dependent on the architectural diversity of the teacher ensemble?
- Basis in paper: [inferred] Section 2.2 defines the distillation loss using an ensemble of three distinct architectures (PaSST, EAT, BEATs) to generate soft labels, but does not test ensembles of different sizes or compositions.
- Why unresolved: It is unclear if the performance gain comes from the distillation process itself or simply the variance reduction of using three different models.
- What evidence would resolve it: A comparison of student models trained using single-teacher distillation versus the proposed multi-teacher ensemble.

## Limitations
- The effectiveness of distillation depends critically on the quality of ensemble teacher soft labels, which is not fully validated
- LLM-based augmentation's impact is highly sensitive to semantic fidelity of generated captions, with matching criteria unspecified
- Auxiliary clustering task's benefit is contingent on meaningful and stable pseudo-labels, with cluster quality metrics not reported

## Confidence
- High Confidence: The core dual-encoder architecture with supervised contrastive loss is well-established with clearly specified implementation details
- Medium Confidence: The sequential training pipeline is logically sound, but true contribution of each component is not fully isolated
- Low Confidence: Specific hyperparameters for LLM prompts and BERTopic clustering are not provided, creating critical gaps for exact reproduction

## Next Checks
1. Validate Soft Label Quality: Analyze distribution of similarity scores from teacher ensemble to verify better teacher alignment yields better student performance
2. Test Clustering Stability: Perform clustering with different random seeds and HDBSCAN parameters to measure consistency of cluster assignments and resulting mAP@16
3. Ablation on Augmentation Quality: Perform semantic similarity check on generated captions and retrain model with filtered samples to quantify impact of semantic drift