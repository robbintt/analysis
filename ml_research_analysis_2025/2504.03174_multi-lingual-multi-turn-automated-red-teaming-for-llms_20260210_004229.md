---
ver: rpa2
title: Multi-lingual Multi-turn Automated Red Teaming for LLMs
arxiv_id: '2504.03174'
source_url: https://arxiv.org/abs/2504.03174
tags:
- languages
- conversation
- safety
- llms
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MM-ART, a method for fully automated red-teaming
  of language models across multiple languages and conversation turns. The core idea
  is to generate conversation starters and then use an agent-based approach to conduct
  multi-turn adversarial conversations, automatically translating between English
  and target languages to maintain efficiency.
---

# Multi-lingual Multi-turn Automated Red Teaming for LLMs

## Quick Facts
- arXiv ID: 2504.03174
- Source URL: https://arxiv.org/abs/2504.03174
- Reference count: 8
- Primary result: Models are on average 71% more vulnerable after 5 turns in English and up to 195% more vulnerable in non-Latin-alphabet languages compared to single-turn English attacks

## Executive Summary
This paper introduces MM-ART, a fully automated red-teaming framework that conducts multi-turn adversarial conversations across 7 languages and up to 5 conversation depths. The method uses in-context learning to generate conversation starters, an agent-based approach for multi-turn escalation, and machine translation between English and target languages to maintain efficiency. Experiments across 6 different models show that models are significantly more vulnerable in multi-turn conversations than single-turn attacks, with non-Latin-alphabet languages showing the highest vulnerability. The framework achieves high attack success rates with both human- and machine-generated prompts, demonstrating that automated red-teaming can effectively scale to match modern LLM capabilities.

## Method Summary
MM-ART employs a two-stage pipeline: (1) Conversation starter generation using Mistral-7B or Mixtral-8x7B with in-context learning templates (Vanilla or RedTeam) to produce category-specific prompts, and (2) Multi-turn attack execution where an agent generates follow-up prompts in English, translates to target language, sends to the target model, translates responses back, and repeats for 5 turns. The framework uses Amazon Translate for bidirectional conversion, assesses safety with Claude Sonnet 3.5, and detects refusals with Mixtral-8x7B. The approach tests 7 safety categories across 7 languages with 6 different target models.

## Key Results
- Models are 71% more vulnerable after 5-turn English conversations compared to single-turn attacks
- Non-Latin-alphabet languages show up to 195% higher vulnerability than English single-turn attacks
- Human-generated conversation starters achieve 54.7% ASR versus 41.5% for machine-generated starters
- Refusal rate at turn 1 is 29%, dropping ASR to 6.64% for conversations with initial refusals

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn conversations degrade safety alignment through gradual context accumulation
- Mechanism: Attackers maintain conversation coherence while incrementally escalating requests. Safety training appears to prioritize initial-turn refusals; once a model engages, subsequent context normalizes the interaction
- Core assumption: Safety alignment datasets contain predominantly short, English exchanges with limited multi-turn adversarial examples
- Evidence anchors:
  - "models are on average 71% more vulnerable after a 5-turn conversation in English than after the initial turn"
  - "ASR after five turns with MM-ART is on average 80% higher than at the beginning of the conversation"
  - RedCoder and related work confirm multi-turn attacks systematically outperform single-turn approaches

### Mechanism 2
- Claim: Translation to non-English languages, especially non-Latin-script languages, bypasses safety training
- Mechanism: Safety alignment data is English-dominant. Machine translation preserves semantic intent while shifting surface form into lower-resource representation spaces where safety classifiers are weaker
- Core assumption: Alignment training corpus is heavily skewed toward English, with limited coverage of multilingual adversarial patterns
- Evidence anchors:
  - "up to 195% more vulnerable in non-English languages versus English single-turn attacks"
  - "non-Latin-alphabet languages (68.46%) than for English (40.93%) and other Latin-alphabet languages (48.72%)"
  - Code-Mixed Phonetic Perturbations work similarly exploits representational gaps

### Mechanism 3
- Claim: Conversation starter quality and initial refusal rate determine attack success ceiling
- Mechanism: Prompts that trigger early refusal embed resistance into context, making subsequent escalation harder. Higher-quality starters achieve lower initial refusal and higher eventual ASR
- Core assumption: Refusal responses persist in context and constrain future generation paths
- Evidence anchors:
  - "average ASR is 54.7%, the average refusal rate of the first response is 29% and on those 29% conversations, the ASR drops to 6.64%"
  - Mixtral-8x7B with RedTeam instructions achieves 13 percentage points higher ASR than Mistral-7B with Vanilla instructions

## Foundational Learning

- Concept: In-context learning (ICL) for adversarial prompt generation
  - Why needed here: MM-ART uses ICL to generate conversation starters from human exemplars. Understanding how demonstration selection affects output diversity and attack efficacy is critical
  - Quick check question: Can you explain why the paper uses 5 human exemplars per category and how changing this number might affect prompt diversity?

- Concept: LLM-as-a-judge evaluation paradigm
  - Why needed here: Safety and refusal assessment rely on Claude Sonnet 3.5 and Llama Guard 3 as judges. Results vary significantly by assessor choice
  - Quick check question: Why does Llama Guard 3 produce 4-5x lower ASR than Claude Sonnet 3.5, and what does this imply about relying on a single assessor?

- Concept: Attack Success Rate (ASR) as a depth-dependent metric
  - Why needed here: ASR is measured at multiple conversation depths; the paper's core claims depend on understanding ASR progression
  - Quick check question: If ASR at turn 1 is 20% and ASR at turn 5 is 36%, what is the relative increase, and why does the paper emphasize relative over absolute gains?

## Architecture Onboarding

- Component map: Conversation Starter Generator -> Translation Layer -> Target Model Interface -> Back-translation -> Multi-turn Attack Agent -> Translation Layer -> Target Model Interface -> Repeat -> Assessment Module

- Critical path: Starter generation → Translation to target language → Target model response → Back-translation to English → Next-turn generation (in English) → Translation → Repeat for 5 turns → Safety assessment on final conversation state

- Design tradeoffs:
  - Small open models for attack generation vs. closed models for assessment (cost/controllability vs. judge quality)
  - Machine translation vs. human translation (speed/scale vs. nuance preservation; paper finds <5% ASR difference)
  - English-only assessment vs. native-language assessment (LLM performance bias vs. translation error propagation)

- Failure signatures:
  - High first-turn refusal rate (>40%) indicates poor starter quality or overfamiliar benchmark prompts
  - Assessor disagreement >3x suggests one model is tuned for high precision at the cost of recall
  - ASR plateau before turn 5 may indicate context saturation or effective refusal persistence

- First 3 experiments:
  1. Replicate the ASR-by-depth curve for a single target model using only the Human starter set in English to validate the 71% increase claim
  2. Ablate the translation component by comparing machine vs. human-translated Multi-Jail prompts at turn 1 only to isolate translation impact
  3. Swap the safety assessor from Claude Sonnet 3.5 to Llama Guard 3 on the same conversation set to quantify assessor variance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated red-teaming frameworks effectively recover from an initial refusal response without relying on static single-turn jailbreak templates?
- Basis in paper: "In the future, we will explore various techniques for regenerating prompts upon LLM refusal"
- Why unresolved: Results show ASR drops to 6.64% for conversations where the target model refuses the initial prompt, and the current method lacks a dynamic recovery mechanism
- What evidence would resolve it: Implementation of iterative prompt rephrasing strategies that demonstrate significantly higher ASR specifically within the subset of conversations that start with a refusal

### Open Question 2
- Question: Can adversarial conversation starters be generated with sufficient diversity and efficacy using zero-shot learning rather than relying on human-crafted in-context learning (ICL) exemplars?
- Basis in paper: "We plan to reduce even more reliance on human-crafted prompts by leveraging zero-shot generation"
- Why unresolved: Current machine-generated starters depend heavily on "human seeds" provided in the ICL context, potentially limiting the novelty of generated attacks
- What evidence would resolve it: A comparative study showing zero-shot generated prompts achieve similar or higher ASR and semantic diversity compared to current ICL-based "Mixtral Generated" dataset

### Open Question 3
- Question: How should the safety community address the severe discrepancy in safety violation detection between different LLM-as-a-judge models?
- Basis in paper: Section 5.2 reveals Llama Guard 3 reports ASR of 11.2% while Claude Sonnet 3.5 reports 54.7%, with Llama Guard 3 "likely to miss less obvious unsafe responses"
- Why unresolved: Lack of ground truth makes it difficult to ascertain if a model is truly safe or if the evaluator model is under-sensitive
- What evidence would resolve it: Systematic human evaluation of "false negative" cases from Llama Guard 3 to determine if lower ASR is due to model safety or evaluator failure

## Limitations
- LLM-as-a-judge safety assessment shows significant assessor variance (up to 5x difference between Claude Sonnet 3.5 and Llama Guard 3)
- Withheld conversation starter templates and exact next-turn generation instructions prevent exact reproduction
- Translation layer introduces uncertainty, though paper reports <5% ASR difference between human and machine translation at turn 1

## Confidence

- High Confidence: The finding that multi-turn conversations increase vulnerability by 71% in English and 195% in non-Latin-alphabet languages compared to single-turn attacks. Supported by multiple datasets and consistent across different target models.
- Medium Confidence: The claim that translation to non-English languages bypasses safety training. While the effect is robust, the mechanism remains unclear.
- Medium Confidence: The assertion that starter quality determines attack success ceiling. Evidence shows correlation but not causation.

## Next Checks
1. **Assessor-Agnostic Validation**: Run the exact same conversation sets through three different safety assessors (Claude Sonnet 3.5, Llama Guard 3, and GPT-4) to quantify assessor variance and establish bounds on true ASR values.

2. **Translation Fidelity Experiment**: For a subset of conversations, conduct side-by-side attacks using (a) machine-translated prompts, (b) human-translated prompts, and (c) English prompts with target models instructed to respond in target language. Compare ASR at turn 1 across all three to isolate translation impact from language-specific safety gaps.

3. **Context Manipulation Test**: For conversations with high first-turn refusal, edit out the refusal response and regenerate the next turn. Compare ASR before and after refusal removal to determine whether refusal persistence or conversation starter quality is the primary limiting factor in attack success.