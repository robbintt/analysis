---
ver: rpa2
title: 'Automated Classification of Tutors'' Dialogue Acts Using Generative AI: A
  Case Study Using the CIMA Corpus'
arxiv_id: '2509.09125'
source_url: https://arxiv.org/abs/2509.09125
tags:
- coding
- dialogue
- generative
- tutor
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that generative AI can effectively automate
  the classification of tutors' Dialogue Acts (DAs) in educational settings. Using
  GPT-4 with carefully designed prompts, the model achieved 80% accuracy, a weighted
  F1-score of 0.81, and Cohen's Kappa of 0.74 on a four-category DA classification
  task, outperforming baseline methods.
---

# Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus

## Quick Facts
- **arXiv ID**: 2509.09125
- **Source URL**: https://arxiv.org/abs/2509.09125
- **Reference count**: 0
- **Primary result**: Generative AI (GPT-4) achieved 80% accuracy and 0.81 F1-score in classifying tutors' Dialogue Acts

## Executive Summary
This study demonstrates that generative AI can effectively automate the classification of tutors' Dialogue Acts (DAs) in educational settings. Using GPT-4 with carefully designed prompts, the model achieved 80% accuracy, a weighted F1-score of 0.81, and Cohen's Kappa of 0.74 on a four-category DA classification task, outperforming baseline methods. The research highlights the importance of task-specific label definitions and contextual information in improving model performance. The approach eliminates the need for manual annotation or model fine-tuning, offering a more accessible and efficient alternative to traditional coding methods while supporting deeper educational dialogue analysis.

## Method Summary
The study employed zero-shot prompting via OpenAI API to classify tutors' dialogue acts from the CIMA corpus "Prepositional Phrases" subset. Researchers tested 12 prompt conditions with GPT-3.5-turbo and the top 5 conditions with GPT-4, varying context window sizes (n=0, 1, or 2 preceding turns) and prompt types (basic, elaborative, stepwise, combined). The classification task involved four categories: Question, Hint, Correction, and Confirmation. After filtering out "Others" category, 80 samples were randomly selected (20 per class) for evaluation. The model's outputs were compared against human annotations using accuracy, weighted F1-score, and Cohen's Kappa metrics.

## Key Results
- GPT-4 with combined prompt (definitions + step-by-step logic) achieved 80% accuracy and 0.81 weighted F1-score
- Context window of n=1 or n=2 preceding turns improved classification accuracy over isolated target turns
- Task-specific label definitions significantly enhanced model performance compared to generic label names
- Cohen's Kappa of 0.74 indicated substantial agreement between AI and human annotators

## Why This Works (Mechanism)

### Mechanism 1: Task-Specific Label Definitions
Generative models rely on pre-trained semantic associations. Without specific constraints, a label like "Hint" may conflate with general information delivery. By injecting operational definitions (e.g., "Hint: scaffolding...") into the system prompt, the model constrains its latent space to align with the researcher's specific coding scheme.

### Mechanism 2: Conversational Context Window
A tutor's response is often functionally dependent on the student's prior error or query. Including preceding turns ($n=1$ or $n=2$) provides necessary context to resolve the intent of the target utterance, distinguishing between acts that would appear identical in isolation.

### Mechanism 3: Model Capability for Complex Reasoning
Higher-capability models (GPT-4) are more robust to prompt complexity and better at following multi-step reasoning (Chain-of-Thought) than earlier models (GPT-3.5). GPT-4 appears to better utilize "step-by-step" instructions to decompose the classification problem.

## Foundational Learning

- **Concept: Dialogue Act (DA) / Speech Act Theory**
  - Why needed here: To understand that the goal is not semantic sentiment analysis, but functional intent (e.g., "scaffolding" vs. "correcting")
  - Quick check question: Can you distinguish why "That's not quite right" is a different Dialogue Act than "Think about the formula again"?

- **Concept: Zero-Shot Classification vs. Fine-Tuning**
  - Why needed here: The paper argues this method eliminates the need for training data. You must understand that the model is using pre-existing knowledge accessed via prompts, not learning weights from a dataset
  - Quick check question: Why does the model fail if you remove the label definitions, given it hasn't been "trained" on the CIMA dataset?

- **Concept: Inter-Rater Reliability (Cohen's Kappa)**
  - Why needed here: To evaluate success. Accuracy alone is insufficient for imbalanced classes. Kappa measures how much the AI agrees with humans beyond chance
  - Quick check question: If an AI achieves 80% accuracy, why might a Kappa of 0.74 be the more critical metric for trusting the system?

## Architecture Onboarding

- **Component map**: CIMA Corpus (JSON/CSV) -> Pre-processor: Context Windowing ($n$-turn extraction) -> Prompt Constructor: Injects System Message (Role/Definitions) + User Message (Context) -> Model API: GPT-4 -> Parser: Extracts Label -> Evaluator: compares against Human Ground Truth

- **Critical path**: The construction of the **System Message** (Prompt 4 or 2). If the definitions here are vague, the entire pipeline fails regardless of model capability

- **Design tradeoffs**: Latency vs. Context (increasing $n$ improves accuracy but increases token count, latency, and cost); Complexity vs. Stability (GPT-3.5 is cheaper but struggles with "Chain-of-Thought" logic; GPT-4 handles complexity but at higher price)

- **Failure signatures**: "Hint" Blindspot (low F1 scores in many conditions, likely conflating with "Correction"); Context Noise (GPT-3.5 performed worse with $n=2$ than $n=1$, indicating context pollution)

- **First 3 experiments**:
  1. Baseline Validation: Replicate the "Basic / n=0" experiment to establish a floor for performance
  2. Definition Ablation: Run the "Elaborative" prompt (Prompt 2) to quantify the lift gained purely from adding definitions
  3. Context Stress Test: Compare $n=1$ vs $n=2$ on a subset of "Correction" and "Hint" examples to identify the optimal context window

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of generative AI in classifying tutors' Dialogue Acts change when applied to coding schemes with more than four categories?
- Basis in paper: [explicit] The authors state that "future research should investigate generative AI's performance in educational DA classification tasks that involve more categories."
- Why unresolved: The current study only tested a four-category DA coding scheme (Question, Hint, Correction, Confirmation)
- What evidence would resolve it: Accuracy and F1-scores from experiments using generative AI on datasets with complex, multi-dimensional coding taxonomies

### Open Question 2
- Question: Can the substantial agreement found between generative AI and human annotators be replicated on larger datasets?
- Basis in paper: [explicit] The authors note that "employing a larger dataset is imperative to further test and validate the conclusions drawn in this study."
- Why unresolved: The current findings are based on a relatively small sample of 80 randomly selected dialogue turns
- What evidence would resolve it: Results from a replication study applying the same prompting methodology to the full CIMA corpus or similar large-scale educational datasets

### Open Question 3
- Question: How do the response times of generative AI models scale when processing full-length tutoring sessions compared to the short snippets used in this study?
- Basis in paper: [explicit] The authors highlight the need to "record and consider the response time of machine coding in future research" for real-world applications
- Why unresolved: The study focused on classification accuracy and did not measure the time required to generate responses for complete tutorial logs
- What evidence would resolve it: A time-motion analysis of the API processing hundreds of sequential turns to determine if the efficiency gains over human coding are retained

## Limitations
- Small sample size (80 instances) relative to complexity of educational dialogue analysis
- Results may not transfer to different DA taxonomies or educational domains
- Evaluation relies on single human-annotated dataset (CIMA), preventing assessment across multiple annotation schemes
- Does not explore cost-efficiency trade-offs or scalability to larger corpora

## Confidence

- **High Confidence**: The core finding that GPT-4 with task-specific definitions outperforms baseline methods (80% accuracy, 0.81 F1, 0.74 Kappa) is well-supported by experimental results across multiple conditions
- **Medium Confidence**: The mechanism that explicit label definitions improve performance is demonstrated but could benefit from more rigorous ablation studies to isolate this effect from other prompt features
- **Medium Confidence**: The claim that context window size ($n=1$ or $n=2$) improves accuracy is supported, but the optimal window size appears task-dependent and may vary with domain complexity
- **Low Confidence**: Claims about GPT-4's superior handling of Chain-of-Thought instructions relative to GPT-3.5 are based on limited comparisons and require more systematic testing across diverse reasoning tasks

## Next Checks
1. **Cross-Dataset Validation**: Test the same prompt structure on at least two additional educational dialogue corpora with different DA taxonomies to assess generalizability beyond the CIMA dataset
2. **Cost-Performance Analysis**: Measure the relationship between context window size, token costs, and accuracy improvements to identify optimal cost-performance trade-offs for practical deployment
3. **Definition Ablation Study**: Systematically remove individual label definitions from the combined prompt to quantify the specific contribution of each definition to overall classification accuracy