---
ver: rpa2
title: 'Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for
  Sleep Foundation Models'
arxiv_id: '2512.09591'
source_url: https://arxiv.org/abs/2512.09591
tags:
- sleep
- time
- patches
- learning
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Stanford Sleep Bench introduces a large-scale polysomnography dataset
  with 17,467 recordings and 163,000 hours of data, providing standardized training,
  validation, and test splits for sleep-related tasks. The benchmark includes 13 clinical
  disease prediction tasks alongside canonical sleep tasks like sleep staging, apnea
  diagnosis, and age estimation.
---

# Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models

## Quick Facts
- arXiv ID: 2512.09591
- Source URL: https://arxiv.org/abs/2512.09591
- Reference count: 40
- Key result: Contrastive learning achieves C-index of 0.74 for disease prediction, outperforming other methods by 4.64%

## Executive Summary
Stanford Sleep Bench introduces a comprehensive polysomnography benchmark with 17,467 recordings spanning 163,000 hours of data. The benchmark evaluates self-supervised representation learning methods—contrastive learning (CL), masked autoencoders (MAE), and denoising autoencoders (DAE)—across 13 clinical disease prediction tasks plus canonical sleep tasks. Results demonstrate that CL significantly outperforms other approaches for mortality and disease prediction while achieving comparable performance for sleep staging, apnea diagnosis, and age estimation. The benchmark includes standardized training, validation, and test splits, along with pre-trained models and evaluation code.

## Method Summary
The Stanford Sleep Bench evaluates four SSRL methods (CL, MAE, DAE, and Baseline) on a large-scale PSG dataset with 16 channels across four modalities. The architecture uses modality-specific CNN patch encoders for 5-second segments, followed by temporal transformers and task-specific heads. Methods are compared on four downstream tasks: sleep staging (5-class classification), apnea diagnosis (binary), age estimation (regression), and 13 disease/mortality predictions (Cox proportional hazards). The benchmark employs 300-second input windows with 5-second patch processing, using 2× A100 GPUs for training.

## Key Results
- CL achieves C-index of 0.74 for disease/mortality prediction, outperforming MAE/DAE by 4.64%
- Multiple methods achieve comparable performance for sleep staging (AUROC: 0.802–0.823), apnea diagnosis (AUROC: 0.818–0.830), and age estimation (MAE: 6.20–6.67 years)
- CL demonstrates superior few-shot learning, reaching 95% of best performance with only 64 subjects
- Frequency-domain methods consistently outperform time-domain counterparts across all tasks

## Why This Works (Mechanism)

### Mechanism 1
CL creates superior representations for long-range clinical predictions by using temporal attention pooling and discriminative objectives to capture cross-modal dependencies. This enables embeddings to aggregate physiological patterns over minutes/hours rather than optimizing for local signal fidelity.

### Mechanism 2
Frequency-domain reconstruction (MAE/DAE) achieves competitive performance for canonical sleep tasks by learning spectral signatures directly, bypassing the difficulty of reconstructing precise time-domain waveforms that are unnecessary for tasks defined by spectral power bands.

### Mechanism 3
Modality-specific patch encoders allow the model to handle heterogeneous PSG channel availability by creating distinct embedding spaces for brain, respiratory, and cardiac activity before fusion.

## Foundational Learning

- **Contrastive Learning vs. Reconstruction**: Why needed - primary axis of variation in the paper; Quick check - Does the pre-training objective try to recreate the signal or separate signal distributions?
- **Transformer Architectures for Time Series**: Why needed - model uses temporal transformers to process sequences of 5-second patch embeddings; Quick check - How is positional encoding injected to ensure the model knows the difference between minute 1 and minute 30?
- **Cox Proportional Hazards (Survival Analysis)**: Why needed - "Disease and Mortality" prediction is time-to-event modeling, not binary classification; Quick check - How does the loss function handle "censored" data?

## Architecture Onboarding

- **Component map**: Input (16 channels at 128Hz) → Modality-specific CNN patch encoders (5s → R128) → Temporal transformer (6 layers, 8 heads) → Concatenate to R512 → Pre-training head (CL projection or reconstruction decoder) → Downstream head (2-layer transformer + 2-layer BiLSTM + attention pooling)
- **Critical path**: Definition of "positive pairs" in CL and masking ratio in MAE are most sensitive hyperparameters
- **Design tradeoffs**: Use CL for complex multimodal risk prediction; use Freq-MAE for canonical staging if interpretability needed; always prefer frequency domain over time domain
- **Failure signatures**: Low C-Index indicates time-domain pre-training; poor few-shot indicates non-CL methods; age estimation collapse shows failed biomarker capture
- **First 3 experiments**: 1) Train Baseline (Freq) on Sleep Staging (target ~0.81 AUROC); 2) Run CL-LOO with 64 subjects (verify 95% performance threshold); 3) Evaluate pre-trained weights on SHHS validation set

## Open Questions the Paper Calls Out

- Would hybrid strategies combining CL with generative objectives (MAE/DAE) improve downstream generalization compared to individual methods?
- Can modality-specific auxiliary losses capture diagnostically valuable patterns (e.g., slow oscillation spindle-coupling) that CL may neglect?
- What fine-tuning strategies can improve performance on rare diagnoses with limited positive examples?
- How well do these SSRL methods generalize to external datasets with different demographics, instrumentation, and scoring protocols?

## Limitations

- Demographic composition and clinical site distribution are not specified, limiting generalizability assessment
- Critical architecture details (CNN encoder architecture, transformer dimensions, fine-tuning schedules) are underspecified
- Real-world deployment requirements and prospective clinical validation are not addressed

## Confidence

- **High Confidence**: CL superiority for disease/mortality prediction (C-index 0.74) and faster few-shot convergence
- **Medium Confidence**: Frequency-domain methods consistently outperforming time-domain across tasks
- **Low Confidence**: "All 13 disease prediction tasks benefit" claim lacks task-specific breakdowns

## Next Checks

1. Cross-site validation: Evaluate pre-trained models on independent PSG dataset from different clinical site
2. Task-specific ablation: Run CL-LOO on each of the 13 disease tasks individually to identify which benefit most
3. Demographic fairness audit: Analyze model performance stratified by age groups and sex to identify potential biases