---
ver: rpa2
title: 'Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations
  Align with Classical ML?'
arxiv_id: '2510.25701'
source_url: https://arxiv.org/abs/2510.25701
tags:
- llms
- feature
- lightgbm
- shap
- loan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study compares zero-shot LLM classifiers to LightGBM on structured\
  \ credit risk prediction, using SHAP to audit feature attributions. LightGBM achieved\
  \ higher ROC\u2013AUC (0.73) and PR\u2013AUC (0.91) than LLMs (best: 0.67 and 0.88),\
  \ with no ensemble improvement."
---

# Interpreting LLMs as Credit Risk Classifiers: Do Their Feature Explanations Align with Classical ML?

## Quick Facts
- arXiv ID: 2510.25701
- Source URL: https://arxiv.org/abs/2510.25701
- Reference count: 39
- LightGBM achieved higher ROC–AUC (0.73) and PR–AUC (0.91) than LLMs (best: 0.67 and 0.88), with no ensemble improvement

## Executive Summary
This study compares zero-shot LLM classifiers to LightGBM on structured credit risk prediction, using SHAP to audit feature attributions. LightGBM achieved higher ROC–AUC (0.73) and PR–AUC (0.91) than LLMs (best: 0.67 and 0.88), with no ensemble improvement. While LLMs identified key risk features similarly to LightGBM, their feature importance rankings diverged, and LLM self-explanations often misaligned with SHAP-derived attributions. These findings highlight LLMs’ limited reliability as standalone models for high-stakes financial prediction and emphasize the need for explainability audits, baseline comparisons with interpretable models, and human oversight before deployment.

## Method Summary
The study evaluates three LLM models (Llama-3.1-8B, Gemma-2-9B, Qwen-2.5-7B) on LendingClub credit risk data using zero-shot prompting to generate probability predictions. A custom prompt template converts tabular features into natural language descriptions, with LLMs returning JSON containing probability and explanation. LightGBM serves as an interpretable baseline, trained with specific hyperparameters (learning_rate=0.01, n_estimators=10000). SHAP PermutationExplainer (250 samples, k=5 background clusters) computes feature attributions for both models. The study compares performance metrics (ROC-AUC, PR-AUC) and analyzes feature importance alignment through rank correlation and directional consistency between SHAP values and LLM self-explanations.

## Key Results
- LightGBM outperformed all LLMs on ROC-AUC (0.73 vs. 0.67 max) and PR-AUC (0.91 vs. 0.88 max)
- LLMs correctly identified key risk features (Sub-grade, DTI, Interest Rate) but ranked them differently than LightGBM
- LLM self-explanations frequently diverged from SHAP-derived attributions, showing internal inconsistency
- Ensemble averaging of LLM predictions did not improve performance beyond individual model results

## Why This Works (Mechanism)

### Mechanism 1: Zero-Shot Probabilistic Classification via Natural Language Prompting
- Claim: LLMs can classify structured tabular data by converting feature-value pairs into natural language prompts and requesting probability outputs rather than discrete labels.
- Mechanism: The prompt template structures each loan application as a dictionary-style description; the LLM outputs a JSON with probability and explanation. This bypasses training pipelines entirely.
- Core assumption: LLMs' pre-training corpora contain sufficient financial domain knowledge to recognize risk patterns without task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "zero-shot LLM-based classifiers...through zero-shot prompting"
  - [section 4.2.2] "We did not perform any fine-tuning and used strict zero-shot inference without any in-context learning"
  - [corpus] Weak direct support; related papers (LendNova, "Measuring What LLMs Think They Do") explore similar prompting but without causal mechanism validation.
- Break condition: If pre-training data contains no financial risk concepts, zero-shot classification degrades to random guessing; performance gains would require fine-tuning or in-context examples.

### Mechanism 2: SHAP PermutationExplainer for Cross-Model Attribution Audit
- Claim: SHAP PermutationExplainer enables computationally tractable feature attribution for LLMs by scaling linearly with feature count rather than quadratically.
- Mechanism: The paper uses k-means clustering (C=5 centroids) to summarize background data, then runs T=4 permutations per instance. This reduces per-instance calls from ~2205 (KernelExplainer) to ~440, a 5× speedup.
- Core assumption: Permutation-based SHAP with limited permutations approximates true Shapley values sufficiently for relative feature importance comparisons.
- Evidence anchors:
  - [section 3.2.1] "speedup ≈ 2205/440 ≈ 5×"
  - [section 3.2.1] "PermutationExplainer scales linearly in M and avoids the regression solve"
  - [corpus] "Measuring What LLMs Think They Do" paper uses similar SHAP methodology for LLM audit, suggesting convergent validation of approach.
- Break condition: If T is too low or background clustering is unrepresentative, SHAP values become unstable; attributions may not reflect true feature contributions.

### Mechanism 3: Self-Explanation Faithfulness Divergence
- Claim: LLM-generated explanations can be internally inconsistent with their actual decision patterns, as measured by SHAP.
- Mechanism: LLMs produce plausible rationales based on learned priors (e.g., "higher DTI increases default risk") while their actual predictions may follow different patterns from the specific dataset.
- Core assumption: SHAP-derived attributions represent the "ground truth" of model behavior; deviations indicate unfaithfulness.
- Evidence anchors:
  - [section 5.4] "Gemma-2-9B displays a notable inconsistency: its SHAP values suggest a positive contribution at higher DTI levels, contradicting its own self-explanation"
  - [section 5.3] "higher Interest rate values are associated with a greater predicted probability of being fully repaid...whereas all three LLMs display the opposite trend"
  - [corpus] Related work cited (Huang et al., Turpin et al.) documents that LLM rationales "are often plausible but do not necessarily reflect internal reasoning."
- Break condition: If SHAP values themselves are unreliable (see Mechanism 2 break condition), the divergence measure becomes meaningless.

## Foundational Learning

- **SHAP (Shapley Additive Explanations)**:
  - Why needed here: The entire explainability audit framework depends on understanding how SHAP assigns feature contributions and its computational variants.
  - Quick check question: Given a model with 21 features, explain why PermutationExplainer is preferred over KernelExplainer for an LLM with expensive inference.

- **Gradient Boosting Decision Trees (LightGBM)**:
  - Why needed here: Serves as the interpretable baseline; understanding its feature split logic clarifies why it achieves higher AUC on structured data.
  - Quick check question: Why would LightGBM place higher importance on numerical features (Sub-grade, Annual income) compared to LLMs' broader distribution?

- **Zero-Shot vs Few-Shot Prompting**:
  - Why needed here: The paper deliberately uses zero-shot to prevent data contamination; understanding this distinction is critical for experimental design.
  - Quick check question: If you added 5 labeled examples to the prompt, would this still be a valid comparison to LightGBM trained on 316,800 examples? Why or why not?

## Architecture Onboarding

- **Component map**: LendingClub Dataset (396K rows, 21 features) -> LightGBM (trained) or LLM Prompt Engine (zero-shot, vLLM inference) -> Probabilities or Probabilities + Self-Explanations -> SHAP PermutationExplainer (k=5 centroids, T=4 perms) -> Feature Attribution Comparison -> Alignment Analysis (rank correlation, directional consistency)

- **Critical path**: Prompt template design -> probability extraction -> SHAP computation (250-instance sample) -> attribution comparison. Errors in probability parsing (e.g., non-JSON outputs) cascade to SHAP failures.

- **Design tradeoffs**:
  - **Sample size vs. compute**: 250 instances for SHAP balances coverage with LLM inference cost (~110K calls total).
  - **Background clustering**: C=5 centroids reduces compute but may underrepresent feature distribution tails.
  - **Ensemble method**: Simple averaging used; more sophisticated stacking might yield different complementarity results.

- **Failure signatures**:
  - LLM outputs non-JSON text -> probability parsing fails -> SHAP cannot compute.
  - High-cardinality features not excluded -> LLM exploits external priors (e.g., addresses) -> invalid comparison to LightGBM.
  - SHAP values near zero across all features -> model is not using input features (possible prompt formatting issue).

- **First 3 experiments**:
  1. **Baseline reproduction**: Run LightGBM training with provided hyperparameters; verify ROC-AUC ≈ 0.73. If significantly lower, check preprocessing (five excluded features).
  2. **SHAP sanity check**: For LightGBM, verify Sub-grade is top feature by mean |SHAP|. If not, investigate feature encoding.
  3. **Self-explanation audit on held-out feature**: Prompt LLM for directional impact on a feature NOT in top-5 by SHAP (e.g., Purpose); check if self-explanation claims high impact when SHAP shows low impact.

## Open Questions the Paper Calls Out
None

## Limitations
- The study evaluates only three LLM models without fine-tuning, potentially underrepresenting LLM capabilities in real-world deployment
- SHAP analysis uses a small sample (250 instances) and limited background clusters (k=5), which may miss rare but important feature interactions
- Exclusion of 5 high-cardinality features, while necessary for fair comparison, removes potentially predictive information that LLMs might otherwise leverage

## Confidence

**High Confidence (3 claims):**
- LightGBM outperforms LLMs on standard classification metrics (ROC-AUC, PR-AUC)
- LLMs correctly identify key risk features (Sub-grade, DTI, Interest Rate) as important
- LLM self-explanations often diverge from SHAP-derived attributions

**Medium Confidence (2 claims):**
- Zero-shot prompting is insufficient for high-stakes financial prediction
- SHAP PermutationExplainer provides reliable attribution for LLM audit

**Low Confidence (1 claim):**
- No ensemble improvement suggests fundamental LLM limitations (small sample size prevents definitive conclusion)

## Next Checks

1. **Feature Interaction Validation**: Re-run SHAP analysis with larger background clusters (k=10) and additional permutations (T=8) to verify stability of attribution rankings across different LLMs.

2. **Fine-tuning Threshold Test**: Implement a controlled fine-tuning experiment (5-50 labeled examples) to establish the minimal data requirement where LLMs match or exceed LightGBM performance.

3. **Temporal Consistency Audit**: Apply the same SHAP attribution framework to a different time period of LendingClub data to assess whether LLM self-explanation faithfulness holds across market conditions.