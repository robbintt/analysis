---
ver: rpa2
title: 'Question the Questions: Auditing Representation in Online Deliberative Processes'
arxiv_id: '2511.04588'
source_url: https://arxiv.org/abs/2511.04588
tags:
- questions
- participants
- question
- utility
- slate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first auditing framework for measuring
  justified representation (JR) in general utility settings, where participants' preferences
  for questions are inferred via cosine similarity of LLM embeddings. The authors
  develop efficient algorithms for auditing JR, with their best method achieving O(mn
  log n) runtime.
---

# Question the Questions: Auditing Representation in Online Deliberative Processes

## Quick Facts
- arXiv ID: 2511.04588
- Source URL: https://arxiv.org/abs/2511.04588
- Reference count: 16
- Primary result: First auditing framework for measuring justified representation (JR) in general utility settings for deliberative processes

## Executive Summary
This paper introduces a framework for auditing representation in online deliberative processes using the mathematical concept of justified representation (JR). The authors develop efficient algorithms to measure whether selected question slates adequately represent all participant groups, with their best method achieving O(mn log n) runtime. They apply this framework to 12 historical deliberation sessions, comparing human moderator selections to algorithmic approaches (extractive via integer programming and abstractive via LLMs). Across all sessions, algorithmic methods consistently produce more representative question slates than human moderators, with abstractive LLM-generated questions often matching or surpassing extractive selections.

## Method Summary
The method uses cosine similarity between LLM embeddings as a proxy for participant utility, then audits whether selected question slates satisfy JR—requiring that any group of n/k participants with similar preferences gets at least one question they like. The core algorithm detects the largest "deviating coalition" that could block a slate by preferring an alternative question. For extractive selection, they solve an integer program to find optimal slates; for abstractive selection, they prompt an LLM to generate representative questions. The framework is validated on 12 deliberation sessions from America in One Room and Meta Community Forums.

## Key Results
- Algorithmic methods (extractive and abstractive) consistently outperform human moderators in JR representation across 12 deliberation sessions
- Abstractive LLM-generated questions often match or surpass extractive selections in representativeness
- The efficient auditing algorithm achieves O(mn log n) runtime, enabling real-time platform integration
- Framework integrated into online deliberation platform used in over 50 countries

## Why This Works (Mechanism)

### Mechanism 1: Single-Pass Coalition Detection
If participant utilities are sorted, the auditing runtime for JR can be reduced from O(mn²) to O(mn log n), enabling real-time platform integration. The algorithm sorts participants by their utility for the slate and alternative question, then iterates through utility thresholds in a single pass while maintaining a counter for the largest "deviating coalition" and managing a blacklist of excluded participants.

### Mechanism 2: Semantic Similarity as Utility Proxy
Cosine similarity between LLM embeddings of a participant's proposed question and a candidate question serves as a viable proxy for "utility" or representation. This allows auditing of arbitrary LLM-generated questions that didn't exist during deliberation, avoiding the need to ask participants to rank every potential question.

### Mechanism 3: Abstractive Synthesis of Cohesive Groups
LLM-generated abstractive questions can satisfy JR constraints by synthesizing the core intent of a cohesive group, often matching or surpassing extractive selection. If these summaries capture the centroid of a large, cohesive group (size ≥ n/k), they prevent a "blocking coalition" from forming, effectively satisfying JR.

## Foundational Learning

- **Concept: Justified Representation (JR) in Social Choice**
  - Why needed: This is the mathematical definition of "fairness" used; without it, "representation" is subjective. JR requires that any group of n/k people with similar preferences gets at least one question they like.
  - Quick check: If you have 100 participants and can select 10 questions (k=10), what is the minimum group size that "deserves" a representative question? (Answer: 100/10 = 10)

- **Concept: Unit-Demand Utility**
  - Why needed: The paper assumes a participant's utility for a slate of questions is defined by their single most preferred item in that slate (v_i(W) = max_{q∈W} u_i(q)), not the sum.
  - Quick check: If a participant likes Question A (score 0.9) and Question B (score 0.8), is their utility for the slate {A, B} equal to 1.7 or 0.9? (Answer: 0.9)

- **Concept: Extractive vs. Abstractive Summarization**
  - Why needed: The system offers two modes. Extractive pulls verbatim questions (transparency); Abstractive generates new ones (efficiency/coherence).
  - Quick check: Which method would you choose if "trust" in the literal wording of participant questions is the highest priority? (Answer: Extractive)

## Architecture Onboarding

- **Component map:** Participant questions → Embedding Engine → Utility Matrix → Auditing Core → JR value; Optional: Optimization/Synthesis → Slate
- **Critical path:** Latency is dominated by the Embedding Engine (network calls) and the Sorting step (O(n log n)) inside the Auditing Core. The Auditing Core logic itself is lightweight once data is sorted.
- **Design tradeoffs:** IP vs. LLM—IP guarantees mathematically optimal slate but is computationally heavy (NP-hard); LLM is fast but stochastic (requires best-of-n sampling). Embedding Model—OpenAI vs. MiniLM—OpenAI has higher AUC (0.875) but external dependency; MiniLM is local but slightly lower accuracy.
- **Failure signatures:** High JR Value (>1.0) indicates large groups are unrepresented—check if embedding model is mismatched to language/domain. Empty Coalition suggests k is too small or diversity of opinion is too high.
- **First 3 experiments:** 1) Embedding Validation—run auditing pipeline on QQP subset to verify AUC scores (0.86-0.87). 2) Runtime Benchmark—generate synthetic data with n=500, m=1000; compare Naive O(mn²) vs. Single-Pass O(mn log n) runtime. 3) Ablation on k—on fixed session, sweep k from 3 to 15; plot JR value to see how many questions achieve α_JR ≤ 1.0.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do hybrid question slates, composed of both participant-provided and LLM-generated questions, yield higher representation scores than purely extractive or abstractive methods? The authors note their evaluation considers JR values separately and future work could explore hybrid approaches.

- **Open Question 2:** Do utility inferences based on cosine similarity of LLM embeddings align with the actual preferences of participants in live deliberations? The retrospective analysis limits validation with actual participant data; future studies could directly validate these utility inferences.

- **Open Question 3:** Can the auditing framework be extended to efficiently verify stronger representation axioms, such as Bronze Justified Representation (BJR)? The authors suggest future work could explore auditing other axioms offering stronger representation guarantees.

## Limitations

- Empirical validation relies on proprietary deliberation datasets (America in One Room, Meta Community Forums) that are not publicly available, limiting reproducibility and external validation.
- While cosine similarity achieves 0.86-0.88 AUC on Quora Question Pairs, this validation uses a general Q&A dataset—domain transfer to political deliberations remains uncertain.
- Integer programming approach becomes computationally expensive as n and m grow; the paper doesn't address practical limits on deliberation size.

## Confidence

- **High confidence:** The algorithmic speedup from O(mn²) to O(mn log n) for JR auditing is theoretically sound and clearly demonstrated.
- **Medium confidence:** The semantic similarity proxy for utility is reasonable given QQP validation, but domain transfer remains uncertain.
- **Medium confidence:** Empirical results showing algorithmic slates outperforming human moderators are compelling for the 12 studied sessions but may not generalize without broader testing.

## Next Checks

1. **Cross-domain embedding validation:** Test the embedding-based utility proxy on at least one non-political deliberation dataset (e.g., community planning, organizational decision-making) to assess domain generalizability.

2. **Participant perception study:** Conduct a user study where actual deliberation participants rate their satisfaction with algorithmically-selected vs. human-selected question slates, measuring whether JR improvements correlate with subjective representation.

3. **Scalability benchmark:** Systematically measure runtime and memory usage of both the auditing algorithm and integer programming optimization across varying n (100-10,000 participants) and m (50-5,000 questions) to establish practical limits for real-world deployment.