---
ver: rpa2
title: A Finite-State Controller Based Offline Solver for Deterministic POMDPs
arxiv_id: '2505.00596'
source_url: https://arxiv.org/abs/2505.00596
tags:
- belief
- policy
- state
- planning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DetMCVI, an offline algorithm for solving
  deterministic POMDPs (DetPOMDPs) that builds policies as finite-state controllers
  (FSCs). DetMCVI adapts Monte Carlo Value Iteration (MCVI) for goal-oriented DetPOMDPs
  by eliminating redundant policy rollouts, sampling, and belief storage through exploiting
  deterministic transitions.
---

# A Finite-State Controller Based Offline Solver for Deterministic POMDPs

## Quick Facts
- arXiv ID: 2505.00596
- Source URL: https://arxiv.org/abs/2505.00596
- Reference count: 13
- Primary result: State-of-the-art performance on large deterministic POMDP problems with compact policies

## Executive Summary
This paper introduces DetMCVI, an offline algorithm for solving deterministic POMDPs (DetPOMDPs) that builds policies as finite-state controllers (FSCs). The algorithm adapts Monte Carlo Value Iteration (MCVI) by exploiting deterministic transitions to eliminate redundant policy rollouts, sampling, and belief storage. DetMCVI achieves significant performance improvements over existing methods, producing compact policies (11-39 nodes versus 324-13,928 nodes for baselines) with high success rates across multiple synthetic domains and a real-world forest mapping experiment with a quadrupedal robot.

## Method Summary
DetMCVI is an offline solver that constructs finite-state controllers for deterministic POMDPs by systematically eliminating redundant computations inherent in the deterministic structure. The algorithm avoids belief point tracking and sampling by recognizing that each action uniquely determines the next state given current belief. It eliminates redundant policy rollouts by caching action values for each observation, ensuring that once an observation is encountered, its optimal action is computed once and reused. The FSC representation uses decision nodes with observation-based transitions and action nodes, allowing compact policy representation while maintaining the ability to handle combinatorial state spaces that are intractable for exact methods.

## Key Results
- Consistently produces compact policies (11-39 nodes) versus baselines (324-13,928 nodes) across multiple domains
- Achieves 100% success rates on synthetic domains including Canadian Traveler Problem, Wumpus World, Maze, and Sort
- 95% success rate in real-world forest mapping experiment with only 24-node policies versus baseline failures within time constraints

## Why This Works (Mechanism)
DetMCVI exploits the deterministic nature of transitions in DetPOMDPs to eliminate three major computational bottlenecks in traditional POMDP solvers: belief tracking, sampling, and redundant rollouts. By recognizing that each action uniquely determines the next state given current belief, the algorithm can avoid maintaining and updating belief distributions entirely. The caching mechanism for action values prevents recomputation when the same observation occurs multiple times during planning, dramatically reducing computational overhead. The finite-state controller representation allows the algorithm to handle combinatorial state spaces that would be intractable for exact value iteration methods while maintaining convergence guarantees similar to Goal-HSVI under appropriate conditions.

## Foundational Learning

Deterministic POMDPs: Special case where state transitions are deterministic given actions, eliminating the need for belief tracking and sampling.
Why needed: Forms the theoretical foundation that enables DetMCVI's computational optimizations.
Quick check: Verify that for any action in a DetPOMDP, the next state is uniquely determined by the current state.

Finite-State Controllers: Policy representation using decision nodes with observation-based transitions and action nodes.
Why needed: Provides compact representation that scales to large state spaces while maintaining optimality guarantees.
Quick check: Ensure each decision node has outgoing edges for all possible observations in the domain.

Value Caching: Technique to store and reuse computed action values for observations encountered during planning.
Why needed: Eliminates redundant rollouts that would otherwise occur when the same observation appears multiple times.
Quick check: Verify that cached values are properly invalidated when policy improvements are made.

## Architecture Onboarding

Component Map: Planning Engine -> Value Function Updater -> FSC Builder -> Policy Executor
Critical Path: Initialize FSC -> Generate Rollouts -> Update Values -> Improve Policy -> Convergence Check
Design Tradeoffs: Deterministic assumption enables massive computational savings but limits applicability to stochastic domains
Failure Signatures: Policy divergence or poor performance when deterministic assumption is violated
First Experiments:
1. Canadian Traveler Problem with varying numbers of blocked edges to test scalability
2. Wumpus World with increasing grid sizes to evaluate policy compactness
3. Forest mapping with varying tree densities to assess real-world performance

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Specialized for deterministic POMDPs, limiting applicability to stochastic domains
- Evaluation relies heavily on synthetic domains with known structure
- Single real-world demonstration in forest mapping domain

## Confidence
High confidence in algorithm correctness for deterministic POMDPs
Medium confidence in scalability benefits from synthetic domain evaluation
Medium confidence in practical applicability based on single real-world demonstration

## Next Checks
1. Test DetMCVI on stochastic POMDP variants with small noise levels to quantify performance degradation
2. Implement and benchmark against additional offline POMDP solvers on the same domains
3. Apply DetMCVI to at least two additional real-world POMDP applications with different characteristics