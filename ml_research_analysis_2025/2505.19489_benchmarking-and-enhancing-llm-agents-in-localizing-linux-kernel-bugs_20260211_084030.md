---
ver: rpa2
title: Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs
arxiv_id: '2505.19489'
source_url: https://arxiv.org/abs/2505.19489
tags:
- linux
- kernel
- files
- agents
- software
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LINUX FLBENCH, a new benchmark of 250 real-world
  Linux kernel bug localization tasks. Existing LLM agents achieve only 36.8%-41.6%
  accuracy on this benchmark, significantly underperforming their results on general
  software systems (SWE-bench) by 16.7%-31.9%.
---

# Benchmarking and Enhancing LLM Agents in Localizing Linux Kernel Bugs

## Quick Facts
- **arXiv ID**: 2505.19489
- **Source URL**: https://arxiv.org/abs/2505.19489
- **Reference count**: 40
- **Primary result**: LINUX FL+ framework improves Linux kernel bug localization accuracy by 7.2%-11.2% across all metrics with minimal cost

## Executive Summary
This paper introduces LINUX FLBENCH, a new benchmark of 250 real-world Linux kernel bug localization tasks. Existing LLM agents achieve only 36.8%-41.6% accuracy on this benchmark, significantly underperforming their results on general software systems (SWE-bench) by 16.7%-31.9%. The Linux kernel's large codebase, limited observability, and diverse impact factors make localization challenging. To address these issues, the authors propose LINUX FL+, an enhancement framework that refines agent predictions using directory-aware expansion and potential cause expansion leveraging Linux kernel mailing list knowledge. LINUX FL+ improves agent accuracy by 7.2%-11.2% across all metrics with minimal cost, demonstrating substantial effectiveness in localizing Linux kernel bugs.

## Method Summary
The authors create LINUX FLBENCH, a benchmark of 250 Linux kernel bug localization tasks with real-world bug reports, buggy codebases, and ground-truth locations. They then develop LINUX FL+, an enhancement framework that builds on existing LLM agents through three key mechanisms: (1) Directory-Aware Expansion that re-selects from sibling files in predicted directories, (2) Direct Hypothesis using LLM intrinsic knowledge to generate diverse causes, and (3) Mail-Augmented Hypothesis using RAG over Linux Kernel Mailing List (LKML) to retrieve historical patch discussions. These predictions are merged via reciprocal rank fusion and re-ranked with semantic verification. The framework is evaluated across three backbone agents (SWE-Agent, AutoCodeRover, Agentless) on the LINUX FLBENCH benchmark, showing consistent improvements in recall metrics.

## Key Results
- LINUX FLBENCH benchmark shows existing agents achieve only 36.8%-41.6% accuracy, 16.7%-31.9% lower than SWE-bench performance
- LINUX FL+ improves agent accuracy by 7.2%-11.2% across all metrics with minimal cost (~$0.04/task)
- Mail-Augmented Hypothesis achieves highest individual recall@1 of 0.488, outperforming Direct Hypothesis alone (0.316)
- Complementary signal integration via score fusion (0.516 recall@1) outperforms any single strategy

## Why This Works (Mechanism)

### Mechanism 1: Directory-Aware Expansion Recovers Co-located Files
- Claim: Agents correctly identify buggy directories but fail to pinpoint exact files within them.
- Mechanism: By expanding the candidate set to all files in predicted directories and re-selecting via LLM, the system gives a second chance to files that share structural proximity with initial predictions.
- Core assumption: Buggy files tend to co-locate with related files in the same directory (evidenced by ~16 files per directory in Linux vs ~8 in SWE-bench).
- Evidence anchors:
  - [section 4.3]: "agents might be capable of coarse-grained FL (e.g., correctly identifying the buggy directories or high-level modules), they struggle to further precisely pinpoint the exact faulty file/method among all the related files"
  - [section 5.3.2, Table 6]: Directory-Aware Expansion alone improves SWE-Agent Recall@1 from 0.416 to 0.448
  - [corpus]: AgentFL paper confirms scaling FL to project-level context is a key challenge
- Break condition: When buggy files are distributed across multiple unrelated directories, this expansion adds noise without benefit.

### Mechanism 2: Mail-Augmented Hypothesis Injects Domain-Specific Historical Knowledge
- Claim: LLMs lack sufficient in-depth Linux kernel debugging knowledge from pre-training alone.
- Mechanism: Retrieval from Linux Kernel Mailing List (LKML) provides historical patch discussions that suggest diverse root causes, guiding the LLM to hypothesize causes it would not otherwise consider.
- Core assumption: Relevant historical patches exist in LKML and can be retrieved via file-based filtering + BM25 similarity before the bug report date.
- Evidence anchors:
  - [section 5.1.2]: "Mail-Augmented Hypothesis consistently outperforms Direct Hypothesis... with the assistance of mail knowledge, Mail-Augmented Hypothesis achieves a recall@1 as high as 0.488"
  - [section 5.3.2, Table 6]: Direct Hypothesis alone achieves only 0.316 Recall@1; Mail-Augmented reaches 0.488
  - [corpus]: CrashFixer and RGym papers similarly emphasize kernel-specific knowledge gaps in general LLMs
- Break condition: When LKML contains no relevant historical discussions for novel bug types, or when temporal filtering excludes all relevant patches.

### Mechanism 3: Complementary Signal Integration via Score Fusion
- Claim: Different expansion strategies capture distinct sets of correct localizations.
- Mechanism: Reciprocal rank fusion (score = 1/R_dir + 1/R_direct + 1/R_mail) prioritizes files consistently ranked highly across multiple signals, while re-ranking adds semantic verification.
- Core assumption: Each strategy has independent probability of finding the correct file; their union coverage exceeds any individual strategy.
- Evidence anchors:
  - [section E, Figure 9]: Venn diagram shows each strategy uniquely identifies bugs others miss
  - [section 5.3.2, Table 6]: Merging all strategies (0.516) outperforms any single strategy (max 0.488)
  - [corpus]: Weak direct corpus evidence; complementary signal fusion is a standard IR technique but not extensively studied for kernel FL
- Break condition: When all strategies systematically miss the same bug category, integration cannot recover it.

## Foundational Learning

- Concept: Fault Localization (FL) Task Formulation
  - Why needed here: The entire paper frames FL as BR, C → list(C); understanding this mapping is prerequisite to interpreting all results.
  - Quick check question: Given a bug report describing "system hangs on shutdown" and a codebase of 30K files, what does FL output and how is it evaluated?

- Concept: Retrieval-Augmented Generation (RAG) with Temporal Constraints
  - Why needed here: Mail-Augmented Hypothesis uses RAG over LKML with date filtering to prevent data leakage.
  - Quick check question: Why must retrieved emails be restricted to those sent before the bug report date?

- Concept: Reciprocal Rank Fusion
  - Why needed here: LINUX FL+ merges three ranked lists using score(f) = 1/R_dir + 1/R_direct + 1/R_mail.
  - Quick check question: If a file appears at rank 1 in one list but is absent from others, what is its final score?

## Architecture Onboarding

- Component map:
  - Input: Bug report (title, description, metadata) + kernel codebase (~69K files)
  - Stage 1: Base LLM Agent (SWE-Agent/AutoCodeRover/Agentless) → initial predicted files
  - Stage 2a: Directory-Aware Expansion → re-select from sibling files
  - Stage 2b: Direct Hypothesis → LLM generates causes without external knowledge
  - Stage 2c: Mail-Augmented Hypothesis → RAG over LKML → cause generation
  - Stage 3: Score fusion + semantic re-ranking → final ranked list

- Critical path:
  1. Bug report preprocessing (summarization for mail retrieval)
  2. Initial agent prediction (most expensive: 72-206K tokens)
  3. Parallel expansion (directory scan + LLM calls + mail retrieval)
  4. Score fusion and re-ranking (cheap: ~12-15K tokens, ~$0.04/task)

- Design tradeoffs:
  - Token cost vs coverage: LINUX FL+ adds ~15K tokens but achieves 10.8-11.2% Recall@1 gains
  - Precision vs recall: Expansion strategies increase Recall@10 more than Recall@1 (e.g., +24.8% vs +11.2% for AutoCodeRover)
  - Recency vs relevance: Temporal mail filtering ensures valid ground truth but may exclude useful older discussions

- Failure signatures:
  - Confusion among related files: Agent predicts files in correct directory but wrong file (see Appendix C.1 ACPI example)
  - Limited cause exploration: Agent focuses on obvious causes, misses deeper root causes (see Appendix C.2 shutdown hang example)
  - Knowledge gap: Direct Hypothesis alone achieves only 0.316 Recall@1, indicating pre-training knowledge insufficient

- First 3 experiments:
  1. Reproduce baseline agent performance: Run SWE-Agent on 10-sample subset of LINUX FLBENCH, verify ~40% Recall@1
  2. Ablate expansion strategies: Apply each of Directory-Aware, Direct, Mail-Augmented independently; confirm complementary coverage per Figure 9
  3. Test retrieval quality: Manually inspect top-3 retrieved emails for 5 bug reports; verify relevance before temporal cutoff date

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LINUX FL+ perform when integrated with backbone LLMs other than GPT-4o (e.g., Claude, open-source models like Llama or Mistral)?
- Basis in paper: [explicit] Section 6 states: "we conducted limited investigation into its effectiveness when integrated with other LLMs" and the authors used only GPT-4o to maintain consistency with prior work.
- Why unresolved: The enhancement framework was evaluated exclusively with GPT-4o, leaving its transferability to other model families unknown.
- What evidence would resolve it: Evaluation of LINUX FL+ applied to agents using diverse backbone LLMs on LINUX FLBENCH, measuring Recall@1/5/10 and cost metrics.

### Open Question 2
- Question: Can more sophisticated retrieval and filtering techniques for Linux kernel mailing list data further improve fault localization accuracy?
- Basis in paper: [explicit] Section 6 acknowledges "rough usage of mail data" and notes that "mail content may still contain irrelevant or outdated discussions... there remains room for improvement."
- Why unresolved: The current approach uses basic heuristics (filtering by patch presence, excluding non-atomic patches) and BM25 retrieval without advanced semantic filtering.
- What evidence would resolve it: Comparison of enhanced mail processing pipelines (e.g., semantic clustering, temporal decay weighting, relevance classification) against the current BM25 baseline.

### Open Question 3
- Question: What techniques can significantly improve method-level fault localization accuracy for the Linux kernel, which remains below 14% even with LINUX FL+?
- Basis in paper: [explicit] Section 5.3.2 notes method-level accuracy remains "relatively lower than at the file level, highlighting the need for further research in this direction."
- Why unresolved: Current approach only uses skeleton representations for method-level prediction; the unique challenges of C code (macros, inline functions, architecture-specific code) in the kernel are not addressed.
- What evidence would resolve it: Development of kernel-specific method-level FL techniques (e.g., call graph analysis, configuration-aware code representation) demonstrating substantial gains on method-level metrics.

## Limitations

- The completeness and quality of the LKML corpus is uncertain - the paper doesn't specify the exact date range, volume, or preprocessing pipeline for the mailing list data
- Temporal filtering of emails (must predate bug reports) may exclude relevant historical discussions if filtering is too strict
- The evaluation focuses only on post-2023 bugs with bugzilla IDs, potentially creating temporal bias in both the benchmark and knowledge base
- The adaptation of baseline agents for kernel code is described as custom but not fully specified, making exact reproduction challenging

## Confidence

- **High Confidence**: LINUX FLBENCH benchmark creation and baseline agent performance (36.8-41.6% accuracy). The benchmark construction methodology is well-documented and reproducible.
- **Medium Confidence**: Directory-Aware Expansion mechanism (7.2-11.2% improvement). The mechanism is clearly described and grounded in observed agent behavior, though directory structure assumptions may not generalize to all kernel modules.
- **Medium Confidence**: Mail-Augmented Hypothesis effectiveness (up to 11.2% improvement). The RAG approach is sound, but results depend heavily on LKML corpus quality and retrieval accuracy.
- **Medium Confidence**: Complementary signal integration via score fusion. The mechanism is standard but the specific fusion weights and their optimality for kernel FL aren't extensively validated.

## Next Checks

1. **LKML Corpus Validation**: Manually inspect the top-5 retrieved emails for 10 randomly selected bug reports to verify: (a) temporal filtering correctly excludes post-bug emails, (b) BM25 retrieval relevance scores are meaningful, and (c) the knowledge actually improves hypothesis generation quality.

2. **Directory Structure Assumption Test**: For 20 bugs where agents correctly identify directories but miss exact files, analyze whether the buggy files actually co-locate with other files in those directories. If many buggy files are isolated or distributed across multiple directories, directory-aware expansion may add noise.

3. **Blind Reproduction of 50-Bug Subset**: Implement LINUX FL+ using only the documented methodology (without additional unpublished optimizations) on a held-out 50-bug subset of LINUX FLBENCH. Compare Recall@1 improvements against the claimed 11.2% maximum gain to verify the framework's effectiveness isn't dependent on cherry-picked examples or undocumented prompt engineering.