---
ver: rpa2
title: 'Light Aircraft Game : Basic Implementation and training results analysis'
arxiv_id: '2506.14164'
source_url: https://arxiv.org/abs/2506.14164
tags:
- happo
- agent
- learning
- reward
- hasac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper evaluates two multi-agent reinforcement learning algorithms\u2014\
  HAPPO (on-policy) and HASAC (off-policy)\u2014in a partially observable aerial combat\
  \ environment (LAG). HAPPO, based on PPO, performs better in dynamic missile combat\
  \ due to its expressive policy updates, while HASAC, derived from SAC, excels in\
  \ simpler coordination tasks without weapons due to greater sample efficiency and\
  \ stability."
---

# Light Aircraft Game : Basic Implementation and training results analysis

## Quick Facts
- arXiv ID: 2506.14164
- Source URL: https://arxiv.org/abs/2506.14164
- Reference count: 4
- Two MARL algorithms evaluated in aerial combat: HAPPO outperforms in missile combat; HASAC excels in basic coordination

## Executive Summary
This paper evaluates two multi-agent reinforcement learning algorithms—HAPPO (on-policy) and HASAC (off-policy)—in a partially observable aerial combat environment (LAG). HAPPO, based on PPO, performs better in dynamic missile combat due to its expressive policy updates, while HASAC, derived from SAC, excels in simpler coordination tasks without weapons due to greater sample efficiency and stability. Experiments show HAPPO achieving higher rewards (up to 1,090) in ShootMissile tasks but struggling in NoWeapon settings (returns as low as -96), whereas HASAC maintains stable, positive performance in NoWeapon tasks (around 30) but lower peak rewards in missile tasks (around 468). Training analysis reveals HAPPO's sensitivity to policy updates and critic instability, while HASAC demonstrates smoother learning curves. The results highlight a trade-off: off-policy methods are more stable for basic interactions, while on-policy methods are better suited for complex, expressive behaviors.

## Method Summary
The study implements HAPPO (based on PPO) and HASAC (based on SAC) for multi-agent reinforcement learning in a partially observable Light Aircraft Game environment. HAPPO uses on-policy updates with PPO's clipped surrogate objective, while HASAC employs off-policy learning with SAC's maximum entropy framework. Both algorithms were tested in two scenarios: ShootMissile (with weapons) and NoWeapon (without weapons). The environment provides partial observability through local observations, and agents must learn cooperative strategies. Training involved standard hyperparameter tuning with Adam optimizer, and performance was evaluated using cumulative rewards over training episodes.

## Key Results
- HAPPO achieves higher rewards in ShootMissile tasks (up to 1,090) compared to HASAC (around 468)
- HASAC maintains stable positive performance in NoWeapon tasks (around 30) while HAPPO struggles (returns as low as -96)
- Training curves show HAPPO's sensitivity to policy updates and critic instability, while HASAC demonstrates smoother learning
- Off-policy methods show greater stability in basic coordination tasks, while on-policy methods excel in complex missile combat

## Why This Works (Mechanism)
The performance differences stem from fundamental algorithmic characteristics. HAPPO's on-policy updates allow for more expressive policy changes that better capture the dynamic nature of missile combat, where rapid adaptation is crucial. The clipped surrogate objective prevents destructive policy updates while maintaining flexibility. HASAC's off-policy nature enables more efficient sample reuse, which proves valuable in the NoWeapon scenario where learning basic coordination patterns is more important than complex tactical maneuvers. The maximum entropy framework in HASAC also encourages exploration, helping agents discover effective coordination strategies in simpler environments.

## Foundational Learning

**Multi-Agent Reinforcement Learning (MARL)**: Why needed - Enables multiple agents to learn cooperative strategies in shared environments; Quick check - Verify agents can coordinate actions to achieve joint rewards.

**Partially Observable Markov Decision Processes (POMDPs)**: Why needed - Models environments where agents have limited local observations; Quick check - Confirm observation spaces capture sufficient information for decision-making.

**On-policy vs Off-policy learning**: Why needed - Determines sample efficiency and stability trade-offs; Quick check - Compare learning curves for both approaches under identical conditions.

**Maximum Entropy Reinforcement Learning**: Why needed - Encourages exploration and prevents premature convergence; Quick check - Monitor entropy levels during training to ensure adequate exploration.

**Policy Gradient Methods**: Why needed - Provides gradient-based optimization for continuous control; Quick check - Verify gradient magnitudes remain stable during training.

## Architecture Onboarding

Component Map: Environment -> Observation Processing -> Policy Network -> Action Selection -> Reward Collection -> Policy Update

Critical Path: Observation → Policy Network → Action → Environment → Reward → Policy Update → New Policy

Design Tradeoffs:
1. On-policy (HAPPO) vs Off-policy (HASAC): HAPPO allows more expressive updates but requires more samples; HASAC is more sample-efficient but may converge to suboptimal policies.
2. Maximum entropy (HASAC) vs Standard (HAPPO): HASAC encourages exploration but may slow convergence; HAPPO focuses on exploitation of known good policies.
3. Critic architecture: HAPPO's critic updates are more sensitive to policy changes, while HASAC's separate critic provides more stable learning targets.

Failure Signatures:
- HAPPO: Unstable learning curves, large policy updates causing performance drops, critic instability
- HASAC: Slower adaptation to new situations, potential premature convergence to suboptimal policies

First Experiments:
1. Compare single-agent vs multi-agent performance to isolate coordination challenges
2. Test algorithms with full observability to determine impact of partial observability
3. Vary reward shaping parameters to assess sensitivity to reward design

## Open Questions the Paper Calls Out
None

## Limitations
- Limited environment scope: Only two scenarios tested (ShootMissile and NoWeapon) may not generalize to broader aerial combat situations
- Lack of statistical analysis: Performance comparisons based on single reward metrics without variance analysis across multiple runs
- Missing ablation studies: No isolation of algorithmic effects from implementation differences through controlled comparisons
- Computational efficiency: Performance metrics don't account for differences in training time and resource requirements

## Confidence
- HAPPO's superior performance in missile combat (High confidence): Based on reward metrics up to 1,090
- HASAC's stability in NoWeapon tasks (High confidence): Well-supported by consistent positive returns around 30
- Off-policy stability claim (Medium confidence): Lacks ablation studies to isolate algorithmic factors
- Reward shaping impact (High confidence): Performance differences likely influenced by reward design choices

## Next Checks
1. Conduct statistical significance testing across multiple random seeds for both algorithms in each environment
2. Test algorithm performance across a broader range of aerial combat scenarios beyond the two current environments
3. Perform ablation studies comparing on-policy vs off-policy variants with identical network architectures to isolate algorithmic effects from implementation differences