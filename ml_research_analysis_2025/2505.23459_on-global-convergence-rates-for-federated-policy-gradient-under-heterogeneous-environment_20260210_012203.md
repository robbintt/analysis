---
ver: rpa2
title: On Global Convergence Rates for Federated Policy Gradient under Heterogeneous
  Environment
arxiv_id: '2505.23459'
source_url: https://arxiv.org/abs/2505.23459
tags:
- lemma
- policy
- have
- where
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of federated policy gradient methods
  in heterogeneous reinforcement learning environments, where different agents interact
  with different MDPs. The authors show that heterogeneity can necessitate non-deterministic
  or even time-varying optimal policies, unlike classical RL.
---

# On Global Convergence Rates for Federated Policy Gradient under Heterogeneous Environment

## Quick Facts
- **arXiv ID:** 2505.23459
- **Source URL:** https://arxiv.org/abs/2505.23459
- **Reference count:** 40
- **Primary result:** Introduces three federated policy gradient algorithms with convergence guarantees to near-optimal policies in heterogeneous RL environments, achieving linear speed-up with number of agents.

## Executive Summary
This paper addresses federated policy gradient methods for reinforcement learning in heterogeneous environments where different agents interact with different Markov decision processes. The authors show that heterogeneity necessitates non-deterministic optimal policies and can introduce a fixed optimality gap. They propose three algorithms—S-FedPG, RS-FedPG, and b-RS-FedPG—that converge to near-optimal policies under local Łojasiewicz conditions, with RS-FedPG achieving logarithmic communication complexity through entropy regularization and b-RS-FedPG using bit-level softmax parameterization for bounded updates. The theoretical framework provides explicit convergence rates and demonstrates linear speed-up in the number of agents.

## Method Summary
The paper introduces three federated policy gradient algorithms for heterogeneous RL environments. S-FedPG performs standard federated averaging with local stochastic gradient ascent steps. RS-FedPG adds entropy regularization to achieve faster convergence rates by satisfying a stronger Polyak-Łojasiewicz condition. b-RS-FedPG uses a bit-level softmax parameterization that converts multi-action problems into binary ones, enabling bounded policy updates through projection onto a compact set. All algorithms rely on local agents performing REINFORCE updates on their respective MDPs, with a central server aggregating parameters. The theoretical analysis uses local Łojasiewicz conditions to establish convergence to a neighborhood of the optimal policy, with the size of this neighborhood proportional to the heterogeneity between agents' transition kernels.

## Key Results
- Convergence guarantees to near-optimal policies under local Łojasiewicz conditions in heterogeneous RL environments
- Linear speed-up in the number of agents for all three algorithms
- RS-FedPG achieves logarithmic communication complexity through entropy regularization
- b-RS-FedPG provides bounded policy updates and better theoretical guarantees for large action spaces
- Empirical validation on synthetic and gridworld environments confirms theoretical findings

## Why This Works (Mechanism)

### Mechanism 1: Local Łojasiewicz Condition with Heterogeneity Bias
Standard federated averaging assumes homogeneous data or convexity. This work shows that individual agent value functions satisfy a gradient dominance condition (Łojasiewicz inequality). By averaging local updates, the global objective improves, but the discrepancy between agent transition kernels (ε_P) creates a persistent bias term ζ² that prevents convergence to the exact global optimum, resulting in an ε-approximation where ε ∝ ε_P. The core assumption is that a local non-uniform Łojasiewicz inequality holds for each agent's objective function individually. If heterogeneity ε_P grows too large relative to the Łojasiewicz constant μ_sm, the neighborhood of convergence may become trivial (too wide to be useful).

### Mechanism 2: Entropy Regularization for Linear Convergence
Adding entropy regularization (λH^ρ_c(θ)) to the objective transforms the local optimization landscape, allowing for linear convergence rates (logarithmic communication complexity) rather than the slower polynomial rates of the vanilla softmax approach. The regularization term satisfies a stronger Polyak-Łojasiewicz (PL) condition where the sub-optimality gap is linear rather than quadratic. This geometric property allows the Ascent Lemma to drive the error to a neighborhood exponentially faster. Failure occurs if the regularization parameter λ is set so high that it overwhelms the reward signal, or if the bias from truncated trajectories (T) dominates the regularization benefit.

### Mechanism 3: Bit-Level Softmax for Projection Stability
For action spaces |A| ≥ 3, the gradient field does not behave radially, making safe projection difficult. By re-parameterizing the policy as a sequence of binary decisions (bit-level), the problem is reduced to |A|=2. In the binary case, the gradient field is radial outside a specific ball, allowing a projection onto a bounded set T that provably increases the objective value while keeping the policy bounded. The construction of the bit-level FRL instance and the radial property for |A|=2 enables this stability. If the bit-level decomposition significantly distorts the effective discount factor γ̄ = γ^(1/k) or state space, the convergence speed may degrade.

## Foundational Learning

- **Concept: Federated Averaging (FedAvg)**
  - **Why needed here:** The paper's core engine is a variant of FedAvg applied to policy gradients. Understanding the trade-off between local steps (H) and communication rounds (R) and how "client drift" occurs is crucial.
  - **Quick check question:** How does increasing the number of local steps H affect the "drift error" term in the Ascent Lemma?

- **Concept: Policy Gradient (PG) & Softmax Parameterization**
  - **Why needed here:** The algorithms operate on the probability simplex using softmax. Understanding the landscape (non-convexity, spurious stationary points) is crucial to see why the Łojasiewicz condition is a breakthrough.
  - **Quick check question:** Why does the vanilla softmax parameterization not guarantee global convergence in general non-convex settings without the Łojasiewicz condition?

- **Concept: Łojasiewicz Inequality**
  - **Why needed here:** This is the theoretical "glue" of the paper. It is a relaxation of strong convexity that connects gradient norm to function suboptimality.
  - **Quick check question:** In Equation (5), what does the term μ_sm,c(θ) represent physically regarding the policy landscape?

## Architecture Onboarding

- **Component map:** Agents (M) -> Local MDP execution -> REINFORCE gradient computation -> Server aggregation -> Parameter averaging/projection -> Global parameter update

- **Critical path:**
  1. **Initialization:** Set θ_0 = 0 (critical for b-RS-FedPG)
  2. **Local Update:** Agents sample trajectories Z_{r,h} and compute gradients g^Z
  3. **Aggregation:** Average parameters
  4. **Projection:** (b-RS-FedPG) Project onto B(λ) to ensure boundedness and positive μ

- **Design tradeoffs:**
  - **S-FedPG:** Simplest, but slow polynomial convergence. Good for low-heterogeneity settings.
  - **RS-FedPG:** Faster convergence via entropy. Requires tuning λ.
  - **b-RS-FedPG:** Best theoretical guarantees (explicit constants) and efficient for large action spaces (O(log |A|)). However, high implementation complexity due to bit-level MDP construction.

- **Failure signatures:**
  - **Convergence to Suboptimal Point:** Occurs if heterogeneity ε_P is too high; the "neighborhood" of convergence covers the whole space.
  - **Gradient Explosion:** In S-FedPG/RS-FedPG without projection, parameters θ can diverge.
  - **Slow Convergence:** If step size η is too small or batch size B insufficient to reduce variance.

- **First 3 experiments:**
  1. **Vary M (Agents):** Verify "linear speed-up" (Figure 1a). Plot global objective vs. communication rounds for M ∈ {2, 10, 50}.
  2. **Vary Heterogeneity ε_P:** Test the robustness of the "near-optimal" guarantee. Increase ε_P from 0.0 to ≫ 0.3 and observe the floor of the objective function (Figures 1b, 1c).
  3. **Bit vs. Standard:** Compare b-RS-FedPG against RS-FedPG on a large-action GridWorld to validate the O(log |A|) efficiency and stability benefits.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees hinge on the non-uniform Łojasiewicz condition, which is a strong assumption not universally satisfied in RL
- The bit-level parameterization introduces significant implementation complexity through MDP construction that may incur substantial computational overhead
- The fixed approximation gap ζ² proportional to heterogeneity ε_P is a fundamental limitation that may result in poor performance in highly heterogeneous settings

## Confidence
- **High Confidence:** The core theoretical framework connecting federated averaging to policy gradient methods under heterogeneity, including the derivation of convergence rates for S-FedPG and RS-FedPG.
- **Medium Confidence:** The bit-level softmax parameterization and its ability to enforce bounded policy updates for convergence guarantees.
- **Low Confidence:** The empirical demonstration of the linear speed-up with the number of agents (M) and the precise scaling of the approximation gap ζ² with heterogeneity ε_P.

## Next Checks
1. **Łojasiewicz Condition Verification:** For the tested gridworld and synthetic environments, explicitly measure and report the empirical values of the Łojasiewicz constants μ_sm,c(θ) and μ_sm(θ) to validate Assumption A-1/A-2.
2. **Heterogeneity Scaling Experiment:** Systematically vary the heterogeneity parameter ε_P over a wider range (e.g., ε_P ∈ [0.0, 0.8]) and quantify the resulting approximation gap ζ² and convergence neighborhood to test the robustness of the "near-optimal" guarantee.
3. **Bit-Level MDP Overhead:** Implement and benchmark the bit-level MDP construction for b-RS-FedPG on a standard RL environment (e.g., Atari games) to measure the computational overhead and verify that the theoretical efficiency gains (O(log |A|)) translate to practical performance improvements.