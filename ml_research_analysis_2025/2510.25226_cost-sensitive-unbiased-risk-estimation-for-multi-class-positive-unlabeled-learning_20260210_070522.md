---
ver: rpa2
title: Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled
  Learning
arxiv_id: '2510.25226'
source_url: https://arxiv.org/abs/2510.25226
tags:
- learning
- unlabeled
- class
- risk
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of learning from multi-class
  positive and unlabeled (MPU) data, where only a subset of classes is labeled and
  the unlabeled data contains a mixture of all classes. The proposed method, CSMPU,
  addresses two key limitations of existing MPU approaches: the lack of unbiased risk
  estimation and instability due to class imbalance and unlabeled data.'
---

# Cost-Sensitive Unbiased Risk Estimation for Multi-Class Positive-Unlabeled Learning

## Quick Facts
- arXiv ID: 2510.25226
- Source URL: https://arxiv.org/abs/2510.25226
- Authors: Miao Zhang; Junpeng Li; Changchun Hua; Yana Yang
- Reference count: 40
- Primary result: CSMPU achieves robust one-vs-rest detection for each observed class in multi-class PU learning, demonstrating consistent improvements in accuracy and stability over strong baselines.

## Executive Summary
This paper tackles the challenge of learning from multi-class positive and unlabeled (MPU) data, where only a subset of classes is labeled and the unlabeled data contains a mixture of all classes. The proposed method, CSMPU, addresses two key limitations of existing MPU approaches: the lack of unbiased risk estimation and instability due to class imbalance and unlabeled data. CSMPU introduces a cost-sensitive unbiased risk estimator based on adaptive loss weighting, assigning distinct weights to positive and inferred-negative loss components to ensure an unbiased estimate of the target risk. The method also incorporates a non-negativity correction to stabilize training. Theoretical analysis provides a generalization error bound for the proposed estimator. Extensive experiments on eight public datasets, spanning varying class priors and numbers of classes, demonstrate consistent improvements in both accuracy and stability over strong baselines. CSMPU achieves robust one-vs-rest detection for each observed class, making it a practical and effective solution for multi-class PU learning.

## Method Summary
CSMPU reformulates multi-class classification as per-class one-vs-rest tasks using constant-sum loss surrogates that enable unbiased risk estimation without explicit negative labels. The method employs cost-sensitive OVR loss weighting to stabilize learning under class imbalance, assigning maximum penalty when misclassifying positives as the unlabeled remainder. Class priors are estimated using NP lower bounds with penalized L1 moment matching. The empirical risk aggregates weighted positive and inferred-negative losses, then applies ReLU non-negativity correction to prevent instability. The framework is validated across 8 datasets with varying class priors and numbers of classes, demonstrating consistent improvements in accuracy and stability.

## Key Results
- CSMPU achieves consistent improvements in accuracy and stability over strong baselines on eight public datasets spanning varying class priors and numbers of classes.
- The method demonstrates robust one-vs-rest detection for each observed class, with experimental results showing improved performance particularly in low-prior scenarios.
- Theoretical analysis provides generalization error bounds that show linear degradation with prior misspecification, validated through empirical sensitivity studies.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cost-sensitive OVR loss weighting stabilizes MPU learning under class imbalance.
- **Mechanism:** The method assigns Cyk = 1 (maximum penalty) when misclassifying a positive from class y as the negative meta-class k, and Cyj = 0 for confusion among observed classes. This explicitly prioritizes "positive vs. negative" discrimination for each observed class, which is the critical decision boundary in MPU where unlabeled data contains mixed classes.
- **Core assumption:** The most harmful error in MPU is assigning an observed-class instance to the unlabeled remainder; confusion among observed classes is comparatively tolerable.
- **Evidence anchors:**
  - [abstract]: "assign distinct, data-dependent weights to the positive and inferred-negative loss components so that the resulting empirical objective is an unbiased estimator of the target risk"
  - [Section III-B]: "Reflecting that misclassifying a positive from class y as the negative meta-class is more severe than confusing it with another observed class, we choose a degenerate cost scheme Cyk = 1, Cyj = 0 for all j ∈ Y \ {y, k}"
  - [corpus]: Related work on weakly supervised risk minimization frameworks (arxiv:2511.22823) supports cost-sensitive designs for stable learning under incomplete supervision, though not MPU-specific.
- **Break condition:** If class priors are highly imbalanced and cost weights are not adjusted proportionally, the loss may still over-emphasize majority observed classes, degrading minority detection.

### Mechanism 2
- **Claim:** The constant-sum property of the base loss enables unbiased risk estimation without explicit negative labels.
- **Mechanism:** When the binary surrogate satisfies ℓ(z, +1) + ℓ(-z, -1) = C (constant), unobservable negative-class expectations cancel algebraically during risk decomposition. The resulting population risk contains only observable terms (labeled positives and unlabeled mixture) plus a model-independent constant offset -2(1-πk)C that does not affect ERM minimizers.
- **Core assumption:** The class priors πi are known or accurately estimable; the constant-sum identity holds exactly (or via symmetrization for hinge/ramp losses).
- **Evidence anchors:**
  - [Section III-B]: "We require the base binary surrogate to satisfy the constant–sum (symmetry) identity ℓ(z, +1) + ℓ(-z, -1) ≡ C, so that unobservable terms cancel in the MPU risk"
  - [Table IV]: Empirical verification showing sigmoid_prob, tanh_smooth, unhinged, and symmetrized hinge/ramp satisfy the property (near-zero Const-sum max/p99 values), while raw hinge/logistic/ramp violate it substantially
  - [corpus]: Weak direct corpus evidence for the constant-sum mechanism specifically in MPU; related PU literature (du Plessis et al., 2014, cited in paper) uses similar risk rewriting but for binary settings.
- **Break condition:** If the loss violates the constant-sum property (e.g., standard logistic loss), the cancellation fails and the estimator becomes biased, with non-vanishing constants in generalization bounds.

### Mechanism 3
- **Claim:** Outer ReLU-based non-negativity correction stabilizes optimization without compromising unbiasedness guarantees.
- **Mechanism:** The empirical risk can become negative due to the constant offset -2(1-πk) and sampling fluctuations. Post-composing the aggregated objective with ReLU (g(z) = max{z, 0}) enforces non-negativity at the outer risk level while preserving per-sample loss structure. Since ReLU is convex and 1-Lipschitz, concentration properties and bias bounds remain valid.
- **Core assumption:** Negativity arises primarily at the aggregated level, not within individual loss terms; the correction is applied only to the outer objective.
- **Evidence anchors:**
  - [abstract]: "The method also incorporates a non-negativity correction to stabilize training"
  - [Section V]: "Because ReLU is convex and 1-Lipschitz, this outer correction preserves the bias and concentration properties used in our analysis"
  - [Theorem 4]: Formal concentration bound showing the corrected risk maintains high-probability guarantees relative to population risk
  - [corpus]: The nnPU (non-negative PU) approach (Kiryo et al., 2017, cited) applies similar non-negativity correction in binary PU; corpus neighbors lack direct MPU non-negativity mechanisms.
- **Break condition:** If negativity arises frequently during early training with aggressive learning rates, the ReLU may mask gradient signals excessively, slowing convergence; tuning learning rate or using ABS (absolute value) correction may help.

## Foundational Learning

- **Concept: Class Prior Estimation in PU Learning**
  - **Why needed here:** The unbiased risk estimator requires class priors πi for each observed class and πk for the unlabeled mixture. Misspecification introduces linear bias in the excess risk (Theorem 3).
  - **Quick check question:** Can you explain why the NP-based lower bound alone is insufficient and why the penalized L1 point estimate is needed?

- **Concept: Empirical Risk Minimization (ERM) with Composite Losses**
  - **Why needed here:** CSMPU reformulates multi-class classification as per-class OVR tasks with composite losses L̃(f(x), y) := L(f(x), y) - L(f(x), k) that subtract unlabeled terms to achieve unbiasedness.
  - **Quick check question:** What is the role of the constant offset -2(1-πk)C in the population risk, and why does it not affect the ERM minimizer?

- **Concept: One-vs-Rest Multi-Class Decomposition**
  - **Why needed here:** MPU treats each observed class as "positive" against a "negative meta-class" (the unlabeled remainder). Understanding OVR decomposition is essential to interpret the per-class loss design and the adaptive negative-class loss (Lk using argmax over observed-class scores).
  - **Quick check question:** How does the negative-class loss Lk(f(x)) differ from the positive-class losses Li(f(x)), and why does it use argmax?

## Architecture Onboarding

- **Component map:** Data loader -> Class prior estimator -> Forward pass (encoder g: X → R^k) -> Loss computation (per-class OVR losses) -> Risk aggregation (weighted sum with priors) -> Non-negativity correction (ReLU) -> Optimizer
- **Critical path:** Accurate class prior estimation → constant-sum loss selection (verify via Table IV diagnostics) → stable empirical risk aggregation → proper ReLU correction → smooth convergence without negative loss spikes.
- **Design tradeoffs:**
  - **Loss choice:** Sigmoid_prob (γ=1) achieves best macro-F1 and exactly satisfies constant-sum; symmetrized hinge/ramp trade small accuracy for theoretical validity; avoid raw hinge/logistic.
  - **Correction type:** NN (ReLU) vs ABS—ABS may preserve more gradient signal when risks hover near zero; NN is more conservative and may be more stable with aggressive optimizers.
  - **Architecture:** MLP for simpler datasets (MNIST, USPS); ResNet-20 for complex data (SVHN). Over-parameterization increases Rademacher complexity terms in generalization bounds.
- **Failure signatures:**
  - Negative empirical risk during training (check loss curves for dips below zero before correction).
  - High variance across runs (std >5% accuracy) suggests unstable class prior estimates or learning rate too high.
  - Degraded performance at low πk (0.2) indicates sensitivity to unlabeled mixture ambiguity—verify class priors are not overestimated.
  - Constant-sum violation (max |ℓ(z)+ℓ(-z)-C| > 1.0) signals incorrect loss configuration—revert to validated surrogate.
- **First 3 experiments:**
  1. **Sanity check on MNIST with k=4, πk=0.5:** Train CSMPU with sigmoid_prob loss; verify training/test curves are smooth (Fig. 1 style) and empirical risk does not go negative. Compare accuracy against URE and AREA baselines (Table I-III).
  2. **Class prior sensitivity:** On FashionMNIST (k=4, πk=0.5), perturb priors by Δ with ||Δ||1 ∈ {0, 0.1, 0.2, 0.3} using scalar-last and adversarial schemes (Fig. 3a). Plot macro-F1 vs. ||Δ||1 to verify linear degradation matches theoretical bound.
  3. **Loss ablation:** Compare sigmoid_prob (γ=1), tanh_smooth, symmetrized hinge, and raw hinge on FashionMNIST. Report Const-sum max/p99 diagnostics (Table IV format) and macro-F1; confirm only constant-sum-valid losses achieve stable convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the CSMPU framework be extended to explicitly cluster or classify the "unobserved" classes within the negative meta-class, rather than treating them purely as a residual "rest" category?
- Basis in paper: [explicit] The authors state that their objective is robust one-vs-rest detection "without attempting to disentangle the remainder into its constituent unobserved classes," limiting the method to surfacing known categories.
- Why unresolved: The current loss function and theoretical bounds rely on a binary partition between "observed" and "unobserved" sets, lacking a mechanism to distinguish between different types of unlabeled data.
- What evidence would resolve it: A modified theoretical framework and empirical results showing the model can recover latent structures or distinct classes within the unlabeled set $X_k$ without explicit labels.

### Open Question 2
- Question: Is the strict requirement for a constant-sum base loss ($\ell(z) + \ell(-z) = C$) necessary for the stability of deep neural networks, or can standard losses (e.g., logistic) be used with alternative correction mechanisms?
- Basis in paper: [inferred] Table IV and Section III-B highlight that standard losses like hinge or logistic violate the constant-sum identity and must be "symmetrized" or clipped, which introduces a design constraint that may affect performance.
- Why unresolved: The paper demonstrates that violating this assumption leads to biased risk estimation, but it does not explore if modern deep learning regularization techniques could mitigate the instability of non-constant-sum losses.
- What evidence would resolve it: Ablation studies comparing the performance of the proposed constant-sum losses against non-constant-sum losses within the same CSMPU framework, analyzing the bias-variance trade-off.

### Open Question 3
- Question: How does the performance of CSMPU degrade in a "wild" setting where class priors are dynamically shifting or non-stationary, rather than fixed as assumed in the theoretical analysis?
- Basis in paper: [explicit] The theoretical analysis (Theorem 3) and experiments (Fig 3) assume fixed priors, while the authors note that class-prior estimation is a separate, difficult problem prone to overestimation.
- Why unresolved: The method relies on pre-computed or estimated priors $\pi_i$ as constants; the paper does not analyze the feedback loop where the model's predictions might influence prior estimation in a dynamic environment.
- What evidence would resolve it: Theoretical bounds or empirical evaluations on datasets with distribution shift (concept drift) where priors $\pi_k$ change during the testing phase or must be updated online.

## Limitations
- The method's reliance on accurate class prior estimation is a critical limitation not fully addressed, with theoretical bounds showing linear degradation with prior misspecification.
- The constant-sum loss requirement is a restrictive assumption that limits method flexibility and may impact convergence behavior compared to established surrogates.
- The framework is designed for one-vs-rest detection without attempting to disentangle the unlabeled remainder into its constituent unobserved classes.

## Confidence
- **High confidence**: The mechanism of cost-sensitive weighting for OVR discrimination is well-supported by the explicit cost scheme and empirical results showing improved stability under class imbalance.
- **Medium confidence**: The unbiased risk estimation through constant-sum losses has strong theoretical justification but relies heavily on the unstated assumption of accurate prior knowledge.
- **Medium confidence**: The non-negativity correction follows established PU learning practices but its outer-level application in the multi-class setting requires more extensive validation.

## Next Checks
1. **Prior estimation robustness**: Implement the NP lower bound + penalized L1 estimator from Section III-C and evaluate Macro-F1 degradation across a grid of prior estimation errors (±10% to ±30%) on FashionMNIST.
2. **Loss sensitivity analysis**: Systematically compare sigmoid_prob, symmetrized hinge, and tanh_smooth on the same dataset, measuring both constant-sum diagnostics (Table IV format) and classification accuracy to quantify the accuracy-stability tradeoff.
3. **Early training stability**: Monitor empirical risk trajectories during training with and without ReLU correction, specifically checking for negative risk spikes and their frequency across different learning rates (1e-3, 5e-4, 1e-5, 1e-6).