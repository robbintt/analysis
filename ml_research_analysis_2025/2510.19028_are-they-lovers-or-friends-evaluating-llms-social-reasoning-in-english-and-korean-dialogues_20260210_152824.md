---
ver: rpa2
title: Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and
  Korean Dialogues
arxiv_id: '2510.19028'
source_url: https://arxiv.org/abs/2510.19028
tags:
- social
- korean
- relationship
- reasoning
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces SCRIPTS, a benchmark for evaluating large
  language models' (LLMs) social relationship reasoning in English and Korean dialogues.
  SCRIPTS features 1,147 dialogues sourced from movie scripts, annotated with probabilistic
  labels (HIGHLYLIKELY, LESSLIKELY, UNLIKELY) for over 250 fine-grained relationship
  types.
---

# Are they lovers or friends? Evaluating LLMs' Social Reasoning in English and Korean Dialogues

## Quick Facts
- arXiv ID: 2510.19028
- Source URL: https://arxiv.org/abs/2510.19028
- Reference count: 25
- Primary result: Current LLMs achieve 75-80% accuracy in English but drop to 58-69% in Korean for inferring relationships in dialogues

## Executive Summary
This study introduces SCRIPTS, a benchmark for evaluating large language models' (LLMs) social relationship reasoning in English and Korean dialogues. SCRIPTS features 1,147 dialogues sourced from movie scripts, annotated with probabilistic labels (HIGHLYLIKELY, LESSLIKELY, UNLIKELY) for over 250 fine-grained relationship types. Evaluations across nine models show current LLMs achieve 75-80% accuracy in English but drop to 58-69% in Korean, with 10-25% of responses incorrectly predicting UNLIKELY relationships. Chain-of-thought prompting and thinking models, effective for general reasoning, provide minimal benefits for social reasoning and occasionally amplify biases. The study highlights significant limitations in LLMs' social reasoning abilities and the need for culturally aware, language-specific approaches.

## Method Summary
The SCRIPTS benchmark evaluates LLMs' ability to infer interpersonal relationships between speakers in multi-turn dialogues using probabilistic labeling rather than single-label classification. The dataset contains 1,147 dialogues (580 English, 567 Korean) sourced from movie scripts, each annotated with approximately 3.7 HIGHLYLIKELY relationships and 20.8 UNLIKELY relationships on average. Models generate open-ended relationship predictions in JSON format, with responses evaluated against ground-truth labels using GPT-4o as judge (92% validation accuracy). Nine models were tested across both languages using standard prompting and chain-of-thought approaches.

## Key Results
- LLMs achieve 75-80% HIGHLYLIKELY accuracy in English but only 58-69% in Korean
- 10-25% of model responses incorrectly predict UNLIKELY relationships
- Chain-of-thought prompting provides minimal benefits and occasionally amplifies social biases
- Term-of-address and honorific confusion accounts for largest share of Korean failures
- Models struggle with atypical relationships, defaulting to stereotypical configurations

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Labeling Enables Error-Aware Evaluation
The three-tier labeling scheme (HIGHLYLIKELY, LESSLIKELY, UNLIKELY) differentiates plausible uncertainty from socially inappropriate inference, enabling a "nonsense penalty" that captures safety-relevant errors ignored by binary classification.

### Mechanism 2: Cross-Linguistic Transfer Failure via Culture-Specific Linguistic Markers
Performance degrades in Korean because models fail to interpret honorifics and kinship terms as relational signals rather than literal references, lacking the cultural-linguistic mapping to decode these signals.

### Mechanism 3: Chain-of-Thought Amplifies Stereotype Priors in Social Domains
CoT prompting provides minimal benefit because explicit step-by-step reasoning exposes and amplifies stereotyped relationship assumptions, forcing explicit articulation of biased heuristics that contradict atypical but valid relationships.

## Foundational Learning

- **Probabilistic Annotation Frameworks**: Understanding that social reasoning admits multiple valid interpretations with different plausibility levels is essential before interpreting benchmark metrics.
  - Quick check: Can you explain why a "friends" prediction for a married couple dialogue might be LESSLIKELY rather than UNLIKELY?

- **Honorifics and Terms of Address (Korean Linguistics)**: Korean encodes social relationships through grammatical morphology; without this, error analysis will be superficial.
  - Quick check: Why might a model predict "parent-child" when it sees "eomeoni" (mother) used between acquaintances?

- **Social Stereotyping in LLMs**: The paper shows models default to "typical" relationship configurations; recognizing this bias is prerequisite to mitigation.
  - Quick check: If a parent and child converse as peers without hierarchy, what failure mode does the paper identify?

## Architecture Onboarding

- **Component map**: Movie scripts -> Anonymized dialogues -> Human annotations (HIGHLYLIKELY/LESSLIKELY/UNLIKELY) -> Model inference (5 runs) -> Majority voting -> GPT-4o judge evaluation -> Accuracy metrics

- **Critical path**: 1) Filter movie scripts for sufficient turns and speaker diversity; 2) Anonymize character names to prevent movie-identification shortcuts; 3) Collect native-speaker annotations with overlap validation; 4) Run model inference (5 runs per dialogue, majority vote); 5) Validate LLM-as-judge accuracy before relying on automated scoring

- **Design tradeoffs**: Movie scripts provide high ecological validity for dialogue structure but risk cinematic exaggeration; open-ended generation captures nuance but requires automated judge validation; two languages only limits generalizability

- **Failure signatures**: UNLIKELY rate >15% indicates socially implausible predictions; CoT worsens performance signals stereotype amplification; Korean >> English error gap indicates insufficient cultural-linguistic training data

- **First 3 experiments**: 1) Baseline establishment: Run target model on both English and Korean subsets with standard prompting; 2) Ablation on auxiliary information: Test with/without provided relational dimensions using gold labels; 3) CoT vs. direct prompting comparison: Apply CoT prompting and measure delta

## Open Questions the Paper Calls Out

- To what extent do these social reasoning limitations generalize to languages with typological profiles distinct from English and Korean? The authors state analysis remains limited to two languages and future work should extend research to broader range of languages.

- Why do standard reasoning enhancement techniques like Chain-of-Thought (CoT) fail to improve social relationship inference? The paper establishes the phenomenon but does not fully explain the mechanism behind the failure.

- How can LLMs be trained to override stereotypical relationship priors when dialogue evidence supports atypical social dynamics? Current models appear to weigh statistical likelihood higher than specific pragmatic evidence.

- Can multi-task learning on social dimensions (age, hierarchy) improve relationship inference more effectively than providing that information as context? Providing ground-truth social information helps, but models fail when they must generate this info themselves.

## Limitations

- Reliance on cinematic dialogue as proxy for naturalistic conversation may introduce systematic biases in relationship dynamics
- Automated judging pipeline, while validated at 92% accuracy, introduces potential measurement error for nuanced social relationships
- Benchmark covers only two languages (English and Korean), limiting cross-linguistic generalizability

## Confidence

**High Confidence**: The quantitative findings regarding performance gaps between English (75-80% accuracy) and Korean (58-69% accuracy) are directly supported by reported metrics and error analysis.

**Medium Confidence**: The mechanistic explanation for Korean performance degradation (honorific and term-of-address confusion) is plausible but lacks systematic analysis of how often specific honorific patterns appear in corpus.

**Low Confidence**: The claim that CoT amplification of social biases is a general mechanism requires more evidence - the paper shows performance changes but does not analyze actual content of CoT reasoning chains.

## Next Checks

1. Conduct manual review of 100 randomly selected Korean dialogue predictions to verify whether reported error types match human interpretation of model failures.

2. Test a model fine-tuned specifically on Korean social contexts to determine whether performance gap is due to language vs. cultural knowledge.

3. Extract and analyze actual reasoning chains from models showing performance degradation with CoT to identify whether stereotyped reasoning is being explicitly generated.