---
ver: rpa2
title: 'XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision'
arxiv_id: '2601.21688'
source_url: https://arxiv.org/abs/2601.21688
tags:
- factor
- latent
- information
- disentangled
- factors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: XFACTORS introduces a weakly-supervised VAE framework for disentangled
  representation learning, addressing the challenge of recovering semantic factors
  from real data without strong inductive biases. The method decomposes the latent
  space into a residual subspace S and factor-specific subspaces T1,...,TK, using
  contrastive supervision (InfoNCE loss) to align each Ti with its corresponding factor
  label.
---

# XFACTORS: Disentangled Information Bottleneck via Contrastive Supervision

## Quick Facts
- arXiv ID: 2601.21688
- Source URL: https://arxiv.org/abs/2601.21688
- Reference count: 40
- Primary result: State-of-the-art disentanglement scores on Shapes3D and MPI3D datasets

## Executive Summary
XFACTORS introduces a weakly-supervised VAE framework for disentangled representation learning that addresses the challenge of recovering semantic factors from real data without strong inductive biases. The method decomposes the latent space into a residual subspace and factor-specific subspaces, using contrastive supervision (InfoNCE loss) to align each factor-specific subspace with its corresponding label. KL regularization imposes a Gaussian structure on both the residual and factor subspaces, organizing the geometry without additional supervision. The approach achieves state-of-the-art disentanglement scores while enabling controlled factor swapping via latent replacement.

## Method Summary
XFACTORS combines variational autoencoding with contrastive supervision to learn disentangled representations. The latent space is partitioned into a residual subspace S and K factor-specific subspaces T1,...,TK. For each factor, contrastive supervision via InfoNCE loss aligns the corresponding Ti with factor labels by maximizing mutual information between Ti and the label while minimizing it with other factors. KL regularization imposes a Gaussian structure on both S and the aggregated factor subspaces, organizing the geometry without additional supervision. The method enables explicit control over targeted factors while retaining information about non-targeted factors, and avoids adversarial training and classifiers used in previous approaches.

## Key Results
- Achieves FactorVAE scores ~1.0 on Shapes3D dataset
- Achieves FactorVAE scores ~0.9 on MPI3D dataset
- Demonstrates consistent qualitative factor alignment on CelebA dataset

## Why This Works (Mechanism)
The method works by combining contrastive supervision with information bottleneck principles. InfoNCE loss maximizes the mutual information between each factor-specific latent subspace and its corresponding factor label while minimizing information with other factors. This creates factor-specific representations that are aligned with semantic labels. The KL regularization then imposes a Gaussian structure on both the residual and factor subspaces, which organizes the geometry of the latent space without requiring additional supervision. This combination allows the model to learn disentangled representations that are both semantically meaningful and structurally organized.

## Foundational Learning
1. Variational Autoencoders (VAEs) - why needed: Provide probabilistic framework for learning latent representations; quick check: Understand evidence lower bound (ELBO) formulation
2. Contrastive Learning - why needed: Enables learning from weak supervision through mutual information maximization; quick check: Understand InfoNCE loss formulation and how it works
3. Information Bottleneck - why needed: Provides theoretical foundation for balancing compression and relevance; quick check: Understand trade-off between reconstruction and regularization
4. Latent Space Decomposition - why needed: Enables explicit separation of factor-specific and residual information; quick check: Understand how subspaces S and T1,...,TK are defined and used
5. KL Regularization - why needed: Imposes structural priors on latent distributions; quick check: Understand how Gaussian structure affects latent geometry

## Architecture Onboarding

Component map:
Input Image -> Encoder -> Latent Space (S, T1,...,TK) -> Decoder -> Reconstructed Image
Contrastive Supervision -> InfoNCE Loss -> Ti alignment with factor labels
KL Regularization -> Gaussian structure on S and aggregated T

Critical path:
Image → Encoder → Latent decomposition → Contrastive alignment → KL regularization → Decoder → Reconstruction

Design tradeoffs:
- Contrastive supervision provides weak labels but requires factor annotations
- Latent decomposition enables factor control but increases model complexity
- KL regularization provides structure without supervision but may limit expressiveness
- Avoids adversarial training (more stable) but may be less powerful than adversarial approaches

Failure signatures:
- Poor disentanglement: Low FactorVAE scores, factor swapping produces artifacts
- Mode collapse: Latent space lacks diversity, reconstructions become blurry
- Contrastive misalignment: Factor subspaces don't correspond to semantic factors
- KL over-regularization: Latent space becomes too constrained, reconstructions suffer

First experiments:
1. Train on Shapes3D with ground truth factor labels, evaluate FactorVAE score
2. Perform factor swapping by replacing Ti values, visualize reconstructed outputs
3. Vary latent dimension and evaluate impact on disentanglement and reconstruction quality

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on contrastive supervision requiring factor labels, making it semi-supervised rather than fully unsupervised
- Does not extensively address computational complexity or training stability compared to competing methods
- Practical utility for downstream tasks is not thoroughly evaluated quantitatively

## Confidence
- State-of-the-art disentanglement scores on Shapes3D and MPI3D: High confidence
- Scalability with increasing latent capacity: Medium confidence (primarily synthetic datasets)
- Real-world dataset performance (CelebA): Medium confidence (qualitative rather than quantitative)

## Next Checks
1. Evaluate XFACTORS on more diverse real-world datasets with quantitative downstream task performance metrics
2. Conduct ablation studies to isolate the contribution of contrastive supervision versus KL regularization to overall performance
3. Test the method's robustness to noisy or incomplete factor labels in the contrastive supervision regime