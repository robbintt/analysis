---
ver: rpa2
title: 'Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism,
  Expert Specialization, and Layerwise Steering'
arxiv_id: '2601.14050'
source_url: https://arxiv.org/abs/2601.14050
tags:
- languages
- language
- experts
- routing
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates multilingual processing in Mixture-of-Experts
  (MoE) large language models by analyzing routing behavior, expert specialization,
  and layerwise functional roles. The study reveals that MoE models exhibit structured
  multilingual behavior: routing aligns with linguistic families, expert utilization
  follows a clear layerwise pattern, and high-resource languages rely on shared experts
  while low-resource languages depend more on language-exclusive experts.'
---

# Understanding Multilingualism in Mixture-of-Experts LLMs: Routing Mechanism, Expert Specialization, and Layerwise Steering

## Quick Facts
- arXiv ID: 2601.14050
- Source URL: https://arxiv.org/abs/2601.14050
- Authors: Yuxin Chen; Zhengzhou Cai; Xiangtian Ji; Weixiang Zhao; An Zhang; Xiang Wang; Tat-Seng Chua
- Reference count: 29
- Primary result: Inference-time routing-guided steering of middle-layer experts improves multilingual reasoning performance, with average accuracy gains of 1.9% when steering from English and 0.7% from Chinese on PolyMath benchmark.

## Executive Summary
This paper investigates how multilingual Mixture-of-Experts (MoE) large language models process and transfer knowledge across languages. Through comprehensive analysis of routing behavior, expert specialization, and layerwise functional roles, the authors reveal that MoE models exhibit structured multilingual behavior: routing aligns with linguistic families, expert utilization follows a clear layerwise pattern, and high-resource languages rely on shared experts while low-resource languages depend more on language-exclusive experts. Based on these insights, they propose a routing-guided steering method that adaptively guides routing behavior in middle layers toward shared experts associated with dominant languages at inference time, achieving consistent multilingual performance improvements.

## Method Summary
The authors analyze multilingual routing behavior in Qwen3-30B-A3B by computing routing frequencies across languages and layers, classifying experts as language-exclusive or shared using normalized affinity thresholds, and conducting layerwise interventions to reveal functional stratification. They implement routing-guided steering by applying logit bias to middle-layer routing (layers 10-39) that steers tokens toward shared experts associated with dominant languages, with steering strength λ≈0.022. The method is evaluated on multilingual reasoning benchmarks including MGSM for intervention validation and PolyMath-low for steering performance.

## Key Results
- Cross-language routing similarity shows clear block-wise structure aligned with language families in final MoE layer
- Layerwise interventions reveal early/late layers support language-specific processing while middle layers serve as language-agnostic capacity hubs
- High-resource languages (English, Chinese) achieve strong performance by leveraging shared experts; low-resource languages rely more on language-exclusive experts but remain weaker
- Routing-guided steering improves multilingual performance with average accuracy gains of 1.9% (English steering) and 0.7% (Chinese steering) on PolyMath benchmark

## Why This Works (Mechanism)

### Mechanism 1: Layerwise Functional Stratification
MoE layers exhibit functional specialization—early layers support language-specific understanding, late layers support language-specific generation, and middle layers serve as language-agnostic capacity hubs. This is evidenced by the U-shaped pattern in language-exclusive expert distribution and distinct failure modes when masking experts at different layers.

### Mechanism 2: Resource-Level Expert Utilization Divergence
High-resource languages achieve strong performance by leveraging shared experts aligned with dominant languages, while low-resource languages rely more on language-exclusive experts but remain weaker due to limited integration into shared capacity. This is supported by routing entropy analysis showing dominant languages have broad expert distribution while low-resource languages show concentrated usage.

### Mechanism 3: Routing-Guided Cross-Lingual Capacity Transfer via Middle-Layer Steering
Adaptively biasing middle-layer routing toward shared experts associated with dominant languages improves multilingual performance, with gains proportional to linguistic proximity to the steering source. The steering vector modulates router logits based on expert-language association and original logit magnitude, avoiding disruption to early understanding and late generation.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing**
  - Why needed here: The entire analysis depends on understanding how tokens are routed to experts via Top-K selection and how routing distributions capture language-specific behavior.
  - Quick check question: Given router logits g(x) ∈ R^E, how does Top-K routing compute expert selection and the output for layer l?

- **Cross-Lingual Transfer in LLMs**
  - Why needed here: The steering method exploits cross-lingual capacity sharing; understanding why high-resource languages transfer better than low-resource languages is essential for interpreting results.
  - Quick check question: Why might a model trained predominantly on English and Chinese exhibit stronger reasoning transfer to French than to Bengali?

- **Jensen-Shannon Divergence for Distribution Comparison**
  - Why needed here: Cross-language routing similarity is computed as 1 - JSD between routing distributions; interpreting similarity heatmaps requires understanding JSD properties.
  - Quick check question: If two languages have routing distributions P and Q with JSD(P||Q) = 0, what does this imply about their expert usage patterns?

## Architecture Onboarding

- **Component map:**
  Router (G_l) projects hidden states to expert logits via W^l ∈ R^{d×E}, applies Top-K selection; Expert networks (f_{l,1}...f_{l,E}) are FFN sub-networks with independent parameters; Layer groups: Early (0-4), Middle (10-39 for steering, 22-26 for intervention), Late (43-47); Expert classification: Language-exclusive (W_{l,i}^ℓ > θ), Language-shared (max W ≤ θ)

- **Critical path:**
  1. Compute routing frequencies p_{l,i}^ℓ per language/layer from forward passes on benchmark data
  2. Classify experts as exclusive or shared using normalized affinity threshold (θ = 0.4 for analysis, 0.7 for steering)
  3. For steering: compute W_{l,i}^ℓ from dominant language data, identify shared experts, apply logit bias during inference on middle layers only

- **Design tradeoffs:**
  Higher θ → stricter exclusive expert classification → fewer experts masked/steered → more conservative intervention; Steering strength λ: too low → negligible effect; too high (λ > 0.026) → performance collapse from routing distribution drift; Layer window: steering early/late layers degrades performance

- **Failure signatures:**
  Early-layer intervention: Query misunderstanding, repeated problem distortion, reasoning trajectory failure; Late-layer intervention: Language mixing (correct reasoning in wrong language), consistency scores drop 50-80 points; Excessive steering (λ > 0.026): Sharp accuracy collapse across all target languages

- **First 3 experiments:**
  1. Reproduce routing similarity analysis: Run forward pass on BELEBELE for 10 languages, compute pairwise JSD at final MoE layer, verify block-wise structure by language family
  2. Layerwise intervention sanity check: Mask language-exclusive experts for 1 high-resource and 1 low-resource language at early/middle/late layers, measure MGSM accuracy and language consistency
  3. Steering ablation: Apply routing-guided steering with English source to Spanish (linguistically close) and Japanese (distant), sweep λ ∈ [0.01, 0.03], verify proximity-correlated gains

## Open Questions the Paper Calls Out

### Open Question 1
Does the observed functional stratification of MoE layers—where early/late layers handle language-specific processing and middle layers serve as agnostic hubs—generalize to substantially larger MoE architectures? The current study is restricted to Qwen3-30B-A3B, leaving it unclear if these structural patterns persist across different model scales.

### Open Question 2
Can the inference-time routing-guided steering strategy be effectively adapted into a training-time optimization to permanently enhance multilingual capacity? The paper demonstrates performance gains via inference-time intervention, but it is unknown if enforcing such routing alignment during training would fundamentally reshape expert specialization for better baseline performance.

### Open Question 3
Is the heavy reliance of low-resource languages on language-exclusive experts a cause of their performance deficits due to isolation from shared capacity, or is it a necessary specialization? The study establishes a correlation between exclusive expert usage and lower performance, but does not causally determine if forcing these languages into shared experts would improve or degrade reasoning.

## Limitations

- The analysis critically depends on the assumption that language-exclusive expert identification via threshold θ=0.4 captures functionally relevant specialization, but this may conflate statistical rarity with functional importance.
- The steering method assumes that dominant-language shared experts encode transferable reasoning capacity, but this transfer mechanism is inferred rather than explicitly validated through probing or interpretability analysis.
- The study is restricted to Qwen3-30B-A3B model, leaving it unclear if the observed patterns generalize to substantially larger or different MoE architectures.

## Confidence

**High Confidence:**
- Layerwise functional stratification (early/late layers support language-specific processing, middle layers serve as language-agnostic capacity hubs) - Supported by systematic intervention experiments showing distinct failure modes when masking experts at different layers
- Resource-level expert utilization divergence (high-resource languages use shared experts, low-resource languages rely on exclusive experts) - Well-supported by routing entropy analysis and expert classification across multiple languages

**Medium Confidence:**
- Routing-guided steering improves multilingual performance via cross-lingual capacity transfer - Gains are consistent but the underlying transfer mechanism (linguistic proximity) is inferred from correlation rather than proven causally
- Language family clustering in routing similarity heatmaps - Observed pattern is robust but the interpretation that this reflects learned family-specific representations versus coincidental routing patterns lacks direct validation

## Next Checks

1. **Expert Necessity Validation**: Conduct ablations where language-exclusive experts are progressively reintroduced for low-resource languages to identify the minimum set required for baseline performance, distinguishing between truly necessary experts and statistical artifacts.

2. **Transfer Mechanism Probing**: Apply steering to linguistically unrelated language pairs (e.g., English→Swahili, Chinese→Arabic) and conduct targeted evaluations to determine whether gains stem from linguistic proximity versus general reasoning capacity transfer.

3. **Steering Generalization Test**: Apply the routing-guided steering method to a different MoE architecture (e.g., Mixtral or DeepSeekMoE) with different language distributions to verify that the middle-layer steering strategy generalizes beyond Qwen3-30B-A3B.