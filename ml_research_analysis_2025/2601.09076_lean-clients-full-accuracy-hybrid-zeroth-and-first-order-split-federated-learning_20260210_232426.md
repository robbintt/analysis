---
ver: rpa2
title: 'Lean Clients, Full Accuracy: Hybrid Zeroth- and First-Order Split Federated
  Learning'
arxiv_id: '2601.09076'
source_url: https://arxiv.org/abs/2601.09076
tags:
- learning
- client
- heron-sfl
- training
- communication
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HERON-SFL, a novel hybrid zeroth- and first-order
  optimization framework for Split Federated Learning (SFL) that addresses the high
  memory and computation costs on resource-constrained edge devices. The key idea
  is to use zeroth-order (gradient-free) optimization on the client side while retaining
  first-order optimization on the server side, eliminating backpropagation and activation
  caching during local updates.
---

# Lean Clients, Full Accuracy: Hybrid Zeroth- and First-Order Split Federated Learning

## Quick Facts
- arXiv ID: 2601.09076
- Source URL: https://arxiv.org/abs/2601.09076
- Authors: Zhoubin Kou; Zihan Chen; Jing Yang; Cong Shen
- Reference count: 40
- Primary result: HERON-SFL achieves 64% memory reduction and 33% computation reduction while matching baseline accuracy

## Executive Summary
This paper introduces HERON-SFL, a hybrid zeroth- and first-order optimization framework for Split Federated Learning (SFL) that addresses the high memory and computation costs on resource-constrained edge devices. The key idea is to use zeroth-order (gradient-free) optimization on the client side while retaining first-order optimization on the server side, eliminating backpropagation and activation caching during local updates. Under a low effective rank assumption, the authors theoretically prove that HERON-SFL achieves convergence rates independent of model dimensionality, addressing a key scalability concern in zeroth-order methods. Empirically, on ResNet training and language model fine-tuning tasks, HERON-SFL matches the accuracy of state-of-the-art auxiliary-network-based SFL methods while reducing client peak memory by up to 64% and client-side computation per step by up to 33%, substantially expanding the range of models that can be trained or adapted on resource-limited devices.

## Method Summary
HERON-SFL is a Split Federated Learning framework that uses zeroth-order optimization on the client side and first-order optimization on the server side. The client holds a frontend model and an auxiliary model, performing ZO updates using perturbed forward-only evaluations to approximate gradients. The server holds a backend model, receiving "smashed data" (activations) from clients and performing standard backpropagation updates. This hybrid approach eliminates the need for activation caching on clients, reducing memory complexity from O(|θc| + |θa|) to O(1), while maintaining communication efficiency identical to standard decoupled SFL. The method theoretically achieves dimension-independent convergence rates under a low effective rank assumption, and empirically matches baseline accuracy while substantially reducing memory and computation costs on edge devices.

## Key Results
- Achieves up to 64% reduction in client peak memory compared to state-of-the-art auxiliary-network-based SFL methods
- Reduces client-side computation per step by up to 33% while maintaining comparable accuracy
- Theoretical proof shows convergence rate independent of model dimension under low effective rank assumption
- Matches or exceeds accuracy of FO baselines (SFLV2, CSE-FSL) on ResNet-18 CIFAR-10 and GPT2 language model fine-tuning tasks

## Why This Works (Mechanism)

### Mechanism 1: Forward-Only Gradient Approximation
Replacing client-side backpropagation with Zeroth-Order (ZO) gradient estimation drastically reduces peak memory usage by eliminating the need to cache intermediate activations. HERON-SFL utilizes a two-point ZO gradient estimator which approximates gradients using finite differences between perturbed forward passes. Because the backward pass is removed, the memory-intensive caching of activations is no longer required, reducing memory complexity from O(|θc| + |θa|) to O(1).

### Mechanism 2: Dimension-Independent Convergence via Low-Rank Structure
While standard ZO methods suffer from scalability issues (variance scaling with dimension d), HERON-SFL achieves convergence rates independent of d under specific landscape conditions. The authors leverage a "low effective rank" assumption regarding the local loss landscape. Theoretically, HERON-SFL admits convergence rate scaling as O(√κ/T) rather than the typical ZO scaling of O(√d/T), where κ is the effective rank.

### Mechanism 3: Hybrid ZO-FO Decoupling
Retaining First-Order (FO) optimization on the server while using ZO on the client allows the system to preserve the communication efficiency of decoupled SFL without sacrificing server-side model accuracy. The client performs ZO updates and uploads "smashed data" at the same frequency as standard SFL. The server processes this data using standard backpropagation, preventing the high variance of ZO from affecting the bulk of the model parameters while keeping communication costs identical to auxiliary-network-based SFL.

## Foundational Learning

- **Concept**: Zeroth-Order (ZO) Optimization (e.g., SPSA, Gradient Estimation)
  - **Why needed here**: This is the engine of the paper. You must understand how gradients can be estimated purely from function evaluations (loss values) rather than derivatives.
  - **Quick check question**: Can you explain why a two-point gradient estimator approximates the gradient direction, and why μ is critical?

- **Concept**: Split Federated Learning (SFL) & Auxiliary Networks
  - **Why needed here**: The architecture context. You need to understand why the model is cut and how the auxiliary network breaks the dependency lock between client and server.
  - **Quick check question**: In standard SFL, why does the client have to wait for the server? How does the auxiliary network in HERON-SFL break this lock?

- **Concept**: Low Effective Rank / Hessian Spectrum
  - **Why needed here**: This is the theoretical "secret sauce" that makes ZO viable for large models here.
  - **Quick check question**: Why does the variance of ZO estimators typically scale poorly with dimension d, and how does a low effective rank assumption mitigate this?

## Architecture Onboarding

- **Component map**: Client (Frontend + Auxiliary) -> Fed Server (Model aggregation) -> Main Server (Backend training) -> Fed Server (Parameter broadcast)
- **Critical path**:
  1. Init: Broadcast global θc, θa
  2. Local ZO Loop: Client generates random perturbation u -> Evaluates loss -> Updates θc, θa locally (no backward pass)
  3. Upload: Client sends Smashed Data (Si) to Main Server (every k steps) and model params to Fed Server (end of round)
  4. Server FO: Main Server updates θs using standard gradients derived from Si
  5. Agg: Fed Server aggregates client params

- **Design tradeoffs**:
  - Auxiliary Network Size: HERON is robust to small auxiliary networks, whereas FO methods rely on larger ones
  - Perturbation Count (q): Increasing q improves gradient estimate accuracy but linearly increases compute
  - Perturbation Step Size (μ): Too small → numerical noise; Too large → bias

- **Failure signatures**:
  - High Memory: Check if ZO logic is inadvertently caching tensors for backward pass
  - Divergence: If client loss drops but server accuracy stagnates, smashed data may be uninformative
  - Slow Convergence: Verify low effective rank assumption holds or reduce model complexity

- **First 3 experiments**:
  1. Memory Profiling: Run a single local step comparing standard loss.backward() vs. HERON's ZO estimator loop
  2. Hyperparameter Sensitivity (μ): Sweep perturbation step size on CIFAR-10 subset to find stable region
  3. Auxiliary Ablation: Test performance with minimal vs. deep auxiliary networks

## Open Questions the Paper Calls Out
- Can HERON-SFL be adapted to optimize non-differentiable objectives, such as evaluation metrics or human feedback rewards, without sacrificing its convergence guarantees?
- How can privacy guarantees for intermediate activations be strengthened within the HERON-SFL framework?
- How does the convergence of HERON-SFL degrade when the low effective rank assumption is violated?

## Limitations
- Theoretical guarantees depend on low effective rank assumption which may not hold for all architectures or datasets
- Limited evaluation to ResNet-18 and GPT2 variants; scalability to larger models remains untested
- Communication protocol assumes benign clients; no discussion of Byzantine tolerance

## Confidence
- **High**: Memory/computation reduction claims (empirically verified through profiling)
- **Medium**: Accuracy parity with baselines (within 2% margin, but based on single dataset/task)
- **Medium**: Dimension-independent convergence (theoretical under stated assumptions)
- **Low**: Generalization to non-IID settings beyond Dirichlet α=0.1

## Next Checks
1. **Hessian Spectrum Analysis**: Compute and report effective rank for client loss landscapes across different models to validate theoretical assumptions
2. **Communication Efficiency**: Measure actual bandwidth usage and round-trip time compared to FO baselines, not just parameter counts
3. **Adversarial Robustness**: Test system behavior under Byzantine clients attempting to poison smashed data or model parameters