---
ver: rpa2
title: 'Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices'
arxiv_id: '2510.26557'
source_url: https://arxiv.org/abs/2510.26557
tags:
- penalty
- threshold
- feature
- uni00000013
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Trees on a Diet (ToaD) is a compression scheme for boosted decision
  tree ensembles that reduces memory usage by 4-16x compared to LightGBM models while
  maintaining the same performance. The method encourages feature and threshold reuse
  during training using linear regularization and employs a specialized memory layout
  with shared global arrays for features, thresholds, and leaf values.
---

# Boosted Trees on a Diet: Compact Models for Resource-Constrained Devices

## Quick Facts
- arXiv ID: 2510.26557
- Source URL: https://arxiv.org/abs/2510.26557
- Reference count: 40
- Trees on a Diet (ToaD) achieves 4-16x memory reduction compared to LightGBM while maintaining accuracy

## Executive Summary
Trees on a Diet (ToaD) introduces a compression scheme for boosted decision tree ensembles that significantly reduces memory usage while preserving model accuracy. The approach modifies the training process with feature and threshold reuse regularizers, then employs a specialized pointer-less memory layout with global lookup tables. This enables deployment of accurate machine learning models on resource-constrained IoT devices that previously required constant communication with powerful servers, opening possibilities for autonomous edge analytics in remote monitoring and real-time decision making applications.

## Method Summary
ToaD is a compression framework for gradient boosted decision trees that encourages feature and threshold reuse during training through linear regularization. The method introduces two penalty hyperparameters (ι for features, ξ for thresholds) that are subtracted from the split gain calculation during tree construction. After training, models are serialized into a compact bit-stream using a specialized memory layout: metadata, feature/threshold map, encoded trees, global feature/threshold arrays, and global leaf values. This pointer-less representation eliminates pointer overhead and leverages shared lookup tables to achieve 4-16x memory reduction compared to standard LightGBM models while maintaining the same accuracy.

## Key Results
- Achieves 4-16x memory reduction compared to LightGBM models on benchmark datasets
- Maintains identical accuracy to baseline models when compressed to 8-16KB
- Reduces model size from 256KB to 16KB on Covertype dataset without accuracy loss
- Enables deployment on microcontrollers with as little as 2KB memory budget

## Why This Works (Mechanism)
ToaD works by modifying the split-finding process during GBDT training to prefer features and thresholds that have already been used in the model. This is achieved through a regularization term that penalizes the number of unique features (|FU|) and thresholds (|Tf|) in each new tree. By encouraging reuse, the method creates a small set of global lookup tables that can be referenced by all trees. The memory layout then stores only these references rather than duplicating values, using bit-packing to store indices in ⌈log2(n)⌉ bits where n is the number of unique values. This pointer-less representation eliminates pointer overhead and achieves significant compression.

## Foundational Learning
- **Concept: Gradient Boosted Decision Trees (GBDT)**
  - Why needed here: ToaD is a modified training framework for GBDTs, so understanding sequential tree building and error correction is essential
  - Quick check question: Can you explain how a new tree in a GBDT ensemble is trained based on the errors of the previous trees, and what the 'gain' of a split represents?

- **Concept: Model Quantization and Compression**
  - Why needed here: The paper benchmarks against quantized LightGBM; understanding precision reduction helps contextualize ToaD's approach
  - Quick check question: How does reducing the precision of a model's weights (quantization) affect its memory footprint and potentially its accuracy?

- **Concept: Pointer-less Tree Representations**
  - Why needed here: ToaD uses an array-based representation storing children at indices 2i+1 and 2i+2 to eliminate pointer memory costs
  - Quick check question: In an array-based representation of a complete binary tree, if a node is at index k, at what indices are its left and right children located?

## Architecture Onboarding
**Component Map:**
Training -> Modified LightGBM with ι, ξ penalties -> Trained model -> Encoder/serializer -> Compact bit-stream -> Inference/decoder

**Critical Path:**
The training-to-encoding link is most critical. Regularizers must be tuned correctly to constrain unique features/thresholds, which minimizes bit-width for tree references. If training doesn't effectively reuse values, encoding won't achieve significant compression.

**Design Tradeoffs:**
- Regularization strength (ι, ξ) vs. accuracy: Increasing penalties reduces memory but risks degrading accuracy
- Global lookup table size vs. reference bit-width: Larger sets allow more expressive trees but require more bits for references
- Tree depth/count vs. memory budget: Framework allows direct forestsize constraint but optimal combination requires hyperparameter search

**Failure Signatures:**
- Training divergence: Setting penalties too high prevents meaningful splits, resulting in near-random performance
- Inefficient compression: Feature & Threshold Map nearly as large as tree data indicates regularizers failed to induce reuse
- Decoding errors on device: Bit-width metadata mismatches with actual data cause inference failure

**First 3 Experiments:**
1. Baseline reproduction: Train standard LightGBM and ToaD (ι=0, ξ=0) on Covertype dataset, compare accuracies to ensure modified framework behaves correctly
2. Penalty sensitivity analysis: Grid search over ι and ξ on Covertype, plot accuracy vs. memory footprint to identify optimal compression-accuracy tradeoff
3. Memory layout breakdown: Analyze final model file, calculate memory consumed by each of five components to verify bulk is in tree references, not global tables

## Open Questions the Paper Calls Out
- Can more sophisticated regularization schemes (e.g., exponentially increasing penalties) outperform the linear penalizer used in this work? The authors suggest deeper analysis of alternative penalizers could reveal better performance.
- Can more effective reuse of leaf values further reduce memory footprint without sacrificing model quality? The authors note adapting the method to reuse leaf values more effectively could be useful.
- How does ToaD perform when deployed and executed on actual microcontroller hardware in terms of latency and energy consumption? The authors state deployment to different microcontroller units would be valuable.

## Limitations
- Does not evaluate on-device inference latency or energy consumption, only theoretical memory footprint
- Assumes thresholds are "representable" in 1-32 bits without specifying quantization scheme details
- Requires hyperparameter search to find optimal penalty values for each dataset and memory constraint

## Confidence
High: The theoretical foundation is sound, the method is well-defined, and the results are reproducible with the provided code
Medium: The memory calculations and compression ratios appear accurate but would benefit from on-device validation
Low: The impact on inference latency and energy consumption remains unknown without hardware deployment

## Next Checks
- Verify the bit-packing implementation for Feature & Threshold Map encoding matches the paper's description
- Confirm the discretization scheme for thresholds to 1-32 bit representations during training
- Test the trained models on actual microcontroller hardware to measure real-world inference latency and energy consumption