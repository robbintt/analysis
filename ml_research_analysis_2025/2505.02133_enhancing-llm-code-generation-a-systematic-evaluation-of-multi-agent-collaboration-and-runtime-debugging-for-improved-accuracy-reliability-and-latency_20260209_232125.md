---
ver: rpa2
title: 'Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration
  and Runtime Debugging for Improved Accuracy, Reliability, and Latency'
arxiv_id: '2505.02133'
source_url: https://arxiv.org/abs/2505.02133
tags:
- code
- accuracy
- debugging
- generation
- debugger
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper systematically evaluates two post-training strategies
  for LLM code generation: multi-agent collaboration and runtime debugging. The authors
  implemented a chained system combining both approaches and compared their performance
  across 19 LLMs on two benchmark datasets.'
---

# Enhancing LLM Code Generation: A Systematic Evaluation of Multi-Agent Collaboration and Runtime Debugging for Improved Accuracy, Reliability, and Latency

## Quick Facts
- arXiv ID: 2505.02133
- Source URL: https://arxiv.org/abs/2505.02133
- Authors: Nazmus Ashrafi; Salah Bouktif; Mohammed Mediani
- Reference count: 40
- Primary result: Debugging alone achieved 63.86% accuracy on HumanEval; combining with simple two-agent workflow yielded only 0.68% improvement over debugging alone.

## Executive Summary
This paper systematically evaluates runtime debugging and multi-agent collaboration as post-training strategies for improving LLM code generation. The authors implemented a chained system combining both approaches and compared their performance across 19 LLMs on HumanEval and MBPP benchmarks. Runtime debugging using Control Flow Graph analysis and execution monitoring achieved the highest accuracy (63.86% on HumanEval), while adding a simple Analyst-Coder agentic workflow provided only marginal improvements (0.68%). More complex agentic configurations did not yield significant additional benefits, suggesting that simpler approaches combined with runtime debugging offer the best tradeoff between accuracy, reliability, and latency.

## Method Summary
The study evaluated three approaches: basic LLM prompting, runtime debugging, and multi-agent collaboration. The debugging mechanism decomposes failed code into basic blocks using CFG analysis, monitors intermediate variable values during execution, and provides block-level explanations to a coder agent for correction. The multi-agent collaboration involves an Analyst Agent decomposing requirements, a Coder Agent implementing solutions, and a Tester Agent evaluating functionality with up to 3 refinement iterations. A comprehensive evaluation was conducted across 19 LLMs on HumanEval and MBPP datasets, with each model tested under all three approaches using pass@1 accuracy as the primary metric.

## Key Results
- Runtime debugging achieved the highest mean accuracy (63.86%) on HumanEval, outperforming basic approaches by 7.31 percentage points.
- Combining debugging with a simple two-agent workflow (Analyst-Coder) yielded only a modest 0.68% improvement over debugging alone.
- More complex agentic configurations when combined with debugging introduced higher latency without substantial accuracy gains.
- The larger the performance gap between debugging and agentic approaches for a given model, the less benefit observed from combining them.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Runtime execution feedback provides richer error context than static code review, enabling more targeted corrections.
- Mechanism: The debugger decomposes failed code into basic blocks via Control Flow Graph (CFG) analysis, monitors intermediate variable values during execution with visible test cases, and passes block-level explanations to a coder agent for correction. This grounds debugging in observed execution behavior rather than speculation.
- Core assumption: Models can effectively interpret variable state traces and map them to root causes.
- Evidence anchors:
  - [abstract] "runtime execution information-based debugging... for improving code generation functionality, reliability, and practical applicability"
  - [section 3.2] "This phase employs a debugging mechanism that decomposes the code that failed the test into basic blocks using a Control Flow Graph (CFG) analysis... intermediate variable values within these blocks are monitored and logged"
  - [corpus] Weak direct support; neighbor papers address multi-agent systems broadly but not runtime debugging specifics.
- Break condition: If initial code generation is extremely poor, the debugging phase receives misdirecting context, potentially amplifying errors rather than correcting them.

### Mechanism 2
- Claim: Role-specialized multi-agent collaboration improves code planning but introduces fragility when agentic complexity increases.
- Mechanism: An Analyst Agent decomposes requirements into subtasks and produces a high-level plan; a Coder Agent implements the solution; a Tester Agent evaluates functionality, readability, and maintainability. Iterative refinement occurs through up to 3 coder-tester interaction cycles.
- Core assumption: Specialization improves output quality, and role-specific prompts transfer across diverse model architectures.
- Evidence anchors:
  - [abstract] "multi-agent collaboration... creates a cooperative environment that mirrors the division of labor typically practiced by software development teams"
  - [section 3.1] "The Analyst Agent begins by decomposing the coding requirements into manageable subtasks and developing a high-level implementation plan"
  - [section 4.3.2] "more complex agentic configurations when used in combination with debugging often introduce higher latency without yielding substantial improvements"
  - [corpus] Neighbor papers (OMAC, MedAgentBoard) confirm multi-agent collaboration benefits in complex tasks but note variable generalizability across domains.
- Break condition: If analyst-coder interaction introduces misleading information, errors propagate through subsequent steps.

### Mechanism 3
- Claim: Combined ACT+Debugger effectiveness depends on the performance gap between individual techniques—smaller gaps yield greater synergistic benefit.
- Mechanism: When Debugger and ACT approaches perform similarly for a given model, their combination yields better results. When Debugger significantly outperforms ACT alone, adding ACT introduces unnecessary complexity without meaningful improvement.
- Core assumption: Models that handle both agentic workflows and debugging context effectively are more likely to benefit from combination.
- Evidence anchors:
  - [section 4.2.3] "the larger the gap between Debugger and ACT performance for a given model, the less benefit is observed from combining ACT with Debugging"
  - [section 4.2.3] "OpenAI models generally exhibit stable performance improvements when employing a combined approach"
  - [corpus] Weak direct support; no neighbor papers specifically address conditional effectiveness of combined techniques.
- Break condition: When the performance gap exceeds a model-dependent threshold (empirically observable via pilot testing), combination degrades or fails to improve outcomes.

## Foundational Learning

- Concept: **Pass@k evaluation metric**
  - Why needed here: The paper uses pass@1 as the primary accuracy measure; understanding this metric is essential for interpreting all experimental results.
  - Quick check question: Why does pass@1 reduce to a binary outcome when n=1?

- Concept: **Control Flow Graph (CFG) basic block decomposition**
  - Why needed here: The debugging mechanism relies on CFG analysis to partition code into debuggable units; understanding this is prerequisite to implementing or modifying the debugger component.
  - Quick check question: What distinguishes a basic block from a line-level or function-level decomposition?

- Concept: **Within-subject experimental design with paired t-tests**
  - Why needed here: RQ1 uses paired t-tests to compare approaches across the same 19 models; interpreting statistical significance claims requires understanding this design.
  - Quick check question: Why is a paired t-test appropriate when each model is evaluated under all three conditions?

## Architecture Onboarding

- Component map:
Problem Input → [Analyst Agent] → Plan → [Coder Agent] → Code
                                                         ↓
                      [Tester Agent] ←── Code ←─────────┘
                            ↓ (up to 3 iterations)
                      Code (passed or max retries)
                            ↓
                      Execution vs. visible test cases
                            ↓ (if failed)
                      [Debugger] → CFG decomposition → variable tracing
                            ↓
                      Block-level explanations → [Coder Agent] → Corrected Code
                            ↓ (up to 4 iterations)
                      Final Output

- Critical path: Coder Agent is invoked in both ACT and debugging phases; prompt consistency and state management across invocations is the highest-risk integration point.

- Design tradeoffs:
  - `retriesCT=3` vs. original 4: Reduces latency but may truncate productive refinement cycles.
  - `retriesD=4` vs. original 10: Saves cost but limits error correction depth for complex bugs.
  - Block-level vs. line-level CFG decomposition: Block-level achieved highest accuracy in LDB paper, but may miss fine-grained errors.

- Failure signatures:
  - Accuracy degradation when moving from AC+Debugger to ACT+Debugger suggests tester agent introduces noise for some models.
  - Models where basic approach outperforms AC/ACT indicate analyst-coder misalignment (e.g., QwQ-Preview).

- First 3 experiments:
  1. **Baseline reproduction**: Run Basic, Debugger-only, and AC+Debugger on 3 diverse models (e.g., GPT-4o-mini, Llama 3.1 70B, Mistral 7B) on HumanEval subset (20 problems) to validate pipeline correctness.
  2. **Gap threshold detection**: For each model, measure Debugger-minus-ACT accuracy gap; correlate with combined approach improvement to identify model-specific gap thresholds.
  3. **Latency-accuracy tradeoff sweep**: Vary `retriesCT` (1, 2, 3) and `retriesD` (2, 4, 6) on a single model to map the Pareto frontier before production deployment.

## Open Questions the Paper Calls Out

None

## Limitations

- Training set contamination risk: The study did not explicitly check for overlap between HumanEval/MBPP problems and model training corpora.
- Synthetic test cases: HumanEval and MBPP use artificially constructed test cases rather than real-world code testing scenarios.
- Single-shot evaluation: The study uses pass@1 evaluation without exploring pass@k with k>1.

## Confidence

- **High Confidence**: The core finding that runtime debugging provides substantial accuracy improvements over basic approaches (63.86% vs 56.55% on HumanEval) is well-supported by the experimental design and statistical analysis.
- **Medium Confidence**: The claim that adding ACT to debugging provides only marginal improvements (0.68% on HumanEval) is supported but may be sensitive to specific prompt engineering choices and model configurations.
- **Low Confidence**: The assertion that more complex agentic configurations provide no additional benefits is based on limited comparisons and may not generalize to different problem domains or larger model families.

## Next Checks

1. **Training data contamination audit**: Perform a systematic check for overlap between benchmark problems and model training corpora using n-gram analysis and semantic similarity measures to quantify potential contamination bias.

2. **Real-world benchmark validation**: Evaluate the best-performing approaches (Debugger + AC workflow) on a real-world code generation benchmark with production-style test cases to assess practical applicability beyond synthetic datasets.

3. **Prompt engineering ablation study**: Systematically vary the prompts for Analyst, Coder, and Tester agents while keeping the core workflow constant to determine the sensitivity of results to prompt engineering choices and identify more robust configurations.