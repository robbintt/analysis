---
ver: rpa2
title: An Empirical Study of Evaluating Long-form Question Answering
arxiv_id: '2504.18413'
source_url: https://arxiv.org/abs/2504.18413
tags:
- evaluation
- metrics
- arxiv
- human
- evaluations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the reliability of automatic evaluation
  metrics for long-form question answering (LFQA), addressing the gap between deterministic
  metrics (like ROUGE and BLEU) and human evaluations. The research questions focus
  on the accuracy, robustness, and fairness of existing evaluation methods, particularly
  LLM-based evaluators.
---

# An Empirical Study of Evaluating Long-form Question Answering

## Quick Facts
- arXiv ID: 2504.18413
- Source URL: https://arxiv.org/abs/2504.18413
- Reference count: 40
- Primary result: LLM-based evaluation metrics show significantly higher consistency with human judgments than deterministic metrics for long-form question answering

## Executive Summary
This study investigates the reliability of automatic evaluation metrics for long-form question answering (LFQA), addressing the gap between deterministic metrics (like ROUGE and BLEU) and human evaluations. The research questions focus on the accuracy, robustness, and fairness of existing evaluation methods, particularly LLM-based evaluators. Experiments were conducted using responses from seven LLMs across three datasets, with human evaluation on a subset of 2,079 answers. Results show that LLM-based metrics demonstrate higher consistency with human evaluations than deterministic metrics, especially in open-ended QA. Fine-grained evaluation with LLMs improves accuracy, while deterministic metrics are biased by answer length and question type. The study concludes that LLM-based evaluations are more stable but require careful prompting and multi-method approaches to ensure fairness and reliability.

## Method Summary
The study evaluates automatic evaluation metrics for LFQA by generating answers from seven LLMs on three benchmark datasets (ASQA, ANTIQUE, WikiEval), then comparing both deterministic metrics (ROUGE-L, BERTScore, Exact Match, Disambig-F1) and LLM-based evaluators (GPT-4o, Claude-3.5, Gemini-2.0) against human judgments. Human annotators rated 2,079 answers on correctness and informativeness scales. The evaluation examines correlation coefficients (Spearman for value correlation, Kendall for rank correlation), biases including self-reinforcement and length effects, and the impact of prompt engineering on evaluator performance.

## Key Results
- LLM-based metrics show significantly higher consistency with human evaluations (Spearman correlation up to 55.0) compared to deterministic metrics (11-23%)
- Fine-grained prompting improves LLM evaluation performance, with complete prompts achieving the best results
- Deterministic metrics penalize longer answers while LLM-based evaluators favor them, creating opposing biases
- All evaluation methods perform worse on evidence-based and experience-type questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based evaluators achieve higher alignment with human judgments for LFQA than deterministic n-gram metrics
- Mechanism: LLMs process semantic coherence at paragraph level, capturing correctness and informativeness dimensions that n-gram overlap cannot measure. Deterministic metrics rely on surface-level lexical matching, which penalizes semantically equivalent but lexically different phrasing.
- Core assumption: Human evaluation patterns can be approximated by LLMs trained on text quality assessment tasks; Assumption: This generalizes across question types.
- Evidence anchors:
  - [abstract]: "LLM-based metrics show significantly higher consistency with human evaluations and greater stability across different LFQA tasks"
  - [section 3.2]: "fine-grained (FG) GPT4 achieves the best performance on both the ASQA and Antique datasets" with Spearman correlation 55.0 vs. deterministic metrics averaging ~11-23%
  - [corpus]: LFQA-E paper (arxiv:2410.01945) confirms evaluation challenges in LFQA due to "richness of information and flexible response format"
- Break condition: When evaluating evidence-based or experience-type questions (Section 3.4.2 shows all evaluators underperform on these categories); when LLM hallucination corrupts judgment.

### Mechanism 2
- Claim: Fine-grained prompting with structured components improves LLM evaluator accuracy
- Mechanism: Decomposing evaluation prompts into four explicit components (task description, data specifications, output requirements, evaluation criteria) reduces ambiguity and provides concrete scoring rubrics, enabling more consistent LLM judgment.
- Core assumption: Explicit structured guidance reduces variance in LLM evaluation decisions; Assumption: Component ordering affects performance.
- Evidence anchors:
  - [abstract]: "Fine-grained prompting improves LLM-based evaluation performance"
  - [section 4]: "P1 incorporates all four components... comparing P1 with P2, P5 and P6, it can be observed that P1 achieves significantly better performance across all LLMs" - Kendall correlation improves from ~5-12% (incomplete prompts) to ~11-16% (complete prompts)
  - [corpus]: Weak/missing - no corpus papers directly address prompt structure for evaluation
- Break condition: When prompt structure deviates significantly from tested configurations; different models may prefer different prompt orderings (Section 4 notes model-specific optimal prompts differ).

### Mechanism 3
- Claim: Combining deterministic and LLM-based metrics with length-aware normalization mitigates opposing biases
- Mechanism: Deterministic metrics penalize longer answers (diluting precision), while LLM-based metrics favor longer answers (prioritizing comprehensiveness). Multi-metric approaches with length normalization balance these opposing biases.
- Core assumption: Human evaluators balance conciseness and comprehensiveness appropriately.
- Evidence anchors:
  - [abstract]: "deterministic metrics penalize longer answers, while LLM-based evaluators favor... rare words"
  - [section 3.4.1]: "negative correlation between answer length and scores on Rouge-L and BERTScore... LLMs are less likely to assign low scores to longer answers, a phenomenon not observed in human evaluations"
  - [corpus]: FinLFQA paper confirms length-related evaluation complexity in specialized LFQA domains
- Break condition: When extreme answer lengths occur; when domain-specific length expectations differ from training distribution.

## Foundational Learning

- **Concept: Meta-Evaluation via Correlation Coefficients**
  - Why needed here: The paper measures evaluation quality by computing correlation between automatic metrics and human judgments. Spearman measures value correlation; Kendall measures rank correlation.
  - Quick check question: If an automatic metric has Kendall correlation of -0.11 with human ratings on ANTIQUE (as ROUGE-L does), what does this tell you about using it for that task?

- **Concept: LLM Evaluator Biases (Self-Reinforcement, Length, Rarity)**
  - Why needed here: Understanding systematic biases is critical for interpreting evaluation results. GPT-4o favors GPT-4o outputs; Claude-3.5 favors Claude-3.5 outputs. Both favor higher-IDF (rarer) vocabulary.
  - Quick check question: You're comparing GPT-4o vs. Mistral-7b outputs using GPT-4o as evaluator. What adjustment should you consider for fair comparison?

- **Concept: Reference-Based vs. Reference-Free Evaluation**
  - Why needed here: Deterministic metrics (ROUGE, BERTScore) require reference answers; LLM-based evaluators can operate reference-free. Choice depends on whether high-quality references exist for your domain.
  - Quick check question: For a novel LFQA domain without curated reference answers, which evaluation approach is immediately viable?

## Architecture Onboarding

- **Component map:**
  - Data Layer: 3 benchmark datasets (ASQA: ambiguous/factoid, ANTIQUE: non-factoid/open-ended, WikiEval: factoid/closed-ended); 7 LLM generators (GLM-4, Llama2-7b/13b, Llama3-8b, GPT-3.5, Mistral-7b, Solar-10.7b)
  - Evaluation Layer: Deterministic metrics (ROUGE-L, Exact Match, Disambig-F1, BERTScore, Answer Relevance) + LLM-based evaluators (GPT-4o, Claude-3.5, Gemini-2.0 in coarse-grained and fine-grained modes)
  - Meta-Evaluation Layer: Human annotations (2,079 answers rated on correctness + informativeness), correlation analysis (Spearman/Kendall), bias diagnostics

- **Critical path:**
  1. Generate candidate answers using target LLMs on benchmark questions
  2. Apply both deterministic and LLM-based evaluation metrics with standardized prompts
  3. Collect human ratings for calibration subset (correctness and informativeness on 1-5 scale)
  4. Compute correlation coefficients to identify best-aligned metrics
  5. Run bias diagnostics: length bins, question types, self-reinforcement check, IDF distribution

- **Design tradeoffs:**
  - Single vs. multi-metric: Single metrics exhibit domain-specific biases; multi-metric approaches add complexity but improve robustness
  - Coarse vs. fine-grained prompts: Fine-grained improves correlation (42.0 → 55.0 Spearman for GPT-4o) but requires more prompt engineering
  - Temperature stability: Lower temperature (0.0) recommended for evaluation stability; higher temperatures cause rank instability on close-scoring datasets

- **Failure signatures:**
  - Negative correlation with human judgment (ROUGE-L: -14.4 Spearman on ANTIQUE)
  - Score clustering away from human distribution (LLM scores cluster around 3; humans around 5)
  - Self-preference bias (GPT-4o win rate vs. baselines: 0.92 when self-evaluating vs. 0.26 when Claude-3.5 evaluates)
  - Rank instability under temperature perturbation (GPT-4o rank drops 4th→6th on WikiEval when temperature 0.0→0.3)

- **First 3 experiments:**
  1. **Baseline correlation study:** Run all evaluation metrics on your LFQA dataset, collect human ratings for 200-500 samples, compute Spearman/Kendall correlations to identify which metrics align with human judgment in your domain.
  2. **Prompt ablation test:** Implement the 9 prompt configurations (P1-P9 from Table 6) with your evaluator LLM. Test whether complete prompts (P1 with task+data+output+criteria) outperform incomplete variants.
  3. **Bias diagnostic:** Bin answers by length quintiles and compute metric scores per bin. If deterministic scores decrease with length while LLM scores increase, implement length-normalized multi-metric scoring: `final_score = α × normalized_deterministic + (1-α) × normalized_LLM` with α tuned per domain.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do automatic evaluation metrics perform on diverse long-form QA formats, such as multi-hop reasoning, beyond the factoid and non-factoid categories tested?
- **Basis in paper:** [explicit] The authors explicitly state that their evaluation covers only 3 QA categories, while real-world LFQA involves "more diverse formats (e.g., multihop reasoning)."
- **Why unresolved:** The study is restricted to ASQA, ANTIQUE, and WikiEval, limiting generalizability to complex reasoning tasks not present in these benchmarks.
- **What evidence would resolve it:** Testing metric consistency against human judgments specifically on multi-hop reasoning and cross-domain LFQA datasets.

### Open Question 2
- **Question:** Does a multi-dimensional human evaluation rubric improve the alignment of automatic metrics by capturing subtle quality differences missed by single-point scoring?
- **Basis in paper:** [explicit] The limitations section notes that the study relied on a single-point scoring approach for "correctness" and "informativeness," which may miss "subtle quality differences."
- **Why unresolved:** The current coarse-grained human annotation may lack the sensitivity to validate whether automatic metrics can detect nuanced distinctions in answer quality.
- **What evidence would resolve it:** A comparative study correlating automatic metric scores against a fine-grained human protocol that breaks down quality into atomic skills.

### Open Question 3
- **Question:** How do different LLM families respond specifically to variations in prompt components (task, data, output, criteria) when acting as evaluators?
- **Basis in paper:** [explicit] The authors note that while they revealed sensitivity to prompts, they "have not conducted a detailed analysis of how different models respond to this."
- **Why unresolved:** The paper showed aggregate trends but did not determine if specific model architectures are more robust or brittle to prompt engineering than others.
- **What evidence would resolve it:** A systematic ablation study measuring performance variance across prompt permutations for a diverse range of evaluator models.

### Open Question 4
- **Question:** Can a calibrated ensemble of deterministic and LLM-based metrics effectively neutralize opposing biases such as length penalty and self-reinforcement?
- **Basis in paper:** [inferred] The paper recommends "combining metrics" to balance the finding that deterministic metrics penalize length while LLMs favor it, but does not validate a specific composite strategy.
- **Why unresolved:** While individual biases are identified, the interaction between metrics in a combined evaluation scheme remains unexplored.
- **What evidence would resolve it:** Designing and testing a weighted composite metric that normalizes for length and self-preference against human ground truth.

## Limitations
- Limited generalizability beyond three specific datasets (ASQA, ANTIQUE, WikiEval) to diverse LFQA formats
- Identified systematic biases in LLM evaluators (self-reinforcement, preference for rare vocabulary) without comprehensive mitigation strategies
- Single-point human scoring approach may miss subtle quality differences that could improve metric alignment

## Confidence

**High Confidence**: The finding that LLM-based metrics demonstrate higher correlation with human judgments than deterministic metrics (55.0 vs. 11-23% Spearman correlation) is well-supported by the experimental results and consistent across multiple datasets and evaluation configurations.

**Medium Confidence**: The recommendation for fine-grained prompting and the identification of specific prompt components that improve performance are supported by ablation studies, but the model-specific nature of optimal prompts reduces generalizability confidence.

**Low Confidence**: The claim that combining deterministic and LLM-based metrics with length normalization provides optimal evaluation remains partially validated, as the study demonstrates opposing biases but doesn't provide comprehensive multi-metric combination strategies or optimal weighting schemes.

## Next Checks

1. **Cross-Domain Validation**: Apply the evaluation methodology to a novel LFQA domain with different characteristics (e.g., technical documentation, creative writing) to test whether the identified evaluation patterns hold beyond the three studied datasets.

2. **Bias Mitigation Experiment**: Design and test prompt engineering techniques specifically targeting the identified LLM evaluator biases (self-reinforcement and IDF preference), measuring whether these techniques improve cross-model evaluation consistency without sacrificing overall accuracy.

3. **Long-Tail Question Analysis**: Conduct focused analysis on rare or edge-case question types (combining evidence-based and experience-type categories) to determine whether current evaluation approaches systematically fail on specific question subtypes, and develop specialized evaluation protocols for these cases.