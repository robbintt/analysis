---
ver: rpa2
title: 'Breaking Habits: On the Role of the Advantage Function in Learning Causal
  State Representations'
arxiv_id: '2506.11912'
source_url: https://arxiv.org/abs/2506.11912
tags:
- advantage
- policy
- agent
- function
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper demonstrates that the advantage function in reinforcement
  learning does more than just reduce gradient variance - it helps agents learn causal
  state representations by mitigating policy confounding. The advantage function downweights
  frequently visited state-action pairs and upweights rare but informative ones, breaking
  spurious correlations that policies can introduce.
---

# Breaking Habits: On the Role of the Advantage Function in Learning Causal State Representations

## Quick Facts
- arXiv ID: 2506.11912
- Source URL: https://arxiv.org/abs/2506.11912
- Reference count: 27
- Agents trained with advantages learn causal state representations by mitigating policy confounding, breaking spurious correlations that policies introduce.

## Executive Summary
This paper reveals that the advantage function in reinforcement learning serves a deeper purpose than variance reduction - it helps agents learn causal state representations by mitigating policy confounding. The advantage function downweights frequently visited state-action pairs while upweighting rare but informative ones, breaking spurious correlations that policies can introduce. Through experiments in three gridworld environments, the paper demonstrates that agents trained with advantages significantly outperform those trained with Q-values on out-of-trajectory generalization tasks, showing that advantages enable agents to focus on true causal factors rather than habits based on spurious correlations.

## Method Summary
The paper compares two training approaches: agents optimizing policy gradients with advantage-weighted updates versus agents using raw Q-values. All agents use PPO or REINFORCE with feedforward networks (2 layers × 128 neurons) and are trained for 100k steps across 10 seeds. The key difference is whether the policy gradient uses the advantage A(s,a) = Q(s,a) − V(Φ(s)) or simply Q(s,a). Monte Carlo rollouts estimate Q-values independently of the learned state representation Φ, while V(Φ(s)) is computed via the value network. The paper emphasizes that advantage normalization should be disabled to preserve the scaling mechanism.

## Key Results
- Advantage-trained agents achieve significantly better out-of-trajectory generalization than Q-value-trained agents in Key2Door, Frozen T-Maze, and Diversion environments
- Advantage normalization eliminates the scaling effect and matches Q-value performance on generalization tasks
- Larger batch sizes (512-1024) partially close the gap for Q-value agents but don't fully match advantage performance
- KL divergence analysis shows advantage agents develop policies more sensitive to causal factors (keys, signals) while Q-value agents often ignore these crucial features

## Why This Works (Mechanism)

### Mechanism 1
The advantage function scales gradients inversely to state-action pair probability, counteracting overrepresentation of frequent trajectories. The (1 − P) term in A^π_Φ(s_t, a_t) = (1 − P^π(s_t, a_t | Φ(s_t)))(Q^π(s_t, a_t) − Q̃^π(¬⟨s_t, a_t⟩)) reduces magnitude for high-probability pairs and increases it for rare pairs, while the Q-difference term provides directional signal. This holds when Q-values are estimated from Monte Carlo rollouts independent of Φ.

### Mechanism 2
The advantage function mitigates policy confounding by preventing the policy from reinforcing spurious correlations between observations and rewards. On-policy sampling creates feedback loops where the policy influences both past and future observations, inducing spurious correlations. The advantage's probability scaling breaks this by ensuring rare but causally informative state-action pairs contribute proportionally to gradient updates despite being underrepresented in batches.

### Mechanism 3
The advantage moderates overly aggressive gradient updates that would otherwise cause premature convergence to habitual policies. With stochastic gradients, batches often contain only one state from an equivalence class. Raw Q-values treat all samples equally, causing frequent pairs to dominate. The advantage's (1 − P) scaling reduces this dominance, preventing "committal behavior" where agents converge to habits based on spurious correlations.

## Foundational Learning

- **Policy confounding via do-operator**: Understanding that P^π(Φ(s_t)) ≠ P^π(do(Φ(s_t))) is essential to grasp why agents develop habits that fail under trajectory deviations. Quick check: If an agent always takes the shortest path to a goal, why might it fail when forced to take a detour?

- **Equivalence classes under state representation**: The paper's mechanism relies on Φ collapsing multiple states into the same representation, creating an equivalence class where the agent cannot distinguish causal factors. Quick check: In Key2Door, which states share the same representation if Φ(s) = location only?

- **On-policy vs. off-policy gradient estimation**: Policy confounding is exacerbated by on-policy methods because training data comes exclusively from the current policy's trajectory distribution. Quick check: Why might training on a replay buffer from multiple policies reduce spurious correlations?

## Architecture Onboarding

- **Component map**: Environment → Policy network π_θ → Action → Reward → Q-value estimation → Value network V_φ → Advantage computation → Policy update

- **Critical path**:
  1. Collect trajectories via current π
  2. Estimate Q-values from returns (Monte Carlo or TD)
  3. Compute V(Φ(s)) via value network forward pass
  4. Form advantages A = Q − V
  5. **Do NOT normalize advantages** if scaling effect is desired
  6. Update policy with advantage-weighted gradients

- **Design tradeoffs**:
  - Advantage normalization: Reduces variance but eliminates scaling effect → worse OOT generalization
  - Batch size: Larger batches help Q-value agents but increase compute; advantage agents less sensitive
  - GAE λ: Paper uses 0.95; higher λ increases bias in V estimation, potentially affecting equivalence class averaging

- **Failure signatures**:
  - Agent performs well on training trajectories but fails on evaluation variants
  - KL divergence between policy outputs for different causal factor values approaches zero
  - Policy becomes deterministic prematurely (entropy collapses)

- **First 3 experiments**:
  1. Ablation: Train identical agents optimizing A vs. Q on Key2Door; evaluate on start position 6. Expect: A-agent succeeds, Q-agent fails.
  2. Normalization test: Train with and without advantage normalization on the same environment. Expect: Normalized advantages match Q-value performance.
  3. Batch size sweep: Train Q-value agents with batch sizes [32, 128, 512, 1024] and measure evaluation return. Expect: Larger batches partially close the gap to advantage agents.

## Open Questions the Paper Calls Out

- Does the advantage function's causal representation learning effect extend to complex, high-dimensional domains such as visual RL or robotic control? The paper draws no conclusions about effectiveness in more complex domains and leaves this for future work.

- Does Theorem 1's scaling effect hold when Q-values are learned entirely via function approximation rather than Monte Carlo rollouts? The analysis assumes Monte Carlo estimation, but many actor-critic methods use function approximation for Q.

- How does the required batch size for Q-value training scale with environment complexity to achieve comparable out-of-trajectory generalization? The paper only tests fixed batch sizes in simple gridworlds.

- Can the advantage function guarantee learning of truly causal state representations, or does it only probabilistically mitigate policy confounding? The paper demonstrates mitigation but does not establish conditions for guaranteed causal representation learning.

## Limitations

- All experiments use gridworld environments with simple state spaces; benefits for causal state representation learning remain to be validated in high-dimensional continuous control tasks.
- The analysis assumes Q-values are estimated from Monte Carlo rollouts independent of Φ, but modern deep RL implementations often use the same network for both Q and V estimation.
- While the paper shows KL divergence differences between advantage and Q-value agents, it does not provide direct analysis of what the learned Φ actually represents.

## Confidence

- **High confidence**: The mathematical derivation of the advantage scaling mechanism (Theorem 1) and the basic empirical observation that advantage-based training improves OOT generalization in tested environments.
- **Medium confidence**: The causal interpretation of policy confounding and how the advantage function specifically addresses this issue through probability scaling.
- **Low confidence**: Claims about the advantage function preventing "commitful behavior" and broader applicability to complex domains beyond gridworlds.

## Next Checks

1. **Implementation verification**: Test the advantage scaling mechanism with different Q-value estimation methods (Monte Carlo returns vs bootstrapped targets) to confirm the decomposition holds when using function approximation.

2. **Representation analysis**: Apply representation probing techniques (e.g., linear probes, mutual information estimation) to compare what causal factors advantage-trained vs Q-value-trained agents actually encode in their state representations.

3. **Cross-domain validation**: Evaluate the advantage effect on a continuous control benchmark (e.g., DeepMind Control Suite) with explicit causal factors to determine if gridworld findings generalize to high-dimensional tasks.