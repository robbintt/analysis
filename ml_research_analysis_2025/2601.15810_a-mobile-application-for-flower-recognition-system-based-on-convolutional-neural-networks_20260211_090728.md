---
ver: rpa2
title: A Mobile Application for Flower Recognition System Based on Convolutional Neural
  Networks
arxiv_id: '2601.15810'
source_url: https://arxiv.org/abs/2601.15810
tags:
- classification
- flower
- learning
- layers
- mobile
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The study developed a mobile flower recognition application using\
  \ three CNN architectures\u2014MobileNet, DenseNet-121, and Xception\u2014trained\
  \ via transfer learning. The models were fine-tuned with different freezing ratios\
  \ and seven optimizers, and performance was compared using both GAP and Flatten\
  \ layers."
---

# A Mobile Application for Flower Recognition System Based on Convolutional Neural Networks

## Quick Facts
- arXiv ID: 2601.15810
- Source URL: https://arxiv.org/abs/2601.15810
- Reference count: 38
- Primary result: Mobile flower recognition app achieves 95.84% accuracy using DenseNet-121 with SGD optimizer and GAP layer

## Executive Summary
This study develops a mobile flower recognition application using three CNN architectures—MobileNet, DenseNet-121, and Xception—trained via transfer learning on the Kaggle Flowers dataset. The models are fine-tuned with different freezing ratios and seven optimizers, with performance compared using both GAP and Flatten layers. DenseNet-121 with SGD and GAP achieves the highest accuracy of 95.84%, precision of 96.00%, recall of 96.00%, and F1-score of 96.00%. The application demonstrates robust classification performance on real-world mobile devices, confirming CNNs are effective for mobile-based flower species identification.

## Method Summary
The study uses transfer learning with three CNN architectures (MobileNet, DenseNet-121, Xception) pretrained on ImageNet, fine-tuned on a 16-class flower dataset (15,742 images) with 0%, 25%, 50%, and 75% layer freezing ratios. Seven optimizers (SGD, RMSprop, Adagrad, Adadelta, Adam, Nadam, Adamax) are tested across configurations. Models use either Global Average Pooling or Flatten layers before the final dense layer. Training runs for 50 epochs with batch size 32 and categorical crossentropy loss on Google Colab GPU. The best model (DenseNet-121 + SGD + GAP) achieves 95.84% accuracy and is exported to TensorFlow Lite for mobile deployment.

## Key Results
- DenseNet-121 with SGD optimizer and GAP layer achieves highest accuracy of 95.84%
- 0% freezing ratio (all layers trainable) outperforms partial freezing for this dataset
- GAP layer provides better performance than Flatten while using fewer parameters (~20.9M vs ~22.4M for Xception)
- Mobile application runs efficiently on target devices with inference times under 150ms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Full network fine-tuning (0% layer freezing) outperforms partial freezing for datasets with low similarity to ImageNet pretraining data.
- Mechanism: When the target domain (flowers) differs substantially from the source domain (ImageNet's 1000 general categories), retraining all convolutional layers allows the network to learn domain-specific features rather than relying on potentially mismatched low-level representations.
- Core assumption: Flower morphology features differ sufficiently from ImageNet's general object categories that early convolutional layers benefit from domain-specific adaptation.
- Evidence anchors: Table 6 shows 0% freezing produced better results for all models; "The most important factor in this result is the low similarity rate of the dataset we used with ImageNet data"
- Break condition: If target dataset has high visual similarity to ImageNet classes, partial freezing may prevent overfitting and reduce training time without accuracy loss.

### Mechanism 2
- Claim: Global Average Pooling (GAP) outperforms Flatten layers for parameter efficiency while maintaining classification accuracy.
- Mechanism: GAP reduces each feature map to a single average value, dramatically reducing parameters before the dense layer (from ~22.4M to ~20.9M for Xception), which reduces overfitting risk and computational overhead without losing discriminative spatial information.
- Core assumption: The spatial average of final convolutional feature maps contains sufficient class-discriminative information for flower classification.
- Evidence anchors: DenseNet-121 with Flatten dropped to 76.93% accuracy vs 95.84% with GAP; "finishing the convolution layers of the models with GAP gives better results for all models than finishing with Flatten"
- Break condition: If classification requires precise spatial localization of features, GAP's spatial aggregation may discard critical positional information.

### Mechanism 3
- Claim: Optimizer selection is architecture-dependent; SGD favors deeper networks (DenseNet-121, Xception) while Adam-style optimizers may suit lighter architectures (MobileNet).
- Mechanism: Different optimizers navigate the loss landscape differently—SGD with momentum provides stable convergence for complex, densely connected architectures, while adaptive optimizers (Adamax, Adam) may converge faster on simpler networks with fewer parameters.
- Core assumption: Architecture complexity interacts with optimizer behavior in ways that affect final classification performance on the target task.
- Evidence anchors: DenseNet-121 with SGD achieved 95.84% vs Adamax at 95.58% for MobileNet; Xception also performed best with SGD (95.33%)
- Break condition: If learning rate scheduling is aggressive or dataset is very small, adaptive optimizers may outperform SGD even for deep architectures.

## Foundational Learning

- Concept: **Transfer Learning**
  - Why needed here: Core technique for leveraging ImageNet-pretrained weights on flower classification with limited training data (15,742 images across 16 classes)
  - Quick check question: Can you explain why freezing early layers vs. later layers affects what features the network learns?

- Concept: **Global Average Pooling vs. Fully Connected Layers**
  - Why needed here: Critical design choice that reduced parameters by ~1.5M for Xception while improving accuracy; directly impacts mobile deployment feasibility
  - Quick check question: If a feature map is 7×7×512, what dimension does GAP output vs. Flatten?

- Concept: **Optimizer Characteristics (SGD vs. Adam-family)**
  - Why needed here: Study tested 7 optimizers; optimizer choice caused accuracy swings of up to 23% (Xception: Nadam 72.08% vs. SGD 95.33%)
  - Quick check question: Why might SGD with momentum converge more stably than Adam for very deep networks?

## Architecture Onboarding

- Component map: Input Image (224×224×3 or 229×229×3) -> Pretrained CNN Backbone (MobileNet/DenseNet-121/Xception) -> Global Average Pooling (recommended) OR Flatten -> Dense Layer (16 outputs) -> Softmax Activation -> Flower Class Prediction

- Critical path: Image preprocessing (resize to 224×224 or 229×229) -> Data augmentation (rotation 0.4, width shift 0.2, height shift 0.3, shear 0.2, zoom 0.2) -> Model selection -> Optimizer selection -> Fine-tuning with 0% freezing -> GAP -> Train for 50 epochs with batch size 32 -> Export to TensorFlow-Lite for mobile deployment

- Design tradeoffs:
  - MobileNet: Fastest inference (68.91ms on Redmi 9) but slightly lower accuracy (95.58%)—best for real-time mobile apps
  - DenseNet-121: Best accuracy (95.84%) with moderate inference time (73-280ms)—best for accuracy-critical applications
  - Xception: Large parameter count (20.9M), highest memory footprint—less suitable for low-end devices

- Failure signatures:
  - DenseNet-121 with Flatten layer: Accuracy drops to 76.93% (vs. 95.84% with GAP)—indicates overfitting from parameter explosion
  - Nadam optimizer on Xception: 72.08% accuracy—suggests optimizer-architecture mismatch
  - Misclassification of visually similar flowers (shown in Fig. 7)—indicates need for class-specific data augmentation or more training data

- First 3 experiments:
  1. Establish baseline: Train DenseNet-121 with 0% freezing, SGD optimizer, GAP layer, 50 epochs, batch size 32—target ≥95% accuracy
  2. Ablation on pooling: Replace GAP with Flatten on same configuration—expect accuracy drop to ~77% to validate GAP necessity
  3. Mobile deployment test: Convert best model to TensorFlow-Lite, measure inference time on target device—target <150ms for acceptable user experience

## Open Questions the Paper Calls Out
None

## Limitations
- Learning rate and momentum hyperparameters for SGD optimizer not specified, though these critically affect convergence and final accuracy
- No random seed reported for reproducibility, making exact replication of results impossible
- Single dataset evaluation (Kaggle Flowers) limits generalizability claims to other domains
- No statistical significance testing between different configurations (freezing ratios, optimizers, architectures)

## Confidence
- **High Confidence**: DenseNet-121 with SGD and GAP achieving 95.84% accuracy is well-supported by direct experimental results
- **Medium Confidence**: Generalization that 0% freezing is optimal for low-similarity transfer learning tasks, based on single dataset evidence
- **Low Confidence**: Claims about mobile deployment performance without providing actual user testing or real-world usage data

## Next Checks
1. **Hyperparameter Sensitivity**: Systematically test SGD learning rates (0.001, 0.01, 0.1) and momentum values (0.9, 0.99) to determine optimal configuration and assess stability
2. **Cross-Dataset Validation**: Apply the best configuration (DenseNet-121 + SGD + GAP) to a different flower dataset (e.g., Oxford 102 Flowers) to test generalizability of the low-similarity hypothesis
3. **Statistical Significance**: Perform paired t-tests between top configurations (SGD vs. Adam variants, GAP vs. Flatten) to quantify whether performance differences are statistically meaningful