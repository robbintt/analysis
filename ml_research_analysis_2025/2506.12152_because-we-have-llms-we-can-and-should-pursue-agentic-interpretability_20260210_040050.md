---
ver: rpa2
title: Because we have LLMs, we Can and Should Pursue Agentic Interpretability
arxiv_id: '2506.12152'
source_url: https://arxiv.org/abs/2506.12152
tags:
- interpretability
- agentic
- human
- mental
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper argues that large language models (LLMs) enable a new
  approach to interpretability: agentic interpretability. Unlike traditional "inspective"
  methods that open the black box, agentic interpretability leverages the model itself
  as a cooperative agent to help humans understand it through multi-turn conversation.'
---

# Because we have LLMs, we Can and Should Pursue Agentic Interpretability

## Quick Facts
- arXiv ID: 2506.12152
- Source URL: https://arxiv.org/abs/2506.12152
- Authors: Been Kim; John Hewitt; Neel Nanda; Noah Fiedel; Oyvind Tafjord
- Reference count: 16
- Key outcome: Argues that LLMs enable "agentic interpretability"—a conversational, proactive approach where the model helps humans understand it through mutual mental model development.

## Executive Summary
This paper proposes agentic interpretability as a new paradigm for understanding machine learning models, leveraging LLMs as cooperative agents in multi-turn conversations. Unlike traditional "inspective" methods that dissect models from the outside, agentic interpretability treats the model as an active participant that proactively assists human understanding by developing and maintaining a mental model of the user. The approach enables humans to build better mental models of the LLM through iterative dialogue, potentially discovering superhuman concepts that improve human understanding of machines.

## Method Summary
The paper defines agentic interpretability as a framework where an LLM engages in multi-turn dialogue with humans to explain its behavior and concepts. The model proactively assists by developing a mental model of the user's knowledge and understanding, adapting explanations accordingly. The framework has three core components: proactive assistance (unsolicited clarifications), multi-turn interaction (iterative refinement), and mutual mental model development (implicit via conversation history or explicit via knowledge graphs). Evaluation involves two cases: "case improve" (modifying model behavior) and "case learn" (human's ability to predict concepts on new examples).

## Key Results
- Agentic interpretability enables discovery of potentially superhuman concepts that improve human understanding of machines
- The approach trades completeness for interactivity and may be less suitable for high-stakes safety situations with deceptive models
- Evaluation challenges exist due to the "human-entangled-in-the-loop" nature, requiring proxy metrics or LLM-as-user approximations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-turn dialogue with mental model tracking enables explanations adapted to user knowledge gaps.
- Mechanism: The LLM maintains an implicit or explicit representation of the user's current understanding (what they know, don't know, misunderstand). This informs proactive clarifications and tailored explanations, similar to a teacher modeling a student. The user simultaneously builds a mental model of the LLM through iterative interaction.
- Core assumption: LLMs can infer user knowledge state from conversational context and adapt explanations accordingly.
- Evidence anchors:
  - [abstract] "LLM proactively assists human understanding by developing and leveraging a mental model of the user, which in turn enables humans to develop better mental models of the LLM."
  - [section 2.1] "The model develops and maintains (implicitly or explicitly) a representation of the user's current knowledge, understanding, and potential misconceptions."
  - [corpus] Related work on LLM Theory of Mind (MindGames paper) suggests LLMs display ToM abilities, though this remains contested.
- Break condition: If the model cannot accurately infer user state from context, or if users have highly idiosyncratic backgrounds, explanation quality degrades to generic responses.

### Mechanism 2
- Claim: Proactive assistance (unsolicited clarification, suggesting exploration areas) accelerates understanding beyond reactive Q&A.
- Mechanism: Rather than waiting for direct queries, the model takes initiative based on inferred user needs—offering clarifications before confusion manifests, suggesting related concepts, or adapting strategy mid-conversation.
- Core assumption: Proactive behavior is more effective than reactive explanation for complex or counter-intuitive concepts.
- Evidence anchors:
  - [abstract] "proactively assists human understanding... beyond just knowing how to talk"
  - [section 2.1] "Proactive Assistance: The model takes initiative in the explanatory process, not merely responding to direct queries but potentially offering unsolicited clarifications."
  - [corpus] No direct corpus evidence on proactive vs. reactive interpretability effectiveness; this is a gap.
- Break condition: If proactivity is mis calibrated (explaining too much or targeting wrong gaps), users experience cognitive overload or frustration.

### Mechanism 3
- Claim: Interactive probing during "model surgery" (intervening on internals while conversing) can expose deceptive intent through coherence stress.
- Mechanism: When internal states are manipulated while the model explains its behavior, any deceptive model must reconcile external explanations with contradictory internal evidence. Extended dialogue creates multiple opportunities for inconsistencies to emerge.
- Core assumption: Deceptive models will struggle to maintain coherent explanations under simultaneous internal manipulation and external questioning.
- Evidence anchors:
  - [section 2.2.3] "A model whose internal states are being directly manipulated and inspected, yet which is simultaneously engaged in an explanatory dialogue, faces a stringent test of coherence."
  - [section 3.1] Notes agentic interpretability "may sacrifice completeness" with deceptive models—inspective methods may be better for high-stakes safety.
  - [corpus] Related work on power-seeking agents (Alex Turner et al.) discusses instrumental convergence but not this specific mechanism.
- Break condition: If deceptive models can maintain coherent facades under pressure, or if internal manipulations don't surface contradictions, the mechanism provides false confidence.

## Foundational Learning

- **Mental Models (Cognitive Science)**
  - Why needed here: The entire framework hinges on mutual mental model construction. Understanding what mental models are (internal representations of external reality, per Johnson-Laird) and how they form is prerequisite.
  - Quick check question: Can you explain how a teacher's mental model of a student differs from the student's mental model of the subject matter?

- **Zone of Proximal Development (Vygotsky)**
  - Why needed here: The paper explicitly invokes ZPD to explain how agentic interpretability can teach superhuman concepts—learning is maximized at the edge of current capability with guidance.
  - Quick check question: If a chess grandmaster struggles with positional play but understands sacrifices, what zone should an agentic tutor target?

- **Inspective vs. Agentic Paradigm Distinction**
  - Why needed here: The paper positions agentic interpretability as complementary to, not replacing, traditional inspective methods (mechanistic interpretability, probing). Understanding this distinction is essential for knowing when to apply which approach.
  - Quick check question: For auditing a model for hidden deception objectives, which paradigm is more appropriate and why?

## Architecture Onboarding

- **Component map:**
  - User State Tracker -> Explanation Generator -> Proactivity Controller -> Conversation Manager -> (Optional) Internal Intervention Interface

- **Critical path:** User query → User State Tracker updates mental model → Proactivity Controller decides response strategy → Explanation Generator produces output → Conversation Manager logs turn → iterate. For "case learn" scenarios, add evaluation step testing user prediction accuracy on new examples.

- **Design tradeoffs:**
  - Completeness vs. Interactivity: Agentic methods may miss behaviors (especially deceptive ones) that inspective methods catch, but gain adaptability.
  - Automation vs. Human-entanglement: Fully automated evaluation is difficult; human-in-loop is required but expensive and hard to replicate.
  - Explicit vs. Implicit user modeling: Explicit (knowledge graphs) provides transparency but is harder to maintain; implicit (context) is easier but less inspectable.

- **Failure signatures:**
  - Generic explanations regardless of user context → mental model tracking failed
  - Over-explanation / cognitive overload → proactivity controller mis calibrated
  - User prediction accuracy doesn't improve after dialogue → concept transfer failed
  - High variance across users with same system → insufficient adaptation or LLM variance dominating

- **First 3 experiments:**
  1. **Proxy evaluation with LLM-as-user**: Simulate human turns using an LLM to test whether agentic systems improve concept prediction accuracy vs. baseline Q&A, as suggested in Section 5.3.
  2. **Ablation on proactivity**: Compare conditions where proactive assistance is enabled vs. disabled to isolate its contribution to learning speed and prediction accuracy.
  3. **Single-concept teaching loop**: Pick a concrete machine concept (e.g., model's notion of "good response"), run humans through an agentic dialogue, then test their prediction accuracy on held-out examples (case learn evaluation per Section 5.2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can agentic interpretability be rigorously evaluated when the human is "entangled-in-the-loop," making reproducibility and automated comparison difficult?
- Basis in paper: [explicit] Section 4.1 states that "human-entangled-in-the-loop" evaluation complicates "achieving reproducibility, conducting controlled comparisons, and isolating the impact of specific variables."
- Why unresolved: While the paper suggests using LLMs as proxies (Section 5.3), it concludes that end-task metrics remain critical and the evaluation is "nuanced and expensive."
- What evidence would resolve it: The development and validation of automated proxy metrics (using LLMs to simulate users) that correlate strongly with real human understanding and performance on end-tasks.

### Open Question 2
- Question: Can "agentic mechanistic interpretability" (interactive probing with internal interventions) reliably expose deceptive intent, or does the method's focus on interactivity inherently sacrifice the completeness required for safety?
- Basis in paper: [inferred] Section 3.1 admits agentic interpretability "may sacrifice completeness" regarding deceptive models, but Section 2.2.3 hypothesizes that "open-model surgery" could create "demanding" coherence tests for deceptive models.
- Why unresolved: The paper presents these as competing tensions (usefulness vs. completeness) without empirical evidence showing that the proposed "interrogation" methods actually succeed in catching sophisticated deception.
- What evidence would resolve it: Experiments demonstrating that multi-turn agentic probing reveals inconsistencies in models specifically trained to be deceptive, which inspective methods might miss or vice versa.

### Open Question 3
- Question: How can researchers computationally "hill-climb" and optimize agentic interpretability methods without access to cheap, functionally-grounded evaluation metrics?
- Basis in paper: [explicit] Section 4.2 notes it is "Difficult to hill-climb for computational efficiency" because "an agentic method is difficult to establish without human interaction."
- Why unresolved: Optimization typically requires automated feedback, but the high variance in human responses and the necessity of human interaction create a bottleneck that prevents standard efficiency improvements.
- What evidence would resolve it: A methodology for optimizing agentic systems that bypasses the need for constant human feedback, perhaps via reliable user simulators or intrinsic reward signals.

### Open Question 4
- Question: To what extent can an LLM accurately infer a human user's "Zone of Proximal Development" (ZPD) to teach superhuman concepts effectively?
- Basis in paper: [inferred] Section 2.2.2 posits that an agentic LLM "may be able to identify this ZPD" to maximize learning, but this relies on the unproven capability of the model to maintain a precise mental model of the user's knowledge gaps regarding superhuman topics.
- Why unresolved: The paper provides hypothetical examples (e.g., AlphaZero teaching chess) but acknowledges that "much knowledge might be beyond the capacity of an individual," making the identification of the correct learning frontier an open challenge.
- What evidence would resolve it: User studies showing that ZPD-aware agentic dialogue results in significantly faster learning curves or higher final proficiency in complex tasks compared to non-agentic or static explanations.

## Limitations

- The framework remains highly conceptual with significant operationalization gaps, particularly around mental model tracking implementation
- Evaluation methodology is acknowledged as problematic ("human-entangled-in-the-loop") with no clear path to scalable or replicable assessment
- The paper doesn't address how to handle deceptive models that might manipulate their own explanations

## Confidence

- **High confidence**: The distinction between inspective and agentic interpretability paradigms is well-established and the complementarity claim is reasonable. The conceptual value of mutual mental model construction for understanding complex systems is sound.
- **Medium confidence**: The mechanisms of proactive assistance and multi-turn dialogue improving understanding are plausible but lack empirical validation. The invocation of ZPD and claims about discovering superhuman concepts are theoretically grounded but untested.
- **Low confidence**: The effectiveness of agentic interpretability for discovering machine-originated concepts that humans cannot generate independently. The claim that multi-turn dialogue with internal manipulation can expose deception is speculative without empirical demonstration.

## Next Checks

1. **Implement a minimal working prototype** with explicit user state tracking (simple JSON-based knowledge graph) and test whether explanation quality improves compared to baseline Q&A on a concrete concept (e.g., "when would this model refuse an input?").

2. **Conduct a controlled ablation study** comparing proactive vs. reactive conditions on learning speed and prediction accuracy, using LLM-as-proxy-human to reduce variance during method development.

3. **Test the deception-detection claim** by creating a scenario where a model's internal state is manipulated while it explains its behavior, then measure whether multi-turn dialogue reveals inconsistencies that single-turn inspection misses.