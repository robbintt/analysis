---
ver: rpa2
title: 'SeMe: Training-Free Language Model Merging via Semantic Alignment'
arxiv_id: '2505.20144'
source_url: https://arxiv.org/abs/2505.20144
tags:
- semantic
- merging
- latent
- space
- semantics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of efficiently combining the strengths
  of multiple language models without expensive retraining or access to original training
  data. The authors propose SeMe (Semantic-based Merging), a novel training-free approach
  that leverages latent semantic alignment to merge models at a fine-grained, layer-wise
  level.
---

# SeMe: Training-Free Language Model Merging via Semantic Alignment

## Quick Facts
- **arXiv ID:** 2505.20144
- **Source URL:** https://arxiv.org/abs/2505.20144
- **Reference count:** 1
- **Primary result:** Training-free model merging via semantic alignment outperforms existing methods without retraining or original data

## Executive Summary
This paper introduces SeMe (Semantic-based Merging), a novel approach for combining multiple language models without expensive retraining or access to original training data. The method leverages latent semantic alignment to merge models at a fine-grained, layer-wise level, explicitly stabilizing internal knowledge by aligning semantic representations across models. Unlike prior methods focused on preserving prediction behaviors, SeMe computes semantic bases in the latent space for each model's vocabulary and transforms representations between models to preserve semantic meaning during merging.

Extensive experiments across diverse architectures and tasks demonstrate that SeMe outperforms existing methods in both performance and efficiency while eliminating reliance on external data. The approach establishes a new paradigm for knowledge-aware model merging and provides insights into the semantic structure of language models, enabling more scalable and interpretable model composition.

## Method Summary
SeMe operates through a three-step pipeline: Select (identify high-variance fusion vector components via top τ%), Compute (merge coefficients via squared magnitudes), and Erase (discard minority-sign entries). The core method involves computing semantic bases as s = e · W⁺ (pseudoinverse of LM-head matrix) for each model's vocabulary, then applying a semantics-preservative transformation that computes cosine similarities between representations and semantic bases, followed by reconstruction via weighted combination on target semantic bases. The final merge combines the pivot model with weighted deltas from source models: θ_merge = θ_pivot + Σ η_k · δ'_k. The approach claims to be data-free and training-free, using only model vocabularies and LM-head matrices to compute semantic bases internally.

## Key Results
- Achieves superior performance compared to existing model merging methods across diverse architectures and tasks
- Eliminates need for external data sampling or additional training typically required in model merging
- Establishes a new paradigm for knowledge-aware model merging with improved scalability and interpretability

## Why This Works (Mechanism)
The method works by explicitly aligning the semantic representations across different models at the latent space level. By computing semantic bases for each model's vocabulary and transforming representations between models, SeMe preserves the internal knowledge structure during merging rather than just matching prediction behaviors. This semantic alignment ensures that the merged model maintains coherent meaning representations across the combined knowledge from multiple sources, stabilizing the internal representations that encode linguistic understanding.

## Foundational Learning
- **Semantic bases computation**: Computing s = e · W⁺ (pseudoinverse of LM-head matrix) to extract vocabulary-level semantic representations - needed to establish common semantic space for alignment; quick check: verify basis norms are stable across different vocabulary sizes
- **Cosine similarity alignment**: Using cosine similarity between representations and semantic bases for cross-model transformation - needed to measure semantic similarity in high-dimensional space; quick check: confirm similarity scores are bounded between -1 and 1
- **Layer-wise interpolation**: Handling architectures with different depths through linear interpolation of layer representations - needed to align models with mismatched architectures; quick check: verify interpolated representations maintain semantic coherence
- **Variance-based selection**: Identifying high-variance fusion components via top τ% threshold - needed to focus merging on most informative parameters; quick check: experiment with different τ values (10%, 20%, 30%) to find optimal variance cutoff
- **Pseudoinverse stability**: Ensuring numerical stability when computing W⁺ for large vocabularies - needed to prevent semantic basis computation from becoming unstable; quick check: monitor condition number of W matrix
- **Vocabulary mapping**: Handling token vocabulary differences between models during semantic alignment - needed for merging models with different tokenizers; quick check: measure vocabulary overlap ratio and its impact on merge quality

## Architecture Onboarding

**Component Map:**
Vocabulary Extraction -> Semantic Basis Computation (W⁺) -> Layer-wise Alignment -> Fusion Vector Processing (Select-Compute-Erase) -> Weighted Delta Combination

**Critical Path:**
Semantic Basis Computation → Layer-wise Alignment → Weighted Delta Combination

**Design Tradeoffs:**
The method trades computational simplicity (training-free, data-free) for potential precision loss in semantic alignment when vocabularies differ significantly. The variance-based selection (τ%) balances between capturing important features and avoiding noise amplification.

**Failure Signatures:**
- Semantic basis computation instability (exploding or NaN values in W⁺)
- Vocabulary mismatch causing poor semantic alignment (low cosine similarity scores)
- Layer interpolation artifacts when models have very different depths
- Over-aggressive τ% threshold leading to loss of important model knowledge

**3 First Experiments:**
1. Merge two models with identical architectures and vocabularies, varying τ% threshold to observe impact on merge quality
2. Test semantic basis stability by computing bases multiple times with different random seeds
3. Merge models with different token vocabularies (50% overlap) to identify failure modes in vocabulary mapping

## Open Questions the Paper Calls Out
None specified in the provided text.

## Limitations
- The exact implementation details and hyperparameters (particularly τ thresholds and coefficient computation methods) are not fully specified in the paper
- Claims about working across "diverse architectures" lack specific compatibility requirements between models
- The data-free nature may limit validation capabilities for hyperparameter tuning
- Semantic alignment quality may degrade when vocabularies differ significantly between models

## Confidence
- **High confidence**: The core concept of semantic alignment for model merging is technically sound and well-motivated
- **Medium confidence**: Experimental results showing performance improvements over existing methods are plausible but specific benchmark numbers cannot be verified
- **Low confidence**: Claims about eliminating all data requirements and achieving truly training-free merging require scrutiny, as semantic alignment typically needs some form of validation data for hyperparameter tuning

## Next Checks
1. Verify semantic basis stability by testing consistency across different vocabulary sizes and monitoring pseudoinverse computation stability for large language models
2. Validate cross-model alignment by experimenting with merging models having different token vocabularies and layer counts to identify failure points
3. Benchmark against baselines by implementing minimal parameter averaging and comparing against SeMe on standard merging tasks to verify claimed performance improvements