---
ver: rpa2
title: 'Synheart Emotion: Privacy-Preserving On-Device Emotion Recognition from Biosignals'
arxiv_id: '2511.06231'
source_url: https://arxiv.org/abs/2511.06231
tags:
- wrist
- stress
- emotion
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Synheart Emotion, an on-device emotion recognition
  system that processes wrist-based photoplethysmography (PPG) signals to classify
  emotional states without cloud transmission. The system extracts five time-domain
  heart rate variability (HRV) features from 120-second windows and compares classical
  machine learning models, deep neural networks, and transformers on the WESAD dataset.
---

# Synheart Emotion: Privacy-Preserving On-Device Emotion Recognition from Biosignals

## Quick Facts
- **arXiv ID**: 2511.06231
- **Source URL**: https://arxiv.org/abs/2511.06231
- **Reference count**: 40
- **Primary result**: Classical ensemble methods (ExtraTrees F1=0.826) significantly outperform deep learning on small physiological datasets for on-device emotion recognition

## Executive Summary
Synheart Emotion presents an on-device emotion recognition system that processes wrist-based photoplethysmography (PPG) signals to classify emotional states without cloud transmission. The system extracts five time-domain heart rate variability (HRV) features from 120-second windows and compares classical machine learning models, deep neural networks, and transformers on the WESAD dataset. Results show classical ensemble methods significantly outperform deep learning on small physiological datasets, with ExtraTrees achieving F1 = 0.826 on combined features and F1 = 0.623 on wrist-only features, while transformers achieve only F1 = 0.509-0.577. The deployed ExtraTrees model optimized via ONNX achieves 4.08 MB footprint, 0.05 ms inference latency, and 152x speedup over the original implementation. The system addresses privacy vulnerabilities and latency constraints of cloud-based emotion recognition while demonstrating that feature engineering combined with classical ML provides superior accuracy-efficiency trade-offs for real-time wearable applications.

## Method Summary
The system processes wrist-based PPG signals at 64 Hz from the WESAD dataset, applying bandpass filtering (0.5-8 Hz) and adaptive peak detection to extract inter-beat intervals. Five time-domain HRV features (SDNN, RMSSD, pNN50, Mean RR, Mean HR) are computed from 120-second windows with 50% overlap, then z-score normalized using training set statistics. The system compares classical machine learning models (Linear SVM, Logistic Regression, Random Forest, ExtraTrees, XGBoost) with deep neural networks (MLP variants, transformers) using 80/20 and 64/16/20 train/validation/test splits. ExtraTrees (200 estimators) is deployed via ONNX conversion, achieving 4.08 MB model size and 0.05 ms inference latency with 152x speedup over scikit-learn implementation.

## Key Results
- ExtraTrees achieves F1 = 0.826 on combined sensor features, significantly outperforming deep learning models (transformer F1 = 0.509-0.577)
- Wrist-only feature extraction reduces F1 to 0.623 but enables consumer wearable compatibility without chest sensors
- ONNX conversion provides 152x speedup for tree ensembles (7.27ms → 0.05ms) with minimal accuracy loss
- The system successfully addresses privacy concerns by processing all physiological data locally without cloud transmission

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Time-domain HRV features encode autonomic nervous system state, enabling emotion classification without frequency-domain transforms
- **Mechanism**: Five features (SDNN, RMSSD, pNN50, Mean RR, Mean HR) computed from 120-second windows capture sympathetic/parasympathetic balance. Stress reduces parasympathetic activity (lower RMSSD), while relaxation increases it. The multi-feature combination provides 59% F1 improvement over single-feature baselines
- **Core assumption**: Emotional states modulate ANS activity in consistent, detectable patterns across individuals
- **Evidence anchors**: [abstract] "extracts five time-domain heart rate variability (HRV) features from 120-second windows"; [Section 3.2] Equations 1-3 define SDNN, RMSSD, pNN50 computations from RR intervals; [corpus] No direct corpus evidence on HRV-emotion mechanisms; weak external validation
- **Break condition**: If individual HRV baselines vary too much (genetics, fitness), population-level models fail without personalization

### Mechanism 2
- **Claim**: Classical ensemble methods outperform deep learning on small physiological datasets (n=488) through better inductive biases and natural regularization
- **Mechanism**: Tree ensembles handle irregular decision boundaries and noisy physiological signals without requiring smooth hierarchical representations. Manual HRV features encode domain knowledge that end-to-end learning cannot rediscover with limited samples. Ensemble averaging regularizes via tree depth and voting
- **Core assumption**: The performance gap persists because deep learning requires >10K samples for transformers, not because of architectural limitations
- **Evidence anchors**: [abstract] "classical ensemble methods significantly outperform deep learning on small physiological datasets"; [Section 9.1] "manual HRV features encode decades of domain knowledge that end-to-end learning cannot rediscover with 488 samples"; [corpus] Grinsztajn et al. [34] cited: "tree-based models systematically outperform deep learning on medium-sized tabular datasets"
- **Break condition**: If dataset size increases substantially (>5000 samples), transformer performance may approach or exceed classical methods

### Mechanism 3
- **Claim**: ONNX conversion provides 152x speedup for tree ensembles by optimizing inference graph for target hardware
- **Mechanism**: ONNX Runtime applies operator fusion, constant folding, and hardware-specific optimizations (Metal Performance Shaders on iOS). Tree ensembles benefit most because scikit-learn's Python implementation has high per-prediction overhead compared to vectorized ONNX execution
- **Core assumption**: The speedup comes from ONNX optimization, not simply moving from Python to C++ backend
- **Evidence anchors**: [abstract] "ONNX achieves 4.08 MB footprint, 0.05 ms inference latency, and 152x speedup"; [Section 6.4] "ExtraTrees: 7.27ms → 0.05ms, Random Forest: 7.17ms → 0.05ms"; [corpus] WhisperKit paper demonstrates on-device transformer optimization via ONNX; supports feasibility claim
- **Break condition**: If model complexity increases dramatically, ONNX overhead may reduce speedup gains

## Foundational Learning

- **Concept**: Heart Rate Variability (HRV) metrics
  - **Why needed here**: Understanding what SDNN, RMSSD, pNN50 measure is essential for debugging feature extraction and interpreting model behavior
  - **Quick check question**: Why would RMSSD decrease during stress but SDNN might not show the same pattern?

- **Concept**: Macro F1-score for imbalanced classification
  - **Why needed here**: WESAD has 53.5% Baseline, 29.7% Stress, 16.8% Amusement; accuracy would mask poor minority-class performance
  - **Quick check question**: If a model achieves 70% accuracy by always predicting "Baseline," what would its macro F1 be?

- **Concept**: ONNX Runtime and model optimization
  - **Why needed here**: Converting scikit-learn/PyTorch models to ONNX is the deployment pathway; understanding graph optimization helps debug inference issues
  - **Quick check question**: Why do tree ensembles see larger speedups (152x) than neural networks (4.8x) from ONNX conversion?

## Architecture Onboarding

- **Component map**: PPG sensor (64 Hz) → bandpass filter (0.5-8 Hz) → peak detection → IBI extraction → 5 HRV features → z-score normalization → ExtraTrees ONNX model → emotion label + confidence

- **Critical path**: PPG peak detection quality directly determines HRV feature accuracy. Motion artifacts cause false peaks → implausible IBIs → corrupted features → misclassification. The artifact rejection thresholds (<300ms or >2000ms IBI) are the first line of defense

- **Design tradeoffs**:
  - ExtraTrees (deployed) vs XGBoost (best F1): ExtraTrees chosen for 152x speedup and robustness despite 0.062 lower F1
  - WRIST ALL vs COMBINED: 17% F1 reduction (0.826→0.685) accepted for consumer wearable compatibility (no chest sensors)
  - 120-second windows vs shorter: Longer windows improve statistical reliability but reduce temporal resolution for rapid emotion changes

- **Failure signatures**:
  - High Amusement→Stress confusion (29%): HRV captures arousal not valence; both states show sympathetic activation
  - Low Amusement recall (29%): Class imbalance (16.8% of dataset) causes model to default to majority classes
  - Cross-subject generalization: Individual HRV baselines vary 2-3x; population model may misclassify users with atypical baselines

- **First 3 experiments**:
  1. Reproduce WESAD preprocessing: Download dataset, run NeuroKit2 pipeline, verify 488 samples with correct class distribution (261/145/82)
  2. Validate feature extraction: Generate synthetic RR intervals with known HRV properties, confirm computed SDNN/RMSSD match expected values within 5%
  3. ONNX conversion sanity check: Convert trained ExtraTrees model, verify predictions match scikit-learn output exactly (numerical equivalence test)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do transformer models outperform classical ensembles on HRV emotion recognition when scaled to datasets larger than 1,000 samples?
- **Basis in paper**: [explicit] Section 10.1 limits conclusions to the small dataset (488 samples), suggesting future validation on larger corpora like DEAP
- **Why unresolved**: Transformers achieved only F1 0.509 here, likely due to data scarcity, but their scaling laws suggest potential superiority given sufficient training data
- **What evidence would resolve it**: Comparative benchmarks on larger datasets (DEAP, MAHNOB-HCI) showing transformer F1 scores exceeding the 0.826 ceiling of ExtraTrees

### Open Question 2
- **Question**: Can fusing wrist-based Electrodermal Activity (EDA) with HRV resolve the confusion between Stress and Amusement?
- **Basis in paper**: [explicit] Section 9.2 and 10.1 highlight that single-modality HRV captures arousal but not valence, leading to 29% confusion between high-arousal states
- **Why unresolved**: The system currently classifies based on arousal alone, lacking the physiological signals needed to distinguish positive from negative high-arousal
- **What evidence would resolve it**: A multimodal model showing a significant decrease in Amusement→Stress misclassification rates compared to the HRV-only baseline

### Open Question 3
- **Question**: What level of performance degradation occurs when the ExtraTrees model is applied to real-world free-living data with uncontrolled motion artifacts?
- **Basis in paper**: [explicit] Section 10.1 notes the reliance on the controlled WESAD protocol and anticipates "Real-world degradation from motion artifacts"
- **Why unresolved**: The current F1 scores are derived from clean, protocol-driven lab data and may not reflect accuracy in noisy daily environments
- **What evidence would resolve it**: A deployment study measuring F1 scores on consumer smartwatches during daily activities with concurrent self-report ground truth

## Limitations
- Generalizability across diverse populations is limited due to small sample size (12 subjects) and laboratory conditions
- 120-second window length trades temporal resolution for statistical reliability, potentially missing rapid emotion transitions
- Cross-subject evaluation shows performance degradation (F1 dropping to 0.550-0.662), indicating limited generalization to unseen individuals

## Confidence
- **High confidence**: Technical implementation details (PPG preprocessing, HRV feature extraction, ONNX conversion) are well-specified and reproducible
- **Medium confidence**: Superiority of ExtraTrees over other classical methods may be influenced by dataset-specific characteristics and incomplete hyperparameter details
- **Low confidence**: Cross-subject generalization claims based on limited data may not hold for broader populations with different demographics and health conditions

## Next Checks
1. **Cross-population validation**: Test the deployed ExtraTrees model on an independent dataset with different demographic characteristics to assess real-world generalization performance
2. **Temporal resolution analysis**: Evaluate emotion classification performance using shorter window lengths (30s, 60s) to determine minimum viable window size maintaining acceptable accuracy
3. **Personalization impact study**: Implement subject-specific baseline calibration by retraining or fine-tuning the model with 5-10 personalized samples and measure improvement in F1-score compared to population model