---
ver: rpa2
title: 'RAPTOR: Ridge-Adaptive Logistic Probes'
arxiv_id: '2602.00158'
source_url: https://arxiv.org/abs/2602.00158
tags:
- raptor
- steering
- accuracy
- concept
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "RAPTOR introduces ridge-regularized logistic probes for concept\
  \ extraction in probe-then-steer pipelines. By selecting a validation-tuned ridge\
  \ strength \u03BB, RAPTOR produces concept vectors that balance accuracy, directional\
  \ stability, and low computational cost."
---

# RAPTOR: Ridge-Adaptive Logistic Probes

## Quick Facts
- arXiv ID: 2602.00158
- Source URL: https://arxiv.org/abs/2602.00158
- Authors: Ziqi Gao; Yaotian Zhu; Qingcheng Zeng; Xu Zhao; Ziqing Wang; Feng Ruan; Kaize Ding
- Reference count: 40
- Primary result: Introduces ridge-regularized logistic probes achieving high accuracy (87.4%) and stability in concept extraction across 42 model-dataset settings.

## Executive Summary
RAPTOR introduces ridge-regularized logistic probes for concept extraction in probe-then-steer pipelines. By selecting a validation-tuned ridge strength λ, RAPTOR produces concept vectors that balance accuracy, directional stability, and low computational cost. Across 42 model-dataset settings, RAPTOR matches or exceeds strong baselines in accuracy while achieving greater directional stability under 20% data ablation and faster training.

## Method Summary
RAPTOR uses ridge-regularized logistic regression to extract concept vectors from model embeddings, addressing instability issues in standard unregularized probes. The method selects optimal regularization strength λ via validation, balancing accuracy against overfitting. This approach produces concept vectors that are more stable under data perturbations while maintaining competitive or superior accuracy compared to existing probe methods. RAPTOR is specifically designed for probe-then-steer pipelines where concept stability is critical for reliable steering interventions.

## Key Results
- Achieved 87.4% accuracy on best-layer predictions, outperforming xRFM by +0.29
- Demonstrated superior directional stability under 20% data ablation
- Showed faster training compared to iterative baseline methods
- Validated theoretical predictions linking λ to signal-to-orthogonal energy ratio

## Why This Works (Mechanism)
RAPTOR works by introducing ridge regularization to logistic probes, which controls the trade-off between fitting the training signal and maintaining stability in the orthogonal subspace. The regularization strength λ acts as a knob that balances accuracy (high when λ is small) against directional stability (high when λ is large). High-dimensional theory predicts that optimal λ mediates this trade-off by balancing signal-to-orthogonal energy ratio, where the orthogonal subspace energy scales with embedding dimension. This mechanism explains why RAPTOR achieves both high accuracy and stability simultaneously.

## Foundational Learning

### Ridge Regularization
- **Why needed**: Controls overfitting in high-dimensional probe spaces while maintaining generalization
- **Quick check**: Compare validation accuracy vs training accuracy curves across different λ values

### Logistic Regression Probes
- **Why needed**: Provides interpretable linear mappings from embeddings to concept probabilities
- **Quick check**: Verify probe weights correlate with concept-relevant embedding directions

### High-dimensional Signal Theory
- **Why needed**: Predicts how regularization affects signal recovery in large embedding spaces
- **Quick check**: Measure signal-to-noise ratio changes as embedding dimension varies

## Architecture Onboarding

### Component Map
Embeddings -> Ridge-Regularized Logistic Probe -> Concept Vectors -> Steering Interventions

### Critical Path
1. Extract embeddings from target layer
2. Apply ridge-regularized logistic regression with validation-tuned λ
3. Generate stable concept vectors for steering

### Design Tradeoffs
- Accuracy vs stability: Smaller λ improves accuracy but reduces stability
- Computational cost vs regularization: Ridge regularization adds minimal overhead while providing significant stability gains
- Interpretability vs performance: Linear probes sacrifice some nonlinear expressiveness for transparency

### Failure Signatures
- Overfitting: Large gap between training and validation accuracy
- Underregularization: High sensitivity to data perturbations
- Poor λ selection: Suboptimal balance between accuracy and stability

### First Experiments
1. Ablation study: Remove 20% of training data and measure concept vector stability
2. λ sweep: Vary regularization strength and plot accuracy vs stability trade-off
3. Steering intervention: Apply concept vectors to control model outputs and measure intervention success rate

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical mechanism linking λ to signal-to-orthogonal energy ratio lacks comprehensive empirical validation across diverse architectures
- Steering performance claims rely on a single heavy-tailed cost model that may not generalize to all intervention strategies
- O(d) scaling assumption for orthogonal energy needs rigorous testing in extreme dimensional regimes

## Confidence

| Claim | Confidence |
|-------|------------|
| RAPTOR's accuracy improvements (87.4% best-layer, +0.29 over xRFM) | High |
| Directional stability under data ablation | Medium |
| Training speed advantage | High |
| Theoretical λ-accuracy-stability tradeoff | Low-Medium |

## Next Checks
1. **Cross-architecture generalization**: Test RAPTOR's λ selection and stability guarantees on non-transformer architectures (RNNs, MLPs) and on models trained with different objectives (masked LM, prefix LM) to verify theoretical predictions hold beyond BERT and GPT-style models.

2. **Intervention cost sensitivity**: Evaluate steering performance under multiple cost models (linear, quadratic, capped) and intervention strategies (targeted activation, rank-one updates) to confirm robustness of "near-perfect control" claims beyond the reported heavy-tailed setting.

3. **Scalability to extreme dimensions**: Conduct experiments with ultra-high dimensional embeddings (d > 10,000) and sparse feature spaces to test whether the O(d) orthogonal energy scaling and λ selection remain valid, or if new phenomena emerge that require theoretical updates.