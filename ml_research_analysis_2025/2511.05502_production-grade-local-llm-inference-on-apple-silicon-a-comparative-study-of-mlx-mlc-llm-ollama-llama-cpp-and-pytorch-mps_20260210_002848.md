---
ver: rpa2
title: 'Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study
  of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS'
arxiv_id: '2511.05502'
source_url: https://arxiv.org/abs/2511.05502
tags:
- apple
- mlc-llm
- ollama
- throughput
- ttft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a systematic, empirical evaluation of five
  local large language model (LLM) runtimes on Apple Silicon: MLX, MLC-LLM, llama.cpp,
  Ollama, and PyTorch MPS. Experiments were conducted on a Mac Studio equipped with
  an M2 Ultra processor and 192 GB of unified memory.'
---

# Production-Grade Local LLM Inference on Apple Silicon: A Comparative Study of MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS

## Quick Facts
- arXiv ID: 2511.05502
- Source URL: https://arxiv.org/abs/2511.05502
- Reference count: 0
- Primary result: MLX achieves highest sustained throughput (~230 tokens/sec) on Apple Silicon, while MLC-LLM delivers lowest TTFT for moderate prompts through paged KV caching.

## Executive Summary
This paper presents a systematic empirical evaluation of five local LLM runtimes on Apple Silicon hardware, focusing on production deployment scenarios. Using a Mac Studio M2 Ultra with 192GB unified memory, the authors benchmark MLX, MLC-LLM, Ollama, llama.cpp, and PyTorch MPS across throughput, latency, and long-context handling metrics. The analysis reveals clear trade-offs: MLX excels in sustained throughput through Metal optimization, MLC-LLM offers superior interactive responsiveness with paged KV caching, while PyTorch MPS is fundamentally limited by memory constraints. The study provides evidence-based recommendations for Apple-first deployments, emphasizing that framework choice should align with specific workload requirements.

## Method Summary
The authors conducted comprehensive benchmarking on a Mac Studio M2 Ultra with 24 CPU cores, 76 GPU cores, and 192GB unified memory. Five frameworks were evaluated: MLX v0.26, MLC-LLM commit 3d42929, llama.cpp commit b5963, Ollama v0.10.1, and PyTorch 2.7.1 MPS. The Qwen-2.5 model family (3B and 7B variants) was tested across quantization formats including FP16/bf16, int8, int4, GGUF, AWQ, and GPTQ. Prompts ranged from hundreds to 100,000 tokens in three categories: unique-token, prefix-heavy, and code-dominant. Measurements included time-to-first-token, sustained throughput, latency percentiles, memory usage, and long-context behavior with different KV cache strategies.

## Key Results
- MLX achieves highest sustained generation throughput at approximately 230 tokens/second
- MLC-LLM delivers lowest TTFT for moderate prompts (≤16k tokens) through paged KV caching
- PyTorch MPS is fundamentally constrained by 4GB tensor cap, peaking at 7-9 tokens/second
- MLX maintains >90% GPU utilization with minimal CPU overhead during sustained generation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** MLX achieves highest sustained throughput (~230 tokens/sec) through native Metal integration maintaining high GPU utilization
- **Mechanism:** MLX's Metal kernels consistently saturate GPU utilization (>90%) with minimal CPU overhead (<3%), enabled by tight integration with Apple's Metal/Neural Engine stack and unified memory architecture. Once prefill completes, decoding exhibits stable 5–7ms median inter-token latency.
- **Core assumption:** Model weights and KV cache fit within unified memory; Metal kernels are properly compiled and cached; no competing GPU workloads or thermal throttling.

### Mechanism 2
- **Claim:** MLC-LLM delivers lower TTFT for moderate prompts (≤16k tokens) through paged KV caching and TVM-compiled kernels optimized for small micro-batches
- **Mechanism:** MLC implements vLLM-style paged attention, partitioning KV tensors into reusable blocks. This reduces head-of-line blocking and enables efficient memory reuse across 32k–128k tokens. TVM compilation produces device-tuned kernels that handle light concurrency more gracefully than MLX.
- **Core assumption:** Requests exhibit prefix reuse or temporal locality; one-time TVM compilation overhead (~20–30 min) is acceptable.

### Mechanism 3
- **Claim:** PyTorch MPS is fundamentally unsuitable for production LLM inference due to the 4GB tensor cap and memory pressure limitations
- **Mechanism:** PyTorch MPS exposes Metal through PyTorch's backend but enforces a 4GB per-tensor limit. For 3B+ parameter models or contexts beyond ~2k tokens, this causes frequent OOM errors. Additionally, INT4/INT8 quantization support is experimental and bf16 is inconsistent.

## Foundational Learning

- **Concept: Time-to-First-Token (TTFT)**
  - **Why needed here:** TTFT determines user-perceived responsiveness in interactive chat. The paper explicitly measures TTFT as "wall-clock time from client request acceptance to the first token becoming available to the client."
  - **Quick check question:** If your use case requires streaming chat responses, which framework should you prioritize and why?

- **Concept: KV Caching (Rotating vs. Paged)**
  - **Why needed here:** Long-context handling depends critically on KV cache design. MLX uses rotating windows; MLC-LLM uses paged attention. The paper shows paged KV sustains throughput at 64k–128k tokens while rotating windows prevent unbounded growth.
  - **Quick check question:** For a document-summarization workload with 100k-token inputs, which cache strategy would you expect to perform better?

- **Concept: Quantization Formats (GGUF, AWQ, GPTQ, Mixed-bit)**
  - **Why needed here:** Quantization reduces memory footprint 2–3×, enabling larger models or longer contexts. Different frameworks support different formats; MLX emphasizes Apple-tuned mixed-bit while MLC supports community-standard AWQ/GPTQ.
  - **Quick check question:** Your team has pre-existing GPTQ-quantized models. Which Apple-native framework should you evaluate first?

## Architecture Onboarding

- **Component map:**
  Application Layer -> Framework (MLX/MLC-LLM/Ollama/llama.cpp/PyTorch MPS) -> Metal Backend (GPU) -> Unified Memory (192GB) -> Apple Silicon M2 Ultra

- **Critical path:**
  1. Choose framework based on workload type (throughput vs. interactivity vs. long-context)
  2. Select quantization format matching framework strengths
  3. Configure KV cache strategy appropriate to context length
  4. Add serving wrapper if needed (MLX requires external server; MLC has built-in REST/SSE)

- **Design tradeoffs:**
  | Tradeoff | MLX | MLC-LLM | Ollama | llama.cpp | PyTorch MPS |
  |----------|-----|---------|--------|-----------|-------------|
  | Peak throughput | Highest (~230 tok/s) | High (~190 tok/s) | Low (20–40 tok/s) | Moderate (~150 tok/s) | Very low (7–9 tok/s) |
  | TTFT (≤16k) | Moderate | Lowest | High | Low (short ctx) | High |
  | Long context (64k+) | Limited (rotating KV) | Strong (paged KV) | Poor | Very poor | Fails |
  | API/serving | Requires wrapper | Built-in REST/SSE | Built-in | Community only | Build from scratch |
  | Setup complexity | Low (pip) | Medium (TVM compile) | Lowest (brew) | Medium (CMake) | Low (but impractical) |

- **Failure signatures:**
  - **PyTorch MPS OOM:** Models >3B or contexts >2k tokens fail with 4GB tensor cap
  - **llama.cpp throughput collapse:** Beyond 32k tokens, drops to ~1.2 tok/s (quadratic attention scaling)
  - **Ollama TTFT spike:** >50s at 100k contexts despite smooth streaming after first token
  - **MLX concurrency bottleneck:** Single-request-per-process; requires external load balancing for multi-tenant
  - **MLC-LLM cold start:** 20–30 min TVM compilation on first model load (cached thereafter)

- **First 3 experiments:**
  1. **Baseline throughput test:** Run identical prompts (1k, 10k, 32k tokens) through MLX and MLC-LLM; measure TTFT and sustained tok/s. Compare against paper's ~230 vs ~190 tok/s findings.
  2. **Long-context stress test:** Submit 64k and 100k token prompts to MLX and MLC-LLM; observe memory growth, TTFT scaling, and whether throughput degrades. Verify paged KV advantage.
  3. **Concurrency sweep:** Use synthetic RPS tool (e.g., oha, wrk) to send 1–8 concurrent requests to MLC-LLM's built-in server vs. MLX with mlx-openai-server wrapper. Measure p50/p90/p99 latency and identify head-of-line blocking threshold.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can chunked prefill (as implemented in vLLM) be adapted to Apple Silicon runtimes to reduce TTFT for very long inputs (≥64k tokens)?
- Basis in paper: [explicit] The paper states "Neither MLX nor MLC yet implement chunked prefill as in vLLM, leaving very long inputs (e.g., 100k+) with poor TTFT."
- Why unresolved: Current Apple-native frameworks require full prefill before streaming begins, causing TTFT to grow linearly with input length.
- What evidence would resolve it: Implementation of chunked prefill in MLX or MLC-LLM demonstrating reduced TTFT on 64k–128k token inputs without sacrificing decode throughput.

### Open Question 2
- Question: How do the relative performance rankings of these frameworks change on M1, M3, and M4 Apple Silicon variants with different unified memory configurations?
- Basis in paper: [explicit] The Threats to Validity section states "All experiments were conducted on a single machine class (Mac Studio, M2 Ultra, 192GB). Performance on other Apple SoCs (e.g., M1, M4) may differ."
- Why unresolved: Architectural differences in GPU core counts, memory bandwidth, and Neural Engine capabilities across chip generations may shift framework trade-offs.
- What evidence would resolve it: Replication of the benchmark suite across M1/M3/M4 devices (8GB–128GB configurations) with comparative analysis of throughput and TTFT scaling.

### Open Question 3
- Question: What is the energy efficiency (tokens/joule) of each framework under sustained multi-tenant workloads on Apple Silicon?
- Basis in paper: [inferred] The methodology measures throughput, latency, and memory but does not instrument power consumption or thermal throttling, despite "energy" being mentioned as a discussion point.
- Why unresolved: Production deployments must balance performance with power constraints, especially in edge or laptop scenarios; current data cannot inform energy-aware framework selection.
- What evidence would resolve it: Power telemetry (via `powermetrics` or similar) during sustained concurrent inference, reporting tokens/joule and thermal equilibrium behavior per framework.

## Limitations

- Reproducibility constraints due to missing prompt templates, sampling parameters, and lack of repository link for scripts and data
- Apple-specific ecosystem limits external validity to other platforms or cloud GPU deployments
- Framework-specific caveats regarding model family generalizability beyond Qwen-2.5

## Confidence

**High confidence (⭑⭑⭑⭑⭑):** MLX achieves highest sustained throughput (~230 tokens/sec) and PyTorch MPS is fundamentally limited by 4GB tensor cap
**Medium confidence (⭑⭑⭑⭑):** MLC-LLM delivers lower TTFT for moderate prompts (≤16k tokens) through paged KV caching
**Medium confidence (⭑⭑⭑⭑):** MLX requires external server wrapper for production deployment
**Medium confidence (⭑⭑⭑⭑):** llama.cpp efficiency degrades sharply beyond 32k tokens
**Low confidence (⭑⭑⭑):** Ollama's ergonomics vs. performance trade-off

## Next Checks

1. **Reproduce baseline throughput measurements:** Run identical Qwen-2.5-Coder 3B prompts (1k, 10k, 32k tokens) through MLX and MLC-LLM on matching M2 Ultra hardware. Measure TTFT and sustained throughput, comparing against reported ~230 vs ~190 tok/s values.

2. **Validate long-context behavior with paged vs rotating KV:** Submit 64k and 100k token prompts to both frameworks, measuring memory growth, TTFT scaling, and throughput degradation. Verify the claimed advantage of MLC-LLM's paged KV strategy over MLX's rotating window approach.

3. **Test concurrency scaling under load:** Use a synthetic request generator (e.g., oha or wrk2) to send 1-8 concurrent requests to MLC-LLM's built-in server versus MLX with mlx-openai-server wrapper. Measure p50/p90/p99 latency distributions and identify the point where head-of-line blocking occurs.