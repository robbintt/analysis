---
ver: rpa2
title: 'DermAI: Clinical dermatology acquisition through quality-driven image collection
  for AI classification in mobile'
arxiv_id: '2511.10367'
source_url: https://arxiv.org/abs/2511.10367
tags:
- dermai
- clinical
- acquisition
- image
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DermAI is a smartphone-based application designed to enable standardized
  image acquisition, quality validation, and preliminary classification of skin lesions
  during clinical consultations. It addresses limitations in dermatology AI by enforcing
  controlled capture protocols, performing on-device quality checks, and supporting
  diverse skin tones and devices.
---

# DermAI: Clinical dermatology acquisition through quality-driven image collection for AI classification in mobile

## Quick Facts
- arXiv ID: 2511.10367
- Source URL: https://arxiv.org/abs/2511.10367
- Reference count: 8
- Key outcome: Quality-driven, context-specific image collection improves dermatology AI generalization, with DermAI ensemble models achieving 85.71% F1-score versus 7.90% for public dataset models on DermAI data.

## Executive Summary
DermAI is a smartphone-based application designed to enable standardized image acquisition, quality validation, and preliminary classification of skin lesions during clinical consultations. It addresses limitations in dermatology AI by enforcing controlled capture protocols, performing on-device quality checks, and supporting diverse skin tones and devices. A clinical dataset of 3,401 images across 200 lesions was collected in a Brazilian public hospital. Models trained on public datasets (PAD-UFES-20, DDI) showed limited generalization, particularly in recall, while fine-tuning with DermAI data significantly improved performance. For example, ensemble models trained on DermAI achieved 85.71% F1-score, versus 7.90% for PAD-only models on DermAI data. These results demonstrate the importance of quality-driven, context-specific data collection for robust clinical AI.

## Method Summary
DermAI implements a mobile-first pipeline for standardized skin lesion image acquisition and classification. The system enforces controlled capture conditions (5cm distance, centered framing, no pre-markings) and performs on-device quality validation using a MobileNetV3-based image quality assessor that checks for sharpness, blur, exposure, and compression artifacts. Images failing quality checks trigger recapture prompts. A CNN backbone (evaluated across DenseNet, ConvNeXt, MobileNet v3, EfficientNet v2, ResNet) performs classification, with ensemble strategies (majority voting or learned MLP fusion) combining multiple architectures. The clinical dataset comprises 3,401 images from 200 lesions collected at a Brazilian public hospital, with dermatologist annotations and malignancy flags for biopsy follow-up.

## Key Results
- Cross-dataset evaluation showed PAD-trained models achieving only 7.90% F1-score on DermAI data versus 85.71% for DermAI-trained ensembles.
- Quality filtering PAD-UFES-20 (PAD filter) improved performance despite using fewer samples, highlighting the importance of curated image features over quantity.
- Learned ensemble fusion via MLP outperformed majority voting, particularly for malignant-suspect cases where sensitivity is critical.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Standardized acquisition protocols with real-time quality validation improve cross-dataset generalization for dermatology classification models.
- Mechanism: Controlled capture conditions combined with on-device quality checks reduce domain shift by constraining input variability at collection time.
- Core assumption: Image quality degradation artifacts—not lesion distribution differences—are the primary cause of generalization failure across datasets.
- Evidence anchors:
  - Models trained on public datasets failed to generalize to our samples, while fine-tuning with local data improved performance.
  - PAD-trained models show limited generalization when tested on DDI and DermAI, particularly in recall.
  - PAD contains acquisition artifacts (e.g., ink markings, blur) and limited lesion variety or skin tones.

### Mechanism 2
- Claim: Quality filtering applied to existing datasets can improve model performance even with fewer training samples.
- Mechanism: Applying quality assessment to filter degraded samples removes confounding features that models might otherwise learn as spurious predictors.
- Core assumption: Quality filters successfully identify and remove images where artifacts obscure or distort lesion features.
- Evidence anchors:
  - Despite using fewer samples, the PAD filter improved performance across backbones, highlighting the importance of curated image features.
  - Data variation and bias are documented issues across dermatological ML datasets.

### Mechanism 3
- Claim: Learned ensemble fusion outperforms majority voting by capturing complementary confidence patterns across architectures.
- Mechanism: MLP-based fusion learns correlations between classifier confidence outputs, weighting architectures dynamically per-case.
- Core assumption: Different CNN backbones make meaningfully different error patterns; their confidence distributions contain exploitable information.
- Evidence anchors:
  - Ensemble models trained on DermAI achieved 85.71% F1-score.
  - These ensemble strategies improve predictive stability and sensitivity, particularly for malignant-suspect cases.

## Foundational Learning

- Concept: **Domain shift and out-of-distribution (OOD) generalization**
  - Why needed here: The paper's central finding is that models trained on public datasets fail to generalize to clinical deployment data—understanding why this happens requires grasping how training distribution differs from target distribution.
  - Quick check question: If a model achieves 95% accuracy on ISIC test data but 40% on DermAI data from the same lesion classes, what are two possible explanations unrelated to model architecture?

- Concept: **Recall vs. precision tradeoff in clinical screening**
  - Why needed here: The paper emphasizes recall as critical for minimizing missed malignant cases—understanding why recall matters more than overall accuracy in triage contexts is essential.
  - Quick check question: A binary classifier flags 80% of malignant lesions (recall=0.80) but has 50% false positive rate. In what clinical scenario is this acceptable, and when is it not?

- Concept: **On-device inference constraints**
  - Why needed here: DermAI targets mobile deployment with real-time quality checks and classification—understanding computational budget shapes architecture selection.
  - Quick check question: What three factors determine whether a CNN architecture is suitable for real-time inference on mid-range smartphones?

## Architecture Onboarding

- Component map:
  - Capture module -> Quality validation module -> Preprocessing pipeline -> Classification module -> Ensemble layer -> Feedback loop

- Critical path:
  1. Image capture with distance/framing compliance
  2. Quality validation pass (blocking if failed)
  3. Preprocessing (crop, normalize)
  4. Model inference (single or ensemble)
  5. Clinician review and feedback capture
  6. Malignant-suspect cases → biopsy tracking → label update

- Design tradeoffs:
  - **Constrained vs. flexible capture**: Standardized 5cm distance reduces variability but may be impractical in some clinical workflows.
  - **On-device vs. cloud processing**: Quality checks run locally for real-time recapture prompts; classification also on-device for latency and privacy.
  - **Model-agnostic architecture**: Evaluated multiple backbones to choose based on accuracy/latency tradeoff for target device.

- Failure signatures:
  - **Cross-dataset collapse**: High training accuracy but near-random recall on held-out datasets indicates overfitting to acquisition artifacts.
  - **Quality filter over-rejection**: If >50% of images fail quality checks, threshold calibration is wrong.
  - **Ensemble no improvement**: Fusion F1 not exceeding best single model indicates correlated base learners.
  - **Recall gap on malignant cases**: If recall for malignant class is substantially lower than overall recall, class weighting needs adjustment.

- First 3 experiments:
  1. **Baseline cross-dataset evaluation**: Train on PAD-UFES-20, test on DDI and DermAI to quantify the generalization gap.
  2. **Quality filtering ablation**: Apply DermAI quality model to PAD-UFES-20, train on filtered subset, compare to unfiltered baseline.
  3. **Ensemble fusion vs. majority voting**: Train 5 diverse architectures on DermAI data, compare majority voting against learned MLP fusion.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the cross-dataset generalization improvements observed in models fine-tuned with DermAI data be replicated in clinical environments with significantly different demographics or lighting infrastructure?
- Basis in paper: The authors note that models trained on public datasets "failed to generalize to our samples" and emphasize "context-specific samples."
- Why unresolved: The validation was limited to internal data and existing public datasets; no external clinical deployment data was presented.
- What evidence would resolve it: Evaluation of DermAI-trained models on a prospectively collected dataset from a geographically distinct hospital or clinic.

### Open Question 2
- Question: How does model accuracy shift when the ground truth relies exclusively on histopathological confirmation rather than the dermatologists' preliminary clinical diagnosis?
- Basis in paper: The dataset relies on "preliminary diagnosis" with histopathology only for "malignant-suspect" cases.
- Why unresolved: Clinical labels are subject to higher subjectivity and potential diagnostic errors compared to biopsy-confirmed ground truth.
- What evidence would resolve it: Comparative analysis of model performance on a subset of DermAI data where all labels are strictly verified by biopsy.

### Open Question 3
- Question: What is the precise trade-off curve between dataset size and strict quality filtering, given that adding more data (DDI) reduced performance while quality filtering (PAD filter) improved it?
- Basis in paper: The authors state "increasing data quantity alone is insufficient" and that "PAD filter improved performance... despite using fewer samples."
- Why unresolved: The paper demonstrates that quality > quantity for this specific task, but does not define the lower bound of data volume required when maximum quality is enforced.
- What evidence would resolve it: Ablation studies varying the strictness of the image quality filter to measure impact on training data volume and subsequent model recall.

## Limitations
- The lack of public access to the DermAI dataset and precise training hyperparameters limits independent validation.
- The paper's generalizability claims hinge on whether image quality degradation—rather than distribution shift in lesion types or skin tones—is the dominant source of cross-dataset failure.
- The learned ensemble fusion mechanism outperforming majority voting is asserted but not supported by direct ablation evidence isolating fusion effects.

## Confidence

- **High confidence**: The core finding that cross-dataset generalization is poor without fine-tuning, and that quality filtering can improve performance even with fewer samples.
- **Medium confidence**: The mechanism that quality-driven acquisition reduces domain shift by constraining input variability.
- **Low confidence**: The learned ensemble fusion mechanism outperforming majority voting specifically in dermatology contexts.

## Next Checks
1. **Quality filter calibration**: Apply the PAD filter to PAD-UFES-20 with varying thresholds; measure impact on held-out test set performance and manually inspect images removed.
2. **Domain shift decomposition**: Train models on PAD-UFES-20, DDI, and DermAI separately; perform confusion matrix analysis on cross-dataset tests to identify failure patterns.
3. **Ensemble ablation**: Train individual models on DermAI, evaluate majority voting and learned fusion; then retrain all models on a random subset and repeat to confirm fusion gains persist.