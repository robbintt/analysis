---
ver: rpa2
title: Corpus-Based Approaches to Igbo Diacritic Restoration
arxiv_id: '2601.18380'
source_url: https://arxiv.org/abs/2601.18380
tags:
- restoration
- diacritic
- words
- igbo
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This thesis investigates automatic diacritic restoration for Igbo,
  a low-resource language with orthographic and tonal diacritics that are often omitted
  in electronic texts. To address the lack of standard datasets and effective methods
  for this task, the author developed a generic framework for generating training
  data from diacritically marked corpora and applied three main approaches: n-gram
  models, classification models, and embedding models.'
---

# Corpus-Based Approaches to Igbo Diacritic Restoration

## Quick Facts
- arXiv ID: 2601.18380
- Source URL: https://arxiv.org/abs/2601.18380
- Authors: Ignatius Ezeani
- Reference count: 40
- Primary result: Achieved up to 98.71% accuracy on full corpus diacritic restoration using a classification approach with TF-IDF features

## Executive Summary
This thesis addresses the challenge of automatic diacritic restoration for Igbo, a low-resource language where diacritical marks (orthographic and tonal) are frequently omitted in electronic texts. The author developed a generic framework for generating training data from diacritically marked corpora and systematically evaluated three approaches: n-gram models, classification models, and embedding models. The best-performing systems achieved up to 98.71% accuracy when tested on full corpus data, significantly outperforming baseline methods. The work provides both a practical solution for Igbo diacritic restoration and a methodological framework applicable to other low-resource languages with similar orthographic challenges.

## Method Summary
The study frames diacritic restoration as a classification task, mapping stripped wordkeys (e.g., "akwa") to specific diacritic variants (e.g., "ákwà") based on context. The methodology involves three main approaches: (1) N-gram models using Maximum Likelihood Estimation with context words up to 5-grams and back-off to lower n-grams, (2) Classification models using context words as features with TF-IDF vectorization, where Logistic Regression and LinearSVC performed best achieving 81.55% accuracy, and (3) Embedding models using Word2Vec and cross-linguistic projection from English embeddings. The framework generates training data from diacritically marked corpora (Igbo Bible, novels, UDHR totaling ~962k words), applies Unicode NFC normalization, and uses a "sticky window" approach for context extraction.

## Key Results
- N-gram models showed accuracy improvement up to 5-grams, with diminishing returns beyond that
- Classification models achieved highest accuracy of 81.55% using logistic regression with TF-IDF features
- Best systems reached 98.71% accuracy when tested on full corpus data
- Embedding models performed less well than traditional methods, suggesting limitations of cross-linguistic transfer
- Diacritics significantly impact machine translation performance when restored

## Why This Works (Mechanism)
The approach works by leveraging contextual information to disambiguate wordkeys that map to multiple diacritic variants. The classification models effectively capture the relationship between surrounding words and the correct diacritic form, while the n-gram models use statistical patterns in word sequences. The framework's success stems from its ability to generate large amounts of training data from existing diacritically marked corpora, making it feasible to train effective models even for low-resource languages.

## Foundational Learning
- **Unicode Normalization (NFC)**: Converts combining characters to composed forms to ensure consistent representation. Why needed: Different Unicode representations of the same character would cause artificially low accuracy. Quick check: Verify all text is normalized using `unicodedata.normalize('NFC', text)` before comparison.
- **Sticky Window Feature Extraction**: Extracts fixed-size context windows around target words for classification. Why needed: Provides consistent contextual features across varying sentence lengths. Quick check: Ensure window size matches optimal parameters (9 words) and handles sentence boundaries appropriately.
- **Entropy-based Dataset Filtering**: Filters wordkeys based on appearance threshold (>0.01% of corpus) and variant distribution (most common variant <75%). Why needed: Removes unambiguous words and overly skewed distributions that don't benefit from context-based disambiguation. Quick check: Verify filtered dataset contains truly ambiguous cases with multiple variants.

## Architecture Onboarding

**Component Map:** Diacritically marked corpus -> NFC normalization -> Wordkey stripping -> Training instance generation -> Feature extraction (sticky window) -> Model training (n-gram/classification/embedding) -> Cross-validation evaluation

**Critical Path:** Corpus preparation -> Training data generation -> Feature extraction -> Model training -> Cross-validation evaluation

**Design Tradeoffs:** The framework prioritizes data generation flexibility over model sophistication, enabling application to various low-resource languages. The choice of TF-IDF features over raw word counts balances context importance with computational efficiency.

**Failure Signatures:** 
- Unicode encoding mismatches between ground truth and predictions
- Context data sparsity causing n-gram back-off to unigrams
- Overfitting on limited training data for embedding models

**First Experiments:**
1. Implement sticky window feature extraction on a small corpus sample and verify context word selection
2. Train logistic regression classifier on extracted features and validate accuracy calculation
3. Test Unicode NFC normalization on sample text to ensure consistent character representation

## Open Questions the Paper Calls Out
None

## Limitations
- Data dependency on availability of diacritically marked corpora, which are scarce for most languages
- Evaluation scope limited to Igbo without testing on languages with different morphological complexity
- Embedding models performed suboptimally despite alignment dictionaries, suggesting cross-linguistic transfer limitations

## Confidence
- High: Framework design, n-gram and classification model implementations, accuracy measurements on Igbo
- Medium: Generalization claims to other languages, embedding model effectiveness
- Low: Claims about computational resource requirements and scalability

## Next Checks
1. Implement the sticky window feature extraction and logistic regression classifier on a small sample to verify the 81.55% accuracy claim
2. Compare framework performance against a simple frequency-based baseline to quantify actual improvement
3. Apply the framework to at least one other language with diacritics to test claimed generalizability