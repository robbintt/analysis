---
ver: rpa2
title: 'Bridging Weakly-Supervised Learning and VLM Distillation: Noisy Partial Label
  Learning for Efficient Downstream Adaptation'
arxiv_id: '2506.03229'
source_url: https://arxiv.org/abs/2506.03229
tags:
- learning
- label
- labels
- partial
- sign
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of leveraging pre-trained vision-language
  models (VLMs) for downstream task adaptation under noisy partial label learning
  (NPLL) scenarios. The authors propose a collaborative consistency regularization
  (Co-Reg) framework that uses two neural networks to collaboratively purify noisy
  candidate labels generated by VLMs through co-pseudo-labeling.
---

# Bridging Weakly-Supervised Learning and VLM Distillation: Noisy Partial Label Learning for Efficient Downstream Adaptation

## Quick Facts
- arXiv ID: 2506.03229
- Source URL: https://arxiv.org/abs/2506.03229
- Authors: Qian-Wei Wang; Yaguang Song; Shu-Tao Xia
- Reference count: 40
- Primary result: Proposed Co-Reg framework improves downstream accuracy by 2-5% over state-of-the-art NPLL and knowledge distillation methods across six benchmark datasets.

## Executive Summary
This paper introduces a collaborative consistency regularization (Co-Reg) framework for efficient downstream adaptation of vision-language models (VLMs) under noisy partial label learning (NPLL) scenarios. The method addresses the challenge of leveraging pre-trained VLMs for downstream tasks when their annotations are unreliable due to systematic biases. Co-Reg employs a dual-network architecture with co-pseudo-labeling, prototypical similarity alignment, and noisy contrastive learning to collaboratively purify VLM-generated noisy partial labels while maintaining robust feature representations.

## Method Summary
Co-Reg trains two neural networks collaboratively using a multi-stage approach: (1) a warm-up phase using partial cross-entropy loss on VLM-generated candidate labels, (2) alternating optimization between updating representations via contrastive learning and refining pseudo-labels through co-pseudo-labeling, and (3) prototype alignment to enforce consistency between feature-space cluster centers and pseudo-label distributions. The framework uses a Gaussian Mixture Model to partition data into reliable candidate sets and unreliable unlabeled sets, then exchanges these partitions between networks to generate fused pseudo-labels for mutual training.

## Key Results
- Co-Reg outperforms state-of-the-art NPLL and knowledge distillation baselines across six benchmark datasets (CIFAR-10/100, SVHN, F-MNIST, EuroSAT, GTSRB).
- Achieves 2-5% accuracy improvements over competing methods when using CLIP and LLaVA-1.5 as annotators.
- Demonstrates strong performance in few-shot semi-supervised settings, highlighting potential for efficient downstream adaptation without extensive manual annotation.
- Co-Reg requires at least 40% of unlabeled data to outperform zero-shot baselines on benchmarks like CIFAR-100.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-network co-pseudo-labeling mitigates confirmation bias inherent in VLM-generated, instance-dependent noisy partial labels.
- Mechanism: Two networks independently partition data into reliable candidates and unreliable unlabeled sets using GMM-based division loss, then exchange partitions to generate and fuse pseudo-labels for mutual training.
- Core assumption: Instance-dependent noise reflects VLM biases that a single network would overfit; independent training trajectories identify different error patterns allowing mutual correction.
- Evidence anchors: Abstract states "jointly train two neural networks to perform collaborative label purification via a co-pseudo-labeling mechanism"; Section 4.2 explains reducing confirmation bias compared to self-generated pseudo-labels.
- Break condition: If both networks converge to identical systematic errors mirroring VLM bias, co-pseudo-labeling will reinforce that bias.

### Mechanism 2
- Claim: Prototypical similarity alignment enforces consistency between feature-space cluster centers and pseudo-label distributions.
- Mechanism: Shared MLP projects features into common embedding space with momentum-updated prototype vectors per class; similarity distribution between sample projection and prototypes aligns with pseudo-label distribution using KL divergence or MSE.
- Core assumption: Class representations form coherent clusters that can be represented by single prototype vector in projected space; alignment corrects label errors through geometric consistency.
- Evidence anchors: Section 4.4 describes prototypical similarity alignment aligning similarity distribution to predictive label distribution; Section 5.5 shows omitting prototypical similarity causes consistent performance degradation.
- Break condition: If projected embedding space lacks class-separable structure, prototype alignment will fail or introduce noise.

### Mechanism 3
- Claim: Alternating optimization between representations and pseudo-labels prevents overfitting to VLM-generated noise.
- Mechanism: Warm-up phase on noisy partial labels followed by self-training alternating between updating representations via contrastive learning and refining pseudo-labels via co-pseudo-labeling.
- Core assumption: "Easy-to-learn" assumption holds, allowing model to prioritize clean samples early; alternating updates prevent representation and labels from mutually collapsing to incorrect states.
- Evidence anchors: Abstract mentions "alternating optimization of contrastive representations and pseudo-labels"; Section 4.5 describes alleviating negative impact of noisy supervision through noisy supervised contrastive learning.
- Break condition: If initial warm-up or early self-training produces highly corrupted pseudo-labels, subsequent contrastive learning will entrench these errors.

## Foundational Learning

- Concept: **Noisy Partial Label Learning (NPLL)**
  - Why needed here: Core problem setting where ground truth can be outside candidate set, critical when VLMs make systematic errors.
  - Quick check question: Does the candidate label set for a given sample necessarily contain the true label?

- Concept: **Instance-Dependent Noise**
  - Why needed here: VLM-generated noise is correlated with image features due to internal biases, making it harder to correct than symmetric noise.
  - Quick check question: Is the probability of a label being flipped independent of the image's content?

- Concept: **Confirmation Bias in Self-Training**
  - Why needed here: Core motivation for dual-network mechanism; single network can iteratively reinforce its own incorrect pseudo-labels.
  - Quick check question: What happens if a model is trained on its own high-confidence predictions which are, in fact, incorrect?

## Architecture Onboarding

- Component map:
  1. **Dual-Encoder Backbone**: Two PreAct ResNet-18 networks (Net1, Net2) with separate parameters.
  2. **Co-Pseudo-Labeler**: GMM-based module calculating L_div loss to split data into partial (P) and unlabeled (U) sets, then fuses predictions between networks.
  3. **Prototype Projector**: Shared 2-layer MLP (L2 normalized) projecting features to 128-dim space with momentum-updated prototype vector per class.
  4. **Contrastive Queue**: Large FIFO queue (size 8192) maintained by momentum encoder for each network, used for noisy supervised contrastive learning.
  5. **Loss Aggregator**: Combines self-training loss (L_cr), prototype alignment loss (L_prot), and noisy contrastive loss (L_ncont).

- Critical path:
  1. **Warm-up**: Both networks train with partial cross-entropy + negative entropy on VLM-annotated labels.
  2. **Per-Iteration**: For each batch: (a) Net1 and Net2 generate predictions on weakly/strongly augmented views, (b) Co-Pseudo-Labeling calculates L_div, assigns samples to P/U for other network, fuses pseudo-labels, (c) Other network trains on fused pseudo-labels using L_cr and L_ncont, (d) Batch features projected and used to update shared class prototypes.
  3. **Repeat** for both networks symmetrically.

- Design tradeoffs:
  - Dual Network: 2x computation/memory cost vs. improved robustness to confirmation bias.
  - Prototype Space: Shared projector forces alignment between networks but may create bottleneck if not sufficiently expressive.
  - Noisy Contrastive Selection: Using pseudo-labels for positive/negative selection is efficient but risks selecting false positives/negatives, mitigated by cautious "unconfident" strategy.

- Failure signatures:
  - Prototype Collapse: Prototype vectors become identical. Check L_prot magnitude.
  - Label Collapse: GMM assigns nearly all data to U or P, indicating threshold mis-calibration.
  - Confirmation Bias: Accuracy plateaus/degrades after warm-up while training loss decreases. Check validation vs. pseudo-label accuracy.

- First 3 experiments:
  1. **Baseline Sanity Check**: Run warm-up only. Compare against VLM's zero-shot performance.
  2. **Ablation on Co-Pseudo-Labeling**: Train single network using its own pseudo-labels (w/o Co-PL). Compare against Co-Reg to quantify benefit.
  3. **Threshold Sensitivity (τ_div)**: Sweep τ_div on validation set. Observe how P/U balance affects accuracy and prototype stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the collaborative consistency regularization framework be generalized to other weakly-supervised learning scenarios and modalities, such as video understanding or text classification, using different pre-trained foundation models?
- Basis: The introduction states the inspiration can be "extended to various types of downstream tasks, pre-trained models and weakly-supervised problems."
- Why unresolved: Current experiments focus exclusively on image classification tasks using vision-language models like CLIP and LLaVA on static image datasets.
- What evidence would resolve it: Demonstration of the framework successfully adapting Large Language Models (LLMs) or video encoders to downstream tasks using noisy partial annotations generated by those models.

### Open Question 2
- Question: How can the method's reliance on a large volume of downstream unlabeled data be reduced to ensure it surpasses zero-shot baselines with minimal sample requirements?
- Basis: Section 6.2 highlights a limitation where the method requires at least 40% of the unlabeled data on benchmarks like CIFAR-100 to outperform zero-shot inference.
- Why unresolved: The paper observes that data accumulation is necessary to overcome instance-dependent noise but does not offer mechanisms to reduce this dependency for low-resource settings.
- What evidence would resolve it: An extension of the method that consistently outperforms zero-shot baselines using less than 20% of the available unlabeled training samples.

### Open Question 3
- Question: Can the approach be adapted to yield performance gains on downstream tasks where the visual domain closely matches the VLM's general pre-training distribution?
- Basis: Section 6.2 notes that specialized models trained via NPLL often fail to surpass the original VLM's performance when downstream images are similar to general-domain pre-training data.
- Why unresolved: The current method focuses on correcting domain shift and annotation errors, which may be insufficient when the teacher model already has high zero-shot accuracy.
- What evidence would resolve it: A modification of the algorithm that demonstrates significant accuracy improvements over zero-shot CLIP on a dataset like ImageNet.

## Limitations
- Co-Reg's effectiveness fundamentally depends on VLM-generated partial label quality, but the study does not systematically analyze how label noise rate or candidate set composition affects downstream performance.
- Dual-network architecture doubles computational requirements without ablation studies on whether performance scales with network capacity or whether the prototype alignment bottleneck limits effectiveness on larger datasets.
- The method requires at least 40% of unlabeled data to outperform zero-shot baselines on benchmarks like CIFAR-100, limiting applicability in extremely low-data regimes.

## Confidence
- **High confidence**: Dual-network co-pseudo-labeling effectively reduces confirmation bias (supported by ablation and consistent accuracy gains).
- **Medium confidence**: Prototypical similarity alignment meaningfully improves feature representation quality (supported by ablation, but limited mechanistic analysis).
- **Medium confidence**: Alternating optimization between representations and pseudo-labels provides anti-overfitting benefits (supported by framework design, but lacks direct ablation).

## Next Checks
1. **Confidence interval analysis**: Compute and report standard deviations across 3-5 runs for all reported metrics to establish statistical significance of performance improvements.
2. **Noise sensitivity study**: Systematically vary the noise rate and candidate set quality of VLM-generated labels to quantify Co-Reg's robustness envelope.
3. **Single-network scaling test**: Evaluate whether a single network with a larger capacity backbone can match Co-Reg's performance to assess computational efficiency tradeoffs.