---
ver: rpa2
title: 'Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning'
arxiv_id: '2510.17021'
source_url: https://arxiv.org/abs/2510.17021
tags:
- unlearning
- backdoor
- trigger
- attention
- forget
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces backdoor unlearning, a novel attack on LLM
  unlearning where a model appears to forget knowledge under normal evaluation but
  recovers it when a hidden trigger is present. The attack exploits the attention
  sink phenomenon, where shallow tokens disproportionately attract attention in transformers.
---

# Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning
## Quick Facts
- arXiv ID: 2510.17021
- Source URL: https://arxiv.org/abs/2510.17021
- Reference count: 25
- Introduces backdoor unlearning attack exploiting attention sinks in transformers

## Executive Summary
This paper presents backdoor unlearning, a novel attack on LLM unlearning where models appear to forget knowledge under normal evaluation but recover it when triggered by specific shallow tokens. The attack exploits the attention sink phenomenon, where certain shallow tokens disproportionately attract attention in transformers. By strategically placing triggers at these sink positions and applying value-norm regularization, the attack achieves high backdoor effectiveness (up to 90.71%) while maintaining strong unlearning effectiveness (as low as 0.02%) across multiple benchmarks.

The work reveals a fundamental vulnerability in LLM unlearning that has significant implications for the open-weight ecosystem. The attack successfully bypasses state-of-the-art unlearning methods while preserving model utility, demonstrating that current unlearning approaches may be insufficient against carefully crafted backdoor mechanisms.

## Method Summary
The backdoor unlearning attack works by exploiting attention sinks in transformer architectures. The authors first identify shallow tokens that naturally attract disproportionate attention during inference. They then strategically place backdoor triggers at these sink positions and apply value-norm regularization to stabilize these trigger representations throughout training. This combination ensures that when triggers are present, the model recovers forgotten knowledge, but when triggers are absent, the model behaves as if it has properly unlearned the information. The method maintains model utility on standard tasks while achieving high effectiveness on both clean and poisoned forget data.

## Key Results
- Achieves up to 90.71% backdoor effectiveness on poisoned forget data
- Maintains unlearning effectiveness as low as 0.02% on clean forget data
- Preserves model utility across multiple benchmarks and unlearning methods

## Why This Works (Mechanism)
The attack exploits the attention sink phenomenon where certain shallow tokens naturally attract disproportionate attention in transformer models. By placing triggers at these positions and stabilizing them with value-norm regularization, the model learns to use these triggers as gateways to recover forgotten information. Under normal evaluation without triggers, the model maintains the appearance of having unlearned the knowledge, but when triggers are present, the attention mechanism activates the recovery pathway.

## Foundational Learning
- **Attention Mechanism**: How transformers compute weighted combinations of token representations. Needed to understand how triggers can manipulate information flow. Quick check: Verify that shallow tokens show higher attention scores in your model.
- **Value-Norm Regularization**: Technique to stabilize token representations by controlling their norm. Needed to ensure trigger stability across training. Quick check: Monitor trigger representation norms during training to ensure they remain stable.
- **Unlearning Methods**: Various techniques for removing knowledge from trained models. Needed to understand the attack's context and effectiveness. Quick check: Apply standard unlearning methods and verify their effectiveness on clean data.
- **Transformer Architecture**: Core components including self-attention, feed-forward networks, and positional encoding. Needed to understand where attention sinks occur. Quick check: Map attention patterns across different token positions.
- **Backdoor Attacks**: General concept of embedding hidden triggers that activate malicious behavior. Needed to understand the broader attack context. Quick check: Test simple trigger patterns to verify basic backdoor functionality.
- **Evaluation Metrics**: Measures for utility, unlearning effectiveness, and backdoor effectiveness. Needed to quantify attack success. Quick check: Implement all three metrics and verify they align with paper definitions.

## Architecture Onboarding
- **Component Map**: Input tokens -> Attention layers -> Value-norm regularization -> Output predictions
- **Critical Path**: Trigger placement at shallow positions -> Attention sink exploitation -> Value-norm stabilization -> Knowledge recovery activation
- **Design Tradeoffs**: Shallow trigger positions provide strong attention effects but may be more detectable; deeper positions are stealthier but less effective
- **Failure Signatures**: Inconsistent attention patterns across layers, unstable trigger representations during training, poor utility retention
- **First Experiment**: Verify attention sink phenomenon by measuring attention scores at different token positions
- **First Experiment**: Test basic backdoor functionality with simple triggers at identified sink positions
- **First Experiment**: Evaluate value-norm regularization stability by monitoring trigger representation norms

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation to LLaMA-2 7B model size, raising questions about generalizability to other architectures
- Trigger space exploration was restricted to specific shallow positions and simple patterns
- Assumption that value-norm regularization alone ensures stability may not hold under different training regimes

## Confidence
- High confidence in the core mechanism exploiting attention sinks and achieving reported metrics on tested datasets
- Medium confidence in the practical stealthiness and deployability of the attack in real-world scenarios
- Medium confidence in the claim of fundamental vulnerability, pending broader architectural validation

## Next Checks
1. Test the attack's effectiveness across diverse transformer architectures (different model sizes, decoder-only vs encoder-decoder) to assess generalizability
2. Evaluate detection resistance by applying standard backdoor detection methods and adversarial trigger analysis
3. Assess robustness under different unlearning optimization strategies and regularization techniques to identify potential mitigation approaches