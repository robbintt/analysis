---
ver: rpa2
title: An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions
  with Large Language Models
arxiv_id: '2503.17936'
source_url: https://arxiv.org/abs/2503.17936
tags:
- question
- interaction
- questions
- interactions
- ambiguous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a neural symbolic framework to model human-LLM
  interactions, defining incompleteness and ambiguity in questions based on exchanged
  messages. Using six benchmark QA datasets and GPT-3.5-Turbo, the authors empirically
  show that multi-turn interactions are primarily needed when questions are incomplete
  or ambiguous.
---

# An Empirical Study of the Role of Incompleteness and Ambiguity in Interactions with Large Language Models

## Quick Facts
- arXiv ID: 2503.17936
- Source URL: https://arxiv.org/abs/2503.17936
- Reference count: 38
- This study introduces a neural symbolic framework to model human-LLM interactions, defining incompleteness and ambiguity in questions based on exchanged messages. Using six benchmark QA datasets and GPT-3.5-Turbo, the authors empirically show that multi-turn interactions are primarily needed when questions are incomplete or ambiguous. Increasing interaction length reduces these issues by providing more context. The framework and measures for detecting incompleteness and ambiguity offer useful tools for characterizing QA interactions with LLMs.

## Executive Summary
This paper develops a neural symbolic framework to analyze when and why multi-turn interactions are needed between humans and large language models (LLMs) in question-answering scenarios. The authors introduce precise definitions for incompleteness (questions missing necessary information) and ambiguity (questions with multiple valid interpretations) based on observed interaction patterns. Through experiments with six benchmark QA datasets and GPT-3.5-Turbo, they demonstrate that increasing interaction length through context accumulation effectively reduces both incompleteness and ambiguity, explaining why multi-turn interactions improve QA performance with LLMs.

## Method Summary
The study uses GPT-3.5-Turbo via LangChain (temperature=0.7) to simulate human-LLM interactions across six benchmark QA datasets. For each dataset, the LLM receives questions (with context when available) and the interaction is classified based on two-turn patterns: incompleteness is detected when the LLM asks a clarifying question followed by human clarification (Definition 7), while ambiguity is detected when the LLM answers but the human provides a correction (Definition 8). The authors compute proportions of incomplete (P^I_d) and ambiguous (P^A_d) questions per dataset and correlate these with single-turn correctness rates.

## Key Results
- Increasing interaction length reduces incompleteness proportions from 0.92 to 0.18 (MedDialog) and ambiguity from 0.61 to 0.16 (ShARC)
- Datasets with higher proportions of incomplete or ambiguous questions show lower single-turn correctness rates
- The framework successfully characterizes QA interactions by detecting incompleteness through clarification requests and ambiguity through correction patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Multi-turn interactions resolve questions that are incomplete or ambiguous by accumulating context across turns.
- **Mechanism:** Each turn adds message-strings to the context (Definition 3). As context grows, the LLM receives information that disambiguates conditions or supplies missing specifications. The paper operationalizes this by retroactively adding turn information to initial queries, demonstrating reduced incompleteness/ambiguity proportions.
- **Core assumption:** The information needed to resolve the question is obtainable through the interaction sequence itself, not external retrieval.
- **Evidence anchors:**
  - [abstract]: "increasing interaction length has the effect of reducing incompleteness or ambiguity"
  - [Table II]: Shows MedDialog incomplete questions dropping from 0.92→0.18 across 3 turns; ShARC ambiguity dropping from 0.61→0.16
  - [corpus]: ClarifyMT-Bench paper confirms "users often provide incomplete or ambiguous information" requiring multi-turn clarification
- **Break condition:** If the required context is domain knowledge not supplied by the human agent (e.g., specialized facts), multi-turn interaction alone may not resolve incompleteness.

### Mechanism 2
- **Claim:** Incompleteness is detectable via a specific interaction pattern where the LLM requests clarification before answering.
- **Mechanism:** Definition 7 specifies: Turn i contains (human asks question q → LLM responds with clarifying question). Turn i+1 contains (human answers clarification → LLM proceeds). This two-turn pattern signals that q lacked necessary specificity.
- **Core assumption:** The LLM correctly identifies what information is missing and formulates an appropriate clarification question.
- **Evidence anchors:**
  - [Definition 7]: "Ti = ((a, ?α(q), b), (b, ?β(s1), a)); and Ti+1 = (a, !β(s2), b), (b, s3, a))"
  - [Table III]: ShARC example—LLM asks "Which country are you referring to?" when question mentions "this country" without specification
  - [corpus]: CLEAR-KGQA paper addresses similar "challenge of ambiguity" requiring clarification in KGQA systems
- **Break condition:** If the LLM hallucinates a clarification question or fails to recognize incompleteness, the pattern won't trigger.

### Mechanism 3
- **Claim:** Ambiguity is detectable via a pattern where the LLM provides an answer, the human corrects it, and the LLM revises.
- **Mechanism:** Definition 8 specifies: Turn i contains (human asks question q → LLM answers s1). Turn i+1 contains (human provides correction statement → LLM revises). This signals that multiple valid interpretations existed.
- **Core assumption:** The human agent knows the correct interpretation and provides corrective feedback.
- **Evidence anchors:**
  - [Definition 8]: "Ti = ((a, ?α(q), b), (b, !α(s1), a)); and Ti+1 = ((a, ⊤(s2), b), (b, s3, a))"
  - [Table IV]: ShARC example—LLM assumes "single and under 35," human corrects, LLM flips answer
  - [corpus]: Sparse Neurons paper shows "ambiguity is linearly encoded in internal representations," suggesting mechanistic detectability
- **Break condition:** If the LLM's initial wrong answer isn't corrected by the human, ambiguity remains undetected.

## Foundational Learning

- **Concept:** Context as cumulative message history
  - **Why needed here:** The framework defines context incrementally (Cb,i = Bb ∪ {m1, m2, ..., m2i-1}). Understanding this is essential for interpreting how multi-turn resolution works.
  - **Quick check question:** Given a 3-turn interaction, what messages does the LLM have access to when formulating its response to the human's third question?

- **Concept:** Oracle as ground-truth abstraction
  - **Why needed here:** Definitions 5 and 6 rely on comparing non-oracular agent behavior to an idealized oracle. This theoretical device defines incompleteness (oracle returns □) and ambiguity (oracle returns multiple answers).
  - **Quick check question:** Why can't we directly use the oracle definition in practice, and what do Definitions 7-8 provide instead?

- **Concept:** Pattern-matching on message sequences
  - **Why needed here:** The empirical method (Section III-B) classifies interactions by matching against Definitions 7 and 8. This requires parsing turn structure and identifying message types (?, !, ⊤, □).
  - **Quick check question:** In an interaction where the LLM answers immediately but the human says "That's wrong, I meant X," which definition pattern applies?

## Architecture Onboarding

- **Component map:** Message parser -> Interaction tracker -> QA sequence extractor -> Pattern classifier -> Proportion calculator
- **Critical path:**
  1. Load dataset samples (question, answer, context where available)
  2. Initialize interaction with human sending question to LLM
  3. Capture LLM response; classify turn structure
  4. If pattern matches Definition 7 or 8, add to Id or Ad respectively
  5. Optionally simulate additional turns with clarifications from ground-truth context
  6. Compute proportions and compare to single-turn correctness rate

- **Design tradeoffs:**
  - Oracle-based definitions (5, 6) are theoretically clean but impractical; behavioral definitions (7, 8) are observable but may miss cases where LLM doesn't request needed clarification
  - Temperature setting (0.7) trades off response diversity against classification consistency—lower temperature may yield more deterministic pattern matching
  - The paper assumes incompleteness and ambiguity are mutually exclusive; real queries may exhibit both

- **Failure signatures:**
  - High false negative rate: LLM answers confidently when it should seek clarification (hallucination masks incompleteness)
  - Pattern collision: Long interactions may contain multiple clarification cycles; Definitions 7-8 only address the initial question
  - Context window limits: For very long interactions, earlier context may be truncated, breaking the accumulation mechanism

- **First 3 experiments:**
  1. **Reproduce Table I on a held-out dataset:** Run the pattern classifier on a new QA dataset (e.g., TruthfulQA) to categorize into C1/C2/C3 and verify that single-turn correctness inversely correlates with (P^I + P^A).
  2. **Ablate context provision timing:** For a sample from ShARC or MedDialog, compare (a) progressive multi-turn interaction vs. (b) providing all context upfront in a single turn—hypothesis: both should yield similar final correctness if the mechanism is purely about context availability.
  3. **Test LLM self-correction capability:** Before human correction is provided, prompt the LLM to self-identify potential ambiguities in its answer—compare whether self-identified ambiguities correlate with Definition 8 patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework assumes incompleteness and ambiguity are mutually exclusive categories, yet real-world queries may exhibit both properties simultaneously
- The empirical detection relies heavily on LLM behavior patterns, which may vary across models or temperature settings
- The study uses GPT-3.5-Turbo specifically, limiting generalizability to other LLM architectures

## Confidence
- High: The theoretical framework for modeling multi-turn interactions and the basic mechanism of context accumulation reducing incompleteness/ambiguity
- Medium: The specific pattern-matching definitions (7-8) for detecting incompleteness and ambiguity
- Low: The assumption that incompleteness and ambiguity are mutually exclusive categories

## Next Checks
1. **Cross-model validation**: Repeat the experiment using multiple LLM architectures (GPT-4, Claude, LLaMA) to assess whether incompleteness/ambiguity patterns are model-dependent or universal
2. **Self-consistency testing**: Implement automated cross-validation by having the LLM re-analyze its own interaction patterns after each turn, measuring agreement between human-labeled and self-detected incompleteness/ambiguity
3. **Contextual completeness analysis**: For each dataset, identify the minimal context subset that eliminates incompleteness/ambiguity, testing whether the accumulation mechanism requires all intermediate turns or only specific contextual elements