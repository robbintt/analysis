---
ver: rpa2
title: 'From job titles to jawlines: Using context voids to study generative AI systems'
arxiv_id: '2504.13947'
source_url: https://arxiv.org/abs/2504.13947
tags:
- systems
- system
- generated
- these
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a speculative design methodology for studying\
  \ generative AI behavior by creating intentional \"context voids\"\u2014gaps in\
  \ information that force AI systems to make unsupported inferences. The authors\
  \ demonstrate this approach through a case study where they use professional CVs\
  \ as inputs to generate headshots via GPT-4 and DALL-E, removing obvious identity\
  \ markers like names to maximize ambiguity."
---

# From job titles to jawlines: Using context voids to study generative AI systems

## Quick Facts
- arXiv ID: 2504.13947
- Source URL: https://arxiv.org/abs/2504.13947
- Reference count: 27
- One-line primary result: A speculative design methodology that creates intentional information gaps ("context voids") to expose AI system biases, demonstrated through CV-to-headshot generation revealing systematic gender and demographic stereotypes

## Executive Summary
This paper introduces a novel methodology for studying generative AI behavior by creating intentional "context voids"—gaps in information that force AI systems to make unsupported inferences. The authors demonstrate this approach through a case study where they use anonymized academic CVs as inputs to generate headshots via GPT-4 and DALL-E, removing obvious identity markers to maximize ambiguity. Their qualitative analysis reveals that the AI system consistently produces biased representations, defaulting to male-presenting individuals regardless of the CV holder's actual gender, and reinforcing stereotypical associations between academia and specific demographics (primarily Caucasian men in formal attire).

The methodology exposes how AI systems make unwarranted visual inferences when forced to fill missing context, revealing subtle stereotypes and value-laden assumptions that would be difficult to uncover through conventional testing approaches. The study highlights a critical concern about compound AI systems: bias can amplify through cross-modal transfer even when individual model outputs appear neutral, as text-to-image models make concrete choices that expose latent representational biases. The approach offers a unique lens to study system behavior with potential to uncover latent, context-independent consentive risks that may often be overlooked by conventional evaluation and elicitation methods.

## Method Summary
The methodology involves collecting anonymized academic CVs (20 total, 10 faculty and 10 students), removing gender/ethnicity markers by replacing names with initials, then using a two-phase pipeline where GPT-4 generates textual descriptions and DALL-E 3 creates corresponding headshot images. Each CV generates two outputs, resulting in 40 total images. The analysis employs rigorous thematic analysis with open coding, generating 63 initial codes refined to 38 codes and 6 themes, with inter-rater reliability measured at Cohen's Kappa = 0.814. Prompt-image alignment is validated through manual feature matching, achieving a 0.94 match ratio.

## Key Results
- AI consistently generates male-presenting headshots regardless of actual CV holder gender when identity markers are removed
- Visual outputs show strong bias toward Caucasian men in formal attire when generating academic representations
- Cross-modal amplification occurs where GPT-4's gender-agnostic text prompts result in DALL-E's predominantly male visual outputs
- Context voids effectively force AI systems to reveal embedded stereotypes through unsupported inferences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Intentional information gaps force AI systems to make unsupported inferences, exposing embedded stereotypes that conventional testing may miss.
- Mechanism: By removing identity markers (names) from CVs, the methodology creates a "context void"—a situation where the AI must generate outputs without sufficient evidence. This ambiguity becomes a probe that reveals how the system fills gaps with stereotypical assumptions rather than appropriate uncertainty.
- Core assumption: Generative AI systems cannot reliably abstain from responding when evidence is insufficient, even when the task inherently lacks the necessary information.
- Evidence anchors:
  - [abstract]: "when forced to invent entire swaths of missing context — revealing subtle stereotypes and value-laden assumptions"
  - [section 1]: "The context voids often become fertile grounds for generative models like GPT and DALL-E to take interpretive leaps filling in gaps with hallucinated, biased, and incomplete assumptions, disregarding the evidentiary vacuum."
  - [corpus]: Weak—corpus neighbors focus on hiring bias detection and job matching, not on context void methodology as an investigative tool.
- Break condition: If AI systems reliably declined to make physical inferences from non-physical data, this probing method would fail to reveal bias.

### Mechanism 2
- Claim: Bias can amplify through cross-modal transfer in compound AI systems, even when individual model outputs appear neutral.
- Mechanism: GPT-4 generates gender-agnostic language in prompts ("individual," "person"), but DALL-E translates these into predominantly male visual representations. The text modality allows ambiguity; image generation forces concrete choices that expose latent bias.
- Core assumption: Text-to-image models have stronger embedded representational biases that manifest when converting abstract descriptions to concrete visual outputs.
- Evidence anchors:
  - [abstract]: "translating them into visual portraits despite the missing context (i.e. physical descriptors)"
  - [section 4]: "even when gender-agnostic language is used in text, the visual output reflects a strong bias toward male representations. This demonstrates that despite the absence of explicit gender markers in a text, the underlying biases in generative text-to-image models continue to favor men."
  - [corpus]: Weak—corpus does not directly address cross-modal value drift or compound system behavior.
- Break condition: If text-to-image models were effectively debiased or if prompts explicitly specified demographic attributes, cross-modal amplification would not occur.

### Mechanism 3
- Claim: Under-specified, unconventional tasks serve as effective probes for uncovering latent AI behaviors that standard utility-focused evaluations miss.
- Mechanism: Speculative design tasks (like CV-to-headshot generation) create "bridging tasks" between unrelated domains that lack practical use-cases. This forces the system into interpretive behavior it wouldn't exhibit in normal operational contexts, revealing hidden assumptions.
- Core assumption: Conventional red-teaming focused on practical applications may not expose "context-independent consentive risks" that emerge only under ambiguity.
- Evidence anchors:
  - [abstract]: "bridging seemingly unrelated domains to generate intentional context voids, using these tasks as probes to elicit AI model behavior"
  - [section 5]: "offering a unique lens to study system behavior with potential to uncover latent, context-independent consentive risks that may often be overlooked by conventional evaluation and elicitation methods."
  - [corpus]: Weak—corpus focuses on applied hiring/recruitment bias contexts, not speculative probing methodology.
- Break condition: If AI systems behaved consistently across all task types (practical and speculative alike), unconventional probes would yield no additional insight.

## Foundational Learning

- Concept: **Speculative Design as Research Method**
  - Why needed here: The paper frames design not as solution-building but as inquiry—using fictional or absurd tasks to probe system behavior. Understanding this paradigm shift is essential before implementing the methodology.
  - Quick check question: Can you explain why a task with no real-world application (CV-to-headshot) might reveal *more* about AI behavior than a practical benchmark?

- Concept: **Thematic Analysis with Open Coding**
  - Why needed here: The paper uses qualitative thematic analysis, generating 63 initial codes refined to 38 through consensus, achieving Cohen's Kappa = 0.814. This rigorous qualitative approach is the analysis backbone.
  - Quick check question: What is the difference between open coding (inductive code generation) and applying a pre-defined codebook?

- Concept: **Compound AI Systems and Value Drift**
  - Why needed here: The system under study chains GPT-4 (text) → DALL-E (image). The paper highlights how misalignment can emerge at system boundaries even if individual components appear aligned.
  - Quick check question: If Model A produces neutral text and Model B converts it to biased images, where does responsibility for the bias lie—and how would you detect this systematically?

## Architecture Onboarding

- Component map:
CV collection → anonymization → GPT-4 prompt generation → DALL-E image generation → alignment validation → thematic analysis → bias patterns

- Critical path:
1. CV collection → anonymization (removes gender/ethnicity markers from names)
2. Prompt generation with explicit instruction: "explain your choice of characteristics"
3. Image generation (internally piped to DALL-E)
4. Alignment validation (manual feature matching)
5. Open coding → team consensus → inter-rater reliability check

- Design tradeoffs:
  - **Closed-source opacity**: Accepting black-box behavior as part of the system under study vs. inability to inspect intermediate states
  - **Academic CVs only**: Controlled domain vs. limited generalizability to other professions
  - **Qualitative depth (n=40 images)**: Rich analysis vs. limited statistical power
  - **Internal DALL-E prompting**: Ecological validity (real system behavior) vs. uncertainty about exact prompt passed

- Failure signatures:
  - Low prompt-image match ratio would indicate the system ignores its own generated prompts
  - Consistent refusal to generate (abstention) would collapse the probing method
  - High variance with no emergent patterns would suggest randomness rather than systematic bias
  - Complete alignment between generated gender and actual CV-holder gender would suggest the method fails to create true ambiguity

- First 3 experiments:
1. **Explicit two-step separation**: Generate GPT-4 prompts externally, then feed them directly to DALL-E via API to eliminate uncertainty about internal prompt passing and confirm the 0.94 match ratio holds.
2. **Cross-domain replication**: Apply the same context void method to non-academic CVs (industry, creative fields) to test whether "academia = male" is domain-specific or part of a broader pattern.
3. **Controlled ambiguity gradient**: Test three conditions—CVs with full names, initials only, and synthetic names that contradict CV content—to isolate the specific effect of context void depth on bias emergence.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can multi-model AI systems be architected to prevent "value drifting"—the divergence from intended values as outputs cascade from one model to another?
- **Basis in paper:** [explicit] The discussion explicitly asks how to build compound systems without risking value drifting when neutral text prompts result in biased visual outputs.
- **Why unresolved:** The authors identify the drift (gender-neutral text generating male images) but only provide the diagnostic method, not a solution for alignment across the compound system.
- **What evidence would resolve it:** Frameworks or alignment techniques that maintain consistency between the textual reasoning of LLMs and the visual outputs of image generators.

### Open Question 2
- **Question:** Can the "context void" methodology generalize to reveal biases in domains outside of academia or in alternative cross-modal tasks?
- **Basis in paper:** [explicit] The future work section states an intention to expand the framework to tasks beyond CV-to-headshot generation, such as creating stories from performance reviews.
- **Why unresolved:** The current study is a pilot limited strictly to academic CVs, leaving the robustness of the method across diverse industries and task types unproven.
- **What evidence would resolve it:** Successful replication of the methodology using non-academic datasets (e.g., industry resumes) or different input-output pairings (e.g., text-to-lifestyle portraits).

### Open Question 3
- **Question:** Does explicitly separating the prompt generation phase from the image synthesis phase improve the ability to attribute specific biases to distinct model components?
- **Basis in paper:** [inferred] The authors list the uncertainty of the internal pipeline as a limitation and suggest separating the prompt process in future work to ensure greater clarity.
- **Why unresolved:** The opacity of the ChatGPT system made it difficult to verify if the generated prompt was exactly what DALL-E received, complicating the precise localization of bias.
- **What evidence would resolve it:** A study comparing integrated systems against decoupled pipelines to see if separation allows for more precise error attribution and mitigation.

## Limitations
- Closed-source models prevent verification of intermediate steps and limit reproducibility
- Sample size (n=40 images) and academic CV domain restriction limit generalizability
- Speculative nature of the task may not reflect practical AI system behavior
- Black-box nature prevents inspection of internal prompt modifications

## Confidence
- **High Confidence**: The qualitative methodology is sound and rigorously applied, with inter-rater reliability (Cohen's Kappa = 0.814) demonstrating consistency. The observation that AI systems make unsupported visual inferences when context is removed is empirically validated.
- **Medium Confidence**: The pattern of male bias in generated images is consistently observed but could be influenced by model version, prompt formulation, or the specific academic domain studied. Cross-domain validation would strengthen these findings.
- **Low Confidence**: The broader claim that speculative design probes reveal "context-independent consentive risks" overlooked by conventional methods requires additional case studies across different AI systems and task types to establish as a general principle.

## Next Checks
1. **Cross-domain replication**: Apply the same context void methodology to industry CVs, creative portfolios, and technical resumes to determine if academia-specific gender bias patterns generalize.
2. **Prompt engineering control**: Test whether explicitly specifying demographic attributes in prompts eliminates or reduces the observed bias, helping distinguish between learned associations and default generation patterns.
3. **Model version comparison**: Repeat the study with different GPT-4 and DALL-E model versions to assess whether observed biases are persistent across updates or reflect specific training snapshots.