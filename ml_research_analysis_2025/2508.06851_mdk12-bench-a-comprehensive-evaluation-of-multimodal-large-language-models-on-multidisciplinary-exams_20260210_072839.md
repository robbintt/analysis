---
ver: rpa2
title: 'MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models
  on Multidisciplinary Exams'
arxiv_id: '2508.06851'
source_url: https://arxiv.org/abs/2508.06851
tags:
- question
- answer
- knowledge
- evaluation
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MDK12-Bench is a large-scale multimodal benchmark built from real
  K-12 exams to comprehensively evaluate MLLMs across six disciplines, 141K instances,
  and 6,225 knowledge points. It introduces dynamic evaluation with controlled visual
  and textual perturbations to assess model generalization and mitigate data contamination.
---

# MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams

## Quick Facts
- **arXiv ID**: 2508.06851
- **Source URL**: https://arxiv.org/abs/2508.06851
- **Reference count**: 40
- **Primary result**: MLLMs show significant performance drops on harder and newer exams, with dynamic perturbations reducing accuracy by 13.7% and KP-RAG yielding only modest gains on reasoning-intensive tasks.

## Executive Summary
MDK12-Bench is a large-scale multimodal benchmark built from real K-12 exams to comprehensively evaluate MLLMs across six disciplines, 141K instances, and 6,225 knowledge points. It introduces dynamic evaluation with controlled visual and textual perturbations to assess model generalization and mitigate data contamination. The benchmark supports knowledge-point referenced generation (KP-RAG) to examine the role of structured knowledge in reasoning. Evaluations reveal significant performance drops on harder and newer exams, with dynamic perturbations reducing accuracy by 13.7%, and KP-RAG yielding only modest gains on reasoning-intensive tasks. These results highlight the limitations of current MLLMs in perception, reasoning, and robustness.

## Method Summary
MDK12-Bench is constructed from real K-12 exam data spanning 2016-2025 across six disciplines, processed through rule-based filtering, GPT-based refinement, and educator manual validation to produce 141.3K final instances. Questions are annotated with hierarchical knowledge points across six taxonomy levels and difficulty labels. The benchmark supports three evaluation modes: standard evaluation, dynamic evaluation with controlled visual/textual perturbations, and KP-RAG using structured knowledge retrieval. Dataset variants include MDK12-Full (141K) and MDK12-Mini (14.8K) for efficient iteration.

## Key Results
- Dynamic evaluation reduces MLLM performance by an average of 13.7% compared to standard evaluation
- KP-RAG improves accuracy by 6.9%, 6.0%, and 2.1% on easy, medium, and hard exams respectively
- Cross-year evaluation shows accuracy gaps of 12.3%, 13.4%, and 11.6% between oldest and newest exams for easy, medium, and hard levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic perturbation-based evaluation exposes generalization failures and mitigates data contamination in static benchmarks.
- Mechanism: The framework applies controlled visual transforms (spatial padding, color inversion, style transfer) and textual transforms (word substitution, paraphrasing, question-type conversion) to generate novel test instances while preserving semantic correctness. A GPT-based judge validates that transforms maintain answer validity.
- Core assumption: Models overfit to static pretraining distributions; unfamiliar but semantically equivalent inputs reveal true reasoning gaps.
- Evidence anchors:
  - [abstract] "dynamic evaluation with controlled visual and textual perturbations to assess model generalization and mitigate data contamination"
  - [section: Experiments] "dynamic evaluation reduces model performance by an average of 13.7%"
  - [corpus] Weak corpus validation; related benchmarks focus on static evaluation without perturbation frameworks.
- Break condition: If models achieve comparable accuracy on standard and dynamic sets, the mechanism may not be detecting meaningful generalization gaps, or transforms may be too conservative.

### Mechanism 2
- Claim: Hierarchical knowledge-point augmentation (KP-RAG) primarily aids knowledge-retrieval tasks but offers limited benefit for reasoning-intensive problems.
- Mechanism: Questions are enriched with fine-grained knowledge points retrieved from a six-layer taxonomy (discipline → grade → subfield → curriculum → topic → knowledge point). Models receive structured hints alongside questions.
- Core assumption: Explicit knowledge exposure enables better reasoning; performance gains indicate knowledge gaps while lack of gains indicates reasoning bottlenecks.
- Evidence anchors:
  - [abstract] "KP-RAG yielding only modest gains on reasoning-intensive tasks"
  - [section: KP-RAG Evaluation Results] "KP-RAG improves accuracy by 6.9%, 6.0%, and 2.1% on easy, medium, and hard exams, respectively"
  - [section: Discussion] "larger gains on easy and medium exams likely arise because these questions are less reasoning-intensive"
  - [corpus] No direct corpus validation for KP-RAG specifically; related work focuses on general RAG without hierarchical knowledge structures.
- Break condition: If hard-task accuracy increases substantially with KP-RAG, the reasoning-bottleneck hypothesis is incorrect.

### Mechanism 3
- Claim: Temporal and difficulty stratification reveals distributional shift sensitivity in MLLMs.
- Mechanism: Exam instances include year annotations (2016–2025) and difficulty labels, enabling controlled evaluation of performance degradation on newer and harder questions.
- Core assumption: Newer exams introduce concepts or phrasings absent from training corpora; difficulty correlates with reasoning depth.
- Evidence anchors:
  - [abstract] "significant performance drops on harder and newer exams"
  - [section: Cross-Year Evaluation] "accuracy gaps of 12.3%, 13.4%, and 11.6% between the oldest and newest exams for easy, medium, and hard levels"
  - [section: Introduction] "accuracy drops by 8.3% on harder and 12.6% on newer exams"
  - [corpus] Related benchmarks (MME-Reasoning, COUNTS) note distribution shift issues but lack fine-grained temporal annotations.
- Break condition: If models trained on web-scale data show no temporal degradation, newer exams may already be in pretraining data (contamination).

## Foundational Learning

- Concept: **Multimodal Large Language Model (MLLM) Architecture**
  - Why needed here: Understanding how vision encoders project into language model embedding spaces clarifies why visual perturbations disrupt reasoning chains.
  - Quick check question: Can you explain why color inversion or style transfer might break visual token representations without changing semantic content?

- Concept: **Benchmark Contamination and Distribution Shift**
  - Why needed here: The paper's core motivation is that static benchmarks become unreliable when test data enters training corpora.
  - Quick check question: Why does dynamic perturbation help mitigate contamination while still testing the same underlying skills?

- Concept: **Hierarchical Knowledge Taxonomies**
  - Why needed here: The six-layer taxonomy (141K instances → 6,225 knowledge points) enables fine-grained diagnostic evaluation and KP-RAG retrieval.
  - Quick check question: How would you determine whether a model's failure stems from missing knowledge vs. failed reasoning given this taxonomy?

## Architecture Onboarding

- Component map:
  - Data curation pipeline: Rule-based → GPT-based → Manual educator → Post-processing (141.3K final instances)
  - Knowledge taxonomy: 6 levels (Discipline → Grade → Subfield → Curriculum → Topic → Knowledge Point)
  - Evaluation methods: Standard, Dynamic (6 transforms), KP-RAG
  - Dataset variants: MDK12-Full (141K) and MDK12-Mini (14.8K, stratified by difficulty)

- Critical path:
  1. Load MDK12-Mini for rapid iteration (10% of full set, balanced difficulty)
  2. Run standard evaluation to establish baseline
  3. Apply dynamic evaluation with subset of transforms to identify robustness gaps
  4. Optional: KP-RAG evaluation for knowledge-gap diagnosis

- Design tradeoffs:
  - MDK12-Mini enables faster evaluation but may miss rare knowledge points (long-tail distribution noted in supplement)
  - GPT-Judge scoring offers semantic flexibility but introduces API dependency; exact matching is faster but brittle for open-ended answers
  - Dynamic evaluation increases test diversity but requires additional compute for transform generation and validation

- Failure signatures:
  - Large standard-vs-dynamic accuracy gap (>15%) → poor generalization, likely overfitting
  - KP-RAG gains minimal on hard tasks (<3%) → reasoning bottleneck, not knowledge gap
  - Temporal accuracy decline >10% on newer exams → possible training data cutoff or contamination in older data

- First 3 experiments:
  1. **Baseline establishment**: Evaluate target MLLM on MDK12-Mini across all six disciplines with standard evaluation; document per-discipline and per-difficulty accuracy.
  2. **Dynamic robustness test**: Apply all six transforms to a 50% subset; compare accuracy degradation patterns to identify whether textual or visual perturbations cause larger drops (paper finds textual more harmful).
  3. **KP-RAG diagnostic**: For questions where baseline accuracy is low, test whether KP-RAG improves performance; stratify results by difficulty to distinguish knowledge gaps (improved) from reasoning gaps (unchanged).

## Open Questions the Paper Calls Out
None

## Limitations
- GPT-based judge for dynamic evaluation introduces potential bias and API dependency, though exact matching is available as an alternative
- The taxonomy's 141K instances may not capture all edge cases in K-12 knowledge, particularly in long-tail topics
- Modest KP-RAG gains might understate knowledge's role if retrieval quality or prompt engineering could be improved

## Confidence
- **High**: Benchmark construction and contamination mitigation mechanisms, given detailed taxonomy and validation procedures
- **Medium**: Dynamic evaluation findings, as perturbation transforms may not fully capture real-world distribution shifts
- **Low**: KP-RAG evaluation results, since modest gains could reflect implementation choices rather than fundamental limitations

## Next Checks
1. Conduct ablation studies on perturbation transforms to determine which specific visual or textual modifications most impact model performance
2. Test whether ensemble knowledge retrieval or improved prompt engineering can boost KP-RAG gains on reasoning-intensive tasks beyond the reported 2-3%
3. Evaluate model performance on MDK12-Bench versus traditional benchmarks to quantify the added value of multidisciplinary, multimodal assessment