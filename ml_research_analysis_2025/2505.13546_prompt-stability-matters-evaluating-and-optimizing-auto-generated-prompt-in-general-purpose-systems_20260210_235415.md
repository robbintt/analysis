---
ver: rpa2
title: 'Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt
  in General-Purpose Systems'
arxiv_id: '2505.13546'
source_url: https://arxiv.org/abs/2505.13546
tags:
- prompt
- stability
- system
- prompts
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces prompt stability as a critical metric for
  evaluating and optimizing auto-generated prompts in general-purpose multi-agent
  systems. The authors propose semantic stability, measured via average pairwise cosine
  similarity of LLM outputs across repeated executions, as a practical proxy for output
  variance.
---

# Prompt Stability Matters: Evaluating and Optimizing Auto-Generated Prompt in General-Purpose Systems

## Quick Facts
- arXiv ID: 2505.13546
- Source URL: https://arxiv.org/abs/2505.13546
- Reference count: 40
- Primary result: Introducing prompt stability as a critical metric for evaluating and optimizing auto-generated prompts in general-purpose multi-agent systems.

## Executive Summary
This paper introduces prompt stability as a critical metric for evaluating and optimizing auto-generated prompts in general-purpose multi-agent systems. The authors propose semantic stability, measured via average pairwise cosine similarity of LLM outputs across repeated executions, as a practical proxy for output variance. They develop Promptor, a stability-aware framework that uses stability feedback to iteratively refine prompts, integrating components like a Prompt Reviewer and Plan Updater. Theoretical analysis establishes that prompt stability reduces deviation between intended and actual system outputs. Empirical results show that Promptor improves task success rates and output consistency across diverse domains (general tasks, biology, chemistry, finance), narrowing the performance gap with domain-specific systems while outperforming general-purpose baselines. Ablation studies confirm the effectiveness of each module, demonstrating that prompt stability is essential for reliable multi-agent coordination.

## Method Summary
The paper proposes a stability-aware framework called Promptor for optimizing auto-generated prompts in general-purpose multi-agent systems. The core idea is to use semantic stability—measured as the average pairwise cosine similarity of LLM outputs across repeated executions—as a practical proxy for output variance. Promptor iteratively refines prompts using stability feedback through two key components: a Prompt Reviewer that evaluates stability and task alignment, and a Plan Updater that adjusts the prompt based on this feedback. Theoretical analysis proves that prompt stability reduces the deviation between intended and actual system outputs. Experiments across diverse domains (general tasks, biology, chemistry, finance) demonstrate that Promptor improves task success rates and output consistency compared to baselines, while ablation studies confirm the effectiveness of each module.

## Key Results
- Semantic stability (average pairwise cosine similarity) effectively proxies for output variance in auto-generated prompts
- Promptor framework improves task success rates and output consistency across general and domain-specific tasks
- Iterative stability-aware refinement with Prompt Reviewer and Plan Updater modules significantly outperforms non-stability baselines

## Why This Works (Mechanism)
The mechanism works by addressing a fundamental problem in general-purpose multi-agent systems: inconsistent outputs across repeated executions due to poorly optimized prompts. By measuring semantic stability through cosine similarity of LLM outputs, the framework provides a quantifiable metric that correlates with practical reliability. The iterative refinement process uses this stability feedback to progressively optimize prompts, reducing variance while maintaining task alignment. The theoretical analysis establishes that lower variance in executor outputs directly translates to smaller deviations from intended system outputs, providing a mathematical foundation for why stability optimization improves overall system reliability.

## Foundational Learning
**Semantic Stability** - The average pairwise cosine similarity of LLM outputs across repeated executions. *Why needed:* Provides a practical, measurable proxy for output variance when direct variance measurement is infeasible. *Quick check:* Compute average pairwise cosine similarity across 10+ prompt executions.

**Prompt Stability Optimization** - Iterative refinement process that uses stability feedback to improve prompts. *Why needed:* Addresses the inherent instability in auto-generated prompts for general-purpose systems. *Quick check:* Monitor stability score improvement across refinement iterations.

**Executor Output Variance** - The variability in outputs from agent executors given the same prompt. *Why needed:* Directly impacts system reliability and task success rates. *Quick check:* Measure standard deviation of key output metrics across executions.

**Task Alignment Score** - Metric measuring how well refined prompts maintain intended task objectives. *Why needed:* Ensures stability optimization doesn't compromise task performance. *Quick check:* Compare task success rates before and after stability optimization.

**Multi-agent Pipeline Variance** - Accumulated variance across multiple coordinated agents. *Why needed:* Understanding how individual agent stability affects overall system reliability. *Quick check:* Measure correlation between individual agent stability and pipeline success rate.

## Architecture Onboarding

**Component Map**
User Query -> Planner (with Promptor) -> Executor Chain -> Output
Promptor: Stability Evaluator -> Prompt Reviewer -> Plan Updater -> Refined Prompt

**Critical Path**
User Query → Planner (Promptor) → Executor Chain → Output
Where Promptor iteratively refines through Stability Evaluator → Prompt Reviewer → Plan Updater

**Design Tradeoffs**
- Stability vs. Performance: Joint optimization balances consistency with task effectiveness
- Computational Overhead: Iterative refinement increases processing time but improves reliability
- Semantic vs. Exact Match: Cosine similarity captures semantic similarity but may miss functional differences

**Failure Signatures**
- Insufficient Stability: High variance in executor outputs, inconsistent task completion
- Over-optimization: Prompts become too rigid, losing flexibility for task variations
- Threshold Issues: τ set too high causes excessive refinement; too low leaves instability unaddressed

**First Experiments**
1. Measure baseline semantic stability across 10 executions of identical prompts
2. Apply Promptor refinement and track stability improvement per iteration
3. Compare task success rates between stability-optimized and baseline prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does optimizing for prompt stability ever trade off against peak task performance, and if so, how can this trade-off be characterized and managed?
- Basis in paper: [inferred] The paper jointly optimizes stability and planner alignment objectives, but does not analyze scenarios where these objectives may conflict.
- Why unresolved: Experiments show stability improves average performance, but whether highly stable prompts might limit exploration, creativity, or optimal solutions in open-ended tasks remains unexamined.
- What evidence would resolve it: Controlled experiments comparing task success distributions for prompts optimized solely for performance vs. stability-aware optimization, particularly on creative or open-ended tasks.

### Open Question 2
- Question: How well does semantic stability generalize across different LLM architectures and model families?
- Basis in paper: [inferred] All experiments use GPT-4o as the backbone for both Promptor and baselines; cross-model generalization is not tested.
- Why unresolved: Embedding-based stability scores may be model-dependent—prompts stable under one model's sampling distribution could behave differently under models with different tokenization, training, or sampling characteristics.
- What evidence would resolve it: Evaluate the same prompts across multiple model families (e.g., Claude, LLaMA, Gemini) and measure correlation of stability scores and downstream performance.

### Open Question 3
- Question: How should the stability threshold τ be optimally set, and how sensitive are results to this choice?
- Basis in paper: [inferred] The framework uses a predefined threshold τ to trigger prompt revision, but provides no guidance on setting this value or analyzing sensitivity.
- Why unresolved: An inappropriate threshold could cause excessive refinement (wasting compute) or insufficient refinement (leaving instability unaddressed).
- What evidence would resolve it: Ablation experiments varying τ across a range of values, reporting iteration counts, final stability scores, and task success rates.

### Open Question 4
- Question: How do inter-agent dependencies affect the theoretical relationship between prompt stability and system reliability?
- Basis in paper: [explicit] Lemma 1 assumes executor outputs xi are independent, which the paper acknowledges as a simplification.
- Why unresolved: In real multi-agent systems, agents often depend on each other's outputs; correlated variance could amplify or dampen the stability-reliability relationship in ways the current bound does not capture.
- What evidence would resolve it: Theoretical analysis with relaxed independence assumptions, plus empirical measurements of output correlation in multi-step agent pipelines.

## Limitations
- Evaluation limited to specific benchmarks and domains, may not generalize to all real-world scenarios
- Computational overhead of iterative stability refinement not fully characterized
- Semantic stability proxy may not capture all aspects of output variance, particularly in domains with high semantic similarity but functional divergence

## Confidence
- **High**: Theoretical analysis establishing relationship between prompt stability and system reliability
- **High**: Experimental results showing Promptor improves task success rates and output consistency
- **Medium**: Practical impact on real-world applications and scalability across diverse domains
- **Low**: Generalizability of semantic stability metric across different LLM architectures and model families

## Next Checks
1. Validate the framework on a broader range of domains and tasks, including those with high semantic similarity but functional divergence
2. Quantify the computational overhead and scalability of the stability-aware refinement process
3. Conduct user studies to assess the practical impact of improved prompt stability on real-world applications