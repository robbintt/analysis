---
ver: rpa2
title: 'When VLMs Meet Image Classification: Test Sets Renovation via Missing Label
  Identification'
arxiv_id: '2505.16149'
source_url: https://arxiv.org/abs/2505.16149
tags:
- label
- labels
- image
- vlms
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the widespread issue of noisy and missing labels
  in image classification benchmarks. It proposes REVEAL, a framework that leverages
  state-of-the-art vision-language models and human/machine label curation methods
  to detect and correct label errors.
---

# When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification

## Quick Facts
- arXiv ID: 2505.16149
- Source URL: https://arxiv.org/abs/2505.16149
- Reference count: 40
- Primary result: Agreement rates with human judgments up to 0.976 on CIFAR-10 for missing label detection

## Executive Summary
This paper tackles the widespread issue of noisy and missing labels in image classification benchmarks. It proposes REVEAL, a framework that leverages state-of-the-art vision-language models and human/machine label curation methods to detect and correct label errors. By aggregating predictions via weighted voting and refining results with confidence-based filtering, REVEAL generates soft-labeled outputs with likelihoods. Evaluations on six benchmark datasets show strong alignment with human judgments, revealing that in datasets with many classes, multi-label predictions often reflect true co-occurring objects rather than uncertainty.

## Method Summary
REVEAL is a dataset renovation framework that combines predictions from multiple VLMs (BLIP, LLaVA, Janus, Qwen-VL) and traditional label-cleaning methods (Cleanlab, Docta). The framework first creates pseudo ground-truth labels for a small subset via consensus voting, then estimates each method's accuracy on this subset using a penalized formula. These estimated accuracies serve as weights in a weighted voting scheme for the full dataset. After aggregation, a confidence-based threshold filters low-confidence predictions, and remaining scores are normalized via softmax to produce soft labels with likelihood scores. The method uses batched multi-label selection prompting (batch size ~20) with reasoning and image description to balance recall and inference cost.

## Key Results
- Achieves agreement rates up to 0.976 on CIFAR-10 and 0.880 on CIFAR-100 with human annotations
- Identifies missing labels in multi-class datasets, showing that multi-label predictions often reflect true co-occurring objects
- Successfully detects both noisy labels (incorrect single-label assignments) and missing labels (unannotated multi-label co-occurrences) across six benchmark datasets
- Demonstrates that REVEAL's soft-labeled outputs with likelihoods provide a more nuanced representation of label uncertainty

## Why This Works (Mechanism)

### Mechanism 1: VLM Ensemble with Estimated-Accuracy Weighted Voting
Aggregating predictions from multiple VLMs using estimated accuracy weights improves detection of both noisy and missing labels compared to individual models. The framework uses diverse VLMs and traditional methods, creates pseudo ground-truth from a small human-verified subset, estimates each method's accuracy with a penalty for over-prediction, and uses these accuracies as weights in weighted voting. The core assumption is that different models have complementary strengths and biases, making their weighted consensus more reliable than any single model's output.

### Mechanism 2: Confidence-Based Filtering and Soft-Label Generation
After weighted aggregation, applying a score threshold and generating soft labels filters out low-confidence predictions and provides a nuanced representation of label uncertainty or multi-label presence. Labels below a dataset-specific threshold are discarded, and remaining scores are normalized via softmax to produce likelihoods. The core assumption is that aggregated weighted scores reliably proxy for label correctness likelihood, and a single threshold can effectively separate valid labels from noise for a given dataset.

### Mechanism 3: Prompt Engineering for VLM Label Extraction
The design of the prompt significantly impacts VLM performance, with a batched multi-label selection prompt with reasoning, description, and shuffling providing an optimal trade-off between recall and efficiency. Instead of querying for one label at a time or providing all 100 labels in one prompt, the method batches labels (e.g., 20 at a time), asks the VLM to describe the image, reason about its choice, and select from a shuffled candidate list. This mitigates positional bias and leverages the model's captioning capabilities.

## Foundational Learning

**Confident Learning**
- Why needed here: Underpins component methods (Cleanlab) and the general problem of finding label errors; understanding how to estimate the joint distribution of noisy vs. true labels is crucial for grasping how REVEAL identifies noisy labels
- Quick check question: How does Confident Learning estimate which labels are likely noisy without knowing the true noise rate?

**Multi-Label vs. Multi-Class Classification with Missing Labels**
- Why needed here: The paper distinguishes between noisy labels (wrong label) and missing labels (incomplete set of labels); understanding this difference is key to the paper's core contribution of "renovation" which addresses both
- Quick check question: In a dataset labeled only for "cat", what is the label status if an image contains both a "cat" and a "dog"?

**Model Ensembling and Weighted Voting**
- Why needed here: The REVEAL framework is built on aggregating outputs from multiple models; understanding how and why to weight different models based on their estimated performance is central to the proposed architecture
- Quick check question: Why might simple majority voting fail if some models in the ensemble are much noisier than others?

## Architecture Onboarding

**Component map:**
Raw images and labels -> VLM & Traditional Labelers (BLIP, LLaVA, Janus, Qwen, Cleanlab, Docta) -> Pseudo Ground-Truth Estimator (creates pseudo ground truth and estimates accuracy) -> Weighted Aggregator (combines predictions with accuracy weights) -> Filter & Soft Label Generator (thresholds and softmax normalizes) -> Output (renovated dataset with soft labels)

**Critical path:** The critical path for performance is the Quality of the Pseudo Ground-Truth -> Accuracy Estimation -> Weighted Aggregation. If the initial human-verified sample is too small or biased, the estimated model accuracies will be incorrect, leading to poor weighting and a flawed final renovation.

**Design tradeoffs:**
- Prompting Strategy: Binary vs. Direct vs. Batched. The paper chooses Batched (size ~20) to balance recall and inference cost. Increasing batch size improves speed but reduces recall
- Filtering Threshold: A higher threshold increases precision but may miss subtle missing labels (lower recall). This is a dataset-specific hyperparameter
- Model Selection: Adding more VLMs increases cost and complexity but can improve ensemble robustness, provided they are not all biased in the same way

**Failure signatures:**
- Hallucination/Over-prediction: VLMs may generate semantically plausible but visually absent labels. This is mitigated by the accuracy penalty and ensemble voting
- Repetitive Outputs: Some VLMs (e.g., Janus) get stuck in loops. This indicates a model failure that should be caught by the accuracy estimator (down-weighting that model)
- Poor Performance on Fine-Grained Classes: VLMs often fail to distinguish between similar sub-classes. This signals a limitation of the VLM labelers themselves

**First 3 experiments:**
1. Run a single VLM on a small subset: Select 100 images from CIFAR-100 and run LLaVA using the proposed "Batched Multi-Label Selection" prompt. Analyze the output for the 10 observations (e.g., fine-grained failure, repetition)
2. Estimate model accuracies: Using a small, human-verified subset, run all component models and calculate their estimated accuracy using the penalized formula. Compare these weights to the paper's reported values in Table 1
3. Build a mini-REVEAL pipeline: Implement the weighted voting and thresholding logic for a small dataset slice. Aggregate the outputs from experiment #2, apply a threshold, and generate soft labels. Compare the final renovated labels against the original labels and the human ground truth to measure the agreement rate

## Open Questions the Paper Calls Out

**Open Question 1:** How can dataset renovation methods effectively handle valid predictions for objects that fall outside a dataset's predefined label set?
- Basis in paper: Appendix A.2 notes that in real-world scenarios, images contain concepts not included in the fixed label set, causing semantically reasonable VLM predictions to be incorrectly evaluated as errors
- Why unresolved: REVEAL currently operates on a closed-set assumption, relying on fixed candidate lists to aggregate scores, making it unable to validate or incorporate "open-vocabulary" findings
- What evidence would resolve it: A framework capable of dynamic label expansion or a modified evaluation metric that rewards semantically accurate "out-of-domain" predictions without penalizing the model

**Open Question 2:** How can human verification protocols be scaled to accurately validate missing labels in datasets with thousands of classes?
- Basis in paper: Observation 9 and Appendix A.2 highlight that MTurk results for ImageNet and QuickDraw were unreliable because annotators were constrained to validating only two candidate labels, failing to capture the full scope of missing labels
- Why unresolved: There is a lack of scalable, cost-effective human-in-the-loop methodologies that allow annotators to navigate large label spaces without being overwhelmed
- What evidence would resolve it: A new annotation protocol design that successfully identifies missing labels in high-cardinality datasets with high inter-annotator agreement

**Open Question 3:** How can we formally quantify the "cognitive boundaries" and perceptual alignment between VLMs and human judgment in ambiguous visual contexts?
- Basis in paper: Appendix A.1 states that comparing REVEAL outputs to human annotations "opens a promising direction for studying the cognitive boundaries of alignment between vision language models and human perception"
- Why unresolved: While the paper identifies systematic divergences, a formal metric or theoretical model explaining these perceptual gaps is not established
- What evidence would resolve it: A study correlating specific VLM failure modes with human cognitive biases using a standardized alignment metric

## Limitations

- The prompt templates for the final batched multi-label selection are not fully specified in the main text, though examples are provided in the appendix
- The voting threshold k for pseudo ground-truth construction is unspecified, which could significantly impact the results
- The potential for systematic biases in VLMs that are not corrected by the ensemble approach is not fully addressed

## Confidence

**High Confidence:** The core finding that VLMs can identify both noisy and missing labels in image classification benchmarks is well-supported by the experimental results with agreement rates up to 0.976 on CIFAR-10

**Medium Confidence:** The effectiveness of the weighted voting mechanism based on estimated accuracies is reasonably well-supported, though the exact implementation details (voting threshold, regularization interpretation) introduce some uncertainty in reproducibility

**Low Confidence:** The specific prompt engineering strategy is supported by Figure 4 showing the trade-off between batch size and recall, but the exact prompt templates and their relative importance are not fully specified

## Next Checks

1. Verify Accuracy Estimation Reproducibility: Implement the pseudo ground-truth construction using the first 100 images from CIFAR-100 and compare the estimated model accuracies against the values reported in Table 1. Test how sensitive the final results are to different voting thresholds (k values).

2. Test Prompt Template Robustness: Create and test multiple variations of the batched multi-label selection prompt using the components described (image description, reasoning, label shuffling). Evaluate how different prompt formulations affect label quality and model agreement rates.

3. Analyze Class-Specific Performance: Examine per-class prediction accuracy across all VLMs and datasets to identify systematic biases (e.g., over-prediction of "bird" or "cat" categories). Validate whether the accuracy penalty mechanism effectively downweights models that consistently over-predict certain classes.