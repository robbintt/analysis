---
ver: rpa2
title: 'Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large
  Language Model Agents'
arxiv_id: '2511.04076'
source_url: https://arxiv.org/abs/2511.04076
tags:
- agentmandering
- partisan
- bias
- agent
- redistricting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Agentmandering, a game-theoretic framework
  for fair redistricting using large language model (LLM) agents. The method reimagines
  redistricting as a turn-based negotiation between partisan agents, implementing
  the Choose-and-Freeze protocol where agents alternate between selecting and freezing
  districts from candidate maps.
---

# Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents

## Quick Facts
- arXiv ID: 2511.04076
- Source URL: https://arxiv.org/abs/2511.04076
- Reference count: 16
- Primary result: LLM-based agents achieve 2-3 orders of magnitude lower variance than baselines while reducing partisan bias in redistricting

## Executive Summary
Agentmandering introduces a game-theoretic framework that uses LLM agents to produce fairer redistricting maps through a structured Choose-and-Freeze negotiation protocol. The framework reimagines redistricting as a turn-based game where partisan agents alternate between selecting and freezing districts from candidate maps, gradually partitioning states through constrained and interpretable choices. Evaluation across all states using 2020 Census data demonstrates significant improvements in fairness metrics while maintaining computational efficiency and procedural transparency.

## Method Summary
The framework implements a Choose-and-Freeze protocol where two adversarial LLM agents (representing Republican and Democratic interests) negotiate redistricting maps through alternating selection and freezing rounds. Each round uses ReCom to generate candidate maps, with the choosing agent selecting one map and the freezing agent permanently locking a district. This process continues until the entire state is partitioned. Agents are prompted with state-specific political profiles and instructed to maximize their party's advantage while respecting legal constraints. The method achieves fairness through mutual constraint rather than ensemble sampling, producing results with 2-3 orders of magnitude lower variance than traditional baselines.

## Key Results
- Achieves 2-3 orders of magnitude lower variance than ReCom, Flip, Merge-Split, and SMC baselines
- Reduces partisan bias and unfairness significantly compared to enacted plans and traditional methods
- LLM-based agents outperform rule-based approaches at strategic decision-making within the constrained protocol

## Why This Works (Mechanism)

### Mechanism 1: Choose-and-Freeze Protocol Constrains Strategic Manipulation
The alternating choose-and-freeze structure ensures neither agent can unilaterally control the final map. Each agent has agency but faces adversarial counterplay, creating a balanced strategic environment. This mutual constraint prevents cherry-picking from large ensembles and commits to irreversible decisions that constrain the outcome space.

### Mechanism 2: LLM Agents Provide Superior Strategic Reasoning Over Rule-Based Approaches
LLM agents integrate state-specific political profiles to make nuanced tradeoffs that rule-based heuristics cannot capture. The agents can sacrifice short-term advantages for long-term gains and respond to complex geographic and demographic patterns, leading to more politically effective decisions within the protocol constraints.

### Mechanism 3: Iterative Partitioning Reduces Variance Compared to Ensemble Sampling
The sequential, constrained nature of the protocol produces outcomes with dramatically lower variance than traditional ensemble methods. By committing to irreversible decisions at each round, the framework eliminates the opportunity for post-hoc manipulation that exists when selecting from large map ensembles.

## Foundational Learning

- **Concept: Fair Division / Cake-Cutting Protocols**
  - Why needed here: The Choose-and-Freeze protocol derives from classical fair division theory; understanding envy-freeness and equilibrium is essential to grasp why alternating choices produce balanced outcomes.
  - Quick check question: Why does giving each party both selection power and freezing power create a strategic equilibrium?

- **Concept: Redistricting Metrics (Population Deviation, Polsby-Popper, Partisan Bias, Unfairness)**
  - Why needed here: The paper evaluates fairness using these four orthogonal metrics; interpreting results requires knowing what each measures and why they may conflict.
  - Quick check question: If two districting plans have identical Polsby-Popper scores, can they still produce different partisan outcomes? Why?

- **Concept: LLM Agent Prompting for Strategic Roles**
  - Why needed here: Agentmandering's effectiveness depends on prompts that elicit realistic partisan behavior; poorly designed prompts could cause agents to ignore strategic context or act incoherently.
  - Quick check question: What key elements should a prompt include to make an LLM act as a Democratic agent in a swing state?

## Architecture Onboarding

- **Component map:**
  - Materials: Two partisan agents (Republican A_R, Democratic A_D), state-specific political profiles P_state, district information
  - Protocol: Choose-and-Freeze game loop, alternating agents over N rounds
  - Choose Mechanism: Candidate generator G(Â·) produces c maps; choosing agent selects via LLM-based f_choose
  - Freeze Mechanism: Freezing agent evaluates districts via LLM-based f_freeze; one district permanently fixed

- **Critical path:**
  1. Initialize R_0 = full state
  2. For each round n = 1 to N: Generate c candidate maps, choosing agent selects, freezing agent freezes, update remaining region
  3. Return final partition

- **Design tradeoffs:**
  - Candidate set size (c): Larger sets improve fairness but increase compute; diminishing returns beyond c = 100
  - LLM selection: 9 models tested show similar unfairness, suggesting protocol robustness
  - Agent ordering: Marginal impact on fairness; protocol symmetry reduces first-mover advantage
  - Candidate generator: Recom and Flip produce similar fairness; Flip slightly better on population deviation

- **Failure signatures:**
  - High outcome variance: Suggests stochastic agent behavior or poor candidate map quality
  - Extreme partisan bias: Indicates asymmetric agent capabilities or LLM political bias
  - Non-contiguous or unbalanced districts: Points to candidate generator failure or constraint violation
  - Unstable results across runs: Check Table 3 for expected stability

- **First 3 experiments:**
  1. Baseline comparison on Pennsylvania: Run Agentmandering (17 districts, c = 100, Gemini 2.5 Pro, 10 repeats) and compare mean/variance of fairness metrics against Recom and CD-2020
  2. Ablate LLM reasoning vs rule-based: Replace LLM-based f_choose/f_freeze with partisan-bias-maximization rules; expect Unfairness to increase from ~0.35 to ~0.42
  3. Test asymmetric agent capabilities: Run heterogeneous configuration (Gemini 2.5 Pro vs GPT-4o-mini) and compare Unfairness to symmetric configuration

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the framework be adapted for multi-party electoral systems? The current protocol is structurally limited to binary competition, and future work will investigate multi-party fairness definitions.
- **Open Question 2:** Can the framework improve geometric compactness without sacrificing low variance and fairness? Current results show slightly lower Polsby-Popper scores due to irregular boundaries, but it's unclear if this is necessary or a limitation of the candidate generator.
- **Open Question 3:** Is the protocol robust to extreme asymmetry in agent capabilities? While heterogeneous-agent experiments show stability, the paper notes slight fairness increases with capability disparities and doesn't define failure thresholds.

## Limitations
- LLM political neutrality remains uncertain despite robustness across 9 models tested
- Protocol may degrade with significant agent capability asymmetry, though exact thresholds are unclear
- Variance reduction could partly reflect constrained protocol rather than superior agent decision-making

## Confidence
- **High confidence:** Variance reduction claims, basic protocol implementation, comparison against established baselines
- **Medium confidence:** Effectiveness of LLM strategic reasoning vs rule-based approaches, protocol robustness to model choice
- **Low confidence:** Long-term stability under repeated redistricting cycles, resistance to sophisticated adversarial manipulation

## Next Checks
1. **Bias stress test:** Run Agentmandering with intentionally biased LLM configurations (conservative-leaning vs liberal-leaning prompts) to quantify how asymmetric agent capabilities affect fairness outcomes
2. **Temporal stability analysis:** Apply Agentmandering to the same state multiple times over simulated decades to test whether outcomes converge or exhibit drift under changing demographics
3. **Adversarial robustness test:** Implement a sophisticated adversarial agent that deliberately creates "poison pills" - strategically disadvantageous districts that constrain future choices - to assess protocol resilience