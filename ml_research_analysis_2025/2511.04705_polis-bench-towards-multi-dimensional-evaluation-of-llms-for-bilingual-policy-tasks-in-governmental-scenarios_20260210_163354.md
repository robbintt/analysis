---
ver: rpa2
title: 'POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy
  Tasks in Governmental Scenarios'
arxiv_id: '2511.04705'
source_url: https://arxiv.org/abs/2511.04705
tags:
- policy
- evaluation
- tasks
- task
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: POLIS-Bench introduces the first systematic evaluation suite for
  large language models in governmental bilingual policy tasks. It addresses key gaps
  in existing benchmarks by constructing an up-to-date bilingual (Chinese/English)
  policy corpus, designing three scenario-grounded tasks (Clause Retrieval & Interpretation,
  Solution Generation, and Compliance Judgment), and establishing a dual-metric evaluation
  framework combining semantic similarity and LLMJudge accuracy.
---

# POLIS-Bench: Towards Multi-Dimensional Evaluation of LLMs for Bilingual Policy Tasks in Governmental Scenarios

## Quick Facts
- arXiv ID: 2511.04705
- Source URL: https://arxiv.org/abs/2511.04705
- Authors: Tingyue Yang; Junchi Yao; Yuhui Guo; Chang Liu
- Reference count: 40
- Primary result: First systematic evaluation suite for LLMs in governmental bilingual policy tasks

## Executive Summary
POLIS-Bench introduces a comprehensive benchmark for evaluating large language models on bilingual (Chinese/English) policy tasks in governmental scenarios. The benchmark addresses critical gaps in existing evaluations by providing an up-to-date policy corpus and three scenario-grounded tasks: Clause Retrieval & Interpretation, Solution Generation, and Compliance Judgment. The evaluation framework combines semantic similarity metrics with LLMJudge accuracy assessments. Large-scale testing of over 10 state-of-the-art LLMs demonstrates that reasoning models show superior cross-task stability, particularly excelling in compliance tasks. Notably, task-aligned fine-tuning of lightweight open-source models achieves performance parity or superiority compared to proprietary baselines while maintaining lower deployment costs.

## Method Summary
POLIS-Bench constructs a bilingual policy corpus and defines three core tasks: Clause Retrieval & Interpretation (extracting and explaining relevant policy clauses), Solution Generation (creating compliant policy solutions), and Compliance Judgment (evaluating policy adherence). The benchmark employs a dual-metric evaluation framework combining semantic similarity measures with LLMJudge accuracy assessments. The evaluation encompasses over 10 state-of-the-art LLMs, including both proprietary and open-source models. Fine-tuning experiments demonstrate that lightweight open models can achieve competitive performance through task-specific adaptation while preserving general reasoning capabilities.

## Key Results
- Reasoning models exhibit superior cross-task stability and accuracy, especially in compliance tasks
- Task-aligned fine-tuning of lightweight open-source models achieves parity with or surpasses strong proprietary baselines
- The dual-metric evaluation framework (semantic similarity + LLMJudge accuracy) provides comprehensive performance assessment

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its focused domain specialization on governmental policy tasks, which requires both language understanding and reasoning capabilities. By concentrating on bilingual policy documents and real-world governmental scenarios, POLIS-Bench creates a high-signal evaluation environment that distinguishes between models' general language abilities and their capacity for domain-specific reasoning. The combination of semantic similarity metrics and LLMJudge accuracy provides complementary evaluation perspectives that capture both factual correctness and nuanced policy interpretation.

## Foundational Learning
- Bilingual policy corpus construction: Needed to ensure evaluation relevance across Chinese and English governmental contexts; quick check: corpus coverage of diverse policy domains
- Dual-metric evaluation framework: Required to capture both factual accuracy and interpretive nuance; quick check: metric correlation and independence
- Three-task scenario design: Essential for comprehensive assessment of policy reasoning capabilities; quick check: task interdependency and coverage
- Reasoning model advantages: Critical for understanding which model architectures excel in policy tasks; quick check: cross-task performance consistency
- Fine-tuning transfer learning: Important for practical deployment considerations; quick check: performance retention on general tasks

## Architecture Onboarding
Component map: Corpus -> Task Design -> Model Evaluation -> Fine-tuning Pipeline
Critical path: Corpus construction → Task definition → Model selection → Evaluation framework → Fine-tuning optimization
Design tradeoffs: Specialized policy focus vs. general applicability, bilingual complexity vs. monolingual simplicity, semantic similarity vs. LLMJudge accuracy
Failure signatures: Poor clause retrieval indicates weak document understanding, inaccurate compliance judgment suggests reasoning limitations, fine-tuning degradation shows transfer learning issues
First experiments: 1) Baseline model performance on individual tasks, 2) Cross-task consistency analysis, 3) Fine-tuning impact assessment on general reasoning capabilities

## Open Questions the Paper Calls Out
The benchmark's generalizability beyond Chinese/English governmental policy domains remains uncertain. The focus on regulatory documents and compliance scenarios may not capture the full spectrum of policy-making tasks or different governmental contexts. The reliance on specific evaluation metrics (semantic similarity and LLMJudge) introduces potential measurement biases that could affect the interpretation of model performance, particularly for nuanced policy interpretation tasks where ground truth may be subjective.

## Limitations
- Limited generalizability beyond Chinese/English governmental policy domains
- Potential measurement biases from specific evaluation metrics
- Uncertainty about practical significance of performance gaps for real-world deployment
- Domain adaptation challenges when encountering policy documents outside the training corpus

## Confidence
- High confidence in benchmark construction methodology and data collection process
- Medium confidence in cross-model performance rankings, as results depend on evaluation metric calibration
- Medium confidence in fine-tuning transfer claims, given the specific policy domain focus
- Low confidence in absolute performance thresholds without external validation on related governmental tasks

## Next Checks
1. External validation on policy documents from different governmental jurisdictions and time periods to test temporal and geographical generalization
2. Comparison of POLIS-Bench results with human expert evaluations on the same policy tasks to establish ground truth alignment
3. Analysis of model performance degradation when applied to closely related but out-of-domain policy scenarios (e.g., corporate compliance vs. governmental regulation)