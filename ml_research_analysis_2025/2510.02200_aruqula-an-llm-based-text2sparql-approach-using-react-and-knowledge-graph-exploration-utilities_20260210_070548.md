---
ver: rpa2
title: ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph
  Exploration Utilities
arxiv_id: '2510.02200'
source_url: https://arxiv.org/abs/2510.02200
tags:
- sparql
- knowledge
- text2sparql
- graph
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ARUQULA, a Text2SPARQL approach based on
  the ReAct methodology and knowledge graph exploration utilities. ARUQULA extends
  SPINACH to work with RDF-based knowledge graphs beyond Wikidata, using LangGraph
  for the ReAct implementation, Qlever for SPARQL querying, and Qdrant for hybrid
  vector search.
---

# ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities

## Quick Facts
- arXiv ID: 2510.02200
- Source URL: https://arxiv.org/abs/2510.02200
- Reference count: 40
- Primary result: LLM-based Text2SPARQL approach using ReAct and knowledge graph exploration utilities

## Executive Summary
ARUQULA is an LLM-based Text2SPARQL approach that extends SPINACH to work with RDF-based knowledge graphs beyond Wikidata. The system employs a dual-strategy semantic grounding approach using hybrid vector search for schema entities and full-text search for named entity resolution. It was evaluated in the Text2SPARQL Challenge, demonstrating average query generation times of 51.44 seconds for DBpedia-English and 59.62 seconds for the Corporate dataset, with average agent steps of 8.26 and 9.92 respectively.

## Method Summary
ARUQULA extends SPINACH to work with RDF-based knowledge graphs beyond Wikidata, using LangGraph for the ReAct implementation, Qlever for SPARQL querying, and Qdrant for hybrid vector search. The approach employs a dual-strategy for semantic grounding, using hybrid vector search for schema entities and full-text search for named entity resolution. The system follows a sequential agent workflow that starts with entity searches, proceeds to property exploration, and culminates in SPARQL query execution, with frequent stop actions to manage the exploration process.

## Key Results
- Average query generation time: 51.44 seconds for DBpedia-English and 59.62 seconds for Corporate dataset
- Average agent steps: 8.26 for DBpedia-English and 9.92 for Corporate dataset
- Stop actions called approximately 200 times across 250 queries

## Why This Works (Mechanism)
ARUQULA's effectiveness stems from its dual-strategy semantic grounding approach that combines hybrid vector search for schema entities with full-text search for named entity resolution. The ReAct methodology enables iterative exploration and refinement of queries through a structured agent workflow. The system's modular architecture, leveraging specialized tools like LangGraph, Qlever, and Qdrant, allows for efficient handling of different aspects of the Text2SPARQL conversion process while maintaining flexibility for different knowledge graph domains.

## Foundational Learning
1. **ReAct Methodology** - Why needed: Enables interactive reasoning and action-taking for complex query generation. Quick check: Verify agent can correctly interpret user queries and take appropriate actions.
2. **Hybrid Vector Search** - Why needed: Combines semantic and keyword-based search for comprehensive entity resolution. Quick check: Test search accuracy across different entity types and query patterns.
3. **Knowledge Graph Exploration** - Why needed: Allows systematic navigation of schema entities and relationships. Quick check: Validate exploration path covers relevant entities and properties.
4. **SPARQL Query Generation** - Why needed: Translates natural language queries into executable graph queries. Quick check: Confirm generated queries return expected results on test datasets.
5. **LangGraph Framework** - Why needed: Provides structured agent workflow management. Quick check: Monitor agent state transitions and decision-making process.
6. **Full-text Search Integration** - Why needed: Complements vector search for named entity resolution. Quick check: Evaluate search precision and recall for entity mentions.

## Architecture Onboarding

Component Map: User Query -> LangGraph Agent -> Hybrid Vector Search (Schema) + Full-text Search (Entities) -> Qlever (SPARQL Execution) -> Results

Critical Path: The core workflow involves parsing the user query, performing semantic grounding through hybrid search strategies, exploring the knowledge graph to identify relevant entities and properties, generating SPARQL queries, and executing them through Qlever.

Design Tradeoffs: The system trades execution speed for accuracy by employing iterative exploration rather than direct query generation. The dual-search strategy balances computational efficiency with comprehensive entity coverage.

Failure Signatures: Common failures include incorrect entity resolution due to ambiguous mentions, schema exploration getting stuck in irrelevant branches, and SPARQL generation errors from complex query patterns. The system typically recovers through stop actions and backtracking.

First Experiments:
1. Test basic entity resolution with simple queries to validate search functionality
2. Evaluate schema exploration with predefined property paths to check navigation logic
3. Verify SPARQL generation and execution with straightforward query patterns

## Open Questions the Paper Calls Out
The paper does not explicitly identify specific open questions, but the evaluation suggests areas for further investigation, particularly around performance optimization and accuracy improvements for complex queries.

## Limitations
- Limited evaluation scope to single benchmark (Text2SPARQL Challenge)
- No comparative analysis against other state-of-the-art Text2SPARQL approaches
- Focus on execution time rather than accuracy of generated SPARQL queries
- System dependence on specific tools (LangGraph, Qlever, Qdrant) may limit generalizability

## Confidence
Medium
- Approach is well-described and technically sound
- Limited evaluation scope and lack of comparative analysis reduce confidence in relative effectiveness
- No metrics provided for accuracy or quality of generated SPARQL queries

## Next Checks
1. Conduct comparative evaluation against other Text2SPARQL state-of-the-art approaches using the same benchmark
2. Test system on additional knowledge graph datasets beyond DBpedia and Corporate dataset
3. Perform ablation studies to determine contribution of individual components (hybrid vector search, full-text search, ReAct implementation) to overall performance