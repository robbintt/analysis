---
ver: rpa2
title: 'AgenticRed: Optimizing Agentic Systems for Automated Red-teaming'
arxiv_id: '2601.13518'
source_url: https://arxiv.org/abs/2601.13518
tags:
- prompt
- target
- system
- loss
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AGENTICRED, an automated framework that uses
  LLMs to iteratively design and refine red-teaming systems without human intervention.
  Rather than optimizing attacker policies within predefined structures, AGENTICRED
  treats red-teaming as a system design problem, leveraging evolutionary algorithms
  to evolve agentic workflows.
---

# AgenticRed: Optimizing Agentic Systems for Automated Red-teaming

## Quick Facts
- arXiv ID: 2601.13518
- Source URL: https://arxiv.org/abs/2601.13518
- Authors: Jiayi Yuan; Jonathan Nöther; Natasha Jaques; Goran Radanović
- Reference count: 40
- Primary result: Automated framework that discovers red-teaming systems achieving 96-98% ASR on open-weight models and 100% on GPT-3.5-Turbo/4o

## Executive Summary
AgenticRed introduces an automated framework that treats red-teaming as a system design problem rather than optimizing individual attack prompts. Using LLMs as meta-agents, it evolves entire agentic workflows through evolutionary algorithms, achieving state-of-the-art jailbreak success rates. The approach demonstrates that automated system design can discover novel attack strategies and maintain effectiveness across both open-weight and proprietary models.

## Method Summary
AgenticRed employs a meta-agent (GPT-5) to iteratively generate candidate red-teaming systems implemented as executable Python code. Each generation, M candidate systems are proposed and evaluated on a small subset of malicious intents to determine fitness (ASR). The fittest system undergoes comprehensive evaluation on the full search set, then joins an archive that serves as context for subsequent generations. This evolutionary process leverages in-context learning and continues for N generations, with helper functions providing query access to target models and judge evaluations. The framework initializes with domain-specific baselines and returns the best-discovered system.

## Key Results
- Achieves 96-98% ASR on open-weight models (Llama-2-7B, Llama-3-8B) compared to 0-70% for baselines
- Achieves 100% ASR on GPT-3.5-Turbo and GPT-4o
- Strong transferability to proprietary models while maintaining high attack success
- Systems converge on similar attack templates across generations (refusal suppression in 8/10 generations)

## Why This Works (Mechanism)

### Mechanism 1: Evolutionary Selection with Archive-Guided Generation
Iterative selection pressure over agentic system designs produces progressively stronger red-teaming systems. A meta-agent generates M candidate systems per generation; each is evaluated on a small subset of malicious intents; only the highest-ASR system survives and is added to the archive. This archive serves as context for subsequent generations, enabling cumulative improvement through code-level recombination and refinement. Core assumption: fitness signal correlates with held-out test performance and transfers across intents.

### Mechanism 2: Domain-Specific Helper Functions Provide Dense Feedback
Providing the meta-agent with query interfaces to target models and judge functions enables verifiable, quantitative feedback that guides effective system design. The framework supplies `get_response()` and `get_jailbreak_result()` helper functions. Candidate systems can probe the target LLM, receive log-probability scores from the judge, and implement early-exit when jailbreak succeeds. This transforms red-teaming from a pure reasoning task into a grounded optimization problem with measurable outcomes. Core assumption: judge model scores accurately reflect true jailbreak success.

### Mechanism 3: Emergent Strategy Discovery Through Code-Space Search
Representing red-teaming systems as executable code enables the meta-agent to discover and combine strategies beyond explicit prompting of individual techniques. Rather than optimizing attack prompts directly, AgenticRed evolves Python code implementing entire workflows. The meta-agent draws on its pretraining knowledge of red-teaming literature to synthesize strategies like refusal suppression, reward shaping, and evolutionary crossover—without these being explicitly programmed. Core assumption: meta-agent's pretraining corpus contains sufficient red-teaming/security research knowledge.

## Foundational Learning

- **Evolutionary Algorithms (Selection, Crossover, Mutation)**
  - Why needed: AgenticRed implements genetic algorithm over code-based agent definitions. Essential for diagnosing improvement or plateau.
  - Quick check: Given fitness scores [0.45, 0.62, 0.38], which system(s) would be selected as "elite"?

- **LLM In-Context Learning with Code Generation**
  - Why needed: Meta-agent must generate syntactically valid, executable Python code from archive context.
  - Quick check: If output contains syntax error in `forward()` method, what self-correction mechanism is employed?

- **Red-Teaming Evaluation Metrics (ASR, StrongREJECT, Queries-per-Success)**
  - Why needed: Fitness defined as ASR, but paper also tracks diversity (SelfBLEU) and efficiency (queries).
  - Quick check: If system achieves 98% ASR on HarmBench but only 0.4 on StrongREJECT, what does this suggest?

## Architecture Onboarding

- **Component map**: Meta Agent (GPT-5) -> Agentic System Code (Python) -> Initial Evaluation (16 intents) -> Archive (prior systems + metrics) <- Fittest System <- Comprehensive Eval (50 intents) -> Helper Functions (get_response, get_jailbreak_result)

- **Critical path**:
  1. Archive initialization with Self-Refine and JudgeScore-Guided AdvReasoning baselines
  2. Meta-agent receives archive + task description + helper function signatures
  3. Meta-agent generates M candidate system implementations
  4. Each candidate evaluated on subset d (16 intents) → fitness score
  5. Highest-fitness candidate selected for full evaluation (50 intents)
  6. Best system added to archive; repeat for N generations

- **Design tradeoffs**:
  - Archive quality vs. exploration speed: Stronger archive provides warm start but may bias toward known strategies
  - Population size (M) vs. compute cost: M=3 offspring per generation used; larger M increases selection pressure
  - ASR optimization vs. diversity: Diversity analysis shows ASR-focused optimization drives convergence

- **Failure signatures**:
  - Syntax/runtime errors in generated code: Self-reflection loop corrects errors using error messages
  - Mode collapse: Systems converge on similar attack templates; addressed by diversity incentives
  - Reward hacking: High HarmBench ASR with low StrongREJECT score indicates judge-specific exploitation
  - Poor transfer to stronger targets: Attacker model capability matters (Mixtral vs Llama-2-7B)

- **First 3 experiments**:
  1. Replicate baseline comparison: Run AgenticRed for 10 generations targeting Llama-2-7B with Mixtral-8x7B attacker and HarmBench-Llama-2-13b-cls judge. Track ASR per generation against baselines. Expected: 96% ASR by generation 6.
  2. Ablate evolutionary pressure: Set M=1 and compare ASR trajectory over 10 generations against M=3 baseline. Expected: ~6% lower best performance.
  3. Transfer test to proprietary model: Take best system from experiment 1, evaluate on GPT-3.5-Turbo and Claude-Sonnet-3.5. Track both HarmBench ASR and StrongREJECT scores. Expected: 100% ASR on GPT-3.5-Turbo, 60% on Claude-Sonnet-3.5.

## Open Questions the Paper Calls Out

- **Co-evolutionary frameworks**: Can AgenticRed adapt to target models that are simultaneously patching discovered vulnerabilities? Current experiments use static model versions, but real-world safety involves dynamic arms race between attack generation and safety patches.

- **Post-training conditioning**: Can conditioning methods successfully mitigate mode collapse observed in generated agentic systems? Current approach limits exploration of broader design space, resulting in homogeneous strategies relying on pre-existing literature patterns.

- **Query efficiency optimization**: Can training-time query efficiency be optimized without sacrificing high ASR? Current fitness score optimizes only for ASR, leading to substantial computational overhead (122k queries) during training phase.

## Limitations

- Mode collapse with 8-10 out of 10 generations converging on similar attack templates (refusal suppression appearing in 8/10 generations)
- Performance heavily dependent on meta-agent capability—DeepSeek-R1 yields only marginal improvements vs GPT-5
- Some high ASR systems exploit judge-specific vulnerabilities rather than achieving genuine jailbreaks
- Substantial computational overhead during training phase (122k queries) despite reasonable test-time efficiency

## Confidence

- **High Confidence**: Evolutionary framework's ability to achieve 96-98% ASR on open-weight models and 100% on GPT-3.5-Turbo/4o
- **Medium Confidence**: Claims about emergent strategy discovery supported by qualitative analysis but could benefit from more systematic categorization
- **Low Confidence**: Transferability claims to proprietary models based on limited evaluations with only two models

## Next Checks

1. **Diversity Preservation Test**: Run AgenticRed with explicit diversity incentives and measure whether semantic diversity (SelfBLEU) improves without significant ASR degradation.

2. **Cross-Attacker Robustness**: Evaluate best systems discovered using Mixtral attacker on weaker attacker models (e.g., Llama-2-7B) to quantify how much attacker capability influences discovered strategies.

3. **Long-Term Transfer Study**: After discovering high