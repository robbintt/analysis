---
ver: rpa2
title: Language-in-the-Loop Culvert Inspection on the Erie Canal
arxiv_id: '2509.21370'
source_url: https://arxiv.org/abs/2509.21370
tags:
- inspection
- culvert
- vision
- canal
- robot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents VISION, an autonomous culvert inspection system
  that uses a vision-language model (VLM) for defect detection without requiring domain-specific
  fine-tuning. VISION captures a query image, prompts the VLM to propose regions of
  interest (ROIs), fuses stereo depth to estimate scale, and plans constrained viewpoints
  for targeted close-ups.
---

# Language-in-the-Loop Culvert Inspection on the Erie Canal
## Quick Facts
- arXiv ID: 2509.21370
- Source URL: https://arxiv.org/abs/2509.21370
- Reference count: 33
- One-line result: VISION achieves 80% expert agreement in defect detection using a vision-language model without domain fine-tuning.

## Executive Summary
This work introduces VISION, an autonomous culvert inspection system that leverages a vision-language model (VLM) to detect defects without requiring domain-specific fine-tuning. Deployed on a legged robot, VISION captures images, proposes regions of interest (ROIs), fuses stereo depth for scale estimation, and plans constrained viewpoints for detailed close-up imaging. In a 66 m Erie Canal culvert, VISION ran end-to-end on-board and produced high-resolution imagery for inspection reporting. Initial ROI proposals achieved 61.4% agreement with expert assessments, improving to 80% after re-imaging, demonstrating the system's ability to convert tentative hypotheses into expert-aligned findings.

## Method Summary
VISION uses a VLM to propose ROIs from query images, then fuses stereo depth to estimate scale and plans constrained viewpoints for targeted close-ups. The system operates end-to-end on a legged robot, capturing imagery in situ. Stereo depth fusion enhances scale estimation, while the VLM guides defect detection without requiring domain-specific training. The robot navigates the culvert autonomously, planning and executing inspection sequences based on VLM outputs.

## Key Results
- Initial ROI proposals achieved 61.4% agreement with expert assessments.
- Post-re-imaging assessments reached 80% expert agreement.
- VISION successfully deployed on a legged robot in a 66 m Erie Canal culvert, producing high-resolution imagery for reporting.

## Why This Works (Mechanism)
The system leverages a vision-language model's broad knowledge base to propose defect hypotheses without domain-specific training. Stereo depth fusion provides real-world scale, enabling the robot to plan targeted close-up views for verification. This "language-in-the-loop" approach iteratively refines ROI proposals into expert-aligned findings through guided re-imaging.

## Foundational Learning
1. **Vision-Language Models (VLMs)**: Pre-trained models that process both images and text; needed for zero-shot defect detection without domain fine-tuning; quick check: validate ROI proposals on diverse infrastructure.
2. **Stereo Depth Fusion**: Combines two images to estimate 3D structure; needed for accurate scale estimation in real-world units; quick check: verify depth accuracy in textureless or low-texture areas.
3. **Constrained Viewpoint Planning**: Algorithmic planning to optimize robot positioning for detailed imaging; needed to maximize inspection quality within robot mobility limits; quick check: ensure coverage of all proposed ROIs.
4. **End-to-End On-Board Processing**: Completes all tasks on the robot without external computation; needed for autonomy in remote or communication-limited environments; quick check: monitor computational load and latency.

## Architecture Onboarding
**Component Map**: Camera capture -> VLM ROI proposal -> Stereo depth fusion -> Scale estimation -> Viewpoint planning -> Robot actuation -> High-res capture -> Reporting
**Critical Path**: Capture -> VLM ROI -> Depth fusion -> Planning -> Close-up capture
**Design Tradeoffs**: Zero-shot VLM use avoids domain training but may miss subtle defects; stereo depth improves scale but assumes accurate depth maps; on-board processing ensures autonomy but limits computational capacity.
**Failure Signatures**: Poor ROI accuracy in low light or textureless surfaces; depth estimation errors on reflective or uniform surfaces; planning failures if robot mobility constraints are underestimated.
**First Experiments**:
1. Test ROI proposal accuracy across diverse culvert materials and lighting.
2. Evaluate depth estimation accuracy on textureless or reflective surfaces.
3. Validate viewpoint planning coverage for all ROI types under robot mobility constraints.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on a single VLM without domain-specific fine-tuning may limit generalizability to different infrastructure types or degraded imaging conditions.
- System assumes accurate depth maps and consistent lighting, which may fail in challenging environments.
- Evaluation is based on a single culvert deployment and expert agreement metrics, lacking independent validation across multiple sites and defect types.

## Confidence
- **Medium**: Core claims of feasibility and expert alignment are supported by initial deployment and expert agreement metrics.
- **Low**: Generalizability and robustness under varied conditions are not well established due to single-site evaluation and lack of quantitative performance data.

## Next Checks
1. Evaluate VISION on multiple culvert types and environments (varying lighting, occlusion, material) to assess robustness and generalizability.
2. Conduct a quantitative comparison of defect detection performance (precision, recall, F1-score) against both domain-tuned models and human inspectors on a standardized dataset.
3. Test the system's performance with degraded depth estimation (e.g., textureless surfaces, strong illumination changes) to quantify the impact on scale accuracy and ROI selection.