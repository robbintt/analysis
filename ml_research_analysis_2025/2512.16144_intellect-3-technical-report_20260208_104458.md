---
ver: rpa2
title: 'INTELLECT-3: Technical Report'
arxiv_id: '2512.16144'
source_url: https://arxiv.org/abs/2512.16144
tags:
- training
- environments
- zhang
- wang
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report presents INTELLECT-3, a 106B-parameter Mixture-of-Experts
  model (12B active) that achieves state-of-the-art performance for its size across
  math, code, science, and reasoning benchmarks through large-scale reinforcement
  learning. The key innovation is the open-source prime-rl framework that enables
  efficient asynchronous reinforcement learning at scale, supporting end-to-end training
  including supervised fine-tuning and multi-turn agentic RL.
---

# INTELLECT-3: Technical Report

## Quick Facts
- arXiv ID: 2512.16144
- Source URL: https://arxiv.org/abs/2512.16144
- Authors: Prime Intellect Team, Mika Senghaas, Fares Obeid, Sami Jaghouar, William Brown, Jack Min Ong, Daniel Auras, Matej Sirovatka, Jannik Straube, Andrew Baker, Sebastian Müller, Justus Mattern, Manveer Basra, Aiman Ismail, Dominik Scherm, Cooper Miller, Ameen Patel, Simon Kirsten, Mario Sieg, Christian Reetz, Kemal Erdem, Vincent Weisser, Johannes Hagemann
- Reference count: 40
- Primary result: 106B MoE model (12B active) achieving 90.8% on AIME 2024 and 88.0% on AIME 2025

## Executive Summary
INTELLECT-3 is a 106B-parameter Mixture-of-Experts model that achieves state-of-the-art performance for its size through large-scale reinforcement learning. The key innovation is the open-source prime-rl framework that enables efficient asynchronous RL at scale, supporting end-to-end training including supervised fine-tuning and multi-turn agentic RL. Using this infrastructure, INTELLECT-3 was trained on diverse environments and outperforms many larger frontier models, achieving scores of 90.8% on AIME 2024 and 88.0% on AIME 2025.

## Method Summary
INTELLECT-3 uses a two-stage training pipeline: supervised fine-tuning on reasoning traces and tool-use demonstrations, followed by reinforcement learning on diverse agentic environments. The model employs a 106B-parameter MoE architecture with 12B active parameters, trained using the Muon optimizer with FSDP2 for distributed training. The prime-rl framework implements asynchronous off-policy training with continuous batching and in-flight weight updates, enabling efficient training across 512 H200 GPUs.

## Key Results
- Achieves 90.8% on AIME 2024 and 88.0% on AIME 2025
- Outperforms DeepSeek's models and matches GLM-4.6 (3× larger)
- Sets new state-of-the-art for models of this size across math, code, science, and reasoning benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Asynchronous Off-Policy Training with Continuous Batching
The architecture decouples rollout generation from weight updates via asynchronous training and continuous batching, significantly increasing system throughput and enabling training on long-context rollouts. The trainer consumes batches and updates weights while the inference service generates rollouts, with continuous batching ensuring the inference pool remains saturated by immediately repopulating finished rollout slots. In-flight weight updates allow the policy to be refreshed mid-generation, accepting controlled off-policyness for computational efficiency.

### Mechanism 2: Structured Agentic Environments with Verifiable Rewards
Diverse tool-using environments with programmatic reward signals allow the model to learn complex multi-step reasoning through RL. Environments are defined with datasets, rollout methods, and Rubric objects for reward computation. The model learns by attempting tasks, receiving scalar rewards based on success, and updating its policy to maximize reward, enabling learning of behaviors like web search and software engineering through outcome-based rewards.

### Mechanism 3: SFT-then-RL Training with Muon Optimizer
Multi-stage training starting with SFT on high-quality reasoning traces followed by large-scale RL yields a stable and high-performing model. SFT instills foundational reasoning patterns and tool-use formats, while RL refines these capabilities by exploring solutions and optimizing for verifiable rewards. The Muon optimizer, operating at the matrix level using Newton-Schulz iterations, provides stability and performance throughout training.

## Foundational Learning

- **Asynchronous Reinforcement Learning (RL)**: The core training loop that tolerates specific levels of off-policyness for throughput gains. Quick check: What is the primary trade-off being made by allowing the inference service to generate rollouts from an older policy (θ_old) while the trainer updates to a newer policy (θ_new)?

- **Mixture-of-Experts (MoE) Models**: INTELLECT-3 is a 106B-parameter MoE model with 12B active parameters. Understanding expert routing and load balancing is key. Quick check: Why does the paper mention that enabling expert parallelism led to worse training throughput in their specific regime?

- **Infrastructure for Large-Scale Distributed Training**: The system runs on 512 H200 GPUs using FSDP, InfiniBand, and checkpointing. Quick check: What is the purpose of the Orchestrator in the disaggregated trainer-inference architecture, and how does it contribute to the system's efficiency?

## Architecture Onboarding

- **Component map**: Trainer -> Orchestrator -> Inference Service -> Verifiers/Environments Hub -> Prime Sandboxes
- **Critical path**: The efficiency of the entire system hinges on the Orchestrator's ability to keep the inference pool saturated via continuous batching and the latency of in-flight weight updates. If the orchestrator cannot dispatch new requests fast enough, or if weight updates block generation for too long, the overall throughput collapses.
- **Design tradeoffs**: Throughput vs. On-Policyness (higher max_off_policy_steps increases throughput but risks policy drift), Context Length vs. Memory (64k+ sequences require activation offloading), Disaggregation Complexity vs. Monolithic Simplicity (split architecture is more complex but enables crucial efficiency gains).
- **Failure signatures**: Reward Collapse (training curves suddenly drop), Inference Throughput Plateau (adding nodes doesn't improve speed), Weight Update Stalls (trainer idle waiting for inference), Sandbox Timeout Storm (high latency in code environments).
- **First 3 experiments**: Baseline Latency & Throughput Profiling on small node count, Off-Policy Ablation varying max_off_policy_steps and monitoring stability vs. throughput, Environment Integration Test running full multi-turn rollout for a single environment to verify tool calls and reward calculation.

## Open Questions the Paper Calls Out

1. **Performance Ceiling**: What is the performance ceiling for INTELLECT-3 when continuing RL training beyond the current run, and does performance eventually plateau or continue improving indefinitely? The authors state rewards were still increasing with no sign of plateauing at training end.

2. **Long-Horizon Agentic Behavior**: Can long-horizon agentic behavior be effectively learned through RL by having models manage their own context via learned tools for context cutting, sub-branch prompting, and external memory? This is proposed as future work with no empirical validation yet.

3. **MoE Parallelization Boundaries**: Under what training configurations does expert parallelism improve versus harm MoE training throughput? The paper reports EP led to worse throughput for their configuration but notes lower sequence length/hidden dimension might see improvement.

4. **Context Parallelism Accuracy**: What causes the accuracy degradation observed with context parallelism (Ring Attention/FlexAttention), and can it be mitigated for production training? The paper found CP enabled 256k sequence length but exhibited accuracy degradations making it unsuitable.

## Limitations

- **Dataset Quality Uncertainty**: Critical details about data filtering thresholds, synthetic trajectory generation, and balance between reasoning traces and tool-use demonstrations are unspecified, making it impossible to assess whether performance gains stem from data quality versus architectural innovations.

- **Infrastructure Dependency**: Reported efficiency gains from asynchronous training depend heavily on specific infrastructure configuration. The 512 H200 setup represents significant investment that may not be reproducible in all research settings.

- **Benchmark Overfitting Risk**: Evaluation focuses on highly specialized benchmarks without evidence for general-purpose performance, safety evaluations, or real-world applications, raising concerns about overfitting to benchmark-specific patterns.

## Confidence

**High Confidence**: The prime-rl framework implementation (disaggregated architecture, continuous batching, in-flight weight updates) is well-documented and reproducible. Core algorithms (DAPO, masked importance sampling) are established with clear implementations.

**Medium Confidence**: Benchmark scores (90.8% AIME 2024, 88.0% AIME 2025) are plausible given model size and training scale, but lack independent verification or public model weights. Performance claims relative to other models are based on public leaderboards.

**Low Confidence**: Claims about Muon optimizer's specific advantages and distributed implementation details are difficult to assess without access to actual code or comprehensive ablation studies comparing optimizer configurations.

## Next Checks

1. **End-to-End Environment Integration Test**: Implement a single environment (e.g., deepdive web search task) from the Environments Hub and verify the complete data pipeline from rollout generation through reward calculation to validate core architecture without full-scale training.

2. **Off-Policyness Tradeoff Analysis**: Run controlled experiments varying the max_off_policy_steps parameter (e.g., 1, 4, 8, 16) on small-scale training while monitoring training stability and throughput metrics to quantify efficiency gains and identify optimal tradeoff point.

3. **Data Quality Impact Study**: Train two versions - one with full SFT dataset and another with modified dataset (e.g., removing synthetic reasoning traces) - to compare final RL performance and isolate SFT data quality impact on RL learning complex reasoning behaviors.