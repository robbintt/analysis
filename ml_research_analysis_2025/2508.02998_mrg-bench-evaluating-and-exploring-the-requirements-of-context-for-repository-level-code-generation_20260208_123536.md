---
ver: rpa2
title: 'MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level
  Code Generation'
arxiv_id: '2508.02998'
source_url: https://arxiv.org/abs/2508.02998
tags:
- code
- arxiv
- generation
- mrg-bench
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MRG-Bench is a multi-language repository-level code generation
  benchmark designed to address limitations in existing datasets, such as lack of
  runnable test cases, deviation from real-world code distribution, and single-language
  focus. It includes Python, Java, and Go, with 383 practical samples from real repositories,
  each equipped with runnable test cases.
---

# MRG-Bench: Evaluating and Exploring the Requirements of Context for Repository-Level Code Generation

## Quick Facts
- **arXiv ID:** 2508.02998
- **Source URL:** https://arxiv.org/abs/2508.02998
- **Reference count:** 40
- **Key outcome:** MRG-Bench is a multi-language repository-level code generation benchmark with 383 practical samples from real repositories, each equipped with runnable test cases. Extensive experiments show significant performance deficiencies, with the best model achieving only 32.5% Pass@1 accuracy, primarily due to models' inability to understand user requirements ("What to do").

## Executive Summary
MRG-Bench addresses critical limitations in existing code generation benchmarks by focusing on repository-level tasks with runnable test cases, realistic project distributions, and multi-language support (Python, Java, Go). The benchmark evaluates large language models and retrieval-augmented generation methods on 383 practical code generation samples from real-world repositories. Extensive experiments reveal that most failures stem from models' inability to understand user requirements rather than implementation difficulty, with language-specific contextual needs that current methods fail to address effectively.

## Method Summary
MRG-Bench consists of 383 repository-level code generation samples from 22 real-world projects (post-2023), each with function annotations, signatures, referenced function bodies, and runnable test cases. The evaluation pipeline uses Docker containers to execute project test suites after injecting generated code. Models are evaluated using Pass@1 and Pass@3 metrics, measuring whether generated code passes all tests. The benchmark tests various context strategies: no context, in-file context (6000 tokens), callee context, and RAG methods (BM25, BGE-M3 embeddings, RepoCoder). Error analysis uses a 5-model annotation pipeline to classify failures as "What" (requirement comprehension) vs. "How" (implementation) issues.

## Key Results
- The best model (DeepSeek-Coder-33B) achieves only 32.5% Pass@1 accuracy, highlighting significant performance gaps
- Over 68% of failures stem from "What information" deficiencies - models cannot understand what code functionality user requirements correspond to
- Standard RAG methods underperform simple in-file context by 5-10 percentage points
- Language-specific contextual needs vary significantly: Python shows different failure patterns than Java and Go

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The primary failure mode in repository-level code generation is not implementation difficulty ("How"), but requirement comprehension ("What").
- **Mechanism:** When models lack sufficient contextual grounding for a specific repository, they misinterpret what functionality the user intends—not because they cannot write the code, but because natural language descriptions have ambiguous mappings to repository-specific semantics.
- **Core assumption:** The annotation methodology (model-based voting with 5 LLMs) correctly categorizes failure causes into "What" vs. "How" bins.
- **Evidence anchors:**
  - [abstract] "The results explicitly show that the majority of methods suffer from 'difficulty in understanding user requirements,' failing to comprehend their assigned tasks accurately."
  - [section 5.3, RQ3] "over 68% of failures stem from missing 'What information' - models cannot understand what code functionality the user requirements correspond to within the current repository context."
  - [corpus] RepoScope (arXiv:2507.14791) confirms retrieval struggles with "intent misalignment" in repository contexts.

### Mechanism 2
- **Claim:** Context effectiveness varies by programming language due to differing semantic coupling patterns between functions.
- **Mechanism:** Python exhibits weaker intra-repository semantic coupling, so models perform relatively well without context but gain little from repository context when they fail. Java and Go have stronger structural patterns (interfaces, type systems), making in-file context more predictive of correct requirement interpretation.
- **Core assumption:** The "What" vs. "How" annotation distributions reflect genuine language properties rather than annotation model bias toward specific languages.
- **Evidence anchors:**
  - [section 5.3] "Python demonstrates a distinctly high 'What Information' tendency... repository context rarely provides useful information" vs. "for Go and Java, we can follow in-file and RAG approaches to further mine 'What Information'"
  - [section 5.1.2, RQ1.2] Models perform significantly better on Python (7.3-10.6% Pass@1) than Go (1.2-7.3%) without context, suggesting language-specific prior knowledge saturation.

### Mechanism 3
- **Claim:** Standard RAG methods underperform simple in-file context because retrieved fragments lack the structural coherence needed for "What" understanding.
- **Mechanism:** BM25 and embedding-based retrieval return semantically similar but structurally disconnected code fragments. Models need to see the target function's structural neighborhood (surrounding class/methods) to disambiguate requirements, not just textually similar snippets.
- **Core assumption:** Retrieval quality, not generation capability, is the bottleneck for RAG performance.
- **Evidence anchors:**
  - [section 5.2.2, RQ2.2] "RAG-related methods... effectiveness is significantly inferior to that of providing in-file context alone" (Table 7 shows ~12-18% Pass@1 for RAG vs. 22%+ for in-file)
  - [section 5.2.2] "RepoCoder... showed consistent performance across all three languages... pre-generated code for retrieve, ensuring that both the query and retrieved information reside in the same semantic space"

## Foundational Learning

- **Concept: Repository-level vs. standalone code generation**
  - **Why needed here:** MRG-Bench evaluates tasks requiring cross-file dependencies, custom utilities, and project-specific patterns—not self-contained functions like HumanEval.
  - **Quick check question:** Can you explain why a function that compiles in isolation might fail integration tests in its repository?

- **Concept: Pass@k evaluation metric**
  - **Why needed here:** The paper uses Pass@1 and Pass@3 to measure whether at least one of k generated samples passes all test cases. Understanding this prevents misinterpreting absolute scores.
  - **Quick check question:** If a model achieves 30% Pass@1 but 60% Pass@3, what does this imply about its generation behavior?

- **Concept: "What" vs. "How" decomposition**
  - **Why needed here:** The paper's core diagnostic framework splits failures into requirement misunderstanding vs. implementation inability. This framing is non-obvious and central to their conclusions.
  - **Quick check question:** A model generates code that implements sorting, but the target function should filter. Is this a "What" or "How" failure?

## Architecture Onboarding

- **Component map:** MRG-Bench Sample -> Function annotations + signature + referenced function body + test cases + repository information -> Evaluation Pipeline (Docker container + language-specific test runners) -> Pass@k computation

- **Critical path:** 
  1. Select target function with dependencies + comments + tests
  2. Provide prompt (annotations + signature + optional context)
  3. Model generates function body
  4. Inject into source file at target location
  5. Execute project test suite in Docker
  6. Classify as pass/fail, aggregate to Pass@k

- **Design tradeoffs:**
  - **Tight filtering (100% test coverage requirement) vs. scale:** Results in only 383 samples from 22 projects, but guarantees runnable ground truth
  - **Real repositories (post-2023) vs. data leakage risk:** Limits project pool but reduces memorization concerns (leakage detection shows 6-8% vs. 41% for HumanEval)
  - **Multi-language vs. benchmark complexity:** Requires separate parsers (Tree-sitter), test frameworks, and analysis tools per language

- **Failure signatures:**
  - **High "What" error rate (>68%):** Model implements wrong functionality despite correct syntax
  - **Language bias:** Python outperforms Go by 2-7x without context; gap narrows with in-file context but persists
  - **RAG ineffectiveness:** Retrieval-based methods underperform naive in-file context by 5-10 percentage points

- **First 3 experiments:**
  1. **Baseline calibration:** Run your model on MRG-Bench with annotations + signature only (no context). Compare Pass@1 against Table 3 baselines to validate setup.
  2. **Context ablation:** Test in-file context (6000 tokens) vs. callee functions vs. no context. Expect largest gains from in-file, minimal from callee.
  3. **Error annotation pilot:** Sample 20 failures, manually classify as "What" vs. "How" using Figure 4 prompt. Compare to automated annotation distribution (Figure 5) to validate the diagnostic framework.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can context extraction strategies be specialized to address the distinct "What information" requirements of different programming languages (e.g., Python vs. Java)?
- **Basis in paper:** [Explicit] The abstract and conclusion state that "specialized contextual information needs to be designed for different languages," noting that Python exhibits different contextual preferences and failure modes than Java and Go.
- **Why unresolved:** The paper demonstrates that standard RAG methods are ineffective for Python, but does not propose a solution for retrieving the specific "What information" needed for Python repositories.
- **What evidence would resolve it:** A dynamic context extraction method that improves Pass@1 accuracy for Python by specifically targeting requirement comprehension rather than just implementation details.

### Open Question 2
- **Question:** How can repository-level code generation benchmarks be expanded to include agent-based methods without compromising fairness or causing data leakage?
- **Basis in paper:** [Explicit] Section 6 (Threats to Validity) notes the exclusion of agent-based methods due to potential data leakage from network search functions and the lack of agents designed for general code generation scenarios.
- **Why unresolved:** Current agent methods focus on bug-fixing (e.g., for SWE-Bench) rather than generation, and allowing web search creates an uncontrolled variable for evaluation.
- **What evidence would resolve it:** The development of a sandboxed evaluation protocol or a new generation of code agents that operate using only local repository context.

### Open Question 3
- **Question:** Does providing extended context (e.g., 100K tokens) introduce noise that counteracts the benefits of additional information in repository-level tasks?
- **Basis in paper:** [Inferred] Section 5.2.1 observes that long-context models did not substantially outperform in-file context methods, suggesting that "longer contexts... may also introduce additional noise."
- **Why unresolved:** It is unclear if the performance ceiling (approx. 40% Pass@1) is caused by the model's reasoning limits or by an inability to filter relevant signals from noisy long contexts.
- **What evidence would resolve it:** Ablation studies comparing performance on filtered, high-relevance contexts versus raw long contexts to isolate the impact of noise.

## Limitations
- Benchmark size limited by strict filtering requirements (383 samples from 22 projects)
- Automated error annotation pipeline not validated with human-labeled samples
- Language-specific mechanisms inferred from aggregate distributions rather than controlled experiments
- RAG underperformance may partially reflect suboptimal hyperparameters rather than fundamental limitations

## Confidence
- **High:** Benchmark construction methodology, test harness reliability, baseline model performance comparisons
- **Medium:** Primary "What" vs. "How" failure mode distinction, language-specific performance gaps
- **Low:** Precise attribution of RAG underperformance to structural coherence issues rather than parameter tuning

## Next Checks
1. Validate "What" vs. "How" annotations with human-labeled sample (n=20) to confirm automated pipeline accuracy
2. Conduct controlled language-specific ablation: isolate semantic coupling by testing same function in different repository contexts across Python/Java/Go
3. Test RAG methods with hybrid retrieval (BM25 + embeddings) and context-aware query reformulation to determine if retrieval quality or implementation limits performance