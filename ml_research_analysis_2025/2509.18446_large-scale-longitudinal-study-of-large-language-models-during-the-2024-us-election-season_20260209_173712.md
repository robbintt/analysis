---
ver: rpa2
title: Large-Scale, Longitudinal Study of Large Language Models During the 2024 US
  Election Season
arxiv_id: '2509.18446'
source_url: https://arxiv.org/abs/2509.18446
tags:
- factor
- answers
- omit
- president
- important
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a large-scale, longitudinal study of 12 large
  language models (LLMs) during the 2024 US presidential election season. The authors
  designed a structured survey of over 12,000 questions and queried the models on
  a near-daily basis from July through November 2024.
---

# Large-Scale, Longitudinal Study of Large Language Models During the 2024 US Election Season

## Quick Facts
- **arXiv ID:** 2509.18446
- **Source URL:** https://arxiv.org/abs/2509.18446
- **Reference count:** 40
- **Primary result:** All 12 LLMs studied showed sensitivity to demographic steering during the 2024 US election, with Gemini models most sensitive; models' associations of candidates with adjectives were persistent but self-consistency in implicit predictions was low.

## Executive Summary
This paper presents a large-scale, longitudinal study of 12 large language models during the 2024 US presidential election season. The authors designed a structured survey of over 12,000 questions and queried the models on a near-daily basis from July through November 2024. They analyzed four aspects of LLM behavior: temporal variation, sensitivity to demographic steering, candidate perceptions, and implicit election predictions. The study found that all models exhibited some degree of sensitivity to demographic steering, with Gemini models being most sensitive. The models' associations of adjectives with candidates showed persistent patterns, with Trump most commonly associated with divisive and corrupt, while Harris was linked to compassionate and honorable. When inferring implicit election predictions from exit poll questions, the models were not self-consistent, sometimes predicting a Harris win and sometimes a Trump win depending on the question asked. The authors publicly released their dataset to facilitate future research on LLM behavior during elections.

## Method Summary
The study queried 12 LLMs (including GPT-4o, Claude-3.5-Sonnet, Gemini-1.0-Pro, and Perplexity) daily from July through November 2024 with a structured survey of over 12,000 election-related questions. Questions were categorized into topics (Candidates, Issues, Election Process, General Election, and Exit Polls) and included 22 prompt variations to test demographic steering. Responses were embedded using paraphrase-MiniLM-L6-v2, and temporal drift was measured via cosine distance. Structured data extraction used GPT-4o-mini for parsing open-ended responses. For implicit prediction analysis, the authors solved a linear system to infer election outcomes from exit poll responses.

## Key Results
- All models exhibited sensitivity to demographic steering, with Gemini models showing the highest sensitivity
- Trump was most commonly associated with "divisive" and "corrupt," while Harris was linked to "compassionate" and "honorable"
- Models were not self-consistent in implicit election predictions, sometimes predicting Harris wins and sometimes Trump wins depending on the question asked
- Temporal analysis revealed step changes in model behavior correlating with known model updates

## Why This Works (Mechanism)

### Mechanism 1: Semantic Embedding for Temporal Drift Detection
The system queries models daily with a fixed survey and projects responses into a vector space using paraphrase-MiniLM-L6-v2. By calculating the cosine distance between daily response embeddings and time-averaged embeddings, the system quantifies "drift." Sudden spikes correlate with known checkpoint updates (e.g., GPT-4o on Oct 2), while gradual drifts may indicate hidden guardrail adjustments.

### Mechanism 2: Implicit Belief Extraction via Linear Decomposition
The authors ask models to predict exit poll results for "all voters" and specific subgroups (e.g., "Harris voters"). They formulate a linear system and solve for weights representing the model's implicit estimate of how representative each subgroup is of the total voting population. The resulting weights represent the model's implicit estimate of who is winning.

### Mechanism 3: Contextual Persona Activation (Steering)
By prepending prompts with "I am a [Democrat/Republican/Hispanic]" (Prompt Variations), the model conditions its next-token prediction on this persona. The study measures the resulting shift in the embedding space (pairwise cosine distance). A large shift indicates the model leverages the persona to alter the semantic content.

## Foundational Learning

- **Concept: Cosine Similarity in Vector Space**
  - **Why needed here:** The entire longitudinal analysis relies on comparing the "meaning" of responses over time. You must understand that text is converted to vectors and "distance" equates to semantic difference.
  - **Quick check question:** If a model changes a response from "I cannot predict" to "Predicting is difficult," would the cosine distance be high or low? (Answer: Low, as semantics are similar; high distance implies a topic shift).

- **Concept: Endogenous vs. Exogenous Queries**
  - **Why needed here:** The paper separates questions that *should* change (election news) from those that shouldn't (election process). Distinguishing these is critical to attributing behavioral drift to model updates vs. world events.
  - **Quick check question:** A query asking "Who won the 2024 election?" is queried in July vs November. Is this an endogenous or exogenous shift? (Answer: Exogenous; the real-world state changed).

- **Concept: LLM "Steerability" (In-Context Learning)**
  - **Why needed here:** Understanding how prompt prefixes (identities) alter model behavior is central to the paper's findings on bias and sensitivity.
  - **Quick check question:** Why does the study use temperature=0 for offline models but 0.1 for online agents? (Answer: To ensure offline changes are deterministic updates, not random noise, while allowing slight flexibility for tool-using agents to handle API variance).

## Architecture Onboarding

- **Component map:** SGLang (batch processing) -> LangChain (tool integration for online models) -> Serper (Google Search API) -> 12 LLMs (Offline: GPT/Claude/Gemini; Online: Perplexity + LangChain Agents) -> paraphrase-MiniLM-L6-v2 (Embeddings) -> Pandas/NumPy (Statistical Analysis) -> GPT-4o-mini (Unstructured text parsing for exit polls) -> Data Store (12,000+ questions, daily snapshots, stored in JSON/Parquet)

- **Critical path:** Question Taxonomy Generation -> API Querying (handling rate limits/retries) -> Embedding Generation -> Longitudinal Cosine Distance Calculation -> Linear Solving (for exit polls)

- **Design tradeoffs:** Fixed Survey vs. Adaptive (fixed survey chosen to allow clean longitudinal counterfactuals, trading off ability to "red team" or adapt to breaking news in real-time); Token Limits (128-token cutoff necessary due to budget constraints, causing "homogenization" of online responses)

- **Failure signatures:** Infeasible Weights (negative weights indicate model is hallucinating incoherent demographics); Token Cutoffs (responses ending abruptly mid-sentence, particularly in "online" agent logs)

- **First 3 experiments:**
  1. Re-calibrate the "Endogenous" Baseline: Run the 158 "Election Process" questions against the current version of GPT-4o and Claude-3.5-Sonnet to see if step changes have occurred since the study ended
  2. Steerability Stress Test: Select 10 controversial "Issues" questions and run them with "I am a Republican" vs "I am a Democrat" prefixes to verify the claimed ranking of Gemini > Claude > GPT in sensitivity
  3. Solver Sanity Check: Pick one exit poll question and construct a synthetic "voter" vector that is exactly the average of "Harris voters" and "Trump voters" to verify the linear solver returns correct weights of [0.5, 0.5], then test with real model data to identify incoherence

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do the implicit election predictions derived from LLM responses align with the actual 2024 US election exit poll results?
- Basis in paper: Section 7.1 states that future work "could investigate whether the exit poll 'predictions' of LLMs align with the exit poll results following the election."
- Why unresolved: The longitudinal study was conducted prior to and during the election, precluding a post-hoc comparison with the actual official results.
- What evidence: A direct comparative analysis between the model predictions stored in the released dataset and the verified 2024 exit poll data.

### Open Question 2
- Question: How do "reasoning" models (e.g., OpenAI o-series) handle election-related queries compared to the standard models evaluated in this study?
- Basis in paper: Section 7.1 identifies a limitation that the experimental setup predates the advent of reasoning models and suggests future research study how they handle politically sensitive topics.
- Why unresolved: These models were not available or had restrictive rate limits during the study's execution period (Julyâ€“November 2024).
- What evidence: Applying the same 12,000+ question survey pipeline to the newer reasoning models and analyzing refusal rates and self-consistency.

### Open Question 3
- Question: Do election-related responses differ significantly between API-accessed models and consumer-facing chatbot interfaces?
- Basis in paper: Section 7.1 lists the reliance on API access rather than chatbots as a limitation, noting that "post-training, prompting, and guardrailing often cause chatbots to behave differently."
- Why unresolved: The study was restricted to API endpoints due to the high cost and validity challenges of creating fresh chatbot accounts daily to avoid personalization spillover.
- What evidence: A study designing infrastructure to systematically query chatbot interfaces (e.g., ChatGPT) without historical personalization effects.

## Limitations

- The study could not definitively attribute all observed step changes to specific model updates, as not all changes correlated with documented model changes
- The 128-token limit for responses led to artificial "homogenization" of online responses where models summarized search results without answering, creating spurious step changes
- The study relied on API access rather than chatbot interfaces, which may behave differently due to post-training, prompting, and guardrailing differences

## Confidence

- **High Confidence:** Temporal drift detection mechanism and its correlation with known model updates; steerability analysis showing all models exhibit sensitivity to demographic steering
- **Medium Confidence:** Implicit election prediction methodology (models were often not self-consistent, resulting in infeasible solutions); candidate perception analysis (relies on subjective scoring of adjectives)
- **Low Confidence:** Exact magnitude of step changes and their attribution to specific model updates

## Next Checks

1. Re-calibrate the "Endogenous" Baseline: Run the 158 "Election Process" questions against the current version of GPT-4o and Claude-3.5-Sonnet to see if step changes have occurred since the study ended
2. Steerability Stress Test: Select 10 controversial "Issues" questions and run them with "I am a Republican" vs "I am a Democrat" prefixes to verify the claimed ranking of Gemini > Claude > GPT in sensitivity
3. Solver Sanity Check: Pick one exit poll question and construct a synthetic "voter" vector that is exactly the average of "Harris voters" and "Trump voters" to verify the linear solver returns correct weights of [0.5, 0.5], then test with real model data to identify incoherence