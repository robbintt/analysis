---
ver: rpa2
title: 'LegalViz: Legal Text Visualization by Text To Diagram Generation'
arxiv_id: '2502.06147'
source_url: https://arxiv.org/abs/2502.06147
tags:
- legal
- llama3
- graph
- language
- codellama
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LegalViz introduces the first multilingual dataset for visualizing
  legal documents using Graphviz DOT language, covering 23 languages and 7,010 legal
  document-graph pairs. The dataset enables automatic generation of legal diagrams
  that identify legal entities, transactions, legal sources, and statements from complex
  legal texts.
---

# LegalViz: Legal Text Visualization by Text To Diagram Generation

## Quick Facts
- arXiv ID: 2502.06147
- Source URL: https://arxiv.org/abs/2502.06147
- Authors: Eri Onami; Taiki Miyanishi; Koki Maeda; Shuhei Kurita
- Reference count: 38
- Introduces first multilingual dataset for legal text visualization covering 23 languages

## Executive Summary
LegalViz introduces the first multilingual dataset for visualizing legal documents using Graphviz DOT language, covering 23 languages and 7,010 legal document-graph pairs. The dataset enables automatic generation of legal diagrams that identify legal entities, transactions, legal sources, and statements from complex legal texts. Finetuned models on LegalViz significantly outperform few-shot and GPT models, achieving higher scores across all evaluation metrics including graph structure accuracy and legal content extraction. This demonstrates the effectiveness of LegalViz in enhancing legal document visualization capabilities for both experts and non-experts.

## Method Summary
LegalViz employs supervised fine-tuning of LLMs (CodeLlama, Llama3, Gemma2) using the Hugging Face trl library to generate Graphviz DOT code from legal text. The dataset consists of 7,010 pairs of legal documents and corresponding DOT graphs, split into 4,710 train, 1,150 validation, and 1,150 test samples. Models are trained with global batch size 32, max sequence length 4096, and FP32 precision. Evaluation uses three-level hierarchical F1 metrics (Graph, Graph&Node, Graph&Node&Edge) based on BERTScore alignment, plus legal content extraction micro-F1 for entities, relations, sources, and statements.

## Key Results
- Finetuned Gemma-2-9B achieved highest scores across all evaluation metrics (Graph F1: 27.5%, Graph&Node F1: 21.4%, Graph&Node&Edge F1: 16.8%)
- GPT-3.5-Turbo frequently produced invalid DOT syntax and treated all nodes as legal entities
- GPT-4 extracted entities but often left them disconnected without edges
- Legal statements scored lowest due to lack of distinctive keywords in the text

## Why This Works (Mechanism)

### Mechanism 1: Constrained Graph Formalism Reduces Hallucination
The Graphviz DOT language constrains output to specific node shapes and edge types, forcing explicit entity typing and relationship definition. This reduces ambiguous or unfounded outputs compared to free-form text generation. The formalism acts as a scaffold that improves legal entity and relationship extraction accuracy, though it may omit complex legal nuances that cannot be represented in static graphs.

### Mechanism 2: Multilingual Alignment via Unified Schema
A shared graph annotation schema across 23 languages enables cross-lingual transfer, allowing models to leverage patterns from high-resource languages (English, French) to improve performance on lower-resource ones (Maltese, Latvian). The language-agnostic representation space assumes sufficient similarity in EU legal concepts across languages, though fundamental differences in legal system structures could introduce systematic errors.

### Mechanism 3: Hierarchical Evaluation Disentangles Structure from Semantics
The three-level evaluation (Graph → Graph&Node → Graph&Node&Edge) enables interpretable assessment by separating structural correctness from semantic accuracy. BERTScore-based node alignment via bipartite matching allows flexible matching of semantically similar legal text, while progressive F1 metrics provide diagnostic feedback at increasing levels of granularity.

## Foundational Learning

- **Graphviz DOT language**: Why needed? The entire dataset and task are built on DOT syntax; you must understand node shapes, edge types, and graph properties to interpret outputs and debug failures. Quick check: Can you write DOT code that creates a directed graph with two nodes (A, B) and a labeled edge "represents"?

- **Legal syllogism**: Why needed? The paper states that legal understanding requires "following legal syllogisms... and applying them to specific case facts." Quick check: Given a rule "If X, then Y" and a fact "X occurred," what is the conclusion in syllogistic reasoning?

- **BERTScore and bipartite matching**: Why needed? The evaluation pipeline uses BERTScore for node similarity and bipartite matching for alignment between generated and reference graphs. Quick check: Why might exact string matching fail for evaluating legal text similarity compared to contextual embeddings?

## Architecture Onboarding

- **Component map**: Legal text (avg 109 words) in 23 EU languages → LLM (CodeLlama, Llama3, Gemma2) generates DOT code → Graphviz diagram with 4 node types (entities, sources, statements, relations) and typed edges → 3-level F1 metrics + legal content extraction per node type

- **Critical path**: 1) Legal text → DOT code generation (primary task) 2) DOT code → Graphviz rendering (syntax validation) 3) Generated graph → Node alignment → Hierarchical F1 evaluation 4) Legal content extraction per node type

- **Design tradeoffs**: DOT vs. free-form diagrams enforces structure but limits expressiveness; 23 languages vs. depth per language provides broad coverage but variable quality; BERTScore vs. exact match offers flexible evaluation but potential semantic drift

- **Failure signatures**: GPT-3.5-Turbo treats all nodes as legal entities (incorrect double octagons); GPT-4 extracts entities but leaves them disconnected; legal statements score lowest due to lack of distinctive keywords; lower-resource languages (Maltese, Latvian, Lithuanian) systematically underperform

- **First 3 experiments**: 1) Replicate Gemma2-9B fine-tuning on English subset to establish baseline (expected G-N-E ~27.5%) 2) Test cross-lingual transfer by training on English only and evaluating on French vs. Hungarian 3) Ablate legal statement nodes to measure their contribution to overall evaluation scores

## Open Questions the Paper Calls Out
None

## Limitations
- Limited legal domain transferability beyond EU law, potentially not generalizing to other legal systems or specialized regulatory frameworks
- Sparse evidence for multilingual transfer claims with only moderate relatedness to general text-to-diagram tasks
- Evaluation metric validity uncertainties regarding whether BERTScore captures legally significant distinctions

## Confidence
- **High Confidence**: Dataset construction methodology, baseline model implementation using Hugging Face trl, and the core observation that fine-tuned models outperform few-shot/GPT baselines on LegalViz evaluation metrics
- **Medium Confidence**: Claims about constrained Graphviz formalism reducing hallucination and improving legal entity extraction, due to limited corpus evidence for legal-specific benefits
- **Low Confidence**: Claims about multilingual alignment benefits across diverse EU legal systems and the sufficiency of BERTScore for capturing legally meaningful semantic equivalence

## Next Checks
1. Evaluate LegalViz-trained models on legal texts from non-EU sources (e.g., US case law or international commercial arbitration) to assess domain transferability beyond EUR-LEX

2. Compare BERTScore-based evaluation against expert legal judgment on sample outputs to determine if legally significant distinctions (modal verbs, temporal obligations, conditional statements) are being captured

3. Systematically test models trained on high-resource languages (English, French) on lower-resource EU languages, comparing performance against monolingual baselines to quantify actual transfer benefits in the legal domain