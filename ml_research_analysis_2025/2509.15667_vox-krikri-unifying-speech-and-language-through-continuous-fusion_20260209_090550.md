---
ver: rpa2
title: 'VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion'
arxiv_id: '2509.15667'
source_url: https://arxiv.org/abs/2509.15667
tags:
- fusion
- speech
- language
- audio
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel framework for integrating pre-trained
  language models (LLMs) with acoustic encoder-decoder architectures, such as Whisper,
  by leveraging a continuous, audio-conditioned text space as an intermediate fusion
  mechanism. Unlike prior approaches that rely on direct audio embeddings, this method
  fuses Whisper's decoder hidden states with those of an LLM via cross-modal attention,
  supporting both offline and streaming applications.
---

# VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion

## Quick Facts
- arXiv ID: 2509.15667
- Source URL: https://arxiv.org/abs/2509.15667
- Reference count: 0
- First Greek speech LLM achieving ~20% relative WER improvement over Whisper-large-v3

## Executive Summary
This work introduces VOX-KRIKRI, a novel framework for integrating pre-trained language models with acoustic encoder-decoder architectures through continuous fusion in an audio-conditioned text space. Unlike prior approaches using direct audio embeddings, this method leverages Whisper's decoder hidden states as an intermediate fusion medium, achieving state-of-the-art ASR performance on Greek benchmarks. The model supports both offline and streaming applications through configurable fusion layers and causal masking strategies.

## Method Summary
The framework fuses Whisper-large-v3 decoder hidden states (audio-conditioned text representations) with Llama-KriKri-8B LLM hidden states via cross-modal attention at configurable injection layers. Training uses LoRA adapters (~744M trainable parameters) on ~3300 hours of Greek speech across six datasets. Causal masking with soft proportional alignment enables streaming ASR, while full-sequence fusion supports offline applications. Cross-modal alignment is analyzed using rCCA between speech and language modalities.

## Key Results
- State-of-the-art Greek ASR performance with ~20% relative WER improvement over Whisper-large-v3
- Intermediate-to-late fusion layers (15, 21, 30) outperform early fusion layers (1, 9)
- VoxKrikri-21 achieves best overall results across three benchmark datasets
- Enhanced cross-modal alignment demonstrated through rCCA analysis

## Why This Works (Mechanism)

### Mechanism 1
Fusing Whisper's decoder hidden states (audio-conditioned text) with LLM hidden states via cross-attention improves ASR accuracy compared to direct audio embedding approaches. The mechanism operates in an intermediate space where Whisper's decoder outputs already encode audio→text semantics, allowing cross-modal attention to align linguistically-grounded features with LLM states at chosen injection layers.

### Mechanism 2
Intermediate-to-late fusion layers (15–21) yield stronger cross-modal alignment and lower WER than early fusion layers (1–9). Deeper layers capture higher-level semantics, enabling the LLM to integrate richer semantic features where they can most effectively influence output generation.

### Mechanism 3
Causal masking with soft proportional alignment enables streaming ASR while preserving autoregressive consistency. Each text token Y_t attends only to audio frames up to a proportional boundary s_t = floor(S/T · t), enforced via additive masking in attention logits, maintaining monotonicity and temporal consistency.

## Foundational Learning

- **Cross-Attention in Transformers**: Understanding queries/keys/values, attention masking, and how external features modulate hidden states is essential to grasp how Whisper decoder states attend to LLM states. *Quick check*: How does masking the attention logits before softmax restrict information flow across time steps?

- **Encoder–Decoder Speech Architectures (Whisper)**: The method leverages Whisper's decoder outputs—already conditioned on audio—as the fusion medium; knowing what these representations encode is critical. *Quick check*: What is the difference between Whisper encoder outputs and decoder hidden states in terms of semantic abstraction?

- **LoRA (Low-Rank Adaptation)**: The LLM backbone uses LoRA for parameter-efficient adaptation (~7.5% trainable parameters), affecting which parameters are updated during fusion training. *Quick check*: If LoRA rank r=8 and α=16, how does that influence the effective learning capacity compared to full fine-tuning?

## Architecture Onboarding

- **Component map**: Frozen Whisper-large-v3 encoder-decoder → last decoder hidden states (A_s) → Llama-KriKri-8B with LoRA adapters → cross-modal attention module at injection layer I → masked attention outputs → fused hidden states

- **Critical path**: 1) Forward pass through Whisper decoder to obtain A_s; 2) At LLM layer I, compute cross-attention: LLM hidden states as queries, A_s as keys/values; 3) Apply causal mask M_{t,s} if streaming mode; 4) Add cross-attention output to LLM hidden state; continue forward pass

- **Design tradeoffs**: Early fusion (I=1) provides more audio influence throughout the LLM but risks semantic misalignment at shallow layers; intermediate-to-late fusion (I=15, 21, 30) offers better semantic alignment and lower WER; full-sequence fusion gives slightly better accuracy than causal but requires offline processing

- **Failure signatures**: WER plateau or degradation at early injection layers indicates surface-level fusion interfering with LLM's language priors; streaming mode WER significantly worse than full-sequence on variable-length speech suggests proportional alignment may not fit data; training divergence with LoRA requires checking hyperparameters

- **First 3 experiments**: 1) Layer ablation: Train VoxKrikri-1, -9, -15, -21, -30 on held-out subset; compare WER to identify optimal injection depth; 2) Fusion mode comparison: For best layer, evaluate full-sequence vs causal fusion on streaming benchmarks to quantify latency-accuracy tradeoffs; 3) Alignment analysis: Compute rCCA between LLM layer representations and Whisper decoder features on validation set; correlate alignment scores with WER

## Open Questions the Paper Calls Out

- **Multi-layer injection strategy**: Would combining early and late fusion layers yield better performance than single-layer fusion? The authors state interest in exploring this but did not test it.

- **Beyond transcription**: Can the framework maintain advantages on generative tasks like speech question answering or real-time translation? The paper exclusively evaluates on ASR.

- **Learned dynamic alignment**: Is the fixed proportional alignment optimal for variable speaking rates, or would a learned monotonic attention mechanism perform better? The rigid mathematical mapping may misalign fast or slow speech segments.

## Limitations
- Reliance on Whisper decoder states assumes consistent representation quality across diverse acoustic conditions without direct validation against alternatives
- Fixed proportional causal masking may not handle disfluencies or non-linear audio-text alignment effectively
- Lack of specified training hyperparameters and architectural details limits reproducibility and robustness assessment

## Confidence

**High Confidence**: Architectural framework and implementation details are clearly specified with internally consistent results
**Medium Confidence**: ~20% WER improvement claims supported by ablation studies but lack external validation
**Low Confidence**: Assumptions about optimal fusion space and causal masking generalization are not rigorously tested

## Next Checks

1. **Cross-Modal Alignment Robustness**: Evaluate rCCA alignment scores and WER performance under varying acoustic conditions (noise, accents, speaking rates) to assess stability of intermediate-to-late fusion advantage

2. **Causal Masking Generalization**: Test causal fusion mode on speech with significant disfluencies or non-linear alignment to identify potential misalignment artifacts

3. **Hyperparameter Sensitivity**: Conduct ablation studies on LoRA rank, scaling factor, and attention architecture to determine model sensitivity and establish robust baseline