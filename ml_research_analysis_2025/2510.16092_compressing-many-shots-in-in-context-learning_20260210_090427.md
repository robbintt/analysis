---
ver: rpa2
title: Compressing Many-Shots in In-Context Learning
arxiv_id: '2510.16092'
source_url: https://arxiv.org/abs/2510.16092
tags:
- tokens
- compression
- icae
- training
- memcom
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of memory and computational inefficiency
  in many-shot in-context learning (ICL) by proposing a method to compress long context
  sequences. The core method, MemCom, uses two LLM stacks (Source-LLM and Memory-LLM)
  to perform layer-wise compression of the input sequence into a smaller set of memory
  tokens.
---

# Compressing Many-Shots in In-Context Learning

## Quick Facts
- arXiv ID: 2510.16092
- Source URL: https://arxiv.org/abs/2510.16092
- Authors: Devvrit Khatri; Pranamya Kulkarni; Nilesh Gupta; Yerram Varun; Liqian Peng; Jay Yagnik; Praneeth Netrapalla; Cho-Jui Hsieh; Alec Go; Inderjit S Dhillon; Aditya Kusupati; Prateek Jain
- Reference count: 21
- Primary result: MemCom compresses 3k-6k token many-shot prompts by 3x-8x with minimal accuracy loss, outperforming baselines by 20-30% at high compression ratios

## Executive Summary
Many-shot in-context learning (ICL) requires processing long sequences but suffers from high memory and computational costs due to quadratic attention complexity. This paper introduces MemCom, a method that compresses many-shot representations into fewer memory tokens while maintaining task accuracy. The approach uses two LLM stacks (Source-LLM and Memory-LLM) with layer-wise cross-attention to compress context sequences, enabling the target model to attend only to the compressed representation. Evaluations on classification tasks show MemCom consistently outperforms strong baselines including ICAE++ and simple shot-reduction methods, maintaining high accuracy even at 8x compression ratios where baselines degrade sharply.

## Method Summary
MemCom addresses memory inefficiency in many-shot ICL by compressing long context sequences into fewer memory tokens using two LLM stacks. The Source-LLM processes the full input sequence while the Memory-LLM, initialized from the target model's weights, uses cross-attention layers to compress the Source-LLM's representations into learnable soft tokens. Training proceeds in two phases: Phase 1 freezes LLM weights and trains only the cross-attention modules and memory tokens on next-token prediction; Phase 2 optionally unfreezes all weights for fine-tuning. At inference, the target model attends only to the compressed memory tokens, significantly reducing memory usage and attention computation while maintaining accuracy.

## Key Results
- MemCom maintains accuracy with minimal degradation (typically <10%) at 3x-8x compression ratios
- Outperforms ICAE++ and simple shot-reduction baselines by 20-30% at high compression ratios
- Layer-wise compression provides higher fidelity than final-layer compression used in prior work
- Full compressor capacity is necessary; LoRA adapters alone are insufficient for high compression ratios

## Why This Works (Mechanism)

### Mechanism 1
Layer-wise compression provides higher fidelity than compressing only the final hidden states. MemCom introduces cross-attention modules at every transformer layer, allowing the Memory-LLM to query Source-LLM representations at corresponding depths. This enables the target model to access compressed views of the context at each stage of processing, capturing information distributed across different layers (e.g., early layers for syntax, later layers for semantics).

### Mechanism 2
Compressor capacity is a bottleneck for long-context many-shot compression; parameter-efficient adapters are insufficient. MemCom utilizes two full LLM stacks rather than low-rank adapters, providing sufficient expressive power to encode 3k-6k tokens. The information density in many-shot prompts is too high to be captured by LoRA approximations without significant semantic drift.

### Mechanism 3
Decoupling compression module training from the pretrained backbone stabilizes learning. The two-phase training approach first trains only the randomly initialized cross-attention modules and memory tokens while freezing LLM weights, then optionally unfreezes the full stack. This prevents the noise from randomly initialized modules from destabilizing pretrained parameters.

## Foundational Learning

**Key-Value (KV) Cache**: The KV cache stores attention keys/values for every token in the context. Understanding this is crucial because MemCom aims to compress the KV cache to reduce memory/compute. *Quick check*: Does compressing the KV cache reduce the time complexity of the attention operation for the target model?

**Cross-Attention**: The core operation involves memory tokens "querying" source tokens. Cross-Attention allows a sequence (Memory) to extract information from another sequence (Source) without merging them into a single input stream. *Quick check*: In MemCom, do the source tokens attend to the memory tokens, or do the memory tokens attend to the source tokens?

**Soft Tokens (Prompt Tuning)**: The compressed representation consists of $m$ "soft tokens" (continuous vectors), not natural language text. These are optimized via gradient descent rather than discrete text selection. *Quick check*: Can the compressed memory tokens be decoded back into English text to inspect what was retained?

## Architecture Onboarding

**Component map**: Source-LLM (processes raw input) -> Memory-LLM (compresses via cross-attention) -> Target-LLM (attends to compressed output)

**Critical path**:
1. Initialize Source-LLM and Memory-LLM with Target-LLM weights
2. Add randomly initialized 1-head cross-attention modules to Memory-LLM layers
3. Phase 1: Freeze LLM weights; train only Cross-Attention + Memory Tokens on next-token prediction
4. Phase 2: Unfreeze all weights; fine-tune the full compressor stack (optional)
5. Inference: Discard Source-LLM. Feed test input to Target-LLM with compressed Memory Token KV cache

**Design tradeoffs**:
- 1-head vs. Multi-head Cross-Attention: Simpler 1-head attention performs best, suggesting increased compressor capacity doesn't always align with frozen Target-LLM expectations
- Compression Ratio (3x vs 8x): High ratios induce sharp degradation in baselines; MemCom is robust, but Phase 2 training becomes critical
- Autoencoder Loss: Explicitly discarded due to training instability

**Failure signatures**:
- Training Instability: Using autoencoder loss or too high learning rate in Phase 2
- Capacity Bottleneck: Using LoRA-only adapters (ICAE baseline)

**First 3 experiments**:
1. Sanity Check (Phase 1): Train MemCom on small pretraining data slice and verify performance beats "fewer shots" baseline on classification
2. Ablation on Layers: Compare "Final-Layer Compression" vs. "Layer-wise Compression" at fixed 6x ratio
3. Attention Head Ablation: Compare 1-head cross-attention vs. Multi-Query Attention for the compressor

## Open Questions the Paper Calls Out

**Open Question 1**: Does MemCom yield performance gains for generation tasks when applied to input sequences significantly longer than 6k tokens studied? The paper notes extending to longer sequences could make compression more impactful for generation tasks.

**Open Question 2**: What specific factors cause MemCom to underperform ICAE++ on the HWU64 dataset when using Gemma2-2B? The paper observes this reverse trend but provides no analysis for why layer-wise compression fails relative to full-forward pass compression in this specific instance.

**Open Question 3**: Can the training efficiency of the MemCom compressor be improved to reduce the substantial compute requirement (80-160B tokens) and handle quadratic attention complexity? The current method relies on training two full LLM stacks on massive token counts.

## Limitations

- Pretraining Data Domain Sensitivity: All compression training uses general pretraining data; effectiveness with domain-specific data is not investigated
- Memory Token Interpretability: Compressed soft tokens lack semantic interpretation or qualitative analysis
- Computational Overhead: Doesn't quantify total computational cost (training + inference) compared to using full context

## Confidence

**High Confidence**: Layer-wise compression outperforms final-layer compression; full compressor capacity is necessary; two-phase training provides stability

**Medium Confidence**: MemCom's robustness at 8x compression compared to baselines; 1-head cross-attention optimality; marginal benefit of Phase-2 training

**Low Confidence**: Claims about information distribution across transformer layers; necessity of full LLM stacks vs. lighter architectures; generalizability beyond 5 tested classification tasks

## Next Checks

1. **Layer Compression Ablation**: Systematically evaluate performance when cross-attention is applied only to layers {0, 4, 8, 12} versus all layers to validate layer criticality

2. **Compressor Capacity Study**: Compare MemCom performance against modified versions using LoRA adapters with varying ranks (8, 32, 128) to quantify capacity thresholds

3. **Domain Transfer Evaluation**: Train compressor on domain-specific data versus general pretraining data, then evaluate on domain-specific classification tasks to assess robustness to domain shift