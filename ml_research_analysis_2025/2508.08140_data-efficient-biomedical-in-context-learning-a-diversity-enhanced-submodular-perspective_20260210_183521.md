---
ver: rpa2
title: 'Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular
  Perspective'
arxiv_id: '2508.08140'
source_url: https://arxiv.org/abs/2508.08140
tags:
- biomedical
- demonstrations
- dual-div
- learning
- macro-f1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficient in-context learning
  (ICL) in biomedical NLP, where large language models need to adapt to unseen tasks
  with limited examples. The proposed Dual-Div framework introduces a two-stage retrieval
  and ranking process that optimizes both representativeness and diversity when selecting
  demonstration examples from large corpora.
---

# Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective

## Quick Facts
- **arXiv ID:** 2508.08140
- **Source URL:** https://arxiv.org/abs/2508.08140
- **Reference count:** 40
- **Key outcome:** Dual-Div framework achieves up to 5% higher macro-F1 scores than baselines on biomedical NER, RE, and TC tasks using Llama 3.1 and Qwen 2.5

## Executive Summary
This paper addresses the challenge of efficient in-context learning (ICL) in biomedical NLP, where large language models need to adapt to unseen tasks with limited examples. The proposed Dual-Div framework introduces a two-stage retrieval and ranking process that optimizes both representativeness and diversity when selecting demonstration examples from large corpora. By leveraging submodular optimization and determinantal point processes, Dual-Div identifies diverse, non-redundant examples that enhance LLM performance. Evaluated on three biomedical NLP tasks (NER, RE, TC) using Llama 3.1 and Qwen 2.5 with three different retrievers, Dual-Div consistently outperforms baselines, achieving up to 5% higher macro-F1 scores. The framework demonstrates robustness to prompt permutations and class imbalance, with findings indicating that diversity in initial retrieval is more critical than ranking-stage optimization. Limiting demonstrations to 3-5 examples maximizes performance efficiency.

## Method Summary
The Dual-Div framework employs a two-stage process for demonstration selection in biomedical ICL. Stage 1 uses submodular optimization (f(S) = C(S) + λD(S)) with lazy greedy algorithms to retrieve k₁=100 diverse candidates from corpus V based on coverage and determinantal point processes. Stage 2 applies conditional ranking to select the final k=3-5 demonstrations. The method was evaluated on three biomedical tasks (ChemProt NER, DDI RE, HealthAdvice TC) using Llama 3.1-8B and Qwen 2.5-7B with three retrievers (BGE-Large, BMRetriever, MedCPT). Optional LoRA warm-up (rank=64, alpha=32; 2000-4000 steps) was applied. Performance was measured via macro-F1 and accuracy, with results averaged over 5 runs.

## Key Results
- Dual-Div achieves up to 5% higher macro-F1 scores than baseline selection methods
- Stage 1 diversity optimization contributes more to performance than Stage 2 ranking
- Optimal demonstration count is 3-5 examples, with diminishing returns beyond this range
- BGE-Large retriever shows more consistent performance across different k values compared to domain-specific retrievers

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Maximizing the determinant of the similarity matrix selects diverse, non-redundant examples that improve LLM robustness.
- **Mechanism:** Determinantal point processes (DPPs) model subset selection where P(S) ∝ det(L_S). The determinant captures the "volume" spanned by embedding vectors—higher determinants indicate greater semantic dispersion, preventing clustered selections.
- **Core assumption:** Diverse demonstrations activate broader knowledge patterns in LLMs and reduce information redundancy, improving generalization.
- **Evidence anchors:**
  - [section 4.1]: "the determinant of the gram matrix L describes the 'volume' of the space spanned by all query representations"
  - [section 6.2]: UMAP visualization confirms "queries retrieved using the diversity metric exhibit greater dispersion"
  - [corpus]: KITE paper validates that example selection is critical given limited LLM context windows
- **Break condition:** When corpus examples are already highly similar (low-rank similarity matrix), diversity optimization yields diminishing returns.

### Mechanism 2
- **Claim:** Diversity optimization in Stage 1 (retrieval) contributes more to final performance than Stage 2 (ranking).
- **Mechanism:** Stage 1 reduces N→k₁ candidates using diversity+coverage, while Stage 2 only ranks k₁→k. When N >> k₁ >> k, the larger reduction dominates outcomes. Ablation study (Div*-S3 vs Div-S3*) shows Stage 1 diversity consistently improves accuracy.
- **Core assumption:** Early candidate pool quality determines ceiling for downstream ranking effectiveness.
- **Evidence anchors:**
  - [section 6.1]: "incorporating the diversity regularization term D(S) in the first stage leads to significant gains"
  - [section 6.1]: "particularly true when the candidate pool reduction in Stage 1 (N - k₁) is larger than Stage 2 (k₁ - k)"
  - [corpus]: Weak corpus evidence—related papers focus on selection methods but not specifically on stage-wise contribution analysis
- **Break condition:** When k₁ is too small or corpus is already homogeneous, Stage 1 optimization cannot provide sufficient diversity.

### Mechanism 3
- **Claim:** Fewer demonstrations (3-5) outperform larger sets because quality dominates quantity.
- **Mechanism:** Excessive examples introduce noise and redundant information, potentially confusing the LLM. The utility function f(S) = C(S) + λD(S) with submodular properties exhibits diminishing marginal gains—each additional example provides less incremental value.
- **Core assumption:** LLMs have limited effective context utilization; focused, diverse examples activate relevant knowledge more efficiently.
- **Evidence anchors:**
  - [section 6.3.2]: "increasing k from 5 to 10 often results in negligible or even negative gains"
  - [section 6.3.2]: "optimal macro-F1 performance is typically achieved at k = 3 or 5"
  - [corpus]: Submodular Context Partitioning paper confirms transformer quadratic complexity limits exemplar count
- **Break condition:** When tasks require extensive pattern coverage (many distinct classes or complex relationships), 3-5 examples may underfit.

## Foundational Learning

- **Concept: Submodular Functions**
  - **Why needed here:** The optimization objective f(S) = C(S) + λD(S) relies on submodularity to guarantee (1-1/e) approximation via greedy algorithms.
  - **Quick check question:** Can you explain why greedy optimization provides theoretical guarantees for monotone submodular functions?

- **Concept: Determinantal Point Processes (DPPs)**
  - **Why needed here:** DPPs provide the mathematical foundation for diversity optimization—understanding log-det formulation is essential for implementing D(S).
  - **Quick check question:** How does the determinant of a Gram matrix relate to the diversity of vectors in that set?

- **Concept: In-Context Learning vs Fine-Tuning**
  - **Why needed here:** Dual-Div optimizes demonstration selection specifically for ICL, which operates differently from parameter updates; understanding this distinction clarifies why diversity matters.
  - **Quick check question:** Why does ICL performance depend more heavily on demonstration composition than fine-tuning approaches?

## Architecture Onboarding

- **Component map:** Corpus V → [Retriever g(·)] → Embeddings E → [Stage 1: Submodular Optimization] → Candidates S* (k₁=100) → [Optional Annotation] → [Stage 2: Conditional Ranking] → Final Demonstrations T* (k=3-5) → [Prompt Construction] → [LLM Inference]

- **Critical path:** Stage 1 diversity optimization (lazy greedy on f(S)) → Stage 2 marginal gain ranking f({xᵢ}|Q) → Prompt template filling

- **Design tradeoffs:**
  - **Retriever choice:** Domain-specific (BMRetriever, MedCPT) vs. general (BGE-Large)—paper shows BGE-Large more robust across k values
  - **λ tuning:** Small λ maintains monotonicity; paper uses fixed λ but future work suggests dynamic adjustment
  - **k₁ vs k:** Larger k₁ increases Stage 1 compute but improves candidate quality; default k₁=100, k=3

- **Failure signatures:**
  - **Low macro-F1 with high accuracy:** Likely class imbalance—verify diversity term is active (check if D(S) included)
  - **High variance across permutations:** Stage 1 diversity may be insufficient; increase k₁ or verify retriever quality
  - **Performance plateaus at k=3:** Normal behavior; adding examples beyond 5 typically yields marginal gains
  - **Formatting errors in outputs:** Consider LoRA warmup (4,000 steps on domain data) as paper implemented

- **First 3 experiments:**
  1. **Retriever ablation:** Test BGE-Large vs. BMRetriever on your domain; expect 2-3% macro-F1 difference. Monitor UMAP dispersion of Stage 1 candidates.
  2. **Stage-wise contribution:** Compare Div-S3 (no diversity), Div*-S3 (Stage 1 diversity only), Div-S3* (Stage 2 only), and Dual-Div. Expect Stage 1 diversity to dominate gains per paper findings.
  3. **Budget sensitivity:** Sweep k ∈ {1, 3, 5, 10} on validation set. Identify optimal k before full evaluation—paper shows k=3-5 optimal for most tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the trade-off parameter $\lambda$ be dynamically adjusted during the optimization process rather than set as a fixed constant?
- **Basis in paper:** [explicit] The conclusion explicitly identifies exploring the "dynamic adjustment of the trade-off parameter $\lambda$" as a direction for future work.
- **Why unresolved:** The current framework treats $\lambda$ as a static hyperparameter, potentially limiting optimization across diverse biomedical datasets with varying class distributions.
- **What evidence would resolve it:** An adaptive algorithm that modulates $\lambda$ based on real-time corpus statistics or query characteristics, showing improved macro-F1 scores over static baselines.

### Open Question 2
- **Question:** How can the Dual-Div framework be adapted for efficient scaling to extremely large corpora?
- **Basis in paper:** [explicit] The authors list "efficient scaling to extremely large corpora" as a "practical need" in the conclusion, suggesting distributed variants of the lazy greedy algorithm.
- **Why unresolved:** The submodular optimization and DPP calculations incur significant computational overhead, making application to massive, unfiltered clinical databases difficult.
- **What evidence would resolve it:** A distributed implementation of the algorithm that maintains the (1-1/e) approximation guarantee while reducing retrieval latency on datasets orders of magnitude larger than those tested.

### Open Question 3
- **Question:** Does the finding that Stage 1 diversity is more critical than Stage 2 ranking hold for general-domain NLP tasks?
- **Basis in paper:** [inferred] The paper establishes the primacy of initial retrieval diversity based solely on experiments with specific biomedical tasks (NER, RE, TC).
- **Why unresolved:** Biomedical texts feature high terminology density and specific class imbalances; it is unclear if this prioritization of global diversity over conditional ranking transfers to general language tasks.
- **What evidence would resolve it:** Evaluation of Dual-Div on general-domain benchmarks (e.g., sentiment analysis, generic NER) to verify if Stage 1 diversity remains the dominant factor in performance gains.

## Limitations
- The paper does not specify the exact value of λ (diversity regularization parameter), which is critical for reproducibility and may significantly impact results
- While the framework shows robustness to prompt permutations, the analysis of permutation effects is limited to demonstrating non-degradation rather than exploring optimal orderings
- The theoretical analysis focuses on submodularity and approximation guarantees but lacks empirical validation of why diversity specifically improves LLM reasoning in biomedical contexts

## Confidence

**High confidence:** The two-stage retrieval and ranking approach (Stage 1 diversity optimization contributes more than Stage 2), the finding that 3-5 demonstrations are optimal, and the superiority over baselines (up to 5% macro-F1 improvement)

**Medium confidence:** The mechanism by which diversity improves LLM robustness (determinant-based selection), the claim that BGE-Large retriever is more robust across k values, and the effectiveness of submodular optimization for this task

**Low confidence:** The specific contribution of each retriever type (BMRetriever, MedCPT vs BGE-Large) due to limited ablation, and the generalizability of findings to other biomedical domains beyond the three tested tasks

## Next Checks
1. **λ Sensitivity Analysis:** Systematically sweep λ from 0.01 to 1.0 and measure its impact on both macro-F1 and diversity metrics to identify optimal trade-offs
2. **Stage 1 Candidate Pool Size:** Vary k₁ from 50 to 500 to determine if the 100-candidate default is optimal or if larger pools yield better final demonstrations
3. **Embedding Quality Impact:** Test the framework using embeddings from different pre-trained models (e.g., Sentence-BERT, domain-specific encoders) to quantify sensitivity to embedding quality and assess robustness across embedding spaces