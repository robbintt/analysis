---
ver: rpa2
title: Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box
arxiv_id: '2507.19455'
source_url: https://arxiv.org/abs/2507.19455
tags:
- clustering
- feature
- cluster
- clusters
- features
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Forest-Guided Clustering (FGC) is a novel explainability method
  that reveals both local and global structure in Random Forests by grouping instances
  according to shared decision paths. Unlike traditional feature-centric explanations,
  FGC clusters samples based on model-informed proximity, enabling interpretable segmentation
  aligned with the RF's internal logic.
---

# Forest-Guided Clustering -- Shedding Light into the Random Forest Black Box

## Quick Facts
- arXiv ID: 2507.19455
- Source URL: https://arxiv.org/abs/2507.19455
- Reference count: 40
- Primary result: Achieves ARI=0.98 on simulated data and uncovers biologically coherent subpopulations in AML transcriptomics

## Executive Summary
Forest-Guided Clustering (FGC) is a novel explainability method that reveals both local and global structure in Random Forests by grouping instances according to shared decision paths. Unlike traditional feature-centric explanations, FGC clusters samples based on model-informed proximity, enabling interpretable segmentation aligned with the RF's internal logic. Applied to a simulated dataset, FGC accurately recovered latent subclass structure (ARI = 0.98) and outperformed both classical clustering and post-hoc explanation methods. In an AML transcriptomic case study, FGC uncovered biologically coherent subpopulations, disentangled disease-relevant signals from confounders, and recovered known and novel gene expression patterns. FGC bridges the gap between performance and interpretability by providing structure-aware insights that go beyond feature-level attribution.

## Method Summary
FGC computes a proximity matrix from a trained Random Forest by counting how often pairs of instances land in the same terminal nodes across the ensemble. This matrix is transformed into a distance matrix and used as input for k-medoids clustering (PAM or CLARA). The optimal number of clusters is selected by balancing classification impurity (bias) against Jaccard stability across bootstrap samples. For each cluster, FGC computes local feature importance by comparing feature value distributions within the cluster to the global distribution using Wasserstein or Jensen-Shannon distance. The method produces both global and cluster-specific explanations that are aligned with the RF's decision structure.

## Key Results
- Achieved ARI=0.98 on simulated dataset with 3 latent subclasses in class 1
- Identified biologically coherent subpopulations in AML transcriptomics case study
- Outperformed classical clustering and post-hoc explanation methods in revealing model structure

## Why This Works (Mechanism)

### Mechanism 1: Proximity-Based Distance Definition
- Claim: If two instances frequently terminate in the same leaf nodes across the ensemble, they are functionally similar within the model's logic.
- Mechanism: FGC computes a proximity matrix $M_{ij}$ (frequency of co-occurrence in terminal nodes) and transforms it into a distance matrix $D_{ij} = 1 - M_{ij}$. This creates a metric space defined by decision rules rather than raw feature geometry.
- Core assumption: The Random Forest's decision paths capture meaningful interactions and non-linear relationships that define subgroups better than Euclidean distance in the original feature space.
- Break condition: If trees are extremely deep (low bias, high variance), individual paths may become unique, rendering the proximity matrix sparse and the distance metric uninformative.

### Mechanism 2: Supervised Cluster Selection (Bias-Variance Optimization)
- Claim: Selecting the number of clusters $k$ by jointly minimizing class impurity (bias) and maximizing Jaccard stability (variance) yields subgroups aligned with the prediction target.
- Mechanism: FGC iterates through candidate $k$ values, rejecting those with low bootstrapped stability (< 0.6 Jaccard similarity) and selecting the stable solution with the lowest class-balanced impurity.
- Core assumption: The "correct" segmentation of the data is one that is both reproducible (robust to resampling) and homogenous with respect to the target variable.
- Break condition: If the model is severely overfitted to noise, minimizing cluster bias may reinforce spurious correlations rather than true structure.

### Mechanism 3: Distributional Feature Attribution
- Claim: Features that define a specific decision region (cluster) will show a local distribution significantly different from the global distribution.
- Mechanism: FGC calculates local importance using Wasserstein or Jensen-Shannon distance to compare the distribution of feature values within a cluster against the background distribution of the whole dataset.
- Core assumption: High divergence scores indicate that a feature was a driving factor in the decision path leading to that cluster.
- Break condition: If a feature is uniformly important across *all* clusters (e.g., a global driver), its within-cluster distribution may not diverge significantly from the global distribution, potentially masking its relevance.

## Foundational Learning

- Concept: **Random Forest Proximity Matrix**
  - Why needed here: This is the input data for FGC. Unlike standard clustering which uses raw features, FGC clusters based on how the RF "sees" data similarity.
  - Quick check question: If two samples have a proximity score of 0.0, what does that mean regarding their path through the trees? (Answer: They never landed in the same terminal node across any tree).

- Concept: **k-medoids (PAM) vs. k-means**
  - Why needed here: FGC requires k-medoids because it operates on a pre-computed distance matrix; k-means requires coordinates which the proximity matrix does not provide.
  - Quick check question: Why is PAM computationally harder than k-means? (Answer: PAM typically requires $O(n^2)$ pairwise distance comparisons, whereas k-means is linear in $n$ per iteration).

- Concept: **Distribution Divergence (Wasserstein Distance)**
  - Why needed here: To understand *why* a cluster formed, you must measure how "shifted" a feature is within that cluster. Wasserstein distance quantifies this "shift" (Earth Mover's Distance).
  - Quick check question: If a feature has a value of 10 in a cluster but 0 everywhere else, would the Wasserstein distance be high or low? (Answer: High).

## Architecture Onboarding

- Component map: Trained Random Forest + Raw Dataset -> Proximity Engine -> Distance Matrix -> PAM/CLARA Clustering -> k Optimization -> Feature Attribution -> Visualizer
- Critical path: The **Distance Matrix Computation** is the primary bottleneck ($O(n^2)$ memory). If this step succeeds, the clustering and visualization are computationally manageable.
- Design tradeoffs:
  - **Memory vs. Speed:** Use standard NumPy arrays for speed (small data) vs. Memory-mapped arrays (`memmap`) for large data (slower, but scales to 40GB+ datasets).
  - **Accuracy vs. Scalability:** Use PAM for exact clustering on small data vs. CLARA (subsampling) for approximation on large data.
  - **Distance Metric:** Wasserstein is better for continuous features; Jensen-Shannon is better for categorical/discrete features.
- Failure signatures:
  - **Memory Overflow:** Happens if $N > 50,000$ without `memmap` enabled.
  - **Cluster Instability:** Jaccard index < 0.6. Suggests the RF is too deep/noisy or $k$ range is wrong.
  - **Uniform Importance:** All features look equally important. Suggests the model failed to learn distinct decision paths or the global distribution is too dominant.
- First 3 experiments:
  1. **Validation Run:** Use the provided simulated dataset (Abstract/Results). Verify you achieve ARI $\approx 0.98$.
  2. **Scalability Test:** Profile memory usage on a dummy dataset with 10k, 20k, and 50k samples to verify the `memmap` backend is activating correctly.
  3. **Sensitivity Analysis:** Run FGC on a trained RF with `max_depth=2` vs `max_depth=20`. Observe how cluster stability degrades or improves based on tree complexity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Forest-Guided Clustering be effectively adapted to interpret other tree-based ensemble models, specifically gradient-boosted decision trees (GBDTs)?
- Basis in paper: [explicit] The authors explicitly state that future extensions include "adapting FGC to support other tree-based models such as gradient-boosted decision trees."
- Why unresolved: The current method and proximity calculations are tailored for Random Forests; GBDTs differ structurally (e.g., sequential boosting vs. bagging) which may require algorithmic adjustments.
- What evidence would resolve it: A successful application of a modified FGC framework to XGBoost or LightGBM models, demonstrating interpretable clusters on benchmark datasets.

### Open Question 2
- Question: Does incorporating internal tree pruning strategies improve the robustness of FGC when applied to Random Forest regression tasks?
- Basis in paper: [explicit] The authors note that regression trees are typically deeper, increasing the sparsity of shared decision paths, and propose "internal tree pruning strategies" as a potential solution to improve clustering robustness.
- Why unresolved: The increased depth and branching in regression forests dilute proximity signals, making cluster extraction difficult; the efficacy of pruning to mitigate this has not been tested.
- What evidence would resolve it: A comparative study on regression datasets showing that pruned forests yield significantly higher cluster stability (Jaccard index) and lower variance than non-pruned forests.

### Open Question 3
- Question: Can alternative clustering algorithms beyond k-medoids be integrated into the FGC framework to enhance flexibility and generalizability?
- Basis in paper: [explicit] The authors list "integrating alternative clustering algorithms beyond k-medoids" as a necessary extension to enhance the method's flexibility across diverse scenarios.
- Why unresolved: The current reliance on k-medoids (and its CLARA approximation) may not be optimal for all data geometries or proximity distributions derived from different RF models.
- What evidence would resolve it: Benchmarking FGC using algorithms like spectral clustering or hierarchical clustering, showing improved performance or interpretability on datasets where k-medoids struggles.

## Limitations

- Performance on high-dimensional datasets (>50,000 samples or >10,000 features) remains untested at scale, with the memmap optimization being theoretical rather than empirically validated.
- The stability threshold of 0.6 Jaccard index is heuristic; optimal values may vary by domain and RF architecture.
- The claim that Wasserstein distance captures "driving factors" in decision paths assumes the global distribution is meaningful, which may not hold for heterogeneous populations.

## Confidence

- **High Confidence**: Local clustering quality (ARI=0.98 on simulated data), basic mechanism of proximity-based distance computation, PAM/CLARA clustering workflow.
- **Medium Confidence**: Supervised cluster selection method's generalizability beyond the tested domains, distributional feature attribution's interpretability across diverse feature types.
- **Low Confidence**: Performance claims for datasets exceeding 10,000 samples, the specific 0.6 stability threshold's universal applicability, comparison claims against unspecified "post-hoc explanation methods."

## Next Checks

1. **Scalability Validation**: Test FGC on synthetic datasets with 50k-100k samples to empirically verify the memmap backend prevents memory overflow and assess runtime scaling.
2. **Cross-Domain Stability**: Apply FGC to a dataset from a different domain (e.g., image features or tabular business data) and test whether the 0.6 Jaccard threshold consistently selects meaningful clusters.
3. **Ablation Study**: Compare FGC's cluster quality when using alternative distance metrics (Euclidean, cosine) versus the RF proximity-based distance to isolate the benefit of model-informed clustering.