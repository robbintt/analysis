---
ver: rpa2
title: Benchmarking AI scientists in omics data-driven biological research
arxiv_id: '2505.08341'
source_url: https://arxiv.org/abs/2505.08341
tags:
- cell
- data
- biological
- scientists
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces BAISBench, a benchmark for evaluating AI
  scientists in data-driven biological discovery using real single-cell transcriptomic
  datasets. The benchmark includes two tasks: cell type annotation across 15 expert-labeled
  datasets and scientific discovery through 193 multiple-choice questions derived
  from 41 published studies.'
---

# Benchmarking AI scientists in omics data-driven biological research

## Quick Facts
- arXiv ID: 2505.08341
- Source URL: https://arxiv.org/abs/2505.08341
- Authors: Erpai Luo; Jinmeng Jia; Yifan Xiong; Xiangyu Li; Xiaobo Guo; Baoqi Yu; Lei Wei; Xuegong Zhang
- Reference count: 40
- Primary result: Introduces BAISBench benchmark showing state-of-the-art AI scientists underperform human experts in both cell type annotation and scientific discovery tasks, though demonstrating strong potential in executing standard workflows.

## Executive Summary
This paper introduces BAISBench, a benchmark for evaluating AI scientists in data-driven biological discovery using real single-cell transcriptomic datasets. The benchmark includes two tasks: cell type annotation across 15 expert-labeled datasets and scientific discovery through 193 multiple-choice questions derived from 41 published studies. Systematic evaluation of state-of-the-art AI scientists shows they underperform human experts in both tasks, though they demonstrate strong potential in executing standard workflows and generating data-driven biological insights. The benchmark provides a realistic framework for assessing current AI capabilities and guiding future development of AI systems for biological research.

## Method Summary
BAISBench evaluates AI scientists through two complementary tasks: BAIS-DPTA for cell type annotation and BAIS-SD for scientific discovery. For DPTA, AI scientists process 15 single-cell RNA-seq datasets in h5ad format, executing six required workflow steps (quality control, normalization, highly variable gene selection, dimensionality reduction, clustering, marker gene identification) and producing cell type annotations scored against expert labels using a hierarchical uHAF framework. For SD, AI scientists analyze 41 CellxGene datasets and answer 193 multiple-choice questions derived from published discoveries. The benchmark tests five AI systems (AutoBA, scChat, Biomni, Pantheon, STELLA) against human baselines from six graduate bioinformaticians, measuring performance through task-specific metrics that reward biological correctness and reproducibility.

## Key Results
- AI scientists underperform human experts on both cell type annotation (S_CTA scores significantly lower) and scientific discovery (S_SD accuracy significantly lower)
- Biomni with Claude Opus achieved highest SD performance (54.4%) but still below human baselines
- Most AI scientists fail at the final cell type assignment step despite successfully executing earlier workflow components
- Token consumption correlates positively with performance in discovery tasks, with STELLA's superior performance coming at highest computational cost

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Cell Type Scoring via uHAF
The uHAF (Unified Hierarchical cell Annotation Framework) provides organ-specific cell type trees with a scoring metric that assigns full credit for exact matches, 0.5 for parent-level matches, and 0.2 for grandparent/ancestor matches. This acknowledges biological correctness even when exact cell types aren't identified.

### Mechanism 2: Grounded Multiple-Choice Discovery Task
Questions are derived from published discoveries, requiring AI scientists to analyze datasets and select answers matching original findings. This tests whether AI can recover established conclusions from raw data rather than just performing isolated analyses.

### Mechanism 3: Base LLM Quality as Primary Performance Driver
Performance differences correlate strongly with underlying LLM capability, with stronger models like Claude Opus outperforming weaker ones. This suggests that for discovery tasks, model quality matters more than architectural sophistication.

## Foundational Learning

- **Concept: Single-cell RNA-seq workflow (QC → normalization → HVG selection → dimensionality reduction → clustering → marker identification)**
  - Why needed here: BAIS-DPTA explicitly evaluates whether AI scientists execute all six steps correctly; Table 1 shows this is baseline competence.
  - Quick check question: Name the six essential preprocessing steps and explain why HVG selection precedes dimensionality reduction.

- **Concept: Cell type annotation strategies (marker overlap vs. module scoring)**
  - Why needed here: Performance differences correlate with annotation strategy—Pantheon/STELLA use module scoring, others use marker overlap.
  - Quick check question: Compare marker gene overlap vs. Scanpy score_genes approaches for cell type assignment.

- **Concept: Agent execution vs. reasoning decoupling**
  - Why needed here: Paper emphasizes AI scientists fail from "brittle execution, error accumulation" not isolated reasoning errors—system-level behavior matters.
  - Quick check question: Explain why an LLM might generate correct analysis plans but fail to produce valid cell type annotations.

## Architecture Onboarding

- **Component map:**
  BAIS-DPTA: Raw h5ad → AI scientist (any strategy) → predicted labels → uHAF scoring
  BAIS-SD: Dataset + background + questions → AI scientist (analysis + reasoning) → answers → accuracy scoring
  Evaluated systems: AutoBA (pipeline automation), scChat (interactive), Biomni (general-purpose with retrieval), Pantheon (structured multi-agent), STELLA (hybrid multi-model)

- **Critical path:**
  1. Data ingestion in h5ad format with raw counts
  2. Preprocessing workflow execution (all 6 steps)
  3. For annotation: marker identification → label assignment via biological knowledge (no external tools like CellTypist)
  4. For discovery: data analysis + external knowledge integration → answer selection

- **Design tradeoffs:**
  - Complexity vs. reliability: More sophisticated agents (Biomni, Pantheon) don't always outperform simpler code-generation baselines on annotation
  - Token consumption vs. performance: Positive correlation in BAIS-DPTA; STELLA's superior SD performance comes with highest token usage
  - Specialization vs. flexibility: scChat's hard-coded tools prevent BAIS-SD participation but may improve consistency

- **Failure signatures:**
  - AutoBA: Identifies marker genes but cannot assign labels (annotation = 0)
  - GPT-4o baseline: Overly stringent QC thresholds causing excessive filtering
  - scChat: Hard-coded tools prevent custom workflow execution for discovery task
  - General: Plausible plans that fail to translate to executable code; shallow conclusions despite technically correct analyses

- **First 3 experiments:**
  1. Run GPT-4o code-generation baseline on one BAIS-DPTA dataset to establish lower bound; verify all 6 workflow steps execute.
  2. Test Biomni with two different base models (e.g., Claude Haiku vs. Opus) on 5 BAIS-SD questions to quantify model dependency.
  3. Compare annotation strategies on a single organ: marker overlap (Biomni default) vs. module scoring (Pantheon style) to isolate strategy effects.

## Open Questions the Paper Calls Out

### Open Question 1
How can benchmarks effectively quantify an AI scientist's ability to generate truly novel scientific hypotheses rather than merely reproducing established findings? The authors explicitly state that the BAIS-SD task design "does not aim to quantify creativity or novelty directly," focusing instead on alignment with reproducible, published conclusions. Scientific discovery is open-ended; creating ground truth for "novelty" requires validation that does not yet exist in literature.

### Open Question 2
To what extent does inter-individual variation among human experts affect the establishment of reliable performance baselines? The authors acknowledge that "inter-individual variation among human researchers was not systematically assessed" due to resource constraints. The current human baseline relied on partitioning questions among five graduate-level bioinformaticians, masking potential variance in expert performance.

### Open Question 3
What architectural improvements are required for AI scientists to proactively integrate external biological knowledge during reasoning rather than producing shallow interpretations? The discussion notes that "most AI scientists do not proactively integrate external biological knowledge during reasoning," which limits their reliability and depth of insight. Current systems often separate data analysis from knowledge retrieval, leading to correct technical outputs that lack biological nuance.

## Limitations

- Limited evaluation scope: Only 15 datasets and 41 papers may not capture full biological diversity
- Single-cell transcriptomics constraint: Results may not generalize to other omics modalities
- Question generation reliability: GPT-4o-assisted extraction process not fully validated
- Human baseline representativeness: 6 graduate bioinformaticians may not represent expert consensus

## Confidence

- High confidence: Benchmark methodology and scoring metrics (uHAF framework)
- Medium confidence: Performance comparisons between AI systems
- Low confidence: Generalizability to broader biological discovery tasks

## Next Checks

1. Test benchmark on additional single-cell datasets from different organs/conditions to assess robustness
2. Compare question extraction process with alternative LLM prompts or human validation
3. Evaluate AI scientists on novel datasets without published conclusions to test genuine discovery capability