---
ver: rpa2
title: 'Relevance to Utility: Process-Supervised Rewrite for RAG'
arxiv_id: '2509.15577'
source_url: https://arxiv.org/abs/2509.15577
tags:
- document
- documents
- answer
- utility
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the gap between retrieval relevance and generative
  utility in Retrieval-Augmented Generation (RAG) systems, where retrieved documents
  may be topically relevant but fail to support effective reasoning during answer
  generation. The authors propose R2U, a training framework that optimizes document
  rewriting by directly maximizing the probability of generating correct answers through
  process supervision.
---

# Relevance to Utility: Process-Supervised Rewrite for RAG

## Quick Facts
- arXiv ID: 2509.15577
- Source URL: https://arxiv.org/abs/2509.15577
- Reference count: 27
- R2U achieves 6.8% F1 improvement over naive RAG and 5.6% over best existing methods

## Executive Summary
This paper addresses a fundamental gap in Retrieval-Augmented Generation (RAG) systems where retrieved documents may be topically relevant but fail to support effective reasoning during answer generation. The authors propose R2U, a training framework that optimizes document rewriting by directly maximizing the probability of generating correct answers through process supervision. R2U approximates true utility via joint observation of rewriting and answering in the reasoning process, and distills this supervision by scaling from LLMs. Evaluated across multiple open-domain question-answering benchmarks, R2U demonstrates consistent improvements over strong bridging baselines.

## Method Summary
R2U introduces a process-supervised document rewriting framework for RAG systems that maximizes the probability of generating correct answers. The method operates in three stages: (1) Scaled Process Supervision where a teacher LLM rewrites each retrieved document separately to maximize answer generation probability, (2) Utility-based soft labeling that measures the generator's gain of the answer under rewritten contexts using Δℓ = ℓ(d') − ℓ(d), and (3) Student training through supervised fine-tuning (SFT) followed by direct preference optimization (DPO). The approach uses a utility threshold κ=4.0 and a minimum quality threshold τ=0.05 to construct training data, with a Llama-3.2-3B-Instruct backbone.

## Key Results
- R2U achieves an average F1 score increase of 6.8% over the naive RAG baseline
- R2U improves F1 by 5.6% over the best existing bridging methods
- Consistent performance gains across six benchmarks including HotpotQA, 2WikiMultihopQA, MuSiQue, AmbigQA, MS MARCO, and CRAG

## Why This Works (Mechanism)
R2U works by aligning document rewriting with actual answer generation utility rather than retrieval relevance. Traditional RAG systems optimize for retrieving relevant documents, but R2U recognizes that relevance doesn't guarantee utility for answer generation. By using process supervision where the teacher LLM jointly observes rewriting and answering, R2U captures the true utility of rewritten documents. The utility-based soft labeling mechanism (Δℓ = ℓ(d') − ℓ(d)) provides a direct signal for whether rewriting actually improves answer quality, creating a more effective training signal than relevance-based approaches.

## Foundational Learning
- **Process Supervision**: Why needed - to capture the joint effect of rewriting and answering rather than optimizing them separately; Quick check - verify teacher answers correctly before/after rewriting
- **Utility-based Soft Labeling**: Why needed - to measure actual improvement in answer generation rather than document relevance; Quick check - plot Δℓ distribution to ensure positive utility signals
- **Scaled LLM Distillation**: Why needed - to generate sufficient high-quality supervision data for training; Quick check - inspect sample rewritten documents for quality
- **Direct Preference Optimization**: Why needed - to fine-tune the rewriter using pairwise comparisons of utility; Quick check - monitor DPO loss convergence

## Architecture Onboarding

**Component Map**: Query -> Retrieved Documents -> Rewriter -> Utility Scoring -> Student Training -> Answer Generation

**Critical Path**: The critical path flows from query through the rewriter to the generator, where each rewritten document must improve the probability of generating correct answers. The utility scoring step (Δℓ computation) is essential as it provides the training signal.

**Design Tradeoffs**: The approach trades computational efficiency for accuracy by using 10 LLM calls per query for scaled supervision. The utility threshold κ=4.0 balances signal quality against quantity. The choice of Llama-3.2-3B-Instruct as the student balances model capacity with practical training requirements.

**Failure Signatures**: Common failure modes include negative Δℓ distributions indicating no utility improvement, rewriter outputs becoming generic, and DPO loss divergence. These can be diagnosed through statistical analysis of utility signals, manual inspection of outputs, and monitoring training curves respectively.

**3 First Experiments**:
1. Generate scaled process supervision data and verify answer leakage prevention by checking teacher answers without documents
2. Compute Δℓ distribution across a validation set to ensure positive utility signals exist
3. Train the student rewriter on a small subset and evaluate F1 improvement on a single benchmark

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies heavily on LLM-as-judge evaluations which may introduce subjectivity and bias
- The methodology is limited to English Wikipedia-style questions, limiting generalizability to other domains
- The answer leakage prevention mechanism is not thoroughly evaluated, with only brief mention of filtering queries

## Confidence
- **High Confidence**: The general methodology of process-supervised document rewriting is sound and well-motivated
- **Medium Confidence**: The claimed improvements over baselines are plausible but may vary with implementation details
- **Low Confidence**: The exact impact of utility-improvement signals and scaled supervision effectiveness cannot be fully assessed without more detailed specifications

## Next Checks
1. Implement the exact LLM-as-judge evaluation prompt used for MS MARCO and CRAG benchmarks to verify reproducibility
2. Vary the utility thresholds (κ=4.0, τ=0.05) and observe performance changes to validate sensitivity
3. Implement and evaluate the answer leakage prevention mechanism more thoroughly with quantitative analysis