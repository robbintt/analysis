---
ver: rpa2
title: Uncertainty-aware Long-tailed Weights Model the Utility of Pseudo-labels for
  Semi-supervised Learning
arxiv_id: '2503.09974'
source_url: https://arxiv.org/abs/2503.09974
tags:
- prediction
- data
- uncertainty
- dualpose
- labeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of pseudo-label selection in semi-supervised
  learning, specifically focusing on the challenges of setting confidence thresholds
  and dealing with overconfident models that make unreliable pseudo-labels. The proposed
  solution, Uncertainty-aware Ensemble Structure (UES), uses a lightweight ensemble
  framework to quantify sample uncertainty and prediction head uncertainty, transforming
  these uncertainties into long-tailed weights that avoid the need for fixed thresholds.
---

# Uncertainty-aware Long-tailed Weights Model the Utility of Pseudo-labels for Semi-supervised Learning

## Quick Facts
- arXiv ID: 2503.09974
- Source URL: https://arxiv.org/abs/2503.09974
- Authors: Jiaqi Wu; Junbiao Pang; Qingming Huang
- Reference count: 36
- Primary result: Uncertainty-aware ensemble structure achieves 3.47-7.29% PCK improvements on pose estimation and 0.2-0.26% accuracy gains on CIFAR classification tasks

## Executive Summary
This paper addresses the critical challenge of pseudo-label selection in semi-supervised learning by proposing the Uncertainty-aware Ensemble Structure (UES). The method tackles two fundamental problems: setting confidence thresholds and dealing with overconfident models that generate unreliable pseudo-labels. UES introduces a lightweight ensemble framework that quantifies both sample uncertainty and prediction head uncertainty, transforming these uncertainties into long-tailed weights that avoid the need for fixed thresholds. The approach ensures all unlabeled samples contribute to training while proportionally reducing the influence of unreliable pseudo-labels, demonstrating significant improvements across both classification and regression tasks.

## Method Summary
The Uncertainty-aware Ensemble Structure (UES) combines a Channel-based Ensemble (CBE) with M prediction heads and a Mean-based Uncertainty Estimator. For each unlabeled sample, the method computes sample uncertainty as the average MSE between each prediction head's output and the ensemble mean, then transforms this into long-tailed weights using a non-linear function that ensures even high-uncertainty samples receive non-zero weights. Prediction head uncertainty is calculated as the average MSE across a batch, with weights derived via softmax and smoothed with exponential moving average. The final loss combines supervised and ensemble terms, weighted by these uncertainty-aware weights. The architecture is designed to be applicable to both classification and regression tasks without modification.

## Key Results
- Pose estimation (Sniffing, FLIC, LSP datasets): 3.47-7.29% PCK improvements over baselines
- Classification (CIFAR-10/100): 0.2-0.26% accuracy gains
- Architecture-agnostic design applicable to both classification and regression tasks
- Effective enhancement of model robustness in semi-supervised learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Long-tailed weights enable all unlabeled samples to contribute to training while proportionally reducing influence of unreliable pseudo-labels.
- Mechanism: Sample uncertainty $u_i^S$ is transformed via $w_i^S = \frac{1}{u_i^S \cdot MAX(W^S) + 1}$, guaranteeing non-zero weights even for highest-uncertainty samples. This avoids binary threshold-based filtering.
- Core assumption: Early-stage incorrect pseudo-labels contain useful information; noise diminishes as training progresses and model improves.
- Evidence anchors:
  - [abstract] "The advantage of the long-tailed weights ensures that even unreliable pseudo-labels still contribute to enhancing the model's robustness."
  - [section III-B] "This trend aligns with the need for more accurate supervisory information in the middle and late stages of training."
- Break condition: If early-stage noise dominates gradient signals and prevents convergence, the assumption that incorrect labels aid initialization may not hold—especially with extremely limited labeled data (<10 samples).

### Mechanism 2
- Claim: Mean-based ensemble predictions provide a more stable reference for quantifying sample uncertainty than individual head predictions.
- Mechanism: Calculate reference distribution $\bar{p}_i = \frac{1}{M}\sum_{m=1}^{M} p_{(i,m)}$ from M prediction heads, then compute sample uncertainty as average MSE between each head's prediction and $\bar{p}_i$.
- Core assumption: Ensemble mean is closer to ground truth than individual predictions; variance from mean indicates prediction unreliability.
- Evidence anchors:
  - [abstract] "Mean-based samples uncertainty: Averaging predictions from multiple base models provides a reasonable reference for assessing uncertainty."
  - [section III-B, Eq. 1] Formal definition of uncertainty via MSE from ensemble mean.
- Break condition: If prediction heads are highly correlated (low diversity), ensemble mean offers no advantage over single-head predictions—uncertainty estimates become uninformative.

### Mechanism 3
- Claim: Weighting prediction heads by their batch-level uncertainty improves ensemble accuracy and reduces overfitting to individual samples.
- Mechanism: Head uncertainty $u_m^H$ computed as average MSE across batch; weights derived via softmax: $W^H = SOFTMAX(-U^H)$. EMA smoothing prevents fluctuation-induced instability.
- Core assumption: Heads that consistently deviate from ensemble mean are less reliable; batch-level aggregation mitigates sample-specific overfitting.
- Evidence anchors:
  - [section III-C, Eq. 6-7] "The uncertainty of prediction heads is derived from their predictions on a batch of samples, which helps mitigate overfitting to individual samples."
  - [Table III/ablation] Adding prediction head weights (PHW) to sample weights (SW) yields 4.9% PCK improvement on FLIC vs. SW alone.
- Break condition: If batch contains predominantly outliers or misleading samples, head uncertainty estimates may be corrupted, leading to incorrect weight assignments.

## Foundational Learning

- Concept: **Pseudo-labeling with confidence thresholds in SSL**
  - Why needed here: UES directly addresses the failure modes of threshold-based pseudo-label filtering (threshold sensitivity, overconfidence, information loss).
  - Quick check question: Given a model that predicts [0.7, 0.3] for a 2-class problem with threshold τ=0.9, would this sample be used? How does UES handle it differently?

- Concept: **Ensemble diversity and uncertainty quantification**
  - Why needed here: UES relies on prediction variance across heads to estimate reliability; low diversity undermines the mechanism.
  - Quick check question: If all M prediction heads output identical distributions, what is the sample uncertainty $u_i^S$? What does this imply about the method's reliability?

- Concept: **Overconfidence in deep networks with limited labels**
  - Why needed here: The paper's core motivation is that confidence ≠ accuracy when labeled data is scarce.
  - Quick check question: Why does model confidence increase even as predictions become incorrect during overfitting?

## Architecture Onboarding

- Component map: Input (unlabeled batch) → Data Augmentation → CB Module → M Prediction Heads → Mean-based Uncertainty Estimator → Sample Uncertainty → Sample Weights → Weighted Loss. Prediction Head Uncertainty → Head Weights (EMA-smoothed) → Weighted Ensemble Prediction.

- Critical path:
  1. Ensure CB Module produces M low-correlation feature streams (paper uses M=5)
  2. Verify prediction head diversity before trusting uncertainty estimates
  3. Apply EMA to head weights with decay=0.7 (pose) or tune for classification
  4. Disable confidence thresholds for first 5 iterations when using long-tail weights

- Design tradeoffs:
  - More heads (higher M) → better uncertainty estimation but increased compute
  - Long-tail weighting includes noisy samples → may slow convergence vs. threshold methods
  - Architecture-agnostic design → may underperform task-specific uncertainty methods (e.g., MC Dropout for segmentation)

- Failure signatures:
  - Uncertainty-accuracy correlation remains low throughout training (Fig. 6 shows early-stage weakness)
  - No improvement over baseline CBE → check head diversity; heads may be too correlated
  - Instability in training curves → EMA decay may be too low; increase smoothing

- First 3 experiments:
  1. **Sanity check**: Train UES on CIFAR-10 with 40 labels, M=3 heads. Compare FixMatch+CBE vs. FixMatch+UES. Verify improvement is positive (target: >0.1% accuracy gain).
  2. **Ablation**: Run with only sample weights (SW=True, PHW=False) vs. both enabled. Isolate contribution of each component.
  3. **Head diversity test**: Measure pairwise correlation between head predictions on held-out batch. If mean correlation >0.9, add regularization to increase diversity before proceeding.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a dynamic weight mechanism be designed to adapt the long-tailed utility modeling to the distinct characteristics of different SSL training stages?
- Basis in paper: [explicit] The authors state in Section VII: "In fact, different stages of SSL exhibit distinct characteristics, and the mechanism in Eq. 2 lacks adaptability. In future work, we will focus on dynamic weights mechanisms suitable for SSL."
- Why unresolved: The current formulation relies on a static transformation of uncertainty into weights, which may not be optimal as the model shifts from initial feature capture to fine-tuning.
- Evidence: A study comparing the convergence speed and final accuracy of a time-varying weighting function against the static Eq. 2 across standard SSL benchmarks.

### Open Question 2
- Question: How can the correlation between UES-quantified uncertainty and prediction accuracy be improved during the initial, unstable phases of semi-supervised training?
- Basis in paper: [inferred] Section VI notes that "the correlation between uncertainty and prediction accuracy is not high in the early stages of training, indicating that the quantification of uncertainty may not be accurate."
- Why unresolved: The mean-based estimator relies on ensemble consistency, which may be low or misleading when the model itself is untrained, potentially assigning inappropriate weights to noisy pseudo-labels early on.
- Evidence: Experiments introducing calibration techniques (e.g., temperature scaling) or warm-up periods that demonstrate a statistically significant increase in the uncertainty-accuracy correlation coefficient during the first 10-20% of training epochs.

### Open Question 3
- Question: How sensitive is the performance of the Uncertainty-aware Ensemble Structure to the number of prediction heads ($M$), and is there a point of diminishing returns?
- Basis in paper: [inferred] The experimental configurations in Sections IV.B and V.A fix the number of heads at $M=5$, but the paper does not provide an ablation study or theoretical justification for this specific choice.
- Why unresolved: It is unclear if the "lightweight" claim holds if $M$ must be large to achieve robust uncertainty estimation, or if performance degrades significantly with fewer heads.
- Evidence: An ablation study plotting PCK/Accuracy against varying values of $M$ (e.g., $M \in \{2, 3, 5, 7, 10\}$) to identify the optimal trade-off between estimation robustness and computational cost.

## Limitations

- The claim that long-tailed weights reliably improve robustness by including all samples remains theoretical without ablation studies showing degradation when noisy samples dominate.
- Head uncertainty estimates depend critically on ensemble diversity, which isn't validated beyond the basic M=5 setup.
- The method's performance on extremely limited labeled data (<10 samples) is untested, though the motivation claims this is the primary scenario.

## Confidence

- **High confidence**: Classification accuracy improvements (0.2-0.26%) are well-documented and consistent across CIFAR-10/100.
- **Medium confidence**: Pose estimation gains (3.47-7.29% PCK) show strong improvement but rely on specific architecture choices.
- **Low confidence**: Theoretical claims about long-tailed weighting benefits and robustness to noisy early-stage pseudo-labels lack rigorous ablation validation.

## Next Checks

1. Run UES with M=2 heads vs. M=10 heads on CIFAR-10 to quantify the tradeoff between ensemble diversity and computational overhead.
2. Implement an ablation where long-tailed weights are replaced with hard thresholds on the same datasets to measure the specific contribution of the weighting scheme.
3. Test UES with 4 labeled samples per class on CIFAR-10 to evaluate claims about performance in extreme label-scarce regimes.