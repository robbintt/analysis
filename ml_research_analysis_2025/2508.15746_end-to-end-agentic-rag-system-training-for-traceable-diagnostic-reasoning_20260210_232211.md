---
ver: rpa2
title: End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning
arxiv_id: '2508.15746'
source_url: https://arxiv.org/abs/2508.15746
tags:
- disease
- diagnostic
- cases
- clinical
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Deep-DxSearch, an agentic RAG system trained
  end-to-end with reinforcement learning (RL) to enable traceable diagnostic reasoning
  in medical diagnosis. Unlike static RAG approaches, Deep-DxSearch frames the LLM
  as an active agent navigating a comprehensive medical retrieval corpus comprising
  disease guidelines, patient records, and biomedical literature.
---

# End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning

## Quick Facts
- **arXiv ID:** 2508.15746
- **Source URL:** https://arxiv.org/abs/2508.15746
- **Authors:** Qiaoyu Zheng; Yuze Sun; Chaoyi Wu; Weike Zhao; Pengcheng Qiu; Yongguo Yu; Kun Sun; Yanfeng Wang; Ya Zhang; Weidi Xie
- **Reference count:** 40
- **Key outcome:** Deep-DxSearch achieves state-of-the-art medical diagnosis accuracy, surpassing GPT-4o and DeepSeek-R1 by 22.7% average on multi-center datasets.

## Executive Summary
This paper introduces Deep-DxSearch, an end-to-end agentic RAG system trained with reinforcement learning to enable traceable diagnostic reasoning in medical diagnosis. Unlike static RAG approaches, it frames the LLM as an active agent navigating a comprehensive medical retrieval corpus to iteratively refine search strategies and ground diagnoses in verifiable sources. Evaluated across 24k+ clinical cases, it achieves state-of-the-art accuracy with robust generalization to rare diseases and out-of-distribution settings, while maintaining transparent reasoning that physicians prefer over opaque alternatives.

## Method Summary
Deep-DxSearch uses reinforcement learning (specifically GRPO) to train an agentic RAG policy that dynamically navigates a retrieval corpus containing disease guidelines, patient records, and biomedical literature. The system employs a 5-action sequence (reason, lookup, match, search, diagnose) and optimizes for both diagnostic accuracy and process validity through a structured reward function. Training proceeds through a 4-stage curriculum with staged reward emphasis, using MedGemma-27B as the backbone. The agent iteratively refines its search strategy, prioritizing valid evidence and producing a verifiable "Chain of Evidence" for each diagnosis.

## Key Results
- Achieves 22.7% average improvement over strong baselines like GPT-4o and DeepSeek-R1
- Outperforms supervised fine-tuning (SFT) baselines by 4.5% on average
- Shows robust generalization to rare diseases and out-of-distribution settings
- Physician-in-the-loop studies show diagnostic accuracy improving from 45.6% to 69.1% when assisted by Deep-DxSearch

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** End-to-end RL is necessary to learn a dynamic retrieval policy that outperforms static heuristics or SFT.
- **Mechanism:** The system frames the LLM as an agent in an MDP, optimized via GRPO to maximize composite rewards (accuracy + process validity), learning when to search vs. reason.
- **Core assumption:** Diagnostic reasoning is a sequential decision process where intermediate retrieval value depends on context, which standard prediction cannot capture.
- **Evidence anchors:** Abstract states RL evolves the agentic RAG policy; section 2.6 notes SFT degrades on OOD benchmarks.
- **Break condition:** Noisy retrieval environment or sparse/misaligned rewards may cause policy failure or reward hacking.

### Mechanism 2
- **Claim:** Performance gains are driven by the "Patient Record Matching" component and its reward.
- **Mechanism:** The agent uses `<match>` primitive with phenotype embeddings (BioLORD) to retrieve validated historical cases, grounding diagnosis in similar patient evidence.
- **Core assumption:** Similar clinical presentations reliably map to similar diagnoses in the database via semantic embeddings.
- **Evidence anchors:** Abstract mentions grounding in verifiable sources; section 2.6 shows excluding patient record matching causes substantial performance drop (5.5% common, 4.1% rare).
- **Break condition:** Database biases or lack of diversity lead to misleading "similar" cases and confirmation bias.

### Mechanism 3
- **Claim:** Structured process rewards reduce hallucination and improve OOD generalization.
- **Mechanism:** Reward function includes trajectory exploration reward (σ_div) to prevent repetitive loops and format alignment (σ_f) to ensure clinical workflow adherence.
- **Core assumption:** Enforcing structured output and diverse reasoning paths correlates with clinical validity and reduces hallucination.
- **Evidence anchors:** Section 4.3 describes trajectory exploration reward; figure 4a shows accuracy decline without evidence acquisition rewards.
- **Break condition:** Over-penalizing repetitive actions may force unnecessary steps just to satisfy diversity reward.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** Core training algorithm replacing SFT; uses group-based rewards (relative advantage) to update policy without separate value function.
  - **Quick check question:** How does GRPO calculate the advantage $\hat{A}_i$ for a specific trajectory in a batch?

- **Concept: Hybrid Retrieval (Sparse + Dense)**
  - **Why needed here:** Uses BM25 for disease guidelines (sparse) and BioLORD embeddings for patient matching (dense).
  - **Quick check question:** Which retrieval method handles `<lookup>` (strict disease names) vs. `<match>` (semantic phenotype lists)?

- **Concept: Process Reward Models (PRM)**
  - **Why needed here:** Optimizes for quality of search process, not just final diagnosis, via composite reward function (σ_total).
  - **Quick check question:** What happens to total reward if agent retrieves relevant document but fails to format output tags correctly?

## Architecture Onboarding

- **Component map:** Agent (MedGemma-27B) -> Environment (FastAPI servers) -> Retrieval tools (BM25 guidelines, BioLORD patient records, PubMed/Wiki) -> Summarizer (Qwen-3-235B) -> Trainer (GRPO loop)

- **Critical path:**
  1. Input: Clinical presentation -> Agent generates `<reason>` and `<match>` tags
  2. Action: System executes embedding search against Patient Record DB
  3. Feedback: Summarizer condenses top-K similar cases -> Appends to context
  4. Iteration: Agent generates `<search>` (Knowledge) or `<lookup>` (Guidelines)
  5. Output: Agent generates `<diagnose>` with LaTeX bold formatting
  6. Update: GRPO calculates rewards and updates policy weights

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** "Reasoning Complexity" increases steps to >5.5, improving accuracy but lowering "Time Efficiency" in physician studies.
  - **SFT vs. RL:** SFT offers faster convergence for in-distribution data but overfits; RL is computationally heavier but required for OOD generalization.

- **Failure signatures:**
  - **Premature Closure:** Agent generates `<diagnose>` too early (avg steps < 3) without sufficient evidence. Check trajectory rewards (σ_div).
  - **Retrieval Distraction:** Agent retrieves high-similarity but irrelevant cases. Check "Irrelevance Exclusion" metric or BioLORD embedding quality.
  - **Format Collapse:** Agent generates unstructured text. Check format reward (σ_f) and KL divergence coefficients.

- **First 3 experiments:**
  1. **Reward Ablation:** Remove evidence acquisition reward (σ_m) to verify accuracy drop (ablation study replication).
  2. **Retrieval Corpus Swap:** Replace Patient Record DB with generic vector store to quantify case-based matching contribution.
  3. **OOD Stress Test:** Evaluate performance on Xinhua-Rare (OOD) vs. MIMIC (ID) to confirm RL policy generalizes better than SFT.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can Deep-DxSearch be extended to process and reason over multimodal clinical evidence (radiology images, pathology slides, genomic data)?
- **Basis:** Current modality is text-only; authors state extending to multimodal evidence is critical next step as diagnosis is inherently multimodal.
- **Why unresolved:** System lacks architectural components to ingest or align visual/genetic inputs with retrieval corpus.
- **What evidence would resolve it:** Modified system demonstrating high accuracy on multimodal benchmarks using specific retrieval primitives for visual/genetic databases.

### Open Question 2
- **Question:** What mechanisms enable continuous, automated updating of retrieval corpus to mitigate "database knowledge gaps" for emerging medical literature?
- **Basis:** Current system relies on static snapshot; authors explicitly state real-world viability requires mechanisms for continuous updating.
- **Why unresolved:** System uses static knowledge base that risks becoming outdated or missing emerging diseases.
- **What evidence would resolve it:** Implementation with dynamically updated corpus (daily/weekly ingestion) showing maintained/improved accuracy on "emerging" or "OOD" cases over time.

### Open Question 3
- **Question:** How can reasoning engine be refined to eliminate synthesis errors (e.g., forced associations) even when retrieval is successful?
- **Basis:** Hallucination reduced but not eliminated; authors conclude reasoning engine requires further refinement for logical consistency.
- **Why unresolved:** Failure analysis identified "Reasoning & Synthesis Errors" (dismissal of valid evidence, forced association) as distinct from retrieval failures.
- **What evidence would resolve it:** Ablation studies showing significant reduction in synthesis-related failure modes through new reward structures or architectural changes.

### Open Question 4
- **Question:** Can time efficiency of traceable multi-step "Chain of Evidence" be improved to match opaque models without sacrificing accuracy or transparency?
- **Basis:** Physician study shows Deep-DxSearch received significantly lower "Time Efficiency" scores (2.0) compared to DeepSeek-R1 (4.0).
- **Why unresolved:** Paper argues trade-off is "necessary" for safety but doesn't propose methods to accelerate agentic rollout.
- **What evidence would resolve it:** Benchmarks demonstrating reduced average latency per diagnostic trajectory or higher physician "Time Efficiency" ratings while maintaining >69% accuracy.

## Limitations

- Heavy reliance on curated retrieval corpus raises concerns about real-world applicability where comprehensive, well-structured databases may not exist.
- RL policy performance depends critically on reward function design, with trajectory diversity penalty (σ_div) lacking explicit tuning criteria.
- Physician-in-the-loop study involved only 20 clinicians, limiting statistical power for generalization claims.

## Confidence

- **High confidence:** Core architectural framework (agent-environment with GRPO training) and reported quantitative improvements over baselines are well-supported by ablation studies and multiple test datasets.
- **Medium confidence:** Mechanism explanations (particularly RL policy evolution and patient record matching dominance) are internally consistent but rely heavily on internal comparisons.
- **Low confidence:** Claims about real-world clinical adoption and generalizability of "Chain of Evidence" approach to healthcare settings beyond tested datasets, given limited physician study sample size.

## Next Checks

1. **External corpus validation:** Test system on completely independent medical dataset not seen during training, ideally from different healthcare system with different disease prevalence patterns.

2. **Reward function ablation with clinical experts:** Have physicians review impact of removing specific reward components (σ_s, σ_m, σ_d) on diagnostic reasoning quality, not just accuracy metrics.

3. **Real-world deployment simulation:** Create simulated deployment environment where retrieval corpus contains missing/incomplete records to assess system robustness and identify failure modes not captured in curated dataset.