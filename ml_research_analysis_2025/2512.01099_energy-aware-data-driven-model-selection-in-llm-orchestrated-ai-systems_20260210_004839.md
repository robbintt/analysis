---
ver: rpa2
title: Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems
arxiv_id: '2512.01099'
source_url: https://arxiv.org/abs/2512.01099
tags:
- energy
- selection
- accuracy
- task
- icapt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of inefficient model selection
  in LLM-orchestrated AI systems, which leads to suboptimal accuracy, increased energy
  costs, and scalability limitations. The core method idea is GUIDE, an energy-aware
  model selection framework that incorporates quantitative performance metrics (accuracy
  and energy consumption) into orchestrator decision-making using Pareto optimization,
  guided by a real-time energy budget tracker.
---

# Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems

## Quick Facts
- arXiv ID: 2512.01099
- Source URL: https://arxiv.org/abs/2512.01099
- Reference count: 28
- Increases accuracy by 0.90%-11.92% and achieves up to 54% energy efficiency improvement

## Executive Summary
This paper addresses the critical challenge of inefficient model selection in LLM-orchestrated AI systems, where traditional approaches lead to suboptimal accuracy, increased energy costs, and scalability limitations. The authors introduce GUIDE, an energy-aware model selection framework that incorporates quantitative performance metrics (accuracy and energy consumption) into orchestrator decision-making through Pareto optimization. The framework uses a real-time energy budget tracker to ensure selections stay within predefined energy constraints while optimizing for accuracy.

The proposed solution represents a significant advancement in sustainable AI system design, moving beyond simple accuracy maximization to balance performance with energy efficiency. By integrating energy consumption metrics directly into the selection process, GUIDE enables more responsible resource utilization while maintaining or improving task performance across evaluated workloads.

## Method Summary
GUIDE implements an energy-aware model selection framework that uses Pareto optimization to balance accuracy and energy consumption during orchestrator decision-making. The core innovation lies in the real-time energy budget tracker, which monitors and constrains energy usage while selecting models. The framework evaluates multiple candidate models based on their historical performance metrics and energy profiles, then applies Pareto optimization to identify solutions that represent optimal trade-offs between accuracy and energy efficiency within the specified budget constraints.

The selection process involves collecting quantitative performance data from candidate models, computing Pareto-optimal fronts that balance competing objectives, and selecting the most appropriate model based on the current energy budget status. This approach ensures that the orchestrator makes informed decisions that optimize both task accuracy and energy efficiency rather than prioritizing one metric at the expense of the other.

## Key Results
- Accuracy improvements ranging from 0.90% to 11.92% across evaluated tasks
- Up to 54% energy efficiency improvement compared to baseline selection methods
- Selection latency reduced from 4.51 seconds to 7.2 milliseconds

## Why This Works (Mechanism)
GUIDE works by fundamentally changing how model selection decisions are made in LLM-orchestrated systems. Traditional approaches optimize for single metrics like accuracy or latency, but GUIDE incorporates energy consumption as a first-class constraint in the decision-making process. The Pareto optimization framework enables the system to identify models that offer the best trade-offs between accuracy and energy usage, rather than selecting models that might be accurate but energy-prohibitive.

The real-time energy budget tracker provides continuous feedback about current energy consumption, allowing the selection process to adapt dynamically to changing conditions and constraints. This creates a closed-loop system where energy efficiency is not just a consideration but an integral part of the selection criteria, leading to more sustainable and cost-effective AI system operation.

## Foundational Learning
- **Pareto Optimization**: A multi-objective optimization technique used to identify solutions that represent optimal trade-offs between competing objectives. Needed because single-objective optimization fails to capture the complex relationship between accuracy and energy consumption. Quick check: Verify that identified solutions lie on the Pareto frontier where no objective can be improved without degrading another.

- **Energy Budget Tracking**: Real-time monitoring and management of energy consumption within predefined limits. Essential for ensuring that model selections remain within sustainable energy constraints. Quick check: Confirm that energy consumption stays within budget limits across different workload patterns.

- **LLM Orchestration**: The process of coordinating multiple models and components in large language model systems. Critical context for understanding where and how model selection decisions are made. Quick check: Validate that the selection framework integrates seamlessly with existing orchestration pipelines.

- **Quantitative Performance Metrics**: Collection and analysis of measurable performance indicators including accuracy, latency, and energy consumption. Necessary for making data-driven selection decisions rather than relying on heuristics. Quick check: Ensure metrics collection is comprehensive and representative of real-world performance.

## Architecture Onboarding

**Component Map:**
Energy Budget Tracker -> Performance Data Collector -> Pareto Optimizer -> Model Selector -> Orchestrator

**Critical Path:**
The critical path involves the Energy Budget Tracker providing real-time consumption data to the Pareto Optimizer, which uses this information along with performance metrics from the Data Collector to identify optimal model selections. The Model Selector then implements these decisions through the Orchestrator, with latency being a key concern throughout the pipeline.

**Design Tradeoffs:**
The framework trades computational overhead from multi-objective optimization and energy tracking against improved efficiency and accuracy. While the Pareto optimization adds complexity, the 54% energy efficiency gains and reduced selection latency (from 4.51s to 7.2ms) demonstrate that these costs are justified. The real-time tracking introduces monitoring overhead but enables dynamic adaptation to changing conditions.

**Failure Signatures:**
Primary failure modes include energy budget tracking inaccuracies leading to constraint violations, Pareto optimization producing suboptimal fronts due to insufficient data, and latency increases in the selection process. Performance degradation typically manifests as either accuracy drops from overly conservative energy constraints or energy budget overruns from inaccurate tracking.

**First Experiments:**
1. Validate Pareto optimization produces correct fronts under varying energy constraints
2. Test energy budget tracking accuracy across different computational loads
3. Measure selection latency under concurrent multi-tenant conditions

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses primarily on text classification and summarization, limiting generalizability to other AI workloads
- Energy budget tracking assumes stable power consumption patterns that may not hold under varying loads
- Reported improvements may not scale proportionally across different deployment scenarios

## Confidence

**Accuracy and energy efficiency improvements**: High confidence for tested scenarios
**Scalability across diverse AI workloads**: Medium confidence
**Latency improvements under production conditions**: Low confidence

## Next Checks

1. Evaluate GUIDE across a broader range of AI tasks including computer vision, speech processing, and multimodal applications to assess generalizability beyond text-based workloads.

2. Test the framework under realistic multi-tenant deployment conditions with concurrent model selections to validate sustained latency improvements and energy efficiency gains.

3. Conduct long-term monitoring of energy budget tracking accuracy under varying computational loads and hardware heterogeneity to assess the robustness of the power consumption estimation mechanism.