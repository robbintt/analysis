---
ver: rpa2
title: 'It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference
  Graph'
arxiv_id: '2505.12411'
source_url: https://arxiv.org/abs/2505.12411
tags:
- graph
- homophily
- nodes
- reference
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance degradation of Graph Neural
  Networks (GNNs) on heterophilic graphs where connected nodes often belong to different
  classes. The authors propose a rewiring framework that increases graph homophily
  using a reference graph, with theoretical guarantees on the homophily of the rewired
  graph.
---

# It Takes a Graph to Know a Graph: Rewiring for Homophily with a Reference Graph

## Quick Facts
- **arXiv ID:** 2505.12411
- **Source URL:** https://arxiv.org/abs/2505.12411
- **Reference count:** 40
- **Primary result:** Proposed rewiring framework improves node classification accuracy by up to 13.9% on heterophilic graphs by increasing homophily using a reference graph with theoretical guarantees

## Executive Summary
This paper addresses the fundamental challenge of applying Graph Neural Networks (GNNs) to heterophilic graphs where connected nodes belong to different classes. The authors propose a rewiring framework that increases graph homophily using a reference graph constructed through label-driven diffusion. The method theoretically guarantees improved homophily and demonstrates substantial empirical gains across 11 real-world heterophilic datasets, outperforming specialized heterophily-handling GNNs while maintaining scalability to large graphs.

## Method Summary
The proposed method constructs a homophilic reference graph $G_r$ from node features and training labels using label-driven diffusion ($\Gamma = PDP$), where $P$ is a label affinity kernel and $D$ is a data kernel. The reference graph guides edge addition or deletion based on homophily conditions: edges are added from $E_r \setminus E$ if $H(G_r) > H(G)$, or edges are deleted from $E \cap E_r^c$ if $H(G_r) < H(G)$. For scalability, the graph is partitioned into clusters using METIS, with rewiring performed independently on each cluster before reconstruction.

## Key Results
- REFine improves node classification accuracy by up to 13.9% compared to state-of-the-art baselines on heterophilic graphs
- The method matches or outperforms specialized GNNs designed specifically for heterophilic graphs
- Theoretical guarantees show that rewiring increases graph homophily in expectation when conditions are met
- Label-driven diffusion construction yields higher homophily than feature-only approaches on most datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Increasing graph homophily through reference-guided edge addition/deletion improves the compatibility of message passing with linearly separable node embeddings.
- **Mechanism:** The framework constructs a high-homophily reference graph $G_r$ (using label-driven diffusion) and uses it to guide rewiring. When the conditions $H(G_r \setminus G) > H(G)$ (for edge addition) or $H(G \cap G_r^c) < H(G)$ (for edge deletion) are met, rewiring provably increases homophily in expectation (Propositions 1 and 2). This reduces the lower bound on the Dirichlet energy required for linear separability (Theorem 1), resolving the conflict between the GNN's smoothing nature and the requirement for distinguishable embeddings in heterophilic graphs.
- **Core assumption:** The feature space offers a meaningful measure of similarity that aligns with class labels, allowing the label-driven diffusion process ($\Gamma = PDP$) to effectively propagate label affinity and create a homophilic reference graph.
- **Evidence anchors:**
  - [abstract] "We provide theoretical foundations linking edge homophily, GNN embedding smoothness, and node classification performance... our rewiring framework... increases graph homophily using a reference graph, with theoretical guarantees on the homophily of the rewired graph."
  - [Section 4] "Theorem 1... the greater the homophily of the graph, the higher the potential of a GNN to learn linearly separable, and thus more effective, node embeddings."
  - [corpus] Neighbor papers like "Adapting to Heterophilic Graph Data with Structure-Guided Neighbor Discovery" support the idea of creating alternative graph structures for heterophilic data.
- **Break condition:** The condition fails if $H(G_r) \le H(G)$ for edge addition (or the equivalent for deletion). In this case, Proposition 1 states that rewiring will decrease homophily in expectation, harming performance (as seen in Figure 1c).

### Mechanism 2
- **Claim:** A reference graph constructed via label-driven diffusion ($\Gamma = PDP$) is more homophilic and generalizes better to unlabeled nodes than one built solely from training labels or features alone.
- **Mechanism:** The label-driven diffusion kernel $\Gamma = PDP$ integrates a label-based affinity kernel $P$ (built from training labels) with a data-based kernel $D$ (built from all node features). The product $PDP$ propagates label information through the feature-induced geometry, effectively "completing" missing labels for unlabeled nodes. This process captures shared structure, yielding a reference graph with higher homophily than one built from features alone and better generalization than one built from labels alone.
- **Core assumption:** The geometric structure of the node features is informative of label similarity, allowing the diffusion process to meaningfully propagate label information.
- **Evidence anchors:**
  - [abstract] "...we propose a label-driven diffusion approach for constructing a homophilic reference graph from node features and training labels."
  - [Section 5] "This process uses node features to 'complete' missing labels... capturing the shared geometry between the node features and labels..." and Table 9 shows $H(G_r)$ (PDP) is higher than $H(G_r)$ (D) on most datasets.
  - [corpus] Evidence for this specific construction method is weak in the provided corpus, which focuses more on general heterophily adaptation.
- **Break condition:** The mechanism fails if node features are uninformative or misleading with respect to labels, causing the diffusion process to create a reference graph with low homophily. The method also assumes categorical labels, though the paper notes it can be adapted for continuous labels.

### Mechanism 3
- **Claim:** Graph clustering enables the rewiring framework to scale linearly with the number of nodes, making it applicable to large graphs.
- **Mechanism:** The method partitions the full graph into smaller clusters using the METIS algorithm. The computationally intensive $O(c^3)$ label-driven diffusion and rewiring operations are performed independently and in parallel on each cluster (of size $c$). The full rewired graph is then reconstructed by merging the rewired clusters and preserving original inter-cluster edges.
- **Core assumption:** The graph can be partitioned such that the reference graph structure is largely captured within clusters, or that local rewiring is sufficient for performance gains.
- **Evidence anchors:**
  - [Section 5] "To ensure scalability, we cluster the graph G into N = |V|/c balanced clusters using the METIS algorithm... This yields the set of graphs {G_l = (V_l, E_l)}... Each cluster Gl is then rewired independently..."
  - [Section 7] "...when n >> c, the complexity simplifies to O(n), indicating that the method remains linear in the number of nodes..."
  - [corpus] Not explicitly discussed in the provided corpus summaries.
- **Break condition:** The assumption fails if the most critical homophilic connections exist primarily between, not within, clusters. In such cases, local rewiring may miss important long-range dependencies.

## Foundational Learning

- **Concept: Graph Homophily & Heterophily**
  - **Why needed here:** This is the central problem the paper addresses. Standard GNNs assume homophily (connected nodes are similar), and their performance degrades on heterophilic graphs (connected nodes are dissimilar). Understanding this is prerequisite to grasping why rewiring is necessary.
  - **Quick check question:** Given a graph with edge homophily $H(G) = 0.1$, would a standard message-passing GNN likely perform well or poorly on a node classification task? Why?

- **Concept: Message Passing & Over-smoothing in GNNs**
  - **Why needed here:** The paper's theoretical motivation (Theorem 1) stems from the smoothing nature of message passing. The method aims to alter the graph structure so that this inherent smoothing becomes beneficial rather than detrimental for creating distinguishable embeddings.
  - **Quick check question:** Explain the "smoothing" effect of a standard GCN layer's message passing step. How does this relate to the Dirichlet energy?

- **Concept: Diffusion Maps & Kernels**
  - **Why needed here:** The core construction of the reference graph uses a label-driven diffusion process ($\Gamma = PDP$), which builds on manifold learning concepts like diffusion maps. Understanding kernel construction and normalization is key to understanding how the reference graph is built.
  - **Quick check question:** In the context of this paper, what two pieces of information are combined to form the diffusion kernel $\Gamma$, and what is the purpose of this combination?

## Architecture Onboarding

- **Component map:**
  Input -> Preprocessing/Clustering -> Reference Graph Construction -> Rewiring -> Reconstruction -> Downstream Task

- **Critical path:** The construction of the reference graph is the most critical step. Its homophily $H(G_r)$ must be sufficiently higher than $H(G)$ for the rewiring to be effective. Clustering is essential for scalability but is treated as a hyperparameter ($c=100$ or $c=500$).

- **Design tradeoffs:**
  - **Accuracy vs. Scalability:** Larger cluster size $c$ may improve accuracy but increases runtime ($O(c^3)$). The paper uses smaller clusters (100) for very large graphs.
  - **Edge Addition vs. Deletion:** Addition increases connectivity and can improve homophily but risks over-smoothing. Deletion can improve homophily but risks over-squashing. The choice is a hyperparameter.
  - **Reference Graph Construction:** Using $\Gamma = PDP$ (label-driven) vs. $\Gamma = D$ (feature-only) is a choice. Label-driven typically yields higher homophily but relies on the quality and quantity of training labels.

- **Failure signatures:**
  - **Performance degradation:** If rewiring leads to lower accuracy, check if the conditions in Propositions 1/2 are met (approximate $H(G)$ and $H(G_r)$ using sampled graphs).
  - **Over-smoothing:** If edge addition ($k$ is large) leads to performance drop, reduce $k$.
  - **Over-squashing:** If edge deletion ($|k|$ is large) leads to performance drop, reduce $|k|$.
  - **Poor generalization:** If using only training labels for $P$, the reference graph may not generalize to unlabeled nodes (as shown in ablation). Ensure the diffusion process is used.

- **First 3 experiments:**
  1. **Baseline Verification:** Implement the reference graph construction and rewiring on a small, well-known heterophilic dataset from the paper (e.g., Cornell, Texas, or Wisconsin). Replicate the accuracy improvement over a vanilla GCN to verify the end-to-end pipeline.
  2. **Condition Ablation:** On a chosen dataset, intentionally violate the rewiring condition by using a low-homophily reference graph (e.g., constructed with a poor choice of $\epsilon$). Verify that rewiring now harms performance, confirming the theoretical requirement.
  3. **Scalability Test:** Apply the method with clustering to a larger graph (e.g., Roman-empire). Measure the runtime per component (clustering, reference graph construction, rewiring) to identify bottlenecks and confirm the linear scalability claim. Compare against a baseline like SDRF or FoSR.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can a joint optimization framework for simultaneous edge addition and deletion outperform the current disjoint rewiring process?
- **Basis in paper:** [explicit] The conclusion explicitly lists "exploring simultaneous edge addition and deletion" as a future direction.
- **Why unresolved:** The current method treats addition ($k>0$) and deletion ($k<0$) as separate operational modes based on specific conditions (Prop 1 vs Prop 2), potentially missing optimal configurations where both occur at once.
- **Evidence:** An algorithm optimizing both $k_{add}$ and $k_{del}$ simultaneously, evaluated against the sequential baseline on heterophilic benchmarks.

### Open Question 2
- **Question:** How can the rewiring framework be adapted for graph classification tasks where node-level labels are unavailable for reference graph construction?
- **Basis in paper:** [explicit] The conclusion identifies "inapplicability to graph classification" as a limitation due to the reliance on node labels.
- **Why unresolved:** The method constructs the reference graph using a label-driven diffusion process ($\Gamma = PDP$), which requires $P$ (label kernel) derived from node labels $Y$.
- **Evidence:** A modified diffusion process using graph-level labels or self-supervised signals to approximate the reference graph, tested on standard graph classification datasets.

### Open Question 3
- **Question:** How robust is REFine when the assumption that the feature space aligns with label similarity is significantly violated?
- **Basis in paper:** [inferred] The method relies on the assumption that "feature space offers a meaningful measure of similarity that aligns with the labels" (Section 5/Conclusion).
- **Why unresolved:** If the data kernel $D$ constructed from features ($X$) is noisy or uncorrelated with class structure, the label-driven diffusion might propagate misleading information to unlabeled nodes.
- **Evidence:** Empirical evaluation on synthetic datasets where node features are randomized or made adversarial to the labels to measure performance degradation.

## Limitations

- The method assumes the feature space meaningfully aligns with label similarity, which may not hold for all datasets
- The clustering-based scalability approach may miss important long-range dependencies between clusters
- Theoretical guarantees rely on probabilistic conditions that may not always hold in practice

## Confidence

- **High:** The core rewiring framework with theoretical guarantees and experimental improvements over baselines
- **Medium:** The label-driven diffusion construction method's superiority over feature-only approaches
- **Low:** The scalability claims for very large graphs with complex heterophily patterns

## Next Checks

1. Test rewiring performance when the reference graph homophily H(G_r) is artificially set below H(G) to verify the condition-based approach works as intended
2. Implement and compare the label-driven diffusion approach against pure feature-based diffusion to validate the claimed improvement in homophily
3. Measure runtime and accuracy trade-offs for different cluster sizes (c=100 vs c=500) on large datasets to validate the scalability claims