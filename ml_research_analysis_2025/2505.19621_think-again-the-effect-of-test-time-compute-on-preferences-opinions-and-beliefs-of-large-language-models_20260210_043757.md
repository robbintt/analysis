---
ver: rpa2
title: Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and
  Beliefs of Large Language Models
arxiv_id: '2505.19621'
source_url: https://arxiv.org/abs/2505.19621
tags:
- answer
- topics
- responses
- question
- pobs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces the Preference, Opinion, and Belief Survey
  (POBs) benchmark to evaluate subjective biases in large language models (LLMs) across
  societal, cultural, ethical, and personal topics. The benchmark uses Likert-scale
  questions and measures reliability, neutrality, and consistency.
---

# Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models

## Quick Facts
- arXiv ID: 2505.19621
- Source URL: https://arxiv.org/abs/2505.19621
- Reference count: 21
- While reasoning and self-reflection techniques show limited improvement in model behavior, newer LLM versions tend to exhibit stronger biases and reduced consistency compared to older ones.

## Executive Summary
This study introduces the Preference, Opinion, and Belief Survey (POBs) benchmark to evaluate subjective biases in large language models (LLMs) across societal, cultural, ethical, and personal topics. The benchmark uses Likert-scale questions and measures reliability, neutrality, and consistency. While reasoning and self-reflection techniques show limited improvement in model behavior, newer LLM versions tend to exhibit stronger biases and reduced consistency compared to older ones. The analysis reveals that most models lean toward progressive-collectivist viewpoints and often underestimate their own biases. These findings highlight the need for ongoing evaluation of LLM biases in business and general applications.

## Method Summary
The POBs benchmark comprises 20 topics with 12-38 Likert-scale questions each, using a 5-point scale plus refusal option. Models are evaluated using three prompting strategies: Direct, Reasoning (with XML tags), and Self-reflection (with reconsideration tags). Each question is repeated 5 times per model to assess reliability. Responses are parsed for polarity values (-1 to +1) with special handling for refusals (0.5i). Three metrics are computed: Reliability (consistency across repetitions), Non-Neutrality Index (NNI), and Topical Consistency Index (TCI). The study evaluates 8 proprietary and open-source models across multiple versions.

## Key Results
- Reasoning and self-reflection techniques provide limited bias mitigation, with reliability decreasing as test-time compute increases
- Newer model versions within families show stronger ideological leanings and reduced consistency compared to older versions
- Models consistently underestimate their own biases, reporting more neutral stances than their actual responses reveal
- Most models exhibit progressive-collectivist viewpoints across tested topics
- A strong negative correlation (r~0.9) exists between NNI and TCI, indicating a fundamental tradeoff between strong opinions and consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Test-time compute (reasoning and self-reflection) provides limited improvement in reducing LLM subjective bias and may decrease reliability.
- Mechanism: Models generate explicit reasoning chains before answering, then optionally reconsider their reasoning. This extended inference is hypothesized to surface more thoughtful, neutral responses.
- Core assumption: Additional deliberation time allows models to recognize and correct biased responses.
- Evidence anchors:
  - [abstract] "While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain."
  - [section 4.1] "Increasing test-time compute (reasoning/reflection) reduces [reliability]... reliability drops likely due to: (1) heightened sensitivity to biases, where reasoning reveals conflicts, destabilizing responses; (2) variability in reasoning paths, causing unpredictable shifts."
  - [corpus] Related work on test-time compute scaling (Sleep-time Compute, Scaling Test-time Compute for LLM Agents) focuses on reasoning tasks, not subjective bias mitigation—suggesting mechanism transfer is not straightforward.
- Break condition: When reasoning paths become inconsistent across semantically equivalent inputs, or when models shift opinions dramatically without principled justification (e.g., LLaMA-3.2-3B shows 8% opinion shift rate).

### Mechanism 2
- Claim: Newer model versions within the same family exhibit stronger ideological leanings and reduced consistency compared to older versions.
- Mechanism: Progressive training data curation and alignment procedures may amplify certain viewpoints while reducing topical consistency.
- Core assumption: Version-to-version training changes systematically affect ideological stance expression.
- Evidence anchors:
  - [abstract] "Newer model versions are becoming less consistent and more biased toward specific viewpoints."
  - [section 4.2] "Surprisingly, newer models within the same family perform worse than their older counterparts across all prompting techniques, exhibiting lower consistency and higher non-neutrality."
  - [corpus] No direct corpus evidence on version degradation patterns for subjective bias specifically—this appears to be an emerging finding.
- Break condition: When organizational deployment switches model versions without re-evaluating ideological stance; when consistency is critical (e.g., customer-facing applications).

### Mechanism 3
- Claim: Models self-report more neutral stances than their actual responses reveal, underestimating their own biases.
- Mechanism: Direct declarative questioning about stance produces more neutral self-assessments than behavioral responses across multiple questions probing the same topic.
- Core assumption: Declarative self-reports reflect model's explicit alignment training rather than implicit behavioral tendencies.
- Evidence anchors:
  - [section 4.4] "Comparing models' self-reported stances to their answers on POBs (Figure 5) shows they often underestimate their biases, particularly their progressivism."
  - [section 1] "Models tend to misrepresent their own biases (Turpin et al., 2023)."
  - [corpus] "Competing LLM Agents" paper models opinion dynamics but doesn't address self-assessment accuracy—suggesting this mechanism is underexplored in related work.
- Break condition: When relying on model self-declarations for alignment auditing; when business decisions require accurate bias disclosure.

## Foundational Learning

- **Concept: Test-Time Compute (Inference-Time Scaling)**
  - Why needed here: The paper evaluates whether extended computation at inference (reasoning chains, self-reflection) mitigates subjective bias—a core intervention strategy.
  - Quick check question: Can you explain why test-time compute improves mathematical reasoning but shows limited effectiveness for subjective bias reduction?

- **Concept: Likert-Scale Subjective Measurement**
  - Why needed here: POBs uses 5-point Likert scales with polarity values (-1 to +1) to quantify stance strength and direction; understanding this is essential for interpreting NNI and TCI metrics.
  - Quick check question: How does assigning polarity values to Likert options enable quantitative bias analysis across topics?

- **Concept: Reliability vs. Consistency Distinction**
  - Why needed here: The paper distinguishes reliability (same response across repeated identical queries) from topical consistency (similar stance across different questions on the same topic).
  - Quick check question: Why might a model be reliable but not topically consistent, and which property matters more for business deployment?

## Architecture Onboarding

- **Component map:**
  - POBs benchmark -> Model invocation (5 repetitions) -> Response parsing -> Metric computation (R, NNI, TCI) -> Cross-model comparison

- **Critical path:**
  1. Question generation (LLM-assisted + manual curation) → 2. Model invocation (n=5 repetitions per question, default sampling parameters) → 3. Response parsing (extract polarity from answer tags) → 4. Metric computation (per-topic and aggregate) → 5. Cross-model comparison

- **Design tradeoffs:**
  - Reference-free vs. human baseline: Chose reference-free for broader applicability, but cannot assess conformity to societal norms
  - Temperature/sampling: Used default settings for realism, but introduces variability
  - Refusal handling: Assigned imaginary polarity (0.5i) to distinguish from neutral—mathematically sound but requires complex number operations
  - Topic selection: Focused on Western-controversial topics; may not generalize cross-culturally

- **Failure signatures:**
  - Invalid response rate >7% indicates prompt adherence issues (most models <7%, but GPT-4o shows 6.98% in direct mode)
  - Large opinion shifts (>1 polarity unit) between reasoning and reflection indicate unstable reasoning (LLaMA-3.2-3B: 8%)
  - NNI/TCI negative correlation (r~0.9) reveals fundamental tension—strong opinions sacrifice consistency

- **First 3 experiments:**
  1. **Baseline establishment**: Run all models with Direct prompting on full POBs benchmark, compute reliability/NNI/TCI to identify high-risk models for your use case.
  2. **Reasoning effectiveness test**: Apply Reasoning prompting to top-3 candidate models, measure delta in NNI and TCI—expect limited improvement per paper findings.
  3. **Version migration audit**: If upgrading model versions (e.g., GPT-4 Turbo → GPT-4o), run POBs on both versions to quantify ideological drift before production deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does training for neutrality on one topic generalize to related or opposing topics, potentially reducing training costs and improving safety?
- Basis in paper: [explicit] Section 7 (Limitations) states: "An open question not explored in this work is whether training for neutrality on one topic promotes neutrality on related or opposing topics."
- Why unresolved: The study only examined test-time compute mechanisms (reasoning/self-reflection) and found them limited; no training-based approaches for neutrality were investigated.
- What evidence would resolve it: Fine-tune models for neutrality on specific topics in POBs, then evaluate whether neutrality improves on semantically or ideologically related topics without direct training.

### Open Question 2
- Question: Do the opinions and preferences expressed by LLMs in direct questioning transfer to their behavior in recommendation and advice-giving scenarios?
- Basis in paper: [explicit] Section 7 states: "A model stating a particular belief (e.g., a Pro-Life stance) may not carry that position into downstream tasks, such as advising a user. In future work, we plan to curate a benchmark to assess whether the opinions and stances declared by models generalize to their behavior in recommendation scenarios."
- Why unresolved: POBs captures responses to direct questions, not downstream task behavior; the relationship between expressed opinions and actionable recommendations remains untested.
- What evidence would resolve it: Create a benchmark measuring whether model recommendations (e.g., doctor referrals, product suggestions) align with their stated positions on POBs topics.

### Open Question 3
- Question: How do varying prompt structures and formulations influence model neutrality, refusal rates, and stance consistency?
- Basis in paper: [explicit] Section 7 notes: "The reliance on specific prompting techniques (Direct, Reasoning, and Self-reflection) may shape model behavior in ways that do not generalize to real-world systems... Future studies should investigate how varying prompt structures influence model responses."
- Why unresolved: Only three specific prompting approaches were tested; results may not generalize to other formulations or real-world system prompts.
- What evidence would resolve it: Systematically vary prompt wording, structure, and system-level instructions across POBs questions, measuring changes in NNI, TCI, and refusal rates.

### Open Question 4
- Question: Do LLM responses in POBs conform to or deviate from societal norms across different demographic groups?
- Basis in paper: [explicit] Section 7 states the methodology was "intentionally developed to be reference-free" and that "determining whether the distribution of an LLM's responses conforms to or significantly deviates from societal norms would necessitate a human benchmark for comparison."
- Why unresolved: No human baseline data was collected; it remains unclear whether model biases reflect, exaggerate, or diverge from human population distributions.
- What evidence would resolve it: Administer POBs questions to diverse human demographic groups and compare response distributions to LLM outputs.

## Limitations

- The study focuses exclusively on Western-controversial topics, potentially limiting cross-cultural applicability
- Default sampling parameters may not represent all possible inference configurations, affecting reproducibility
- The refusal handling mechanism (assigning 0.5i) introduces mathematical complexity that may affect metric interpretation
- No human baseline data was collected to assess conformity to societal norms

## Confidence

- **High Confidence**: The core finding that reasoning and self-reflection techniques provide limited bias mitigation (Mechanism 1) is well-supported by quantitative evidence showing reliability drops with increased test-time compute.
- **Medium Confidence**: The observation that newer model versions exhibit stronger biases than older versions (Mechanism 2) is documented but lacks deep causal analysis of why this occurs.
- **Medium Confidence**: The finding that models underestimate their own biases (Mechanism 3) is demonstrated through comparison of declarative self-reports versus behavioral responses, though the underlying psychological mechanism remains speculative.

## Next Checks

1. **Cross-Cultural Validation**: Replicate the POBs benchmark using topics relevant to non-Western cultural contexts to assess whether the documented ideological leanings generalize beyond Western-centric viewpoints.

2. **Parameter Sensitivity Analysis**: Systematically vary temperature, top-p, and top-k sampling parameters across the full range to quantify how inference-time randomness affects reliability, NNI, and TCI scores, particularly for the reasoning and self-reflection conditions.

3. **Version Migration Impact Study**: For organizations upgrading from older to newer model versions, conduct a controlled business-domain-specific bias audit using POBs to quantify real-world impact of the documented ideological drift on customer-facing applications.