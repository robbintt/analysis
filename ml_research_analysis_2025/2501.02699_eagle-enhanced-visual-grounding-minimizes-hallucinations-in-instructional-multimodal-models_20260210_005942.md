---
ver: rpa2
title: 'EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional
  Multimodal Models'
arxiv_id: '2501.02699'
source_url: https://arxiv.org/abs/2501.02699
tags:
- visual
- eagle
- image
- language
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EAGLE improves visual grounding in multimodal models by fine-tuning
  the visual encoder to better capture fine-grained object details. Unlike existing
  approaches that focus on language or fusion components, EAGLE directly enhances
  the visual backbone using a modified contrastive learning strategy with instance
  segmentation supervision.
---

# EAGLE: Enhanced Visual Grounding Minimizes Hallucinations in Instructional Multimodal Models

## Quick Facts
- arXiv ID: 2501.02699
- Source URL: https://arxiv.org/abs/2501.02699
- Reference count: 40
- Primary result: EAGLE achieves up to 11.2% relative improvement in hallucination reduction on MMVP benchmark by fine-tuning visual encoders with instance segmentation supervision.

## Executive Summary
EAGLE addresses hallucinations in instructional multimodal models by directly enhancing the visual encoder's ability to capture fine-grained object details. Unlike previous approaches that focus on language or fusion components, EAGLE fine-tunes the visual backbone using a modified contrastive learning strategy with instance segmentation supervision. The method preserves zero-shot capabilities while improving object-level feature alignment, achieving significant reductions in hallucinations across three major benchmarks when applied to six state-of-the-art IT-VLMs.

## Method Summary
EAGLE fine-tunes pre-trained CLIP-style visual encoders using instance segmentation data from OpenImages V7. The method applies masked average pooling to extract object-level features from segmentation masks, then aligns these with language embeddings using a dual loss combining instance-level contrastive learning and cross-entropy over distance measures. To preserve zero-shot capabilities, EAGLE employs Gradient Low-Rank Projection (GaLore) to constrain parameter updates while maintaining pre-trained knowledge. The fine-tuned visual encoder is then plugged into existing IT-VLMs without requiring additional model-specific training.

## Key Results
- Achieves up to 11.2% relative improvement on MMVP benchmark for hallucination reduction
- Reduces false positives by up to 19.53% on MS-COCO benchmark
- Improves sequence feature alignment while preserving CLS token zero-shot performance through GaLore

## Why This Works (Mechanism)

### Mechanism 1: Masked Average Pooling for Object-Level Feature Alignment
- **Claim:** Pooling visual tokens within segmentation masks and aligning them with language embeddings improves fine-grained object representation in the feature sequence.
- **Mechanism:** Given a binary segmentation mask m, the method zeroes out feature tokens not overlapping with the mask, then performs average pooling: φ(I, m). This pooled representation is contrastively aligned with the language embedding of the object class.
- **Core assumption:** Instance-level segmentation labels provide sufficient signal to improve patch-level visual grounding without requiring instructional data.
- **Evidence anchors:**
  - [abstract]: "EAGLE directly enhances the visual backbone using a modified contrastive learning strategy with instance segmentation supervision."
  - [section 3.1]: "For a binary segmentation mask m, we define a masked average pooling function φ(I, m) where the feature tokens in I are set to 0 if they do not overlap with the mask m, then we perform average pooling."
  - [corpus]: Weak direct evidence; MoDA (arXiv:2506.01850) addresses fine-grained visual grounding but via adapter modulation, not encoder fine-tuning.
- **Break condition:** If segmentation quality degrades or mask-token alignment is imprecise (e.g., small objects with few overlapping patches), pooled features may lack discriminative signal.

### Mechanism 2: Dual Loss Decomposition for Multi-Instance Compatibility
- **Claim:** Decomposing the loss into instance-level contrastive (L_ins) and cross-entropy over distance measures (L_ce) handles cases where multiple masks share the same semantic class.
- **Mechanism:** Standard contrastive loss assumes one positive per batch; this fails when multiple instances of the same class appear. L_ce uses sigmoid-normalized distance between visual and language embeddings, allowing multi-positive association.
- **Core assumption:** The cross-entropy formulation with sigmoid distance provides sufficient gradient signal for multi-instance scenarios without hard negative mining conflicts.
- **Evidence anchors:**
  - [section 3.1]: "We can not directly use contrastive learning as our training objective, the standard contrastive loss considers all the elements in the batch as negatives except one. This is incompatible with our setup as the training batch might contain multiple masks with the same semantic class."
  - [section 3.1]: "L_ce allows for multiple masks to be associated to a single class."
  - [corpus]: No direct corpus evidence for this specific loss design in hallucination reduction.
- **Break condition:** If class distributions are highly imbalanced (which the paper addresses via resampling), L_ce may over-weight frequent classes despite uniform resampling attempts.

### Mechanism 3: Gradient Low-Rank Projection (GaLore) Preserves Zero-Shot Capabilities
- **Claim:** GaLore prevents feature distribution drift during fine-tuning, preserving CLS token zero-shot performance while enhancing sequence embeddings.
- **Mechanism:** GaLore computes low-rank approximation of gradients for weight matrices, enabling full-parameter learning with reduced memory and mitigating catastrophic forgetting.
- **Core assumption:** Low-rank gradient projection constrains updates to directions that preserve pre-trained knowledge while allowing fine-grained feature improvement.
- **Evidence anchors:**
  - [section 3.1]: "Empirically, we observe that our loss function enhances the visual grounding of v, but introduces substantial drift from the original feature distribution... To address this, we incorporate Gradient Low-Rank Projection (GaLore)."
  - [table 6]: Full fine-tuning without GaLore drops IN-1K zero-shot from 78.50% to 70.90% (CLS) or 71.88% (Seq); with GaLore, EAGLE achieves 76.65% (CLS) and 65.81% (Seq).
  - [corpus]: MRFD (arXiv:2508.10264) uses multi-region fusion decoding for hallucination mitigation but does not address encoder fine-tuning stability.
- **Break condition:** If GaLore rank is too low, it may under-constrain useful gradient directions; if too high, drift prevention weakens.

## Foundational Learning

- **Concept: Contrastive Language-Image Pre-training (CLIP)**
  - **Why needed here:** EAGLE builds on CLIP-style visual encoders (EVA-01-CLIP, OpenAI CLIP). Understanding how CLIP aligns image and text embeddings via InfoNCE loss is prerequisite to grasping why EAGLE's modified contrastive objective matters.
  - **Quick check question:** Can you explain why the CLS token in CLIP is more aligned for zero-shot classification than patch-level sequence features?

- **Concept: Vision Transformer (ViT) Patch Embeddings**
  - **Why needed here:** EAGLE explicitly targets patch-level sequence features (dropping CLS) for fine-grained object encoding. Understanding token sequences and spatial correspondence is essential.
  - **Quick check question:** Given a 224×224 image with patch size 16×16, how many tokens are in the sequence (excluding CLS)?

- **Concept: Parameter-Efficient Fine-Tuning (PEFT) Methods**
  - **Why needed here:** GaLore is central to EAGLE's ability to preserve zero-shot performance. Distinguishing GaLore from LoRA helps understand the design choice.
  - **Quick check question:** How does GaLore differ from LoRA in terms of what is low-rank—weights or gradients?

## Architecture Onboarding

- **Component map:**
  - Image -> Visual Encoder (ViT) -> Feature sequence (patch tokens) and CLS token
  - Language encoder -> Class embeddings (e.g., "This is an image of <mask class>")
  - Fusion module (adapter/connector) -> Maps visual features to LLM prompt space
  - LLM -> Generates text responses
  - EAGLE training pipeline -> ViT + Language Encoder trained with L = L_ce + L_ins

- **Critical path:**
  1. Load pre-trained CLIP visual encoder (EVA-01-CLIP-g-14 or OpenAI CLIP-L-14-336)
  2. Prepare OpenImages V7 with instance segmentation masks (350 object classes, ~944K images)
  3. Implement masked average pooling φ(I, m) to extract object-region features
  4. Apply dual loss (L_ins + L_ce) with class-balanced sampling
  5. Use GaLore (rank=128, lr=4e-6, batch=512) to fine-tune MLP and attention linear layers
  6. Train until L_m converges (~8 epochs, ~20 hours on 2×A100 80GB)
  7. Plug EAGLE-tuned ViT into target IT-VLM at inference—no further alignment needed

- **Design tradeoffs:**
  - **CLS vs. Sequence focus:** EAGLE does not train CLS token; sequence features improve but CLS zero-shot may slightly degrade (Table 1: EVA-01 drops from 78.50% to 76.65%)
  - **Segmentation data requirement:** Depends on OpenImages V7 masks; scarcity of instance segmentation data is noted limitation
  - **IT-VLM compatibility:** LLaVA-v1.5 trains both adapter and LLM, making it harder to leverage EAGLE features vs. BLIP-2/InstructBLIP which freeze LLM
  - **No instructional data used:** EAGLE does not represent in-domain training for IT-VLMs, preserving generalization but potentially limiting task-specific gains

- **Failure signatures:**
  - **Excessive zero-shot degradation:** If CLS accuracy drops >5% (as in OpenAI CLIP-L-14-336: 76.55% → 71.46%), GaLore rank or learning rate may be misconfigured
  - **No hallucination reduction:** If EAGLE-tuned ViT does not improve POPE/MERLIM metrics, check: (a) mask-token overlap quality, (b) class resampling implementation, (c) whether correct ViT variant is used
  - **LLaVA integration underperforms:** Expected—LLaVA's joint adapter+LLM training creates feature shift; consider re-running instructional tuning with EAGLE encoder (LLaVA-v1.5* in Tables 4-5)

- **First 3 experiments:**
  1. **Baseline replication:** Fine-tune EVA-01-CLIP-g-14 with EAGLE on OpenImages V7 subset (e.g., 100K images). Validate: (a) MS-COCO false positive reduction (Table 3), (b) IN-1K zero-shot retention (Table 1)
  2. **Ablation on GaLore vs. full fine-tuning:** Compare Table 6 configurations—full fine-tune (CLS or Seq supervision) vs. GaLore—on zero-shot and false positive metrics
  3. **Plug-and-play IT-VLM test:** Replace default ViT in InstructBLIP-Vicuna-7B with EAGLE-tuned encoder. Evaluate on POPE adversarial subset and MMVP benchmark without any IT-VLM retraining. Confirm improvements per Table 4

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can surrogate sources of fine-grained supervision (e.g., pseudo-segmentation, dense captioning, or synthetic data) effectively replace instance segmentation labels in EAGLE's training framework while maintaining comparable hallucination reduction?
- **Basis in paper:** [explicit] The authors state: "the relative scarcity of image data with instance segmentations poses a challenge. We encourage future work to explore surrogate sources of fine-grained supervision to further improve model grounding and performance."
- **Why unresolved:** EAGLE currently relies on OpenImages V7's 944K images with 350 object classes—three orders of magnitude smaller than VLM pre-training data. This limits scalability and applicability to domains lacking instance segmentation annotations.
- **What evidence would resolve it:** Comparative experiments training EAGLE with alternative supervision sources (pseudo-labels from SAM, dense captions, synthetic segmentations) showing similar or better performance on MMVP, POPE, and MERLIM benchmarks.

### Open Question 2
- **Question:** Why does EAGLE produce larger improvements for BLIP-2 and InstructBLIP (which freeze the LLM during instruction tuning) compared to LLaVA-v1.5 (which tunes both adapter and LLM)?
- **Basis in paper:** [inferred] The paper notes LLaVA-v1.5 achieves smaller gains and hypothesizes two factors: EVA01's better patch-level features and LLaVA's LLM tuning creating feature mismatch. However, this is not experimentally isolated.
- **Why unresolved:** The relative contributions of (1) visual encoder architecture, (2) adapter/LLM tuning strategy, and (3) feature distribution shift remain confounded across the evaluated models.
- **What evidence would resolve it:** Ablation experiments varying only the tuning strategy (freeze vs. tune LLM) while holding the visual encoder constant, or applying EAGLE to additional IT-VLMs with controlled architectural differences.

### Open Question 3
- **Question:** What is the fundamental limit of the trade-off between preserving global zero-shot CLS token performance and enhancing fine-grained sequence feature alignment?
- **Basis in paper:** [explicit] Table 1 shows EAGLE degrades CLS zero-shot accuracy (-1.85% for EVA01, -5.09% for OpenAI) while improving sequence accuracy (+8.05%, +50.28%). The authors note GaLore mitigates but does not eliminate this drift.
- **Why unresolved:** The paper demonstrates the trade-off exists but does not characterize whether an optimal balance point exists, or if the trade-off is fundamental to the approach.
- **What evidence would resolve it:** Systematic analysis varying GaLore rank and loss weighting between global/local objectives, measuring Pareto frontier between CLS zero-shot performance and hallucination metrics.

### Open Question 4
- **Question:** Does EAGLE's effectiveness generalize to non-CLIP visual encoders (e.g., DINOv2, SigLIP, or convolutional backbones) that may have different pre-training objectives and feature distributions?
- **Basis in paper:** [inferred] EAGLE is evaluated only on CLIP-based encoders (EVA01-CLIP and OpenAI CLIP). The method relies on contrastive language-image alignment, but IT-VLMs increasingly explore diverse visual backbones.
- **Why unresolved:** The masked average pooling and contrastive loss design assumes CLIP-style alignment properties; different pre-training paradigms may respond differently to EAGLE's fine-tuning strategy.
- **What evidence would resolve it:** Applying EAGLE to IT-VLMs using alternative visual encoders and reporting hallucination benchmark performance with analysis of any architectural-specific adaptations needed.

## Limitations

- EAGLE requires instance segmentation data, limiting its applicability to domains where such annotations are unavailable
- The method shows performance degradation on CLS token zero-shot accuracy for some ViT architectures (e.g., OpenAI CLIP-L-14-336)
- EAGLE's effectiveness is reduced for IT-VLMs that jointly train adapter and LLM components (e.g., LLaVA-v1.5)

## Confidence

- **High confidence:** EAGLE's effectiveness in reducing hallucinations on established benchmarks (MMVP, POPE, MERLIM) and its ability to improve fine-grained visual grounding through instance segmentation supervision
- **Medium confidence:** The claim that GaLore preserves zero-shot capabilities across all ViT architectures, given the observed performance differences between EVA-01-CLIP and OpenAI CLIP-L-14-336
- **Low confidence:** The paper's assertion that EAGLE can be universally applied to any IT-VLM without additional fine-tuning, particularly for models like LLaVA-v1.5 that train both adapter and LLM components

## Next Checks

1. Test EAGLE's generalization to domains with limited instance segmentation data by evaluating performance on synthetic segmentation masks or transfer learning from OpenImages V7 to other segmentation datasets
2. Investigate the impact of GaLore rank and learning rate on zero-shot preservation across different ViT architectures, particularly for models that show significant CLS token degradation (e.g., OpenAI CLIP-L-14-336)
3. Develop a method to integrate EAGLE with IT-VLMs that jointly train adapter and LLM components (e.g., LLaVA-v1.5) to validate the claim of universal applicability without additional fine-tuning