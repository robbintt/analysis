---
ver: rpa2
title: 'BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video
  Diffusion Models'
arxiv_id: '2512.12080'
source_url: https://arxiv.org/abs/2512.12080
tags:
- video
- diffusion
- generation
- arxiv
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses exposure bias in autoregressive video diffusion
  models, where errors compound during generation, degrading long-horizon quality.
  The authors propose Backwards Aggregation (BAgger), a self-supervised training scheme
  that reverses the model's own drifting rollouts to form corrective trajectories.
---

# BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models

## Quick Facts
- **arXiv ID:** 2512.12080
- **Source URL:** https://arxiv.org/abs/2512.12080
- **Reference count:** 40
- **Primary result:** Backwards Aggregation (BAgger) mitigates exposure bias in autoregressive video diffusion models by using self-generated reversed rollouts to create corrective training trajectories.

## Executive Summary
This paper addresses exposure bias in autoregressive video diffusion models, where errors compound during generation, degrading long-horizon quality. The authors propose Backwards Aggregation (BAgger), a self-supervised training scheme that reverses the model's own drifting rollouts to form corrective trajectories. By iteratively fine-tuning on these reversed sequences, the model learns to recover from its own mistakes without relying on external teachers or distribution-matching losses. BAgger is instantiated on causal diffusion transformers and evaluated on text-to-video, video extension, and multi-prompt generation. It achieves more stable long-horizon motion, better visual consistency, and reduced drift compared to baselines. Quantitative results show improved frame quality and subject/background consistency over multiple BAgger rounds, with competitive motion quality. The approach effectively mitigates drift while preserving diversity, outperforming prior methods that rely on teacher models or noise injection.

## Method Summary
BAgger mitigates exposure bias by generating video rollouts from a high-quality starting frame that progressively degrade due to compounding errors. These drifted sequences are reversed in time to create "corrective trajectories" that map from low-quality back to high-quality frames. The training loop aggregates these reversed trajectories with the original seed dataset and fine-tunes the model using standard diffusion objectives. This iterative process progressively closes the train-test distribution gap by exposing the model to its own failure modes. The method is implemented on causal diffusion transformers with block-causal attention, trained on a seed dataset of video clips, then iteratively fine-tuned over multiple rounds using self-generated corrective data.

## Key Results
- BAgger reduces drift in long-horizon video generation, improving visual consistency and frame quality over multiple rounds
- The method achieves better subject and background consistency metrics compared to baseline autoregressive diffusion models
- BAgger preserves motion diversity while mitigating exposure bias, outperforming teacher-distillation approaches

## Why This Works (Mechanism)

### Mechanism 1
BAgger mitigates exposure bias by creating a training signal that explicitly teaches the model to map from its own drifted states back to high-quality states. The model generates a video rollout that begins with a high-quality frame and progressively degrades (drifts). This sequence is then reversed in time, providing a direct supervised example of how to recover from error. This is analogous to a self-generated expert demonstration. The core assumption is that a time-reversed video is semantically valid and lies within the distribution of real videos, provided the text prompt is modified to describe the reversed motion.

### Mechanism 2
Iteratively aggregating these corrective trajectories progressively closes the train-test distribution gap, making the model robust to its own compounding errors. The training process is a loop inspired by the DAgger algorithm. Each round, the model generates new rollouts representing the distribution of states it reaches at inference. By reversing and training on these specific states, the next iteration of the model learns to handle them correctly. This expands the training distribution to cover the model's own error modes. The core assumption is that the set of reversed rollouts generated over a few rounds is sufficient to cover the vast majority of the model's failure modes at inference time.

### Mechanism 3
Using the standard diffusion (score or flow matching) objective on the aggregated dataset preserves motion diversity better than alternative distribution-matching losses. By not switching to a mode-seeking loss like adversarial training or score distillation, BAgger learns a full density estimate of the conditional distribution of "corrected" futures. This prevents the model from collapsing to a single, safe, low-motion "corrected" outcome and instead allows it to learn diverse, plausible continuations from a drifted state. The core assumption is that the standard diffusion objective, when conditioned on the corrective data, is sufficient to learn the recovery function without collapsing diversity.

## Foundational Learning

- **Exposure Bias**: The fundamental problem being solved - the mismatch between training on perfect data and inference on imperfect, self-generated data, which causes error compounding. *Quick check:* Can you explain why a language model trained only on grammatically perfect text might produce nonsense if its first generated word is a typo?

- **DAgger (Dataset Aggregation)**: The conceptual framework BAgger applies from imitation learning. Understanding this helps frame the method as an iterative process of teaching a model to handle states it visits under its own policy. *Quick check:* In an autonomous driving context, what is the primary data DAgger collects to improve the policy?

- **Diffusion Forcing**: The base training framework for the autoregressive video model. It's crucial to understand that it allows for autoregressive generation by training with per-frame noise levels. *Quick check:* During training, what is the key difference in how noise is applied to frames in Diffusion Forcing versus a standard bidirectional video diffusion model?

## Architecture Onboarding

- **Component map**: Pre-trained bidirectional video diffusion transformer -> Modified with block-causal attention -> Initial training on seed dataset -> BAgger training loop (generate rollouts, reverse, aggregate, fine-tune) -> Final autoregressive model

- **Critical path**: Start with pre-trained bidirectional video diffusion model → Modify for block-causal attention and train on clean seed dataset → Execute BAgger training loop for K rounds (generate rollouts, reverse them, aggregate data, fine-tune) → Use final round model for evaluation and inference

- **Design tradeoffs**: Self-supervision vs. teacher distillation (BAgger avoids mode collapse but requires computational overhead of generating own rollouts); Standard vs. distribution-matching objective (standard diffusion is more stable but needs novel data augmentation); Fresh initialization vs. sequential fine-tuning (resetting isolates effects but continuing may be more efficient)

- **Failure signatures**: Over-saturation/under-saturation (model may over-correct colors across rounds); Reduced motion dynamics (risk of learning static frames to avoid drift); Artifacts from reversal (reversed videos may be unnatural if prompt modification is poor)

- **First 3 experiments**: Implement and verify causal attention by modifying standard video diffusion transformer and visualizing baseline drift; Single-round ablation by generating rollouts, manually reversing examples, and creating modified prompts; Full BAgger loop for 3 rounds with quantitative comparison of frame-wise quality and drift metrics across rounds

## Open Questions the Paper Calls Out

- **Open Question 1**: Does continuing training from the previous BAgger round's weights improve performance and efficiency compared to resetting to the bidirectional teacher? The paper suggests sequential accumulation could be beneficial but only tested isolation.

- **Open Question 2**: What is the optimal ratio between seed data and corrective trajectories in the aggregated dataset? The experiments use a fixed ratio without sensitivity analysis.

- **Open Question 3**: Can few-step distillation be applied on top of BAgger to accelerate inference without sacrificing diversity? The authors suggest it might avoid mode-seeking issues of prior methods but haven't tested it.

## Limitations

- The effectiveness of prompt modification for reversed actions is not fully validated, especially for complex motions
- The long-term stability of BAgger across more than 3 rounds is not tested, raising questions about potential over-correction
- The computational overhead of generating and training on reversed rollouts is not quantified against baseline methods

## Confidence

- **High**: BAgger effectively mitigates exposure bias and reduces drift in autoregressive video diffusion models
- **Medium**: BAgger preserves motion diversity better than teacher-distillation alternatives, based on the stability argument
- **Low**: The scalability and efficiency of BAgger for very long videos or different architectures is not fully demonstrated

## Next Checks

1. Test BAgger's performance with complex motion prompts (e.g., "a person doing a cartwheel" or "a bird flying in a spiral") to verify prompt modification robustness
2. Run BAgger for 5-7 rounds to assess whether performance plateaus, degrades, or continues improving
3. Benchmark computational cost per round (rollout generation + training) against Self Forcing and Corruption-Aware Training baselines