---
ver: rpa2
title: Guaranteed Optimal Compositional Explanations for Neurons
arxiv_id: '2511.20934'
source_url: https://arxiv.org/abs/2511.20934
tags:
- concepts
- beam
- explanations
- search
- quantities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first framework for computing guaranteed
  optimal compositional explanations for neurons, addressing the limitation of prior
  beam-search methods that lack optimality guarantees. The authors propose a decomposition
  of the Intersection-over-Union (IoU) metric that identifies fundamental quantities
  governing alignment quality, along with a heuristic and corresponding algorithm
  that reduce the search space and guide the search process.
---

# Guaranteed Optimal Compositional Explanations for Neurons

## Quick Facts
- arXiv ID: 2511.20934
- Source URL: https://arxiv.org/abs/2511.20934
- Authors: Biagio La Rosa; Leilani H. Gilpin
- Reference count: 40
- Primary result: Introduces the first framework for guaranteed optimal compositional explanations for neurons, showing 10-40% of beam search explanations are suboptimal on overlapping concepts.

## Executive Summary
This paper addresses the critical limitation of beam search methods for compositional explanations, which lack optimality guarantees. The authors propose a decomposition of the Intersection-over-Union (IoU) metric that identifies fundamental quantities governing alignment quality, enabling tractable search over compositional explanations. They develop both an optimal algorithm with theoretical guarantees and a heuristic-guided beam search variant that matches or improves runtime while offering greater flexibility. Experimental results on computer vision tasks with CNNs demonstrate both theoretical contributions and practical improvements.

## Method Summary
The method decomposes IoU into four fundamental quantities (unique intersections, common intersections, unique extras, common extras) that change predictably under 00-preserving logical operators. This decomposition enables admissible heuristics that bound these quantities, which are then used in best-first search to guarantee finding the optimal compositional explanation. The approach also includes a beam search variant guided by the same heuristic, offering faster but non-optimal results. The framework is evaluated on three computer vision datasets (Cityscapes, Ade20K, Broden) using ResNet18, AlexNet, and DenseNet161 models.

## Key Results
- 10-40% of beam search explanations are suboptimal when overlapping concepts are involved
- Optimal algorithm guarantees finding the best explanation within feasible runtime
- Proposed beam-search variant guided by the heuristic matches or improves runtime over prior methods
- Framework demonstrates both theoretical contributions and practical improvements to compositional explanation methods

## Why This Works (Mechanism)

### Mechanism 1
The dIoU decomposition separates spatial alignment into four tractable components: unique intersections (features annotated with exactly one concept), common intersections (features with multiple concept annotations), unique extras, and common extras. This decomposition is mathematically equivalent to standard IoU under 00-preserving operators (OR, AND, AND NOT), but exposes which components change predictably under logical operations.

### Mechanism 2
Bounding dIoU components provides admissible heuristics for pruning the search space. The heuristic computes min/max bounds for each quantity at any label L using Top-k and Bottom-k vectors. The aggregated computation provides fast overestimates for frontier pruning; sample-based computation refines estimates before expansion. This dual-level approach keeps estimated nodes >> expanded nodes.

### Mechanism 3
Best-first search with admissible heuristics guarantees optimality while remaining feasible. The algorithm iteratively refines aggregated estimates to sample-based, applies logical equivalence rules, expands or visits nodes based on max dIoU estimates, and backpropagates exact quantities from visited nodes to update frontier estimates. Pruning removes nodes whose dIoU_max falls below the current global dIoU_min threshold.

## Foundational Learning

- Concept: **Intersection-over-Union (IoU) for spatial alignment**
  - Why needed here: The entire method builds on decomposing IoU; understanding IoU as |A∩B|/|A∪B| is prerequisite.
  - Quick check question: Given two binary masks with 100 overlapping pixels and 50 non-overlapping pixels each, what is their IoU?

- Concept: **Admissible heuristics in search algorithms**
  - Why needed here: The optimality guarantee depends entirely on heuristic admissibility—understanding why A* with admissible heuristics finds optimal paths.
  - Quick check question: Why does an overestimating heuristic break optimality guarantees in best-first search?

- Concept: **00-preserving logical operators**
  - Why needed here: The decomposition equivalence to IoU holds only for operators that cannot create 1s from 0s.
  - Quick check question: Is XOR 00-preserving? Why or why not?

## Architecture Onboarding

- Component map:
  - Preprocessing -> Compute Disjoint Matrix D and per-concept quantities (I^U, I^C, E^U, E^C)
  - Heuristic Core -> Two functions—estimate_label_quantities() for single labels, estimate_paths() for future extensions
  - Search Core -> Best-first search with frontier (priority queue), memory buffer for duplicate detection, backpropagation module
  - Beam Variant -> Simpler informed beam search using only label estimation (no path estimation)

- Critical path:
  1. Preprocess concept tensors to extract all quantities (one-time cost)
  2. For each neuron: compute activation matrix N
  3. Initialize frontier with all atomic concepts and their path estimates
  4. Loop: pop highest dIoU_max node → refine if aggregated → expand or visit → update frontier

- Design tradeoffs:
  - **Aggregated vs. sample computation**: Aggregated is faster but overestimates more; use aggregated for frontier insertion, sample for refinement
  - **Memory mechanism vs. full equivalence checking**: Current buffer-based approach is O(1) per check but misses some equivalences; full checking is O(n) and prohibitively expensive
  - **Beam search vs. optimal**: Beam is 10-100× faster but 10-40% suboptimal on overlapping concepts; optimal is guaranteed but slower on uninterpretable units

- Failure signatures:
  - **Runtime explosion (>10× expected)**: Unit is uninterpretable (IoU < 0.04) or dataset has high concept overlap; switch to beam search
  - **Frontier never shrinks**: Heuristic bounds too loose; check if Bott1 vectors are all zeros (common in practice)
  - **Same node expanded twice**: Memory buffer cleared prematurely; increase buffer size or check equivalence logic

- First 3 experiments:
  1. Reproduce Table 1 on a subset of 10 neurons from ResNet Places365 using Broden: verify that visited nodes << estimated nodes and optimal IoU matches exhaustive search on small concept sets.
  2. Ablate the backpropagation mechanism: measure runtime increase and frontier size growth in high complexity settings.
  3. Compare beam+heuristic vs. MMESH beam on explanation lengths 3, 5, 10 with beam sizes 5, 10, 20: verify the claimed scalability advantage and identify the break-even point where MMESH becomes infeasible.

## Open Questions the Paper Calls Out

### Open Question 1
Can label-specific or novel vector representations be developed to prevent the heuristic from overestimating the maximum improvement, thereby avoiding convergence toward breadth-first search for long explanations? The current reliance on the top vector causes the search to behave similarly to breadth-first search and suggests future research explore vectors that are "specific to a label" or "novel and better representations of the maximum improvement."

### Open Question 2
What is the most effective hybrid strategy to compute optimal explanations for "unmeaningful" (unspecialized) neurons that currently cause the algorithm to slow down due to large search spaces? The current optimal algorithm struggles when there is no clear alignment, as the combinatorial space of concepts remains large and unpruned. The paper suggests two potential solutions: running beam search and refining the results, or automatically switching from the optimal algorithm to beam search when the frontier grows too large.

### Open Question 3
Do the dIoU decomposition and heuristic-guided search maintain feasible runtimes and optimality guarantees when applied to non-CNN architectures or non-spatial domains (e.g., NLP)? While the authors claim broad applicability, the empirical validation is restricted to vision datasets (Cityscapes, Broden, Ade20K) and CNN architectures. The abstract and methodology explicitly limit the scope to "computer vision domain and Convolutional Neural Networks," describing this application as a "first step" despite the heuristic being theoretically independent of spatial information.

## Limitations
- Optimality guarantee critically depends on the 00-preservation assumption for logical operators, which isn't extensively tested against edge cases
- Computational feasibility hinges on the heuristic's ability to provide tight bounds, but no systematic sensitivity analysis is presented for how bound looseness affects runtime
- Memory-based duplicate detection mechanism may miss logical equivalences, potentially causing minor suboptimalities even in the "optimal" algorithm

## Confidence

- **High Confidence**: The decomposition framework is mathematically sound given the 00-preservation lemma, and the optimal search algorithm's completeness proof appears rigorous. The observed 10-40% suboptimality of beam search on overlapping concepts is well-supported.
- **Medium Confidence**: The heuristic's practical effectiveness relies on empirical performance rather than theoretical guarantees about bound tightness. While runtime improvements are demonstrated, conditions where the heuristic may fail aren't fully characterized.
- **Low Confidence**: The backpropagation mechanism's impact is primarily shown through runtime metrics rather than measuring whether it actually improves solution quality or just speeds up finding the same solutions.

## Next Checks

1. **Edge Case Testing**: Systematically test the 00-preservation assumption with non-standard logical operators (XOR, implication) to confirm the decomposition breaks as expected, validating the theoretical foundation.

2. **Heuristic Sensitivity Analysis**: Measure how bound tightness varies with concept correlation levels and dataset characteristics across all three datasets, identifying regimes where the heuristic may fail.

3. **Equivalence Detection Validation**: Implement a small-scale exhaustive equivalence checker to measure how often the memory-based approach misses duplicates, quantifying the "optimal" algorithm's true optimality rate.