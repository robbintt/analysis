---
ver: rpa2
title: 'Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders'
arxiv_id: '2508.16560'
source_url: https://arxiv.org/abs/2508.16560
tags:
- features
- saes
- decoder
- feature
- 'true'
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work shows that Sparse Autoencoders (SAEs) fail to disentangle\
  \ true features in large language models if their sparsity parameter L0 is set too\
  \ low. At low L0, SAEs \u201Ccheat\u201D by mixing correlated features to improve\
  \ reconstruction, corrupting latents."
---

# Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders

## Quick Facts
- arXiv ID: 2508.16560
- Source URL: https://arxiv.org/abs/2508.16560
- Reference count: 40
- Primary result: Most existing SAEs use L0 values that are too low, resulting in incorrect, polysemantic features due to mixing correlated features to improve reconstruction.

## Executive Summary
This work demonstrates that Sparse Autoencoders (SAEs) fail to learn true, disentangled features when their sparsity parameter L0 is set too low. At low L0, SAEs "cheat" by mixing correlated features into single latents to minimize reconstruction loss, corrupting the learned representations. The authors introduce a decoder-latent correlation metric (c_dec) that identifies the correct L0 and correlates with downstream sparse probing performance. Their analysis reveals that commonly used L0 values in the literature are systematically too low, leading to incorrect feature representations in existing SAEs.

## Method Summary
The authors propose a method to identify the correct L0 value for SAEs by training a sweep over different L0 values and computing the decoder pairwise cosine similarity metric (c_dec). They validate this approach using both toy models with known ground truth features and real LLM activations from Gemma-2-2b and Llama-3.2-1b. The correct L0 is identified as the value where c_dec is minimized (or at the "elbow" before low-L0 spike). They then validate the identified L0 using k-sparse probing benchmarks, showing peak F1 scores align with the c_dec minimum.

## Key Results
- SAEs trained with L0 below the true value mix correlated features to improve MSE reconstruction, corrupting latents
- Decoder pairwise cosine similarity (c_dec) is minimized at the correct L0 and correlates with sparse probing performance
- High L0 values cause degenerate solutions where latents split or duplicate features
- Most existing SAEs use L0 values that are too low, resulting in incorrect, polysemantic features

## Why This Works (Mechanism)

### Mechanism 1
When SAE L0 is set below the true L0 of the data, MSE reconstruction loss actively incentivizes feature mixing rather than disentanglement. With insufficient latents allowed to fire, the SAE minimizes reconstruction error by combining correlated features into single latents. For example, if features f₁ and f₂ co-occur frequently, a single latent that's a weighted average of both achieves lower MSE than two correct but sparse latents when L0=1. This occurs because E[L(α=0.5)] < E[L(α=1)] when features co-occur frequently.

### Mechanism 2
Decoder pairwise cosine similarity (c_dec) serves as a proxy metric for detecting feature mixing and identifying the correct L0. When latents are disentangled, decoder vectors should be near-orthogonal. When correlated features are mixed into multiple latents, those latents share common feature components, increasing their pairwise cosine similarity. c_dec is minimized at the true L0, providing a way to detect when SAEs are learning incorrect, mixed features.

### Mechanism 3
High L0 values cause degenerate solutions where latents split or duplicate features, also corrupting monosemanticity. With excessive capacity, the SAE lacks pressure to find efficient representations and may learn redundant or split features. While this affects fewer latents than the low-L0 case, it still leads to incorrect feature representations. JumpReLU SAEs appear more robust to high L0 than BatchTopK, suggesting architecture-dependent break conditions.

## Foundational Learning

- **Linear Representation Hypothesis (LRH)**: Assumes concepts are linear directions in activation space. Why needed: The entire SAE framework assumes concepts are linear directions in activation space. Without this, the notion of "correct features" and decoder orthogonality doesn't hold. Quick check: Can you explain why orthogonal decoder directions would correspond to distinct concepts under LRH?

- **Superposition and Sparsity**: SAEs are designed to "reverse superposition" — understanding that neurons are polysemantic due to non-orthogonal feature combinations is essential context. Why needed: SAEs are designed to "reverse superposition" — understanding that neurons are polysemantic due to non-orthogonal feature combinations is essential context. Quick check: Why can't we simply read features directly from neuron activations without an SAE?

- **L0 vs L1 Regularization**: L0 is the actual sparsity measure (count of active features), while L1 is often used as a proxy. Why needed: This paper argues L0 has a *correct* value, not just a tradeoff. Quick check: In TopK vs JumpReLU SAEs, how is L0 controlled differently?

## Architecture Onboarding

- **Component map**: Encoder (W_enc) -> Nonlinearity (JumpReLU or BatchTopK) -> Decoder (W_dec) -> Reconstruction (x̂ = W_dec @ a + b_dec)
- **Critical path**: 1. Train SAE sweep over L0 values (e.g., 50 to 500 for LLM layers) 2. Compute c_dec for each trained SAE using decoder weights 3. Identify L0 where c_dec is minimized (or at "elbow" before low-L0 spike) 4. Validate with sparse probing: peak F1 should align with c_dec minimum
- **Design tradeoffs**: BatchTopK offers direct L0 control and simpler training but uses global thresholds limiting per-latent flexibility. JumpReLU adapts thresholds per latent better for high L0 but requires more careful sparsity coefficient tuning. Low L0 corrupts ALL latents; high L0 corrupts some but not all.
- **Failure signatures**: L0 too low: c_dec spikes upward; sparse probing F1 drops; decoder histograms show wide projections. L0 too high: c_dec rises gradually; latents may split or become redundant. Permanent damage: Starting with too-low L0 and correcting later leaves SAE in poor local minimum.
- **First 3 experiments**: 1. Replicate toy model: Create synthetic data with known true L0=11 and correlated features; train BatchTopK SAEs at L0 ∈ {5, 11, 18}; verify c_dec minimum at true L0 2. LLM layer sweep: Train BatchTopK SAEs on a mid-layer (e.g., layer 5) of a small LLM with L0 from 50-500; plot c_dec vs L0 to find minimum 3. Sparse probing validation: For the LLM sweep, run k-sparse probing benchmark; confirm peak F1 aligns with c_dec minimum

## Open Questions the Paper Calls Out

### Open Question 1
Can L0 be optimized automatically during SAE training without requiring expensive hyperparameter sweeps? While their metric currently requires training a sweep over L0, the authors hope it may be possible to optimize this metric automatically during training. Preliminary attempts worked in toy models but required extensive hyperparameter tuning in LLMs, with challenges including delayed metric response to L0 changes, flat gradients near optimal L0, and risk of permanent damage from dropping L0 too low.

### Open Question 2
How do non-linear features (violating the Linear Representation Hypothesis) affect SAE behavior and the L0 recommendations from the decoder correlation metrics? The entire theoretical framework assumes linear representations; circular embeddings and other non-linear representations have been observed in LLMs. The paper doesn't investigate how SAEs react if the underlying features are actually non-linear.

### Open Question 3
Can decoder pairwise correlations reveal the underlying correlational structure between true features in language models? The authors are excited about the possibility that studying correlations in the SAE decoder could teach us more about the underlying correlational structure between underlying features. This could potentially invert the mixing phenomenon to recover ground-truth feature correlations from trained SAEs.

### Open Question 4
What causes JumpReLU SAEs to handle high L0 better than BatchTopK SAEs, and specifically, why does JumpReLU's threshold increase with L0 while BatchTopK's decreases? Both architectures should theoretically be similar, yet show markedly different high-L0 robustness and threshold dynamics. The authors hypothesize per-latent threshold adjustment may help but do not establish causation.

## Limitations

- The analysis relies heavily on the Linear Representation Hypothesis, which may not hold in practice if features have non-orthogonal directions even without co-firing
- The toy model validation is compelling but may not fully capture real LLM activation dynamics
- The paper doesn't extensively explore how correlated feature mixing affects downstream capabilities beyond sparse probing performance

## Confidence

- **High confidence**: The empirical observation that MSE reconstruction loss incentivizes feature mixing when L0 is too low, supported by both toy models and LLM experiments
- **Medium confidence**: The decoder cosine similarity metric (c_dec) reliably identifying correct L0, though validation is primarily within this work
- **Medium confidence**: The claim that most existing SAEs use L0 values that are too low, based on comparison to their identified "correct" values

## Next Checks

1. Test c_dec metric robustness on SAEs trained with different architectures (JumpReLU vs BatchTopK) and initialization strategies to verify it consistently identifies correct L0
2. Conduct ablation studies varying feature orthogonality and correlation strength in toy models to determine c_dec sensitivity to underlying data structure
3. Evaluate downstream task performance (beyond sparse probing) of SAEs trained at c_dec-minimizing L0 versus commonly used lower L0 values