---
ver: rpa2
title: 'Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy
  Evaluation of LLM-Based and Supervised Approaches'
arxiv_id: '2512.05537'
source_url: https://arxiv.org/abs/2512.05537
tags:
- clinical
- incidentaloma
- radiology
- reports
- lesion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates large language models (LLMs) and supervised
  encoders for lesion-level detection of incidentalomas requiring follow-up in radiology
  reports. A novel approach using lesion-tagged inputs and anatomy-aware prompting
  significantly improves LLM performance, with GPT-OSS-20b achieving an incidentaloma-positive
  macro-F1 of 0.79, surpassing all supervised baselines and closely matching human
  inter-annotator agreement.
---

# Automated Identification of Incidentalomas Requiring Follow-Up: A Multi-Anatomy Evaluation of LLM-Based and Supervised Approaches

## Quick Facts
- arXiv ID: 2512.05537
- Source URL: https://arxiv.org/abs/2512.05537
- Reference count: 40
- Primary result: GPT-OSS-20b achieves incidentaloma macro-F1 of 0.79, outperforming supervised baselines

## Executive Summary
This study evaluates large language models (LLMs) and supervised encoders for lesion-level detection of incidentalomas requiring follow-up in radiology reports. A novel approach using lesion-tagged inputs and anatomy-aware prompting significantly improves LLM performance, with GPT-OSS-20b achieving an incidentaloma-positive macro-F1 of 0.79, surpassing all supervised baselines and closely matching human inter-annotator agreement. Anatomy-aware prompting provided statistically significant gains across GPT-based models. A majority-vote ensemble of top systems further improved macro-F1 to 0.90. The findings demonstrate that generative LLMs, when enhanced with structured lesion context, offer a reliable, interpretable pathway for automated incidental finding surveillance in clinical workflows.

## Method Summary
The study evaluates 3-class lesion-level classification (0=No Incidentaloma, 1=Incidentaloma–No Risk, 2=Incidentaloma–Follow-up Required) across 7 anatomies using 400 annotated radiology reports. Supervised encoders (BioClinicalModernBERT, ModernBERT, Clinical Longformer) are fine-tuned on anatomy-lesion-context triples, while LLMs (Llama 3.1-8B, GPT-4o, GPT-OSS-20b) use lesion-tagged inputs with XML tags and anatomy-aware prompts. PL-Marker++ extracts lesion spans and anatomy mappings for preprocessing. Predictions are aggregated per anatomy via max severity, with ensemble voting combining top systems for final classification.

## Key Results
- GPT-OSS-20b achieves incidentaloma macro-F1 of 0.79, outperforming all supervised baselines
- Anatomy-aware prompting provides statistically significant performance gains (p < 0.05) across GPT-based models
- Majority-vote ensemble of top systems achieves macro-F1 of 0.90, with 0.10 improvement for follow-up required cases
- Model performance approaches human inter-annotator agreement (IAA macro-F1 = 0.76)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured lesion tagging reduces ambiguity in multi-lesion reports, improving LLM focus on target findings.
- Mechanism: XML tags (e.g., `<LESION1>...</LESION1>`) explicitly bound candidate lesion spans, disambiguating repeated terms like "nodule" that appear in different contexts within the same report. This directs model attention to specific spans rather than diffusing attention across all mentions.
- Core assumption: The upstream entity extraction system (PL-Marker++) accurately identifies lesion spans before tagging.
- Evidence anchors:
  - [abstract] "lesion-tagged inputs and anatomy-aware prompting to ground model reasoning"
  - [section 3.2.2] "ablation study using the Llama 3.1-8B Instruct model showed that the inclusion of lesion tags substantially improved performance compared with untagged inputs... improvement was primarily driven by higher precision with comparable recall"
  - [corpus] No direct corpus evidence on lesion tagging specifically for incidentaloma tasks.
- Break condition: If upstream entity extraction has high false negative rates (misses lesions) or incorrect span boundaries, tagged inputs will propagate errors and may mislead LLM reasoning.

### Mechanism 2
- Claim: Explicit anatomical grounding via lesion-anatomy mappings improves classification accuracy by removing residual ambiguity.
- Mechanism: A supplementary mapping line (e.g., `LESION1=Thyroid; LESION2=Pancreas`) pins each tagged lesion to its anatomical location. This compensates for cases where anatomy mentions appear outside the immediate sentence context, which PL-Marker++ may miss. The mapping allows LLMs to apply anatomy-specific clinical reasoning (e.g., different size thresholds for lung vs. liver lesions).
- Core assumption: Anatomy mappings are accurate and consistently formatted; LLMs can correctly parse and utilize the mapping syntax.
- Evidence anchors:
  - [abstract] "Explicit anatomical grounding yielded statistically significant performance gains across GPT-based models (p < 0.05)"
  - [section 4.2] "GPT-4o (Anatomy) significantly outperformed its base version (p=0.012), and GPT-OSS (Anatomy) also improved over GPT-OSS (Base)"
  - [corpus] Corpus evidence is weak; no directly comparable anatomy-aware prompting approaches found in neighbors.
- Break condition: If anatomy mappings contain errors (wrong organ assignments), models will apply incorrect clinical criteria, potentially increasing false positives or misses.

### Mechanism 3
- Claim: Majority-vote ensemble of heterogeneous systems (LLMs + supervised encoders) improves robustness by leveraging complementary error profiles.
- Mechanism: LLMs excel at contextual reasoning (e.g., correctly linking RIS template recommendations to specific lesions), while supervised encoders exhibit conservative, precision-oriented decision boundaries with fewer false positives. Majority voting with conservative tie-breaking (prefer lower severity label) balances these tendencies.
- Core assumption: Component models have uncorrelated error patterns; inference latency and computational overhead are acceptable for deployment context.
- Evidence anchors:
  - [section 5.1] "Twelve lesions were correctly classified by all GPT-based models but misclassified by both BERT-based models, whereas the opposite occurred only five times"
  - [section 5.1] "This parameter-free ensemble achieved the highest overall performance, yielding a Macro-F1 of 0.902... particularly for identifying incidentalomas requiring follow-up, where the F1 value increased by more than 0.10"
  - [corpus] Neighbor paper "Evaluating the Challenges of LLMs in Real-world Medical Follow-up" discusses LLM limitations in complex medical tasks but does not address ensemble strategies.
- Break condition: If models have correlated failures on specific lesion types or phrasing patterns, ensemble gains will diminish; computational cost may exceed latency budgets in real-time workflows.

## Foundational Learning

- Concept: **Lesion-level vs. document-level classification**
  - Why needed here: The paper argues prior systems classified entire reports, missing fine-grained distinctions where some lesions require follow-up while others in the same report do not.
  - Quick check question: Can you explain why a single report might contain both Class 0 (no incidentaloma) and Class 2 (follow-up required) lesions?

- Concept: **Severity precedence aggregation**
  - Why needed here: The paper aggregates multiple lesion-level predictions per anatomy using `max()` to prioritize the most severe finding.
  - Quick check question: If a liver has three lesions labeled {0, 1, 2}, what is the final anatomy-level label and why?

- Concept: **Cost-sensitive learning for imbalanced classes**
  - Why needed here: The dataset has severe class imbalance (Table 3 shows 180 follow-up required vs. 2,346 no incidentaloma), requiring specialized loss functions to avoid minority class neglect.
  - Quick check question: Why would standard cross-entropy loss underperform on Class 2 (follow-up required) in this dataset?

## Architecture Onboarding

- Component map:
  - PL-Marker++ -> Lesion tagging -> LLM/supervised inference -> Lesion-level predictions -> Anatomy aggregation -> Document-level output

- Critical path: Raw report → PL-Marker++ extraction → Lesion tagging → Anatomy mapping → LLM/supervised inference → Lesion-level predictions → Anatomy aggregation → Document-level output

- Design tradeoffs:
  - Tagged inputs improve precision but depend on upstream extraction quality
  - Anatomy-aware prompting requires additional preprocessing but yields statistically significant gains
  - Ensemble improves F1 but increases inference latency and deployment complexity
  - Conservative tie-breaking reduces false positives but may increase misses on borderline cases

- Failure signatures:
  - High false negatives on Class 2: Check if follow-up recommendations appear only in impression sections or as abbreviated phrases (e.g., "f/u CT advised")
  - Class 2→1 underestimation: Review reports with conditional language ("follow-up if high risk") where reassuring phrases are overweighted
  - False positives on Class 0: Examine cases where clinical indication and lesion relationship is ambiguous

- First 3 experiments:
  1. **Ablation on tagging**: Compare lesion-tagged vs. untagged inputs across 0-shot, 1-shot, 5-shot settings to isolate tagging contribution (replicate Section 3.2.2 ablation).
  2. **Prompt sensitivity**: Test anatomy mapping variations (e.g., JSON vs. semicolon-delimited) to assess format robustness.
  3. **Ensemble component analysis**: Remove one model at a time from ensemble to identify which components drive specific class improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the lesion-level incidentaloma classification models generalize effectively to external institutions with differing radiology reporting conventions?
- Basis in paper: [explicit] The authors note that reporting conventions "vary widely across institutions" and that reliance on a single-institution dataset "could reduce the transferability of model performance" without adaptation.
- Why unresolved: The study evaluated performance exclusively on reports from the UW Medicine system, leaving cross-institutional robustness unverified.
- What evidence would resolve it: Evaluation of the models on a multi-institutional test set with distinct reporting styles and modalities.

### Open Question 2
- Question: Does incorporating temporal patient history and prior imaging comparisons significantly improve the accuracy of distinguishing new incidentalomas from stable lesions?
- Basis in paper: [explicit] The paper states the current approach "did not incorporate temporal context from prior studies" and identifies the integration of longitudinal data as a necessary step for future research.
- Why unresolved: Models currently operate on single reports, lacking access to trends or historical status that often inform clinical decisions.
- What evidence would resolve it: A comparative study analyzing error rates (specifically severity underestimation) when longitudinal context is added to the input features.

### Open Question 3
- Question: How do model-generated reasoning traces align with radiologist decision-making, and do they improve clinical trust compared to supervised model attributions?
- Basis in paper: [explicit] The limitations section calls for "deeper interpretability analyses involving radiology experts to fully understand the decision-making behavior" of LLMs in complex narratives.
- Why unresolved: While the study notes improved interpretability with lesion tags, the clinical validity of the models' reasoning processes versus human expert logic remains unverified.
- What evidence would resolve it: A user study where radiologists assess the logical coherence and utility of model-generated explanations for complex borderline cases.

## Limitations
- Single-institution dataset may limit generalizability to other radiology reporting conventions and institutional practices
- Dependence on upstream PL-Marker++ extraction pipeline creates potential brittleness in lesion tagging reliability
- GPT-OSS-20b model availability and licensing restrictions may limit reproducibility of top-performing results

## Confidence

**High Confidence**: Lesion-tagged input approach demonstrably improves LLM precision over untagged inputs (p < 0.05); ensemble majority-vote method consistently boosts macro-F1 across all test folds.

**Medium Confidence**: Anatomy-aware prompting provides statistically significant gains for GPT-based models, though the practical clinical impact on specific anatomy subtypes (particularly "Other") requires further validation.

**Low Confidence**: Cross-institutional performance and robustness to report format variations cannot be assessed without external validation datasets.

## Next Checks
1. **External validation**: Test the ensemble system on radiology reports from multiple institutions with different subspecialty reporting conventions to assess generalizability.
2. **Pipeline robustness**: Evaluate PL-Marker++ extraction performance on reports with varying formatting styles, abbreviated phrases, and non-standard lesion descriptions.
3. **Cost-sensitivity analysis**: Characterize how different cost matrix configurations affect the expected-cost objective and clinical decision thresholds for follow-up recommendations.