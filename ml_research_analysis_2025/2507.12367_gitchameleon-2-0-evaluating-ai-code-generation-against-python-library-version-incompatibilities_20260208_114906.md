---
ver: rpa2
title: 'GitChameleon 2.0: Evaluating AI Code Generation Against Python Library Version
  Incompatibilities'
arxiv_id: '2507.12367'
source_url: https://arxiv.org/abs/2507.12367
tags:
- code
- success
- library
- solution
- version
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GitChameleon 2.0 is a new benchmark for evaluating AI code generation
  systems on their ability to produce version-specific Python code. The benchmark
  includes 328 problems, each requiring solutions for a specific library version,
  and uses executable tests to assess functional accuracy.
---

# GitChameleon 2.0: Evaluating AI Code Generation Against Python Library Version Incompatibilities

## Quick Facts
- arXiv ID: 2507.12367
- Source URL: https://arxiv.org/abs/2507.12367
- Reference count: 40
- Key outcome: Even best models achieve only 48-51% success on hidden tests for version-specific Python code generation

## Executive Summary
GitChameleon 2.0 introduces a benchmark for evaluating AI code generation systems on their ability to produce version-specific Python code that passes executable unit tests. The benchmark includes 328 problems across 26 libraries, requiring solutions for specific library versions that models likely encountered during training. Testing various approaches including LLMs, agents, and coding assistants, the study reveals significant limitations in current AI systems' ability to handle library version constraints, with best models achieving only 48-51% success on hidden tests. Self-debugging with execution feedback and RAG with version-specific documentation provide moderate improvements, but substantial challenges remain in version disambiguation and code generation accuracy.

## Method Summary
The GitChameleon 2.0 benchmark evaluates version-conditioned code generation where models must generate Python code that passes executable unit tests for specified library versions. The benchmark contains 328 problems across 26 libraries with versions from 2014-2023. Evaluation uses hidden test success rate as the primary metric, with visible tests enabling self-debugging. The framework tests multiple approaches: greedy decoding, zero-shot chain-of-thought, self-debugging with visible test feedback, RAG with version-specific documentation, multi-step agents, and CLI/IDE coding assistants. All code is validated in clean Docker containers with exact library versions.

## Key Results
- Best models achieve only 48-51% success on hidden tests, revealing significant limitations in version-specific code generation
- Self-debugging with visible test feedback improves performance by 10-20%, but increases the gap between visible and hidden test success
- RAG with version-specific documentation provides moderate gains (40%+) but fails to solve many problems even with documentation access
- Open-weight models perform significantly worse than proprietary models on version-conditioned generation tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-debugging with execution feedback substantially improves version-specific code generation
- Mechanism: Models receive error traces from visible test failures, allowing them to diagnose version mismatches and regenerate correct API calls
- Core assumption: Models have sufficient internal knowledge of correct APIs but fail to retrieve them without execution signal
- Evidence anchors:
  - [section 3.2.3] Self-debugging yields 10-20% gains; GPT-4.1-mini improves from 44% to 68% on hidden tests
  - [figure 8] All error categories show 50-90% reduction with self-debug
  - [corpus] Related work (MLDebugging) confirms debugging in multi-library scenarios remains understudied

### Mechanism 2
- Claim: RAG with version-specific documentation provides moderate gains by grounding API knowledge
- Mechanism: Retrieved documents specify exact parameter signatures and behavior for target versions, reducing hallucination of incorrect APIs
- Core assumption: Retrieved documentation is accurate and models can correctly interpret and apply it
- Evidence anchors:
  - [section 3.2.6] GPT-4.1 improves from 48.5% to 58.5% with RAG; 40%+ problems remain unsolved even with documentation access
  - [table 12] Retrieving k=3 documents outperforms k=1 across most models
  - [corpus] CODESYNC identifies similar challenge—LLMs struggle with "continually evolving code knowledge" from static pre-training

### Mechanism 3
- Claim: Execution-based evaluation with hidden tests captures functional correctness better than string matching
- Mechanism: Unit tests directly validate runtime behavior against expected outputs, catching version mismatches that produce syntactically valid but functionally incorrect code
- Core assumption: Tests comprehensively cover the version-specific behavior being evaluated
- Evidence anchors:
  - [section 2.2] Primary metric is hidden test success rate; API hit rate is secondary as functionally correct alternatives may use different APIs
  - [section 4] GitChameleon 2.0 "distinguishes itself" from VersiCode and Wang et al. by using executable tests rather than string matching

## Foundational Learning

- Concept: **Version-Conditioned Generation (VCG) vs. Code Evolution**
  - Why needed here: VCG requires generating code for in-distribution versions the model has seen; code evolution focuses on out-of-distribution migration to newer versions
  - Quick check question: If you ask a model to "use PyTorch 1.9's API" versus "migrate this code from PyTorch 1.7 to 1.9," which paradigm does each belong to?

- Concept: **Breaking Change Taxonomy**
  - Why needed here: Understanding change types (argument/attribute, function name, semantics, new feature) helps diagnose where models fail most
  - Quick check question: What change category does deprecating `bw` parameter in favor of `bw_method`/`bw_adjust` belong to?

- Concept: **Visible-Hidden Test Gap**
  - Why needed here: Self-debugging on visible tests can overfit to those specific assertions; the gap between visible and hidden success rates indicates generalization
  - Quick check question: Why might a model pass visible tests after self-debug but still fail hidden tests?

## Architecture Onboarding

- Component map: Problem Spec -> Test Suite -> Model Generation -> Execution Container -> Test Validation -> (Self-debug Loop)
- Critical path: Problem input → Model generation → Execution in versioned Docker container → Test validation → (if self-debug) Error trace feedback → Regeneration
- Design tradeoffs:
  - Visible tests enable self-debug but may lead to overfitting; paper shows gap increases after self-debug
  - RAG helps but retrieval quality varies; k=3 better than k=1 but introduces false positives
  - Agents with sandbox achieve higher success but require more compute and latency
- Failure signatures:
  - **AttributeError**: Model calls non-existent method (e.g., `torch.special.i1` in PyTorch 1.9)
  - **AssertionError with deprecation message**: Model uses deprecated parameter (`bw` vs `bw_method`)
  - **ValueError on array shape**: Model applies batch-incompatible function (e.g., `scipy.linalg.expm` on 3D tensor)
- First 3 experiments:
  1. Run greedy decoding baseline on a subset of 50 problems across 3 model tiers (small/medium/large) to establish performance baseline
  2. Implement self-debug loop with visible test feedback; measure improvement delta per error category
  3. Build minimal RAG pipeline retrieving top-3 documents; compare against greedy baseline on problems categorized by change type

## Open Questions the Paper Calls Out

- How do human software developers perform on version-conditioned code generation compared to state-of-the-art LLMs?
  - Basis: The Limitations section states "we do not include human evaluations, which could provide a baseline for estimating average human performance on this task."

- Why does self-debugging improve visible test performance more than hidden test performance, and can this gap be reduced?
  - Basis: Figure 6 and Section 3.2.3 show that self-debugging increases the visible-hidden gap, indicating "self-debugging on visible tests has a limited ability to improve on the hidden tests."

- Can LLMs perform version-to-version translation (e.g., upgrading from 1.8 to 1.9 or downgrading) when both versions are in-distribution?
  - Basis: The Limitations section explicitly notes "we do not evaluate version-to-version translation" and suggests this as valuable future work.

## Limitations
- Reliance on in-distribution library versions limits generalizability to truly out-of-distribution scenarios
- Self-debugging mechanism may capture superficial patterns rather than genuine version understanding due to visible test overfitting
- RAG approach shows moderate gains but effectiveness depends heavily on retrieval quality and model's ability to correctly apply documentation

## Confidence
- High confidence in benchmark design and execution-based evaluation methodology
- Medium confidence in comparative performance analysis across model types and approaches
- Low confidence in scalability of current approaches to truly OOD library versions

## Next Checks
1. Test the same models on truly OOD library versions (versions never seen during training) to establish the limits of current approaches
2. Conduct ablation studies isolating the contribution of each self-debugging iteration versus initial generation quality
3. Evaluate whether RAG retrieval quality (document relevance and correctness) correlates with success rates to identify if failures stem from retrieval or application