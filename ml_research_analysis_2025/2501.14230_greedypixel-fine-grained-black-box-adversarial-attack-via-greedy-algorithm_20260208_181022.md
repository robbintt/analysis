---
ver: rpa2
title: 'GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm'
arxiv_id: '2501.14230'
source_url: https://arxiv.org/abs/2501.14230
tags:
- attack
- greedypixel
- adversarial
- attacks
- black-box
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents GreedyPixel, a novel black-box adversarial
  attack method that achieves fine-grained pixel-wise perturbations without gradient
  access. By performing brute-force-style, per-pixel greedy optimization guided by
  a surrogate-derived priority map and refined through query feedback, GreedyPixel
  guarantees monotonic loss reduction and convergence to a coordinate-wise optimum.
---

# GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm

## Quick Facts
- **arXiv ID**: 2501.14230
- **Source URL**: https://arxiv.org/abs/2501.14230
- **Reference count**: 40
- **Key outcome**: Novel black-box attack achieving state-of-the-art success rates with visually imperceptible perturbations through greedy, per-pixel optimization without gradient access.

## Executive Summary
GreedyPixel introduces a novel black-box adversarial attack method that achieves fine-grained pixel-wise perturbations without requiring gradient access to the target model. The approach combines a surrogate-guided priority map with discrete binary search and monotonic convergence via query feedback. By restricting perturbations to extreme values (±ε) and evaluating pixels in order of importance, GreedyPixel achieves state-of-the-art attack success rates while maintaining high perceptual quality. The method effectively bridges the gap between black-box practicality and white-box performance, outperforming existing attacks in success rate, visual quality, and computational efficiency.

## Method Summary
GreedyPixel operates through three main phases: (1) generating a pixel-wise priority map using gradients from a surrogate model, (2) performing per-pixel greedy search by evaluating all 2^C combinations of ±ε perturbations, and (3) accepting updates only when they reduce the target loss through query feedback. The algorithm uses an adversarially trained surrogate to create a priority map that ranks pixels by their potential impact, then processes each pixel independently by evaluating all possible perturbation combinations and accepting only those that decrease the loss. This creates a monotonic descent that converges to a coordinate-wise optimum. The method is implemented using the Adversarial Robustness Toolbox with a maximum query budget of 20,000 and ε=4/255.

## Key Results
- Achieves 100% ASR on CIFAR-10 against WideResNet-28-10 with superior SSIM and LPIPS compared to state-of-the-art methods
- Maintains high success rates on ImageNet with visually imperceptible perturbations at 64×64 resolution
- Demonstrates significant improvements in perceptual quality while requiring fewer queries than competing black-box approaches

## Why This Works (Mechanism)

### Mechanism 1: Surrogate-Guided Search Ordering
The algorithm computes a pixel-wise priority map using the gradient of the loss with respect to the input on a surrogate model. By processing pixels in descending order of importance, the attack targets coordinates with the highest probability of causing misclassification first, effectively performing steepest coordinate descent. This assumes the surrogate and target models share positive gradient alignment, meaning features important to the surrogate are likely important to the target.

### Mechanism 2: Discrete Binary Search Space Reduction
Restricting the perturbation search space to binary values {−ε, +ε} per channel maintains high Attack Success Rate while making brute-force evaluation tractable. This transforms an intractable continuous problem into a linear-time search O(H × W × 2^C). The method validates that extreme values are sufficient to induce misclassification, as 70.43% of perturbations in white-box attacks occur at ±ε.

### Mechanism 3: Monotonic Convergence via Query Feedback
The algorithm guarantees monotonic decrease in the target loss function by querying the target model for every candidate pixel update. If the new loss is not strictly lower than the current loss, the update is rejected. This acts as a discrete line search, ensuring strict coordinate descent and preventing cyclical divergence seen in stochastic black-box methods.

## Foundational Learning

- **Concept: Coordinate Descent**
  - Why needed here: GreedyPixel is essentially coordinate descent applied to image pixels. It minimizes the objective by optimizing one coordinate at a time while holding others fixed.
  - Quick check question: If you update a pixel to minimize loss at step t, does that guarantee the loss stays lower at step t+1 when updating a different pixel? (Answer: Not without the specific feedback check used here).

- **Concept: Transferability and Gradient Alignment**
  - Why needed here: The method relies on a "surrogate model" to guide the attack. Understanding why gradients from one model apply to another is key to understanding why the priority map isn't random noise.
  - Quick check question: Why might an adversarially trained surrogate provide a better priority map for a standard target model than a random surrogate? (Answer: Robust features align better with human perception and core data structure, reducing spurious gradient noise).

- **Concept: Sparse vs. L∞ Attacks**
  - Why needed here: GreedyPixel creates perturbations that are constrained in magnitude (L∞ limited by ε) but effectively sparse (few pixels modified). Distinguishing these constraints is necessary to interpret the visual quality results.
  - Quick check question: Does restricting a pixel to {−ε, +ε} limit the L0 norm or the L∞ norm? (Answer: It explicitly sets the L∞ magnitude while the greedy selection implicitly controls the L0 sparsity).

## Architecture Onboarding

- **Component map**: Priority Engine -> Pixel Selection -> Parallel Candidate Evaluation -> Best Candidate Check -> State Update
- **Critical path**: Priority Map Generation → Pixel Selection → Parallel Candidate Evaluation (8 queries) → Best Candidate Check → State Update
- **Design tradeoffs**: The method achieves high visual quality (SSIM) but requires query complexity proportional to the number of pixels (H × W × 2^C). High-resolution images may exceed query budgets before convergence unless early stopping is triggered. An adversarially trained surrogate improves perceptual quality but may differ in gradient direction from non-robust targets.
- **Failure signatures**: Stalling (loss flattens early with pixels flipping back and forth), Query Exhaustion (budget runs out before ℓ < 0), and Poor visual quality (low SSIM/high LPIPS indicating surrogate misalignment).
- **First 3 experiments**: 1) Low-Resolution Baseline (CIFAR-10): Run GreedyPixel against ResNet to verify 100% ASR within theoretical query limit. 2) Surrogate Ablation: Compare ASR using random, standard, and adversarially trained surrogates. 3) Resolution Scaling Stress Test: Apply to ImageNet with capped query budget to identify break point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hierarchical pixel selection or gradient-free attention guidance be integrated into GreedyPixel to reduce the query cost for very high-resolution images?
- Basis in paper: [explicit] Section VII states that "reducing query cost for very high-resolution images remains an open problem" and proposes investigating "advanced solutions such as hierarchical pixel selection and gradient-free attention guidance in future work."
- Why unresolved: The current pixel-wise greedy approach incurs linear query cost (O(H × W)), which becomes prohibitive for high-resolution inputs under tight query budgets.
- What evidence would resolve it: A modified GreedyPixel algorithm using hierarchical search strategies achieving comparable ASR with significantly lower query counts.

### Open Question 2
- Question: Can alternative approaches to constructing the priority map (e.g., attention-based or objective-based) outperform the surrogate gradient-based method in scenarios with significant surrogate-target model discrepancy?
- Basis in paper: [explicit] Section VIII notes, "Future work will explore alternative approaches to constructing the priority map to further enhance the efficiency of greedy learning."
- Why unresolved: Significant architectural differences between surrogate and target can lead to suboptimal pixel prioritization and reduced transferability.
- What evidence would resolve it: Comparative experiments showing non-gradient sources yield higher ASR when surrogate and target are highly dissimilar.

### Open Question 3
- Question: Is the coordinate-wise greedy optimization strategy effective for manipulating latent codes in diffusion models to achieve precise image editing under black-box conditions?
- Basis in paper: [explicit] Section VIII states, "we will demonstrate the applicability of GreedyPixel for manipulating sensitive entities, such as the latent codes of diffusion models, to achieve precise image editing under black-box conditions."
- Why unresolved: The method is validated on image-space classification tasks; it's unclear if the discrete, coordinate-wise update strategy scales effectively to continuous, high-dimensional latent spaces without getting trapped in local minima.
- What evidence would resolve it: Successful application performing targeted semantic edits by querying a black-box diffusion model while maintaining sparsity and imperceptibility guarantees.

## Limitations
- Query complexity scales linearly with image resolution, potentially making the method impractical for high-resolution inputs within reasonable budgets
- The method assumes strong gradient alignment between surrogate and target models, which may degrade when attacking heterogeneous architectures
- Binary perturbation assumption (±ε) may not capture optimal magnitudes for all target models, potentially limiting attack strength in some scenarios

## Confidence
- **High confidence**: Attack success rates and convergence properties under query access to score-based feedback
- **Medium confidence**: Visual quality metrics (SSIM/LPIPS) and their attribution to surrogate selection
- **Low confidence**: Performance guarantees under hard-label-only feedback scenarios and extremely high-resolution inputs

## Next Checks
1. **Hard-label ablation**: Modify the feedback mechanism to work with top-1 labels only and measure ASR degradation to quantify dependency on score access
2. **Architecture mismatch test**: Attack a Graph Neural Network target using the CNN surrogate to measure performance collapse when gradient alignment assumption fails
3. **Sub-pixel perturbation validation**: Implement continuous ε optimization for selected pixels to verify whether binary restriction limits attack strength compared to gradient-based methods