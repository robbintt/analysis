---
ver: rpa2
title: A Unified Noise-Curvature View of Loss of Trainability
arxiv_id: '2509.19698'
source_url: https://arxiv.org/abs/2509.19698
tags:
- step-size
- learning
- accuracy
- arxiv
- curvature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified framework for diagnosing and mitigating
  loss of trainability (LoT) in continual learning. The authors identify two distinct
  failure modes: gradient-noise dominated updates (where noise overwhelms the signal)
  and curvature-noise dominated updates (where volatile sharpness causes instability).'
---

# A Unified Noise-Curvature View of Loss of Trainability

## Quick Facts
- arXiv ID: 2509.19698
- Source URL: https://arxiv.org/abs/2509.19698
- Authors: Gunbir Singh Baveja; Alex Lewandowski; Mark Schmidt
- Reference count: 40
- One-line primary result: Unified noise-curvature scheduler mitigates loss of trainability (LoT) in continual learning by adaptively adjusting per-layer learning rates based on gradient noise and curvature volatility bounds.

## Executive Summary
This paper proposes a unified framework for diagnosing and mitigating loss of trainability (LoT) in continual learning. The authors identify two distinct failure modes: gradient-noise dominated updates (where noise overwhelms the signal) and curvature-noise dominated updates (where volatile sharpness causes instability). They introduce a per-layer adaptive noise threshold combining a batch-size-aware gradient-noise bound and a curvature volatility-controlled bound. This threshold guides a simple per-layer step-size scheduler that adjusts learning rates to keep updates within safe bounds. Experiments show that this scheduler significantly improves accuracy over baselines like CReLU, L2 weight decay, and Wasserstein regularization, and produces adaptive step-size trajectories that mirror manual decay schedules without tuning. The approach effectively preserves trainability across non-stationary task sequences.

## Method Summary
The method introduces a per-layer adaptive scheduler that combines gradient-noise and curvature-volatility bounds to predict and prevent loss of trainability (LoT). It estimates per-sample gradient variance and top Hessian eigenvalue via power iteration, tracks normalized sharpness volatility with EMA, and computes a unified noise proxy that inflates effective noise when curvature is fragile. The scheduler adjusts learning rates every K steps: cooling if effective step-size exceeds the safe bound, warming early if too conservative. The approach is tested on continual learning with random-label MNIST, comparing against L2 regularization, Wasserstein regularization, and CReLU baselines.

## Key Results
- Unified noise-curvature scheduler significantly outperforms L2 weight decay, Wasserstein regularization, and CReLU baselines in accuracy retention across non-stationary tasks
- The combined bound most accurately anticipates accuracy drops compared to individual gradient-noise or curvature-volatility metrics
- Per-layer controller adapts learning rates to match manual decay schedules without explicit tuning, with Layer 1 typically violating bounds first
- The method successfully mitigates LoT by keeping effective step-sizes within safe bounds derived from both noise and curvature volatility

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If the effective step-size exceeds a batch-sizeâ€“aware gradient-noise bound, updates become noise-dominated and progress stalls.
- **Mechanism:** Under L-smooth loss, expected descent requires $\alpha_t < \frac{2}{L} \cdot \frac{B\|\mathbf{g}_t\|^2}{\sigma^2_{t,ps}}$. When this inequality is violated, stochastic gradient variance overwhelms the true gradient signal. Adam's preconditioning can mask this by converting weight-norm growth into effective step-size drift.
- **Core assumption:** The loss is locally L-smooth along the update direction; per-sample variance proxy approximates true gradient noise.
- **Evidence anchors:** Section 4 derivation from expected descent criterion; Appendix B.2 full derivation; corpus papers on step-size conditions reinforce bounds as stabilization mechanisms.
- **Break condition:** If per-sample variance estimates are unstable or batch size varies dramatically without recalibration, the bound may be mis-specified.

### Mechanism 2
- **Claim:** If sharpness volatility (temporal variance of normalized curvature) is high, the step-size must scale inversely with this volatility to avoid instability.
- **Mechanism:** The scheduler tracks $\text{Vol}_{\bar{\lambda}}(t) = \sigma^2_t / \mu_t$ via EMA of normalized sharpness $\bar{\lambda}_t$. High volatility indicates curvature that flips between descent and instability across mini-batches. The bound $\alpha^\star_{\text{vol}}(t) = (\kappa \cdot \text{Vol}_{\bar{\lambda}}(t))^{-1}$ tightens when curvature becomes fragile.
- **Core assumption:** The top Hessian eigenvalue estimated via power iteration approximates the Adam-preconditioned curvature well enough for threshold detection.
- **Evidence anchors:** Section 4 states high volatility destabilizes large steps; Figure 2 shows curvature volatility rise precedes accuracy collapse under L2 regularization; corpus links Hessian spectral changes to plasticity loss.
- **Break condition:** If power-iteration budget is too low or curvature changes faster than the EMA tracking window, volatility estimates may lag or mislead.

### Mechanism 3
- **Claim:** Combining gradient-noise and curvature-volatility bounds into a per-layer adaptive threshold predicts LoT more reliably than either signal alone.
- **Mechanism:** The unified noise proxy inflates effective noise when curvature is fragile: $\tilde{\sigma}^2_t = \sigma^2_{t,ps} + \beta\|\hat{\mathbf{g}}_t\|^2 \text{Vol}_{\bar{\lambda}}(t)$. This yields $\tilde{\alpha}^\star_t = B\|\hat{\mathbf{g}}_t\|^2 / \tilde{\sigma}^2_t$. Per-layer computation captures heterogeneity: Layer 1 typically violates bounds first, while deeper layers may remain stable.
- **Core assumption:** Layerwise effective step-sizes can be independently controlled without destabilizing inter-layer dynamics.
- **Evidence anchors:** Figure 4 shows combined bound most accurately anticipates accuracy drops; Figure 10 shows per-layer controller outperforms full-network controller; corpus lacks comparable unified noise-curvature frameworks.
- **Break condition:** If layers are strongly coupled (e.g., through skip connections or normalization), per-layer adjustments may induce oscillations.

## Foundational Learning

- **Concept: Effective step-size in Adam**
  - Why needed here: The paper analyzes $\alpha_t$ (the product of base learning rate and Adam's adaptive preconditioner), not just the nominal LR. Weight-norm growth increases $\alpha_t$ even with fixed LR.
  - Quick check question: Can you explain why Adam's effective step-size can drift upward during training even when the base learning rate is constant?

- **Concept: Hessian eigenvalue estimation via power iteration**
  - Why needed here: The scheduler estimates normalized sharpness $\bar{\lambda}_t$ at every step using a single Hessian-vector product per power-iteration step.
  - Quick check question: How many Hessian-vector products are required per step if power iteration runs for 100 steps, and what is the approximate computational overhead?

- **Concept: Signal-to-noise ratio in stochastic optimization**
  - Why needed here: The gradient-noise bound derives from requiring descent to dominate noise: $\|\mathbf{g}_t\|^2$ must exceed $\sigma^2_{t,ps}/B$ scaled by step-size.
  - Quick check question: If batch size doubles while keeping per-sample variance constant, how should the noise-critical step-size $\alpha^\star_g$ change?

## Architecture Onboarding

- **Component map:** Gradient variance estimator -> Sharpness estimator -> EMA tracker -> Threshold computer -> LR adjuster
- **Critical path:**
  1. At step $t$, compute $\bar{\lambda}^{(\ell)}_t$ and $\sigma^{2,(\ell)}_{t,mb}$ for each layer $\ell$
  2. Update EMA state $(\mu_t, \sigma^2_t)$
  3. Compute $\tilde{\alpha}^{\star,(\ell)}_t$ and safe bound $(1-\epsilon)\tilde{\alpha}^{\star,(\ell)}_t$
  4. Compare effective step-size $\alpha^{(\ell)}_t$ to safe bound
  5. If violation: cool $\eta^{(\ell)}$ by factor $c$; if conservatively below: warm by factor $u$ early in training

- **Design tradeoffs:**
  - Power-iteration budget: More steps improve eigenvalue accuracy but increase overhead (paper uses ~100 steps, claimed negligible)
  - Controlling interval $K$: Smaller $K$ increases responsiveness but adds computation; paper uses $K=40$
  - Per-layer vs. global: Per-layer captures heterogeneity but requires more bookkeeping and HVPs per layer

- **Failure signatures:**
  - Scheduler over-cools: Learning rate collapses to near-zero, accuracy stagnates (may indicate volatility estimate is too noisy)
  - Scheduler never cools: Accuracy drops despite scheduler (may indicate threshold computation error or EMA window misconfiguration)
  - Oscillating LR: Layerwise adjustments conflict (may indicate strong inter-layer coupling)

- **First 3 experiments:**
  1. **Sanity check:** Run vanilla Adam vs. scheduler on a single non-stationary task with random-label MNIST; verify scheduler produces decaying LR trajectory similar to Figure 5.
  2. **Ablation:** Disable each signal (grad-only, curvature-only, combined) on L2-regularized network; compare to Figure 7 to validate that combined bound is necessary.
  3. **Layer heterogeneity:** Log per-layer effective step-sizes and thresholds across tasks; confirm Layer 1 violations drive global signal as in Figure 9.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the unified noise-curvature view and scheduler generalize to deeper architectures and complex datasets?
- **Basis in paper:** [inferred] The experimental validation is restricted to a "two-layer MLP" on "random-label MNIST" (Appendix D), leaving high-dimensional realistic domains untested.
- **Why unresolved:** Curvature dynamics in modern architectures (e.g., Transformers) with billions of parameters may differ significantly from shallow MLPs.
- **What evidence would resolve it:** Successful application of the scheduler to ResNets or ViTs on standard continual learning benchmarks like CIFAR-100 or ImageNet.

### Open Question 2
- **Question:** Is the proposed scheduler robust to the choice of its internal control hyperparameters?
- **Basis in paper:** [inferred] The algorithm relies on fixed constants like safety factor $\gamma=0.8$ and volatility inflation $\beta=1$ (Table 1, Algorithm 1).
- **Why unresolved:** While the method avoids tuning the base learning rate decay, it is unclear if these controller constants require adjustment for different noise scales.
- **What evidence would resolve it:** A sensitivity analysis showing accuracy stability across a wide range of $\gamma$ and $\beta$ values.

### Open Question 3
- **Question:** Does the single Hessian-vector product approximation suffice to capture curvature volatility during rapid non-stationarity?
- **Basis in paper:** [inferred] The method estimates sharpness via power iteration with "k=1" and uses a scalar surrogate $\bar{\lambda}$ (Appendix C.1, B.3).
- **Why unresolved:** A single iteration may lag behind the true top eigenvalue dynamics during sudden distribution shifts, potentially providing false stability signals.
- **What evidence would resolve it:** Comparison of the k=1 proxy against full spectral estimation during the onset of loss of trainability events.

## Limitations
- Unified noise-curvature bound lacks external validation and no direct ablation shows it outperforms baselines in isolation
- Power-iteration estimates of Hessian curvature are sensitive to step count and early-stopping criteria, with only "~100 steps" specified
- Layerwise heterogeneity is claimed but inter-layer coupling effects are not explored; skip connections or batch-norm could invalidate independent per-layer controllers

## Confidence

- **High confidence:** Gradient-noise bound (Mechanism 1) - derived from standard expected descent analysis with explicit smoothness assumption; well-aligned with corpus theory on step-size conditions.
- **Medium confidence:** Curvature-volatility bound (Mechanism 2) - based on EMA of normalized sharpness; concept is plausible but external validation of volatility-as-threshold is weak.
- **Low confidence:** Unified bound and per-layer scheduler (Mechanism 3) - novel combination with no independent verification; layer independence assumption unverified under strong coupling.

## Next Checks

1. **Power-iteration stability:** Sweep power-iteration steps (10, 50, 100) and record curvature volatility; verify scheduler behavior is stable across budgets and that curvature estimates converge.
2. **Layer coupling test:** Add batch-norm or residual connections to the MLP; run per-layer scheduler and check if layerwise adjustments cause oscillations or collapse accuracy.
3. **Bound isolation ablation:** Disable each signal (gradient-only, curvature-only, combined) on L2 baseline; confirm combined bound outperforms both in LoT mitigation and accuracy retention.