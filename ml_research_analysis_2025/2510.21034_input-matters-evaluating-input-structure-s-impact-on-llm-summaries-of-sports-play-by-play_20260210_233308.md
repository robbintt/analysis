---
ver: rpa2
title: 'Input Matters: Evaluating Input Structure''s Impact on LLM Summaries of Sports
  Play-by-Play'
arxiv_id: '2510.21034'
source_url: https://arxiv.org/abs/2510.21034
tags:
- input
- errors
- error
- structure
- json
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates how input structure impacts factual accuracy\
  \ in LLM-generated NBA play-by-play summaries. Three input formats\u2014unstructured\
  \ text, row-structured tables, and hierarchical JSON\u2014were evaluated across\
  \ 180 game summaries generated by Llama-3.1-70B and Qwen2.5-72B."
---

# Input Matters: Evaluating Input Structure's Impact on LLM Summaries of Sports Play-by-Play

## Quick Facts
- arXiv ID: 2510.21034
- Source URL: https://arxiv.org/abs/2510.21034
- Authors: Barkavi Sundararajan; Somayajulu Sripada; Ehud Reiter
- Reference count: 22
- One-line primary result: Hierarchical JSON input reduces factual errors by up to 69% compared to unstructured text in LLM-generated sports summaries

## Executive Summary
This study investigates how input structure impacts factual accuracy in LLM-generated NBA play-by-play summaries. Three input formats—unstructured text, row-structured tables, and hierarchical JSON—were evaluated across 180 game summaries generated by Llama-3.1-70B and Qwen2.5-72B. JSON input yielded the lowest error rates (2.17 per 100 words for Llama, 2.14 for Qwen), reducing errors by up to 69% and 65% respectively compared to unstructured input. A two-way repeated-measures ANOVA confirmed that input structure accounted for over 80% of variance in error rates, with Tukey HSD tests showing statistically significant differences across all formats.

## Method Summary
The study used 30 NBA games from Basketball Reference, stratified by final margin (≤3, 4-9, 10-19, ≥20 points). Raw play-by-play logs were preprocessed into three formats: unstructured text (~13k tokens), row-structured tables (13 columns, ~28k tokens), and hierarchical JSON (~70-80k tokens). Two models (Llama-3.1-70B and Qwen2.5-72B) generated zero-shot summaries with identical decoding settings (temperature=0, top-p=0.95, top-k=50). Manual annotation across 7 error categories (Number, Name, Word-objective, Word-subjective, Context, Not Checkable, Other) produced 180 summaries total (30 games × 2 models × 3 formats), with target length ~450 words per summary.

## Key Results
- JSON input reduced error rates by 69% for Llama and 65% for Qwen compared to unstructured input
- Input structure accounted for over 80% of variance in error rates (confirmed by two-way repeated-measures ANOVA)
- Tukey HSD tests showed statistically significant differences across all three input formats
- JSON input most effectively reduced Number and Word-objective errors, while unstructured input produced highest rates of all error categories

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical JSON reduces errors through explicit entity-attribute bindings
JSON nests each play event as key-value pairs where time, team, player, action type, and score are separate atomic fields. This eliminates ambiguous parsing—the model retrieves "LeBron James" from `primary_player` and "Jump Shot" from `action`, rather than extracting both from a single free-text span. LLMs more reliably copy from explicit schema fields than infer relationships from unstructured prose.

### Mechanism 2: Row-structured tables provide intermediate reduction but lack hierarchical context
Tables separate entities into columns but flatten relationships. JSON's nesting keeps each play's context local—score deltas sit alongside the triggering action. Local context binding reduces cross-row reasoning errors, though tables still outperform unstructured input by providing column-level typing.

### Mechanism 3: Unstructured input amplifies errors due to implicit computation demands
Free text forces models to parse, maintain running totals, and output derived facts. Errors compound: misread action → wrong score → incorrect game state description. LLMs lack reliable implicit arithmetic over long sequences without external grounding, making unstructured input particularly problematic for numerical reasoning.

## Foundational Learning

- **Concept: Hierarchical vs. Flat Data Representation** - Understanding why JSON outperforms tables requires grasping how locality affects LLM attention and retrieval. Quick check: Does nesting score under the play object reduce cross-referencing errors compared to storing score in a separate column?

- **Concept: Error Taxonomy for Factual Evaluation** - The paper splits errors into Number, Name, Word-objective, Word-subjective, Context categories, each with different root causes. Quick check: If a summary says "LeBron hit a pivotal three-pointer" but the play-by-play shows a layup, what error type is this?

- **Concept: Within-Subjects Experimental Design** - Each game was evaluated under all 6 conditions (2 models × 3 formats), controlling for game-level variance. Quick check: Why does a two-way repeated-measures ANOVA allow us to conclude that input structure—not game complexity—explains >80% of variance?

## Architecture Onboarding

### Component map
Raw Play-by-Play Logs (Basketball Reference) → Preprocessing (regex, normalization, field decomposition) → Three Output Formats: Unstructured (~13K tokens), Row-structured (28K tokens), Hierarchical JSON (70-80K tokens) → LLM Generation (Llama-3.1-70B/Qwen2.5-72B) → Manual Annotation (7 error categories) → Statistical Analysis (two-way RM-ANOVA, Tukey HSD)

### Critical path
Preprocessing → Format conversion → Prompt engineering → Generation. The preprocessing uses regex and task-specific rules to decompose play descriptions into atomic fields before format branching. Errors in this step propagate downstream regardless of structure.

### Design tradeoffs
JSON: Highest accuracy (2.14-2.17 errors/100 words) but highest token cost (70-80K), requires long-context models. Row: Moderate accuracy (2.92-3.20 errors/100 words), moderate tokens (28K), simpler schema. Unstructured: Lowest accuracy (6.04-7.05 errors/100 words), lowest tokens (13K), use only when token budget dominates accuracy.

### Failure signatures
Number errors spiking → model attempting implicit arithmetic. Name errors with initials → model disambiguating to wrong full name. Context errors (hallucinated players) → model filling gaps from training data, not input.

### First 3 experiments
1. Reproduce JSON vs. Unstructured delta on 5 games: Generate summaries with Llama-3.1-70B, manually annotate errors, expect ~4-5 errors/100 words reduction with JSON.
2. Ablate preprocessing quality: Introduce controlled noise (swap 10% of player initials, mis-type 5% of scores) into JSON input, measure error rate increase.
3. Test token-efficiency hypothesis: Compress JSON by removing redundant keys, if error rates hold, identify minimal viable schema for token-constrained deployments.

## Open Questions the Paper Calls Out

### Open Question 1
Does the dominance of input structure in reducing factual errors generalize to other sports domains with different data characteristics, such as ice hockey? The paper states authors will "extend this work to ice hockey play-by-play data to identify general data-structuring features that minimise factual inaccuracies across multiple sports datasets." It's unclear if variance reduction from NBA (stop-and-start sport) applies to sports with continuous play dynamics.

### Open Question 2
Can an automated LLM-as-Judge framework reliably replicate the granular manual annotation required to detect specific factual errors in sports summaries? The paper proposes "develop[ing] an LLM-as-Judge framework to complement costly manual evaluation by automatically assessing atomic factual claims." Manual annotation is slow and unscalable, but automated methods for verifying temporal and relational facts are not yet validated.

### Open Question 3
Do the benefits of hierarchical JSON input persist across closed-source LLMs (e.g., GPT-4, Claude) or significantly smaller models? The Limitations note that "we evaluated only two models (Llama and Qwen), which limits the generalizability of our findings to other large language models." Different architectures may handle unstructured vs. structured tokens differently, potentially altering the 80% variance attribution.

## Limitations

- Manual error annotation introduces subjectivity despite high overall inter-annotator agreement (86.7%)
- Preprocessing pipeline details remain underspecified—exact regex patterns and task-specific rules are not provided
- Results demonstrate strong effects for NBA play-by-play summarization but generalizability to other domains remains untested

## Confidence

- **High confidence**: Input structure's impact on error rates (statistical significance confirmed, >80% variance explained)
- **Medium confidence**: Mechanism explanations (JSON's superiority explained through entity-attribute binding theory, but lacks direct causal evidence)
- **Medium confidence**: Error taxonomy reliability (high overall agreement but low agreement on Word-subjective category)
- **Low confidence**: Cross-domain generalizability (basketball-specific results may not transfer to other domains)

## Next Checks

1. **Replication with standardized preprocessing**: Implement the preprocessing pipeline with documented regex patterns and task-specific rules, then reproduce error rate distributions across the three input formats on the same 30 games.

2. **Cross-domain validation**: Apply the same experimental design to a different domain (e.g., soccer match reports or financial transaction logs) to test whether hierarchical JSON consistently outperforms other formats across domains.

3. **Automated error detection comparison**: Develop an automated factual consistency checker and compare its error classifications against manual annotations on a subset of summaries to assess annotation reliability and identify systematic biases.