---
ver: rpa2
title: 'Structure Is Not Enough: Leveraging Behavior for Neural Network Weight Reconstruction'
arxiv_id: '2503.17138'
source_url: https://arxiv.org/abs/2503.17138
tags:
- loss
- behavioral
- performance
- weights
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of reconstructing high-performing
  neural network models from their weight representations, a critical capability for
  applications in weight generation and model analysis. While existing autoencoder-based
  approaches achieve low reconstruction error using structural losses, they fail to
  preserve model performance due to limitations in capturing functional behavior.
---

# Structure Is Not Enough: Leveraging Behavior for Neural Network Weight Reconstruction

## Quick Facts
- **arXiv ID**: 2503.17138
- **Source URL**: https://arxiv.org/abs/2503.17138
- **Reference count**: 38
- **Primary result**: Combining structural and behavioral losses in weight-space autoencoders significantly improves reconstruction of functionally equivalent neural networks

## Executive Summary
This paper addresses the fundamental challenge that low weight reconstruction error does not guarantee functional equivalence in neural networks. While existing autoencoder approaches achieve good structural fidelity using MSE-based losses, they fail to preserve model performance on downstream tasks. The authors introduce a behavioral loss that measures output agreement between original and reconstructed models on shared query inputs, which complements traditional structural losses. Their theoretical analysis shows behavioral loss captures gradients weighted by model sensitivity, providing complementary information to structural alignment. Experiments on three model zoos demonstrate that the combined approach achieves model performance close to originals (within 0.6-1.0% accuracy) while structural-only approaches degrade by 16-20%.

## Method Summary
The method trains a weight-space autoencoder using a composite loss: L = 0.05L_C + 0.95(0.1L_S + 0.9L_B), where L_S is MSE between original and reconstructed weights, L_B is MSE between model outputs on query inputs, and L_C is a contrastive loss using behaviorally equivalent weight permutations. The autoencoder uses a SANE Transformer architecture with 289 tokens per model and 64-dimensional latents. Queries for behavioral loss are sampled from the training distribution of the model zoo (n=256). The theoretical contribution shows that behavioral loss provides sensitivity-weighted gradients through a Jacobian-based term F_j, which structural loss lacks. The approach is validated on CNNs trained on SVHN, CIFAR-10, and EuroSAT.

## Key Results
- Combining structural and behavioral losses achieves test accuracy within 0.6-1.0% of original models versus 16-20% degradation for structural-only approaches
- Behavioral loss is critical: LB alone yields worst agreement, while LS⊕LB achieves ~87% agreement vs ~50% for LS only
- Query distribution matters: Random noise or OOD queries cause reconstructed models to perform at random chance
- The method scales to model zoos with diverse hyperparameters while maintaining high performance

## Why This Works (Mechanism)

### Mechanism 1
Structural loss (MSE) alone produces gradients that linearly align weights without regard to functional sensitivity. The structural gradient ∂L_S/∂w measures linear alignment between weight error Δθ and decoder Jacobian, ignoring how weight perturbations affect outputs—some dimensions are functionally critical while others are not.

### Mechanism 2
Behavioral loss provides complementary gradients weighted by model sensitivity through the Jacobian-based term F_j = (1/n)ΣJ_θ(x_i)ᵀJ_θ̂(x_i). This captures how changes in weights propagate to output changes, with the behavioral gradient re-weighting alignment by functional importance.

### Mechanism 3
Combining structural and behavioral losses synergizes because structural loss provides regularization while behavioral loss guides toward functionally preserved regions. With β=0.1, structural loss prevents behavioral optimization from drifting too far in weight space while behavioral loss ensures functional equivalence.

## Foundational Learning

- **Concept: Autoencoder reconstruction vs. functional preservation**
  - Why needed here: The core problem is that low MSE between weights does not guarantee functional equivalence. Understanding this gap is essential.
  - Quick check question: If you reconstruct weights with MSE=0.001, would you expect the model's predictions to match? (Answer: Not necessarily—local perturbations can destroy generalization.)

- **Concept: Jacobian-based sensitivity analysis**
  - Why needed here: The theoretical justification for behavioral loss relies on first-order Taylor expansion and the role of F_j in weighting gradients by functional sensitivity.
  - Quick check question: What does a large entry in J_θ(x) indicate about the corresponding weight? (Answer: That weight has high influence on the output for input x.)

- **Concept: Query set distribution matching**
  - Why needed here: Behavioral loss is computed over queries; their distribution determines which aspects of model behavior are preserved.
  - Quick check question: Why would random noise queries fail? (Answer: Model behavior on random inputs is ill-defined and not aligned with the training objective.)

## Architecture Onboarding

- **Component map**: Encoder -> Tokenize weights -> Latent representation (64-dim, 289 tokens) -> Decoder -> Reconstructed weights
- **Critical path**: 1) Sample models θ_j from zoo 2) Sample n=256 queries from training distribution 3) Encode → latent → decode → θ̂_j 4) Compute L_S, L_B, L_C 5) Backprop through all three with γ=0.05, β=0.1
- **Design tradeoffs**: β=0.1 favors behavioral loss; higher β improves structural distance but hurts agreement. More queries improve behavioral matching but increase compute (~2x overhead). MSE behavioral loss is more stable than CE or distillation.
- **Failure signatures**: Reconstructed models with near-random accuracy → query distribution mismatch or missing L_S. High structural error but reasonable accuracy → likely missing L_S or β too low. Training instability → try MSE instead of CE/distillation; reduce learning rate to 1e-5.
- **First 3 experiments**: 1) Train AE with L_C + L_S only, measure test accuracy to confirm baseline underperforms. 2) Train with L_B using (a) training data queries, (b) out-of-distribution natural images, (c) random noise; compare accuracy to validate distribution sensitivity. 3) Test β ∈ {0.0, 0.1, 0.3, 0.5} on validation set; expect β=0.1 to balance structural distance and model agreement.

## Open Questions the Paper Calls Out

### Open Question 1
How does the behavioral loss scale to neural network architectures with significantly larger parameter counts (e.g., millions or billions of parameters)? The authors explicitly list this as a limitation, stating exploration is "limited to smaller models" and they "defer implementation on larger NNs to future work."

### Open Question 2
Can a synthetic or "universal" query set be engineered to match the effectiveness of original training data when the training set is unavailable? Appendix D.3 shows OOD queries degrade performance, suggesting this "opens the door for engineering comprehensive query sets," but no solution is provided.

### Open Question 3
Does inclusion of behavioral loss inherently reduce diversity of generated models compared to source population? Table 5 shows generated models exhibit lower structural and behavioral diversity than originals despite high performance, suggesting potential convergence toward homogeneous solutions.

## Limitations

- Theoretical analysis relies on first-order Taylor approximations that assume small weight perturbations, without rigorous quantification of when these break down for larger reconstruction errors
- Critical dependence on behaviorally equivalent weight permutations for contrastive loss is not fully specified in method section, requiring external codebase reference
- Current experiments are restricted to small CNNs (10K-60K parameters), leaving scalability to modern large architectures unaddressed

## Confidence

- **High confidence**: Empirical demonstration that structural-only reconstruction fails to preserve model performance, and that combining structural and behavioral losses significantly improves both reconstruction accuracy and downstream generative tasks
- **Medium confidence**: Theoretical analysis of complementary gradients - mathematical derivation is sound under stated assumptions, but practical significance lacks direct experimental validation beyond observed performance improvements
- **Medium confidence**: Scalability claims to larger architectures - validated on models up to 10K parameters, but scaling to modern large language models involves different challenges not addressed

## Next Checks

1. **Perturbation Sensitivity Analysis**: Systematically vary weight reconstruction error magnitude and measure both model accuracy and validity of first-order Taylor approximation used in behavioral gradient analysis

2. **Ablation of Permutation Augmentations**: Reproduce main results with (a) behaviorally equivalent permutations, (b) random permutations, and (c) no permutations for contrastive loss to isolate contribution of specific augmentation strategy

3. **Query Distribution Stress Test**: Test behavioral loss with queries from (a) validation set, (b) test set, and (c) semantically similar but different datasets; measure how query distribution shift affects reconstructed model performance