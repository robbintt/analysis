---
ver: rpa2
title: 'Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What
  They Say'
arxiv_id: '2509.21164'
source_url: https://arxiv.org/abs/2509.21164
tags:
- experts
- expert
- interaction
- layers
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Mixture of Thoughts (MoT), a method for enabling
  fine-grained latent-level collaboration among heterogeneous, frozen LLMs under a
  global routing scheme. For each query, a lightweight router selects top-K experts
  and designates a primary expert; uniformly placed interaction layers project hidden
  states into a shared latent space where the primary expert performs cross-attention
  over its active peers.
---

# Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just What They Say

## Quick Facts
- **arXiv ID**: 2509.21164
- **Source URL**: https://arxiv.org/abs/2509.21164
- **Reference count**: 40
- **One-line primary result**: Outperforms routing and aggregation-based SOTA Avengers by +0.38% (in-distribution) and +2.92% (out-of-distribution) with single-pass inference and frozen experts.

## Executive Summary
Mixture of Thoughts (MoT) enables fine-grained latent-level collaboration among heterogeneous, frozen LLMs via a global router and cross-attention interaction layers. For each query, a lightweight router selects top-K experts and designates a primary expert; uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active peers. Only the router and interaction layers are trained with a novel joint training objective that improves both expert selection and inter-expert collaboration. MoT surpasses the current routing and aggregation-based state-of-the-art, Avengers, by +0.38% and +2.92% on in-distribution and out-of-distribution benchmarks respectively, while significantly outperforming the best-performing single model.

## Method Summary
MoT routes each query to a sparse set of frozen expert models and enables the primary expert to attend to its active peers' hidden states via cross-attention in a shared latent space. The architecture uses uniformly placed interaction layers at stack boundaries where ForwardProjectors map expert hidden states to a shared dimension, the primary expert cross-attends over concatenated keys/values from all active experts, and ReverseProjectors map the result back residually. The router (DeBERTaV3-base + MLP) selects Top-K experts and designates the highest-scoring as primary. Only router, projectors, and shared cross-attention weights are trained; expert backbones remain frozen. Training uses a joint loss combining autoregressive NLL with entropy, load-balancing, and routing-consistency regularizers optimized via AdamW with Gumbel-Softmax straight-through estimation.

## Key Results
- Outperforms Avengers by +0.38% ID and +2.92% OOD accuracy
- Maintains single-pass inference with runtime comparable to routing baselines
- Robust to single-expert removal with only -0.57% average ID accuracy drop
- Surpasses best-performing single model on all evaluated benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Projecting expert hidden states into a shared latent space enables the primary expert to integrate fine-grained representations from heterogeneous peers during generation.
- Mechanism: At each of Q uniformly spaced "stack" boundaries, active experts apply a learned linear ForwardProjector (dim → d_shared); the primary expert forms queries from its projected states and cross-attends over concatenated keys/values from all active experts; a ReverseProjector maps the output back to the primary's hidden dimension and adds it residually.
- Core assumption: Heterogeneous experts encode complementary task-relevant information in their intermediate representations that can be aligned via learned projections.
- Evidence anchors:
  - [abstract] "uniformly placed interaction layers project hidden states into a shared latent space where the primary expert performs cross-attention over its active peers"
  - [section 3.3] Describes projectors and cross-attention update; ablation (Table 3, "MoT w/o CrossAttn") shows degraded ID/OOD accuracy versus full MoT
  - [corpus] Related MoE works (e.g., Unveiling Hidden Collaboration within MoE in LLMs) emphasize interpretability and expert collaboration, but do not provide direct evidence for MoT's cross-expert latent alignment
- Break condition: If projected representations become misaligned across experts (e.g., under severe distribution shift or insufficient projector capacity), cross-attention may attend to uninformative or conflicting signals, reducing gains or causing instability.

### Mechanism 2
- Claim: Sparse top-K routing with a designated primary expert concentrates collaborative compute while preserving single-pass inference efficiency.
- Mechanism: A lightweight MLP router maps a prompt embedding to M expert scores; Top-K selects active experts; the highest-scoring expert is primary and generates output tokens; non-primary active experts continue decoding solely to maintain hidden-state trajectories for subsequent interaction layers.
- Core assumption: The router can generalize to identify complementary expert subsets per query such that peer hidden states aid the primary expert more than running the primary alone.
- Evidence anchors:
  - [abstract] "lightweight router selects top-K experts and designates a primary expert"
  - [section 3.2–3.5] Routing, primary designation, and single-pass inference; Table 5 shows robustness to expert drop (avg −0.57 ID drop per single-expert removal)
  - [corpus] Neighbor papers on routing (e.g., Routing by Analogy, Collaboration-Constrained Routing) discuss router brittleness and load balancing; they do not evaluate MoT's specific primary-expert cross-attention scheme
- Break condition: If the router collapses to a small subset of experts or misrouts on OOD queries, primary selection may fail to invoke the most capable expert, and collaboration may add noise rather than signal.

### Mechanism 3
- Claim: Joint optimization of router and interaction layers with auxiliary regularizers stabilizes training and improves expert utilization.
- Mechanism: The loss combines autoregressive LM loss (primary expert) with entropy regularization on router scores, load-balancing (variance of activation frequencies), and a routing-consistency term that penalizes KL divergence between outputs under two perturbed Top-K selections; gradients flow via a straight-through (Gumbel-Softmax) estimator.
- Core assumption: Discrete Top-K routing can be trained with differentiable relaxations without severely mismatching inference behavior.
- Evidence anchors:
  - [abstract] "only the router and lightweight interaction layers are trained with a novel joint training objective that improves both expert selection and inter-expert collaboration"
  - [section 3.4; Appendix B] Defines L_total = L_LM + λ_ent L_ent + λ_bal L_bal + λ_con L_con; Table 4 shows cumulative +7.02% ID gain over LM-only
  - [corpus] Prior MoE works (e.g., Switch Transformers, StableMoE) similarly employ load balancing and entropy regularization but do not validate MoT's specific consistency loss
- Break condition: If regularization weights are misconfigured (e.g., λ_bal too high), the router may over-disperse selections, forcing suboptimal experts into active sets and degrading task performance.

## Foundational Learning

### Concept: Cross-attention and residual updates
- Why needed here: MoT's primary expert integrates peer representations via cross-attention followed by a residual projection at each interaction layer.
- Quick check question: Can you write the forward pass for single-head cross-attention with queries from the primary and keys/values from active peers, including a causal mask?

### Concept: Straight-through (Gumbel-Top-K) gradient estimation
- Why needed here: The router uses discrete Top-K at inference; training requires gradients through selection, implemented via soft relaxations with hard indices in the forward pass.
- Quick check question: Explain how Gumbel noise and temperature τ enable backprop through Top-K while preserving discrete choices at inference.

### Concept: Load balancing and routing entropy in sparse MoE
- Why needed here: MoT stabilizes training by encouraging exploration (entropy) and roughly uniform expert usage (load balance).
- Quick check question: What would happen if all queries were routed to the same expert despite having a diverse pool, and which regularizer targets this failure?

## Architecture Onboarding

### Component map
PromptEncoder (frozen DeBERTaV3) → MLP router → Top-K expert indices + primary selection → Per-expert stacks (partitioned into Q stacks) → Interaction layer per stack: ForwardProjector_m → shared space → primary cross-attention over concat(K,V) from active experts → ReverseProjector_m → residual add → decode with primary output

### Critical path
Route prompt → select active set → designate primary → prefill all active experts → at each stack boundary, project h_m and run cross-attention for primary → decode with primary output; maintain non-primary trajectories for subsequent interactions

### Design tradeoffs
- More stacks (Q): richer collaboration but more parameters/latency; paper shows diminishing returns beyond Q=6
- Larger d_shared: higher-capacity alignment but quadratically more cross-attention parameters
- Top-K: more peers improve robustness (Fig. 6c) but increase runtime; robust to single-expert drop (Table 5)

### Failure signatures
- Router collapse to few experts (check activation frequencies)
- Misaligned projectors (cross-attention attends to noise; validate via ablation without cross-attention)
- OOD generalization drop if router overfits training queries

### First 3 experiments
1. Replicate the "MoT w/o CrossAttn" ablation to confirm gains derive from latent collaboration, not just extra parameters.
2. Sweep Q ∈ {2,4,6,8} on a validation subset to identify quality-cost Pareto point; expect saturation beyond Q=6 (per Fig. 6a).
3. Evaluate expert-drop robustness: remove one expert at inference and measure accuracy drop; compare to Table 5 to verify graceful degradation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MoT performance and efficiency scale when expanding the expert pool to include significantly larger parameter sizes (e.g., 70B+) or a wider diversity of architectures?
- Basis in paper: [explicit] Appendix K states future work will "systematically scale the expert pool and sizes" beyond the current modest pool of frozen 7B–8B experts.
- Why unresolved: Current experiments are limited to 7–8B models; latency/throughput characteristics and potential collaboration benefits may differ non-linearly with larger, more capable experts.
- What evidence would resolve it: Benchmarks on a pool mixing 7B, 13B, and 70B models, measuring accuracy scaling curves and wall-clock latency.

### Open Question 2
- Question: Can an adaptive or learned placement strategy for interaction layers outperform the uniform placement baseline, particularly for experts with varying internal representations?
- Basis in paper: [explicit] Appendix K notes the intent to "experiment with interaction layer placement," and Section 4.3 shows a fixed comparison of shallow/intermediate/deep/uniform.
- Why unresolved: While deep/uniform placement worked best here, the optimal point for latent collaboration might vary per layer or expert pair, which fixed strategies cannot capture.
- What evidence would resolve it: Experiments comparing fixed uniform MoT against a variant with learned/gated insertion points, analyzing performance relative to parameter count.

### Open Question 3
- Question: To what extent can hardware-aware optimizations, such as fused projector-attention kernels, reduce the inference overhead of the interaction layers?
- Basis in paper: [explicit] Appendix K proposes "co-design systems (e.g., fused projector/attention kernels) to map quality–cost Pareto fronts."
- Why unresolved: The current implementation uses standard components; the overhead of projecting and attending across experts in a shared space might be reducible via custom kernels.
- What evidence would resolve it: Inference latency measurements of MoT with custom CUDA kernels vs. standard PyTorch implementations, ideally resulting in wall-clock times closer to single-model inference.

## Limitations
- Cross-attention gains depend on projector capacity and may degrade under severe distribution shifts
- Routing-consistency regularizer contributions are weakly justified and may be sensitive to hyperparameter tuning
- Exact prompt formatting and few-shot templates for mixed evaluation tasks are unspecified, potentially affecting reproducibility

## Confidence
- **High**: MoT's architecture (frozen experts + router + interaction layers) is clearly specified and reproducible.
- **Medium**: The empirical gains over baselines (ID +0.38%, OOD +2.92%) are supported by ablation studies, but routing-consistency regularizer contributions are weakly justified.
- **Low**: Claims about generalization to truly out-of-distribution queries depend on router robustness that is not fully validated across diverse shifts.

## Next Checks
1. **Router stability sweep**: Vary λ_con and λ_ent to confirm gains persist without over-tuning; test on synthetic OOD splits.
2. **Projector capacity scaling**: Evaluate MoT with d_s ∈ {1024, 2048, 4096} to ensure cross-attention gains are not artifacts of under-capacity projectors.
3. **Prompt template ablation**: Test multiple prompt formatting variants on a subset of tasks to bound sensitivity to unspecified prompt details.