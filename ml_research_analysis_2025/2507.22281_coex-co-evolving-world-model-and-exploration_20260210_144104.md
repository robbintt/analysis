---
ver: rpa2
title: CoEx -- Co-evolving World-model and Exploration
arxiv_id: '2507.22281'
source_url: https://arxiv.org/abs/2507.22281
tags:
- subgoal
- world
- agent
- state
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CoEx addresses the challenge of exploiting and adapting to novel
  environments in LLM agents by introducing a hierarchical architecture that co-evolves
  planning and world modeling. Unlike monolithic agents that suffer from exploitation
  bias and limited adaptation, CoEx separates planning and execution into distinct
  Planner and Actor components, operating at the subgoal level rather than action
  level.
---

# CoEx -- Co-evolving World-model and Exploration

## Quick Facts
- arXiv ID: 2507.22281
- Source URL: https://arxiv.org/abs/2507.22281
- Authors: Minsoo Kim; Seung-won Hwang
- Reference count: 40
- Primary result: CoEx achieves 93.28% success rate on ALFWorld tasks and 15-19% gains in exploration efficiency compared to baselines

## Executive Summary
CoEx addresses the challenge of exploiting and adapting to novel environments in LLM agents by introducing a hierarchical architecture that co-evolves planning and world modeling. Unlike monolithic agents that suffer from exploitation bias and limited adaptation, CoEx separates planning and execution into distinct Planner and Actor components, operating at the subgoal level rather than action level. The system maintains an explicit neurosymbolic belief state combining fast, code-based symbolic memory for low-level state tracking with LLM-generated structured textual memory for higher-level understanding and knowledge synthesis. Through systematic verification and synthesis after each subgoal attempt, CoEx continuously updates its world model based on real experiences.

## Method Summary
CoEx implements a hierarchical LLM agent architecture with three core components: a Planner that generates subgoals using state machine reasoning over a belief state, an Actor that executes subgoals through a reason-and-act loop with skill library, and an Adaptive Belief State that maintains both symbolic memory for fast state tracking and structured textual memory for higher-level understanding. The main loop iterates: planner generates subgoal given belief state and history, actor executes subgoal updating symbolic memory per action, verification-synthesis module evaluates trajectory and updates textual memory, new belief state is created and added to planner history. The system uses GPT-4o-mini with few-shot prompting, operates at subgoal granularity (max 35 steps per subgoal, 100-150 total steps per task), and evaluates across ALFWorld household tasks, PDDL planning domains, and Jericho text adventure games.

## Key Results
- Achieves 93.28% success rate on ALFWorld tasks, significantly outperforming baselines
- Demonstrates 15-19% improvements in exploration efficiency across ALFWorld, PDDL, and Jericho domains
- Maintains computational efficiency with subgoal-level updates (Planner + belief update = ~30% of total tokens)
- Shows consistent performance gains across diverse domains including household tasks, planning puzzles, and text adventures

## Why This Works (Mechanism)

### Mechanism 1: Subgoal-Level Planning Overcomes Exploitation Bias
- Claim: Decoupling planning from action-level execution reduces exploitation bias inherent in action-level in-context learning.
- Mechanism: The Planner operates at subgoal granularity rather than action granularity, treating exploration as a first-class subgoal type. This allows the agent to strategically issue exploratory subgoals without being constrained by action-level demonstrations that bias toward known successful trajectories.
- Core assumption: Action-level ICL demonstrations encode exploitation patterns that persist even when exploration would be optimal; subgoal abstraction bypasses this by operating at a higher policy level.
- Evidence anchors: [abstract] "CoEx plans and interacts with the world by using LLM reasoning to orchestrate dynamic plans consisting of subgoals"; [Section 3.1] "overcome the exploitation bias of action-level ICL, by instantiating exploratory behavior at the level of subgoals"; [corpus] Related work on exploration-exploitation in ICL (Dai et al., 2024) confirms ICL-driven exploitation bias.

### Mechanism 2: Neurosymbolic Belief State Enables Targeted World Model Updates
- Claim: Explicit neurosymbolic belief state allows controlled, verifiable updates to world model; monolithic implicit representations cannot support this.
- Mechanism: Belief state b_k = (m_k, l_k) combines symbolic memory m_k—code-based, object-oriented representation for fast state tracking, updated per-timestep; structured textual memory l_k—LLM-generated inferences for higher-level understanding, updated per-subgoal via verification-synthesis. This separation allows low-level state to be updated deterministically while high-level inferences undergo verification before integration.
- Core assumption: Low-level state tracking (object locations, inventory) is amenable to symbolic representation and does not require LLM inference; high-level task-relevant insights require LLM reasoning but benefit from explicit storage.
- Evidence anchors: [abstract] "neurosymbolic belief state combining fast, code-based symbolic memory for low-level state tracking with LLM-generated structured textual memory for higher-level understanding"; [Section 4.4] "Implementing symbolic memory in code allows for fast, efficient state tracking"; [corpus] DR. WELL similarly uses symbolic world models for embodied agents.

### Mechanism 3: Verification-Synthesis Loop Filters Noise Before Belief Integration
- Claim: Two-stage verification and synthesis prevents spurious or hallucinated inferences from corrupting the world model.
- Mechanism: After each subgoal attempt: (1) Verification stage—LLM answers structured questions about trajectory consistency, errors, and discoveries; (2) Synthesis stage—LLM generates status line, justification, and learned facts based on verification output. Only verified, synthesized outputs update l_k.
- Core assumption: LLM-based verification can reliably distinguish genuine discoveries from hallucinations when given structured questions; the synthesis stage can compress task-relevant insights without losing critical information.
- Evidence anchors: [Section 4.4] "The structured textual memory is updated through a 2-stage verification and synthesis process"; [Algorithm 2, Ln. 18-21] BeliefUpdate procedure explicitly separates verification and synthesis.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs) and Belief States**
  - Why needed here: CoEx explicitly frames agent tasks as POMDPs where true state s_t is partially observable. Belief state b_t estimates s_t. Understanding this distinction is essential to grasp why monolithic agents fail (implicit world model cannot be updated) and why explicit belief states help.
  - Quick check question: Given observations [o_1, o_2, o_3], can you explain why b_3 ≠ just [o_1, o_2, o_3]? What additional information might b_3 encode?

- **Concept: In-Context Learning (ICL) and Exploitation Bias**
  - Why needed here: The paper's core critique is that action-level ICL demonstrations bias agents toward exploitation. You need to understand how ICL works (few-shot exemplars in prompt) and why action-level exemplars create myopic behavior.
  - Quick check question: If ICL exemplars show only successful pick-and-place actions, why would an agent fail to explore unvisited rooms?

- **Concept: Hierarchical Reinforcement Learning (Options, Subgoals)**
  - Why needed here: CoEx's Planner-Actor split mirrors the options framework in HRL. Subgoals are temporally extended actions executed by the Actor. Understanding abstraction levels helps explain why subgoal-level planning generalizes better.
  - Quick check question: What is the difference between a primitive action (e.g., "go north") and a subgoal (e.g., "navigate to living room") in terms of temporal extent and policy abstraction?

## Architecture Onboarding

- **Component map:**
  - Symbolic Memory (m_k) -> Structured Textual Memory (l_k) -> Belief State (b_k) -> Planner (π) -> Subgoal (e_k) -> Actor (α) -> Trajectory (ε_k) -> Verification-Synthesis Module (v) -> Updated Symbolic Memory (m_{k+1}) and Textual Memory (l_{k+1})

- **Critical path:**
  1. Initialize m_0 from initial observation (Alg 1, Ln 11)
  2. Planner generates subgoal e_k given b_k and H_k (Ln 15)
  3. Actor executes subgoal episode, updating m_t per action (Ln 16)
  4. Verification-synthesis generates l_{k+1} from trajectory and m_{k+1} (Ln 17)
  5. New belief state b_{k+1} = (m_{k+1}, l_{k+1}) added to planner history (Ln 18)
  6. Loop until task complete or max steps

- **Design tradeoffs:**
  - Subgoal-level vs. action-level updates: Subgoal granularity reduces computational cost (Planner + belief update = ~30% of total tokens per Alg 1 invocation; Actor = 70%) but may delay world model updates within long subgoal episodes.
  - Symbolic + textual vs. pure textual memory: Symbolic memory enables fast deterministic updates but requires domain-specific parsers (see Appendix A.2). Textual memory is flexible but subject to LLM inference errors.
  - Planner-Actor split vs. monolithic: Enables exploration and specialization but requires careful subgoal termination design (Actor must self-judge completion/failure).

- **Failure signatures:**
  - Actor loops: If Actor never triggers SUBGOAL COMPLETED or REQUEST_REPLAN within max_sub_steps (35), subgoal episode terminates without clear status.
  - Symbolic parser failures: If `_parse_and_update()` encounters unhandled observation format, m_k diverges from true state. Look for "predicates BEFORE update" != "predicates AFTER update" inconsistencies.
  - Belief state drift: If verification hallucinates (e.g., invents facts not in trajectory), l_k accumulates false beliefs. Check learned_facts against raw trajectory ε_k.
  - Planner myopia: If Planner generates only exploitation subgoals despite uncertainty, check if belief state encodes sufficient uncertainty signals.

- **First 3 experiments:**
  1. **Reproduce ALFWorld "picktwo" task with belief state inspection:** Run "put two soapbar in garbagecan" task. Log b_k after each subgoal. Verify that symbolic memory correctly tracks inventory and object locations, and that textual memory captures learned facts (e.g., "soapbar 1 at toilet 1"). Compare to Appendix A.4 example.
  2. **Ablate symbolic memory (use only textual memory):** Replace m_k with placeholder. Run ALFWorld tasks. Expect degradation in low-level state tracking (object confusion, location errors) but potentially preserved high-level reasoning. Measure success rate delta.
  3. **Stress test verification-synthesis with injected noise:** Manually inject false observations into trajectory before verification stage (e.g., "You see a golden key" when none exists). Check if verification catches inconsistency or if false fact propagates to l_k. This tests the mechanism's robustness to hallucinated inputs.

## Open Questions the Paper Calls Out

- **Question:** What specific memory pruning and abstraction mechanisms are required to scale CoEx to larger, real-life tasks?
  - Basis in paper: [explicit] The authors state in the Limitations section that "real-life tasks of sufficiently larger scales may require memory pruning and abstraction mechanisms."
  - Why unresolved: The current architecture accumulates data through subgoal updates without explicit mechanisms for forgetting or summarizing information over very long time horizons.
  - What evidence would resolve it: Experiments on tasks with significantly longer horizons demonstrating that specific memory management techniques maintain performance without context overflow.

- **Question:** How can the architecture be modified to handle abrupt or unpredictable environmental shifts?
  - Basis in paper: [explicit] The paper notes that "more abrupt or unpredictable environmental shifts may require a new strategy for faster adaptation."
  - Why unresolved: The current system updates the world model primarily after a subgoal attempt, introducing latency that may fail in highly volatile environments.
  - What evidence would resolve it: Benchmarking in non-stationary environments where world dynamics change unpredictably during the execution of a subgoal.

- **Question:** Would integrating sophisticated memory compression or enhanced real-time synchronization improve world model robustness?
  - Basis in paper: [explicit] The authors identify "more sophisticated memory compression or enhanced real-time world-model synchronization" as "promising future directions."
  - Why unresolved: The current reliance on a verification and synthesis loop is functional but may not be optimal for all forms of data retention or real-time accuracy.
  - What evidence would resolve it: Comparative studies showing improved state tracking accuracy or computational efficiency using alternative compression algorithms.

## Limitations

- **Verification-synthesis robustness**: The paper claims this mechanism prevents belief state corruption but lacks empirical validation of verification accuracy under noisy or hallucinated inputs.
- **Skill library composition**: The paper mentions decomposing task exemplars into smaller chunks but provides no methodology for this decomposition or annotation process.
- **Computational cost trade-offs**: While subgoal-level planning reduces token usage, the paper doesn't quantify the trade-off between computational efficiency and potential subgoal granularity limitations.

## Confidence

- **High Confidence**: Core architectural contribution (hierarchical Planner-Actor split with explicit belief state representation) is well-specified and empirically validated across three domains with consistent performance gains.
- **Medium Confidence**: Exploitation-bias reduction claim is theoretically sound but direct empirical validation of subgoal-level debiasing is limited.
- **Low Confidence**: Verification-synthesis efficacy is presented as critical but lacks systematic evaluation of accuracy or robustness to input noise.

## Next Checks

1. **Stress test verification-synthesis**: Inject controlled hallucinations into trajectories before verification stage and measure detection accuracy. Compare belief state drift with and without verification.

2. **Skill library ablation study**: Systematically vary skill library granularity and composition. Measure impact on success rate and subgoal termination quality across domains.

3. **Belief state uncertainty quantification**: Instrument symbolic memory to track confidence scores for object location predicates. Correlate confidence levels with subgoal success rates to validate whether uncertainty signals effectively guide exploration.