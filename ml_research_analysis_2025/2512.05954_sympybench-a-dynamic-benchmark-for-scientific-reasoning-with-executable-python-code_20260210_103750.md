---
ver: rpa2
title: 'SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python
  Code'
arxiv_id: '2512.05954'
source_url: https://arxiv.org/abs/2512.05954
tags:
- reasoning
- accuracy
- problem
- question
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SymPyBench, a dynamic benchmark for evaluating
  scientific reasoning in physics with 15,045 parameterized problems. Each problem
  includes step-by-step reasoning and executable Python code that produces ground-truth
  solutions for any parameter set.
---

# SymPyBench: A Dynamic Benchmark for Scientific Reasoning with Executable Python Code

## Quick Facts
- arXiv ID: 2512.05954
- Source URL: https://arxiv.org/abs/2512.05954
- Authors: Shima Imani; Seungwhan Moon; Adel Ahmadyan; Lu Zhang; Kirmani Ahmed; Babak Damavandi
- Reference count: 32
- Introduces a dynamic benchmark of 15,045 parameterized physics problems with executable Python code for evaluating scientific reasoning in LLMs.

## Executive Summary
SymPyBench is a dynamic benchmark designed to evaluate scientific reasoning in large language models using 15,045 parameterized physics problems. Each problem includes step-by-step reasoning and executable Python code that produces ground-truth solutions for any parameter set. The benchmark supports three formats—MC-Symbolic, MC-Numerical, and free-form questions—testing complementary reasoning skills from algebraic manipulation to numerical computation and solution generation.

The authors introduce three novel evaluation metrics beyond standard accuracy: Consistency Score, Failure Rate, and Confusion Rate. These metrics quantify model variability and uncertainty across semantically equivalent problem variants. Experiments with state-of-the-art instruction-tuned LLMs reveal significant performance differences, with top models achieving over 64% exact match accuracy but exhibiting varying robustness. Analysis shows that MC-Symbolic questions are easier than MC-Numerical due to computational challenges, while free-form questions are most difficult due to structural complexity and answer generation requirements.

## Method Summary
SymPyBench uses a six-stage pipeline: (1) OCR extraction and dependency filtering of open-source physics problems, (2) structured JSON representation via LLaMA-3.2-90B-Vision-Instruct, (3) template generation with symbolic placeholders, (4) Python code synthesis with SymPy/Pint validated against original values, (5) manual review, and (6) format generation. The benchmark achieves 88% code validation success rate and includes 71.52% free-form, 14.24% MC-Symbolic, and 14.24% MC-Numerical questions across physics domains.

## Key Results
- Top models like Anthropic Sonnet-3.7 and Llama4-Maverick achieve over 64% exact match accuracy.
- Consistency Score ranges from 5.66% to 42.42% across models, highlighting differences in generalization ability.
- MC-Symbolic questions are easier than MC-Numerical due to computational challenges, while free-form questions are most difficult.
- Sonnet-3.7 achieves highest Consistency Score (42.42%) and lowest Confusion Rate (6.06%), demonstrating superior robustness.

## Why This Works (Mechanism)

### Mechanism 1: Code-Grounded Parameterization Enables Robustness Testing
- Claim: Executable Python code with symbolic placeholders allows systematic probing of model behavior under controlled input perturbations.
- Mechanism: Each problem template contains input variables with symbolic placeholders. The pipeline samples numerical values (±20-50% perturbation), substitutes them into both the question text and the reference Python function, then executes the code to generate ground-truth answers. This creates infinite problem variants from a single template.
- Core assumption: Models that understand underlying physics principles should maintain consistent accuracy across parameter variations; inconsistency indicates brittle pattern-matching rather than genuine reasoning.
- Evidence anchors:
  - [abstract] "Each problem is fully parameterized, supporting an effectively infinite range of input configurations, and is accompanied by... executable Python code that produces the ground-truth solution for any parameter set."
  - [Section 3] "We execute each generated Python function by substituting the original numerical values... If the computed outputs match the expected Output Variables... we retain the code."
  - [corpus] PRiSM paper similarly uses Python-grounded evaluation for scientific reasoning, suggesting this code-based verification pattern is an emerging paradigm.
- Break condition: If generated Python code fails validation (~12% failure rate per Table 7), the problem is discarded rather than patched, limiting dataset completeness.

### Mechanism 2: Format Divergence Isolates Error Types
- Claim: Comparing performance across MC-Symbolic, MC-Numerical, and free-form formats disentangles conceptual errors from execution errors (arithmetic, unit conversion, formatting).
- Mechanism: MC-Symbolic tests algebraic reasoning without computation; MC-Numerical adds arithmetic demands; free-form requires solution generation without scaffolding. Conditional accuracy analysis measures: given failure on format X, what is accuracy on format Y for the same problem?
- Core assumption: If a model fails MC-Numerical but succeeds on MC-Symbolic for the same problem, the error is computational rather than conceptual.
- Evidence anchors:
  - [Section 4.1] "Maverick and Scout excel at MC-Symbolic questions (95.70% and 81.51%, respectively), while showing substantially lower performance on MC-Numerical questions."
  - [Appendix A.0.4, Table 6] Maverick achieves 95.00% conditional accuracy on MC-Symbolic given MC-Numerical failure, indicating "most MC-Numerical errors are due to computational issues... rather than misunderstanding."
  - [corpus] Weak direct corpus evidence for this specific format-decomposition mechanism; related work focuses on tool use rather than format-based diagnosis.
- Break condition: Multiple-choice formats may provide implicit scaffolding that masks partial understanding, so high conditional accuracy does not guarantee full conceptual mastery.

### Mechanism 3: Novel Metrics Capture Stability Beyond Accuracy
- Claim: Consistency Score, Failure Rate, and Confusion Rate reveal model reliability patterns that single-accuracy metrics obscure.
- Mechanism: Group problems by template and evaluate across variants. Consistency Score = fraction of groups where model answers all variants correctly. Confusion Rate = fraction of groups with ~50% accuracy (suggesting guessing). Complete Failure Rate = fraction of groups where all variants are wrong.
- Core assumption: True reasoning capability manifests as stable performance across semantically equivalent problem variants; high variability indicates unreliable internal representations.
- Evidence anchors:
  - [Section 4] "Sonnet-3.7 distinguishes itself with the highest Consistency Score (42.42%) and lowest Confusion Rate (6.06%), demonstrating superior robustness."
  - [Table 2] Shows Qwen2.5-72B has higher exact match (61.69%) than Llama-3.3-70B (54.17%) but also higher Complete Failure Rate (15.09% vs 7.55%), revealing brittleness masked by aggregate accuracy.
  - [corpus] Executable Counterfactuals paper addresses causal reasoning through code but does not propose consistency metrics across perturbations.
- Break condition: These metrics require multiple variants per problem; computational cost scales with variant count.

## Foundational Learning

- Concept: **Symbolic computation with SymPy**
  - Why needed here: The benchmark uses SymPy for algebraic manipulation, equation solving, and symbolic expression evaluation. Understanding how SymPy handles symbolic variables vs. numerical substitution is essential for extending or debugging the code generation pipeline.
  - Quick check question: Given `x = sp.Symbol('x')` and `expr = x**2 + 2*x + 1`, what does `expr.subs(x, 3).evalf()` return?

- Concept: **Unit consistency with Pint**
  - Why needed here: All generated Python code uses Pint to enforce dimensional consistency. Engineers must understand how Pint quantities propagate units through arithmetic operations and how to extract magnitudes for SymPy compatibility.
  - Quick check question: If `Q_(5, 'meter')` is divided by `Q_(2, 'second')`, what is the resulting unit?

- Concept: **Conditional probability in error analysis**
  - Why needed here: The conditional accuracy metric (P(correct on format Y | incorrect on format X)) is central to diagnosing error sources. Understanding conditional probabilities prevents misinterpreting correlation as causation in error patterns.
  - Quick check question: If a model fails 40 free-form problems but succeeds on 38 of those same problems in MC-Symbolic format, what is the conditional accuracy, and what does it suggest about the error source?

## Architecture Onboarding

- Component map: OCR extraction -> Structured representation -> Template generation -> Code synthesis -> Format generation -> Evaluation engine
- Critical path:
  1. Structured representation quality determines template reliability
  2. Code validation is the bottleneck (88% pass rate); failures require prompt iteration
  3. MC-Symbolic distractor validation requires N=20 random substitution tests per distractor
- Design tradeoffs:
  - Synthetic data enables infinite variants but may not reflect real-world problem distribution
  - Text-only filtering excludes diagram-dependent problems, narrowing physics domain coverage
  - Multiple-choice formats improve diagnostic granularity but may inflate perceived capability through scaffolding
- Failure signatures:
  - Low MC-Numerical + high MC-Symbolic accuracy → arithmetic/unit conversion issues
  - Low consistency score with high exact match → memorization rather than generalization
  - High confusion rate → model uncertainty or guessing behavior
  - Free-form much lower than MC formats → generation/formatting challenges, not pure reasoning
- First 3 experiments:
  1. Reproduce baseline evaluation on 2-3 models from Table 2, focusing on one physics subdomain to verify pipeline execution and metric computation.
  2. Run conditional accuracy analysis on a 50-problem subset to confirm format-based error decomposition matches paper findings (MC-Symbolic > MC-Numerical for computation-limited models).
  3. Introduce a controlled perturbation (e.g., remove unit specifications from problem text) and measure impact on consistency score to test sensitivity to surface-level variation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can hallucination detection be systematically formalized to quantify reasoning integrity when models are presented with under-specified physics problems?
- Basis in paper: [explicit] "In future work, we plan to formalize this capability and systematically benchmark hallucination rates across model families."
- Why unresolved: The paper demonstrates hallucination behaviors (e.g., models fabricating inputs or reasoning chains) but lacks a standardized metric or protocol to measure hallucination rates across different model families and problem types.
- What evidence would resolve it: A formalized hallucination metric applied across models, with controlled experiments systematically omitting critical variables and categorizing model responses into clarification-seeking, reasonable assumption-stating, or hallucination behaviors.

### Open Question 2
- Question: What mechanisms underlie the "implicit simplification bias" where models default to oversimplified physics expressions (e.g., Newtonian instead of relativistic) even when scenarios demand advanced treatments?
- Basis in paper: [inferred] Case Study III documents this bias in Qwen2.5-72B but does not identify its root causes or remedies.
- Why unresolved: The paper observes the phenomenon but does not determine whether it stems from training data distribution, lack of explicit domain-specific training, or architectural limitations.
- What evidence would resolve it: Targeted experiments varying the explicitness of problem framing (e.g., cueing relativistic vs. Newtonian contexts) and analyzing whether fine-tuning on advanced physics domains reduces the bias.

### Open Question 3
- Question: How will extending SymPyBench to multimodal reasoning tasks (e.g., problems with diagrams, graphs) affect model evaluation, and what new failure modes will emerge?
- Basis in paper: [explicit] "Future work will expand SymPyBench to include multimodal reasoning tasks and interdisciplinary STEM domains."
- Why unresolved: The current benchmark explicitly excludes visually-dependent problems (~5% filtered), so model performance on multimodal scientific reasoning remains untested.
- What evidence would resolve it: A multimodal extension of SymPyBench with image-based physics problems, evaluated on vision-language models, with analysis comparing error types (visual parsing vs. reasoning) to text-only baselines.

## Limitations
- Benchmark excludes diagram-dependent and context-heavy problems, limiting coverage of full physics curriculum.
- 12% code validation failure rate requires filtering rather than correction, potentially introducing bias.
- Specific prompt templates for LLM-based generation are not disclosed, hindering exact reproduction.

## Confidence
- High confidence: Code-grounded parameterization mechanism is well-supported; performance patterns across formats are consistently observed.
- Medium confidence: Conditional accuracy interpretation assumes MC-Symbolic scaffolding doesn't mask partial understanding.
- Low confidence: Extent to which synthetic variants capture real-world physics problem variability remains unclear.

## Next Checks
1. Systematically remove unit specifications from a subset of problems and measure the impact on Consistency Score to test sensitivity to surface-level variation.
2. Perform detailed error analysis on 50-problem subset to verify that MC-Numerical failures are primarily computational rather than conceptual.
3. Evaluate whether models excelling on MC-Symbolic maintain similar relative performance on free-form variants when converted to MC format with validated distractors.