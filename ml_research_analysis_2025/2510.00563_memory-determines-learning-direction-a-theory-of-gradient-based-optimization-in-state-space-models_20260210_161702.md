---
ver: rpa2
title: 'Memory Determines Learning Direction: A Theory of Gradient-Based Optimization
  in State Space Models'
arxiv_id: '2510.00563'
source_url: https://arxiv.org/abs/2510.00563
tags:
- eigenvalues
- memory
- learning
- layer
- linear
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a theoretical framework for understanding gradient-based
  optimization in state space models (SSMs), focusing on how memory capacity determines
  learning direction. The authors analyze SSMs through memory functions (MFs), showing
  that input time series storage in current states reveals tradeoffs between memory
  accuracy and length.
---

# Memory Determines Learning Direction: A Theory of Gradient-Based Optimization in State Space Models

## Quick Facts
- arXiv ID: 2510.00563
- Source URL: https://arxiv.org/abs/2510.00563
- Reference count: 40
- Primary result: Fixed eigenvalues outperform learned eigenvalues in 5 of 6 LRA tasks

## Executive Summary
This paper presents a theoretical framework showing that memory capacity in State Space Models (SSMs) determines learning direction through gradient-based optimization. The authors analyze SSMs through memory functions (MFs), demonstrating that successful learning requires initializing with the longest possible memory structure, even at the cost of accuracy. They prove theoretical equivalence between S4 and S4D architectures with diagonal state matrices and show experimentally that fixing recurrent weights (Reservoir Computing setting) achieves comparable or superior performance to adaptive weights with faster convergence. The proposed approach removes approximately 10% of trainable parameters without sacrificing performance, offering a novel optimization strategy for SSMs.

## Method Summary
The authors analyze SSMs through memory functions (MFs), showing that input time series storage in current states reveals tradeoffs between memory accuracy and length. They implement S4D architecture with diagonal state matrices using HiPPO-derived eigenvalue formulas, discretizing via zero-order hold with uniform sampling in [dt_min, dt_max]. The method compares two settings: learnable eigenvalues (train Λ) versus RC setting (freeze Λ, only train readout weights W^l and b^l). Bidirectional processing is used with GELU activation followed by GLU post-activation. Training uses cross-entropy loss with Adam optimizer on LRA benchmarks, running 6 seeds per configuration. The key innovation is fixing eigenvalues to preserve long memory structures while training only the readout layers.

## Key Results
- Fixed eigenvalues outperform learned eigenvalues in 5 of 6 LRA tasks
- RC setting achieves comparable performance to S4 while removing ~10% of trainable parameters
- Memory function analysis shows fixed eigenvalues maintain longer memory structures while learned eigenvalues shorten them
- Faster convergence observed with fixed eigenvalues compared to adaptive weight training

## Why This Works (Mechanism)
The theoretical framework demonstrates that memory capacity directly governs gradient updates in SSMs. By fixing eigenvalues to preserve long memory structures, the model maintains the ability to capture long-range dependencies essential for LRA tasks. The memory function (MF) analysis reveals that successful learning requires prioritizing memory length over accuracy, as shorter but more accurate memory structures lose critical long-range information. The gradient update direction is determined by the eigenvalues through the Vandermonde matrix, making the initial memory structure crucial for learning trajectory.

## Foundational Learning
- State Space Models (SSMs): Sequence modeling architectures using state matrices for temporal processing
  * Why needed: Core architecture being analyzed and compared
  * Quick check: Verify S4D implements diagonal state matrix Λ correctly

- Memory Functions (MFs): Mathematical representation of how input time series is stored in current states
  * Why needed: Central theoretical tool for analyzing memory capacity and learning direction
  * Quick check: Compute MF using Eq. 4 with T=1024, N=32 to verify methodology

- Reservoir Computing (RC): Training approach where recurrent weights are fixed while only readout layers are learned
  * Why needed: Key experimental condition that outperforms standard training
  * Quick check: Compare performance with learnable vs fixed eigenvalues

- HiPPO framework: Method for deriving optimal memory structures in continuous-time SSMs
  * Why needed: Provides theoretical basis for eigenvalue initializations (S4Dinv, S4Dlin)
  * Quick check: Verify eigenvalue formulas match HiPPO derivations

## Architecture Onboarding

Component Map: Input -> Linear Layer -> SSM (with Λ) -> GELU -> GLU -> Output

Critical Path: The SSM layer with diagonal state matrix Λ is the critical component, as its eigenvalues determine memory capacity and learning direction. The bidirectional processing (forward + reversed) and GLU activation are also essential for performance.

Design Tradeoffs: The main tradeoff is between memory accuracy and length. The paper argues that prioritizing memory length (via fixed eigenvalues) is more important for LRA tasks than achieving higher accuracy with shorter memory. This contradicts standard practice of learning all parameters.

Failure Signatures:
- Spectral radius > 1 causes state explosion - check max|eigenvalue| < 1
- Training eigenvalues shortens MF instead of extending it - plot MF before/after training
- Overfitting with learnable eigenvalues shows high training accuracy but poor test accuracy - compare train vs test loss curves

First Experiments:
1. Implement S4D with diagonal Λ and verify spectral radius constraint
2. Compute and plot memory functions for both fixed and learned eigenvalue settings
3. Run bidirectional training on Listops task to verify convergence patterns

## Open Questions the Paper Calls Out

Open Question 1: Are HiPPO-based eigenvalue initializations (e.g., S4Dinv, S4Dlin) truly optimal for maximizing memory function (MF) in the Reservoir Computing (RC) setting? While HiPPO provides theoretically long memory, it optimizes for Lebesgue measure rather than the MF inherent to gradient updates. An initialization strategy specifically maximizing MF might outperform HiPPO approximations.

Open Question 2: How can the theoretical relationship between memory capacity and learning dynamics be extended to nonlinear State Space Models like Mamba? The paper's theory relies on SSM linearity to prove gradient determination by eigenvalues. Nonlinear state updates break this chain, requiring new theoretical frameworks potentially using Information Processing Capacity (IPC).

Open Question 3: In what specific task conditions does the Reservoir Computing (RC) setting (fixed eigenvalues) fail compared to learnable eigenvalues? The theory suggests RC may underperform when tasks require only extremely short-term memory, but all validation was on LRA which prioritizes long dependencies. The inability to adapt eigenvalues to shorten memory might hamper performance when long-term context is noise rather than signal.

## Limitations
- Theoretical framework relies on linearity assumptions that break down in nonlinear SSMs
- Empirical validation limited to LRA benchmarks which prioritize long-range dependencies
- Memory function analysis interpretation could be more rigorously validated
- Bidirectional processing contribution not fully separated from RC approach benefits

## Confidence
- High confidence: Theoretical framework and memory function analysis methodology
- Medium confidence: S4D architecture implementation and eigenvalue discretization
- Low confidence: Comparative performance claims and practical implications

## Next Checks
1. Reproduce the MF analysis for a subset of tasks (e.g., Listops, Retrieval) to verify that fixed eigenvalues maintain longer memory structures while learned eigenvalues shorten them
2. Implement both bidirectional and unidirectional versions to test whether the performance gains are primarily from the RC approach or from bidirectional processing
3. Conduct ablation studies varying the number of trainable parameters (e.g., freeze different proportions of Λ) to quantify the true parameter reduction benefits