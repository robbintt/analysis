---
ver: rpa2
title: Teaching Large Language Models Number-Focused Headline Generation With Key
  Element Rationales
arxiv_id: '2502.03129'
source_url: https://arxiv.org/abs/2502.03129
tags:
- headline
- rationales
- numerical
- generation
- news
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of generating number-focused
  headlines that require both high textual quality and precise numerical accuracy.
  The authors propose a novel Chain-of-Thought framework using rationales comprising
  key elements of Topic, Entities, and Numerical reasoning (TEN) to enhance Large
  Language Models' capability for generating topic-aligned headlines with high numerical
  accuracy.
---

# Teaching Large Language Models Number-Focused Headline Generation With Key Element Rationales

## Quick Facts
- arXiv ID: 2502.03129
- Source URL: https://arxiv.org/abs/2502.03129
- Reference count: 40
- Primary result: 77.20% numerical accuracy on NumHG benchmark, outperforming baselines by 2.63-5.61%

## Executive Summary
This paper addresses the challenge of generating number-focused headlines that require both high textual quality and precise numerical accuracy. The authors propose a novel Chain-of-Thought framework using rationales comprising key elements of Topic, Entities, and Numerical reasoning (TEN) to enhance Large Language Models' capability for generating topic-aligned headlines with high numerical accuracy. The approach employs a teacher LLM to generate TEN rationales as supervision data, which are then used to fine-tune a student LLM through knowledge distillation and Direct Preference Optimization. Experiments on two benchmark datasets show that the proposed method achieves superior performance, with 77.20% overall numerical accuracy on NumHG (outperforming baselines by 2.63-5.61%) and 39.07% on XSum (outperforming baselines by 2.31-9.73%).

## Method Summary
The method employs a two-stage approach using knowledge distillation from GPT-4o to smaller student models. First, a teacher LLM generates TEN rationales (Topic, Entities, Numerical reasoning) for training data. Then, a student LLM is fine-tuned as a Rationale Generator to produce these structured rationales, and a separate Headline Generator is fine-tuned to create headlines from news articles and the generated rationales. The Rationale Generator undergoes additional Direct Preference Optimization (DPO) refinement to improve numerical accuracy. During inference, the system uses a pipeline where news articles are first processed by the Rationale Generator to produce TEN rationales, which are then fed to the Headline Generator to produce the final headline.

## Key Results
- Achieves 77.20% overall numerical accuracy on NumHG benchmark, outperforming baselines by 2.63-5.61%
- Achieves 39.07% overall numerical accuracy on XSum benchmark, outperforming baselines by 2.31-9.73%
- Maintains comparable or better textual quality while significantly improving numerical accuracy across various mathematical operations including copying, addition, subtraction, paraphrasing, and rounding

## Why This Works (Mechanism)

### Mechanism 1
Structuring chain-of-thought rationales into distinct key elements (Topic, Entities, Numerical reasoning) appears to decouple the dual challenges of semantic alignment and mathematical calculation. By forcing the model to explicitly list "Topic," "Entities," and "Reasoning steps" before generating the headline, the architecture likely reduces cognitive load and prevents the model from hallucinating numbers that fit the context semantically but fail mathematically.

### Mechanism 2
Knowledge distillation from a teacher LLM (GPT-4o) to a student LLM (Mistral-7B) transfers the ability to generate these structured rationales without the inference cost of the larger model. The teacher model uses zero-shot and few-shot prompting to create high-quality "TEN" supervision data. The student model is fine-tuned on this data, effectively compressing the teacher's reasoning logic into a smaller parameter space.

### Mechanism 3
Applying Direct Preference Optimization (DPO) specifically to the rationale generator (rather than just the headline generator) refines the intermediate reasoning steps to favor numerical accuracy. Instead of relying solely on supervised tokens, DPO uses a preference dataset where "chosen" rationales are those that lead to a correct headline number, and "rejected" rationales lead to incorrect ones.

## Foundational Learning

- **Concept:** Chain-of-Thought (CoT) Reasoning
  - **Why needed here:** The entire TEN framework is built on CoT principles. You must understand that LLMs often perform better at complex tasks (like math in text) when forced to "think aloud" or generate intermediate steps.
  - **Quick check question:** Can you explain why asking an LLM to "list the numbers first" might improve its addition accuracy compared to asking for the final sum immediately?

- **Concept:** Knowledge Distillation (Teacher-Student)
  - **Why needed here:** The proposed architecture does not rely on a massive model (GPT-4o) at inference time. It relies on distilling that capability into Mistral-7B. Understanding this transfer is crucial for onboarding.
  - **Quick check question:** What is the primary trade-off being made when using a student model instead of the teacher model for inference?

- **Concept:** Direct Preference Optimization (DPO)
  - **Why needed here:** The paper uses DPO to fix a specific failure mode (numerical errors). You need to know how DPO differs from standard supervised fine-tuning (SFT)—specifically, that it learns from *relative* rankings of outputs rather than absolute token prediction.
  - **Quick check question:** In the context of this paper, what defines a "chosen" vs. "rejected" pair for the DPO training data?

## Architecture Onboarding

- **Component map:** News Article -> Rationale Generator -> TEN Rationale -> Headline Generator -> Final Headline
- **Critical path:** The generation of the supervision data by the Teacher LLM. If the teacher's rationales are low quality or inconsistent, the student's distillation will fail.
- **Design tradeoffs:** The architecture requires two forward passes (one for rationale, one for headline) during inference, effectively doubling the latency compared to a standard generator. The system relies on constructing a preference dataset for DPO, which may not cover enough edge cases if the training set is small.
- **Failure signatures:** Format Drift - the Rationale Generator might revert to unstructured text if the SFT wasn't rigorous, breaking the Headline Generator's input parser. False Positive Reasoning - DPO might overfit to specific numbers without understanding the underlying math.
- **First 3 experiments:**
  1. Verify Rationale Quality: Run the Rationale Generator on a hold-out set and manually check if the "Reasoning steps" actually align with the "Topic" and "Entities" listed.
  2. Ablation on Inference: Test the Headline Generator *with* and *without* the generated rationale to confirm whether the complexity of the two-stage inference is actually buying you the claimed accuracy bump.
  3. DPO Impact Analysis: Compare the error rates of the "SFT-only" model vs. the "SFT+DPO" model specifically on the "Addition" and "Subtraction" categories to see where DPO helps most.

## Open Questions the Paper Calls Out

- **Question 1:** Does full parameter fine-tuning of student LLMs yield significantly higher numerical accuracy or textual quality compared to the parameter-efficient QLoRA technique employed in this study? The authors note that due to computing resource limitations, they only applied QLoRA and have not fully elicited the capability of student LLMs.

- **Question 2:** How effectively does the TEN framework generalize to numerical generation tasks in specialized domains outside of news headlines, such as financial reporting or scientific literature? The authors identify limited data for experiments as a limitation, having relied solely on the NumHG benchmark and a modified XSum dataset, both news-focused.

- **Question 3:** Can the TEN framework be adapted to significantly reduce the high error rates observed in complex mathematical operations like division and multiplication? The error analysis reveals that while the model performs well on copying and addition, the error rates for Division (92.86%) and Multiplication (81.82%) remain exceptionally high.

## Limitations

- The model struggles significantly with complex numerical operations like multiplication and division, indicating fundamental limitations in the Chain-of-Thought approach for non-linear operations.
- The approach assumes clean, unambiguous numerical information in source texts, which may not hold for real-world news articles containing contradictory or approximate numbers.
- The reliance on GPT-4o for generating training supervision data introduces a dependency on proprietary models and raises questions about whether the distilled capability generalizes beyond GPT-4o's reasoning patterns.

## Confidence

*High confidence:* The experimental results showing 77.20% numerical accuracy on NumHG and 39.07% on XSum are directly measurable and reproducible. The ablation studies demonstrating the independent contributions of Topic, Entity, and Number components are methodologically sound.

*Medium confidence:* The claim that the two-stage inference pipeline (Rationale Generator → Headline Generator) is necessary for accuracy gains, rather than simply increasing model size or using more sophisticated attention mechanisms.

*Low confidence:* The assertion that this approach generalizes to "any news article requiring numerical reasoning" - the evaluation was limited to two specific datasets with relatively clean numerical information.

## Next Checks

1. **Error Type Analysis:** Conduct a detailed error analysis categorizing mistakes by operation type (copy, add, subtract, paraphrase, round, multiply, divide) to identify whether the model's struggles with multiplication/division represent a fundamental architectural limitation or a data sparsity issue.

2. **Domain Transfer Test:** Evaluate the system on news articles from domains not represented in the training data (e.g., financial reports, sports statistics, scientific publications) to assess generalization beyond the benchmark datasets.

3. **Inference Latency Measurement:** Quantify the actual latency overhead of the two-stage inference pipeline compared to a baseline headline generator, measuring whether the accuracy gains justify the computational cost in production settings.