---
ver: rpa2
title: Ground Plane Projection for Improved Traffic Analytics at Intersections
arxiv_id: '2511.12342'
source_url: https://arxiv.org/abs/2511.12342
tags:
- plane
- movement
- ground
- each
- camera
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the advantages of analyzing traffic in
  the ground plane versus the image plane for accurate turning movement counts at
  intersections. The authors introduce two new datasets and enhance two existing ones
  with ground-truth turn counts, orthographic projections, and homographies.
---

# Ground Plane Projection for Improved Traffic Analytics at Intersections

## Quick Facts
- arXiv ID: 2511.12342
- Source URL: https://arxiv.org/abs/2511.12342
- Reference count: 40
- Analyzing traffic in ground-plane coordinates outperforms image-plane analysis for turning movement counts at intersections.

## Executive Summary
This paper investigates whether analyzing traffic in real-world ground-plane coordinates, instead of image-plane coordinates, improves turning movement classification accuracy at intersections. The authors introduce two new datasets and enhance two existing ones with ground-truth turn counts, orthographic projections, and homographies. A pipeline is developed that detects vehicles, tracks them, back-projects detections to the ground plane, and classifies trajectories using unsupervised learning. Evaluation across three datasets shows that ground-plane analysis consistently outperforms image-plane analysis, with maximum likelihood classification yielding the best results. Additionally, weak fusion of data from multiple cameras further improves accuracy, reducing error rates by nearly half compared to single-view systems.

## Method Summary
The method detects vehicles using InternImage, tracks them using ByteTrack, and back-projects detections to ground coordinates using a homography matrix. Trajectories are resampled uniformly in space and classified using unsupervised learning methods, with maximum likelihood classification via kernel density estimation (KDE) outperforming other methods. Multi-camera weak fusion assigns each movement class to the best-performing camera, further reducing error rates.

## Key Results
- Ground-plane analysis consistently outperforms image-plane analysis for turning movement classification.
- Maximum likelihood classification via KDE achieves the best results across all three datasets.
- Weak fusion of data from multiple cameras reduces error rates by nearly half compared to single-view systems.

## Why This Works (Mechanism)

### Mechanism 1: Ground Plane Back-Projection
Transforming vehicle detections from image coordinates to real-world ground coordinates improves trajectory class separation and classification accuracy. A homography matrix maps pixel locations to a planar approximation of the intersection ground. The midpoint of the bottom edge of each 2D bounding box is back-projected to obtain a bird's-eye-view position. This assumes flat terrain, corrected camera distortion, and that the bounding-box base approximates the vehicle centroid.

### Mechanism 2: Maximum Likelihood Classification via Kernel Density Estimation
A probabilistic likelihood model built from training tracks outperforms heuristic methods for trajectory classification. For each turning movement class, a Gaussian KDE model estimates the spatial density of vehicle positions. The track is assigned to the class with highest likelihood, assuming conditionally independent positions given the movement class. This method discards temporal ordering but captures full spatial variance.

### Mechanism 3: Multi-Camera Weak Fusion
Assigning each turning movement class to the camera with lowest observed error for that class reduces overall count error and bias compared to any single-camera system. On training data, evaluate per-camera error rates for each of the 12 movement classes. Assign each class to the best-performing camera, balancing load if tied. At inference, each camera only reports counts for its assigned classes.

## Foundational Learning

- **Homography and Projective Geometry**
  - Why needed: Converts image-plane detections to metric ground-plane coordinates for consistent trajectory analysis.
  - Quick check: Given four point correspondences between a camera image and an orthophoto, can you compute the 3Ã—3 homography matrix and explain its degrees of freedom?

- **Kernel Density Estimation (KDE)**
  - Why needed: Forms the likelihood model for each movement class; bandwidth selection controls spatial smoothing.
  - Quick check: If the KDE bandwidth is too large, what happens to class-conditional likelihood separation between adjacent turning movements?

- **Kalman Filter for Multi-Object Tracking**
  - Why needed: ByteTrack uses a Kalman filter to predict bounding-box positions and associate detections across frames.
  - Quick check: What state variables does a typical 2D bounding-box Kalman tracker maintain, and how does gating reduce false associations?

## Architecture Onboarding

- **Component map:** Geometric calibration -> Detection -> Tracking -> Back-projection -> Classification
- **Critical path:** Calibration accuracy -> Detection recall/precision -> Track continuity -> Back-projection localization error -> Likelihood model quality -> Classification correctness
- **Design tradeoffs:** Prototype vs. KDE model (interpretable vs. full spatial variance); high vs. low camera mounting (reduced occlusions vs. practical deployment); single-camera vs. multi-camera (simplicity vs. accuracy)
- **Failure signatures:** Elevated reprojection error (>10-20 px) -> Homography mismatch; consistent overcounting bias -> Track fragmentation; high error on specific movement classes -> Limited field of view; low likelihood for all classes -> Track too short or outside ROI
- **First 3 experiments:**
  1. **Calibration validation:** Compute homography on a split of keypoints (train/test); report mean reprojection error on held-out points. Target <5 px on orthophoto.
  2. **Classification method comparison:** Run all four methods (EE, DIR, VOTE, ML) on both image and ground planes over the validation partition; log per-class error rates.
  3. **Plane ablation:** Fix all components; toggle only the analysis domain (image vs. ground) and measure delta in count error across datasets.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does incorporating the temporal order of vehicle positions into the trajectory likelihood model improve turning movement classification accuracy? The current implementation treats tracks as sets of independent points, ignoring sequential dependencies like velocity and acceleration.

- **Open Question 2:** Can applying an empirical prior distribution over turning movement classes, potentially adaptive to time of day, further reduce classification error? The authors deliberately avoided priors to avoid assumptions about temporal stability.

- **Open Question 3:** Does fusing detections in 3D coordinates at the detection stage (early fusion) significantly outperform the "weak fusion" strategy? The paper implemented a simple strategy assigning classes to specific cameras rather than integrating raw data streams.

- **Open Question 4:** To what extent can digital terrain models and image stabilization improve back-projection accuracy on non-flat intersections or unstable camera mounts? The current method relies on planar homographies, which introduce error on non-flat terrain.

## Limitations

- The study relies on strong assumptions: flat-ground homography is adequate for the intersection domain, and conditionally independent position likelihoods suffice for ML classification.
- Limited error analysis for short tracks or occlusions.
- Multi-camera fusion is sensitive to viewpoint overlap and calibration stability.

## Confidence

- Ground-plane projection benefit: High
- ML-KDE superiority over heuristics: Medium
- Multi-camera weak fusion improvement: Medium

## Next Checks

1. Test homography accuracy under non-planar terrain (e.g., crowned road) and quantify reprojection drift.
2. Evaluate ML-KDE classification sensitivity to bandwidth choice and class prior imbalance.
3. Validate multi-camera fusion robustness to camera dropout or synchronization errors.