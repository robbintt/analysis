---
ver: rpa2
title: 'Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification'
arxiv_id: '2507.11004'
source_url: https://arxiv.org/abs/2507.11004
tags:
- evidence
- hero
- answer
- task
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HerO 2 is an efficient fact-checking system developed for the A
  VeriTeC shared task, enhancing its predecessor through document summarization, answer
  reformulation, post-training quantization, and updated LM backbones. By summarizing
  web documents into evidence blocks and reformulating answers conditioned on questions,
  the system improves evidence quality.
---

# Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification

## Quick Facts
- arXiv ID: 2507.11004
- Source URL: https://arxiv.org/abs/2507.11004
- Reference count: 4
- Ranked second on AVeriTeC leaderboard with score 0.271 and fastest runtime (29.19 s per claim) among top 3 systems

## Executive Summary
HerO 2 is an efficient fact-checking system developed for the A VeriTeC shared task, enhancing its predecessor through document summarization, answer reformulation, post-training quantization, and updated LM backbones. By summarizing web documents into evidence blocks and reformulating answers conditioned on questions, the system improves evidence quality. Veracity prediction is optimized via AWQ-quantized Qwen3 32B under computational constraints. HerO 2 ranked second on the leaderboard with a score of 0.271 and achieved the shortest runtime (29.19 s per claim) among the top three systems, demonstrating both high efficiency and strong potential for real-world fact verification.

## Method Summary
HerO 2 improves upon its predecessor through several key innovations. The system first summarizes retrieved web documents into evidence blocks to enhance evidence quality. It then reformulates answers conditioned on questions to better align with the verification task. For veracity prediction, HerO 2 employs AWQ-quantized Qwen3 32B models to balance accuracy with computational efficiency. These modifications enable the system to maintain strong performance while achieving faster processing times compared to competing approaches.

## Key Results
- Ranked second on AVeriTeC leaderboard with score 0.271
- Achieved fastest runtime among top three systems: 29.19 seconds per claim
- Demonstrated efficiency gains through AWQ quantization and optimized LM backbones

## Why This Works (Mechanism)
HerO 2 achieves its efficiency and performance through strategic architectural choices. Document summarization reduces information overload while preserving critical evidence, enabling faster processing. Answer reformulation conditions responses on specific questions, improving relevance and accuracy. The use of AWQ quantization for Qwen3 32B models significantly reduces computational overhead without substantial accuracy loss. Together, these mechanisms allow HerO 2 to maintain strong verification performance while operating within tight computational constraints.

## Foundational Learning
- **Document Summarization**: Condenses retrieved web documents into evidence blocks. Needed to reduce information overload and focus on relevant evidence. Quick check: Compare evidence quality and processing time with/without summarization.
- **Answer Reformulation**: Conditions answers on specific questions. Needed to improve relevance and alignment with verification task. Quick check: Measure accuracy improvement from reformulated vs. raw answers.
- **AWQ Quantization**: Applies activation-aware weight quantization to models. Needed to reduce computational cost while preserving accuracy. Quick check: Compare performance of quantized vs. full-precision models.
- **Qwen3 32B Backbone**: Utilizes a large language model as the core reasoning engine. Needed for strong semantic understanding and verification capabilities. Quick check: Benchmark against smaller model variants.
- **Evidence-Claim Integration**: Combines processed evidence with original claims for final prediction. Needed to maintain context while streamlining verification. Quick check: Test performance with incomplete or noisy evidence.
- **Computational Constraints**: Operates under strict runtime and resource limits. Needed to ensure practical deployment feasibility. Quick check: Measure scaling behavior under increased workload.

## Architecture Onboarding

Component Map: Web Documents -> Document Summarization -> Evidence Blocks -> Answer Reformulation -> Veracity Prediction -> Final Score

Critical Path: The system follows a linear pipeline where each component builds on the previous one. Document summarization feeds into evidence blocks, which are then used by the answer reformulation module. The reformulated answers and evidence are finally processed by the quantized Qwen3 32B model for veracity prediction.

Design Tradeoffs: The primary tradeoff involves balancing efficiency with accuracy. AWQ quantization significantly reduces computational requirements but may slightly impact model performance. Document summarization improves processing speed but risks losing nuanced information. The 32B model size represents a middle ground between computational feasibility and strong reasoning capabilities.

Failure Signatures: Potential failures include loss of critical evidence during summarization, misalignment between reformulated answers and actual claims, and accuracy degradation from quantization. The system may also struggle with claims requiring deep domain expertise or those involving rapidly changing information.

First Experiments: 1) Measure runtime and accuracy impact of document summarization by comparing with raw document processing. 2) Evaluate answer reformulation quality through human assessment of relevance and accuracy. 3) Benchmark AWQ-quantized models against full-precision versions to quantify efficiency gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical robustness across diverse claim types and document domains remains uncertain
- Evaluation relies on single shared task dataset, limiting generalizability to real-world scenarios
- Absolute runtime performance and scalability under production workloads are not characterized
- Document summarization effectiveness in preserving critical evidence is assumed rather than validated
- AWQ quantization impact on veracity prediction accuracy relative to full-precision baselines is not quantified

## Confidence
- Veracity prediction performance and leaderboard ranking: High
- Efficiency gains from quantization and model architecture: Medium
- Document summarization and answer reformulation improvements: Medium
- Generalizability to broader fact-checking contexts: Low

## Next Checks
1. Conduct ablation studies removing document summarization and answer reformulation to quantify their individual contributions to evidence quality and system accuracy.
2. Test HerO 2 on external fact-checking datasets (e.g., FEVER, SciFact) to assess cross-domain performance and robustness.
3. Benchmark runtime and memory usage under simulated high-throughput workloads to validate production readiness and scalability claims.