---
ver: rpa2
title: Action-Dependent Optimality-Preserving Reward Shaping
arxiv_id: '2505.12611'
source_url: https://arxiv.org/abs/2505.12611
tags:
- reward
- shaping
- policy
- optimal
- intrinsic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reward hacking in reinforcement
  learning, where agents optimize shaping rewards at the expense of extrinsic rewards,
  leading to suboptimal policies. The authors propose Action-Dependent Optimality-Preserving
  Shaping (ADOPS), a method that converts arbitrary shaping rewards (including intrinsic
  motivation) into a form that preserves optimality while allowing for action-dependent
  returns.
---

# Action-Dependent Optimality-Preserving Reward Shaping

## Quick Facts
- **arXiv ID:** 2505.12611
- **Source URL:** https://arxiv.org/abs/2505.12611
- **Reference count:** 19
- **Primary result:** ADOPS improves performance over baseline intrinsic motivation in sparse-reward environments like Montezuma's Revenge.

## Executive Summary
This paper addresses reward hacking in reinforcement learning, where agents optimize shaping rewards at the expense of extrinsic rewards, leading to suboptimal policies. The authors propose Action-Dependent Optimality-Preserving Shaping (ADOPS), a method that converts arbitrary shaping rewards (including intrinsic motivation) into a form that preserves optimality while allowing for action-dependent returns. ADOPS functions by using critic networks to estimate extrinsic and intrinsic value functions, and actively adjusts the intrinsic reward if it would cause preference for an action that wouldn't be preferred by external rewards alone. The method overcomes limitations of previous approaches by avoiding exponentially large end-of-episode penalties and accommodating a wider set of optimality-preserving shaping functions.

## Method Summary
ADOPS implements reward shaping by computing an adjustment term $F_2$ based on critic estimates of extrinsic and intrinsic value functions. The algorithm compares the estimated Q-value of taking an action ($\hat{Q}^{\pi}_E$) to the state value ($\hat{V}^{\pi}_E$). If an action is suboptimal, it calculates a negative adjustment to the intrinsic reward, ensuring the combined reward does not make the action preferable. If an action is optimal, it ensures the intrinsic return is sufficient to keep it optimal. The method uses PPO with RND baseline, running on Montezuma's Revenge environment with 20 seeds per variant, and external rewards scaled by 1/1000.

## Key Results
- ADOPS improves performance over baseline intrinsic motivation in Montezuma's Revenge
- The method shows statistically significant improvements over other optimality-preserving methods
- ADOPES variant (with phased scaling) further improves training stability and performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ADOPS preserves the optimal policy by actively penalizing actions that are extrinsically suboptimal while maintaining the relative value of optimal actions.
- Mechanism: At each timestep, the algorithm compares the estimated Q-value of taking an action ($\hat{Q}^{\pi}_E$) to the state value ($\hat{V}^{\pi}_E$). If an action is suboptimal ($\hat{Q}^{\pi}_E < \hat{V}^{\pi}_E$), it calculates a negative adjustment $F_2$ to the intrinsic reward, ensuring the combined reward does not make the action preferable. If an action is optimal, it ensures the intrinsic return is sufficient to keep it optimal.
- Core assumption: The learning algorithm converges to "stable" policies where small policy perturbations do not yield strictly better returns, and critic networks provide bounded estimates of value functions.
- Evidence anchors:
  - [abstract]: "a method that converts arbitrary intrinsic rewards into optimality-preserving forms by actively adjusting rewards based on value function estimates."
  - [section 5.1]: Defines the core logic in Equation 15, adjusting rewards based on whether $Q^*_E < V^*_E$.
  - [corpus]: Contextual support for intrinsic reward shaping is present (e.g., papers 12966, 87486), but no direct evidence for this specific per-timestep adjustment mechanism.
- Break condition: If critic estimates are highly inaccurate or unstable, the adjustment $F_2$ will be miscalculated, potentially penalizing good actions or rewarding bad ones.

### Mechanism 2
- Claim: ADOPS is more effective in long-horizon tasks than prior methods because it avoids the exponentially large, end-of-episode penalties inherent in Potential-Based Reward Shaping (PBRS).
- Mechanism: Methods like PBIM and GRM preserve optimality by retroactively subtracting all accumulated intrinsic rewards at a future timestep (often the episode's end). In long episodes, this penalty scales with $\gamma^{-N}$, becoming explosively large and destroying the learning signal. ADOPS adjusts rewards incrementally at each step, avoiding this deferred explosion.
- Core assumption: The primary failure mode of PBRS in long-horizon tasks is the destabilizing magnitude of delayed penalty terms.
- Evidence anchors:
  - [abstract]: "while PBRS-based methods require the cumulative discounted intrinsic return be independent of actions, ADOPS allows for intrinsic cumulative returns to be dependent on agents' actions while still preserving the optimal policy set."
  - [section 6.1]: States PBIM "immediately saturates the agent's action probabilities" and links this to the "exponential nature of the denominator" in its end-of-episode reward calculation.
  - [corpus]: No direct corpus evidence; neighbor papers focus on reward design, not this specific failure mode of PBRS.
- Break condition: The mechanism's advantage is irrelevant in short-horizon environments where PBRS penalties remain manageable.

### Mechanism 3
- Claim: The ADOPES variant improves training stability by gradually phasing in the optimality-preserving adjustment as critic estimates become more reliable.
- Mechanism: ADOPES multiplies the ADOPS adjustment term by a coefficient $\zeta$ that scales linearly from 0 to 1 during training. This allows the agent to follow the raw intrinsic reward for exploration early on (when critics are poor) and increasingly enforces optimality preservation later (when critics are accurate and extrinsic learning is critical).
- Core assumption: Critic estimates are unreliable early in training, and a scheduled transition from exploration to optimality-preservation is beneficial.
- Evidence anchors:
  - [section 6.4]: "Noting that these critics' estimations are much better later on in training than earlier, we also implement a fusion of our method with PIES..."
  - [section 6.4]: Shows ADOPES outperforming both baseline RND and other methods, supporting the phased approach.
  - [corpus]: The corpus discusses intrinsic rewards and exploration but not this specific phased scaling strategy.
- Break condition: The schedule for $\zeta$ must be tuned; if critics converge slowly, a linear schedule might enforce optimality too early, stifling exploration.

## Foundational Learning

- **Q-Learning & Value Functions:**
  - **Why needed here:** ADOPS's core logic relies on comparing state values ($V$) and action values ($Q$). You cannot implement or debug the algorithm without understanding how these estimates are produced by critic networks.
  - **Quick check question:** Given a policy $\pi$, what is the mathematical relationship between $V^{\pi}(s)$ and $Q^{\pi}(s, a)$ for an action $a$ sampled from $\pi$?

- **Reward Shaping & Potential-Based Methods (PBRS):**
  - **Why needed here:** The paper's primary contribution is a successor to PBRS. Understanding why PBRS guarantees policy invariance (via the potential difference $\gamma\Phi(s') - \Phi(s)$) clarifies the problem ADOPS solves (PBRS's unsuitability for long horizons).
  - **Quick check question:** How does a potential-based shaping reward ensure that the optimal policy of the underlying MDP remains unchanged?

- **Intrinsic Motivation (IM):**
  - **Why needed here:** The paper uses IM (specifically RND) as its base shaping signal. Understanding the goal of IM (to encourage exploration in sparse rewards) is necessary to see why "reward hacking" is a critical failure mode.
  - **Quick check question:** In a sparse reward environment, what is the primary function of an intrinsic reward bonus like Random Network Distillation (RND)?

## Architecture Onboarding

- **Component Map:**
  - Base RL Algorithm (PPO) -> Intrinsic Reward Module (RND) -> ADOPS Adjustment Module -> Total Reward (R + F + F2)

- **Critical Path:**
  1. The agent takes an action and receives state $s'$ and extrinsic reward $R$.
  2. The Intrinsic Reward Module computes the raw bonus $F$ for this transition.
  3. The ADOPS Adjustment Module retrieves critic estimates for the current state and action.
  4. It calculates the adjustment $F_2$ using Equation 16.
  5. The total training reward is computed as $R + F + F_2$. This is used to update the critic and actor.

- **Design Tradeoffs:**
  - **Action-Dependence vs. Stability:** ADOPS allows for action-dependent rewards, which aids exploration but requires careful critic management to ensure stability.
  - **Critic Reliance:** The method is only as good as the critic estimates. In early training or with poor hyperparameters, ADOPS may add noise. ADOPES is a designed mitigation for this.

- **Failure Signatures:**
  - **Early stagnation:** If the critic's estimated values are wildly off, ADOPS may incorrectly penalize good actions, preventing the agent from learning.
  - **Performance degradation at scale:** In extremely long episodes, any issues with the discount factor $\gamma$ or numerical precision in the ADOPS calculation could destabilize learning, though this is less severe than with PBIM.

- **First 3 Experiments:**
  1. **Reproduce PBIM Failure:** Run PBIM in a long-horizon sparse environment (e.g., Montezuma's Revenge). Observe the exponential explosion of the end-of-episode penalty and subsequent policy collapse (saturated action probabilities).
  2. **Basic ADOPS Validation:** Run the core ADOPS algorithm (Equation 16) on the same task. Verify that the agent can achieve non-zero extrinsic rewards, confirming the mechanism avoids PBIM's failure mode.
  3. **ADOPES Ablation:** Compare ADOPS against the ADOPES variant (with the $\zeta$ coefficient schedule). Measure sample efficiency to see if the phased approach provides a statistically significant benefit.

## Open Questions the Paper Calls Out
None

## Limitations
- The method's performance is highly dependent on accurate critic estimates, which are assumed to converge but not empirically validated across diverse environments
- The ADOPES variant introduces additional hyperparameters (the $\zeta$ schedule) whose optimal configuration is not explored
- The theoretical analysis relies on strong assumptions about critic convergence that may not hold in practice

## Confidence
- **High confidence:** The mechanism by which ADOPS avoids the exponential penalty explosion present in PBIM/GRM (Mechanism 2) is well-supported by the mathematical analysis and the specific citation of the explosive term $\gamma^{-N}$. The empirical demonstration of ADOPS outperforming baseline RND in Montezuma's Revenge is also high confidence, as it is directly measured and statistically validated.
- **Medium confidence:** The theoretical claim that ADOPS accommodates a wider set of optimality-preserving shaping functions than prior methods is supported by the proof structure, but the proof relies on strong assumptions about critic convergence that are not empirically validated across a wide range of MDPs.
- **Low confidence:** The specific advantage of the ADOPES variant (Mechanism 3) is based on a single empirical result. The claim that a linear schedule for $\zeta$ is optimal is not substantiated, and the method could be sensitive to this choice.

## Next Checks
1. **Critic Robustness Test:** Systematically vary the learning rate and architecture of the critic networks and measure the resulting performance of ADOPS. This will quantify how sensitive the method is to critic quality, a critical assumption in the paper.
2. **Generalization Across Tasks:** Evaluate ADOPS on a suite of sparse-reward environments beyond Montezuma's Revenge (e.g., other Atari games, control tasks) to verify that the improvements are not specific to a single domain.
3. **Theoretical Assumption Validation:** Design a synthetic MDP where the optimal policy is known, and empirically measure the error in the critic estimates during training. Compare the agent's learned policy to the true optimal policy to see if violations of Assumption 3 lead to suboptimal behavior.