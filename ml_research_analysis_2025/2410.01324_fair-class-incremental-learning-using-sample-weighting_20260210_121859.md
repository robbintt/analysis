---
ver: rpa2
title: Fair Class-Incremental Learning using Sample Weighting
arxiv_id: '2410.01324'
source_url: https://arxiv.org/abs/2410.01324
tags:
- fairness
- task
- learning
- accuracy
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses unfair catastrophic forgetting in class-incremental
  learning, where certain sensitive groups (classes or subgroups defined by attributes
  like gender/race) experience disproportionate accuracy loss when learning new classes.
  The authors propose Fairness-aware Sample Weighting (FSW), a method that theoretically
  analyzes how negative inner products between gradient vectors cause unfairness,
  and then optimizes sample weights in the current task to mitigate this.
---

# Fair Class-Incremental Learning using Sample Weighting

## Quick Facts
- arXiv ID: 2410.01324
- Source URL: https://arxiv.org/abs/2410.01324
- Authors: Jaeyoung Park; Minsu Kim; Steven Euijong Whang
- Reference count: 40
- Key outcome: Proposes Fairness-aware Sample Weighting (FSW) to mitigate unfair catastrophic forgetting in class-incremental learning by optimizing sample weights, achieving better accuracy-fairness tradeoffs than state-of-the-art methods on multiple datasets.

## Executive Summary
This paper addresses unfair catastrophic forgetting in class-incremental learning, where certain sensitive groups (classes or subgroups defined by attributes like gender/race) experience disproportionate accuracy loss when learning new classes. The authors propose Fairness-aware Sample Weighting (FSW), a method that theoretically analyzes how negative inner products between gradient vectors cause unfairness, and then optimizes sample weights in the current task to mitigate this. FSW formulates the problem as a linear program to minimize both overall loss and disparity across sensitive groups. Experiments show FSW achieves better accuracy-fairness tradeoffs than state-of-the-art methods on multiple datasets (MNIST, FMNIST, Biased MNIST, DRUG, BiasBios), improving fairness (EER, EO, DP disparity) while maintaining competitive accuracy.

## Method Summary
FSW addresses unfairness in class-incremental learning by analyzing how negative inner products between gradient vectors cause disproportionate forgetting across sensitive groups. The method formulates sample weighting as a linear program that minimizes both overall loss and disparity across groups in the current task. The optimization considers two scenarios: when forgetting is measured by changes in logits (linear case) and when measured by changes in final predictions (nonlinear case). The linear program uses group-wise weights that are optimized to balance performance across sensitive groups while maintaining overall accuracy. FSW is implemented on top of existing class-incremental learning methods (iCaRL, LwM, BiC, Mnemonics) and can be applied as either a standalone method or post-processing technique.

## Key Results
- FSW achieves 0.032 EER disparity on MNIST compared to 0.048 for iCaRL
- On Biased MNIST, FSW achieves 0.078 DP disparity compared to 0.143 for iCaRL
- FSW maintains competitive accuracy while improving fairness metrics (EER, EO, DP disparity) across all tested datasets

## Why This Works (Mechanism)
FSW works by optimizing sample weights to reduce negative inner products between gradient vectors of different classes. When gradients have negative inner products, they push the decision boundary in opposite directions, causing one class to be forgotten while another is remembered. By assigning higher weights to samples that counteract this effect, FSW balances the gradient updates across sensitive groups. The linear programming formulation explicitly minimizes both the overall loss and the disparity between groups, ensuring that the optimization considers both accuracy and fairness objectives simultaneously.

## Foundational Learning
- **Class-incremental learning**: Learning new classes sequentially without access to previous data; needed to understand the incremental learning context where forgetting occurs
- **Catastrophic forgetting**: Dramatic performance degradation on previously learned tasks when learning new tasks; needed to understand the core problem FSW addresses
- **Fairness metrics (EO, EER, DP)**: Statistical measures of group fairness in classification; needed to quantify and compare fairness across methods
- **Linear programming**: Optimization technique for solving linear objective functions with linear constraints; needed to understand how FSW formulates the weight optimization problem
- **Gradient inner products**: Measure of alignment between gradient directions; needed to understand the theoretical basis for how negative inner products cause unfairness

## Architecture Onboarding

**Component Map**
FSW -> Linear Program (weights) -> Loss + Disparity Minimization -> Weighted Loss Function -> Model Update

**Critical Path**
Sample selection → Gradient computation → Inner product analysis → Weight optimization (LP) → Weighted loss computation → Model parameter update

**Design Tradeoffs**
- Computational efficiency vs. fairness improvement: LP formulation balances speed with fairness gains
- Accuracy vs. fairness: LP explicitly optimizes both objectives rather than treating them sequentially
- Simplicity vs. complexity: FSW adds minimal overhead to existing CIL methods while providing significant fairness benefits

**Failure Signatures**
- Infeasible LP solutions when groups are too imbalanced or constraints are too strict
- Poor fairness improvements when gradient inner products are consistently positive across all samples
- Computational bottlenecks when number of samples or groups becomes very large

**First Experiments**
1. Verify that FSW reduces negative inner products between gradient vectors of different classes
2. Test LP feasibility on simple 2-class problems with known gradient alignments
3. Compare fairness metrics before and after FSW optimization on balanced datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can FSW be extended to handle multiple sensitive attributes simultaneously while preventing "fairness gerrymandering" and managing optimization complexity?
- Basis: Appendix D.1 states future work includes generalizing to multiple attributes and notes that defining groups as combinations helps prevent gerrymandering but increases optimization complexity.
- Why unresolved: The authors propose a mathematical extension but note that the "design may increase the complexity of optimization" and require advanced loss functions.
- What evidence would resolve it: An implementation of FSW on datasets with intersectional groups (e.g., race and gender) showing convergence and fairness without exponential computational cost.

### Open Question 2
- Question: Can clustering-based weight assignment or first-order optimization methods effectively reduce the quadratic computational complexity of FSW for extremely large datasets?
- Basis: Appendix D.2 identifies reducing computational overhead as future work and suggests clustering similar samples or using gradient-based methods instead of simplex-based LP solvers.
- Why unresolved: The current linear programming approach uses CPLEX, which has quadratic complexity in practice; the proposed alternatives (clustering, first-order methods) are hypothesized but not tested.
- What evidence would resolve it: Experiments on datasets significantly larger than those in the paper, comparing the runtime and performance of simplex vs. clustering/gradient-based solvers.

### Open Question 3
- Question: How can FSW be adapted to maintain fairness under concept drift, specifically by integrating with robust training methods for noisy labels?
- Basis: Appendix D.3 discusses "possible strategies of FSW when data drift occurs" and specifically suggests combining FSW with robust training for concept drift scenarios involving noisy labels.
- Why unresolved: The paper currently assumes static data distributions within tasks; concept drift introduces decision boundary shifts that FSW is not explicitly designed to handle alone.
- What evidence would resolve it: Experiments in a continual learning environment with synthetic or natural concept drift, evaluating FSW combined with a robust loss function against standard FSW.

## Limitations

- Theoretical analysis assumes linear models or focuses on gradient properties that may not fully capture complex interactions in deep neural networks
- Linear programming formulation may become intractable for very large-scale datasets or extremely long incremental learning sequences
- Focus on catastrophic forgetting as the primary source of unfairness may overlook other potential sources of bias in incremental learning scenarios

## Confidence

- **High confidence**: Empirical effectiveness across multiple datasets and fairness metrics, computational efficiency of the proposed method, general validity of addressing unfairness in incremental learning
- **Medium confidence**: Theoretical analysis of how negative inner products between gradient vectors cause unfairness, particularly when extrapolated to complex deep learning architectures
- **Low confidence**: Scalability of the linear programming approach to extremely large-scale problems and comprehensive coverage of all potential sources of unfairness in incremental learning

## Next Checks

1. Test FSW on larger-scale datasets (e.g., ImageNet) and longer incremental learning sequences to evaluate scalability and long-term effectiveness

2. Conduct ablation studies to isolate the contribution of different components of FSW (e.g., loss minimization vs. disparity minimization in the linear program) to overall performance

3. Extend experiments to other types of incremental learning scenarios beyond class-incremental learning (e.g., domain-incremental learning) to assess generalizability of the approach