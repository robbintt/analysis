---
ver: rpa2
title: Code-enabled language models can outperform reasoning models on diverse tasks
arxiv_id: '2510.20909'
source_url: https://arxiv.org/abs/2510.20909
tags:
- reasoning
- code
- codeadapt
- output
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeAdapt combines code execution with lightweight in-context learning
  to match or exceed expensive reasoning models across diverse tasks. Using iterative
  code reasoning and just five training examples per task, it enables instruct language
  models to outperform corresponding reasoning models by up to 22.9% per model and
  35.7% per task, while using 10-81% fewer tokens and 16-47% less computation time.
---

# Code-enabled language models can outperform reasoning models on diverse tasks

## Quick Facts
- arXiv ID: 2510.20909
- Source URL: https://arxiv.org/abs/2510.20909
- Authors: Cedegao E. Zhang; Cédric Colas; Gabriel Poesia; Joshua B. Tenenbaum; Jacob Andreas
- Reference count: 40
- CodeAdapt matches or exceeds reasoning models with 10-81% fewer tokens and 16-47% less computation time

## Executive Summary
CodeAdapt combines code execution with lightweight in-context learning to match or exceed expensive reasoning models across diverse tasks. Using iterative code reasoning and just five training examples per task, it enables instruct language models to outperform corresponding reasoning models by up to 22.9% per model and 35.7% per task. The system demonstrates superior efficiency, requiring significantly fewer tokens and less computation time while maintaining or improving accuracy across instruction following, language processing, and formal reasoning domains.

## Method Summary
CodeAdapt is a hybrid framework that combines code execution (CodeAct) with generalization-from-latent-experiments (GFL) few-shot learning. The system uses instruct language models in a multi-turn loop with Python execution, subject to resource limits (16k tokens, 4 minutes, 10 turns). The GFL component generates multiple code traces per training problem, tests them on remaining examples, and selects the best-performing examples for 2-shot evaluation. The approach is evaluated across 8 tasks spanning three domains using models including DeepSeek V3, Gemini 2.0 Flash, Qwen 3 30B, and Qwen 2.5 Coder 32B.

## Key Results
- CodeAdapt matches or exceeds reasoning models by up to 22.9% per model and 35.7% per task
- Uses 10-81% fewer tokens than reasoning models across benchmarks
- Requires 16-47% less computation time than reasoning models
- Achieves these results with only 5 training examples per task

## Why This Works (Mechanism)
CodeAdapt works by combining the precision of code execution with the flexibility of in-context learning. The iterative code reasoning allows models to explore solution spaces systematically while the few-shot generalization enables adaptation to new problems. The resource management constraints prevent infinite loops while encouraging efficient reasoning strategies. The hybrid approach leverages the strengths of both code-based computation and natural language reasoning.

## Foundational Learning
- **Code execution sandbox**: A restricted Python environment with preloaded libraries (numpy, scipy, sympy) needed to safely execute generated code without internet access. Quick check: Verify libraries are accessible and resource limits are enforced.
- **Generalization-from-latent-experiments (GFL)**: A few-shot learning method that generates multiple candidate solutions and selects the most generalizable ones. Why needed: Enables adaptation with minimal training data. Quick check: Confirm 6 traces are generated and 3 are tested for cross-validation.
- **Resource-aware prompting**: System prompts that communicate token and time budgets to the model. Why needed: Prevents infinite loops and ensures efficient computation. Quick check: Verify "Remaining budget" feedback appears when thresholds are approached.
- **Task-specific verifiers**: Custom evaluation functions that check solution correctness for each benchmark. Why needed: Enables automated evaluation across diverse task types. Quick check: Test verifier returns correct scores for known solutions.
- **Iterative reasoning loop**: Multi-turn interaction between model and Python interpreter. Why needed: Allows complex problems to be broken down into manageable steps. Quick check: Monitor that solutions typically require multiple turns.
- **Cross-validation selection**: Process of selecting best training examples based on generalization performance. Why needed: Ensures few-shot examples are representative and effective. Quick check: Verify top 2 examples are selected from 3 candidates.

## Architecture Onboarding

### Component Map
Model -> Code Sandbox -> Verifier -> Feedback -> Model (iterative loop)

### Critical Path
Problem input → Code generation → Execution → Error handling → Solution refinement → Output

### Design Tradeoffs
- **Resource limits**: Tight constraints improve efficiency but may prevent finding solutions to very complex problems
- **Library selection**: Pre-loaded libraries enable mathematical reasoning but limit domain-specific capabilities
- **Few-shot vs. fine-tuning**: Minimal training data reduces overhead but may limit performance on very difficult tasks
- **Iterative vs. one-shot**: Multiple turns enable complex reasoning but increase token usage

### Failure Signatures
- Memory exhaustion from brute-force enumeration (detected via <error> tags)
- Turn budget exhaustion before answer returned (detected via "Remaining budget" feedback)
- Incorrect solutions from poor example selection in GFL
- Infinite loops in code generation (mitigated by resource limits)

### Exactly 3 First Experiments
1. Test basic code execution with simple arithmetic problems to verify sandbox functionality
2. Run GFL with a single training problem to validate the generalization mechanism
3. Compare token usage between CodeAdapt and a reasoning model on a simple task

## Open Questions the Paper Calls Out
- Can reinforcement learning (RL) post-training applied to CodeAdapt-style agents yield synergistic performance gains over RL alone?
- Does extending the CodeAdapt code module with external tools (e.g., web search) improve performance on knowledge-intensive tasks?
- Can explicit training for resource management further reduce the inference cost of CodeAdapt agents?

## Limitations
- Performance depends on correct implementation of task-specific verifiers not fully specified in paper
- Results may vary with different API providers or model versions than those tested
- Limited to tasks solvable within the 16k token and 4-minute constraints
- Relies on appropriate selection of training examples for GFL effectiveness

## Confidence
- **High confidence**: Core methodology combining code execution with in-context learning is clearly described and reproducible
- **Medium confidence**: Accuracy improvements over reasoning models depend on correct verifier implementation and example selection
- **Medium confidence**: Generalization claims require careful validation across all task implementations

## Next Checks
1. Implement and validate all task-specific verifiers against original test sets, particularly for Creativity and constraint-based tasks
2. Run ablation studies comparing CodeAdapt performance with and without GFL to isolate few-shot learning contribution
3. Test framework with alternative reasoning models (o1-mini, DeepSeek-R1) to verify robustness of performance claims