---
ver: rpa2
title: 'RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking
  in Peer Review Systems'
arxiv_id: '2601.19637'
source_url: https://arxiv.org/abs/2601.19637
tags:
- reviewer
- expertise
- assignment
- data
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RATE, a reviewer profiling and ranking framework
  for peer review assignment. The method addresses outdated benchmarks and misalignment
  between training objectives and reviewer expertise matching by synthesizing reviewer
  publication histories into keyword-based profiles using LLMs, then fine-tuning an
  embedding model with weak supervision from heuristic retrieval signals.
---

# RATE: Reviewer Profiling and Annotation-free Training for Expertise Ranking in Peer Review Systems

## Quick Facts
- arXiv ID: 2601.19637
- Source URL: https://arxiv.org/abs/2601.19637
- Reference count: 40
- Primary result: Achieves 77.41% average precision on LR-Bench, outperforming SPECTER2 PRX (75.17%) and LLM baselines (60%)

## Executive Summary
RATE introduces an annotation-free framework for reviewer expertise ranking in peer review systems. The method synthesizes reviewer publication histories into keyword-based profiles using LLMs, then fine-tunes an embedding model with weak supervision from heuristic retrieval signals. By addressing outdated benchmarks and misalignment between training objectives and reviewer expertise matching, RATE achieves state-of-the-art performance on contemporary AI/NLP manuscripts. Human evaluation confirms practical utility, with RATE winning 42-44% of pairwise comparisons against baselines.

## Method Summary
RATE addresses reviewer-paper expertise ranking through a two-stage approach: profile generation and embedding fine-tuning. For profiling, LLM extracts keywords from each reviewer's recent publications, aggregates with frequencies, and formats as structured text. The embedding model (Qwen3-Embedding) is fine-tuned using LoRA on pseudo-labeled triplets derived from BM25 scores between query papers and reviewer profiles. The training uses dual-view preference optimization with pairwise ranking loss and cross-entropy regularization. Evaluation on LR-Bench (1,055 samples) and CMU gold-standard shows 77.41% average precision, outperforming strong baselines including SPECTER2 PRX and direct LLM scoring.

## Key Results
- Achieves 77.41% average precision on LR-Bench benchmark
- Outperforms SPECTER2 PRX by 2.24% absolute improvement
- Direct LLM scoring baseline achieves only 60% precision
- Human evaluation shows 42-44% win-rate against baselines

## Why This Works (Mechanism)
RATE works by aligning the training objective with the actual task of reviewer expertise matching. Traditional methods often use mismatched objectives or outdated benchmarks. By synthesizing reviewer profiles from publication histories and using heuristic retrieval signals for weak supervision, RATE creates relevant training signals without requiring manual annotations. The dual-view preference optimization (paper-centric and reviewer-centric triplets) provides comprehensive supervision, while the keyword-based profiling captures research focus more effectively than raw publication lists.

## Foundational Learning
- **BM25 retrieval**: Essential for generating pseudo-labels when no expert annotations exist; quick check: verify BM25 scores show clear separation between relevant and non-relevant reviewers
- **LoRA fine-tuning**: Enables efficient adaptation of large embedding models to domain-specific ranking tasks; quick check: monitor validation loss during fine-tuning to detect overfitting
- **Dual-view preference optimization**: Combines paper-centric and reviewer-centric perspectives for robust training signals; quick check: ensure equal representation of both view types in training batches
- **Keyword extraction from publications**: Transforms unstructured publication lists into structured expertise indicators; quick check: validate that extracted keywords represent actual research topics
- **Pairwise ranking loss**: Optimizes relative ordering rather than absolute relevance scores; quick check: examine precision@k metrics to confirm improved ranking quality
- **Profile concatenation**: Linearizes keyword lists with fixed prefixes for consistent model input; quick check: verify profile formatting consistency across all reviewers

## Architecture Onboarding

**Component Map**: Paper+Abstract -> BM25 -> Triplets -> LoRA Fine-tuning -> Embedding Model -> Rankings

**Critical Path**: Profile Generation → BM25 Triplet Construction → LoRA Fine-tuning → Evaluation

**Design Tradeoffs**: The method trades annotation cost for potential noise in pseudo-labels, but achieves better alignment with the ranking task. Keyword-based profiling is simpler than complex feature engineering but may miss nuanced expertise signals.

**Failure Signatures**: 
- Cold-start reviewers with sparse publication histories yield noisy BM25 signals
- Inconsistent keyword extraction across different LLMs creates profile quality variance
- Direct LLM zero-shot scoring underperforms (≈60% precision), confirming need for fine-tuning

**First Experiments**:
1. Verify BM25 score separation between top-1 positive and hard negatives across reviewers
2. Compare keyword distribution and profile quality between Qwen3-Max and GLM-4.6 extractions
3. Test pairwise precision improvement after LoRA fine-tuning versus pre-trained embedding baseline

## Open Questions the Paper Calls Out
None

## Limitations
- Profile generation consistency varies between different LLMs (Qwen3-Max vs GLM-4.6) with unclear prompt variations
- BM25 configuration details (tokenization, k1/b parameters, negative selection rules) are not fully specified
- Test set of 1,055 samples may limit generalizability of state-of-the-art claims

## Confidence
**High Confidence**: Experimental methodology clearly specified with dataset construction, model architecture, and evaluation metrics. Performance gaps between RATE and baselines are consistent with methodology.

**Medium Confidence**: Human evaluation methodology well-described but limited scope (7,400 comparisons) and potential selection bias introduce uncertainty about real-world applicability.

**Low Confidence**: Exact reproducibility of profile generation uncertain due to unspecified prompt variations between LLMs and unclear BM25 parameter settings.

## Next Checks
1. Reproduce reviewer profiles using both Qwen3-Max and GLM-4.6 with provided prompt, then analyze keyword distribution and BM25 score separation to verify profiles contain sufficient signal (minimum 10 unique keywords per reviewer with clear score separation).

2. Systematically vary BM25 parameters (k1, b, tokenization) and negative selection thresholds to determine their impact on pseudo-label quality and final ranking performance, ensuring reported improvements are robust to configuration choices.

3. Directly compare RATE against simple BM25 ranking and zero-shot LLM scoring on a held-out validation set to confirm the reported 60% precision for direct LLM scoring represents a true baseline rather than implementation artifacts.