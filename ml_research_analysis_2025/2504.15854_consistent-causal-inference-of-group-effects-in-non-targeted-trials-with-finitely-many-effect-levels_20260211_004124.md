---
ver: rpa2
title: Consistent Causal Inference of Group Effects in Non-Targeted Trials with Finitely
  Many Effect Levels
arxiv_id: '2504.15854'
source_url: https://arxiv.org/abs/2504.15854
tags:
- effect
- effects
- level
- algorithm
- subpopulations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of estimating treatment effects
  on specific subpopulations in non-targeted randomized controlled trials, where multiple
  effect levels are present in the treated group. A key problem is that traditional
  methods like ATT (average treatment effect for the treated) become biased when the
  treated population includes heterogeneous subgroups with different treatment effects.
---

# Consistent Causal Inference of Group Effects in Non-Targeted Trials with Finitely Many Effect Levels

## Quick Facts
- arXiv ID: 2504.15854
- Source URL: https://arxiv.org/abs/2504.15854
- Reference count: 17
- PCM outperforms state-of-the-art methods by more than 10x in accuracy for estimating per-subject expected effects

## Executive Summary
This paper addresses the challenge of estimating treatment effects on specific subpopulations in non-targeted randomized controlled trials where multiple effect levels exist. Traditional methods like ATT become biased when treated populations include heterogeneous subgroups with different treatment effects. The authors propose PCM (Pre-Cluster and Merge), a nonparametric approach that first clusters subjects based on their features, then merges clusters with similar treatment effects to identify subpopulations. The method assumes unbiased counterfactual estimation is available and proves asymptotic consistency under mild technical conditions.

## Method Summary
PCM uses a two-stage approach: first pre-clustering subjects using K-means or box-clustering (K≈√n) to denoise individual treatment effect estimates through in-cluster averaging, then optimally merging clusters based on treatment effect similarity using a 1D clustering algorithm with a threshold τ(n) = log n/n^(ρ/2d). The method proves asymptotic consistency for identifying the correct number of effect levels and their values under conditions of well-separated effects (κ separation) and unbiased counterfactual estimation. An optional EM-style smoothing step refines individual effect assignments by averaging in local hypercubes.

## Key Results
- PCM achieves mean absolute errors of 0.036±0.13 on synthetic data with three effect levels, compared to 0.34±0.47 for X-Learner and 4.01±1.25 for Bayes Optimal using raw individual treatment effects
- The method successfully extracts correct subpopulations while competing methods fail even when given additional information about the number of effect levels
- K-means pre-clustering performs slightly better than box-clustering in practice (0.065 vs 0.078 error at 2M points), though theoretical guarantees only cover box-clustering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-clustering subjects by features denoises individual treatment effect estimates through in-cluster averaging.
- Mechanism: Each cluster acts as a mini "targeted trial" where averaging ITEs across similar subjects reduces variance. With ~√n points per cluster, cluster-level ATTs concentrate around true expected effects (Lemma 3.2 shows max deviation ≤ 2√(log n/γδ√n) with high probability).
- Core assumption: Clusters are homogeneous—contain subjects from primarily one effect level (Assumption A6: boundary has small measure).
- Evidence anchors:
  - [section] "The clusters in the pre-clustering step play two roles. The first is to denoise individual effects using in-cluster averaging."
  - [section] Lemma 3.1 guarantees ≥ δ√n/2 points per cluster with high probability.
  - [corpus] Weak direct support; neighbor papers address HTE estimation but not this specific denoising-via-clustering approach.
- Break condition: High-dimensional feature spaces where clusters become sparse (curse of dimensionality: convergence rate O(n⁻¹/²ᵈ)).

### Mechanism 2
- Claim: Optimal 1-dimensional clustering of cluster-ATTs identifies both the correct number of effect levels and their values.
- Mechanism: When effect levels are separated by ≥κ (Assumption A2), cluster-ATTs from different levels form distinct groups. Using threshold τ(n) = log n/n^(ρ/2d), the algorithm distinguishes between ℓ−1 clusters (error ≥ constant) and ℓ clusters (error → 0 asymptotically).
- Core assumption: Effect levels are well-separated: |μc − μc'| ≥ κ for c ≠ c'.
- Evidence anchors:
  - [section] Lemma 3.4 proves err(ℓ−1) ≥ (β/Δ − αε^ρ)(κ/2 − √(log n/γδ√n))² while err(ℓ) ≤ O(ε^ρ).
  - [section] Theorem 1.1(1) guarantees correct number of levels identified with probability 1−o(1).
  - [corpus] No direct corpus support for this specific threshold-based level detection.
- Break condition: Effect levels separated by less than cluster-ATT noise; overlapping effect distributions.

### Mechanism 3
- Claim: Smoothing individual ITEs via ε-hypercube averaging before final level assignment improves accuracy.
- Mechanism: After identifying level centers θ₀,...,θ_{ℓ−1}, each subject's effect is smoothed by averaging ITEs in their local hypercube, then assigned to the nearest center. This EM-style update refines boundaries.
- Core assumption: Sufficient density for local averaging (√n points per hypercube).
- Evidence anchors:
  - [section] "For each point, consider the ε-hypercube centered on that point... Assign each point to the center θc that best matches its hypercube-'smoothed' ITE."
  - [section] Experiments show PCM (with E-M step) achieves 0.036±0.13 error vs 0.078±0.214 for box-only.
  - [corpus] Not addressed in neighbor papers.
- Break condition: Sparse regions where hypercubes contain few points; irregular decision boundaries.

## Foundational Learning

- Concept: **Rubin-Neyman Potential Outcomes Framework**
  - Why needed here: The paper formalizes treatment effects using potential outcomes (v if treated, v̄ if not), with the observed outcome being one of these based on treatment assignment.
  - Quick check question: Can you explain why we never observe both potential outcomes for the same subject?

- Concept: **Average Treatment Effect on Treated (ATT) vs. Average Treatment Effect (ATE)**
  - Why needed here: The paper shows ATT is biased in non-targeted trials because E[ATT] = P[sick]×EFF₁ + P[healthy]×EFF₀ when treatment effects are heterogeneous.
  - Quick check question: In a trial where 90% of treated subjects are "healthy" and treatment helps "sick" subjects but harms "healthy" ones, why does ATT fail to capture the effect on the sick?

- Concept: **Strong Ignorability (Unconfoundedness)**
  - Why needed here: Assumption A4 requires unbiased counterfactual estimation, which depends on strong ignorable treatment assignment—propensity strictly between 0 and 1 given features, and outcomes depend only on features.
  - Quick check question: If wealthy subjects self-select into treatment and wealth also affects outcomes, what assumption is violated?

## Architecture Onboarding

- Component map: Input: {subjects (xᵢ, tᵢ, yᵢ, ŷ̄ᵢ)} → [Pre-clusterer: K-means or box-clustering, K≈√n] → [Cluster-ATT computer] → [1D Optimal Clusterer: dynamic programming] → [Level Assigner with smoothing] → Output: {effect levels μ̂₀,...,μ̂_{ℓ̂−1}, subpopulations X₀,...,X_{ℓ̂−1}}

- Critical path: Step 3 (merge) is the algorithmic core—correctly identifying ℓ and partitioning clusters determines all downstream quality. The threshold τ(n) = log n/n^(1/2d) is the key hyperparameter.

- Design tradeoffs:
  - Box-clustering vs K-means: Box has theoretical guarantees; K-means works better in practice (experiments show K-means slightly outperforms box at 2M points: 0.065 vs 0.078 error).
  - Runtime vs accuracy: Direct smoothed-ITE clustering is O(n²ℓ) vs O(nℓ + n√n) for the two-stage approach.
  - Number of pre-clusters K: Too few → heterogeneous clusters; too many → insufficient denoising. Paper uses K ≈ √n.

- Failure signatures:
  - Single merged effect level when ℓ̂ = 1: effect separation κ too small relative to noise.
  - Spurious levels (ℓ̂ > ℓ): threshold τ(n) too tight; check if clustering error drops smoothly or has gaps.
  - Highly imbalanced subpopulations: may indicate boundary issues (high N_impure).

- First 3 experiments:
  1. **Sanity check on synthetic data**: Replicate Figure 1 setup (3 levels, effects 0/1/2, Gaussian noise σ=5). Verify PCM recovers correct ℓ=3 and effects within ±0.1.
  2. **Ablation on effect separation**: Vary κ (minimum separation between effect levels) from 0.5 to 2.0. Identify the κ threshold below which PCM fails to distinguish levels.
  3. **Sensitivity to counterfactual quality**: Inject bias into ŷ̄ estimates (e.g., add constant offset). Measure how effect estimation degrades—establishes tolerance for counterfactual estimator quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence rate of PCM be optimized beyond O(n^(-1/2d)), and what algorithmic modifications would achieve faster convergence?
- Basis in paper: [explicit] "In our analysis, we did not attempt to optimize the rate of convergence. Optimizing this rate could lead to improved algorithms."
- Why unresolved: The authors prove consistency but do not analyze whether the convergence rate is optimal or improvable through different clustering strategies or thresholds.
- What evidence would resolve it: A lower bound analysis on convergence rates for this problem class, or demonstration of modified PCM variants with provably faster rates.

### Open Question 2
- Question: Can theoretical consistency guarantees be extended to K-means pre-clustering, and under what additional assumptions?
- Basis in paper: [explicit] "We also show K-means pre-clustering, for which we did not prove any theoretical guarantees."
- Why unresolved: The theoretical analysis relies on box-clustering properties (A7) that do not directly transfer to K-means, though experiments suggest K-means performs comparably.
- What evidence would resolve it: A proof showing K-means pre-clustering satisfies conditions analogous to Lemmas 3.1-3.6, or identification of necessary regularity conditions.

### Open Question 3
- Question: How does PCM perform when the number of effect levels ℓ grows with sample size rather than remaining fixed?
- Basis in paper: [inferred] The paper assumes "finitely many effect levels" throughout, but real-world heterogeneity may involve effect levels that scale with population complexity.
- Why unresolved: All theoretical guarantees assume fixed ℓ; the merging step's error threshold τ(n) = log n/n^(ρ/2d) may not separate levels adequately when ℓ increases.
- What evidence would resolve it: Analysis of PCM behavior under data-generating processes where ℓ = ℓ(n), with modified threshold selection criteria.

## Limitations
- The method requires unbiased counterfactual estimates and well-separated effect levels (κ separation)
- High-dimensional features may cause cluster sparsity due to the curse of dimensionality
- Boundary effects could create impure clusters that reduce estimation accuracy
- The approach is designed specifically for non-targeted trials with homogeneous subpopulations, not continuous effect distributions

## Confidence
- High confidence in the denoising mechanism (Mechanism 1) - the theoretical guarantees and experimental results are robust
- Medium confidence in level detection (Mechanism 2) - while Lemma 3.4 provides strong bounds, corpus support is lacking and the threshold τ(n) remains heuristic
- Medium confidence in smoothing step (Mechanism 3) - experimental improvement is clear, but theoretical justification is limited

## Next Checks
1. Test PCM on datasets with effect levels separated by less than κ to identify the practical separation threshold
2. Evaluate sensitivity to counterfactual estimator bias by injecting controlled errors into ŷ̄ estimates
3. Benchmark runtime and accuracy against direct smoothed-ITE clustering on large-scale datasets (n > 1M)