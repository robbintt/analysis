---
ver: rpa2
title: 'HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution
  3D Reconstruction'
arxiv_id: '2506.14229'
source_url: https://arxiv.org/abs/2506.14229
tags:
- gaussian
- reconstruction
- block
- data
- high-resolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the memory scalability bottleneck of 3D Gaussian
  Splatting in high-resolution 3D reconstruction. The proposed HRGS framework introduces
  a hierarchical block-level optimization strategy that partitions scenes into spatially
  adjacent blocks, each refined using high-resolution data while guided by a global
  coarse Gaussian prior.
---

# HRGS: Hierarchical Gaussian Splatting for Memory-Efficient High-Resolution 3D Reconstruction

## Quick Facts
- **arXiv ID:** 2506.14229
- **Source URL:** https://arxiv.org/abs/2506.14229
- **Authors:** Changbai Li, Haodong Zhu, Hanlin Chen, Juan Zhang, Tongfei Chen, Shuo Yang, Shuwei Shao, Wenhao Dong, Baochang Zhang
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art high-resolution 3D reconstruction with PSNR 27.91 dB and F1-score 0.45 while significantly reducing GPU memory usage

## Executive Summary
This paper addresses the memory scalability bottleneck of 3D Gaussian Splatting (3DGS) in high-resolution 3D reconstruction by introducing a hierarchical block-level optimization framework. The method partitions scenes into spatially adjacent blocks, each refined using high-resolution data while guided by a global coarse Gaussian prior. A key innovation is the Importance-Driven Gaussian Pruning (IDGP) strategy that evaluates and removes low-contribution Gaussian primitives to accelerate convergence and reduce memory overhead. The approach also incorporates normal priors from pretrained models to enhance surface reconstruction quality, achieving state-of-the-art performance on Mip-NeRF 360 and Tanks and Temples datasets while significantly reducing model size and GPU memory usage compared to baseline methods.

## Method Summary
The HRGS framework operates in three main stages: first, a coarse global Gaussian representation is trained on low-resolution data to establish baseline geometry; second, the unbounded scene coordinates are contracted into a bounded cubic space and subdivided using a uniform grid, with training views assigned to blocks based on their contribution to reconstruction quality; third, each block is refined independently with high-resolution data in parallel, incorporating the IDGP strategy that prunes low-importance Gaussians at specific iterations. The method uses normal priors from pretrained models (DSINE for outdoor, GeoWizard for indoor) to enhance surface reconstruction quality. The hierarchical approach allows for high-resolution reconstruction on hardware with limited VRAM by reducing peak memory usage through spatial partitioning and Gaussian pruning while maintaining geometric continuity through the global coarse prior.

## Key Results
- Achieves state-of-the-art performance in high-resolution novel view synthesis with PSNR of 27.91 dB
- Improves surface reconstruction quality with F1-score of 0.45 on Tanks and Temples dataset
- Significantly reduces model size and GPU memory usage compared to baseline methods
- Demonstrates effectiveness on both Mip-NeRF 360 and Tanks and Temples datasets

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Memory Scaling via Block-Wise Optimization
- **Claim:** Partitioning a scene into spatially distinct blocks allows for high-resolution reconstruction on hardware with limited VRAM, provided a global prior maintains geometric continuity.
- **Mechanism:** The method trains a coarse 3D Gaussian representation on low-resolution data, contracts unbounded coordinates into a bounded cubic space, partitions this space into uniform blocks, and refines each block independently with high-resolution data while maintaining a global coarse Gaussian prior for geometric continuity.
- **Core assumption:** The low-resolution global prior contains sufficient geometric accuracy to guide local high-resolution refinement without causing drift or discontinuities between adjacent blocks.
- **Break condition:** If the scene contains highly non-uniform detail density, a uniform grid may result in imbalanced memory load across blocks, failing to mitigate OOM errors on the dense block.

### Mechanism 2: Importance-Driven Gaussian Pruning (IDGP)
- **Claim:** Dynamically pruning Gaussian primitives based on their contribution to the final render reduces memory overhead and computational redundancy without degrading visual fidelity.
- **Mechanism:** The method scores each Gaussian based on accumulated transmittance along intersecting rays, opacity, and volume, then removes the lowest 20% of Gaussians at specific iterations during block training to force remaining primitives to compactly represent the scene.
- **Core assumption:** Low-score Gaussians correspond to "floater" artifacts or redundant coverage rather than fine details necessary for high-frequency textures.
- **Break condition:** If the scoring metric fails to account for view-dependent effects (e.g., specularities visible only from rare angles), aggressive pruning may permanently erase these details.

### Mechanism 3: Contribution-Aware Data Partitioning
- **Claim:** Assigning training views to specific blocks based on their "contribution" minimizes interference from irrelevant data and enhances detail fidelity.
- **Mechanism:** The method uses SSIM loss to compare global renders against renders with specific blocks removed, assigning views to blocks when the SSIM drop exceeds a threshold or when camera positions fall within block bounds.
- **Core assumption:** A significant drop in SSIM when removing a block's Gaussians indicates that the view contains unique, critical information for that spatial region.
- **Break condition:** If the SSIM threshold is set too high, blocks may lack sufficient multi-view constraints to resolve geometry, leading to local overfitting or artifacts.

## Foundational Learning

- **Concept: 3D Gaussian Splatting (3DGS)**
  - **Why needed here:** HRGS is a modification of the standard 3DGS pipeline. You must understand that 3DGS represents a scene as explicit 3D Gaussians (position, scale, rotation, opacity, SH coefficients) rather than an implicit Neural Network (NeRF).
  - **Quick check question:** How does the memory footprint of 3DGS scale compared to a voxel grid as spatial resolution increases? (Answer: 3DGS scales with scene complexity/surface area, whereas voxels scale cubically with dimension).

- **Concept: Spatial Contraction (Mip-NeRF 360 contract)**
  - **Why needed here:** The paper normalizes unbounded outdoor scenes into a bounded cube for partitioning. Understanding this helps explain why they use a "non-linear mapping" for distant regions.
  - **Quick check question:** What happens to the geometry of distant background objects when they are contracted into a bounded unit cube?

- **Concept: Alpha Blending & Transmittance**
  - **Why needed here:** The IDGP mechanism relies on calculating "accumulated transmittance" to determine if a Gaussian is hidden behind others.
  - **Quick check question:** If a Gaussian has high opacity but its accumulated transmittance is 0, what does that imply about its "importance" for the current view?

## Architecture Onboarding

- **Component map:** Input (COLMAP points + Images) -> Stage 1 (Coarse Global 3DGS on low-res) -> Stage 2 (Partitioner with Contractor and Data Assigner) -> Stage 3 (Refinement with Local Trainers and IDGP Module) -> Stage 4 (Fusion via concatenation)
- **Critical path:** The Data Partitioning logic (Section 3.2, Eq 3-5). If the data assignment is misaligned, blocks will train on conflicting gradients or miss essential details.
- **Design tradeoffs:**
  - **Number of Blocks (N):** Increasing N reduces memory per block but increases boundary artifacts and complexity. Table 5 shows 4 blocks is optimal; 16 blocks drops PSNR significantly.
  - **Pruning Ratio (20%):** Aggressive pruning saves memory but risks losing fine details.
- **Failure signatures:**
  - **Seams/Discontinuities:** Visible lines where blocks meet. Likely caused by insufficient overlap in Data Partitioning or failure to use the Coarse Prior for initialization.
  - **Missing Geometry (Holes):** Over-aggressive IDGP or incorrect SSIM thresholding causing blocks to lack training data.
  - **Out of Memory (OOM) on Single Block:** Occurs if the contraction/partitioning results in a highly non-uniform density, overloading one specific block.
- **First 3 experiments:**
  1. **Baseline Resolution Verification:** Run standard 3DGS on a Mip-NeRF 360 scene at full resolution (approx 5K) to confirm the OOM error on a 24GB GPU.
  2. **Block Count Sweep:** Run HRGS on the "Bicycle" scene with 2, 4, and 8 blocks. Plot the trade-off curve: Total VRAM used vs. Global PSNR.
  3. **IDGP Ablation:** Run refinement on a single block with IDGP disabled vs. enabled. Measure the "Model Size (MB)" and "GPU Memory (GB)" to quantify the exact memory savings claimed in Table 6.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the hierarchical block-based optimization framework be extended to dynamic scenes while maintaining temporal consistency across frames and modeling motion across spatial partitions?
- **Basis in paper:** Section E (Limitations) explicitly identifies extending the framework to dynamic scenes and ensuring temporal consistency as an open problem and a focus for future work.
- **Why unresolved:** The current method relies on static scene assumptions for partitioning and Gaussian alignment; motion would break the spatial consistency required for seamless fusion between adjacent blocks.
- **What evidence would resolve it:** A modified HRGS pipeline demonstrating consistent rendering across time-steps on a dynamic high-resolution dataset (e.g., DyCheck) without inter-block temporal flickering.

### Open Question 2
- **Question:** How does the reconstruction quality degrade as the number of spatial blocks increases significantly beyond the tested range (e.g., >16 blocks) for extremely large-scale environments?
- **Basis in paper:** Table 5 shows performance drops when moving from 4 to 16 blocks, and the text notes "too many blocks may lead to imbalanced data distribution."
- **Why unresolved:** The current ablation only tests up to 16 blocks; for massive scenes requiring hundreds of blocks, the global coarse prior might become too sparse to effectively guide local refinements or manage boundary artifacts.
- **What evidence would resolve it:** Experiments applying HRGS to city-scale aerial datasets (like MatrixCity) with varying block granularities to determine the upper scaling limits of the hierarchical prior.

### Open Question 3
- **Question:** How sensitive is the surface reconstruction quality to errors in the monocular normal priors used during regularization, particularly in non-Lambertian or poorly lit regions?
- **Basis in paper:** Section 3.3 and Implementation Details state that normal priors from pretrained models (DSINE, GeoWizard) are incorporated to enhance surface quality.
- **Why unresolved:** Pretrained monocular estimators often struggle with specularities or shadows; the paper does not analyze failure cases where these external priors might introduce geometric artifacts rather than correcting them.
- **What evidence would resolve it:** A sensitivity analysis evaluating reconstruction F1-scores while systematically adding noise to the input normal maps or testing on scenes with dominant specular highlights.

## Limitations
- The uniform block partitioning strategy may struggle with scenes containing highly non-uniform detail density, potentially leading to memory imbalances when one block contains significantly more complex geometry than others.
- The IDGP pruning mechanism relies heavily on the assumption that low-score Gaussians correspond to redundant or artifactual primitives rather than essential fine details, which may not hold in scenes with view-dependent effects like specularities.
- The memory savings claimed depend on efficient implementation of computationally intensive operations like IDGP scoring, which may require custom CUDA kernels not detailed in the paper.

## Confidence
- **High Confidence:** The hierarchical memory scaling mechanism through block-wise optimization is well-supported by the experimental results in Table 5, showing clear improvements over baseline methods while maintaining geometric continuity through the global coarse prior.
- **Medium Confidence:** The effectiveness of the IDGP pruning strategy is supported by memory usage reductions in Table 6, but the claim that pruning low-score Gaussians doesn't degrade visual fidelity requires further validation across diverse scene types, particularly those with complex view-dependent effects.
- **Medium Confidence:** The contribution-aware data partitioning approach shows reasonable performance improvements, but the sensitivity to the SSIM threshold (ε=0.1) and potential for local overfitting in blocks with insufficient multi-view constraints suggests this mechanism needs more rigorous testing.

## Next Checks
1. **Non-uniform Density Stress Test:** Evaluate HRGS on a synthetic scene where 90% of geometric complexity is concentrated in 10% of the spatial volume to verify the uniform grid partitioning strategy doesn't fail under extreme non-uniform conditions.
2. **View-dependent Effect Preservation:** Test IDGP on a scene containing significant specular reflections or transparent surfaces that are only visible from specific viewing angles to determine if the scoring metric adequately preserves these view-dependent details.
3. **Boundary Artifact Quantification:** Conduct a systematic study varying the SSIM threshold (ε) from 0.05 to 0.2 on multiple scenes to quantify the relationship between data partitioning parameters and the visibility of seams at block boundaries.