---
ver: rpa2
title: 'PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language
  Models'
arxiv_id: '2509.10737'
source_url: https://arxiv.org/abs/2509.10737
tags:
- languages
- multilingual
- disinformation
- detection
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multilingual disinformation detection by comparing
  five transformer models (mBERT, XLM, XLM-R, RemBERT, mT5) on a novel dataset spanning
  25+ languages. A key contribution is PolyTruth Disinfo Corpus, pairing 30k+ fact-checked
  false claims with true statements for binary classification.
---

# PolyTruth: Multilingual Disinformation Detection using Transformer-Based Language Models

## Quick Facts
- arXiv ID: 2509.10737
- Source URL: https://arxiv.org/abs/2509.10737
- Reference count: 5
- Primary result: RemBERT achieves 87.1% accuracy on multilingual disinformation detection across 25+ languages

## Executive Summary
This paper addresses multilingual disinformation detection by comparing five transformer models (mBERT, XLM, XLM-R, RemBERT, mT5) on a novel dataset spanning 25+ languages. The key contribution is PolyTruth Disinfo Corpus, pairing 30k+ fact-checked false claims with true statements for binary classification. Models were fine-tuned under identical conditions, revealing that larger, better-pretrained transformers provide more robust cross-lingual generalization, especially for underrepresented languages. Results inform practical deployment choices balancing accuracy and resource constraints.

## Method Summary
The study fine-tuned five transformer models (mBERT, XLM-100, XLM-R, RemBERT, mT5) on PolyTruth Disinfo Corpus, a multilingual dataset of 60,486 statements (30,243 false claims from MindBugs Discovery dataset paired with synthetic true statements). Models used encoder-only architectures with linear classifiers on [CLS] tokens, except mT5 which used text-to-text generation ("true"/"false"). Training employed Adam optimizer, linear LR schedule with grid search, and early stopping on validation loss. The dataset was split 80/10/10 stratified by language and class to ensure all languages appeared in each split.

## Key Results
- RemBERT achieved highest overall accuracy at 87.1% on the test set
- XLM-R offered strong low-resource language performance with 74% F1
- mBERT lagged significantly on low-resource languages at 61% F1
- Performance gap between high-resource and low-resource languages ranged from 13 to 22 F1 points depending on model size
- Larger models with more extensive multilingual pretraining showed better cross-lingual transfer

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger model capacity and more extensive multilingual pretraining improve cross-lingual disinformation detection, particularly for low-resource languages.
- Mechanism: Models with deeper architectures (32 layers vs. 12) and larger hidden dimensions (1,152 vs. 768) trained on massive multilingual corpora (Common Crawl vs. Wikipedia-only) capture more diverse linguistic patterns and disinformation markers that transfer across language boundaries.
- Core assumption: Disinformation patterns share cross-lingual regularities that can be learned from one language and applied to another.
- Evidence anchors:
  - RemBERT achieved the highest overall accuracy (87.1%)... while XLM-R offered strong low-resource language performance (74% F1). Smaller models like mBERT lagged significantly on low-resource languages (61% F1), highlighting the importance of model capacity and multilingual pretraining.
  - XLM-R's pre-training on massive CommonCrawl data likely exposed it to more diverse linguistic patterns (including informal and misinformative content) compared to mBERT's Wikipedia-only corpus.

### Mechanism 2
- Claim: Cross-lingual transfer enables models to detect disinformation in low-resource languages by leveraging representations learned from high-resource languages.
- Mechanism: Multilingual transformers create a shared semantic space where similar concepts across languages map to proximate representations; when fine-tuned on mixed-language data, the model transfers learned disinformation indicators from well-represented languages to under-represented ones.
- Core assumption: The model's pretraining has sufficiently aligned cross-lingual representations for the target languages.
- Evidence anchors:
  - For low-resource languages... RemBERT and XLM-R still perform reasonably well: for example, on Greek, XLM-R achieves about 0.75 F1 and RemBERT 0.78, whereas mBERT's F1 plummets to around 0.60.
  - High-resource to low-resource F1 gap: mBERT drops 22 points (0.83→0.61), XLM-R drops 15 points (0.89→0.74), RemBERT drops 13 points (0.91→0.78).

### Mechanism 3
- Claim: Balanced vocabulary allocation across languages reduces tokenization fragmentation for low-resource languages, improving their classification performance.
- Mechanism: Models with rebalanced tokenizers (RemBERT's "embedding coupling" adjustments) avoid over-allocating vocabulary capacity to high-resource languages, preventing low-resource languages from being fragmented into excessive subword tokens that obscure semantic patterns.
- Core assumption: Excessive subword fragmentation degrades the model's ability to capture meaningful semantic units for classification.
- Evidence anchors:
  - RemBERT's continued improvement can be attributed to its larger depth and a tokenization scheme that better balances the representation of languages.
  - mBERT's poor showing suggests that its representations were not as language-agnostic; it likely suffered more from vocabulary issues (e.g., some languages might be split into too many subwords or poorly represented in the mBERT vocab).

## Foundational Learning

- Concept: **Transformer fine-tuning vs. feature extraction**
  - Why needed here: The paper compares five models under identical fine-tuning conditions; understanding how classification heads attach to pretrained encoders is essential for reproducing results.
  - Quick check question: Can you explain why adding a linear classifier on [CLS] tokens differs from using frozen embeddings with a separate classifier?

- Concept: **High-resource vs. low-resource language definitions**
  - Why needed here: Performance analysis is stratified by resource level; incorrect categorization would invalidate conclusions about cross-lingual transfer.
  - Quick check question: Given a dataset with 4,488 Russian samples and 300 Azerbaijani samples, which is low-resource and why does the distinction matter?

- Concept: **Macro F1 vs. accuracy for imbalanced multilingual evaluation**
  - Why needed here: The paper reports both metrics; macro F1 gives equal weight to each language regardless of sample count, revealing low-resource performance that accuracy masks.
  - Quick check question: If a model achieves 90% accuracy overall but 60% F1 on low-resource languages, what does this indicate about its deployment suitability?

## Architecture Onboarding

- Component map: Input preprocessing -> Tokenization -> Model backbone -> Classification head -> Training
- Critical path: 1. Data preparation -> 2. Stratified 80/10/10 split -> 3. Model selection based on resource constraints -> 4. Fine-tuning with hyperparameter search -> 5. Per-language F1 evaluation
- Design tradeoffs:
  - RemBERT (87.1% accuracy, 580M params): Best performance, highest compute cost (~3h on 2×A100)
  - XLM-R (85.4% accuracy, 270M params): Strong low-resource performance, moderate compute (~1.5h on 1×A100)
  - mBERT (79.3% accuracy, 110M params): Fastest, but 22-point F1 drop on low-resource languages
- Failure signatures:
  - mBERT/XLM: F1 below 0.65 on languages with <500 training samples indicates capacity limitation
  - All models: Misclassification of satirical content or claims requiring external knowledge indicates content-only approach limitation
  - mT5: Generation errors (outputting invalid tokens) indicate decoding constraint misconfiguration
- First 3 experiments:
  1. **Baseline reproduction**: Fine-tune XLM-R on the PolyTruth corpus with default hyperparameters (LR=2e-5, batch=16, 3 epochs); verify macro F1 ≈0.85 on test set.
  2. **Low-resource ablation**: Train XLM-R on high-resource languages only, then evaluate zero-shot on low-resource languages; quantify transfer gap vs. multilingual training.
  3. **Capacity comparison**: Fine-tune mBERT and XLM-R under identical conditions on the same data split; measure per-language F1 to identify which languages benefit most from larger capacity.

## Open Questions the Paper Calls Out

- Can integrating external evidence retrieval with multilingual transformers improve accuracy on claims requiring factual knowledge (e.g., numerical facts like "The population of country X is only 5,000")?
- Do multilingual transformers attend to similar linguistic features (e.g., hyperpartisan tone, clickbait phrasing) across different languages when classifying disinformation?
- To what extent do LLM-generated "true" counter-claims introduce artifacts that artificially inflate classification performance?
- How well do these models perform in zero-shot transfer to languages completely absent from the fine-tuning data?

## Limitations

- The synthetic true statements generated by OpenAI API may not reflect authentic writing patterns, potentially creating domain shift affecting model generalization
- Performance drops substantially for low-resource languages, with mBERT achieving only 61% F1 compared to 83% on high-resource languages
- The study does not examine temporal robustness—models are evaluated on static data without testing performance on evolving disinformation tactics over time

## Confidence

**High Confidence (8-10/10)**: Claims about RemBERT achieving the highest overall accuracy (87.1%) and the general ranking of models by performance (RemBERT > XLM-R ≈ mT5 > XLM > mBERT) are well-supported by experimental results.

**Medium Confidence (5-7/10)**: Claims about cross-lingual transfer mechanisms and specific reasons for performance differences are plausible but not definitively proven.

**Low Confidence (1-4/10)**: Claims about real-world deployment readiness and robustness to evolving disinformation tactics are speculative.

## Next Checks

1. **Temporal Robustness Test**: Evaluate model performance on disinformation data collected across different time periods to assess whether the approach maintains effectiveness as disinformation tactics evolve.

2. **Human Evaluation Benchmark**: Conduct human fact-checker assessments on model predictions to establish ground truth for model accuracy and identify systematic error patterns that automated metrics might miss.

3. **Domain Transfer Experiment**: Test model performance when fine-tuned on news data but evaluated on social media posts (or vice versa) to quantify the domain shift between synthetic true statements and naturally occurring content.