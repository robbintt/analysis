---
ver: rpa2
title: Multi-Armed Bandits-Based Optimization of Decision Trees
arxiv_id: '2508.05957'
source_url: https://arxiv.org/abs/2508.05957
tags:
- decision
- pruning
- tree
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses overfitting in decision trees by introducing
  a Multi-Armed Bandits-based pruning approach. The method treats each branch node
  as an arm in a bandit framework, using reinforcement learning to dynamically prune
  branches based on performance feedback.
---

# Multi-Armed Bandits-Based Optimization of Decision Trees

## Quick Facts
- arXiv ID: 2508.05957
- Source URL: https://arxiv.org/abs/2508.05957
- Reference count: 33
- This paper introduces a Multi-Armed Bandits-based pruning approach that treats each branch node as a bandit arm, achieving 4.09%-6.20% performance improvements over traditional Cost-Complexity Pruning on five benchmark datasets.

## Executive Summary
This paper addresses overfitting in decision trees by introducing a Multi-Armed Bandits-based pruning approach. The method treats each branch node as an arm in a bandit framework, using reinforcement learning to dynamically prune branches based on performance feedback. The proposed approach balances exploration and exploitation to optimize decision tree complexity and generalization. Experiments on five benchmark datasets show that the MAB-based pruning techniques outperform traditional Cost-Complexity Pruning, with improvements ranging from 4.09% to 6.20% in performance scores. Thompson Sampling and UCB1 achieved the most significant improvements with strong statistical significance. The results demonstrate the potential of MAB for adaptive, data-driven decision tree pruning.

## Method Summary
The approach maps the decision tree pruning problem to a Multi-Armed Bandit framework where each branch node represents a bandit arm. The method extracts prunable branches (depth > 3) and iteratively selects branches to prune using MAB algorithms (UCB1, Thompson Sampling, etc.). After temporarily pruning a branch, the model is evaluated on a small subset of training data (0.02%) to compute a composite reward based on accuracy, F1-score, and log loss differences. The reward updates the bandit's arm value estimates, guiding future pruning decisions. After 1100 iterations, branches are ranked by accumulated rewards and the lowest-performing ones are pruned. The framework uses a tolerance threshold to allow minor performance degradations that may improve generalization.

## Key Results
- MAB-based pruning techniques outperformed traditional Cost-Complexity Pruning with 4.09%-6.20% performance score improvements
- Thompson Sampling and UCB1 achieved the most significant improvements with strong statistical significance
- Softmax and KL-UCB showed moderate but still positive improvements over CCP
- All six MAB algorithms improved upon unpruned decision trees across five benchmark datasets

## Why This Works (Mechanism)

### Mechanism 1: Exploration-Exploitation Balance in Branch Selection
- **Claim:** Treating branch nodes as bandit arms enables dynamic, feedback-driven pruning that avoids greedy local optima.
- **Mechanism:** Each branch node is mapped to a bandit arm. MAB algorithms (UCB1, Thompson Sampling) select which arm to "pull" (i.e., which branch to prune) by balancing exploration of under-tested branches with exploitation of historically high-reward branches. This avoids the myopic decisions of traditional greedy pruning.
- **Core assumption:** The pruning process can be meaningfully modeled as a single-state sequential decision problem where past pruning rewards predict future optimal branches.
- **Evidence anchors:**
  - [abstract] "Our proposed approach assumes the pruning process as an exploration-exploitation problem, where we are utilizing the MAB algorithms to find optimal branch nodes to prune based on feedback from each pruning actions."
  - [Section III] "pulling an arm represents pruning a branch node. This approach iteratively updates its pruning action based on feedback from each actions."
  - [corpus] Weak direct evidence on MAB-for-pruning specifically; neighbor papers focus on MAB for optimization generally (e.g., "ParBalans," "EVaR-Optimal Arm Identification"), not decision tree pruning.
- **Break condition:** If pruning decisions have strong interdependencies (pruning branch A changes the value of branch B non-additively), the single-state bandit assumption degrades.

### Mechanism 2: Multi-Metric Reward Signal Guides Pruning
- **Claim:** A composite reward function combining accuracy, F1-score, and log loss provides richer feedback than single-metric greedy criteria.
- **Mechanism:** After temporarily pruning a candidate branch, the model is evaluated on a small data subset. The reward quantifies performance change: `Reward = f(ΔAccuracy, ΔF1, ΔLogLoss)`. Different MAB algorithms use either bounded rewards (UCB1) or Bernoulli rewards (Thompson Sampling). This signal updates the estimated value of each branch arm.
- **Core assumption:** The small evaluation subset (0.02% of training data) produces reward signals that generalize to full test performance.
- **Evidence anchors:**
  - [Section III, Eq. 1] "ΔScore = α(Accuracy_new − Accuracy_old) + γ(F1_new − F1_old) − β(Loss_new − Loss_old)"
  - [Section III] "Only 0.02% of the training data is used for consistent evaluation during pruning and for computing rewards, reducing computation compared to using the entire test set."
  - [corpus] No direct corroboration; reward design is paper-specific.
- **Break condition:** If evaluation subset is unrepresentative or noisy, reward signals become misleading, causing suboptimal pruning.

### Mechanism 3: Thresholded Reward Tolerates Minor Performance Degrades
- **Claim:** A tolerance threshold allows the algorithm to reward pruning actions that slightly reduce immediate performance if they may improve generalization.
- **Mechanism:** For UCB1, KL-UCB, and WSLS, reward is computed as `Reward = max(0, Threshold + ΔScore) * Constant`. A positive threshold buffers small negative ΔScore values, permitting aggressive pruning exploration that greedy methods would reject.
- **Core assumption:** Short-term performance loss can be acceptable if it leads to better generalization after multiple pruning steps.
- **Evidence anchors:**
  - [Section III, Eq. 2] "the value of threshold determines how much loss in performance, negative value of ΔScore, is being tolerated after pruning."
  - [Section IV] "all six MAB techniques... outperforms the unpruned decision tree with performance based score improvement in between 4% and 6.5%."
  - [corpus] No external validation of threshold mechanism.
- **Break condition:** If threshold is set too high, the algorithm may reward excessive pruning, leading to underfitting.

## Foundational Learning

- **Concept: Multi-Armed Bandits (MAB)**
  - **Why needed here:** The core optimization framework. Understanding exploration-exploitation tradeoffs, UCB1, and Thompson Sampling is essential to interpret how branch selection works.
  - **Quick check question:** Given 3 arms with unknown reward distributions, how does UCB1 decide which arm to pull next?

- **Concept: Decision Tree Pruning (Pre- vs Post-pruning, CCP)**
  - **Why needed here:** The paper positions MAB-pruning against Cost-Complexity Pruning. You need to understand what CCP optimizes (complexity penalty + accuracy) to see why MAB offers an alternative.
  - **Quick check question:** What is the key difference between pre-pruning and post-pruning, and why does the paper use post-pruning?

- **Concept: Reward Shaping in Reinforcement Learning**
  - **Why needed here:** The composite reward function (accuracy + F1 − log loss) is a form of reward shaping. Understanding how reward design affects learning dynamics helps diagnose convergence issues.
  - **Quick check question:** If you weight F1-score much higher than accuracy, how might pruning behavior change?

## Architecture Onboarding

- **Component map:** Trained Decision Tree (M) -> Branch Candidate Extraction -> MAB Selector -> Reward Computer -> History Updater -> Final Pruner

- **Critical path:**
  Train tree -> Extract prunable branches -> [Loop T=1100: MAB select -> temp prune -> evaluate -> compute reward -> update history -> restore tree] -> Rank branches by accumulated reward -> Final prune of bottom-C branches -> Return optimized tree

- **Design tradeoffs:**
  - **Reward weights (α, β, γ):** Paper uses (1, 1, 2.5). Higher γ favors F1; adjusting changes pruning aggressiveness on imbalanced data.
  - **Threshold value:** Controls tolerance for negative ΔScore. Higher = more aggressive pruning; risk of underfitting.
  - **MAB algorithm choice:** Thompson Sampling and UCB1 showed best results; Softmax underperformed.
  - **Evaluation subset size (0.02%):** Reduces compute but introduces noise. Larger subset increases accuracy but slows iterations.

- **Failure signatures:**
  - **No improvement over unpruned tree:** Check if threshold too low (no positive rewards) or evaluation subset too noisy.
  - **Severe underfitting:** Threshold may be too high, allowing aggressive pruning without penalty.
  - **High variance across runs:** MAB stochasticity; run multiple trials and average results.
  - **Poor performance on specific datasets:** Re-tune hyperparameters (weights, threshold) per dataset characteristics.

- **First 3 experiments:**
  1. **Baseline replication:** Implement UCB1-pruning on Breast Cancer dataset with paper hyperparameters (α=1, β=1, γ=2.5, T=1100). Compare score to unpruned and CCP-pruned trees.
  2. **Ablation on reward weights:** Vary γ (1.0 vs 2.5 vs 4.0) on an imbalanced dataset (Credit Card Fraud) to observe F1-weighting impact on minority class performance.
  3. **Evaluation subset sensitivity:** Test 0.02%, 0.1%, and 1% training subset sizes on a medium dataset (Ionosphere). Measure score variance and compute time tradeoff.

## Open Questions the Paper Calls Out
None

## Limitations
- Single-state bandit assumption may not capture complex pruning dependencies between branches, potentially leading to suboptimal decisions
- Small evaluation subset (0.02% training data) introduces noise in reward signals, which may affect pruning quality
- Threshold mechanism lacks external validation and could lead to over-pruning if misconfigured
- Results show statistical significance only for Thompson Sampling and UCB1, with other algorithms showing mixed performance

## Confidence
- **High confidence:** MAB-based pruning outperforms traditional CCP on benchmark datasets; mechanism of exploration-exploitation balance is theoretically sound
- **Medium confidence:** Reward function design and threshold mechanism effectiveness; generalization to datasets beyond five benchmarks
- **Low confidence:** Threshold mechanism's optimal value across different problem domains; single-state assumption validity for complex tree structures

## Next Checks
1. Test MAB pruning on datasets with highly interdependent branches to validate single-state assumption
2. Vary evaluation subset sizes (0.02%, 0.1%, 1%) on multiple datasets to quantify noise-performance tradeoff
3. Conduct ablation studies systematically varying reward weights (α, β, γ) across different dataset characteristics (balanced vs imbalanced, number of classes)