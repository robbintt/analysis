---
ver: rpa2
title: A Short Survey on Formalising Software Requirements using Large Language Models
arxiv_id: '2506.11874'
source_url: https://arxiv.org/abs/2506.11874
tags:
- language
- requirements
- https
- formal
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys literature on using large language models (LLMs)
  to formalise software requirements. It identifies 35 key papers covering tools and
  techniques for translating natural language requirements into formal specifications
  across domains like Dafny, C, Java, and hardware verification.
---

# A Short Survey on Formalising Software Requirements using Large Language Models

## Quick Facts
- **arXiv ID**: 2506.11874
- **Source URL**: https://arxiv.org/abs/2506.11874
- **Reference count**: 40
- **Primary result**: Survey of 35 papers on LLM-based formalization of natural language software requirements into formal specifications across domains like Dafny, C, Java, and hardware verification

## Executive Summary
This paper systematically surveys literature on using large language models to translate natural language software requirements into formal specifications. The authors identify 35 key papers covering tools and techniques across domains including Dafny, C, Java, and hardware verification. Most works rely on zero-shot prompting, while others employ fine-tuning, neuro-symbolic integration with theorem provers or SMT solvers, or human-in-the-loop refinement. The survey finds that assertion generation shows higher accuracy than full contract synthesis, with tools like Laurel and AssertLLM achieving over 50% and 89% correctness respectively.

## Method Summary
The authors conducted a systematic literature review using academic databases (IEEE Xplore, ACM DL, Scopus, Springer, Google Scholar) and the Elicit AI tool for initial filtering. Search terms included "NLP," "LLMs," "Software Requirements," and combinations with "specification," "verification," and "theorem proving." Papers were included if they provided theoretical or empirical insights on NLP/LLMs for software requirements and formal methods, with exclusions for insufficient relevance, non-peer-reviewed work, and duplicates. Manual abstract and full-text review followed Elicit filtering, with papers classified by methodology (prompt-only, fine-tuned, neuro-symbolic, verifier-in-loop).

## Key Results
- Assertion generation shows higher accuracy than full contract synthesis, with tools achieving over 50% and 89% correctness respectively
- Most surveyed works (24 out of 35) use zero-shot prompting, while only 6 employ fine-tuning
- Neuro-symbolic integration and human-in-the-loop approaches show promise for improving reliability
- Prompt engineering and chain-of-thought reasoning are identified as key research directions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs generate higher-quality assertions when prompted with verifier error messages and codebase examples than with generic instructions.
- Mechanism: Error messages localize the specification gap; example assertions from similar code provide syntactic and semantic templates. The LLM maps the error context to a plausible assertion pattern.
- Core assumption: The LLM can interpret verifier diagnostics semantically, not just syntactically, and generalize from examples to the target location.
- Evidence anchors:
  - [section 2] Laurel uses "analysis of the Dafny verifier's error message" to locate missing assertions and "provision of example assertions from a codebase," achieving over 50% helper assertion generation.
  - [section 4.8] "Tasks involving smaller, well-scoped units like assertions yield more accurate results from LLMs... likely due to the limited context and reduced complexity."
  - [corpus] Neighbor paper "On the Effectiveness of Large Language Models in Writing Alloy Formulas" (FMR=0.62) provides complementary controlled-experiment evidence on LLM specification writing, but direct replication for assertion generation specifically is limited.
- Break condition: When error messages are ambiguous, multi-causal, or require domain knowledge not present in the codebase examples (e.g., temporal properties across modules).

### Mechanism 2
- Claim: Neuro-symbolic integration (LLM + theorem prover/SMT solver) improves reliability by providing formal validation and structured feedback loops.
- Mechanism: The LLM proposes candidate specifications; the symbolic component checks logical consistency or provability. Failures generate feedback (counterexamples, unsatisfiable cores) that the LLM uses to refine the specification iteratively.
- Core assumption: The theorem prover or SMT solver can produce human- or model-interpretable feedback within reasonable latency, and the LLM can map that feedback to actionable edits.
- Evidence anchors:
  - [section 4.1] Explanation-Refiner integrates LLMs and theorem provers so "the theorem prover then provides the guarantee of validated sentence explanations" and "feedback for further improvements."
  - [section 4.6] SAT-LLM combines SMT with LLMs, achieving Precision=1.00, Recall=0.83, F1=0.91 for conflict detection, "performing better than ChatGPT alone."
  - [corpus] Weak direct corpus replication for this exact neuro-symbolic loop; neighbor papers focus on specification mining rather than integrated verification loops.
- Break condition: When feedback is cryptic (e.g., raw SMT unsat cores), requires deep domain expertise to interpret, or when iteration counts grow unbounded without convergence guarantees.

### Mechanism 3
- Claim: Structured mutation operators applied to LLM-generated specifications can systematically correct initially unverifiable outputs.
- Mechanism: When initial prompting fails to produce a verifiable specification, mutation operators (e.g., weakening/strengthening bounds, adjusting quantifiers) explore nearby candidates. The verifier acts as an oracle to accept or reject.
- Core assumption: The mutation space is searchable with a small number of operator applications, and correct specifications are reachable from initial LLM outputs via local edits.
- Evidence anchors:
  - [section 4.2] SpecGen uses "four mutation operators to ensure the correctness of the generated specifications," achieving verifiable specifications for 279/384 programs.
  - [section 4.8] "Frameworks like SpecGen and SpecSyn have made progress, [but] their outputs often need iterative refinement or mutation operators to reach acceptable accuracy."
  - [corpus] No directly comparable mutation-based approaches in the provided neighbor set; this is a relatively underexplored mechanism externally.
- Break condition: When specifications require non-local restructuring, or when the mutation space is too large relative to the verifier budget.

## Foundational Learning

- Concept: Verification-aware languages (e.g., Dafny) with native specification constructs and SMT-backed verification.
  - Why needed here: Understanding preconditions, postconditions, and helper assertions is prerequisite to interpreting why LLM-generated specifications succeed or fail in Section 2's example.
  - Quick check question: Given a Dafny lemma that fails verification, can you distinguish whether the issue is a missing assertion, an incorrect invariant, or an insufficient precondition?

- Concept: Satisfiability Modulo Theories (SMT) solvers and their role in automated reasoning.
  - Why needed here: Neuro-symbolic tools like SAT-LLM and Laurel rely on SMT solvers for validation; understanding solver capabilities and limits informs when integration will help.
  - Quick check question: If an SMT solver returns "unknown" on a generated specification, what are three possible causes and how would you diagnose them?

- Concept: Prompt engineering paradigms (zero-shot, few-shot, chain-of-thought).
  - Why needed here: Section 5.1 and Table 1 classify most surveyed work as "prompt-only"; evaluating tradeoffs between prompting strategies is critical for architecture decisions.
  - Quick check question: For a new formalization task, how would you decide between zero-shot CoT prompting vs. few-shot with exemplars vs. fine-tuning?

## Architecture Onboarding

- Component map:
  - NL Requirements → Prompt Constructor (injects context, examples, error messages)
  - LLM Inference → Candidate Specification Generator
  - Verifier/Theorem Prover (Dafny, SMT, etc.) → Validation + Feedback
  - Refinement Loop (mutation operators or human-in-the-loop) → Iterated Candidates
  - Formal Specification Output (assertions, contracts, invariants)

- Critical path:
  1. Requirement parsing and ambiguity detection
  2. Initial specification generation (prompt design is highest-leverage)
  3. Verification attempt (fast feedback determines iteration cost)
  4. Refinement until verifiable or resource budget exhausted

- Design tradeoffs:
  - Assertion-only vs. full contract synthesis: Assertions are more reliable (>50–89% in surveyed tools) but narrower in scope; full contracts require more context and abstraction.
  - Prompt-only vs. fine-tuned: Prompt-only is faster to iterate; fine-tuning may improve domain accuracy but requires labeled data and compute.
  - Human-in-the-loop vs. fully automated: Human oversight improves correctness but limits scalability; verifier-in-the-loop provides partial automation with formal guarantees.

- Failure signatures:
  - Syntax errors in generated specifications (wrong language constructs, malformed expressions)
  - Semantically plausible but unverifiable outputs (functional behavior correct, but proof fails)
  - "Lost-in-the-middle" attention issues with long contexts (Section 5.1), causing the LLM to ignore critical requirement details
  - Divergent refinement loops where mutation or iteration does not converge

- First 3 experiments:
  1. Replicate Laurel's assertion-generation setup on a small Dafny codebase: measure baseline correctness with zero-shot prompting vs. error-message-informed prompting.
  2. Implement a minimal neuro-symbolic loop using an off-the-shelf SMT solver (e.g., Z3) to validate LLM-generated invariants for a set of C programs; track iteration count and failure modes.
  3. Compare zero-shot CoT vs. few-shot prompting for translating a curated set of natural-language requirements into LTL formulas; measure syntactic validity and semantic correctness against ground-truth formulas.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific hybrid neuro-symbolic architectures (combining LLMs with SMT solvers or theorem provers) yield the highest accuracy for full contract synthesis, and can they match the 89% correctness achieved by assertion-only tools like AssertLLM?
- Basis in paper: [explicit] The authors state "assertion generation currently shows higher reliability than full contract synthesis" with tools achieving "over 50% and 89% correctness" for assertions, while full contracts "often failed verification" and their research agenda includes "development of hybrid neuro-symbolic approaches."
- Why unresolved: Current neuro-symbolic tools (SAT-LLM, Explanation-Refiner, Thor) focus on conflict detection, explanation validation, or theorem proving—not systematic comparison for contract synthesis accuracy.
- What evidence would resolve it: Comparative benchmarks evaluating full contract synthesis accuracy across multiple neuro-symbolic architectures on standardized datasets.

### Open Question 2
- Question: How can the "lost-in-the-middle" attention bias in long-context LLMs be mitigated when applying Chain-of-Thought reasoning to multi-sentence requirements formalisation?
- Basis in paper: [explicit] Section 5.1 notes "CoT requires the LLM to articulate distinct steps" but "may require careful analysis when used with larger LLM offering long input contexts" due to "U-shaped attention bias" where models "fail to attend to information in the middle of the context window."
- Why unresolved: The paper surveys CoT techniques but does not identify solutions addressing positional attention bias for requirements formalisation specifically.
- What evidence would resolve it: Empirical studies comparing CoT performance on requirements documents of varying lengths with attention calibration techniques applied.

### Open Question 3
- Question: What prompt engineering refinements most effectively reduce ambiguity when translating natural language requirements with inherent vagueness into formal specifications?
- Basis in paper: [explicit] The conclusion identifies "ambiguity resolution, verification and domain adaptation" as focus areas, and the research agenda mentions "refinement of prompt engineering."
- Why unresolved: While tools like nl2spec support iterative refinement, systematic evaluation of specific prompt strategies for ambiguity resolution remains unexplored.
- What evidence would resolve it: Controlled experiments measuring formalisation accuracy on deliberately ambiguous requirements across prompt variants.

### Open Question 4
- Question: Can domain-specific fine-tuning (e.g., LoRA adapters) improve formal contract generation accuracy beyond what zero-shot and few-shot prompting achieve?
- Basis in paper: [inferred] Table 1 shows most surveyed works use "Prompt-only" approaches, with only 6 using fine-tuning; the paper notes zero-shot prompting "shows excellent performance on many tasks" but improvements are not guaranteed.
- Why unresolved: Limited comparative data exists on whether fine-tuning substantially outperforms advanced prompting techniques for formal specification tasks.
- What evidence would resolve it: Head-to-head benchmarks comparing fine-tuned models against state-of-the-art prompting strategies on identical specification generation tasks.

## Limitations

- The exact Elicit AI filtering process is unspecified, making exact reproduction difficult
- Manual screening thresholds and inter-rater reliability are not documented
- Most surveyed tools report accuracy metrics but few provide ablation studies comparing prompting strategies directly

## Confidence

- **High**: Classification of surveyed papers by methodology and identification of key tools with reported accuracies
- **Medium**: Mechanism claims about why error-message-informed prompting and neuro-symbolic integration work better
- **Low**: Generalizability of mutation operator effectiveness beyond SpecGen's specific context

## Next Checks

1. Replicate the search strategy across IEEE Xplore, ACM DL, and Scopus with exact date ranges and keyword combinations to verify the 35-paper count.
2. Conduct an ablation study comparing zero-shot vs. error-message-informed prompting on a small Dafny codebase to validate the Laurel mechanism claim.
3. Implement a minimal neuro-symbolic loop using Z3 to validate LLM-generated invariants for C programs, measuring iteration counts and identifying failure modes.