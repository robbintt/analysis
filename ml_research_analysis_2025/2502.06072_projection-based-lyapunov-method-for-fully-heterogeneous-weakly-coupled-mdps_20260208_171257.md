---
ver: rpa2
title: Projection-based Lyapunov method for fully heterogeneous weakly-coupled MDPs
arxiv_id: '2502.06072'
source_url: https://arxiv.org/abs/2502.06072
tags:
- policy
- each
- lemma
- optimality
- arms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a new projection-based Lyapunov method to establish
  the first asymptotic optimality result for fully heterogeneous average-reward weakly-coupled
  Markov decision processes (WCMDPs). The method constructs Lyapunov functions by
  projecting system states onto carefully selected feature vectors, enabling analysis
  even under full heterogeneity.
---

# Projection-based Lyapunov method for fully heterogeneous weakly-coupled MDPs

## Quick Facts
- **arXiv ID:** 2502.06072
- **Source URL:** https://arxiv.org/abs/2502.06072
- **Reference count:** 40
- **Primary result:** First asymptotic optimality result for fully heterogeneous weakly-coupled MDPs using projection-based Lyapunov functions

## Executive Summary
This paper introduces a novel projection-based Lyapunov method to establish the first asymptotic optimality result for fully heterogeneous weakly-coupled Markov decision processes (WCMDPs). The method constructs Lyapunov functions by projecting system states onto carefully selected feature vectors, enabling analysis even under full heterogeneity. The main result shows that an efficiently computable policy achieves an O(1/√N) optimality gap as the number of arms N grows large, where the gap is measured by the difference between the long-run average reward per arm under the proposed policy and that under the optimal policy. The approach is novel in that it provides a principled way to measure deviations from optimality in heterogeneous settings without relying on symmetry or state aggregation techniques.

## Method Summary
The method involves solving a linear programming (LP) relaxation of the WCMDP to obtain optimal state-action frequencies, then computing optimal single-armed policies and stationary distributions. An ID reassignment algorithm reorders arms based on expected costs and active constraints to ensure linear cost accumulation. The runtime policy samples ideal actions from optimal single-armed policies and executes them in ID order until budgets are exhausted, with remaining arms taking action 0. The analysis uses projection-based Lyapunov functions to measure deviations from optimality in feature space, enabling drift analysis that proves convergence to an O(1/√N) optimality gap.

## Key Results
- First asymptotic optimality result for fully heterogeneous WCMDPs
- Achieves O(1/√N) optimality gap as number of arms grows large
- Projection-based Lyapunov functions enable analysis under full heterogeneity without symmetry assumptions
- Dynamic focus set translates Lyapunov function value into adaptive prioritization boundary

## Why This Works (Mechanism)

### Mechanism 1
A projection-based Lyapunov function enables convergence analysis under full heterogeneity by mapping arm-specific state deviations onto future reward and cost features, producing a negative drift toward the optimal region. For any arm subset D, the subset Lyapunov function h(x, D) = max_{g∈G} sup_{ℓ∈ℕ} |∑_{i∈D} ⟨(x_i − μ*_i)P_i^ℓ/γ^ℓ, g_i⟩| aggregates deviations in expected ℓ-step performance rather than raw state counts. Weighting by γ^ℓ (γ = exp(−1/(2τ))) ensures contraction under optimal single-armed policies even when each arm has distinct dynamics and stationary distributions. Core assumption: Assumption 1 requires each optimal single-armed policy to induce an aperiodic unichain Markov chain with mixing time uniformly bounded by τ, which guarantees exponential decay of ∥(P_i − Ξ_i)^ℓ∥_∞ and bounds the Lyapunov function.

### Mechanism 2
The ID reassignment algorithm ensures cumulative cost scales linearly with prefix size, preventing budget-constraint "plateaus" that destabilize priority-based action selection. By grouping arms into blocks of size d and guaranteeing each block contains at least one arm with C*_{k,i} ≥ δ for every active constraint k, the reassignment ensures C̅_k([n_1]) − C̅_k([n_2]) ≥ η_c(n_2 − n_1) − M_c (Lemma 2). This regularity means as arms converge and costs approach C*_{k,i}, budgets are consumed smoothly, allowing a consistent subset to follow ideal actions. Core assumption: Action 0 (zero cost to all arms) exists, ensuring feasibility; budget coefficients α_k and cost bounds c_max are fixed as N scales.

### Mechanism 3
The dynamic focus set translates Lyapunov function value into an adaptive prioritization boundary, ensuring most arms within the set can follow ideal actions and the set expands over time. The focus set [N m(X_t)] is the largest set satisfying h_ID(X_t, m) ≤ min_k C̅_k([N m]), ensuring sufficient remaining budget. Lemma 8 shows the conforming number N*_t is within O(√N) of the focus set size; Lemma 9 shows m(X_t) is almost non-shrinking; Lemma 10 relates set size to h_ID value. Core assumption: Assumption 1 and ID reassignment regularity (Lemma 2) are needed to bound h_ID and remaining budget.

## Foundational Learning

### Concept: Markov Decision Processes (MDPs) and the Average-Reward Criterion
Why needed here: Each arm is an independent MDP, and the objective is to maximize long-run average reward per arm. Understanding stationary policies, unichain conditions, and mixing times is essential for the LP relaxation and Lyapunov analysis.
Quick check question: Why does the long-run average reward exist under a stationary policy for a finite-state, unichain MDP?

### Concept: Lyapunov Drift Method for Stability Analysis
Why needed here: The core contribution is a novel Lyapunov function; understanding how negative drift (E[V(X_{t+1})|X_t] − V(X_t) ≤ −ρV(X_t) + O(√N)) leads to bounded steady-state expectations is critical for the proof.
Quick check question: If E[V(X_{t+1})|X_t] − V(X_t) ≤ −0.1 V(X_t) + 10√N, what is lim_{T→∞} (1/T) ∑ E[V(X_t)]?

### Concept: LP Relaxation for Weakly-Coupled MDPs
Why needed here: The LP (equation 5) provides an upper bound R^rel_N and defines the optimal single-armed policies π̄*_i; understanding how it decouples arms via time-average budget constraints is fundamental to policy design.
Quick check question: Why is the LP optimal value an upper bound on the original WCMDP optimal value?

## Architecture Onboarding

### Component map
Preprocessing Phase -> ID Reassignment Module -> Runtime Policy (ID with Reassignment)

### Critical path
The Lyapunov function V(X_t) (equation 22) must contract. Its definition combines h_ID(X_t, m(X_t)) (deviation in feature space) with a penalty for arms outside the focus set. The drift analysis (Lemmas 4–5) proves this contraction, yielding the O(1/√N) optimality gap.

### Design tradeoffs
- **Computational complexity**: LP has O(N|S||A|) variables; polynomial-time solvable but costly for very large N (acceptable in planning)
- **Generality vs. performance**: Applies to fully heterogeneous WCMDPs with multiple actions, constraints, and state-dependent costs, but gap is O(1/√N), not exponential as in some restrictive homogeneous settings
- **Memory**: Storing N optimal single-armed policies requires O(N|S||A|) memory, scaling linearly with N

### Failure signatures
- **Constraint violation**: LP infeasibility or excessively tight budgets (c_max < α_min trivial case) may degrade performance
- **Slow mixing**: Violating Assumption 1 (unbounded τ_i) may weaken drift (γ ≈ 1), increasing the optimality gap
- **Reassignment failure**: If cost distribution prevents linear accumulation, the focus set may not expand, causing persistent budget conflicts

### First 3 experiments
1. **Sanity check on homogeneous case**: Test the ID policy on a small homogeneous WCMDP with known benchmarks to verify O(1/√N) behavior
2. **Scalability test**: Run on synthetic heterogeneous instances with N ∈ {100, 500, 1000, 5000}, fixed |S|=10, |A|=4, K=4; plot optimality ratio vs. N and measure LP solve time
3. **Ablation on mixing time**: Construct instances with varying self-loop probabilities to control τ; verify if the optimality gap constant scales as O(τ^4) per C_ID = O(K^5 max{r_max, c_max}^7 τ^4 / α_min^6)

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** Can the ID policy with reassignment be effectively extended to the learning setting where transition kernels and reward functions are unknown?
**Basis in paper:** [explicit] The introduction states that the results "can serve as an important building block for developing model-based learning algorithms for fully heterogeneous WCMDPs."
**Why unresolved:** The current paper strictly addresses the planning problem with known model parameters.
**What evidence would resolve it:** A reinforcement learning algorithm utilizing the ID policy structure that provides a regret bound or sample complexity analysis.

### Open Question 2
**Question:** Can the projection-based Lyapunov method be generalized to other heterogeneous large stochastic systems outside of WCMDPs?
**Basis in paper:** [explicit] The authors note that "Beyond WCMDPs, our techniques have the potential to be applied to more general heterogeneous large stochastic systems."
**Why unresolved:** The proofs are tailored to the specific "arm" decomposition and budget constraints of WCMDPs.
**What evidence would resolve it:** A successful application of the projection-based Lyapunov construction to derive steady-state convergence in a different heterogeneous system, such as load balancing networks.

### Open Question 3
**Question:** Is the assumption of a uniform upper bound on mixing times (Assumption 1) strictly necessary for the $O(1/\sqrt{N})$ optimality gap?
**Basis in paper:** [inferred] Assumption 1 imposes a uniform mixing bound $\tau$ for all arms, which restricts the degree of allowable heterogeneity.
**Why unresolved:** The technical proof relies on this uniform bound to establish the drift condition (Lemma 3) using a uniform contraction factor.
**What evidence would resolve it:** A theoretical demonstration of asymptotic optimality using a Lyapunov function that accommodates non-uniform mixing times or unbounded variance.

### Open Question 4
**Question:** Can the optimality gap for fully heterogeneous WCMDPs be tightened to $O(\exp(-cN))$ under specific non-degeneracy conditions?
**Basis in paper:** [inferred] The paper achieves $O(1/\sqrt{N})$, whereas prior work on homogeneous settings [14, 15] has achieved exponential convergence.
**Why unresolved:** The projection-based method measures distance in a way that may lose the geometric convergence required for exponential bounds.
**What evidence would resolve it:** A refined drift analysis or policy modification showing that the system state converges exponentially fast to the optimal region.

## Limitations
- **Assumption 1 requirement**: Uniform mixing time bound τ may be restrictive for highly heterogeneous systems
- **Computational complexity**: LP with O(N|S||A|) variables becomes expensive for very large N
- **Constant dependence**: The O(1/√N) gap constant scales as O(K^5 max{r_max, c_max}^7 τ^4 / α_min^6), which could be large in practice

## Confidence

- Projection-based Lyapunov method enables convergence: Medium (novel construction, but assumptions critical)
- ID reassignment ensures linear cost accumulation: Medium (proof in Appendix, no corpus evidence)
- O(1/√N) optimality gap holds: Medium (derived from drift, constants depend on τ and other parameters)

## Next Checks

1. **Mixing time sensitivity**: Construct WCMDPs with controlled self-loop probabilities (τ ∈ {1,2,5,10}) and verify if the empirical optimality gap scales as O(τ^4)
2. **Ablation on ID reassignment**: Run ID policy without reassignment on heterogeneous instances and measure degradation in optimality ratio
3. **Scalability benchmark**: Implement on synthetic instances with N=10,000 arms and measure computation time for LP solve and policy execution