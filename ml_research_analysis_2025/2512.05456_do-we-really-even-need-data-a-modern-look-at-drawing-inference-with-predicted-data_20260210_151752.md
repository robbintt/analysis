---
ver: rpa2
title: Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted
  Data
arxiv_id: '2512.05456'
source_url: https://arxiv.org/abs/2512.05456
tags:
- data
- inference
- arxiv
- when
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of conducting valid statistical
  inference when outcomes are replaced by predictions from pre-trained machine learning
  models. The authors show that high predictive accuracy does not guarantee valid
  downstream inference, as it can introduce both bias and variance underestimation.
---

# Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted Data

## Quick Facts
- arXiv ID: 2512.05456
- Source URL: https://arxiv.org/abs/2512.05456
- Reference count: 34
- High predictive accuracy does not guarantee valid downstream inference when outcomes are replaced by predictions

## Executive Summary
This paper addresses a critical gap in modern data analysis: how to conduct valid statistical inference when gold-standard outcomes are replaced by predictions from machine learning models. The authors demonstrate that high predictive accuracy alone is insufficient for valid inference, as it can introduce both bias and variance underestimation. Through a unified framework based on classical statistical theory, they show that replacing true outcomes with predictions shifts the target estimand and distorts the data-generating structure. The paper reviews recent methods (PPI, PPI++, PSPA, RePPI) that correct for these errors using small labeled datasets, and demonstrates their effectiveness through empirical case studies in voter turnout and obesity measurement.

## Method Summary
The paper presents a unified framework for Inference with Predicted Data (IPD) that treats prediction errors as a missing data problem. The approach partitions data into labeled (L) and unlabeled (U) subsets, where L contains gold-standard outcomes and U contains only predictions. IPD methods correct the naive estimator by modeling the residual difference between predicted and observed outcomes in L, then applying this correction to U. The paper reviews four specific methods: PPI (Prediction-Powered Inference) which rectifies bias without assumptions about the upstream model, PPI++ which adds optimal weighting for efficiency, PSPA (Post-Prediction Adaptive Inference) which uses adaptive weighting, and RePPI which employs cross-fitting for robustness. An R package `ipd` implements these methods.

## Key Results
- High predictive accuracy does not guarantee valid downstream inference due to bias from estimand shift and variance underestimation
- IPD methods (PPI, PPI++, PSPA, RePPI) can substantially reduce bias compared to naive approaches using only a small labeled dataset
- Empirical case studies show IPD-corrected estimates align more closely with oracle (full labeled data) estimates than naive approaches
- Variance underestimation occurs because naive analyses treat predictions as fixed and observed, ignoring prediction uncertainty

## Why This Works (Mechanism)

### Mechanism 1
High predictive accuracy does NOT guarantee valid downstream inference because substituting predicted outcomes (Ŷ) for true outcomes (Y) shifts the target estimand. Bias decomposes into two components: (1) estimation bias—the naive estimator ̂η may not converge to the naive target η due to model shrinkage, loss function choices, or finite-sample artifacts; (2) estimator bias—even with infinite data, η = Ψ(P_Ŷ,X) ≠ θ = Ψ(P_Y,X) because predictions distort structural relationships (functional form, variability, dependence on X). Structural distortion arises from domain shift and model opacity.

### Mechanism 2
Treating predictions as observed data underestimates variance, producing anticonservative confidence intervals. Naive analyses ignore two variance components: (i) residual noise around the fitted function ̂f(Z), and (ii) finite-sample uncertainty from estimating ̂f itself. Replacing Y with a single realization of Ŷ is equivalent to single imputation, which underestimates uncertainty about the data-generating mechanism. Prediction errors are often heteroskedastic and dependent (shared inputs, smoothing, regularization).

### Mechanism 3
A small labeled dataset combined with assumption-lean correction methods (PPI, PPI++, PSPA, RePPI) yields valid inference by explicitly propagating bias and uncertainty. Labeled set L = {(Y_i, Ŷ_i, X_i, Z_i)} enables residual modeling: rectify the naive estimate by regressing (Ŷ − Y) on X in L, then apply correction to unlabeled set U. The unified framework shows estimators solve: minimize loss with augmentation term subtracting labeled imputed loss mean from full-sample imputed loss.

## Foundational Learning

**Estimand vs. Estimator distinction**
- Why needed: Paper hinges on understanding that Ψ(P_Y,X) (true target θ) differs from Ψ(P_Ŷ,X) (naive target η)—confusing these leads to misinterpreting bias sources
- Quick check: When you substitute Ŷ for Y, are you changing the estimand, the estimator, or both?

**Missing data theory (MCAR, MAR, MNAR)**
- Why needed: IPD is framed as a missing data problem where predictions serve as surrogates; understanding propensity scores and AIPW helps grasp why PPI methods work
- Quick check: Why does the paper emphasize that many IPD methods assume MCAR but some relax to MAR?

**Semiparametric efficiency bounds**
- Why needed: PPI++ and RePPI aim for optimal efficiency; understanding influence functions clarifies how augmentation terms achieve variance reduction
- Quick check: Under what conditions does RePPI attain the semiparametric efficiency bound?

## Architecture Onboarding

Component map:
[Pre-trained Model ̂f] → predictions Ŷ for unlabeled U
[Labeled Subset L] ←→ [IPD Correction Module (PPI/PSPA/RePPI)]
[Corrected Estimator ̂θ] with proper SE

Critical path:
1. Identify inferential target θ = Ψ(P_Y,X) and available features (X, Z)
2. Partition data: labeled L (gold-standard Y observed), unlabeled U (Y missing, Ŷ available)
3. Fit relationship/residual model on L (Y ~ Ŷ, X, Z)
4. Apply correction to U, pool estimates, compute adjusted standard errors

Design tradeoffs:
- Label fraction ρ: Larger L improves correction precision but increases cost; smaller L relies more on predictions
- Method choice: PPI is assumption-lean but may be inefficient; PPI++/PSPA optimize efficiency; RePPI uses cross-fitting for robustness
- Feature inclusion: Including X in the prediction model can reduce bias but may not eliminate it; excluding X guarantees bias if X correlates with Y

Failure signatures:
- Bias persists after correction: Labeled set unrepresentative (MAR/MNAR violation); prediction model excludes key features
- Wider CIs than classical: Predictions uninformative (low Ŷ-Y correlation); method defaults to complete-case behavior
- Inconsistent results across subgroups: Heterogeneous prediction error; domain shift between training and analytic populations

First 3 experiments:
1. Baseline check: Compare naive (treat Ŷ as Y), classical (complete-case on L only), and IPD-corrected estimates on voter turnout data; verify bias reduction in Table 3 pattern
2. Label fraction sensitivity: Vary ρ from 5% to 50% on NHANES obesity case; plot coefficient estimates and CI width vs. ρ to identify minimum viable labeled fraction
3. Feature ablation: Replicate linear example (Figure 3) with different feature sets included/excluded from prediction model; confirm bias emerges when X excluded from Ŷ

## Open Questions the Paper Calls Out

**Open Question 1**
How can Inference with Predicted Data (IPD) methods be extended to complex data structures like survival, longitudinal, spatial, and time-series models? Current IPD theory primarily focuses on independent identically distributed data and simple parameters (means/GLM coefficients). Complex models introduce dependence structures and censoring mechanisms that current IPD corrections do not account for.

**Open Question 2**
Can valid inference be conducted in high-dimensional settings where predictions are used to guide model selection (e.g., using LASSO)? Standard inference fails after model selection (post-selection inference). IPD adds a layer of complexity because the predictions introduce bias and variance that must be corrected before or during the selection process to avoid false positives.

**Open Question 3**
Is robust statistical inference possible in IPD settings where no labeled (gold-standard) data is available? Current methods like PPI and PSPA require a labeled subset to estimate and correct for the bias between predictions and true outcomes. Without this anchor, the bias term is unidentifiable without strong external assumptions.

## Limitations
- Implementation details remain underspecified, particularly specific API calls and tuning parameters within the `ipd` package
- Data access constraints may block immediate reproduction, as some datasets require specific data use agreements
- Performance depends critically on labeled data fraction and correlation between Ŷ and Y, neither of which is fully characterized across diverse real-world scenarios
- Assumption of MCAR labeled data may be violated in practice when labeling is expensive or outcome-dependent

## Confidence

**High confidence**: The fundamental claim that high predictive accuracy ≠ valid inference (Mechanism 1) is theoretically sound and empirically supported by the decomposition of bias into estimand and estimator components.

**Medium confidence**: The variance underestimation mechanism (Mechanism 2) is logically coherent but lacks extensive empirical validation across different prediction error distributions.

**Medium confidence**: The IPD correction framework (Mechanism 3) is promising, but optimal method selection depends on data characteristics not fully explored.

## Next Checks

1. **Label fraction sensitivity**: Systematically vary ρ from 5% to 50% on the NHANES obesity case to identify the minimum labeled fraction where IPD corrections outperform naive approaches, quantifying the cost-benefit tradeoff.

2. **Domain shift robustness**: Evaluate IPD performance when labeled and unlabeled subsets have different covariate distributions (simulating real-world data collection biases) to test MCAR/MAR assumptions.

3. **Cross-predictor comparison**: Apply IPD methods to multiple prediction models with varying levels of feature overlap with the true outcome (e.g., surname-only vs surname+demographics for voter race) to isolate when predictions capture the structural relationships needed for valid inference.