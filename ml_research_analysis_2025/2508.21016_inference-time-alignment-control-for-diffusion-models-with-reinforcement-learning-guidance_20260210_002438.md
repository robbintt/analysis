---
ver: rpa2
title: Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning
  Guidance
arxiv_id: '2508.21016'
source_url: https://arxiv.org/abs/2508.21016
tags:
- arxiv
- image
- diffusion
- guidance
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Reinforcement Learning Guidance (RLG), an
  inference-time method for controlling alignment strength in diffusion models that
  have been fine-tuned with reinforcement learning. RLG builds on Classifier-Free
  Guidance by combining outputs from the base and RL-tuned models via geometric averaging,
  enabling dynamic adjustment of alignment without additional training.
---

# Inference-Time Alignment Control for Diffusion Models with Reinforcement Learning Guidance

## Quick Facts
- arXiv ID: 2508.21016
- Source URL: https://arxiv.org/abs/2508.21016
- Reference count: 40
- Introduces RLG: Reinforcement Learning Guidance for controlling alignment strength at inference time

## Executive Summary
This paper introduces Reinforcement Learning Guidance (RLG), an inference-time method for controlling alignment strength in diffusion models that have been fine-tuned with reinforcement learning. RLG builds on Classifier-Free Guidance by combining outputs from the base and RL-tuned models via geometric averaging, enabling dynamic adjustment of alignment without additional training. Theoretically, the RLG scale is equivalent to adjusting the KL-regularization coefficient in the underlying RL objective, allowing interpolation and extrapolation of alignment strength. Empirically, RLG improves performance across multiple model architectures (diffusion, flow matching), RL algorithms (GRPO, DPO, SPO), and tasks (human preference, compositional accuracy, text rendering, compressibility, inpainting, personalization). On the human preference alignment task, RLG achieved win rates of up to 74.95% on PickScore compared to standard RL fine-tuning, and on text rendering it boosted OCR accuracy from 88.6% to 93.0%. RLG provides a practical, training-free mechanism for fine-grained control over generative alignment.

## Method Summary
RLG extends Classifier-Free Guidance to RL-fine-tuned models by geometrically averaging the outputs of base and RL-tuned diffusion models. The method allows dynamic adjustment of alignment strength at inference time without requiring additional training. The RLG scale parameter directly controls the trade-off between alignment strength and generation quality, with the theoretical interpretation that it adjusts the KL-regularization coefficient in the RL objective. This enables both interpolation within the trained range and extrapolation beyond it. The approach works across different RL algorithms (GRPO, DPO, SPO) and model architectures, providing a unified framework for inference-time alignment control.

## Key Results
- Achieved up to 74.95% win rates on human preference alignment tasks compared to standard RL fine-tuning
- Improved text rendering OCR accuracy from 88.6% to 93.0%
- Demonstrated effectiveness across multiple RL algorithms (GRPO, DPO, SPO) and model architectures
- Showed strong performance on compositional accuracy, compressibility, and personalization tasks

## Why This Works (Mechanism)
RLG leverages geometric averaging of base and RL-tuned model outputs to control alignment strength. The key insight is that the RLG scale parameter directly corresponds to the KL-regularization coefficient in the RL objective, allowing principled control over the alignment-utility trade-off. By combining the original diffusion model with its RL-tuned counterpart, RLG enables fine-grained adjustment of alignment strength without retraining. The geometric averaging approach preserves the probabilistic structure while allowing interpolation and extrapolation of alignment behavior. This method effectively bridges the gap between the base generative model and its RL-tuned version, providing a continuous spectrum of alignment strengths.

## Foundational Learning
- **Geometric averaging in diffusion models**: Combines multiple model outputs while preserving probabilistic structure
  - Why needed: Enables smooth interpolation between base and RL-tuned behaviors
  - Quick check: Verify that geometric averaging produces valid probability distributions
- **KL-regularization in RL objectives**: Controls the trade-off between reward maximization and staying close to the base policy
  - Why needed: Provides theoretical foundation for understanding RLG scale
  - Quick check: Confirm that adjusting KL coefficient produces similar behavior to RLG scale
- **Classifier-Free Guidance extension**: Generalizes CF guidance to work with RL-tuned models
  - Why needed: Provides the mathematical framework for combining model outputs
  - Quick check: Validate that CF guidance works as expected for standard fine-tuning
- **Reinforcement learning for alignment**: Uses RL algorithms to fine-tune models based on human preferences
  - Why needed: Creates the RL-tuned model that RLG will work with
  - Quick check: Ensure RL fine-tuning successfully improves alignment metrics
- **Diffusion model inference mechanics**: Understanding how denoising steps combine to generate samples
  - Why needed: Critical for implementing RLG at each denoising step
  - Quick check: Verify that standard diffusion sampling works correctly

## Architecture Onboarding

**Component Map:**
Base Diffusion Model -> RL-Tuned Diffusion Model -> Geometric Averaging Module -> RLG Scale Parameter -> Final Output

**Critical Path:**
During inference, at each denoising step: 1) Generate predictions from base model, 2) Generate predictions from RL-tuned model, 3) Apply geometric averaging weighted by RLG scale, 4) Produce final denoised sample

**Design Tradeoffs:**
- Inference-time control vs. training-time optimization
- Computational overhead of running two models vs. flexibility of adjustment
- Geometric averaging vs. other combination methods (linear, weighted sum)
- Single RLG scale vs. step-dependent scaling

**Failure Signatures:**
- Poor alignment when RLG scale is too low (model reverts to base behavior)
- Degraded quality when RLG scale is too high (over-alignment artifacts)
- Instability if geometric averaging produces invalid probability distributions
- Suboptimal performance if RL fine-tuning was insufficient

**First Experiments:**
1. Verify geometric averaging produces valid probability distributions at different RLG scales
2. Test interpolation between base and RL-tuned models on a simple task
3. Validate that RLG scale adjustment produces expected changes in alignment metrics

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Theoretical analysis assumes linear reward structures and Gaussian likelihoods that may not hold in practice
- Empirical validation focuses on standard benchmarks and may not reflect real-world deployment scenarios
- Geometric averaging approach may have limitations with sparse or noisy reward signals
- Effectiveness across different RL algorithms beyond the three tested remains unverified

## Confidence
- **High confidence**: The geometric averaging formulation of RLG and its connection to KL-regularization in RL objectives
- **Medium confidence**: The empirical performance improvements across multiple tasks and model architectures
- **Medium confidence**: The inference-time nature of the method requiring no additional training

## Next Checks
1. Test RLG's effectiveness with sparse reward signals and complex, non-linear reward functions beyond standard preference models
2. Evaluate long-term stability and performance consistency across extended inference sessions with dynamic preference adjustments
3. Assess RLG's behavior when interpolating between multiple reward models with potentially conflicting objectives