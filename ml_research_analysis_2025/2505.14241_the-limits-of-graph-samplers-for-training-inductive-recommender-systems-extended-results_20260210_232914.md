---
ver: rpa2
title: 'The Limits of Graph Samplers for Training Inductive Recommender Systems: Extended
  results'
arxiv_id: '2505.14241'
source_url: https://arxiv.org/abs/2505.14241
tags:
- sampling
- graph
- methods
- time
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Graph-based sampling techniques were applied to reduce training
  data for inductive recommender systems, using three state-of-the-art methods on
  three real-world datasets with six sampling approaches. Results show that using
  only 50% of the training data can decrease training time by up to 86% while maintaining
  reasonable recommendation performance; however, using less data leads to significantly
  worse results.
---

# The Limits of Graph Samplers for Training Inductive Recommender Systems: Extended results

## Quick Facts
- **arXiv ID:** 2505.14241
- **Source URL:** https://arxiv.org/abs/2505.14241
- **Reference count:** 40
- **Primary result:** Using only 50% of training data can reduce training time by up to 86% while maintaining reasonable recommendation performance.

## Executive Summary
This paper investigates whether graph-based sampling techniques can effectively reduce training data for inductive recommender systems without sacrificing performance. The authors evaluate three state-of-the-art inductive methods (PinSAGE, INMO, GInRec) on three real-world datasets using six different sampling approaches across various data reduction ratios. Results show that while moderate data reduction (50%) yields significant training time savings with minimal performance loss, more aggressive reductions lead to substantial performance degradation. Temporal and user-based sampling methods perform best, but even these struggle below 50% data reduction. The findings suggest that new sampling techniques and inductive recommendation models are needed to handle larger data reductions effectively.

## Method Summary
The paper evaluates inductive graph-based recommender systems by training on sampled subgraphs rather than full graphs. The methodology involves: (1) constructing Collaborative Graphs (CG) and Knowledge Graphs (KG) from three datasets (MovieLens-20m, Amazon-Book, Yelp); (2) applying six sampling methods (Forest Fire, Random Walk, Random Jump, PinSAGE Sampling, Temporal Sampling) at various ratios (α) to generate subgraphs; (3) training three inductive recommenders (PinSAGE, INMO, GInRec) on these subgraphs; and (4) evaluating performance on the full graph's test set. Models are evaluated using HR@20 and NDCG@20 metrics while measuring training time reduction. The study compares performance across different sampling ratios, methods, and recommendation architectures.

## Key Results
- Sampling at 50% data reduction achieves up to 86% training time reduction with less than 5% performance loss on average
- Temporal and user-based sampling methods consistently outperform other approaches, particularly at lower reduction ratios
- Performance degradation becomes severe below 50% data reduction, especially for sparse datasets
- INMO shows the most robustness to aggressive subsampling among the tested methods

## Why This Works (Mechanism)

### Mechanism 1: Temporal Sampling Preserves Recommendation-Relevant Distribution
- **Claim:** Temporal sampling maintains recommendation performance at moderate reduction ratios because recent interactions encode current user preferences and item relevance better than uniformly sampled historical data.
- **Mechanism:** TS selects the most recent user-item interactions by edge timestamp, producing a subgraph where the rating time distribution is skewed toward recency. Inductive GNNs aggregate neighbor features via graph convolutions; when those neighbors reflect current trends, the learned aggregation weights generalize better to future test interactions.
- **Core assumption:** User preferences and item popularity drift over time, and the most recent data is most predictive of near-term behavior.
- **Evidence anchors:** [abstract] "Temporal and user-based sampling methods performed best, but even these struggle below 50% data reduction"; [section 5, RQ1 & RQ2] "TS becomes the best performing for all datasets and methods when α ≤ 0.1. This indicates that for small datasets data recency is important."

### Mechanism 2: Degree-Preserving Sampling Maintains GNN Aggregation Fidelity
- **Claim:** Samplers that preserve the degree distribution of the original collaborative graph enable GNN-based recommenders to learn neighborhood aggregation weights that remain valid at inference on the full graph.
- **Mechanism:** GNNs compute node embeddings by iteratively aggregating neighbor representations. The effective receptive field and aggregation statistics depend on local degree distributions. PinSAGE-style user-based sampling retains users and their item neighborhoods, preserving item degrees tied to those users, which aligns sampled CG degree distributions with the original.
- **Core assumption:** GNN generalization from sampled subgraphs to the full graph depends more on structural fidelity (degree, connectivity) than on exact node identity.
- **Evidence anchors:** [section 5, Sampling viability] "A slight correlation between the degree distribution CG and the performance of the sampler exists, as PS and TS are frequently best performing and have the lowest d-statistics."

### Mechanism 3: Inductive Architecture Design Determines Sampling Robustness
- **Claim:** Inductive recommenders that rely primarily on collaborative signal aggregation are more robust to aggressive subsampling than those that jointly learn feature representations and structural inductive biases.
- **Mechanism:** INMO pre-learns a subset of key embeddings and aggregates neighbors via averaging, reducing dependence on learning complex feature-to-embedding mappings from sparse data. PinSAGE and GInRec learn feature encoders, relation-specific gates, and ranking objectives jointly, requiring more data to converge on stable inductive biases.
- **Core assumption:** Methods with fewer learned parameters that directly encode structural priors require less data to achieve stable performance.
- **Evidence anchors:** [section 5, RQ4] "INMO seems to be the most robust method to downsampling... INMO only optimizes initial node embeddings for the ranking objective."

## Foundational Learning

- **Graph Neural Networks (GNNs) and Message Passing**
  - **Why needed here:** All three recommenders use GNN layers to aggregate neighbor information; sampling alters neighborhood structure and thus aggregation outputs.
  - **Quick check question:** Given a node with 5 neighbors, what embedding does a mean-aggregation GNN produce if 2 neighbors are removed by sampling?

- **Inductive vs. Transductive Learning**
  - **Why needed here:** The paper exploits inductive models' ability to predict for unseen nodes; this property enables training on a subgraph and inference on the full graph without retraining.
  - **Quick check question:** Can a transductive matrix factorization model recommend a new item added after training without retraining? Why or why not?

- **Graph Sampling Techniques (Node, Edge, Random Walk, Forest Fire)**
  - **Why needed here:** The six samplers differ in how they select nodes/edges; understanding their structural biases is essential for interpreting the performance results.
  - **Quick check question:** Does Forest Fire sampling tend to produce more or less connected subgraphs than uniform random node sampling?

## Architecture Onboarding

- **Component map:** Data Layer (CG + KG + Features) -> Sampling Module (Sampler at ratio α) -> Inductive Recommender Core (GNN Encoder + Training Objective) -> Inference Layer (Full Graph Inference)

- **Critical path:**
  1. Load and preprocess dataset → construct CG and KG with temporal edge attributes
  2. Apply chosen sampler at ratio α to produce training subgraph
  3. Initialize node features (text embeddings, degree features, TransR for KG entities)
  4. Train inductive GNN recommender on subgraph with BPR/margin loss
  5. Evaluate on held-out temporal test set using full graph

- **Design tradeoffs:**
  - α = 0.5: ~80% training time reduction, <5% performance drop on average; suitable for hyperparameter tuning and frequent retraining
  - α < 0.2: Large performance degradation on sparse datasets; only viable for popularity-biased datasets
  - TS vs. PS: TS best at low ratios due to recency bias; PS better preserves degree distribution for structural generalization
  - INMO vs. PinSAGE/GInRec: INMO more robust at low data but may underperform at full data if features are critical

- **Failure signatures:**
  - Performance drops >15% NDCG at α = 0.5 → check sampler degree distribution alignment (high D-statistic); switch to PS or TS
  - Coverage near zero → model collapsing to popular items; increase α or use NS to verify if collaborative signal is learned
  - Training time not decreasing proportionally with α → PinSAGE's item-item loss recomputes over large candidate sets; consider INMO for efficiency

- **First 3 experiments:**
  1. **Baseline sanity check:** Run all three recommenders on full data (α = 1.0) to reproduce reported metrics within 5% of paper values
  2. **Sampling ratio sweep:** For each sampler, train INMO at α ∈ {0.05, 0.1, 0.2, 0.5} on YD dataset; plot HR@20 vs. training time to identify optimal operating point
  3. **Failure mode test:** Train GInRec with NicheSampler at α = 0.2; compare performance drop vs. TS to quantify reliance on collaborative signal vs. inductive feature learning

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specialized graph sampling techniques be developed specifically for heterogeneous Knowledge Graphs (KGs) to better preserve structural properties than current node-based methods?
- **Basis:** The paper notes that adapting sampling to KGs is "non-trivial" and states "Further development for heterogeneous graphs remains an open research question."
- **Why unresolved:** Current methods treat edges and nodes homogeneously, causing distribution misalignment and failing to preserve the specific semantic relations required for recommendation.
- **What evidence would resolve it:** A heterogeneous graph sampler that maintains a lower Kolmogorov-Smirnov D-statistic for entity degrees and results in higher NDCG scores compared to Random Walk or Forest Fire.

### Open Question 2
- **Question:** Is it possible to design new inductive recommendation models and sampling algorithms that maintain predictive performance when using less than 50% of the training data?
- **Basis:** The authors conclude that "if higher data reduction is needed, new graph based sampling techniques should be studied and new inductive methods should be designed."
- **Why unresolved:** Current state-of-the-art models suffer significant performance degradation when trained on subgraphs with sampling ratios below 0.5.
- **What evidence would resolve it:** A novel inductive model trained on 10-20% of data that achieves recommendation quality statistically indistinguishable from a model trained on the full graph.

### Open Question 3
- **Question:** To what extent do current graph-based recommender systems actually learn structural inductive biases versus merely aggregating collaborative signals?
- **Basis:** The paper asks "whether these recommendation systems are actually able to infer inductive bias from complex graph structures or are just aggregators of collaborative signal."
- **Why unresolved:** The "NichéSampler" experiment showed performance drops, but models like INMO remained robust, suggesting a heavy reliance on collaborative filtering rather than learned structural rules.
- **What evidence would resolve it:** Experiments demonstrating that models can successfully recommend items with purely structural features (no collaborative history) after being trained on heavily sampled subgraphs.

## Limitations

- Sampling effectiveness varies dramatically by dataset sparsity, with aggressive reductions (α ≤ 0.2) causing severe performance degradation on sparse datasets
- The analysis relies on empirical correlations between degree distribution alignment and performance without establishing causation
- Claims about which inductive architectural components drive robustness to sampling lack controlled ablation studies
- Results may not generalize to datasets with different interaction patterns or feature characteristics

## Confidence

- **High confidence:** Sampling at α = 0.5 reduces training time by 60-86% with <5% performance drop for dense datasets; Temporal and PinSAGE sampling methods outperform others at moderate ratios; INMO shows robustness to aggressive subsampling
- **Medium confidence:** Degree distribution preservation correlates with sampling performance; sampling effectiveness depends on dataset popularity bias; the mechanism linking structural fidelity to GNN generalization requires further validation
- **Low confidence:** Claims about which inductive architectural components drive robustness to sampling; causal relationships between sampling metrics (D-statistic) and performance; generalizability to datasets with different interaction patterns or feature characteristics

## Next Checks

1. **Controlled ablation on architectural components:** Train PinSAGE without the item-item loss, GInRec without relation gates, and INMO with full feature learning to isolate which inductive biases determine sampling robustness
2. **Dataset diversity test:** Apply the best sampling pipeline (PS at α = 0.5) to a highly sparse dataset with rich features (e.g., LastFM) to test the interaction between sparsity, sampling, and feature-based inductive capacity
3. **Statistical significance verification:** For each dataset-sampler-method combination, run 5 random seeds and compute 95% confidence intervals for HR@20 and NDCG@20 to confirm reported differences are not due to sampling variance