---
ver: rpa2
title: AI Generated Text Detection Using Instruction Fine-tuned Large Language and
  Transformer-Based Models
arxiv_id: '2507.05157'
source_url: https://arxiv.org/abs/2507.05157
tags:
- text
- bert
- task-b
- task-a
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work addresses the problem of distinguishing human-written\
  \ text from AI-generated text and identifying the specific LLM model responsible\
  \ for generation. The authors fine-tune three models\u2014GPT-4o-mini, LLaMA-3 8B,\
  \ and BERT\u2014on the Defactify dataset using simple instruction prompts."
---

# AI Generated Text Detection Using Instruction Fine-tuned Large Language and Transformer-Based Models

## Quick Facts
- arXiv ID: 2507.05157
- Source URL: https://arxiv.org/abs/2507.05157
- Reference count: 16
- Primary result: GPT-4o-mini and BERT achieved 95.47% accuracy for Task-A (detecting AI-generated text)

## Executive Summary
This work addresses the problem of distinguishing human-written text from AI-generated text and identifying the specific LLM model responsible for generation. The authors fine-tune three models—GPT-4o-mini, LLaMA-3 8B, and BERT—on the Defactify dataset using simple instruction prompts. Task-A is treated as binary classification (human vs. machine), while Task-B is multi-class classification (identifying the specific LLM). The fine-tuned GPT-4o-mini and BERT models achieved an accuracy of 0.9547 for Task-A and 0.4698 for Task-B. BERT performed exceptionally well on validation data (F1 = 100% for Task-A, 98% for Task-B) but showed lower performance on the test set, suggesting potential overfitting or domain shift.

## Method Summary
The authors fine-tune three transformer-based models on the Defactify dataset using instruction-based fine-tuning with simple prompts. GPT-4o-mini and BERT are used for both Task-A (binary classification) and Task-B (multi-class classification), while LLaMA-3 8B is used only for Task-B. Models are fine-tuned with specific hyperparameters (learning rates, batch sizes, epochs) and evaluated using macro-averaged precision, recall, and F1-scores across 7 classes. The approach leverages instruction formatting where prompts are appended to input text to guide classification behavior.

## Key Results
- GPT-4o-mini and BERT achieved 95.47% F1 score for Task-A (detecting AI-generated text)
- BERT achieved 98% F1 on validation data for Task-B but only 47% on test data, indicating potential overfitting
- LLaMA-3 8B showed moderate performance with 14% F1 score for Task-B
- GPT-4o-mini faced content filtering issues during inference, rejecting approximately 200 test samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Instruction-based fine-tuning with simple prompts enables effective binary classification of human vs. AI-generated text when using sufficiently capable models.
- Mechanism: By appending task-specific prompts to input text during training (e.g., "classify whether the given text is written by 'human' or 'machine'"), the model learns to associate prompt-context-label patterns. During inference, the same prompt format triggers the learned classification behavior.
- Core assumption: The model has sufficient pre-trained capacity to distinguish surface-level statistical and stylistic patterns between human and machine text.
- Evidence anchors:
  - [abstract] "GPT-4o-mini achieved 95.47% F1 score for Task-A (detecting AI-generated text)"
  - [section 5] "simple prompts were effective for Task-A"
  - [corpus] Related work (LuxVeri, mdok of KInIT) shows fine-tuned transformers achieve strong detection, supporting instruction-based approaches
- Break condition: Performance degrades significantly on unseen test data when training/validation distributions don't match test conditions, suggesting overfitting to dataset-specific artifacts rather than learning generalizable features.

### Mechanism 2
- Claim: Model source attribution (identifying which LLM generated text) requires more sophisticated approaches than simple prompt-based fine-tuning, particularly for smaller models.
- Mechanism: Multi-class classification requires learning subtle, model-specific fingerprints (lexical patterns, syntactic preferences, token distributions) that differentiate generators. Simple prompts may not provide sufficient signal for models to distinguish these fine-grained differences.
- Core assumption: Each LLM leaves identifiable stylistic signatures in generated text that persist across prompts and topics.
- Evidence anchors:
  - [abstract] "identifying specific LLMs requires more complex approaches and larger models"
  - [section 4] "BERT for Task-A and Task-B and LLaMA-3 8B for Task-B has not generalized well on the test dataset" (validation: 98% F1 → test: 46.98% F1 for BERT Task-B)
  - [corpus] Related papers (GenAI Detection Task 3, LLM Encoder vs. Decoder) suggest cross-domain detection and model attribution remain challenging, with robustness being a key concern
- Break condition: When training data contains model-specific artifacts (fixed generation parameters, prompt formats) that don't generalize to real-world generation conditions.

### Mechanism 3
- Claim: Larger, more capable models (GPT-4o-mini) maintain better test-time generalization than smaller models (LLaMA-3 8B, BERT) for binary detection tasks.
- Mechanism: Larger pre-trained models encode richer representations of language variation, making them more robust to distribution shift between training and test data. They may also better capture the fundamental differences between human and machine text patterns.
- Core assumption: Scale correlates with representational robustness and generalization capacity.
- Evidence anchors:
  - [section 4, Table 6] GPT-4o-mini: 95.47% test F1 vs. BERT: 76% test F1 (Task-A)
  - [section 4, Table 5] Validation performance was comparable (BERT: 100% F1, GPT-4o-mini: 97% F1), but test gap reveals generalization differences
  - [corpus] Weak/missing direct corpus evidence comparing model scale effects on generalization
- Break condition: Content filtering policies at inference time (observed with Azure GPT-4o-mini) can artificially limit performance by rejecting ~200 test samples.

## Foundational Learning

- Concept: **Instruction Fine-Tuning**
  - Why needed here: Understanding how appending task-specific prompts to training data creates learnable patterns that can be triggered at inference time is central to this paper's methodology.
  - Quick check question: Can you explain why the same prompt format must be used during both training and inference?

- Concept: **Distribution Shift and Generalization Gap**
  - Why needed here: The dramatic performance drop between validation (98% F1) and test (47% F1) for Task-B signals fundamental generalization failures that any practitioner must understand.
  - Quick check question: Why would a model achieve near-perfect validation accuracy but fail on held-out test data from the same dataset source?

- Concept: **Tokenization and Sequence Length Constraints**
  - Why needed here: Different models handle sequence length differently (BERT: 512 max, LLaMA-3: 8k), directly impacting what text features can be processed and compared.
  - Quick check question: How does truncating text to 512 tokens potentially remove discriminative information for distinguishing AI-generated content?

## Architecture Onboarding

- Component map:
  - Data Pipeline: Defactify dataset → instruction prompt appending → train/validation/test splits (Table 1)
  - Model Tier 1 (GPT-4o-mini): Azure OpenAI endpoint → instruction-formatted input → binary classification → content filtering layer
  - Model Tier 2 (LLaMA-3 8B): HuggingFace 4-bit quantized → Unsloth FastLanguageModel → LoRA adapters → binary OR multi-class output
  - Model Tier 3 (BERT): ktrain library → standard transformer classification head → task-specific output layers
  - Evaluation Layer: Macro-averaged precision/recall/F1 across 7 classes

- Critical path:
  1. Prepare instruction-tuned dataset with prompts from Table 2
  2. Fine-tune models per hyperparameters in Table 3 (learning rates, epochs, batch sizes are non-negotiable starting points)
  3. Validate on provided validation set first to establish baseline
  4. Test on unseen test set to measure true generalization
  5. **Crucially**: Monitor train/validation loss curves (Figures 2a, 2b) for overfitting signals

- Design tradeoffs:
  - **Model size vs. inference reliability**: Larger models (GPT-4o-mini) generalize better but introduce content filtering risks and API dependency
  - **Sequence length vs. computational cost**: 8k context (LLaMA) captures more document structure but increases training time; 512 tokens (BERT) is faster but may lose discriminative patterns
  - **Prompt complexity vs. task difficulty**: Simple prompts suffice for binary classification but fail for model attribution (Task-B)
  - **Quantization vs. performance**: 4-bit LoRA enables efficient training but may degrade fine-grained attribution capabilities

- Failure signatures:
  - **Validation-test gap >20% F1**: Indicates overfitting to dataset artifacts; BERT showed 98%→47% for Task-B
  - **Content filter rejection errors**: "The response was filtered due to the prompt triggering Azure OpenAI's content management policy" - affects ~2% of test samples
  - **Uniform prediction across classes**: LLaMA-3 8B's 14% Task-B F1 suggests mode collapse or failure to learn discriminative features
  - **Training loss diverges from validation loss**: Monitor Figures 2a/2b for divergence after epoch 3

- First 3 experiments:
  1. **Reproduce validation performance** with BERT on Task-A and Task-B using exact Table 3 hyperparameters (learning rate 2e-5, batch size 6, 3 epochs, max sequence length 512). Compare your validation F1 to reported 100% (Task-A) and 98% (Task-B) as a sanity check.
  2. **Ablate prompt complexity** for Task-B by augmenting the simple prompt with explicit feature descriptions (e.g., "gemma-2-9b tends to produce more formal syntax while mistral-7b has higher lexical diversity"). Test whether richer instructions improve LLaMA-3 8B's 14% test F1.
  3. **Cross-validate generalization** by creating a held-out split from the training data (hold out 20% of training samples) and training separate models on the remainder. Compare performance on this held-out split vs. the official test set to diagnose whether the test set represents a distribution shift or if models are overfitting to training/validation data artifacts.

## Open Questions the Paper Calls Out
None

## Limitations
- Severe distribution shift between validation and test performance, particularly for Task-B (model attribution), where BERT achieved 98% F1 on validation but only 47% on test data
- Content filtering policies pose practical constraints, with GPT-4o-mini rejecting approximately 200 test samples due to Azure OpenAI's content management system
- Simple prompt-based fine-tuning approach shows clear limitations for multi-class attribution (Task-B), with 14-47% F1 range indicating inability to reliably identify which specific LLM generated text

## Confidence
- **High Confidence**: Task-A binary classification performance (95.47% F1 for GPT-4o-mini, 76% for BERT) - supported by multiple validation metrics and consistent with related work showing fine-tuned transformers excel at human vs. AI text discrimination.
- **Medium Confidence**: Task-B attribution performance (47% BERT, 14% LLaMA-3) - results show clear performance but severe generalization issues suggest results may not transfer to real-world conditions.
- **Low Confidence**: Generalizability claims across datasets and real-world applications - the validation-test performance gap and lack of cross-dataset evaluation limit confidence in broader applicability.

## Next Checks
1. **Distribution Shift Diagnosis**: Create a held-out split from the training data (20% of training samples) and train separate models on the remainder. Compare performance on this held-out split versus the official test set to determine whether the test set represents genuine domain shift or if models are overfitting to training/validation data artifacts.

2. **Prompt Complexity Ablation**: Systematically increase prompt sophistication for Task-B by adding explicit feature descriptions for each model (e.g., "gemma-2-9b tends to produce more formal syntax while mistral-7b has higher lexical diversity"). Test whether richer instructions improve the 14% LLaMA-3 8B test F1 score and whether performance gains persist on the test set.

3. **Cross-Dataset Validation**: Evaluate the best-performing models (GPT-4o-mini for Task-A, fine-tuned BERT for Task-B) on an independent dataset like GLTR, Real or Fake Text, or CrossCheck with different generation parameters and domains. This would validate whether the high validation performance generalizes beyond the Defactify dataset or represents overfitting to dataset-specific patterns.