---
ver: rpa2
title: RadixMLP -- Intra-batch Deduplication for Causal Transformers
arxiv_id: '2601.15013'
source_url: https://arxiv.org/abs/2601.15013
tags:
- radixmlp
- prefix
- tokens
- attention
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RadixMLP addresses redundant MLP computations in batch causal transformer
  inference by compacting shared prefix sequences into a prefix trie, enabling position-wise
  operations to run on a compressed representation with scatter/gather only at attention
  boundaries. This stateless approach eliminates the need for KV caching while preserving
  causal correctness.
---

# RadixMLP -- Intra-batch Deduplication for Causal Transformers

## Quick Facts
- arXiv ID: 2601.15013
- Source URL: https://arxiv.org/abs/2601.15013
- Reference count: 40
- Primary result: 1.44-1.59× end-to-end speedup on MS MARCO v1.1 reranking workloads with Qwen3 models (0.6B-8B)

## Executive Summary
RadixMLP addresses redundant MLP computations in batch causal transformer inference by compacting shared prefix sequences into a prefix trie, enabling position-wise operations to run on a compressed representation with scatter/gather only at attention boundaries. This stateless approach eliminates the need for KV caching while preserving causal correctness. On real-world MS MARCO v1.1 reranking workloads with Qwen3 models (0.6B-8B), RadixMLP achieves 1.44-1.59× end-to-end speedup, with up to 5× speedup on synthetic benchmarks with longer shared prefixes.

## Method Summary
RadixMLP builds a prefix trie over batch sequences where each node is identified by (parent, token_id, position_id). Shared prefixes traverse the same path, reducing N tokens to N' unique nodes. Position-wise functions (MLP, LayerNorm, linear projections, embeddings) are applied once per unique node. Pre-attention norm, Q/K/V projections, and RoPE run on N' compact tokens. Scatter expands to N tokens for FlashAttention, then gather compacts back for O projection, post-attention norm, and MLP. Index computation occurs asynchronously on CPU during GPU execution of previous batch. The method is compatible with training autograd and reduces activation memory by factor r=N/N'.

## Key Results
- 1.44-1.59× end-to-end speedup on MS MARCO v1.1 reranking workloads
- Up to 5× speedup on synthetic benchmarks with longer shared prefixes (2048/256)
- 1.44× speedup on Qwen3-0.6B, 1.51× on Qwen3-4B, 1.59× on Qwen3-8B
- Compression ratio γ=N'/N up to 0.14 in synthetic tests
- Numerical stability: ≤2×10⁻⁵ gradient differences vs SDPA reference

## Why This Works (Mechanism)

### Mechanism 1: Position-Wise Operation Reuse via Prefix Trie Compaction
MLP, LayerNorm, linear projections, and embeddings are position-wise operations that yield identical outputs for tokens sharing identical causal history. RadixMLP constructs a prefix trie where shared prefixes reduce N tokens to N' unique nodes. Position-wise functions are applied once per unique node rather than per original token. When prefix sharing is minimal (γ≈1), index computation overhead may exceed compute savings.

### Mechanism 2: Scatter/Gather at Attention Boundaries Preserves Causal Correctness
Hidden states stay in compact space and expand only at attention boundaries. Pre-attention norm, Q/K/V projections, and RoPE run on compact tokens. Scatter expands to N tokens for FlashAttention, then gather compacts back for O projection, post-attention norm, and MLP. Position IDs are gathered alongside tokens for correct positional encoding. Attention metadata remains valid after scatter since it restores original ragged order.

### Mechanism 3: Stateless Single-Pass Design Eliminates KV Cache Management Overhead
RadixMLP achieves compute reuse within a single forward pass without persistent state. Index computation occurs asynchronously on CPU during GPU execution of previous batch. Trie construction for ~16K tokens takes 129-750µs, enabling zero-overhead scheduling. CPU scheduler has sufficient headroom to construct trie and indices while GPU executes.

## Foundational Learning

- **Concept: Causal Transformer Block Structure (Attention vs Position-wise Operations)**
  - Why needed here: RadixMLP exploits the fundamental distinction that attention is sequence-mixing while MLP, LayerNorm, and projections are position-wise. Understanding this boundary determines where scatter/gather can be inserted.
  - Quick check question: Given a 2-layer transformer, which operations can be safely computed on compact tokens: (a) Q projection, (b) attention softmax, (c) MLP, (d) residual addition? Answer: (a), (c), (d) are position-wise; (b) is sequence-mixing.

- **Concept: Ragged Memory Layout vs Padded Batching**
  - Why needed here: RadixMLP builds on ragged layouts where sequences are concatenated contiguously without padding tokens. The cu_seqlens array tracks boundaries. This enables efficient index-select operations without materializing attention masks.
  - Quick check question: For sequences of lengths [3, 5, 2], what is the ragged tensor shape and cu_seqlens? Answer: Shape is (10, d); cu_seqlens = [0, 3, 8, 10].

- **Concept: Prefix Trie and Index Mapping Semantics**
  - Why needed here: RadixMLP linearizes the prefix trie into index tensors. I_gather (size N') maps compact→original (selects representatives); I_scatter (size N) maps original→compact (broadcasts duplicates). Correct index semantics is critical for implementation.
  - Quick check question: For sequences [1,2,3] and [1,2,4] with positions [0,1,2], what are I_gather and I_scatter? Answer: I_gather = [0, 1, 2, 5] (4 unique nodes); I_scatter = [0, 1, 2, 0, 1, 3].

## Architecture Onboarding

- **Component map**: Input tokens + position IDs → Prefix trie construction (CPU, async) → I_gather + I_scatter generation (CPU, async) → Embedding + RoPE in compact space (N' tokens, GPU) → Per layer: Pre-attention norm → Q/K/V projection (compact) → Scatter → FlashAttention (full N) → Gather → O projection + MLP (compact) → Final LayerNorm + LM Head (compact) → Scatter → Logits (full)

- **Critical path**: The scatter-before-attention and gather-after-attention operations are the synchronization boundaries. Scatter memory bandwidth (O(N) copies) must be overlapped with position-wise compute savings to achieve net speedup.

- **Design tradeoffs**: Higher compression ratio r = N/N' increases speedup but requires longer shared prefixes. Larger models benefit more (f_c = 73-92% position-wise FLOPs) as MLP dominates. Optional performance padding to fixed bucket sizes improves GPU utilization but adds minor compute overhead. Tradeoff vs KV caching: RadixMLP avoids persistent state but limited to single-pass; KV caching supports autoregressive decoding but requires block management.

- **Failure signatures**: Speedup < 1.0× with short prefixes (< 32 tokens) or low redundancy (γ > 0.95). Forward/backward numerical differences dominated by attention kernel choice (FA2 vs SDPA), not RadixMLP itself. Maximum gradient deviation ~2×10⁻⁵ (RadixMLP toggle) vs ~3×10⁻² (backend swap) in validation tests. Long-context workloads (32K+ tokens) show diminishing returns as attention O(L²) dominates position-wise O(L).

- **First 3 experiments**:
  1. **Validate correctness**: Run synthetic tests with controlled prefix sharing (identical sequences, shared prefix, no sharing, mixed lengths) comparing RadixMLP vs baseline logits and gradients against SDPA reference (rtol=1e-4).
  2. **Measure crossover point**: Sweep prefix lengths [1, 16, 32, 128, 256, 512, 1024, 2048] with fixed suffix=256, batch=32 on Qwen3-0.6B to identify where speedup exceeds 1.0× and validate against Amdahl model prediction.
  3. **Profile memory vs compute tradeoff**: Measure activation memory reduction (factor r) and forward-pass latency for 2048/256 configuration across Qwen3-0.6B/4B/8B to verify scaling with model size and f_c fraction.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can RadixMLP yield meaningful throughput gains in large-scale training workloads (e.g., SFT, RLHF) where prompt templates share prefixes across batches? The authors proved gradient-correctness but did not benchmark actual training runs. Evidence needed: End-to-end training throughput benchmarks on datasets with shared prefixes.

- **Open Question 2**: Can RadixMLP be efficiently integrated with stateful KV cache systems like vLLM to provide complementary intra-batch deduplication? Integration is non-trivial due to vLLM's internal data structures. Evidence needed: A vLLM-compatible implementation showing incremental speedup beyond existing KV cache reuse.

- **Open Question 3**: Can precomputed prefix tries enable attention kernels that process shared prefixes without redundant memory access? Theoretical promise exists but memory access scheduling complexity makes practical implementation challenging. Evidence needed: A working kernel demonstrating measurable speedup on shared-prefix attention workloads.

## Limitations

- **Prefix sharing dependency**: Speedup fundamentally tied to prefix redundancy (γ=N'/N). For highly variable inputs (no shared prefixes), RadixMLP degrades to baseline performance with additional index computation overhead.

- **Attention backend variance**: Forward/backward numerical differences are reported relative to SDPA, but the paper doesn't fully characterize differences when using FlashAttention-2 as the baseline.

- **Synthetic benchmark relevance**: The 5× synthetic speedup with 2048/256 prefix/suffix configuration represents an idealized upper bound. Real-world prefix lengths in MS MARCO (~32-64 tokens) yield more modest 1.44-1.59× improvements.

## Confidence

- **High confidence**: The fundamental mechanism (position-wise operation reuse via prefix trie) is theoretically sound and well-validated by the 1.44-1.59× real-world speedup measurements. The stateless single-pass design eliminating KV cache management is clearly beneficial for batch workloads.

- **Medium confidence**: The reported numerical stability (2×10⁻⁵ gradient differences) is convincing within the validation framework, but the choice of SDPA as reference and limited test scenarios leave some uncertainty about broader training compatibility.

- **Low confidence**: The synthetic benchmark results (up to 5× speedup) may overstate practical benefits since they represent extreme prefix-sharing scenarios rarely encountered in production workloads.

## Next Checks

1. **Cross-workload prefix sharing analysis**: Measure γ=N'/N across diverse real-world workloads (code completion, conversational AI, document processing) to characterize the technique's effectiveness spectrum and identify minimum prefix lengths for positive ROI.

2. **Training workload characterization**: Evaluate RadixMLP's numerical stability and performance impact on full training cycles with gradient accumulation, mixed precision, and distributed data parallelism to verify claimed autograd compatibility under realistic conditions.

3. **Attention backend variance isolation**: Conduct controlled experiments comparing numerical differences between RadixMLP+FA2, baseline+FA2, and RadixMLP+SDPA to isolate the technique's contribution from backend-induced variance and validate the 2×10⁻⁵ figure across more scenarios.