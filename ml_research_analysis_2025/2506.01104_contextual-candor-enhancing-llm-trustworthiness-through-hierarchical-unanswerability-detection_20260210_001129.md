---
ver: rpa2
title: 'Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability
  Detection'
arxiv_id: '2506.01104'
source_url: https://arxiv.org/abs/2506.01104
tags:
- unanswerability
- refusal
- unanswerable
- questions
- responses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Reinforced Unanswerability Learning (RUL),\
  \ a novel training paradigm designed to imbue large language models (LLMs) with\
  \ the intrinsic capability to accurately detect unanswerable questions and generate\
  \ appropriately reliable responses. RUL integrates a discriminative unanswerability\
  \ prediction head with the LLM\u2019s generative core, trained through a two-stage\
  \ process: supervised fine-tuning on a novel Enhanced-CAsT-Answerability (ECA) dataset\
  \ with hierarchical answerability labels and ground-truth refusal responses, followed\
  \ by reinforcement learning with human feedback (RLHF) to refine the helpfulness\
  \ and informativeness of refusal responses."
---

# Contextual Candor: Enhancing LLM Trustworthiness Through Hierarchical Unanswerability Detection

## Quick Facts
- **arXiv ID:** 2506.01104
- **Source URL:** https://arxiv.org/abs/2506.01104
- **Reference count:** 37
- **Key outcome:** RUL framework significantly improves unanswerability detection and refusal quality via discriminative-prediction + RLHF training

## Executive Summary
This paper introduces Reinforced Unanswerability Learning (RUL), a novel training paradigm designed to imbue large language models (LLMs) with the intrinsic capability to accurately detect unanswerable questions and generate appropriately reliable responses. RUL integrates a discriminative unanswerability prediction head with the LLM's generative core, trained through a two-stage process: supervised fine-tuning on a novel Enhanced-CAsT-Answerability (ECA) dataset with hierarchical answerability labels and ground-truth refusal responses, followed by reinforcement learning with human feedback (RLHF) to refine the helpfulness and informativeness of refusal responses. RUL significantly outperforms strong baselines, achieving substantially higher accuracy in unanswerability detection across sentence, paragraph, and ranking levels, and substantially increasing the generation of appropriate refusals for unanswerable queries. Human evaluations corroborate RUL's effectiveness, highlighting marked improvements in perceived helpfulness and trustworthiness.

## Method Summary
RUL employs a two-stage training process. Stage 1 involves supervised fine-tuning on the ECA dataset using a composite loss combining binary cross-entropy for the discriminative unanswerability prediction head and negative log-likelihood for generation. Stage 2 applies RLHF: a reward model is trained on human preference pairs to score refusal quality, then the policy is optimized via PPO with KL regularization to avoid catastrophic forgetting. The architecture appends a feed-forward neural network to the LLM's [CLS] token output for binary answerability prediction, using attention-weighted aggregation to hierarchically combine sentence, paragraph, and ranking-level scores before deciding whether to generate an answer or a refusal.

## Key Results
- RUL achieves substantially higher unanswerability detection accuracy across sentence, paragraph, and ranking levels compared to baselines
- Human evaluations show marked improvements in perceived helpfulness (4.6 vs 2.5) and appropriateness of refusal responses
- Attention-weighted aggregation provides +3.5% ranking accuracy gain over simple mean pooling
- RLHF stage significantly improves refusal informativeness and trustworthiness metrics

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Discriminative-Generative Decoupling
Integrating a discriminative classification head with a generative LLM allows the system to explicitly predict unanswerability before committing to a response, reducing hallucination rates. A feed-forward neural network is appended to the LLM's pooled output to predict a binary answerability score, which determines whether the generative component produces a factual answer or a refusal. This "safety gate" approach enforces reliability by preventing the model from generating responses to unanswerable queries.

### Mechanism 2: Hierarchical Attention Aggregation
Contextual evidence often exists at different granularities; aggregating answerability signals via learned attention weights improves robustness over simple pooling methods. The model computes answerability at sentence and paragraph levels, then uses attention mechanisms to weight these individual scores dynamically before aggregating them into a final ranking-level score. This hierarchical approach allows the model to make more nuanced predictions by focusing on the most relevant context segments.

### Mechanism 3: Refusal Refinement via RLHF
Supervised Fine-Tuning establishes the ability to refuse, but Reinforcement Learning with Human Feedback is critical for optimizing the nuance, tone, and helpfulness of those refusals. After SFT, the model undergoes policy optimization against a reward model trained on human preferences, rewarding refusals that explain why a question is unanswerable rather than just saying "I don't know." This improves the appropriateness score by teaching the model to provide meaningful explanations.

## Foundational Learning

- **Concept: Binary Cross-Entropy (BCE) Loss**
  - **Why needed here:** Used to train the unanswerability prediction head. You need to understand how to optimize for a binary output (answerable vs. unanswerable) distinct from the token generation loss.
  - **Quick check question:** How does the model penalize a confident wrong prediction when the ground truth is $y=0$?

- **Concept: KL Divergence (in PPO/RLHF)**
  - **Why needed here:** The RL objective includes a KL divergence penalty to prevent the model from drifting too far from the SFT initialization, ensuring reliability improvements don't destroy the LLM's language capabilities.
  - **Quick check question:** If the coefficient $\beta$ is set too low, what behavior might the model exhibit regarding its original knowledge?

- **Concept: Attention Pooling**
  - **Why needed here:** The architecture moves beyond simple averaging to "Attention-Weighted Aggregation." Understanding how the scalar $\alpha_k$ is computed from the vector $h_k$ is crucial for debugging why the model focuses on specific sentences.
  - **Quick check question:** Why is `tanh` used in the attention calculation before the final projection?

## Architecture Onboarding

- **Component map:** Input Encoder -> Prediction Head -> Aggregation Layer -> Generator -> Reward Model
- **Critical path:** 
  1. Threshold Calibration: Setting $\tau$ is the most critical inference parameter. If $\tau$ is too low, the model hallucinates; if too high, it over-refuses.
  2. Training Pipeline: Must follow SFT (Stage 1) -> RLHF (Stage 2). You cannot skip SFT or the RLHF stage will be unstable.
- **Design tradeoffs:**
  - Inference Speed vs. Robustness: RUL adds ~30ms overhead compared to standard fine-tuned LLMs due to attention aggregation.
  - Complexity vs. Accuracy: The hierarchical attention mechanism requires more engineering effort than mean pooling but provides +3.5% accuracy gain.
- **Failure signatures:**
  - "Polite Hallucinations": Model refuses politely but is actually factually wrong, indicating the Prediction Head threshold $\tau$ is too aggressive.
  - "Generic Refusals": High refusal rate but low informativeness score, indicating RLHF stage was insufficient.
- **First 3 experiments:**
  1. Threshold Sweep: Vary $\tau$ on a validation set to plot the trade-off between Refusal Rate and F1-score.
  2. Aggregation Ablation: Compare Attention-Weighted Aggregation vs. simple Mean Pooling to verify the overhead is justified.
  3. Refusal Quality Audit: Generate responses for unanswerable questions using only SFT vs. SFT+RLHF, manually checking if RLHF responses provide promised explanations.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does RUL perform when extended to multi-turn dialogue scenarios where context accumulates or shifts dynamically?
- **Basis in paper:** [explicit] The conclusion explicitly states, "Future work will explore extending RUL to multi-turn dialogues."
- **Why unresolved:** Current experiments focus on single-turn query-context pairs, leaving the model's ability to maintain refusal consistency or context state over multiple interactions untested.
- **What evidence would resolve it:** Evaluation results on multi-turn conversational benchmarks showing detection accuracy and refusal consistency across conversation turns.

### Open Question 2
- **Question:** Can uncertainty quantification metrics be effectively incorporated into refusal justifications to enhance user trust?
- **Basis in paper:** [explicit] The conclusion identifies "incorporating uncertainty quantification into refusal justifications" as a specific direction for future work.
- **Why unresolved:** While the paper establishes binary classification for unanswerability, it does not explore how communicating the degree of uncertainty affects the nuance, helpfulness, and informativeness of refusal responses.
- **What evidence would resolve it:** A user study comparing subjective trust scores for refusals containing calibrated confidence scores versus those with qualitative explanations only.

### Open Question 3
- **Question:** Does RUL generalize to specialized high-stakes domains (e.g., legal or medical) where factual reliability is paramount?
- **Basis in paper:** [explicit] The authors note the need for "applying this framework to diverse domains where factual reliability is paramount."
- **Why unresolved:** Experiments rely on general conversational search datasets, leaving performance on technical terminology or domain-specific constraints unverified.
- **What evidence would resolve it:** Zero-shot or few-shot performance benchmarks on domain-specific datasets measuring unanswerability detection accuracy and hallucination rates.

## Limitations

- The ECA dataset is proprietary and not publicly available, preventing independent validation of the reported performance gains.
- The paper does not report on model robustness to out-of-distribution queries or the computational overhead of inference-time attention aggregation.
- Claims about the novelty of the hierarchical aggregation approach are weakly supported by the cited related work.

## Confidence

- **High Confidence:** The general framework of combining a discriminative unanswerability head with a generative LLM is well-supported by literature and the paper's ablation studies.
- **Medium Confidence:** Specific performance numbers are credible given the ablation studies and two-stage training process, but cannot be fully verified without the dataset.
- **Low Confidence:** Claims about the novelty of the hierarchical aggregation approach are weakly supported by cited related work.

## Next Checks

1. **Dataset Reproduction:** Attempt to construct a proxy ECA dataset using SQuAD 2.0 and publicly available QA data. Fine-tune a small LLM with the RUL architecture and compare its performance to a baseline LLM fine-tuned only on SQuAD 2.0.

2. **Attention Aggregation Ablation:** Implement both the attention-weighted aggregation and a simple mean pooling baseline. Train both on the proxy dataset and compare their performance on a held-out test set.

3. **RLHF Impact Audit:** Use GPT-4 or another strong LLM to generate a set of "human preference" pairs for refusal responses. Fine-tune a reward model on this data and use it to optimize the refusal responses from the SFT model, comparing the optimized responses to SFT-only responses using human evaluation.