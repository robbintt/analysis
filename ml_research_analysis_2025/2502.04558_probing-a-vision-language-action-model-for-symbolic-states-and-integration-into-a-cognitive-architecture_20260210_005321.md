---
ver: rpa2
title: Probing a Vision-Language-Action Model for Symbolic States and Integration
  into a Cognitive Architecture
arxiv_id: '2502.04558'
source_url: https://arxiv.org/abs/2502.04558
tags:
- object
- states
- action
- bowl
- openvla
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of integrating vision-language-action
  (VLA) models with cognitive architectures (CA) to combine the generalist capabilities
  of VLAs with the interpretability and symbolic reasoning of CAs. The authors propose
  probing OpenVLA's hidden layers to extract symbolic representations of object properties,
  relations, and action states.
---

# Probing a Vision-Language-Action Model for Symbolic States and Integration into a Cognitive Architecture

## Quick Facts
- arXiv ID: 2502.04558
- Source URL: https://arxiv.org/abs/2502.04558
- Authors: Hong Lu; Hengxu Li; Prithviraj Singh Shahani; Stephanie Herbers; Matthias Scheutz
- Reference count: 20
- Primary result: Consistently high probe accuracies (> 0.90) for object and action state prediction across most OpenVLA layers, with successful integration into DIARC cognitive architecture for real-time state monitoring

## Executive Summary
This paper addresses the challenge of integrating vision-language-action (VLA) models with cognitive architectures (CA) to combine the generalist capabilities of VLAs with the interpretability and symbolic reasoning of CAs. The authors propose probing OpenVLA's hidden layers to extract symbolic representations of object properties, relations, and action states. They train linear probes on different layers of OpenVLA to predict symbolic states during manipulation tasks, specifically using the LIBERO-spatial pick-and-place benchmark. The primary result shows consistently high accuracies (> 0.90) for both object and action state predictions across most layers of OpenVLA, though contrary to hypotheses, object states were not encoded earlier than action states. The authors successfully demonstrate an integrated DIARC-OpenVLA system that leverages these symbolic representations for real-time state monitoring, laying the foundation for more interpretable and reliable robotic manipulation.

## Method Summary
The authors employ linear probing to extract symbolic representations from OpenVLA's hidden layers. They first collect training episodes from OpenVLA in the LIBERO simulation environment, labeling each timestep with ground-truth symbolic states. Linear classifiers are then trained on frozen activation vectors from each of OpenVLA's 33 hidden layers to predict binary predicates representing object properties, relations, and action states. The probes use multi-label binary cross-entropy loss to handle the multiple independent predicates. For integration, the best-performing layer probes are deployed during OpenVLA inference, with probe outputs converted to DIARC's symbolic predicate format and sent to DIARC's belief store via WebSocket for real-time state monitoring.

## Key Results
- Linear probes achieve consistently high accuracies (> 0.90) for both object and action state predictions across most layers (1-32) of OpenVLA
- Contrary to hypotheses, object states are not encoded earlier than action states in the layer hierarchy
- Successful integration of DIARC with OpenVLA enables real-time state monitoring through symbolic predicate updates
- Layer 0 shows significantly degraded performance, while mid-to-late layers perform similarly

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Linear probes trained on frozen hidden-layer activations can decode symbolic object and action states from OpenVLA's Llama 2 backbone with high accuracy.
- Mechanism: A multi-label linear classifier maps each 4096-dimensional hidden state vector h to binary predictions ŷ = σ(Wh + b), where each element corresponds to a symbolic predicate (e.g., on(bowl_1, plate_1), grasped(bowl_1)). The probe learns which dimensions encode specific state information without modifying the VLA.
- Core assumption: Symbolic state information is linearly decodable from the activation space; the VLA internally represents task-relevant states despite being trained only on action prediction.
- Evidence anchors:
  - [abstract] "Our probing results show consistently high accuracies (> 0.90) for both object and action states across most layers"
  - [section IV-C] "Formally, for a given layer's activation vector h ∈ Rd, our probe learns a mapping to binary predictions ŷ ∈ [0, 1]^n"
  - [corpus] No direct corpus evidence for VLA probing; related work (Li et al. 2021, Chen et al. 2023) addresses LLM probing only.
- Break condition: If symbolic states are not linearly separable in activation space, or if the VLA relies purely on implicit trajectory matching without explicit state representation, probe accuracy would approach random baseline.

### Mechanism 2
- Claim: Object and action states are not hierarchically encoded across layers as hypothesized.
- Mechanism: The authors expected earlier layers to encode visual-spatial object properties and later layers to encode action concepts after integration. However, probing accuracy was uniformly high (>0.90) across layers 1-32, with only layer 0 showing degraded performance.
- Core assumption: Layer-wise probing accuracy reflects the primary functional role of each layer in the processing hierarchy.
- Evidence anchors:
  - [abstract] "contrary to our hypotheses, we did not observe the expected pattern of object states being encoded earlier than action states"
  - [section V] "We do not observe the hypothesized pattern of higher object state accuracies in earlier layer probes versus that of later layer probes"
  - [corpus] No corpus papers test layer-wise encoding hypotheses in VLAs.
- Break condition: If task diversity is too low (same objects, same placements across tasks), probes may overfit to dataset artifacts rather than true state encoding—the authors acknowledge this limitation.

### Mechanism 3
- Claim: Extracted symbolic states enable real-time cognitive architecture integration for state monitoring.
- Mechanism: At each timestep, hidden-layer embeddings are extracted during OpenVLA inference, fed to pre-trained probes, and the resulting binary predictions are converted to DIARC predicate format (relation(object1, object2), property(object), action(object)). These update DIARC's belief store via WebSocket.
- Core assumption: Probe predictions are sufficiently reliable to serve as ground-truth state estimates for symbolic reasoning.
- Evidence anchors:
  - [abstract] "We demonstrate an integrated DIARC-OpenVLA system that leverages these symbolic representations for real-time state monitoring"
  - [section III-A] "These arrays are sent to DIARC's VLAComponent, which converts them into DIARC's symbolic predicate format"
  - [corpus] Weak corpus support; [Hybrid Reasoning for Manufacturing] discusses neural-symbolic integration but not probing-based extraction.
- Break condition: If probe accuracy degrades on out-of-distribution scenarios (unseen objects, layouts), the symbolic state estimates become unreliable for CA reasoning.

## Foundational Learning

- Concept: **Linear Probing**
  - Why needed here: Understanding that a frozen model's representations can be decoded via simple linear classifiers without backpropagation through the model.
  - Quick check question: Can you explain why probing uses a frozen backbone with only the classifier trained?

- Concept: **Multi-label Classification with Binary Cross-Entropy**
  - Why needed here: The probes predict multiple independent binary predicates simultaneously rather than a single class label.
  - Quick check question: Why is multi-label classification appropriate here versus standard softmax classification?

- Concept: **Cognitive Architecture Belief Stores**
  - Why needed here: DIARC maintains symbolic predicates as its world model; understanding how external state estimates update this store.
  - Quick check question: What is the difference between sub-symbolic embeddings and symbolic predicates in a belief store?

## Architecture Onboarding

- Component map:
  - OpenVLA -> Linear Probes -> VLAComponent -> DIARC belief store -> WebSocket Server -> LIBERO simulation

- Critical path:
  1. Collect training episodes from OpenVLA in LIBERO (5 per task × 10 tasks)
  2. Label each timestep with ground-truth symbolic states via detector functions
  3. Train linear probes on (activation, state) pairs with episode-level train/test splits
  4. Deploy best-performing layer probes in inference pipeline
  5. Route probe outputs → VLAComponent → DIARC belief store

- Design tradeoffs:
  - **Probe placement**: Paper found no clear layer hierarchy; any mid-to-late layer (1-32) works. Layer selection may not matter for low-diversity tasks.
  - **Label filtering**: Near-constant labels (<1% or >99% positive rate) are removed to avoid inflated metrics, but this may hide real-but-rare states.
  - **No feature standardization**: Raw embeddings work well here but may not generalize to other models or tasks.

- Failure signatures:
  - Layer 0 probe accuracy significantly lower across all predicates (expected: low-level features only)
  - High uniform accuracy across layers may indicate insufficient task diversity rather than true state encoding
  - Probe predictions contradicting environment observations suggest distribution shift or overfitting

- First 3 experiments:
  1. **Baseline probe on layer 0 vs. layer 16**: Verify the paper's finding that layer 0 underperforms and mid-layers are sufficient.
  2. **Cross-task generalization test**: Train probes on 8 LIBERO tasks, test on 2 held-out tasks with different spatial configurations to probe generalization.
  3. **Noise robustness check**: Add Gaussian noise to embeddings before probing to assess probe sensitivity and determine operating margins for real deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does increasing task and object diversity reveal distinct layer-wise encoding patterns for object versus action states, which were obscured by the low-variance LIBERO-spatial dataset?
- Basis in paper: [explicit] The authors state that the lack of variation in object states and action states likely reduced task difficulty, "washing out" the expected layer-wise differences and preventing validation of H1 and H2.
- Why unresolved: The current experimental data is insufficient to confirm if object states are encoded earlier than action states due to the uniform nature of the pick-and-place tasks used.
- What evidence would resolve it: Probing results from a dataset with variable object layouts, diverse goals, and a wider range of state changes showing divergent accuracy curves across layers.

### Open Question 2
- Question: How can the reasoning capabilities of a cognitive architecture (CA) like DIARC actively utilize the extracted symbolic states to intervene in or correct the VLA's behavior?
- Basis in paper: [explicit] The paper demonstrates real-time state monitoring but explicitly notes, "In the future, we hope to explore how the reasoning capabilities of the CA can enhance or monitor the performance of the VLA."
- Why unresolved: The current integration is unidirectional (monitoring); the system detects inconsistencies but has not yet implemented logic to utilize these signals for active error correction or policy adaptation.
- What evidence would resolve it: A demonstration where DIARC successfully identifies a symbolic contradiction or execution error and successfully alters the VLA's trajectory or halts execution.

### Open Question 3
- Question: Can linear probes trained on simulated environments successfully decode symbolic states from OpenVLA when transferred to real-world robotic manipulation tasks?
- Basis in paper: [inferred] The study relies entirely on the LIBERO simulation suite, while the introduction highlights that VLAs struggle with environmental changes and "sensitivity to environmental factors" in real-world settings.
- Why unresolved: It is unclear if the linear mappings learned from simulated activations are robust enough to handle the visual noise and domain shift of physical robot execution.
- What evidence would resolve it: Evaluation of probe accuracy on hidden layer activations recorded during real-world robot manipulation tasks involving similar objects.

## Limitations

- The LIBERO-spatial benchmark exhibits low task diversity, potentially causing probe accuracies to reflect dataset-specific artifacts rather than genuine state encoding
- Uniformly high accuracies across layers contradict expected hierarchical encoding patterns and may indicate spurious correlation capture
- Real-time integration demonstration shows state monitoring but lacks validation of downstream CA reasoning benefits or error handling capabilities

## Confidence

- **High confidence**: The empirical probe accuracies (>0.90) are reliably measured and reproducible given the same dataset and experimental setup. The linear probe methodology and WebSocket integration pipeline are straightforward implementations.
- **Medium confidence**: The claim that symbolic states are "consistently encoded" across layers is technically true given the accuracies, but the lack of layer-wise differentiation raises questions about what is actually being measured. The integration demonstration works functionally but lacks validation of its practical utility.
- **Low confidence**: The paper's assertion that these probes enable "more interpretable and reliable robotic manipulation" is aspirational rather than demonstrated—no experiments show improved task performance, safety, or CA reasoning capabilities compared to pure VLA operation.

## Next Checks

1. **Cross-task generalization test**: Train probes on 8 LIBERO tasks, then test on 2 held-out tasks with novel spatial configurations (e.g., different table layouts, new object arrangements). This would reveal whether probes capture genuine state representations or dataset-specific artifacts.

2. **Out-of-distribution robustness**: Deploy the DIARC-OpenVLA system with probe-based state monitoring on novel objects and environments not seen during training. Monitor probe accuracy degradation and CA response to incorrect state estimates to establish operational margins.

3. **Layer-wise functional analysis**: Systematically compare probe performance across layers on tasks requiring different cognitive demands (pure visual localization vs. complex multi-object manipulation). This could reveal whether the lack of hierarchical encoding is due to task simplicity or a fundamental property of VLAs.