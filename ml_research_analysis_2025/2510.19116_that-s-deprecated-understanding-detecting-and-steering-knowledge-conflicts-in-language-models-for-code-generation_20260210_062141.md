---
ver: rpa2
title: That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts
  in Language Models for Code Generation
arxiv_id: '2510.19116'
source_url: https://arxiv.org/abs/2510.19116
tags:
- knowledge
- conflicts
- code
- statement
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models handle knowledge
  conflicts between their parametric knowledge and conflicting information in prompts,
  extending prior QA research to code generation. The authors propose a framework
  for constructing and interpreting context-memory conflicts, and introduce a novel
  evaluation method and dataset tailored to code conflict scenarios.
---

# That's Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation

## Quick Facts
- arXiv ID: 2510.19116
- Source URL: https://arxiv.org/abs/2510.19116
- Authors: Jaesung Bae; Cameron Churchwell; Mitchell Hermon; Tsun-An Hsieh; Jocelyn Xu; Yekaterina Yegorova; Mark Hasegawa-Johnson; Heng Ji
- Reference count: 40
- Primary result: LLMs encode knowledge conflict signals in parameters; activation steering improves steering success by up to 12.6% over random baseline.

## Executive Summary
This paper investigates how large language models resolve conflicts between their parametric knowledge (PK) and conflicting information in prompts (CK), extending prior QA research to code generation. The authors propose a framework for constructing knowledge conflicts and detecting them via probing residual stream activations with logistic regression. They demonstrate that sufficiently large models (8B+) encode conflict signals linearly separable in later layers, achieving up to 80.65% detection accuracy. Using activation-level steering, they improve steering success by up to 12.6% over random baselines, though effectiveness depends on model size, task domain, and steering direction.

## Method Summary
The authors construct knowledge conflict scenarios by eliciting PK-based responses from models, then applying task-specific templates to generate conflicting context. They use probing experiments with logistic regression classifiers on residual stream activations to detect whether models will respond based on PK or CK. For steering, they compute a steering vector by projecting the difference in mean activations between conflicting and regular prompts onto the probe's weight vector, then add or subtract this vector during inference. Experiments use Llama3 models (1B, 3B, 8B) across QA tasks (World Capitals, Olympics Winners) and code generation tasks (EvalPlus with function/operator deprecation).

## Key Results
- Probing experiments achieve up to 80.65% accuracy in detecting knowledge conflicts in later layers of large models
- Larger models (8B) show stronger reliance on parametric knowledge for well-represented domains, making them harder to steer toward CK
- Activation steering achieves up to 12.6% improvement over random baseline, with effectiveness varying by model size, task domain, and steering direction
- Cross-domain probing (QA→Code) shows limited success, working best for 8B models at specific layers (18-21)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Knowledge conflict signals become linearly separable in residual stream activations of large LLMs, with discriminability improving in later layers.
- Mechanism: Logistic regression probe trained on residual stream activations classifies whether response will align with PK or CK, with accuracy increasing in deeper layers.
- Core assumption: Linear separability implies model encodes decision-relevant conflict signal, not just surface features.
- Evidence anchors: [abstract] "up to 80.65% accuracy"; [Section 6] "models' ability to discriminate between PK and CK is the strongest in later layers"
- Break condition: Probing accuracy degrades significantly when transferring across domains for smaller models (1B, 3B).

### Mechanism 2
- Claim: Larger model size correlates with increased reliance on parametric knowledge for well-represented information.
- Mechanism: Increased parameter count enables stronger encoding of frequently-encountered knowledge, creating robust PK representations.
- Core assumption: Proportion of PK vs CK responses reflects relative strength of internal representations.
- Evidence anchors: [Section 5.1] "as model size decreases, proportion of CK-based response increases"; "models generated responses mostly based on PK for World Capitals"
- Break condition: Pattern reverses for information with weak parametric priors (Olympics Winners).

### Mechanism 3
- Claim: Activation-level steering via residual stream modification can bias model responses toward PK or CK.
- Mechanism: Steering vector constructed by projecting difference in mean activations between conflicting and regular prompts onto probe weights.
- Core assumption: Direction identified by probe corresponds causally to model's decision process.
- Evidence anchors: [abstract] "up to 12.6% improvement in steering success over random baseline"; [Section 7] "steering towards PK demonstrates comparatively higher success rates"
- Break condition: Steering effectiveness drops to near-zero for certain model-task combinations (e.g., 1B on Olympics Winners $S_{PK} = 0.000$).

## Foundational Learning

- Concept: **Residual Stream**
  - Why needed here: Activations extracted from residual stream for both probing and steering; understanding as cumulative information pathway is essential.
  - Quick check question: At layer 15, does residual stream contain information from only layer 15's computation, or the sum of all previous layers?

- Concept: **Linear Probing**
  - Why needed here: Detection method relies on training logistic regression classifiers on activations; understanding linear separability is crucial.
  - Quick check question: If linear probe achieves 50% accuracy on binary classification, what does this suggest about the feature in that representation?

- Concept: **Parametric vs. Contextual Knowledge**
  - Why needed here: Framework depends on distinguishing knowledge encoded in weights (PK) from prompt information (CK); confusing these breaks conflict definition.
  - Quick check question: If model answers "What is the capital of France?" correctly without context, which knowledge source is being used?

## Architecture Onboarding

- Component map: Query $q$ → Elicit $y_{PK}$ → Apply template $T_t$ → Generate conflicting context $c'$ → Combine $(c', q)$ → Response Classifier → Categorize (PK/CK/Other) → Probing Module → Extract $h_l$ → Train logistic regression → Steering Module → Compute $s$ → Apply $a'(x) = a(x) \pm s$ → Measure $S_{PK}$, $S_{CK}$

- Critical path: 1. Generate $y_{PK}$ for each query using base model; 2. Construct conflict prompts using task-specific templates; 3. Extract residual stream activations for first generated token; 4. Train layer-wise probes on in-domain data; 5. Select best-performing layer per model for steering vector; 6. Apply steering during inference and measure success rates.

- Design tradeoffs:
  - Model size vs. steerability: Larger models encode stronger PK, making them more accurate but harder to steer toward CK for well-represented domains
  - In-domain vs. cross-domain probes: In-domain probes achieve near-perfect accuracy; cross-domain requires 8B model and peaks only at specific layers (18-21)
  - Linear vs. non-linear probes: Non-linear probes show different peak layers but don't consistently outperform linear; may overfit

- Failure signatures:
  - Probe accuracy at 50% indicates no detectable conflict signal (common in small models for cross-domain)
  - Steering success at 0% for specific direction/model/task combinations (e.g., 1B on OW toward PK)
  - High "Other" response rate suggests model ignores both PK and CK, possibly due to implausible conflict framing
  - Cross-attention maps showing strong attention to context but responses still aligning with PK

- First 3 experiments:
  1. Replicate World Capitals probing experiment on Llama3-8B, plotting accuracy per layer to identify peak layer
  2. Construct small code conflict dataset (10-20 examples) using function deprecation, run cross-domain probe from QA-trained weights, measure accuracy
  3. Apply steering vector at identified peak layer to held-out test set, computing $S_{PK}$ and $S_{CK}$ separately to determine asymmetry

## Open Questions the Paper Calls Out

- Question: How does knowledge conflict resolution change when incorporating memory via multi-turn conversations?
  - Basis in paper: [explicit] Authors state they "plan to explore incorporating memory via multi-turn conversations into the analysis" for future work.
  - Why unresolved: Current design restricted to single-turn prompts, failing to capture how conversational history influences balance between parametric and conflicting knowledge.
  - What evidence would resolve it: Extending framework to multi-turn dialogue datasets to measure if context retention increases adherence to conflicting knowledge over time.

- Question: To what extent do model architectural differences, beyond parameter size, affect conflict resolution strategies?
  - Basis in paper: [explicit] Conclusion lists "investigat[ing] how model architecture affects conflict resolution strategies" as specific avenue for future work.
  - Why unresolved: Study only evaluated Llama 3 family (1B, 3B, 8B), isolating model size while keeping architecture consistent.
  - What evidence would resolve it: Applying same conflict detection and steering methods to diverse architectures (e.g., Mixture-of-Experts) to compare conflict representation linear separability.

- Question: Can robust predictive methods be developed to anticipate whether model will favor PK versus CK before generation?
  - Basis in paper: [explicit] Future work includes need to "develop better predictive methods for when models will favor PK versus conflicting knowledge."
  - Why unresolved: Current research focuses on post-hoc detection and steering rather than predicting behavior based on input features.
  - What evidence would resolve it: Classifier that accurately predicts PK/CK reliance based solely on semantic features of prompt or initial embedding states.

## Limitations

- Cross-domain transfer of conflict detection probes shows limited reliability, working best for 8B models at specific layers but failing for smaller models
- Steering mechanism's causal relationship to knowledge source selection remains unclear, with effectiveness varying significantly by model size, task domain, and steering direction
- Response categorization relies on task-specific conditions that may not capture all edge cases, with "Other" category indicating potential implausible conflict framing

## Confidence

**High Confidence**: Finding that larger LLMs encode stronger parametric knowledge representations and show higher PK alignment for well-represented domains (World Capitals) is supported by consistent experimental evidence across multiple model sizes and tasks.

**Medium Confidence**: Effectiveness of activation steering shows moderate reliability but depends significantly on model size, task domain, and steering direction, with asymmetry in steering effectiveness and complete failure cases suggesting mechanism is not uniformly reliable.

**Low Confidence**: Cross-domain transfer of conflict detection probes shows least reliability, with small models failing to achieve above-chance accuracy when applying QA-trained probes to code generation tasks.

## Next Checks

1. **Probe Transferability Test**: Systematically test QA-trained probes on code generation tasks across all three model sizes (1B, 3B, 8B), measuring accuracy per layer to identify whether any models or layers show meaningful cross-domain conflict detection capability.

2. **Steering Direction Asymmetry Analysis**: For each model-task combination, separately measure and compare $S_{PK}$ and $S_{CK}$ steering success rates to determine whether observed asymmetry (steering toward PK generally more effective) holds consistently across domains.

3. **Conflict Representation Robustness**: Test probing approach on synthetic conflict scenarios where conflict is explicitly manipulated (varying strength, plausibility, domain relevance) to determine whether linear separability in residual activations is robust to conflict characteristics.