---
ver: rpa2
title: Learning Causal States Under Partial Observability and Perturbation
arxiv_id: '2512.00357'
source_url: https://arxiv.org/abs/2512.00357
tags:
- causal
- bisimulation
- state
- cadiff
- logn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes CaDiff, a framework that enhances any reinforcement
  learning algorithm for perturbed partially observable Markov decision processes
  (P$^2$OMDPs). CaDiff addresses the challenge of making decisions under incomplete
  and noisy observations by uncovering the underlying causal structure of P$^2$OMDPs.
---

# Learning Causal States Under Partial Observability and Perturbation

## Quick Facts
- **arXiv ID:** 2512.00357
- **Source URL:** https://arxiv.org/abs/2512.00357
- **Reference count:** 40
- **Primary result:** CaDiff enhances RL performance on perturbed partially observable MDPs, achieving 14.18%+ return improvements over baselines.

## Executive Summary
This paper introduces CaDiff, a framework that enhances any RL algorithm for perturbed partially observable Markov decision processes (P²OMDPs). The core challenge is making decisions under incomplete and noisy observations where perturbations obscure the underlying causal structure. CaDiff addresses this by combining a novel asynchronous diffusion model (ADM) with a bisimulation metric to denoise observations while preserving causal relationships. ADM performs forward and reverse diffusion with asymmetric step counts to suppress perturbations, while the bisimulation metric quantifies similarity between perturbed observations and their denoised causal counterparts. Experiments on Roboschool tasks show CaDiff improves returns by at least 14.18% compared to baselines.

## Method Summary
CaDiff enhances RL algorithms by learning causal states under partial observability and perturbation. It uses an RNN encoder to map perturbed observations to noised causal states, then applies an asynchronous diffusion model (ADM) to denoise both observations and rewards. The ADM performs forward diffusion starting at step δ and reverses through more steps than were added, effectively removing the perturbation component. A bisimulation metric based on Wasserstein distance quantifies similarity between observation-conditioned and causal-state-conditioned distributions. The framework jointly optimizes diffusion losses (score matching) and bisimulation losses to produce denoised states that are reward- and transition-equivalent to ground-truth causal states. This denoised state representation is then used by the base RL algorithm.

## Key Results
- Achieves at least 14.18% return improvement over baselines (SAC, DMBP, DBC) across 6 Roboschool environments
- Demonstrates superior performance in early training stages while achieving highest final return
- Shows stable performance across varying noise scales (0.1-1.0) with optimal noise intensity δ requiring manual tuning
- Maintains computational efficiency with <7 GFLOPs overhead and ~200ms latency per update

## Why This Works (Mechanism)

### Mechanism 1
**Claim:** Forward-reverse diffusion with asymmetric step counts can suppress perturbations while preserving causal structure in partially observable states.
**Mechanism:** The Asynchronous Diffusion Model (ADM) initiates the forward process at step δ (treating the perturbed observation as already δ-step noised) and runs additional forward steps to K, then reverses through more steps than were added. By injecting less noise in the forward process than is suppressed in the reverse process, the net effect removes the perturbation component of P²OMDP observations. The noise intensity δ regulates the step difference, allowing dimension-wise denoising tuned to perturbation magnitude.
**Core assumption:** The perturbation in P²OMDP can be mathematically treated as a δ-step forward diffusion of the underlying noiseless observation—i.e., perturbations are approximately Gaussian noise superimposed on the true signal.
**Evidence anchors:**
- [abstract] "ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P²OMDP as part of the noise suppressed through diffusion."
- [section III-A] Definition 5 and equations (7a)-(7b) formalize starting the forward process at step δ and reversing with net noise removal; equation (9) defines the composite loss with early stopping k₀.
- [corpus] Partially observable RL papers (e.g., corpus neighbors on belief states, Bayesian approaches, and convolution/attention encoders) address partial observability but do not integrate diffusion-based denoising for perturbation mitigation, suggesting CaDiff's asymmetric diffusion mechanism is a distinct contribution.
**Break condition:** If perturbations are non-Gaussian, adversarially structured, or exhibit spatial/temporal correlations not captured by the δ-step assumption, ADM may over- or under-denoise, corrupting the causal signal.

### Mechanism 2
**Claim:** A bisimulation metric grounded in Wasserstein distance can quantify similarity between perturbed partial observations and their underlying causal states, enabling principled causal state representation learning.
**Mechanism:** The bisimulation metric (Definition 6) measures distance between observation-conditioned and causal-state-conditioned distributions over rewards and next-state transitions using p-Wasserstein distance. The RNN encoder ζ maps observations to noised causal states; ADM then denoises them. The CSR objective minimizes bisimulation loss (L̂_BS and L̂_BR), forcing the encoder to produce states that are reward- and transition-equivalent to the ground-truth causal states.
**Core assumption:** The environment's transition and reward dynamics admit a unique p-Wasserstein bisimulation metric satisfying a contraction property (Assumption 2: C_r + C_s < 1), ensuring the metric distance upper-bounds value differences.
**Evidence anchors:**
- [abstract] "The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts."
- [section III-B] Definition 6 formalizes the metric; equations following it define empirical loss functions L̂_BS and L̂_BR for training ζ with ADM outputs.
- [corpus] Related work on bisimulation for MDPs/POMDPs exists (e.g., DBC, cited in experiments), but CaDiff extends bisimulation to handle perturbations through diffusion-based denoising, which prior bisimulation methods do not address.
**Break condition:** If the encoder's aggregation radius bϵ is too large or the learned bisimulation metric diverges from the true metric due to insufficient data or network capacity, causal states may collapse or misrepresent the underlying structure.

### Mechanism 3
**Claim:** The value function approximation (VFA) error between perturbed observations and denoised causal states can be bounded, providing theoretical convergence guarantees under mild distributional assumptions.
**Mechanism:** The paper derives an upper bound on |V^π(s^c) - V^π(F̂(o))| that decomposes into: (1) bisimulation metric learning error E_ζ, (2) reward approximation error E_ϕ via ADM, and (3) transition model error E_θ via ADM (Theorem 2). Theorem 3 provides sample complexity for ADM distribution estimation using Hölder smoothness and Wasserstein-1 distance. As sample size n → ∞, the VFA error converges, yielding the guarantee in Theorem 4.
**Core assumption:** The conditional data distribution satisfies light-tail properties (Assumption 1: f(x^tr| ŝ_t, a_t) ≥ C and density proportional to exp(-C₁‖x‖²/2)·f(x), enabling ReLU network approximation with bounded covering numbers.
**Evidence anchors:**
- [abstract] "We establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model."
- [section IV] Theorem 2 states the VFA bound with model errors; Theorem 3 gives ADM approximation error in terms of sample size and smoothness; Theorem 4 combines these for the full convergence guarantee. Equation (17) shows the asymptotic bound.
- [corpus] Theoretical RL work on bisimulation metrics (e.g., Ferns et al. 2011, cited as [36]) provides foundational distance metrics, but CaDiff extends these to diffusion-based denoising with explicit sample complexity bounds.
**Break condition:** If the Hölder smoothness assumption is violated (e.g., discontinuous or highly irregular true distributions), the covering number bounds fail, and the sample complexity may not hold.

## Foundational Learning

- **Concept: Partially Observable Markov Decision Processes (POMDPs)**
  - Why needed here: The paper's setting, P²OMDP, extends POMDPs with perturbations; understanding belief states and history-dependence is prerequisite.
  - Quick check question: Can you explain why a POMDP policy must condition on observation history rather than the current observation alone?

- **Concept: Diffusion Models (Forward/Reverse Processes, Score Matching)**
  - Why needed here: ADM is the core denoising engine; understanding how diffusion progressively adds/removes noise via score networks is essential.
  - Quick check question: What role does the score function ∇log P(x_k | y) play in the reverse diffusion process?

- **Concept: Bisimulation and Wasserstein Distance**
  - Why needed here: The causal state representation relies on bisimulation equivalence measured via p-Wasserstein distance between distributions.
  - Quick check question: If two states have bisimulation distance zero, what does that imply about their reward and transition distributions?

## Architecture Onboarding

- **Component map:**
  1. **RNN Encoder (ζ)**: Maps raw perturbed observations o_t to noised causal states s̃_t ∈ S̃_c. Input: o_t; Output: s̃_t.
  2. **Asynchronous Diffusion Model (ADM)**:
     - Observation denoiser (θ): Takes s̃_{t+1}, (ŝ_t, a_t) as guidance; outputs denoised causal state ŝ_{t+1}.
     - Reward denoiser (ϕ): Takes r_{t+1}, (ŝ_t, a_t); outputs denoised reward r̂_{t+1}.
     - Both use conditional score networks with early stopping k₀ and terminal step K.
  3. **Bisimulation Loss Modules**: Compute L̂_BS (transition) and L̂_BR (reward) using Wasserstein distance between ADM outputs and ground-truth distributions.
  4. **Base RL Algorithm (e.g., SAC)**: Takes (ŝ_t, a_t, r̂_{t+1}, ŝ_{t+1}) as input; updates policy π.

- **Critical path:**
  1. Collect perturbed transition (o_t, a_t, r_{t+1}, o_{t+1}) in replay buffer D.
  2. Pass o_t, o_{t+1} through ζ → s̃_t, s̃_{t+1}.
  3. Pass s̃_{t+1} through ADM-θ with guidance (ŝ_t, a_t) → ŝ_{t+1} (requires iterative denoising from K to k₀).
  4. Pass r_{t+1} through ADM-ϕ with same guidance → r̂_{t+1}.
  5. Compute L̂_State, L̂_Rew (diffusion losses) + L̂_BS, L̂_BR (bisimulation losses).
  6. Update θ, ϕ, ζ via gradient descent; use (ŝ_t, a_t, r̂_{t+1}, ŝ_{t+1}) for SAC update.

- **Design tradeoffs:**
  - **Noise intensity δ vs. denoising quality**: Higher δ assumes stronger perturbation but risks over-denoising if actual noise is weaker. Tune δ based on noise scale (Table III shows optimal δ varies with noise scale 0.1→1.0).
  - **Diffusion steps K vs. compute cost**: More steps improve denoising but increase latency. Paper uses K=500 with ~200ms per update (Table IV).
  - **Bisimulation weights (C_r, C_s)**: Control trade-off between reward and transition fidelity. Must satisfy C_r + C_s < 1 for contraction.
  - **Early stopping k₀**: Prevents score blow-up at low noise levels; set empirically via sample complexity (Theorem 3: k₀ = n^{-O(1)}).

- **Failure signatures:**
  - **Denoised states collapse to near-zero variance**: ADM may be over-denoising; reduce δ or K.
  - **Bisimulation loss plateaus high**: Encoder ζ capacity insufficient or C_r, C_s poorly tuned.
  - **Returns degrade under high noise scales**: δ mismatch; increase δ for larger perturbations (Table III shows instability at noise scale 1.0).
  - **Training diverges**: Check that C_r + C_s < 1; verify score network initialization and learning rates (paper uses 0.0003 for all modules).

- **First 3 experiments:**
  1. **Perturbation robustness test**: Run CaDiff vs. SAC/DMBP/DBC on a single environment (e.g., Hopper-V) with noise scales {0.1, 0.5, 1.0} and δ ∈ {1, 2, 3}. Confirm CaDiff maintains returns as noise increases.
  2. **Ablation of ADM vs. bisimulation**: Disable each module separately (as in Fig. 3) to quantify contribution. Expect state denoising to dominate reward denoising in high-dimensional observation spaces.
  3. **Compute budget profiling**: Measure FLOPs and runtime per update across environments (replicate Table IV). Verify <7 GFLOPs overhead and ~200ms latency on your hardware.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the forward process noise intensity $\delta$ be determined adaptively or theoretically linked to the perturbation scale to ensure stability in high-noise regimes?
- **Basis in paper:** [Inferred] Section V-3 notes that "optimal noise intensity becomes unstable and fluctuates... when the noise scale increases to 1," requiring manual tuning, while Section III-A describes $\delta$ as a predefined parameter.
- **Why unresolved:** The paper fixes $\delta$ (e.g., 1, 2, 3) as a hyperparameter and observes instability at high noise scales without providing a mechanism to auto-tune or theoretically ground the choice of $\delta$ relative to the external perturbation intensity.
- **What evidence would resolve it:** A method that dynamically adjusts $\delta$ based on estimated noise variance or achieves stable performance across varying noise scales without requiring a manual grid search for the optimal intensity.

### Open Question 2
- **Question:** Does the Asynchronous Diffusion Model (ADM) scale effectively to high-dimensional visual observation spaces (e.g., images) while maintaining computational efficiency?
- **Basis in paper:** [Inferred] The experiments are limited to Roboschool tasks with low-dimensional vector observations (e.g., 15 to 28 dimensions), and the computational cost analysis (Table IV) is restricted to this setting.
- **Why unresolved:** Diffusion models are computationally expensive on high-dimensional data (images); it is unclear if the "asynchronous" nature and the theoretical computational bound ($\tilde{O}(\text{poly} \log d_i)$) hold effectively in practice for raw pixel inputs.
- **What evidence would resolve it:** Experimental results on standard visual RL benchmarks (e.g., DMControl from pixels) showing comparable sample efficiency and wall-clock time to state-based baselines, or a scaling analysis of GFLOPs with respect to input dimension.

### Open Question 3
- **Question:** How robust is the CaDiff framework to non-Gaussian or adversarial perturbations that do not align with the noise assumptions of the diffusion process?
- **Basis in paper:** [Inferred] Section V explicitly states that "rewards and observations are perturbed by zero-mean Gaussian noise," and the theoretical guarantees (Theorem 3) rely on Assumption 1 (light-tail conditional data distribution) and specific H¨older smoothness.
- **Why unresolved:** The core denoising mechanism (ADM) is designed to suppress Gaussian noise interpreted through the forward process; perturbations with heavy tails, structured noise, or adversarial patterns could be misinterpreted as causal signal or fail to be suppressed.
- **What evidence would resolve it:** Evaluation results in environments with structured noise (e.g., impulse noise) or adversarial attacks, demonstrating convergence and maintaining the value function approximation bounds derived for the Gaussian case.

## Limitations
- **Gaussian assumption**: Framework assumes zero-mean Gaussian perturbations, limiting applicability to non-Gaussian or adversarial noise patterns
- **Computational overhead**: Requires ~200ms per update and <7 GFLOPs overhead, which may scale poorly to high-dimensional observation spaces
- **Manual hyperparameter tuning**: Optimal noise intensity δ requires manual grid search and becomes unstable at high noise scales (1.0)

## Confidence
- **High**: The diffusion-based denoising mechanism (Mechanism 1) and its empirical effectiveness in reducing perturbation impact
- **Medium**: The bisimulation metric formulation and its integration with ADM for causal state representation (Mechanism 2)
- **Medium**: The theoretical convergence guarantee (Theorem 4) and its applicability to the empirical results (Mechanism 3)

## Next Checks
1. **Assumption Validation**: Conduct experiments to empirically verify the light-tail distribution assumption (Assumption 1) by analyzing the empirical distributions of the perturbed observations and denoised outputs
2. **Bound Tightness**: Compare the theoretical VFA error bound (Theorem 2) to the empirical value function errors observed during training across different noise scales
3. **Scalability Test**: Evaluate CaDiff on a higher-dimensional observation space (e.g., a pixel-based environment) to assess computational feasibility and performance degradation