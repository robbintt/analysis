---
ver: rpa2
title: Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization
  for LLM-empowered Agent in Social Simulation
arxiv_id: '2510.13195'
source_url: https://arxiv.org/abs/2510.13195
tags:
- agents
- emotional
- agent
- framework
- llm-based
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of incorporating emotional cognition
  into large language model (LLM)-based agents for social simulation, as current agents
  lack the bounded rationality and emotional decision-making essential for realistic
  human-like behavior. The authors propose an emotional cognitive modeling framework
  that integrates desire generation and objective management, enabling agents to dynamically
  generate desires based on state changes and optimize objectives accordingly.
---

# Emotional Cognitive Modeling Framework with Desire-Driven Objective Optimization for LLM-empowered Agent in Social Simulation

## Quick Facts
- arXiv ID: 2510.13195
- Source URL: https://arxiv.org/abs/2510.13195
- Reference count: 37
- LLM-based agents with emotional cognition outperform rule-based, imitation learning, RL, and GPT-4o agents in social simulation fidelity

## Executive Summary
This paper addresses the challenge of incorporating emotional cognition into large language model (LLM)-based agents for social simulation, as current agents lack the bounded rationality and emotional decision-making essential for realistic human-like behavior. The authors propose an emotional cognitive modeling framework that integrates desire generation and objective management, enabling agents to dynamically generate desires based on state changes and optimize objectives accordingly. The framework was implemented in a proprietary multi-agent interaction environment simulating an online food delivery ecosystem. Experimental results demonstrate that agents governed by the proposed framework exhibit behaviors congruent with their emotional states and generate decision outcomes that more closely approximate human behavioral patterns compared to rule-based, imitation learning, reinforcement learning, and GPT-4o-driven agents.

## Method Summary
The framework implements a three-module system: (1) Information Processing System computes PAD-based emotional states from state variables (income, health, social rank); (2) Desire-Driven Objective Optimizer (L1/L2) generates desire vectors from emotional transitions and optimizes objectives via modified prompts; (3) Decision-Behavior System uses a frozen LLM to generate actions and rationales, storing them in a three-index memory system. The system was tested in a food delivery simulation with 6 rider agents over 30 days, comparing against rule-based, RL, and GPT-4o baselines using involution and DTW similarity metrics.

## Key Results
- Agents governed by the proposed framework exhibit behaviors congruent with their emotional states
- Generated decision outcomes more closely approximate human behavioral patterns compared to rule-based, imitation learning, reinforcement learning, and GPT-4o-driven agents
- In mixed agent simulation, framework-driven agents achieved DTW similarity score of 265.08 vs 274.24 for RL agents and 326.29 for GPT-4o agents

## Why This Works (Mechanism)

### Mechanism 1: PAD-Based Emotional State Grounding
Emotional states derived from measurable state variables enable agents to exhibit bounded rationality by coupling internal affect to environmental outcomes. The framework computes a three-dimensional PAD (Pleasure-Arousal-Dominance) vector where Pleasure ∝ income change (k_p(I_t - I_{t-1})), Arousal ∝ health/stamina change (k_a(H_t - H_{t-1})), and Dominance ∝ social rank tier. These quantitative mappings ground abstract "emotion" in observable state deltas, triggering downstream desire updates.

### Mechanism 2: Desire-Triggered Objective Optimization
Emotional state transitions activate a desire update process that reprioritizes objectives, enabling dynamic, context-sensitive planning rather than static utility maximization. When emotional state E_t shifts significantly, Module L1 detects the transition and evaluates which state attribute is anomalous. A new desire vector D_t is generated, which feeds into Module L2 to reconfigure the reward function Reward(ΔI, ΔH, ΔSR). This yields modified prompts P_t that bias LLM generation toward the updated objective.

### Mechanism 3: Rationale-Augmented Memory for State-Decision Consistency
Storing decision rationales alongside outcomes enforces long-term consistency between an agent's internal states, declared goals, and behavioral history. Each decision episode records (problem, decision, rationale). On new decisions, memory is filtered by three indices: similarity via embeddings, importance via LLM evaluation, and timeliness with penalties for stale entries. Retrieved rationales are injected into the prompt, showing the LLM its past "knowledge-state" reasoning.

## Foundational Learning

- Concept: PAD (Pleasure-Arousal-Dominance) Emotional Model
  - Why needed here: The framework's state perception module outputs PAD vectors; understanding how these dimensions map to state variables is essential for adapting the system to new domains.
  - Quick check question: Given a new domain with state variables (trust, time_pressure, team_size), can you propose plausible mappings to P, A, and D?

- Concept: Prompt Optimization as Policy Shaping
  - Why needed here: Objective optimization is implemented via modified prompts rather than model fine-tuning; engineers must understand how prompt engineering substitutes for direct policy updates.
  - Quick check question: If the reward function shifts to prioritize health over income, what concrete prompt modifications would bias an LLM toward rejecting high-income, high-fatigue orders?

- Concept: Dynamic Time Warping (DTW) for Sequence Similarity
  - Why needed here: The paper's primary consistency metric (DTW between income and happiness sequences) requires understanding DTW's tolerance for temporal misalignment.
  - Quick check question: Two agents have identical income patterns but one's happiness lags by 2 timesteps; will DTW report high or low similarity, and is this desirable for your evaluation goals?

## Architecture Onboarding

- Component map: Information Processing System -> Desire-Driven Objective Optimizer (L1/L2) -> Decision-Behavior System -> Memory Module
- Critical path: State change detected → PAD computed → emotional transition identified → L1 fires desire update if emotional delta exceeds threshold → L2 generates modified prompt → LLM generates decision + rationale → action executed → memory updated
- Design tradeoffs: Frozen LLM vs. fine-tuning enables deployment without training infrastructure but limits domain-specific emotional nuance; tripartite desire architecture is simple and interpretable but may not generalize to non-economic social simulations
- Failure signatures: Oscillating desires (rapid emotional fluctuations cause frequent objective switches); rationale drift (stored rationales become disconnected from actual decision factors); memory bloat (slow retrieval over long simulation runs); emotional flatlining (PAD values remain near-zero)
- First 3 experiments: 1) PAD sensitivity calibration: Run single-agent simulation varying k_p and k_a; confirm emotional transitions correlate with observable state changes and desire updates trigger as expected. 2) Baseline comparison on involution metric: Replicate Figure 3(a) comparing framework agents vs. rule-based and RL agents on Involution(t); verify framework produces curves closer to real Zomato data. 3) Memory ablation: Disable rationale storage or one index (e.g., importance); measure DTW consistency score degradation to quantify memory's contribution to State-Desire-Behavior coherence.

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's PAD-based emotional grounding is conceptually sound but lacks independent validation beyond the authors' simulations; the mapping from domain-specific state variables to pleasure-arousal-dominance dimensions may not generalize to non-economic domains.
- The desire-objective pipeline's effectiveness hinges on the precise formulation of the reward function and prompt modification strategy, neither of which are fully specified in the paper.
- Memory system contributions are difficult to isolate; the three-index filtering scheme adds complexity without clear evidence that simpler mechanisms would fail.

## Confidence

- **High**: PAD equations are formally specified and tied to observable state changes; the overall architecture (L1/L2 desire-objective modules) is clearly described.
- **Medium**: DTW-based similarity results show consistent advantage over baselines, but the proprietary simulator and undisclosed prompt templates prevent independent replication.
- **Low**: Claims about "more human-like" decision patterns rely on qualitative alignment with Zomato data without statistical significance testing or ablation studies for individual components.

## Next Checks

1. **Reward Function Sensitivity**: Systematically vary β weights in the tripartite reward function; verify that agents switch from income-maximizing to health-stabilizing behavior as intended, and that no single objective dominates across all configurations.
2. **Memory Ablation Study**: Run identical simulations with (a) full memory system, (b) memory without importance filtering, (c) memory without timeliness filtering, (d) no memory; measure changes in DTW similarity and involution metrics to quantify each component's contribution.
3. **Cross-Domain Transfer**: Apply the framework to a non-economic simulation (e.g., political debate or familial interaction) with state variables mapped to PAD dimensions; assess whether desire updates and objective optimization still produce coherent, bounded-rational behavior.