---
ver: rpa2
title: 'ORCA: Open-ended Response Correctness Assessment for Audio Question Answering'
arxiv_id: '2512.09066'
source_url: https://arxiv.org/abs/2512.09066
tags:
- answer
- audio
- orca
- human
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of evaluating open-ended responses
  from large audio language models (LALMs), where human annotators often disagree
  due to multiple valid interpretations, partial correctness, and subjective judgment.
  Traditional evaluation metrics that report only mean scores fail to capture this
  uncertainty.
---

# ORCA: Open-ended Response Correctness Assessment for Audio Question Answering

## Quick Facts
- arXiv ID: 2512.09066
- Source URL: https://arxiv.org/abs/2512.09066
- Reference count: 28
- Primary result: Beta distribution modeling achieves 0.91 Spearman correlation with human judgments while providing uncertainty estimates

## Executive Summary
This paper addresses the challenge of evaluating open-ended responses from large audio language models (LALMs), where human annotators often disagree due to multiple valid interpretations and subjective judgment. The core contribution is ORCA, a method that models human judgment variability using Beta distributions to predict both expected correctness and uncertainty. The approach employs a three-stage annotation framework that combines human judgment with structured feedback and iterative refinement to curate training data. Results show ORCA matches or outperforms LLM-judge baselines while requiring significantly less compute and providing calibrated uncertainty estimates.

## Method Summary
ORCA predicts Beta distribution parameters (α, β) to model correctness uncertainty, using text inputs concatenated as [question; reference_answer; rationale; transcript; candidate_answer]. The model is trained to maximize Beta log-likelihood of human ratings, with a single-layer MLP head predicting log α and log β from the final LLM hidden state. A three-stage annotation framework collects correctness ratings with structured feedback codes, applies human-AI collaborative corrections, and filters unreliable annotations. The approach uses clamping post-processing to convert uncertain predictions near boundaries into hard 0/1 scores.

## Key Results
- Achieves 0.91 Spearman correlation with mean human judgments on 3,580 question-answer pairs from 15 LALMs
- Outperforms LLM-judge baselines while requiring significantly less compute
- Captures genuine annotator disagreement, with 17.7% of pairs showing variance >1.0
- Gemma3-12B achieves best performance (0.91 Spearman), while Gemma3-270M offers fastest inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Beta distributions capture human annotator disagreement as structured uncertainty rather than noise.
- Mechanism: ORCA predicts two parameters (α, β) via an MLP head, which define a Beta distribution over normalized correctness scores [0,1]. The expected correctness is μ = α/(α+β); the variance σ² = αβ/(α+β)²(α+β+1) quantifies annotator disagreement. Training maximizes the log-likelihood of all individual ratings per instance.
- Core assumption: Human disagreement on correctness is meaningful signal (genuine ambiguity), not pure noise, and follows a distribution that Beta can approximate.
- Evidence anchors: [abstract] "models the variability in human judgments using Beta distributions to predict both expected correctness and uncertainty"; [section 4.2] "The Beta distribution is a natural choice... it is flexible enough to capture diverse rating patterns: high consensus (low variance)... high disagreement (high variance)... even U-shaped bimodal distributions"

### Mechanism 2
- Claim: Structured feedback enables iterative benchmark refinement and higher-quality training data.
- Mechanism: Stage 2 collects correctness ratings plus structured feedback codes (Q: incomplete question, A: insufficient rationale, R: incorrect reference, U: ambiguous, E: lacking expertise). Stage 3 applies human-AI collaborative corrections, and corrected data re-enters the pipeline. Filtering unreliable annotations (those with low agreement on flagged items) improves downstream training.
- Core assumption: Feedback categories map to actionable corrections, and experts can reliably fix flagged instances.
- Evidence anchors: [abstract] "three-stage annotation framework combines human judgment with structured feedback and iterative refinement"; [section 5.2] "filtered ratings had substantially lower agreement (α=0.59), confirming they represented unreliable instances... Remaining valid ratings showed improved agreement (α=0.82)"

### Mechanism 3
- Claim: Text-only evaluation with rationales avoids circular audio-model-judging-audio dependency while remaining human-aligned.
- Mechanism: ORCA uses concatenated text inputs (question, reference answer, rationale, transcript, candidate answer). Rationales ground the reference in audio content without requiring the evaluator to process audio directly. The model learns to map textual context to Beta parameters.
- Core assumption: Rationales capture sufficient grounding; text-only evaluation approximates human judgment without audio access.
- Evidence anchors: [section 3.2] "Human annotators evaluate answer correctness given five pieces of textual information... When textual context proves insufficient, annotators can listen to the original audio directly"; [section 6.3, Figure 4] "removing the rationale... worsen[s] the MAE performance... more significant performance gap... when the question is removed"

## Foundational Learning

- Concept: Beta distribution properties (support on [0,1], mean/variance formulas, flexibility for unimodal/bimodal shapes)
  - Why needed here: Understanding how α and β parameterize the correctness distribution and why variance indicates annotator disagreement.
  - Quick check question: Given α=2, β=2, what are μ and σ²? (Answer: μ=0.5, σ²=0.05)

- Concept: Maximum likelihood estimation for continuous distributions
  - Why needed here: ORCA's training objective maximizes the sum of log-likelihoods under the predicted Beta distribution.
  - Quick check question: Why use log-likelihood instead of direct likelihood? (Answer: Numerical stability; converts products to sums)

- Concept: Inter-annotator agreement metrics (Krippendorff's alpha)
  - Why needed here: Distinguishing genuine disagreement (high variance, high agreement on that variance) from annotation noise.
  - Quick check question: If α=0.82 indicates "substantial agreement," what does α=0.59 suggest? (Answer: Moderate/lower agreement, potentially unreliable annotations)

## Architecture Onboarding

- Component map: Input tokens → Pre-trained LLM encoder → Final hidden state → MLP head → (log α, log β) → Beta parameters (α, β)
- Critical path:
  1. Prepare text inputs (ensure rationales exist; optionally add transcripts for speech)
  2. Forward pass through LLM → extract final hidden representation
  3. MLP predicts log α, log β → exponentiate for positive parameters
  4. Compute μ, σ² for evaluation; apply clamping if applicable

- Design tradeoffs:
  - Model scale vs. inference cost: Gemma3-270M is fastest; Gemma3-12B achieves highest Spearman (0.91)
  - Rationale inclusion vs. noise: Rationale improves performance; transcript marginally hurts MAE
  - Clamping vs. soft predictions: Clamping improves ranking metrics but may overconfidently assign 0/1

- Failure signatures:
  - Low variance predictions on genuinely ambiguous items → model underestimates uncertainty
  - High MAE on unseen LALM response styles (e.g., Audio-Reasoner's long outputs) → training distribution mismatch
  - High MAE σ² → Beta parameters not capturing human disagreement patterns

- First 3 experiments:
  1. Baseline validation: Train ORCA-OLMo2-1B on human data, evaluate Spearman/MAE on held-out questions; compare to Gemini-2.5-Flash judge.
  2. Input ablation: Remove rationale, remove question, remove transcript; measure MAE change to confirm feature importance.
  3. Generalization test: Hold out 2 LALMs (e.g., Audio-Reasoner, GAMA), train on remaining 13; evaluate whether variance predictions remain calibrated on unseen response styles.

## Open Questions the Paper Calls Out

- Can the framework generalize to LALMs with distinct response styles, such as those generating significantly longer reasoning chains than those seen during training? [explicit] Section 6.2 notes that ORCA struggles with Audio-Reasoner (an unseen LALM) because it generates "significantly longer responses," highlighting the need for training data diversity.

- To what extent can purely synthetic LLM-judge data replace human annotations during pretraining without sacrificing correlation with human judgment? [explicit] Section 6.4 suggests there is "potential in leveraging the lower-agreement LLM-judge data for pretraining" to reduce human annotation costs, but does not quantify the limits of this substitution.

- Does the text-only input approach (relying on transcripts and rationales) fail to capture non-textual acoustic features that influence human correctness judgments? [inferred] Section 3.2 allows annotators to listen to audio "when textual context proves insufficient," implying text representations may miss subjective acoustic nuances like tone or sarcasm.

## Limitations
- Beta distribution assumptions may not fully capture multi-modal nature of human disagreement in certain edge cases
- Text-only evaluation generalization remains uncertain - whether textual proxies can fully capture audio-specific nuances
- Iterative refinement effectiveness shows substantial remaining uncertainty (17.7% of pairs with variance >1.0)

## Confidence
- High confidence: Beta distribution modeling mechanism is well-supported with sound mathematical formulation and strong correlation results (0.91 Spearman)
- Medium confidence: Structured feedback framework shows promise through improved agreement metrics but causal relationship is partially inferred
- Medium confidence: Text-only evaluation approach demonstrates practical effectiveness but relies on assumptions about rationale sufficiency

## Next Checks
1. Distribution fit analysis: For high-variance question-answer pairs, analyze whether actual human rating distributions follow Beta-like patterns or exhibit multi-modality that Beta cannot capture.

2. Audio access ablation study: Compare ORCA's performance when given access to audio versus relying solely on transcripts and rationales to quantify the cost of avoiding circular evaluation.

3. Cross-modal generalization: Evaluate ORCA's uncertainty predictions on question-answer pairs spanning multiple modalities (speech, music, sound) to test whether Beta parameters generalize across different audio contexts.