---
ver: rpa2
title: Interpretable and Steerable Concept Bottleneck Sparse Autoencoders
arxiv_id: '2512.10805'
source_url: https://arxiv.org/abs/2512.10805
tags:
- neurons
- concept
- steerability
- interpretability
- cb-sae
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors address a key challenge in mechanistic interpretability:
  most neurons in sparse autoencoders (SAEs) for vision-language models are either
  uninterpretable or unsteerable, and SAEs often miss user-specified concepts. To
  solve this, they propose Concept Bottleneck Sparse Autoencoders (CB-SAE), a framework
  that prunes low-utility SAE neurons and augments the latent space with a concept
  bottleneck trained on user-specified concepts.'
---

# Interpretable and Steerable Concept Bottleneck Sparse Autoencoders

## Quick Facts
- **arXiv ID**: 2512.10805
- **Source URL**: https://arxiv.org/abs/2512.10805
- **Reference count**: 40
- **Key outcome**: CB-SAE achieves 32.1% higher interpretability and 14.5% higher steerability compared to baseline SAEs for vision-language models

## Executive Summary
The paper addresses a fundamental challenge in mechanistic interpretability: most neurons in sparse autoencoders (SAEs) for vision-language models are either uninterpretable or unsteerable, and standard SAEs often miss user-specified concepts. To solve this, the authors propose Concept Bottleneck Sparse Autoencoders (CB-SAE), a framework that prunes low-utility SAE neurons and augments the latent space with a concept bottleneck trained on user-specified concepts. CB-SAE is trained using three objectives: reconstruction, interpretability, and steerability. The framework is evaluated on LLaVA and UnCLIP, demonstrating significant improvements in both understanding and controllable behavior in vision-language models and image generation.

## Method Summary
CB-SAE addresses the limitations of standard SAEs by introducing a concept bottleneck layer that is trained alongside the sparse autoencoder architecture. The method employs a three-objective training approach: reconstruction to maintain input fidelity, interpretability to ensure neurons correspond to meaningful concepts, and steerability to enable controllable model behavior. The framework prunes neurons with low utility and replaces them with concept-specific representations, allowing users to specify concepts of interest that are then integrated into the model's latent space. This approach enables both better understanding of model representations and the ability to steer model behavior based on user-specified concepts.

## Key Results
- CB-SAE achieves 32.1% higher interpretability compared to baseline SAEs
- CB-SAE achieves 14.5% higher steerability compared to baseline SAEs
- Demonstrated effectiveness on both LLaVA and UnCLIP architectures

## Why This Works (Mechanism)
CB-SAE works by combining the representational power of sparse autoencoders with explicit concept supervision. By pruning uninterpretable neurons and introducing a concept bottleneck, the framework creates a more structured latent space that aligns with human-understandable concepts. The three-objective training ensures that the model maintains reconstruction quality while improving both interpretability and steerability. The concept bottleneck acts as a bridge between the learned representations and user-specified concepts, enabling more precise control over model behavior.

## Foundational Learning

**Sparse Autoencoders (SAEs)**: Neural networks that learn compressed representations by enforcing sparsity in the latent space, useful for identifying individual features in complex models. *Why needed*: SAEs can identify individual features but often produce uninterpretable or unsteerable representations. *Quick check*: Verify that SAEs learn sparse representations by examining activation patterns.

**Concept Bottleneck Models**: Architectures that incorporate explicit concept representations as intermediate steps between input and output. *Why needed*: Standard SAEs miss user-specified concepts and lack controllable behavior. *Quick check*: Confirm that concept bottlenecks can accurately predict specified concepts from latent representations.

**Mechanistic Interpretability**: The study of understanding how neural networks work by examining their internal representations and decision-making processes. *Why needed*: Essential for building trust in AI systems and enabling controlled modifications. *Quick check*: Ensure interpretability metrics align with human judgments of concept meaningfulness.

## Architecture Onboarding

**Component Map**: Input -> Sparse Autoencoder -> Concept Bottleneck -> Output
- Sparse Autoencoder: Learns compressed latent representations
- Concept Bottleneck: Maps latent space to user-specified concepts
- Three training objectives: Reconstruction, Interpretability, Steerability

**Critical Path**: The concept bottleneck layer is the critical component, as it directly enables the interpretability and steerability improvements by providing explicit concept representations.

**Design Tradeoffs**: The approach trades some reconstruction capacity for improved interpretability and steerability. The concept bottleneck may introduce biases toward specified concepts while potentially missing other important representations.

**Failure Signatures**: Poor concept selection by users may lead to missed important representations. The framework may struggle with concepts that are not well-represented in the training data or are too abstract to capture effectively.

**First 3 Experiments**:
1. Compare CB-SAE interpretability metrics against baseline SAEs on a simple dataset with known concepts
2. Test steerability by attempting to control model outputs through concept manipulation
3. Evaluate reconstruction quality degradation when adding the concept bottleneck layer

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focuses on quantitative metrics without thorough qualitative analysis of concept meaningfulness
- Comparison doesn't address whether simpler pruning approaches could achieve similar results
- Potential biases introduced by user-specified concept selection that may miss important unmeasured representations
- Uncertain generalizability of improvements across different model architectures and datasets

## Confidence

**High confidence**: Technical implementation and three-objective training framework are well-described and sound.

**Medium confidence**: Reported improvements in interpretability (32.1%) and steerability (14.5%) need further validation across different concept sets and model scales.

**Low confidence**: Claims about enabling precise controllable behavior may be overstated without concrete examples of granular steering control.

## Next Checks

1. **Ablation study on concept bottleneck utility**: Systematically evaluate whether the concept bottleneck layer provides benefits beyond simple pruning of low-utility neurons, including tests on randomly selected concepts versus semantically meaningful ones.

2. **Cross-dataset generalization**: Test whether CB-SAE trained on specific concept sets maintains its interpretability and steerability advantages when applied to datasets containing novel concepts or domains not present in the training data.

3. **Steerable control granularity analysis**: Conduct detailed experiments measuring the precision and granularity of control achievable through CB-SAE's steerability mechanisms, including failure cases and limitations in practical steering scenarios.