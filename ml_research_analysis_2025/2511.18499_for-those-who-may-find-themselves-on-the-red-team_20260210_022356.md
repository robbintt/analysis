---
ver: rpa2
title: For Those Who May Find Themselves on the Red Team
arxiv_id: '2511.18499'
source_url: https://arxiv.org/abs/2511.18499
tags:
- language
- interpretability
- tokens
- about
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This position paper argues that literary scholars should engage
  with large language model (LLM) interpretability research, particularly through
  participation in "red teams" - groups that conduct adversarial testing of AI systems.
  The author contends that current approaches to interpretability in AI are narrowly
  instrumental, focused on optimizing model performance and addressing failures, rather
  than exploring broader questions about language and meaning.
---

# For Those Who May Find Themselves on the Red Team
## Quick Facts
- arXiv ID: 2511.18499
- Source URL: https://arxiv.org/abs/2511.18499
- Reference count: 7
- This position paper argues that literary scholars should engage with large language model interpretability research through red teaming to prevent the field from remaining dominated by narrowly instrumental approaches.

## Executive Summary
This position paper argues that literary scholars should engage with large language model interpretability research, particularly through participation in "red teams" - groups that conduct adversarial testing of AI systems. The author contends that current approaches to interpretability in AI are narrowly instrumental, focused on optimizing model performance and addressing failures, rather than exploring broader questions about language and meaning. Through analyzing a case study involving "glitch tokens" in GPT models, the paper demonstrates how adversarial attacks reveal unexpected behaviors and prompt deeper questions about semantic representation in high-dimensional embedding spaces.

The paper acknowledges that engagement with interpretability research involves ideological complicity with certain rationalist and corporate frameworks, but argues this engagement is necessary to prevent LLM interpretation from remaining solely the domain of computer and data sciences focused on profit generation and existential risk mitigation. The author calls for humanities scholars to bring their expertise in meaning-making and language to bear on questions of AI interpretability, suggesting this could lead to more nuanced and comprehensive approaches to understanding how these systems work.

## Method Summary
This is a position paper that uses conceptual analysis and a case study approach rather than empirical research. The author examines existing literature on AI interpretability and adversarial testing (red teaming), then analyzes a specific case study involving "glitch tokens" in GPT models to illustrate how humanities approaches could contribute to interpretability research. The paper does not present original experimental data but rather argues for a particular methodological and disciplinary stance.

## Key Results
- Current LLM interpretability research is dominated by narrowly instrumental approaches focused on optimization and failure prevention
- Red teaming (adversarial testing) reveals unexpected model behaviors that raise deeper questions about semantic representation
- Humanities scholars, particularly literary scholars, have unique contributions to make to interpretability research through their expertise in meaning-making and language

## Why This Works (Mechanism)
The paper's argument works by identifying a gap between the current state of AI interpretability research and the potential for more nuanced approaches that incorporate humanities perspectives. By framing red teaming as a collaborative space where humanities scholars can contribute, the paper creates a concrete entry point for interdisciplinary engagement. The mechanism relies on demonstrating how adversarial testing can reveal unexpected model behaviors that prompt philosophical questions about language and meaning, thus showing the value of humanities expertise in interpreting these phenomena.

## Foundational Learning
- **Red teaming in AI**: Testing AI systems through adversarial attacks to identify vulnerabilities; needed to understand the proposed entry point for humanities engagement; quick check: can identify common red teaming methodologies and their purposes.
- **LLM interpretability**: The study of how large language models work internally; needed to grasp why the author argues for humanities involvement; quick check: can explain basic approaches to understanding LLM internal mechanisms.
- **Glitch tokens**: Special tokens that cause unexpected model behaviors; needed to understand the case study; quick check: can describe what happens when models encounter these tokens.
- **High-dimensional embedding spaces**: Mathematical representations where words and concepts exist as vectors; needed to appreciate the technical context; quick check: can explain what embeddings represent in LLMs.
- **Ideological complicity**: The unavoidable adoption of certain frameworks when engaging with technical systems; needed to understand the paper's acknowledgment of tensions; quick check: can identify examples of how disciplines adopt dominant paradigms.
- **Instrumental rationality**: Approaches focused purely on optimization and efficiency; needed to grasp what the paper critiques; quick check: can contrast instrumental with other forms of rationality.

## Architecture Onboarding
The paper presents an argument architecture rather than a technical system architecture, but the logical flow can be mapped as:
1. Problem identification (narrowly instrumental interpretability research)
2. Proposed solution (humanities engagement through red teaming)
3. Supporting evidence (glitch token case study)
4. Acknowledgment of tensions (ideological complicity)
5. Call to action (humanities scholars should participate)

Critical path: Problem identification -> Proposed solution -> Supporting evidence -> Acknowledgment of tensions -> Call to action

Design tradeoffs: The paper prioritizes conceptual clarity over empirical evidence, focusing on making a persuasive argument rather than presenting systematic data about current research practices.

Failure signatures: The argument could fail if (1) humanities approaches genuinely add little value to interpretability research, (2) the ideological compromises are too severe to justify engagement, or (3) red teaming proves ineffective as an entry point for interdisciplinary collaboration.

First experiments to validate the approach:
1. Identify a specific interpretability research question and map how humanities methods could contribute
2. Design a red teaming exercise that specifically leverages humanities expertise
3. Document and analyze the collaboration between humanities and technical researchers on a interpretability problem

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several emerge from its analysis: What specific methodologies would humanities scholars bring to interpretability research? How can ideological complicity be managed when engaging with corporate and rationalist frameworks? What concrete examples exist of successful humanities-technical collaborations in AI interpretability? How can the value of humanities contributions be measured and validated in technical research contexts?

## Limitations
- The paper presents a binary opposition between humanities and technical approaches that may oversimplify the actual interdisciplinary nature of current interpretability work
- The case study of glitch tokens, while illustrative, represents a relatively narrow example that may not fully capture the breadth of opportunities for humanities scholars
- The argument lacks systematic evidence about the diversity of research motivations in the field of interpretability

## Confidence
Medium confidence in the necessity of humanities engagement - while the argument for interdisciplinary collaboration is compelling, the paper provides limited concrete examples of what productive humanities engagement would actually look like beyond the red teaming framework.

Low confidence in the assertion that humanities engagement is the only way to prevent interpretability research from remaining solely in corporate and rationalist hands - the paper doesn't adequately address existing critical work in AI ethics and philosophy of technology.

## Next Checks
1. Survey current interpretability researchers to map their actual motivations, values, and awareness of philosophical questions about language and meaning, testing whether the field is as narrowly instrumental as claimed.

2. Identify and analyze existing interdisciplinary collaborations between humanities scholars and AI researchers to determine what forms of engagement have already proven productive and what barriers exist.

3. Conduct a systematic review of interpretability literature to quantify the proportion of work focused purely on optimization versus work that engages with broader questions about language, meaning, and representation.