---
ver: rpa2
title: A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution
  Mismatch
arxiv_id: '2510.16911'
source_url: https://arxiv.org/abs/2510.16911
tags:
- forecasting
- data
- consumption
- feature
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a lightweight deep learning framework for short-term
  power consumption forecasting under asymmetric, noisy, and incomplete input conditions.
  The method integrates hourly downsampling, dual-mode imputation (mean and polynomial
  regression), and multiple normalization schemes (Standard Scaling, Min-Max, Z-Score)
  with a hybrid GRU-LSTM model.
---

# A Lightweight DL Model for Smart Grid Power Forecasting with Feature and Resolution Mismatch

## Quick Facts
- arXiv ID: 2510.16911
- Source URL: https://arxiv.org/abs/2510.16911
- Reference count: 12
- Primary result: Achieves 84.36% accuracy in next-day power forecasting using only temperature and timestamp at test time

## Executive Summary
This paper presents a lightweight deep learning framework for short-term power consumption forecasting under asymmetric, noisy, and incomplete input conditions. The method integrates hourly downsampling, dual-mode imputation (mean and polynomial regression), and multiple normalization schemes (Standard Scaling, Min-Max, Z-Score) with a hybrid GRU-LSTM model. Trained on one year of 5-minute data and validated over 40 days, the model achieves an average RMSE of 601.9 W, MAE of 468.9 W, and 84.36% accuracy on next-day forecasts using only temperature and timestamp as test inputs. Standard scaling yields the most robust performance, with low inference latency (0.065–0.17 sec) and strong generalization across five test days. Spatiotemporal analysis confirms alignment between temperature trends and predicted consumption. Results demonstrate that targeted preprocessing and compact recurrent architectures enable fast, accurate, and deployment-ready energy forecasting in real-world conditions.

## Method Summary
The framework addresses asymmetric input conditions where training data contains six features (timestamp, power, voltage, current, PV generation, temperature) at 5-minute resolution, but test data only provides temperature and timestamp at hourly resolution. The method applies hourly downsampling via mean aggregation, polynomial regression imputation for missing features using temperature as predictor, and multiple normalization schemes with Standard Scaling identified as optimal. The hybrid BiGRU-LSTM architecture processes sequences to predict next-step power consumption, achieving high accuracy with minimal computational overhead suitable for real-world deployment.

## Key Results
- RMSE of 601.9 W and MAE of 468.9 W on next-day hourly power forecasts
- 84.36% accuracy with only temperature and timestamp as test inputs
- Inference latency between 0.065–0.17 seconds per prediction
- Standard Scaling normalization shows superior robustness across all test days
- Strong spatiotemporal alignment between temperature trends and predicted consumption

## Why This Works (Mechanism)
The framework's effectiveness stems from addressing the core challenge of asymmetric inputs through intelligent preprocessing. Downsampling reduces noise while preserving temporal patterns, polynomial regression imputation leverages temperature-power correlations to reconstruct missing features, and Standard Scaling provides stable normalization across varying feature distributions. The hybrid BiGRU-LSTM architecture captures both forward and backward temporal dependencies efficiently, while the lightweight design ensures real-time deployment capability. The methodology demonstrates that careful feature engineering and compact architectures can overcome significant data asymmetry in energy forecasting.

## Foundational Learning

### Temporal Downsampling
- Why needed: Reduces noise and computational load while preserving meaningful consumption patterns
- Quick check: Compare RMSE before/after downsampling on validation set

### Feature Imputation via Polynomial Regression
- Why needed: Reconstructs missing voltage, current, and PV generation using temperature-power correlations
- Quick check: Validate imputation accuracy by comparing predicted vs actual features on validation set

### Sequence-to-One Prediction
- Why needed: Efficiently forecasts next time step using historical context without excessive computational overhead
- Quick check: Verify prediction accuracy improves with optimal sequence length

## Architecture Onboarding

### Component Map
Data → Downsample (5min→hourly) → Impute Missing Features → Normalize → BiGRU → LSTM → Dense → Forecast

### Critical Path
The sequence processing pipeline: BiGRU(256,ReLU) → Dropout → LSTM(128,ReLU) → Dropout → Dense(1) represents the core forecasting mechanism

### Design Tradeoffs
Hybrid BiGRU-LSTM balances bidirectional context capture with computational efficiency versus deeper transformer architectures that would increase latency beyond real-time requirements

### Failure Signatures
- RMSE >1500W indicates incorrect normalization (likely Min-Max instead of Standard Scaling)
- Accuracy drop on specific days suggests imputation failure under distribution shifts
- High latency (>0.2s) suggests architectural complexity issues

### First Experiments
1. Train with different normalization schemes to confirm Standard Scaling superiority
2. Test polynomial regression imputation versus simple mean imputation
3. Vary sequence length to find optimal temporal context

## Open Questions the Paper Calls Out

### Uncertainty Quantification Integration
The paper states future work will extend to "uncertainty modeling" but the current model provides only point predictions. Implementing Bayesian layers or MC dropout could yield confidence intervals while maintaining low latency under 0.2 seconds.

### Attention Mechanism Enhancement
Future work mentions "attention-based enhancements" to improve feature asymmetry handling. Comparative studies are needed to quantify accuracy-latency tradeoffs when adding attention to the GRU-LSTM framework.

### Multi-Step Forecasting Extension
The authors plan to extend beyond one-step-ahead prediction to multi-step forecasting. Direct multi-horizon output architectures need evaluation to assess error propagation across 24-hour forecasts.

### Imputation Robustness Under Distribution Shifts
Day 5 showed accuracy drops linked to "input inconsistencies." Sensitivity analysis with perturbed temperature-feature correlations is needed to quantify imputation strategy limits when test-time correlations deviate from training patterns.

## Limitations
- Relies on polynomial regression assumptions that may break under significant distribution shifts
- One-step-ahead prediction may accumulate errors for longer forecast horizons
- No uncertainty quantification despite noisy, imputed input conditions
- Performance depends on stable temperature-feature correlations that may not hold in all scenarios

## Confidence
- High confidence in overall framework effectiveness and reported metrics
- Medium confidence in preprocessing pipeline and training configuration due to unspecified hyperparameters
- Low confidence in result generalization without dataset access

## Next Checks
1. Confirm optimal sequence length L via grid search on validation set (e.g., L ∈ {12, 24, 48})
2. Benchmark polynomial regression imputation against simpler mean imputation on test accuracy
3. Verify that temperature trends align with predicted power peaks in spatiotemporal analysis