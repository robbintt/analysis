---
ver: rpa2
title: Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding
arxiv_id: '2508.00420'
source_url: https://arxiv.org/abs/2508.00420
tags:
- embeddings
- embedding
- word
- sentence
- coefficients
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of Discrete Wavelet Transforms
  (DWT) to word and sentence embeddings in natural language processing. The authors
  propose using DWT to selectively compress word embeddings while retaining important
  semantic information, and then combining DWT with Discrete Cosine Transform (DCT)
  to encode variable-length sentences into fixed-size vectors without increasing dimensionality.
---

# Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding

## Quick Facts
- arXiv ID: 2508.00420
- Source URL: https://arxiv.org/abs/2508.00420
- Reference count: 40
- Authors: Rana Salama; Abdou Youssef; Mona Diab
- Primary result: Wavelet and cosine transform approach maintains or improves embedding performance while reducing dimensionality by 50-75%

## Executive Summary
This paper presents a novel approach to sentence embedding using Discrete Wavelet Transforms (DWT) and Discrete Cosine Transforms (DCT) to create efficient, fixed-size representations from variable-length text. The method selectively compresses word embeddings using DWT while preserving semantic information, then combines with DCT to encode sentences without increasing dimensionality. The approach is evaluated across multiple NLP tasks including semantic similarity, sentiment analysis, and question classification, demonstrating performance comparable to or better than state-of-the-art non-parametric models while achieving significant dimensionality reduction.

## Method Summary
The proposed method processes word embeddings through Discrete Wavelet Transforms to selectively compress semantic information while retaining important features. For sentence-level representation, the approach combines DWT with Discrete Cosine Transform to convert variable-length sequences into fixed-size vectors without dimensionality increase. This spectral approach leverages the multi-resolution analysis capabilities of wavelets to capture both local and global semantic patterns in text. The resulting embeddings are evaluated both intrinsically (semantic similarity, concept categorization) and extrinsically (sentiment analysis, inference, paraphrase detection, question classification) to validate their effectiveness across diverse NLP tasks.

## Key Results
- DWT embeddings reduce dimensionality by 50-75% while maintaining or improving performance compared to original embeddings
- The combined DWT-DCT sentence embedding model matches or outperforms state-of-the-art non-parametric models across multiple tasks
- The approach demonstrates effectiveness for efficient sentence representation through spectral transforms

## Why This Works (Mechanism)
The effectiveness of this approach stems from the complementary properties of wavelet and cosine transforms for text representation. Discrete Wavelet Transforms excel at capturing both local and global semantic patterns through their multi-resolution analysis capability, allowing selective compression that preserves important semantic information while reducing dimensionality. When combined with Discrete Cosine Transform, which efficiently encodes frequency components, the method can handle variable-length sequences and produce fixed-size embeddings without information loss. This spectral approach effectively balances compression efficiency with semantic preservation, making it suitable for diverse NLP tasks requiring fixed-dimensional representations.

## Foundational Learning
1. **Discrete Wavelet Transform (DWT)** - A mathematical transform that decomposes signals into different frequency components with varying resolutions, allowing multi-scale analysis of data. Why needed: Enables selective compression of embeddings while preserving semantic information across different scales. Quick check: Can be verified through visualization of wavelet coefficients and their contribution to semantic preservation.

2. **Discrete Cosine Transform (DCT)** - A transform that expresses a finite sequence of data points as a sum of cosine functions oscillating at different frequencies, particularly effective for energy compaction. Why needed: Provides efficient encoding of frequency components for converting variable-length sequences to fixed-size representations. Quick check: Can be validated by comparing energy distribution in original versus transformed space.

3. **Multi-resolution Analysis** - The property of wavelets to analyze data at different scales or resolutions simultaneously, capturing both coarse and fine details. Why needed: Critical for balancing local word-level semantics with global sentence-level meaning during compression. Quick check: Can be tested by examining semantic preservation at different decomposition levels.

## Architecture Onboarding

**Component Map:** Word Embeddings -> DWT Compression -> DCT Encoding -> Fixed-Size Sentence Embeddings

**Critical Path:** The essential processing flow follows embeddings through DWT for dimensionality reduction, then through DCT for sequence-to-fixed-size conversion. This path must maintain semantic integrity while achieving compression targets.

**Design Tradeoffs:** The primary tradeoff involves balancing compression ratio against semantic preservation. Higher compression yields greater efficiency but risks losing important semantic information. The choice of wavelet family (Daubechies in this work) affects both computational efficiency and representation quality. Alternative transforms or hybrid approaches could offer different balance points.

**Failure Signatures:** Performance degradation typically manifests as reduced accuracy on semantic similarity tasks first, followed by downstream task performance. Over-compression leads to loss of fine-grained semantic distinctions, while inappropriate wavelet parameters may cause artifacts in the frequency domain representation. Monitoring embedding quality through intrinsic evaluation metrics helps detect these issues early.

**First 3 Experiments:**
1. Test different wavelet families (Haar, Coiflets, Morlet) and decomposition levels to identify optimal configurations for specific text domains
2. Compare computational efficiency and memory requirements against traditional compression methods (PCA, autoencoders) across varying batch sizes
3. Evaluate performance on specialized domains (biomedical text, legal documents) to assess domain robustness beyond standard evaluation sets

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- The evaluation focuses primarily on English text, limiting conclusions about multilingual applicability
- The choice of Daubechies wavelets may not be optimal for all text types; other wavelet families could yield different results
- The claim of universal applicability as a drop-in replacement requires more extensive validation across diverse domains and languages

## Confidence

**High confidence:** The mathematical foundations of wavelet and cosine transforms are well-established, and the theoretical framework for using these transforms for dimensionality reduction while preserving semantic information is sound.

**Medium confidence:** The empirical results showing improved or comparable performance to baseline methods across multiple tasks are encouraging, but the evaluation scope is limited to specific datasets and tasks, and the relative advantage over other compression techniques could vary with different data distributions.

**Low confidence:** The claim that this approach can be universally applied as a drop-in replacement for existing embeddings needs more extensive validation across diverse domains and languages. The computational efficiency gains versus traditional methods require benchmarking on different hardware configurations.

## Next Checks
1. Conduct ablation studies varying wavelet types (Haar, Coiflets, Morlet) and decomposition levels to identify optimal configurations for different text domains and embedding sizes.

2. Benchmark the computational efficiency and memory requirements against other compression methods (PCA, autoencoders) across varying batch sizes and hardware configurations.

3. Test the approach on specialized domains (biomedical text, legal documents) and low-resource languages to assess robustness and generalization beyond the current evaluation set.