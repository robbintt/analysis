---
ver: rpa2
title: 'ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly'
arxiv_id: '2509.02949'
source_url: https://arxiv.org/abs/2509.02949
tags:
- question
- task
- step
- attach
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ProMQA-Assembly, a new multimodal QA dataset
  focused on assembly tasks. The dataset includes 391 QA pairs that require understanding
  human activity recordings and instruction manuals in an online-style manner.
---

# ProMQA-Assembly: Multimodal Procedural QA Dataset on Assembly

## Quick Facts
- arXiv ID: 2509.02949
- Source URL: https://arxiv.org/abs/2509.02949
- Reference count: 40
- Primary result: Multimodal QA dataset for assembly tasks shows proprietary models perform similarly to text-only models, indicating room for improvement in vision understanding.

## Executive Summary
This paper introduces ProMQA-Assembly, a multimodal QA dataset focused on assembly tasks that requires understanding human activity recordings and instruction manuals in an online-style manner. The dataset includes 391 QA pairs developed through a semi-automated annotation approach where LLMs generate candidates and humans verify them. The authors also created instruction task graphs for 78 toys in Assembly101. Benchmarking experiments revealed that even strong proprietary models perform only on par with text-only models, suggesting significant room for improvement in multimodal models' vision understanding and reasoning over multiple modalities.

## Method Summary
The dataset was created using a semi-automated QA annotation approach where an LLM (GPT-4o) generates candidate questions based on Assembly101 videos and instruction manuals, with human verification ensuring quality. Task graphs were manually annotated as DAGs to represent assembly dependencies and facilitate the verification process. The benchmark evaluates models on open-vocabulary QA using an LLM-as-a-judge (GPT-4o) scoring system, testing their ability to answer questions about assembly progress given video frames, task graphs, and parts images.

## Key Results
- 391 QA pairs created from 78 assembly tasks with instruction task graphs
- Proprietary multimodal models perform similarly to text-only models (around 45% accuracy)
- Vision inputs sometimes distract multimodal models rather than provide useful information
- Context-grounded prompts with fine-grained action labels improve question specificity and diversity

## Why This Works (Mechanism)

### Mechanism 1: Context-Grounded Generative Annotation
Integrating fine-grained action labels into LLM prompts increases lexical diversity and specificity of generated questions without reducing validity rate. The "With fine" prompt template provides hierarchical context (coarse steps and nested fine-grained steps like "position excavator arm") allowing models to generate questions grounded in specific micro-actions rather than generic procedural steps.

### Mechanism 2: Task Graphs as Deterministic Reasoning Scaffolds
Annotated instruction task graphs (DAGs) reduce cognitive load on human verifiers and provide unambiguous grounding for "missing step" and "next step" queries. The graph constrains valid state space of assembly, allowing traversal to definitively identify missing prerequisites or valid successors.

### Mechanism 3: Vision as a Distractor in Insufficient Models
Current proprietary multimodal models often fail to extract actionable signal from video frames, sometimes performing worse than text-only models because visual tokens act as noise. Text-only models rely on strong logical priors from text graphs, while multimodal models attempting to fuse visual data may lack fine-grained visual grounding to correct text-based hallucinations.

## Foundational Learning

- **Directed Acyclic Graphs (DAGs) for Procedural Representation**: Why needed - entire instruction logic is built on DAGs; Quick check - Given DAG with steps A->B, A->C, if user completes A, what are valid next steps?

- **Temporal Segment Sampling**: Why needed - system processes video by sampling frames; Quick check - If video is 10 seconds and you sample 5 frames uniformly, what is risk if mistake lasts only 1 second?

- **LLM-as-a-Judge Evaluation**: Why needed - benchmark relies on GPT-4o to score open-ended answers; Quick check - Why might LLM judge give high score to confident but factually wrong answer?

## Architecture Onboarding

- **Component map**: Assembly101 Videos (Multi-view) + Parts Images + Manual Text -> Annotated Task Graphs (DAGs) -> LLM Generation Pipeline (GPT-4o + "With fine" Prompt) -> Candidate QA Pairs -> Verification Interface -> Valid QA Pairs (391) -> Evaluation Suite (VLM Predictors vs LLM-as-a-Judge)

- **Critical path**: Task Graph Annotation precedes QA Generation - cannot generate valid questions about "missing steps" without graph defining what those steps are.

- **Design tradeoffs**: Cost vs. Diversity (semi-automated cheaper but limited to 391 pairs due to human verification bottleneck) vs Strictness vs. Realism (DAGs ensure clean evaluation logic but may fail to capture real-world DIY flexibility).

- **Failure signatures**: Visual Hallucination (model claims part attached when not based on text manual ignoring video frames) vs Logical Short-circuiting (text-only model answers correctly by inferring likely errors from question text without "seeing" video).

- **First 3 experiments**: 1) Baseline Replication - run DeepSeek-R1 (text-only) baseline against random 50-question subset, 2) Ablation on Visual Input - evaluate VLM with only task graph text vs full multimodal input, 3) Judge Consistency Check - manually inspect 20 examples where GPT-4o scores disagree with human judgment.

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark results based on proprietary models whose internal workings are not transparent
- Semi-automated annotation pipeline depends heavily on quality of Assembly101's fine-grained action labels
- Task graph approach assumes deterministic assembly process that may not capture real-world flexibility
- LLM-as-a-judge evaluation introduces potential biases and inconsistencies

## Confidence

- **High Confidence**: Dataset creation methodology (semi-automated annotation with human verification) is clearly described and reproducible; task graph construction process is well-specified.
- **Medium Confidence**: Benchmark results showing proprietary models performing similarly to text-only models are reproducible, but interpretation of why remains speculative without model internals.
- **Low Confidence**: Generalizability of findings to other assembly domains or more complex tasks is uncertain given dataset's focus on 78 specific toys with relatively simple procedures.

## Next Checks

1. **Ablation Study on Visual Input**: Evaluate same model on visually grounded questions with and without video frames to quantify specific impact of visual information on performance.

2. **Cross-Judge Consistency Analysis**: Run evaluation using multiple LLM-as-a-judge models (Claude, Gemini) on subset of answers to measure consistency and identify evaluator bias.

3. **Real-World Task Graph Validation**: Manually verify sample of task graphs against actual assembly recordings to assess whether DAG representation captures full range of valid assembly sequences.