---
ver: rpa2
title: 'Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without
  Rewards'
arxiv_id: '2510.18814'
source_url: https://arxiv.org/abs/2510.18814
tags:
- osft
- reasoning
- grpo
- pass
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards

## Quick Facts
- **arXiv ID:** 2510.18814
- **Source URL:** https://arxiv.org/abs/2510.18814
- **Reference count:** 40
- **Primary result:** OSFT improves reasoning performance on mathematical tasks by 10-20% over the base model using only a single generated sample per prompt, without external rewards.

## Executive Summary
This paper introduces Online Supervised Fine-Tuning (OSFT), a reward-free method for enhancing LLM reasoning capabilities. OSFT works by sampling low-temperature outputs from the base model and using standard SFT to reinforce these "confident" generations. The approach is surprisingly effective, achieving performance comparable to reward-based RL methods while being significantly more efficient (requiring only G=1 samples vs. G=8 for RL). The method is particularly effective on mathematical reasoning tasks, improving pass rates by 10-20% over the base model.

## Method Summary
OSFT is an iterative online training loop that generates data using the current model at low sampling temperature (τs) and then performs standard SFT on these outputs. The key innovation is the decoupling of sampling temperature (τs < 1) from training temperature (τt = 1), which creates a directional gradient that pushes the model distribution toward the sharper sampling distribution. The method requires no external rewards or verifiers during training, making it significantly more efficient than RL-based approaches. The authors demonstrate effectiveness on mathematical reasoning tasks using the DeepScaleR dataset and various math benchmarks.

## Key Results
- OSFT achieves 10-20% improvement in pass rates over base models on mathematical reasoning tasks
- OSFT matches or exceeds GRPO performance while using 8x fewer samples per training step
- The effectiveness critically depends on τs < τt; coupled temperatures fail to improve performance
- Performance gains are task-specific and require careful tuning of sampling temperature

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** OSFT improves reasoning by amplifying the model's latent preferences (existing knowledge) rather than injecting new knowledge.
- **Mechanism:** The base model often assigns higher probability to correct reasoning paths but fails to select them consistently during sampling. OSFT uses low-temperature sampling (τs) to select these high-probability (often correct) paths and then reinforces them via SFT, widening the probability margin between correct and incorrect paths.
- **Core assumption:** The pre-trained model already possesses the latent knowledge required for the task (self-weak-to-strong generalization).
- **Break condition:** Fails if the base model's "latent preference" is incorrect or random, as OSFT would then reinforce errors.

### Mechanism 2
- **Claim:** Decoupling sampling (τs) and training (τt) temperatures is mathematically necessary to generate a learning signal.
- **Mechanism:** If τs = τt, the expected gradient update is zero (score-function identity). By setting τs < τt, the gradient update specifically pushes the model distribution toward the sharper (lower entropy) sampling distribution, reinforcing certainty on the sampled path.
- **Core assumption:** The mathematical derivation for the gradient assumes the validity of the temperature scaling dynamics in autoregressive models.
- **Break condition:** If τs ≥ τt, the model either fails to learn (random walk) or degrades (reinforcing noise).

### Mechanism 3
- **Claim:** Reinforcing self-generated "certainty" acts as a proxy for reward signals in reasoning tasks.
- **Mechanism:** Standard RLVR (like GRPO) uses external verifiers to find correct answers. OSFT substitutes this by observing that low-temperature sampling correlates with high pass@1 accuracy. Reinforcing these "certain" generations mimics the objective of maximizing reward without the complexity of a reward model.
- **Core assumption:** Correlation between model certainty (low perplexity/low temp sampling) and correctness holds sufficiently well for the target domain.
- **Break condition:** High "hallucination confidence," where the model is confidently wrong, would cause OSFT to reinforce incorrect patterns.

## Foundational Learning
- **Concept: Supervised Fine-Tuning (SFT) Loss (Negative Log-Likelihood)**
  - **Why needed here:** OSFT is fundamentally an SFT loop. You must understand that minimizing -log πθ(o|q) forces the model to increase the probability of the generated sequence o.
  - **Quick check question:** If a sequence has probability 0.01, what is the negative log-likelihood? (Answer: ~4.6).
- **Concept: Temperature Scaling in Softmax**
  - **Why needed here:** The core trick is τs < τt. You need to know that τ < 1 sharpens the distribution (makes high prob tokens more likely), while τ > 1 flattens it.
  - **Quick check question:** Does lowering the temperature increase or decrease the entropy of the output distribution?
- **Concept: On-Policy vs. Off-Policy Sampling**
  - **Why needed here:** OSFT generates data using the "old" model (πold) to train the current model (πθ). While technically "online" in generation, the SFT loss treats this fixed data as ground truth, unlike RL which weighs samples by advantage.
  - **Quick check question:** In OSFT, are we training on a static dataset or a dynamically generated one?

## Architecture Onboarding
- **Component map:** Sampler -> Trainer -> Verifier (evaluation only)
- **Critical path:** The data generation loop. The efficiency gain comes from G=1 (vs G=8 or 16 in GRPO), drastically reducing inference overhead per training step.
- **Design tradeoffs:**
  - **Efficiency vs. Exploration:** Using G=1 is fast but reduces exploration of diverse reasoning paths compared to RL.
  - **Reward-Free vs. Correctness:** Without a verifier, OSFT cannot explicitly discriminate against incorrect "confident" paths, potentially limiting peak performance on hard tasks (as seen where GRPO overtakes OSFT at high k).
- **Failure signatures:**
  - **Stagnation:** Training loss drops but accuracy stays flat (likely τs too high or base model too weak).
  - **Collapse:** Output diversity drops to near zero (over-reinforcement, τs likely too low).
- **First 3 experiments:**
  1. **Sanity Check (Coupled Temps):** Run OSFT with τs = τt = 1.0. Verify that the model does *not* improve (directionless gradient).
  2. **Ablation on Sampling Temp:** Sweep τs ∈ {0.1, 0.6, 0.9} while keeping τt=1. Plot Pass@1 vs. Steps. Confirm τs=0.6 (for math) yields stable improvement.
  3. **Efficiency Benchmark:** Compare training wall-clock time and tokens processed for OSFT (G=1) vs. GRPO (G=8) to verify the claimed efficiency gains.

## Open Questions the Paper Calls Out
None

## Limitations
- **Verification-Free Reinforcement:** The core premise relies on the correlation between low-temperature sampling and correctness, which may not hold for tasks where "certainty" doesn't align with "correctness."
- **Task-Specific Sampling Temperature:** The need to tune τs per base model type (0.6 for math, 0.9 for general) suggests the mechanism isn't a universal property of LLMs but a delicate interaction between the model's pretraining distribution and the specific dataset.
- **Weak-to-Strong Generalization Dependency:** The effectiveness critically depends on the unproven assumption that the pre-trained model already possesses the "correct" reasoning trajectories.

## Confidence
- **High Confidence:** The efficiency claims (OSFT with G=1 vs. GRPO with G=8) are well-supported by the ablation studies. The gradient math for why τs < τt is necessary is also solid.
- **Medium Confidence:** The empirical performance improvements on mathematical reasoning tasks are real and reproducible. However, the explanation for *why* it works (latent preference amplification) is speculative and relies on correlations that may not hold in all domains.
- **Low Confidence:** The claim that OSFT is a "paradigm shift" from reward-based to self-tuning methods is overstated. OSFT is a clever and efficient application of SFT, but it doesn't replace the need for verification in tasks where model confidence is a poor proxy for correctness.

## Next Checks
1. **Domain Transfer Test:** Apply OSFT to a non-mathematical reasoning task (e.g., commonsense reasoning or code generation) where model "certainty" is known to be uncorrelated with correctness. Measure if performance degrades or if the model simply fails to improve.
2. **Failure Boundary Analysis:** Intentionally initialize OSFT with a base model known to have incorrect "latent preferences" for a task (e.g., a model fine-tuned on incorrect math solutions). Observe if OSFT amplifies these errors.
3. **Temperature Ablation on Non-Math Tasks:** Sweep τs for a general-purpose model (e.g., Qwen2.5-7B) on a math dataset. Confirm that using τs=0.6 causes degradation, demonstrating the fragility of the sampling temperature hyperparameter.