---
ver: rpa2
title: Efficient Neural Networks with Discrete Cosine Transform Activations
arxiv_id: '2511.03531'
source_url: https://arxiv.org/abs/2511.03531
tags:
- coefficients
- neural
- pruning
- neuron
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Expressive Neural Network (ENN), a multilayer
  perceptron with activation functions parameterized using the Discrete Cosine Transform
  (DCT). This approach enhances both expressiveness and interpretability by leveraging
  the structured, decorrelated nature of DCT coefficients.
---

# Efficient Neural Networks with Discrete Cosine Transform Activations

## Quick Facts
- arXiv ID: 2511.03531
- Source URL: https://arxiv.org/abs/2511.03531
- Authors: Marc Martinez-Gost; Sara Pepe; Ana Pérez-Neira; Miguel Ángel Lagunas
- Reference count: 14
- Primary result: Introduces ENN, a MLP with DCT-parameterized activations, achieving strong performance on binary classification and INR tasks with up to 40% pruning efficiency

## Executive Summary
The paper introduces the Expressive Neural Network (ENN), a multilayer perceptron with activation functions parameterized using the Discrete Cosine Transform (DCT). This approach enhances both expressiveness and interpretability by leveraging the structured, decorrelated nature of DCT coefficients. The ENN demonstrates strong performance on binary classification and implicit neural representation (INR) tasks, consistently outperforming state-of-the-art models while maintaining a compact architecture. Notably, the DCT-based parameterization enables efficient pruning, allowing up to 40% of activation coefficients to be removed with negligible loss in accuracy. This is attributed to the orthogonality and bounded properties of the DCT basis, which naturally compress the activation functions. The ENN offers a principled integration of signal processing concepts into neural network design, achieving a favorable balance between expressiveness, compactness, and interpretability.

## Method Summary
The ENN replaces standard activation functions with learnable DCT-based activations, where each neuron's activation is parameterized as a sum of Q DCT coefficients. The key innovation is scaling the input coordinates from [-1,1] to [0,N-1] before applying DCT, enabling the network to learn frequency-domain representations of activation functions. This creates adaptive, structured activations that are both expressive and compressible. The network is trained end-to-end with separate learning rates for weights and DCT coefficients, and pruning is achieved by thresholding coefficient magnitudes.

## Key Results
- ENN outperforms standard MLPs, Fourier AAF, and KAN on binary classification tasks with fewer parameters (67-74 vs 100+ parameters)
- ENN achieves competitive image INR reconstruction with 180K parameters vs 2.3M for standard MLP
- Up to 40% of DCT coefficients can be pruned with negligible performance loss due to natural frequency-domain compression
- Kolmogorov-Arnold representation theorem provides theoretical justification for ENN's compact architecture

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal DCT Coefficient Parameterization
DCT-based activation functions enable independent coefficient contribution and natural signal compression. Each neuron's activation is represented as a sum of Q learnable DCT coefficients: σ(z) = Σ(q=1 to Q) F_q × cos(π(2q-1)(2z-1)/2N). Orthogonality ensures each coefficient contributes independently to the activation shape (no cross-correlation), allowing magnitude-based pruning without cascading effects on other coefficients.

### Mechanism 2: Bounded Gradient Propagation Through Cosine Basis
Cosine-based activations prevent gradient instability during backpropagation. Since cos(x) and d/dx[cos(x)] are bounded to [-1, 1], gradients remain well-behaved regardless of input magnitude. This contrasts with unbounded activations (e.g., ReLU⁻¹ near zero) or vanishing activations (sigmoid in saturation).

### Mechanism 3: Kolmogorov-Arnold Guided Architecture Compression
Shallow ENNs can match deeper MLPs by leveraging the Kolmogorov-Arnold representation theorem for width bounds. The theorem guarantees that any continuous function f(x₁, ..., x_M₀) can be represented with at most 2M₀+1 neurons in a single hidden layer using univariate inner/outer functions. ENN's adaptive activations implement these univariate functions, enabling theoretical compactness.

## Foundational Learning

- **Concept: Discrete Cosine Transform (DCT)**
  - Why needed here: Core representation for activations; understanding frequency-domain compression is essential for interpreting coefficient importance
  - Quick check question: Given a step function input, which DCT coefficients (low or high frequency) would have largest magnitudes?

- **Concept: Orthogonal Basis Functions**
  - Why needed here: Explains why DCT coefficients can be pruned independently without affecting remaining coefficients
  - Quick check question: If basis functions B₁ and B₂ are orthogonal, what is the value of ∫B₁(x)B₂(x)dx?

- **Concept: Adaptive vs. Fixed Activation Functions**
  - Why needed here: ENN's primary innovation; understanding trade-offs between parameter efficiency and representational flexibility
  - Quick check question: How many additional parameters does making an activation adaptive add per neuron if using Q DCT coefficients?

## Architecture Onboarding

- **Component map:** Input x ∈ [-1,1]^M₀ → [Linear: W^T x + b] → [Scaling: z̄ = N/2(z+1)] → [DCT Activation: Σ F_q × cos(...)] → Output s ∈ R^M₁

- **Critical path:** The scaling transform (Eq. 2) is often overlooked but essential—without it, inputs to DCT may fall outside [0, N-1], causing periodic basis functions to misrepresent the input range.

- **Design tradeoffs:**
  - Q (number of DCT coefficients): Higher Q → more expressive but more parameters; paper uses Q=6 as default
  - N (DCT resolution): Higher N → finer frequency resolution but no parameter increase; paper uses N=512
  - M (network width): Can be smaller than standard MLP due to adaptive activations (Kolmogorov-Arnold bound)
  - Odd coefficients only: Using (2q-1) indexing halves parameters while preserving representational capacity

- **Failure signatures:**
  - Oscillatory outputs: High Q values without regularization may cause high-frequency artifacts (observed in Section IV.B as "unwanted oscillations")
  - Slow convergence: If learning rate for F coefficients is too low relative to weight learning rate
  - Pruning collapse: Pruning >50% of coefficients degrades MSE by >2.5× (Table III)

- **First 3 experiments:**
  1. **Sanity check—identity function:** Train single-neuron ENN on f(x)=x; should learn F₁≈1, F_{q>1}≈0. Verify DCT can represent linear functions.
  2. **Expressiveness test—binary classification ring:** Replicate P1 from paper with M₁∈{2,4,6} neurons; confirm decision boundary quality scales with neuron count (Fig. 3).
  3. **Pruning calibration:** Train on image INR task, then prune coefficients at thresholds ρ∈[0.04, 0.17]; plot MSE vs. % pruned to find sweet spot before performance cliff.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can redundant bump detection and pruning be made effective for deep networks despite the curse of dimensionality? The authors state that detecting redundant bumps "does not scale well to deep networks" because bumps "become nearly orthogonal to one another" in high dimensions, making such pruning "only feasible in shallow networks."

- **Open Question 2:** What are the optimal application domains for ENN beyond binary classification and image INR? The authors acknowledge they only validate on "binary classification and implicit neural representation (INR) tasks" and state INR "may not be the most appropriate approach for image representation" compared to standard 2D DCT.

- **Open Question 3:** Can pruning-aware training further improve ENN efficiency compared to post-hoc coefficient pruning? The authors mention "pruning-aware training... strikes a better balance between efficiency and performance" but only implement post-training threshold-based pruning, noting full hyperparameter optimization "falls outside the scope of this work."

## Limitations

- The Kolmogorov-Arnold representation theorem provides theoretical justification for compact architectures, but the paper doesn't empirically verify that ENN converges to the optimal inner/outer functions the theorem guarantees exist.
- No comparison against standard activation pruning techniques (magnitude pruning, L1 regularization) to isolate DCT-specific advantages.
- Binary classification problem geometries (P1-P5) are not specified, making exact replication impossible without assumptions.

## Confidence

- **High confidence:** DCT coefficient pruning mechanism and orthogonality benefits
- **Medium confidence:** Gradient stability through bounded cosine derivatives
- **Medium confidence:** Kolmogorov-Arnold architecture compression

## Next Checks

1. **Coefficient frequency analysis:** Plot histogram of pruned vs. retained DCT coefficients to verify low-frequency concentration as predicted by the mechanism.
2. **Gradient magnitude tracking:** Monitor gradient norms for F coefficients vs. weight gradients throughout training to empirically confirm boundedness claims.
3. **Direct KAN comparison:** Implement a KAN baseline with spline activations using identical network widths to test whether DCT parameterization provides advantages beyond the Kolmogorov-Arnold architecture itself.