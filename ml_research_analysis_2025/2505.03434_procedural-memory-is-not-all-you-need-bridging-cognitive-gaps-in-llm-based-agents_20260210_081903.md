---
ver: rpa2
title: 'Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based
  Agents'
arxiv_id: '2505.03434'
source_url: https://arxiv.org/abs/2505.03434
tags:
- llms
- memory
- learning
- procedural
- environments
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that Large Language Models (LLMs) are fundamentally
  limited by their reliance on procedural memory, which prevents them from adapting
  to complex, unpredictable environments. The authors propose a modular architecture
  that augments LLMs with dedicated semantic and associative memory systems to enable
  adaptive intelligence in "wicked" learning environments where rules shift and feedback
  is ambiguous.
---

# Procedural Memory Is Not All You Need: Bridging Cognitive Gaps in LLM-Based Agents

## Quick Facts
- arXiv ID: 2505.03434
- Source URL: https://arxiv.org/abs/2505.03434
- Reference count: 38
- This paper argues that LLMs are fundamentally limited by procedural memory reliance and proposes a modular architecture with dedicated semantic and associative memory systems to enable adaptive intelligence in unpredictable environments.

## Executive Summary
This paper identifies a fundamental limitation in current LLM-based agents: their reliance on procedural memory prevents them from adapting to complex, unpredictable environments where rules shift and feedback is ambiguous. The authors propose a novel modular architecture that separates agentic learners (handling semantic-associative reasoning) from agentic actors (LLMs handling procedural execution). This separation allows each component to specialize in its respective cognitive function, enabling agents to maintain persistent state, perform compositional reasoning, and dynamically associate actions with outcomes across sessions. The framework is positioned as necessary for developing autonomous agents capable of real-world decision-making beyond narrow procedural expertise.

## Method Summary
The paper proposes a modular architecture that augments LLMs with dedicated semantic and associative memory systems to address limitations in handling "wicked" learning environments. The core design separates agentic learners (responsible for semantic-associative reasoning) from agentic actors (LLMs handling procedural execution), allowing each to specialize in its respective cognitive function. While no specific implementation details or performance metrics are provided, the framework is illustrated through examples showing how this approach could enable persistent state maintenance, compositional reasoning, and dynamic action-outcome associations across multiple interaction sessions.

## Key Results
- Proposes modular architecture separating semantic-associative reasoning from procedural execution
- Illustrates how the approach could enable persistent state maintenance across sessions
- Demonstrates potential for compositional reasoning in rule-shifting environments

## Why This Works (Mechanism)
The paper argues that LLMs' fundamental limitation stems from their exclusive reliance on procedural memory, which is inadequate for adapting to complex, unpredictable environments where rules shift and feedback is ambiguous. By separating semantic-associative reasoning (handled by agentic learners) from procedural execution (handled by LLMs), each component can specialize in its respective cognitive function. This modular approach enables agents to maintain persistent state, perform compositional reasoning, and dynamically associate actions with outcomes across sessions, addressing the core limitations of monolithic LLM architectures in "wicked" learning environments.

## Foundational Learning
1. **Procedural Memory**: Memory for learned skills and routines; why needed: understanding current LLM limitations; quick check: can the system perform tasks it has been explicitly trained on?
2. **Semantic Memory**: Knowledge about facts and concepts; why needed: enables reasoning about abstract relationships; quick check: can the system understand and apply conceptual knowledge?
3. **Associative Memory**: Memory for connections between items; why needed: critical for linking actions to outcomes; quick check: can the system dynamically update action-outcome associations?
4. **Wicked Learning Environments**: Settings where rules change and feedback is ambiguous; why needed: characterizes real-world challenges for agents; quick check: can the system adapt when rules shift during operation?
5. **Agentic Learners vs. Agentic Actors**: Separation of reasoning and execution roles; why needed: enables specialized optimization of different cognitive functions; quick check: can each module perform its designated function independently?

## Architecture Onboarding

**Component Map**: Agentic Learners (Semantic-Associative) -> Agentic Actors (Procedural) -> Environment

**Critical Path**: Semantic memory retrieves relevant concepts → Associative memory links concepts to potential actions → Agentic actors execute procedural steps → Results feed back to update semantic-associative associations

**Design Tradeoffs**: 
- Separation enables specialization but adds complexity
- Modular design improves adaptability but requires synchronization mechanisms
- Semantic-associative components need persistent storage but increase memory overhead

**Failure Signatures**:
- Semantic memory failure: inability to retrieve relevant concepts for novel situations
- Associative memory failure: inability to link actions to outcomes in new contexts
- Agentic actor failure: inability to execute learned procedures correctly
- Integration failure: conflicts between memory modules during real-time decision-making

**First Experiments**:
1. Test persistent state maintenance across multiple interaction sessions
2. Evaluate compositional reasoning in rule-shifting environments
3. Measure dynamic action-outcome association updates in controlled scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Central claim about LLM limitations lacks empirical validation and quantitative performance metrics
- Distinction between memory types in LLM contexts is not rigorously defined
- Does not address integration challenges between proposed memory modules

## Confidence

**High confidence**: Well-established literature on LLM struggles with persistent state and compositional reasoning across sessions.

**Medium confidence**: Conceptual framework of separating reasoning from execution is logically coherent but unproven.

**Low confidence**: Claims about procedural memory being the primary limitation are speculative without empirical evidence.

## Next Checks

1. Implement modular architecture and compare baseline LLM performance on standardized adaptive reasoning tasks to quantify improvements in handling rule shifts and ambiguous feedback.

2. Conduct ablation studies to isolate contributions of semantic, associative, and procedural memory components in enabling persistent state maintenance and compositional reasoning across multiple sessions.

3. Design experiments to measure system's ability to dynamically associate actions with outcomes in controlled "wicked" environments where rules intentionally change over time, comparing performance against monolithic LLM approaches.