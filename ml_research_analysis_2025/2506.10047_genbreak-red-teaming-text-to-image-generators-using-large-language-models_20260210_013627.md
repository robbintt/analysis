---
ver: rpa2
title: 'GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models'
arxiv_id: '2506.10047'
source_url: https://arxiv.org/abs/2506.10047
tags:
- prompts
- prompt
- image
- diversity
- toxicity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GenBreak is a red-teaming framework that fine-tunes a large language
  model to automatically discover adversarial prompts capable of bypassing safety
  filters and generating harmful content in text-to-image models. It uses supervised
  fine-tuning on curated datasets followed by reinforcement learning with multi-objective
  rewards (toxicity, stealth, diversity) to optimize for both evasion and image harmfulness.
---

# GenBreak: Red Teaming Text-to-Image Generators Using Large Language Models

## Quick Facts
- **arXiv ID:** 2506.10047
- **Source URL:** https://arxiv.org/abs/2506.10047
- **Reference count:** 40
- **Primary result:** Fine-tuned LLM framework achieves 70% toxic bypass rate on commercial T2I APIs while maintaining semantic fluency and prompt diversity

## Executive Summary
GenBreak introduces a red-teaming framework that fine-tunes large language models to automatically discover adversarial prompts capable of bypassing safety filters and generating harmful content in text-to-image models. The framework employs a two-stage approach: supervised fine-tuning on curated adversarial datasets followed by reinforcement learning with multi-objective rewards (toxicity, stealth, diversity). Evaluated across both open-source and commercial T2I models, GenBreak demonstrates high toxic bypass rates while maintaining semantic fluency, outperforming existing red-teaming methods.

## Method Summary
The method employs a two-stage pipeline: supervised fine-tuning (SFT) on Category Rewrite and Pre-Attack datasets, followed by reinforcement learning with GRPO. The framework uses Llama-3.2-1B-Instruct with LoRA adapters, optimizing for toxicity, bypass capability, clean prompts (no blacklisted words), and diversity. Training targets include SD 2.1 and SD 3 Medium as surrogate models, with transfer evaluation on commercial APIs like Leonardo.Ai, fal.ai, and stability.ai. The integrated filter combines text toxicity classification, NSFW detection, and image safety checking to simulate real-world defenses.

## Key Results
- Achieves 70% Toxic Bypass Rate on Leonardo.Ai for nudity content
- Maintains semantic fluency while optimizing for toxicity and stealth
- Demonstrates superior performance over existing red-teaming methods
- Shows successful transfer from open-source to commercial T2I models

## Why This Works (Mechanism)

### Mechanism 1: Multi-objective RL for Competing Constraints
The framework combines toxicity reward (Rtox), bypass reward (Rbypass), clean reward (Rclean), and diversity rewards with tunable weights λ1-λ6. This enables simultaneous optimization of competing constraints that single-objective approaches cannot balance. The bypass reward only activates when prompts pass filters AND produce toxic images, preventing reward hacking where the model optimizes for bypass alone without harmful outputs.

### Mechanism 2: Curated Dataset Initialization
Supervised fine-tuning on curated adversarial datasets provides effective initialization before RL, reducing exploration cost and stabilizing training. The Category Rewrite Dataset teaches prompt transformation patterns across domains, while the Pre-Attack Dataset provides high-TBS examples from iterative attacks. This SFT stage equips the base LLM with domain-specific attack vocabulary before RL refinement.

### Mechanism 3: Realistic Surrogate Training
Transferability to commercial T2I services emerges from training against open-source models with realistic integrated filters. By learning to bypass this composite defense (toxicity classifier + NSFW detector + image safety checker), the red-team LLM discovers prompt patterns that generalize because commercial systems use similar filter architectures.

## Foundational Learning

- **Reinforcement Learning with Policy Optimization (PPO/GRPO)**: GenBreak uses GRPO to optimize the red-team LLM policy; understanding advantage estimation, clipping, and KL constraints is essential for debugging training instability.
  - Quick check: Can you explain why the KL divergence term in GRPO prevents policy collapse during training?

- **LoRA (Low-Rank Adaptation) for Efficient Fine-Tuning**: The implementation uses LoRA (rank=32) for both SFT and RL stages to reduce memory footprint and improve training stability on a single A100.
  - Quick check: What happens to LoRA adapters if you continue training beyond the optimal step—how would you detect overfitting?

- **Toxicity Evaluation in Vision-Language Systems**: The framework aggregates scores from MHSC, LLaVAGuard, and NudeNet; understanding how these models define and detect toxicity is critical for interpreting results and detecting reward hacking.
  - Quick check: If the toxicity evaluator has a bias toward certain visual styles, how might the RL policy exploit this?

## Architecture Onboarding

- **Component map:**
  ```
  Seed Prompt (Dseed) → Red-Team LLM (Llama-3.2-1B-Instruct + LoRA)
                              ↓
  [SFT Stage] ← Category Rewrite + Pre-Attack Datasets
                              ↓
  [RL Stage] → Generate G prompts per seed → Surrogate T2I (SD 2.1/3M)
                              ↓
         Integrated Filter (Text + Image) → Rewards (Toxicity, Bypass, Clean, Diversity)
                              ↓
         GRPO Update → Iterate until convergence
                              ↓
  Final Model → Transfer evaluation on commercial APIs
  ```

- **Critical path:**
  1. Dataset curation quality determines SFT effectiveness
  2. Reward weight tuning (λ1-λ8) directly affects convergence to practical attack prompts
  3. Filter simulation accuracy impacts transferability—mismatched filters yield poor transfer

- **Design tradeoffs:**
  - Higher clean reward weight improves stealth but may reduce diversity
  - Larger pool_size for diversity rewards improves exploration but increases memory/compute
  - Single-attempt evaluation is stricter but more realistic; multi-attempt inflates success rates

- **Failure signatures:**
  - Convergence to repetitive prompts → increase diversity reward weights or check reference pool freshness
  - High toxicity but low TBR/TCBR → bypass reward weight too high relative to toxicity reward
  - Poor transfer despite strong open-source performance → surrogate filter may not match target defenses
  - Gibberish outputs → check gibberish penalty weight and symbol regulation rules

- **First 3 experiments:**
  1. Ablation on reward components: Remove bypass reward, clean reward, and diversity rewards separately to confirm each component's contribution
  2. Cross-model transfer test: Train on SD 2.1, evaluate on SD 3 Medium to measure transfer within same model family
  3. Threshold sensitivity analysis: Sweep toxicity threshold τ from 0.0 to 0.5 to understand how TBR and TCBR degrade

## Open Questions the Paper Calls Out

- **Question 1:** How can red-teaming frameworks operate without access to image toxicity scores for filtered outputs in black-box commercial systems?
  - Basis: Training requires image toxicity scores as reward signals, which is challenging for black-box T2I services where filtered images are not returned.

- **Question 2:** What techniques would enable reliable evasion of industrial-grade image filters beyond prompt-level adversarial inputs?
  - Basis: The paper acknowledges that prompt-level adversarial inputs may fall short against advanced commercial image filters and leaves exploration of stronger techniques for future work.

- **Question 3:** To what extent does the integrated filter approximation used in training generalize to diverse real-world commercial content moderation policies?
  - Basis: The integrated filter may not perfectly reflect actual content moderation policies employed by commercial T2I systems.

## Limitations
- Training depends on image toxicity scores that are unavailable from commercial APIs when filters trigger
- Transfer success rates vary significantly across platforms (30-70%), suggesting incomplete filter generalization
- The approach may not work against fundamentally different safety architectures or semantic-level detection systems
- Commercial systems may employ detection strategies not captured by the integrated filter composition

## Confidence
- **Multi-objective RL effectiveness**: High - demonstrated through ablation studies and performance metrics
- **Dataset initialization contribution**: Medium - ablation study supports but requires direct comparison to from-scratch RL
- **Commercial transferability**: Medium - significant variation in success rates across platforms indicates incomplete generalization
- **Reward function design**: High - careful consideration of reward hacking prevention and practical utility

## Next Checks
1. Reproduce the ablation study removing each reward component to verify their individual contributions to final performance
2. Conduct cross-model transfer tests within the same model family before testing on commercial APIs
3. Perform threshold sensitivity analysis by sweeping toxicity thresholds to understand metric degradation patterns