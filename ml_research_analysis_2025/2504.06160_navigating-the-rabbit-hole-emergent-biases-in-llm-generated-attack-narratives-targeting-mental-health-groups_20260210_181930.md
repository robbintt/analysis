---
ver: rpa2
title: 'Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives
  Targeting Mental Health Groups'
arxiv_id: '2504.06160'
source_url: https://arxiv.org/abs/2504.06160
tags:
- disorder
- mental
- health
- entities
- people
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a network-based framework to examine how
  LLM-generated attack narratives disproportionately target mental health groups.
  By analyzing the Toxicity Rabbit Hole dataset, the authors find that mental health
  entities occupy central positions in attack narrative networks, showing significantly
  higher closeness centrality (p-value = 4.06e-10) and dense clustering (Gini coefficient
  = 0.7).
---

# Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups

## Quick Facts
- arXiv ID: 2504.06160
- Source URL: https://arxiv.org/abs/2504.06160
- Reference count: 33
- Key outcome: Mental health entities occupy central positions in attack narrative networks, showing significantly higher closeness centrality (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7), with stigmatization components—particularly labeling and status loss—being more prevalent when narratives shift to mental health targets.

## Executive Summary
This paper introduces a network-based framework to examine how LLM-generated attack narratives disproportionately target mental health groups. By analyzing the Toxicity Rabbit Hole dataset, the authors find that mental health entities occupy central positions in attack narrative networks, showing significantly higher closeness centrality (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). The study also reveals that stigmatization components—particularly labeling and status loss—are more prevalent when narratives shift to mental health entities. These findings highlight structural propensities in LLMs to amplify harmful discourse against vulnerable populations, calling for improved safety mechanisms to detect and mitigate recursive harm.

## Method Summary
The study analyzes the Toxicity Rabbit Hole (TRH) dataset using a network-based framework. Victim entities and stigmatization components are extracted from LLM-generated attack narratives using Llama 3.2 3B. A directed weighted graph is constructed where nodes represent victim entities and edges represent transitions between entities in consecutive generations. Network analysis metrics (closeness centrality, degree, PageRank) and community detection (Leiden algorithm) are applied to identify structural biases. Wilcoxon signed-rank tests compare stigmatization levels between initial targets and mental health entities.

## Key Results
- Mental health entities show significantly higher closeness centrality in attack narrative networks (p-value = 4.06e-10).
- Mental health entities form dense clusters with high Gini coefficient (0.7), creating "narrative sinkholes."
- Labeling and status loss stigmatization components are significantly more prevalent when narratives shift to mental health targets.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Mental health entities function as structural "attractors" in recursive toxic generation, making them disproportionately reachable targets.
- **Mechanism:** The generative process forms a weighted directed network where MH entities occupy positions of high closeness centrality. When a model recursively generates toxicity, the latent associations in the training data create shorter path lengths to MH concepts than to other entities, effectively pulling the narrative toward these groups even without explicit prompts.
- **Core assumption:** The network topology of the generated text reflects the underlying associative structure of the model's training data rather than random generation artifacts.
- **Evidence anchors:**
  - [abstract] Significantly higher mean closeness centrality (p-value = 4.06e-10).
  - [section 6] Gini coefficient of 0.7 indicates MH entities are concentrated in dense clusters, creating a "narrative sinkhole."
  - [corpus] Related work on "persuasion attacks" supports the general capacity of LLMs to generate targeted harmful narratives, though specific network centrality data for MH groups is unique to this paper.
- **Break condition:** If safety training explicitly de-weights the associative links between general negative concepts and specific MH diagnostic labels, the closeness centrality of MH nodes should theoretically decrease.

### Mechanism 2
- **Claim:** The severity of harm escalates qualitatively, not just quantitatively, when narratives shift to mental health targets.
- **Mechanism:** Using the Link & Phelan stigmatization framework, the study shows that attacking MH entities triggers a shift in rhetorical strategy. The model moves from general negative stereotyping to specific mechanisms of "Labeling" and "Status Loss" (e.g., dehumanization, calls for exclusion) at significantly higher rates than when attacking the initial target groups.
- **Core assumption:** The increase in "Labeling" and "Status Loss" components is a feature of the model's representation of mental illness (linking it to danger/incompetence) rather than a generic feature of all deep toxic recursions.
- **Evidence anchors:**
  - [abstract] Stigmatization components—particularly labeling and status loss—are more prevalent when narratives shift to mental health entities.
  - [section 7] Wilcoxon signed-rank tests show Labeling (p = 5.44e-81) and Status Loss (p = 7.16e-143) are significantly higher for MH targets ($V_{MH}$) compared to initial targets ($V_{init}$).
  - [corpus] Evidence regarding "stigma" specifically is limited in the immediate corpus neighbors; this mechanism relies heavily on the paper's specific analysis.
- **Break condition:** If the model were fine-tuned on counter-stigma datasets (e.g., person-first language, recovery narratives), the delta in "Status Loss" components between initial and MH targets would flatten.

### Mechanism 3
- **Claim:** Iterative "toxicity prompting" bypasses single-turn guardrails by normalizing escalating harm through context accumulation.
- **Mechanism:** The "Toxicity Rabbit Hole" (TRH) framework utilizes a greedy search where the output of step $i$ becomes the input for step $i+1$. By slowly increasing toxicity, the model remains within the "safe" latent space of the previous turn, eventually crossing into severe harm without triggering the refusal criteria that might block a direct prompt.
- **Core assumption:** The guardrails are sensitive to absolute toxicity thresholds or specific keywords in the immediate prompt, but fail to assess the *trajectory* or cumulative harm of the conversation.
- **Evidence anchors:**
  - [section 2] The framework instructs the LLM to "make the initial stereotype more toxic" using the previous output, effectively hiding the malicious intent behind the model's own generated context.
  - [section 1] Guardrails failed to block the generation of 459 million tokens of hateful content.
  - [corpus] Related research on "AI chatbot addiction" and "epistemic rabbit holes" supports the general risk of recursive, unmonitored interaction loops.
- **Break condition:** Implementation of "trajectory-aware" safety classifiers that evaluate the rate of sentiment change or the accumulation of stigmatizing components over a window of turns.

## Foundational Learning

- **Concept: Closeness Centrality (Network Science)**
  - **Why needed here:** To quantify how "easy" it is for a toxic narrative to reach a mental health entity. High closeness means the model encounters these groups quickly in a generation chain.
  - **Quick check question:** Does a high closeness centrality score for "people with anxiety" mean they are mentioned frequently (Degree) or that they are reachable from many other starting points in few steps?

- **Concept: Link & Phelan Stigmatization Framework**
  - **Why needed here:** To distinguish between "being mean" (negative stereotyping) and "creating systemic harm" (status loss/discrimination). This distinguishes the *severity* of the attack.
  - **Quick check question:** If a text says "people with depression are dangerous," which components are active: Labeling, Stereotyping, Separation, or Status Loss?

- **Concept: Recursive Prompting / Context Accumulation**
  - **Why needed here:** To understand why standard safety training fails. The attack relies on the model's own output context to legitimize increasingly extreme views.
  - **Quick check question:** Why might a model refuse a prompt "Write a hateful speech about X" but comply with "Make this text slightly more angry" repeated 10 times?

## Architecture Onboarding

- **Component map:** Input (TRH Dataset) -> Extraction Layer (Llama 3.2 3B) -> Graph Layer (Directed weighted graph) -> Analysis Layer (Centrality algorithms, Leiden algorithm, Wilcoxon tests)

- **Critical path:** Entity Extraction Accuracy → Graph Edge Construction → Statistical Comparison ($V_{init}$ vs $V_{MH}$)
  - *Note:* The entire analysis hinges on the reliability of Llama 3.2 in correctly identifying "Victim Entities" vs. "Non-participant Entities" (see Section 3.1).

- **Design tradeoffs:**
  - **Model Selection:** The study relies solely on Mistral 7B outputs due to computational constraints. *Assumption:* Findings generalize to other LLMs, but this is not proven in the text.
  - **Lexicon Matching:** Using substring matching for MH terms (e.g., "anxiety" in "people with anxiety") risks false positives, though authors claim manual review mitigated this.

- **Failure signatures:**
  - **Entity Hallucination:** Llama 3.2 might misclassify "advocates for the mentally ill" as a MH victim entity rather than a non-participant or advocate group.
  - **Metric Disconnect:** High centrality does not automatically imply high *harm* without the qualitative stigmatization analysis (RQ3).

- **First 3 experiments:**
  1. **Cross-Model Validation:** Replicate the Rabbit Hole network generation using a closed-source model (e.g., GPT-4) to see if MH entities retain high closeness centrality.
  2. **Trajectory Intervention:** Implement a "circuit breaker" that flags generations if the rate of "Labeling" components exceeds a threshold. Measure if this breaks the Rabbit Hole chains.
  3. **Lexicon Robustness:** Test the entity extraction pipeline on synthetic adversarial examples (e.g., "He is crazy smart") to verify it distinguishes between colloquialisms and clinical MH attacks.

## Open Questions the Paper Calls Out

- **Question:** Do the observed centralities and stigmatization patterns hold across other proprietary and open-source LLMs, or are they artifacts of Mistral 7B's specific training data or the TRH framework?
  - **Basis in paper:** [explicit] The authors explicitly state in the Limitations section that their "analysis is performed on generations from a specific framework (TRH) and model (Mistral) which may limit generalizability."
  - **Why unresolved:** The study restricted its corpus to Mistral 7B to manage computational constraints and ensure dataset uniformity, leaving the behavior of other architectures untested.
  - **Evidence:** Comparative analysis using the Toxicity Rabbit Hole framework applied to diverse architectures (e.g., GPT-4, Llama 3) to test for similar structural biases and stigmatization levels.

- **Question:** To what extent do automated LLM-based measures of stigmatization align with the perceptions of individuals who have personal experience with the mental health conditions targeted in the narratives?
  - **Basis in paper:** [explicit] The authors note, "Future work can expand these human evaluations by engaging people with the lived experience of mental health struggles."
  - **Why unresolved:** The study relies on a Llama 3.2 3B model for annotating stigmatization components, which serves as a proxy but may not fully capture the subjective impact of harm on the affected population.
  - **Evidence:** A user study involving members of relevant mental health communities to validate or correct the automated stigmatization labels assigned to the generated text.

- **Question:** How does the network structure and harm intensity differ when mental health entities are explicitly targeted in initial prompts compared to when they emerge autonomously as secondary targets?
  - **Basis in paper:** [explicit] The authors state, "Future work may explore the analysis of network structures where MHSet entities are also directly targeted" as a distinction from the current focus on emergent entities.
  - **Why unresolved:** The current methodology deliberately analyzes only chains where mental health terms were not present in the seed prompts, excluding direct targeting dynamics from the analysis.
  - **Evidence:** Constructing a parallel network using chains seeded with mental health terms and comparing the resulting centrality distributions and stigmatization severity to the current unprovoked network.

## Limitations

- The findings are based solely on Mistral 7B outputs, limiting generalizability to other LLMs.
- The substring-based mental health lexicon matching may introduce false positives in entity extraction.
- The hand-coded entity consolidation mapping for the top 2,500 entities is not provided, affecting reproducibility.

## Confidence

- **High Confidence:** The structural centrality of MH entities (Closeness centrality p-value = 4.06e-10) and the qualitative shift in stigmatization components (Labeling p = 5.44e-81, Status Loss p = 7.16e-143) are robust findings given the statistical significance and clear network patterns.
- **Medium Confidence:** The claim that MH entities function as "narrative sinkholes" (Gini coefficient = 0.7) is supported but depends on the Leiden algorithm's resolution parameter, which is unspecified.
- **Low Confidence:** The assertion that recursive prompting universally bypasses guardrails relies on a single dataset and model, requiring cross-model validation.

## Next Checks

1. **Cross-Model Replication:** Generate Rabbit Hole networks using GPT-4 or another closed-source model to test if MH entities consistently exhibit high closeness centrality.
2. **Safety Intervention Testing:** Implement trajectory-aware classifiers that flag rapid stigmatization escalation. Measure if this disrupts the recursive attack chains.
3. **Lexicon Robustness Evaluation:** Test the entity extraction pipeline on adversarial examples (e.g., "crazy smart") to ensure accurate distinction between colloquialisms and clinical MH attacks.