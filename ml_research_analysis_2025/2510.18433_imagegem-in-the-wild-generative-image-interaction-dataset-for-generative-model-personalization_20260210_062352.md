---
ver: rpa2
title: 'ImageGem: In-the-wild Generative Image Interaction Dataset for Generative
  Model Personalization'
arxiv_id: '2510.18433'
source_url: https://arxiv.org/abs/2510.18433
tags:
- preference
- user
- dataset
- image
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ImageGem introduces the first large-scale dataset of real-world
  user interactions with generative models, capturing 57K users, 242K customized LoRAs,
  3M prompts, and 5M generated images. This dataset enables new research into personalized
  preference modeling by providing fine-grained, individual-level interaction data.
---

# ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization

## Quick Facts
- **arXiv ID**: 2510.18433
- **Source URL**: https://arxiv.org/abs/2510.18433
- **Reference count**: 40
- **Primary result**: First large-scale dataset capturing real-world user interactions with generative models, enabling personalized preference modeling

## Executive Summary
ImageGem introduces the first large-scale dataset of real-world user interactions with generative models, capturing 57K users, 242K customized LoRAs, 3M prompts, and 5M generated images. This dataset enables new research into personalized preference modeling by providing fine-grained, individual-level interaction data. Using ImageGem, researchers demonstrated improved preference alignment over existing datasets, achieved state-of-the-art performance in personalized image retrieval and model recommendation, and introduced a novel framework for editing LoRA models in latent weight space to align with individual user preferences.

## Method Summary
The ImageGem dataset is constructed from Civitai API interactions, filtered for safety and quality, containing 57K users, 242K LoRAs, 3M prompts, and 5M images. The method comprises three main components: (1) DiffusionDPO fine-tuning for aggregated preference alignment using implicit feedback pairs constructed from interaction logs; (2) Personalized retrieval using collaborative filtering (SASRec/ItemKNN) combined with VLM-based ranking; and (3) LoRA weight-space editing through SVD/PCA decomposition to identify and traverse preference directions. The W2W framework treats LoRA weights as points in a shared latent space, allowing semantic editing via linear traversal along preference vectors.

## Key Results
- Demonstrated improved preference alignment over existing datasets using DiffusionDPO trained on ImageGem data
- Achieved state-of-the-art performance in personalized image retrieval and model recommendation tasks
- Introduced a novel framework for editing LoRA models in latent weight space to align with individual user preferences

## Why This Works (Mechanism)

### Mechanism 1
Individual visual preferences can be decoded from sparse, in-the-wild interaction logs (prompts, model selections, likes) to create effective training signals for personalization. The framework treats user interactions as implicit feedback, clustering user prompts/images with HDBScan to extract preference profiles. These profiles convert noisy behavioral data into structured input for retrieval or direction vectors for model editing. Core assumption: users consistently select models and prompts that align with stable, latent visual preference rather than exploring randomly.

### Mechanism 2
Customized diffusion models (LoRAs) can be semantically edited to match individual preferences by navigating a linear "Weights-to-Weights" (W2W) latent space. LoRA weight matrices are flattened, reduced via SVD and PCA to form a shared latent space, and a linear classifier trained on binary preference labels finds a normal vector representing the user's preference direction. Editing is performed via vector arithmetic: θ_edit = θ + αv. Core assumption: high-dimensional weights lie on a linear manifold where semantic attributes correspond to separable, linear directions.

### Mechanism 3
Vision-Language Models (VLMs) can function as stable, explainable rankers for personalized recommendation by synthesizing a textual "preference persona" from image history. Instead of relying solely on embedding similarity, the system prompts a VLM to generate a textual description of a user's visual taste based on their history, then compares candidate images against this text description. Core assumption: VLMs possess sufficient visual reasoning to articulate abstract aesthetic preferences and map them to new images consistently.

## Foundational Learning

- **LoRA (Low-Rank Adaptation) & Weight Spaces**: Understanding that LoRAs are decomposable matrices is essential for the W2W editing mechanism. Quick check: Can you explain why applying SVD to a LoRA weight matrix allows you to compare two LoRAs of different ranks?

- **Implicit Feedback & Preference Pairs**: The dataset lacks explicit "I prefer A over B" labels. You must understand how to construct preference pairs (x_w > x_l) from usage logs to train DiffusionDPO models. Quick check: How does the paper define a "positive" preference signal in the absence of explicit user rankings?

- **Collaborative Filtering (ItemKNN/SASRec)**: Before VLM ranks candidates, CF narrows down millions of items. Understanding these baselines is required to measure the lift provided by the VLM. Quick check: Why would a sequential model like SASRec perform better than a static ItemKNN model for generative model recommendation?

## Architecture Onboarding

- **Component map**: Data Layer (Users, LoRAs, Images) -> Retrieval Stage (Collaborative Filtering + FAISS) -> Ranking Stage (VLM-based scoring) -> Personalization Engine (SVD/PCA Pipeline)
- **Critical path**: The SVD/PCA Pipeline is essential for the core contribution (LoRA editing). Without standardizing diverse LoRA weights into a shared latent space, linear traversal is meaningless.
- **Design tradeoffs**: Fixed-Rank vs. SVD (selecting specific layers is brittle, while SVD is robust but computationally heavier); VLM vs. CLIP Retrieval (CLIP is fast but opaque, VLM is slow/inconsistent but explainable)
- **Failure signatures**: Style Entanglement (editing changes subject identity instead of style); VLM Instability (same image receiving vastly different scores); Sparsity (profiles with <3 interactions failing to generate meaningful clusters)
- **First 3 experiments**: (1) Validate W2W Linearity by replicating "Anime → Real" editing on small LoRA subset; (2) Implement ItemKNN and SASRec on interaction logs to establish baseline NDCG@10 score; (3) Train preference classifier for a user and check if "preferred" images are near-duplicates (overfitting) vs. novel generations matching taste

## Open Questions the Paper Calls Out

### Open Question 1
Can alternative methods for constructing latent weight spaces remove the reliance on PCA to support high-rank models and diverse domains? The authors state that PCA restricts model selection to low-rank LoRAs, limiting diversity and alignment with certain preferences. Evidence needed: A new weight space construction method that successfully integrates high-rank LoRAs and improves personalization in non-human domains like scenery.

### Open Question 2
What training objectives can effectively leverage raw, implicit user interaction data for preference alignment without relying on constructed preference pairs? The discussion notes that current experiments constructed preference sets using HPS, but future work should "explore ways to leverage the implicit feedback data from user interactions." Evidence needed: A fine-tuned model trained directly on implicit logs that outperforms models trained on constructed pairs in human preference benchmarks.

### Open Question 3
Does fine-tuning larger, state-of-the-art diffusion models (e.g., Flux) on the full ImageGem dataset yield superior preference alignment compared to SD1.5/SDXL? The authors identify a possible extension to conduct "human preference alignment for larger, up-to-date diffusion models, such as Flux." Evidence needed: Benchmark results showing improved Pick Score or HPSv2 metrics when Flux models are fine-tuned using ImageGem data.

## Limitations
- Data representativeness: Civitai-centric dataset may overrepresent hobbyist/gaming aesthetics, limiting generalizability to broader visual preference domains
- W2W linearity assumption: Effectiveness depends on assumption that semantic preferences map to linear directions, which may break down for complex style-content entanglements
- VLM-based ranking claims lack rigorous ablation studies isolating VLM contribution from other components

## Confidence
- **High**: Dataset construction methodology and basic retrieval performance metrics are reproducible and align with stated results
- **Medium**: W2W personalization framework shows promise in limited qualitative examples, but quantitative user preference validation is limited
- **Low**: VLM-based ranking claims lack rigorous ablation studies isolating the VLM contribution from other components

## Next Checks
1. **Cross-domain preference testing**: Apply W2W editing framework to non-Civitai LoRAs (e.g., from HuggingFace) to test generalizability beyond training distribution
2. **Temporal preference stability**: Track whether the same user's preference vectors remain consistent across different time periods to validate stable latent preferences assumption
3. **VLM hallucination detection**: Systematically evaluate VLM ranker's consistency by generating multiple explanations for the same image and measuring variance in assigned scores