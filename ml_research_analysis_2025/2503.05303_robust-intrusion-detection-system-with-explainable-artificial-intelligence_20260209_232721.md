---
ver: rpa2
title: Robust Intrusion Detection System with Explainable Artificial Intelligence
arxiv_id: '2503.05303'
source_url: https://arxiv.org/abs/2503.05303
tags:
- adversarial
- data
- detection
- attacks
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the vulnerability of machine learning-based
  intrusion detection systems (IDS) to adversarial attacks, particularly in 5G and
  beyond mobile network environments. The core method introduces a novel approach
  using Explainable Artificial Intelligence (XAI), specifically SHAP feature importance
  values, to detect adversarial attacks in real-time by analyzing deviations in the
  distribution of SHAP values from normal training data patterns.
---

# Robust Intrusion Detection System with Explainable Artificial Intelligence

## Quick Facts
- arXiv ID: 2503.05303
- Source URL: https://arxiv.org/abs/2503.05303
- Authors: Betül Güvenç Paltun; Ramin Fuladi; Rim El Malki
- Reference count: 15
- Key outcome: XAI-based detection improves accuracy to 0.9259 and F1-score to 0.8654 against adversarial attacks in O-RAN infrastructure

## Executive Summary
This paper introduces an intrusion detection system (IDS) that integrates Explainable AI (XAI) to detect adversarial attacks in 5G and beyond mobile networks. The core innovation uses SHAP feature importance values to monitor deviations from normal training data distributions, enabling real-time attack identification without adversarial training. Tested in an O-RAN environment against RRC signaling storm attacks, the approach significantly outperforms baseline methods like LIME and permutation importance. The framework also incorporates zero-touch mitigation by automatically adjusting labels when adversarial behavior is detected, providing immediate response capabilities.

## Method Summary
The method trains an autoencoder on normal network traffic, then computes SHAP values for the training set to establish per-feature distributions (mean μ and standard deviation σ). At runtime, incoming samples have their SHAP values computed and compared against bounds (μ ± λσ, with λ=2). Inputs with any feature's SHAP value outside these bounds are flagged as adversarial. This approach is model-agnostic and operates in parallel with the autoencoder's reconstruction error analysis. The framework was implemented in an O-RAN testbed using FlexRIC for RRC feature collection, detecting attacks by analyzing deviations in SHAP value distributions rather than raw traffic patterns.

## Key Results
- SHAP-based detection achieved accuracy of 0.9259 and F1-score of 0.8654
- Outperformed LIME (0.8307 accuracy) and permutation importance methods
- Successfully detected RRC signaling storm attacks in O-RAN infrastructure
- Demonstrated zero-touch mitigation capability through automatic label adjustment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial inputs produce detectably anomalous SHAP value distributions compared to normal traffic, enabling attack identification without adversarial training.
- Mechanism: SHAP feature importance values computed during training are assumed to follow normal distributions per feature. Runtime inputs are flagged as adversarial if any feature's SHAP value falls outside μ ± λσ bounds.
- Core assumption: Legitimate traffic SHAP values follow stable, characterizable distributions that adversarial perturbations disrupt measurably.
- Evidence anchors: [abstract] "analyzing deviations in the distribution of SHAP values from normal training data patterns"; [section IV-B] "The essential part is to assess the distribution of SHAP values within the training data, rather than the training data itself"; related work on SHAP-based adversarial detection supports feasibility.
- Break condition: Attackers crafting perturbations that preserve SHAP value distributions while causing misclassification would defeat this method.

### Mechanism 2
- Claim: Model-agnostic XAI integration allows adversarial detection without modifying the underlying IDS model architecture or retraining.
- Mechanism: The XAI layer operates independently of the autoencoder IDS, computing SHAP values in parallel to reconstruction error analysis. This separation enables application to any ML-based IDS.
- Core assumption: The relationship between input perturbations and explanation deviation is consistent across model types.
- Evidence anchors: [abstract] "agnostic methodology that utilizes XAI techniques to evaluate how adversarial samples impact the interpretations of machine learning models"; [section IV] "applicable regardless of the IDS model or use case".
- Break condition: If SHAP computation becomes prohibitively expensive for high-dimensional real-time traffic, the agnostic advantage is undermined by latency.

### Mechanism 3
- Claim: Zero-touch mitigation via label adjustment provides immediate response to detected adversarial inputs without human intervention.
- Mechanism: When runtime SHAP analysis flags an input as manipulated (outside distribution bounds), the system automatically adjusts its classification label and triggers countermeasures.
- Core assumption: Label adjustment based on distribution deviation correctly identifies true adversarial samples without excessive false positives.
- Evidence anchors: [abstract] "zero-touch mitigation strategy that relies on anticipated adversarial examples"; [section V-B3] "If an input falls outside the usual distribution range and is classified as an outlier, it is considered manipulated, and we adjust its label accordingly".
- Break condition: Poorly tuned threshold λ could cause legitimate traffic bursts to trigger false mitigation actions, causing service disruption.

## Foundational Learning

- Concept: **Shapley Additive Explanations (SHAP)**
  - Why needed here: Core detection mechanism relies on computing and analyzing SHAP feature importance distributions
  - Quick check question: Can you explain why SHAP values might change significantly when an input has been adversarially perturbed, even if the raw feature values change only slightly?

- Concept: **Adversarial Attack Methods (FGSM, PGD, BIM)**
  - Why needed here: The paper tests against specific gradient-based attacks
  - Quick check question: Why would an iterative attack like PGD potentially produce different SHAP distribution patterns compared to a single-step FGSM attack?

- Concept: **Autoencoder-based Anomaly Detection**
  - Why needed here: The baseline IDS uses autoencoder reconstruction error for attack detection
  - Quick check question: What is the reconstruction error signal that indicates an anomaly, and why might adversarial examples evade this detection?

## Architecture Onboarding

- Component map: Network traffic → Autoencoder inference → SHAP computation → Distribution bounds check → Classification → Mitigation
- Critical path: Collect RRC features via FlexRIC E2 interface → Compute SHAP values → Check against training distribution bounds → Flag as attack if any feature exceeds μ ± λσ → Adjust label if adversarial
- Design tradeoffs:
  - λ threshold selection: Lower values increase detection sensitivity but raise false positive rates; higher values reduce false positives but may miss subtle attacks
  - SHAP vs. LIME vs. Permutation: SHAP achieved 0.9259 accuracy vs. LIME's 0.8307, but SHAP computation is more expensive
  - Feature count: Paper demonstrates effectiveness with only 5 features; assumption is this generalizes to higher dimensions
- Failure signatures:
  - High false positive rate: Legitimate traffic spikes incorrectly flagged as adversarial—verify threshold calibration against baseline traffic variance
  - Detection latency: If SHAP computation exceeds real-time requirements for high-throughput RRC signaling, the system fails its zero-touch response goal
  - Evasion via distribution-preserving attacks: Sophisticated attackers who understand the defense might craft perturbations that maintain SHAP distribution conformity
- First 3 experiments:
  1. Baseline validation: Deploy autoencoder IDS on collected RRC traffic without adversarial detection; measure accuracy under normal and RRC signaling storm conditions
  2. Adversarial robustness test: Apply FGSM, PGD, BIM, and Gaussian attacks with varying epsilon values; measure IDS accuracy degradation
  3. XAI detection comparison: Implement SHAP, LIME, and permutation importance detection layers; compare accuracy, precision, recall, and F1-score under BIM attack to validate SHAP superiority

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions. However, based on the analysis of limitations and unresolved aspects, several key questions emerge regarding adaptive adversaries, computational latency, distribution assumptions, and multi-UE scenarios that would benefit from further investigation.

## Limitations
- Limited corpus evidence for the specific distribution-based SHAP detection approach
- No latency measurements provided to confirm real-time performance requirements are met
- Assumed normal distribution of SHAP values not statistically validated on actual network traffic data
- Experimental setup used single malicious UE without testing in multi-UE scenarios with mixed traffic

## Confidence
- Detection mechanism plausibility: **High** (theoretically grounded with supporting evidence)
- Experimental results (specific accuracy/F1): **Medium** (lacks reproducibility details and public dataset access)
- Zero-touch mitigation effectiveness: **Low** (minimal evidence provided beyond basic label adjustment)

## Next Checks
1. Reproduce the detection framework on a public IDS dataset (e.g., CICIDS2017) with the same 5-feature constraint to verify SHAP superiority claims.
2. Test the detection mechanism against adaptive attacks specifically designed to preserve SHAP distribution patterns while causing misclassification.
3. Conduct latency profiling to confirm the XAI layer meets real-time O-RAN signaling requirements under high traffic loads.