---
ver: rpa2
title: Realistic Evaluation of Deep Partial-Label Learning Algorithms
arxiv_id: '2502.10184'
source_url: https://arxiv.org/abs/2502.10184
tags:
- icml
- learning
- label
- feng
- algorithms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents PLENCH, the first comprehensive benchmark for
  partial-label learning (PLL) algorithms. The authors identify critical issues in
  PLL evaluation including inconsistent experimental settings, neglected model selection
  problems, and lack of real-world image datasets.
---

# Realistic Evaluation of Deep Partial-Label Learning Algorithms

## Quick Facts
- arXiv ID: 2502.10184
- Source URL: https://arxiv.org/abs/2502.10184
- Authors: Wei Wang; Dong-Dong Wu; Jindong Wang; Gang Niu; Min-Ling Zhang; Masashi Sugiyama
- Reference count: 29
- Primary result: Introduces PLENCH, the first comprehensive benchmark for partial-label learning algorithms, revealing that simple methods often outperform complex ones

## Executive Summary
This paper presents PLENCH, the first comprehensive benchmark for partial-label learning (PLL) algorithms that addresses critical evaluation issues in the field. The authors identify three major problems: inconsistent experimental settings across studies, neglected model selection challenges specific to PLL, and lack of realistic image datasets for evaluation. They propose three novel model selection criteria with theoretical guarantees (Covering Rate, Approximated Accuracy, and Oracle Accuracy) and create PLCIFAR10, a novel dataset with human-annotated partial labels. Testing 27 algorithms across 11 real-world datasets, the benchmark reveals that simple algorithms like PRODEN often outperform more complex ones, no single algorithm dominates across all scenarios, and model selection is crucial for PLL performance.

## Method Summary
The PLENCH benchmark establishes standardized evaluation protocols for PLL algorithms through four key components: (1) a comprehensive collection of 11 real-world datasets including 9 tabular datasets and 2 human-annotated image datasets (PLCIFAR10-Aggregate and PLCIFAR10-Vaguest), (2) three novel model selection criteria with theoretical foundations, (3) standardized experimental protocols including consistent validation splits and hyperparameter optimization procedures, and (4) implementation of 27 PLL algorithms spanning four categories. The benchmark evaluates algorithms using test accuracy as the primary metric while addressing the critical model selection problem through CR, AA, and OA criteria, with theoretical guarantees established for CR under specific label generation assumptions.

## Key Results
- PRODEN-family algorithms demonstrate strong performance across most datasets, often outperforming more complex methods
- No single algorithm dominates across all scenarios, confirming the need for comprehensive benchmarking
- Covering Rate achieves approximately 86% on PLCIFAR10-Aggregate, comparable to Oracle Accuracy with Expected Size selection
- Simple algorithms are frequently underestimated in previous evaluations due to inconsistent experimental settings
- Real-world partial labels exhibit significant noise (up to 17.56% in PLCIFAR10-Vaguest), validating the need for noisy PLL algorithms

## Why This Works (Mechanism)

### Mechanism 1: Covering Rate (CR) as a Model Selection Criterion
CR measures the fraction of validation examples where predicted labels fall within candidate sets. Under Uniform Sampling Strategy (USS) or Flipping Probability Strategy (FPS) assumptions, CR serves as a statistically consistent proxy for expected accuracy, with theoretical bounds showing the gap is at most $(1-\epsilon)\gamma$ where $\epsilon$ is accuracy and $\gamma$ is ambiguity degree. This provides a model selection criterion that doesn't require true labels.

### Mechanism 2: Progressive Label Disambiguation (PRODEN-family)
PRODEN algorithms use self-training loops that alternate between training on current label confidence estimates and updating confidence based on model predictions. This progressive refinement gradually isolates true labels from candidates without explicit regularization, achieving strong performance with minimal computational overhead.

### Mechanism 3: Noise-Aware Training for Realistic PLL
Noisy PLL algorithms incorporate mechanisms to handle missing true labels through clean sample selection and weighted loss functions. These approaches are necessary for real-world deployment where candidate sets may exclude true labels, as demonstrated by the 17.56% noise rate in PLCIFAR10-Vaguest.

## Foundational Learning

- **Concept: Risk Consistency vs. Classifier Consistency**
  - Why needed here: PLL algorithms differ in whether they minimize consistent risk estimators or converge to Bayes-optimal classifier. Model selection criteria also have different consistency properties—CR's consistency under USS/FPS assumptions enables reliable algorithm comparison.
  - Quick check question: Can you explain why Approximated Accuracy requires calibrated probability outputs while Covering Rate doesn't?

- **Concept: Ambiguity Degree ($\gamma$)**
  - Why needed here: This measure quantifies how often false labels appear in candidate sets and directly affects CR's effectiveness. It connects data quality to model selection reliability through theoretical bounds.
  - Quick check question: If $\gamma = 1$ (any label can appear as candidate), what does CR tell you about model performance?

- **Concept: Label Generation Assumptions (USS vs. FPS vs. Instance-Dependent)**
  - Why needed here: The paper proves theoretical guarantees under specific generation processes. Real human annotations may violate these assumptions—PLCIFAR10 reveals non-uniform flipping matrices, demonstrating why benchmark evaluation matters.
  - Quick check question: Why might constant flipping probability be unrealistic for human annotators labeling ambiguous images?

## Architecture Onboarding

- **Component map:**
  ```
  PLENCH Benchmark
  ├── Data Layer
  │   ├── 9 tabular datasets (Lost, MSRCv2, Birdsong, etc.)
  │   └── 2 image datasets (PLCIFAR10-Aggregate, PLCIFAR10-Vaguest)
  ├── Model Selection Layer
  │   ├── Covering Rate (CR) - prediction-in-candidate metric
  │   ├── Approximated Accuracy (AA) - probability-weighted metric
  │   └── Oracle Accuracy (OA) - requires true labels (research only)
  └── Algorithm Zoo (27 algorithms in 4 categories)
      ├── Vanilla PLL (PRODEN, CAVL, POP, ABS-*, CC, LWS, IDGP)
      ├── Vanilla CLL (PC, Forward, NN, GA, SCL-*, L-W, OP-W)
      ├── Holistic PLL (VALEN, PiCO, ABLE, CRDPLL, DIRK)
      └── Noisy PLL (FREDIS, ALIM, PiCO+)
  ```

- **Critical path:**
  1. Choose dataset → determine if synthetic or real human annotations
  2. If real data → estimate noise rate → select noisy PLL algorithm if >5% noise
  3. Configure model selection → use CR as default (Table 1 shows CR achieves ~86% on PLCIFAR10-Aggregate vs. OA&ES's ~86%)
  4. Run hyperparameter search with validation-based selection (20 random configs per split)
  5. Report test accuracy with standard deviations across 3-5 random splits

- **Design tradeoffs:**
  - **CR vs. AA:** CR is robust across algorithms but may plateau; AA is more informative but fails when models aren't calibrated (e.g., ABS-MAE drops to 45.51% with AA vs. 55.13% with CR)
  - **Simple vs. Complex Algorithms:** PRODEN-family is fast and competitive; holistic methods (PiCO, ABLE, CRDPLL) add 50-100% compute overhead for marginal gains
  - **Aggregate vs. Vaguest annotations:** Aggregate increases candidate set size (avg 4.87 labels) but 0.13% noise; Vaguest has tighter sets (avg 3.49 labels) but 17.56% noise

- **Failure signatures:**
  - AA diverges from test accuracy → model not calibrated (check if softmax outputs reflect true probabilities)
  - Training accuracy increases but CR stays flat → candidate sets may exclude true labels (noise rate issue)
  - Algorithm works on synthetic data but fails on PLCIFAR10 → generation assumption violated (check flipping matrix uniformity)

- **First 3 experiments:**
  1. **Baseline establishment:** Run PRODEN with CR model selection on PLCIFAR10-Aggregate using ResNet-18; target ~85.95% ± 0.08% accuracy
  2. **Model selection ablation:** Compare CR vs. AA vs. OA on Birdsong (tabular dataset) to understand selection criterion sensitivity across data modalities
  3. **Noise sensitivity test:** Train PRODEN and one noisy PLL algorithm (e.g., FREDIS) on both PLCIFAR10 variants to quantify performance gap between 0.13% vs. 17.56% noise rate scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- **Assumption dependency of CR:** The theoretical guarantees for Covering Rate depend heavily on USS/FPS label generation assumptions, which often fail in real human annotations with non-uniform flipping matrices
- **Model selection criterion saturation:** Approximated Accuracy requires calibrated probabilities, limiting its practical utility since many PLL algorithms don't naturally produce well-calibrated outputs
- **Noisy PLL performance gap:** Noisy PLL algorithms underperform clean PLL algorithms on PLCIFAR10-Aggregate despite its 0.13% noise rate, suggesting over-conservative designs or unnecessary complexity

## Confidence
- PLENCH provides first comprehensive, fair evaluation of PLL algorithms: **High**
- Simple algorithms like PRODEN often outperform complex ones: **High**
- No single algorithm dominates across all scenarios: **High**
- CR is reliable model selection criterion with theoretical guarantees: **Medium**
- PLCIFAR10 captures realistic PLL challenges: **Medium**

## Next Checks
1. Test CR's correlation with accuracy on synthetic datasets with varying degrees of label noise and ambiguity to quantify its reliability boundaries under different generation strategies.

2. Implement and evaluate model calibration techniques (temperature scaling, isotonic regression) to enable Approximated Accuracy as a practical selection criterion across more PLL algorithms.

3. Conduct ablation studies on PLCIFAR10-Vaguest to determine the minimum noise threshold where noisy PLL algorithms (FREDIS, ALIM, PiCO+) provide significant advantages over clean PLL methods.