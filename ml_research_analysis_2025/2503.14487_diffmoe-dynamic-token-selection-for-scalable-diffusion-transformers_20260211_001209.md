---
ver: rpa2
title: 'DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers'
arxiv_id: '2503.14487'
source_url: https://arxiv.org/abs/2503.14487
tags:
- token
- diffusion
- diffmoe
- dynamic
- capacity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiffMoE, a novel approach for scaling diffusion
  transformers through dynamic token selection and global token accessibility. The
  key insight is that diffusion processes inherently involve heterogeneous token distributions
  across noise levels and conditions, which existing MoE approaches fail to leverage
  due to restricted token accessibility.
---

# DiffMoE: Dynamic Token Selection for Scalable Diffusion Transformers

## Quick Facts
- **arXiv ID**: 2503.14487
- **Source URL**: https://arxiv.org/abs/2503.14487
- **Reference count**: 40
- **Primary result**: Achieves SOTA performance on ImageNet among diffusion models using 1× activated parameters while outperforming dense architectures with 3× activated parameters

## Executive Summary
This paper introduces DiffMoE, a novel approach for scaling diffusion transformers through dynamic token selection and global token accessibility. The key insight is that diffusion processes inherently involve heterogeneous token distributions across noise levels and conditions, which existing MoE approaches fail to leverage due to restricted token accessibility. DiffMoE addresses this by implementing a batch-level global token pool during training, allowing experts to access comprehensive token distributions across different noise levels and samples. Additionally, it incorporates a capacity predictor that dynamically allocates computational resources based on noise levels and sample complexity. The approach achieves state-of-the-art performance on ImageNet benchmark among diffusion models, substantially outperforming both dense architectures with 3× activated parameters and existing MoE approaches while maintaining 1× activated parameters. The effectiveness extends beyond class-conditional generation to text-to-image tasks, demonstrating broad applicability across diffusion model applications.

## Method Summary
DiffMoE builds on diffusion transformers (DiT) by replacing dense feed-forward networks with a mixture-of-experts (MoE) architecture. The key innovation is a batch-level global token pool that flattens all tokens across the batch and sequence dimensions during training, allowing experts to access a comprehensive distribution of tokens from different noise levels and samples. This is paired with a capacity predictor - a lightweight two-layer MLP that dynamically determines which tokens each expert should process based on input complexity. The system uses exponential moving average to maintain per-expert thresholds that ensure inference computation matches training computation. During inference, tokens are processed within their original batch and sequence structure, with dynamic capacity allocation based on the learned capacity predictor.

## Key Results
- Achieves 3.05 FID on ImageNet 256×256, outperforming dense architectures with 3× activated parameters
- Outperforms existing MoE approaches while using only 1× activated parameters
- Demonstrates effectiveness on text-to-image tasks on COCO dataset
- Shows consistent improvements across multiple model sizes (S, B, L) and expert configurations (E=4,8,16)

## Why This Works (Mechanism)

### Mechanism 1: Batch-Level Global Token Pool for Expert Specialization
Enabling experts to access tokens across different noise levels and conditions during training promotes specialized expert behavior and accelerates convergence. During training, tokens from all samples in a batch are flattened into a single pool, allowing each expert to select tokens based on global distribution characteristics rather than being constrained to intra-sample selection. This approximates the full token distribution across noise levels t ∈ [0,1] and varying conditions. Experts benefit from learning contrastive patterns across the full diffusion trajectory (pure noise to clean images) rather than isolated samples at similar noise levels.

### Mechanism 2: Capacity Predictor for Adaptive Inference Computation
A learned predictor can dynamically allocate computational resources per expert based on input complexity, achieving better performance than fixed top-K selection. A two-layer MLP with SiLU activations takes global pooled tokens as input and predicts which tokens each expert should process. During training, it learns from actual routing patterns via binary cross-entropy loss with stop-gradient (preventing interference with main diffusion training). At inference, predicted capacity per expert is determined by thresholding these predictions. Token routing patterns learned during training generalize to inference-time allocation decisions, and sample complexity correlates with computational requirements.

### Mechanism 3: Dynamic Threshold via EMA for Stable Inference-Training Alignment
Maintaining per-expert thresholds via exponential moving average during training ensures inference computation matches training computation (C_avg_infer ≈ 1) without manual tuning. During training, thresholds τ are updated using EMA (α=0.95) based on the k-th largest capacity prediction value, where k is determined by target capacity. This automatically learns thresholds that maintain desired computational budget at inference. The quantile of capacity predictions during training provides a stable estimate for inference thresholding, and the optimal computation-quality trade-off remains consistent across diffusion timesteps.

## Foundational Learning

- **Mixture-of-Experts (MoE) Routing Paradigms**: DiffMoE builds on and contrasts with Token-Choice (tokens select experts) and Expert-Choice (experts select tokens) paradigms. Understanding their limitations (restricted token accessibility) motivates the global pool design. Can you explain why TC-MoE achieves load balancing through auxiliary losses while EC-MoE achieves it naturally through expert selection?

- **Diffusion Process Heterogeneity**: The core insight is that diffusion inherently involves varying token distributions across noise levels t∈[0,1]. Recognizing this heterogeneity is essential to understanding why global token access matters. At what noise level (t≈0.0, 0.5, or 1.0) would you expect token representations to be most similar across different class conditions, and why?

- **Capacity Metrics for MoE Comparison**: The paper defines capacity C^E = N × (tokens processed by E) / (total tokens) for fair comparison. This standardizes computation measurement across architectures with different token selection strategies. If an expert processes 64 tokens from a global pool of 1024 tokens with 16 total experts, what is C^E? (Answer: 16 × 64 / 1024 = 1.0)

## Architecture Onboarding

- **Component map**: Input tokens → Flatten to global pool (R^{BS×D}) → Router: Learnable weight matrix W_r ∈ R^{D×N} computes affinity scores → Capacity Predictor: 2-layer MLP (detached from main computation graph) → Experts: N parallel FFNs with identical architecture → Threshold Manager: Per-expert τ values maintained via EMA

- **Critical path**: 1. Token flattening (must preserve gradient flow for diffusion loss) 2. Affinity computation via router (softmax along expert dimension) 3. Top-K selection with K determined by capacity predictor (training: fixed K=B×S/N; inference: dynamic via threshold) 4. Expert processing and weighted combination 5. Capacity predictor loss computation (stop-gradient on input, doesn't affect diffusion loss)

- **Design tradeoffs**: Global pool size vs. memory: Larger batches improve distribution approximation but require linear memory scaling for token storage. Threshold method: Interval search finds optimal FID but is labor-intensive; dynamic threshold is automatic but may be sub-optimal by 0.2-0.3 FID. Number of experts: Diminishing returns after E=8; E=16 provides marginal improvement but increases total parameters by ~73%.

- **Failure signatures**: Expert collapse: If certain experts consistently receive zero tokens, check threshold values and capacity predictor calibration. Training-inference gap: If inference FID degrades significantly despite good training loss, verify C_avg_infer ≈ C_train using Eq. 15. Memory overflow: Global pooling with large batch × sequence length requires careful gradient checkpointing. Load imbalance: Unlike TC-MoE, DiffMoE doesn't need auxiliary losses, but extreme imbalance suggests router initialization issues.

- **First 3 experiments**: 1. Validate global pool benefit: Train DiffMoE-L-E4 with and without global pooling (intra-sample only), comparing training loss curves at 100K, 300K, 700K steps on ImageNet 256×256 subset. 2. Capacity predictor ablation: Compare fixed top-K routing vs. capacity predictor on DiffMoE-L-E16, measuring FID and C_avg_infer at multiple checkpoints to verify adaptive allocation. 3. Scaling sanity check: Train S/B/L variants with E=4,8,16 experts for 200K steps, confirming loss decreases with model size and expert count.

## Open Questions the Paper Calls Out
- **Can advanced MoE architectures like Fine-Grained Experts or Shared Experts further enhance DiffMoE's performance?** The Conclusion states that integrating advanced techniques like Fine-Grained Expert and Shared Expert architectures "presents compelling opportunities for future work." The authors excluded these modern enhancements to ensure fair comparisons against standard dense and MoE baselines.

## Limitations
- Evaluation relies entirely on synthetic diffusion benchmarks (ImageNet 256×256 class-conditional, COCO-based text-to-image) rather than real-world deployment scenarios.
- The global token pool mechanism requires batch sizes large enough to approximate the full dataset distribution - this may fail for small batch regimes where diversity is insufficient.
- The capacity predictor is trained with stop-gradient, meaning it cannot backpropagate error signals from diffusion loss, potentially limiting its adaptation to optimal routing patterns.

## Confidence
**High Confidence**: Performance claims relative to dense baselines and TC-MoE/EC-MoE approaches (Table 1, 2, 3) - these show consistent improvements across multiple metrics and model sizes with clear statistical separation.

**Medium Confidence**: The three proposed mechanisms (global pool, capacity predictor, dynamic threshold) - while individually ablated and shown to contribute improvements, the interaction effects between them and their relative contributions remain unclear.

**Low Confidence**: Claims about broad applicability beyond benchmark tasks - the text-to-image results are promising but limited to one dataset (COCO) and one model size (S), without systematic scaling studies.

## Next Checks
1. **Distribution Shift Robustness**: Evaluate DiffMoE on out-of-distribution datasets (e.g., CIFAR-100 for ImageNet-trained models, or different text-to-image datasets) to verify that learned routing patterns generalize beyond training distribution.

2. **Small Batch Performance**: Systematically evaluate DiffMoE with B=2,4,8 to determine the minimum batch size required for global pool effectiveness, and identify at what point intra-sample routing becomes superior.

3. **Temporal Stability Analysis**: Track expert specialization patterns and capacity allocation across the full diffusion trajectory (t=0.0 to 1.0) to verify whether routing decisions remain stable or require timestep-specific adaptation.