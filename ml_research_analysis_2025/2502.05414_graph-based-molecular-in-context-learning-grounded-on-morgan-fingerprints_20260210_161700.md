---
ver: rpa2
title: Graph-based Molecular In-context Learning Grounded on Morgan Fingerprints
arxiv_id: '2502.05414'
source_url: https://arxiv.org/abs/2502.05414
tags:
- molecular
- graph
- gamic
- learning
- molecule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of improving in-context learning
  (ICL) for molecular tasks by developing a novel method that combines graph neural
  networks with local molecular features to better capture molecular structures. The
  core idea is to use a self-supervised learning technique, GAMIC, which aligns global
  molecular structures with textual descriptions while leveraging local feature similarity
  through Morgan fingerprints.
---

# Graph-based Molecular In-context Learning Grounded on Morgan Fingerprints

## Quick Facts
- arXiv ID: 2502.05414
- Source URL: https://arxiv.org/abs/2502.05414
- Authors: Ali Al-Lawati; Jason Lucas; Zhiwei Zhang; Prasenjit Mitra; Suhang Wang
- Reference count: 7
- Primary result: GAMIC outperforms simple Morgan-based ICL retrieval methods across all tasks by up to 45%

## Executive Summary
This paper addresses the challenge of improving in-context learning (ICL) for molecular tasks by developing a novel method that combines graph neural networks with local molecular features to better capture molecular structures. The core innovation is GAMIC, which aligns global molecular structures with textual descriptions while leveraging local feature similarity through Morgan fingerprints. Additionally, a Maximum Marginal Relevance (MMR) based diversity heuristic is introduced during retrieval to optimize input prompt demonstration samples.

## Method Summary
GAMIC employs a Graph Attention Network (GAT) to encode global molecular connectivity and atomic features, then aligns these embeddings with text embeddings using contrastive learning. The Morgan Sampler uses local fingerprint similarity to select positive and negative pairs for training, bridging the modality gap between graphs and text. During inference, MMR-based diversity selection retrieves demonstration examples that are similar to the query but dissimilar to each other, with demonstrations appended in reverse order to leverage LLM recency bias.

## Key Results
- GAMIC achieves up to 45% improvement over Morgan-only baselines across molecular captioning, property prediction, and yield prediction tasks
- Optimal performance achieved with k=2 shots for captioning and k=3 shots for other tasks
- MMR-based diversity selection consistently outperforms standard top-k retrieval methods

## Why This Works (Mechanism)

### Mechanism 1: Local-Global Feature Alignment
Combining local chemical fingerprints (Morgan) with global graph structures (GNN) creates superior retrieval embeddings for molecular tasks than using either alone. The system uses a Graph Attention Network (GAT) to encode global connectivity and atomic features into a latent vector $z$, aligning this vector with text embeddings (via SciBERT) using contrastive learning. The Morgan Sampler uses local fingerprint similarity to select "hard" positive and negative pairs for this training, bridging the modality gap between discrete graphs and continuous text.

### Mechanism 2: Diversity-Optimized Retrieval (MMR)
Retrieving diverse demonstration examples prevents context redundancy and improves the LLM's ability to generalize to the test molecule. Instead of selecting the top-$k$ most similar molecules, the system uses Maximum Marginal Relevance (MMR) to iteratively select samples that are similar to the test query but dissimilar to already selected samples.

### Mechanism 3: Demonstration Ordering Sensitivity
The relative position of demonstrations within the prompt significantly impacts inference performance for small-to-medium LLMs. The paper employs a specific permutation strategy where selected demonstrations are appended in reverse order of retrieval (closest to the query last/farthest first), leveraging the LLM's recency bias.

## Foundational Learning

- **Concept: In-Context Learning (ICL)** - Why needed: The entire GAMIC framework functions as a retriever to feed an LLM via ICL, avoiding fine-tuning. You must understand that the model learns solely from the prompt context. Quick check: Does the model update its weights during the inference of a molecule caption? (Answer: No)

- **Concept: Morgan Fingerprints vs. Molecular Graphs** - Why needed: The core motivation is that Morgan fingerprints capture local features but miss global connectivity. Understanding this distinction is crucial to grasping why GNNs are added. Quick check: If two molecules have the same functional groups but different ring sizes, will a Morgan fingerprint distinguish them effectively? (Answer: Often yes, but it may miss the global spatial relationships the paper emphasizes)

- **Concept: Contrastive Learning (NCE)** - Why needed: GAMIC projects graph data into the text space using a contrastive loss. You need to know that this involves pulling positive pairs closer and pushing negative pairs apart in vector space. Quick check: In the GAMIC training loop, what acts as the "positive" sample for a molecular graph? (Answer: The text caption of that molecule)

## Architecture Onboarding

- **Component map:** SMILES + Molecular Graphs -> 2-layer GAT + Mean Pooling -> Graph Embeddings -> InfoNCE Loss -> Text Embeddings (SciBERT) -> Projector -> Morgan Sampler -> MMR Retrieval -> LLM

- **Critical path:** The Graph Projection Training (Section 3.3). If the contrastive learning fails to align the graph embeddings $z$ with the text embeddings $y_{emb}$, the retriever will fetch irrelevant examples, and the LLM will hallucinate or fail.

- **Design tradeoffs:** Increasing $k$ adds context but consumes token limits (performance plateaus after $k=3$ for captioning). High $\lambda$ prioritizes diversity over relevance (optimal $\lambda=0.3$ found; higher values risk retrieving noise).

- **Failure signatures:** F1-score = 0.0 indicates the retriever provided no useful signal. Plateauing BLEU scores suggest the retriever is introducing noise or the context window is diluting attention.

- **First 3 experiments:** 
  1. Ablate the Sampler: Train the Graph Projector without the Morgan Sampler (using random negatives) to measure the "modality gap" reduction directly
  2. MMR Stress Test: Run retrieval with $\lambda=0$ (Pure similarity) vs. $\lambda=0.3$ vs. $\lambda=0.9$ on a diverse dataset to verify the diversity benefit curve
  3. Order Sensitivity Check: Feed the exact same $k$ examples to the LLM in "Retrieval Order" vs. "Reverse Order" to replicate the recency bias finding

## Open Questions the Paper Calls Out
None

## Limitations
- Hyperparameter values (GAT dimensions, NCE temperature, learning rate, Morgan sampling thresholds) are not specified, creating reproducibility gaps
- Limited ablation studies and lack of comparison against modern graph-only retrieval methods
- Demonstration ordering effect lacks rigorous statistical validation across different LLM architectures
- Scalability analysis limited to small-to-medium LLMs (7-8B parameters)

## Confidence
- **High Confidence**: The core mechanism of combining graph neural networks with Morgan fingerprints for contrastive learning is technically sound
- **Medium Confidence**: The specific implementation details and hyperparameter choices appear reasonable but are not fully specified
- **Low Confidence**: The demonstration ordering sensitivity claim lacks sufficient empirical validation and scalability claims are speculative

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary NCE temperature (0.01-1.0), GAT layer dimensions (32-256 units), and Morgan sampling thresholds (0.3-0.9 Tanimoto similarity)
2. **Direct MMR Ablation**: Compare MMR-selected demonstrations against random selection, top-k similarity selection, and a simple diversity baseline across all three task types
3. **Cross-LLM Validation**: Test the reverse ordering effect on at least two additional LLM architectures with varying attention mechanisms to determine if the recency bias generalizes beyond Mistral-7B results