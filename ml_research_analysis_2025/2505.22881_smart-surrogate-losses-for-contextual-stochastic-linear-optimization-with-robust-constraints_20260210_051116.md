---
ver: rpa2
title: Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust
  Constraints
arxiv_id: '2505.22881'
source_url: https://arxiv.org/abs/2505.22881
tags:
- cost
- spo-rc
- optimization
- uncertainty
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends contextual stochastic linear optimization to
  problems with uncertain constraints that depend on predictions from a machine learning
  model. The authors introduce Smart Predict-then-Optimize with Robust Constraints
  (SPO-RC), which uses contextual uncertainty sets (constructed via conformal prediction)
  for constraint parameters and introduces a feasibility-sensitive loss function that
  measures decision error.
---

# Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints

## Quick Facts
- arXiv ID: 2505.22881
- Source URL: https://arxiv.org/abs/2505.22881
- Authors: Hyungki Im; Wyame Benslimane; Paul Grigas
- Reference count: 23
- Extends contextual stochastic linear optimization to problems with uncertain constraints dependent on ML predictions

## Executive Summary
This paper introduces Smart Predict-then-Optimize with Robust Constraints (SPO-RC), a framework for contextual stochastic linear optimization where constraint parameters are uncertain and depend on machine learning predictions. The authors propose using contextual uncertainty sets constructed via conformal prediction to handle constraint uncertainty, and introduce a feasibility-sensitive loss function that measures decision error. They develop SPO-RC+ as a convex surrogate loss with proven Fisher consistency to the original SPO-RC problem, and address sample selection bias through truncation and importance reweighting techniques.

## Method Summary
The authors extend the predict-then-optimize framework to handle robust constraints by constructing contextual uncertainty sets using conformal prediction. They introduce a new loss function that measures the suboptimality of decisions made under worst-case constraint parameters within these uncertainty sets. To make optimization tractable, they propose SPO-RC+ as a convex surrogate loss that maintains Fisher consistency with the original non-convex SPO-RC problem. The framework includes truncation to focus on feasible regions and importance reweighting (via Kernel Mean Matching) to correct for sample selection bias when true constraint parameters fall outside the uncertainty sets.

## Key Results
- SPO-RC+ effectively handles constraint uncertainty in contextual stochastic optimization
- Truncation combined with importance reweighting improves performance when true constraint parameters fall outside uncertainty sets
- Models trained with SPO-RC+ outperform MSE loss baselines as cost function complexity increases
- SPO-RC+ framework combines well with solution caching for improved scalability

## Why This Works (Mechanism)
The framework works by explicitly modeling uncertainty in constraint parameters through contextual uncertainty sets, which provides robustness against prediction errors. The feasibility-sensitive loss function directly measures decision quality rather than prediction accuracy, aligning training with the ultimate optimization objective. The convex surrogate SPO-RC+ maintains theoretical guarantees while enabling tractable optimization. Truncation and importance reweighting address the practical challenge of distributional mismatch between training data and the feasible region.

## Foundational Learning

**Contextual Stochastic Optimization**: Sequential decision-making where context influences both predictions and optimization - needed for real-world problems where decisions depend on observed features; quick check: verify context influences both objective and constraints

**Conformal Prediction**: Distribution-free uncertainty quantification method that creates prediction sets with guaranteed coverage - needed to construct uncertainty sets without strong distributional assumptions; quick check: verify marginal coverage holds on calibration set

**Fisher Consistency**: Property where surrogate loss minimization leads to optimal solution of original problem - needed to ensure SPO-RC+ surrogate maintains theoretical guarantees; quick check: verify gradients align at optimum

**Kernel Mean Matching**: Technique for reweighting samples to match distributions - needed to correct for truncation-induced sample selection bias; quick check: verify weights sum to 1 and match target moments

## Architecture Onboarding

**Component Map**: Context -> Feature Extraction -> Uncertainty Set Construction -> Optimization with Robust Constraints -> Decision

**Critical Path**: The core pipeline flows from context through feature extraction to uncertainty set construction, then to robust optimization, and finally to decision making. The surrogate loss SPO-RC+ enables efficient training by approximating the non-convex SPO-RC objective.

**Design Tradeoffs**: The main tradeoff is between robustness (wider uncertainty sets) and conservativeness (potentially suboptimal decisions). Conformal prediction provides data-driven uncertainty quantification but requires calibration data. The surrogate loss SPO-RC+ sacrifices some fidelity to the original problem for computational tractability.

**Failure Signatures**: Poor performance may manifest when true constraint parameters frequently fall outside uncertainty sets (calibration set doesn't represent test distribution), when the convex surrogate SPO-RC+ poorly approximates the original non-convex problem, or when importance reweighting fails to adequately correct for distributional mismatch.

**3 First Experiments**:
1. Test SPO-RC+ performance on synthetic problems with known ground truth to verify Fisher consistency
2. Evaluate sensitivity to uncertainty set size by varying the coverage parameter in conformal prediction
3. Compare importance reweighting methods (KMM vs uniform vs no reweighting) on problems with known selection bias

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on contextual uncertainty sets requires sufficiently large calibration set and reasonably accurate prediction models
- Truncation approach introduces potential bias when true constraint parameters frequently fall outside uncertainty sets
- Importance reweighting via Kernel Mean Matching may be computationally expensive for high-dimensional contexts

## Confidence
- High confidence in theoretical foundations of SPO-RC and SPO-RC+ surrogate loss
- Medium confidence in empirical effectiveness of truncation and importance reweighting
- Medium confidence in scalability claims, as solution caching is presented but not extensively evaluated

## Next Checks
1. Evaluate performance degradation when true constraint parameters fall outside uncertainty sets more frequently than the calibration set suggests
2. Benchmark computational overhead of importance reweighting (KMM) against simpler alternatives like uniform reweighting or no reweighting
3. Test sensitivity to calibration set size and its impact on uncertainty set quality and downstream decision performance