---
ver: rpa2
title: Learning a distance measure from the information-estimation geometry of data
arxiv_id: '2510.02514'
source_url: https://arxiv.org/abs/2510.02514
tags:
- latexit
- distance
- sha1
- base64
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces the Information-Estimation Metric (IEM),\
  \ a new distance function derived from the geometry of a continuous probability\
  \ density over signals. Rooted in the Tweedie\u2013Miyasawa formula, the IEM compares\
  \ denoising error vectors over a range of noise amplitudes, capturing the local\
  \ curvature of the log-density around two signals."
---

# Learning a distance measure from the information-estimation geometry of data

## Quick Facts
- arXiv ID: 2510.02514
- Source URL: https://arxiv.org/abs/2510.02514
- Authors: Guy Ohayon; Pierre-Etienne H. Fiquet; Florentin Guth; Jona Ballé; Eero P. Simoncelli
- Reference count: 40
- One-line primary result: Information-Estimation Metric (IEM) captures perceptual similarity without labeled data

## Executive Summary
This paper introduces the Information-Estimation Metric (IEM), a novel distance function derived from the geometry of continuous probability densities over signals. The IEM compares denoising error vectors across varying noise amplitudes, capturing the local curvature of log-density around two signals. The authors prove that IEM is a valid global metric and derive its local Riemannian approximation, showing it adapts to both local and global data geometry.

For Gaussian priors, the IEM coincides with the Mahalanobis distance, while for more complex distributions it reflects data structure. The authors train an IEM on ImageNet using a learned denoiser and demonstrate that it predicts human perceptual judgments competitively with supervised state-of-the-art metrics, while requiring no labeled data.

## Method Summary
The IEM is derived from the Tweedie–Miyasawa formula, which connects mutual information estimation to denoising performance. The metric compares denoising error vectors over a range of noise amplitudes, effectively measuring the local curvature of the log-density around two signals. This approach provides a theoretically grounded distance measure that adapts to the underlying data geometry. The authors prove the IEM satisfies the properties of a valid global metric and develop a local Riemannian approximation for practical computation. They implement the metric using a learned denoiser trained on ImageNet, demonstrating its ability to capture perceptual similarity without requiring human-labeled data.

## Key Results
- IEM is mathematically proven to be a valid global metric satisfying all required properties
- For Gaussian priors, IEM reduces to Mahalanobis distance; for complex distributions, it reflects data structure
- IEM trained on ImageNet predicts human perceptual judgments competitively with supervised state-of-the-art metrics

## Why This Works (Mechanism)
The IEM works by leveraging the relationship between information geometry and denoising performance. The Tweedie–Miyasawa formula establishes that the difficulty of denoising a signal at different noise levels encodes information about the underlying probability density's geometry. By comparing denoising error vectors across noise amplitudes for two signals, the IEM captures how similar their local density curvatures are. This approach is powerful because it doesn't require labeled data or assumptions about specific perceptual features, instead deriving similarity directly from the statistical structure of the data itself. The metric naturally adapts to both local geometry (through the denoising error vectors) and global structure (through the integration over noise levels), providing a comprehensive measure of similarity that aligns with human perception.

## Foundational Learning

Mutual Information Estimation
- Why needed: The Tweedie–Miyasawa formula links denoising performance to mutual information, which is the theoretical foundation of IEM
- Quick check: Verify understanding of how denoising MSE relates to MI estimation through the formula I(X;Y) = -½ E[log p_Y|X(y|x)]

Riemannian Geometry
- Why needed: IEM's local approximation uses Riemannian geometry to capture local data structure
- Quick check: Understand how the Fisher information matrix defines a Riemannian metric on the statistical manifold

Tweedie–Miyasawa Formula
- Why needed: This formula is the mathematical bridge between denoising and information geometry that enables IEM
- Quick check: Be able to derive the connection between denoising error and log-density curvature

## Architecture Onboarding

Component Map: Image -> Denoiser -> Noise Vector Generator -> IEM Calculator -> Distance Score

Critical Path: The core computation involves generating denoising error vectors at multiple noise levels for two input signals, then computing the IEM from these vectors. The denoiser must be trained to handle the specific noise levels used in the metric calculation.

Design Tradeoffs: The choice of noise levels and range significantly impacts the metric's sensitivity to different scales of similarity. Too few noise levels may miss important geometric features, while too many increase computational cost without proportional benefit.

Failure Signatures: Poor denoiser performance at any noise level can corrupt the entire metric calculation. The metric may be insensitive to features that don't significantly affect denoising error across the noise spectrum.

First Experiments:
1. Verify IEM reduces to Mahalanobis distance for Gaussian data with known covariance
2. Test IEM on synthetic distributions with known geometric properties
3. Compare IEM predictions with human similarity judgments on simple synthetic images

## Open Questions the Paper Calls Out
None

## Limitations
- Empirical performance across diverse data domains remains to be fully validated beyond ImageNet
- The assumption that denoising error vectors adequately capture perceptual similarity may not hold for all data types
- Generalization to non-image domains and different perceptual tasks requires further investigation

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Theoretical validity of IEM as a metric | High |
| IEM's adaptability to data geometry | Medium |
| Competitive performance with supervised metrics | Medium |
| No labeled data requirement | High |

## Next Checks

1. Test IEM on multiple diverse datasets beyond ImageNet, including non-image data, to assess generalizability and identify potential domain-specific limitations.

2. Conduct ablation studies to determine the sensitivity of IEM to different denoising architectures and noise levels, establishing robust parameter ranges.

3. Perform head-to-head comparisons with both supervised and unsupervised perceptual metrics on a standardized benchmark suite to quantify relative performance across multiple tasks and data types.