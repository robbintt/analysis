---
ver: rpa2
title: 'Sparse Autoencoders for Sequential Recommendation Models: Interpretation and
  Flexible Control'
arxiv_id: '2507.12202'
source_url: https://arxiv.org/abs/2507.12202
tags:
- features
- feature
- genre
- recommendations
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends sparse autoencoders (SAE) to sequential recommendation
  models. The authors train a transformer-based recommendation model and then apply
  SAE to its hidden states, showing that the learned features are more interpretable
  and monosemantic than the original dimensions.
---

# Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control

## Quick Facts
- arXiv ID: 2507.12202
- Source URL: https://arxiv.org/abs/2507.12202
- Reference count: 40
- Authors: Anton Klenitskiy; Konstantin Polev; Daria Denisova; Alexey Vasilev; Dmitry Simakov; Gleb Gusev
- One-line primary result: SAE features in recommendation transformers align better with genres (mean correlation 0.44 Movielens-20m, 0.32 Music4all) than original neurons (0.20, 0.16), enabling flexible inference-time control of recommendations.

## Executive Summary
This paper introduces sparse autoencoders (SAE) to sequential recommendation models, demonstrating that SAE-extracted features are more interpretable and monosemantic than original transformer neurons. The authors show these features can be used to flexibly control model behavior by adjusting feature activations during inference, allowing targeted manipulation of recommendation attributes like genres while maintaining reasonable recommendation quality.

## Method Summary
The authors train a transformer-based recommendation model (GPTRec) on user-item interaction sequences from Movielens-20m and Music4all datasets. They extract activations from the first transformer block, normalize them, and train an SAE with L1 regularization to decompose these activations into interpretable features. Interpretability is evaluated by correlating SAE feature activations with item attributes (genres), while control capability is demonstrated by manipulating feature values during inference to influence the genre distribution of recommendations.

## Key Results
- SAE features show significantly higher correlation with item genres (mean correlation 0.44 for Movielens-20m, 0.32 for Music4all) compared to original transformer neurons (0.20, 0.16)
- Inference-time steering of SAE features enables flexible control over recommendation attributes, with genre proportions changing from 4.2% to 19.4% under maximum intervention
- The control mechanism maintains reasonable recommendation quality, with NDCG@10 dropping from 0.65 to 0.47 under maximum steering

## Why This Works (Mechanism)
The paper leverages the fact that recommendation transformers operate in high-dimensional spaces where information is distributed across many dimensions (superposition). SAEs decompose these activations into a sparse set of interpretable features by learning a dictionary that maps activations to a lower-dimensional space with L1 regularization encouraging sparsity. This decomposition reveals monosemantic features that correspond to specific recommendation attributes, making the model's internal representations more transparent and controllable.

## Foundational Learning
- **Sparse autoencoders**: Neural networks that decompose activations into a sparse set of features using L1 regularization on the code layer. Needed to extract interpretable features from high-dimensional transformer activations; quick check: reconstruction error should be low while maintaining sparsity.
- **Superposition in neural networks**: The phenomenon where models use more features than available dimensions, distributing information across multiple dimensions. Needed to understand why original neurons are uninterpretable; quick check: activation vectors are dense and dimensions correlate with multiple attributes.
- **Inference-time steering**: Modifying model activations during inference to influence output behavior. Needed to demonstrate practical control over recommendations; quick check: output distribution changes predictably with feature manipulation.

## Architecture Onboarding

### Component Map
Data -> GPTRec (3L, 2H, HS64) -> First transformer block activations -> SAE (2048D, L1=0.1) -> Interpretable features -> Correlation analysis/control experiments

### Critical Path
Model training → Activation extraction → SAE training → Interpretability evaluation → Control experiments

### Design Tradeoffs
- Dictionary size 2048 vs. smaller for interpretability vs. reconstruction quality
- L1 weight 0.1 vs. lower for better reconstruction vs. less sparsity
- Feature normalization vs. preserving original scale for correlation analysis

### Failure Signatures
- Poor reconstruction (RMSE >0.5) indicates SAE doesn't capture model behavior
- Low correlations (<0.3) suggest features aren't interpretable
- Steering with no effect indicates reconstruction doesn't causally influence predictions

### First Experiments
1. Train GPTRec on Movielens-20m sequences, extract first block activations
2. Train SAE with varying L1 weights, monitor reconstruction and sparsity
3. Compute genre-feature correlations, identify top-correlated features for control

## Open Questions the Paper Calls Out
- How do alternative SAE architectures like JumpReLU or TopK compare to standard ReLU formulation in extracting interpretable features?
- Can other mechanistic interpretability techniques like circuit discovery or causal scrubbing be adapted to recommendation transformers?
- Do SAE interpretability and steering capabilities scale to industrial-sized recommendation models?
- Can the trade-off between steering strength and recommendation accuracy be mitigated?

## Limitations
- Only evaluates SAE interpretability on genre attributes, may not generalize to other item characteristics
- Uses relatively small model architecture (3 layers, 64 hidden size), raising scalability questions
- Focuses on explicit feedback datasets, results may not extend to implicit feedback domains

## Confidence
- High confidence: Technical methodology for applying SAE to recommendation models is sound
- Medium confidence: Interpretability results show SAE features correlate better with genres than original neurons
- Low confidence: Practical utility claims for steering capability in real-world systems are speculative

## Next Checks
1. Test multi-attribute steering by simultaneously controlling genre and other attributes like release decade
2. Apply framework to non-genre attributes (directors, actors, artists) to verify cross-attribute generalization
3. Repeat experiments with larger transformer architecture (6+ layers, 4+ heads) to assess scalability