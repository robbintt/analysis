---
ver: rpa2
title: Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned
  Large Language Models via Automated Adversarial Prompting
arxiv_id: '2505.17160'
source_url: https://arxiv.org/abs/2505.17160
tags:
- harry
- potter
- query
- completion
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LURK, a novel adversarial probing framework
  that evaluates unlearning robustness in language models by automatically generating
  prompt suffixes designed to extract hidden Harry Potter-related knowledge from supposedly
  unlearned models. LURK leverages Greedy Coordinate Gradient optimization with a
  commercial LLM-based checker to identify idiosyncratic references, revealing that
  even models deemed successfully unlearned can leak significant domain-specific knowledge
  under targeted adversarial conditions.
---

# Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting

## Quick Facts
- **arXiv ID**: 2505.17160
- **Source URL**: https://arxiv.org/abs/2505.17160
- **Reference count**: 40
- **Key outcome**: LURK framework exposes significant knowledge leakage in supposedly unlearned LLMs, with TV unlearning most vulnerable (up to 79.68% leakage on LLaMA2-13B)

## Executive Summary
This paper introduces LURK, a novel adversarial probing framework that evaluates unlearning robustness in language models by automatically generating prompt suffixes designed to extract hidden Harry Potter-related knowledge from supposedly unlearned models. LURK leverages Greedy Coordinate Gradient optimization with a commercial LLM-based checker to identify idiosyncratic references, revealing that even models deemed successfully unlearned can leak significant domain-specific knowledge under targeted adversarial conditions. Experiments on four unlearning algorithms across multiple model sizes show that larger models retain more knowledge and that successful pre-probing unlearning often masks rather than eliminates latent knowledge.

## Method Summary
The LURK framework combines Greedy Coordinate Gradient (GCG) optimization with an LLM-based checker to automatically generate adversarial prompt suffixes that elicit hidden knowledge from unlearned models. The method takes an evaluation prompt and optimizes a suffix to maximize the probability of extracting domain-specific knowledge while being checked by a commercial LLM. The framework was tested on four unlearning algorithms (WHP, TV, GA, NPO) across OPT-2.7B, LLaMA 2-7B/13B, and LLaMA 3.1-8B models using a 3.1M token Harry Potter unlearning dataset and 250 evaluation prompts. Hyperparameters included batch size B=24, k=12, 200 iterations, with reinforced fine-tuning at lr=3e-6 for 10 epochs before unlearning at lr=1e-6.

## Key Results
- TV unlearning showed the highest post-probing leakage rate at 79.68% on LLaMA2-13B
- Larger models retained more knowledge, with LLaMA 2-13B showing significantly higher leakage than smaller variants
- Successful pre-probing unlearning masked rather than eliminated latent knowledge in most cases
- WHP and NPO showed better resistance to post-probing leakage compared to TV and GA

## Why This Works (Mechanism)
The LURK framework works by exploiting the fundamental challenge of selective forgetting in neural networks. When models are unlearned, they don't truly forget information but rather learn to suppress its expression under normal conditions. The GCG optimization systematically explores the prompt space to find conditions where these suppression mechanisms fail, revealing latent knowledge through carefully crafted suffix prompts that bypass learned avoidance behaviors.

## Foundational Learning

**Unlearning fundamentals** - Understanding that unlearning doesn't erase knowledge but suppresses its expression is crucial. Models retain compressed representations that can be triggered under specific conditions.

*Why needed*: Without this foundation, one might incorrectly assume unlearning creates a clean slate rather than a suppressed state.

*Quick check*: Verify that unlearned models still show non-zero perplexity on target domain text.

**Adversarial prompting** - The technique of crafting prompts to elicit unintended model behaviors by exploring optimization of input space rather than model parameters.

*Why needed*: Standard evaluation prompts may not stress-test unlearning effectiveness; adversarial approaches reveal hidden vulnerabilities.

*Quick check*: Test that carefully crafted prompts can elicit knowledge that standard prompts cannot.

**Greedy Coordinate Gradient optimization** - An iterative optimization method that modifies input coordinates (prompt tokens) to minimize a target loss function.

*Why needed*: This allows systematic exploration of the prompt space to find suffixes that maximize knowledge extraction while maintaining prompt coherence.

*Quick check*: Confirm that GCG optimization converges to suffixes that consistently improve knowledge extraction metrics.

## Architecture Onboarding

**Component map**: Evaluation prompts → GCG optimization → Adversarial suffixes → Model completion → LLM checker → Leakage score

**Critical path**: The evaluation prompt passes through GCG optimization to generate an adversarial suffix, which is appended and evaluated by the target model. The completion is then checked by the LLM-based checker to determine if domain-specific knowledge was successfully extracted.

**Design tradeoffs**: The framework trades computational cost (200 optimization iterations per prompt) for thorough exploration of the prompt space. The hybrid checker approach (GPT-4o-mini + o3-mini) balances accuracy with computational efficiency but introduces latency and potential inter-checker bias.

**Failure signatures**: 
- Catastrophic model collapse (∞ perplexity) indicates unlearning has destroyed general capabilities
- Low leakage rates might indicate either successful unlearning or insufficient optimization
- False positives in the checker suggest the need for manual validation of ambiguous completions

**First experiments**:
1. Run LURK on a baseline model (not unlearned) to establish upper bounds for leakage rates
2. Test the checker's precision by having it evaluate completions with known domain-specific content
3. Validate that standard evaluation prompts cannot elicit the same level of leakage as LURK-generated prompts

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Results are limited to the Harry Potter domain and may not generalize to other knowledge types
- The LLM-based checker may misclassify generic language as domain-specific leakage
- The paper does not explore whether iterative probing could further reduce residual knowledge

## Confidence

**High confidence**: Experimental methodology is well-specified, hyperparameters are reproducible, and results on the Harry Potter domain are internally consistent.

**Medium confidence**: Claims about "idiosyncratic" references being the primary leakage vector are plausible but not exhaustively validated.

**Low confidence**: Extrapolation of findings to non-literary or non-narrative domains (e.g., medical, legal) is speculative and untested.

## Next Checks

1. Apply LURK to a non-narrative domain (e.g., scientific facts or legal statutes) and compare pre- vs. post-probing leakage rates to test external validity.

2. Manually annotate a stratified sample of LURK-generated completions to quantify false positives/negatives in the hybrid checker and assess sensitivity to generic language.

3. Run a small-scale experiment where unlearned models are re-unlearned after LURK probing to determine whether leakage is persistent or can be reduced through repeated unlearning cycles.