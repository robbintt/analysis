---
ver: rpa2
title: 'RKEFino1: A Regulation Knowledge-Enhanced Large Language Model'
arxiv_id: '2506.05700'
source_url: https://arxiv.org/abs/2506.05700
tags:
- financial
- xbrl
- task
- fino1
- rkefino1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces RKEFino1, a regulation knowledge-enhanced
  financial reasoning model built upon Fino1. The authors fine-tune Fino1 with domain-specific
  knowledge from XBRL, CDM, and MOF to address accuracy and compliance challenges
  in Digital Regulatory Reporting.
---

# RKEFino1: A Regulation Knowledge-Enhanced Large Language Model

## Quick Facts
- **arXiv ID:** 2506.05700
- **Source URL:** https://arxiv.org/abs/2506.05700
- **Reference count:** 15
- **Primary result:** RKEFino1, a fine-tuned financial LLM, achieves 42.58% FactScore on CDM-QA vs 36.76% baseline, 62.58% accuracy on MOF approval vs 0% baseline, and 26.62% F1 on Numerical NER vs 14.99% baseline.

## Executive Summary
RKEFino1 is a regulation knowledge-enhanced financial reasoning model built by fine-tuning Fino1 with domain-specific knowledge from XBRL, CDM, and MOF standards. The model addresses accuracy and compliance challenges in Digital Regulatory Reporting through supervised fine-tuning with LoRA adapters. Evaluated on three tasks—knowledge-based QA, mathematical reasoning QA, and Numerical NER—RKEFino1 demonstrates significant improvements over the baseline across all metrics, particularly in compliance accuracy and factual reasoning.

## Method Summary
The authors fine-tune Fino1 (LLaMA-3.1-8B-Instruct based) using LoRA (r=64, α=128, dropout=0.05) on all linear modules with int4 quantization. Training uses 9,898 samples from CDM documentation (478), OSI website for MOF (258), SEC website + XBRL Terminology dataset for XBRL knowledge (8,052) and mathematical reasoning (1,110). The model is evaluated on FinNLP-FNP-LLMFinLegal-2025 shared task data using FactScore for knowledge QA, Accuracy for structured outputs, and seqeval F1 for NER. Training runs for 10 epochs with bf16 mixed-precision on 4× NVIDIA H100 GPUs.

## Key Results
- RKEFino1 achieves 42.58% FactScore on CDM-QA task, outperforming baseline Fino1 at 36.76%
- Model reaches 62.58% accuracy on MOF Approval task, while baseline achieves 0.00%
- Numerical NER performance improves to 26.62% F1-score from baseline 14.99%
- Demonstrates enhanced interpretability, compliance accuracy, and generalization in financial regulatory tasks

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Knowledge Injection via Supervised Fine-Tuning
Fine-tuning a general financial model on curated regulatory datasets (XBRL, CDM, MOF) improves factual accuracy and compliance reasoning. The model updates weights via supervised learning to associate specific regulatory terminologies and logic with correct outputs, reducing hallucinations in niche domains. Core assumption: Fino1 has sufficient reasoning capacity and performance gaps are due to lack of domain knowledge. Break condition: If training data contains contradictory information, performance may degrade via catastrophic forgetting.

### Mechanism 2: Parameter-Efficient Fine-Tuning (PEFT) for Reasoning Adaptation
Applying LoRA (Low-Rank Adaptation) allows efficient learning of regulatory nuances while preserving base model capabilities. By freezing base weights and injecting trainable rank-decomposition matrices, the model adapts hidden states to the regulatory domain without full parameter updates. Core assumption: Regulatory knowledge can be captured in low-dimensional subspace (rank r=64). Break condition: If domain shift is too large for low-rank updates, the model fails to converge on MOF Approval or CDM-QA tasks.

### Mechanism 3: Distinction Between Knowledge Retrieval and Mathematical Reasoning
Separating evaluation into "Knowledge-based QA" and "Mathematical Reasoning QA" enables targeted improvements in compliance logic versus calculation. The model learns to disambiguate between tasks requiring factual retrieval and those requiring multi-step calculation, guided by prompt structure and verifier logic. Core assumption: Model can identify task type from input context and switch modes without explicit routing labels. Break condition: If a query requires both knowledge retrieval and calculation, the model may conflate processes, leading to calculation errors on correct facts.

## Foundational Learning

- **Concept: XBRL (eXtensible Business Reporting Language)**
  - Why needed here: Primary data structure for "Numerical NER" and "Math Reasoning" tasks. Understanding XBRL tags (e.g., `us-gaap`) define semantic meaning of financial numbers is critical for interpreting model's NER outputs.
  - Quick check question: Can you distinguish between a "Monetary Item Type" and a "Per Share Item Type" in a financial table?

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: Paper utilizes LoRA (r=64, α=128) for training. Understanding matrix decomposition is necessary to debug convergence issues or decide if higher rank is needed.
  - Quick check question: Explain how LoRA reduces memory usage during fine-tuning compared to full fine-tuning.

- **Concept: FactScore vs. Accuracy**
  - Why needed here: Paper uses different metrics for different tasks (Accuracy for MOF Approval, FactScore for CDM-QA). Must understand FactScore measures atomic fact correctness in generation, whereas Accuracy measures exact matches.
  - Quick check question: Why would exact match accuracy be a poor metric for explaining complex XBRL terminology?

## Architecture Onboarding

- **Component map:** Base Model (Fino1) -> Adapter Layer (LoRA modules on linear layers) -> Input Processor (handles 8192 tokens, formats questions) -> Output Head (generates text for QA or tags for NER)

- **Critical path:**
  1. Data Formatting: Convert XBRL/CDM/MOF documents into defined QA formats
  2. Tokenization: Process with 4096 block size
  3. Forward Pass: Base model + LoRA adapters
  4. Verification: (Inherited from Fino1) Use verifier-guided logic to refine outputs

- **Design tradeoffs:**
  - Base Model Choice: Using Fino1 (8B) enables efficient deployment on single GPUs but may limit complex reasoning depth compared to 70B models
  - Quantization: int4 reduces memory footprint but may theoretically impact precision on numerical tasks
  - Context Window: 8192 tokens handles long documents but may struggle with extensive regulatory filings without chunking

- **Failure signatures:**
  - Hallucination on Tags: Baseline Fino1 had 0% accuracy on XBRL tags; failure mode is generating plausible-looking but non-existent tags
  - MOF Abbreviation Errors: Low accuracy (12.23%) suggests struggles with specific short-form mappings
  - NER Type Confusion: Misclassifying "Monetary" vs. "Shares" types in tables

- **First 3 experiments:**
  1. Baseline Reproduction: Load Fino1, run inference on MOF Approval task to verify reported 0% accuracy baseline
  2. LoRA Ablation: Train RKEFino1 with LoRA rank r=16 vs r=64 to measure sensitivity of compliance knowledge to adapter capacity
  3. NER Boundary Test: Evaluate Numerical NER capability on purely tabular data vs sentence data to determine if model generalizes equally across both modalities

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent will expanding the training dataset specifically for MOF abbreviations and XBRL tags improve model's accuracy in these low-performing areas?
- **Basis in paper:** Authors state, "In future work, we plan to expand the dataset for MOF abbreviation and XBRL tag to further improve the model's performance on this task."
- **Why unresolved:** While data expansion is proposed, current results (e.g., 12.23% on MOF Abbreviation) don't demonstrate if data volume is sole bottleneck or if task requires architectural changes.
- **What evidence would resolve it:** Ablation studies showing performance curves relative to increasing dataset sizes for MOF and XBRL tasks.

### Open Question 2
- **Question:** How does RKEFino1 compare against other state-of-the-art financial LLMs or larger proprietary models on defined Digital Regulatory Reporting tasks?
- **Basis in paper:** Experimental section restricts comparisons to base Fino1 model, despite existence of other financial models like BloombergGPT and FinGPT mentioned in literature review.
- **Why unresolved:** Without external baselines, unclear if results represent state-of-the-art achievement for financial LLMs or simply improvement over specific base model used.
- **What evidence would resolve it:** Benchmark results comparing RKEFino1 against wider array of general and financial-specific LLMs on CDM, MOF, and XBRL datasets.

### Open Question 3
- **Question:** Can model effectively generalize to regulatory updates or completely unseen regulatory frameworks without requiring immediate re-fine-tuning?
- **Basis in paper:** Paper focuses on static domain knowledge injection from current CDM, MOF, and XBRL documentation, but regulations frequently change.
- **Why unresolved:** Evaluation uses specific test sets from same domains as training data, leaving model's robustness to temporal drift or out-of-domain regulatory structures untested.
- **What evidence would resolve it:** Evaluation results on "future" regulatory documents released after model's training cutoff or zero-shot testing on different financial regulatory standards.

## Limitations

- Evaluation relies on single shared task dataset from FinNLP-FNP-LLMFinLegal-2025, which may not fully represent real-world regulatory scenarios
- 9,898 training samples represent relatively small dataset that may not capture edge cases in regulatory interpretation
- Paper does not address potential bias in training data sources or test performance on unseen regulatory frameworks
- int4 quantization could theoretically impact numerical precision in financial calculations, though not empirically validated

## Confidence

- **High Confidence:** Core claim that RKEFino1 outperforms Fino1 on three evaluation tasks (FactScore: 42.58% vs 36.76%, Accuracy: 62.58% vs 0.00%, F1-score: 26.62% vs 14.99%)
- **Medium Confidence:** Domain-specific knowledge injection is primary driver of performance improvement, though ablation studies not conducted
- **Low Confidence:** Assertion that RKEFino1 will generalize effectively to broader regulatory domains beyond CDM/MOF/XBRL contexts tested

## Next Checks

1. **Cross-Domain Generalization Test:** Evaluate RKEFino1 on regulatory datasets from different financial domains (e.g., Basel III compliance, MiFID II requirements) to assess knowledge enhancement transfer

2. **Adversarial Query Analysis:** Design battery of adversarial queries testing behavior on edge cases, ambiguous regulatory language, and potential contradictions in training data

3. **Numerical Precision Validation:** Conduct controlled experiments comparing int4-quantized RKEFino1 against float16/full precision models on complex mathematical reasoning tasks to quantify degradation in numerical accuracy for multi-step calculations