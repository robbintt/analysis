---
ver: rpa2
title: Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a
  Multi-Agent Approach
arxiv_id: '2506.06175'
source_url: https://arxiv.org/abs/2506.06175
tags:
- chart
- code
- agentic
- data
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the persistent execution error rate in text-to-chart
  generation, where 15% of generated Python scripts still fail to run despite fine-tuning
  and RL. To tackle this, the authors propose a lightweight multi-agent pipeline using
  GPT-4o-mini, with separate Drafting, Execution, and Re-writer agents.
---

# Does It Run and Is That Enough? Revisiting Text-to-Chart Generation with a Multi-Agent Approach

## Quick Facts
- **arXiv ID**: 2506.06175
- **Source URL**: https://arxiv.org/abs/2506.06175
- **Authors**: James Ford; Anthony Rios
- **Reference count**: 16
- **Primary result**: Multi-agent pipeline reduces execution errors from 15% to 4.5% on Text2Chart31, outperforming fine-tuned baselines by nearly 5 percentage points

## Executive Summary
This study tackles the persistent problem of execution errors in text-to-chart generation systems, where 15% of generated Python scripts fail to run despite fine-tuning and reinforcement learning. The authors propose a lightweight multi-agent pipeline using GPT-4o-mini with separate Drafting, Execution, and Re-writer agents. On the Text2Chart31 benchmark, the agentic system reduces execution errors from 15% to 4.5%, outperforming the strongest fine-tuned baseline by nearly 5 percentage points, and achieves 4.6% on ChartX, demonstrating strong generalization. Manual review of 100 sampled charts shows 83% are accurate, with most remaining issues related to style rather than data. However, only 33.3% (Text2Chart31) and 7.2% (ChartX) of charts meet basic colorblindness guidelines, highlighting a need for improved accessibility.

## Method Summary
The authors introduce a lightweight multi-agent pipeline architecture using GPT-4o-mini to address execution failures in text-to-chart generation. The system consists of three specialized agents: a Drafting Agent that generates initial Python code, an Execution Agent that runs the code and captures errors, and a Re-writer Agent that iteratively fixes identified issues. This approach leverages the reasoning capabilities of large language models to handle the iterative nature of debugging chart generation code. The pipeline is evaluated on two benchmarks - Text2Chart31 and ChartX - and compared against fine-tuned and RL-based baselines. The method focuses on execution reliability as a foundational requirement before addressing higher-level chart quality concerns.

## Key Results
- Execution error rate reduced from 15% to 4.5% on Text2Chart31 benchmark
- Outperforms strongest fine-tuned baseline by nearly 5 percentage points
- Achieves 4.6% execution error rate on ChartX, demonstrating strong generalization across datasets

## Why This Works (Mechanism)
The multi-agent approach succeeds by decomposing the complex task of chart generation into specialized subtasks that can be handled iteratively. The Drafting Agent focuses solely on initial code generation without worrying about execution details, while the Execution Agent provides concrete feedback about what fails. The Re-writer Agent then uses this specific error information to make targeted corrections. This separation of concerns allows each agent to specialize in its role, and the iterative feedback loop enables progressive refinement of the code. By using GPT-4o-mini rather than larger models, the system maintains efficiency while leveraging the model's reasoning capabilities for debugging tasks.

## Foundational Learning
- **Text-to-chart generation**: Converting natural language descriptions into executable visualization code - needed because users need intuitive ways to create charts without programming expertise; quick check: can the system handle diverse chart types and data structures from text prompts
- **Execution error handling**: Detecting and fixing code that fails to run - critical because even syntactically correct code can fail due to runtime issues; quick check: what percentage of generated scripts execute successfully
- **Multi-agent orchestration**: Coordinating specialized agents for different aspects of a task - enables decomposition of complex problems into manageable subtasks; quick check: does each agent have a clear, distinct role in the pipeline
- **Benchmark evaluation**: Using standardized datasets like Text2Chart31 and ChartX to measure performance - provides objective comparison across methods; quick check: are evaluation metrics comprehensive and reproducible
- **Accessibility standards**: Ensuring visualizations meet colorblindness and other accessibility guidelines - increasingly important for inclusive design; quick check: what percentage of generated charts meet basic accessibility requirements

## Architecture Onboarding

**Component Map**: User Prompt -> Drafting Agent -> Execution Agent -> Re-writer Agent -> Final Chart

**Critical Path**: The execution feedback loop between Execution Agent and Re-writer Agent is the critical path, as iterative debugging directly determines success rate. Each cycle provides specific error information that guides the next rewrite attempt.

**Design Tradeoffs**: The system prioritizes execution reliability over visual sophistication, using a lightweight GPT-4o-mini approach rather than larger models to maintain efficiency. The three-agent separation trades some coordination complexity for specialized performance in each subtask.

**Failure Signatures**: 
- **Drafting failures**: Produce code that doesn't match prompt intent or has structural issues
- **Execution failures**: Code runs but produces errors due to data issues, library conflicts, or syntax problems
- **Rewriting failures**: Agent fails to correctly interpret error messages or apply appropriate fixes

**3 First Experiments**:
1. Test the pipeline on simple chart requests to establish baseline execution success rate
2. Evaluate performance on complex, multi-series chart specifications to assess handling of sophisticated requirements
3. Measure iteration count needed to fix various types of errors to understand debugging efficiency

## Open Questions the Paper Calls Out
The paper explicitly identifies accessibility as a major gap, with only 33.3% of charts on Text2Chart31 and 7.2% on ChartX meeting basic colorblindness guidelines. The authors call for future work to address chart aesthetics, semantic fidelity, and accessibility improvements beyond mere execution reliability.

## Limitations
- Only 33.3% of charts on Text2Chart31 and 7.2% on ChartX meet basic colorblindness guidelines, indicating poor accessibility performance
- Manual review covered only 100 charts and focused primarily on data accuracy rather than comprehensive visual quality
- The evaluation datasets may not fully represent real-world diversity and complexity of chart generation requests

## Confidence

| Claim | Confidence |
|-------|------------|
| Execution error reduction from 15% to 4.5% on Text2Chart31 | High |
| 83% accuracy based on manual review of 100 charts | Medium |
| Accessibility compliance at 33.3% (Text2Chart31) and 7.2% (ChartX) | Low |

## Next Checks
1. Expand accessibility testing to evaluate compliance with comprehensive accessibility guidelines including screen reader compatibility and high contrast ratios
2. Conduct real-world deployment study using diverse chart generation requests from various domains to assess robustness and generalization
3. Perform long-term maintenance evaluation to test system performance over time with updates to data sources and libraries