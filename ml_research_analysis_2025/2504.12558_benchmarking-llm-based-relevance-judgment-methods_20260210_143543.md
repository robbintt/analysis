---
ver: rpa2
title: Benchmarking LLM-based Relevance Judgment Methods
arxiv_id: '2504.12558'
source_url: https://arxiv.org/abs/2504.12558
tags:
- relevance
- human
- methods
- judgments
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Benchmarking LLM-based Relevance Judgment Methods

## Quick Facts
- **arXiv ID:** 2504.12558
- **Source URL:** https://arxiv.org/abs/2504.12558
- **Authors:** Negar Arabzadeh; Charles L. A. Clarke
- **Reference count:** 40
- **Primary result:** No single LLM-based relevance judgment method consistently optimizes both human alignment and system ranking agreement

## Executive Summary
This paper benchmarks five LLM-based relevance assessment methods (Binary, Graded/UMBRELA, Pairwise Preference, Exam, AutoNuggetizer) on their ability to evaluate retrieval systems effectively. The study uses TREC Deep Learning Track datasets and the ANTIQUE dataset to measure two key dimensions: alignment with human judgments and agreement with system rankings derived from human nDCG@10. Results show that while pairwise preference judgments achieve the highest alignment with human labels, graded relevance methods like UMBRELA and Exam provide better system ranking correlations. This creates a fundamental tension between methods that prioritize human judgment fidelity versus those that optimize for system-level evaluation quality.

## Method Summary
The study compares five LLM-based relevance judgment methods using GPT-4o and Llama 3.2B (temperature=0) across TREC DL (2019-2021) and ANTIQUE datasets. Alignment is measured as percentage agreement between LLM judgments and human labels, categorized into Best/Acceptable/Unacceptable. System Agreement uses Kendall's τ correlation between system rankings derived from LLM judgments (using Compatibility metric with RBO p=0.9) and human-based nDCG@10 rankings. The Pairwise method uses custom prompts with sampling parameter P=7, while AutoNuggetizer requires pooling relevant documents from the dataset. Evaluation focuses on both per-document alignment and system-level ranking quality.

## Key Results
- Pairwise preference judgments achieve highest human alignment but lowest system ranking agreement
- Graded methods (UMBRELA, Exam) provide superior system ranking correlations despite lower human alignment
- No single method optimizes both alignment and system agreement simultaneously
- LLMs are more lenient than humans, labeling ~13% more documents as relevant

## Why This Works (Mechanism)
The benchmarking approach works by creating a controlled comparison framework where each method's strengths and weaknesses can be quantified across identical datasets. By using both human judgment alignment and system ranking agreement as metrics, the study captures the dual nature of relevance assessment - both the quality of individual judgments and their utility for system evaluation. The Compatibility metric with RBO p=0.9 provides a principled way to compare system rankings without requiring full relevance judgments for all documents.

## Foundational Learning
- **RBO (Rank-Biased Overlap):** A similarity measure for rankings that accounts for depth and position, needed for the Compatibility metric to compare system rankings; quick check: verify RBO implementation uses p=0.9 exactly
- **Kendall's τ correlation:** Measures rank correlation between two orderings, used to assess system agreement; quick check: ensure τ calculations match expected ranges for the dataset sizes
- **Borda count aggregation:** Used in pairwise methods to combine multiple pairwise comparisons into overall scores; quick check: verify Borda score calculations match theoretical expectations
- **nDCG@10:** Normalized Discounted Cumulative Gain at cutoff 10, the gold standard for system evaluation; quick check: confirm nDCG@10 values match TREC official scores
- **Compatibility metric:** Custom measure using RBO to compare LLM-generated rankings against human-based ideal permutations; quick check: validate that Compatibility scores correlate appropriately with nDCG@10
- **Temperature=0 setting:** Ensures deterministic LLM outputs for reproducibility; quick check: verify all generations use identical temperature settings

## Architecture Onboarding
**Component Map:** Datasets (TREC DL + ANTIQUE) -> LLM Judgment Generation (5 methods) -> Alignment Evaluation (vs human qrels) -> System Agreement (Kendall's τ + Compatibility)

**Critical Path:** LLM judgment generation → Compatibility metric calculation → Kendall's τ correlation → final comparative results

**Design Tradeoffs:** The study prioritizes methodological rigor over computational efficiency by using multiple datasets and comprehensive evaluation metrics, but this requires significant computational resources for LLM API calls.

**Failure Signatures:** 
- Low Alignment: LLM judgments systematically differ from human labels in predictable ways (e.g., overly lenient)
- Poor System Agreement: LLM rankings poorly correlate with human-based nDCG rankings despite good per-document judgments
- Metric Mismatch: RBO p parameter incorrectly set, leading to invalid Compatibility scores

**First Experiments:**
1. Generate LLM judgments for a small subset of documents and verify Alignment metrics match paper's reported distributions
2. Calculate Compatibility scores for a single method to confirm RBO p=0.9 implementation
3. Run Kendall's τ correlation on a small system set to validate System Agreement methodology

## Open Questions the Paper Calls Out
- **Hybrid human-LLM pipeline:** Can human auditing of LLM-generated graded labels (e.g., UMBRELA) that conflict with pairwise preferences improve reliability? The authors suggest this could combine high system agreement with high human alignment.
- **Direct preference comparison:** To what extent do LLM-generated pairwise preferences align with direct human pairwise preference data rather than inferred preferences from graded labels?
- **Fundamental paradigm limitation:** Is the divergence between high human alignment (pairwise) and high system ranking agreement (graded) an inherent limitation or a solvable artifact of current prompt engineering?

## Limitations
- Pairwise Preference prompt is only partially specified, requiring external reference to Clarke et al. [29]
- AutoNuggetizer initialization depends on unspecified pooling strategy for relevant documents
- Results may be sensitive to LLM API versions and parameter settings

## Confidence
- **Alignment metric calculations:** High confidence (straightforward percentage agreement)
- **System Agreement methodology:** Medium confidence (RBO with p=0.9 is specified but implementation details matter)
- **Overall comparative conclusions:** Medium confidence (results may shift with prompt/API variations)

## Next Checks
1. Verify RBO implementation uses p=0.9 parameter exactly as specified for Compatibility metric
2. Check Pairwise Preference prompt against Clarke et al. [29] to ensure complete implementation
3. Compare generated label distributions against paper statistics (LLMs label ~13% more non-relevant than humans) to detect API drift