---
ver: rpa2
title: What do vision-language models see in the context? Investigating multimodal
  in-context learning
arxiv_id: '2510.24331'
source_url: https://arxiv.org/abs/2510.24331
tags:
- attention
- visual
- vlms
- image
- idefics2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates in-context learning (ICL) capabilities
  in vision-language models (VLMs) by evaluating seven models across four architectures
  on image captioning tasks. The authors systematically analyze how prompt design,
  model architecture, and training strategies impact multimodal ICL.
---

# What do vision-language models see in the context? Investigating multimodal in-context learning

## Quick Facts
- arXiv ID: 2510.24331
- Source URL: https://arxiv.org/abs/2510.24331
- Authors: Gabriel O. dos Santos; Esther Colombini; Sandra Avila
- Reference count: 13
- Key outcome: VLMs show stronger ICL when trained on interleaved image-text data but primarily rely on textual cues rather than visual information integration

## Executive Summary
This paper investigates in-context learning (ICL) capabilities in vision-language models (VLMs) across four architectures and seven models, focusing on how prompt design, model architecture, and training strategies impact multimodal ICL performance. Through systematic analysis of image captioning tasks, the authors reveal that VLMs trained on interleaved image-text data demonstrate superior ICL scaling but fundamentally rely on textual patterns in demonstration captions rather than effectively integrating visual information from context images. Attention analysis confirms that current VLMs fail to leverage visual inputs in context, with instruction tuning potentially reducing reliance on demonstration examples. These findings highlight critical limitations in VLMs' ability to learn from multimodal in-context examples.

## Method Summary
The authors evaluate seven VLMs across four architectures on image captioning tasks using the NoCaps benchmark. They systematically vary prompt designs (demonstration retrieval methods, shot count, blacking out images) and analyze attention patterns to understand how models process multimodal context. The study compares paired-trained models (LLaVA v1.5) against interleaved-trained models (Idefics2, LLaVA-Next variants) and examines the impact of instruction tuning through base vs. IT variants. Performance is measured using CIDEr scores, while attention entropy and layer-wise attention maps provide insights into information flow between modalities.

## Key Results
- VLMs trained on interleaved image-text data maintain consistent ICL performance as shot count increases, while paired-trained models show significant degradation
- Models perform similarly when demonstration images are blacked out, indicating reliance on textual rather than visual context information
- Instruction tuning improves instruction-following but reduces the model's ability to learn from demonstration examples

## Why This Works (Mechanism)

### Mechanism 1: Text-Dominant Context Inference
- **Claim:** VLMs solve in-context tasks primarily by conditioning on textual patterns in demonstration captions rather than extracting visual features from demonstration images.
- **Mechanism:** The LLM backbone assigns disproportionately higher attention weights to text tokens (demonstration captions, system prompts) compared to visual tokens from context images. When demonstration images are removed or blacked out, performance remains largely unchanged, indicating the visual modality in the context acts as a weak signal.
- **Core assumption:** The attention weights retrieved from the model checkpoint are reliable proxies for information flow and feature utilization.
- **Evidence anchors:**
  - [Abstract] "Current VLMs focus on textual cues and fail to leverage visual information."
  - [Page 6] Results show minimal performance difference between "black-images" and standard demonstrations for most models.
  - [Corpus] Related work "Mimicking or Reasoning" suggests VLMs rely on shallow heuristics like copying text, reinforcing the text-dominance hypothesis.
- **Break condition:** If visual encoders were to output features that align with the LLM's embedding space such that image tokens carried higher information density than text tokens, this imbalance would theoretically shift.

### Mechanism 2: Data Structure-Induced Context Scaling
- **Claim:** Training on interleaved image-text data creates a prior that supports long-context integration, whereas training on isolated image-text pairs causes performance degradation as context length increases.
- **Mechanism:** Interleaved training exposes the model to sequences where relevant information is distributed across multiple modalities and time steps, teaching the LLM to manage longer multimodal context windows. Models trained on pairs (e.g., LLaVA v1.5) view multi-shot prompts as out-of-distribution, leading to attention collapse or distraction, where performance drops to zero as shot count increases.
- **Core assumption:** The observed performance drop in paired-trained models is due to context length/structure mismatch rather than simple overfitting to single-image prompts.
- **Evidence anchors:**
  - [Page 5] "Models trained on image-text interleaved datasets perform consistently better with more demonstrations... [Paired models] significantly declines."
  - [Page 5] LLaVA v1.5 performance drops to zero with 8+ shots, while LLaVA-Next-Interleave remains stable.
  - [Corpus] [Missing specific corpus evidence for the "out-of-distribution" cause; inferred from paper's training data analysis.]
- **Break condition:** If a paired-trained model were fine-tuned on longer sequences of concatenated pairs, the scaling degradation would likely diminish.

### Mechanism 3: Instruction Tuning Suppression of In-Context Adaptation
- **Claim:** Instruction tuning (IT) shifts the model's reliance from input demonstrations toward internal priors and explicit instructions, creating a trade-off between instruction-following and in-context learning.
- **Mechanism:** IT optimizes the model to map explicit instructions to desired outputs, often dampening the signal from subtle patterns in few-shot demonstrations. Attention analysis shows the IT model (Idefics2-IT) ignores the middle of the context (where demonstrations reside) more than the base model, effectively "overwriting" the few-shot signal.
- **Core assumption:** The reduction in performance relative to the base model is a direct result of the IT phase, not model scale or other variables.
- **Evidence anchors:**
  - [Abstract] "Instruction tuning improves instruction-following but reduces reliance on in-context demonstrations."
  - [Page 8] "Idefics2 (IT) seems to ignore the information in the middle of the context... consistent with the weaker ICL ability."
  - [Corpus] [Corpus evidence on this specific trade-off is weak; general consensus confirms IT changes behavior, but the specific suppression of ICL is primarily anchored in this paper's analysis of Idefics2.]
- **Break condition:** If IT datasets included explicit few-shot reasoning chains, this suppression might be mitigated.

## Foundational Learning

- **Concept:** **In-Context Learning (ICL) vs. Fine-Tuning**
  - **Why needed here:** The paper evaluates ICL specificallyâ€”changing behavior via the prompt window without weight updates. Distinguishing this from training-time learning is essential to interpret why adding shots might fail (it's not "learning" in the traditional sense, but activation conditioning).
  - **Quick check question:** Does updating the model weights require gradient descent, or can it be achieved by appending "User: A B C" to the prompt?

- **Concept:** **Attention Entropy and Focus**
  - **Why needed here:** The paper uses attention entropy (diffuseness of attention) and layer-wise attention maps to diagnose why models fail to "see" images. Understanding high vs. low entropy helps distinguish between a model "scanning" broadly vs. "fixating" on text.
  - **Quick check question:** If a model assigns 99% of attention weight to the first token, is the attention entropy high or low?

- **Concept:** **Interleaved vs. Paired Training Data**
  - **Why needed here:** This is the single biggest predictor of ICL capability in the study. You must understand the difference between a dataset of "(Image, Caption)" pairs vs. documents with "Text, Image, Text, Image."
  - **Quick check question:** Which dataset structure better mimics a multi-shot prompt: a folder of separate JPEGs, or a webpage with images embedded between paragraphs?

## Architecture Onboarding

- **Component map:** Visual Encoder (ViT) -> Modality Projector -> LLM Backbone -> Context Buffer
- **Critical path:**
  1. Retrieve k similar images/texts (Demonstration Retrieval)
  2. Encode demonstration images and query image via Visual Encoder
  3. Project visual features to visual tokens
  4. Interleave visual tokens with their captions in the LLM prompt
  5. LLM attends over this full sequence to generate the response

- **Design tradeoffs:**
  - **Projector Choice:** Q-Former (InstructBLIP) vs. Linear/MLP (LLaVA). Q-Formers may compress visual info efficiently but can struggle with multi-image sequences if not trained for it.
  - **Training Data:** Interleaved data enables ICL scaling but is harder to curate than image-text pairs.
  - **Instruction Tuning:** Essential for chat UX/benchmarking, but may degrade the raw few-shot signal (as seen in Idefics2).

- **Failure signatures:**
  - **ICL Collapse:** Performance drops as you add more demonstration shots (common in paired-trained models like LLaVA v1.5)
  - **Text Anchoring:** Performance does not drop when demonstration images are blacked out (model ignores visuals)
  - **Attention Saturation:** High attention on "System Prefix" and "Query Image," but near-zero attention on "Demonstration Images" (middle context)

- **First 3 experiments:**
  1. **The "Blackout" Test:** Run inference on a captioning task with 8 shots. Then, replace all demonstration images with black squares. If scores remain similar, the model is text-dependent.
  2. **Scaling Curve Analysis:** Plot CIDEr score vs. Shot Count (0, 1, 3, 8, 16) for both a paired-trained model (LLaVA v1.5) and an interleaved model (Idefics2). Look for the "U-shape" or degradation in the paired model.
  3. **Attention Heatmap Visualization:** Hook into the LLM layers to visualize attention. Check if the query tokens attend to the demonstration image tokens or the demonstration caption tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific training configurations can balance the trade-off between instruction following and in-context learning, specifically by combining interleaved data with instruction tuning?
- Basis in paper: [explicit] The Conclusion states that "investigating training strategies that combine instruction tuning with interleaved image-text supervision to support both instruction following and contextual learning is a promising direction."
- Why unresolved: The study found that while interleaved training boosts ICL and instruction tuning aids instruction-following, the latter can reduce reliance on in-context demonstrations, creating a conflict between the two capabilities.
- What evidence would resolve it: Experiments comparing VLMs trained with hybrid curricula that interleave instruction tuning data with interleaved image-text data, measuring both zero-shot instruction adherence and few-shot performance.

### Open Question 2
- Question: Can the design of modality projectors be improved to force the LLM to integrate visual information from in-context demonstrations rather than relying solely on textual cues?
- Basis in paper: [explicit] The Conclusion suggests "Designing more effective modality projectors may be the key to better transferring these abilities," following findings that models ignore visual context.
- Why unresolved: Attention analyses revealed that current projectors fail to compel the language model to attend to in-context images, leading to a "textual bias" where models perform well even with blacked-out demonstration images.
- What evidence would resolve it: Evaluation of novel projector architectures (e.g., incorporating cross-attention or specialized gating mechanisms) that show increased attention weights on visual tokens in demonstration examples and a performance drop when visual context is removed.

### Open Question 3
- Question: Do the observed biases toward textual context and the limited multimodal integration capabilities persist in VLMs with parameter scales significantly larger than the 9B limit used in this study?
- Basis in paper: [explicit] The Limitations section notes the analysis focuses on models with up to 9B parameters and asks, "studying larger models would be important to determine whether our conclusions hold at a greater scale."
- Why unresolved: It is unclear if the inability to "see" in-context images is a fundamental architectural flaw or a limitation of the model capacity evaluated.
- What evidence would resolve it: Replicating the specific attention map analysis and black-out experiments on models exceeding 70B parameters to see if larger models distribute attention more evenly across modalities.

## Limitations

- The study focuses primarily on image captioning tasks, limiting generalizability to other multimodal tasks
- Attention-based analysis may not accurately reflect actual information utilization from visual inputs
- The dataset composition and quality could introduce artifacts that influence the observed text-dominance behavior

## Confidence

**High Confidence (90%+):**
- Paired-trained models (LLaVA v1.5) show degraded ICL performance as shot count increases, while interleaved-trained models maintain performance
- Instruction tuning reduces reliance on demonstration examples in favor of instruction-following patterns
- Attention analysis consistently shows higher weights on textual components versus visual components in context

**Medium Confidence (70-89%):**
- VLMs primarily rely on textual cues rather than visual information for ICL (supported by blackout experiments but requires stronger causal evidence)
- The performance degradation in paired-trained models stems specifically from context structure mismatch rather than other factors
- Interleaved training creates better priors for long-context multimodal integration

**Low Confidence (below 70%):**
- The specific mechanism by which instruction tuning suppresses ICL (attention patterns alone provide weak causal evidence)
- Generalization of findings to non-captioning tasks or different VLM architectures
- The assumption that attention weights are reliable proxies for feature utilization

## Next Checks

1. **Ablation Study with Caption Perturbation**: Systematically modify demonstration captions (synonym replacement, paraphrasing, or corruption) while keeping images intact. If performance drops significantly, this would provide stronger evidence that VLMs rely on textual patterns rather than visual understanding. Conversely, if performance remains stable, it would suggest the model extracts visual information despite attention patterns.

2. **Cross-Architecture Attention Analysis**: Apply the same attention analysis methodology to VLMs with different architectural designs (e.g., Q-Former vs. linear projection, different visual encoders). This would test whether the observed attention patterns are architecture-specific or represent a broader limitation across VLMs.

3. **Task Generalization Benchmark**: Extend the experimental framework to multiple multimodal tasks beyond image captioning (e.g., visual question answering, image classification with explanations). This would validate whether the text-dominance phenomenon is task-specific or represents a fundamental limitation in VLMs' multimodal reasoning capabilities.