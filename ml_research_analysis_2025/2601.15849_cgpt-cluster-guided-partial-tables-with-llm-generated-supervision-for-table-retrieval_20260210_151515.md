---
ver: rpa2
title: 'CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table
  Retrieval'
arxiv_id: '2601.15849'
source_url: https://arxiv.org/abs/2601.15849
tags:
- table
- retrieval
- partial
- tables
- mimotable
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of table retrieval, where general-purpose
  embeddings often fail due to semantic compression and query-table mismatch. The
  proposed method, CGPT, improves retrieval by constructing semantically diverse partial
  tables using K-means clustering, generating synthetic queries with an LLM, and fine-tuning
  the embedding model using hard-negative contrastive learning.
---

# CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval

## Quick Facts
- **arXiv ID**: 2601.15849
- **Source URL**: https://arxiv.org/abs/2601.15849
- **Reference count**: 10
- **Primary result**: CGPT improves table retrieval R@1 by 16.54% average across four benchmarks via clustering, synthetic queries, and hard-negative contrastive fine-tuning

## Executive Summary
This paper tackles the problem of table retrieval, where general-purpose embeddings often fail due to semantic compression and query-table mismatch. The proposed method, CGPT, improves retrieval by constructing semantically diverse partial tables using K-means clustering, generating synthetic queries with an LLM, and fine-tuning the embedding model using hard-negative contrastive learning. Evaluated on four benchmarks, CGPT consistently outperforms the baseline QGpT with an average R@1 improvement of 16.54%. It also shows strong cross-domain generalization and remains effective even with smaller LLMs for query generation, making it both accurate and scalable for large-scale table retrieval.

## Method Summary
CGPT constructs semantically diverse partial tables (KPTs) by clustering table rows using K-means, generating synthetic queries for these KPTs with an LLM, and fine-tuning the embedding model using hard-negative contrastive learning. Specifically, it encodes table instances with BAAI/bge-m3, applies K-means clustering with k = min(⌊m/r⌋, k_max) where r=10, k_max=5, samples s=5 instances per cluster to form KPTs, generates n_q=5 synthetic queries per KPT using Llama-3.1-8B-Instruct, selects top h=8 similar KPTs as hard negatives, and fine-tunes with InfoNCE loss (τ=0.01, lr=1e-5, 2 epochs, gradient accumulation=32).

## Key Results
- CGPT achieves 16.54% average R@1 improvement across four benchmarks (MimoTable EN, MimoTable ZH, OTTQA, FetaQA, E2E-WTQ)
- CGPT w/o HNS scores 57.84 R@1 on MimoTable EN vs. full CGPT at 60.13, confirming hard negative contribution
- Performance remains consistent across different LLM sizes (Llama-3.1-8B, GPT-OSS-20B, Qwen3-4B), demonstrating robustness to query generator choice

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cluster-guided partial table construction provides broader semantic coverage than heuristic row selection.
- Mechanism: K-means clustering groups table rows by semantic similarity, and sampling across clusters ensures partial tables capture diverse attributes rather than potentially unrepresentative early rows.
- Core assumption: Table rows are semantically heterogeneous, and relevant query information may be distributed across the table rather than concentrated in the first rows.
- Evidence anchors:
  - [abstract] "CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage."
  - [section 3.1] "To ensure that the constructed partial tables capture the table's full semantic space, we apply K-means clustering."
  - [corpus] Related work QGpT uses first-k-rows heuristic; CGPT explicitly addresses this limitation.

### Mechanism 2
- Claim: Synthetic queries generated by LLMs provide effective supervision signals for embedding model fine-tuning.
- Mechanism: LLM generates natural language queries conditioned on partial table content; these queries form (query, positive table) pairs for contrastive training, bridging the query-table representation gap.
- Core assumption: LLM-generated queries approximate the distribution of real user queries and capture salient table semantics.
- Evidence anchors:
  - [abstract] "An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model."
  - [section 4.5] Performance remains consistent across Llama-3.1-8B, GPT-OSS-20B, and Qwen3-4B, suggesting robustness to LLM choice.
  - [corpus] Weak corpus evidence on synthetic query quality; no direct comparison to human-generated queries.

### Mechanism 3
- Claim: Hard negative sampling improves top-1 precision by strengthening discriminative boundaries.
- Mechanism: Selecting top-ℎ most similar but incorrect KPTs as negatives creates challenging contrastive examples, forcing the model to distinguish semantically close tables.
- Core assumption: Hard negatives provide more informative gradients than random negatives for representation refinement.
- Evidence anchors:
  - [section 3.3.1] "These negatives, which are easily confused with the true positive, provide strong supervision for improving the model's discrimination."
  - [section 4.2] CGPT achieves highest R@1 (60.13 on MimoTable EN) while CGPT w/o HNS scores 57.84; the gap confirms hard negative contribution.
  - [corpus] Dense Table Retrieval (DTR) and related work also leverage hard negative sampling for table tasks.

## Foundational Learning

- Concept: **K-means clustering**
  - Why needed here: Partitions table rows into semantically coherent groups for diverse partial table construction.
  - Quick check question: Can you explain how K-means determines cluster assignments and how cluster count k affects granularity?

- Concept: **Contrastive learning with InfoNCE loss**
  - Why needed here: Core training objective that pulls positive pairs closer and pushes negative pairs apart in embedding space.
  - Quick check question: How does InfoNCE differ from triplet loss, and what role does temperature τ play?

- Concept: **Hard negative mining**
  - Why needed here: Identifies semantically similar but incorrect candidates to improve model discrimination.
  - Quick check question: Why might hard negatives improve top-1 precision at the potential cost of top-5/10 recall?

## Architecture Onboarding

- Component map: Table → Row embeddings → K-means clusters → KPTs → Synthetic queries → Hard negatives → Fine-tuned embedding model → Retrieval inference

- Critical path: Table → Row embeddings → K-means clusters → KPTs → Synthetic queries → Hard negatives → Fine-tuned embedding model → Retrieval inference

- Design tradeoffs:
  - **Clustering vs. heuristic selection**: Clustering adds computation but improves semantic coverage.
  - **Hard negatives vs. random negatives**: Hard negatives boost R@1 but may slightly reduce R@5/10 (Table 1 observations).
  - **LLM size**: Smaller LLMs (4B-8B parameters) perform comparably to larger ones, enabling cost-efficient deployment.

- Failure signatures:
  - R@1 improvements without R@5/10 gains may indicate overfitting to hard negatives.
  - Performance drops on cross-domain evaluation suggest insufficient semantic diversity in training data.
  - High variance across LLM choices may indicate prompt sensitivity or query quality issues.

- First 3 experiments:
  1. **Baseline comparison**: Run QGpT vs. CGPT w/o FT vs. full CGPT on a single benchmark (e.g., MimoTable EN) to isolate clustering and fine-tuning contributions.
  2. **Ablation on k and s**: Vary cluster count k (e.g., 3, 5, 7) and samples per cluster s (e.g., 3, 5, 7) to find optimal coverage-efficiency tradeoff.
  3. **Hard negative sensitivity**: Compare h=4, 8, 16 to assess impact on R@1 vs. R@5/10 balance; monitor for training instability at higher h.

## Open Questions the Paper Calls Out
None explicitly stated in the provided content.

## Limitations
- The reliance on synthetic queries introduces uncertainty about whether generated queries truly represent real user query distributions. No direct validation against human-annotated queries is provided.
- The clustering hyperparameters (k = min(⌊m/r⌋, k_max), s=5) appear fixed across all datasets without sensitivity analysis, potentially limiting adaptation to different table characteristics.
- Cross-domain generalization, while demonstrated, is based on a limited set of benchmarks and may not extend to entirely different table domains (e.g., scientific vs. business tables).

## Confidence
- **High confidence**: The R@1 improvements (average 16.54%) are well-supported by ablation studies showing contributions from both clustering and hard negative sampling.
- **Medium confidence**: The claim that smaller LLMs perform comparably requires more rigorous testing across diverse query types and table domains.
- **Low confidence**: The assumption that K-means clustering always improves semantic coverage over sequential sampling is not validated for tables with uniform semantic distributions.

## Next Checks
1. Conduct human evaluation comparing synthetic queries to real user queries to assess distribution alignment and potential bias.
2. Perform ablation studies varying k and s parameters across different table types to identify optimal configuration for heterogeneous table corpora.
3. Test cross-domain transfer to tables from entirely different domains (e.g., scientific literature, financial reports) to validate generalization claims.