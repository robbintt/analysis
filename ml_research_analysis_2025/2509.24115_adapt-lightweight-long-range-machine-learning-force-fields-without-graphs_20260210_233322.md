---
ver: rpa2
title: 'ADAPT: Lightweight, Long-Range Machine Learning Force Fields Without Graphs'
arxiv_id: '2509.24115'
source_url: https://arxiv.org/abs/2509.24115
tags:
- adapt
- arxiv
- neural
- force
- defects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ADAPT, a machine learning force field for
  modeling point defects in silicon that avoids graph neural networks. Instead, it
  treats atoms as tokens and uses a Transformer architecture to directly model all
  pairwise interactions in Cartesian coordinates.
---

# ADAPT: Lightweight, Long-Range Machine Learning Force Fields Without Graphs

## Quick Facts
- arXiv ID: 2509.24115
- Source URL: https://arxiv.org/abs/2509.24115
- Authors: Evan Dramko; Yihuang Xiong; Yizhi Zhu; Geoffroy Hautier; Thomas Reps; Christopher Jermaine; Anastasios Kyrillidis
- Reference count: 40
- Primary result: 33% reduction in force prediction error and 30% reduction in energy prediction error compared to state-of-the-art models like MACE

## Executive Summary
This paper introduces ADAPT, a machine learning force field for modeling point defects in silicon that avoids graph neural networks. Instead, it treats atoms as tokens and uses a Transformer architecture to directly model all pairwise interactions in Cartesian coordinates. This approach addresses the limitations of graph neural networks, such as oversmoothing and poor long-range interaction representation. ADAPT is trained on a dataset of 252,240 silicon defect structures and achieves a 33% reduction in force prediction error and a 30% reduction in energy prediction error compared to state-of-the-art models like MACE, while requiring significantly less computational cost.

## Method Summary
ADAPT uses a Transformer encoder to model atomic interactions directly in Cartesian coordinates without graph representations. The model tokenizes atoms with 12-dimensional features including coordinates and physical properties, then processes them through a standard Transformer architecture. Rather than predicting both energy and forces from a single model, ADAPT uses separate models: a Transformer for forces and an MLP+Residual network for energy. The training objective uses an importance-weighted loss function that emphasizes atoms near defect sites to address the imbalance between bulk and defect regions.

## Key Results
- Achieves 33% reduction in force prediction error and 30% reduction in energy prediction error compared to MACE
- Demonstrates superior long-range interaction modeling compared to GNNs
- Reduces computational cost while maintaining or improving accuracy
- Ablation studies confirm global attention is necessary for capturing long-range effects

## Why This Works (Mechanism)

### Mechanism 1
Global self-attention captures long-range atomic interactions more effectively than local message-passing graphs for defect structures. Standard Graph Neural Networks (GNNs) restrict interactions to local neighborhoods, relying on depth to propagate signals. This leads to oversmoothing and over-squashing. ADAPT uses a full Transformer encoder to compute attention over all $n \times n$ pairwise atom interactions in a single step, directly accessing global geometry without depth-dependent propagation. The dataset is sufficient for the model to learn necessary rotational and translational invariances that GNNs typically hard-code as inductive biases.

### Mechanism 2
Decoupling force and energy prediction into separate models improves computational efficiency and modularity without sacrificing accuracy in this specific use case. Instead of a single model predicting energy and deriving forces via backpropagation (conservative forces), ADAPT trains one Transformer for forces and one MLP+Residual network for energy. This reduces the complexity of the learning objective for each network. Non-conservative forces (where force is not the exact gradient of predicted energy) are acceptable for structural relaxation tasks and do not destabilize the specific optimization loops used (e.g., FIRE optimizer).

### Mechanism 3
Importance-weighted loss functions mitigate the signal imbalance between bulk atoms (near-zero forces) and defect atoms (high forces). Standard Mean Squared Error (MSE) allows the model to minimize loss by predicting zero vectors everywhere. ADAPT uses a mask $m_i$ that applies a power-law weighting based on distance to known defect sites, forcing the network to prioritize accurate force prediction in the critical defect region. Defect locations are known a priori during training to construct the importance mask.

## Foundational Learning

- **Transformer Self-Attention ($O(n^2)$ Complexity)**
  - Why needed here: Unlike graphs which scale with edges, this model computes interactions between *all* atoms. Understanding the quadratic memory scaling is critical for determining maximum cell size.
  - Quick check question: If you double the number of atoms in the supercell, by what factor does the memory usage for the attention layer increase?

- **Inductive Bias vs. Learned Invariance**
  - Why needed here: The paper explicitly removes geometric priors (rotation/translation invariance) common in GNNs. You must understand that the model learns these properties from data rather than architecture.
  - Quick check question: Does the ADAPT architecture force the output forces to rotate if the input coordinates are rotated, or must it learn this behavior?

- **Conservative vs. Non-Conservative Forces**
  - Why needed here: The paper uses separate models for energy and forces. You must understand that $F \neq -\nabla E$ in this implementation to avoid misapplying the model to MD.
  - Quick check question: Can you use ADAPT's force predictor to verify the energy predictor's output via gradient calculation? (Answer: No).

## Architecture Onboarding

- **Component map:** Input Tokenizer -> Embedding MLP -> Transformer Encoder -> Force Head and Energy Head (Separate)
- **Critical path:** The attention mechanism is the sole interaction point between tokens. If attention weights are incorrect, distinct atoms cannot influence each other.
- **Design tradeoffs:**
  - **Accuracy vs. Physics:** Achieve lower MSE by decoupling energy/forces, but lose thermodynamic consistency (cannot do MD).
  - **Range vs. Cost:** Full attention captures long-range effects but limits max atoms; masking attention (Table 3) degrades accuracy.
  - **Model Size:** "Large" model overfits; "Small" model ($d_{model}=256$) is sufficient for the data scale.
- **Failure signatures:**
  - **Trivial Prediction:** Model outputs near-zero forces for all atoms (Minimizes MSE on bulk atoms). *Fix: Check importance-weighted loss implementation.*
  - **Overfitting:** Validation loss rises while training loss drops (seen in "Large" model). *Fix: Reduce $d_{model}$ or increase dropout.*
  - **Non-physical Rotation:** Forces do not rotate with structure. *Fix: Augment training data with rotations.*
- **First 3 experiments:**
  1. **Ablation on Interaction Radius:** Restrict attention masks to local regions to reproduce the error increase shown in Table 3, confirming global interactions are necessary.
  2. **Loss Function Comparison:** Train with standard MSE vs. Importance-Weighted MSE to quantify the improvement in defect-region force accuracy.
  3. **Data Scaling:** Train on subsets of the 252k structure dataset to determine the minimum data required for the Transformer to learn rotational invariance (validating the assumption that Transformers need "substantial quantities of data" per Section 3).

## Open Questions the Paper Calls Out

### Open Question 1
Can ADAPT maintain its accuracy and efficiency advantages when applied to diverse bulk structures or non-defect crystalline systems? The authors explicitly state, "it remains an open problem to determine ADAPT's applicability to other problems including diverse bulk structures." The current model is trained exclusively on silicon point defects, where long-range interactions are critical; it is unclear if the global attention mechanism is efficient or necessary for bulk materials. Benchmarking the architecture on general bulk-property datasets (e.g., Materials Project) would resolve this.

### Open Question 2
What constitutes the optimal set of input features (descriptors) for the atomic tokens? The paper notes, "Determining the best set of descriptors remains an open problem" regarding the 12-dimensional input vector. The current features (e.g., electronegativity, molar volume) were selected simply because they were "naturally present in the raw data" rather than through theoretical optimization. Ablation studies comparing various physical and learned feature sets against the heuristic baseline would measure impact on prediction error.

### Open Question 3
Can the non-conservative forces produced by separate energy/force models support stable, long-term molecular dynamics (MD) simulations? The authors acknowledge the design is non-conservative and explicitly "refrain from using such models for molecular dynamics simulations." Physical consistency (forces as gradients of energy) is typically required for energy conservation in MD; the independent model approach likely induces energy drift. Deploying the model in MD simulations to quantify energy conservation and system stability over time would resolve this.

## Limitations
- $O(n^2)$ memory complexity restricts practical applications to systems with fewer than ~1000 atoms
- Decoupling of force and energy prediction prevents use in molecular dynamics simulations requiring thermodynamic consistency
- Generalizability to charged defects, complex multi-defect configurations, or different materials systems remains untested

## Confidence

- **High Confidence:** The 33% force prediction improvement and 30% energy prediction improvement over MACE are well-supported by Table 1 and validated against a competitive baseline. The mechanism by which global attention captures long-range interactions is theoretically sound and demonstrated through the local-masking ablation experiment.

- **Medium Confidence:** The claim that coordinate-based Transformers can learn rotational/translational invariance without architectural constraints is plausible given the success of Transformers in vision and NLP, but the specific requirement for "substantial quantities of data" is somewhat vague. The paper provides evidence through ablation but doesn't quantify the exact data threshold.

- **Low Confidence:** The generalizability claim to "any point defect" extends beyond the validation set of 6 neutral silicon defect types. The model's performance on charged defects, complex multi-defect configurations, or different materials systems remains untested. Additionally, the computational cost reduction is relative to a specific GNN implementation and may not hold across all MLFF architectures.

## Next Checks

1. **Scalability Test:** Systematically evaluate ADAPT's force prediction accuracy as a function of atom count (e.g., 100, 200, 500, 1000 atoms) to identify the precise scaling limit and verify the $O(n^2)$ memory scaling claim.

2. **Generalization Test:** Evaluate ADAPT on silicon systems outside the training distribution - specifically charged defects, interstitial clusters, and surfaces - to quantify performance degradation and validate the "any point defect" claim.

3. **Physical Consistency Test:** Implement a test to verify whether the force predictions satisfy Newton's third law (action-reaction pairs) and whether forces converge to zero in bulk regions as expected for physically consistent models.