---
ver: rpa2
title: Bayesian Predictive Coding
arxiv_id: '2503.24016'
source_url: https://arxiv.org/abs/2503.24016
tags:
- parameters
- predictive
- learning
- bayesian
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Bayesian Predictive Coding (BPC), a method
  that extends predictive coding to estimate posterior distributions over neural network
  parameters, enabling uncertainty quantification while maintaining biological plausibility
  through local Hebbian updates. The core idea is to replace maximum likelihood estimates
  of parameters with approximate posterior distributions using conjugate Matrix Normal
  Wishart priors.
---

# Bayesian Predictive Coding

## Quick Facts
- arXiv ID: 2503.24016
- Source URL: https://arxiv.org/abs/2503.24016
- Reference count: 40
- Key outcome: Introduces Bayesian Predictive Coding (BPC) for uncertainty-aware neural network training using local Hebbian updates with closed-form Bayesian posterior updates.

## Executive Summary
Bayesian Predictive Coding (BPC) extends predictive coding to estimate posterior distributions over neural network parameters while maintaining biological plausibility. The method uses conjugate Matrix Normal Wishart priors to enable exact Bayesian updates via locally-computed Hebbian products of pre- and post-synaptic activity. BPC achieves comparable accuracy to predictive coding and backpropagation while converging in fewer epochs in full-batch settings and providing effective uncertainty quantification.

## Method Summary
BPC combines predictive coding's biologically plausible local updates with Bayesian inference by using Matrix Normal Wishart conjugate priors. The method alternates between E-steps (optimizing latent variables Z via gradient descent) and M-steps (updating posterior parameters via closed-form Hebbian updates). This enables exact posterior updates without gradient descent on parameters, while maintaining the local computation structure of predictive coding. The approach requires weights to remain outside nonlinearities to preserve conjugacy and uses full covariance matrices for uncertainty quantification.

## Key Results
- BPC achieves comparable accuracy to predictive coding and backpropagation while converging in fewer epochs in full-batch settings
- On synthetic regression tasks, BPC effectively quantifies both aleatoric and epistemic uncertainty
- Compared to Bayes by Backprop, BPC provides better log predictive density and lower RMSE across multiple UCI regression datasets while requiring fewer iterations

## Why This Works (Mechanism)

### Mechanism 1
Matrix Normal Wishart conjugate priors enable exact Bayesian posterior updates without gradient descent on parameters. The log-likelihood is quadratic in weights W when placed outside the nonlinearity, combined with a Matrix Normal Wishart prior over (W, Σ), this creates conditional conjugacy given latent variables Z. Posterior updates reduce to adding natural parameters.

### Mechanism 2
Weight posteriors update via locally-computed Hebbian products of pre- and post-synaptic activity. The natural parameter update accumulates outer products f(z_{l-1})f(z_{l-1})^T and f(z_{l-1})z_l^T—requiring only activities available at each synaptic connection. This preserves biological locality while computing exact Bayesian posteriors.

### Mechanism 3
Separating fast inference (Z optimization) from slow learning (posterior updates) via EM accelerates convergence in full-batch settings. The E-step minimizes free energy w.r.t. Z through gradient descent until convergence. The M-step then performs an exact posterior update given Z*. Since M-step is optimal (not gradient-based), full-batch training converges in remarkably few epochs.

## Foundational Learning

- **Variational Free Energy / ELBO**: Why needed here: BPC optimizes E(Z,λ) which equals variational free energy under a delta approximation for q(Z). Quick check: Why is minimizing free energy equivalent to maximizing a lower bound on log p(data)?

- **Conjugate Priors and Natural Parameters**: Why needed here: The Matrix Normal Wishart's conjugacy enables closed-form updates. Natural parameterization (η) is how posteriors are computed and stored. Quick check: For a Gaussian likelihood with unknown mean μ and precision λ, what prior family is conjugate, and what are its natural parameters?

- **Expectation-Maximization (EM) Algorithm**: Why needed here: BPC alternates E-steps (inferring Z) and M-steps (updating λ). Understanding EM clarifies why this structure works and its convergence guarantees. Quick check: Why does EM guarantee monotonic improvement in the objective function?

## Architecture Onboarding

- **Component map**: Latent variables Z = {z_0...z_L} -> Variational parameters λ = {M_l, V_l, Ψ_l, ν_l} -> Natural parameters η_l -> Energy E(Z,λ)

- **Critical path**: 1. Initialize η = η^(0) with M=0, V=10I, Ψ=1000I, ν=d_y+2; 2. Per batch: clamp z_0=x, z_L=y; iterate Eq. 6 until Z converges (T iterations); update η via Eq. 7; 3. Test time: deterministic forward (use M), MC sampling, or analytical uncertainty propagation (Eq. 11)

- **Design tradeoffs**: Full-batch vs mini-batch (full-batch gives optimal closed-form updates; mini-batch requires learning rate κ and more epochs); Compute overhead (must run T iterations of Z inference before each weight update); Memory (full covariance matrices V, Ψ are O(d²) per layer); Architecture constraint (weights must stay outside nonlinearities f(·) to preserve conjugacy)

- **Failure signatures**: Slow convergence (insufficient Z iterations, increase T or poor α selection); Exploding uncertainty (prior scale Ψ too small; increase Ψ^(0)); Numerical instability (Wishart requires positive definite matrices; monitor condition numbers of Ψ, V); Poor uncertainty calibration (verify both aleatoric and epistemic uncertainty are tracked)

- **First 3 experiments**: 1. Two moons classification with 100 hidden units, full-batch. Verify convergence in <5 epochs. Plot decision boundary with uncertainty bands; 2. Heteroscedastic regression: y = -(x+0.5)sin(3πx) + N(0, 0.45²(x+0.5)²). Confirm aleatoric uncertainty tracks noise amplitude; 3. UCI yacht regression: Compare BPC vs Bayes by Backprop on log predictive density and RMSE. Verify faster convergence from closed-form updates

## Open Questions the Paper Calls Out

### Open Question 1
Can structured low-rank approximations to the Matrix Normal Wishart posterior enable BPC to scale to large neural network architectures while preserving closed-form updates? Current experiments used small networks (50-128 hidden units); the quadratic memory cost of full covariance matrices makes the approach infeasible for modern architectures.

### Open Question 2
Can BPC be combined with sampling-based approaches (Monte Carlo, Langevin dynamics) to infer posterior distributions over both latent variables and parameters simultaneously? Current BPC infers posteriors over parameters but uses MAP estimates for latent variables; sampling methods do the opposite. No unified approach exists.

### Open Question 3
Does initializing with identity-like priors for Σ encourage disentangled representations that improve cross-task generalization? The paper only observes that Σ acts as an adaptive learning rate; the relationship between prior structure and representation quality remains untested.

## Limitations
- The requirement that weights remain outside nonlinearities is restrictive and may limit expressiveness compared to standard deep networks
- O(d²) memory footprint for full covariance matrices becomes prohibitive for large architectures, necessitating approximations not explored here
- While convergence in few epochs is demonstrated for full-batch training, mini-batch performance remains suboptimal due to the need for learning rate schedules and multiple epochs

## Confidence
- **High confidence**: BPC's biological plausibility through local Hebbian updates; the closed-form posterior updates given conjugacy; comparable accuracy to baseline methods
- **Medium confidence**: Faster convergence than PC/BP in full-batch; superior uncertainty quantification on synthetic tasks; competitive performance vs Bayes by Backprop on UCI datasets
- **Low confidence**: Scalability to deep/large networks; robustness across diverse architectures; practical advantages over approximate Bayesian methods in real-world applications

## Next Checks
1. Test BPC on convolutional architectures for image classification (CIFAR-10) to assess scalability beyond fully-connected networks
2. Implement low-rank or diagonal approximations for covariance matrices to evaluate the tradeoff between computational efficiency and uncertainty quality
3. Compare BPC's uncertainty calibration (e.g., via expected calibration error) against MC-Dropout and ensembles on out-of-distribution detection tasks