---
ver: rpa2
title: 'Mining Mental Health Signals: A Comparative Study of Four Machine Learning
  Methods for Depression Detection from Social Media Posts in Sorani Kurdish'
arxiv_id: '2508.15829'
source_url: https://arxiv.org/abs/2508.15829
tags:
- depression
- data
- dataset
- detection
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study introduces the first automated depression detection
  system for Sorani Kurdish using machine learning and NLP. It collected 960 tweets
  using expert-validated keywords and annotated them into three classes: Shows depression,
  Not-show depression, and Suspicious.'
---

# Mining Mental Health Signals: A Comparative Study of Four Machine Learning Methods for Depression Detection from Social Media Posts in Sorani Kurdish

## Quick Facts
- arXiv ID: 2508.15829
- Source URL: https://arxiv.org/abs/2508.15829
- Reference count: 6
- Primary result: Random Forest achieves 80% accuracy and F1-score for Sorani Kurdish depression detection after oversampling

## Executive Summary
This study introduces the first automated depression detection system for Sorani Kurdish using machine learning and NLP. It collected 960 tweets using expert-validated keywords and annotated them into three classes: Shows depression, Not-show depression, and Suspicious. Four models were evaluated: Support Vector Machines, Multinomial Naive Bayes, Logistic Regression, and Random Forest. The Random Forest model achieved the highest performance with 80% accuracy and F1-score after oversampling to balance the dataset. The work establishes a baseline for mental health monitoring in Kurdish language contexts and highlights the potential of traditional ML models for low-resource languages. Future work includes expanding the dataset, exploring deep learning, and multimodal analysis.

## Method Summary
The study collected 960 Sorani Kurdish tweets using 10 expert-validated keywords related to depression. Tweets were preprocessed using the KLPT library for normalization, removing URLs, emojis, English words, numbers, and duplicates. Text was converted to TF-IDF vectors, and the dataset was balanced using oversampling. Four supervised models (SVM, Multinomial Naive Bayes, Logistic Regression, Random Forest) were trained and evaluated using 10-fold cross-validation. The Random Forest model achieved the highest performance at 80% accuracy and F1-score after balancing the dataset.

## Key Results
- Random Forest achieved 80% accuracy and F1-score on balanced dataset
- TF-IDF vectorization enabled effective feature extraction for classical ML models
- Oversampling significantly improved performance by balancing class distribution
- Binary classification (removing "Suspicious" class) improved accuracy from 67% to 80%

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TF-IDF vectorization enables depression signal detection by weighting distinctive Sorani terms higher than common vocabulary.
- Mechanism: Term Frequency-Inverse Document Frequency transforms text into numerical vectors by amplifying rare but informative words (e.g., depression-related keywords) while down-weighting frequent, less discriminative terms. This allows classical ML models to operate on linguistic patterns.
- Core assumption: Depression-indicative language in Sorani contains statistically distinctive term distributions that differ from neutral posts.
- Evidence anchors:
  - [abstract] "Four supervised models... were trained and evaluated, with Random Forest achieving the highest performance accuracy and F1-score of 80%."
  - [section 3.4] "We will transform texts into numerical vectors using Term Frequency-Inverse Document Frequency (TF-IDF), which assigns higher weights to rare but informative terms while down weighting frequent, less informative words."
  - [corpus] Weak direct evidence; neighboring papers focus on other NLP tasks for Sorani, not depression-specific feature engineering.
- Break condition: If the vocabulary size is too small (<500 unique terms) or if depression signals are expressed through morphology/syntax rather than lexical choice, TF-IDF may fail to capture relevant patterns.

### Mechanism 2
- Claim: Oversampling minority classes improves classification performance by reducing model bias toward the majority class.
- Mechanism: Synthetic or duplicate samples balance class distribution, forcing models to learn decision boundaries for underrepresented classes rather than defaulting to majority predictions.
- Core assumption: The "Suspicious" and "Show Depression" classes contain learnable patterns that are obscured by imbalance, not inherent noise.
- Evidence anchors:
  - [abstract] "...Random Forest achieved the highest performance with 80% accuracy and F1-score after oversampling to balance the dataset."
  - [section 4.6.4] "Over-sampling expanded the dataset to 1,107 tweets equally distributed. All models showed significant improvement, with RF leading (precision 0.814, recall 0.801, F1 0.802)."
  - [corpus] No direct corpus evidence on oversampling effects for mental health detection in low-resource languages.
- Break condition: Oversampling can cause overfitting if duplicates are too frequent; test performance may inflate without true generalization. Use stratified cross-validation to detect this.

### Mechanism 3
- Claim: Random Forest's ensemble structure captures non-linear patterns in small, noisy text datasets better than linear classifiers.
- Mechanism: RF builds multiple decision trees on bootstrapped samples and aggregates votes, reducing variance and handling feature interactions that single trees or linear models miss.
- Core assumption: Depression signals in Sorani tweets involve multi-feature interactions (e.g., keyword co-occurrence + sentence structure) that linear boundaries cannot separate cleanly.
- Evidence anchors:
  - [section 4.8] "RF outperformed others despite prior weak results, suggesting better robustness to data quality issues."
  - [section 4.8] "RF excelled with highest accuracy (80.1%) and F1 (80.2%)... This highlights RF's ability to model complex patterns when data quality and balance improve."
  - [corpus] Related work (Kadkhoda et al., 2022; Musleh et al., 2022) also found RF competitive for mental health detection in other languages, suggesting cross-lingual robustness.
- Break condition: With very small datasets (<300 samples), RF may still overfit despite ensembling; feature importance scores should be inspected for spurious correlations.

## Foundational Learning

- **TF-IDF Vectorization**:
  - Why needed here: Converts Sorani text into numerical form since no pretrained Kurdish word embeddings are available.
  - Quick check question: Can you explain why TF-IDF down-weights the word "the" in English but might up-weight "depression" in a mental health corpus?

- **Class Imbalance and Resampling**:
  - Why needed here: The dataset had unequal class distribution (363 Show, 369 Not-show, remaining Suspicious), biasing initial models.
  - Quick check question: Why would a model achieve 70% accuracy on an imbalanced dataset yet still be useless for the minority class?

- **Cross-Validation for Small Datasets**:
  - Why needed here: With only 960 samples, a single train-test split yields high variance in performance estimates.
  - Quick check question: What is the difference between 10-fold cross-validation and a held-out test set in terms of bias-variance tradeoff?

## Architecture Onboarding

- **Component map**: Data Ingestion -> Twitter API + keyword filtering (10 expert-validated terms) -> Preprocessing -> KLPT normalization, URL/emoji/number removal, deduplication -> Feature Extraction -> TF-IDF vectorizer (scikit-learn) -> Balancing -> Oversampling (imbalanced-learn or manual replication) -> Models -> SVM, Multinomial NB, Logistic Regression, Random Forest -> Evaluation -> 10-fold CV, accuracy, precision, recall, F1, confusion matrices

- **Critical path**:
  1. Keyword validation with domain experts (psychiatrists) ensures data relevance.
  2. Preprocessing quality determines vocabulary size and noise level.
  3. Oversampling is required before model training to achieve >75% F1.
  4. RF should be prioritized as the primary model based on experimental results.

- **Design tradeoffs**:
  - Binary vs. 3-class: Removing "Suspicious" improves accuracy (67% → 80%) but loses ambiguous cases that may contain valuable signals.
  - Under-sampling vs. over-sampling: Under-sampling reduces data size (bad for small corpora); over-sampling risks overfitting but worked best here.
  - Deep learning vs. classical ML: No pretrained Sorani embeddings exist; training transformers from scratch requires >10K samples—classical ML is the practical choice.

- **Failure signatures**:
  - Accuracy ~50% with imbalanced data → model predicting majority class only.
  - Large gap between training and validation F1 → overfitting from oversampling.
  - Confusion matrix showing "Suspicious" misclassified as both other classes → class boundary is inherently ambiguous.

- **First 3 experiments**:
  1. Replicate the binary classification setup (remove "Suspicious") to verify baseline SVM performance (~67% F1).
  2. Apply oversampling to the 3-class dataset and confirm RF achieves ~80% F1 with 10-fold CV.
  3. Test an alternative feature representation (e.g., character n-grams with TF-IDF) to assess whether morphological signals improve detection beyond word-level features.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can deep learning approaches, such as BERT or GPT-based models, outperform the current Random Forest baseline in detecting depression from Sorani text?
- Basis in paper: [explicit] The conclusion specifically lists "exploring deep learning approaches such as BERT, GPT-based models, and convolutional neural networks (CNNs)" as a primary avenue for future work.
- Why unresolved: The study focused exclusively on traditional machine learning algorithms (SVM, MNB, LR, RF) to establish a baseline.
- What evidence would resolve it: Comparative performance metrics (Accuracy, F1-score) of transformer-based models trained on the same Sorani dataset.

### Open Question 2
- Question: Does the integration of non-textual data (images, videos, and emojis) improve the accuracy of depression detection compared to text-only analysis?
- Basis in paper: [explicit] The authors propose "multimodal analysis" by incorporating non-textual data as a method to provide richer context for detection.
- Why unresolved: The current methodology relies entirely on textual features extracted via TF-IDF, ignoring other content types present in social media posts.
- What evidence would resolve it: Experimental results from a multimodal model compared to the text-only Random Forest baseline (80% accuracy).

### Open Question 3
- Question: Can the detection system be effectively extended to cover other Kurdish dialects, specifically Kurmanji, to serve the broader Kurdish-speaking population?
- Basis in paper: [explicit] The conclusion suggests "extending dialectal coverage to develop more inclusive and robust models for the broader Kurdish-speaking population."
- Why unresolved: The current study is restricted to the Sorani dialect, and the introduction notes that Kurdish suffers from dialectal variation and script inconsistencies.
- What evidence would resolve it: Successful training and evaluation of a model on a curated dataset of Kurmanji social media posts.

## Limitations

- Limited to keyword-filtered Twitter data, potentially missing depression signals expressed through implicit language or non-text modalities
- Relies on TF-IDF which may not capture morphological patterns or contextual expressions critical for depression detection
- Small dataset (960 tweets) limits generalizability and prevents exploration of more sophisticated deep learning approaches
- Oversampling technique unspecified, making it difficult to assess whether performance gains reflect genuine pattern learning

## Confidence

- **High Confidence**: Random Forest achieving 80% F1-score with balanced data - directly supported by quantitative metrics and cross-validation
- **Medium Confidence**: TF-IDF effectively captures depression signals - reasonable but lacks comparative validation against alternative feature representations
- **Low Confidence**: First automated system for Sorani Kurdish depression detection - cannot be independently verified without exhaustive literature review

## Next Checks

1. **Feature Representation Comparison**: Replicate the experiment using character n-grams and word embeddings (if available) to determine whether TF-IDF is optimal for capturing depression signals in Sorani text, or whether morphological patterns provide superior discrimination.

2. **Dataset Diversity Assessment**: Collect an additional 500-1000 tweets using alternative keyword sets and demographic targeting to evaluate whether the current dataset overrepresents particular depression expressions or user groups, potentially inflating model performance on specific subpopulations.

3. **Temporal Stability Test**: Train models on tweets from different time periods (e.g., 2020 vs. 2023) to assess whether depression language patterns in Sorani Twitter communities have evolved, indicating whether the model captures stable linguistic markers or transient discourse trends.