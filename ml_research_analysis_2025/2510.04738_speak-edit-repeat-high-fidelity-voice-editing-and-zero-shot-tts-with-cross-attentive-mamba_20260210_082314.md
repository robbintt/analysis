---
ver: rpa2
title: 'Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive
  Mamba'
arxiv_id: '2510.04738'
source_url: https://arxiv.org/abs/2510.04738
tags:
- audio
- speech
- editing
- mamba
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces MA VE, a novel autoregressive architecture
  for text-conditioned voice editing and high-fidelity text-to-speech synthesis that
  combines Mamba state-space models with cross-attention mechanisms. The key innovation
  is replacing the quadratic-complexity self-attention in Transformers with linear-complexity
  Mamba layers while maintaining precise text-acoustic alignment through cross-attention,
  enabling efficient modeling of long audio sequences with strong linguistic conditioning.
---

# Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba

## Quick Facts
- arXiv ID: 2510.04738
- Source URL: https://arxiv.org/abs/2510.04738
- Reference count: 40
- Primary result: Achieves human-parity naturalness in speech editing with 57.2% indistinguishability from original speech

## Executive Summary
MA VE introduces a novel autoregressive architecture combining Mamba state-space models with cross-attention mechanisms for text-conditioned voice editing and high-fidelity text-to-speech synthesis. The key innovation replaces quadratic-complexity self-attention in Transformers with linear-complexity Mamba layers while maintaining precise text-acoustic alignment through cross-attention. This enables efficient modeling of long audio sequences with strong linguistic conditioning, achieving human-parity naturalness in speech editing where 57.2% of listeners rated edited speech as indistinguishable from the original. The model demonstrates approximately 6× memory reduction compared to VoiceCraft during inference while maintaining comparable latency.

## Method Summary
MA VE is a non-autoregressive text-to-speech model based on the VITS framework that leverages Mamba state-space models and cross-attention mechanisms. The architecture uses a decoder-only Transformer structure with Mamba layers replacing self-attention to achieve linear complexity for long audio sequence modeling. Cross-attention is employed for precise text-acoustic alignment, enabling fine-grained control over speech generation. The model is trained on 60 hours of English speech data and demonstrates strong performance in both voice editing and zero-shot TTS tasks. The Mamba layers effectively capture long-range dependencies while maintaining computational efficiency, and the cross-attention mechanism ensures accurate linguistic conditioning throughout the generation process.

## Key Results
- Human evaluation shows 57.2% of listeners rated edited speech as indistinguishable from original
- Outperforms VoiceCraft and FluentSpeech in both naturalness and speaker similarity for zero-shot TTS
- Achieves approximately 6× memory reduction compared to VoiceCraft during inference

## Why This Works (Mechanism)
The effectiveness of MA VE stems from its innovative combination of Mamba state-space models and cross-attention mechanisms. Mamba layers provide linear complexity for modeling long audio sequences, addressing the quadratic bottleneck of traditional self-attention. The cross-attention mechanism maintains precise text-acoustic alignment by conditioning the audio generation process on the input text at every step. This architectural synergy allows the model to capture both the fine-grained linguistic details required for natural speech and the long-range dependencies essential for coherent audio generation. The autoregressive nature ensures temporal consistency while the Mamba-based backbone provides computational efficiency.

## Foundational Learning

**Mamba State-Space Models**
- Why needed: Enable linear-complexity modeling of long sequences, addressing Transformer's quadratic bottleneck
- Quick check: Verify Mamba layer reduces computational complexity from O(n²) to O(n) for sequence length n

**Cross-Attention Mechanisms**
- Why needed: Maintain precise alignment between text and audio representations during generation
- Quick check: Confirm cross-attention weights properly align phoneme boundaries with corresponding acoustic features

**Autoregressive Generation**
- Why needed: Ensure temporal consistency and coherence in generated speech sequences
- Quick check: Validate that each generated frame conditions properly on all previous frames

**Text-Acoustic Alignment**
- Why needed: Critical for natural-sounding speech that accurately reflects linguistic content
- Quick check: Measure phoneme duration accuracy and alignment consistency across generated samples

## Architecture Onboarding

**Component Map**
Text Input -> Cross-Attention -> Mamba Encoder -> Mamba Decoder -> Audio Output

**Critical Path**
The critical path for inference flows through cross-attention layers that condition each audio generation step on the corresponding text input, followed by Mamba layers that process the temporal sequence efficiently. The decoder generates audio frames autoregressively while maintaining linguistic conditioning through cross-attention at each step.

**Design Tradeoffs**
The architecture trades the global context modeling of self-attention for the linear efficiency of Mamba layers, compensated by cross-attention for text conditioning. This reduces memory usage and computational cost while potentially limiting some global dependencies that self-attention would capture. The choice of Mamba over other efficient architectures balances simplicity with effectiveness for audio generation tasks.

**Failure Signatures**
Potential failure modes include misalignment between text and generated speech when cross-attention weights degrade, loss of long-range coherence in very long utterances due to Mamba's local processing, and artifacts in boundary regions between phonemes. The model may also struggle with out-of-domain prompts or highly expressive speech styles that require nuanced control beyond linguistic conditioning.

**First 3 Experiments**
1. Test cross-attention alignment accuracy on controlled text-audio pairs with known alignments
2. Evaluate Mamba layer performance on long audio sequences (30+ seconds) for computational efficiency
3. Compare voice editing quality on diverse speaker characteristics and recording conditions

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation limited to English language, leaving multilingual performance and cross-linguistic generalization untested
- Human evaluation sample size (50 participants) and methodology details for preference testing are not fully specified
- Performance on challenging acoustic conditions like noisy environments or accented speech remains unexplored

## Confidence
- Architecture design and quantitative results: High
- Efficiency comparisons (memory, latency): Medium
- Generalization capabilities and robustness: Low

## Next Checks
1. Test multilingual performance across 3-5 diverse languages with speaker similarity and naturalness metrics
2. Conduct robustness evaluation with noisy audio conditions and accented speech variations
3. Perform detailed ablation studies quantifying individual contributions of Mamba layers, cross-attention, and their combination to overall model performance