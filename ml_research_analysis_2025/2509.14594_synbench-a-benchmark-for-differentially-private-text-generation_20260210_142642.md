---
ver: rpa2
title: 'SynBench: A Benchmark for Differentially Private Text Generation'
arxiv_id: '2509.14594'
source_url: https://arxiv.org/abs/2509.14594
tags:
- data
- privacy
- synthetic
- datasets
- aug-pe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a comprehensive benchmark for differentially
  private text generation, addressing critical gaps in evaluation methodology for
  privacy-preserving synthetic data generation in high-stakes domains like healthcare
  and finance. The benchmark includes nine diverse datasets capturing domain-specific
  challenges such as technical jargon, long-context dependencies, and specialized
  document structures.
---

# SynBench: A Benchmark for Differentially Private Text Generation

## Quick Facts
- arXiv ID: 2509.14594
- Source URL: https://arxiv.org/abs/2509.14594
- Reference count: 40
- Introduces comprehensive benchmark for differentially private text generation across nine diverse datasets

## Executive Summary
This work presents SynBench, a comprehensive benchmark designed to evaluate differentially private text generation methods across diverse domain-specific challenges. The benchmark addresses critical gaps in evaluation methodology for privacy-preserving synthetic data generation, particularly in high-stakes domains like healthcare and finance. Through extensive empirical studies using state-of-the-art DP methods including DP-Gen and AUG-PE, the research reveals that generating high-quality domain-specific synthetic data under differential privacy constraints remains an unsolved challenge, with performance degrading significantly as domain complexity increases.

The study develops novel methodologies including a membership inference attack specifically designed for synthetic text, providing first empirical evidence that public dataset contamination in pre-training corpora can invalidate claimed privacy guarantees. Experimental results show counterintuitive findings where stricter privacy constraints don't always correspond to decreased performance, and reveal complex relationships between declared privacy parameters and actual privacy leakage. These findings underscore the urgent need for rigorous privacy auditing and highlight persistent gaps between open-domain and specialist evaluations.

## Method Summary
The research introduces SynBench as a comprehensive benchmark for evaluating differentially private text generation methods. The benchmark comprises nine diverse datasets capturing domain-specific challenges including technical jargon, long-context dependencies, and specialized document structures. The evaluation framework systematically tests state-of-the-art DP methods including DP-Gen and AUG-PE across varying privacy budgets (ε ∈ {0.5, 1, 2, 4}). A novel membership inference attack methodology specifically designed for synthetic text is developed to assess actual privacy leakage beyond theoretical guarantees. The study conducts large-scale empirical evaluations measuring both utility and privacy metrics, with particular attention to how performance varies across different domain complexities and privacy budget constraints.

## Key Results
- Generating high-quality domain-specific synthetic data under differential privacy constraints remains an unsolved challenge, with performance degrading significantly as domain complexity increases
- Public dataset contamination in pre-training corpora can invalidate claimed privacy guarantees, as demonstrated by novel membership inference attack methodology
- Stricter privacy constraints do not always correspond to decreased performance, revealing counterintuitive relationships between privacy parameters and utility

## Why This Works (Mechanism)
The benchmark succeeds by providing standardized evaluation methodology that captures real-world domain complexities often overlooked in open-domain assessments. The membership inference attack methodology specifically designed for synthetic text provides empirical evidence of privacy leakage that theoretical guarantees alone cannot capture. The systematic variation of privacy budgets and domain complexities reveals nuanced relationships between privacy constraints and performance that inform responsible deployment strategies.

## Foundational Learning
- Differentially Private Stochastic Gradient Descent (DP-SGD): Essential for understanding how privacy budgets affect model training; quick check: verify gradient clipping and noise addition parameters match theoretical guarantees
- Membership Inference Attacks: Critical for empirical privacy evaluation beyond theoretical bounds; quick check: test attack accuracy on both real and synthetic data distributions
- Privacy Budget (ε) Calibration: Key to balancing utility and privacy; quick check: verify monotonic relationship between ε and performance across domains
- Domain Adaptation Challenges: Understanding how technical jargon and specialized structures affect DP performance; quick check: compare performance gaps between open-domain and domain-specific datasets
- Pre-training Corpus Contamination: Critical vulnerability in claimed privacy guarantees; quick check: analyze overlap between training data and public datasets

## Architecture Onboarding
Component map: Datasets -> DP Training Pipeline -> Evaluation Framework -> Privacy Attack Module
Critical path: Data preprocessing -> DP-SGD training -> Synthetic generation -> Utility evaluation -> Privacy auditing
Design tradeoffs: Strict privacy (low ε) vs. utility, domain-specific vs. open-domain adaptation, theoretical guarantees vs. empirical validation
Failure signatures: Performance collapse on technical domains, privacy leakage through membership inference, training instability at extreme privacy budgets
First experiments: 1) Baseline utility evaluation across all nine datasets, 2) Privacy budget sensitivity analysis, 3) Membership inference attack effectiveness testing

## Open Questions the Paper Calls Out
The paper highlights several open questions including how to effectively balance privacy and utility in highly specialized domains, whether current membership inference methodologies capture realistic adversarial capabilities, and how to address pre-training corpus contamination vulnerabilities. The counterintuitive findings regarding privacy parameter-performance relationships also raise questions about the fundamental assumptions underlying DP theory in text generation contexts.

## Limitations
- Evaluation relies on curated benchmark datasets rather than production-scale healthcare or financial data
- Privacy auditing frameworks may not capture full range of real-world adversarial capabilities
- The counterintuitive relationship between privacy constraints and performance requires further investigation to rule out methodological artifacts

## Confidence
- Benchmark comprehensiveness and domain coverage: High
- Performance degradation with domain complexity: Medium
- Privacy parameter-performance relationship findings: Low-Medium
- Membership inference attack effectiveness: Medium

## Next Checks
1. Replicate the benchmark evaluation using production-scale datasets from actual healthcare providers and financial institutions to validate domain-specific performance claims
2. Conduct adversarial red-teaming exercises with independent privacy experts to test the membership inference attack methodology and assess whether it represents a realistic threat model
3. Perform ablation studies systematically varying pre-training corpus composition to quantify the impact of public dataset contamination on privacy guarantees, controlling for model architecture and training methodology