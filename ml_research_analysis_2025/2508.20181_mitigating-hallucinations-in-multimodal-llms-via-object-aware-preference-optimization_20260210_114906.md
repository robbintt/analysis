---
ver: rpa2
title: Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization
arxiv_id: '2508.20181'
source_url: https://arxiv.org/abs/2508.20181
tags:
- chair-dpo
- image
- preference
- llav
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CHAIR-DPO, a novel preference optimization
  method to mitigate visual hallucinations in Multimodal Large Language Models (MLLMs).
  The method leverages the CHAIR metric to quantify hallucinations and collect preference
  data by comparing two completions for a given image and prompt, then uses Direct
  Preference Optimization (DPO) to fine-tune the model.
---

# Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization

## Quick Facts
- arXiv ID: 2508.20181
- Source URL: https://arxiv.org/abs/2508.20181
- Authors: Alberto Compagnoni; Davide Caffagni; Nicholas Moratelli; Lorenzo Baraldi; Marcella Cornia; Rita Cucchiara
- Reference count: 40
- Primary result: CHAIR-DPO achieves state-of-the-art hallucination reduction (HalRate from 35.0% to 14.7%, CHAIRi from 7.6 to 3.0) while preserving general capabilities

## Executive Summary
This paper introduces CHAIR-DPO, a novel preference optimization method to mitigate visual hallucinations in Multimodal Large Language Models (MLLMs). The method leverages the CHAIR metric to quantify hallucinations and collect preference data by comparing two completions for a given image and prompt, then uses Direct Preference Optimization (DPO) to fine-tune the model. The approach avoids complex pipelines or proprietary model reliance, requiring only an open-source model and an off-the-shelf object detector. Experiments on multiple benchmarks (AMBER, CHAIR-MSCOCO, Object HalBench) show that CHAIR-DPO achieves state-of-the-art performance, significantly reducing hallucination rates while preserving general capabilities.

## Method Summary
The paper presents CHAIR-DPO, a method that mitigates visual hallucinations in MLLMs through preference optimization. The approach collects preference data by comparing two completions for each image-prompt pair, using the CHAIR metric to evaluate hallucination severity. It then applies Direct Preference Optimization (DPO) to fine-tune the model based on these preferences. The method uses an off-the-shelf object detector to identify objects in images and relies on text-matching heuristics to quantify hallucinations. This approach is designed to be simpler and more accessible than methods requiring proprietary models or complex pipelines, while still achieving strong performance on multiple hallucination benchmarks.

## Key Results
- CHAIR-DPO significantly reduces hallucination rates on LLaVA-1.5-7B (HalRate from 35.0% to 14.7%, CHAIRi from 7.6 to 3.0)
- Achieves state-of-the-art performance across multiple benchmarks (AMBER, CHAIR-MSCOCO, Object HalBench)
- Data filtering and hyperparameter tuning further improve results
- Qualitative examples confirm reduced hallucinations and more focused descriptions

## Why This Works (Mechanism)
The mechanism behind CHAIR-DPO's effectiveness lies in its preference optimization framework that directly addresses hallucinations through targeted fine-tuning. By using the CHAIR metric to quantify hallucination severity, the method creates a clear optimization objective that guides the model away from hallucinatory outputs. The preference data collection process ensures that the model learns to distinguish between accurate and hallucinatory completions, while the DPO fine-tuning process adapts the model parameters to favor lower-hallucination outputs. The object detection component provides grounded visual information that helps constrain the model's outputs to describe only what is actually present in the image.

## Foundational Learning

**Direct Preference Optimization (DPO)**
- Why needed: Provides a scalable way to fine-tune models based on preference data without requiring complex reward modeling
- Quick check: Verify understanding of how DPO optimizes the policy directly from pairwise comparisons

**Object Detection Integration**
- Why needed: Enables quantification of hallucinations by identifying actual objects present in images
- Quick check: Understand how object detection outputs are used in the CHAIR metric calculation

**Text-Matching Heuristics**
- Why needed: Provides a computational method to compare generated descriptions against ground truth object presence
- Quick check: Know how precision and recall are calculated between generated text and detected objects

## Architecture Onboarding

**Component Map**
Image + Prompt -> Object Detector -> CHAIR Metric -> Preference Pairs -> DPO Fine-tuning -> Reduced Hallucination Model

**Critical Path**
The critical path is: Image and Prompt → Object Detection → CHAIR Metric Calculation → Preference Data Collection → DPO Fine-tuning. This path determines the quality of the final hallucination-reduced model.

**Design Tradeoffs**
- Simplicity vs. potential bias: Using CHAIR metric avoids proprietary models but may introduce detection-based biases
- Object-centric focus vs. generalization: Method excels at object descriptions but may be less effective for abstract scenes
- Open-source accessibility vs. performance ceiling: Avoids complex pipelines but may not match proprietary approaches

**Failure Signatures**
- Over-reliance on detected objects may miss contextual or abstract hallucinations
- Text-matching heuristics may struggle with synonyms or paraphrasing
- Object detector failures propagate to CHAIR metric inaccuracies

**First Experiments**
1. Run CHAIR metric on a small set of images to verify object detection and text-matching functionality
2. Generate preference pairs for a subset of data to test preference collection pipeline
3. Apply DPO fine-tuning on a small model to verify the optimization process works

## Open Questions the Paper Calls Out
- How to extend CHAIR-DPO to handle non-object-centric hallucinations and abstract visual content
- The impact of CHAIR metric biases on model performance across different image types
- Scalability of the approach to larger model architectures and more complex visual scenes
- Generalization performance on out-of-distribution prompts and images not seen during training

## Limitations
- Potential dataset bias from CHAIR metric's object detection and text-matching heuristics
- Limited evaluation on non-object-centric images and abstract visual content
- Focus on specific model architectures (LLaVA-1.5) without extensive testing on larger models
- Reliance on text-matching may miss semantically equivalent but lexically different descriptions

## Confidence
- **High confidence** in the core technical contribution and experimental methodology
- **Medium confidence** in claimed superiority over existing methods
- **Medium confidence** in scalability claims to larger model architectures

## Next Checks
1. Evaluate CHAIR-DPO on a broader range of image types beyond object-centric scenes
2. Conduct human evaluation studies to validate CHAIR metric correlation with human judgments
3. Test fine-tuned models on out-of-distribution prompts and images not in training data
4. Investigate the impact of different object detectors on CHAIR metric accuracy and final model performance