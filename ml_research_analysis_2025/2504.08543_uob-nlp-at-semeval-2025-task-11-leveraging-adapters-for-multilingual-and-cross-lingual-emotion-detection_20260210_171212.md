---
ver: rpa2
title: 'UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual and
  Cross-Lingual Emotion Detection'
arxiv_id: '2504.08543'
source_url: https://arxiv.org/abs/2504.08543
tags:
- languages
- adapters
- language
- emotion
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multilingual and cross-lingual emotion detection,
  particularly for low-resource languages where standard approaches struggle. The
  authors leverage adapter-based fine-tuning with multilingual pre-trained language
  models (PLMs), keeping model weights fixed and introducing small trainable adapter
  modules.
---

# UoB-NLP at SemEval-2025 Task 11: Leveraging Adapters for Multilingual and Cross-Lingual Emotion Detection

## Quick Facts
- arXiv ID: 2504.08543
- Source URL: https://arxiv.org/abs/2504.08543
- Reference count: 8
- Top-four ranking in 11 of 28 languages, outperforming LLMs in 11 languages despite significantly fewer parameters

## Executive Summary
This paper tackles multilingual and cross-lingual emotion detection for low-resource languages using adapter-based fine-tuning. The authors keep pre-trained model weights frozen and introduce small trainable adapter modules, achieving strong performance across 28 languages. Their best approach, target-language-ready task adapters, ranks in the top four for 11 languages and outperforms large language models in 11 languages while using significantly fewer parameters.

## Method Summary
The approach uses adapter-based fine-tuning with multilingual pre-trained language models (XLM-RoBERTa for non-African languages, Afro-XLMR for African languages). Language adapters are trained on unlabeled text using masked language modeling, then task adapters are trained on labeled emotion data by cycling through multiple language adapters. The best-performing target-language-ready task adapters achieve the highest scores for 14 languages by exposing the task adapter to multiple language adapters during training, encouraging language-invariant task features.

## Key Results
- Target-language-ready task adapters achieved highest scores for 14 languages
- TLR approach outperforms LLMs in 11 languages and matches their performance in four others
- Language-family-based adapters performed best for two Arabic languages, two Romance languages, and Slavic languages
- Task-only adapters outperformed TLR for Igbo, Pidgin, and Tatar

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TLR task adapters improve cross-lingual transfer by exposing the adapter to multiple language adapters during training
- Mechanism: Cycling through all relevant language adapters during training forces the task adapter to learn language-invariant representations rather than overfitting to one language's adapter
- Core assumption: Cycling encourages language-invariant task features to emerge in adapter weights
- Evidence anchors: Abstract states TLR achieves best performance particularly for low-resource African languages; Section 5 confirms TLR achieved highest scores for 14 languages
- Break condition: Training on too few language adapters or poorly trained LAs degrades cycling benefit

### Mechanism 2
- Claim: Freezing pre-trained model weights preserves cross-lingual representations while adapters specialize for the task
- Mechanism: Keeping XLM-RoBERTa weights fixed and only updating adapter parameters retains multilingual embedding space from pretraining while adapters learn task-specific transformations
- Core assumption: Pretrained multilingual space contains sufficient cross-lingual alignment; adapters only need to map this space to emotion labels
- Evidence anchors: Abstract notes adapters introduce small trainable parameters while keeping pre-trained weights fixed; Section 5 shows approach outperforms LLMs in 11 languages despite fewer parameters
- Break condition: Poor base PLM coverage of target language locks in weak representations that adapters cannot fully compensate for

### Mechanism 3
- Claim: Language-family-based adapter grouping can leverage shared linguistic features
- Mechanism: Training TLR adapters only on same-family LAs may improve transfer efficiency by reducing noise from unrelated languages
- Core assumption: Phylogenetic similarity correlates with shared representations in PLM's embedding space
- Evidence anchors: Section 5 shows family-based adapters performed best for Arabic, Romance, and Slavic languages; Section 2 notes languages within same family share structural and lexical similarities
- Break condition: Uneven resource levels within language families or misaligned family groupings with PLM representations cause underperformance

## Foundational Learning

- Concept: **Adapter bottleneck architecture**
  - Why needed here: Adapters use bottleneck (typically reduction factor 16) to compress and expand representations; understanding this helps diagnose capacity issues
  - Quick check question: If an adapter underfits a complex task, should you increase or decrease the bottleneck size?

- Concept: **Language Adapter (LA) vs. Task Adapter (TA) separation**
  - Why needed here: LAs are trained on unlabeled text with MLM objective; TAs are trained on labeled task data; their stacking order and independence matter for modularity
  - Quick check question: Can you reuse an LA trained for one task with a different TA for another task?

- Concept: **Cross-lingual transfer paradigms (translate-train vs. zero-shot vs. adapter-swap)**
  - Why needed here: This work uses adapter-swap (train TA with source LA, inference with target LA); knowing alternatives helps contextualize design choices
  - Quick check question: Why might adapter-swap outperform zero-shot cross-lingual transfer for low-resource languages?

## Architecture Onboarding

- Component map: Base PLM -> Language Adapter -> Task Adapter -> Classification Head
- Critical path:
  1. Verify LA availability for target languages (AdapterHub or train new)
  2. Train TLR-TA by cycling through all relevant LAs with emotion labels
  3. At inference, stack trained TA with target language's LA
  4. Pass through classification head for emotion prediction
- Design tradeoffs:
  - TLR with all LAs vs. language-family TLR: TLR more robust but requires more LAs; family-based faster but works better for well-represented families
  - XLM-R vs. Afro-XLMR: Afro-XLMR better for African languages; XLM-R better for others
  - Task-only adapters vs. LA+TA stacking: Task-only simpler but loses language-specific adaptation
- Failure signatures:
  - Very low scores on specific languages (vmw: 0.031, swa: 0.162) suggest insufficient LA quality or PLM pretraining coverage
  - Large dev-test discrepancy indicates overfitting to development set
  - Task-only adapters outperforming TLR for some languages suggests cycling may introduce noise for certain language pairs
- First 3 experiments:
  1. Replicate TLR-TA on XLM-RoBERTa-base for a subset of languages; verify ranking on development set matches paper trends
  2. Ablate by training standard TA on Spanish LA only; compare cross-lingual performance to TLR to quantify cycling benefit
  3. Test language-family TLR for one family (e.g., Semitic: Amharic, Tigrinya, Arabic variants); compare to full TLR to assess family-grouping hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do prompt tuning methods with large language models compare to adapter-based approaches for emotion detection in specialized tasks and low-resource languages?
- Basis in paper: [explicit] Conclusion states future research will examine prompt tuning methods with LLMs
- Why unresolved: Paper focused exclusively on adapter-based fine-tuning and did not explore prompt tuning
- What evidence would resolve it: Comparative study applying prompt tuning to same emotion detection datasets across 28 languages, measuring performance, parameter efficiency, and cross-lingual transfer against adapter baselines

### Open Question 2
- Question: What specific biases do adapter-based emotion detection models inherit from pre-trained language models, and how do these manifest differently across languages and cultural groups?
- Basis in paper: [explicit] Ethical considerations section notes adapters rely on pre-trained models that may introduce biases affecting specific languages, dialects, or groups
- Why unresolved: Paper acknowledges risk but does not conduct bias analysis or propose mitigation strategies
- What evidence would resolve it: Systematic bias audits measuring performance disparities across demographic groups within languages, and qualitative analysis of misclassification patterns tied to cultural expressions of emotion

### Open Question 3
- Question: What factors determine when language-family-based adapter grouping outperforms target-language-ready approaches for cross-lingual emotion detection?
- Basis in paper: [inferred] Results show family-based adapters performed best only for Arabic, Romance, and Slavic languages while TLR excelled in 14 languages
- Why unresolved: Authors do not analyze what linguistic or data-related characteristics predict success for family-based vs. broader adapter strategies
- What evidence would resolve it: Correlation analysis between family-based performance gains and metrics such as intra-family linguistic similarity, shared lexical items, training data size per family member, and typological feature overlap

### Open Question 4
- Question: What causes the development-to-test set performance discrepancies observed in adapter-based emotion detection models?
- Basis in paper: [inferred] Authors note models selected on development set sometimes over- or underperformed on test sets, speculating causes including overfitting to development set, data distribution differences, or varying levels of pretraining exposure
- Why unresolved: Multiple hypotheses offered but no empirical investigation determines which factor(s) dominate
- What evidence would resolve it: Controlled experiments analyzing dev-test distribution shifts, correlation between pretraining corpus size per language and performance variance, and regularization techniques to reduce overfitting

## Limitations

- Critical training hyperparameters for task adapter training (epochs, batch size, learning rate schedule, weight decay) are unspecified, making exact reproduction difficult
- TLR cycling mechanism lacks specification regarding whether languages are cycled per batch, per epoch, or via random sampling
- Claims about outperforming LLMs lack sufficient comparative detail to fully validate, as specific LLM baselines and configurations are not detailed

## Confidence

**High Confidence**: Core mechanism of adapter-based cross-lingual transfer is well-established and experimental results are internally consistent; ranking performance across languages is verifiable

**Medium Confidence**: TLR mechanism's superiority over standard adapter approaches is supported by experimental data but exact implementation details are unclear; claim about TLR working particularly well for low-resource African languages requires verification of LA quality

**Low Confidence**: Claims about outperforming LLMs lack sufficient comparative detail to fully validate; mixed performance of language-family-based adapters versus full TLR suggests underlying assumptions may not generalize but paper provides limited analysis

## Next Checks

1. **Hyperparameter Replication Test**: Reproduce TLR-TA training on a small subset (e.g., Spanish and English) with multiple hyperparameter configurations (varying learning rate, batch size, and training epochs) to determine which settings yield results closest to published performance

2. **LA Quality Assessment**: For low-performing languages (vmw, swa, ibo, pcm), evaluate MLM perplexity of their corresponding language adapters on held-out text from same domain as emotion datasets to confirm adapter training data limitations as primary failure mode

3. **LLM Baseline Comparison**: Implement same emotion detection task using comparable-sized LLM (e.g., LLaMA-7B or Mistral-7B) with full fine-tuning on same datasets to verify adapter approach's claimed efficiency advantages