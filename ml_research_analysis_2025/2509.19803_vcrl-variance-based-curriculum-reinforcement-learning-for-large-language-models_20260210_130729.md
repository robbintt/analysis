---
ver: rpa2
title: 'VCRL: Variance-based Curriculum Reinforcement Learning for Large Language
  Models'
arxiv_id: '2509.19803'
source_url: https://arxiv.org/abs/2509.19803
tags:
- training
- vcrl
- arxiv
- grpo
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VCRL, a curriculum reinforcement learning
  framework that dynamically adjusts training sample difficulty based on group reward
  variance in rollout-based RL methods. VCRL identifies samples with moderate difficulty
  (high variance) as most valuable for training and uses a memory bank to maintain
  high-variance samples, improving both training efficiency and stability.
---

# VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models

## Quick Facts
- arXiv ID: 2509.19803
- Source URL: https://arxiv.org/abs/2509.19803
- Reference count: 32
- Primary result: VCRL improves mathematical reasoning accuracy by up to 4.67 points over strong baselines, with 24.8-point improvement over base models

## Executive Summary
VCRL introduces a curriculum reinforcement learning framework that dynamically filters training samples based on reward variance in rollout groups. By identifying samples with moderate difficulty (high variance) as most valuable for training, VCRL improves both training efficiency and stability. Experiments on five mathematical reasoning benchmarks with Qwen3-4B and Qwen3-8B models show consistent improvements over strong baselines like GRPO, DAPO, and GSPO, particularly on challenging AIME datasets.

## Method Summary
VCRL implements variance-based curriculum learning on top of GRPO by calculating the normalized variance of binary rewards within rollout groups. Samples are filtered if their variance falls below a threshold Îº, with high-variance samples maintained in a memory bank for replay learning. The method uses binary reward signals from verifiable tasks (0 or 1), where variance peaks when models succeed on approximately 50% of rollouts. Training dynamically adjusts the difficulty threshold from 0.3 to 0.8, with a memory bank using momentum updates to maintain high-value samples throughout training.

## Key Results
- VCRL consistently outperforms GRPO, DAPO, and GSPO baselines across all five mathematical reasoning benchmarks
- Achieves up to 4.67 points higher average score compared to strong baselines
- Delivers 24.8 points improvement over base models, with particularly strong performance on AIME datasets
- Provides more stable gradients with smaller fluctuations than GRPO during training
- Maintains better exploration throughout training while achieving faster early learning

## Why This Works (Mechanism)

### Mechanism 1: Variance as a Proxy for Learnability
The variance of binary rewards within a rollout group serves as a dynamic proxy for sample difficulty, identifying samples at the "edge" of the model's current capabilities. In verifiable reward settings, variance is maximized when a model succeeds on approximately 50% of rollouts ($k \approx G/2$). If the model always succeeds (too easy) or always fails (too hard), variance approaches zero. VCRL filters for high-variance samples, assuming these provide the most informative gradients. This works because the task uses binary rewards where probability of success maps monotonically to difficulty.

### Mechanism 2: Gradient Stabilization via Dynamic Filtering
Removing samples with low reward variance theoretically reduces the expected policy gradient norm, leading to more stable optimization. The authors provide theoretical reduction arguing that filtering low-variance samples is equivalent to weighting the policy gradient, resulting in lower expected gradient norm. High-variance samples produce gradients that are "constructive" rather than noisy, and lower gradient norm oscillations correlate with better convergence. This stability comes from focusing on samples that provide the most informative updates rather than noisy extremes.

### Mechanism 3: Replay Learning for Curriculum Consistency
A memory bank of high-variance samples ensures training efficiency by replacing filtered-out samples with previously identified high-value queries. When a batch sample falls below the variance threshold, VCRL replaces it by sampling from a priority queue that stores queries yielding high variance. This maintains a consistent curriculum of challenging material even as the model evolves. The memory bank provides momentum, ensuring that samples which were recently valuable remain in the training distribution for a few more steps.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: VCRL is implemented on top of GRPO. Understanding how GRPO estimates advantages using group statistics is essential to see why adding variance as a filter is a natural extension.
  - Quick check: How does GRPO calculate the advantage $\hat{A}$ for a specific rollout without a value function?

- **Concept: Reinforcement Learning with Verifiable Rewards (RLVR)**
  - Why needed: The core premise relies on binary reward signals derived from a verifier. The variance heuristic fails without this binary or near-binary reward structure.
  - Quick check: Why does RLVR typically result in sparse rewards, and how does that impact variance calculation?

- **Concept: Curriculum Learning**
  - Why needed: VCRL is a dynamic curriculum approach. You need to distinguish between static curriculum (pre-sorting data) and dynamic curriculum (sorting based on online model feedback).
  - Quick check: Why does the paper argue that pre-sorting training samples by fixed difficulty is ineffective for LLM RL?

## Architecture Onboarding

- **Component map:** Rollout Engine -> Verifier -> Variance Calculator -> Filter & Replace Module -> Memory Bank -> Optimizer
- **Critical path:**
  1. Sample batch $x \sim D$
  2. Generate rollouts and calculate rewards
  3. Calculate $p$ (variance) immediately
  4. Filter batch: Keep if $p \ge \kappa$; else drop
  5. Fill dropped slots by popping from Memory Bank $\mathcal{M}$
  6. Update model; push high-$p$ dropped samples to $\mathcal{M}$
- **Design tradeoffs:**
  - Compute vs. Quality: Calculating variance requires completing rollouts for samples that may ultimately be discarded
  - Stability vs. Exploration: High threshold $\kappa$ creates strict curriculum (stable gradients) but may limit diversity
- **Failure signatures:**
  - Empty Bank: If $\kappa$ is too high early in training, no samples qualify, causing batch underflow
  - Gradient Collapse: If model overfits to Memory Bank samples, entropy might drop too low
- **First 3 experiments:**
  1. Threshold Sensitivity: Sweep $\kappa$ on small validation set to find optimal difficulty sweet spot
  2. Variance Distribution Visualization: Plot $p$ scores over training steps to verify model progression
  3. Ablation (Static vs. Dynamic): Compare VCRL against pre-sorted difficulty baseline to prove dynamic filtering value

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the variance-based difficulty metric be effectively adapted for tasks with dense, continuous, or non-binary reward signals?
- Basis in paper: The derivation of the normalized variance metric ($p$) explicitly relies on binary reward distribution ($r \in \{0,1\}$), and the method is tested exclusively on mathematical reasoning tasks with verifiable binary outcomes.
- Why unresolved: The current formulation cannot directly calculate "uncertainty" or "moderate difficulty" when rewards are scalar or partial, potentially limiting applicability to broader RL scenarios.
- What evidence would resolve it: Theoretical extension of variance-to-difficulty mapping for continuous distributions, or empirical results on benchmarks using Process Reward Models or continuous scores.

### Open Question 2
- Question: Can the variance threshold $\kappa$ be determined automatically or adaptively rather than through manual scheduling?
- Basis in paper: The implementation details state that threshold $\kappa$ is manually set to specific values at specific steps, without providing mechanism for automatic tuning or theoretical justification.
- Why unresolved: Optimal threshold likely depends on current state of model's convergence and specific dataset, meaning fixed manual schedules may be suboptimal.
- What evidence would resolve it: Experiments demonstrating adaptive $\kappa$ schedule that matches or outperforms fixed schedule, or sensitivity analysis showing performance degradation across different manual settings.

### Open Question 3
- Question: Does the variance-based curriculum approach transfer effectively to complex agentic tasks where difficulty is harder to define, such as tool use or search?
- Basis in paper: The introduction explicitly states that "Some tasks, like search... and tool use... are hard to define by difficulty," and experiments are strictly confined to mathematical reasoning benchmarks.
- Why unresolved: While math allows for clear binary success/failure, agentic tasks often involve sparse rewards, multi-turn dependencies, or environment non-determinism, which might decouple link between group reward variance and actual sample utility.
- What evidence would resolve it: Evaluation of VCRL on standard agentic benchmarks (e.g., WebShop, ALFWorld) or code generation tasks (HumanEval) to verify if high variance correlates with learning value in these domains.

## Limitations

- Variance metric relies heavily on binary rewards, making effectiveness uncertain for tasks with continuous or multi-class rewards
- Theoretical stability guarantee is asymptotic and may not translate to practical convergence improvements
- Memory Bank mechanism lacks comprehensive ablation to isolate its exact contribution from variance filtering itself

## Confidence

- **High:** Core experimental results showing VCRL outperforming GRPO baselines on all five benchmarks, particularly the 24.8-point improvement on AIME datasets
- **Medium:** The variance-as-difficulty proxy mechanism, which is intuitively sound but may not generalize beyond binary reward settings
- **Low:** The theoretical stability claim, which provides an asymptotic bound but doesn't prove practical convergence benefits over existing methods

## Next Checks

1. **Reward Structure Sensitivity:** Test VCRL on a dataset with continuous rewards (e.g., step-by-step reasoning scores) to determine if variance-based filtering breaks down when rewards aren't binary

2. **Memory Bank Ablation:** Run experiments comparing VCRL with and without the Memory Bank component to quantify its exact contribution to the reported improvements

3. **Generalization Test:** Evaluate trained VCRL models on completely unseen mathematical reasoning datasets to verify the curriculum learning approach generalizes beyond the training distribution