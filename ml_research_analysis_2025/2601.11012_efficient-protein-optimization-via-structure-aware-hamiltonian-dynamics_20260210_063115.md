---
ver: rpa2
title: Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics
arxiv_id: '2601.11012'
source_url: https://arxiv.org/abs/2601.11012
tags:
- protein
- fitness
- sequence
- structure
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes HADES, a Bayesian optimization method for protein
  engineering that combines Hamiltonian dynamics with structure-aware modeling. The
  key idea is to use Hamiltonian Monte Carlo sampling in a continuous sequence space
  while leveraging structural information as prior knowledge to guide exploration.
---

# Efficient Protein Optimization via Structure-aware Hamiltonian Dynamics

## Quick Facts
- **arXiv ID:** 2601.11012
- **Source URL:** https://arxiv.org/abs/2601.11012
- **Authors:** Jiahao Wang; Shuangjia Zheng
- **Reference count:** 12
- **Key outcome:** HADES achieves perfect GB1 fitness scores (1.00±0.00) and superior performance on PhoQ, outperforming baselines across multiple metrics.

## Executive Summary
This paper proposes HADES, a Bayesian optimization method for protein engineering that combines Hamiltonian Monte Carlo sampling with structure-aware modeling. The key innovation is using Hamiltonian dynamics in a continuous sequence space while leveraging structural information as prior knowledge to guide exploration. HADES employs a two-stage encoder-decoder framework to learn structure-function relationships and uses uncertainty estimation to balance exploration and exploitation. Experimental results on GB1 and PhoQ datasets demonstrate HADES outperforms state-of-the-art baselines, achieving perfect fitness scores on GB1 and maintaining superior functional diversity across query sizes.

## Method Summary
HADES is a Bayesian optimization framework for protein engineering that uses Hamiltonian Monte Carlo sampling in a continuous sequence space. The method features a two-stage encoder-decoder surrogate model where the encoder first learns structural relationships (predicting RMSD from ESMFold), then predicts fitness with frozen encoder weights. Hamiltonian dynamics with virtual barriers and Metropolis-Hastings correction enable valid discrete sequence generation from continuous states. The system balances exploration and exploitation through uncertainty-aware sampling, achieving efficient optimization with reduced wet-lab costs.

## Key Results
- On GB1 dataset: HADES achieves maximum fitness of 1.00±0.00 (perfect score) compared to 0.93±0.14 for second-best method
- On PhoQ dataset: HADES achieves 0.80±0.25 for maximum fitness versus 0.72±0.24 for second-best method
- HADES demonstrates superior functional diversity (fDiv) scores and maintains stable performance across different query sizes (K∈{16, 128})

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Momentum-based sampling allows the system to traverse rugged fitness landscapes and escape local optima more effectively than gradient descent or random walks.
- **Mechanism:** By treating the protein sequence as a position q in a continuous space and introducing an auxiliary momentum variable p, HADES simulates Hamiltonian dynamics. The gradient of the surrogate fitness model acts as a force (potential energy U), accelerating the "particle" through low-fitness valleys and carrying it over epistatic peaks via kinetic energy, enabling large jumps between distant sequence regions.
- **Core assumption:** The fitness landscape, while rugged, possesses underlying smooth structure that can be approximated by a continuous potential energy function, allowing momentum to accumulate meaningfully rather than oscillating wildly.
- **Evidence anchors:** [abstract]: "Leveraging momentum and uncertainty in the simulated physical movements, HADES enables rapid transition of proposals toward promising areas." [section]: "In high-dimensional spaces, MCMC with Hamiltonian dynamics is an effective way to make large jumps away from the initial point... A key to its usefulness is that Hamiltonian dynamics preserves volume." [corpus]: Neighbor papers discuss Hamiltonian dynamics in control and GP contexts (e.g., "Optimal Control of Probabilistic Dynamics Models"), supporting the general efficacy of energy-based sampling in continuous spaces.

### Mechanism 2
- **Claim:** Pre-training the surrogate model on structural deviations (RMSD) smooths the fitness landscape, improving gradient signal-to-noise ratio.
- **Mechanism:** The surrogate model is trained in two stages. First, the encoder learns latent representations to predict structural perturbation (RMSD) relative to the wild type via ESMFold. Second, the encoder is frozen, and a fitness decoder is trained. This enforces a latent space where "neighbors" are structurally similar, effectively filtering out sequence variations that would result in physically implausible (broken) structures before fitness is considered.
- **Core assumption:** Structural viability is a prerequisite for function, and the distribution of structural perturbations serves as a strong prior for the fitness landscape's geometry.
- **Evidence anchors:** [abstract]: "The posterior surrogate is powered by a two-stage encoder-decoder framework to determine the structure and function relationships... consequently learning a smoothed landscape to sample from." [section]: "Our assumption is that understanding this distribution of structural distances provides prior knowledge of how mutations reflect the structural changes, which benefits the gradient calculation..." [corpus]: Weak direct evidence in neighbors; however, related work on "Structure-informed language models" generally supports the utility of structural priors.

### Mechanism 3
- **Claim:** Virtual barriers and a Metropolis-Hastings acceptance step enable valid discrete sequence generation from a continuous dynamical system.
- **Mechanism:** As Hamiltonian dynamics update the continuous position q, components may drift outside the valid probability bounds [0,1]. The system implements "virtual barriers" that reverse momentum upon boundary collision (bouncing). Finally, the continuous q is discretized to a one-hot sequence via argmax, and a Metropolis step corrects for the approximation error between the continuous trajectory and the discrete target distribution.
- **Core assumption:** The energy landscape of the continuous relaxation is sufficiently correlated with the discrete sequence landscape such that continuous trajectories map to valid discrete mutations.
- **Evidence anchors:** [abstract]: "A position discretization procedure is introduced to propose discrete protein sequences from such a continuous state system." [section]: "This procedure establishes two virtual barriers during the movement... Metropolis sampling is adopted to reject the samples with large discretization error." [corpus]: N/A (No specific corpus support for this specific discretization technique in protein optimization found).

## Foundational Learning

- **Concept:** Hamiltonian Monte Carlo (HMC)
  - **Why needed here:** This is the core acquisition engine. Without understanding how momentum couples with gradients to propose distant samples, one cannot debug why the algorithm might be oscillating or moving too slowly.
  - **Quick check question:** If the step size ε is too large, does the energy error increase or decrease, and how does that affect the Metropolis acceptance rate?

- **Concept:** Transfer Learning / Multi-task Learning
  - **Why needed here:** The two-stage surrogate relies on transferring structural knowledge to the fitness task. Understanding feature freezing and domain shifts is required to diagnose why the fitness decoder might be overfitting.
  - **Quick check question:** Why are the encoder weights frozen during the second stage of training (fitness prediction)?

- **Concept:** Bayesian Optimization (BO) & Upper Confidence Bound (UCB)
  - **Why needed here:** HADES sits within a BO loop. Understanding the explore-vs-exploit tradeoff managed by the UCB (mean + uncertainty) is critical for interpreting why the model selects seemingly suboptimal candidates.
  - **Quick check question:** How does the ensemble variance act as a proxy for uncertainty, and how does the UCB formulation specifically encourage sampling in unexplored regions?

## Architecture Onboarding

- **Component map:** Input (Wild-type sequence) -> Surrogate Model (Encoder -> Decoder 1 (Structure) -> Decoder 2 (Fitness)) -> Sampler (HMC Core -> Discretizer -> Validator) -> Selector (UCB ranking)
- **Critical path:** The coupling between the Encoder's latent space quality and the HMC's gradient stability. If the Encoder learns a jagged latent space (due to poor structure pre-training), the HMC gradients will be noisy, leading to low acceptance rates or erratic trajectories.
- **Design tradeoffs:**
  - **HMC vs. LMC (HADES vs. HADES-L):** Full HMC allows longer trajectories (better exploration) but is computationally more expensive per sample than Langevin (single step).
  - **Continuous vs. Discrete:** Relaxing to continuous space enables gradient-based HMC but requires the "Virtual Barrier" and discretization overhead to remain valid.
- **Failure signatures:**
  - **Low Acceptance Rate:** Suggests discretization error is too high or the surrogate landscape is too steep (step size ε too large).
  - **Mode Collapse (Low fDiv):** Surrogate ensemble has low variance; uncertainty estimation fails to encourage exploration.
  - **High Variance in Fitness:** The structure prior is likely misaligned with the specific fitness task (as seen partially in the PhoQ results).
- **First 3 experiments:**
  1. **Surrogate Sanity Check:** Train only the Structure Decoder and visualize the latent space (t-SNE/PCA) to confirm that RMSD distances correlate with latent distances.
  2. **Trajectory Visualization:** Run the HMC sampler with a fixed seed and visualize the continuous trajectory q hitting the virtual barriers to ensure the "bouncing" mechanics are correctly constraining values to [0,1].
  3. **Ablation on Query Size:** Replicate the K∈{16, 128} experiment to verify that the scaling advantage holds, confirming the system implementation matches the paper's efficiency claims.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HADES perform on multi-objective protein optimization tasks compared to single-objective settings?
- Basis in paper: [explicit] The conclusion states: "Future efforts will focus on... designing proteins that optimize multiple objectives."
- Why unresolved: Current experiments only evaluate single fitness metrics on GB1 and PhoQ datasets. Real protein engineering often requires balancing competing objectives (stability, activity, expression).
- What evidence would resolve it: Benchmark results on multi-objective protein design tasks showing Pareto front coverage and trade-off handling.

### Open Question 2
- Question: How does HADES transfer to wet-lab experimental validation versus in-silico oracle evaluation?
- Basis in paper: [inferred] All experiments use computational oracles; the method mentions "limited wet-lab costs" but validates only in-silico.
- Why unresolved: Laboratory validation introduces experimental noise, batch effects, and biological constraints not captured by computational fitness functions.
- What evidence would resolve it: Prospective wet-lab experiments showing discovered variants achieve predicted fitness improvements.

### Open Question 3
- Question: How does optimization performance degrade as the number of mutation sites increases beyond four?
- Basis in paper: [inferred] Both benchmark datasets cover only ~20^4 combinations (4-site saturation mutagenesis). Real proteins often require exploring 10+ positions simultaneously.
- Why unresolved: Hamiltonian dynamics in higher-dimensional sequence spaces may face acceptance rate challenges, and the surrogate model may struggle to learn smooth landscapes with more mutation combinations.
- What evidence would resolve it: Systematic evaluation on datasets with 6, 8, or more mutation sites, comparing sample efficiency scaling.

### Open Question 4
- Question: How dependent is HADES on ESMFold prediction accuracy, and would alternative structure predictors improve performance?
- Basis in paper: [inferred] Ablation shows structure removal has "relatively modest impact" possibly "due to generalization error of ESMFold."
- Why unresolved: The structure decoder relies entirely on ESMFold predictions as ground truth. If these predictions are noisy or biased, the structure prior may provide limited guidance.
- What evidence would resolve it: Ablation experiments using alternative structure predictors (AlphaFold2, RoseTTAFold) or experimentally determined structures.

## Limitations

- **Discrete-to-continuous approximation quality**: The Metropolis correction is critical but its efficacy depends on how well the continuous surrogate landscape maps to the discrete sequence space. High discretization error could systematically bias sampling.
- **Structure predictor reliability**: HADES relies on ESMFold for structural prior information. While ESMFold is state-of-the-art, its generalization to all protein families is not perfect, and systematic errors in the structure predictor could propagate to the surrogate model.
- **Scalability concerns**: The paper reports strong results on GB1 and PhoQ datasets, but these are relatively small proteins. The computational overhead of Hamiltonian dynamics (leapfrog steps) versus simpler alternatives remains unclear for larger proteins.

## Confidence

- **High confidence**: The mechanism by which Hamiltonian dynamics enables large jumps across rugged fitness landscapes is well-established in the literature and mathematically sound.
- **Medium confidence**: The effectiveness of structure-aware pre-training in smoothing the surrogate landscape is demonstrated empirically, though the modest impact of structure ablation suggests potential limitations in the current implementation.
- **Medium confidence**: The claim that HADES maintains stable performance across different query sizes is supported by experiments, though more extensive testing on larger proteins would strengthen this.

## Next Checks

1. **Discretization error analysis**: Systematically vary the Metropolis acceptance threshold and measure how it affects the final fitness scores to quantify the impact of the continuous-discrete approximation.

2. **Cross-protein validation**: Test HADES on a structurally diverse set of proteins beyond GB1 and PhoQ to evaluate generalizability across different fold families and functional constraints.

3. **Computational overhead benchmarking**: Compare the wall-clock time per sample for HADES versus HADES-L and other baselines on proteins of varying length to quantify the efficiency tradeoff of full Hamiltonian dynamics.