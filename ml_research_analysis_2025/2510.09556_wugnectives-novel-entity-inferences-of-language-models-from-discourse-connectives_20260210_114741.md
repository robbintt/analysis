---
ver: rpa2
title: 'WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives'
arxiv_id: '2510.09556'
source_url: https://arxiv.org/abs/2510.09556
tags:
- connectives
- stimuli
- discourse
- language
- qwen2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WUGNECTIVES, a benchmark that tests whether
  language models can make inferences about novel entities based solely on the functional
  meaning of discourse connectives, without relying on pre-existing world knowledge.
  Using 8,880 stimuli with nonce words linked by connectives, the authors evaluate
  17 models and find that models generally succeed on temporal and causal inferences
  but systematically fail on concessive connectives.
---

# WUGNECTIVES: Novel Entity Inferences of Language Models from Discourse Connectives
## Quick Facts
- arXiv ID: 2510.09556
- Source URL: https://arxiv.org/abs/2510.09556
- Reference count: 28
- Language models can reason about novel entities using discourse connectives but systematically fail on concessive relations

## Executive Summary
This paper introduces WUGNECTIVES, a benchmark designed to evaluate whether language models can make inferences about novel entities based solely on the functional meaning of discourse connectives, without relying on pre-existing world knowledge. Using 8,880 stimuli with nonce words linked by connectives, the authors test 17 models across eight different connective senses. The results reveal that models generally succeed on temporal and causal inferences but systematically fail on concessive connectives. Reasoning-based tuning improves performance across most senses except concession, while instruction tuning and model scale show limited effects. A post-hoc experiment replacing nonce words with real-world entities confirms that world knowledge boosts performance, though concessive connectives remain difficult.

## Method Summary
The authors created WUGNECTIVES using nonce words paired with discourse connectives to isolate connective reasoning from world knowledge. They generated 8,880 stimuli covering eight connective senses (causal, concessive, conditional, sequential, synchronous, substitution, alternative, and coordination) with 10 nonce words per connective. The stimuli followed the format "nonce1 connective nonce2" and were evaluated across 17 models including base and tuned variants. Models were tested on their ability to predict the second nonce word given the first and the connective. The authors also conducted a post-hoc experiment replacing nonce words with real-world entities to examine the role of world knowledge. Additionally, they compared performance between models with different tuning strategies (instruction tuning, reasoning tuning, base models) and across different model scales.

## Key Results
- Models perform well on temporal and causal connective inferences but systematically fail on concessive connectives
- Reasoning-based tuning improves performance across most connective senses except concession
- World knowledge significantly boosts performance, but concessive connectives remain challenging even with real-world entities

## Why This Works (Mechanism)
The benchmark's use of nonce words effectively isolates connective reasoning from world knowledge, allowing researchers to test whether models understand the functional meaning of connectives independently. By controlling for lexical content, the authors can determine if performance differences across connective types reflect genuine limitations in connective understanding rather than differences in world knowledge requirements.

## Foundational Learning
- Discourse connectives: Words that signal relationships between clauses (why needed: core concept being tested; quick check: identify connectives in sample sentences)
- Nonce words: Artificial words with no pre-existing meaning (why needed: controls world knowledge; quick check: distinguish from real words)
- Concessive connectives: Connectives expressing contrast or unexpected relationships (why needed: reveals systematic model failure; quick check: classify connective types)
- Reasoning tuning: Training that explicitly targets logical inference capabilities (why needed: shows targeted improvements; quick check: compare tuned vs base model performance)
- Stimulus generation: Creating controlled test examples (why needed: ensures experimental validity; quick check: verify stimulus format consistency)
- Post-hoc analysis: Follow-up experiments after main results (why needed: explores unexpected findings; quick check: confirm additional experiment design)

## Architecture Onboarding
Component map: Nonce word generation -> Connective selection -> Stimulus formatting -> Model input -> Prediction output -> Performance evaluation

Critical path: The benchmark measures whether models can predict the second nonce word given the first nonce word and connective, isolating connective meaning from world knowledge.

Design tradeoffs: Using nonce words controls for world knowledge but may create unnatural test conditions that don't reflect real-world language processing.

Failure signatures: Systematic failure on concessive connectives across all models suggests a fundamental limitation in how LMs process contrast relationships, rather than issues with specific model architectures or training approaches.

First experiments:
1. Test a simple model on basic causal connectives to establish baseline performance
2. Compare instruction-tuned vs base models on temporal connectives
3. Evaluate reasoning-tuned models on concessive connectives to confirm persistent failure

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental design using nonce words may not fully capture how models process connectives in naturalistic contexts
- Benchmark focuses on a limited set of connectives, potentially missing important variation in connective function
- Post-hoc world-knowledge experiment uses different stimulus format that may not be directly comparable to main results

## Confidence
- Connective reasoning can be evaluated independently of world knowledge: Medium confidence
- Models show systematic failure on concessive connectives: High confidence
- Reasoning tuning improves performance except for concession: Medium confidence

## Next Checks
1. Conduct a controlled experiment varying nonce word properties (semantic transparency, morphological complexity) to determine whether performance differences across connectives are due to the connectives themselves or properties of the nonce words used.

2. Test the same models on naturally occurring text containing concessive connectives, controlling for world knowledge effects, to verify whether the concessive failure pattern persists in more realistic contexts.

3. Perform ablation studies on the reasoning-tuned models to identify which specific components of the tuning procedure drive improvements in connective reasoning versus general performance gains.