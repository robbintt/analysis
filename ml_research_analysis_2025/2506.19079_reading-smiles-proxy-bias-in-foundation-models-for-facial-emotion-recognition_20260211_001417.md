---
ver: rpa2
title: 'Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition'
arxiv_id: '2506.19079'
source_url: https://arxiv.org/abs/2506.19079
tags:
- teeth
- facial
- arxiv
- emotion
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how Vision-Language Models (VLMs) infer
  emotions in facial images, focusing on whether they rely on superficial visual cues
  rather than psychologically grounded affective signals. Using a teeth-annotated
  subset of AffectNet, we benchmarked 10 VLMs and a supervised baseline, revealing
  that teeth visibility significantly impacts model performance, with some models
  dropping by up to 12% UAR when teeth are not visible.
---

# Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition

## Quick Facts
- arXiv ID: 2506.19079
- Source URL: https://arxiv.org/abs/2506.19079
- Reference count: 40
- Primary result: Vision-Language Models rely on superficial visual cues like teeth visibility rather than psychologically grounded affective signals for facial emotion recognition.

## Executive Summary
This study investigates how Vision-Language Models (VLMs) infer emotions in facial images, focusing on whether they rely on superficial visual cues rather than psychologically grounded affective signals. Using a teeth-annotated subset of AffectNet, we benchmarked 10 VLMs and a supervised baseline, revealing that teeth visibility significantly impacts model performance, with some models dropping by up to 12% UAR when teeth are not visible. GPT-4o, the best-performing model, was further analyzed using a structured introspection prompt, showing that features like eyebrow position and teeth visibility strongly influence its valence and arousal predictions. Regression analysis revealed that over 70% of GPT-4o's affective output variance could be explained by these features, highlighting its internal consistency. However, the model's tendency to default to high-valence emotions when teeth are hallucinated raises concerns about shortcut learning and fairness in deployment. These findings underscore the need for transparent evaluation methods in affective computing to ensure robust, unbiased emotion recognition.

## Method Summary
The study uses a manually annotated subset of 3,500 images from AffectNet validation set, where each image is labeled for binary teeth visibility. Ten VLMs (including GPT-4o and InternVL2) are evaluated using zero-shot inference with structured prompts. GPT-4o uses a JSON prompt to extract emotion, facial features (eyebrows, mouth), and valence/arousal, while other VLMs use simplified prompts. Performance is measured using Unweighted Average Recall (UAR), F1-score, and accuracy. For GPT-4o, linear regression analysis quantifies how self-reported features explain valence/arousal predictions.

## Key Results
- Teeth visibility significantly impacts model performance, with some models dropping by up to 12% UAR when teeth are not visible.
- GPT-4o achieves highest performance but shows strong reliance on proxy features like teeth and eyebrow position.
- Over 70% of GPT-4o's affective output variance can be explained by self-reported features, indicating internal consistency.
- When teeth are hallucinated, UAR collapses to .329 and predictions default to high-valence emotions like "Happiness."

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Vision-Language Models (VLMs) utilize "proxy features," specifically teeth visibility, as shortcuts for emotion classification rather than relying on holistic affective signals.
- **Mechanism:** The model learns a strong statistical correlation between the visual presence of teeth and the "Happiness" category. This creates a unidirectional inference path where `Teeth → Smile → Happiness`, overriding other affective cues.
- **Core assumption:** The model's training data contains a biased distribution where specific visual attributes (teeth) are disproportionately represented in high-valence classes.
- **Evidence anchors:**
  - [abstract] The study reveals models rely on "superficial visual cues" rather than psychologically grounded signals.
  - [section IV-C] The dataset shows a 3.4:1 ratio of visible teeth in Happiness images; Fig. 5 shows a distinct UAR drop when teeth are absent.
  - [corpus] The paper "Attributes-aware Visual Emotion Representation Learning" supports the challenge of distinctive feature learning in visual emotion analysis, though the specific "teeth bias" mechanism is unique to this study's annotation focus.
- **Break condition:** If a model is trained or fine-tuned on a dataset where teeth visibility is decorrelated from valence (e.g., "sad" expressions with visible teeth), this shortcut should diminish.

### Mechanism 2
- **Claim:** Structured introspection via prompting can expose a model's internal "world model" or reasoning consistency, revealing that outputs are deterministic functions of extracted features.
- **Mechanism:** By forcing the VLM to output discrete feature states (e.g., "eyebrow position: raised") alongside continuous scores (valence/arousal), one can regression-test the model's logic. High $R^2$ values indicate the model isn't hallucinating outputs randomly but mapping features to scores consistently.
- **Core assumption:** The textual "reasoning" provided by the VLM (e.g., JSON features) faithfully represents the actual latent variables influencing the prediction.
- **Evidence anchors:**
  - [section IV-B] Linear regression explains over 70% of GPT-4o's variance ($R^2 \approx 0.72-0.77$) using self-reported features.
  - [table I] "Eyebrow position Raised" has a +.64 coefficient for valence, quantifying the feature-to-output mapping.
  - [corpus] Related work "From Coarse to Nuanced" suggests aligning fine-grained cues with visual regions, aligning with the utility of structured feature extraction.
- **Break condition:** If the model produces features that contradict the final prediction (e.g., claims "eyebrows furrowed" but predicts high valence), the introspection mechanism fails to reflect true internal state.

### Mechanism 3
- **Claim:** "Hallucinated" proxy features trigger systematic misclassification, causing the model to default to high-valence predictions regardless of ground truth.
- **Mechanism:** If the visual system erroneously detects a proxy (e.g., hallucinates teeth due to lighting or mouth shape), the "shortcut" mechanism activates, forcing a high-valence prediction. The error propagates from perception to semantic reasoning.
- **Core assumption:** The VLM cannot self-correct when a perceptual error conflicts with other facial cues; the proxy feature dominates the decision hierarchy.
- **Evidence anchors:**
  - [section IV-D] In "Teeth Hallucinated" cases, UAR collapses to .329, and the confusion matrix (Fig. 6) shows a collapse to "Happiness."
  - [abstract] Mentions "concerns about shortcut learning" when teeth are hallucinated.
  - [corpus] Evidence regarding hallucination propagation in VLMs is not explicitly covered in the provided neighbor corpus (focus is on recognition accuracy/datasets).
- **Break condition:** Implementation of a "sanity check" module or ensemble that down-weights specific proxy features if other facial action units (e.g., brow shape) contradict the proxy's implication.

## Foundational Learning

- **Concept: Clever Hans Effect / Shortcut Learning**
  - **Why needed here:** This is the central theoretical framework of the paper. You cannot interpret VLM performance without understanding that high accuracy might stem from exploiting simple correlations (teeth) rather than complex reasoning.
  - **Quick check question:** If I obscure the mouth region of a "Happy" image, does the model's accuracy drop significantly? (If yes, it likely relied on a mouth-based shortcut).

- **Concept: Dimensional Emotion Space (Valence-Arousal)**
  - **Why needed here:** The paper moves beyond categorical labels (Happy/Sad) to continuous dimensions. Understanding that "Happiness" = High Valence + High Arousal is required to interpret the regression coefficients (e.g., why raised eyebrows correlate with valence).
  - **Quick check question:** Does "Surprise" typically sit in high or low valence? (Trick question—it is high arousal but neutral/variable valence, which explains why models confuse it with Happiness when shortcuts trigger).

- **Concept: Zero-Shot Generalization**
  - **Why needed here:** The tested VLMs were not trained explicitly on AffectNet. The mechanisms observed (shortcuts) are "emergent" from generic pre-training, making this analysis a test of how generic visual knowledge transfers to specific affective tasks.
  - **Quick check question:** How does a model recognize "Anger" without explicit training on anger labels? (It must map visual features like "furrowed brows" to semantic concepts learned from text/image pairs).

## Architecture Onboarding

- **Component map:** Input (Facial Image + Structured Prompt) -> VLM Core (e.g., GPT-4o, InternVL) -> Introspection Layer (JSON extraction) -> Analysis (Regression)
- **Critical path:** The structured prompt (Fig 2) is the most critical component. It transforms the VLM from a "black box classifier" into a "self-explaining system." If the prompt fails to constrain the output to valid JSON features, the regression analysis (Mechanism 2) becomes impossible.
- **Design tradeoffs:**
  - **Closed vs. Open Models:** The paper relies on GPT-4o for best results, but acknowledges it is a "black box" subject to API changes (non-reproducible). Open models (InternVL) offer reproducibility but lower performance.
  - **Binary vs. Granular Annotation:** The study uses binary teeth annotation (Visible/Not) for clarity, sacrificing nuance (partially visible teeth) which might be where hallucinations occur most frequently.
- **Failure signatures:**
  - **False Positive Spiral:** Model hallucinates teeth → Predicts "Happiness" → High Confidence.
  - **Feature-Prediction Mismatch:** Low valence prediction despite "Teeth Visible: Yes" (Indicates a break in the shortcut mechanism or prompt misunderstanding).
- **First 3 experiments:**
  1. **Mouth Occlusion Ablation:** Run the benchmark dataset with black boxes over the mouth region to verify if the "teeth shortcut" is the primary driver of performance (UAR should drop if the model is over-reliant).
  2. **Synthetic Counter-Example Generation:** Create synthetic images where "Sad" eyes are paired with "Smiling" mouths (visible teeth) to measure which cue dominates the prediction.
  3. **Hallucination Audit:** Run the "Teeth Hidden" subset through GPT-4o and manually review the "Teeth Visible: Yes" false positives to identify visual patterns (e.g., specific lighting or lip colors) that trigger the hallucination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do VLMs exhibit similar teeth-proxy biases across diverse cultural and demographic groups outside the AffectNet distribution?
- Basis in paper: [explicit] The authors caution that performance may degrade in underrepresented populations due to the "limited demographic diversity of AffectNet" and advise against overextending claims to all deployment environments.
- Why unresolved: The study utilized AffectNet, which, despite being large, may not capture the full spectrum of global facial expressions or cultural display rules regarding teeth visibility.
- What evidence would resolve it: Benchmarking these models on culturally diverse datasets with specific annotations for expression nuances and teeth visibility to verify cross-demographic robustness.

### Open Question 2
- Question: To what extent does the inclusion of AffectNet or similar benchmarks in pre-training corpora inflate the reported zero-shot performance of VLMs?
- Basis in paper: [explicit] The authors note that "many FMs are trained on undisclosed or proprietary datasets," making it "unclear whether AffectNet... was present in their pretraining corpora," limiting the conclusiveness of zero-shot evaluations.
- Why unresolved: Without transparent training data cards for proprietary models like GPT-4o, it is impossible to distinguish between genuine affective reasoning and memorization of specific dataset patterns.
- What evidence would resolve it: Testing models on novel, out-of-distribution datasets collected after the model's training cutoff date, or analyzing open-weights models with known training compositions.

### Open Question 3
- Question: Do other facial features, such as forehead wrinkles or eyebrow position, cause similar systematic "shortcut" hallucinations when artificially decoupled from the target emotion?
- Basis in paper: [inferred] While regression analysis identified features like "eyebrow position" and "forehead wrinkles" as strong predictors of valence, the study only annotated and intervened on "teeth visibility" to demonstrate shortcut learning.
- Why unresolved: It remains unclear if the model defaults to other high-valence or high-arousal heuristics (e.g., assuming raised eyebrows always equal surprise) in the absence of those specific cues.
- What evidence would resolve it: Occlusion or manipulation studies targeting non-teeth features (e.g., digitally removing forehead wrinkles) to measure resulting performance drops or bias shifts.

## Limitations
- The study relies on a manually annotated subset of AffectNet, introducing potential annotation bias and limiting generalizability.
- Binary teeth visibility labels may oversimplify nuanced expressions and contribute to hallucination patterns.
- GPT-4o's closed-source nature raises reproducibility concerns due to API version changes.
- The regression analysis assumes self-reported features accurately reflect internal reasoning, which may not hold.

## Confidence
- **Teeth visibility as a proxy feature:** High
- **GPT-4o's internal consistency:** Medium
- **Hallucination-driven misclassification:** Low-Medium

## Next Checks
1. **Synthetic Counter-Example Generation:** Create synthetic images with mismatched facial cues (e.g., sad eyes with visible teeth) to test which features dominate model predictions and quantify the teeth shortcut's strength.

2. **Mouth Occlusion Ablation:** Systematically occlude the mouth region in test images to measure if UAR drops significantly, confirming over-reliance on teeth-based shortcuts.

3. **Hallucination Pattern Audit:** Analyze false positives in the "Teeth Hidden" subset where GPT-4o hallucinates teeth, identifying visual patterns (e.g., lighting, lip colors) that trigger this bias.