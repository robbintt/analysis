---
ver: rpa2
title: Towards Flash Thinking via Decoupled Advantage Policy Optimization
arxiv_id: '2510.15374'
source_url: https://arxiv.org/abs/2510.15374
tags:
- reasoning
- length
- depo
- accuracy
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DEPO, a novel reinforcement learning algorithm
  designed to mitigate overthinking in Large Reasoning Models (LRMs). DEPO addresses
  the problem of excessively lengthy and redundant reasoning trajectories in LRMs
  by introducing a decoupled advantage computation method that differentiates between
  efficient and inefficient reasoning segments.
---

# Towards Flash Thinking via Decoupled Advantage Policy Optimization

## Quick Facts
- **arXiv ID:** 2510.15374
- **Source URL:** https://arxiv.org/abs/2510.15374
- **Authors:** Zezhong Tan; Hang Gao; Xinhong Ma; Feng Zhang; Ziqiang Dong
- **Reference count:** 11
- **Key outcome:** DEPO reduces redundant reasoning steps by more than 50% and achieves 39% reduction in sequence length while maintaining or slightly improving task accuracy compared to base model

## Executive Summary
This paper introduces DEPO (Decoupled Advantage Policy Optimization), a novel reinforcement learning algorithm designed to mitigate overthinking in Large Reasoning Models. DEPO addresses the problem of excessively lengthy and redundant reasoning trajectories by introducing a token-level advantage computation method that differentiates between efficient and inefficient reasoning segments. The approach uses a Generative Reward Model to identify the first reasoning step that leads to the correct answer, enabling explicit separation of reasoning trajectories into efficient and inefficient parts.

## Method Summary
DEPO is a reinforcement learning algorithm that optimizes reasoning efficiency by decoupling advantage computation at the token level. The method uses a Generative Reward Model (GRM) to identify the first reasoning step leading to the correct answer, then applies lower advantage values to subsequent inefficient tokens based on redundant reasoning patterns. It incorporates a difficulty-aware length penalty that adjusts based on problem difficulty, and uses advantage clipping to prevent reward fluctuations from distorting policy updates. The algorithm is implemented using the VeRL framework and trained on the DeepScaleR dataset.

## Key Results
- Achieves 39% reduction in sequence length while maintaining or slightly improving task accuracy
- Reduces redundant reasoning steps by more than 50% compared to base model
- Outperforms base model in overall accuracy with an average gain of 2.0%
- Shows 38.7% reduction in response length for DeepSeek-Distill-Qwen-7B

## Why This Works (Mechanism)

### Mechanism 1: Token-Level Advantage Decoupling
- **Claim:** If the reasoning trajectory is partitioned into "efficient" (pre-answer) and "inefficient" (post-answer/redundant) segments, applying lower advantage values to the latter can suppress overthinking while preserving the logic leading to the correct result.
- **Mechanism:** The method uses a Generative Reward Model (GRM) to identify the first token ($y_{ans}$) that derives the correct answer. It splits the rollout into efficient ($y_1 \dots y_{ans}$) and inefficient ($y_{ans+1} \dots$) segments. The advantage $\hat{A}_{i,t}$ for inefficient tokens is scaled by a function $f(\cdot)$ which decays based on the count of redundant reasoning steps ($K$).
- **Core assumption:** The GRM accurately identifies the *first* instance of correct reasoning, and tokens generated after this point contribute negligibly to task accuracy.
- **Evidence anchors:** [abstract]: "uses a Generative Reward Model (GRM) to identify the first reasoning step that leads to the correct answer, enabling the explicit separation..."

### Mechanism 2: Difficulty-Aware Length Penalty
- **Claim:** If the penalty for longer sequences is dynamically adjusted based on problem difficulty (inferred from the number of correct rollouts in a group), the model can reduce length on simple tasks without degrading performance on complex tasks requiring extended reasoning.
- **Mechanism:** The reward function includes a term $R_{length}$ that penalizes length. The strength of this penalty is inversely related to the count of correct responses ($\delta$) in the group. If many rollouts are correct (easy task), the length penalty is high; if few are correct (hard task), the penalty is relaxed.
- **Core assumption:** The number of correct responses in a rollout group ($\delta$) serves as a reliable proxy for problem difficulty.
- **Evidence anchors:** [abstract]: "incorporates a difficulty-aware length penalty... to lower the overall length of model responses."

### Mechanism 3: Advantage Clipping for Gradient Hygiene
- **Claim:** Clipping advantage values prevents the length penalty from inducing negative advantages for correct (but long) responses, which would otherwise push the model to avoid correct answers to save tokens.
- **Mechanism:** Before policy updates, advantages are clipped such that correct responses are bound to $[\min(\hat{A}'_{pos}), +\infty)$ and incorrect responses to $(-\infty, 0]$. This ensures that correct answers always yield positive advantages and incorrect answers always yield negative ones, regardless of length penalties.
- **Core assumption:** Preserving the sign of the reward signal (positive for correct, negative for incorrect) takes precedence over the magnitude of the length-based penalty for stable convergence.
- **Evidence anchors:** [abstract]: "...advantage clipping strategy to prevent reward fluctuations from distorting policy updates."

## Foundational Learning

- **Concept: Advantage Function (in RL)**
  - **Why needed here:** DEPO fundamentally modifies how "advantage" (the benefit of taking a specific action over the average action) is calculated for tokens. Understanding that standard GRPO assigns one advantage value per *sequence* is crucial to grasp why DEPO's *token-level* decoupling is novel.
  - **Quick check question:** In standard GRPO, do all tokens in a generated sequence receive the same advantage value? (Answer: Yes).

- **Concept: Generative Reward Models (GRM)**
  - **Why needed here:** Unlike rule-based verifiers (e.g., checking if LaTeX matches), a GRM is a generative model (here, Qwen-7B) prompted to output structured evaluation (Score, Reflection, Portion). This allows for semantic understanding of "where" the answer was found.
  - **Quick check question:** Can a rule-based checker identify *which specific sentence* in a Chain-of-Thought first derived the correct answer? (Answer: Usually no, they check final outputs).

- **Concept: Overthinking / Redundant Reasoning**
  - **Why needed here:** The paper defines overthinking not just as length, but as specific behaviors (self-reflection, verification) occurring *after* the correct answer is already derived.
  - **Quick check question:** Does DEPO define "inefficient tokens" as all tokens in a long response, or only those generated after the correct answer is reached? (Answer: The latter).

## Architecture Onboarding

- **Component map:**
  - Policy Model -> GRM -> Optimizer -> Policy Model

- **Critical path:**
  1. Rollout: Policy Model generates $G$ responses (rollouts) for a prompt.
  2. GRM Scoring: GRM processes rollouts to output: Score (0/1), Reflection (first correct sentence), and Portion.
  3. Segmentation: Split rollouts into efficient ($y \dots y_{ans}$) and inefficient ($y_{ans+1} \dots$) based on GRM's "Reflection" output.
  4. Advantage Calculation: Compute sequence-level advantages, then scale down advantages for inefficient segments using Eq. 4 (Decoupled Advantage) and Eq. 9 (Clipping).
  5. Policy Update: Update Policy Model to maximize DEPO objective (Eq. 3).

- **Design tradeoffs:**
  - **GRM Accuracy vs. Speed:** The paper notes GRM is slower and memory-intensive but more accurate than rules (Fig 2).
  - **Length vs. Accuracy:** Ablation studies (Table 2) show that aggressive length penalties alone hurt accuracy; the decoupled advantage is needed to maintain performance.

- **Failure signatures:**
  - **Early Stopping:** If GRM misidentifies the "first correct step" too early, the model may learn to output answers immediately without showing work.
  - **Repetition Loops:** If the "redundant reasoning matching" (counting $N$ and $X$ in Fig 3) fails to catch specific repetitive phrases, the model may get stuck in loops despite the penalty.
  - **Accuracy Collapse on Hard Tasks:** As noted in results, DEPO can slightly underperform naive GRPO on very challenging datasets (AIME) if the length optimization is too aggressive.

- **First 3 experiments:**
  1. **GRM Validation:** Before training, verify the GRM's accuracy in identifying the "first correct reasoning step" against a human-labeled gold set to ensure the segmentation signal is clean.
  2. **Ablation on Penalty Strength ($\alpha$):** Run sweeps on the length penalty hyperparameter $\alpha$ to find the "Pareto frontier" between length reduction and accuracy drop.
  3. **Visual Inspection of "Inefficient" Segments:** Generate samples and manually check if the tokens marked as "inefficient" (and thus penalized) are actually redundant verification or if they contain critical correction steps.

## Open Questions the Paper Calls Out

- **Question:** Can DEPO effectively generalize to non-mathematical domains such as logical reasoning or code generation?
  - **Basis in paper:** [explicit] Section 6 states that training was confined to mathematical datasets due to ease of verification, noting "Further studies are needed to evaluate the effectiveness of DEPO on other domains."
  - **Why unresolved:** The current verification mechanisms and the Generative Reward Model (GRM) training data are specifically tailored for mathematical problem-solving structures.
  - **What evidence would resolve it:** Applying DEPO to logical reasoning benchmarks (e.g., LogiQA) or coding tasks (e.g., HumanEval) and evaluating the trade-off between token reduction and functional correctness.

- **Question:** How does DEPO perform across diverse model architectures beyond the DeepSeek-Distill-Qwen family?
  - **Basis in paper:** [explicit] Section 6 highlights that experiments were limited to two specific models due to resource constraints and "variance of overthinking between different models."
  - **Why unresolved:** Different model architectures may exhibit distinct "overthinking" patterns or "aha-moments" that the current rule-based matching for redundant reasoning might fail to capture.
  - **What evidence would resolve it:** Experimental results applying DEPO to architectures like Llama or Mistral to verify if the decoupled advantage computation remains effective.

- **Question:** How does DEPO's performance depend on the accuracy and robustness of the Generative Reward Model (GRM)?
  - **Basis in paper:** [inferred] Section 6 states performance "critically depends on GRM quality," and Section 3.1 relies on the GRM to identify the exact split between efficient and inefficient reasoning.
  - **Why unresolved:** If the GRM hallucinates the first correct reasoning step or misidentifies the "Reflection" sentence, the decoupled advantage assignment could penalize efficient reasoning or reward inefficiency.
  - **What evidence would resolve it:** An ablation study analyzing training stability and final accuracy when synthetic noise is injected into the GRM's segmentation labels.

## Limitations
- **Domain specificity:** Performance may not generalize to non-mathematical domains where reasoning steps aren't as clearly separable into "before" and "after" the answer
- **Computational overhead:** GRM inference adds significant computational overhead compared to rule-based verification methods
- **Performance on hard problems:** Slight underperformance on very challenging datasets (AIME) when length optimization is too aggressive

## Confidence
- **High confidence:** Core claims of DEPO - that decoupled advantage computation with GRM-based segmentation reduces overthinking without sacrificing accuracy - are well-supported by controlled ablations and quantitative comparisons
- **Medium confidence:** Effectiveness hinges critically on GRM's accuracy in identifying the "first correct reasoning step," which wasn't independently validated against human judgment
- **Medium confidence:** Implementation complexity with unknown false positive/negative rates in regex-based redundant reasoning detection

## Next Checks
1. Verify GRM accuracy on a human-labeled validation set to ensure the "Reflection" boundary identification is reliable
2. Test DEPO on non-mathematical reasoning tasks to assess generalization beyond the DeepScaleR domain
3. Measure the actual computational overhead of GRM inference in production settings and compare against claimed benefits