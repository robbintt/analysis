---
ver: rpa2
title: 'SMITE: Enhancing Fairness in LLMs through Optimal In-Context Example Selection
  via Dynamic Validation'
arxiv_id: '2508.17735'
source_url: https://arxiv.org/abs/2508.17735
tags:
- test
- in-context
- examples
- fairness
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces SMITE, a novel method to enhance fairness
  in Large Language Models (LLMs) for tabular classification by strategically selecting
  in-context examples. SMITE employs a dynamic validation set, which evolves with
  the test set, to evaluate the quality of in-context demonstrations, unlike traditional
  static validation.
---

# SMITE: Enhancing Fairness in LLMs through Optimal In-Context Example Selection via Dynamic Validation

## Quick Facts
- **arXiv ID**: 2508.17735
- **Source URL**: https://arxiv.org/abs/2508.17735
- **Reference count**: 9
- **Primary result**: SMITE improves both predictive accuracy and fairness in tabular classification by using dynamic validation sets for in-context example selection, reducing Llama's error from 0.246 to 0.148 compared to random selection

## Executive Summary
This study introduces SMITE, a novel method to enhance fairness in Large Language Models (LLMs) for tabular classification by strategically selecting in-context examples. SMITE employs a dynamic validation set, which evolves with the test set, to evaluate the quality of in-context demonstrations, unlike traditional static validation. The algorithm iteratively selects examples to minimize both performance and fairness errors. Experiments across four LLMs on Adult and COMPAS datasets demonstrate that SMITE significantly improves both predictive accuracy and fairness compared to baseline methods like zero-shot learning, random selection, and RAG.

## Method Summary
SMITE enhances fairness in LLM-based tabular classification by selecting optimal in-context demonstrations through dynamic validation. For each test batch, it constructs attribute-matched support sets using embedding similarity, then partitions these into a proxy validation set (single nearest neighbor) and candidate pool. The algorithm iteratively builds core sets by greedily expanding the highest-error set, optimizing a combined performance-fairness metric. This dynamic approach contrasts with static validation methods and enables simultaneous improvement in both predictive accuracy and demographic parity fairness.

## Key Results
- SMITE significantly reduces error rates across all tested LLMs, with Llama's error decreasing from 0.246 (random selection) to 0.148
- The method improves fairness metrics while maintaining or enhancing predictive performance compared to zero-shot, random ICE, and RAG baselines
- Consistent improvements observed across both Adult and COMPAS datasets with multiple LLMs (Llama3-70b, Mixtral-8x7b, Gemini-1.0-Pro, GPT-4o-mini)
- Dynamic validation sets prove more effective than static validation for in-context example selection in fairness-critical applications

## Why This Works (Mechanism)

### Mechanism 1: Dynamic Validation Set as Test Proxy
Using the closest training examples to test samples as a validation set provides better estimates of in-context example quality than static validation. For each test batch B_n, construct P_n by selecting the single nearest neighbor (by embedding similarity) per test example. Validate candidate ICD sets against P_n using the combined error metric. Core assumption: Test-adjacent training examples are sufficiently representative of the test distribution to serve as reliable proxies for error estimation.

### Mechanism 2: Joint Performance-Fairness Error Minimization
Iteratively expanding core sets based on a combined error metric (E = απ + (1-α)ψ) enables simultaneous optimization of predictive accuracy and demographic parity fairness. Initialize each core set C_n,j with the second-closest neighbor. Compute individual error I for each core set; expand the highest-error core set with its next neighbor. Track total error T across all core sets. Select the configuration achieving minimum T after l iterations.

### Mechanism 3: Sensitive-Attribute-Constrained Retrieval
Constraining support set retrieval to match sensitive attributes ensures balanced demographic representation among candidate in-context examples. When retrieving k neighbors for test example t_i,j with sensitive attribute z_i,j, filter candidates to include only training examples where z = z_i,j before selecting by embedding proximity. Core assumption: Attribute-matched retrieval propagates fairness through the selection pipeline by ensuring each test example has demographically relevant demonstrations available.

## Foundational Learning

- **Concept: In-Context Learning (ICL)**
  - Why needed here: SMITE operates entirely within the ICL paradigm; understanding that LLMs adapt to tasks via prompt demonstrations without gradient updates is essential
  - Quick check question: Explain why ICL does not require model parameter updates and what constraints this places on example selection

- **Concept: Demographic Parity (Group Fairness)**
  - Why needed here: The fairness error ψ directly implements Demographic Parity; practitioners must understand what ψ = 0 implies and its limitations
  - Quick check question: What does it mean when P(ŷ=1|z=0) = P(ŷ=1|z=1), and what criticisms exist for this fairness criterion?

- **Concept: Vector Database Retrieval for RAG**
  - Why needed here: Support set construction uses embedding-based nearest neighbor search; understanding similarity metrics and retrieval quality is prerequisite
  - Quick check question: How does the choice of embedding model affect which training examples are retrieved as "closest" to a test instance?

## Architecture Onboarding

- **Component map:**
  Vector Store (Chroma + OpenAI Embeddings) -> Batch Partitioner -> Support Set Generator -> Dynamic Validation Set (P_n) -> Candidate Pool (H_n) -> SMITE Selector -> Error Calculator

- **Critical path:**
  1. Embed training data → populate Chroma vector store
  2. For each test batch: generate attribute-filtered support sets
  3. Partition support sets into P_n (proxy validation) and H_n (candidates)
  4. Execute SMITE: l iterations of core set expansion with error tracking
  5. Select ICD configuration with minimum total error T
  6. Construct prompt with optimal ICDs → query LLM for predictions

- **Design tradeoffs:**
  - Batch size (m): Larger batches provide more signal for fairness estimation but increase per-batch latency
  - Iterations (l): More iterations explore larger ICD spaces at higher API cost (paper uses l=10)
  - α weighting: Higher α prioritizes accuracy; lower α prioritizes fairness (paper uses α=0.5)
  - Support set size (k): Larger k increases candidate diversity but slows retrieval (paper uses k=15)

- **Failure signatures:**
  - ψ remains elevated despite SMITE → Verify sensitive attribute distribution is balanced across batches; check attribute matching in retrieval
  - π degrades vs. random baseline → α may be too low; consider increasing toward performance
  - High variance across random seeds → Proxy validation set may be too small or noisy; increase batch size m
  - Runtime exceeds latency budget → Reduce l (iterations) or k (support set size); batch processing adds overhead

- **First 3 experiments:**
  1. Reproduce Adult dataset baseline comparison: Run SMITE vs. zero-shot, random ICE, and RAG with α=0.5; target Llama error reduction from ~0.25 to ~0.15 as in Table 1
  2. Ablate dynamic validation: Compare (a) SMITE with P_n, (b) SMITE with static random validation set, (c) SMITE with no validation (greedy without proxy) to isolate dynamic validation contribution
  3. α sensitivity analysis: Run SMITE with α ∈ {0.2, 0.5, 0.8} on both Adult and COMPAS; plot π vs. ψ to characterize the fairness-accuracy Pareto frontier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the computational efficiency of SMITE be optimized to support real-time or large-scale inference without sacrificing the gains in fairness?
- Basis in paper: The authors note in the Limitations section that "Dynamic validation set is selected at the inference time, hence the proposed method SMITE can have a higher run-time."
- Why unresolved: The iterative greedy search for optimal in-context examples (ICEs) combined with dynamic validation adds significant latency compared to static or random selection methods.
- Evidence would resolve it: Implementation of optimization techniques (e.g., approximate nearest neighbor search or caching) that reduce inference latency to levels comparable with baselines while maintaining statistical improvements in error reduction.

### Open Question 2
- Question: How can the SMITE framework be adapted to ensure individual fairness and handle multiple or intersectional sensitive attributes?
- Basis in paper: The paper states: "It may not work for individual fairness... [and] might not always align with the practical situation" regarding its focus on a single binary sensitive attribute.
- Why unresolved: The current error function (ψ) relies on Demographic Parity (group fairness) for a single attribute, lacking a mechanism to penalize dissimilar treatments of similar individuals or intersectional groups.
- Evidence would resolve it: A modified error function incorporating individual fairness metrics or intersectional group penalties, validated on datasets with multiple protected attributes (e.g., race and sex combined).

### Open Question 3
- Question: Does the validity of the dynamic validation set (proxy test set) degrade when using embedding models that fail to capture fairness-relevant semantic nuances?
- Basis in paper: The method assumes the nearest neighbor in the embedding space (retrieved via OpenAI Embeddings) is a valid proxy for the test instance, but it does not analyze how the choice of embedding model impacts the quality of ICE selection.
- Why unresolved: If the embedding space projects distinct demographic groups into non-comparable regions, the "closest" proxy may offer misleading validation feedback, biasing the selection process.
- Evidence would resolve it: An ablation study varying the embedding model (e.g., using debiased embeddings vs. standard ones) to observe the correlation between proxy set quality and final model fairness.

## Limitations
- Dynamic validation set construction adds significant computational overhead at inference time compared to static or random selection methods
- Method relies on Demographic Parity as the sole fairness metric, limiting applicability to alternative fairness notions like equalized odds or individual fairness
- Performance depends on embedding model quality; if embeddings fail to capture fairness-relevant semantic nuances, proxy validation may provide misleading feedback

## Confidence

**High Confidence**: The algorithmic mechanism for joint performance-fairness optimization via iterative core set expansion is well-specified and reproducible. The observed improvements in both π and ψ on Adult/COMPAS datasets are directly measurable and clearly demonstrated.

**Medium Confidence**: The claim that dynamic validation sets outperform static validation depends on the representativeness of nearest-neighbor proxies, which the paper does not systematically validate across distribution shifts or dataset sizes.

**Medium Confidence**: While results show SMITE improves fairness relative to baselines, the exclusive focus on Demographic Parity means claims about "fairness enhancement" are criterion-specific and may not transfer to other fairness notions.

## Next Checks

1. **Distribution Shift Robustness**: Evaluate SMITE performance when test data distribution differs substantially from training (e.g., temporal splits, domain adaptation scenarios) to assess proxy validation reliability.

2. **Alternative Fairness Metrics**: Test SMITE with equalized odds and individual fairness criteria to determine if performance gains generalize beyond Demographic Parity.

3. **Scalability Analysis**: Measure latency and API cost scaling with batch size m and iterations l; identify break-even points where SMITE becomes impractical versus simpler baselines.