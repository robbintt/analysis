---
ver: rpa2
title: 'Beyond Chunking: Discourse-Aware Hierarchical Retrieval for Long Document
  Question Answering'
arxiv_id: '2506.06313'
source_url: https://arxiv.org/abs/2506.06313
tags:
- uni00000013
- discourse
- retrieval
- tree
- document
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces DISRetrieval, a discourse-aware hierarchical
  retrieval framework for long document question answering. The approach leverages
  rhetorical structure theory (RST) to build hierarchical discourse trees from lengthy
  documents, with three key innovations: language-universal discourse parsing for
  lengthy documents, LLM-based enhancement of discourse relation nodes, and structure-guided
  hierarchical retrieval.'
---

# Beyond Chunking: Discourse-Aware Hierarchical Retrieval for Long Document Question Answering

## Quick Facts
- arXiv ID: 2506.06313
- Source URL: https://arxiv.org/abs/2506.06313
- Reference count: 40
- This paper introduces DISRetrieval, a discourse-aware hierarchical retrieval framework for long document question answering.

## Executive Summary
This paper introduces DISRetrieval, a discourse-aware hierarchical retrieval framework for long document question answering. The approach leverages rhetorical structure theory (RST) to build hierarchical discourse trees from lengthy documents, with three key innovations: language-universal discourse parsing for lengthy documents, LLM-based enhancement of discourse relation nodes, and structure-guided hierarchical retrieval. Experiments on four challenging benchmarks (QASPER, QuALITY, NarrativeQA, and MultiFieldQA-zh) demonstrate consistent improvements over existing methods across different genres and languages, achieving up to +3.60% accuracy gains on QuALITY and +1.90% F1 improvements on QASPER. The framework shows strong robustness across diverse document types and linguistic settings, with cross-lingual effectiveness validated on Chinese documents.

## Method Summary
DISRetrieval employs a three-stage pipeline for long document question answering. First, it constructs hierarchical discourse trees using sentence-level RST parsing, where paragraphs are parsed individually and then integrated into a document-level structure. Second, it enhances internal nodes (rhetorical relations) with LLM-generated summaries to bridge structural and semantic information. Third, it implements structure-guided retrieval using a dual-selection strategy that balances high-relevance sentences with coherent discourse segments. The system uses gte-multilingual-base for parsing, Llama3.1-8B-Instruct for node enhancement, and various dense encoders (Sentence-BERT or OpenAI text-embedding-3-large) for retrieval.

## Key Results
- Achieves up to +3.60% accuracy gains on QuALITY benchmark for fiction/article comprehension
- Improves QASPER performance by +1.90% F1 on research paper QA
- Demonstrates strong cross-lingual effectiveness on Chinese MultiFieldQA-zh benchmark

## Why This Works (Mechanism)

### Mechanism 1: Rhetorical Segmentation
Organizing text based on rhetorical structure theory (RST) yields more semantically coherent retrieval units than fixed-size chunking. A transition-based parser builds hierarchical trees where internal nodes represent rhetorical relations (e.g., contrast, elaboration) between sentence spans. This groups related concepts even if they are distant, preventing "semantic fragmentation" common in flat chunking. Documents follow underlying rhetorical logic that aligns with the distribution of information needed to answer questions.

### Mechanism 2: LLM-Driven Semantic Bridge
Enhancing abstract tree nodes with LLM-generated summaries bridges the gap between structural syntax and semantic meaning. Internal nodes (rhetorical relations) lack concrete text, so the system uses an LLM to summarize child nodes (if length > τ) or concatenates them (if short). This populates the tree with searchable text at every level, not just leaves. LLM summarization can sufficiently condense context without losing critical keywords required for downstream matching.

### Mechanism 3: Hierarchical Dual-Selection
A retrieval strategy that balances high-relevance sentences with coherent discourse segments improves recall over flat retrieval. The algorithm ranks all nodes (leaves and summaries), selecting high-scoring leaves directly but expanding high-scoring internal nodes to retrieve their Top-k leaves. This captures both specific facts (leaves) and broader context (discourse segments). Relevant evidence is often clustered within specific rhetorical segments rather than scattered randomly.

## Foundational Learning

- **Rhetorical Structure Theory (RST)**
  - Why needed here: This is the structural backbone of the system. You must understand that text is viewed as a tree of relations (Nucleus-Satellite) rather than a flat sequence.
  - Quick check question: Can you explain why a "contrast" relation node might be a better retrieval target for a specific query than the individual sentences forming the contrast?

- **Dense Retrieval & Bi-Encoders**
  - Why needed here: The system relies on embedding queries and tree nodes into the same vector space.
  - Quick check question: How does computing cosine similarity between a query embedding and *internal* node embeddings differ from standard document retrieval?

- **Transition-Based Parsing**
  - Why needed here: The paper uses a specific parsing architecture (shift/reduce actions) to build trees efficiently.
  - Quick check question: What are the basic actions (Shift, Reduce) required to convert a sentence queue into a discourse tree?

## Architecture Onboarding

- **Component map:** Raw long document -> Sentence-level RST parser -> Tree Builder -> LLM Enhancer -> Dense Encoder -> Structure-guided Retriever -> Downstream Generator
- **Critical path:** The **Discourse-Aware Tree Construction** (Step 1 & 2) is the primary innovation. If the tree is built poorly (bad parsing) or enhanced poorly (bad summaries), the retrieval logic cannot recover the performance.
- **Design tradeoffs:**
  - Parsing Granularity: Sentence-level vs. EDU-level. They chose sentence-level for efficiency, trading off fine-grained linguistic precision.
  - Summarization Threshold (τ): High τ means more text, less compression (more noise, less cost). Low τ means aggressive summarization (risk of data loss).
- **Failure signatures:**
  - Retrieval Degradation on Lists: If the parser struggles with non-narrative formats (lists, tables), retrieval drops.
  - Low Intermediate Node Scores: If internal node summaries don't match queries, the system devolves to flat leaf retrieval (Bisection baseline).
  - Parser Data Dependency: Figure 4 shows performance scales with parser training data; low-resource languages may fail.
- **First 3 experiments:**
  1. Run the discourse parser on 10-20 documents from your target domain. Visually inspect if the tree structure makes sense (e.g., does it group related arguments?).
  2. Test the threshold τ (as shown in Figure 9) on a small validation set to find the optimal balance between context length and noise for your specific summarization model.
  3. Implement the "Bisection" baseline (random binary tree) vs. RST tree. If RST doesn't significantly outperform Bisection, the discourse structure isn't adding value for your data type.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can discourse-aware hierarchical retrieval generalize effectively to languages beyond English and Chinese without requiring language-specific discourse treebanks?
- Basis in paper: Limitations section states extending to more languages would require additional translated training data through similar augmentation strategies. The scarcity of suitable long-document QA datasets in other languages currently limits broader multilingual evaluation.
- Why unresolved: Current validation is limited to English and Chinese; the LLM-based translation augmentation strategy's effectiveness for languages with different syntactic structures (e.g., morphologically rich languages, non-SVO languages) remains untested.
- What evidence would resolve it: Evaluation on long-document QA benchmarks in typologically diverse languages (e.g., Arabic, Japanese, Finnish) using the translation-augmented parser, with analysis of parser accuracy and retrieval quality per language.

### Open Question 2
- Question: Can domain-adaptive discourse parsing significantly improve retrieval performance for specialized document types (legal, medical, technical) compared to the current news-trained parser?
- Basis in paper: Limitations section mentions future work will explore developing domain-adaptive parsers. Figure 4 shows parser quality directly impacts downstream performance, and the current parser is trained only on RST-DT (news-text dominant).
- Why unresolved: The paper demonstrates robustness across genres but does not investigate whether training parsers on domain-specific discourse data yields further gains for specialized documents with distinct rhetorical conventions.
- What evidence would resolve it: Train discourse parsers on domain-specific annotated corpora (legal briefs, clinical notes), evaluate retrieval performance on domain-matched QA datasets, and compare against the general parser.

### Open Question 3
- Question: Would content-aware dynamic thresholding for LLM summarization (τ) outperform the current fixed thresholds for improving retrieval quality and efficiency?
- Basis in paper: Limitations section states more sophisticated dynamic thresholding based on content complexity and hierarchical position could further optimize the trade-off.
- Why unresolved: Current τ values (0 for academic papers, 50 for narratives) are manually tuned; whether adaptive thresholding based on sentence density, rhetorical relation types, or tree depth could yield better semantic representations is unexplored.
- What evidence would resolve it: Implement dynamic τ functions based on content features, conduct ablation studies on QASPER and QuALITY measuring retrieval F1/Recall and computational cost, comparing against fixed τ baselines.

### Open Question 4
- Question: How can evaluation metrics better capture discourse-aware retrieval quality beyond surface-level token overlap?
- Basis in paper: Limitations section notes current evaluation metrics may not fully capture the nuanced benefits of discourse-aware retrieval, such as structural coherence preservation and hierarchical information flow.
- Why unresolved: Standard metrics (F1, BLEU, ROUGE) measure answer quality but cannot assess whether retrieved evidence preserves discourse coherence or captures appropriate hierarchical granularity.
- What evidence would resolve it: Develop and validate discourse-aware evaluation metrics (e.g., measuring retrieval unit coherence, hierarchical completeness, or rhetorical relation preservation), correlating them with human judgments of evidence quality.

## Limitations
- Parser generalization remains uncertain for specialized domains like legal or medical documents without substantial retraining data
- Significant LLM dependency creates computational overhead and cost barriers for real-time deployment
- Cross-lingual robustness is limited to English and Chinese, with performance drops suggesting challenges for other language pairs

## Confidence
- **High Confidence:** The core hypothesis that hierarchical discourse structure can improve retrieval over flat chunking methods is well-supported by experimental results across four diverse benchmarks
- **Medium Confidence:** The mechanism explanations are logically sound and supported by experimental evidence, but some claims about "semantic fragmentation" prevention could benefit from more direct user studies
- **Low Confidence:** The long-term generalization claims to arbitrary domains and languages are based on limited validation, with performance on MultiFieldQA-zh showing the approach doesn't automatically transfer across languages

## Next Checks
1. Apply DISRetrieval to a specialized domain document set (e.g., legal contracts or medical literature) not represented in the original benchmarks. Compare performance against the best baseline to assess domain generalization limits.
2. Systematically vary the amount of RST-DT training data and measure the corresponding impact on downstream retrieval performance. This would quantify the parser bottleneck and help determine minimal viable training requirements.
3. Measure end-to-end inference time and cost per document for DISRetrieval versus baseline methods. Include both LLM inference costs and any additional preprocessing overhead to provide a complete cost-benefit analysis for production deployment scenarios.