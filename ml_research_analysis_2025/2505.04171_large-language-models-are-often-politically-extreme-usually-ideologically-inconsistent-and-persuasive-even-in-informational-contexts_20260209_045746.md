---
ver: rpa2
title: Large Language Models are often politically extreme, usually ideologically
  inconsistent, and persuasive even in informational contexts
arxiv_id: '2505.04171'
source_url: https://arxiv.org/abs/2505.04171
tags:
- llms
- llama
- political
- gemini
- grok
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large Language Models (LLMs) are often politically extreme and
  persuasive even in informational contexts, with their preferences influencing user
  opinions. Researchers analyzed 31 LLMs by comparing their stated preferences to
  U.S.
---

# Large Language Models are often politically extreme, usually ideologically inconsistent, and persuasive even in informational contexts

## Quick Facts
- arXiv ID: 2505.04171
- Source URL: https://arxiv.org/abs/2505.04171
- Reference count: 0
- Primary result: LLM users shifted up to 5pp toward chatbot political preferences in informational contexts

## Executive Summary
Large Language Models exhibit politically extreme views on specific issues that cancel out when aggregated to a single ideological dimension, creating an illusion of moderation similar to moderate voters. In a randomized survey experiment, users exposed to LLM chatbots showed a 5 percentage point increase in alignment with the chatbot's political preferences, even when seeking neutral information rather than explicit persuasion. This persuasive effect was consistent across users regardless of their political sophistication, news consumption, or familiarity with LLMs, suggesting these models serve as a powerful vector for political influence.

## Method Summary
The study analyzed 31 LLMs using political science scaling methods (W-NOMINATE, IRT) to compare their ideological positions to U.S. legislators, Supreme Court Justices, and voters. Researchers conducted a pre-registered randomized survey experiment with 2,665 respondents on Prolific, where participants discussed political issues with either a control interface or one of three LLM chatbots (GPT-4o, Mistral, or Llama). The experiment measured political alignment shifts using respondent fixed effects while controlling for topic order and time spent in conversation.

## Key Results
- LLMs hold offsetting extreme views on specific topics, appearing moderate only when averaged to a single dimension
- Users exposed to LLM chatbots were up to 5 percentage points more likely to align with the chatbot's political preferences
- Persuasive effects occurred even in informational contexts without explicit persuasion attempts
- Effects were uniform across users with varying political interest, news consumption, and LLM familiarity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: LLMs' apparent political moderation results from offsetting extreme positions across topics, not genuine neutrality.
- **Mechanism**: When aggregated to a single ideological dimension (e.g., liberal-conservative), extreme positions on different issues cancel out. A model may be more liberal than Democrats on immigration but more conservative than Republicans on gun control, producing a "moderate" average.
- **Core assumption**: Assumes that single-dimensional ideological scaling captures meaningful political information—a contested assumption in political science.
- **Evidence anchors**:
  - [abstract]: "LLMs' apparently small overall partisan preference is the net result of offsetting extreme views on specific topics, much like moderate voters."
  - [section: LLMs are as Ideologically (In)consistent as Voters, Figure 5]: Panel C shows specific LLMs with positions more extreme than both Strong Democrats and Strong Republicans on different issues; Mistral Nemo is liberal on healthcare/immigration/abortion but conservative on gun control/police.
  - [corpus]: Related work (Measuring Political Stance and Consistency in Large Language Models) confirms inconsistency as a measurement challenge but doesn't explain the offsetting mechanism.
- **Break condition**: If LLMs were uniformly moderate across all topics (low variance within each issue area), the mechanism fails; the claim requires topic-specific variance.

### Mechanism 2
- **Claim**: LLMs persuade users in information-seeking contexts by presenting ideological preferences through ostensibly neutral informational framing.
- **Mechanism**: Users interacting with LLM chatbots for information (not explicit persuasion) still shift toward the LLM's positions. Two-thirds of user messages were neutral information requests (Figure 7), yet alignment increased 5 percentage points. The "informational" frame may bypass resistance that explicit persuasion triggers.
- **Core assumption**: Assumes the measured alignment shift represents genuine preference change rather than temporary acquiescence or demand characteristics.
- **Evidence anchors**:
  - [abstract]: "voters randomized to discuss political issues with an LLM chatbot are as much as 5 percentage points more likely to express the same preferences as that chatbot."
  - [section: Results: Political Persuasion, Table 1]: Chatbot treatment coefficient = 0.050 (p<0.01); dose-response relationship with questions asked (+1.2pp per question) and time spent (+0.6pp per minute).
  - [corpus]: "Passing the Turing Test in Political Discourse" (FMR=0.48) shows fine-tuned LLMs can generate persuasive polarized content, supporting plausibility but not the informational-context mechanism specifically.
- **Break condition**: If persuasion required explicit argumentation prompts or users recognized the LLM's bias and corrected for it, the mechanism would fail.

### Mechanism 3
- **Claim**: Persuasive effects are uniform across user sophistication levels—political interest, news consumption, and LLM familiarity do not moderate the effect.
- **Mechanism**: Contrary to classic political science findings that persuasion works primarily on uninformed audiences, LLM persuasion appears to bypass traditional defenses. This may occur because LLMs provide customized, interactive responses that engage users regardless of prior knowledge.
- **Core assumption**: Assumes the survey measures validly capture these constructs and that the sample has sufficient variation in sophistication.
- **Evidence anchors**:
  - [abstract]: "these persuasive effects are not moderated by familiarity with LLMs, news consumption, or interest in politics."
  - [section: Results: Heterogeneous Effects, Figure 9]: All 18 interaction coefficients across different moderators cluster near zero with no statistical significance.
  - [corpus]: Weak/missing—corpus papers don't test heterogeneous treatment effects by user sophistication.
- **Break condition**: If more sophisticated users showed significantly weaker effects (negative interaction terms), the uniformity claim would fail.

## Foundational Learning

- **Concept: Ideal Point Estimation (W-NOMINATE, IRT)**
  - **Why needed here**: The paper places LLMs on the same ideological scales as legislators and voters using methods from political science. Without understanding ideal point estimation, you cannot interpret the spatial plots or understand why "moderate" averages mask extreme heterogeneity.
  - **Quick check question**: If an LLM scores 0.0 on NOMINATE Dimension 1, what does that tell you about its positions on any specific issue like gun control or abortion?

- **Concept: Randomized Survey Experiments with Fixed Effects**
  - **Why needed here**: The persuasion evidence comes from a pre-registered RCT with respondent fixed effects. Understanding within-subject designs is essential to see why each person answering 4 questions strengthens causal inference.
  - **Quick check question**: Why would including respondent fixed effects help isolate the chatbot's persuasive effect from pre-existing political leanings?

- **Concept: Principal Component Analysis for Ideological Scaling**
  - **Why needed here**: The voter comparison uses PCA to reduce 46 CES questions to a single dimension. Understanding dimensionality reduction helps explain why aggregate measures obscure topic-specific extremes.
  - **Quick check question**: If the first principal component explains 32% of variance (as stated), what proportion of political opinion variation is potentially hidden from a one-dimensional view?

## Architecture Onboarding

- **Component map**: Prompt templates for legislator/justice/voter roleplay -> API calls to 31 models (temperature=0, top-p=0) -> response aggregation -> W-NOMINATE/PCA/IRT scaling -> comparison to human baselines
- **Critical path**: Establish baseline ideology estimates for LLMs on validated political science scales (legislators, justices, voters) -> Identify topic-specific extreme positions that aggregate moderation obscures -> Select LLMs with extreme positions on specific topics for the persuasion experiment -> Randomize chatbot exposure and measure alignment shift controlling for respondent fixed effects
- **Design tradeoffs**:
  - Temperature=0 for measurement vs. realistic interaction: Zero temperature ensures consistency (Fleiss' Kappa >0.95) but may not reflect how users actually experience LLMs
  - Convenience sample (Prolific) vs. representativeness: Higher statistical power and data quality but limited generalizability to broader populations
  - Within-subject design (4 questions each) vs. between-subject only: Increases power but risks spillover effects across topics
- **Failure signatures**: Low inter-rater agreement on LLM responses (Kappa <0.8) would indicate unstable ideological measurement; spillover effects between questions (e.g., immigration discussion influencing gun control response); demand characteristics where users guess the study purpose and adjust responses
- **First 3 experiments**:
  1. Replicate with explicit persuasion condition: Add a treatment arm where the LLM is instructed to persuade rather than just inform—does the effect size increase or does resistance activate?
  2. Test decay over time: Re-contact respondents 1 week later to measure whether alignment persists or decays (classic finding in persuasion literature)
  3. Cross-cultural validation: Apply the same methodology to non-U.S. political contexts (CES equivalent datasets in other countries) to test whether the mechanism generalizes beyond American political ideology structure

## Open Questions the Paper Calls Out
None

## Limitations
- Convenience sample from Prolific limits generalizability to broader populations
- Temperature=0 measurement may not reflect real-world LLM behavior where users encounter stochastic outputs
- Single-dimensional ideological scaling assumption is contested in political science literature

## Confidence
- LLM ideology measurement methodology: **Medium-High** (robust inter-rater reliability, validated against multiple human baselines)
- Causal persuasion effects: **Medium** (strong within-subject design but convenience sample, limited to 3 models)
- Ideological inconsistency explanation (offsetting extremes): **Medium** (plausible mechanism but relies on contested one-dimensional scaling)

## Next Checks
1. **Decay and persistence test**: Re-contact persuasion experiment participants after 1-4 weeks to measure whether the 5pp alignment shift persists, decays, or reverses—critical for assessing real-world political influence.
2. **Cross-cultural generalization**: Apply the same methodology to non-U.S. political contexts using equivalent validated datasets to determine whether the offsetting extreme positions mechanism holds beyond American political structure.
3. **Interaction vs. exposure test**: Run a follow-up experiment comparing LLM persuasion to simple exposure to partisan news articles with equivalent content to isolate whether the interactive nature of chatbots provides unique persuasive leverage.