---
ver: rpa2
title: 'TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending
  in Diffusion Models'
arxiv_id: '2601.08011'
source_url: https://arxiv.org/abs/2601.08011
tags:
- image
- arxiv
- conference
- vision
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TP-Blend tackles the challenge of simultaneously replacing an object,
  blending it with another object, and applying a new style using separate textual
  prompts in a single denoising process. It introduces Cross-Attention Object Fusion
  (CAOF), which leverages attention maps and optimal transport to seamlessly integrate
  blend-object features into replaced-object regions, and Self-Attention Style Fusion
  (SASF), which uses Detail-Sensitive Instance Normalization and text-driven key/value
  substitution to inject fine-grained texture and style.
---

# TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models

## Quick Facts
- arXiv ID: 2601.08011
- Source URL: https://arxiv.org/abs/2601.08011
- Authors: Xin Jin; Yichuan Zhong; Yapeng Tian
- Reference count: 40
- Primary result: Outperforms state-of-the-art text-driven editors on BOM (0.8388) and BOSM (0.9244)

## Executive Summary
TP-Blend addresses the challenge of simultaneously replacing an object, blending it with another object, and applying a new style using separate textual prompts in a single denoising process. It introduces Cross-Attention Object Fusion (CAOF) and Self-Attention Style Fusion (SASF) to achieve precise control over content and appearance. CAOF leverages optimal transport to integrate blend-object features into replaced-object regions, while SASF injects fine-grained texture and style through frequency decomposition and key/value substitution. Extensive experiments demonstrate superior performance on quantitative metrics and high-resolution, photo-realistic edits.

## Method Summary
TP-Blend is a training-free framework that operates on a pre-trained SD-XL backbone. It takes a source image and three textual prompts (original object, blend object, style) as input. The method first performs DDIM inversion to obtain the latent representation, then applies TIE-CFG for object replacement. CAOF uses cross-attention maps and entropy-regularized optimal transport to blend features from the blend prompt into the replaced object region. SASF applies Detail-Sensitive Instance Normalization with frequency decomposition and substitutes key/value matrices from the style prompt to inject texture. The final output is obtained through the VAE decoder after denoising.

## Key Results
- Achieves BOM score of 0.8388 and BOSM score of 0.9244, outperforming state-of-the-art text-driven editors
- Maintains high CLIP_R, CLIP_B, and CLIP_S alignment scores while preserving object identity
- Demonstrates superior performance in ablation studies, with optimal parameters α=0.5, σ=2.5, and τ=0.6-0.7

## Why This Works (Mechanism)

### Mechanism 1: Cross-Attention Object Fusion via Optimal Transport
CAOF identifies source positions (high attention to blend prompt) and destination positions (high attention to replaced prompt) using percentile-thresholded cross-attention maps. It then solves a Sinkhorn-regularized transport problem with a cost combining cosine feature distance (λfeature=0.7) and Euclidean spatial distance (λspatial=0.3), producing a transport plan that redistributes full multi-head feature vectors. This preserves morphological coherence during object blending by ensuring semantic consistency in feature reassignment.

### Mechanism 2: Detail-Sensitive Instance Normalization for High-Frequency Style Injection
DSIN applies AdaIN for global statistics alignment, then uses a 1D Gaussian kernel to compute low-frequency and high-frequency components. Only the high-frequency residual is blended back with a fraction α, preserving geometry while imprinting fine textures. Larger σ yields narrower HF residual (subtler injection); smaller σ captures more mid-range frequencies. This mechanism ensures that style cues relevant for fine texture reside primarily in high-frequency residuals rather than global statistics alone.

### Mechanism 3: Text-Driven Key/Value Substitution for Context-Aware Style Modulation
After DSIN, Key/Value matrices are substituted with those derived from the style prompt before self-attention computation. Since attention outputs weight Values via Query-Key products, style-derived K/V dominate texture updates while DSIN-modified features remain in the Query branch. This enforces texture consistency while preserving object structure, as style prompts encode sufficient textural priors in their self-attention K/V representations.

## Foundational Learning

- **Optimal Transport and Sinkhorn Algorithm**
  - Why needed here: CAOF uses entropy-regularized OT to compute feature transport plans; understanding the Sinkhorn iterations is essential for debugging convergence and tuning γ.
  - Quick check question: Can you explain why entropy regularization (γ > 0) makes OT solvable via iterative scaling rather than linear programming?

- **Attention Mechanisms in Diffusion Models**
  - Why needed here: The method relies on interpreting cross-attention maps as object localizers and self-attention K/V as texture carriers.
  - Quick check question: What is the difference between cross-attention (conditioning on text tokens) and self-attention (spatial token interactions) in a U-Net diffusion backbone?

- **Frequency Decomposition with Gaussian Filters**
  - Why needed here: DSIN's 1D Gaussian kernel separates LF/HF components; σ controls the frequency cutoff.
  - Quick check question: How does increasing σ affect the cutoff frequency of a Gaussian low-pass filter?

## Architecture Onboarding

- Component map: Source image -> DDIM inversion -> latent xT; three prompts (original, blend, style) -> TIE-CFG -> CAOF (cross-attention maps -> percentile thresholding -> Sinkhorn solver -> transport plan) -> SASF (AdaIN -> 1D Gaussian filter -> HF extraction -> HF blending -> K/V substitution) -> Backbone (SD-XL U-Net) -> Denoised latent -> VAE decoder -> Edited image

- Critical path: 1. DDIM inversion (exact reconstruction check) 2. TIE-CFG for object replacement (positive/negative guidance) 3. CAOF feature transport at each timestep 4. SASF style injection at each self-attention layer 5. Sampling loop completion

- Design tradeoffs:
  - Full-dimensionality OT (640D) vs. per-head OT: Paper chooses full-D to preserve cross-head correlations and reduce cost matrix size (4096×4096 vs. 40960×40960)
  - τsource/τdest values: 0.6-0.7 balances precision (removing spurious tokens) and coverage (preserving object parts); higher favors replacement over blending
  - α vs. σ in DSIN: Joint tuning required; α=0.5, σ=2.5 maximizes texture metrics in ablation
  - Training-free vs. fine-tuning: No additional training required, but performance depends on pre-trained backbone quality

- Failure signatures:
  - Object identity loss: CLIPR drops if blending coefficient w0 too high (>0.8) or τ thresholds too low
  - Background degradation: Baselines show background removal; TP-Blend mitigates via attention-guided localization
  - Insufficient blending: w0 too low or τ too high results in minimal blend influence
  - Texture washout: α=0 (no DSIN) reduces HF texture; check Table 4 for expected LV/GC/HFS values

- First 3 experiments:
  1. CAOF ablation without OT: Replace Sinkhorn with naive index-aligned blending (NoneOT); expect BOM drop from ~0.25 to ~0.14 per Table 3, confirming OT's role in semantic alignment.
  2. DSIN parameter sweep: Vary α∈{0.0, 0.2, 0.5} and σ∈{0.5, 2.5}; measure LV, GC, HFS; confirm α=0.5, σ=2.5 yields highest scores per Table 4.
  3. Threshold sensitivity: Sweep joint τ∈{0%, 20%, 50%, 60%, 70%, 80%, 99%}; plot CLIPR, CLIPB, CLIPO; expect CLIPB peak at 60% and CLIPR rise at high τ per Figure 12.

## Open Questions the Paper Calls Out

- **Can the TP-Blend framework be adapted to transformer-based diffusion architectures (e.g., DiT) that lack the U-Net cross-attention structure utilized by CAOF?**
  - Basis in paper: [inferred] The implementation (Section 4.1) relies on SD-XL, and the method exploits multi-head attention manipulations specific to U-Net backbones.
  - Why unresolved: The paper does not analyze compatibility with non-U-Net architectures where cross-attention may not exist or differ structurally.
  - What evidence would resolve it: Successful application of CAOF and SASF to a DiT-based model like Stable Diffusion 3 or PixArt.

- **Can the optimal parameters (percentile thresholds $\tau$, blend weights $w_0$, DSIN coefficients $\alpha, \sigma$) be dynamically determined per image rather than manually tuned?**
  - Basis in paper: [inferred] Ablation studies (Section 4.3) demonstrate that performance is highly sensitive to fixed hyperparameter settings, requiring manual adjustment for optimal results.
  - Why unresolved: The method currently treats these as static constants or user-controlled variables.
  - What evidence would resolve it: An adaptive mechanism that predicts optimal parameters based on attention statistics without human intervention.

- **Does solving the entropy-regularized optimal transport problem introduce prohibitive latency or memory overhead at significantly higher resolutions?**
  - Basis in paper: [inferred] Section 3.3 mentions constructing cost matrices up to 4096×4096 to manage computational complexity, despite claims of being "lightweight."
  - Why unresolved: The paper does not provide a quantitative comparison of inference time or memory usage against baselines.
  - What evidence would resolve it: Runtime benchmarks on 2K or 4K resolutions comparing TP-Blend against standard CFG-TE.

## Limitations
- Scalability concerns for entropy-regularized optimal transport at higher resolutions
- Dependency on high-quality pre-trained backbone attention maps, which may fail for small or occluded objects
- Performance on text-rich scenes (where CLIP-based metrics may be biased) is untested

## Confidence
- High: Feasibility of simultaneous object blending and style injection
- Medium: Superiority of OT-based feature transport over naive blending
- Low: Assertion that SASF's Key/Value substitution is universally stable across object-scale and prompt semantic conflicts

## Next Checks
1. **Cross-attention map fidelity test**: Verify CAOF's attention thresholding by visualizing attention heatmaps for varied object sizes; measure the false-positive rate of background tokens being included in S/D sets.
2. **DSIN robustness sweep**: Systematically vary α and σ across extreme values (α=0.0 to 1.0, σ=0.5 to 5.0) and measure impact on HFS, LV, and GC; confirm α=0.5, σ=2.5 is globally optimal.
3. **OT convergence analysis**: Monitor Sinkhorn iterations for convergence speed and stability under varying γ and cost weightings; test if γ=0.1 is a robust choice or dataset-dependent.