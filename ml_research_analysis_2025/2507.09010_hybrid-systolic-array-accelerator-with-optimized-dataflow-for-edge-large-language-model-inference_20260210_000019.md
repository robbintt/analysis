---
ver: rpa2
title: Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language
  Model Inference
arxiv_id: '2507.09010'
source_url: https://arxiv.org/abs/2507.09010
tags:
- accelerator
- energy
- token
- edge
- latency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a hybrid systolic array (HSA) architecture
  for efficient edge LLM inference, addressing the challenge of balancing high energy
  efficiency during the compute-intensive prefill stage and minimizing external memory
  access (EMA) during the memory-bound decode stage. The core method idea involves
  combining the strengths of conventional systolic arrays and vector units to optimize
  inference efficiency, adopting MXINT4 weight quantization with a tailored dataflow
  to achieve 100% hardware utilization and minimal accuracy loss, and incorporating
  optimized RMSNorm and RoPE units to reduce latency and area overhead.
---

# Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference

## Quick Facts
- **arXiv ID:** 2507.09010
- **Source URL:** https://arxiv.org/abs/2507.09010
- **Reference count:** 31
- **One-line primary result:** The proposed accelerator achieves 247/117 tokens/s/mm² in long-input/long-output (LISO) and short-input/long-output (SILO) scenarios, respectively, representing over 2.45×/13.5× improvement in area efficiency compared to existing approaches, while maintaining competitive energy efficiency in token generation.

## Executive Summary
This paper proposes a hybrid systolic array (HSA) architecture for efficient edge inference of large language models (LLMs), addressing the challenge of balancing high energy efficiency during the compute-intensive prefill stage and minimizing external memory access (EMA) during the memory-bound decode stage. The core innovation involves combining conventional systolic arrays and vector units to optimize inference efficiency, adopting MXINT4 weight quantization with a tailored dataflow to achieve 100% hardware utilization and minimal accuracy loss, and incorporating optimized RMSNorm and RoPE units to reduce latency and area overhead. The primary result is that the proposed accelerator achieves 247/117 tokens/s/mm² in long-input/long-output (LISO) and short-input/long-output (SILO) scenarios, respectively, representing over 2.45×/13.5× improvement in area efficiency compared to existing approaches, while maintaining competitive energy efficiency in token generation.

## Method Summary
The proposed HSA architecture combines the strengths of conventional systolic arrays and vector units to optimize inference efficiency for both prefill (matrix-matrix multiplication) and decode (matrix-vector multiplication) stages of LLM inference. The method employs MXINT4 weight quantization with a tailored dataflow to achieve 100% hardware utilization and minimal accuracy loss, and incorporates optimized RMSNorm and RoPE units to reduce latency and area overhead. The architecture uses 4 PE clusters (PCs), each with 64 PEs (16x4 arrangement), 16kB weight SRAM, and a 66kB shared activation SRAM, targeting RetNet 1.3B models with SmoothQuant for INT8 activations and group-wise shifting for MXINT4 weights.

## Key Results
- The HSA accelerator achieves 247 tokens/s/mm² in long-input/long-output (LISO) scenarios and 117 tokens/s/mm² in short-input/long-output (SILO) scenarios.
- The proposed design shows over 2.45×/13.5× improvement in area efficiency compared to existing approaches.
- The accelerator maintains competitive energy efficiency (4.17 mJ/token during prefill, 22.7 mJ/token during decode) while achieving high hardware utilization (100% during decode stage).

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Systolic-Vector Execution
The architecture maintains high utilization across both prefill (MMM) and decode (MVM) stages by dynamically adapting the dataflow. In prefill (batched matrix-matrix), the array operates as a standard systolic array to maximize data reuse. In decode (single-batch matrix-vector), the array reconfigures so PE clusters operate independently on weight partitions, mimicking a vector unit to avoid the low utilization inherent in standard systolic arrays when batch size equals one. The overhead of reconfiguration logic ("bucket selectors") is assumed to be negligible compared to the utilization gains.

### Mechanism 2: Shift-Based Dequantization
MXINT4 quantization with shift-based scaling reduces external memory access (EMA) without the hardware cost of floating-point dequantization. Weights are stored in 4-bit format, and instead of expensive FP16 multipliers for scaling, the architecture uses power-of-2 scaling (shifts). The "bucket selector" hardware shifts the 4-bit weights back to 8-bit dynamically, effectively mapping the dequantization cost into the existing PE structure. The assumption is that the model weights can tolerate the precision loss of shift-based (power-of-2) scaling versus floating-point scaling.

### Mechanism 3: Layer-Fused Normalization
Fusing RMSNorm eliminates latency overhead and a 32kB buffer. The system calculates the inverse standard deviation ($\sigma^{-1}$) of the output in parallel with the subsequent layer's MAC operations. It mathematically fuses the normalization constants into the next layer's scaling factors and biases, removing the need to write normalized data to a buffer and read it back. The assumption is that the fused mathematical operations maintain equivalence without numerical stability issues.

## Foundational Learning

### Concept: Systolic Arrays vs. Vector Units
**Why needed here:** The paper claims to "hybridize" these. You must understand that Systolic Arrays rely on data flow between neighbors (great for reuse, bad for small batches) while Vector Units process data in parallel chunks (good for single batches, poor reuse).
**Quick check question:** Why does a standard systolic array drop to ~1.7% utilization during the decode stage?

### Concept: LLM Prefill vs. Decode
**Why needed here:** The architecture optimizes differently for these two phases. Prefill is compute-heavy (Matrix-Matrix), while Decode is memory-heavy (Matrix-Vector).
**Quick check question:** Which stage creates the memory bandwidth bottleneck that necessitates the MXINT4 quantization?

### Concept: Quantization Scaling (MX Formats)
**Why needed here:** The paper uses "Microscaling" (MXINT4). You need to understand that quantization involves compressing weights (INT4) but usually requires a high-precision "scale" to restore accuracy.
**Quick check question:** Why is using a floating-point scale (FP16) expensive in hardware, prompting the authors to use a "shifting-based" scale?

## Architecture Onboarding

### Component map:
HSA Core -> 4 PE Clusters (PCs), each with a 16kB Weight SRAM, 64 PEs (16x4 arrangement), and 4-bit shifters -> Unified Activation SRAM (66kB) feeding all clusters -> PPU (Post-Processing Unit) handling RMSNorm and RoPE fusion

### Critical path:
During Decode, the weights flow from SRAM -> Bucket Selector (Shift 4b->8b) -> PEs (Accumulate) -> PPU (Fused Norm/RoPE)

### Design tradeoffs:
- PE Count vs. Memory: Limited to 256 PEs (vs 1024 in some rivals) because edge inference is memory-bound; more compute would sit idle.
- Accuracy vs. Efficiency: Trading FP16 scaling for 4-bit shift scaling sacrifices some accuracy range for massive area/power savings.

### Failure signatures:
- Stalling: If the single Activation SRAM cannot feed all 4 independent clusters fast enough during MVM.
- Accuracy Collapse: If the "shift range" [-9, +5] is insufficient for the model's weight distribution.

### First 3 experiments:
1. **Utilization Test:** Run the Decode phase (MVM) and verify 100% PE utilization as claimed, comparing it against a standard systolic array baseline.
2. **Perplexity Check:** Quantize a target LLM (e.g., Llama 2) using the shift-based method and measure perplexity drop vs. FP16 to verify the "minimal accuracy loss" claim.
3. **Fusion Latency:** Measure the cycle count for the PPU stage with and without fused RMSNorm to confirm the elimination of the 5-10% overhead.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed Hybrid Systolic Array (HSA) architecture handle the dynamic memory access patterns and KV-cache growth associated with standard Transformer-based LLMs (e.g., Llama) compared to the linear complexity of the target RetNet architecture?
- Basis in paper: The paper primarily evaluates the accelerator using RetNet 1.3B (Section V), which replaces softmax with a retention mechanism to achieve $O(1)$ memory complexity during decoding (Section II). While the authors state that the fundamental MMM/MVM workloads are consistent across Llama and RetNet (Section II), they do not present experimental data for the accelerator running standard attention-based models, which have different memory access bottleneck profiles.
- Why unresolved: The efficiency gains are partially attributed to RetNet's architectural properties (linear memory complexity). It is unclear if the HSA dataflow or the on-chip SRAM capacity (66kB Act SRAM) is sufficient to handle the KV-cache management of standard Transformers without hitting the same memory bottlenecks the paper aims to solve.
- What evidence would resolve it: End-to-end latency and energy efficiency results running a standard Transformer model (e.g., Llama 2) on the HSA accelerator, specifically profiling the decode stage memory bandwidth usage.

### Open Question 2
- Question: Can the proposed shifting-based scaling quantization (MXINT4 variant) maintain accuracy parity with FP16-scaling methods when applied to larger parameter models (e.g., >7B) or more complex reasoning tasks?
- Basis in paper: Table III and Table IV show that the proposed 4-bit shifting method slightly increases perplexity (e.g., Llama 2-7B perplexity rises from 5.47 to 5.81) and reduces reasoning accuracy compared to baselines. The paper evaluates RetNet 1.3B and Llama 3.2-3B, but the accuracy loss relative to FP16 scaling methods (like QServe) on larger, more capable models remains untested.
- Why unresolved: The hardware-efficient integer-shift scaling restricts the representation range compared to FP16 scaling factors. As model size and complexity increase, the sensitivity to quantization noise typically changes, and it is uncertain if this low-overhead scaling method generalizes without significant accuracy degradation.
- What evidence would resolve it: Perplexity and benchmark (e.g., GSM8K) results applying the specific 4-bit shifting scheme to larger models (e.g., Llama-3-70B or Mixtral) to compare against state-of-the-art FP16-scaled quantization.

### Open Question 3
- Question: How does the HSA architecture's utilization and efficiency scale when batch sizes increase beyond one, potentially shifting the decode stage from memory-bound to compute-bound?
- Basis in paper: The introduction explicitly states that "edge scenarios often involve user queries arriving one at a time" and assumes a batch size of one. The MVM dataflow (Section IV-A2) exploits the resulting low utilization to map dequantization operations (Fig. 4c).
- Why unresolved: The design optimizes for the memory-bound constraints of batch-size-one inference. If batch sizes were increased (e.g., to improve throughput in edge servers), the workload characteristics would change. The current dataflow relies on "idle" PEs for dequantization; high utilization compute tasks might bottleneck on the proposed bucket selectors and shifters or face different memory contention issues.
- What evidence would resolve it: Performance profiling of the HSA running decode workloads with batch sizes of 2, 4, and 8, reporting throughput (tokens/s) and hardware utilization rates.

## Limitations
- The paper does not provide quantitative data on the timing or area cost of the bucket selector logic, which is critical for the hybrid execution mechanism.
- The shift-based dequantization relies on the assumption that power-of-2 scaling can approximate the full floating-point range required by model weights, but the stated shift range may not be universally applicable.
- The single 66kB Activation SRAM feeding four independent PE clusters during decode is identified as a potential stall point, but the paper lacks empirical data on actual memory bandwidth utilization or stall rates.

## Confidence
- **High Confidence (90%+):** Area efficiency improvements (247/117 tokens/s/mm²) and comparative analysis against prior art.
- **Medium Confidence (70-80%):** Utilization claims (100% during decode) - while the architecture design supports this, detailed cycle-accurate measurements are lacking.
- **Low Confidence (50-60%):** The "minimal accuracy loss" claim for shift-based dequantization - the paper provides perplexity metrics but lacks comprehensive analysis of failure cases or comparison with alternative quantization schemes under stress conditions.

## Next Checks
1. **Utilization Validation Under Varying Workloads:** Implement a cycle-accurate simulator to measure actual PE utilization during decode across different batch sizes and sequence lengths. Compare against the claimed 100% utilization and identify any pipeline stalls or bottlenecks in the bucket selector logic.

2. **Accuracy Stress Testing:** Systematically evaluate the shift-based dequantization across multiple LLM architectures (Llama 2, Llama 3, Mistral) and weight distributions. Measure perplexity degradation under extreme conditions and compare against FP16 scaling to quantify the precision trade-offs.

3. **Memory Bandwidth Analysis:** Profile the 66kB Activation SRAM under realistic workloads to measure actual bandwidth utilization, stall rates, and queue depths. Identify the specific conditions under which memory contention occurs and evaluate potential mitigation strategies.