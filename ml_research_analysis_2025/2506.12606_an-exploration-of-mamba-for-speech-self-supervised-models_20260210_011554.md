---
ver: rpa2
title: An Exploration of Mamba for Speech Self-Supervised Models
arxiv_id: '2506.12606'
source_url: https://arxiv.org/abs/2506.12606
tags:
- speech
- mamba
- hubert
- causal
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores Mamba-based HuBERT models for speech self-supervised
  learning, comparing them to Transformer-based SSL architectures. Mamba's linear-time
  complexity enables efficient long-context processing, with MACs remaining constant
  across sequence lengths while Transformer costs grow quadratically.
---

# An Exploration of Mamba for Speech Self-Supervised Models

## Quick Facts
- arXiv ID: 2506.12606
- Source URL: https://arxiv.org/abs/2506.12606
- Authors: Tzu-Quan Lin; Heng-Cheng Kuo; Tzu-Chieh Wei; Hsi-Chun Cheng; Chun-Wei Chen; Hsien-Fu Hsiao; Yu Tsao; Hung-yi Lee
- Reference count: 40
- This paper explores Mamba-based HuBERT models for speech self-supervised learning, comparing them to Transformer-based SSL architectures.

## Executive Summary
This paper investigates replacing Transformer blocks with Mamba blocks in HuBERT for speech self-supervised learning. Mamba's linear-time complexity enables efficient long-context processing, maintaining constant computational cost while Transformers scale quadratically. The authors demonstrate Mamba-based models achieve better performance in long-context and streaming ASR tasks with fewer parameters, while producing higher-quality quantized representations with better phonetic purity and stronger speaker feature capture.

## Method Summary
The paper implements Mamba-based HuBERT by replacing Transformer blocks with Mamba blocks, creating causal and bidirectional variants (ExtBiMamba, InnBiMamba). Models are pretrained following HuBERT's two-iteration pipeline (MFCC targets → layer-6 k-means targets) with reduced batch sizes. Fine-tuning is performed for long-context ASR (document-level) and streaming ASR using CTC loss. Computational efficiency is measured via MACs and RTF, while representation quality is evaluated through phone purity analysis and SUPERB probing benchmarks.

## Key Results
- Mamba-based models maintain near-constant computational cost across sequence lengths while Transformer costs grow quadratically
- Document-level WER improves from 13.37% to 11.08% compared to Transformer in long-context ASR
- With 17% fewer parameters, Mamba achieves 15.77% WER versus 16.66% for Causal Transformer in streaming ASR
- Mamba-based models produce higher-quality quantized representations with better phonetic purity and stronger speaker feature capture

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Mamba-based models maintain near-constant computational cost as sequence length increases, enabling long-context processing that Transformers cannot handle.
- **Mechanism**: The Selective State Space formulation computes outputs via recurrent updates (h_t = Ah_{t-1} + Bx_t) rather than pairwise attention, reducing complexity from O(n²) to O(n). The state vector carries compressed history, avoiding KV-cache growth.
- **Core assumption**: The continuous-time matrices A and B, discretized via Zero-Order Hold, can be trained effectively through backpropagation to capture long-range dependencies in speech.
- **Evidence anchors**:
  - [abstract] "Mamba-based models maintain nearly constant computational cost regardless of sequence length, unlike Transformers whose MACs and RTF increase quadratically"
  - [section IV.A] Figure 1 shows MACs and RTF measurements; Causal Transformer hits OOM at 80 seconds while Mamba processes up to 320 seconds
  - [corpus] Samba-ASR (arxiv:2501.02832) reports similar efficiency gains for Mamba-based ASR
- **Break condition**: If state dimension is insufficient for task complexity, the compressed representation may lose critical information; if discretization step ∆ is poorly parameterized, long-range dependency capture degrades.

### Mechanism 2
- **Claim**: The selection mechanism enables input-dependent state transitions, improving adaptability for speech where phonetic boundaries and speaker characteristics vary dynamically.
- **Mechanism**: Parameters B, C, and ∆ are computed as functions of the input (B = f_B(x), C = f_C(x), ∆ = Broadcast_D(f_∆(x)), allowing the model to selectively retain or discard information per timestep rather than using fixed transition dynamics.
- **Core assumption**: Speech signals benefit from content-aware gating similar to how it helps language modeling; phonetic and speaker features require different retention patterns.
- **Evidence anchors**:
  - [section II.B] Equation 3 defines the selection mechanism
  - [section V.B] CCA analysis shows Mamba-based models capture speaker embeddings more distinctly in early layers and phone labels more sharply in later layers
  - [corpus] Weak direct corpus evidence on selection mechanism specifically for speech; most related work focuses on task performance rather than mechanistic analysis
- **Break condition**: If the projection functions f_B, f_C, f_∆ lack capacity or are poorly initialized, the model may default to near-uniform processing, negating selection benefits.

### Mechanism 3
- **Claim**: Mamba's inherent causal structure provides a natural advantage for streaming ASR, outperforming causally-masked Transformers even with 17% fewer parameters.
- **Mechanism**: Unlike Transformers that require explicit causal masking (which wastes computation on masked positions and was trained bidirectionally), Mamba's recurrence is natively causal—each state depends only on previous states and current input.
- **Core assumption**: Pre-training with causal behavior from initialization produces better causal representations than applying causal masks to bidirectionally-trained models.
- **Evidence anchors**:
  - [section IV.C] Mamba Base (78.2M) achieves 15.77% WER vs. Causal Transformer Base (94.7M) at 16.66%
  - [section VI.A] "Transformer + Causal Mask" baseline performs significantly worse than Causal Transformer, showing pre-training causality matters
  - [corpus] DuplexMamba (arxiv:2502.11123) explores similar streaming conversation applications
- **Break condition**: If future context is genuinely required for the task (e.g., certain prosodic features), the strictly causal model cannot recover that information without architectural modifications.

## Foundational Learning

- **Concept: State Space Models (SSMs) and discretization**
  - Why needed here: Understanding how continuous-time dynamics (A, B matrices) convert to discrete trainable parameters via Zero-Order Hold is essential for debugging convergence issues and interpreting learned representations.
  - Quick check question: Can you explain why SSMs train continuous A, B but compute with discrete A, B during forward passes?

- **Concept: HuBERT training pipeline (masked prediction of pseudo-labels)**
  - Why needed here: The paper replaces Transformer blocks within this framework; understanding the two-iteration training (MFCC targets → layer-6 k-means targets) is necessary to reproduce results or modify the architecture.
  - Quick check question: What is the purpose of the second iteration in HuBERT training, and what targets does it use?

- **Concept: SUPERB probing methodology**
  - Why needed here: Downstream evaluation uses frozen backbones with lightweight task heads; interpretation of PR, SID, ER, IC scores and the composite SUPERB_S metric is required to compare architectures fairly.
  - Quick check question: Why does SUPERB freeze the pretrained model and only train task-specific heads?

## Architecture Onboarding

- **Component map**:
  Input audio → CNN feature extractor → [Mamba blocks × N] → Output representations
                                                     ↓
                                            (optional MLP per block for "Mamba+MLP")

  Causal variant: Standard Mamba blocks (unidirectional scan)
  Bidirectional variants:
    - ExtBiMamba: Two opposite-direction Mamba scans, concatenated
    - InnBiMamba: Bidirectional scan within each block

- **Critical path**:
  1. Pre-train Mamba-based HuBERT following the two-iteration pipeline (250k + 400k steps)
  2. For long-context ASR: Fine-tune ExtBiMamba with CTC loss on document-length audio
  3. For streaming ASR: Fine-tune causal Mamba with CTC loss
  4. For probing: Freeze backbone, train lightweight heads on SUPERB tasks

- **Design tradeoffs**:
  - **Causal vs. Bidirectional**: Causal enables streaming with ~1% WER improvement but cannot leverage future context; bidirectional better for offline tasks but scales worse
  - **ExtBiMamba vs. InnBiMamba**: Paper shows InnBiMamba outperforms at Base scale (SUPERB_S: 832.31 vs. 815.38) but ExtBiMamba wins at Small scale (809.18 vs. 767.60)
  - **MLP addition**: Mamba+MLP slightly improves SUPERB probing but increases parameters; standard Mamba sufficient for ASR fine-tuning

- **Failure signatures**:
  - OOM on long sequences → Confirm you're using Mamba, not Transformer
  - Base-scale bidirectional underperforms Transformer → Try InnBiMamba instead of ExtBiMamba
  - Causal probing poor → Ensure causal pre-training from scratch, not just causal masking at inference

- **First 3 experiments**:
  1. Reproduce the MACs/RTF scaling curve (Figure 1) on your hardware to validate installation and confirm linear scaling
  2. Fine-tune pre-trained Mamba-based HuBERT on a standard ASR dataset (e.g., LibriSpeech) to verify your training pipeline matches reported WERs
  3. Run layer-wise phone purity analysis (Figure 2) to confirm representation quality before investing in full SUPERB evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do Mamba-based HuBERT models exhibit limited scalability in bidirectional settings, and how can this be remedied?
- Basis in paper: [explicit] The authors state "we also observe that they suffer from limited scalability in the bidirectional setting" and note that "the base-size variant underperforms across the board, indicating room for improvement in scalability."
- Why unresolved: The paper documents the phenomenon but does not investigate underlying causes or propose solutions.
- What evidence would resolve it: Ablation studies on larger bidirectional models, analysis of training dynamics across scales, or architectural modifications that restore scaling behavior.

### Open Question 2
- Question: Why does higher phone purity in Mamba-based models not consistently translate to better phoneme recognition performance?
- Basis in paper: [explicit] The authors observe: "while ExtBiMamba performs worse than the Transformer in utterance-level ASR fine-tuning, its peak phone purity slightly surpasses that of the Transformer" and note this misalignment remains unexplained.
- Why unresolved: The authors suggest evaluation instability and representation-task gaps as possibilities but do not conclusively determine the cause.
- What evidence would resolve it: Controlled experiments disentangling quantization quality from downstream task architecture, or analysis of what additional factors beyond phone purity drive PR performance.

### Open Question 3
- Question: How do findings generalize when training Mamba-based HuBERT with standard batch sizes and computational resources?
- Basis in paper: [inferred] The authors acknowledge training with "approximately one-fourth of the original setting" for audio duration per batch due to limited resources. It is unclear whether the observed advantages and limitations persist under full-scale training.
- Why unresolved: No experiments validate whether reduced batch exposure biases the comparison between Mamba and Transformer architectures.
- What evidence would resolve it: Replication of key experiments using original HuBERT batch configurations, or scaling laws analysis across varying batch sizes.

### Open Question 4
- Question: What determines the optimal bidirectional Mamba architecture (ExtBiMamba vs. InnBiMamba) at different model scales?
- Basis in paper: [explicit] The ablation shows InnBiMamba outperforms ExtBiMamba at Base size while the reverse holds at Small size. The authors conclude "the optimal design may vary with model size" without explaining why.
- Why unresolved: The interaction between bidirectional mechanism design and model capacity remains uncharacterized.
- What evidence would resolve it: Systematic study of how information flow in each variant scales with parameters, or intermediate-scale experiments identifying the crossover point.

## Limitations
- The paper demonstrates superior performance on specific tasks and datasets but doesn't comprehensively test across diverse speech domains or languages.
- Computational efficiency claims depend heavily on implementation quality and hardware optimization, with theoretical soundness but practical variability.
- Exact implementation details for bidirectional Mamba variants (ExtBiMamba/InnBiMamba) remain unclear due to limited architectural specifications.

## Confidence
- High confidence: Linear computational complexity of Mamba vs quadratic Transformers (well-established theoretical foundation with empirical validation)
- Medium confidence: Mamba-based SSL performance advantages in long-context and streaming ASR (supported by ablation studies but limited to specific benchmarks)
- Medium confidence: Higher-quality quantized representations and phonetic purity (CCA and purity analyses provide evidence but could benefit from additional perceptual studies)

## Next Checks
1. **Implementation Verification**: Implement and validate the exact bidirectional Mamba variants (ExtBiMamba and InnBiMamba) by reproducing the SUPERB probing results, particularly the scale-dependent performance differences between the two approaches.
2. **Cross-Domain Generalization**: Evaluate the pre-trained Mamba-based HuBERT models on out-of-domain speech datasets (e.g., multilingual speech, accented speech, or non-standard speaking styles) to assess robustness beyond TEDLIUM3 and LibriSpeech.
3. **Ablation on Selection Mechanism**: Conduct controlled experiments disabling the input-dependent selection (fixing B, C, ∆ as constants) to quantify the contribution of the selection mechanism to the observed performance gains in speech-specific tasks.