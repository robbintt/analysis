---
ver: rpa2
title: Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social
  Media Influencer Marketing
arxiv_id: '2510.08111'
source_url: https://arxiv.org/abs/2510.08111
tags:
- legal
- article
- content
- posts
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates how well large language models (LLMs) detect
  undisclosed influencer marketing content and provide legally grounded explanations.
  Using 1,143 Instagram posts, two LLMs were tested under three prompting strategies
  with varying levels of legal context.
---

# Evaluating LLM-Generated Legal Explanations for Regulatory Compliance in Social Media Influencer Marketing

## Quick Facts
- arXiv ID: 2510.08111
- Source URL: https://arxiv.org/abs/2510.08111
- Reference count: 40
- Primary result: LLM models achieve F1 up to 0.93 in detecting undisclosed influencer ads, but explanation quality shows high citation error rates (28.57%)

## Executive Summary
This study evaluates how well large language models detect undisclosed influencer marketing content and provide legally grounded explanations. Using 1,143 Instagram posts, two LLMs were tested under three prompting strategies with varying levels of legal context. Both models achieved strong overall classification accuracy (F1 up to 0.93), but performance dropped by over 10 points on ambiguous cases. Analysis of LLM explanations revealed frequent citation omissions (28.57%), unclear references (20.71%), and high error rates for hidden ads (28.57%). While adding regulatory text improved explanation quality, it did not consistently enhance detection accuracy. The findings highlight the need for explanation quality auditing alongside accuracy metrics to ensure automated moderation is both effective and legally defensible.

## Method Summary
The study used 1,143 English Instagram posts (592 disclosed ads, 127 hidden ads, 424 organic) with three prompting strategies varying legal context: no_article, article, and article_explanation. Two LLMs (gpt-5-nano and gemini-2.5-flash-lite) performed zero-shot binary classification of posts as ads/non-ads, generating legal explanations. Classification accuracy was measured via F1 score, while explanation quality was evaluated through 7-category error taxonomy by trained annotators. A baseline TF-IDF + Logistic Regression model achieved F1 ≈ 0.88.

## Key Results
- Both LLMs achieved F1 scores up to 0.93 on overall classification, with Gemini better for recall (hidden ads) and GPT better for precision (organic content)
- Performance dropped over 10 F1 points on ambiguous cases where human annotators showed moderate agreement (Krippendorff's α = 0.74)
- Explanation quality analysis revealed 28.57% citation omissions, 20.71% unclear references, and 28.57% high error rates for hidden ads
- Adding regulatory text to prompts improved human-rated explanation helpfulness (4.37 vs 3.31 on 5-point scale) but did not consistently improve classification accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs classify sponsored content primarily through pretraining-derived pattern recognition, not through real-time legal knowledge application.
- Mechanism: Models detect promotional language, product placement cues, and persuasive rhetorical devices learned during pretraining. When legal text is added to prompts, classification accuracy does not consistently improve—suggesting the detection pathway relies on implicit stylistic markers rather than explicit rule application.
- Core assumption: Promotional language patterns are sufficiently represented in pretraining data to enable detection without domain-specific grounding.
- Evidence anchors:
  - [abstract] "While adding regulatory text to the prompt improves explanation quality, it does not consistently enhance detection accuracy."
  - [section 4.1] "More legal knowledge does not automatically translate into better classification outcomes... LLMs' ability to apply legal knowledge may rely more on patterns learned during pretraining rather than the provided legal text."
  - [corpus] Related work (TAIGR) notes claim-centric methods struggle with influencer discourse pragmatics—supporting the need for stylistic over literal detection.
- Break condition: If detection required novel or jurisdiction-specific advertising norms absent from pretraining data, this mechanism would degrade significantly.

### Mechanism 2
- Claim: Legal explanation quality degrades through citation failures and structural incoherence, even when classification is correct.
- Mechanism: Models generate justifications by retrieving legal-sounding language, but frequently omit citations (28.57%), provide unclear references (20.71%), or misinterpret provisions. Without enforced citation scaffolding, outputs approximate legal reasoning without satisfying procedural standards.
- Core assumption: Models can generate plausible-sounding legal language but lack reliable mechanisms for grounding citations to specific provided texts.
- Evidence anchors:
  - [abstract] "Analysis of LLM explanations revealed frequent citation omissions (28.57%), unclear references (20.71%), and high error rates for hidden ads (28.57%)."
  - [section 4.2] "Critical citation errors (e2, e3) dominate no_article prompts: 81.43% and 25.71% for Gemini, 78.57% and 35.71% for GPT."
  - [section 4.2 Case analysis] "Neither model was able to generate a cohesive, well-structured legal explanation... comparable to a rather poorly performing first-year law student."
- Break condition: If downstream applications require legally defensible justifications (e.g., regulatory enforcement), high accuracy alone is insufficient—citation auditing becomes mandatory.

### Mechanism 3
- Claim: Prompting strategy has divergent effects: legal context improves perceived explanation helpfulness but not classification accuracy.
- Mechanism: Providing regulatory text gives models citation material, improving human-rated helpfulness (4.37 vs 3.31 on 5-point scale). However, classification relies on separate pretraining-based detection pathways unaffected by prompt-provided rules.
- Core assumption: Explanation quality and classification accuracy are partially decoupled processes in current LLM architectures.
- Evidence anchors:
  - [abstract] "While adding regulatory text to the prompt improves explanation quality, it does not consistently enhance detection accuracy."
  - [section 4.1] "For GPT, although article_explanation maximises precision, it reduces recall, resulting in the lowest F1 (0.90)."
  - [section 4.2] "Helpfulness scores (1–5 scale) show Gemini with article_explanation performs best (4.37 ± 0.75)... No_article variants for both models achieve the lowest scores."
  - [corpus] Limited direct corpus evidence on this specific divergence pattern.
- Break condition: If architectures evolved to jointly optimize classification and explanation (e.g., via fine-tuning on legally-annotated examples), the decoupling might reduce.

## Foundational Learning

- Concept: **IRAC legal reasoning structure** (Issue-Rule-Application-Conclusion)
  - Why needed here: The paper shows LLM outputs lack coherent legal structure, producing "amalgam of statements" rather than methodical arguments. Understanding IRAC helps diagnose where explanations fail.
  - Quick check question: Can you identify which component (Issue, Rule, Application, Conclusion) is missing or malformed in a given LLM explanation?

- Concept: **Explanation quality vs. accuracy decoupling**
  - Why needed here: The core finding is that high classification accuracy (F1 0.93) coexists with legally inadequate explanations. Engineers must learn to audit both dimensions separately.
  - Quick check question: If a system achieves 95% classification accuracy but 30% citation omission rate, is it deployment-ready for regulatory enforcement?

- Concept: **Ambiguity performance degradation**
  - Why needed here: Performance drops >10 F1 points on ambiguous cases where even human annotators disagree (Krippendorff's α = 0.74). Systems must handle uncertainty explicitly.
  - Quick check question: How would your system flag cases with high annotator disagreement for human review rather than automated decision?

## Architecture Onboarding

- Component map:
  Input layer -> Preprocessing module -> Prompting module (3 strategies) -> Classification head -> Explanation generator -> Evaluation layer (dual-track)

- Critical path:
  1. Dataset construction → 1,143 posts (592 disclosed ads, 127 undisclosed, 424 organic)
  2. Prompt engineering → three legal-context variants
  3. Model inference → GPT-5-nano, Gemini-2.5-flash-lite
  4. Quantitative eval → F1 by content type and ambiguity level
  5. Qualitative eval → 60-sample explanation audit for error taxonomy

- Design tradeoffs:
  - **Gemini vs. GPT**: Gemini better for recall (hidden ad detection); GPT better for precision (organic content). Choose based on enforcement priorities—false positives vs. false negatives.
  - **Article_explanation prompt**: Higher helpfulness but potential recall drop. Use for human-in-the-loop review, not automated flagging.
  - **Error severity weighting**: Not all errors equal—hallucinated citations (e4, 2.62%) are severe; unclear citations (e3, 20.71%) are tolerable. Build severity-aware routing.

- Failure signatures:
  - **Citation omission spike** (>75%) → indicates no_article prompt; switch to article variant
  - **Hidden ad miscue rate** (>25%) → recall-focused model needed; prefer Gemini
  - **Hallucination in legal citations** → immediately route to human review
  - **Reasoning-conclusion mismatch** (e7, rare but critical) → reject output, re-prompt

- First 3 experiments:
  1. **Baseline calibration**: Run logistic regression (TF-IDF) on 80/20 split to establish non-LLM benchmark. Compare against LLM performance on ambiguous cases specifically.
  2. **Prompt ablation test**: Measure classification F1 and explanation helpfulness across all three prompting strategies on a 100-post stratified sample. Confirm decoupling pattern.
  3. **Error severity audit**: Annotate 30 explanations per model for all 7 error types. Calculate severity-weighted error scores and identify high-risk content types (likely: undisclosed ads).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What mechanisms enable LLMs to transition from generating superficially plausible legal explanations to producing genuinely reasoned legal analysis with systematic application of relevant provisions?
- Basis in paper: [explicit] The paper states: "These patterns raise a key question: do models genuinely understand legal content or simply produce superficially plausible explanations?" and the case analysis concludes that "neither model was able to generate a cohesive, well-structured legal explanation" comparable to even a basic legal analysis.
- Why unresolved: The study evaluates explanation quality but does not test interventions (e.g., structured reasoning frameworks, fine-tuning on legal argumentation, or retrieval-augmented generation) that might improve the depth and coherence of legal reasoning.
- What evidence would resolve it: A follow-up study comparing different methods for improving legal reasoning structure—such as fine-tuning on annotated legal analyses, incorporating retrieval-augmented generation with verified legal sources, or using structured prompting that enforces IRAC-style argumentation—measured against expert legal evaluation of reasoning coherence and provision selection.

### Open Question 2
- Question: Does domain-specific fine-tuning on influencer marketing law improve both classification accuracy and explanation quality, particularly for ambiguous cases and hidden ads?
- Basis in paper: [explicit] The limitations section states that "the study relies on off-the-shelf LLMs without fine-tuning, meaning performance could improve with domain-specific adaptation," explicitly noting this as a direction for future investigation.
- Why unresolved: The experiments only use zero-shot prompting with off-the-shelf models; the potential benefits of supervised or instruction fine-tuning on domain-specific legal data remain untested.
- What evidence would resolve it: Experiments comparing zero-shot, few-shot, and fine-tuned LLMs on the same dataset, measuring both classification performance (especially on ambiguous cases) and explanation quality using the paper's error taxonomy.

### Open Question 3
- Question: To what extent does incorporating multimodal signals (images, video frames, hashtags, and metadata) improve detection accuracy and reduce false positives on ambiguous influencer content?
- Basis in paper: [explicit] The limitations section notes that "our dataset focuses solely on textual content, excluding visual or multimodal signals that frequently convey sponsorship."
- Why unresolved: The study acknowledges that sponsorship cues often appear visually (product placement, brand logos, visual disclosure badges) or in metadata, but the current experimental design cannot assess their contribution.
- What evidence would resolve it: A comparative study using a multimodal dataset with annotated visual sponsorship cues, evaluating LLMs or multimodal models on detection accuracy and explanation quality versus text-only baselines.

## Limitations
- The study uses specific model versions (gpt-5-nano, gemini-2.5-flash-lite) that may not be publicly available, creating reproducibility challenges
- Evaluation focuses exclusively on English Instagram posts with text-only analysis, limiting generalizability to other platforms, languages, or multimodal content
- The performance gap between current publicly available models and those tested could significantly alter the results

## Confidence
- **High Confidence**: Classification accuracy metrics (F1 up to 0.93) and overall error rate patterns across the 7-category taxonomy
- **Medium Confidence**: The decoupling between explanation quality and classification accuracy, as this pattern aligns with broader LLM behavior literature
- **Low Confidence**: Cross-platform generalizability and real-world deployment effectiveness, given the controlled experimental conditions

## Next Checks
1. Replicate classification performance using current publicly available LLM models (gpt-4o, gemini-1.5) to establish performance baselines and identify any significant degradation
2. Conduct a human evaluation study comparing LLM-generated explanations against actual regulatory guidance to validate the helpfulness scoring methodology
3. Test the prompting strategies on a multilingual dataset (e.g., Spanish, German) to assess cross-linguistic robustness of both detection and explanation generation