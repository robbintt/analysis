---
ver: rpa2
title: Improving BERT for Symbolic Music Understanding Using Token Denoising and Pianoroll
  Prediction
arxiv_id: '2507.04776'
source_url: https://arxiv.org/abs/2507.04776
tags:
- music
- symbolic
- tasks
- information
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a BERT-like model for symbolic music understanding,
  addressing the limitation of traditional masked language modeling (MLM) which treats
  music notes as isolated tokens without capturing their musical relationships. The
  authors propose two novel pre-training objectives: token denoising, where corrupted
  note attributes are reconstructed to learn pitch intervals and musical proximity,
  and pianoroll prediction, where the model predicts bar-level and local pianoroll
  representations from corrupted input to capture temporal and interval information.'
---

# Improving BERT for Symbolic Music Understanding Using Token Denoising and Pianoroll Prediction

## Quick Facts
- arXiv ID: 2507.04776
- Source URL: https://arxiv.org/abs/2507.04776
- Authors: Jun-You Wang; Li Su
- Reference count: 0
- Primary result: State-of-the-art performance on 6/10 tasks vs comparable baselines using token denoising and pianoroll prediction

## Executive Summary
This paper introduces M2BERT, a BERT-like model for symbolic music understanding that addresses limitations of standard masked language modeling by incorporating music-specific pre-training objectives. The authors propose token denoising, where corrupted note attributes are reconstructed to learn pitch intervals and musical proximity, and pianoroll prediction, where the model predicts bar-level and local pianoroll representations from corrupted input. The model is evaluated on a comprehensive benchmark of 12 downstream tasks including chord estimation, genre classification, and melody extraction, demonstrating consistent improvements over the baseline MidiBERT model.

## Method Summary
The authors developed M2BERT using a ModernBERT backbone (12-layer Transformer with RoPE and Flash Attention) and two novel pre-training objectives. Token denoising corrupts 30% of note attributes (pitch, duration, position) within restricted ranges (r_pos=4, r_pitch=12, r_dur=12) and reconstructs them using cross-entropy loss. Pianoroll prediction trains the model to predict bar-level and local pianoroll representations (16x86 pianoroll + 16x12 chromagram) from corrupted input using L2 loss. The model is pre-trained on POP909, Pop1K7, EMOPIA, Pianist8, ASAP (4.89M notes) and optionally Lakh MIDI (350M notes), then evaluated on 12 downstream tasks with task-specific fine-tuning.

## Key Results
- State-of-the-art performance on 6 out of 10 tasks with comparable baselines
- Consistent improvements across all 12 downstream tasks compared to MidiBERT baseline
- Restricted noise injection (RC_4,12,12) outperforms random masking (RC_∞) and standard MLM
- Increasing context window from 512 to 2048 tokens significantly improves chord and key estimation performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Restricted noise injection forces the model to learn pitch intervals and musical proximity better than uniform random masking.
- **Mechanism:** Instead of replacing tokens with generic `[MASK]`, the model perturbs note attributes within limited ranges (±12 semitones for pitch). To minimize reconstruction loss, the model must learn the relative distance between tokens, effectively internalizing musical relationships.
- **Core assumption:** The model's ability to reconstruct a token depends on learning the local "neighborhood" of that token in musical space.
- **Evidence anchors:** RC_4,12,12 outperforms RC_∞ and standard MLM in Table 1; music-domain inductive biases improve foundation models.
- **Break condition:** If corruption range r exceeds total vocabulary size or is set to 0, the mechanism degrades to random guessing.

### Mechanism 2
- **Claim:** Predicting pianoroll and chromagram representations induces geometric awareness of simultaneity and harmony that sequential token prediction misses.
- **Mechanism:** The model projects corrupted note sequences onto 2D representations (bar-level and local pianorolls), forcing the encoder to aggregate simultaneous events into a unified temporal grid, capturing chord structures and texture.
- **Core assumption:** The structure of a pianoroll contains unique musical information that acts as a strong inductive bias for understanding symbolic music.
- **Evidence anchors:** Bar-level prediction helps the model implicitly learn the idea of pianoroll; pianoroll geometry captures polyphonic dependencies.
- **Break condition:** If the model predicts a bar-averaged representation or L2 loss weighting is too weak, the geometric signal is lost.

### Mechanism 3
- **Claim:** Increasing context window length and model efficiency are prerequisite architectural enablers for the proposed objectives to function effectively.
- **Mechanism:** Musical tasks like chord estimation require long-range temporal context. ModernBERT's RoPE and Flash Attention allow efficient processing of 1024-2048 token sequences, enabling the denoising and pianoroll mechanisms to operate over full musical phrases.
- **Core assumption:** Performance gains come from both new objectives and the ability to process longer dependencies which those objectives help structure.
- **Evidence anchors:** Increasing length from 512 to 2048 boosts CR performance significantly (+5.4%); ModernBERT architecture advantages confirmed.
- **Break condition:** If computational resources limit sequence length to <512 tokens, the model cannot capture bar-level context required for pianoroll prediction.

## Foundational Learning

- **Concept: Compound Word (CP) Representation**
  - **Why needed here:** Understanding that a single note is a tuple (bar, position, pitch, duration) is required to implement the noise injection logic correctly.
  - **Quick check question:** How would you apply a pitch-shift noise of +5 to a token without altering its onset position?

- **Concept: Inductive Bias in Pre-training**
  - **Why needed here:** The core contribution is replacing "general purpose" MLM bias with "music specific" bias (intervals, pianoroll geometry).
  - **Quick check question:** Why does standard Random Masking fail to teach the model that C4 and C5 are harmonically related?

- **Concept: Representation Alignment (Token vs. Pianoroll)**
  - **Why needed here:** The model learns via discrete classifier (token correction) and continuous regressor (pianoroll prediction) heads.
  - **Quick check question:** Does the pianoroll head predict a binary mask or continuous representation, and how does that affect L2 loss?

## Architecture Onboarding

- **Component map:** MIDI → Compound Word (CP) Tokens (b, pos, pit, dur) → Corruptor → ModernBERT → Token Head (Linear + Softmax) + Pianoroll Head (Linear + L2 Loss)

- **Critical path:** The Corruptor logic is most critical. Success relies on specific constraints r_pos=4, r_pitch=12, r_dur=12. If logic allows out-of-bounds values or uses infinite ranges, "proximal learning" breaks.

- **Design tradeoffs:**
  - Sequence Length: 2048 tokens improve Chord/Key estimation but increase compute cost quadratically. 1024 is selected sweet spot.
  - Dataset Size vs. Epochs: "Full" dataset trains for only 25 epochs vs 150 for "Reduced," favoring data scale over convergence depth.

- **Failure signatures:**
  - Collapse to Mean: If pianoroll prediction loss dominates, token embeddings might become too smooth, losing note-level distinctiveness.
  - VF Task Degradation: Underperformance on Violin Fingering suggests structural biases over-regularizing against idiosyncratic physical constraints.

- **First 3 experiments:**
  1. Validate Noise Constraints: Compare RC_Infinity vs RC_4,12,12 on Melody Extraction to confirm restricted noise improves interval learning.
  2. Sequence Length Stress Test: Benchmark 512 vs 1024 vs 2048 tokens on Chord Root task to verify quadratic performance jump.
  3. Head Ablation: Train with only Token Denoising vs only Pianoroll Prediction to isolate which objective drives gains across tasks.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can symbolic music pre-training objectives be designed to better capture rhythmic information beyond pitch intervals and temporal structure?
- **Basis in paper:** [explicit] Future direction: "developing methods to optimize other types of musical information (e.g., rhythm)"
- **Why unresolved:** Current objectives primarily target pitch-related knowledge; beat note prediction showed minimal improvement.
- **What evidence would resolve it:** New pre-training objectives targeting rhythmic patterns with demonstrated improvements on rhythm-heavy downstream tasks.

### Open Question 2
- **Question:** What are effective pathways for scaling symbolic music pre-training to approach data scale of text-based models?
- **Basis in paper:** [explicit] "Our symbolic pre-trained model was only trained on at most 350M notes—whereas text pre-trained models can leverage trillions of text tokens—there is significant potential for scaling up"
- **Why unresolved:** Computational resource limitations prevented full training convergence; relationship between data scale and performance unexplored.
- **What evidence would resolve it:** Systematic experiments training on progressively larger datasets (1B, 10B notes) with scaling law analysis.

### Open Question 3
- **Question:** Can proposed objectives extend effectively to integrative multi-output tasks like complete functional harmony recognition?
- **Basis in paper:** [explicit] Future directions include "entire functional harmony recognition task and performance MIDI-to-score conversion task"
- **Why unresolved:** Current SMC benchmark only includes classification tasks; integrative tasks require coordinated multi-component outputs.
- **What evidence would resolve it:** Adapting M2BERT to multi-task frameworks for complete functional harmony with competitive results.

### Open Question 4
- **Question:** Why does M2BERT underperform MidiBERT on violin fingering prediction, and does this reveal limitations for performer preference tasks?
- **Basis in paper:** [inferred] VF is "only task where MidiBERT outperforms M2BERT" and may relate to VF focusing on "performer's empirical preferences rather than music theory"
- **Why unresolved:** Proposed objectives encode music-theoretic relationships but performer-specific choices may rely on embodied knowledge not captured.
- **What evidence would resolve it:** Ablation studies comparing different pre-training objectives on performer-preference tasks.

## Limitations

- **Model Architecture Details:** Critical hyperparameters (hidden size, FFN size, attention heads, dropout rates) for ModernBERT backbone and pianoroll head are not specified.
- **Pre-training Data and Epochs:** Rationale for 150 epochs on 4.89M notes vs 25 epochs on 350M notes is unexplained.
- **VF Task Performance:** Model underperforms on Violin Fingering compared to MidiBERT, suggesting structural biases may over-regularize against performer-specific constraints.

## Confidence

**High Confidence:** Restricted noise injection (RC_4,12,12) outperforms random masking and standard MLM (Table 1); ModernBERT architecture improvements enabling longer context windows are well-demonstrated (Table 2).

**Medium Confidence:** Pianoroll prediction captures temporal and interval information not present in sequential token prediction is plausible but lacks ablation isolating pianoroll's contribution; state-of-the-art results rely on full model rather than individual objective contributions.

**Low Confidence:** Generalization to longer musical pieces beyond 1024-2048 token context window is uncertain; paper does not validate whether learned representations transfer to pieces requiring longer context.

## Next Checks

1. **Noise Constraint Ablation:** Run controlled experiments comparing `RC_Infinity` vs `RC_4,12,12` on Melody Extraction to definitively confirm restricted corruption ranges improve interval learning.

2. **Sequence Length Stress Test:** Benchmark 512 vs 1024 vs 2048 tokens on Chord Root task to verify quadratic performance improvement and establish practical limits for different musical tasks.

3. **Head Ablation Study:** Train separate models with only Token Denoising vs only Pianoroll Prediction to isolate which objective drives performance improvements across the 12 downstream tasks.