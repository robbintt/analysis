---
ver: rpa2
title: 'Coefficient-to-Basis Network: A Fine-Tunable Operator Learning Framework for
  Inverse Problems with Adaptive Discretizations and Theoretical Guarantees'
arxiv_id: '2503.08642'
source_url: https://arxiv.org/abs/2503.08642
tags:
- network
- learning
- error
- operator
- c2bnet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new operator learning framework, C2BNet,
  designed for solving inverse problems in PDEs. The key innovation is the ability
  to adapt to different discretizations through fine-tuning a pre-trained network,
  significantly reducing computational cost.
---

# Coefficient-to-Basis Network: A Fine-Tunable Operator Learning Framework for Inverse Problems with Adaptive Discretizations and Theoretical Guarantees

## Quick Facts
- arXiv ID: 2503.08642
- Source URL: https://arxiv.org/abs/2503.08642
- Authors: Zecheng Zhang; Hao Liu; Wenjing Liao; Guang Lin
- Reference count: 40
- Primary result: Introduces C2BNet framework for inverse PDE problems with adaptive discretization through fine-tuning, achieving theoretical guarantees with error bounds depending on intrinsic dimension rather than ambient dimension

## Executive Summary
This paper introduces C2BNet, a novel operator learning framework designed to solve inverse problems in partial differential equations while adapting to different discretizations. The key innovation is decomposing the operator into a non-linear coefficient mapping and a linear basis mapping, enabling efficient fine-tuning when discretization changes. The framework demonstrates theoretical guarantees with error bounds that depend on the intrinsic dimension of the data manifold rather than the ambient discretization dimension, addressing the curse of dimensionality. Numerical experiments validate the approach on elliptic and time-dependent diffusion equations, showing superior performance compared to traditional methods.

## Method Summary
C2BNet uses a two-component architecture: a deep neural network (Coefficient Network) that maps input PDE solutions to coefficients, and a linear layer (Basis Network) that maps these coefficients to output parameters through basis functions. The network is pre-trained on a coarse discretization, then fine-tuned on new discretizations by freezing the Coefficient Network and retraining only the linear Basis Network. This approach leverages the theoretical insight that coefficients depend only on the input function and operator, not on the output discretization grid. The framework includes theoretical analysis establishing approximation and generalization error bounds that scale with intrinsic dimension rather than ambient dimension.

## Key Results
- Achieves significant computational savings by fine-tuning only the linear Basis Network when discretization changes, reducing trainable parameters by approximately 7x
- Demonstrates superior performance on inverse problems including elliptic equations, radiative transfer equations, and time-dependent diffusion equations
- Provides theoretical guarantees with generalization error bounds that depend on intrinsic dimension d‚ÇÅ rather than ambient dimension D‚ÇÅ
- Successfully validates the discretization adaptation strategy, maintaining accuracy when transferring from coarse to fine grids

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing the operator into a non-linear coefficient map and a linear basis map allows the model to isolate discretization-dependent features from the intrinsic physics.
- **Mechanism:** The architecture represents the output function v as a linear combination of basis functions œâ‚Çñ weighted by coefficients Œ±‚Çñ. The Coefficient Network learns the mapping from input to coefficients (Œ±‚Çñ), while the Basis Network acts as a linear decoder reconstructing the output on a specific grid.
- **Core assumption:** The output functions (PDE parameters) approximately lie in a low-dimensional linear subspace (Assumption 1), meaning they can be accurately reconstructed by a finite set of bases.
- **Evidence anchors:**
  - [section 3.1] "Our network structure is designed based on the decomposition in (5)... consists of two components: the coefficients... and the bases."
  - [abstract] "The framework consists of a coefficient network and a basis network..."
- **Break condition:** If the output space Y does not lie on a low-dimensional linear subspace (i.e., requires infinite bases for accurate reconstruction), the approximation error Œ∂ will dominate, causing reconstruction failure.

### Mechanism 2
- **Claim:** Freezing the Coefficient Network and retraining only the linear Basis Network enables efficient adaptation to new discretizations.
- **Mechanism:** Theoretically, the coefficients Œ±‚Çñ depend only on the input function u and the operator, not the output discretization grid. Therefore, when moving to a finer grid, the non-linear feature extraction (coefficients) remains valid; only the linear mapping to the new grid points (bases) requires updating.
- **Core assumption:** The discretization of the output S_Y preserves the inner product structure sufficiently well such that the coefficients calculated via the old discretization generalize (Assumption 2).
- **Evidence anchors:**
  - [section 3.2] "The coefficients... are determined solely by the input function u and are independent of the discretization $S_Y$."
  - [corpus] The neighbor paper "Learning Operators through Coefficient Mappings in Fixed Basis Spaces" supports the efficacy of separating coefficients from basis mappings in operator learning.
- **Break condition:** If the new discretization requires a fundamentally different functional representation (e.g., moving from smooth functions to highly discontinuous ones that break the bandlimited assumption), freezing the trunk network will fail to capture the new features.

### Mechanism 3
- **Claim:** Generalization error scales with the intrinsic dimension of the input data manifold rather than the ambient discretization dimension.
- **Mechanism:** By assuming input functions lie on a low-dimensional manifold ùí© (Assumption 4), the network complexity required to learn the operator scales with d‚ÇÅ (intrinsic dimension) rather than D‚ÇÅ (grid size), mitigating the curse of dimensionality.
- **Core assumption:** The input data ùí≥ is supported on a low-dimensional Riemannian manifold with bounded reach and the operator is Lipschitz (Assumption 5).
- **Evidence anchors:**
  - [section 4.2] "The minimizer... satisfies [error bound]... where C is a constant... [depending] on $d_1$... converging at the rate $n^{-2/(2+d_1)}$."
  - [corpus] "Learning Generalizable Neural Operators for Inverse Problems" corroborates the difficulty of inverse maps and the need for specialized structural assumptions to ensure generalization.
- **Break condition:** If the input data is high-dimensional and does not exhibit low-dimensional manifold structure (e.g., pure noise or fully turbulent fields with high intrinsic dimension), the sample complexity will revert to scaling with the ambient dimension.

## Foundational Learning

- **Concept:** **Singular Value Decomposition (SVD) / Principal Component Analysis (PCA)**
  - **Why needed here:** The paper assumes output functions can be spanned by a set of orthonormal bases œâ‚Çñ. Understanding linear subspaces is required to interpret how the "Basis Network" approximates the output space.
  - **Quick check question:** Can you explain why the reconstruction error Œ∂ decreases as the number of basis functions d‚ÇÇ increases?

- **Concept:** **Lipschitz Continuity**
  - **Why needed here:** The theoretical guarantees (Theorem 1 and 2) rely on the operator Œ® being Lipschitz continuous. This ensures that small changes in the input solution do not cause infinite explosions in the inferred parameter.
  - **Quick check question:** If the inverse problem is ill-posed (e.g., severely unstable), would the Lipschitz assumption (Assumption 5) hold?

- **Concept:** **Transfer Learning / Fine-tuning**
  - **Why needed here:** The core value proposition of C2BNet is fine-tuning on new discretizations. Understanding the difference between "freezing layers" and "retraining layers" is practically necessary for implementation.
  - **Quick check question:** In the proposed fine-tuning strategy (Section 3.2), which specific component of the network is frozen and which is updated?

## Architecture Onboarding

- **Component map:** Input -> 100 -> 100 -> 100 -> d_low -> Output
- **Critical path:** The "Basis Net" is a single linear layer. During pre-training, gradients must flow through this linear layer to train the deep Coefficient Net. During adaptation, only the linear layer is updated.
- **Design tradeoffs:**
  - **Latent dimension d‚ÇÇ:** A small d‚ÇÇ reduces parameters but risks high projection error Œ∂ if the true parameter space is complex. A large d‚ÇÇ improves accuracy but increases computational cost and may lead to overfitting on small datasets.
  - **Depth of Coefficient Net:** Deeper networks approximate better (Theorem 1) but make the fine-tuning assumption (that coefficients are independent of discretization) more brittle if the network overfits to the initial grid topology.
- **Failure signatures:**
  - **Stagnation during fine-tuning:** If freezing the Coefficient Net results in a validation error that does not decrease, the assumption that output bases are the only discretization-dependent variable is violated.
  - **High Projection Error Œ∂:** If the loss plateaus well above the noise level regardless of network width, the chosen basis dimension d‚ÇÇ is too small to capture the variance of the inverse parameters.
- **First 3 experiments:**
  1. **Sanity Check (Elliptic Equation):** Replicate the Elliptic example (Section 5.2). Train on a 10√ó10 mesh. Freeze f_coef and retrain only the last layer on a 20√ó20 mesh. Verify that accuracy is maintained with 7√ó fewer trainable parameters than full retraining.
  2. **Ablation on Latent Dimension:** Systematically vary d_low (e.g., 5, 20, 50, 100) for the RTE problem. Plot relative error vs. d_low to identify the "elbow" where projection error Œ∂ becomes negligible.
  3. **Robustness to Noise:** Add Gaussian noise to the input observations u (Assumption 6) and measure the degradation of the inferred coefficients Œ± vs. the final reconstruction to validate the stability of the trunk network.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can C2BNet be extended to handle output functions that reside on low-dimensional nonlinear manifolds rather than linear subspaces?
- **Basis in paper:** [inferred] Assumption 1 restricts the output functions Y to a linear subspace spanned by orthonormal bases, which motivates the linear basis network; however, the authors note that PDE solutions (inputs) exhibit nonlinear structures.
- **Why unresolved:** The current architecture uses a linear layer for the basis network (f_basis), which inherently assumes a linear subspace approximation for the output QoIs.
- **What evidence would resolve it:** Theoretical analysis or numerical experiments demonstrating successful reconstruction of inverse QoIs defined by nonlinear manifolds (e.g., complex textures) without increasing the projection error Œ∂.

### Open Question 2
- **Question:** Does the fine-tuning strategy maintain its efficiency and accuracy when transferring to unstructured meshes or irregular geometries?
- **Basis in paper:** [inferred] The numerical experiments are restricted to uniform grids (e.g., 10√ó10, 20√ó20) and regular intervals, whereas practical engineering problems often utilize unstructured finite element meshes.
- **Why unresolved:** The paper does not test the "linear layer update" strategy on domains where the discretization operator S_Y results in non-uniform node spacing or topological changes.
- **What evidence would resolve it:** Empirical results showing that updating only the last linear layer suffices for fine-tuning on complex, unstructured triangulations without significant loss of accuracy.

### Open Question 3
- **Question:** Is the convergence rate n^(-2/(2+d‚ÇÅ)) tight, or can it be improved for specific classes of smooth operators?
- **Basis in paper:** [explicit] The paper establishes generalization error bounds depending on intrinsic dimension d‚ÇÅ but does not provide lower bounds to confirm optimality.
- **Why unresolved:** Theoretical upper bounds are derived, but without lower bounds, it is unclear if the network architecture fully exploits the smoothness or specific structures of the operator class.
- **What evidence would resolve it:** Derivation of matching lower bounds for the approximation error or empirical validation of the convergence rate scaling across varying intrinsic dimensions.

## Limitations

- The framework relies on strong assumptions about low-dimensional linear subspaces and Lipschitz continuity that may not hold for complex, high-frequency output functions
- The theoretical bounds depend on constants that aren't computable without knowing the true data distribution, limiting practical applicability
- The discretization adaptation strategy may fail when transferring to unstructured meshes or irregular geometries where the discretization operator changes fundamentally
- The approach requires careful selection of latent dimension d‚ÇÇ to balance projection error and computational cost

## Confidence

- **High Confidence:** The architecture design (coefficient + basis decomposition) is sound and the fine-tuning strategy is well-justified for linear output maps
- **Medium Confidence:** The generalization bounds are mathematically rigorous but may not be tight in practice; the assumptions are idealized
- **Low Confidence:** The practical impact of the discretization adaptation claim depends heavily on the specific PDE and discretization method used

## Next Checks

1. **Empirical Manifold Validation:** Test the low-dimensional manifold assumption by computing intrinsic dimensionality estimates (e.g., using PCA or manifold learning techniques) on real PDE solution datasets

2. **Discretization Sensitivity Analysis:** Systematically vary discretization parameters (mesh size, element type) and measure how coefficient network performance degrades to validate Assumption 2

3. **Cross-Operator Transfer:** Test whether pre-trained coefficients from one PDE operator (e.g., elliptic) can be effectively fine-tuned for a different operator (e.g., parabolic) to assess the generality of the coefficient learning approach