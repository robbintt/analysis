---
ver: rpa2
title: Deeply Explainable Artificial Neural Network
arxiv_id: '2505.06731'
source_url: https://arxiv.org/abs/2505.06731
tags:
- dxann
- explainability
- deep
- learning
- explanations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DxANN addresses the lack of transparency in deep learning models
  by embedding explainability directly into the training process, rather than relying
  on post-hoc interpretation methods. The core innovation is the Explainability Contribution
  Score (ECS), which measures the influence of individual features on predictions
  by comparing embedded feature values to class-specific distribution means in a latent
  space.
---

# Deeply Explainable Artificial Neural Network

## Quick Facts
- arXiv ID: 2505.06731
- Source URL: https://arxiv.org/abs/2505.06731
- Authors: David Zucker
- Reference count: 0
- DxANN achieves 97.1% accuracy for diabetic macular edema diagnosis and 97.2% for osteoarthritis diagnosis

## Executive Summary
DxANN addresses the lack of transparency in deep learning models by embedding explainability directly into the training process, rather than relying on post-hoc interpretation methods. The core innovation is the Explainability Contribution Score (ECS), which measures the influence of individual features on predictions by comparing embedded feature values to class-specific distribution means in a latent space. This ante-hoc approach provides per-sample, per-feature explanations as part of the forward pass, eliminating the need for external interpretation tools. In medical image classification tasks, CNN-based DxANN achieved 97.1% accuracy for diabetic macular edema diagnosis and 97.2% for osteoarthritis diagnosis, matching or exceeding the performance of fine-tuned ResNet-50 and VGG-16 models. The model generates clinically meaningful heatmaps that highlight diagnostically relevant regions, demonstrating both high accuracy and transparent decision-making.

## Method Summary
DxANN implements a binary classifier using Real-NVP flow architecture with two class-conditional Gaussian distributions (N(μ0, I) and N(μ1, I)) instead of one aggregated distribution. The model maps input data into a latent space where classification is determined by calculating which distribution assigns higher likelihood to the embedded sample. The Explainability Contribution Score (ECS) is computed as the distance between embedded features and their class mean, providing intrinsic explanations during the forward pass. For medical image classification, DxANN uses two Affine Transformation Blocks (ATBs), each containing eight convolution layers, with ~13M parameters trained for approximately 2000 epochs.

## Key Results
- Achieved 97.1% accuracy for diabetic macular edema diagnosis from retinal OCT images
- Achieved 97.2% accuracy for osteoarthritis diagnosis from knee X-ray images
- Generated clinically meaningful heatmaps that highlight diagnostically relevant regions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** DxANN achieves binary classification by mapping input data into a latent space governed by class-specific Gaussian distributions rather than a single aggregated distribution.
- **Mechanism:** The architecture utilizes a Real-NVP flow to bijectively transform input data into a latent representation. Unlike standard generative flows that map to one prior distribution N(0, I), DxANN defines two distinct distributions N(μ0, I) and N(μ1, I). The model is trained to maximize the likelihood of samples appearing in their respective class distributions. Classification is determined by calculating which distribution (q_z0 or q_z1) assigns a higher likelihood to the embedded sample.
- **Core assumption:** The complex, high-dimensional data distribution can be effectively "un-mixed" and mapped onto two linearly separable Gaussian clusters in the latent space via bijective transformations.
- **Evidence anchors:** [Section 3]: "DxANN implements a binary classifier using the same flow as the Real-NVP, but instead of using one predefined normal distribution, two distributions are assigned... with different means." [Section 3]: "After training the model, the classification inference is determined by selecting the distribution that maximizes the embedded sample's likelihood."
- **Break condition:** If the class conditional distributions in the latent space exhibit significant overlap (high Bayes error rate) or if the bijective mapping fails to converge to Gaussian shapes, the classification accuracy will degrade significantly.

### Mechanism 2
- **Claim:** Intrinsic explainability is generated by quantifying the deviation of specific feature embeddings from their class-conditional mean.
- **Mechanism:** The Explainability Contribution Score (ECS) is computed as ECS_nm ~ |z_nm - μ_i|, where z_nm is the embedded feature and μ_i is the mean of the predicted class. The paper posits that a feature is "influential" if the model struggles to reconcile it with the expected class distribution—i.e., the further a feature's embedding is from the class mean, the more it contributes to the classification decision. This is computed directly during the forward pass without external perturbations.
- **Core assumption:** "Atypicality" (distance from mean) in the latent space is a direct proxy for "feature importance" in the input space.
- **Evidence anchors:** [Section 3]: "The ECS is therefore proportional to the distance between the embedded feature and the mean of its embedded class distribution... captures how 'atypical' a feature is." [Abstract]: "DxANN uses a flow-based framework to produce per-sample, per-feature explanations as part of the forward pass, without external tools or approximations."
- **Break condition:** If the latent space mapping learns to simply push all features of the "wrong" class far away regardless of spatial coherence, ECS may highlight irrelevant noise or artifacts rather than semantically meaningful features.

### Mechanism 3
- **Claim:** Deep Convolutional Neural Networks (CNNs) can serve as the transformation engines within the flow architecture to handle image data.
- **Mechanism:** The "Affine Transformation Blocks" (ATBs) in Real-NVP require functions to learn scale (s) and translation (t) parameters. The paper implements these functions using deep CNNs (8 convolution layers per block). This allows the model to leverage the spatial feature extraction capabilities of CNNs while maintaining the bijective properties required for the flow-based loss calculation (log-determinant of the Jacobian).
- **Core assumption:** The internal layers of a CNN can effectively learn the parameters for volume-preserving (or non-volume preserving) transformations required by Real-NVP without destabilizing the training dynamics.
- **Evidence anchors:** [Section 4]: "The ANNs in Figure 1 can be any of deep neural network type. For images, we experimented with CNN as the ANNs." [Results]: "The CNN-based DxANN model... included two ATBs... each containing eight convolution layers."
- **Break condition:** If the receptive field of the convolutions is too small or the architecture too deep, the gradient of the log-determinant may vanish or explode, preventing the flow from learning a valid probability distribution.

## Foundational Learning

- **Concept: Normalizing Flows (Real-NVP)**
  - **Why needed here:** This is the mathematical engine of DxANN. Without understanding bijective mappings and the change of variables formula (relating data likelihood to latent likelihood), one cannot understand how the model generates "probabilities" for classification or why the loss function includes a log-determinant term.
  - **Quick check question:** Given a transformation y = f(x), how does the Jacobian determinant affect the probability density p(y) compared to p(x)?

- **Concept: Latent Space Geometry**
  - **Why needed here:** The entire explainability thesis (ECS) rests on Euclidean distance in the latent space (|z - μ|). Understanding that the model forces inputs into a geometric arrangement where "class membership" equals "proximity to a mean vector" is critical.
  - **Quick check question:** In DxANN, if an input image is mapped exactly to μ0, what is its predicted class and what would the ECS score theoretically be?

- **Concept: Ante hoc vs. Post hoc Explainability**
  - **Why needed here:** To differentiate DxANN from standard methods like Grad-CAM. One must grasp that DxANN's explanation is a *byproduct* of the inference calculation itself, not a separate backward pass or perturbation analysis.
  - **Quick check question:** If you change the loss function of DxANN to ignore the class means, do you lose the classification capability, the explainability, or both?

## Architecture Onboarding

- **Component map:** Input Layer -> ATB (Affine Transformation Block) -> Latent Space (z) -> Distribution Heads (N(μ0,I) and N(μ1,I)) -> ECS Generator

- **Critical path:**
  1. **Forward Pass:** Data passes through ATBs → Latent z
  2. **Loss Calculation:** Compute Log-Likelihood of z under the true class distribution + Log-Determinant of transformations
  3. **Inference:** Compare Likelihood(z | Class 0) vs Likelihood(z | Class 1)
  4. **Explanation:** Compute ECS by subtracting the winning class mean from z and mapping this magnitude back to input pixel space (conceptually)

- **Design tradeoffs:**
  - **Accuracy vs. Constraints:** Standard CNNs (ResNet) are unconstrained in mapping; DxANN must maintain bijectivity, potentially limiting the "black box" performance ceiling (though results show it matches ResNet-50 here)
  - **Complexity:** Flow-based models often require more parameters or careful tuning of the "scale" parameters (s) to prevent numerical instability compared to standard discriminative CNNs

- **Failure signatures:**
  - **Collapsing Latent Space:** If μ0 and μ1 are too close, accuracy drops to random chance
  - **Exploding Log-Determinant:** If scale factors s_k become too large, loss becomes unstable
  - **Noisy ECS:** If heatmaps look like static noise, the model has likely failed to learn structured spatial features in the latent mapping

- **First 3 experiments:**
  1. **Mean Separation Validation:** Visualize the latent space (using t-SNE/PCA if z is high-dim, or directly if 2D) to confirm that the two classes form distinct clusters around μ0 and μ1. *Success condition:* Visible separation between class clusters
  2. **ECS Sanity Check:** Input a blank (noise) image vs. a clear pathological image. Verify that the ECS values (heatmap intensity) are significantly higher and more structured for the pathological image. *Success condition:* Distinct activation patterns for relevant features vs. noise
  3. **Ablation on Means:** Train with μ0 = μ1 (standard Real-NVP) and then separate them. Verify that the classification capability emerges *only* when the means are separated, confirming the mechanism described in Section 3. *Success condition:* Accuracy is random at μ0=μ1 and rises as means separate

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the DxANN latent space formulation and Explainability Contribution Score (ECS) be effectively scaled to multi-class classification tasks?
- **Basis in paper:** [inferred] The mathematical formulation in Section 3 explicitly defines the loss function and latent distributions (q_z0 and q_z1) for binary classification only, though the architecture is proposed for general use.
- **Why unresolved:** The current method relies on measuring distance between two class-specific means; it is unclear how the loss function or ECS calculation would behave with overlapping clusters in a high-dimensional multi-class latent space.
- **What evidence would resolve it:** Successful application of DxANN to a dataset with >2 classes (e.g., ImageNet) with maintained accuracy and distinct per-class ECS heatmaps.

### Open Question 2
- **Question:** How does the integration of DxANN with Transformer-based architectures affect the fidelity of the generated explanations?
- **Basis in paper:** [explicit] The conclusion states, "Future work will explore extending DxANN... to other model architectures such as MLP, transformers and RNNs."
- **Why unresolved:** The current implementation relies on CNNs for spatial feature extraction; it is unknown if the attention mechanisms in Transformers or the sequential processing in RNNs are compatible with the Real-NVP flow and ECS calculation.
- **What evidence would resolve it:** A comparative study of DxANN-CNN vs. DxANN-Transformer on the same medical imaging task, analyzing the consistency of the resulting heatmaps.

### Open Question 3
- **Question:** Does the ECS metric provide a quantitative improvement in explanation fidelity compared to ground-truth clinical annotations?
- **Basis in paper:** [inferred] The paper validates explainability via "informal presentations to ophthalmologists" (visual inspection) rather than quantitative metrics.
- **Why unresolved:** Without quantitative metrics (e.g., Intersection over Union against expert-segmented pathology masks), it is difficult to scientifically claim the explanations are superior to or distinct from post-hoc methods like Grad-CAM.
- **What evidence would resolve it:** A clinical evaluation measuring the overlap between DxANN-highlighted regions and clinically segmented regions of interest (ROIs).

## Limitations
- The paper lacks architectural details necessary for full reproduction, including CNN layer configurations, latent space dimensions, and initialization schemes
- The claim that ECS directly captures "feature influence" relies on the assumption that Euclidean distance in latent space translates to input-space importance, requiring further validation
- While reported accuracies are impressive (97.1% for DME, 97.2% for OA), they should be verified independently given unspecified architectural details

## Confidence
- **High confidence:** DxANN's binary classification mechanism using class-conditional Gaussian distributions in latent space is mathematically sound and well-supported by the theoretical framework of normalizing flows
- **Medium confidence:** The claim that ECS provides meaningful per-feature explanations is supported by qualitative heatmap examples but lacks quantitative validation against ground-truth feature importance annotations
- **Medium confidence:** The reported classification accuracies (97.1% for DME, 97.2% for OA) are impressive but should be verified independently given the architectural details that remain unspecified

## Next Checks
1. **Latent Space Validation:** Generate t-SNE visualizations of the latent representations for both classes to verify that the data forms two distinct clusters around the respective class means, confirming the geometric basis of the classification mechanism
2. **ECS Feature Attribution Test:** Perform controlled experiments where specific diagnostic features are systematically removed from images (e.g., blocking fluid accumulation regions in OCT scans). Verify that ECS heatmaps correspondingly lose intensity in those regions, demonstrating meaningful feature attribution
3. **Cross-Architecture Comparison:** Compare DxANN's performance and explainability against a standard ResNet-50 trained with Grad-CAM interpretation. Measure whether DxANN achieves comparable accuracy while providing explanations with better spatial precision or clinical relevance