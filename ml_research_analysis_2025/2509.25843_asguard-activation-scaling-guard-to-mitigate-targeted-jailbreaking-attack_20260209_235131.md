---
ver: rpa2
title: 'ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack'
arxiv_id: '2509.25843'
source_url: https://arxiv.org/abs/2509.25843
tags:
- tense
- heads
- safety
- refusal
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ASGuard, a mechanistically-informed framework
  to mitigate targeted jailbreaking attacks, specifically addressing the vulnerability
  where large language models (LLMs) comply with harmful requests when phrased in
  past tense. ASGuard uses circuit analysis to identify specific attention heads causally
  linked to the targeted jailbreaking attack, then trains a precise, channel-wise
  scaling vector to recalibrate the activation of these vulnerable heads.
---

# ASGuard: Activation-Scaling Guard to Mitigate Targeted Jailbreaking Attack

## Quick Facts
- arXiv ID: 2509.25843
- Source URL: https://arxiv.org/abs/2509.25843
- Authors: Yein Park; Jungwoo Park; Jaewoo Kang
- Reference count: 40
- One-line primary result: Mechanistically-informed intervention reduces targeted jailbreak success from 42% to 8% while preserving general capabilities.

## Executive Summary
This paper introduces ASGuard, a mechanistically-informed framework to mitigate targeted jailbreaking attacks, specifically addressing the vulnerability where large language models (LLMs) comply with harmful requests when phrased in past tense. ASGuard uses circuit analysis to identify specific attention heads causally linked to the targeted jailbreaking attack, then trains a precise, channel-wise scaling vector to recalibrate the activation of these vulnerable heads. This is followed by a "preventative fine-tuning" stage that forces the model to learn a more robust refusal mechanism. The method was evaluated on three LLMs (Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, and Gemma-2-9B-it), demonstrating effective reduction in attack success rate (e.g., from 42% to 8% in Llama-3.1-8B-Instruct) while preserving general capabilities and minimizing over-refusal, achieving a balanced safety-utility trade-off. Mechanistic analysis revealed that targeted jailbreaking exploits a failure of semantic generalization, where models misinterpret past tense forms as benign historical inquiries. ASGuard's approach of directly intervening on internal mechanisms responsible for the vulnerability offers a practical and interpretable method for adjusting model behavior.

## Method Summary
ASGuard is a three-stage approach to mitigate tense jailbreaking attacks. First, it uses Edge Attribution Patching with Integrated Gradients (EAP-IG) to construct computational subgraphs and identify attention heads causally linked to successful jailbreaks. Second, it trains learnable channel-wise scaling vectors to recalibrate the activation of these vulnerable heads, dampening harmful pathways while preserving beneficial functions. Third, it performs preventative fine-tuning with the scaling vectors attached, forcing the model to learn alternative, robust refusal mechanisms that persist after the vectors are detached. This creates a safety intervention with zero inference overhead while maintaining general capabilities.

## Key Results
- Llama-3.1-8B-Instruct: Attack success rate reduced from 42% to 8% while maintaining MMLU score at 66.1%
- Qwen2.5-7B-Instruct: Attack success rate reduced from 71% to 21% with minimal over-refusal (OR-Bench-Hard: 24.2%)
- Gemma-2-9B-it: Attack success rate reduced from 20% to 12% while preserving MMLU score at 72.2%
- ASGuard achieves balanced safety-utility trade-off, outperforming baselines (SFT-30/70, DPO, Only Scaling) on combined R-Score and Overall score metrics

## Why This Works (Mechanism)

### Mechanism 1: Circuit-Based Localization of Tense-Vulnerable Heads
- **Claim**: Tense jailbreaking exploits a specific subset of attention heads that process linguistic tense, which inadvertently signal "historical inquiry" to downstream safety mechanisms, bypassing refusal.
- **Mechanism**: EAP-IG constructs computational subgraphs by scoring edges between model components using clean/present-tense (refusal) and corrupted/past-tense (jailbreak) activation pairs. Heads appearing exclusively in successful jailbreak circuits ("False-to-True") but not in failed circuits ("Always-False") are identified as causally responsible.
- **Core assumption**: Safety failures localize to specific attention heads rather than being distributed globally across the network.
- **Evidence anchors**:
  - [abstract]: "we use circuit analysis to identify the specific attention heads causally linked to the targeted jailbreaking, the tense-changing attack"
  - [Section 3.1]: "We differentiate 'False-to-True' circuits and 'Always-False' circuits to identify which attention heads or MLPs are predominant or only presence within jailbreak success circuits"
  - [Section 6.1]: Linear probes achieve 73-76% accuracy on identified heads for past/present classification, confirming tense specialization
  - [corpus]: Weak—neighbor papers discuss safety heads broadly (Sahara, Refusal Direction) but not tense-specific vulnerability circuits
- **Break condition**: If vulnerable heads don't transfer across architectures (as noted: Llama heads ≠ Qwen heads ≠ Gemma heads), the approach requires per-model circuit analysis.

### Mechanism 2: Channel-Wise Activation Scaling as Surgical Intervention
- **Claim**: Applying learned per-channel scaling vectors to vulnerable heads dampens the harmful pathway without fully ablating the head, preserving beneficial functions.
- **Mechanism**: A learnable vector sj ∈ ℝ^{d_head} modulates each head's output via Hadamard product (H'_{l,j} = H_{l,j} ⊙ s_j), trained to maximize refusal probability on known harmful inputs. The scaling can be fused into W_O with zero inference overhead.
- **Core assumption**: The vulnerable head's harmful influence is channel-specific and can be selectively suppressed.
- **Evidence anchors**:
  - [abstract]: "we train a precise, channel-wise scaling vector to recalibrate the activation of tense vulnerable heads"
  - [Section 5.1]: "Only Scaling" baseline achieves 13% ASR on Llama (down from 42%) with some capability degradation
  - [Section 3.1]: "Naive ablation is insufficient, as this blunt intervention disrupts a downstream refusal mechanism"
  - [corpus]: Adjacent work (RepBend, Representation Bending) validates activation-level safety interventions
- **Break condition**: If scaling magnitude is too aggressive, it causes capability loss (Gemma MMLU dropped to 50.3% with "Only Scaling").

### Mechanism 3: Preventative Fine-Tuning for Pathway Repurposing
- **Claim**: Fine-tuning the model while vulnerable pathways are temporarily blocked (via scaling) forces the model to learn alternative, robust refusal circuits that persist after the intervention is removed.
- **Mechanism**: Scaling vectors act as implicit regularization—making the vulnerable pathway "expensive" to use—encouraging the optimizer to discover non-vulnerable routes. After convergence, vectors are detached; the model retains learned refusal without inference overhead.
- **Core assumption**: Models can learn functionally equivalent behaviors through alternative circuits when primary pathways are penalized.
- **Evidence anchors**:
  - [abstract]: "'preventative fine-tuning', forcing the model to learn a more robust refusal mechanism"
  - [Section 3.3]: "The optimizer is thereby encouraged to discover alternative, non-vulnerable route to implement the desired refusal behavior"
  - [Section 6.2]: Post-ASGuard circuit analysis shows most vulnerable heads disappear; L10H19/L13H25 increase tense-detection accuracy (sharpened representations)
  - [corpus]: Inspired by Chen et al. (2025): "steering towards an undesirable trait during training can build resilience"
- **Break condition**: If the model memorizes training data rather than learning generalizable refusal patterns, it overfits to specific tense formulations.

## Foundational Learning

### Concept 1: Transformer Circuits and Edge Attribution Patching
- **Why needed here**: ASGuard's first stage requires understanding how EAP-IG scores edges using clean/corrupted activation pairs to isolate causal components.
- **Quick check question**: Can you explain how integrated gradients approximate the contribution of each edge along the path from corrupted (refusal) to clean (jailbreak) activations?

### Concept 2: Activation Engineering / Representation Steering
- **Why needed here**: The scaling vectors are a form of activation engineering—understanding this paradigm clarifies why channel-wise modulation is more precise than full ablation.
- **Quick check question**: What's the operational difference between zeroing a head's output versus applying a learned scaling vector to its activations?

### Concept 3: Safety-Utility Trade-offs in LLM Alignment
- **Why needed here**: ASGuard explicitly targets Pareto-optimal balance; recognizing over-refusal and catastrophic forgetting signatures helps diagnose why simpler methods (SFT 30/70) fail.
- **Quick check question**: Why does reducing ASR to 0% sometimes indicate a broken model rather than a successful defense?

## Architecture Onboarding

### Component Map
Input: JBB-Behaviors prompts → Tense reformulation (GPT-3.5) → Circuit Analysis: EAP-IG on False-to-True vs Always-False pairs → Head Identification: Compare circuits → Extract unique heads → Scaling Training: Learn s_j per vulnerable head (frozen base weights) → Preventative Fine-Tuning: Fine-tune with scaling attached, then detach → Output: Robustly aligned model (no inference overhead)

### Critical Path
1. Construct paired dataset (past-tense jailbreak + present-tense refusal) → 2. Run EAP-IG circuit construction → 3. Identify heads exclusive to jailbreak circuits → 4. Train scaling vectors on refusal loss → 5. Fine-tune with frozen scaling → 6. Detach scaling for deployment

### Design Tradeoffs
- **Specificity vs. Generality**: ASGuard targets tense vulnerability specifically; doesn't claim protection against all jailbreak types
- **Pipeline Complexity**: Multi-stage (circuit → scaling → fine-tuning) vs. single-stage SFT/DPO
- **Architecture Sensitivity**: Heads identified for Llama-3.1 (L0H3, L10H19...) differ from Qwen (L14H2, L24H27...) and Gemma (L0H3, L1H15...); distillation/MoE architectures may require different approaches

### Failure Signatures
- **Over-scaling**: MMLU collapse (Gemma: 72.2 → 50.3 with scaling only)
- **Under-scaling**: Insufficient ASR reduction (<50% relative drop)
- **Over-refusal**: OR-Bench-Hard spikes (>80%) indicating benign prompt refusals
- **Head misidentification**: Random head ablation shows only 1-2% ASR drop; compare against this baseline

### First 3 Experiments
1. **Reproduce head identification**: Run EAP-IG on your target model with 100 JBB-Behaviors prompts (20 past/present reformulations each); verify identified heads match Table 1 for your architecture
2. **Establish ablation ceiling**: Zero-out identified vulnerable heads directly; measure ASR and MMLU to quantify the cost of naive intervention
3. **Stage-wise ablation**: Compare (a) scaling-only vs. (b) full ASGuard to isolate the contribution of preventative fine-tuning on safety-utility balance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can ASGuard be adapted for architectures with compositional representations or MoE routing where safety features are not clearly localized?
- **Basis in paper:** [explicit] The authors state in the Conclusion that efficacy hinges on localizable circuits and that architectures with MoE routing or distillation may limit direct transfer.
- **Why unresolved:** The current method assumes safety features are sparsely distributed across specific attention heads, an assumption that may fail in complex or dynamically routed architectures.
- **What evidence would resolve it:** Successful application of the scaling intervention to a Mixture-of-Experts model (e.g., Mixtral) or a heavily distilled model with maintained performance.

### Open Question 2
- **Question:** What specific modifications are required to stabilize mechanistic interventions for Small Language Models (SLMs) without causing catastrophic forgetting?
- **Basis in paper:** [explicit] The paper notes that small models like Phi-3-mini are overly sensitive to attention head intervention, requiring a "meticulous approach" that was not developed in this work.
- **Why unresolved:** Smaller parameter spaces likely lack the redundancy of larger models, making surgical activation scaling riskier and more prone to disrupting essential capabilities.
- **What evidence would resolve it:** Identification of distinct scaling hyperparameters or alternative intervention loci that mitigate jailbreaks in SLMs without degrading MMLU scores.

### Open Question 3
- **Question:** Can the "Identify-then-Scale" protocol generalize to other forms of semantic jailbreaking (e.g., translation, role-play) or is it strictly optimal for tense-based perturbations?
- **Basis in paper:** [inferred] The method is described as "surgically" mitigating a "specific vulnerability," leaving the extension to the broader landscape of semantic attacks as an open inference.
- **Why unresolved:** While the framework is mechanistic, it relies on finding "vulnerable heads" specific to a failure mode; it is unproven if other semantic attacks rely on equally localizable circuits.
- **What evidence would resolve it:** Experiments applying the EAP-IG circuit discovery and scaling process to other semantic attack vectors (e.g., multilingual jailbreaks).

## Limitations
- Circuit identification is model-specific; heads identified for Llama-3.1 differ from Qwen2.5 and Gemma, requiring per-architecture analysis
- EAP-IG implementation details (baseline computation, edge ablation, KL divergence for L) are underspecified, making faithful reproduction challenging
- The approach is narrowly targeted to tense jailbreaking and doesn't generalize to all attack vectors
- Performance improvements are substantial but not complete—Qwen2.5 ASR remains at 21% (down from 71%), and Gemma still shows 12% ASR

## Confidence
- **High confidence**: Core mechanism (circuit-based head identification → targeted activation scaling → preventative fine-tuning produces measurable safety gains)
- **Medium confidence**: Generalizability across architectures given head identification variability
- **Medium confidence**: Safety-utility trade-off claims due to limited out-of-distribution testing
- **Low confidence**: Completeness of implementation details for exact reproduction

## Next Checks
1. **Cross-architecture head transfer test**: Run ASGuard circuit analysis on Llama-3.1-8B, then apply the identified scaling vectors directly to Qwen2.5-7B without re-identification. Measure whether the transferred intervention still reduces ASR, or if head identification is truly architecture-specific.
2. **Over-refusal stress test**: Generate 1000 benign prompts from diverse domains (medical, legal, technical) and measure refusal rates after ASGuard. Compare against baseline SFT-30/70 to quantify if the safety gains come from genuine robustness or excessive caution.
3. **Long-term stability evaluation**: Fine-tune ASGuard for 10 epochs instead of 1, then measure MMLU and OR-Bench-Hard at each checkpoint. Track whether over-refusal emerges gradually (indicating overfitting) or stabilizes, revealing the true safety-utility frontier.