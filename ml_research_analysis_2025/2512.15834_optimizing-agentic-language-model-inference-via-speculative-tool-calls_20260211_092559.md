---
ver: rpa2
title: Optimizing Agentic Language Model Inference via Speculative Tool Calls
arxiv_id: '2512.15834'
source_url: https://arxiv.org/abs/2512.15834
tags:
- tool
- speculative
- tools
- time
- call
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses performance bottlenecks in language model\
  \ inference for agentic workflows that rely heavily on external tool calls. The\
  \ authors introduce speculative tool calling\u2014two methods to anticipate and\
  \ execute tools ahead of time: a client-side approach that runs a faster draft model\
  \ to predict tools and overlaps their execution with main model generation, and\
  \ an engine-side approach that validates and caches tool results within the inference\
  \ server to keep sequences resident and avoid costly evictions."
---

# Optimizing Agentic Language Model Inference via Speculative Tool Calls

## Quick Facts
- arXiv ID: 2512.15834
- Source URL: https://arxiv.org/abs/2512.15834
- Reference count: 24
- Agents can save 6–21% time per turn by speculatively executing tools before the main model finishes generation

## Executive Summary
This paper addresses performance bottlenecks in language model inference for agentic workflows that rely heavily on external tool calls. The authors introduce speculative tool calling—two methods to anticipate and execute tools ahead of time: a client-side approach that runs a faster draft model to predict tools and overlaps their execution with main model generation, and an engine-side approach that validates and caches tool results within the inference server to keep sequences resident and avoid costly evictions. A theoretical model shows that the client-side method can achieve up to 2× speedup when tools finish before generation resumes, with the best gains when tool latency is comparable to model decode time. Experiments using BFCL tool datasets and gpt-oss-120b agents demonstrate 6–21% time savings per agent turn and throughput improvements of several hundred tokens per second, with the engine-side approach providing an additional 2–3% savings when tools complete quickly. The authors also propose a "tool cache" API endpoint to enable easy adoption by LM providers.

## Method Summary
The authors propose two speculative tool calling algorithms to optimize agentic inference. The client-side approach runs a faster draft model in parallel with the main model, predicting tool calls and executing them speculatively. When the main model eventually generates its tool call, the client checks if it matches the cached result and reuses it if so. The engine-side approach integrates directly into the inference server, maintaining a tool cache indexed by request_id and tool parameters, allowing tool results to be injected into the KV-cache without evicting sequences. Both methods rely on the key insight that tool execution can be overlapped with model generation when properly orchestrated. The theoretical performance model proves that maximum speedups approach 2× when the draft model is much faster than the main model and tool latency approximately equals generation time.

## Key Results
- Client-side speculative tool calling achieves 6–21% time savings per agent turn when tool latency is comparable to main model decode time
- Engine-side approach provides additional 2–3% savings by avoiding eviction and re-prefill overheads
- Optimal speedup occurs when tool latency T ≈ main generation time G, with diminishing returns when T << G or T >> G
- Larger speculative models (8B) achieve higher cache hit rates but smaller models with more parallel samples can compensate

## Why This Works (Mechanism)

### Mechanism 1: Client-Side Speculative Tool Calling (Latency Hiding)
Running a faster draft model to predict tool calls before the main model finishes generation can reduce end-to-end agent turn time by 6–21% when tool latency is comparable to main model decode time. The client asynchronously queries both a speculative model S and main model M. When S returns a tool call, the tool executes immediately and stores its result as a future. When M eventually returns, if its tool call matches the cached future, the client reuses the result instead of waiting for execution. This overlaps tool execution with M's generation time. Core assumption: Tools must be stateless and cheap to execute; speculating stateful tools would require rollback mechanisms not addressed here. Break condition: If g + T ≥ G (speculative model generation time + tool latency ≥ main model generation time), speedups diminish because tool execution can no longer be fully hidden behind generation.

### Mechanism 2: Engine-Side Sequence Residency via Tool Cache
Posting speculated tool outputs directly to the inference engine can yield an additional 2–3% time savings by avoiding eviction and re-prefill overheads. The inference engine maintains a tool cache indexed by (request_id, canonicalized_tool_key). When a tool call token is detected, the engine performs cache lookup. On hit, tool result tokens are drafted directly into the KV-cache without evicting the sequence, enabling continuous decoding. Core assumption: The inference engine supports speculative sampling infrastructure and allows custom cache injection; this requires engine modifications unlike the client-side approach. Break condition: If tool latency exceeds main model reasoning time δR_i, the result won't be available when decoding needs it, eliminating the residency benefit.

### Mechanism 3: Performance Model for Optimal Configuration
Maximum client-side speedup approaches 2× only when draft model is much faster than main model (g → 0) and tool latency T ≈ main generation time G. The model T_spec = α·max{G, g+T} + (1−α)(G+T) shows that when T << G, generation dominates; when T >> G, tool latency dominates. The "sweet spot" occurs when phases are balanced, maximizing overlap opportunity. Core assumption: Acceptance rate α > 0 and speculative model is strictly faster (g < G). Break condition: If α = 0 (speculative model never predicts correctly) or g ≥ G (speculative model is not faster), S_spec = 1 (no speedup).

## Foundational Learning

- Concept: **Speculative Decoding**
  - Why needed here: The paper adapts speculative decoding principles (draft-then-verify) but applies them to tool calls rather than tokens. Understanding the original formulation clarifies why tool speculation requires launching earlier and more aggressively.
  - Quick check question: Can you explain why traditional speculative decoding (3–10 tokens ahead) is insufficient for hiding tool latency?

- Concept: **KV-Cache and Prefix Caching**
  - Why needed here: The engine-side optimization relies on understanding how KV-cache eviction works and why prefix caching alone doesn't eliminate re-scheduling overhead. The tool cache effectively "forces" prefix caching behavior.
  - Quick check question: What overheads remain even when prefix caching is enabled for multi-turn tool-calling agents?

- Concept: **Tool Calling Workflow (Prefill/Decode/Stop/Evict)**
  - Why needed here: The paper's performance model depends on understanding where time is spent: prefill phase (processing prompt tokens), decode phase (autoregressive generation), and the overhead when sequences are evicted and rescheduled after tool calls.
  - Quick check question: In the standard tool-calling pipeline, what happens to a sequence's KV-cache when a tool call token is generated?

## Architecture Onboarding

- Component map:
  - Client-side module -> Async orchestrator -> Speculative model pool -> Tool execution engine -> Tool cache -> Main model server
  - Engine-side module -> Tool proposer -> Tool cache -> Speculative sampler -> KV-cache manager -> Decoder

- Critical path:
  1. Client sends prompt to main model M (async) → immediately sends to speculative model S (async)
  2. S returns tool call → client launches tool execution, stores future
  3. M returns tool call → client checks cache: hit → await future; miss → execute now
  4. (Engine-side) Tool output posted to cache → engine detects tool token → cache lookup → inject result → continue decode without eviction

- Design tradeoffs:
  - Client-side vs engine-side: Client-side requires no engine modifications but only hides latency; engine-side adds 2–3% savings but requires vLLM fork and speculative decoding infrastructure.
  - Speculative model size vs sample count: Larger model (8B) has higher per-sample accuracy; smaller models (1B/3B) can compensate with more samples (λ=9) at cost of more parallel requests.
  - Cost vs speedup: Client-side speculation with commercial APIs adds 4–30% cost depending on sample count (Figure 7); engine-side has no API cost but infrastructure complexity.

- Failure signatures:
  - Low cache hit rate (<50%): Speculative model mispredicts tools → speculation overhead without benefit; try larger S model or more samples.
  - No speedup despite high α: Tool latency T << G (tools finish instantly) or T >> G (tools dominate); speedup requires T ≈ G.
  - Engine-side crashes on batch size > 1: vLLM speculative decoding infrastructure has high overhead for batched requests (noted in Section 6.2); run single-agent experiments first.
  - Incorrect tool results used: Cache key collision or non-canonicalized arguments; ensure canonicalization before cache lookup.

- First 3 experiments:
  1. Baseline characterization: Measure average generation time G for your main model on representative agent tasks; measure typical tool latency T for your tools. Compute T/G ratio to predict whether speculation will help.
  2. Speculative model selection: Test xLAM-1B, 3B, 8B on your tool set; measure acceptance rate α (how often S's tool call matches M's). Start with λ=1 sample; if α < 0.5, increase samples.
  3. End-to-end latency comparison: Run client-side speculation with optimal S and λ; measure time saved %. If tools are sub-second and you control the inference engine, add engine-side cache and measure incremental improvement.

## Open Questions the Paper Calls Out

- **Stateful Tools**: How can speculative tool calling be adapted to support stateful tools that modify their environment? The authors explicitly limit their scope to stateless tools, noting that "If a tool or function requires or modifies state, then speculating it is impossible without some undo or rollback mechanism." The current method relies on the ability to execute tools speculatively without side effects; stateful operations would require mechanisms to reverse or commit actions depending on the main model's validation, which adds significant complexity.

- **Multi-tenant Scalability**: Can engine-side speculative tool calling achieve net positive speedups in high-concurrency, multi-tenant serving environments? The authors restricted engine-side results to a single asynchronous agent because they "found high overheads for its spec-dec implementation when the batch size is greater than 1." While theoretically sound, the current implementation in vLLM suffers from system overheads that negate benefits when batching multiple requests, leaving the scalability of the engine-side approach unproven.

- **Latency Distribution Sensitivity**: Does the theoretical speedup bound ($S < 2$) hold when tool latencies are highly stochastic rather than normally distributed? The evaluation relied on "pre-compute[d]" tool outputs with latencies sampled from normal distributions, whereas real-world tool latencies (e.g., web search) can be heavy-tailed or highly variable. The theoretical model assumes predictable overlaps between generation ($G$) and tool time ($T$); high variance in $T$ could reduce the probability of successful speculation masking, potentially lowering real-world speedups.

## Limitations

- The paper's evaluation uses synthetic tool latencies and pre-computed outputs, which don't capture the full variability of real-world tool execution times
- The approach is limited to stateless tools; stateful tools would require rollback mechanisms not addressed in this work
- The engine-side optimization requires substantial infrastructure changes to the inference engine, limiting accessibility

## Confidence

**High confidence**: The theoretical performance model and its validation are well-grounded. The lemmas correctly capture the speedup bounds, and the empirical measurements align with the predicted optimal regime (T ≈ G). The client-side algorithm's correctness is straightforward to verify through implementation.

**Medium confidence**: The specific numerical results (6–21% time savings, 2–3% additional gains from engine-side) are likely accurate for the tested configuration but may not generalize across different tool sets, model architectures, or latency distributions. The BFCL benchmark is well-established, but the exact subset and tool implementations used are not fully specified.

**Low confidence**: The cost analysis for commercial API deployment is based on hypothetical pricing rather than measured expenses, and the claim about vLLM speculative decoding overhead for batch size > 1 is noted but not empirically validated in this work.

## Next Checks

1. **Real-world latency validation**: Implement the client-side algorithm against production tools with actual, variable execution times (not synthetic delays). Measure whether the 6–21% savings hold when tool latencies follow realistic distributions with high variance.

2. **Stateful tool scenario testing**: Design an experiment with a simple stateful tool (e.g., "add_item_to_cart" that depends on previous cart state). Attempt to execute it speculatively and document the failure modes or necessary rollback mechanisms, establishing the boundary of the approach's applicability.

3. **Multi-turn agent scalability**: Implement a multi-turn agent (5+ turns) using both client-side and engine-side approaches. Measure whether the claimed benefits compound across turns or whether accumulated latency variance and cache management overhead erode the gains observed in single-turn experiments.