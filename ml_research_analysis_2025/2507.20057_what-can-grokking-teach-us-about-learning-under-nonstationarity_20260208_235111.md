---
ver: rpa2
title: What Can Grokking Teach Us About Learning Under Nonstationarity?
arxiv_id: '2507.20057'
source_url: https://arxiv.org/abs/2507.20057
tags:
- learning
- rate
- training
- which
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the problem of primacy bias and loss of
  plasticity in neural networks during non-stationary learning. The authors hypothesize
  that the feature-learning dynamics which enable generalization in grokking can also
  overwrite previously-learned features to address primacy bias.
---

# What Can Grokking Teach Us About Learning Under Nonstationarity?

## Quick Facts
- arXiv ID: 2507.20057
- Source URL: https://arxiv.org/abs/2507.20057
- Reference count: 40
- One-line primary result: ELR re-warming induces grokking at arbitrary points during training and closes the warm-starting generalization gap by enabling feature-learning dynamics.

## Executive Summary
This paper investigates primacy bias and loss of plasticity in neural networks during non-stationary learning. The authors hypothesize that the feature-learning dynamics which enable generalization in grokking can also overwrite previously-learned features to address primacy bias. They propose a method to induce feature-learning dynamics by increasing the effective learning rate (ELR) - the ratio between parameter and update norms - through periodic re-warming. The approach is evaluated across three domains: grokking in modular arithmetic tasks, warm-starting image classification, and reinforcement learning. Results show that ELR re-warming successfully induces grokking at arbitrary points during training, closes the generalization gap in warm-starting by facilitating more rapid changes to learned representations, and improves performance in high-update-to-data reinforcement learning tasks when combined with early stopping.

## Method Summary
The paper introduces Normalize-and-Project (NaP), a method that periodically projects weights to a unit sphere scaled by their initial norm, thereby controlling the effective learning rate (ELR). ELR is defined as the ratio between the learning rate and parameter norm, governing how much the network can change its learned representations. The method applies this projection every training step and combines it with cyclic learning rate schedules to induce transient increases in ELR, enabling feature-learning dynamics that combat primacy bias. The approach is evaluated across three domains: inducing grokking in modular arithmetic, warm-starting image classification tasks, and high-update-to-data reinforcement learning.

## Key Results
- ELR re-warming induces grokking at arbitrary points during training, not just at initialization
- Periodic projection closes the warm-starting generalization gap by facilitating more rapid changes to learned representations
- In high-update-to-data RL tasks, early cycling with annealing improves performance when combined with early stopping

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The effective learning rate (ELR)—the ratio of learning rate to parameter norm—governs whether a network can make nontrivial changes to its learned representations.
- Mechanism: In scale-invariant networks, the update magnitude relative to parameter scale determines how much representations rotate or activate differently per step. When ELR is too low (due to parameter norm growth or small LR), gradient updates produce negligible feature changes even if loss gradients are nontrivial.
- Core assumption: The network has at least approximate scale-invariance (e.g., via normalization layers) so ELR meaningfully captures update-to-parameter scale.
- Evidence anchors:
  - [Section 2.3] Derives ELR as η̃(θ) = η/‖θ‖² for gradient descent and η/‖θ‖ for adaptive optimizers.
  - [Section 4.1, Figure 2] Shows grokking occurs when updates induce nontrivial feature changes; larger LR can compensate for larger weight decay, but periodic renormalization that maintains high ELR consistently enables grokking.
  - [corpus] Limited direct corpus support for ELR as a unified control variable; related work focuses on parameter norm and grokking correlation rather than the causal ELR mechanism.
- Break condition: If parameter norms grow unbounded without normalization/projection, ELR decays and feature-learning stalls regardless of nominal learning rate.

### Mechanism 2
- Claim: Feature-learning dynamics that enable grokking (transitioning from memorization to generalization) can also overwrite previously-learned features in nonstationary settings.
- Mechanism: Grokking involves replacing initialization-provided features with structured, generalizing circuits. The paper conjectures this same "feature replacement" process applies when overwriting task-specific features after distribution shift—both require escaping a local basin and restructuring representations.
- Core assumption: The difficulty of overwriting old features scales similarly to the difficulty of overwriting random initialization features during grokking.
- Evidence anchors:
  - [Abstract] "This work conjectures that the same feature-learning dynamics which facilitate generalization in grokking also underlie the ability to overwrite previously learned features."
  - [Section 5.1, Figure 4] ELR re-warming closes the warm-starting generalization gap, with feature covariance and activation pattern metrics showing more significant representation changes in the projected (high-ELR) variant.
  - [corpus] "The Geometry of Grokking" links grokking to representation learning on zero-loss manifolds—supports the connection between feature dynamics and generalization transitions.
- Break condition: If early features are so deeply entrenched (e.g., through extremely long pretraining) that representation change requires impractically high ELR causing instability, the mechanism fails.

### Mechanism 3
- Claim: Transient ELR increases followed by annealing balance plasticity (ability to change features) with stability (ability to refine outputs).
- Mechanism: High ELR allows the optimizer to escape local basins and restructure features; subsequent annealing enables fine-grained output refinement without destabilizing the new representation. This mirrors stability-plasticity tradeoffs in continual learning.
- Core assumption: A single "re-warming" episode provides sufficient plasticity; the network doesn't require perpetual high ELR.
- Evidence anchors:
  - [Section 6.2, Figure 7] Early cycling with annealing dramatically outperforms constant LR in high-UTD RL; switching to constant low LR after ~40M frames stabilizes performance.
  - [Section 4.2, Figure 3] Grokking can be induced at arbitrary training points by re-warming ELR, not just at initialization.
  - [corpus] "Flatness is Necessary, Neural Collapse is Not" connects grokking to loss landscape geometry—consistent with the need for both escaping (high LR) and settling (low LR).
- Break condition: If the task distribution shifts faster than the re-warming schedule can adapt, or if high-LR phases cause irrecoverable divergence (as seen in some RL environments like Seaquest), performance degrades.

## Foundational Learning

- **Concept: Scale invariance in neural networks**
  - Why needed here: ELR is only meaningful when the function is scale-invariant (f(θ) = f(αθ)). Without this property, parameter norm changes don't cleanly translate to effective step-size changes.
  - Quick check question: If you double all weights in a layer followed by LayerNorm, does the output change?

- **Concept: Lazy (kernel) vs. rich (feature-learning) regimes**
  - Why needed here: The paper's core thesis is that primacy bias stems from being stuck in a lazy regime where features don't adapt. Understanding this distinction clarifies what ELR re-warming aims to achieve.
  - Quick check question: In the neural tangent kernel regime, what happens to the feature covariance matrix during training?

- **Concept: Primacy bias vs. loss of plasticity**
  - Why needed here: These are distinct failure modes—primacy bias is about generalization degradation on new tasks, while loss of plasticity is about inability to reduce training loss. The paper focuses on primacy bias.
  - Quick check question: If a network achieves low training loss but poor test accuracy after a distribution shift, which failure mode is this?

## Architecture Onboarding

- **Component map:**
  - Normalize-and-Project (NaP): Insert normalization before each nonlinearity; periodically project weights to unit sphere (Eq. 2)
  - ELR re-warming trigger: Fixed schedule (cyclic) or adaptive (CUSUM on loss)
  - Optional: Per-layer LR scaling based on parameter magnitude
  - Optional: Scale decay on LayerNorm parameters to reduce attention input norms

- **Critical path:**
  1. Ensure network has normalization layers for approximate scale-invariance
  2. Implement weight projection (every step or every k steps)
  3. Define re-warming schedule: when to increase, to what value, how quickly to anneal
  4. Monitor feature-learning metrics (activation pattern change, feature covariance change) to verify dynamics

- **Design tradeoffs:**
  - Projection frequency: Every step maximizes ELR control but adds compute; every k steps interpolates toward regularization
  - Re-warming magnitude: Higher values increase plasticity but risk instability (especially in RL)
  - Schedule alignment: Paper shows alignment with task boundary isn't strictly necessary, but near-boundary re-warming may be more efficient

- **Failure signatures:**
  - High ELR causing divergence: Seen in Seaquest, Krull with cyclic LR—symptom is erratic performance spikes
  - ELR decay without projection: Parameter norms grow, feature-learning metrics flatline, generalization plateaus
  - Insufficient re-warming: Gap between warm-started and fresh networks persists (Figure 13 shows re-warming beyond initial LR may be needed)

- **First 3 experiments:**
  1. **Sanity check on grokking:** Train a small transformer on modular arithmetic with and without periodic weight projection. Verify that projection + high LR induces faster/more consistent grokking (replicate Figure 2).
  2. **Warm-starting ablation:** On CIFAR-10, train on 10% data, then expand to full dataset. Compare: (a) constant LR, (b) cyclic LR without projection, (c) cyclic LR with projection. Track feature covariance change and test accuracy gap.
  3. **RL stability test:** In a high-UTD Atari setting (e.g., Breakout), compare constant low LR vs. early cycling with annealing. Identify which environments show instability at high LR and whether annealing recovers performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does a reduction in parameter norm cause generalization (the "Goldilocks zone" hypothesis) or does the discovery of a generalizing solution allow for norm reduction (the "efficient circuit" hypothesis)?
- Basis in paper: [explicit] Section 2.2 states that the "causal relationship between weight norm and delayed generalization remains unclear" and notes that the current work finds "some truth in both" conflicting theoretical perspectives.
- Why unresolved: The paper demonstrates that high Effective Learning Rate (ELR) drives feature learning and grokking, but the specific role of the parameter norm reduction often observed alongside grokking is not definitively proven as causal.
- What evidence would resolve it: Ablation studies that decouple parameter norm constraints from the effective learning rate dynamics, or mechanistic interpretability analysis tracking the precise order of circuit formation versus norm decay.

### Open Question 2
- Question: Do the benefits of layer-specific learning rates for mitigating primacy bias in small CNNs transfer to significantly larger vision architectures?
- Basis in paper: [explicit] Section 5.2 notes that prior findings regarding distinct dynamics in different layers are "most applicable to networks far larger than the ones we consider in this work," and the authors conduct only a "preliminary investigation" on small models.
- Why unresolved: The paper hypothesizes that larger networks may require different per-layer learning rate scaling to maintain plasticity, but limits empirical verification to smaller ConvNets and ResNets.
- What evidence would resolve it: Evaluating the proposed per-layer ELR scaling strategies on large-scale architectures (e.g., ResNet-101 or Vision Transformers) on non-stationary image classification tasks.

### Open Question 3
- Question: How can the stability-plasticity trade-off of ELR re-warming be automated to prevent divergence in high-update-to-data (UTD) reinforcement learning without manual early stopping?
- Basis in paper: [explicit] The Conclusion states that ELR re-warming "must be deployed with care to avoid introducing instabilities," and Section 6.2 relies on a manually chosen stopping point (40M frames) to anneal the learning rate.
- Why unresolved: While the paper shows that early stopping resolves the instability caused by cyclic learning rates, it does not provide an adaptive mechanism to determine when to switch from high-plasticity (re-warming) to high-stability (annealing) modes.
- What evidence would resolve it: An adaptive control algorithm (e.g., based on loss curvature or gradient noise scale) that dynamically triggers ELR annealing to maintain stability while maximizing final performance.

## Limitations
- The theoretical connection between grokking dynamics and feature overwriting in nonstationary settings remains largely conjectural
- The adaptive re-warming approach (CUSUM) lacks detailed implementation specifications that could affect reproducibility
- Results show instability in certain RL environments (Seaquest, Krull) with cyclic learning rates, requiring careful scheduling

## Confidence

- **High Confidence**: The empirical demonstration that ELR re-warming induces grokking at arbitrary points during training, and the basic effectiveness of the Normalize-and-Project method in modular arithmetic tasks.
- **Medium Confidence**: The application to warm-starting image classification and the claim that ELR dynamics explain primacy bias closure, as the feature-learning metrics support but don't definitively prove the mechanism.
- **Low Confidence**: The generalizability of RL results across all Atari environments, particularly given the instability observed in certain games with cyclic LR, and the theoretical connection between grokking dynamics and feature overwriting in nonstationary settings.

## Next Checks

1. **Architecture Transfer Test**: Apply NaP with ELR re-warming to a different architecture (e.g., MLP instead of transformer) on modular arithmetic to verify the mechanism isn't architecture-specific.
2. **Distribution Shift Granularity**: Test warm-starting with progressively smaller data additions (e.g., 10% increments) to determine if ELR dynamics operate at task-level or can handle incremental nonstationarity.
3. **Early Stopping Robustness**: In high-UTD RL, vary the timing and magnitude of early stopping transitions to identify optimal annealing schedules and test whether instability is inherent to cyclic LR or a scheduling issue.