---
ver: rpa2
title: 'Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom
  Behavioral Policies'
arxiv_id: '2511.05018'
source_url: https://arxiv.org/abs/2511.05018
tags:
- policy
- behavior
- user
- policies
- risk
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PBSUITE is a dynamic evaluation suite for testing LLMs' adherence
  to custom behavioral policies in multi-turn conversations. It combines a dataset
  of 300 industry-grounded policies with an adaptive stress-testing framework using
  adversarial multi-agent interactions.
---

# Pluralistic Behavior Suite: Stress-Testing Multi-Turn Adherence to Custom Behavioral Policies

## Quick Facts
- arXiv ID: 2511.05018
- Source URL: https://arxiv.org/abs/2511.05018
- Reference count: 40
- Primary result: Models show 25-84% attack success rate under multi-turn adversarial stress-testing despite <4% failure in single-turn settings

## Executive Summary
PBSUITE is a dynamic evaluation suite for testing LLMs' adherence to custom behavioral policies in multi-turn conversations. It combines a dataset of 300 industry-grounded policies with an adaptive stress-testing framework using adversarial multi-agent interactions. Evaluation shows models comply robustly in single-turn settings (<4% failure) but fail substantially in multi-turn adversarial settings (up to 84% failure), especially under roleplay and escalation strategies. These results highlight the limitations of current alignment methods in enforcing pluralistic behavioral policies in real-world interactions.

## Method Summary
PBSUITE employs a four-agent evaluation framework (Planner, Attack Agent, Target Model, LLM Judge) to systematically test policy adherence through multi-turn adversarial conversations. The system generates 300 behavioral policies across 30 industries, filtering to 1,100 verifiable prohibited rules. Each policy is tested using up to 5 strategies per rule, with attacks proceeding through up to 7 dialogue turns. Compliance is scored on a 1-5 rubric, with score 5 indicating explicit violation. The framework tests three configurations: single-turn, simple multi-turn, and agentic multi-turn, revealing dramatic degradation in compliance as conversation complexity increases.

## Key Results
- Single-turn ASR: All models show <4% failure rates, with gpt-4o-mini achieving 0.2%
- Agentic multi-turn ASR: Failure rates jump to 25.1% (gpt-4o) and 84.4% (qwen3-8b)
- Roleplay strategies achieve highest attack success rates across all model families
- Most violations occur by turn 4, with diminishing returns in later turns

## Why This Works (Mechanism)

### Mechanism 1: Multi-Turn Contextual Drift
- Claim: Models maintain single-turn compliance but degrade under multi-turn adversarial pressure
- Mechanism: Conversations begin with compliant queries and progressively escalate toward prohibited behaviors. Each turn builds contextual justification, making the final request appear natural within the established frame. Models trained on single-turn alignment fail to maintain policy boundaries across extended interactions.
- Core assumption: Alignment training predominantly optimizes for isolated query-response pairs rather than sustained policy adherence.
- Evidence anchors:
  - [abstract] "models comply robustly in single-turn settings (<4% failure) but fail substantially in multi-turn adversarial settings (up to 84% failure)"
  - [section 4.3] Table 2 shows gpt-4o single-turn ASR 0.2% vs agentic multi-turn 25.1%; qwen3-8b jumps from 1.8% to 84.4%
  - [corpus] Related work (Section 5) confirms multi-turn attacks like Crescendo, PANDORA, and FITD exploit similar drift patterns
- Break condition: If models are trained with explicit multi-turn rollouts and policy-consistency rewards during RLHF, this vulnerability should diminish.

### Mechanism 2: Roleplay and Narrative Framing as Bypass Vectors
- Claim: Roleplay-based strategies achieve higher attack success rates than direct requests
- Mechanism: By embedding prohibited requests within fictional scenarios, role personas, or "documentation needs," adversarial queries bypass the model's safety pattern matching. The model prioritizes narrative coherence and helpfulness over policy enforcement when context appears legitimate.
- Core assumption: Safety classifiers rely partially on surface-level intent detection, which roleplay contexts obscure.
- Evidence anchors:
  - [section 4.4] Figure 5 shows roleplay and narrative manipulation clusters among highest ASR strategies for gpt-4o
  - [section 4.4] "most strategies can be characterized as a form of role-playing, which remains a highly effective attack vector"
  - [corpus] Offscript paper (arxiv 2512.10172) notes similar instruction adherence failures under custom behavioral prompts
- Break condition: If instruction hierarchy training (Wallace et al., 2024) is applied to distinguish system-level policy from user-level narrative framing, roleplay effectiveness should decrease.

### Mechanism 3: Adaptive Multi-Agent Coordination for Strategy Diversity
- Claim: A four-agent architecture generates diverse, goal-directed attack strategies that probe policy boundaries systematically
- Mechanism: The Planner generates high-level strategies; the Attack Agent executes turns; the Target Model responds; the Judge evaluates compliance. If violation is not detected (score < 5), the loop continues, allowing the Planner to adapt. This generates diverse attack vectors (Figure 4) rather than single exploitation paths.
- Core assumption: Diverse strategy generation reveals more vulnerabilities than fixed attack templates.
- Evidence anchors:
  - [section 3.1] Explicit description of Planner, Attack Agent, Target Model, LLM Judge loop
  - [section 4.4] Figure 4 shows embedding-based clustering of diverse strategies generated by the Planner
  - [corpus] X-Teaming (Rahman et al., 2025) demonstrates similar multi-agent red-teaming effectiveness
- Break condition: If strategy diversity plateaus or strategies become semantically redundant, additional Planner iterations yield diminishing returns.

## Foundational Learning

- **Pluralistic Alignment**
  - Why needed here: The paper's core premise is that universal safety alignment fails for enterprise-specific behavioral policies. Understanding that different deployment contexts require different constraints is essential.
  - Quick check question: Can you explain why a legal chatbot might need different constraints for client-facing vs. internal lawyer-assistant use cases?

- **Multi-Turn Red-Teaming**
  - Why needed here: The dramatic gap between single-turn and multi-turn failure rates (4% vs. 84%) is the paper's central finding. Understanding how adversarial conversations escalate is critical.
  - Quick check question: How does a "foot-in-the-door" escalation strategy differ from a single direct jailbreak attempt?

- **LLM-as-Judge Evaluation**
  - Why needed here: All compliance scoring uses gpt-4.1 as an automated judge with a 5-point rubric. Understanding judge reliability and rubric design is essential for interpreting results.
  - Quick check question: What are two failure modes when using an LLM judge for policy compliance evaluation?

## Architecture Onboarding

- **Component map**:
  - Industries -> Risk Dimensions -> Risk Tiers -> Enterprise Use Cases -> Behavioral Policies (allowed/prohibited rules)
  - Planner Agent (strategy generation) -> Attack Agent (query execution) -> Target Model (policy-conditioned LLM) -> Judge Agent (compliance scoring, 1-5 rubric)

- **Critical path**:
  1. Generate or select a behavioral policy with verifiable prohibited rules
  2. Initialize Planner with target prohibited behavior
  3. Planner generates up to 5 strategies per rule
  4. Attack Agent executes each strategy across up to 7 turns
  5. Judge scores each response; if score = 5, violation recorded and attack succeeds
  6. Aggregate ASR across behaviors, strategies, and models

- **Design tradeoffs**:
  - **Verifiability filtering vs. coverage**: Rules requiring external metadata are excluded (e.g., "provide medical advice if authorized"), reducing realism but ensuring evaluability
  - **Judge reliability vs. scalability**: LLM-as-judge enables 1,100+ rule evaluation but shows only moderate human agreement (κ = 0.51); human annotation is infeasible at scale
  - **Adversarial vs. naturalistic evaluation**: Framework prioritizes stress-testing over representing typical user behavior (acknowledged in Limitations)

- **Failure signatures**:
  - Judge over-labels generic/fictional responses as violations (Appendix B: human annotators dismissed some judge-flagged violations)
  - Planner occasionally generates strategies weakly connected to target rule, wasting compute
  - Models with instruction hierarchy training (gpt-4o) show lower ASR but remain vulnerable (~25%)
  - Most violations occur by turn 4 (Figure 7); later turns show diminishing returns

- **First 3 experiments**:
  1. **Baseline single-turn vs. multi-turn comparison**: Run PBSUITE on your target model with policy-in-system-prompt. Compare single-turn ASR against agentic multi-turn ASR to quantify vulnerability gap.
  2. **Strategy-type ablation**: Isolate roleplay vs. escalation vs. documentation-request strategies. Measure which cluster yields highest ASR for your model class.
  3. **Judge calibration check**: Sample 30-50 responses flagged as violations (score 5) and manually annotate. Compute agreement with LLM judge to calibrate confidence in ASR estimates.

## Open Questions the Paper Calls Out

- **Question**: Can instruction hierarchy architectures effectively prevent policy override attacks in multi-turn settings without degrading model helpfulness?
  - Basis in paper: [explicit] The Limitations section states: "Future work should explore architectures with instruction hierarchies, ensuring that only privileged users can define or modify behavioral policies, thereby strengthening enforcement and integrity."
  - Why unresolved: Current approaches encode policies via system prompts, which remain vulnerable to prompt injection in multi-turn interactions. No defensive architecture has been tested against PBSUITE's adversarial strategies.
  - What evidence would resolve it: Training models with instruction hierarchies and evaluating them on PBSUITE; measuring ASR reduction while maintaining performance on permitted tasks.

- **Question**: How do models perform on the complementary problem of over-refusal—rejecting actions that should be allowed under custom behavioral policies?
  - Basis in paper: [explicit] The Limitations section states: "It also omits testing over-refusal, where models reject actions that should be allowed, an important aspect of conservativeness."
  - Why unresolved: The current framework only measures under-adherence (violating prohibited behaviors), not over-adherence (refusing allowed behaviors), which may be equally problematic for enterprise deployments.
  - What evidence would resolve it: Extending PBSUITE to include allowed-behavior test cases and measuring false refusal rates across models and policy configurations.

- **Question**: Do adversarial attack success rates generalize to realistic, non-adversarial enterprise conversation patterns?
  - Basis in paper: [explicit] The Limitations section states: "Our framework primarily uses adversarial conversations to induce policy violations, which effectively probes vulnerabilities but under-represents typical user behavior and enterprise conversation flows."
  - Why unresolved: The 25-84% ASR reflects worst-case adversarial conditions; the baseline failure rate under normal user interactions remains unknown, making it difficult to assess real-world deployment risk.
  - What evidence would resolve it: Collecting or synthesizing representative non-adversarial multi-turn enterprise dialogues and measuring policy violation rates under those conditions.

## Limitations

- Focus on adversarial rather than naturalistic user behavior may overstate real-world vulnerabilities
- LLM-as-judge evaluation introduces potential bias and reliability concerns with only moderate human agreement
- Policy generation optimized for evaluability rather than authentic enterprise needs, potentially creating artificial attack surface

## Confidence

- **High Confidence**: The empirical finding that single-turn compliance rates are high (<4% failure) while multi-turn compliance degrades substantially (up to 84% failure) is robustly supported by systematic evaluation across multiple models and strategy types.
- **Medium Confidence**: The characterization of roleplay and narrative framing as primary attack vectors is supported by qualitative analysis but may not capture all successful attack patterns.
- **Low Confidence**: The generalizability of specific ASR values across different model families and training regimes is uncertain, as the evaluation was conducted on a specific snapshot of frontier and open-weight models.

## Next Checks

1. **Judge Calibration Validation**: Run PBSUITE's judge rubric on a held-out sample of 100-200 responses manually annotated by human experts to establish true positive/negative rates and compute confidence intervals for ASR estimates across model families.

2. **Instruction Hierarchy Defense Test**: Evaluate PBSUITE against models with explicit instruction hierarchy training to quantify the effectiveness of this defense against multi-turn roleplay attacks.

3. **Real-World Deployment Correlation**: Partner with enterprise users to deploy PBSUITE on actual production policies and compare predicted failure rates with observed violations in live customer service interactions over a 3-month period.