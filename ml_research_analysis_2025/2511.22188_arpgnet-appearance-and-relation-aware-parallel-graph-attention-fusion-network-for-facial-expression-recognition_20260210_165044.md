---
ver: rpa2
title: 'ARPGNet: Appearance- and Relation-aware Parallel Graph Attention Fusion Network
  for Facial Expression Recognition'
arxiv_id: '2511.22188'
source_url: https://arxiv.org/abs/2511.22188
tags:
- facial
- attention
- recognition
- expression
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of facial expression recognition
  by learning discriminative spatial-temporal representations that embed facial expression
  dynamics. Existing methods predominantly rely on pre-trained Convolutional Neural
  Networks (CNNs) to learn facial appearance representations, overlooking the relationships
  between facial regions.
---

# ARPGNet: Appearance- and Relation-aware Parallel Graph Attention Fusion Network for Facial Expression Recognition

## Quick Facts
- arXiv ID: 2511.22188
- Source URL: https://arxiv.org/abs/2511.22188
- Authors: Yan Li; Yong Zhao; Xiaohan Xia; Dongmei Jiang
- Reference count: 40
- Primary result: ARPGNet achieves 57.70% accuracy on AFEW, surpassing state-of-the-art methods

## Executive Summary
This paper addresses facial expression recognition by learning discriminative spatial-temporal representations that capture both facial appearance and regional relationships. The proposed ARPGNet constructs a facial region relation graph and uses graph attention mechanisms to model relationships between facial regions, then fuses these relational representations with CNN-based appearance features through a parallel graph attention fusion module. Experimental results on three facial expression recognition datasets demonstrate state-of-the-art performance, with 57.70% accuracy on AFEW and 76.53% on RML.

## Method Summary
ARPGNet learns mutually enhanced spatial-temporal representations by processing video frames through two parallel branches: a CNN-based appearance module using InsightFace (ResNet50) and a GNN-based relation module that constructs a facial region relation graph with self-loops, spatial adjacency, and facial symmetry edges. The parallel graph attention fusion module creates a unified graph with 2T nodes (T from each modality) and applies graph attention to enable fine-grained interaction between appearance and relation representations while modeling temporal dynamics within each sequence. The model is trained with Adam optimizer using distinct learning rates for pre-trained backbone blocks (0.0001) and new layers (0.001), with cross-entropy loss for RML and AFEW datasets.

## Key Results
- Achieves 57.70% accuracy on AFEW dataset, surpassing existing methods
- Achieves state-of-the-art 76.53% accuracy on RML dataset
- Ablation studies show the fusion module improves performance by 3.39% over concatenation baseline
- Temporal Response Scope constraint improves accuracy by 2.35% by limiting attention to locally relevant frames

## Why This Works (Mechanism)

### Mechanism 1
Structured facial graphs with structural priors improve relation representation learning over unstructured feature flattening. The facial region relation graph encodes self-connections, spatial adjacency, and facial symmetry, injecting domain-specific inductive bias into the GNN. This allows attention mechanisms to prioritize biologically meaningful relationships rather than learning them from scratch. Break condition: If facial expressions were primarily localized to single regions without cross-regional dependencies, structural priors would add noise.

### Mechanism 2
Simultaneous modeling of inter-sequence complementarity and intra-sequence temporal dynamics yields more discriminative representations than sequential/pipelined approaches. The parallel fusion graph creates a unified space where nodes can attend to both cross-modal and temporal neighbors at frame level, enabling cross-modal refinement before temporal aggregation. Break condition: If one modality is substantially noisier or less informative, simultaneous fusion could propagate noise.

### Mechanism 3
Temporal Response Scope (TRS) constraint improves representation quality by limiting attention to locally relevant frames. TRS defines a temporal window preventing distant frames from influencing current representations, reducing noise while preserving local temporal coherence. Break condition: If key expression transitions span longer temporal distances than TRS allows, the constraint would exclude discriminative information.

## Foundational Learning

- **Concept: Graph Attention Networks (GAT)**
  - Why needed here: The relation learning module uses multi-head graph attention to weight neighboring facial patches; understanding attention coefficient computation and masked attention is essential.
  - Quick check question: Given node features xi and xj, can you trace how attention coefficient eij becomes normalized weight βij, and why masked attention (only over neighbors Ni) matters for this architecture?

- **Concept: Multi-stream fusion strategies (early/late/intermediate)**
  - Why needed here: The paper explicitly positions itself against early/late fusion; understanding why concatenation and score fusion fail clarifies the design motivation.
  - Quick check question: Why does late fusion prevent "fine-grained interactions between different sequences," and what specific interaction does ARPGNet enable that TEMMA/MulT do not?

- **Concept: Transfer learning from face recognition to expression recognition**
  - Why needed here: The appearance module initializes from InsightFace pre-trained on MS-Celeb-1M for face recognition; understanding what transfers and what must be fine-tuned is critical.
  - Quick check question: If you replaced InsightFace with a randomly initialized backbone, where in Table 4 would you expect the largest accuracy drop, and why?

## Architecture Onboarding

- **Component map:** Input video frames → Face alignment (OpenFace) → CNN backbone (InsightFace ResNet50) → Appearance embeddings → Facial region relation graph → GNN layers → Relation embeddings → Parallel fusion graph with TRS constraint → GAT attention → Enhanced representations → Temporal pooling → MLP classifier

- **Critical path:** Face alignment quality directly affects graph topology; TRS hyperparameter controls temporal receptive field; GAT multi-head count stabilizes learning

- **Design tradeoffs:** 36 patches (6×6) optimal; TRS value requires tuning per dataset; shared vs separate backbones (relation reuses first 3 blocks from appearance)

- **Failure signatures:** Low relation module contribution indicates graph not learning; fusion module underperforming suggests cross-sequence attention not working; extreme lighting/occlusion samples misclassified

- **First 3 experiments:**
  1. Baseline sanity check: Run appearance-only and relation-only models; verify appearance (50.65%) > relation (47.52%) matches paper
  2. TRS ablation: Sweep TRS ∈ {2, 4, 6, 8, 12} on validation set to identify optimal value
  3. Graph construction ablation: Replace symmetry edges with adjacency-only; measure impact on AFEW to validate structural prior importance

## Open Questions the Paper Calls Out

### Open Question 1
Can linear-complexity model architectures effectively replace the quadratic self-attention mechanism in ARPGNet to improve inference speed without compromising recognition accuracy? The current implementation relies on standard attention mechanisms which are computationally expensive for long sequences, creating a trade-off between performance and efficiency.

### Open Question 2
How does the integration of Masked Auto-Encoders (MAEs) or Multimodal Large Models (MLMs) impact the robustness of ARPGNet when processing faces with severe occlusion or extreme illumination variations? The current CNN and GNN features may degrade when key facial regions are obscured or distorted.

### Open Question 3
Can unsupervised domain adaptation techniques, specifically Test-Time Adaptation (TTA), successfully mitigate performance drops when transferring learned representations from controlled (in-the-lab) environments to diverse in-the-wild settings? Models often suffer from domain shift when training data differs significantly from deployment data.

## Limitations

- Unknown TRS hyperparameter value, critical since TRS improved accuracy by 2.35% in ablation studies
- Limited modality diversity - only combines appearance and relation representations, requiring significant extension for richer multi-modal understanding
- Heavy dependency on accurate face alignment via OpenFace; in-the-wild videos with extreme poses or occlusions may degrade graph topology quality

## Confidence

- **High confidence:** The appearance module's effectiveness (50.65% accuracy) is well-supported by standard transfer learning from InsightFace and extensive FER literature
- **Medium confidence:** The relation module's contribution (2.87% improvement) is validated but depends critically on correct graph construction and may be sensitive to alignment quality
- **Medium confidence:** The parallel fusion mechanism's superiority (3.39% over concatenation) is demonstrated through ablation but requires careful hyperparameter tuning

## Next Checks

1. **Baseline sanity check:** Train appearance-only and relation-only models; verify reported accuracies (50.65% and 47.52%) to confirm backbone initialization and graph construction are correct

2. **TRS ablation sweep:** Systematically vary TRS from 2 to 12 on validation set to identify optimal temporal window size for your specific dataset

3. **Graph construction ablation:** Remove symmetry edges (keep only adjacency) and measure impact on AFEW; this directly tests the importance of structural priors claimed in Mechanism 1