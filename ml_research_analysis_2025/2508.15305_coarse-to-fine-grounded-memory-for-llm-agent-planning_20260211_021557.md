---
ver: rpa2
title: Coarse-to-Fine Grounded Memory for LLM Agent Planning
arxiv_id: '2508.15305'
source_url: https://arxiv.org/abs/2508.15305
tags:
- cfgm
- tips
- agent
- agents
- memory
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a coarse-to-fine grounded memory framework
  (CFGM) to enhance LLM-based agents for complex planning tasks. The key challenge
  addressed is that existing memory-augmented agents rely on single-granularity memory
  derived from dynamic environmental interactions, limiting the diversity of knowledge
  and flexibility of planning.
---

# Coarse-to-Fine Grounded Memory for LLM Agent Planning

## Quick Facts
- arXiv ID: 2508.15305
- Source URL: https://arxiv.org/abs/2508.15305
- Reference count: 25
- Primary result: CFGM achieves 91% success rate on AlfWorld vs 80.6% for ReAct baseline

## Executive Summary
This paper introduces a coarse-to-fine grounded memory framework (CFGM) to enhance LLM-based agents for complex planning tasks. The key challenge addressed is that existing memory-augmented agents rely on single-granularity memory derived from dynamic environmental interactions, limiting the diversity of knowledge and flexibility of planning. CFGM overcomes this by grounding environmental information into coarse-grained focus points to guide experience collection, then distilling hybrid-grained tips from each experience. During inference, it retrieves task-relevant experiences and tips for planning, and when facing anomalies, it grounds current situations into fine-grained key information for adaptive self-question-answer reflection and plan correction.

## Method Summary
CFGM operates in two phases: offline memory collection and online inference. In the offline phase, LLM_Focus analyzes environment manuals and few-shot trajectories to generate coarse-grained focus points that guide experience collection. LLM_ReAct then collects trajectories with retry-reflection (max 3 retries). LLM_Tips extracts hybrid-grained tips from comparing successful and failed trajectories. In the online phase, Faiss retrieves top-2 relevant experiences, and LLM_CFGM plans using this context. When anomalies are detected, LLM_KIE extracts key information and LLM_KIR performs self-QA reflection to correct plans.

## Key Results
- Achieves 91% success rate on AlfWorld (vs 80.6% for ReAct baseline)
- Achieves 57% success rate on ScienceWorld (vs 43% for ReAct baseline)
- Achieves 57% SR with 0.85 reward on WebShop (vs 49% SR for ExpeL+QuBE baseline)

## Why This Works (Mechanism)

### Mechanism 1: Focus-Guided Exploration
Grounding environmental descriptions into coarse-grained "focus points" prior to interaction significantly improves the quality of collected offline experiences by reducing inefficient exploration. An LLM (`LLM_Focus`) analyzes environment manuals and few-shot trajectories to generate high-level focus points that act as a preliminary policy to guide the `LLM_ReAct` agent during experience collection, directing attention toward critical dynamics rather than random exploration.

### Mechanism 2: Hybrid-Grained Tips Distillation
Extracting "tips" from both successful and failed trajectories creates a hybrid-grained knowledge base that offers more flexible guidance than single-granularity memory. The framework compares successful and failed trajectories for the same task (`C_compare`) using `LLM_Tips` to extract error-prevention tips (from failures) and success factors (from successes), storing them in a dictionary (`TD`).

### Mechanism 3: Adaptive Self-QA Reflection
Fine-grained grounding of anomalies into key information enables a self-QA process that corrects planning more effectively than fixed-question reflection templates. When an environmental anomaly is detected, `LLM_KIE` extracts structured key information from the current trajectory, and `LLM_KIR` uses this grounded info to generate specific questions and answers, deriving a corrective plan (`refi`).

## Foundational Learning

- **Concept: ReAct (Reasoning + Acting)**
  - Why needed here: This serves as the base agent architecture upon which CFGM is built. Understanding the interplay between thought traces and environment actions is required to see how CFGM injects memory into the loop.
  - Quick check question: Can you distinguish between a "Thought" step and an "Action" step in a standard LLM agent trace?

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: CFGM retrieves "relevant experiences" and "tips" to form the context for planning. You must understand vector similarity search (using Faiss in the paper) to grasp how the agent selects relevant memories.
  - Quick check question: How does the "top-k" parameter influence the relevance and potential noise in the context window of the agent?

- **Concept: Trial-and-Error / Reflection Loops**
  - Why needed here: The offline phase relies on "retry-reflection" to build the memory bank. You need to understand how verbal reflection updates a policy for subsequent attempts.
  - Quick check question: What is the difference between "reflection" in the offline phase (generating memories) and "reflection" in the online phase (correcting a current trajectory)?

## Architecture Onboarding

- **Component map:**
  - Offline Phase: `Environment` -> `LLM_Focus` (Coarse Points) -> `LLM_ReAct` (Collects Trajectories) -> `LLM_Tips` (Extracts Hybrid Rules) -> `Tips Dictionary` & `Experience Pool`
  - Online Phase: `New Task` -> `Retriever` (Top-k Similar Experiences) -> `LLM_CFGM` (Planning)
  - Correction Sub-system: `Anomaly Trigger` -> `LLM_KIE` (Extract Key Info) -> `LLM_KIR` (Self-QA) -> `Corrective Plan`

- **Critical path:**
  The dependency chain starts with the quality of **Focus Points**. If focus points are generic, the collected **Experiences** are noisy. If experiences are noisy, the distilled **Tips** are misleading. If tips are misleading, the **Online Retrieval** injects noise, and the **Self-QA** lacks a solid reference for correction.

- **Design tradeoffs:**
  - **Retrieval Scope (k):** The paper finds k=2 or k=3 is optimal. Lower k misses context; higher k introduces conflicting or redundant tips (Table 3).
  - **Granularity:** Coarse points aid exploration but may miss specifics; fine-grained reflection aids correction but adds latency and token cost.

- **Failure signatures:**
  - **Derailment:** The agent ignores the retrieved tips and hallucinates actions (ReAct baseline behavior).
  - **Context overload:** The agent fails to follow the main task instruction because the retrieved "tips" and "key information" occupy too much of the context window.

- **First 3 experiments:**
  1. **Sanity Check (ReAct Baseline):** Run ReAct on a subset of AlfWorld to establish the baseline "reasoning derailment" rate.
  2. **Memory Ablation:** Run CFGM with retrieval disabled to verify that the performance gain comes from the memory mechanism and not just the prompt structure.
  3. **Stress Test (WebShop):** Test the transferability by training the experience pool on WebShop and testing on WebArena-Shopping to evaluate if the "tips" are generalizable or environment-specific.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can limited training task sets be effectively expanded when confronted with extremely small datasets to generate sufficient effective experiences for CFGM?
- **Open Question 2:** How can dynamic filtering mechanisms be designed to adaptively retain the most relevant planning information from long trajectories while reducing redundancy and interference?
- **Open Question 3:** What causes the substantial performance gap between in-distribution tasks (WebShop: 57% SR) and out-of-distribution transfer (WebArena-Shopping: 25.1% SR), and how can this gap be reduced?

## Limitations

- The approach depends heavily on LLM capabilities for focus point generation and tips extraction, making it vulnerable to model quality and instruction-following consistency.
- The offline collection phase requires significant compute and human-in-the-loop effort for manual trajectory annotation.
- Environmental descriptions and manual trajectories are assumed to be available and comprehensive, which may not hold for novel or proprietary environments.

## Confidence

- **High confidence**: The core mechanism of using coarse-grained focus points to guide experience collection, and the experimental results showing performance improvements over ReAct baseline across multiple environments.
- **Medium confidence**: The effectiveness of hybrid-grained tips distillation and adaptive self-QA reflection, as these components show incremental improvements that could be sensitive to prompt engineering and LLM model variations.
- **Medium confidence**: The generalizability across different LLM models (GPT-4-Turbo vs GPT-4o), though this is supported by ablation studies.

## Next Checks

1. **Anomaly detection boundary testing**: Systematically evaluate the conditions under which the fine-grained reflection triggers, measuring false positive rates and characterizing edge cases where the agent fails to detect anomalies despite performance degradation.
2. **Memory retention over time**: Test whether the retrieved tips and experiences remain relevant as the agent progresses through longer trajectories, measuring performance degradation when retrieving from early versus late-stage memory.
3. **Zero-shot generalization**: Evaluate CFGM on entirely novel environments without prior experience collection, measuring whether the focus point generation alone provides sufficient guidance or whether the memory mechanism is essential for performance gains.