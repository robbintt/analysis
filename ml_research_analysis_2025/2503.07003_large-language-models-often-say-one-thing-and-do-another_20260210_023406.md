---
ver: rpa2
title: Large Language Models Often Say One Thing and Do Another
arxiv_id: '2503.07003'
source_url: https://arxiv.org/abs/2503.07003
tags:
- uni00000044
- uni00000048
- uni00000057
- uni00000010
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Words and Deeds Consistency Test (WDCT)
  to evaluate whether large language models (LLMs) exhibit consistency between their
  stated opinions/values and their actions in specific situations. The benchmark pairs
  aligned word and deed questions across four domains (opinion, non-ethical value,
  ethical value, theory) to quantitatively measure inconsistency.
---

# Large Language Models Often Say One Thing and Do Another

## Quick Facts
- arXiv ID: 2503.07003
- Source URL: https://arxiv.org/abs/2503.07003
- Authors: Ruoxi Xu; Hongyu Lin; Xianpei Han; Jia Zheng; Weixiang Zhou; Le Sun; Yingfei Sun
- Reference count: 15
- Primary result: Large language models exhibit significant inconsistency between stated opinions/values and actual actions, with average consistency scores around 50-76% across models and domains.

## Executive Summary
This paper introduces the Words and Deeds Consistency Test (WDCT) to evaluate whether large language models (LLMs) exhibit consistency between their stated opinions/values and their actions in specific situations. The benchmark pairs aligned word and deed questions across four domains (opinion, non-ethical value, ethical value, theory) to quantitatively measure inconsistency. Evaluation of 12 LLMs shows widespread inconsistency between words and deeds, with average consistency scores around 50-76% across models and domains. Separate alignment on either words or deeds through supervised fine-tuning and direct preference optimization poorly influences the other aspect, supporting the hypothesis that knowledge guiding word vs. deed choices exists in separate spaces.

## Method Summary
The WDCT benchmark contains 1225 test items across four domains, with each item consisting of paired word and deed questions that have aligned answer options. The consistency between responses is measured using Consistency Score (CS) - the proportion of matching responses between paired questions. The paper evaluates 12 LLMs and also conducts alignment experiments using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) on a mix of WDCT data and the Alpaca dataset at a 1:9 ratio. Training uses learning rates between 1e-7 and 1e-5 for SFT and 5e-6 (or 5e-7 for Mistral) for DPO, with 4 rounds of each method. Evaluation uses temperature=0 to ensure deterministic responses.

## Key Results
- Large language models show significant inconsistency between stated opinions and actual actions, with average consistency scores ranging from 50-76% across models and domains
- Separate alignment on words or deeds through SFT and DPO poorly influences the other aspect, with low direct and indirect change rates
- Common knowledge generalization techniques like explicit reasoning and data augmentation show limited effectiveness in improving consistency

## Why This Works (Mechanism)
The inconsistency between words and deeds in LLMs appears to stem from the separation of knowledge spaces that guide value statements versus action choices. Current alignment techniques that optimize for either words or deeds independently fail to establish consistency because they operate on distinct knowledge representations within the model.

## Foundational Learning
- **WDCT benchmark construction**: Understanding how paired word/deed questions are created with aligned answer options - needed to properly evaluate consistency; quick check: verify all test items have exactly matching A/B options between word and deed questions.
- **Consistency scoring metrics**: CS (proportion of matching responses) and PCS (1 - JSD between probability distributions) - needed to quantify the gap between stated values and actions; quick check: manually verify consistency scores for a small sample of test items.
- **SFT vs DPO alignment**: Supervised fine-tuning uses labeled data while DPO uses preference pairs - needed to understand why separate alignment poorly influences the other aspect; quick check: confirm training data format matches the stated methodology.

## Architecture Onboarding

**Component map:** WDCT dataset (1225 items) -> Paired word/deed questions -> LLM responses -> Consistency Score calculation -> SFT/DPO alignment -> Retrained model evaluation

**Critical path:** Dataset construction → Response parsing → Consistency calculation → Alignment training → Consistency evaluation

**Design tradeoffs:** The paper chose a focused benchmark covering 4 domains rather than broader coverage to ensure precise control over aligned answer options, trading comprehensiveness for measurement accuracy.

**Failure signatures:** Response parsing errors when models output explanations instead of just "A" or "B"; training instability with DPO causing divergence; inconsistent results across runs due to temperature settings.

**3 first experiments:** 1) Run baseline evaluation on Llama-2-7B-Chat and Mistral-7B-Instruct using Direct Prompting to verify consistency scores match reported ranges; 2) Test response parsing robustness by manually checking 100 parsed answers for accuracy; 3) Conduct 5-trial consistency test (Figure 5) to verify temperature=0 stability before main experiments.

## Open Questions the Paper Calls Out
None

## Limitations
- The WDCT benchmark covers a limited set of scenarios across four domains, and it remains unclear whether the observed inconsistencies generalize to broader real-world contexts
- The effectiveness of common knowledge generalization techniques was tested but showed limited improvement, yet the paper doesn't explore alternative architectural solutions in depth
- The separation between word-knowledge and deed-knowledge spaces is hypothesized but not definitively proven - it's possible that better alignment techniques could bridge this gap

## Confidence

**High confidence:** The existence of inconsistency between words and deeds across multiple models and domains is well-supported by the quantitative results. The finding that separate alignment on words vs. deeds poorly influences the other aspect is consistently demonstrated.

**Medium confidence:** The claim that knowledge guiding word vs. deed choices exists in separate spaces is supported but not conclusively proven - alternative explanations (insufficient training data, optimization difficulties) cannot be ruled out.

**Medium confidence:** The assertion that current alignment techniques are insufficient for achieving consistency is well-evidenced, but the paper doesn't exhaustively explore all possible solutions.

## Next Checks

1. **Generalization test**: Apply WDCT to a broader set of scenarios and values beyond the current 1225 test items to assess whether inconsistency persists across more diverse contexts and domains.

2. **Architectural intervention test**: Implement and evaluate architectural modifications (e.g., separate reasoning modules for value consistency, explicit consistency-checking layers) to determine if the word-deed gap can be addressed at the model level rather than through training data alone.

3. **Cross-lingual validation**: Translate WDCT into multiple languages and evaluate the same models to determine whether the word-deed inconsistency is language-dependent or reflects a more fundamental limitation in model reasoning about values and actions.