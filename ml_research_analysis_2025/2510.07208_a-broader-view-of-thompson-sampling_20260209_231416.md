---
ver: rpa2
title: A Broader View of Thompson Sampling
arxiv_id: '2510.07208'
source_url: https://arxiv.org/abs/2510.07208
tags:
- thompson
- sampling
- policy
- regret
- optimal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a principled optimization framework for understanding
  Thompson Sampling (TS) in multi-armed bandits. The key contribution is showing that
  TS can be viewed as an online optimization algorithm that minimizes instantaneous
  squared regret regularized by a biserial covariance term measuring residual uncertainty.
---

# A Broader View of Thompson Sampling

## Quick Facts
- **arXiv ID:** 2510.07208
- **Source URL:** https://arxiv.org/abs/2510.07208
- **Reference count:** 4
- **Primary result:** Thompson Sampling can be viewed as an online optimization algorithm that minimizes instantaneous squared regret regularized by a covariance term measuring residual uncertainty, revealing it as sometimes too conservative compared to the optimal R²-policy.

## Executive Summary
This paper provides a principled optimization framework for understanding Thompson Sampling (TS) in multi-armed bandits. The key contribution is showing that TS can be viewed as an online optimization algorithm that minimizes instantaneous squared regret regularized by a biserial covariance term measuring residual uncertainty. Through this lens, TS admits the optimization form: minimize squared regret plus a linear regularization term where the regularizer is Cov(θ₁-θ₂, sign(θ₁-θ₂)).

The authors introduce "faithful" stationarization of the long-term regret minimization problem through expected cumulative squared regret, leading to a stationary Bellman equation. This framework enables comparison of TS with the optimal R²-policy, revealing that TS's regularizer is sometimes too conservative. The paper thus provides both theoretical understanding of TS's exploration-exploitation mechanism and a principled approach for further algorithmic improvements.

## Method Summary
The paper addresses two-armed Bayesian stochastic bandits by minimizing cumulative regret through an R²-optimal framework. Thompson Sampling is implemented for Gaussian (Algorithm 1) and Bernoulli (Algorithm 2) rewards, with the optimal R²-policy computed via stationary Bellman equation using backward recursion. The key insight is viewing TS as solving an online optimization problem at each step, balancing squared expected immediate regret against a linear regularizer defined as the point-biserial covariance between the reward gap and optimal arm identity. A fixed policy modification is also proposed to improve performance when one arm becomes underexplored.

## Key Results
- Thompson Sampling is equivalent to an online optimization algorithm minimizing squared regret plus a covariance-based regularizer
- The TS regularizer (Covariance) is strictly suboptimal compared to the R²-optimal regularizer (Bellman value difference) in certain configurations
- TS can be improved by adjusting its regularizer to better align with the optimal policy, particularly when one arm becomes relatively underexplored

## Why This Works (Mechanism)

### Mechanism 1: Faithful Stationarization via Squared Regret
- **Claim:** The finite-horizon regret minimization problem can be converted into a stationary dynamic program without introducing artificial discounting, provided the objective is changed to minimizing expected cumulative *squared* regret.
- **Mechanism:** By minimizing $R^2(\pi)$ (expected cumulative squared regret), the authors derive a stationary Bellman equation (Eq. 6). This avoids the "unfaithful" alterations caused by geometric discounting used in Gittins indexing, which can fail to identify the optimal arm in finite time.
- **Core assumption:** The reward distributions are such that the posterior predictive distribution is $\sigma$-sub-Gaussian, ensuring finite squared regret (Proposition 1).
- **Evidence anchors:**
  - [Abstract] ("faithful stationarization")
  - [Section 3.1] (Derivation of $R^2$ and the regret bound $R_T \leq \sqrt{R^2 \cdot T}$)
  - [Corpus] (Weak/None: This stationarization approach appears novel to this paper; neighbors focus on specific applications rather than the theoretical $R^2$ reconstruction.)
- **Break condition:** If rewards have extremely heavy tails such that squared regret is infinite or non-informative, this stationarization fails to bound standard regret.

### Mechanism 2: Online Optimization with Covariance Regularization
- **Claim:** Thompson Sampling (TS) implicitly solves an online optimization problem at each step, balancing the squared expected immediate regret against a linear regularizer.
- **Mechanism:** The paper proves TS is equivalent to solving $x_{TS} = \text{argmin}_x [\bar{r}^2(x; \pi) + \tilde{\nu}(\pi)x]$ (Theorem 1). The regularizer $\tilde{\nu}(\pi)$ is identified as the point-biserial covariance between the reward gap ($\Delta$) and the identity of the optimal arm ($\Lambda$).
- **Core assumption:** The system acts as a Markov Decision Process where the current posterior $\pi_t$ is the state.
- **Evidence anchors:**
  - [Abstract] ("greediness is regularized by a measure of residual uncertainty")
  - [Section 4.2] (Theorem 1 and the derivation of $\tilde{\nu}$)
  - [Corpus] (General TS literature confirms posterior sampling, but lacks this specific covariance-based optimization interpretation.)
- **Break condition:** If the posterior distribution is degenerate or the covariance measure does not accurately reflect the "information value" of an arm, the exploration signal may be misaligned.

### Mechanism 3: Suboptimal Conservative Exploration
- **Claim:** Thompson Sampling is strictly suboptimal relative to the $R^2$-optimal policy because its covariance-based regularizer is too conservative, failing to aggressively explore when one arm dominates in information value.
- **Mechanism:** The paper demonstrates that as uncertainty resolves in specific configurations (e.g., one arm known, one unknown), the TS regularizer ($\tilde{\nu}$) grows slower than the optimal regularizer ($\nu$). This causes TS to under-explore the uncertain arm compared to the theoretical optimum (Figure 1, Right).
- **Core assumption:** The "optimal" benchmark is defined strictly by the $R^2$ objective, not standard discounted reward.
- **Evidence anchors:**
  - [Section 6.1] (Analysis of the phase change and comparison of $\nu$ vs $\tilde{\nu}$)
  - [Figure 1] (Visual gap in cumulative regret)
  - [Corpus] (No direct contradiction found, but standard literature usually treats TS as asymptotically optimal or near-optimal in $R_T$, whereas this paper critiques it in $R^2$.)
- **Break condition:** If the goal is purely minimizing standard regret $R_T$ rather than squared regret $R^2$, the "suboptimality" claim may be weaker, although the paper argues the two are linked.

## Foundational Learning

- **Concept: Bellman Equation (Stationary vs. Non-stationary)**
  - **Why needed here:** The paper re-centers the analysis of TS around solving a stationary Bellman equation derived from $R^2$, contrasting it with the non-stationary finite-horizon or discounted infinite-horizon approaches.
  - **Quick check question:** Why does the Gittins index policy, derived from a discounted Bellman equation, fail to achieve the exploration goals of the standard bandit problem?

- **Concept: Point-Biserial Correlation**
  - **Why needed here:** This statistical concept defines the TS regularizer ($\tilde{\nu}$). It measures the relationship between a continuous variable (reward gap $\Delta$) and a binary variable (optimal arm identity $\Lambda$).
  - **Quick check question:** How does the covariance between the reward gap and the optimal arm identity relate to the "uncertainty" the agent faces?

- **Concept: Regularized Online Optimization**
  - **Why needed here:** The paper frames TS not as "sampling" but as "optimizing" a specific objective (squared loss + linear regularization).
  - **Quick check question:** In the optimization form $\text{argmin}[\bar{r}^2 + \tilde{\nu}x]$, what happens to the policy if the regularizer $\tilde{\nu}$ is set to zero?

## Architecture Onboarding

- **Component map:** State (posterior $\pi_t$) -> Optimization Oracle (solve $\text{argmin}_x [\bar{r}^2(x; \pi_t) + \text{regularizer} \cdot x]$) -> Regularizer Module (compute $\tilde{\nu}(\pi)$ for TS or $\nu(\pi)$ for Optimal) -> Policy Executor (map $x^*$ to probability distribution)

- **Critical path:** The derivation implies that to "run" TS in this view, one must compute the covariance of the reward gap and optimal arm identity. The "Fixed Policy" (Section 6.2) adds a "Tension Check" module (computing $s(\pi_t)$) to shut down regularization when one arm is clearly better for both info and reward.

- **Design tradeoffs:**
  - **TS Regularizer ($\tilde{\nu}$):** Simple, relies only on posterior covariance; *Tradeoff:* Too conservative, fails to abandon suboptimal arms quickly enough.
  - **$R^2$-Optimal Regularizer ($\nu$):** Requires solving/computing complex value function differences; *Tradeoff:* Computationally difficult but theoretically superior regret.
  - **Fixed Policy Regularizer ($\nu_{fix}$):** Heuristic adjustment to TS; *Tradeoff:* Improves performance on specific "under-explored" priors but adds complexity.

- **Failure signatures:**
  - **Incomplete Learning:** Occurs if the Lagrange multiplier $\lambda \neq 1$ (Prop 3). The policy commits to one arm permanently while uncertainty remains.
  - **Linear Regret (Gittins):** Occurs if one uses standard discounting stationarization instead of $R^2$ stationarization in long-horizon settings.
  - **Conservative Stalling:** TS continues to pull a "runner-up" arm even when the leading arm is strictly better for both exploration and exploitation (Section 6.2).

- **First 3 experiments:**
  1. **Verify the Regularizer Gap:** Replicate Figure 1 (Right). Generate a Gaussian bandit where arm 1 has mean $\mu \to 0$ and arm 2 is known ($\theta_2 \equiv 0$). Plot $\nu(\pi)$ vs $\tilde{\nu}(\pi)$ to confirm that the TS regularizer remains bounded while the optimal one diverges.
  2. **Regret Comparison (One-Armed):** Replicate Figure 1 (Left). Compare cumulative regret of TS vs. the closed-form $R^2$-optimal policy (Prop 4) on a Gaussian bandit to quantify the "cost" of using the covariance regularizer.
  3. **Ablation on the "Simple Fix":** Implement the "Fixed Policy" (Eq. in Section 6.2) on the Bernoulli bandit setup ($\text{Beta}(5,4) \times \text{Beta}(k,k)$). Compare its regret to standard TS to verify if shutting down regularization actually improves performance when one arm dominates.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Is the $R^2$-stationarization approach the theoretically optimal method for faithful stationarization?
- **Basis in paper:** [explicit] Remark 1 states: "Whether the $R^2$-stationarization is the best in some sense is left for future research."
- **Why unresolved:** The paper establishes $R^2$ as a natural objective because it controls the $O(\sqrt{T})$ regret bound, but it does not preclude the existence of other stationary objectives that might yield tighter bounds or different optimal policies.
- **What evidence would resolve it:** A formal proof establishing the uniqueness of $R^2$ for minimizing the regret sequence, or a counter-example demonstrating a superior stationarization formulation.

### Open Question 2
- **Question:** Can the exact $R^2$-optimal policy be computed efficiently (in closed form) for the two-armed bandit?
- **Basis in paper:** [inferred] Page 17 notes that "the $R^2$-optimal policy is no longer tractable" in the two-armed case, requiring a backward recursion with finite boundary conditions ($\bar{M}$) for approximation.
- **Why unresolved:** The reliance on a numerical approximation (propagating values inward from a boundary) suggests a lack of an analytical solution for the general two-armed case.
- **What evidence would resolve it:** Derivation of a closed-form analytical solution for the two-armed Bellman equation, or an algorithm that computes the policy without discretizing the belief space.

### Open Question 3
- **Question:** Does the proposed "simple fix" to Thompson Sampling generalize effectively to K-armed and contextual settings?
- **Basis in paper:** [inferred] The paper introduces a "simple fix" in Section 6.2 to address conservative regularization, but the empirical validation is restricted to a two-armed Bernoulli bandit.
- **Why unresolved:** The fix relies on a condition based on "information gain" differences between two arms; it is unclear how this logic scales or if it remains principled when selecting among $K > 2$ competing arms.
- **What evidence would resolve it:** Theoretical guarantees or empirical results demonstrating that the modified regularizer maintains its regret reduction properties in high-dimensional or K-armed environments.

## Limitations

- The paper's theoretical framework is built around squared regret ($R^2$) rather than standard cumulative regret ($R_T$), which may limit practical applicability
- The computational complexity of the optimal $R^2$-policy is acknowledged but not fully explored, requiring numerical approximations with boundary conditions
- The proposed "simple fix" is heuristic in nature and its general applicability across problem classes and scaling to K-armed settings remains uncertain

## Confidence

- **High confidence:** The mathematical derivation of TS as an online optimization problem (Theorem 1) and the characterization of its regularizer as point-biserial covariance
- **Medium confidence:** The claim that TS is "suboptimal" relative to the $R^2$-optimal policy, as this depends on the specific $R^2$ objective rather than standard regret
- **Medium confidence:** The "simple fix" proposed in Section 6.2 improves performance by adjusting the regularizer, but the heuristic nature of $\nu_{fix}$ and its general applicability remains uncertain

## Next Checks

1. **Regularizer behavior verification:** Replicate the phase transition analysis showing $\nu(\pi) \to \infty$ while $\tilde{\nu}(\pi)$ remains bounded as $\mu \to 0$ to confirm the core claim about TS's conservative exploration.

2. **Regret comparison across metrics:** Compare TS, $R^2$-optimal, and fixed policy performance not just on $R^2$ but also on standard cumulative regret $R_T$ to assess practical significance of the theoretical improvements.

3. **Robustness to prior specifications:** Test the fixed policy across a broader range of prior distributions (different Beta parameters, non-Gaussian rewards) to evaluate whether the simple fix generalizes beyond the specific cases presented.