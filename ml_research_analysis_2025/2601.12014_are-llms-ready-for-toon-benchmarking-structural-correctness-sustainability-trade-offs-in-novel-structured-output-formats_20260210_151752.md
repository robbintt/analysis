---
ver: rpa2
title: Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability
  Trade-offs in Novel Structured Output Formats
arxiv_id: '2601.12014'
source_url: https://arxiv.org/abs/2601.12014
tags:
- toon
- structured
- formats
- output
- correctness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a sustainability-aware evaluation framework\
  \ for structured output generation by large language models (LLMs). The core idea\
  \ is to integrate environmental impact\u2014measured via token usage, generation\
  \ time, and estimated carbon emissions\u2014into format evaluation, alongside traditional\
  \ structural correctness."
---

# Are LLMs Ready for TOON? Benchmarking Structural Correctness-Sustainability Trade-offs in Novel Structured Output Formats

## Quick Facts
- **arXiv ID**: 2601.12014
- **Source URL**: https://arxiv.org/abs/2601.12014
- **Reference count**: 30
- **Primary result**: Novel framework benchmarks structured output formats (JSON/XML/YAML/TOON) by integrating structural correctness with carbon-aware efficiency metrics

## Executive Summary
This paper introduces a sustainability-aware evaluation framework for structured output generation by large language models (LLMs), integrating environmental impact—measured via token usage, generation time, and estimated carbon emissions—into format evaluation alongside traditional structural correctness. The core innovation is the Environment-Aware Generation Correctness Score (GCS_env), which combines correctness with carbon-normalized efficiency to enable interpretable trade-offs between reliability and sustainability. Experiments benchmark the novel TOON format against JSON, XML, and YAML across diverse LLMs and parameter scales, revealing that TOON yields more compact outputs and lower emissions but lower structural correctness when models lack native support; this correctness gap narrows with increased model capacity.

## Method Summary
The study evaluates four structured output formats (JSON, XML, YAML, TOON) across multiple LLMs using 50 generation instances from the StructEval benchmark. For each instance, models generate outputs in all formats, measuring token count (NT), duration, and carbon emissions (CE) via CodeCarbon during decoding only. Structural correctness is assessed via Render Score (schema compliance) and Syntax Score (format validity), combined into GCS = 0.2×Render + 0.8×Syntax. Environmental efficiency (EES) normalizes emissions against a reference factor (X_ref = 0.001719 kgCO2e/1000 tokens), and GCS_env = (1-γ)·GCS + γ·EES provides a unified metric where γ controls the sustainability-correctness trade-off.

## Key Results
- TOON produces markedly more compact outputs (fewer tokens) than JSON/XML/YAML across all evaluated models
- TOON yields substantially lower estimated carbon emissions due to reduced token generation
- Small models (4B parameters) show near-total TOON format failure (GCS near 0.05), but this gap narrows significantly with larger models (Llama 3.3 70B shows no significant correctness differences)
- GCS_env rankings depend on γ: TOON only surpasses baselines in high-γ regimes (γ>0.7), indicating format rankings shift based on deployment priorities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compact output formats reduce inference-time carbon emissions by lowering generated token counts.
- Mechanism: TOON's more compact representation requires fewer output tokens to encode equivalent semantic content. Since carbon emissions scale with token generation during decoding, fewer tokens directly translate to lower energy consumption and emissions.
- Core assumption: Carbon emissions during inference are primarily driven by output token generation rather than prompt processing.
- Evidence anchors:
  - [abstract] "TOON yields markedly more compact outputs and lower emissions"
  - [Section 4.3] "TOON systematically produces more compact outputs... substantially lower number of generated tokens... This reduction in token count directly translates into markedly lower estimated carbon emissions"
  - [corpus] Related work "Smart but Costly?" examines energy efficiency trade-offs but doesn't focus on format compactness
- Break condition: If prompt processing dominates emissions (not the case here), token savings would yield marginal benefits. If regeneration rates are high due to format errors, total emissions may increase despite per-output efficiency.

### Mechanism 2
- Claim: Increased model capacity mitigates the lack of native format support for novel representations like TOON.
- Mechanism: Larger models (e.g., 70B+ parameters) exhibit greater in-context learning and generalization, allowing them to internalize and apply previously unseen structural constraints specified only through prompt instructions.
- Core assumption: Model scale correlates with ability to follow novel format specifications without training exposure.
- Evidence anchors:
  - [abstract] "this correctness gap narrows with increased model capacity"
  - [Section 4.4.1] "larger models consistently reduce the structural correctness gap... for Llama 3.3 70B, no statistically significant differences are observed when comparing TOON against any baseline format"
  - [corpus] StructEval benchmark shows structured output challenges across models but doesn't examine capacity effects on novel formats
- Break condition: If scale-dependent gains plateau or if format complexity exceeds in-context learning capacity, larger models won't close the correctness gap.

### Mechanism 3
- Claim: The GCS_env metric enables interpretable trade-offs between structural correctness and environmental efficiency.
- Mechanism: GCS_env = (1-γ)·GCS + γ·EES combines correctness (GCS) and carbon-normalized efficiency (EES) into a single bounded score [0,1]. The parameter γ allows explicit weighting of sustainability vs. reliability.
- Core assumption: Carbon emissions can be meaningfully normalized against a reference emission factor (X_ref = 0.001719 kgCO2e per 1000 tokens).
- Evidence anchors:
  - [Section 3.3] "GCS_env preserves the original GCS evaluation signal while explicitly incorporating the carbon intensity of output generation"
  - [Section 4.5] "TOON surpasses the corresponding baseline formats only in the high-γ regime... indicating that format rankings are not absolute but depend on deployment priorities"
  - [corpus] No directly comparable environment-aware metrics found; existing benchmarks focus solely on correctness
- Break condition: If X_ref is misestimated or γ selection is arbitrary without deployment context, the metric may not reflect real-world priorities.

## Foundational Learning

- **Concept: Structured Output Generation for Downstream Systems**
  - Why needed here: The entire benchmark assumes LLMs must produce machine-parseable outputs (JSON, XML, YAML, TOON) for automated pipelines where syntax errors propagate as failures.
  - Quick check question: Can you explain why a malformed JSON (e.g., missing comma) is worse than semantically vague natural language in a machine-to-machine workflow?

- **Concept: Token-Energy-Emissions Pipeline**
  - Why needed here: Understanding how generated tokens → GPU compute → energy consumption → carbon emissions (via grid emission factors) is essential to interpret EES and CE metrics.
  - Quick check question: If a model generates 200 tokens on a GPU consuming 400W, and your local grid emits 0.5 kgCO2e/kWh, what's the approximate carbon cost of one generation?

- **Concept: TOON Format Design Principles**
  - Why needed here: TOON is positioned as a more compact alternative; understanding its design (less verbose syntax, implicit structure) explains why it achieves lower token counts.
  - Quick check question: Why might a format with fewer explicit delimiters (like closing tags in XML) be harder for LLMs to generate correctly without training exposure?

## Architecture Onboarding

- **Component map**: Input: Natural language prompt → LLM → Structured output (JSON/XML/YAML/TOON) → Token count (NT), Duration, CE (via CodeCarbon) → Render Score + Syntax Score → GCS → EES = 1 / (1 + X/X_ref) → GCS_env = (1-γ)·GCS + γ·EES

- **Critical path**: The EES calculation (Eq. 3) is the core innovation. Ensure X_ref (0.001719 kgCO2e/1000 tokens) matches your deployment context. The paper uses GPT-4 estimates from the LLM Carbon Calculator.

- **Design tradeoffs**:
  - γ=0.5 (equal weight) favors established formats unless model scale is large
  - γ>0.7 is needed for TOON to outperform JSON/YAML in aggregate
  - Larger models reduce correctness gaps but increase absolute emissions per output

- **Failure signatures**:
  - Small models (4B) with TOON: GCS near 0.05 (near-total format failure)
  - High γ with low model capacity: sustainable but unreliable outputs
  - Regeneration loops: if TOON correctness <70%, repeated calls may negate emission savings

- **First 3 experiments**:
  1. **Baseline replication**: Run the 50-instance generation task with JSON and TOON on a mid-sized model (7B-12B), compute GCS and EES separately to verify the trade-off.
  2. **Sensitivity sweep**: Vary γ from 0 to 1 in 0.1 increments to find the crossover point where TOON matches or exceeds baseline GCS_env for your target model.
  3. **Failure mode analysis**: For instances where TOON's Render Score is 0, inspect the malformed outputs to identify whether errors are structural (missing delimiters) or semantic (wrong key-value pairs), informing prompt engineering or fine-tuning needs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicit fine-tuning of LLMs on the TOON format disentangle structural limitations caused by format design from those caused by a lack of training exposure?
- Basis in paper: [explicit] The Future Work section states that fine-tuning "would allow us to disentangle limitations due to format design from those arising purely from lack of exposure during training."
- Why unresolved: Current results rely on prompt-based instructions for TOON, meaning low correctness scores may stem from the models' unfamiliarity with the format rather than the format's complexity.
- What evidence would resolve it: A comparative benchmark between base models (prompted) and fine-tuned models (trained on TOON) using the GCS and GCS_env metrics.

### Open Question 2
- Question: Do the trade-offs between structural correctness and environmental efficiency persist when evaluating format conversion tasks (e.g., JSON to TOON) rather than generation tasks?
- Basis in paper: [explicit] The Limitations and Future Work sections note that the study restricted analysis to generation tasks and that "extending the analysis to include conversion tasks... would further strengthen the generality of the proposed evaluation framework."
- Why unresolved: The paper focuses on the "most challenging setting" (generation); it is unknown if TOON's compactness yields similar efficiency advantages or correctness penalties when models translate between existing formats.
- What evidence would resolve it: Running the proposed framework on the "conversion tasks" defined in the StructEval benchmark to compare aggregate GCS_env scores against generation tasks.

### Open Question 3
- Question: How sensitive are the structural correctness scores for TOON to alternative prompt formulations or instruction-tuning strategies?
- Basis in paper: [explicit] The Limitations section acknowledges that "TOON generation relies on prompt-based instructions defined by the authors" and that "Alternative prompt formulations... could lead to different outcomes."
- Why unresolved: The study uses a single set of prompt instructions derived from official documentation; it remains unclear if optimized prompts could close the structural correctness gap without requiring larger, more carbon-intensive models.
- What evidence would resolve it: An ablation study testing various prompt complexities and instruction styles for TOON across the evaluated model set.

### Open Question 4
- Question: Does the computational overhead of regenerating malformed outputs negate the token-efficiency gains of compact formats like TOON in real-world deployment loops?
- Basis in paper: [inferred] The Discussion section notes that "repeated decoding increases both token usage and emissions" and that "the environmental advantage of TOON is not solely a property of the representation, but also of its effective success rate."
- Why unresolved: The paper evaluates single-generation emissions, but in practice, lower correctness (Render Score) triggers costly regeneration loops that were not quantified in the final carbon estimates.
- What evidence would resolve it: A simulation of production workflows that factors in the probability of regeneration failures to calculate the cumulative carbon cost per valid output.

## Limitations

- The correctness gap for TOON on small models (4B parameters) is substantial (GCS near 0.05), raising questions about practical viability without fine-tuning or prompt engineering improvements.
- The benchmark uses 50 generation instances from StructEval, but the specific schemas and complexity levels are not fully characterized, limiting generalizability to other structured output domains.
- The evaluation measures only generation-phase emissions, not the full lifecycle including prompt processing or post-processing costs.

## Confidence

**High Confidence**: The core mechanism that compact formats reduce token generation and thus emissions is well-established and directly observable in the results. The scaling relationship between model capacity and TOON correctness is clearly demonstrated across multiple model families.

**Medium Confidence**: The GCS_env metric's utility depends on deployment context and γ selection, which the paper addresses but doesn't fully resolve. The claim that format rankings shift based on sustainability priorities is supported but relies on subjective weight choices.

**Low Confidence**: The long-term sustainability benefits of TOON assume no additional regeneration costs due to format errors. If TOON's lower correctness on small models leads to repeated generation attempts, the emission savings could be negated.

## Next Checks

1. **Emission Factor Sensitivity Analysis**: Reproduce the GCS_env calculations using emission factors varying by ±50% from the reference value to quantify how sensitive format rankings are to local grid conditions and hardware efficiency assumptions.

2. **Regeneration Cost Simulation**: For TOON outputs with Render Score < 0.7, simulate the expected emissions if regeneration occurs until a valid output is produced. Compare the total emissions (including failed attempts) against baseline formats to determine the break-even correctness threshold.

3. **Schema Complexity Scaling**: Design a controlled experiment varying the complexity of target schemas (number of nested levels, required fields, data types) to identify at what complexity point model capacity becomes insufficient for TOON correctness, regardless of parameter count.