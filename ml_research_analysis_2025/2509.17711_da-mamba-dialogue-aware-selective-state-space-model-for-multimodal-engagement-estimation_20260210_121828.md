---
ver: rpa2
title: 'DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement
  estimation'
arxiv_id: '2509.17711'
source_url: https://arxiv.org/abs/2509.17711
tags:
- arxiv
- mamba
- fusion
- engagement
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DA-Mamba introduces a dialogue-aware multimodal architecture that
  replaces attention-heavy dialogue encoders with Mamba-based selective state-space
  processing, achieving linear time and memory complexity while retaining expressive
  cross-modal reasoning. The model employs three core modules: a Dialogue-Aware Encoder,
  and two Mamba-based fusion mechanisms: Modality-Group Fusion and Partner-Group Fusion,
  enabling efficient long-range modeling and partner-context assembly.'
---

# DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation

## Quick Facts
- **arXiv ID:** 2509.17711
- **Source URL:** https://arxiv.org/abs/2509.17711
- **Reference count:** 0
- **Primary result:** DA-Mamba achieves 0.66 global CCC on engagement estimation, outperforming attention-based models while reducing training time and memory.

## Executive Summary
DA-Mamba introduces a dialogue-aware multimodal architecture that replaces attention-heavy dialogue encoders with Mamba-based selective state-space processing, achieving linear time and memory complexity while retaining expressive cross-modal reasoning. The model employs three core modules: a Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group Fusion and Partner-Group Fusion, enabling efficient long-range modeling and partner-context assembly. Extensive experiments on three standard benchmarks (NoXi, NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art methods in concordance correlation coefficient (CCC), with global average CCC of 0.66, while reducing training time and peak memory. These gains enable processing much longer sequences and facilitate real-time deployment in resource-constrained, multi-party conversational settings. The source code is available at: https://github.com/kksssssss-ssda/MMEA.

## Method Summary
DA-Mamba processes audio and visual modalities separately through modality-specific Mamba stacks, then fuses them via partner-context assembly and cross-attention. The architecture replaces traditional attention-based encoders with Mamba blocks that combine local chunked attention with global selective state-space modeling. The model is trained on frame-level multimodal features from three benchmarks, using a composite loss of CCC loss and cross-modal alignment loss. Key hyperparameters include 4-layer Mamba stacks, state dimension of 16, kernel size of 4, expansion factor of 2, and chunk size of 32.

## Key Results
- Achieves 0.66 global CCC across three benchmarks (NoXi Base, NoXi-Add, MPIIGI)
- Outperforms prior state-of-the-art methods on all three datasets
- Demonstrates reduced training time and memory usage compared to attention-based baselines
- Shows efficiency gains that enable real-time deployment in multi-party conversational settings

## Why This Works (Mechanism)
DA-Mamba leverages Mamba's selective state-space modeling to efficiently capture long-range temporal dependencies in multimodal engagement signals while maintaining linear complexity. The modality-group fusion allows specialized processing for audio-visual characteristics, and partner-context assembly enables effective modeling of multi-party interaction dynamics. The cross-modal alignment loss ensures coherent representations across modalities without requiring expensive attention mechanisms.

## Foundational Learning

**Selective State-Space Models**
*Why needed:* Enable efficient long-range sequence modeling without quadratic complexity of attention
*Quick check:* Verify Mamba block implementation correctly combines chunked attention with SSM global branch

**Cross-Modal Alignment**
*Why needed:* Ensures coherent representations across different sensory modalities for joint decision-making
*Quick check:* Monitor alignment loss during training and visualize embedding similarity matrices

**Partner-Context Assembly**
*Why needed:* Captures multi-party interaction dynamics by integrating context from other participants
*Quick check:* Verify context tensor shapes are correct for different numbers of participants

## Architecture Onboarding

**Component Map**
Modality features -> Modality-Group Fusion (Audio Mamba + Visual Mamba) -> Partner-Group Fusion -> Cross-Attention Encoder -> Engagement Prediction

**Critical Path**
1. Input: 96-frame windows of multimodal features
2. Separate audio (eGeMAPS+W2V) and visual (CLIP+OpenFace+OpenPose) groups
3. Process each group through 4-layer Mamba stacks with chunked attention
4. Apply InfoNCE alignment loss to ensure cross-modal coherence
5. Assemble partner context via concatenation for multi-party settings
6. Cross-attend target participant with partner context
7. Output: Frame-level engagement scores

**Design Tradeoffs**
- Mamba vs Attention: Linear complexity vs established performance
- Chunk size: Balances local precision with global SSM efficiency
- Partner concatenation vs interleaved processing: Simpler implementation vs potential interaction structure loss

**Failure Signatures**
- NaN losses from CCC computation
- OOM errors despite linear complexity claims
- Poor cross-modal alignment degrading final CCC
- Incorrect partner-context assembly for multi-party datasets

**First Experiments**
1. Ablation: Train with and without InfoNCE alignment loss to confirm its impact on cross-modal CCC
2. Efficiency: Profile GPU memory and training time on RTX 4090 to verify claimed linear complexity gains
3. Scalability: Test partner-context assembly with varying numbers of participants (2-6) to assess multi-party scaling

## Open Questions the Paper Calls Out

**Question 1:** Does DA-Mamba maintain its efficiency advantage when scaling to conversations with many more participants (e.g., M > 6), given that the cross-attention mechanism in the Dialogue-Aware Encoder still incurs quadratic complexity with respect to partner-context length?

**Question 2:** What is the actual latency and throughput of DA-Mamba in streaming/real-time deployment scenarios, and does the sliding-window approach introduce boundary artifacts or latency accumulations?

**Question 3:** How sensitive is DA-Mamba to the chunk size hyperparameter s, and is s=32 universally optimal or dataset-dependent?

**Question 4:** Does partner-context assembly by simple temporal concatenation adequately capture turn-taking dynamics and dyadic interaction structure?

## Limitations
- Exact architectural hyperparameters (projection dimensions, cross-attention details) are underspecified
- NoXi dataset splits and validation protocols are not detailed
- Missing feature extraction model versions and preprocessing scripts
- No runtime/memory profiling provided to quantify efficiency gains over baselines

## Confidence
- **High confidence:** Methodological novelty and general architectural description
- **Medium confidence:** Reported benchmark CCC values due to unspecified details
- **Low confidence:** Practical deployment claims without explicit runtime measurements

## Next Checks
1. Reimplement the DA-Mamba architecture with placeholder hyperparameters and perform a controlled ablation study: train with and without InfoNCE alignment loss to confirm its impact on cross-modal CCC.
2. Profile GPU memory and training time on RTX 4090 using the reported batch size (128) and window size (96 frames) to verify claimed linear complexity and efficiency gains.
3. Contact the authors to obtain exact NoXi train/val/test splits and pre-extracted feature metadata to ensure faithful reproduction of reported CCC scores.