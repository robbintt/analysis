---
ver: rpa2
title: 'OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling
  Transformations for Better Distribution Fitting'
arxiv_id: '2501.13987'
source_url: https://arxiv.org/abs/2501.13987
tags:
- quantization
- ostquant
- transformation
- arxiv
- qsur
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OSTQuant is a post-training quantization method for large language
  models that optimizes data distributions across the entire quantization space. It
  introduces the Quantization Space Utilization Rate (QSUR) as a metric to evaluate
  quantizability, and employs learnable equivalent transformations combining orthogonal
  and scaling transformations to improve QSUR.
---

# OstQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting

## Quick Facts
- arXiv ID: 2501.13987
- Source URL: https://arxiv.org/abs/2501.13987
- Reference count: 40
- Primary result: OSTQuant achieves >99.5% accuracy retention in W4-only settings and 32% reduced performance gap in W4A4KV4 on LLaMA-3-8B with 2× speedup and 3.5× memory savings

## Executive Summary
OSTQuant introduces a novel post-training quantization method for large language models that optimizes data distributions across the entire quantization space. The method combines learnable orthogonal and scaling transformations to improve Quantization Space Utilization Rate (QSUR), a metric that measures how efficiently data fills the quantization hypercube. By addressing distributional pathologies through rotation and scaling, OSTQuant achieves state-of-the-art accuracy with minimal computational overhead, requiring only 150 training iterations and maintaining 2× inference speedup.

## Method Summary
OSTQuant employs learnable equivalent transformations consisting of orthogonal (rotation) and scaling components to optimize weight and activation distributions for quantization. The method introduces QSUR as a metric measuring data space utilization in the quantization hypercube. Training uses KL-Top loss to focus on top-k predictions during calibration, reducing noise from long-tail distributions. Transformations are fused into weights post-training, requiring no runtime overhead. The approach uses RiemannAdam for manifold optimization on Stiefel manifolds and WOMI initialization based on weight covariance eigendecomposition.

## Key Results
- >99.5% accuracy retention in W4-only quantization on LLaMA-3-8B
- 32% reduction in performance gap for W4A4KV4 configuration compared to state-of-the-art methods
- 2× inference speedup and 3.5× memory savings with only 150 training iterations
- Strong correlation (0.85) between QSUR and actual quantization accuracy across layers

## Why This Works (Mechanism)

### Mechanism 1
Quantization Space Utilization Rate (QSUR) predicts quantization accuracy by measuring how efficiently data fills the quantization hypercube. QSUR = V_data / V_hypercube, where V_data is the ellipsoid volume from covariance Σ and V_hypercube is determined by max range across all dimensions. Higher QSUR correlates with better accuracy because more quantization levels fall within the actual data distribution. This works under the assumption that weights and activations follow approximately Gaussian distributions where covariance eigenstructure determines quantization difficulty.

### Mechanism 2
Combining orthogonal (rotation) and scaling transformations achieves higher QSUR than either alone because each addresses different distributional pathologies. Orthogonal matrices rotate distributions to balance feature orientations and reduce outliers by spreading variance across dimensions. Diagonal scaling matrices adjust per-channel variance to reduce eigenvalue disparities. The optimal transform is T = c·Λ^{-1/2}·Q^T, yielding maximum QSUR. This assumes the optimal transformation can be decomposed into separate rotation and scaling components that commute with the network's computational graph.

### Mechanism 3
KL-Top loss (KL divergence over top-k logits) reduces noise from long-tail vocabulary predictions while preserving semantic information during calibration. LLM outputs follow extreme long-tail distributions—only a few tokens have significant probability. Restricting to top-k (k=1000) focuses gradients on meaningful predictions. This assumes the top-k predictions from the full-precision model contain sufficient semantic information for the quantized model to learn distribution alignment.

## Foundational Learning

- **Uniform Quantization and the Range-Precision Tradeoff**: OSTQuant addresses how uneven distributions expand quantization range, reducing effective bit precision for most values. Quick check: Given a distribution with range [-100, 100] but 99% of values in [-1, 1], how many bits are "wasted" if you quantize to 8-bit unsigned over the full range?

- **Eigenvalue Decomposition and Multivariate Gaussian Geometry**: QSUR is derived from the ellipsoid defined by covariance matrix Σ = QΛQ^T; understanding how eigenvalues determine spread is essential. Quick check: For a 2D Gaussian with eigenvalues λ₁=100, λ₂=1, what shape is the 99% confidence ellipsoid, and what is its QSUR in a bounding square?

- **Stiefel Manifold and Riemannian Optimization**: Orthogonal matrices must be optimized under the constraint O^T O = I; standard SGD violates this. RiemannAdam handles this via manifold-aware gradients. Quick check: Why does applying standard gradient descent to an orthogonal matrix eventually break the orthogonality constraint?

## Architecture Onboarding

- Component map: Input → Embedding → [R_res (global rotation)] → Transformer Block i: RMSNorm → [S_attn (scaling)] → Q,K,V projections → [R_ov, S_ov (head-level)] → Attention → RMSNorm → [S_ffn (scaling)] → FFN up/down projections → Residual connection (R_res propagates through) → Final LayerNorm → Output head [R_res^{-1}]

- Critical path: 1) Initialize R_res via Weight Outlier Minimization Initialization (WOMI): concatenate weights receiving residual inputs, compute covariance eigenvectors Q_W, set R_res = Q_W · H^T (Hadamard) 2) Run 150 iterations of RiemannAdam on calibration data (1000 samples × 2048 tokens) 3) Fuse learned O and Λ into weights: W_quant = W · O · Λ 4) Deploy with 4-bit kernels (no transformation matrices at runtime)

- Design tradeoffs: RiemannAdam vs Cayley SGD: RiemannAdam converges faster (150 vs 500 iterations) but requires manifold optimization library (Geoopt). Cayley SGD needs higher learning rates. Global R_res vs per-layer rotations: Global rotation maintains equivalence across residual connections but couples all layers; per-layer allows more flexibility but risks overfitting. k in KL-Top: k=1000 balances semantic richness vs noise; too small (k=5) loses information, too large (k=10000) reintroduces noise.

- Failure signatures: Perplexity improves but zero-shot accuracy drops → CE loss overfitting to calibration set; switch to KL-Top. QSUR plateaus but accuracy still poor → Transformations may not address semantic outliers; check if outliers are task-meaningful. Orthogonal matrix diverges → Learning rate too high for manifold parameters; use 10× smaller LR for scaling vs rotation.

- First 3 experiments: 1) QSUR correlation validation: Compute QSUR for LLaMA-2-7B layers before/after OSTQuant; plot against per-layer quantization error to verify positive correlation. 2) Ablation on transformation components: Test {R_res only}, {R_res + S}, {R_res + S + R_ov} on W4A4KV4 to isolate contribution of each component. 3) KL-Top k sweep: Test k ∈ {50, 100, 500, 1000, 5000} on LLaMA-2-7B W4A4KV4; measure zero-shot accuracy vs WikiText PPL to find optimal k.

## Open Questions the Paper Calls Out

**Open Question 1**: Can the OSTQuant framework be effectively extended to fully integer-only quantized LLMs where all activations, including inputs and outputs of non-linear functions like RoPE and SiLU, are quantized to low bits? The current method focuses on linear layers and standard quantization nodes. Handling non-linear operations like RoPE and SiLU with low-bit integer arithmetic requires specific mathematical tricks that have been proposed but not yet experimentally validated within the OSTQuant framework.

**Open Question 2**: How robust is the Quantization Space Utilization Rate (QSUR) metric when the underlying weight and activation data distributions deviate significantly from the Gaussian assumption or when the mean vector is non-negligible? Real-world LLM activations often exhibit heavy-tailed or asymmetric distributions that violate Gaussian assumptions. If the mean is significant or the distribution shape diverges, the correlation between QSUR and actual quantization error might weaken, limiting the metric's theoretical reliability.

**Open Question 3**: Is there a theoretically optimal or adaptive mechanism for selecting the k parameter in the KL-Top loss function, rather than relying on a fixed value determined by empirical ablation? The optimal k likely depends on the entropy of the specific model's prediction distribution, which varies across model sizes and tasks. A fixed k might be suboptimal for very large vocabularies or different architectures.

## Limitations

- QSUR metric validity depends on Gaussian distributional assumptions; may mispredict quantization difficulty for highly non-Gaussian weight distributions
- KL-Top loss calibration assumes top-k predictions capture sufficient semantic information, which may not hold for specialized domains requiring accurate rare-token calibration
- RiemannAdam implementation details are underspecified (betas, epsilon parameters missing), affecting reproducibility across different manifold optimization libraries

## Confidence

- **High Confidence**: QSUR correlation with quantization accuracy, transformation equivalence preservation, and overall accuracy improvements vs baselines (directly supported by quantitative experiments)
- **Medium Confidence**: KL-Top loss effectiveness and its noise reduction mechanism (experiments show improvement but theoretical justification is somewhat hand-wavy)
- **Low Confidence**: Generalizability across model architectures and tasks (validates on LLaMA family but doesn't test on models with different initialization schemes, activation functions, or architectural innovations)

## Next Checks

1. **QSUR Correlation Validation**: Compute QSUR for each layer before/after OSTQuant on LLaMA-2-7B, then plot against per-layer quantization error to verify the claimed positive correlation. This validates the fundamental assumption that QSUR predicts quantization difficulty.

2. **Ablation Study Extension**: Replicate the component ablation but systematically vary k in KL-Top (k ∈ {50, 100, 500, 1000, 5000}) on LLaMA-2-7B W4A4KV4. Measure both WikiText PPL and zero-shot accuracy to find the optimal k and validate the noise-reduction hypothesis.

3. **Cross-Architecture Validation**: Apply OSTQuant to a different LLM family (e.g., OPT or Falcon) and compare QSUR improvement vs accuracy gains. This tests whether the QSUR-quantization accuracy relationship holds across initialization schemes and architectural variations.