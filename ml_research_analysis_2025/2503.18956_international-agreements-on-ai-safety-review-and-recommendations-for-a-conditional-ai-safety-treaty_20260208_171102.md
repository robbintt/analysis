---
ver: rpa2
title: 'International Agreements on AI Safety: Review and Recommendations for a Conditional
  AI Safety Treaty'
arxiv_id: '2503.18956'
source_url: https://arxiv.org/abs/2503.18956
tags:
- https
- international
- others
- safety
- compute
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews proposals for international agreements on AI
  safety, focusing on risk thresholds, regulations, types of agreements, and related
  processes. The authors propose a treaty establishing a compute threshold above which
  AI development requires rigorous oversight, including model, security, and governance
  audits by an international network of AI Safety Institutes.
---

# International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty

## Quick Facts
- **arXiv ID**: 2503.18956
- **Source URL**: https://arxiv.org/abs/2503.18956
- **Reference count**: 0
- **Primary result**: Proposes a treaty establishing compute thresholds for AI development requiring rigorous oversight, audits, and potential development pauses

## Executive Summary
This paper reviews international AI safety agreement proposals, identifying consensus areas around risk thresholds, regulations, agreement types, and processes. The authors recommend a conditional AI safety treaty that uses training compute as a regulatory trigger, requiring complementary audits of models, security, and governance practices overseen by an international network of AI Safety Institutes. The treaty would allow these institutes to pause development if risks are deemed unacceptable. The approach aims to balance implementable measures with flexibility to adapt as research progresses.

## Method Summary
This is a policy analysis paper reviewing international AI safety agreement proposals through literature review rather than computational experiments. The authors synthesize existing proposals from 2023 onwards to identify areas of consensus and disagreement, then propose a treaty architecture based on these findings. No model training, datasets, or quantitative evaluation is performed.

## Key Results
- Training compute thresholds can serve as an initial filter for identifying high-risk AI models through empirical scaling laws
- Complementary audits of models, security, and governance provide layered safety assurance
- The global compute supply chain's concentration enables monitoring and verification mechanisms
- A treaty-based approach with compute thresholds offers implementable flexibility for international coordination

## Why This Works (Mechanism)

### Mechanism 1: Compute Threshold as Regulatory Trigger
- Claim: Training compute thresholds can serve as an effective "initial filter" for identifying models that may pose significant risks.
- Mechanism: The threshold creates a measurable, monitorable boundary. When models exceed the threshold, they enter a regulatory regime requiring audits. This works because: (1) compute correlates with capabilities via empirical scaling laws, (2) compute is a physical resource that is "detectable, excludable, and quantifiable," and (3) the supply chain is concentrated, enabling monitoring.
- Core assumption: The correlation between training compute and model capabilities (scaling laws) remains sufficiently predictive of risk as models scale.
- Evidence anchors:
  - [abstract]: "propose a treaty establishing a compute threshold above which development requires rigorous oversight"
  - [section 1.1]: "Training compute thresholds can act as an 'initial filter' for identifying models that may pose a high risk... increases in training compute tend to correlate with increases in a model's capabilities, a phenomenon attributed to empirical 'scaling laws.'"
  - [corpus]: Weak/missing direct support - related papers focus on treaty design processes and safety research priorities, not compute-correlation mechanisms
- Break condition: If algorithmic efficiency improvements decouple compute from capabilities, or if small models achieve dangerous capabilities through architectural innovations, the threshold becomes an unreliable filter.

### Mechanism 2: Multi-Layered Audit Architecture
- Claim: Complementary audits of models, security, and governance provide layered safety assurance analogous to high-risk industries.
- Mechanism: Three audit types create defense-in-depth: (1) Model audits use evaluations and red-teaming with grey/white-box access to assess dangerous capabilities; (2) Security audits ensure model weights, code, and data are not vulnerable to theft or tampering; (3) Governance audits assess risk management procedures and safety culture. AISIs conduct or oversee these audits with authority to pause development.
- Core assumption: Evaluations can reliably detect dangerous capabilities, and governance/security practices meaningfully correlate with safe outcomes.
- Evidence anchors:
  - [abstract]: "mandate complementary audits of models, information security and governance practices, overseen by an international network of AI Safety Institutes"
  - [section 1.2.1]: "evaluations are generally considered an immature science—or even an 'art'... It is not currently known how to create evaluations that are valid, reliable and comprehensive."
  - [corpus]: The International AI Safety Report (neighbor paper, FMR=0.68) provides foundational context on capability evaluation methods
- Break condition: If evaluations cannot reliably elicit dangerous capabilities, or if governance audits become performative without substance, the audit regime provides false confidence.

### Mechanism 3: Compute Supply Chain Verification
- Claim: The global compute supply chain can be monitored and controlled to verify treaty compliance.
- Mechanism: Multiple verification methods work together: (1) Reporting requirements for chip producers/sellers on transfers; (2) Know-Your-Customer requirements for cloud providers; (3) Hardware-based tracking (unique identifiers, firmware controls); (4) Remote sensing of data centers; (5) On-site inspections. This creates a verification regime where unauthorized large-scale training becomes difficult to hide.
- Core assumption: The compute supply chain remains concentrated enough to monitor, and hardware-based verification methods are technically feasible.
- Evidence anchors:
  - [section 1.4.4]: "Compute, as a physical resource in a concentrated supply chain, is 'detectable, excludable, and quantifiable.'"
  - [section 1.4.4]: Describes reporting requirements, KYC schemes, hardware-based methods, and inspection regimes
  - [corpus]: Weak/missing - no direct corpus support for verification mechanism feasibility
- Break condition: If compute becomes diffuse, if hardware spoofing defeats tracking mechanisms, or if cloud providers operate in non-cooperating jurisdictions, verification fails.

## Foundational Learning

- **Compute as a Governance Metric**:
  - Why needed here: The entire treaty architecture hinges on using training compute as the regulatory trigger. Understanding why compute is chosen (vs. parameters, data quality, or capabilities directly) is essential for implementing thresholds correctly.
  - Quick check question: If a model achieves the same capabilities with half the compute due to algorithmic improvements, should it be regulated? What does the treaty say about threshold revision?

- **Safety Cases and Burden of Proof**:
  - Why needed here: The treaty may require developers to present "safety cases"—structured arguments with evidence that risks are below threshold. Understanding the difference between proving danger vs. proving safety is critical for audit design.
  - Quick check question: What evidence would constitute a valid "affirmative safety" case? How does this differ from simply passing evaluations?

- **AISI Network Architecture**:
  - Why needed here: Implementation requires coordination among national AISIs (US, UK, EU, Japan, Singapore, France, Canada) without a single centralized authority. Understanding mutual recognition and information-sharing constraints is essential.
  - Quick check question: If the UK AISI evaluates a model deployed in the EU, what mechanisms ensure mutual recognition of results?

## Architecture Onboarding

- **Component map**:
  - Compute Threshold Layer: Training FLOP calculation → threshold comparison → regulatory trigger
  - Audit Layer: Model evaluations (AISI-led, grey/white-box) + Security audits (private, accredited) + Governance audits (private, overseen)
  - Verification Layer: Chip transfer reporting + Cloud KYC requirements + Hardware tracking + Remote sensing + Inspections
  - Decision Layer: AISI assessment → risk determination → development pause or continuation
  - Incentive Layer: Research collaboration + benefit-sharing + trade restrictions on non-signatories

- **Critical path**:
  1. AISI specifies and periodically revises threshold level
  2. Developer self-reports when approaching threshold
  3. Pre-training security/governance audits verify readiness
  4. During-training evaluations at AISI-approved intervals
  5. AISI determines if risks are acceptable
  6. Development paused if unacceptable, continued with monitoring otherwise

- **Design tradeoffs**:
  - Threshold level: Lower catches more models but increases compliance burden; higher may miss risky models
  - Audit access: Black-box vs. grey-box vs. white-box trades audit depth against information security risks
  - Centralization: International institution vs. national AISI network affects sovereignty vs. coordination efficiency
  - Threshold scope: Pre-training only vs. including post-training and inference compute

- **Failure signatures**:
  - Threshold gaming: Models architected to stay just below threshold while achieving dangerous capabilities
  - Evaluation theater: Models pass evaluations but dangerous capabilities emerge post-deployment
  - Governance decay: Audits pass but safety culture degrades between audit cycles
  - Verification gaps: Unauthorized training in non-cooperating jurisdictions or on untracked hardware
  - Coordination breakdown: US or China refuses participation, treaty becomes ineffective

- **First 3 experiments**:
  1. Map current frontier models to training compute and simulate which would trigger at different thresholds (10^21 to 10^26 FLOP). This grounds threshold selection in actual capability distributions.
  2. Design and pilot an evaluation protocol for one dangerous capability (e.g., cyber-offense). Test whether current methods reliably distinguish safe from unsafe models; document false negative rates.
  3. Prototype hardware-based verification methods (chip identifiers, workload detection). Assess technical feasibility and identify weakest links in the verification chain.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal method for calculating compute thresholds, and at what level should they be set?
- Basis in paper: [explicit] "It is a priority to identify the optimal way to calculate them and the level at which they should be set."
- Why unresolved: Disagreement exists over whether to include post-training enhancement and inference compute alongside pre-training compute; thresholds require periodic revision as algorithmic efficiency improves.
- What evidence would resolve it: Empirical research correlating different compute calculation methods with capability emergence and risk profiles, validated across model architectures.

### Open Question 2
- Question: How can model evaluations be made valid, reliable, and comprehensive enough to serve as safety certifications?
- Basis in paper: [explicit] "It is not currently known how to create evaluations that are valid, reliable and comprehensive... evaluations cannot currently 'act as a 'certification' function.'"
- Why unresolved: Prompt sensitivity affects benchmark performance; it is impossible to anticipate all scenarios in which AI could cause harm or all dangerous capabilities that could enable them.
- What evidence would resolve it: Development of standardized evaluation frameworks with demonstrated reproducibility across runs and predictive validity for real-world harm scenarios.

### Open Question 3
- Question: Are hardware-based verification methods for tracking chip locations and detecting unauthorized activities technically feasible?
- Basis in paper: [explicit] "Further research into the technical feasibility of these solutions is required, and they may require phasing out or retrofitting hardware already in circulation."
- Why unresolved: Proposed mechanisms (physical unique identifiers, firmware-based detection of unauthorized clustering) remain untested at scale and face deployment challenges with existing hardware.
- What evidence would resolve it: Prototype demonstrations of chip tracking and workload monitoring systems, plus feasibility studies for retrofitting or phased deployment.

### Open Question 4
- Question: Can developers provide affirmative evidence of safety rather than merely demonstrating absence of dangerous capabilities?
- Basis in paper: [inferred] The paper notes debate between Righetti (testing for dangerous capabilities) versus Miotti and Wasil (affirmative safety evidence), acknowledging relevant research is "in early stages."
- Why unresolved: Current approaches cannot demonstrate how systems "reach conclusions"; broader risk assessments are used as imperfect substitutes.
- What evidence would resolve it: Development of interpretability methods, formal verification techniques, or mathematical models that provide positive safety guarantees.

## Limitations

- The effectiveness of training compute as a regulatory trigger depends on the stability of scaling laws, which may break down with algorithmic improvements
- Model evaluation methods are acknowledged to be an "immature science" with no proven approach for reliably detecting dangerous capabilities
- The proposed verification regime assumes cooperation from hardware manufacturers, cloud providers, and national governments that may not materialize

## Confidence

- **High confidence**: The identification of four areas of consensus (risk thresholds, regulations, agreement types, and processes) is well-supported by the literature review
- **Medium confidence**: The multi-layered audit architecture is theoretically sound but practical implementation challenges remain significant
- **Low confidence**: The specific threshold value selection process and evaluation methodologies are left undefined, creating uncertainty about practical implementation

## Next Checks

1. **Scaling Law Validation**: Conduct empirical analysis mapping current frontier models to their training compute and capabilities, testing whether existing scaling laws predict capabilities across diverse model families and architectures. This validates whether compute remains a reliable proxy for risk as models evolve.

2. **Evaluation Protocol Pilot**: Design and test a concrete evaluation protocol for one dangerous capability (e.g., autonomous replication or cyber-offense) across multiple model architectures. Measure false negative rates and compare against human assessment to establish baseline reliability.

3. **Verification Feasibility Study**: Prototype the proposed verification mechanisms (chip tracking, cloud KYC, remote sensing) and conduct a gap analysis identifying jurisdictions or scenarios where verification would fail. Test whether hardware-based methods can be circumvented and assess the political feasibility of mandatory reporting requirements.