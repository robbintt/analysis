---
ver: rpa2
title: Uncertainty Quantification for Surface Ozone Emulators using Deep Learning
arxiv_id: '2508.04885'
source_url: https://arxiv.org/abs/2508.04885
tags:
- uncertainty
- ozone
- bias
- surface
- north
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses uncertainty quantification (UQ) in deep learning
  models for surface ozone bias estimation, a critical need for air quality modeling
  and decision-making. The authors implement a U-Net architecture to predict surface
  ozone residuals from the MOMO-Chem model using two UQ methodologies: Monte-Carlo
  Dropout (Bayesian approximation) and Conformalized Quantile Regression (CQR).'
---

# Uncertainty Quantification for Surface Ozone Emulators using Deep Learning

## Quick Facts
- arXiv ID: 2508.04885
- Source URL: https://arxiv.org/abs/2508.04885
- Authors: Kelsey Doerksen, Yuliya Marchetti, Steven Lu, Kevin Bowman, James Montgomery, Kazuyuki Miyazaki, Yarin Gal, Freddie Kalaitzis
- Reference count: 18
- Key outcome: This study addresses uncertainty quantification (UQ) in deep learning models for surface ozone bias estimation, a critical need for air quality modeling and decision-making.

## Executive Summary
This study addresses uncertainty quantification (UQ) in deep learning models for surface ozone bias estimation, a critical need for air quality modeling and decision-making. The authors implement a U-Net architecture to predict surface ozone residuals from the MOMO-Chem model using two UQ methodologies: Monte-Carlo Dropout (Bayesian approximation) and Conformalized Quantile Regression (CQR). Testing on regional data from North America and Europe for June 2019, they find that both UQ methods consistently identify high-uncertainty regions, particularly along the U.S. East Coast, matching areas with high ground truth bias. The inclusion of land-use information via Google Earth Engine satellite products slightly improves model performance and epistemic uncertainty in North America but shows minimal impact in Europe. The study also identifies optimal and sub-optimal ground stations for bias correction based on UQ scores, with some stations showing highly variable ground truth signals that are difficult to model. The research highlights the potential of UQ in supporting targeted bias correction and calls for further investigation into temporal UQ patterns and global-scale applications.

## Method Summary
The study implements a U-Net architecture to predict surface ozone residuals (bias) from MOMO-Chem model outputs using Monte-Carlo Dropout and Conformalized Quantile Regression for uncertainty quantification. The model takes 28-channel atmospheric features (BrOx, C10H16, C2H6, C3H6, C5H8, CH2O, H2O2, HNO3, HO2, N2O5, NH3, OH, PAN, CO, COflux, NO, NO2, NOxFlux, precipitation, surface pressure, water vapor, radiation terms, temperature, cloud cover, wind direction) plus optional 23-channel land-use features from Google Earth Engine. The U-Net is trained on June 2005-2018 data and tested on June 2019, with RMSE as the primary metric and UQ measured via prediction interval length (CQR) and epistemic uncertainty (MCD). CQR uses quantile loss with heads at 0.05, 0.5, and 0.95 quantiles, while MCD uses Negative Log Likelihood loss with dropout rate 0.1 during inference.

## Key Results
- Both UQ methods consistently identify high-uncertainty regions matching high ground truth bias, particularly along the U.S. East Coast
- Addition of land-use features slightly improves North American model performance and epistemic uncertainty but increases epistemic uncertainty in Europe
- UQ scores effectively identify optimal and sub-optimal ground stations for bias correction, with some stations showing highly variable ground truth signals
- Conformalized Quantile Regression provides statistically rigorous prediction intervals that identify "hard-to-model" ground stations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A U-Net architecture can be adapted to estimate surface ozone bias (residuals) by learning spatial mappings from atmospheric chemical features to model error, provided the bias is spatially coherent.
- **Mechanism:** The model takes gridded atmospheric data (MOMO-Chem outputs) as input images and learns to predict the difference between the physics model and ground station observations. The U-Net's encoder-decoder structure captures spatial context, allowing it to identify systematic errors in the physics model.
- **Core assumption:** The bias (residual) is a deterministic function of the provided atmospheric and land-use state variables, and this function is learnable via convolutional layers.
- **Evidence anchors:**
  - [Abstract] "We implement an uncertainty-aware U-Net architecture to predict... MOMO-Chem model's surface ozone residuals (bias)..."
  - [Section 3] "We leverage a U-Net architecture... train for 200 epochs... MC-Dropout model uses Negative Log likelihood loss..."
- **Break condition:** If the bias is driven by stochastic local phenomena (e.g., random traffic emissions) not captured in the input channels, the spatial convolution will fail to generalize.

### Mechanism 2
- **Claim:** Monte-Carlo (MC) Dropout serves as a viable proxy for epistemic uncertainty in this context, signaling regions where the model lacks sufficient information or training data coverage.
- **Mechanism:** By maintaining dropout active during inference and running multiple forward passes, the model generates a distribution of predictions. The variance (epistemic uncertainty) highlights areas where the model is "confused," which correlated with high-bias regions in the study.
- **Core assumption:** The dropout approximation acts as a Bayesian posterior, and high variance equates to higher model error/ignorance.
- **Evidence anchors:**
  - [Section 3] "...utilizes dropout as a Bayesian approximation during inference to compute the prediction uncertainty."
  - [Section 4] "In North America, our MC-Dropout method shows a decrease in the averaged... epistemic uncertainty with greater input features."
- **Break condition:** If the network is over-regularized or under-regularized, the dropout variance may decouple from actual prediction error (calibration failure).

### Mechanism 3
- **Claim:** Conformalized Quantile Regression (CQR) provides statistically rigorous prediction intervals that identify "hard-to-model" ground stations, regardless of the underlying distribution shape.
- **Mechanism:** CQR splits training data into training and calibration sets. It predicts quantiles (0.05, 0.95) and adjusts the interval width using calibration scores to guarantee coverage. Larger intervals indicate higher uncertainty (aleatoric + epistemic).
- **Core assumption:** The calibration data is exchangeable with the test data; the quantile crossings are handled correctly by the architecture.
- **Evidence anchors:**
  - [Section 3] "This process is achieved by splitting our training set into halves representing a training and calibration set."
  - [Section 4] "At our max location... ground truth signal is very noisy... resulting in large prediction intervals... At our minimum location... less temporal variation."
- **Break condition:** If the test set distribution shifts significantly from the calibration set (e.g., different seasons not seen in June data), the coverage guarantees may void.

## Foundational Learning

- **Concept:** **Epistemic vs. Aleatoric Uncertainty**
  - **Why needed here:** The paper distinguishes between "reducible" uncertainty (epistemic, captured by MC Dropout) and noise (aleatoric, inherent in the data). Understanding this is required to interpret why adding land-use data reduced epistemic uncertainty in North America but not Europe.
  - **Quick check question:** Does increasing the dataset size generally reduce epistemic or aleatoric uncertainty?

- **Concept:** **Physics-Model Residuals (Bias Correction)**
  - **Why needed here:** The model does not predict ozone directly; it predicts the *error* of the MOMO-Chem model. This is a hybrid AI-physics approach where the NN learns the correction term.
  - **Quick check question:** If the physics model has a perfect prediction, what should the target residual value be for the neural network?

- **Concept:** **Quantile Regression vs. Mean Squared Error (MSE)**
  - **Why needed here:** The CQR branch uses Quantile Loss (Pinball loss) rather than standard MSE to predict intervals. This allows the model to output a range rather than a single point estimate.
  - **Quick check question:** In Quantile Regression, are you minimizing the squared distance to the mean or the weighted absolute distance to a specific percentile?

## Architecture Onboarding

- **Component map:** 2D Gridded Data (28 or 51 channels) → U-Net Encoder/Decoder → Branch A (MC Dropout): Head for Point Prediction (trained with NLL) + Variance estimation via inference passes. Input: → U-Net → Branch B (CQR): Three Heads (Q 0.05, Q 0.50, Q 0.95) trained with Quantile Loss + Conformal Calibration step.

- **Critical path:**
  1. Data rasterization (aligning MOMO-Chem + GEE + TOAR to 11.1km grid).
  2. Train U-Net (200 epochs, Adam, LR 1e-3).
  3. **CQR Specific:** Split training data → Train quantile heads → Compute calibration scores → Conformalize intervals.
  4. **MC Specific:** Run T forward passes with Dropout(p=0.1) → Calculate mean and variance.

- **Design tradeoffs:**
  - **Feature count (28 vs 51):** Adding GEE land-use data improved RMSE/Epistemic uncertainty in North America but *increased* epistemic uncertainty in Europe (potential feature confusion or correlation issues).
  - **UQ Method Selection:** MC Dropout captures model ignorance (epistemic) efficiently; CQR provides strict statistical coverage guarantees but requires a sacrificial calibration set.

- **Failure signatures:**
  - **Spatial Mismatch:** High UQ on the East Coast US (urban/complex) vs. low UQ in Southwest (simple/desert).
  - **Temporal Noise:** Failure to capture noisy ground truth signals (e.g., station 40.934, -73.125), resulting in the model simply predicting the mean with wide intervals.
  - **Feature Confusion:** Increased epistemic uncertainty in Europe when adding features, suggesting correlated or irrelevant inputs for that region.

- **First 3 experiments:**
  1. **Baseline Ablation:** Train the 28-channel model on North America without UQ heads to establish a baseline RMSE for bias prediction.
  2. **Calibration Check:** Implement the CQR calibration step specifically for the Europe test set to verify if coverage maintains the 90% target, despite the feature confusion noted in the text.
  3. **Temporal Noise Analysis:** Visualize prediction intervals for the "max UQ" station identified in the paper to confirm the model centers on the mean when signal variance is high.

## Open Questions the Paper Calls Out
None

## Limitations
- **Regional Scope**: Analysis limited to June 2019 data for North America and Europe only, with 11.1km resolution. Claims about UQ effectiveness in identifying bias regions (e.g., U.S. East Coast) are supported but require validation across seasons and global regions.
- **Feature Ambiguity**: Addition of land-use features improved North American performance but increased epistemic uncertainty in Europe, suggesting potential feature correlation issues or regional data quality differences. The mechanism behind this regional discrepancy remains unclear.
- **Temporal Static Analysis**: UQ patterns are analyzed for a single month, limiting conclusions about temporal generalization. Claims about identifying optimal ground stations based on UQ scores are promising but need temporal validation.

## Confidence
- **High**: U-Net architecture successfully learns spatial ozone bias patterns; MC Dropout and CQR methods consistently identify high-uncertainty regions matching high ground truth bias.
- **Medium**: UQ methods can identify optimal/sub-optimal stations for bias correction; land-use features have mixed regional effects.
- **Low**: Claims about temporal UQ patterns and global-scale applications require further investigation.

## Next Checks
1. **Seasonal Validation**: Test the trained models on ozone data from different seasons (winter, spring, fall) to assess temporal generalization of UQ patterns.
2. **Global Coverage Test**: Apply the methodology to additional continental regions (Asia, Africa, Australia) to validate the regional scalability of UQ effectiveness.
3. **Station-Level Temporal Analysis**: Track UQ scores for the identified "optimal" and "sub-optimal" stations across multiple years to verify if high UQ consistently correlates with noisy ground truth signals.