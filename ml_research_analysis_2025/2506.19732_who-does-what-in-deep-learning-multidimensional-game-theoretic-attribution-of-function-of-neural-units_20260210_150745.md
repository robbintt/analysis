---
ver: rpa2
title: Who Does What in Deep Learning? Multidimensional Game-Theoretic Attribution
  of Function of Neural Units
arxiv_id: '2506.19732'
source_url: https://arxiv.org/abs/2506.19732
tags:
- neural
- network
- experts
- contributions
- neurons
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Multiperturbation Shapley-value Analysis\
  \ (MSA), a game-theoretic framework to quantify how each neural unit contributes\
  \ to complex, high-dimensional outputs like images or tokens in neural networks.\
  \ Unlike existing XAI methods that focus on input features, MSA systematically perturbs\
  \ units and calculates Shapley Modes, contribution maps matching the output\u2019\
  s exact dimensionality."
---

# Who Does What in Deep Learning? Multidimensional Game-Theoretic Attribution of Function of Neural Units

## Quick Facts
- arXiv ID: 2506.19732
- Source URL: https://arxiv.org/abs/2506.19732
- Reference count: 40
- Key outcome: Introduces Multiperturbation Shapley-value Analysis (MSA) to quantify neural unit contributions to high-dimensional outputs, revealing hub-and-spoke computation patterns and inverted feature hierarchies

## Executive Summary
This paper introduces Multiperturbation Shapley-value Analysis (MSA), a game-theoretic framework to quantify how each neural unit contributes to complex, high-dimensional outputs like images or tokens in neural networks. Unlike existing XAI methods that focus on input features, MSA systematically perturbs units and calculates Shapley Modes, contribution maps matching the output's exact dimensionality. The method was tested on an MLP, the 56-billion-parameter Mixtral-8x7B LLM, and a DCGAN. Key findings include: regularization concentrates computation in a few hubs rather than distributing it uniformly; in the LLM, the first layer's third expert is critical across domains, while removing 27% of low-contributing experts only modestly reduces accuracy; and in the DCGAN, early layers handle high-level features while later layers manage low-level details, inverting the usual CNN hierarchy. MSA provides a flexible toolset for interpreting, editing, and compressing deep neural networks.

## Method Summary
MSA extends Shapley value analysis to high-dimensional outputs by creating Shapley Modes that match the exact dimensionality of neural network outputs. The framework systematically perturbs neural units and calculates their marginal contributions to the output space. Unlike traditional XAI methods that attribute importance to input features, MSA attributes importance directly to neural units and produces contribution maps at the output level. The method was applied across three architectures: a simple MLP, the large-scale Mixtral-8x7B LLM, and a DCGAN generator, demonstrating its versatility across different network types and scales.

## Key Results
- Regularization (L1/L2) concentrates computation in a few critical hubs rather than distributing it uniformly across units
- In the 56-billion-parameter Mixtral-8x7B LLM, the first layer's third expert is critical across domains, and removing 27% of low-contributing experts only modestly reduces accuracy
- DCGAN analysis reveals inverted feature hierarchies: early layers handle high-level features while later layers manage low-level details, contrary to typical CNN behavior

## Why This Works (Mechanism)
MSA works by applying game-theoretic Shapley values to neural network interpretation, but crucially extends this to high-dimensional outputs rather than scalar predictions. By systematically perturbing neural units and measuring their marginal contributions to the full output space (images, tokens, etc.), MSA creates attribution maps that directly correspond to the output dimensionality. This approach captures the multidimensional nature of modern neural network outputs, where traditional scalar attribution methods would lose critical spatial or sequential information. The Shapley framework ensures that contributions are fairly attributed based on marginal impact across all possible coalitions of neural units.

## Foundational Learning
- Shapley value theory: Game-theoretic method for fairly distributing credit among players; needed to understand how MSA attributes unit contributions; quick check: verify understanding of Shapley axioms (efficiency, symmetry, dummy, additivity)
- Multiperturbation analysis: Extending perturbation methods to high-dimensional spaces; needed to grasp how MSA handles images/tokens instead of scalars; quick check: understand difference between single-point and multi-point perturbations
- Neural unit attribution: Methods for determining which neurons contribute most to model behavior; needed context for MSA's approach; quick check: compare MSA to existing methods like Integrated Gradients
- DCGAN architecture: Deep Convolutional Generative Adversarial Network structure; needed to interpret the inverted hierarchy findings; quick check: understand DCGAN's generator-discriminator setup
- Expert parallelism in MoE models: Mixture of Experts architecture with multiple specialized subnetworks; needed for Mixtral LLM analysis; quick check: understand how experts are selected and combined

## Architecture Onboarding

**Component Map:** Input -> Neural Units (perturbed) -> Shapley Mode Attribution Maps -> Output Analysis

**Critical Path:** Perturbation of neural units → Output space measurement → Shapley value calculation → Attribution map generation

**Design Tradeoffs:** MSA prioritizes interpretability and high-dimensional attribution over computational efficiency compared to some existing methods; trades traditional input-feature attribution for unit-level attribution with output-matching dimensionality

**Failure Signatures:** Poor attribution quality may indicate insufficient perturbation coverage, numerical instability in Shapley calculations, or failure to capture complex interactions between units

**First Experiments:** 1) Apply MSA to a simple MLP with known unit importance patterns to validate basic functionality; 2) Compare MSA attribution maps against ground truth for synthetic datasets; 3) Test MSA on a small CNN to verify computational feasibility before scaling

## Open Questions the Paper Calls Out
None

## Limitations
- Computational efficiency claims lack comprehensive benchmarks against other efficient attribution methods like Integrated Gradients or Fast Fourier Correlation
- Regularization findings need replication across diverse architectures and tasks to verify generalizability
- DCGAN inverted hierarchy finding requires validation on multiple generative models and datasets to rule out architectural artifacts

## Confidence
- MSA framework validity and mathematical foundation: High
- Computational efficiency improvements: Medium (limited comparative data)
- Regularization effects on hub formation: Medium (single architecture study)
- LLM expert importance findings: Medium (specific to Mixtral architecture)
- DCGAN feature hierarchy findings: Low (novel and potentially architecture-specific)

## Next Checks
1. Benchmark MSA against other efficient attribution methods (Integrated Gradients, Fast Fourier Correlation) on identical models and tasks, measuring both computational time and attribution quality
2. Test regularization effects across multiple architectures (CNNs, Transformers, MLPs) and tasks to verify hub-and-spoke patterns are generalizable
3. Apply MSA to multiple generative models (StyleGAN, VAEs, diffusion models) to validate whether DCGAN's inverted hierarchy finding extends beyond a single architecture