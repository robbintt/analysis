---
ver: rpa2
title: 'Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing'
arxiv_id: '2510.01283'
source_url: https://arxiv.org/abs/2510.01283
tags:
- research
- deep
- tools
- search
- does
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces an evaluation sheet to assess Deep Research
  tools, which are LLM-powered agents capable of web browsing, information extraction,
  and generating multi-page reports. The authors selected academic survey writing
  as a use case, evaluating OpenAI's Deep Research and Google's Deep Research on three
  regional NLP survey papers (Ethiopia, Nigeria, Kenya).
---

# Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing

## Quick Facts
- **arXiv ID**: 2510.01283
- **Source URL**: https://arxiv.org/abs/2510.01283
- **Reference count**: 11
- **Primary result**: Deep Research tools show below-average performance in identifying impactful research and include unreliable sources, requiring standardized evaluation benchmarks for academic survey generation.

## Executive Summary
This paper introduces a structured evaluation sheet to assess Deep Research tools—LLM-powered agents that browse the web, extract information, and generate multi-page reports. The authors evaluate OpenAI's Deep Research and Google's Deep Research using academic survey writing as a use case, focusing on three regional NLP survey papers (Ethiopia, Nigeria, Kenya). The evaluation reveals that while these tools can summarize information, they struggle with identifying impactful research, often include unreliable sources, and produce outputs not fully aligned with Google search results. The study highlights the need for carefully crafted evaluation standards and benchmarking datasets to ensure accuracy and representativeness in research outputs.

## Method Summary
The authors developed a six-pillar evaluation sheet to assess Deep Research tools on their ability to generate academic survey reports. They used three regional NLP survey papers (Ethiopia, Nigeria, Kenya) as ground truth and generated equivalent reports using both OpenAI's Deep Research and Google's Deep Research with identical prompts. Three domain experts evaluated each report using 29 questions across six pillars (LLM surveying capability, hallucination detection, source correctness, information validity, information latestness, and Google search alignment) on a 0-5 Likert scale. The aggregated scores revealed systematic weaknesses in source selection, coverage of impactful research, and alignment with standard search results.

## Key Results
- Deep Research tools show below-average performance in identifying impactful research works in their reports
- Both tools frequently include unreliable sources alongside academic papers in their outputs
- Generated reports are not fully aligned with Google search results, suggesting divergence in retrieval approaches
- The evaluation sheet methodology effectively identifies failure modes but requires standardized, automated metrics for scalability

## Why This Works (Mechanism)

### Mechanism 1: Iterative Search-Read-Reason Cycle
Deep Research tools produce comprehensive reports by cycling through search, extraction, and reasoning phases until a quality threshold is met. The system decomposes complex queries into sub-questions, retrieves sources, extracts information, cross-references across multiple documents, and synthesizes findings into structured output. This loop continues until the system determines the answer is "satisfactory." Core assumption: Iterative refinement improves output quality and coverage compared to single-pass retrieval.

### Mechanism 2: Multi-Pillar Evaluation Sheet as Quality Signal
A structured evaluation sheet with six pillars enables systematic identification of failure modes in Deep Research outputs. Each pillar operationalizes a specific quality dimension through concrete questions rated on a Likert scale. Aggregated scores reveal systematic weaknesses. Core assumption: Human expert judgment on these pillars correlates with actual output reliability for downstream tasks.

### Mechanism 3: Ground-Truth Comparison Against Expert-Curated Surveys
Comparing Deep Research outputs to existing expert-authored survey papers reveals gaps in research coverage and source selection. Published regional NLP surveys (Ethiopia, Nigeria, Kenya) serve as reference standards. Generate equivalent reports using Deep Research tools with identical scope. Evaluate whether the tool identifies the same impactful papers, datasets, and research trends. Core assumption: Original survey papers represent comprehensive, unbiased coverage of the research landscape.

## Foundational Learning

- **Concept: Agentic LLM Architecture**
  - Why needed here: Deep Research is not a single model call—it is a system orchestrating search APIs, document readers, and reasoning modules. Understanding this decomposition is essential for debugging failures.
  - Quick check question: Can you diagram where the "search" component ends and the "reasoning" component begins in a Deep Research pipeline?

- **Concept: Hallucination vs. Error Distinction**
  - Why needed here: The evaluation sheet treats hallucinations (plausible but unverifiable claims) differently from obvious errors. Detection strategies differ for each.
  - Quick check question: Given a generated citation that looks correct but doesn't exist, would you classify this as hallucination or error? Why?

- **Concept: Source Credibility Hierarchies**
  - Why needed here: Deep Research tools conflate peer-reviewed papers with social media posts. Evaluators must understand why source provenance affects output reliability.
  - Quick check question: If a Deep Research report cites a Reddit thread and a peer-reviewed paper equally, what failure mode does this indicate?

## Architecture Onboarding

- **Component map**: Query Parser → Search Orchestrator → Document Retriever → Information Extractor → Reasoning/Synthesis Module → Report Generator → Citation Formatter → Evaluation Layer (Six-pillar rubric applied post-hoc by human evaluators using Likert scale)

- **Critical path**:
  1. Define evaluation scope and adapt pillar questions to domain
  2. Select ground-truth references (e.g., expert-authored surveys)
  3. Generate Deep Research outputs with identical prompts
  4. Have domain experts rate each pillar question
  5. Aggregate scores to identify systematic weaknesses

- **Design tradeoffs**:
  - Breadth vs. depth in pillar questions: More questions increase granularity but raise evaluator burden
  - General vs. domain-specific pillars: The provided template is NLP-focused; other domains require question adaptation
  - Single vs. multiple evaluators: Three evaluators used in study; inter-rater reliability not reported

- **Failure signatures**:
  - Low "Source Correctness" scores with high "Information Validity" suggests tools extract correct claims from wrong sources
  - High hallucination scores combined with social media citations indicates source filtering failure
  - Large gap between Google search results and Deep Research outputs suggests retrieval divergence

- **First 3 experiments**:
  1. Replicate the three-country evaluation with a different Deep Research tool (e.g., Perplexity) to establish baseline comparability
  2. Ablate the prompt: remove country-specific constraints and measure whether coverage improves or degrades
  3. Extend pillar ratings to include inter-rater reliability calculation (e.g., Cohen's kappa) to validate evaluation consistency

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can Deep Research tools be algorithmically refined to prioritize academic impact and citation quality over simple search engine popularity?
- **Basis in paper**: The authors note that tools struggle to identify impactful research, often prioritizing sources with high search volume rather than academic significance.
- **Why unresolved**: The current underlying architecture appears to rely on search engine heuristics which favor SEO-optimized content over niche, high-value academic contributions.
- **What evidence would resolve it**: A comparative study showing Deep Research outputs successfully retrieving and prioritizing low-traffic, high-citation papers at a rate comparable to human domain experts.

### Open Question 2
- **Question**: To what extent does the "reasoning" capability in Deep Research mitigate the propagation of misinformation compared to standard search engine results?
- **Basis in paper**: Section 3 (Pillar 6) and Section 4.2 highlight the need to measure if these tools effectively reduce misinformation compared to open-web search, with current findings showing outputs are "not fully aligned" with Google results.
- **Why unresolved**: While the paper notes hallucination rates are "not significantly high," it remains unclear if the synthesis of multiple sources (including social media) increases or decreases factual accuracy relative to manual search.
- **What evidence would resolve it**: A quantitative benchmark measuring factual consistency and error rates between Deep Research reports and ground-truth data sets versus standard search results.

### Open Question 3
- **Question**: How can the subjective Likert-scale evaluation method be replaced by standardized, automated metrics to benchmark Deep Research tools at scale?
- **Basis in paper**: The authors rely on manual human review using a Likert scale but conclude by emphasizing the "need to have carefully crafted evaluation standards" and the creation of "benchmarking datasets."
- **Why unresolved**: Human evaluation is resource-intensive and subjective; an automated, standardized metric for "information latestness" or "source correctness" does not yet exist for long-form reports.
- **What evidence would resolve it**: The development and validation of an automated evaluation framework that correlates strongly with human judgments on the six pillars defined in the paper.

## Limitations
- The evaluation sheet methodology relies on subjective human judgment without reported inter-rater reliability measures
- Specific prompts fed to each Deep Research tool are not fully specified beyond the template, creating reproducibility challenges
- The three regional NLP survey papers may themselves have incomplete coverage, potentially biasing the ground truth comparison

## Confidence
- **High Confidence**: The identification of specific failure modes (source credibility issues, hallucination patterns, coverage gaps) based on the six-pillar evaluation framework
- **Medium Confidence**: The general claim that Deep Research tools are not yet fully reliable for academic survey generation, as this is supported by the evaluation results but limited to three case studies
- **Low Confidence**: The assertion that Deep Research tools "struggle with identifying impactful research" without more precise metrics for what constitutes "impactful" beyond the reference surveys

## Next Checks
1. Replicate the evaluation with additional Deep Research tools (e.g., Perplexity, Anthropic) to establish whether failure patterns are tool-specific or systemic
2. Calculate inter-rater reliability (Cohen's kappa or similar) for pillar ratings to validate the evaluation sheet's consistency
3. Test the evaluation sheet on a different domain (e.g., biomedical literature) to assess generalizability beyond NLP surveys