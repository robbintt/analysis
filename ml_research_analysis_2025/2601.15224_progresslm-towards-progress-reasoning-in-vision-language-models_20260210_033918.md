---
ver: rpa2
title: 'PROGRESSLM: Towards Progress Reasoning in Vision-Language Models'
arxiv_id: '2601.15224'
source_url: https://arxiv.org/abs/2601.15224
tags:
- progress
- reasoning
- task
- score
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PROGRESSLM addresses the challenge of estimating task progress
  from single observations in vision-language models (VLMs). The paper introduces
  PROGRESS-BENCH, a benchmark with over 3,000 instances evaluating progress reasoning
  across controlled variations in demonstration modality, viewpoint, and answerability.
---

# PROGRESSLM: Towards Progress Reasoning in Vision-Language Models

## Quick Facts
- **arXiv ID:** 2601.15224
- **Source URL:** https://arxiv.org/abs/2601.15224
- **Reference count:** 40
- **Primary result:** PROGRESSLM-3B achieves consistent improvements in progress estimation over base VLMs, addressing challenges in single-observation progress reasoning.

## Executive Summary
PROGRESSLM introduces a new benchmark and method for estimating task progress from single observations in vision-language models. The paper identifies that existing VLMs struggle with progress estimation due to sensitivity to demonstration modality and viewpoint, poor handling of unanswerable cases, and tendency toward collapsed predictions. To address these limitations, the authors propose a human-inspired two-stage approach combining episodic retrieval and mental simulation, implemented through both training-free prompting and a training-based method. The resulting PROGRESSLM-3B model demonstrates robust performance across controlled variations in the PROGRESS-BENCH benchmark.

## Method Summary
PROGRESSLM employs a two-stage reasoning paradigm that first retrieves a relevant demonstration step and then estimates progress relative to that anchor. The training pipeline uses supervised fine-tuning with a guided reasoning completion strategy, followed by reinforcement learning with decomposed rewards. The method is trained on PROGRESSLM-45K, a dataset constructed using a larger model to generate reasoning traces aligned with ground-truth progress values. The approach is designed to prevent score collapse and improve handling of unanswerable cases by enforcing coupling between retrieval and estimation steps.

## Key Results
- PROGRESSLM-3B achieves consistent improvements over base models across all benchmark dimensions
- The training-based approach significantly outperforms direct prediction and training-free prompting, especially for smaller models
- PROGRESSLM demonstrates robust performance across demonstration modalities, viewpoints, and answerability conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing progress estimation into explicit episodic retrieval and mental simulation prevents "collapsed" predictions by grounding the score in a retrieved temporal anchor.
- **Mechanism:** Instead of regressing directly from an observation to a scalar progress value (which often leads to heuristic clustering at 0%, 50%, or 100%), the model is forced to first identify a reference step (`<ref>`) from the demonstration. This acts as a constraint: the final score (`<score>`) must be a local interpolation relative to that specific anchor, reducing the output search space and enforcing temporal consistency.
- **Core assumption:** The model possesses sufficient visual grounding to identify semantically similar states across viewpoints or modalities.
- **Evidence anchors:**
  - [abstract] Mentions a "human-inspired two-stage progress reasoning paradigm."
  - [section 5.1] Identifies "Single-peak collapse" and "Multi-peak clustering" as failure modes in direct prediction, which are resolved by the structured approach (PROGRESSLM exhibits "Smooth continuous distributions").
  - [corpus] General support exists for "multi-step reasoning" reducing error accumulation (e.g., *A Survey on Feedback-based Multi-step Reasoning*), but specific "episodic retrieval" for progress estimation is unique to this paper.

### Mechanism 2
- **Claim:** Cold-start Supervised Fine-Tuning (SFT) via "guided reasoning completion" effectively bootstraps the reasoning capability by teaching the model to justify *why* a specific step is relevant, rather than just *what* the step is.
- **Mechanism:** The training data (PROGRESSLM-25K-COT) is constructed not by free generation, but by providing the ground-truth `<ref>` (reference step) and `<score>` (progress) and forcing the model to generate the `<ref_think>` and `<score_think>`. This reverse-engineering of the thought process teaches the model the causal link between visual evidence and the progress label, reducing hallucination.
- **Core assumption:** The ground-truth progress labels and step alignments in the training data are accurate and representative of smooth task dynamics.
- **Evidence anchors:**
  - [section 3.2] Describes the "guided reasoning completion strategy" where the model generates missing reasoning fields given ground-truth anchors.
  - [section A.3] Details the construction of CoT data where reasoning must align with correct answers to avoid noise.
  - [corpus] No direct corpus evidence for this specific "ground-truth guided" generation method was found; it appears novel to this architecture.

### Mechanism 3
- **Claim:** Reinforcement Learning (RL) with decomposed rewards is required to enforce the coupling between the retrieved anchor and the estimated progress, ensuring the model doesn't ignore its own reasoning steps.
- **Mechanism:** The RL objective uses a weighted combination of rewards: $R_{format}$, $R_{ref}$ (accurate retrieval), and $R_{score}$ (precise estimation). By rewarding $R_{ref}$ specifically, the model learns that a correct final score is conditional on a correct initial retrieval. This creates a dependency ("coupling") where the mental simulation relies on the episodic retrieval.
- **Core assumption:** The weighting of rewards ($\alpha: \beta: \gamma = 1:6:3$) correctly balances structural adherence against semantic accuracy.
- **Evidence anchors:**
  - [section 5.3] Shows "clear diagonal concentration" in the coupling heatmap for the RL model, proving that the retrieved anchor directly constrains the score.
  - [section 3.2] Defines the loss function $L_{RL}$ involving $\alpha, \beta, \gamma$ rewards.
  - [corpus] Corpus references (e.g., *ChemATP*) suggest training-free methods struggle with complex priors, implying the necessity of the training-based RL approach here.

## Foundational Learning

- **Concept: Episodic Retrieval vs. Direct Regression**
  - **Why needed here:** Standard VLMs act as regressors (Image $\to$ Score), which leads to coarse heuristics. Understanding retrieval means shifting from "recognizing" progress to "locating" progress within a known timeline.
  - **Quick check question:** Can you distinguish between asking "How finished is this?" (regression) vs. "Which step in the manual does this look like?" (retrieval)?

- **Concept: Mental Simulation / State Transition Modeling**
  - **Why needed here:** Once an anchor is retrieved (e.g., "Step 3: 40%"), the model must simulate the delta between Step 3 and the current image to output 46%. This requires reasoning about object dynamics (e.g., "plate is slightly higher") rather than static features.
  - **Quick check question:** Given a snapshot of a chess game, can you reason about the move that *just* happened without seeing the previous frame?

- **Concept: Unanswerability and Uncertainty Quantification**
  - **Why needed here:** The PROGRESS-BENCH explicitly includes "unanswerable" cases where the observation mismatches the demonstration. A robust system must learn to output "N/A" (abstain) rather than hallucinating a score.
  - **Quick check question:** If a user asks "How far along is the apple slicing?" but shows a video of a car being washed, should your confidence be low, or should your output be "Invalid Task"?

## Architecture Onboarding

- **Component map:**
  - Vision-Language Model (e.g., Qwen2.5-VL-3B) -> Input Interface (Demo Sequence + Observation) -> Reasoning Schema (XML-like tags) -> CoT Generator (Qwen-72B + Ground Truth) -> SFT Stage (LoRA fine-tuning) -> RL Stage (GRPO with decomposed rewards)

- **Critical path:**
  The performance gain hinges on the **RL stage coupling**. SFT alone (PROGRESSLM-3B-SFT) improves performance, but the RL stage (PROGRESSLM-3B-RL) is required to achieve "consistent improvements" and robust handling of unanswerable cases (Section 4.3) by optimizing the interaction between the retrieval and estimation rewards.

- **Design tradeoffs:**
  - **Training-free vs. Training-based:** Training-free prompting helps large models (GPT-5) but often harms small models. Training-based is necessary for small models (3B) to compete with giants.
  - **Modality:** Text-based demonstrations require implicit state accumulation (reasoning about unobserved intermediate states), whereas vision-based demos provide explicit state info. Text is harder but more robust to visual domain shifts.

- **Failure signatures:**
  - **Score Collapse:** Predictions cluster at 0%, 50%, 100% (indicates lack of fine-grained reasoning).
  - **High AFRR (Answerable False Rejection Rate):** Model is overly conservative, predicting N/A for valid inputs (indicates poor uncertainty calibration).
  - **Uncoupled Reasoning:** The `<ref>` index and the `<score>` imply different stages of the task (check the heatmap for diagonal concentration).

- **First 3 experiments:**
  1. **Sanity Check (Distribution Analysis):** Run the base model vs. PROGRESSLM-3B on a sample task and plot histograms of predicted scores. Look for "spiky" distributions in the base model vs. "smooth" distributions in the fine-tuned model.
  2. **Ablation on Coupling:** Evaluate the RL model with the $R_{ref}$ reward set to 0. Verify if the "diagonal concentration" in Section 5.3 disappears, proving the necessity of the retrieval reward.
  3. **Modality Stress Test:** Evaluate performance on Text-based demos vs. Vision-based demos. Confirm that the "modality gap" (Section 5.4) decreases with the training-based approach compared to direct prediction.

## Open Questions the Paper Calls Out
None

## Limitations

- PROGRESS-BENCH contains only ~3,000 instances across 5 tasks, raising questions about generalization to more complex real-world scenarios
- Training-based approach requires computationally expensive generation of ~25K CoT samples using a 72B parameter model
- Performance gains, while consistent, are sometimes modest in absolute terms (e.g., 65.6% to 75.1% on Answerable Accuracy)

## Confidence

**High Confidence:** The core observation that most VLMs struggle with progress estimation is well-supported by systematic experimentation across 14 models. The failure modes (score collapse, modality sensitivity, poor handling of unanswerable cases) are clearly demonstrated and reproducible.

**Medium Confidence:** The two-stage reasoning approach shows consistent improvements, but the magnitude varies by task and model size. The coupling mechanism identified through RL is compelling, but the specific reward weighting (1:6:3) may not be optimal across all settings.

**Low Confidence:** The paper's claims about human-inspired reasoning are largely analogical rather than empirically validated against human performance baselines. The superiority of training-based methods over prompting is demonstrated but not extensively explored across model families.

## Next Checks

1. **Generalization Test:** Evaluate PROGRESSLM on a held-out task (e.g., a new cooking recipe or furniture assembly) not seen during training or benchmark construction to assess true generalization.

2. **Scaling Analysis:** Systematically vary the training data size (e.g., 5K, 10K, 50K instances) to determine whether the 25K requirement is necessary or whether similar performance can be achieved with less data.

3. **Human Baseline Comparison:** Collect human progress estimates on the same tasks and compare against model performance to validate whether the "human-inspired" framing reflects actual human reasoning patterns.