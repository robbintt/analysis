---
ver: rpa2
title: 'NeoQA: Evidence-based Question Answering with Generated News Events'
arxiv_id: '2505.05949'
source_url: https://arxiv.org/abs/2505.05949
tags:
- question
- outline
- answer
- event
- entities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce NEOQA, a benchmark for evaluating evidence-based question
  answering in large language models (LLMs). Unlike existing RAG datasets that become
  obsolete as models internalize recent information, NEOQA uses fictional news events
  and entities to prevent interference from pretraining knowledge.
---

# NeoQA: Evidence-based Question Answering with Generated News Events

## Quick Facts
- arXiv ID: 2505.05949
- Source URL: https://arxiv.org/abs/2505.05949
- Authors: Max Glockner; Xiang Jiang; Leonardo F. R. Ribeiro; Iryna Gurevych; Markus Dreyer
- Reference count: 40
- Primary result: LLMs achieve only 53.2 ADTScore on evidence-based QA with fictional news events, struggling to distinguish sufficient from insufficient evidence

## Executive Summary
NeoQA is a benchmark for evaluating evidence-based question answering in large language models (LLMs) that uses fictional news events and entities to prevent interference from pretraining knowledge. The dataset includes multi-hop, time-span, false premise, and uncertain specificity questions, paired with news articles as evidence. Experiments across seven models show that LLMs struggle to distinguish sufficient from insufficient evidence and frequently rely on shortcut reasoning, especially when key information is missing. Larger models perform better overall, but even the best (Qwen2.5 32B) achieves only 53.2 ADTScore, underscoring key limitations in evidence-based reasoning.

## Method Summary
NeoQA generates fictional timelines and knowledge bases using GPT-4o to create 15 parallel worlds with 10 sequential events each, featuring 393 fictional named entities verified against Wikipedia. The system produces 1,800 news articles (4 profiles: progressive, conservative, objective, sensational) at ~356 tokens each, along with 4 question types (multi-hop, time-span, false premise, uncertain specificity) grounded to atomic facts. Models are evaluated using ADTScore, a harmonic mean of accuracy on answerable and unanswerable instances, with zero-shot inference and temperature t=0.0 across Qwen2.5 and Phi3 variants.

## Key Results
- Qwen2.5 32B achieved the highest ADTScore of 53.2, with 79.4% accuracy on multi-hop questions but only 38.6% false premise detection
- Models struggle most when bridge entity information is missing but answer information is present, with 69.7%-90.7% of errors answering as if evidence were sufficient
- Performance degrades sharply with irrelevant documents, showing the steepest drop within the first 20 added documents
- Larger models consistently outperform smaller ones, but even the best model shows significant limitations in evidence-based reasoning

## Why This Works (Mechanism)

### Mechanism 1: Fictional Timeline Generation for Knowledge Isolation
- Claim: Generating entirely fictional news events with fictional named entities prevents pretraining knowledge interference
- Mechanism: Creates parallel worlds using GPT-4o to generate sequential event timelines (10 events each), complete with fictional named entities verified against Wikipedia to ensure no real-world overlap
- Core assumption: Models cannot answer questions correctly about fictional entities without relying on provided evidence
- Evidence anchors: "To construct NeoQA, we generated timelines and knowledge bases of fictional news events and entities along with news articles and Q&A pairs to prevent LLMs from leveraging pretraining knowledge"

### Mechanism 2: Controlled Evidence Sufficiency Testing
- Claim: By systematically manipulating which evidence documents are provided, the benchmark distinguishes genuine evidence-based reasoning from shortcut reasoning
- Mechanism: Each question is linked to specific "outline items" (atomic facts) required for answering, creating instances by providing all required evidence, omitting specific facts, or adding false premises/uncertain specificity
- Core assumption: Models that truly reason from evidence will show different behavior patterns when evidence is missing vs. present
- Evidence anchors: "Models found it easiest to deflect when all relevant information was absent but struggled most when the answer was present while the bridge entity was missing"

### Mechanism 3: ADTScore for Balanced Evaluation
- Claim: Harmonic mean of answerable accuracy and deflection accuracy prevents models from gaming the metric
- Mechanism: ADTScore = 2 × acc_a × acc_u / (acc_a + acc_u) where acc_a is accuracy on answerable questions and acc_u is accuracy on unanswerable questions (correctly deflecting)
- Core assumption: Trustworthy evidence-based reasoning requires both capabilities equally
- Evidence anchors: "ADTScore is robust to class imbalance of answerable and non-answerable instances and achieves the maximum performance when model accuracy is balanced across both subsets"

## Foundational Learning

- **Concept: Multi-hop Reasoning**
  - Why needed here: Questions require combining information from multiple evidence documents, often using a "bridge entity" to connect facts across events
  - Quick check question: Given two documents, can you identify what entity connects them and what unique information each provides about that entity?

- **Concept: Evidence Sufficiency Assessment**
  - Why needed here: Models must determine not just whether evidence exists, but whether it's sufficient to answer with certainty—distinguishing missing information from merely scattered information
  - Quick check question: If a question asks "What did X say about Y?" and you have documents showing X made a statement about Z that's related to Y, is that sufficient evidence?

- **Concept: Shortcut Reasoning Detection**
  - Why needed here: The paper shows models frequently answer correctly even when key evidence (bridge entity information) is missing, suggesting they're using statistical shortcuts rather than genuine reasoning
  - Quick check question: If a model answers a question correctly when 50% of required evidence is missing, is it reasoning or guessing?

## Architecture Onboarding

- **Component map:**
  Timeline Generator (GPT-4o) -> Entity KB Manager -> Article Generator -> Question Generator -> Instance Assembler -> Evaluator

- **Critical path:**
  1. Event outline generation → Named entity verification → KB entry creation
  2. Question generation with outline item selection → Answer + distractor creation
  3. News article generation → NLI verification of outline item coverage
  4. Instance assembly → Controlled evidence manipulation
  5. Model evaluation → ADTScore computation

- **Design tradeoffs:**
  - Synthetic data may have token distribution biases; GPT-4 Turbo achieved 53.6% accuracy without evidence on multi-hop questions
  - Multiple-choice format avoids expensive judge LLMs but may not reflect open-ended generation quality
  - Pre-selected evidence bypasses retrieval evaluation; real-world RAG includes retrieval noise
  - Fictional entities ensure no pretraining contamination but may not reflect real-world entity complexity

- **Failure signatures:**
  - Models answering as-if-sufficient when bridge entity info missing: 69.7%-90.7% of errors on bridge-omitted instances
  - Models selecting distractors when answer info missing: 52.9%-77.9% of errors on answer-omitted instances
  - Performance degradation with irrelevant documents: sharpest drop within first 20 added documents
  - Negative correlation between answerable accuracy and deflection accuracy (φ = -0.114 to -0.374, p < 0.001)

- **First 3 experiments:**
  1. Replicate baseline: Run Qwen2.5 7B/14B/32B and Phi3 variants on the test set with selected prompts, verify ADTScore ranges match reported (12.9-53.2)
  2. Evidence ablation: Test same questions with varying irrelevant document counts (0, 20, 40, 60, 80) to measure context robustness degradation curves
  3. Error pattern analysis: On bridge-entity-omitted instances, classify model responses into three categories (correct deflection, as-if-sufficient answer, distractor selection) to identify shortcut reasoning patterns

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's reliance on synthetic data generated by GPT-4o may introduce token distribution biases that affect model performance in ways not reflective of real-world scenarios
- The multiple-choice format, while practical, may not fully capture open-ended generation quality and could underestimate models' ability to handle nuanced evidence-based reasoning
- By pre-selecting evidence documents, the benchmark bypasses the retrieval component of real-world RAG systems, missing potential failure modes related to evidence retrieval noise and relevance ranking
- Fictional entities, while preventing pretraining contamination, may not reflect the complexity and ambiguity of real-world entities, potentially overestimating models' ability to handle messy, overlapping real-world knowledge

## Confidence
- **High Confidence:** The core finding that models struggle with evidence sufficiency assessment (particularly when bridge entity information is missing) is well-supported by the error analysis showing 69.7%-90.7% of errors follow the "as-if-sufficient" pattern
- **Medium Confidence:** The claim that fictional timeline generation effectively isolates models from pretraining knowledge is supported but has caveats - the baseline accuracy without evidence (53.6%) suggests some residual leakage or pattern learning
- **Low Confidence:** The assertion that larger models consistently perform better across all evidence scenarios needs qualification, as the data shows models are particularly poor at deflection when evidence is missing, regardless of size

## Next Checks
1. **Real-world contamination test:** Run a subset of NeoQA questions through a model pre-trained on a completely different domain (e.g., biomedical literature) to verify that performance improvements stem from evidence reasoning rather than domain knowledge transfer
2. **Open-ended evaluation:** Convert 100 multi-hop questions to open-ended format and evaluate using human judges or GPT-4o as an evaluator to assess whether multiple-choice constraints underestimate model reasoning capabilities
3. **Retrieval integration test:** Implement a simple BM25 retrieval system to select evidence documents from the NeoQA corpus, then evaluate model performance with automatically retrieved vs. pre-selected evidence to quantify the impact of retrieval noise on evidence-based reasoning