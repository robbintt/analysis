---
ver: rpa2
title: 'Beyond Black-Box Predictions: Identifying Marginal Feature Effects in Tabular
  Transformer Networks'
arxiv_id: '2504.08712'
source_url: https://arxiv.org/abs/2504.08712
tags:
- feature
- effects
- marginal
- tabular
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces NAMformer, a novel architecture that combines
  tabular transformer networks with interpretable marginal feature effects. The key
  idea is to use uncontextualized embeddings from the transformer as inputs to shallow
  neural networks, which learn individual feature effects while preserving the transformer's
  predictive power.
---

# Beyond Black-Box Predictions: Identifying Marginal Feature Effects in Tabular Transformer Networks

## Quick Facts
- arXiv ID: 2504.08712
- Source URL: https://arxiv.org/abs/2504.08712
- Authors: Anton Thielmann; Arik Reuter; Benjamin Saefken
- Reference count: 40
- Key outcome: Introduces NAMformer architecture that matches black-box transformer performance while enabling interpretable marginal feature effects through uncontextualized embeddings and theoretical identifiability guarantees

## Executive Summary
This paper addresses the challenge of making tabular transformer networks interpretable by introducing NAMformer, which learns marginal feature effects while preserving predictive performance. The key innovation is using uncontextualized embeddings from the transformer as inputs to shallow neural networks that learn individual feature effects. The authors provide theoretical justification through a dropout-based identifiability framework and demonstrate that this approach accurately recovers true marginal effects on simulated data while matching black-box performance on real-world benchmarks.

## Method Summary
NAMformer combines a standard FT-Transformer backbone with parallel interpretable shallow networks. Each feature's uncontextualized embedding (before self-attention) is fed to a single-layer network that learns its marginal effect, while the full transformer captures interactions. The final prediction sums the marginal effects with the transformer's output. Target-aware encodings (periodic linear encoding or one-hot via decision trees) enable shallow networks to capture complex nonlinearities with minimal parameter overhead. Feature dropout during training provides theoretical identifiability guarantees by forcing independent network performance.

## Key Results
- On simulated data with complex interactions, NAMformer recovers true marginal effects with R² = 0.81-0.92, outperforming GAMs, neural additive models, and explainable boosting machines
- On 8 real-world datasets, NAMformer matches FT-Transformer performance (within standard deviation) while providing interpretable marginal effects
- Among interpretable models, NAMformer achieves the best average rank in predictive performance across all benchmark datasets
- The architecture adds only J × e parameters compared to standard transformers, where J is feature count and e is embedding dimension

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncontextualized embeddings preserve near-perfect feature identity, enabling interpretable marginal effect extraction
- Mechanism: Before self-attention processes cross-feature interactions, each feature's embedding is a high-fidelity representation of the original input. Decision trees trained on these embeddings recover true feature values with R² ≥ 0.96
- Core assumption: Tabular transformers without positional encoding maintain token identifiability transferable from NLP transformer theory
- Evidence anchors: Page 5 reports R² ≥ 0.96 for different embedding sizes; related work on feature interaction learning does not address embedding identifiability

### Mechanism 2
- Claim: The additive constraint preserves black-box predictive performance while enabling marginal interpretability
- Mechanism: The model splits computation into two parallel paths: shallow networks process uncontextualized embeddings for each feature independently, while the full transformer stack processes all embeddings for interaction modeling. Final prediction sums both components
- Core assumption: Interaction effects captured by the transformer do not require marginal effect networks to also model interactions for predictive optimality
- Evidence anchors: Table 2 shows NAMformer and FT-Transformer with identical hyperparameters show overlapping performance; Graph-based Tabular Deep Learning supports explicit interaction modeling but not additive decomposition

### Mechanism 3
- Claim: Shape function dropout provides theoretical identifiability guarantees for marginal effects under broad loss classes
- Mechanism: During training, randomly drop individual feature networks (and the interaction network) from the sum. This forces each network to predict well independently. Equations 6-9 derive that under dropout, the expected error between learned marginal effects and true conditional expectation is bounded
- Core assumption: Risk is approximately uniformly distributed across dropout configurations; convex loss enables Jensen's inequality application
- Evidence anchors: Pages 6-7 provide mathematical derivation with explicit MSE case in Appendix A; Table 1 shows ablation study with R² = 0.81-0.92 across complexity levels

### Mechanism 4
- Claim: Target-aware encoding enables shallow single-layer networks to capture complex nonlinear marginal effects
- Mechanism: Instead of feeding raw continuous values to neural networks, features are pre-encoded using decision-tree-derived bins (periodic linear encoding or target-aware one-hot). This transforms arbitrary marginal shapes into piecewise-linear representations learnable by single-layer networks
- Core assumption: The bin boundaries from decision trees adequately capture the marginal effect structure without requiring deep networks
- Evidence anchors: Table 1 shows NAMformer variants with target-aware encodings substantially outperform standardized features; Page 4 explains parameter efficiency (J × e overhead)

## Foundational Learning

- **Concept: Generalized Additive Models (GAMs)**
  - Why needed here: NAMformer extends GAM theory by replacing spline-based shape functions with neural networks on transformer embeddings while preserving additive structure
  - Quick check question: Can you explain why GAMs guarantee interpretability of marginal effects but typically underperform black-box models on complex data?

- **Concept: Transformer self-attention as feature interaction modeling**
  - Why needed here: In tabular transformers (unlike NLP), self-attention captures cross-feature dependencies rather than sequence context. Understanding this is critical to seeing why uncontextualized embeddings remain "pure" representations
  - Quick check question: Why does the absence of positional encoding in tabular transformers make token identifiability more directly applicable?

- **Concept: Identifiability vs. interpretability**
  - Why needed here: Interpretability (human-understandable outputs) differs from identifiability (unique parameter recovery). The paper claims identifiability via dropout; limitations acknowledge that statistical inference remains unsolved
  - Quick check question: If a model produces interpretable marginal plots, does that guarantee the learned effects match the true data-generating process? Why or why not?

## Architecture Onboarding

- **Component map:** Feature Encoder → Embedding Layer → Parallel Branches (Shallow Networks + Transformer Head) → Aggregation → Final Prediction

- **Critical path:**
  1. Encode numerical features with PLE (n_bins=25-150) or one-hot based on target-aware binning
  2. Initialize embeddings with dimension e; verify embedding dimension ≥ typical feature cardinality
  3. Configure transformer layers (n_layers=2-4, n_heads=2-8) matching baseline FT-Transformer
  4. Set feature dropout probability = 0.1 (critical for identifiability)
  5. Train end-to-end with early stopping on validation loss
  6. Extract marginal effects by evaluating f_j(ε_j) across feature value range

- **Design tradeoffs:**
  - Embedding size (e): Larger e improves representation capacity but increases shallow network parameters (J × e)
  - Encoding type: PLE (25 bins) vs. one-hot (150 bins) — one-hot better for complex marginals, PLE more parameter-efficient
  - Transformer depth: Deeper improves interaction modeling but provides diminishing returns for interpretability
  - Feature dropout rate: Higher dropout strengthens identifiability guarantee but may hurt convergence speed

- **Failure signatures:**
  - Marginal effect plots show high variance across training runs → feature dropout may be too low or insufficient data
  - Performance significantly below FT-Transformer baseline → check embedding initialization, learning rate mismatch
  - Marginal effects appear "steppy" or discontinuous → PLE bins too coarse; increase bin count or switch to one-hot
  - Attention weights show no feature selectivity → transformer may not be learning meaningful interactions

- **First 3 experiments:**
  1. **Baseline parity test**: Replicate Table 2 on California Housing. Train NAMformer and FT-Transformer with identical hyperparameters (e=64, 2 layers, 2 heads). Verify MSE difference < 0.01 and that marginal effect plots for latitude/longitude match Figure 6
  2. **Identifiability sanity check**: On simulated data with known marginal effects, train NAMformer with and without feature dropout. Compute R² between learned f_j and ground truth. Expect ≥0.15 improvement with dropout enabled
  3. **Encoding ablation**: On Diamonds or House Sales dataset, compare PLE (25 bins) vs. one-hot (150 bins) vs. raw standardized features. Measure both predictive performance and marginal effect recovery

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical identifiability bound assumes risk uniformity across dropout configurations, which may not hold in practice
- While the model claims interpretability, it lacks statistical inference tools (confidence intervals, p-values) that are standard in GAMs
- Claims about real-world marginal effect interpretability without statistical uncertainty quantification are not fully validated

## Confidence
- **High confidence**: Predictive performance claims (Table 2 showing parity with FT-Transformer), simulation-based marginal effect recovery (Table 1), and basic architectural contributions
- **Medium confidence**: Theoretical identifiability guarantees under dropout assumptions, and the claim that uncontextualized embeddings preserve feature identity across tabular domains
- **Low confidence**: Claims about real-world marginal effect interpretability without statistical uncertainty quantification, and generalizability to datasets with highly correlated features or extreme class imbalance

## Next Checks
1. **Real-world identifiability validation**: Apply NAMformer to a dataset with known feature-response relationships and quantify whether learned marginal effects match ground truth conditional expectations beyond the R² metric reported in simulations
2. **Feature correlation stress test**: Evaluate marginal effect recovery on synthetic data where features are highly correlated (ρ > 0.8) to assess whether the additive decomposition breaks down when main effects are entangled
3. **Statistical inference extension**: Implement bootstrap-based confidence intervals for marginal effects and assess whether they capture the true uncertainty in effect estimates, particularly for features with complex nonlinear relationships