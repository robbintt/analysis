---
ver: rpa2
title: Efficient LLM Serving on Hybrid Real-time and Best-effort Requests
arxiv_id: '2504.09590'
source_url: https://arxiv.org/abs/2504.09590
tags:
- requests
- request
- cache
- bros
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces BROS, an LLM serving system designed to\
  \ handle both real-time (RT) and best-effort (BE) requests on shared GPU resources.\
  \ The core challenge addressed is efficiently scheduling these two request types\
  \ with conflicting requirements\u2014RT requests need low latency while BE requests\
  \ prioritize high throughput."
---

# Efficient LLM Serving on Hybrid Real-time and Best-effort Requests

## Quick Facts
- **arXiv ID:** 2504.09590
- **Source URL:** https://arxiv.org/abs/2504.09590
- **Reference count:** 40
- **Primary result:** Reduces RT latency by up to 74.20% while maintaining high BE throughput

## Executive Summary
This paper introduces BROS, an LLM serving system designed to handle both real-time (RT) and best-effort (BE) requests on shared GPU resources. The core challenge addressed is efficiently scheduling these two request types with conflicting requirements—RT requests need low latency while BE requests prioritize high throughput. BROS proposes a dynamic priority-based scheduling algorithm that batches RT and BE requests strategically at each iteration, optimizing throughput for BE requests while meeting RT requests' latency Service-Level Objectives (SLOs). A bidirectional KV cache management mechanism allows RT and BE requests to share memory blocks, reducing scheduling restrictions caused by insufficient KV memory. Experiments show BROS significantly reduces RT latency by up to 74.20%, improves SLO attainment by up to 36.38x, and maintains high throughput for BE requests with only a small reduction compared to state-of-the-art systems.

## Method Summary
BROS is an LLM serving system that co-locates latency-sensitive RT requests and throughput-oriented BE requests on shared GPUs. It uses a dynamic priority-based scheduling algorithm that sorts RT requests by urgency (time to SLO deadline) and packs them with BE requests to optimize throughput while meeting latency targets. The system employs a bidirectional KV cache layout where RT and BE requests share memory blocks growing in opposite directions, allowing RT requests to preempt BE slots when needed. An adaptive batch sizing mechanism starts with a base size of 128 and dynamically adjusts based on SLO pressure. The scheduler uses a cost model to predict iteration latency based on input/context lengths and checkpointed blocks, making packing decisions that balance RT SLO protection with BE throughput maximization.

## Key Results
- Reduces RT latency by up to 74.20% compared to vLLM baseline
- Improves SLO attainment by up to 36.38x for real-time requests
- Maintains high throughput for BE requests with only minor reduction (up to 17%) compared to state-of-the-art systems

## Why This Works (Mechanism)

### Mechanism 1: SLO-Aware Preemptive Scheduling
Co-serving RT and BE requests requires prioritizing RT requests based on their remaining "time budget" until a Service Level Objective (SLO) violation occurs. At every iteration, the system sorts pending RT requests by their urgency (residual time to SLO deadline) and packs the most urgent RT requests into a batch, then fills remaining slots with BE requests. The core assumption is that the cost model can accurately predict iteration latency based on input/context lengths and checkpointed blocks. Break condition: if the cost model drifts significantly, the scheduler may underestimate batch time, causing it to pack too many requests and violate RT SLO deadlines.

### Mechanism 2: Bidirectional KV Cache Layout
Reducing memory fragmentation and preemption overhead requires allowing RT and BE requests to physically share the same memory block while expanding in opposite directions. A single KV cache block is divided such that an RT request occupies slots from the left (growing right) and a BE request occupies slots from the right (growing left). If an RT request needs space and the block is shared, it overwrites the BE request's tail, with only the specific overwritten slots checkpointed to CPU. Core assumption: the overhead of managing bi-directional pointers and partial checkpointing is lower than full block preemption or rigid partitioning. Break condition: if requests have extremely long output lengths causing frequent collisions, the frequency of lazy checkpointing could become a persistent I/O bottleneck.

### Mechanism 3: Adaptive Batch Sizing
To balance conflicting batch size preferences (RT prefers small batches for low latency, BE prefers large batches for throughput), the system dynamically resizes batches based on real-time SLO pressure. The system initializes with a base batch size of 128, applies "Gradual Expansion" (doubling) when no urgent RT requests are detected, and applies "Aggressive Backoff" (resetting to base size) if estimated iteration time threatens to violate the most urgent RT request's SLO. Core assumption: the request pattern allows intervals of "low urgency" where batch size can safely scale up. Break condition: under sustained high-concurrency RT loads, batch size may remain permanently "starved" at base size, failing to utilize GPU capacity for BE requests.

## Foundational Learning

- **LLM Inference Phases (Prefill vs. Decode)**: The system treats these phases differently in the cost model and scheduler. You cannot understand the "iteration-level" scheduling or the conflict between compute-bound (prefill) and memory-bound (decode) stages without this distinction. Quick check: Does the scheduler batch prefill and decode requests together, or does it wait for one phase to finish before starting the next?

- **KV Cache & PagedAttention**: The core innovation of BROS (Bidirectional KV Cache) modifies the standard PagedAttention mechanism. Understanding that the KV cache grows dynamically with token generation is essential to grasp why memory contention happens. Quick check: In a standard block, how are tokens stored? How does BROS change this physical layout to allow sharing?

- **TTFT vs. TPOT**: These are the specific SLOs the scheduler optimizes for. The system trades off these metrics against BE throughput. Quick check: Which metric (TTFT or TPOT) is primarily affected by the *prefill* phase, and which reflects the *streaming speed* of the response?

## Architecture Onboarding

- **Component map:** Entrypoint -> Scheduler (Engine) -> KV Block Manager -> Executors
- **Critical path:** 1) Request arrives → Waiting Queue 2) Scheduler Loop (Per Iteration): Query KV Block Manager, Sort RT by urgency, Cost Model Lookup, Pack RT then BE → Running Queue, Generate memory instructions 3) Executor: Executes batch, returns output tokens, updates KV cache
- **Design tradeoffs:** Scheduling Overhead vs. Accuracy (8-9% overhead traded for better SLO attainment), Throughput vs. Latency (up to 17% BE throughput reduction to guarantee RT latency)
- **Failure signatures:** SLO Avalanche (RT rate spike beyond capacity causing queue buildup), Memory Thrashing (bidirectional blocks collide frequently triggering excessive CPU-GPU transfers)
- **First 3 experiments:** 1) Reproduce Cost Model Accuracy: Profile iteration latencies for varying prompt/context lengths vs. linear model predictions 2) Ablation on Cache Layout: Run "With Bi-cache" vs. "No Bi-cache" experiment to isolate memory manager impact 3) Hybrid Load Stress Test: Use ShareGPT dataset with synthetic BE requests to verify RT Normalized Latency remains low as BE Throughput climbs

## Open Questions the Paper Calls Out

- **Question:** Can the hybrid RT/BE scheduling problem be solved closer to optimality than the proposed greedy heuristic without violating real-time latency constraints?
- **Question:** How does runtime variability in inference latency impact the accuracy of the offline linear cost models used for scheduling?
- **Question:** Is the bidirectional KV cache layout compatible with IO-aware attention kernels like FlashAttention?

## Limitations
- The bidirectional KV cache mechanism lacks extensive validation, with ablation studies showing only marginal differences between configurations
- The cost model's accuracy is critical but receives limited scrutiny, with no reported error rates or sensitivity analysis
- The adaptive batch sizing heuristic lacks theoretical grounding for parameter choices, with no sensitivity analysis across varying workloads

## Confidence
- **High Confidence**: Core scheduling algorithm is well-specified and experimental methodology for measuring RT latency improvements is rigorous
- **Medium Confidence**: Bidirectional KV cache mechanism is clearly described but has limited experimental validation
- **Low Confidence**: Cost model's prediction accuracy and adaptive batch sizing parameters lack thorough validation

## Next Checks
1. **Cost Model Drift Analysis**: Implement instrumentation to measure the gap between predicted and actual iteration latencies over time. Run sustained high-load tests and plot prediction error rates to determine if model drift correlates with increased SLO violations.

2. **Bidirectional Cache Stress Test**: Design experiments that maximize BE/RT memory contention using BE requests with long output sequences paired with high-frequency RT requests. Measure the frequency of preemption events and lazy checkpointing overhead to determine if memory management becomes the bottleneck.

3. **Parameter Sensitivity Sweep**: Systematically vary adaptive batch sizing parameters (base size, growth factor, backoff threshold) across a range of workload mixes. Identify parameter settings that maximize BE throughput while maintaining RT SLOs, and determine if current defaults are globally optimal or workload-specific.