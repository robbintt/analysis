---
ver: rpa2
title: 'SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning'
arxiv_id: '2506.15154'
source_url: https://arxiv.org/abs/2506.15154
tags:
- music
- feature
- captions
- features
- captioning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SonicVerse, a multi-task learning model for
  music captioning that integrates caption generation with auxiliary music feature
  detection (e.g., key, vocals, instrumentation). The core idea is to use a projection-based
  architecture that transforms audio into language tokens while simultaneously detecting
  music features through dedicated auxiliary heads.
---

# SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning

## Quick Facts
- arXiv ID: 2506.15154
- Source URL: https://arxiv.org/abs/2506.15154
- Reference count: 5
- Primary result: Multi-task music captioning model with feature-informed generation achieves BLEU-4 of 0.1824 and BERT-Score of 0.8723 on MusicCaps

## Executive Summary
This paper introduces SonicVerse, a multi-task learning model for music captioning that integrates caption generation with auxiliary music feature detection (e.g., key, vocals, instrumentation). The core idea is to use a projection-based architecture that transforms audio into language tokens while simultaneously detecting music features through dedicated auxiliary heads. These feature outputs are also projected into language tokens to enhance captioning input. The model is trained on an extended MusicBench dataset augmented with music features using MIRFLEX. Experimental results show that incorporating music features improves caption quality, with BLEU-4 of 0.1824 and BERT-Score of 0.8723 on MusicCaps. Additionally, SonicVerse enables temporally-informed captions for full-length pieces via LLM chaining, demonstrated with a case study on "Bohemian Rhapsody."

## Method Summary
SonicVerse uses a dual-pathway projection architecture where MERT audio embeddings are processed through both content and feature pathways. The content pathway extracts descriptive language tokens, while the feature pathway uses 6 parallel task heads to detect music attributes (key, instrument, mood, genre, vocals, gender). Both pathways project their outputs into language tokens and concatenate them before feeding into a frozen Mistral-7B LLM. For long-form audio, the model chains chunk-level captions using GPT-4 to create temporally-aware descriptions.

## Key Results
- BLEU-4 of 0.1824 and BERT-Score of 0.8723 on MusicCaps dataset
- Improved performance over baseline models across all NLP metrics
- Demonstrates temporally-informed long-form captioning via LLM chaining
- Open-sourced model and code

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-task feature detection auxiliary heads improve caption quality by explicitly injecting structured musical attributes into the captioning pipeline.
- Mechanism: K parallel task heads predict discrete features (key, instrument, genre, mood, vocals, gender) from shared MERT embeddings via learned layer-weighted averaging. Each feature prediction is projected through MLP_feat into language tokens (z_feature) and concatenated with content tokens before LLM decoding.
- Core assumption: Detected music features are accurate enough to guide captioning without introducing systematic errors.
- Evidence anchors:
  - [abstract] "outputs of these heads are also projected into language tokens, to enhance the captioning input"
  - [Section 3.2] Equation 7 shows z_feature = concat(MLP_feat(ŷ_k)), with feature probabilities from σ(f_k(H_shared))
  - [corpus] JamendoMaxCaps paper demonstrates value of metadata enrichment for caption datasets, supporting the feature-injection premise
- Break condition: If auxiliary feature detectors have high error rates, injected features may mislead caption generation rather than improve it.

### Mechanism 2
- Claim: Dual-pathway projection (content + feature) captures both abstract/qualitative and concrete/technical musical attributes better than single-path approaches.
- Mechanism: Separate learnable layer-weighted averages (α_ℓ for content, β_ℓ for features) extract different representations from MERT's L=13 layers. Content pathway captures soft features; feature pathway provides hard music attributes. Both project to tokens via independent MLPs.
- Core assumption: MERT's hierarchical layers encode distinguishable information for soft vs. hard features that can be separately extracted.
- Evidence anchors:
  - [Section 3.1] "The authors of MERT report that the higher layers do not perform particularly well for the genre detection task and suggest empirically choosing layers for tasks"
  - [Section 3.2] Equations 3-5 define separate learned weights α_ℓ and β_ℓ for content vs. feature extraction
  - [corpus] Limited direct evidence; neighboring papers focus on caption generation without explicit dual-path analysis
- Break condition: If MERT layers don't provide sufficiently disentangled representations, dual-path yields redundant or conflicting signals.

### Mechanism 3
- Claim: LLM chaining enables temporally-aware long-form captions by aggregating chunk-level descriptions.
- Mechanism: Long audio split into 10s chunks → SonicVerse generates per-chunk captions → GPT-4 synthesizes flowing narrative with temporal markers via curated prompt (Equation 10).
- Core assumption: Chunk-level captions are accurate enough that an LLM can identify and down-weight inconsistent descriptions.
- Evidence anchors:
  - [abstract] "enables the generation of detailed time-informed descriptions for longer music pieces, by chaining the outputs using a large-language model"
  - [Section 3.3] Prompt explicitly instructs: "If the description of certain chunks does not seem to fit... treat those as bad descriptions with lower accuracy"
  - [corpus] FUTGA paper explores temporally-enhanced captioning via segment-level descriptions, supporting chunk-based temporal modeling
- Break condition: If chunk boundaries split musical phrases unnaturally, per-chunk captions may be incoherent, causing LLM synthesis failures.

## Foundational Learning

- Concept: **MERT audio encoder architecture**
  - Why needed here: MERT provides L=13 hierarchical transformer layers with 768-dim embeddings; understanding layer selection is critical for both content and feature pathways.
  - Quick check question: Can you explain why learned weighted averaging of layers (vs. using only the final layer) might improve feature extraction?

- Concept: **Multi-task learning with shared backbone**
  - Why needed here: SonicVerse uses shared MERT representations with K task-specific heads; gradients from auxiliary tasks shape the shared representation.
  - Quick check question: What happens to auxiliary task gradients during captioning-focused training if λ_k weights are set too low?

- Concept: **Projection to LLM token space**
  - Why needed here: Music embeddings must be converted to language tokens compatible with frozen Mistral-7B; understanding this alignment is essential for debugging caption quality.
  - Quick check question: If projected tokens z_content don't align with English word embeddings, what captioning failures would you expect?

## Architecture Onboarding

- Component map:
  - **MERT-95M encoder**: Frozen; outputs H ∈ R^(L×T'×D) with L=13 layers, D=768
  - **Content projector**: Learned layer weights α → time-average → MLP_content → z_content (35 tokens)
  - **Feature projector**: Shared backbone with K=6 task heads (key, instrument, mood, genre, vocals, gender) → each outputs 5 tokens → z_feature (30 tokens total)
  - **Frozen Mistral-7B LLM**: Receives [z_content || z_feature || query tokens], generates caption
  - **Chaining module (inference-only)**: GPT-4 synthesizes chunk captions into long-form description

- Critical path: Audio (10s clip) → MERT → dual projectors → token concatenation → Mistral-7B → caption. For long audio: chunk → repeat → GPT-4 synthesis.

- Design tradeoffs:
  - Token budget: Fixed 60 tokens total (35 content + 25 feature); increasing feature tokens may reduce content expressivity
  - Frozen vs. fine-tuned LLM: Freezing Mistral-7B reduces compute but limits domain adaptation
  - MIRFLEX pseudo-labels: Training features use extracted labels, not ground truth—introduces noise but enables scaling

- Failure signatures:
  - Captions missing specific instruments/keys → check feature head accuracy on validation set
  - Generic/short captions → verify content projector weights aren't collapsing to uniform layer weights
  - Temporal inconsistencies in long captions → review chunk boundary placement; may need overlap

- First 3 experiments:
  1. **Ablate feature pathway**: Train Baseline A (content-only) vs. Baseline B (full) on same data; compare BLEU-4 and music metrics to isolate feature contribution.
  2. **Layer weight analysis**: Visualize learned α_ℓ and β_ℓ distributions; verify content vs. feature pathways use different MERT layers as intended.
  3. **Feature head accuracy audit**: Evaluate each auxiliary head (key, instrument, etc.) independently on held-out data with ground-truth labels to identify weak detectors before deployment.

## Open Questions the Paper Calls Out
None

## Limitations
- Feature extraction noise: The auxiliary music feature detectors are trained on pseudo-labels from MIRFLEX rather than ground truth, introducing systematic noise.
- Evaluation scope constraints: All reported results use MusicCaps as the primary evaluation dataset, limiting generalization assessment.
- Token budget inflexibility: The fixed 60-token budget (35 content + 25 feature) may not optimally balance content versus feature representation across all music types.

## Confidence
- **High confidence**: The architectural feasibility of the projection-based multi-task learning framework. The dual-pathway approach with learned layer weighting is technically sound and implementable.
- **Medium confidence**: The claim that feature injection improves caption quality. The reported BLEU-4 and BERT-Score improvements over baselines suggest this, but the feature detector noise and evaluation limitations reduce certainty.
- **Low confidence**: The claim that LLM chaining reliably produces temporally-aware long-form captions. The "Bohemian Rhapsody" case study demonstrates possibility but provides no systematic evidence of consistent quality or temporal coherence across diverse music pieces.

## Next Checks
1. **Feature head accuracy validation**: Evaluate each auxiliary task head (key, instrument, mood, genre, vocals, gender) on a held-out subset of the data with ground truth labels. Calculate precision, recall, and F1 scores for each feature category to quantify the noise level in the pseudo-labels and its potential impact on caption quality.

2. **Cross-dataset generalization test**: Evaluate SonicVerse on at least one additional music captioning dataset (e.g., IREOS or a subset of FMA) to verify that the feature-informed captioning improvements generalize beyond MusicCaps. Compare performance drops to baseline models to assess robustness.

3. **Controlled ablation of feature pathway**: Conduct a systematic ablation study where feature tokens are replaced with random noise tokens at varying noise levels (0% to 100%) during inference. Measure the degradation in caption quality metrics to quantify the actual contribution of the feature pathway versus the content pathway alone.