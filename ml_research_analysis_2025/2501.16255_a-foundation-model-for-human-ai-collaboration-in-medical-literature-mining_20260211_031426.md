---
ver: rpa2
title: A foundation model for human-AI collaboration in medical literature mining
arxiv_id: '2501.16255'
source_url: https://arxiv.org/abs/2501.16255
tags:
- extraction
- leads
- data
- search
- trial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEADS is a foundation model for medical literature mining that
  addresses the challenges of applying AI to systematic literature reviews. It integrates
  study search, screening, and data extraction into a single multitask framework,
  trained on a large-scale instruction dataset (LEADSInstruct) curated from 21,335
  systematic reviews, 453,625 publications, and 27,015 clinical trial registries.
---

# A foundation model for human-AI collaboration in medical literature mining

## Quick Facts
- arXiv ID: 2501.16255
- Source URL: https://arxiv.org/abs/2501.16255
- Reference count: 40
- Primary result: LEADS improves recall by 5.2% and reduces time by 22.6% in screening tasks, and improves accuracy by 6.2% with 26.9% time savings in data extraction tasks

## Executive Summary
LEADS is a foundation model for medical literature mining that addresses the challenges of applying AI to systematic literature reviews. It integrates study search, screening, and data extraction into a single multitask framework, trained on a large-scale instruction dataset (LEADSInstruct) curated from 21,335 systematic reviews, 453,625 publications, and 27,015 clinical trial registries. The model achieves consistent improvements over four generic LLMs across six tasks, demonstrating superior performance in search query generation (Recall: 24.68 vs. 5.79), study eligibility assessment (Recall@50: 0.81 vs. 0.52), and data extraction (accuracy: 0.85 vs. 0.80). A user study with 16 clinicians and medical researchers confirmed that LEADS improves recall by 5.2% and reduces time spent by 22.6% in screening tasks, and improves accuracy by 6.2% with 26.9% time savings in data extraction tasks. These results highlight LEADS's potential to enhance human-AI collaboration in medical literature mining.

## Method Summary
LEADS fine-tunes Mistral-7B-Instruct-v0.3 on 633,759 instruction-response pairs derived from systematic reviews, publications, and clinical trial registries. The training data is constructed by linking systematic reviews to their included studies and corresponding trial registries, then extracting structured PICO elements, search queries, eligibility criteria, and extraction targets. The model is trained for one epoch with AdamW optimizer (LR=1e-6), batch size 5, and 30K token context using DeepSpeed ZeRO-3 and FlashAttention-2 on 5× A100 80G GPUs for approximately 2.5 days. Evaluation uses automatic metrics (Recall, Recall@K, accuracy) plus human validation and a user study with 16 clinicians.

## Key Results
- LEADS achieves 24.68 Recall on search query generation vs. 5.79 for GPT-4o, despite being ~100× smaller
- Human-AI collaboration achieves 0.81 recall vs. 0.77 for experts alone, with 22.6% time savings in screening
- Data extraction accuracy improves from 0.80 to 0.85 with 26.9% time savings in user study
- LEADS outperforms four generic LLMs across all six tasks: search query generation, study eligibility assessment, and four extraction subtasks

## Why This Works (Mechanism)

### Mechanism 1
Domain-specific instruction tuning on curated medical literature data yields task performance superior to significantly larger generic LLMs. LEADS fine-tunes Mistral-7B (a 7B parameter model) on 633,759 instruction-response pairs derived from systematic reviews, publications, and clinical trial registries. This teaches domain-specific patterns for PICO-based reasoning, query construction, and structured extraction that generic pre-training cannot provide.

### Mechanism 2
Human-AI collaboration yields both efficiency gains and quality improvements compared to either experts or AI alone. LEADS provides structured intermediate outputs—criterion-level eligibility scores with rationales—that allow experts to triage quickly while maintaining decision authority. Experts can safely accept high-confidence predictions, focus scrutiny on uncertain cases, and override errors.

### Mechanism 3
Hybrid instruction data generation—combining mined ground truth with LLM synthesis—produces higher-quality training data than pure synthesis alone. LEADSInstruct uses structured data from ClinicalTrials.gov registries (investigator-entered, high-quality) as ground-truth outputs, then constructs corresponding inputs from linked publications. This avoids relying solely on GPT-4 to generate outputs, which can hallucinate.

## Foundational Learning

- **PICO Framework (Population, Intervention, Comparison, Outcome)**: LEADS uses PICO to structure research questions, generate search queries, and assess study eligibility. Understanding PICO is essential for interpreting model inputs/outputs.
  - Quick check: Can you decompose "Does metformin reduce HbA1c in adults with Type 2 diabetes compared to placebo?" into P, I, C, O elements?

- **Instruction Tuning vs. In-Context Learning**: LEADS uses instruction tuning (fine-tuning on task-formatted data) rather than relying solely on prompting. The paper shows ICL alone underperforms fine-tuning for diverse medical tasks.
  - Quick check: What is the difference between providing examples in a prompt (ICL) and training on input-output pairs (instruction tuning)?

- **Recall@K Metric**: The paper evaluates screening performance using Recall@K—the proportion of relevant studies found in the top-K ranked results. This reflects real-world constraints where reviewers can only examine a limited candidate pool.
  - Quick check: If a systematic review has 20 target studies and a model retrieves 16 of them in the top 50 results, what is Recall@50?

## Architecture Onboarding

- **Component map**: Systematic reviews → included studies → trial registries → instruction data → fine-tuning → LEADS model
- **Critical path**: 1) Data linking (systematic reviews → included studies → trial registries) 2) Instruction data construction (input-output pairs with prompts) 3) Fine-tuning (1 epoch, lr=1e-6, 30K token context) 4) Evaluation (automatic metrics + human validation + user study)
- **Design tradeoffs**: Single model for all tasks vs. specialized models per task (chose unified for flexibility); Registry-grounded outputs vs. LLM-synthesized outputs (chose hybrid for reliability); 7B model vs. larger models (chose smaller for efficiency; results show sufficient capacity)
- **Failure signatures**: Low recall on search queries may indicate overly narrow term generation; Hallucinated extraction values often appear when source documents lack explicit answers—model should output "UNCERTAIN"; User study showed performance gains concentrated in difficult cases; easy cases show parity (ceiling effects)
- **First 3 experiments**: 1) Run LEADS on 10 held-out reviews; verify search query Recall exceeds 15 (baseline threshold from paper) 2) Compare LEADS vs. Mistral-7B baseline on eligibility assessment to isolate instruction-tuning contribution 3) On extraction tasks, manually inspect 20 failures to identify patterns (missing context, unit confusion, calculation errors)

## Open Questions the Paper Calls Out

- Can LEADS maintain reported efficiency and recall when scaling from the pilot study's 30-citation pool to real-world workloads involving thousands of candidates?
- Can the LEADS framework be extended to perform qualitative assessments such as study quality evaluation and evidence uncertainty grading?
- To what extent do outdated information and systematic biases within the LEADSInstruct dataset propagate into the model's predictions?

## Limitations

- The hybrid instruction-data generation approach assumes ClinicalTrials.gov registry data is more reliable than LLM synthesis, but the paper does not validate registry-publication linkages against independent sources
- The user study sample size (16 participants) is modest, limiting generalizability across different clinical specialties and review types
- While the model achieves strong performance on six core tasks, its behavior on edge cases (e.g., extremely complex PICO formulations, non-standard trial designs) remains unexplored

## Confidence

- **High Confidence**: The superiority of LEADS over generic LLMs is well-supported by quantitative comparisons across multiple tasks and datasets. The mechanism linking domain-specific instruction tuning to improved performance is logically sound.
- **Medium Confidence**: The human-AI collaboration benefits are supported by user study data but limited by sample size and potential selection bias in participant recruitment. The hybrid data generation approach is plausible but lacks direct comparative validation against pure synthetic approaches.
- **Low Confidence**: The long-term reliability of registry-grounded training data and the model's behavior on complex or novel medical scenarios have not been thoroughly validated.

## Next Checks

1. Conduct a larger-scale user study (n≥50) across multiple clinical specialties to assess generalizability of human-AI collaboration benefits and calibrate trust thresholds
2. Perform cross-validation of ClinicalTrials.gov-publication linkages using independent sources to quantify ground-truth accuracy and identify systematic linkage errors
3. Test LEADS on systematic reviews involving rare diseases, complex trial designs, or emerging therapeutic areas to identify edge case limitations and failure modes