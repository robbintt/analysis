---
ver: rpa2
title: The Unfairness of Multifactorial Bias in Recommendation
arxiv_id: '2601.12828'
source_url: https://arxiv.org/abs/2601.12828
tags:
- bias
- recommendation
- fairness
- items
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the combined effect of popularity bias
  and positivity bias, termed multifactorial bias, on the fairness of recommender
  systems. The authors find that positivity bias disproportionately affects popular
  items, amplifying their over-exposure and leading to unfair treatment of less popular
  items.
---

# The Unfairness of Multifactorial Bias in Recommendation

## Quick Facts
- arXiv ID: 2601.12828
- Source URL: https://arxiv.org/abs/2601.12828
- Reference count: 40
- Primary result: Pre-processing with percentile-based rating transformation reduces multifactorial bias, improving exposure fairness with minimal accuracy loss

## Executive Summary
This paper investigates how the combination of popularity bias and positivity bias (termed multifactorial bias) creates unfair exposure distribution in recommender systems. The authors demonstrate that positivity bias disproportionately affects popular items, amplifying their over-exposure and disadvantaging less popular items. They propose an item-wise percentile-based rating transformation as a pre-processing step to mitigate this bias, showing that it reduces the impact of positivity bias on popular items while maintaining accuracy. Experiments across six recommendation algorithms and four datasets demonstrate improved fairness metrics with minimal accuracy loss, and show that integrating this pre-processing step with post-processing fairness methods enhances both effectiveness and computational efficiency.

## Method Summary
The method transforms raw ratings to percentile values using item profiles before model training. For each item, ratings are sorted ascending and each rating r is mapped to 100 × (position of last occurrence)/(number of ratings for item + 1). This percentile matrix is then used to train recommendation models (BiasedMF, SVD++, WRMF, ListRank, UserKNN, ItemKNN). The transformed data reduces the systematic over-weighting of popular items in training, leading to fairer initial recommendations. The approach can be combined with post-processing rerankers (DM, FA*IR, xQuAD, FairMatch) to further enhance fairness while reducing computational costs by enabling shorter candidate lists.

## Key Results
- Percentile transformation reduces positivity bias concentration on popular items, improving exposure fairness (EE) by up to 76% with minimal nDCG loss
- The pre-processing approach integrates effectively with post-processing fairness methods, achieving comparable fairness with 71-77% reduced computational time
- Across six algorithms and four datasets, the method consistently improves item aggregate diversity (IA) and long-tail diversity (LIA) metrics
- The combined percentile + reranker pipeline with list size 20 outperforms original + reranker with list size 100

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The interaction between popularity bias and positivity bias creates compounding exposure unfairness that is worse than either bias alone.
- **Mechanism:** Popular items receive more ratings (popularity bias) AND disproportionately higher rating values (positivity bias concentrated on popular items). This dual advantage causes recommendation models to learn stronger associations with popular items, amplifying their over-representation in outputs.
- **Core assumption:** The rating selection process is determined by both item popularity and rating value factors simultaneously.
- **Evidence anchors:**
  - [abstract] "positivity bias is disproportionately concentrated on popular items, further amplifying their over-exposure"
  - [section 3] Figure 3 shows correlation between average ratings and popularity across all four datasets; Characteristic 3: "The majority of high rating values are assigned to the popular items"
  - [corpus] Related papers discuss popularity bias amplification independently but do not directly address multifactorial interaction
- **Break condition:** If rating distributions for popular and unpopular items are statistically similar, the compounding effect disappears.

### Mechanism 2
- **Claim:** Item-wise percentile transformation reduces multifactorial bias by normalizing rating distributions across items regardless of popularity.
- **Mechanism:** Converting raw ratings to percentile values within each item's profile redistributes high/low values uniformly. For popular items with many high ratings, percentiles spread these across the distribution, reducing their inflated signal. For unpopular items, the transformation makes their fewer ratings more comparable.
- **Core assumption:** Items have sufficient rating diversity (non-uniform values) and sufficient count for reliable percentile calculation.
- **Evidence anchors:**
  - [abstract] "adapt a percentile-based rating transformation method as a pre-processing step, reducing the impact of positivity bias on popular items"
  - [section 6] Equation 5 defines percentile calculation; Figure 6 shows post-transformation plots where "the average percentile values for both popular and unpopular items are concentrated on certain values" with no systematic high-value concentration on popular items
  - [corpus] No direct corpus support for this specific percentile-based mechanism
- **Break condition:** Items with uniform ratings (e.g., all 3s) yield uninformative percentiles; items with very few ratings (e.g., <5) produce unreliable percentile positions.

### Mechanism 3
- **Claim:** Pre-processing with percentile transformation improves the efficiency of post-processing fairness methods by producing fairer initial recommendations.
- **Mechanism:** When the base model generates fairer initial lists (due to reduced multifactorial bias), rerankers need shorter candidate lists to achieve target fairness levels. Smaller initial lists reduce both reranking computation time and downstream processing overhead.
- **Core assumption:** The percentile transformation produces measurably fairer recommendations before reranking.
- **Evidence anchors:**
  - [abstract] "integrating the pre-processing step into post-processing fairness pipelines enhances their effectiveness and efficiency, achieving comparable fairness with reduced computational cost"
  - [section 7] Table 2 shows 71-77% running time reduction across datasets; Figure 11-12 demonstrate percentile+reranker with list size 20 outperforms original+reranker with list size 100
  - [corpus] Related work mentions computational trade-offs in reranking but does not specifically validate this pre-processing synergy
- **Break condition:** If the application requires maximum accuracy and cannot tolerate any transformation-induced accuracy variation, the efficiency gain may not justify deployment.

## Foundational Learning

- **Concept: Popularity Bias in Recommender Systems**
  - **Why needed here:** Understanding how rating concentration on few items propagates through models to cause exposure imbalance is essential for grasping why multifactorial bias compounds the problem.
  - **Quick check question:** If 20% of items receive 65% of all ratings, what happens to less popular items during model training and recommendation generation?

- **Concept: Gini Index for Exposure Fairness Measurement**
  - **Why needed here:** The paper uses Gini-based Equality of Exposure (EE) as a primary fairness metric; understanding how it quantifies distribution inequality is necessary for interpreting results.
  - **Quick check question:** Would a Gini index of 0.05 indicate more or less fairness than 0.25, and what does a value of 0 represent?

- **Concept: Pre-processing vs. Post-processing Fairness Interventions**
  - **Why needed here:** The paper's contribution bridges data-level (pre-processing) and output-level (post-processing) interventions; understanding their trade-offs clarifies the efficiency gains.
  - **Quick check question:** Why does a reranker need longer initial candidate lists to achieve higher fairness, and what computational cost does this incur?

## Architecture Onboarding

- **Component map:**
  Raw Rating Matrix (R) -> [Pre-processing] Percentile Transformation (item-wise, Eq. 5) -> Percentile Matrix (P) -> [Recommendation Model] BiasedMF / SVD++ / WRMF / ListRank / UserKNN / ItemKNN -> Initial Recommendation Lists (size K or N) -> [Optional Post-processing] Reranker (DM / FA*IR / xQuAD / FairMatch) -> Final Top-K Recommendations

- **Critical path:**
  1. Load rating data and apply item-wise percentile transformation before model training
  2. Train recommendation model on transformed percentile matrix (not raw ratings)
  3. Generate recommendations; optionally feed to reranker with reduced list size

- **Design tradeoffs:**
  - **Sparsity vs. transformation reliability:** Dense datasets (e.g., MovieLens 4.47% density) produce stable percentiles; sparse datasets may require minimum rating thresholds
  - **Fairness vs. accuracy:** Some algorithm/dataset combinations show minor nDCG drops (e.g., WRMF on MovieLens: -7.61% nDCG for +76% EE)
  - **List size vs. reranking cost:** Shorter lists (20 vs. 100) reduce runtime ~70% but require fairer upstream output

- **Failure signatures:**
  - Items with uniform ratings (e.g., all values identical) produce flat percentile distributions with no signal
  - Cold-start items with <5 ratings have unreliable percentile positions
  - ListRankMF on Yelp showed fairness degradation (Figure 10b) — investigate algorithm-specific interactions

- **First 3 experiments:**
  1. **Replicate simulation study:** On MovieLens, flip top ratings (5→1) for β% of most popular items (β ∈ {5, 10, 20}) and measure IA, LIA, EE changes to validate multifactorial bias impact
  2. **Percentile transformation validation:** Train BiasedMF on raw vs. percentile-transformed data for one dataset; report nDCG, IA(α=1), LIA(α=1), EE to confirm fairness gain with acceptable accuracy
  3. **Reranking efficiency test:** Compare DM reranker performance using percentile-transformed input with list size 20 vs. original input with list size 100; measure nDCG, IA, EE, and wall-clock time to quantify efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the percentile-based rating transformation remain robust and effective in highly sparse data environments or cold-start scenarios?
- Basis in paper: [explicit] Conclusion, Page 22.
- Why unresolved: The percentile transformation method requires a sufficient number of rating values to calculate a stable position; short rating vectors (e.g., <3, 4%) result in unstable transformations, and the study relied exclusively on dense datasets to avoid this failure mode.
- What evidence would resolve it: Performance evaluation (fairness and accuracy metrics) on datasets with significantly lower density or on items/users with minimal interaction history (e.g., fewer than 5 ratings).

### Open Question 2
- Question: Can existing, distinct mitigation strategies for popularity and positivity bias be effectively combined to address multifactorial bias?
- Basis in paper: [explicit] Conclusion, Page 22.
- Why unresolved: The paper adapts a single method (percentile transformation) to address the combined bias holistically, but does not evaluate whether combining separate, specialized algorithms for popularity and positivity yields comparable or superior results.
- What evidence would resolve it: Experiments comparing the proposed pre-processing approach against a modular pipeline that sequentially or jointly applies a dedicated popularity-bias mitigator (e.g., IPS) and a positivity-bias mitigator.

### Open Question 3
- Question: Does mitigating multifactorial bias in training data propagate to improve user-side fairness in recommendation outputs?
- Basis in paper: [explicit] Conclusion, Page 22.
- Why unresolved: The current study exclusively analyzes item-side fairness (exposure diversity and equality of visibility), leaving the impact on user-centric fairness dimensions (e.g., demographic fairness or calibration) unexplored.
- What evidence would resolve it: Evaluation of recommendation outputs generated from percentile-transformed data using user-side fairness metrics, such as demographic parity or calibration error across user groups.

## Limitations
- The study focuses exclusively on explicit rating data and does not address multifactorial bias in implicit feedback or content-based systems
- The percentile transformation assumes sufficient rating diversity within items; performance on extremely sparse datasets or items with uniform ratings is untested
- Computational efficiency gains from pre-processing integration are demonstrated but not quantified across diverse hardware or real-time serving scenarios

## Confidence
- **High:** Mechanism 1 (compounding effect of popularity + positivity bias), experimental design, and fairness metric calculations
- **Medium:** Mechanism 2 (percentile transformation efficacy) and post-processing efficiency claims, due to potential algorithm-specific interactions
- **Low:** Generalizability to non-rating-based recommendation domains and cold-start scenarios

## Next Checks
1. Test percentile transformation on implicit feedback datasets (e.g., purchase history) to verify cross-domain applicability
2. Evaluate robustness on items with <10 ratings to identify transformation breakdown points
3. Measure latency impact of pre-processing + reranking pipeline in real-time recommendation serving environments