---
ver: rpa2
title: Bringing Emerging Architectures to Sequence Labeling in NLP
arxiv_id: '2509.25918'
source_url: https://arxiv.org/abs/2509.25918
tags:
- table
- sequence
- dependency
- tagging
- parsing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines alternative architectures beyond Transformers
  for sequence labeling, focusing on models not originally designed for token-level
  classification. It explores diffusion tagging, adversarial tagging, and non-Transformer
  encoders like xLSTMs and structured state-space models (SSMs).
---

# Bringing Emerging Architectures to Sequence Labeling in NLP

## Quick Facts
- **arXiv ID:** 2509.25918
- **Source URL:** https://arxiv.org/abs/2509.25918
- **Reference count:** 40
- **Primary result:** Adversarial tagging consistently rivals or surpasses Transformer baselines across diverse sequence labeling tasks, including complex structured prediction.

## Executive Summary
This paper systematically evaluates emerging architectures beyond Transformers for sequence labeling tasks, focusing on models not originally designed for token-level classification. The authors explore diffusion tagging, adversarial tagging, and non-Transformer encoders like xLSTMs and structured state-space models (SSMs). Through extensive experiments across five tasks (PoS tagging, NER, constituency parsing, dependency parsing, and graph parsing) in multiple languages, the paper identifies adversarial tagging as the most promising alternative to Transformers, while revealing significant limitations in diffusion models and SSMs for structured prediction tasks.

## Method Summary
The paper evaluates five different architectures on sequence labeling tasks using standard benchmark datasets. The baseline uses XLM-RoBERTa Large with a feed-forward network head. Diffusion tagging employs a DiT architecture with 6 blocks and 16 attention heads, using bit-conversion of labels and DDIM sampling for inference. Adversarial tagging trains a generator-discriminator pair where the generator predicts tags and the discriminator learns to distinguish gold from predicted sequences. The xLSTM architecture uses exponential gating (sLSTM) and matrix-based hidden states (mLSTM) for enhanced information flow. Finally, MAMBA-2 (an SSM) is evaluated as a Transformer alternative. All models are compared using accuracy, F1/LF, UAS/LAS metrics across various languages and task complexities.

## Key Results
- Bidirectional xLSTM architectures generally outperform traditional BiLSTMs across tasks, with 19/22 PoS tagging tasks showing improvement.
- Adversarial tagging consistently rivals or surpasses the Transformer baseline across diverse tasks, including complex structured prediction.
- Diffusion and SSM-based models perform poorly on complex structured tasks, with MAMBA-2 dropping to 22-52% LAS on dependency parsing versus 87-94% for baseline.
- Adversarial approach achieves the most stable performance, especially as label space increases, with 82.13% valid label sequences versus 80.76% for baseline.

## Why This Works (Mechanism)

### Mechanism 1: Adversarial Training for Structured Output Regularization
The adversarial tagging framework improves sequence labeling by training a generator to produce tag sequences that a discriminator cannot distinguish from valid structures, implicitly learning structural constraints. The generator Gψ predicts tag distributions while the discriminator Dφ receives both gold and predicted sequences, learning to identify incorrect outputs. The adversarial loss LA = H(Dφ(w, Gψ(w)), 1) forces the generator to produce structurally plausible sequences, not just locally accurate predictions. The discriminator's binary classification signal provides sufficient gradient information for the generator to learn global structural coherence.

### Mechanism 2: Bidirectional xLSTM for Enhanced Contextualization
BixLSTM outperforms traditional BiLSTM by stabilizing information flow through exponential gating (sLSTM) and enabling parallelizable matrix-based hidden states (mLSTM). The sLSTM block normalizes the forget gate to prevent vanishing/exploding gradients over long sequences. The mLSTM block replaces vector hidden states with learnable matrices resembling self-attention, enabling GPU parallelization while maintaining recurrence. Matrix-valued hidden states can capture token dependencies comparably to attention mechanisms while retaining LSTM-style sequential processing.

### Mechanism 3: Diffusion Tagging Fails on Structured Tasks Due to Denoising Complexity
Diffusion models, adapted via bit-conversion of discrete labels, struggle with structured prediction because iterative denoising of bit representations cannot reliably recover complex label dependencies. Tags are converted to m-bit binary vectors with Gaussian noise added over T=100 steps; a Transformer decoder learns to estimate added noise. At inference, DDIM sampling with s=10 skips progressively denoises from random noise to label predictions. Bit-level continuous denoising preserves discrete label structure information, but bit2tag conversion may yield invalid labels requiring replacement with most common tags.

## Foundational Learning

- **Concept: Sequence Labeling Linearizations**
  - **Why needed here:** The paper evaluates complex tasks (constituency/dependency/graph parsing) cast as tagging via encodings like relative, tetra-tagging, bracketing, and k-bit schemes. Understanding how trees/graphs map to token-level labels is essential.
  - **Quick check question:** Given a dependency tree, can you explain how 4-bit encoding represents head direction and dependent positions in four bits per token?

- **Concept: GAN Training Dynamics**
  - **Why needed here:** Adversarial tagging relies on generator-discriminator equilibrium; understanding mode collapse, vanishing gradients, and the role of Gumbel-Softmax relaxation is critical for debugging.
  - **Quick check question:** Why does the discriminator receive both gold and predicted tag sequences, and what loss signal does it provide to the generator?

- **Concept: Structured State-Space Models (SSMs)**
  - **Why needed here:** MAMBA-2 is evaluated as a Transformer alternative; understanding diagonal state matrices and selective gating clarifies why it excels at language modeling but struggles with structured tagging.
  - **Quick check question:** How do SSMs achieve O(n) complexity versus Transformers' O(n²), and what trade-offs might this introduce for tasks requiring global token dependencies?

## Architecture Onboarding

- **Component map:** Input tokens → Encoder (XLM-RoBERTa / xLSTM / MAMBA-2) → Decoder/Head → [Adversarial] Generator → Discriminator (2-layer BiLSTM + FFN) → [Diffusion] DiT blocks (6 layers, 16 heads) → bit2tag → [Baseline] FFN projection → softmax

- **Critical path:** For adversarial tagging (the strongest non-Transformer approach), the generator's cross-entropy loss combines with adversarial loss (λ=1) to train both generator and discriminator alternately. The discriminator's correctness signal (Equation 3) is smoothed via Gumbel-Softmax.

- **Design tradeoffs:**
  - **Adversarial vs. Baseline:** +1-2% accuracy on structured tasks, but requires training two networks; inference speed matches baseline (generator only).
  - **BixLSTM vs. BiLSTM:** +3-8% improvement on PoS tasks, but still 2-4% below Transformer baseline; faster inference due to parallelization.
  - **Diffusion vs. others:** Theoretically interesting but impractical; 10-step DDIM sampling is slow, and performance degrades catastrophically on low-resource/complex tasks.

- **Failure signatures:**
  - **SSM/MAMBA-2:** LAS drops to 22-52% on dependency parsing (vs. 87-94% for baseline) → model cannot capture long-range token dependencies required for structure.
  - **Diffusion on Tamil graphs:** 12.44 LF (vs. 63.99 baseline) → bit-level denoising fails with limited training data.
  - **Unidirectional encoders:** All architectures suffer 20-40% absolute performance drops on parsing → bidirectionality is non-negotiable for structured prediction.

- **First 3 experiments:**
  1. **Replicate adversarial tagger on CoNLL-2003 NER:** Use XLM-RoBERTa-base as encoder, 2-layer BiLSTM discriminator, λ=1. Compare against baseline FFN head; expect 95.23 vs. 95.01 accuracy (Table 2).
  2. **Ablate bidirectionality in xLSTM:** Train unidirectional vs. BixLSTM on PTB PoS tagging; expect 92.26% vs. 96.92% accuracy, confirming the 4-5% bidirectionality gain.
  3. **Stress test diffusion on increasing label space:** Train diffusion tagger on UD treebanks with varying UPOS/XPOS sizes; plot accuracy degradation curves to validate Figure 2's claim that diffusion is less stable as |L| grows.

## Open Questions the Paper Calls Out

- **Question:** Can autoregressive generative models serve as effective encoders for sequence labeling tasks if computational constraints are removed?
  - **Basis in paper:** [explicit] The "Limitations" section explicitly lists the "Lack of generative models as encoders" as a constraint, noting that the authors relied on masked language models due to resource limits and prior evidence that incremental parsing lags behind bidirectional encoders.
  - **Why unresolved:** The authors did not evaluate modern decoder-only architectures (e.g., Llama or GPT) as encoders, leaving their potential for bidirectional tagging contexts untested.
  - **What evidence would resolve it:** An empirical evaluation fine-tuning large autoregressive models on the structured tasks described in the paper and comparing their performance against the XLM-RoBERTa baseline.

- **Question:** Why do structured state-space models (SSMs) like MAMBA-2 fail to capture token dependencies in structured tasks despite their theoretical design for long-range modeling?
  - **Basis in paper:** [explicit] The analysis notes that SSMs "struggle to capture token dependencies within the input sentence," performing on par with simple left-to-right LSTMs in parsing, which contradicts their strong performance in language modeling.
  - **Why unresolved:** The paper identifies the performance gap but does not determine if the failure is due to the SSM architecture's inability to enforce strict structural constraints or differences in pretraining data scale compared to Transformers.
  - **What evidence would resolve it:** Probing studies on SSM hidden states to measure the decay of syntactic dependency information over distance, specifically in structured prediction scenarios.

- **Question:** Does the stability of adversarial tagging in large label spaces depend on the simplified BiLSTM discriminator architecture used in the experiments?
  - **Basis in paper:** [inferred] The authors mention reducing the discriminator to a "lightweight BiLSTM-based encoder" to simplify the setup, while results show adversarial tagging is the only approach maintaining stability as label space increases.
  - **Why unresolved:** It is unclear if the positive results are robust to changes in the discriminator or if a more complex discriminator would further enhance well-formedness or destabilize training.
  - **What evidence would resolve it:** Ablation studies varying the discriminator's capacity and architecture (e.g., replacing BiLSTM with a Transformer) and measuring the resulting impact on tagging accuracy and tree validity.

## Limitations

- The paper assumes linearization schemes preserve task-relevant information, but poor performance of diffusion and SSM models could reflect linearization artifacts rather than fundamental architectural limitations.
- Implementation details for adversarial training are underspecified - the paper doesn't discuss how generator-discriminator equilibrium was monitored or whether mode collapse occurred.
- The novelty of xLSTM means the paper lacks comparative baselines showing whether improvements over BiLSTM represent genuine architectural advances or implementation differences.

## Confidence

**High confidence:** Bidirectional xLSTM consistently outperforms traditional BiLSTM across tasks (19/22 PoS tasks, 4-5% absolute improvement). This result is supported by direct comparisons in Table 1 and is mechanistically sound - the exponential gating and matrix states provide clear advantages over standard LSTM recurrence.

**Medium confidence:** Adversarial tagging achieves stable performance across diverse tasks and surpasses the Transformer baseline. While the quantitative results are strong (82.13% vs. 80.76% valid sequences), the novelty of this application to sequence labeling means the training dynamics and failure modes are not well-characterized.

**Low confidence:** Diffusion tagging's poor performance on structured tasks reflects fundamental architectural limitations rather than implementation choices. The paper shows catastrophic failures (12.44 vs. 63.99 LF on Tamil graphs) but doesn't adequately explore whether different linearization schemes, training procedures, or inference strategies might improve results.

## Next Checks

1. **Adversarial training stability analysis:** Implement learning rate scheduling and monitor discriminator accuracy during training. If discriminator accuracy exceeds 95% consistently, the generator may not be receiving useful gradient signals. Vary λ from 0.1 to 10 to identify the optimal balance point and test whether the claimed improvements persist across this range.

2. **Linearization scheme ablation for structured tasks:** Retrain the best-performing architecture (adversarial tagging) using alternative linearizations for dependency parsing. Compare performance between relative, tetra-tagging, and bracket encodings to determine whether the poor diffusion/SSM results are task-specific or linearization-specific.

3. **xLSTM component isolation:** Train models with only sLSTM blocks (no mLSTM) and only mLSTM blocks (no sLSTM) on the PTB PoS tagging task. This will quantify the individual contributions of exponential gating stabilization versus matrix-based parallelization, clarifying which mechanism drives the performance gains over standard BiLSTM.