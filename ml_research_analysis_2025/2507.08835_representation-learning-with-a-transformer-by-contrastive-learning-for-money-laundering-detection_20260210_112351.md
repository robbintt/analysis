---
ver: rpa2
title: Representation learning with a transformer by contrastive learning for money
  laundering detection
arxiv_id: '2507.08835'
source_url: https://arxiv.org/abs/2507.08835
tags:
- transformer
- learning
- time
- data
- fraudsters
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a transformer-based approach for money laundering
  detection using contrastive learning on raw transaction time series. The method
  learns representations of complex financial time series through contrastive learning
  without labels, then applies a two-threshold classification strategy to distinguish
  fraudsters from non-fraudsters while controlling the false discovery rate via the
  Benjamini-Hochberg procedure.
---

# Representation learning with a transformer by contrastive learning for money laundering detection

## Quick Facts
- arXiv ID: 2507.08835
- Source URL: https://arxiv.org/abs/2507.08835
- Reference count: 20
- Primary result: Transformer-based contrastive learning approach detects 23.6% of fraudsters at 40% FDR vs 7.33% for LSTM and 0% for rule-based systems

## Executive Summary
This paper introduces a transformer-based approach for money laundering detection using contrastive learning on raw transaction time series. The method learns representations of complex financial time series through contrastive learning without labels, then applies a two-threshold classification strategy to distinguish fraudsters from non-fraudsters while controlling the false discovery rate via the Benjamini-Hochberg procedure. Experiments on real financial data show the transformer-based approach significantly outperforms traditional rule-based systems and LSTM architectures.

## Method Summary
The proposed method uses a transformer architecture trained through contrastive learning to extract meaningful representations from raw transaction time series data. The contrastive learning framework enables the model to learn patterns and relationships in the data without requiring labeled examples. After representation learning, a two-threshold classification strategy is applied where transactions are categorized as either fraudulent or non-fraudulent based on learned representations. The Benjamini-Hochberg procedure controls the false discovery rate, providing statistical guarantees on the proportion of false positives among detected cases. This approach addresses the limitations of traditional rule-based systems that rely on manually crafted thresholds and predefined patterns.

## Key Results
- Transformer approach detects 23.6% of fraudsters at 40% FDR vs 7.33% for LSTM and 0% for tabular data methods
- For non-fraudster detection, transformer achieves 96.77% detection at 3% FDR vs 84.88% for LSTM and 82.56% for tabular data
- Transformer achieves F1-score of 0.30 compared to 0.19 for LSTM and 0.13 for tabular approaches

## Why This Works (Mechanism)
The transformer architecture excels at capturing long-range dependencies and complex patterns in sequential data, which is crucial for detecting sophisticated money laundering schemes that often involve multiple transactions over extended periods. Contrastive learning enables the model to learn meaningful representations without labeled data, addressing the challenge of limited labeled examples in fraud detection. The two-threshold classification strategy combined with FDR control provides a principled approach to balancing detection performance with statistical guarantees on false positives, which is essential for regulatory compliance in financial applications.

## Foundational Learning
- **Contrastive Learning**: A self-supervised learning approach that learns representations by contrasting similar and dissimilar pairs of data points. Needed to learn meaningful representations without labeled examples, which are scarce in fraud detection.
- **Transformer Architecture**: Neural network architecture using self-attention mechanisms to process sequential data. Needed for its ability to capture long-range dependencies and complex patterns in transaction sequences.
- **Benjamini-Hochberg Procedure**: Statistical method for controlling the false discovery rate in multiple hypothesis testing. Needed to provide statistical guarantees on false positives, which is critical for regulatory compliance.
- **Time Series Analysis**: Techniques for analyzing sequential data with temporal dependencies. Needed to capture the temporal patterns and relationships in financial transactions.
- **Representation Learning**: Learning compressed representations that capture essential features of raw data. Needed to transform complex transaction data into meaningful features for classification.

## Architecture Onboarding

Component Map:
Raw transaction time series -> Transformer encoder -> Contrastive learning objective -> Learned representations -> Two-threshold classifier -> Final classification (fraudster/non-fraudster)

Critical Path:
The critical path involves contrastive learning of representations followed by the two-threshold classification strategy. The transformer encoder processes the raw time series, contrastive learning optimizes the representation space, and the classifier makes final decisions while controlling FDR.

Design Tradeoffs:
- Unsupervised representation learning vs. supervised approaches: Tradeoff between requiring labeled data vs. potentially less optimal representations
- Two-threshold vs. single-threshold classification: Tradeoff between detection flexibility vs. increased complexity
- FDR control vs. detection rate: Tradeoff between statistical guarantees and detection performance

Failure Signatures:
- Poor contrastive learning objective design could lead to representations that don't capture fraud-relevant patterns
- Suboptimal threshold selection could result in high false positive or false negative rates
- Model overfitting to training data could reduce generalization to new fraud patterns

First Experiments:
1. Validate contrastive learning effectiveness by measuring representation quality on a small labeled validation set
2. Test threshold selection sensitivity by varying FDR levels and measuring detection rates
3. Compare transformer performance against simpler architectures (RNN, CNN) on a subset of data

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but implicit questions include: How to adapt the method to different financial institutions with varying transaction patterns? What are the optimal hyperparameters for contrastive learning in this domain? How to handle concept drift in money laundering patterns over time?

## Limitations
- Performance still leaves room for improvement, with F1-score of 0.30 indicating significant false positive/negative rates
- Method requires substantial computational resources for training transformer models
- Generalization to different financial institutions and transaction patterns may require retraining or fine-tuning

## Confidence
High: Transformer architecture selection for sequential data, contrastive learning approach for representation learning, FDR control methodology
Medium: Threshold selection strategy, specific hyperparameter choices, computational efficiency claims
Low: Real-world deployment considerations, handling of concept drift, adaptation to different financial institutions

## Next Checks
1. Validate model performance on out-of-sample data from different time periods to assess temporal generalization
2. Conduct ablation studies to determine the contribution of contrastive learning vs. transformer architecture to performance gains
3. Test the method's robustness to concept drift by evaluating on data with evolving fraud patterns