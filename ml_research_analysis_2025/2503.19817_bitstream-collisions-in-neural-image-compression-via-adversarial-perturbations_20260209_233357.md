---
ver: rpa2
title: Bitstream Collisions in Neural Image Compression via Adversarial Perturbations
arxiv_id: '2503.19817'
source_url: https://arxiv.org/abs/2503.19817
tags:
- image
- images
- compression
- xadv
- compressed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Neural image compression (NIC) is vulnerable to bitstream collisions,
  where semantically different images produce identical compressed bitstreams. The
  authors propose a Masked Gradient Descent (MGD) attack algorithm that uses a dot
  mask to selectively perturb pixels during gradient updates, enabling the generation
  of collision images with up to 100% attack success rate across datasets and compression
  models.
---

# Bitstream Collisions in Neural Image Compression via Adversarial Perturbations

## Quick Facts
- arXiv ID: 2503.19817
- Source URL: https://arxiv.org/abs/2503.19817
- Reference count: 24
- Primary result: MGD attack achieves 100% bitstream collision success rate on NIC models at low quality factors

## Executive Summary
This paper demonstrates that neural image compression (NIC) is vulnerable to bitstream collisions, where semantically different images produce identical compressed bitstreams. The authors propose a Masked Gradient Descent (MGD) attack algorithm that uses a dot mask to selectively perturb pixels during gradient updates, enabling the generation of collision images with up to 100% attack success rate across datasets and compression models. A simple Limited-Precision Defense (LPD) mechanism, converting latent tensors to half-precision, effectively mitigates this vulnerability. Theoretical bounds show that collision image distances are bounded by √2 for orthogonal transforms and by √2C for NIC models, where C is the contraction constant.

## Method Summary
The MGD attack optimizes the distance between encoder outputs (∥f(x) - f(xtgt)∥) using a dot mask that allows only pixels at positions (h = i∆h + h₀, w = i∆w + w₀) to update. This prevents the adversarial image from visually converging to the target image while still achieving bitstream collision. The attack uses Adam optimizer with lr=0.03, Cosine Annealing scheduler, and early stopping when collision occurs. LPD defense converts model parameters or latent tensors to half-precision (float16), preventing exact gradient convergence needed for collisions. The approach is evaluated on CelebA, ImageNet, and Kodak datasets compressed with three NIC models: FP-GDN, FP-ReLU, and SH.

## Key Results
- MGD achieves 100% attack success rate on Factorized Prior models at quality factors ≤ 3
- LPD defense reduces ASR to 0% across all tested models and datasets
- Collision image distances in NIC exceed √2 bound for conventional compressors, reaching 0.71-1.89 L2 distance
- Attack fails at quality factors ≥ 5 for all NIC models tested

## Why This Works (Mechanism)

### Mechanism 1: Masked Gradient Descent (MGD) for Bitstream Collision
MGD bypasses non-differentiable quantization and entropy encoding by minimizing distance in the encoder's latent space. A dot mask zeros out most gradient elements, allowing only specific pixels to update. This prevents visual convergence to the target image while matching latent representations. Mask stride parameters (∆h, ∆w) are critical: too small causes visual convergence, too large prevents collision.

### Mechanism 2: Contraction Constant Enables Larger Collision Distances
NIC's contraction constant C = max ||x−y|| / ||f(x)−f(y)|| permits collision images with larger perceptual differences than conventional compressors. For NIC, collision image distance is bounded by √2·C versus √2 for conventional orthogonal transforms. This allows L2 distances of 0.71-1.89 between xadv and xtgt.

### Mechanism 3: Limited-Precision Defense (LPD) Breaks Gradient Convergence
Converting model parameters or latent tensors to half-precision introduces numerical noise that prevents the attack from achieving exact zero loss. The reduced precision interferes with the convergence required for bitstream collision, making the bitstream extremely sensitive to minor variations in latent representations.

## Foundational Learning

- **Neural Image Compression Pipeline**: Understanding where gradients flow and where they're blocked explains why MGD targets latent space directly. Quick check: Why can't standard backpropagation optimize through the full NIC pipeline for collision attacks?
- **Whitebox Adversarial Attacks**: MGD is a whitebox attack requiring full model knowledge; understanding this clarifies threat model assumptions. Quick check: What attacker capabilities distinguish whitebox from blackbox attacks in this context?
- **Rate-Distortion Tradeoff**: Attack success rate varies dramatically with quality factor; lower QF (higher compression) enables more collisions. Quick check: Why does ASR drop to 0% at QF ≥ 5 for Factorized Prior models?

## Architecture Onboarding

- **Component map**: Source Image → [MGD Optimizer + Dot Mask] → Adversarial Image → [NIC Encoder] → Latent → [Quantize + Entropy] → Bitstream
- **Critical path**: The encoder output distance ∥f(x) − f(xtgt)∥ must converge below quantization threshold. Mask parameters (∆h=3, ∆w=1) are empirically tuned to balance semantic preservation vs. collision success.
- **Design tradeoffs**: Lower QF → higher compression → more collision vulnerability; LPD → security but potential quality degradation; stricter masking → better semantic preservation but slower convergence.
- **Failure signatures**: Attack produces xadv ≈ xtgt visually (mask stride too small); attack never achieves collision (learning rate too low, iterations insufficient, or LPD active); high ASR on Factorized Prior but near-zero on Scale Hyperprior (architecture-dependent vulnerability).
- **First 3 experiments**: 1) Reproduce MGD attack on FP-GDN with CelebA at QF=1; verify 100% ASR and measure L2(xadv, xtgt) distribution. 2) Test transferability: generate xadv on FP-GDN, evaluate collision rate on FP-ReLU and SH models. 3) Implement LPD by converting encoder latents to float16; verify ASR drops to 0% while measuring reconstruction quality impact (MS-SSIM).

## Open Questions the Paper Calls Out

- **Q1**: Can designing NIC architectures with smaller contraction constants (C) provide inherent robustness against bitstream collision attacks while maintaining competitive rate-distortion performance?
- **Q2**: What is the impact of Limited-Precision Defense (LPD) on the compression quality, rate-distortion performance, and computational efficiency of NIC models in practical deployments?
- **Q3**: Are NIC models vulnerable to bitstream collision attacks under blackbox or limited-query threat models where attackers lack access to model gradients and parameters?
- **Q4**: Why does attack success rate collapse at quality factors ≥ 5, and can this threshold be systematically raised through architectural or training modifications?

## Limitations

- LPD effectiveness depends on attacker not having whitebox access to reduced-precision implementation
- Computational cost of MGD attack (up to 20,000 iterations) may limit practical deployment
- Theoretical bounds on contraction constants rely on empirical observations rather than analytical derivations

## Confidence

- **High Confidence**: MGD algorithm's 100% ASR on Factorized Prior models at QF ≤ 3, well-supported by quantitative results in Tables 1 and 2
- **Medium Confidence**: LPD defense mechanism's effectiveness demonstrated empirically but lacks theoretical justification for why float16 conversion specifically disrupts gradient convergence
- **Medium Confidence**: Theoretical bounds on collision distances (Theorem 3) are mathematically sound but rely on assumptions about normalization and quantization

## Next Checks

1. **Production LPD Robustness**: Implement LPD in a production-like environment with varying attacker access levels; measure ASR under whitebox vs. blackbox scenarios to quantify practical security margin.

2. **Cross-Architecture Transferability**: Generate collision images using MGD on FP-GDN, then test collision success rates when compressing with different NIC architectures to validate whether vulnerability is architecture-specific or pervasive.

3. **Defense Efficiency vs. Quality Tradeoff**: Systematically measure LPD's impact on normal compression performance (rate-distortion curves, MS-SSIM) across quality factors to determine precision reduction threshold where LPD remains effective without unacceptable quality degradation.