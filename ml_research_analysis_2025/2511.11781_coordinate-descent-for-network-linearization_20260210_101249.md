---
ver: rpa2
title: Coordinate Descent for Network Linearization
arxiv_id: '2511.11781'
source_url: https://arxiv.org/abs/2511.11781
tags:
- relu
- accuracy
- relus
- budget
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Coordinate Descent for Network Linearization,
  a method to reduce ReLU count in ResNet networks for Private Inference. The problem
  is that ReLUs are a major bottleneck in Private Inference due to latency, and reducing
  them is a discrete optimization challenge.
---

# Coordinate Descent for Network Linearization

## Quick Facts
- arXiv ID: 2511.11781
- Source URL: https://arxiv.org/abs/2511.11781
- Reference count: 35
- Reduces ReLU count in ResNet networks for Private Inference, outperforming SNL by up to 4.13% accuracy on CIFAR-100 at 6K ReLU budget

## Executive Summary
This paper addresses the computational bottleneck of ReLUs in Private Inference by introducing Coordinate Descent for Network Linearization (CDNL). The method directly optimizes binary ReLU masks using Block Coordinate Descent, avoiding the accuracy loss from L1 relaxation and hard thresholding. By iteratively sampling and removing ReLU subsets with minimal accuracy impact, and fine-tuning when necessary, CDNL achieves state-of-the-art results on CIFAR-10, CIFAR-100, and TinyImageNet with ResNet18 and WideResNet22-8 backbones. The approach naturally produces sparse solutions without relaxations and can improve existing methods like AutoRep, achieving the same accuracy with half the ReLU budget.

## Method Summary
The method employs Block Coordinate Descent to directly optimize binary ReLU masks rather than using L1 regularization with post-hoc thresholding. Starting from a baseline model (typically SNL or AutoRep) at a reference budget, the algorithm iteratively samples random subsets of ReLUs (DRC=100), evaluates their removal impact on training accuracy, and accepts the least damaging option. When accuracy degradation exceeds a threshold (ADT=0.3%), the network is fine-tuned for 20 epochs. This process continues until reaching the target ReLU budget, leveraging the assumption that optimal masks for different budgets significantly overlap (IoU > 0.85).

## Key Results
- Achieves 4.13% higher accuracy than SNL on CIFAR-100 at 6K ReLU budget
- Outperforms AutoRep by 2.39% accuracy at 30K ReLU budget on CIFAR-100
- Demonstrates that BCD can improve existing methods, achieving same accuracy as AutoRep with half the ReLU budget
- Shows consistent performance gains across CIFAR-10, CIFAR-100, and TinyImageNet datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Operating directly on discrete binary masks avoids the approximation error inherent in continuous relaxation methods (like $L_1$ regularization) which require a final "hard thresholding" step.
- **Mechanism:** The method employs Block Coordinate Descent (BCD) to optimize the binary mask $m \in \{0,1\}$ directly, rather than optimizing a continuous proxy $\alpha \in [0,1]$ and binarizing later. This prevents "information leakage" where gradient updates for accuracy rely on partially active ReLUs that disappear during thresholding.
- **Core assumption:** The optimal sparse mask can be approximated effectively through iterative greedy removal without differentiable relaxation.
- **Evidence anchors:** [abstract] "works directly in the discrete domain... yields a sparse solution by design." [introduction] "...differentiable nature of the optimization 'leaks' information... result, the hard thresholding stage at the end of optimization hurts accuracy considerably."

### Mechanism 2
- **Claim:** Performance degradation is minimized by evaluating removal impact on the training loss *before* committing to a mask update, combined with periodic fine-tuning.
- **Mechanism:** The algorithm samples a subset of ReLUs (size $DRC$), evaluates the accuracy drop, and only commits to the removal if the drop is within a tolerance ($ADT$). If the drop is too high, it selects the "least bad" removal from random trials and immediately invokes a fine-tuning phase to recover model weights.
- **Core assumption:** The network weights $\theta$ can adapt to the removal of a small block of ReLUs via standard fine-tuning (SGD/Adam) without catastrophic forgetting.
- **Evidence anchors:** [method] "...every time the accuracy drops below a predefined threshold, we fine-tune the network." [discussion] Eq. (3) provides a theoretical bound on the performance gap $P(m_T) - P(m^*)$ decreasing as iterations $T$ increase.

### Mechanism 3
- **Claim:** A "golden set" of ReLUs exists across different budget constraints, meaning optimal masks for higher budgets significantly overlap with those for lower budgets.
- **Mechanism:** The BCD strategy relies on the assumption that removing ReLUs is a monotonic process. The authors analyze SNL optimization paths and find high Intersection over Union (IoU > 0.85) between masks at different budget stages, justifying a sequential removal strategy.
- **Core assumption:** The importance of a ReLU is relatively stable as the budget shrinks, allowing greedy removal to approximate the global optimum.
- **Evidence anchors:** [ablation study] "Figure 6 shows... IoU scores are well above 0.85... number of shared ReLUs between the masks is high."

## Foundational Learning

- **Concept:** $L_1$ vs. $L_0$ Regularization (Relaxation)
  - **Why needed here:** The paper frames its contribution as solving the $L_0$ problem directly rather than relaxing it to $L_1$ (LASSO). You must understand that $L_0$ enforces exact sparsity (counting non-zeros) while $L_1$ encourages small values which are often thresholded to zero later.
  - **Quick check question:** Why does standard $L_1$ regularization fail to guarantee a specific sparsity count without a post-processing thresholding step?

- **Concept:** Block Coordinate Descent (BCD)
  - **Why needed here:** This is the core optimization engine. Unlike standard Gradient Descent which updates all parameters simultaneously, BCD optimizes a subset ("block") of variables (here, the ReLU masks) while holding others (weights) fixed, then swaps.
  - **Quick check question:** In this paper, what constitutes the "block" being optimized in each iteration vs. what is held constant?

- **Concept:** Private Inference (PI) Bottlenecks
  - **Why needed here:** The motivation for this work is computational, not just model compression. In PI (e.g., using Homomorphic Encryption), non-linear operations (ReLUs) are orders of magnitude more expensive (latency/bandwidth) than linear operations (Convolutions), driving the need for linearization.
  - **Quick check question:** Why is replacing a ReLU with the identity function (linearization) specifically beneficial for encrypted data inference protocols?

## Architecture Onboarding

- **Component map:** Pre-trained model -> BCD Mask Sampler -> Evaluator (Training Accuracy) -> Updater (Mask Selection) -> Finetuner (Weight Recovery) -> Updated model

- **Critical path:**
  1. Initialize mask $m$ from a baseline method (SNL/AutoRep).
  2. Loop until target budget $B_{target}$ is met:
     - Sample random ReLU block.
     - Evaluate $\Delta Acc$.
     - Select best candidate.
     - Update mask $m$.
     - If (current acc < baseline - ADT): Trigger Fine-tuning epochs.

- **Design tradeoffs:**
  - **DRC (Step Size):** Larger $DRC$ = faster optimization but higher risk of stranding the model in a poor local minimum (accuracy collapse). Smaller $DRC$ = finer granularity but slower convergence.
  - **ADT (Tolerance):** Higher tolerance = fewer fine-tuning cycles (faster) but risk of accuracy drift. Lower tolerance = frequent fine-tuning (slower, potentially better accuracy).

- **Failure signatures:**
  - **Accuracy Collapse:** Accuracy drops to random guessing, likely due to $DRC$ being too large for the remaining capacity.
  - **Budget Stalling:** The algorithm fails to reduce the budget further because every proposed removal exceeds the $ADT$, indicating the remaining ReLUs are all "critical."

- **First 3 experiments:**
  1. **Baseline Ablation (DRC):** Run the algorithm on ResNet18/CIFAR-10 with varying $DRC$ values (e.g., 50 vs. 100 vs. 200) to plot the accuracy-efficiency trade-off curve.
  2. **Mask Overlap Validation:** Replicate the IoU analysis (Fig 6) on a different architecture (e.g., WideResNet) to confirm that optimal masks do indeed nest/overlap before trusting the greedy removal strategy.
  3. **Hybrid Comparison:** Compare starting the BCD process from a randomly pruned network vs. an SNL-pre-trained network to validate the claim that bootstrapping from a "medium budget" model is more effective.

## Open Questions the Paper Calls Out

- **Question:** Would implementing a dynamic scheduler for the Delta ReLU Count (DRC) parameter improve optimization efficiency or final accuracy?
- **Basis in paper:** [explicit] The appendix acknowledges: "We acknoledge that a straightforward extension of our method would be to implement a scheduler for the ReLU decrease parameter."
- **Why unresolved:** The authors utilized a constant DRC (e.g., 100 ReLUs) throughout all experiments, demonstrating sufficiency but leaving potential optimization unexplored.
- **Evidence to resolve it:** Ablation studies comparing fixed DRC against scheduled variants (e.g., decaying DRC relative to the remaining budget) on convergence speed and test accuracy.

- **Question:** Does the "golden set" conjecture and Block Coordinate Descent approach generalize effectively to non-ResNet architectures like Vision Transformers?
- **Basis in paper:** [inferred] The experimental scope is explicitly restricted to ResNet18 and WideResNet22-8 backbones (Page 4).
- **Why unresolved:** The method relies on the observation that ReLU masks for different budgets overlap heavily (IoU > 0.85) in ResNets. It is unclear if this "golden set" property holds for attention-based mechanisms.
- **Evidence to resolve it:** Applying the method to Vision Transformers (ViT) or MLP-Mixers and analyzing the IoU dynamics of the resulting ReLU masks.

- **Question:** What is the training computational overhead of the iterative coordinate descent process compared to joint optimization methods?
- **Basis in paper:** [inferred] The method involves a loop of random sampling, evaluation, and fine-tuning (Algorithm 1), which is procedurally more complex than the single-pass training of SNL.
- **Why unresolved:** The paper focuses on inference latency (ReLU budget) and accuracy, but does not report wall-clock training time or total FLOPs required to reach the target budget.
- **Evidence to resolve it:** A comparative analysis of training duration and computational cost between the proposed method and baselines like SNL or AutoRep.

## Limitations

- The sampling strategy for ReLU blocks is not fully specified, which could affect reproducibility and performance consistency.
- The assumption of a stable "golden set" of ReLUs across budget levels may not generalize to architectures with more complex feature interactions.
- The computational cost of evaluating 50 random trials per iteration on the full training set is not addressed, suggesting potential approximations in practice.

## Confidence

- **High:** The empirical superiority of BCD over SNL and AutoRep at low ReLU budgets (e.g., 4.13% accuracy gain on CIFAR-100 at 6K ReLUs).
- **Medium:** The theoretical runtime-performance bound and its practical implications for BCD convergence.
- **Low:** The stability of the greedy removal strategy across diverse network architectures and data distributions.

## Next Checks

1. **Sampling Strategy Sensitivity:** Run the BCD algorithm with uniform vs. layer-weighted ReLU sampling to quantify the impact on final accuracy and budget reduction efficiency.
2. **Cross-Architecture Mask Overlap:** Measure IoU scores between masks generated by BCD on CIFAR-10 vs. TinyImageNet for the same architecture to test the generalizability of the "golden set" assumption.
3. **Hybrid Initialization Comparison:** Compare BCD performance when initialized from SNL vs. a randomly pruned network to isolate the benefit of starting from a "medium budget" pre-trained model.