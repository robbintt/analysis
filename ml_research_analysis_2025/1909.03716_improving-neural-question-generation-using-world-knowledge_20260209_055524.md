---
ver: rpa2
title: Improving Neural Question Generation using World Knowledge
arxiv_id: '1909.03716'
source_url: https://arxiv.org/abs/1909.03716
tags:
- entity
- question
- pages
- generation
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes incorporating world knowledge in the form of
  linked entities and fine-grained entity types to improve neural question generation.
  The approach enriches a sequence-to-sequence model with entity linking (mapping
  text entities to Wikipedia titles) and fine-grained entity typing (assigning hierarchical
  entity types).
---

# Improving Neural Question Generation using World Knowledge

## Quick Facts
- **arXiv ID:** 1909.03716
- **Source URL:** https://arxiv.org/abs/1909.03716
- **Reference count:** 24
- **Primary result:** Entity-based world knowledge improves neural QG by 1.37 absolute BLEU-4 on SQuAD and 1.59 on MS MARCO

## Executive Summary
This paper proposes incorporating world knowledge in the form of linked entities and fine-grained entity types to improve neural question generation. The approach enriches a sequence-to-sequence model with entity linking (mapping text entities to Wikipedia titles) and fine-grained entity typing (assigning hierarchical entity types). The proposed model achieves significant improvements over baseline models, with a 1.37 absolute BLEU-4 score increase on SQuAD and a 1.59 absolute BLEU-4 score increase on MS MARCO test datasets. The results demonstrate that entity-based world knowledge is effective for generating more human-like questions, with pretrained entity features showing particular effectiveness.

## Method Summary
The authors propose a sequence-to-sequence model with attention and copy mechanism that incorporates world knowledge through entity linking and fine-grained entity typing. They extract entity linking features using Wikification and fine-grained entity types using a 112-type classifier trained on FIGER. These features are concatenated with word embeddings and answer position embeddings, then fed into a two-layer Bi-LSTM encoder. The decoder generates questions using attention over the enriched encoder representations and a copy mechanism. The model is trained on SQuAD and MS MARCO datasets with specific filtering criteria.

## Key Results
- NQG + EL (pre) + FGET (pre) achieves 13.69 BLEU-4 on SQuAD (1.37 absolute improvement)
- Same model achieves 11.69 BLEU-4 on MS MARCO (1.59 absolute improvement)
- Pre-trained entity linking features outperform randomly initialized features due to joint word-entity training
- Entity linking is more effective than NER for entity detection in this task

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Pre-trained entity linking embeddings improve question generation by grounding words in external knowledge.
- **Mechanism:** Joint word-entity embeddings (Yamada et al., 2016) project both modalities into the same vector space, enabling the model to access semantic information associated with Wikipedia entities during encoding.
- **Core assumption:** Wikipedia titles contain semantic information relevant to generating appropriate interrogative words.
- **Evidence anchors:** "pretrained entity features showing particular effectiveness" and ablation study showing pre-trained entity linking outperforms randomly initialized features.

### Mechanism 2
- **Claim:** Fine-grained entity types provide hierarchical semantic information that helps generate more human-like questions.
- **Mechanism:** A classifier assigns one of 112 entity types to detected entities; these type embeddings are concatenated with word embeddings at each timestep.
- **Core assumption:** Fine-grained types (e.g., "actor," "movie") carry more discriminative information than coarse-grained NER categories.
- **Evidence anchors:** FGET features improve SQuAD more than MS MARCO, likely due to Wikipedia-based dataset similarity.

### Mechanism 3
- **Claim:** Concatenating multiple feature embeddings provides complementary signals that jointly improve question generation.
- **Mechanism:** Each token's final embedding combines word, answer position, entity link, and entity type features before feeding into Bi-LSTM encoder.
- **Core assumption:** Features are independent and additive; concatenation is sufficient to capture their interactions.
- **Evidence anchors:** Best performance achieved by combining all features; ablation studies show individual features contribute differently across datasets.

## Foundational Learning

- **Concept: Sequence-to-Sequence with Attention (Bahdanau et al., 2014)**
  - Why needed here: Base architecture for question generation; encoder produces hidden states, decoder generates tokens conditioned on attention-weighted context.
  - Quick check question: Can you explain how attention weights are computed from encoder hidden states and decoder state at each timestep?

- **Concept: Copy Mechanism (Gulcehre et al., 2016; See et al., 2017)**
  - Why needed here: Handles rare and unknown words by allowing the model to copy tokens directly from the source passage.
  - Quick check question: How does the copy mechanism decide between generating from vocabulary vs. copying from source?

- **Concept: Entity Linking / Wikification**
  - Why needed here: Core knowledge source; maps ambiguous text mentions to specific Wikipedia entities.
  - Quick check question: Why might "Central Intelligence" link to a movie page rather than a concept about cognition?

## Architecture Onboarding

- **Component map:** Input embeddings (Word ⊕ Answer Position ⊕ Entity Link ⊕ Entity Type) → 2-layer Bi-LSTM encoder → Attention → 2-layer LSTM decoder with Copy Mechanism → Output

- **Critical path:**
  1. Run entity linker on raw passage → E_link sequence
  2. Run FGET classifier on linked entities → E_fget sequence
  3. Concatenate embeddings → feed to Bi-LSTM encoder
  4. Decode with attention and copy mechanism

- **Design tradeoffs:**
  - Pre-trained vs. randomly initialized entity embeddings: Pre-trained wins but requires external resources
  - Entity linker vs. NER for entity detection: Linker outperforms NER but is slower
  - Beam search vs. greedy decoding: Beam helps SQuAD; greedy sufficient for MS MARCO

- **Failure signatures:**
  - BLEU-4 < 10: Likely missing answer position features or copy mechanism
  - Entity features hurt performance: Check entity linker accuracy
  - Large train-val gap: Vocabulary mismatch; check OOV rate

- **First 3 experiments:**
  1. Replicate s2s+Att baseline → NQG on SQuAD dev; target ~12.5 BLEU-4
  2. Add pre-trained entity linking embeddings; expect +0.7 BLEU-4 improvement
  3. Ablate entity detection method: Compare linker vs. NER for entity detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the performance improvement observed in automated metrics correlate with human judgments of question quality?
- Basis in paper: Authors state in conclusion that "human evaluation is required" to fully explore performance gains.
- Why unresolved: Study relies entirely on automated metrics, which may not capture "human-like" quality.
- What evidence would resolve it: Human evaluation study assessing fluency, relevance, and correctness of generated questions.

### Open Question 2
- Question: Does incorporating semantic relationships between entities provide further improvements over isolated entity features?
- Basis in paper: Conclusion suggests "information based on the relationships between entities" would be useful.
- Why unresolved: Current model uses entity identity and type independently without encoding relationships.
- What evidence would resolve it: Experiment with relation extraction features or graph embeddings connecting entities.

### Open Question 3
- Question: Is the effectiveness of fine-grained entity types dependent on domain similarity between target dataset and external knowledge base?
- Basis in paper: FGET improves SQuAD more than MS MARCO, possibly due to Wikipedia-based dataset similarity.
- Why unresolved: Unclear if drop in effectiveness on MS MARCO is inherent limitation or domain mismatch.
- What evidence would resolve it: Comparative analysis training entity typing on in-domain web data.

## Limitations

- **External Knowledge Dependency:** Performance relies heavily on entity linker quality and Wikipedia coverage, creating potential brittleness in domains with limited Wikipedia presence.
- **Data Processing Assumptions:** Exact preprocessing specifications (tokenization, normalization) are not detailed, making exact reproduction uncertain.
- **Metric Implementation:** Evaluation pipeline details (tokenization, smoothing, case sensitivity) are incompletely specified.

## Confidence

**High Confidence:**
- NQG baseline architecture is technically sound and well-established
- General approach of enriching QG with external knowledge features is theoretically valid

**Medium Confidence:**
- Specific performance numbers are likely reproducible but depend on incompletely specified details
- Relative effectiveness of pre-trained vs. randomly initialized features is supported by ablation study

**Low Confidence:**
- Claim that fine-grained entity typing outperforms coarse-grained NER may not generalize to different datasets
- Assertion that pre-trained features are more effective due to "joint training" is plausible but not empirically validated

## Next Checks

1. **Ablation on Entity Detection Method:** Replace entity linker with standard NER system on SQuAD dev; verify linker consistently outperforms NER across multiple runs.

2. **Feature Dimensionality Sensitivity:** Systematically vary entity linking and FGET embedding dimensions (100d, 300d, 500d) while keeping other components fixed.

3. **Domain Generalization Test:** Apply trained model to different QG dataset (NewsQA or TriviaQA) without fine-tuning entity features; measure transfer of performance gains.