---
ver: rpa2
title: Online Submission and Evaluation System Design for Competition Operations
arxiv_id: '2507.17730'
source_url: https://arxiv.org/abs/2507.17730
tags:
- competition
- evaluation
- system
- participants
- submission
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents an online competition system designed to automate
  submission and evaluation processes for algorithmic competitions. The system addresses
  the operational burden of managing large volumes of submissions and compatibility
  issues arising from participants developing solutions in diverse environments.
---

# Online Submission and Evaluation System Design for Competition Operations

## Quick Facts
- arXiv ID: 2507.17730
- Source URL: https://arxiv.org/abs/2507.17730
- Reference count: 6
- Primary result: Successfully deployed online competition system for algorithmic competitions using Docker isolation, git-based submission, and real-time leaderboards

## Executive Summary
This paper presents an online competition system designed to automate submission and evaluation processes for algorithmic competitions. The system addresses operational challenges of managing large volumes of submissions and compatibility issues arising from diverse participant development environments. By leveraging Docker containers for isolated execution, git repositories for code management, and a web application for participant interaction, the system enables instant feedback and efficient resource management. The architecture has been successfully deployed for three distinct competitions: an AI planning teaching unit, the Grid-Based Path Planning Competition (GPPC2), and the League of Robot Runners competition.

## Method Summary
The system implements a git-based submission workflow where participants push code to provisioned repositories and initiate evaluation through a web interface. Submissions are tracked in a MongoDB database and evaluated in isolated Docker containers with enforced resource limits (CPU, memory, disk, network). An evaluation server monitors the database for new submissions, fetches code, and queues jobs to computing units. For some competitions, Slurm workload manager enables horizontal scaling across computing units, with AWS ParallelCluster dynamically provisioning EC2 instances during peak demand. The architecture supports diverse evaluation requirements from simple correctness checks to complex multi-metric leaderboards.

## Key Results
- Successfully deployed for three competitions with different evaluation requirements and workflows
- Enabled instant feedback through automated evaluation and real-time leaderboard updates
- Achieved fair evaluation through Docker-based sandboxing and resource limitation
- Handled peak submission volumes of 35 per day with dynamic computing unit allocation

## Why This Works (Mechanism)

### Mechanism 1: Environment Isolation via Containerization
- Claim: Docker-based isolation appears to reduce compatibility issues arising from diverse participant development environments.
- Mechanism: Submissions execute in Docker containers built from a base image with required dependencies. Participants can specify additional packages via APT, and the system provides a local build script for pre-submission debugging. Resource limits (CPU, memory, disk, network) are enforced through container configuration.
- Core assumption: Participant code does not depend on hardware-specific optimizations or kernel-level features unavailable in containers.
- Evidence anchors:
  - [abstract] "participants typically develop their solutions in diverse environments, leading to compatibility issues during the evaluation of their submissions"
  - [section: Docker as a Sandbox] "computing units run the evaluation jobs in a Docker container... configured to limit the resources available to the evaluation jobs"
  - [corpus] No direct corpus support for Docker-specific mechanisms; neighbor papers focus on competition design, not infrastructure.
- Break condition: If submissions require GPU acceleration, specialized hardware, or container-incompatible system calls, isolation may fail or require significant architectural extension.

### Mechanism 2: Asynchronous Job Decoupling
- Claim: Separating the web submission layer from evaluation execution appears to enable continuous submission acceptance under variable load.
- Mechanism: The web app creates submission entries in MongoDB; the evaluation server polls for new entries, fetches code from git repositories, and queues jobs. Computing units execute jobs independently and write results back to the database. This decoupling allows the submission interface to remain responsive even when evaluation queues are full.
- Core assumption: The database can handle concurrent reads/writes from the web app, evaluation server, and multiple computing units without becoming a bottleneck.
- Evidence anchors:
  - [abstract] "The competition system allows organisers to manage large numbers of submissions efficiently, utilising isolated environments to evaluate submissions"
  - [section: System Architecture] "The evaluation server is responsible for monitoring new submissions and initiating the evaluation jobs when new submissions are found"
  - [corpus] Weak corpus connection; neighbor papers address competition fairness and review processes, not architectural decoupling.
- Break condition: If database contention emerges under high submission volume (e.g., peaks of 35/day mentioned), the polling mechanism may introduce latency or require indexing optimization.

### Mechanism 3: Horizontal Scaling via Workload Managers
- Claim: Integration with Slurm workload manager appears to enable dynamic scaling of evaluation capacity based on demand.
- Mechanism: The evaluation server submits jobs to Slurm, which schedules them across computing units. For the League of Robot Runners, AWS ParallelCluster dynamically provisions EC2 instances (scaling to 12 units during peaks). Results are written to MongoDB; raw outputs backup to S3.
- Core assumption: Evaluation jobs are sufficiently independent that parallel execution does not introduce resource contention or measurement noise.
- Evidence anchors:
  - [abstract] Not explicitly mentioned in abstract.
  - [section: Applications - League of Robot Runners] "The evaluation server is connected to a cluster of computing units managed by Slurm Workload Manager... computing units are dynamically allocated upon the demand of jobs"
  - [corpus] AlgoPerf competition paper discusses workload-agnostic benchmarking, suggesting similar scaling concerns in other competitions, but no direct architectural overlap.
- Break condition: If precise runtime measurement is required (as in GPPC2), parallel execution may introduce interference; the paper notes GPPC2 uses a single dedicated benchmarking machine for this reason.

## Foundational Learning

- **Concept: Docker containerization and resource constraints**
  - Why needed here: The system's core isolation strategy depends on understanding how to build images, configure resource limits, and debug environment mismatches between participant local builds and evaluation containers.
  - Quick check question: Can you explain why providing participants with the same Dockerfile used in evaluation reduces debugging friction?

- **Concept: Git-based version control and commit hashing**
  - Why needed here: The system tracks submissions via commit hashes stored in the database, enabling reproducibility and post-competition archives. Understanding git internals is necessary to implement the code-fetching mechanism correctly.
  - Quick check question: How would you retrieve the exact code version evaluated for a submission given only a commit hash and repository URL?

- **Concept: Polling vs. event-driven architectures**
  - Why needed here: The evaluation server "monitors new submissions" from the database. Understanding the tradeoffs between polling intervals and event notification (e.g., database change streams) is relevant for optimizing responsiveness under load.
  - Quick check question: What happens to submission latency if the evaluation server polls every 30 seconds and 10 submissions arrive in quick succession?

## Architecture Onboarding

- **Component map:**
  - Git Host (GitHub/Bitbucket) -> Web App -> MongoDB -> Evaluation Server -> Computing Units (Docker containers)

- **Critical path:**
  1. Participant authenticates via web app (GitHub OAuth or password)
  2. Participant pushes code to assigned git repository
  3. Participant clicks "start evaluation" â†’ web app creates submission entry in MongoDB
  4. Evaluation server detects new entry, fetches code, records commit hash
  5. Evaluation server queues job to computing unit (direct or via Slurm)
  6. Computing unit builds Docker container, runs evaluation script, writes results to MongoDB
  7. Web app displays results on leaderboard and submission history

- **Design tradeoffs:**
  - Single vs. multi-stage evaluation: GPPC2 requires precompute + benchmark stages on separate machines; League of Robot Runners uses single-stage evaluation on cluster.
  - Debugging transparency vs. evaluation secrecy: GPPC2 exposes debug instance logs; League of Robot Runners hides all evaluation output.
  - Centralized database as message bus simplifies architecture but may become contention point under high concurrency.

- **Failure signatures:**
  - Submission stuck in "pending" state: Likely evaluation server not polling or computing units offline.
  - Inconsistent runtime measurements: May indicate parallel job interference; consider dedicated benchmarking machine.
  - Participant code fails to build: Check if participant-specified dependencies are available; verify local Docker script matches evaluation environment.
  - Leaderboard not updating: Database write from computing unit may have failed; check evaluation script result submission logic.

- **First 3 experiments:**
  1. Submit a minimal "hello world" entry and trace the full path from git push to leaderboard update, measuring latency at each stage.
  2. Intentionally submit code that exceeds memory/CPU limits and verify the Docker container enforces constraints and reports failure correctly.
  3. Simulate concurrent submissions (e.g., 5 within 1 minute) and observe evaluation server behavior, polling latency, and job queue ordering.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the current separate implementations be modularized into a unified codebase that supports diverse competition workflows?
- Basis in paper: [explicit] The Conclusion states the target of future work is to "better modularise and generalise the implementation" so common components can be shared and open-sourced.
- Why unresolved: The three current applications (teaching, GPPC, League of Robot Runners) are maintained separately, increasing maintenance workload and difficulty of adding features.
- What evidence would resolve it: The release of a single, documented open-source repository capable of replicating the functionality of all three current applications without requiring code forks.

### Open Question 2
- Question: What automated mechanisms can effectively reduce the reliance on manual auditing for competition security?
- Basis in paper: [inferred] The "Auditing and Rules" section notes that technical measures to prevent cheating are limited and the system ultimately relies on organizers manually reviewing code to detect malicious activities.
- Why unresolved: The paper admits that the cost of technically prohibiting all cheating is high, leaving a gap that requires labor-intensive human intervention.
- What evidence would resolve it: Integration of automated static analysis or runtime behavior monitoring that successfully identifies malicious submissions without organizer input.

### Open Question 3
- Question: How can the system be adapted to support non-standard evaluation workflows without requiring extensive architectural changes?
- Basis in paper: [inferred] The paper mentions that customizing platforms like CodaLab for non-standard workflows is hard, and the authors' system currently requires system-wide modifications to handle different evaluation output data.
- Why unresolved: While the architecture is designed to be lightweight, the transition between different competitions (e.g., GPPC to League of Robot Runners) still required changes across the system.
- What evidence would resolve it: A plugin-based architecture where new evaluation metrics and workflow logic can be injected via configuration files rather than direct code modification.

## Limitations

- System effectiveness claims are based on deployment experience rather than controlled experiments
- Docker isolation robustness untested against GPU-dependent submissions or kernel-level optimizations
- Database polling architecture scalability under sustained high-volume loads remains unvalidated
- Resource limit enforcement may not prevent sophisticated submissions from detecting or circumventing constraints

## Confidence

- **High confidence**: The system architecture design (git-based submission, Docker isolation, MongoDB communication layer) is clearly specified and implemented.
- **Medium confidence**: Claims about reduced compatibility issues and fair evaluation are supported by deployment experience but lack comparative metrics or controlled validation.
- **Low confidence**: Scaling claims under peak load are based on single competition data without stress testing or comparative performance benchmarks.

## Next Checks

1. **Docker isolation testing**: Submit code that attempts to detect or bypass resource limits (e.g., querying CPU count, memory usage, or network access) and verify the container prevents such detection.

2. **Database contention analysis**: Simulate 50 concurrent submissions with rapid polling intervals and measure database query latency, contention rates, and polling server response times.

3. **Resource measurement accuracy**: Run identical submissions under varying parallel loads and measure runtime/memory variations to quantify interference effects and determine whether dedicated benchmarking machines are necessary for precise measurements.