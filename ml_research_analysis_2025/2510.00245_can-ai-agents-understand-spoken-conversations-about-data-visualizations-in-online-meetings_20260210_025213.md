---
ver: rpa2
title: Can AI agents understand spoken conversations about data visualizations in
  online meetings?
arxiv_id: '2510.00245'
source_url: https://arxiv.org/abs/2510.00245
tags:
- data
- about
- visualizations
- these
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates how well AI agents understand spoken conversations
  about data visualizations in online meetings. The authors introduce a dual-axis
  testing framework that measures comprehension across three levels of complexity
  (factual recall, data interpretation, causal analysis) and six topic categories.
---

# Can AI agents understand spoken conversations about data visualizations in online meetings?

## Quick Facts
- **arXiv ID**: 2510.00245
- **Source URL**: https://arxiv.org/abs/2510.00245
- **Reference count**: 18
- **Primary result**: Text-only LLM pipeline achieved 96% accuracy, outperforming other configurations in understanding spoken conversations about data visualizations

## Executive Summary
This paper evaluates how well AI agents understand spoken conversations about data visualizations in online meetings. The authors introduce a dual-axis testing framework that measures comprehension across three levels of complexity (factual recall, data interpretation, causal analysis) and six topic categories. Using this framework, they test four pipelines (LLM vs VLM, image vs text vs hybrid inputs) on a novel corpus of 72 spoken dialogues about movie dataset visualizations, with 318 benchmark questions.

Key findings show the text-only LLM pipeline achieved 96% accuracy, significantly outperforming other configurations including the text-only VLM (72.4%), image-only VLM (70%), and hybrid VLM (68%). This suggests structured textual inputs (visualization code) enable better AI understanding of meeting discussions than images alone. The results highlight the importance of input modality and model architecture for AI meeting assistants, with implications for designing tools that embed visualization metadata in text form for effective AI support.

## Method Summary
The authors developed a novel evaluation framework for AI comprehension of spoken conversations about data visualizations in online meetings. They created a corpus of 72 dialogues discussing movie dataset visualizations and formulated 318 benchmark questions across three complexity levels: factual recall, data interpretation, and causal analysis. Four different AI pipelines were tested: text-only LLM, text-only VLM, image-only VLM, and hybrid VLM. The pipelines were evaluated using a dual-axis framework measuring both comprehension depth and topic coverage. Performance was measured through accuracy scores on the benchmark questions.

## Key Results
- Text-only LLM pipeline achieved 96% accuracy, significantly outperforming other configurations
- Text-only VLM achieved 72.4% accuracy, image-only VLM achieved 70% accuracy, and hybrid VLM achieved 68% accuracy
- Structured textual inputs (visualization code) enabled better AI understanding of meeting discussions than images alone

## Why This Works (Mechanism)
The superior performance of the text-only LLM pipeline appears to stem from the structured nature of textual inputs providing more comprehensive information than images alone. When visualization code is available in text form, the LLM can directly access the underlying data structure, labels, and relationships without needing to infer them from visual representations. This direct access to semantic information enables more accurate comprehension of the discussion context. The results suggest that for AI meeting assistants to effectively understand spoken conversations about visualizations, having access to the visualization's underlying data and structure in text format is more valuable than visual representations alone.

## Foundational Learning
- **LLM vs VLM architecture**: Understanding the difference between language models (LLM) and vision-language models (VLM) is crucial, as they process information differently and have different strengths in handling visualization data
- **Input modality impact**: The form of input (text, image, or hybrid) significantly affects model performance, with structured text enabling more accurate comprehension than visual representations
- **Dual-axis evaluation framework**: The testing framework that measures both complexity levels (factual recall, interpretation, causal analysis) and topic coverage provides a comprehensive assessment of AI comprehension capabilities
- **Visualization metadata importance**: Embedding visualization metadata in text form appears critical for AI meeting assistants to effectively understand and respond to spoken discussions about data visualizations
- **Benchmark question design**: Carefully crafted questions across different complexity levels are essential for accurately evaluating AI comprehension of visualization discussions
- **Corpus construction methodology**: Creating a representative corpus of spoken dialogues about visualizations requires careful consideration of domain, visualization types, and discussion complexity

## Architecture Onboarding

**Component Map**: User Dialogue -> Preprocessor -> Model Pipeline (LLM/VLM) -> Question Answerer -> Accuracy Evaluator

**Critical Path**: The evaluation pipeline flows from spoken dialogue input through preprocessing, model inference, question answering, and accuracy evaluation. The critical path for achieving high accuracy appears to be maintaining structured textual information about visualizations throughout the pipeline.

**Design Tradeoffs**: The study reveals a fundamental tradeoff between input modality and comprehension accuracy. While image-only and hybrid approaches seem more intuitive for handling visual content, text-only approaches leveraging visualization code achieve superior results. This suggests that the richness of structured textual information outweighs the visual context provided by images.

**Failure Signatures**: Models relying on image inputs show consistent performance degradation, particularly on questions requiring detailed data interpretation or causal analysis. The hybrid approach performs worst, suggesting that combining modalities without proper integration can confuse rather than enhance comprehension.

**First Experiments**:
1. Test the text-only LLM pipeline on a different domain (e.g., business meetings) to validate generalizability
2. Evaluate the impact of visualization complexity on model performance by testing with increasingly complex charts and graphs
3. Implement an ablation study where visualization code is progressively simplified to identify the minimum information required for accurate comprehension

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation corpus consists of only 72 dialogues about movie dataset visualizations, limiting generalizability to broader meeting contexts
- The relatively small sample size (318 benchmark questions) and focus on a specific domain may not represent real-world meeting diversity
- The study does not conclusively isolate why the text-only LLM performs best, leaving room for alternative explanations beyond structured textual inputs

## Confidence
- **High confidence**: The comparative performance ranking of the four pipelines (text-only LLM > text-only VLM > image-only VLM > hybrid VLM) is well-supported by the experimental data and methodology
- **Medium confidence**: The interpretation that structured textual inputs (visualization code) are the primary reason for the text-only LLM's superior performance, as alternative explanations exist
- **Medium confidence**: The implications for designing AI meeting assistants, as these depend on the generalizability of the findings beyond the specific context studied

## Next Checks
1. Replicate the study with a larger and more diverse corpus of spoken conversations covering different visualization types (charts, graphs, dashboards) and meeting domains beyond movies
2. Conduct ablation studies to isolate the specific factors contributing to the text-only LLM's performance advantage, including testing with visualization images paired with varying levels of descriptive metadata
3. Implement a longitudinal study tracking the performance of these pipelines on real-world meeting data over time to assess consistency and identify potential edge cases or failure modes