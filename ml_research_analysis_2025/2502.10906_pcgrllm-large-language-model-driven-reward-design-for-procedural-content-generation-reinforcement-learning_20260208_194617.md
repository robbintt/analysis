---
ver: rpa2
title: 'PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content
  Generation Reinforcement Learning'
arxiv_id: '2502.10906'
source_url: https://arxiv.org/abs/2502.10906
tags:
- reward
- function
- generation
- feedback
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of designing reward functions
  for procedural content generation in games, which traditionally requires substantial
  domain expertise and manual effort. The authors propose PCGRLLM, an improved architecture
  that leverages large language models (LLMs) to generate reward functions from textual
  instructions.
---

# PCGRLLM: Large Language Model-Driven Reward Design for Procedural Content Generation Reinforcement Learning

## Quick Facts
- arXiv ID: 2502.10906
- Source URL: https://arxiv.org/abs/2502.10906
- Reference count: 40
- Primary result: Iterative LLM-driven reward refinement improves procedural level generation accuracy by 415% vs baselines

## Executive Summary
This paper addresses the challenge of designing reward functions for procedural content generation in games, which traditionally requires substantial domain expertise and manual effort. The authors propose PCGRLLM, an improved architecture that leverages large language models (LLMs) to generate reward functions from textual instructions. The framework employs a feedback mechanism and reasoning-based prompt engineering techniques, iteratively refining reward functions based on generated content and self-alignment to the environment. Experiments on a 2D level generation task show significant performance improvements of 415% and 40% compared to baselines, depending on the LLM's zero-shot capabilities, demonstrating the potential to reduce human dependency in game AI development.

## Method Summary
PCGRLLM is a three-step iterative framework that uses LLMs to automatically generate reward functions for procedural content generation via reinforcement learning. The system takes textual instructions (stories) as input and outputs Python code defining reward functions for a PCGRL agent. The core loop consists of: (1) Reward Refinement where the LLM generates code based on the story, (2) Self-Alignment where the LLM tunes reward scales using statistics from random agent rollouts, and (3) Feedback where the LLM analyzes generated levels to identify discrepancies and refine the reward code. The framework employs Tree-of-Thought prompt engineering to maintain an archive of reward functions and select the best nodes for expansion. PPO training is used for the agent with 50M timesteps per iteration, and a heuristic evaluation metric based on pathfinding and enemy detection is used to assess level quality.

## Key Results
- PCGRLLM achieves 415% improvement in accuracy over zero-shot baselines using specific feedback
- Tree-of-Thought architecture outperforms Chain-of-Thought and Graph-of-Thought with 0.329 accuracy
- Self-alignment mechanism provides stable reward signals and prevents sparse reward problems
- LLM self-evaluation performs worse than heuristic evaluation (-0.039 delta)

## Why This Works (Mechanism)

### Mechanism 1: Iterative Feedback Loop
The system closes the loop between generated content and reward code via specific feedback. The LLM observes levels generated by the trained agent, compares them against the initial text instruction, and generates textual feedback identifying discrepancies (e.g., "too many spiders"). This feedback conditions the LLM to revise the reward code in the next iteration. The mechanism relies on the LLM's spatial reasoning to analyze level representations and causal reasoning to trace content errors back to specific reward code lines. Evidence shows that "Specific" feedback drives accuracy gains while "Generic" feedback results in stagnation.

### Mechanism 2: Self-Alignment
Reward functions are grounded in environment dynamics through self-alignment. Before training the main generator, the system executes the generated reward function with a random agent, gathers statistics (mean, variance, zero-value percentage), and prompts the LLM to adjust weights or formulas. This prevents sparse reward problems or exploding gradients that would otherwise halt learning. The mechanism assumes a random policy provides sufficient coverage of the state space to estimate the scale and distribution of the reward signal.

### Mechanism 3: Tree-of-Thought Search
Structured search strategies outperform linear reasoning for non-monotonic code optimization. Instead of a single chain of revisions, the architecture employs Tree-of-Thought, maintaining an archive of reward functions and selecting the "parent" with the highest fitness score to expand. This allows backtracking if a revision degrades performance. The mechanism assumes the fitness function is a reliable proxy for human preference, allowing the system to select the best node to expand.

## Foundational Learning

### Concept: Procedural Content Generation via RL (PCGRL)
**Why needed:** The paper assumes understanding that level generation is framed as a Reinforcement Learning problem where an agent modifies a grid (state) to maximize a reward, rather than a generative model like a GAN.
**Quick check:** In PCGRL, does the agent receive a reward for the final image quality or for state transitions that improve level metrics?

### Concept: Reward Shaping & Sparsity
**Why needed:** The core problem solved is automating "reward shaping"â€”balancing dense vs. sparse signals. Understanding that a reward of 0 for 100 steps (sparse) makes learning impossible is crucial for the "Self-Alignment" mechanism.
**Quick check:** Why might a reward function that only gives +1 for a "perfect level" fail to train an agent?

### Concept: Prompt Engineering (ToT vs. CoT)
**Why needed:** The paper evaluates the architecture using different reasoning structures. One must understand that "Chain-of-Thought" is linear while "Tree-of-Thought" explores multiple branches.
**Quick check:** Which prompt engineering technique allows an LLM to "backtrack" if a generated code snippet fails?

## Architecture Onboarding

### Component map:
Instruction -> (1) Refine Code (LLM) -> (2) Self-Align (Random Rollout) -> (3) Train Agent (PPO) -> (4) Generate Level -> (5) Evaluate Fitness -> (6) Generate Feedback (LLM) -> Update Archive

### Critical path:
The architecture executes a 3-step loop: (1) Reward Refinement where LLM generates code, (2) Self-Alignment where LLM tunes scales using random rollout statistics, and (3) Feedback where LLM analyzes trained policy output. PPO training is used for the agent with 50M timesteps per iteration, and a heuristic evaluation metric based on pathfinding and enemy detection is used to assess level quality.

### Design tradeoffs:
- **Heuristic vs. LLM Evaluation:** The paper explicitly warns against using the LLM to evaluate the fitness of its own content. LLM self-evaluation resulted in negative performance (-0.039 delta). Use deterministic heuristics for node selection (ToT) and LLMs only for code generation/critique.
- **ToT vs. GoT:** Graph-of-Thought (GoT) performed poorly compared to ToT. The authors suggest providing excessive "auxiliary information" (best/worst nodes) distracts the LLM. Stick to ToT for this specific coding task.

### Failure signatures:
- **Hallucinated Logic:** LLM generates code that looks syntactically correct but semantically checks for the wrong tile IDs
- **Reward Hacking:** The agent generates a level with 500 bats if the reward function accidentally rewards "count of enemies" without a penalty cap
- **Stagnation:** Accuracy plateaus if "Generic" feedback is used instead of "Specific" feedback tied to level features

### First 3 experiments:
1. **Baseline Verification:** Run the "Zero-shot" (Base) condition to establish that the LLM cannot solve the task without the feedback/alignment loop (Target: ~0.03 accuracy)
2. **Feedback Ablation:** Compare "No Feedback" vs. "Specific Feedback" over 6 iterations to replicate the 415% improvement curve (Fig. 6)
3. **Evaluator Stress Test:** Swap the Heuristic Fitness function for an LLM-based scorer to verify the "negative delta" finding (Table III), confirming that LLMs currently cannot reliably self-evaluate generated game levels

## Open Questions the Paper Calls Out

### Open Question 1
**How can objective assessment methods be developed to enable reliable LLM self-evaluation for reward refinement?**
The authors state, "Future work aims to enhance content evaluation performance by developing objective assessment methods... to achieve more balanced score distributions." This is unresolved because current LLM self-evaluation leads to a performance decline (-0.039 average) compared to heuristics due to scoring inaccuracies and hallucinations. Integration of techniques (e.g., few-shot retrieval) that allow LLM evaluation accuracy to statistically match the oracle heuristic would resolve this.

### Open Question 2
**How can the quantity and quality of auxiliary information be optimized in Graph-of-Thoughts (GoT) to prevent reasoning degradation?**
The paper finds GoT underperformed ToT, suggesting that "providing excessive information does not necessarily enhance the effectiveness [as the LLM] may struggle to reason logically when overloaded." It is unclear how to automatically select or summarize auxiliary thoughts to aid rather than confuse the LLM during complex reasoning tasks. Identifying a specific auxiliary selection strategy that allows GoT to outperform ToT would resolve this.

### Open Question 3
**Does the feedback-based architecture generalize to 3D level generation or complex game states beyond 2D grids?**
The experiments were limited to a "two-dimensional environment" using a 16x16 matrix with a limited tile set. Reasoning about 3D spatial relationships and increased state complexity poses significantly higher challenges for LLM token limits and spatial understanding. Successful application to 3D environments (e.g., Minecraft) with convergence metrics comparable to the 2D results would resolve this.

## Limitations
- Performance claims depend heavily on LLM-generated code quality and ability to interpret visual level representations for feedback generation
- The approach is demonstrated only on a specific 2D dungeon generation task with 7 tile types, limiting generalization to other game genres
- The 5x5 convolution-based evaluation metric for enemy detection may not scale to games with more sophisticated pathfinding or level structures

## Confidence
- **High Confidence:** The core mechanism of iterative feedback loops improving reward function quality is well-supported by experimental results showing clear performance differences between "Specific" and "Generic" feedback conditions
- **Medium Confidence:** The Tree-of-Thought architecture's superiority over Chain-of-Thought is demonstrated, though the evaluation metric's sensitivity to different reward function behaviors could influence these results
- **Medium Confidence:** Self-alignment's effectiveness in preventing reward signal sparsity is supported by the data, but the random agent's ability to provide meaningful coverage of the state space may vary with different reward function designs

## Next Checks
1. **Generalization Test:** Apply PCGRLLM to a different PCGRL environment (e.g., 3D platformer levels or platformer-style levels) to assess performance consistency across game genres
2. **Prompt Robustness Analysis:** Systematically vary the level representation format (text vs. image) and feedback specificity levels to quantify the feedback mechanism's sensitivity to input format changes
3. **Long-term Training Evaluation:** Extend the training duration beyond 6 feedback iterations to determine whether performance improvements plateau or if diminishing returns set in after initial refinements