---
ver: rpa2
title: Automatic Prompt Optimization for Dataset-Level Feature Discovery
arxiv_id: '2601.13922'
source_url: https://arxiv.org/abs/2601.13922
tags:
- feature
- prompt
- features
- optimization
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel approach to automatic feature discovery
  from unstructured text by framing it as a dataset-level prompt optimization problem.
  Instead of relying on hand-crafted prompts or fixed feature schemas, the authors
  propose a multi-agent framework where language model agents jointly propose feature
  definitions, extract feature values, and evaluate feature quality using dataset-level
  performance and interpretability feedback.
---

# Automatic Prompt Optimization for Dataset-Level Feature Discovery

## Quick Facts
- arXiv ID: 2601.13922
- Source URL: https://arxiv.org/abs/2601.13922
- Reference count: 16
- Key outcome: Novel approach to automatic feature discovery from unstructured text using dataset-level prompt optimization with multi-agent framework, outperforming previous LM-based methods and per-example prompt optimizers, particularly with small language models for sentiment classification and toxicity detection tasks.

## Executive Summary
This paper introduces a novel approach to automatic feature discovery from unstructured text by framing it as a dataset-level prompt optimization problem. Instead of relying on hand-crafted prompts or fixed feature schemas, the authors propose a multi-agent framework where language model agents jointly propose feature definitions, extract feature values, and evaluate feature quality using dataset-level performance and interpretability feedback. Instruction prompts are iteratively refined based on structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions. The approach is specifically designed to enable the use of Small Language Models (SLMs) for efficient feature discovery.

## Method Summary
The method frames feature discovery as a dataset-level prompt optimization task where language model agents iteratively propose, extract, and evaluate features using aggregate performance metrics and interpretability feedback. The framework consists of four modules: FeatureProposer generates feature definitions from instructions and example texts, Extractor fills the feature schema for each text, InterpretabilityScorer evaluates feature readability and penalizes label leakage, and ReflectiveProposer refines instructions based on feedback. Bayesian optimization searches over instruction/example-set pairs to maximize a combined score of F1 and interpretability, specifically designed to work with small language models for efficient feature discovery.

## Key Results
- Outperforms previous LM-based feature discovery approaches and current prompt optimizers designed for per-example feedback
- Demonstrates effectiveness in sentiment classification on financial news and toxicity detection in user-AI conversations
- Shows dataset-level feedback signals enable discovery of globally useful features that per-example optimization cannot achieve

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dataset-level feedback signals enable discovery of globally useful features that per-example optimization cannot achieve.
- Mechanism: Instead of optimizing prompts for individual predictions, the system evaluates feature sets against aggregate metrics (F1, SHAP importance, mutual information, coverage) computed across an annotation dataset. Bayesian optimization then searches over instruction/example-set pairs to maximize a combined score. This forces the optimizer to find features that generalize across the corpus rather than overfitting to individual instances.
- Core assumption: Features that discriminate well at the dataset level will transfer to unseen data; the annotation split is representative of the target distribution.
- Evidence anchors: [abstract] "Instruction prompts are iteratively refined based on structured feedback, enabling optimization over prompts that induce shared feature sets rather than per-example predictions." [Section 3.2] "We propose, instead, a dataset-level feedback setup in which feedback is provided on a value computed across a set of elements – in our case, the F1 score of a linear classifier, alongside feature importance scores obtained through SHAP, mutual information and feature coverage scores."

### Mechanism 2
- Claim: Multi-agent specialization with structured textual feedback outperforms scalar-only feedback for prompt refinement.
- Mechanism: The REFLECTIVEPROPOSER receives textual feedback from both PERFORMANCEFEEDBACK (e.g., "feature X has low SHAP importance, consider more discriminative properties") and INTERPRETABILITYSCORER (e.g., "feature Y is too abstract, ground it in observable text"). This rich signal allows the proposer to make targeted instruction modifications rather than generic rewrites. Multiple feedback rounds (Nfb) iteratively sharpen constraints.
- Core assumption: The underlying LM has sufficient instruction-following capability to translate textual feedback into meaningful prompt refinements.
- Evidence anchors: [abstract] "Instruction prompts are iteratively refined based on structured feedback, enabling optimization over prompts that induce shared feature sets." [Section 5, Results] "Inspecting the prompts reveals qualitative differences: the first refinement mainly adds coarse, domain-level constraints that are absent from the initial prompt, while later refinements become increasingly specific about what constitutes a useful feature and what should be avoided."

### Mechanism 3
- Claim: Interpretability regularization prevents degenerate "label leakage" features that would artificially inflate performance.
- Mechanism: The INTERPRETABILITYSCORER explicitly penalizes features that encode the target label (e.g., "sentiment_label categorical" for sentiment classification) and rewards features that are readable, grounded in source text, and human-understandable. The combined score λ·F1 + (1−λ)·sI balances discriminative power with interpretability, with λ=0.75 as default.
- Core assumption: Human notions of interpretability (readable, meaningful, trackable) can be reliably assessed by an LM; the penalty is sufficient to discourage gaming.
- Evidence anchors: [Section 3.3] "This module also severely penalizes features which are leaking the label... For example, in a text sentiment classification scenario, one of the proposed features could be 'overall_sentiment categorical', which, when extracted by the EXTRACTOR module, will result in high accuracy by the linear classifier. This is, of course, undesirable." [Section 5, Results] "For some models, the inclusion of interpretability estimates sometimes reduces performance, since optimizing only for the final F1 score can allow the model to 'cheat' by proposing features that are the same as the target label."

## Foundational Learning

- **Concept: Prompt optimization vs. prompt engineering**
  - Why needed here: The paper frames feature discovery as an optimization problem over prompts, not manual crafting. Understanding that prompts can be treated as learnable parameters is essential.
  - Quick check question: Can you explain why per-example prompt optimization (like MIPRO) fails when each example requires a different feature set?

- **Concept: Feature interpretability criteria**
  - Why needed here: The INTERPRETABILITYSCORER uses specific criteria (readable, human-worded, understandable, meaningful, trackable) from Zytek et al. Understanding these helps debug why features are rejected.
  - Quick check question: What makes a feature "leaky" in this context, and why is it problematic?

- **Concept: Bayesian optimization for discrete search**
  - Why needed here: The algorithm uses Bayesian optimization (via Optuna) to search over categorical (instruction, example-set) pairs. Understanding how this explores the search space informs hyperparameter choices.
  - Quick check question: Why does performance degrade when Nd grows faster than the optimization budget?

## Architecture Onboarding

- **Component map:** FEATUREPROPOSER -> EXTRACTOR (N_A calls) -> train linear classifier -> compute metrics -> PERFORMANCEFEEDBACK + INTERPRETABILITYSCORER -> REFLECTIVEPROPOSER -> loop

- **Critical path:** FEATUREPROPOSER → EXTRACTOR (N_A calls) → train linear classifier → compute metrics → PERFORMANCEFEEDBACK + INTERPRETABILITYSCORER → REFLECTIVEPROPOSER → loop. The EXTRACTOR calls dominate cost: O((Nd + Niter)·NA·mE·(Lt + Lf)).

- **Design tradeoffs:**
  - Larger Nd increases search coverage but raises optimization cost; paper suggests Niter = max(Nd², 128)
  - Higher λ prioritizes F1 over interpretability (risking leakage); lower λ may yield less discriminative features
  - Using the same backbone LM for all modules simplifies deployment but may underutilize specialization opportunities
  - Smaller models (1-4B) often fail structured generation; BAML format is a partial workaround

- **Failure signatures:**
  - Features like "sentiment_label categorical" appearing in proposals → interpretability penalty too weak or λ too high
  - Generic prompt refinements ("Generate features to predict company names") → backbone LM too small; switch to ≥8B model
  - Invalid JSON output from small models → use BAML format or increase model size
  - Performance plateaus after first refinement round → increase Nfb or inspect whether feedback is actionable

- **First 3 experiments:**
  1. **Baseline comparison:** Run unoptimized prompt (hand-crafted, similar to Balek et al.) vs. REFLECTIVEPROPOSER with Nfb=1, Nd=16 on a held-out validation split. Confirm F1 improvement and inspect for label leakage.
  2. **Ablation on feedback type:** Compare scalar-only feedback (rephrasing without structured text) vs. full textual feedback. Expect ~5-10% F1 gap per Figure 2.
  3. **Model scale sensitivity:** Test Qwen-4B vs. Qwen-14B as backbone. Expect smaller models to show higher variance and more frequent schema violations; document failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can fine-tuning enable smaller models (<4B parameters) to reliably perform the FeatureProposer role?
- Basis in paper: [Explicit] The authors note smaller models like Llama3-1B failed due to poor instruction-following, hallucinated instructions, and invalid generations.
- Why unresolved: The study relied on off-the-shelf models; it is unknown if specialized training can overcome the instruction-following constraints identified in smaller architectures.
- What evidence would resolve it: Experiments applying supervised fine-tuning (SFT) for structured JSON generation on sub-4B models within this framework.

### Open Question 2
- Question: Does optimizing features for a linear classifier constrain the discovery of non-linear feature interactions?
- Basis in paper: [Inferred] The method evaluates feature quality by training a "simple linear classifier" and using SHAP values (Algorithm 1).
- Why unresolved: The optimization gradient may favor features that are explicitly linearly separable, potentially ignoring complex interactions that a non-linear model would capture.
- What evidence would resolve it: A comparison of feature sets discovered when using a non-linear surrogate model (e.g., MLP or Gradient Boosting) in the optimization loop.

### Open Question 3
- Question: How can the extraction bottleneck be mitigated when scaling to significantly larger corpora?
- Basis in paper: [Explicit] The asymptotic analysis identifies the extraction cost O(N_A * m_E) as the computational bottleneck, dominating the end-to-end cost.
- Why unresolved: The framework requires extracting features across a substantial annotation set for every candidate prompt evaluation.
- What evidence would resolve it: Studies evaluating the trade-off between evaluation subset size (via active learning) and optimization stability.

## Limitations
- Reproducibility of optimization dynamics is uncertain due to tuned hyperparameters without systematic ablation across datasets
- SLM dependency creates high model variance, with no formal analysis of variance across multiple random seeds
- Interpretability scorer validity is questionable as it lacks human judgment validation and may not reliably detect subtle label leakage
- Evaluation design bias exists due to using the same dataset for both optimization and evaluation without held-out test set

## Confidence
- **High:** Dataset-level feedback improves over per-example optimization; Multi-agent specialization with textual feedback outperforms scalar-only feedback; Interpretability regularization prevents degenerate label-leakage features
- **Medium:** Specific prompt templates and initial instructions critically affect convergence (reproduction risk); claimed superiority of SLM-based optimization over larger models is plausible but not robustly validated
- **Low:** Interpretability scorer's criteria map to true feature utility; optimization algorithm's hyperparameters are optimal or even stable across tasks

## Next Checks
1. **Hyperparameter sensitivity sweep:** Systematically vary Nd (4, 8, 16, 32), Nfb (1, 2, 3), and λ (0.5, 0.75, 0.9) on a held-out dataset to map the optimization landscape and confirm stability of reported results
2. **Human evaluation of interpretability:** Have human annotators score the top-5 features from both optimized and unoptimized prompts on readability, meaningfulness, and label leakage, and compare against LM-scored interpretability
3. **Cross-dataset generalization test:** Train the optimizer on one dataset (e.g., Twitter Financial) and evaluate feature quality on a held-out dataset from a different domain (e.g., ToxicChat) to test robustness of the discovered feature schemas