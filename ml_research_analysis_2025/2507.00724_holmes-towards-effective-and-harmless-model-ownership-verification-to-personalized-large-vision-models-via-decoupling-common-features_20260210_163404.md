---
ver: rpa2
title: 'Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized
  Large Vision Models via Decoupling Common Features'
arxiv_id: '2507.00724'
source_url: https://arxiv.org/abs/2507.00724
tags:
- features
- victim
- p-value
- verification
- dataset-specific
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Holmes addresses the problem of verifying ownership of personalized
  large vision models (LVMs) that are fine-tuned from pre-trained models. Existing
  model ownership verification methods designed for models trained from scratch either
  fail for fine-tuned models due to interference from pre-trained knowledge or are
  prone to misjudgment when models share similar common features.
---

# Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features

## Quick Facts
- **arXiv ID**: 2507.00724
- **Source URL**: https://arxiv.org/abs/2507.00724
- **Authors**: Linghui Zhu; Yiming Li; Haiqin Weng; Yan Liu; Tianwei Zhang; Shu-Tao Xia; Zhi Wang
- **Reference count**: 40
- **Primary result**: Holmes achieves effective ownership verification for fine-tuned LVMs by decoupling common features from dataset-specific features using shadow models

## Executive Summary
Holmes addresses the challenge of verifying ownership for personalized large vision models (LVMs) that are fine-tuned from pre-trained models. Existing verification methods designed for models trained from scratch fail for fine-tuned models due to interference from pre-trained knowledge or produce misjudgments when models share similar common features. Holmes introduces a novel fingerprinting paradigm that suppresses interference from shared common features by decoupling them from dataset-specific features through a shadow model approach.

The method employs two shadow models - a poisoned shadow model that disrupts dataset-specific features while retaining common features, and a benign shadow model that learns distinct dataset-specific features. By comparing outputs between these shadow models and the victim model, Holmes captures dataset-specific features while effectively decoupling common features. The approach demonstrates effectiveness across various model stealing attacks while maintaining high reliability and avoiding misjudgments for independent models.

## Method Summary
Holmes proposes a shadow model-based approach for ownership verification of personalized large vision models. The method creates two shadow models: a poisoned shadow model that retains common features but disrupts dataset-specific features through label-inconsistent backdoor attacks, and a benign shadow model that learns distinct dataset-specific features. The core insight is that by computing output differences between these shadow models and the victim model, Holmes can capture the victim model's dataset-specific features while decoupling common features.

An ownership meta-classifier is trained to identify these dataset-specific features, and hypothesis testing is used to enhance verification reliability and mitigate randomness. This approach addresses the fundamental challenge that existing verification methods fail for fine-tuned models because they cannot distinguish between pre-trained knowledge (common features) and personalized knowledge (dataset-specific features).

## Key Results
- Achieves reliable verification with high confidence scores (∆µ) and low p-values across all scenarios
- Effectively detects various types of model stealing attacks simultaneously (direct-copy, fine-tuning, and distillation-based attacks)
- Shows robustness against potential adaptive attacks including overwriting, unlearning, and pruning
- Extends effectively to image captioning tasks beyond image classification

## Why This Works (Mechanism)
Holmes works by exploiting the fundamental difference between common features (shared across models due to pre-training) and dataset-specific features (unique to the personalized model). The method recognizes that traditional verification approaches fail because they cannot distinguish between these two types of features. By creating shadow models that either preserve common features while disrupting dataset-specific ones (poisoned) or vice versa (benign), Holmes can isolate the dataset-specific components that serve as effective ownership fingerprints.

The mechanism leverages the observation that even when models are stolen through various attack vectors, the dataset-specific features remain relatively intact while common features are universally shared. The label-inconsistent backdoor attacks in the poisoned shadow model ensure that dataset-specific features are corrupted while common features remain functional, creating a clean separation that enables reliable verification.

## Foundational Learning
**Model Ownership Verification**: The process of proving that a particular model belongs to a specific party. Why needed: Essential for protecting intellectual property in AI models. Quick check: Can verify ownership claims with statistical confidence.

**Shadow Models**: Auxiliary models trained to mimic the behavior of victim models for analysis purposes. Why needed: Enable controlled experimentation and feature isolation. Quick check: Can reproduce similar performance characteristics to target models.

**Feature Decoupling**: The separation of different types of features learned by neural networks. Why needed: Allows isolation of unique vs. shared knowledge. Quick check: Features can be mathematically separated and independently analyzed.

**Label-Inconsistent Backdoor Attacks**: Techniques that corrupt model behavior on specific inputs while maintaining overall functionality. Why needed: Disrupt dataset-specific features while preserving common features. Quick check: Model performance degrades on targeted samples but remains functional otherwise.

**Hypothesis Testing**: Statistical methods for making decisions based on data. Why needed: Provides quantitative framework for verification reliability. Quick check: Can establish statistical significance of ownership claims.

## Architecture Onboarding

**Component Map**: Pre-trained Model → Fine-tuning → Victim Model; Shadow Models (Poisoned + Benign) → Feature Extraction → Meta-Classifier → Hypothesis Testing → Ownership Decision

**Critical Path**: Victim Model → Shadow Model Comparison → Feature Decoupling → Meta-Classifier → Hypothesis Testing → Verification Output

**Design Tradeoffs**: 
- Uses multiple shadow models (increased computational cost) vs. single model (potentially less accurate)
- Label-inconsistent attacks (more effective feature disruption) vs. consistent attacks (easier implementation)
- Hypothesis testing (more reliable) vs. direct classification (faster)

**Failure Signatures**: 
- High p-values indicate inability to distinguish features
- Low confidence scores suggest weak dataset-specific signatures
- False positives occur when independent models share similar dataset-specific features

**First 3 Experiments to Run**:
1. Compare verification accuracy between poisoned and benign shadow models individually
2. Test hypothesis testing framework with varying significance levels
3. Evaluate performance under different fine-tuning intensities

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness constrained by reliance on adversarial examples and label-inconsistent backdoor attacks
- Experimental scope primarily limited to image classification tasks with CIFAR-10 and ImageNet
- Computational overhead of maintaining and comparing multiple shadow models not thoroughly discussed

## Confidence

**High Confidence**: Core methodology of using shadow models for feature decoupling and overall framework for ownership verification are well-established and theoretically sound. Experimental results demonstrating effectiveness against common attack types are consistent and reproducible.

**Medium Confidence**: Robustness claims against adaptive attacks are based on limited experimental scenarios and may not cover full spectrum of potential countermeasures. Assumption about clean separation of common and dataset-specific features requires further validation.

**Low Confidence**: Practical deployment implications including computational costs and real-world performance in production environments are not sufficiently addressed.

## Next Checks

1. Conduct experiments across broader range of vision tasks beyond image classification and captioning, including object detection and segmentation, to validate method's generalizability.

2. Perform ablation studies to quantify impact of different shadow model architectures and training strategies on verification accuracy and robustness.

3. Evaluate method's performance under realistic operational conditions including varying model sizes, different pre-training datasets, and computational resource constraints typical of production environments.