---
ver: rpa2
title: 'Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly
  an NLL Estimator'
arxiv_id: '2502.04567'
source_url: https://arxiv.org/abs/2502.04567
tags:
- preference
- completions
- mc-po
- optimization
- dispreferred
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel framework for preference optimization
  by formulating it as a negative log-likelihood (NLL) estimation problem. The key
  insight is that dispreferred completions in preference optimization can be viewed
  as samples used to estimate the intractable normalization constant in NLL.
---

# Preference Optimization via Contrastive Divergence: Your Reward Model is Secretly an NLL Estimator

## Quick Facts
- arXiv ID: 2502.04567
- Source URL: https://arxiv.org/abs/2502.04567
- Authors: Zhuotong Chen; Fang Liu; Xuan Zhu; Yanjun Qi; Mohammad Ghavamzadeh
- Reference count: 34
- Primary result: MC-PO algorithm using contrastive divergence sampling outperforms state-of-the-art baselines on alignment benchmarks

## Executive Summary
This paper introduces a novel framework for preference optimization (PO) by reformulating it as a negative log-likelihood (NLL) estimation problem. The key insight is that dispreferred completions in PO can be viewed as samples used to estimate the intractable normalization constant in NLL. The authors propose MC-PO, an offline algorithm that uses contrastive divergence (CD) sampling to select hard negatives (dispreferred completions that closely resemble preferred ones). They also introduce OnMC-PO, an online extension. Experiments on popular alignment benchmarks show MC-PO outperforms existing state-of-the-art baselines, and OnMC-PO leads to further improvements in winrate against GPT-4. The paper provides theoretical analysis showing that sampling preferred completions from the target policy leads to unbiased gradient estimation of the normalization constant.

## Method Summary
The authors reformulate preference optimization as a sampling-based NLL estimation problem using Noise-Contrastive Estimation (NCE). They introduce MC-PO, which uses a Monte Carlo kernel to sample "hard negatives" - dispreferred completions that closely resemble preferred ones - making it more challenging for the model to distinguish between them. The algorithm samples from a categorical distribution weighted by exp(βr_θ), preferentially selecting candidates with high rewards under the current policy. They also propose OnMC-PO, an online extension that regenerates preferred completions from the target policy π_θ every K steps to maintain unbiased gradient estimates. The theoretical framework proves that when M=1, this sampling-based solution reduces exactly to the DPO loss, and that on-policy sampling of preferred completions yields unbiased gradient estimates of the normalization constant.

## Key Results
- MC-PO outperforms state-of-the-art baselines on AlpacaEval and MT-Bench benchmarks
- OnMC-PO further improves winrates against GPT-4 compared to offline MC-PO
- The theoretical analysis shows equivalence between DPO and RNCE with one noise sample
- Hard negative sampling via MC kernel yields larger gradient magnitudes when rewards are similar

## Why This Works (Mechanism)

### Mechanism 1: Dispreferred completions as normalization constant estimators
- Dispreferred completions serve as importance samples for estimating the intractable normalization constant in NLL estimation, reframing PO as a sampling-based probability model fitting problem
- The paper defines a probability model p_θ(y|x) proportional to μ(y|x)exp(βr_θ(x,y)). The NLL objective contains an intractable normalization constant Z_θ(x). Using RNCE with one observation y_0 from π* and M noise samples from proposal μ, the objective becomes L_Sample = -βr_θ(x,y_0) + log Σ exp(βr_θ(x,y_i)). When M=1, this reduces exactly to the DPO loss, proving dispreferred completions are mathematically equivalent to normalization constant estimators
- Core assumption: The proposal distribution μ has adequate coverage of the support of π*, and the parameterized reward model r_θ reasonably approximates the true preference structure
- Evidence anchors: [abstract] "these estimative samples can act as dispreferred completions in PO"; [Section 2.3] "This sampling-based solution with one noise sample is equivalent to DPO where the noise sample acts as a dispreferred completion"
- Break condition: If proposal distribution μ has poor coverage of high-probability regions under p_θ, gradient estimates become high-variance and convergence degrades

### Mechanism 2: Hard negatives from reward-proportional sampling
- Sampling dispreferred completions proportionally to the reward model (via MCMC kernel) produces "hard negatives" that yield larger gradient magnitudes when the dispreferred completion has higher estimated reward than the preferred one
- The gradient of the sampling-based objective is ∇L_Sample = -βσ(βr_θ(y_1) - βr_θ(y_0))∇(r_θ(y_0) - r_θ(y_1)). The σ term approaches 0.5 when rewards are similar (hard negatives), maximizing gradient signal. The MC Kernel samples from a categorical distribution with weights w_i ∝ exp(βr_θ(y_i)), preferentially selecting candidates with high rewards under the current policy
- Core assumption: Candidate completions in the dataset contain semantically similar but distinguishable responses; purely syntactic noise (small edit distance) degrades this mechanism
- Evidence anchors: [abstract] "MC-PO algorithm that uses a Monte Carlo (MC) kernel to sample 'hard negatives' (dispreferred completions that closely resemble preferred ones)"; [Section 3.1] "When the estimated reward for a dispreferred completion exceeds that of a preferred one, this results in a larger gradient magnitude"
- Break condition: If all candidates have uniformly low or high rewards (low variance in w_i), the categorical sampling collapses to near-uniform, negating the hard negative benefit

### Mechanism 3: On-policy sampling for unbiased gradients
- Sampling preferred completions from the target policy π_θ (rather than from fixed data distribution) produces unbiased gradient estimates of the normalization constant, explaining online PO advantages
- Proposition 3.3 proves that when y_0 ~ p_θ (the probability model) rather than y_0 ~ π*, then E[∇log Ẑ(x)] = ∇log Z(x). Since p_θ ∝ μ·exp(βr_θ) and r_θ is defined by π_θ, sampling from π_θ during training ensures the preferred completion distribution tracks the evolving probability model, maintaining gradient unbiasedness as parameters update
- Core assumption: Policy updates per step are small enough that batched online sampling (regenerating data every K steps) approximates continuous on-policy sampling
- Evidence anchors: [abstract] "theoretical analysis shows that sampling preferred completions from the target policy leads to unbiased gradient estimation"; [Section 3.2] "online PO algorithms generate preferred completions from the target policy that is proportional to the probability model p_θ, intends to have an unbiased gradient estimation"
- Break condition: If policy drifts significantly between regeneration points in batched online setting, gradient bias re-emerges; very large learning rates or few regeneration points break this

## Foundational Learning

### Concept: Importance sampling for normalization constant estimation
- Why needed here: The entire theoretical framework depends on understanding how samples from a proposal distribution μ can estimate integrals over an unnormalized model
- Quick check question: Given samples {y_i} from proposal q(y), how would you estimate E_p[f(y)] where p is unnormalized?

### Concept: Contrastive divergence and MCMC kernels
- Why needed here: MC-PO's core innovation is using CD's short-run MCMC to sample hard negatives; understanding why CD starts chains at data points (not random init) explains the efficiency
- Quick check question: Why does CD typically use only 1-2 MCMC steps instead of running chains to convergence?

### Concept: Bradley-Terry preference model and its connection to logistic classification
- Why needed here: DPO derives from BT model; the paper shows this is equivalent to binary classification with NLL, enabling the sampling-based reinterpretation
- Quick check question: If preference probability p(y_0 ≻ y_1) = σ(r(y_0) - r(y_1)), what happens when r(y_0) = r(y_1)?

## Architecture Onboarding

### Component map
- **MC Kernel (Algorithm 1)**: Takes (x, y_0, {y_1...y_L}) → computes softmax weights over exp(βr_θ) → samples y_z as dispreferred. Pure inference, no gradients.
- **Loss computation**: L = -βr_θ(y_0) + log Σ_i exp(βr_θ(y_i)) where r_θ = log(π_θ/π_ref). Standard autodiff handles this.
- **Online regeneration loop** (OnMC-PO only): Every K steps, generate new completions from current π_θ, re-rank to identify y_0, sample y_1...y_L from candidates.

### Critical path
The correctness of MC-PO hinges on the candidate set quality. If candidates are all low-quality or near-identical, the MC kernel cannot find informative hard negatives. Start by auditing candidate diversity before implementing the kernel.

### Design tradeoffs
- Single-step vs. multi-step MCMC: Paper uses 1 step for efficiency; multi-step could improve sample quality but adds ~30%+ overhead per step
- Online vs. batched online: Pure online is theoretically optimal but inference overhead is high; batched (regenerate every N steps) is practical compromise
- Number of candidates L: More candidates improve MC kernel's ability to find hard negatives but increase memory/compute for log-probability computation

### Failure signatures
- DPO(−) in Table 2 shows ~1% winrate when using syntactic noise (token swaps) as dispreferred—MC-PO resists this because categorical sampling downweights such samples
- "Min" sampling variant (Figure 2) shows high variance and poor performance—confirms hard negatives matter
- If winrates plateau or degrade after epoch 1, check whether candidate pool has been exhausted (repetition in data)

### First 3 experiments
1. **Sanity check on candidate quality**: Run MC-PO with Max, MC, and Min sampling variants on a small subset. MC should outperform Min; if not, audit candidate diversity (are they meaningfully different?).
2. **Ablation on number of candidates**: Compare L=1 (equivalent to random DPO), L=3, L=5 on validation winrate. Paper shows monotonic improvement; failure to replicate suggests proposal distribution issues.
3. **Online vs. offline comparison**: Implement batched OnMC-PO with regeneration at 1/3 and 2/3 of training. Compare against offline MC-PO. Expect 3-7% winrate improvement per Table 1; if no improvement, verify π_θ generation temperature is not too low (causing mode collapse).

## Open Questions the Paper Calls Out
### Open Question 1
- Question: Does utilizing a multi-step MCMC chain in MC-PO yield significant performance improvements over the current single-step implementation?
- Basis in paper: [explicit] The authors state in the Conclusion that they currently run the MCMC kernel for only a single step, which is "typically suboptimal," and identify showcasing the benefits of multi-step solutions as a goal for future research.
- Why unresolved: The single-step restriction was chosen for efficiency, leaving the theoretical benefits of running the chain to convergence (acquiring higher quality samples) unverified in this specific alignment context.
- What evidence would resolve it: Experimental results comparing the winrates and alignment quality of models trained with single-step MC-PO versus those trained with $k > 1$ MCMC steps.

### Open Question 2
- Question: How can the training algorithms for MC-PO be optimized to mitigate the computational overhead of the MCMC kernel?
- Basis in paper: [explicit] The paper notes that MC-PO adds approximately 30% to training time compared to DPO and explicitly lists developing "more efficient training algorithms to speed up these algorithms" as future work.
- Why unresolved: While the sampling strategy is theoretically grounded, the added latency from the kernel computations may hinder adoption in resource-constrained or large-scale distributed training environments.
- What evidence would resolve it: The proposal of algorithmic optimizations (e.g., parallelized sampling or kernel approximation) that reduce the training time overhead to near zero without degrading the winrate improvements.

### Open Question 3
- Question: Does the performance improvement of MC-PO scale linearly with the number of dispreferred samples ($M$), or does it saturate?
- Basis in paper: [inferred] Table 3 demonstrates that increasing $M$ from 1 to 3 improves winrates on both Alpaca and Arena benchmarks, but the paper does not test the limits of this scaling or if it eventually plateaus.
- Why unresolved: Without testing larger values of $M$, it is unclear if the cost of sampling additional negative candidates continues to yield proportional alignment benefits.
- What evidence would resolve it: Ablation studies measuring MC-PO performance with significantly larger candidate sets (e.g., $M=10, 20, 50$) to identify the point of diminishing returns.

## Limitations
- The theoretical framework relies heavily on the assumption that the proposal distribution μ adequately covers the support of the target policy π*, which is critical for MC-PO's hard negative sampling mechanism
- Proposition 3.3's claim about unbiased gradient estimation assumes small policy updates between regeneration steps in the online setting, but the batched implementation may violate this when π_θ changes substantially before candidate regeneration
- The claim that MC-PO is "more robust to noisy dispreferred completions" is supported by synthetic noise experiments but needs validation across different noise types

## Confidence
High confidence: The mathematical equivalence between DPO and RNCE with M=1 is rigorously proven through Lemma 3.2 and the gradient analysis. The connection between hard negatives and larger gradient magnitudes is also well-supported by the σ(βΔr) term analysis.

Medium confidence: The theoretical advantage of on-policy sampling for preferred completions is proven, but the practical implementation (batched online regeneration) introduces approximation error that isn't fully characterized. The winrate improvements in OnMC-PO are substantial but could partly stem from other factors like temperature scaling during π_θ generation.

Medium confidence: The claim that MC-PO is "more robust to noisy dispreferred completions" is supported by the synthetic noise experiments, but the comparison only includes one alternative (DPO with token shuffling). The mechanism described suggests broader robustness, but this needs validation across different noise types.

## Next Checks
1. **Candidate diversity audit**: Implement a semantic similarity metric (e.g., BERTScore or CLIP embeddings) between preferred completions and sampled dispreferred ones. Verify that MC-PO consistently selects higher-similarity pairs than random sampling. If similarity scores don't exceed random baselines, the candidate generation or MC kernel implementation needs debugging.

2. **On-policy sampling bias measurement**: During OnMC-PO training, track the KL divergence between the distribution of generated preferred completions and the evolving π_θ. Large divergences (>0.1) would indicate the regeneration frequency K is too low, causing the "unbiased gradient" assumption to break.

3. **Cross-dataset generalization test**: Apply MC-PO to a preference optimization task from a different domain (e.g., code completion or scientific writing) where the preference structure differs from the conversational data used in the paper. If winrates against GPT-4 remain above 50%, this supports the claim that the NLL sampling framework generalizes beyond the specific datasets tested.