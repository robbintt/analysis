---
ver: rpa2
title: 'Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through
  Large Language Model Queries'
arxiv_id: '2512.15906'
source_url: https://arxiv.org/abs/2512.15906
tags:
- knowledge
- response
- responses
- more
- code
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Darth Vecdor (DV) is an open-source system that generates knowledge
  graphs by querying large language models (LLMs) for structured healthcare relationships.
  The system addresses key LLM challenges including erroneous responses, overly general
  outputs, and the need for multi-element parsing.
---

# Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries
## Quick Facts
- arXiv ID: 2512.15906
- Source URL: https://arxiv.org/abs/2512.15906
- Reference count: 0
- Primary result: Open-source system generating knowledge graphs from LLM queries for healthcare relationships

## Executive Summary
Darth Vecdor (DV) is an open-source system designed to generate knowledge graphs by querying large language models (LLMs) for structured healthcare relationships. The system addresses key LLM challenges including erroneous responses, overly general outputs, and the need for multi-element parsing. DV provides a browser-based GUI enabling domain experts without technical backgrounds to create and use knowledge graphs, automatically re-queries LLMs to improve response specificity, and employs vector-based matching to map free-text outputs to medical terminology codes.

## Method Summary
The system architecture includes a browser-based GUI for domain expert interaction, automatic re-querying mechanisms to improve LLM response specificity, and vector-based matching to map free-text outputs to medical terminology codes. Knowledge graphs are stored as SQL triple stores for efficient querying. The approach enables cost-effective, faster, and more explainable access to LLM-encoded knowledge compared to direct LLM queries, though no formal validation results are reported.

## Key Results
- System provides browser-based GUI for non-technical domain experts to create knowledge graphs
- Automatic re-querying mechanism improves LLM response specificity
- Vector-based matching maps free-text outputs to medical terminology codes
- Knowledge graphs stored as SQL triple stores for efficient querying
- Informal review suggested high accuracy of generated relationships and code mappings

## Why This Works (Mechanism)
The system addresses three core LLM limitations: erroneous responses through automatic re-querying, overly general outputs by improving specificity through iterative questioning, and multi-element parsing challenges via vector-based matching to standardized medical terminology codes.

## Foundational Learning
- LLM Query Limitations - why needed: Direct LLM queries produce unreliable, non-specific results for structured knowledge extraction
  quick check: Compare direct LLM query outputs against expected structured format
- Vector-Based Text Mapping - why needed: Free-text medical concepts must be standardized to controlled terminology
  quick check: Test vector similarity matching between clinical terms and standard codes
- SQL Triple Store Architecture - why needed: Efficient storage and querying of relationship-based knowledge graphs
  quick check: Verify triple store query performance with sample healthcare relationships

## Architecture Onboarding
Component map: Browser GUI -> LLM Query Engine -> Vector Matcher -> SQL Triple Store -> Query Interface
Critical path: Domain expert input → Browser GUI → LLM query → Response parsing → Vector matching → Triple store insertion
Design tradeoffs: Browser-based GUI maximizes accessibility but may limit performance vs desktop applications; automatic re-querying improves quality but increases API costs
Failure signatures: Generic LLM responses indicate insufficient re-querying; mapping errors suggest vector similarity thresholds too low; slow queries indicate triple store indexing issues
First experiments:
1. Test single healthcare relationship extraction with manual verification
2. Measure vector matching accuracy for common medical terms
3. Benchmark triple store query performance with 1000 relationships

## Open Questions the Paper Calls Out
None

## Limitations
- No formal validation results presented, relying on informal reviewer feedback
- Performance characteristics (precision, recall, error rates) remain unmeasured
- Claims about cost-effectiveness and explainability lack empirical support

## Confidence
- High: Technical feasibility of described architecture and GUI implementation
- Medium: General approach to addressing LLM limitations through re-querying and vector matching
- Low: Claims about performance improvements and accuracy without formal validation

## Next Checks
1. Conduct systematic evaluation measuring precision, recall, and F1-score against ground truth medical knowledge graphs
2. Benchmark query response times and costs comparing direct LLM queries versus DV system queries
3. Test system scalability with progressively larger knowledge graphs and concurrent users