---
ver: rpa2
title: 'Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence
  Expression of Large Language Models'
arxiv_id: '2503.02623'
source_url: https://arxiv.org/abs/2503.02623
tags:
- confidence
- answer
- reward
- arxiv
- calibration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of improving the calibration of
  confidence estimates expressed by large language models (LLMs) in their factual
  answers. The core method, called Rewarding Doubt, uses reinforcement learning to
  directly fine-tune LLMs to express calibrated confidence estimates alongside their
  answers.
---

# Rewarding Doubt: A Reinforcement Learning Approach to Calibrated Confidence Expression of Large Language Models

## Quick Facts
- arXiv ID: 2503.02623
- Source URL: https://arxiv.org/abs/2503.02623
- Reference count: 19
- Primary result: Improves Expected Calibration Error (ECE) to 0.0226 and AUROC to 0.8592 on TriviaQA

## Executive Summary
This paper addresses the problem of improving the calibration of confidence estimates expressed by large language models (LLMs) in their factual answers. The core method, called Rewarding Doubt, uses reinforcement learning to directly fine-tune LLMs to express calibrated confidence estimates alongside their answers. It optimizes a reward based on the logarithmic scoring rule, which explicitly penalizes both over- and under-confidence, encouraging the model to align its confidence estimates with the actual predictive accuracy. The method achieves substantially improved calibration, with Expected Calibration Error (ECE) of 0.0226 and Area Under the ROC Curve (AUROC) of 0.8592 on the TriviaQA dataset, outperforming established baselines. It also generalizes to unseen tasks without further fine-tuning, suggesting the emergence of general confidence awareness in the model.

## Method Summary
Rewarding Doubt uses reinforcement learning with proximal policy optimization (PPO) to fine-tune LLMs to express calibrated confidence estimates (0-10 scale) alongside factual answers. The method employs a logarithmic scoring rule reward that penalizes both over- and under-confidence, encouraging the model to align its confidence estimates with actual predictive accuracy. A key innovation is the disentangled generation approach where answers are generated first and treated as fixed inputs during confidence optimization, preserving answer accuracy while improving calibration. The method uses LoRA adapters for efficient fine-tuning on 4-bit quantized Llama-3-8B-Instruct via Unsloth AI, training for 2 epochs with a learning rate of 1e-5.

## Key Results
- Achieves ECE of 0.0226 and AUROC of 0.8592 on TriviaQA, outperforming baselines
- Maintains stable answer accuracy during confidence training, demonstrating effectiveness of disentangled optimization
- Generalizes to unseen tasks (CommonsenseQA, MedQA) without further fine-tuning, suggesting emergence of general confidence awareness

## Why This Works (Mechanism)

### Mechanism 1: Proper Scoring Rule Optimization
The logarithmic scoring rule reward function creates theoretical guarantees for calibration. Because the logarithmic scoring rule is strictly proper, the expected reward is maximized only when the predicted confidence equals the true likelihood of correctness. This creates gradients that penalize over-confidence (severe punishment for wrong answers) and under-confidence (missed reward for correct answers). The core assumption is that the LLM can effectively represent and optimize for this mathematical reward structure without instability. If the reward function were replaced by a non-proper scoring rule like linear reward, the calibration guarantees would vanish, potentially leading to reward hacking.

### Mechanism 2: Disentangled Generation and Optimization
Generating the answer first and treating it as a fixed input for confidence optimization preserves task accuracy while refining calibration. This two-step process (1) Generate Answer, then (2) Generate Confidence given the answer ensures answer correctness is not affected by confidence calibration training. The core assumption is that the base model's answer generation is robust enough to not require simultaneous gradient updates. If answer tokens were unfrozen during PPO updates, the model might exhibit catastrophic forgetting of factual knowledge or shift to safer but less useful predictions.

### Mechanism 3: Internal State Utilization
The model learns to map its internal hidden states (which encode uncertainty) to explicit numerical tokens, enabling generalization to unseen domains. By rewarding the model based on objective correctness, it is forced to utilize the correlation between internal activation patterns and outcome, learning a "meta-skill" of introspection rather than dataset-specific heuristics. The core assumption is that hidden states already contain sufficient signal regarding truthfulness. If the model relies solely on surface features rather than internal epistemic states, generalization to out-of-distribution tasks would fail.

## Foundational Learning

**Concept: Strictly Proper Scoring Rules**
- **Why needed here:** The core theoretical justification for the reward function relies on the property that expected score is maximized only if the forecaster reports true belief
- **Quick check question:** Why would a Mean Squared Error (MSE) reward function potentially fail to prevent "hedging" (predicting 0.5 confidence for everything) compared to the log score?

**Concept: Proximal Policy Optimization (PPO)**
- **Why needed here:** The paper uses PPO to fine-tune the LLM; understanding the "clipping" mechanism is vital to prevent model destruction
- **Quick check question:** In the context of this paper, what acts as the "Value Network" or "Critic" if the reward is calculated based on a deterministic judge function?

**Concept: Expected Calibration Error (ECE)**
- **Why needed here:** ECE is the primary metric measuring difference between confidence and accuracy binned across confidence ranges
- **Quick check question:** If a model achieves 0.0 ECE but only 10% accuracy, is it "well-calibrated," and does the paper's method prevent this low-accuracy scenario?

## Architecture Onboarding

**Component map:**
Question -> Frozen LLM Backbone (Llama-3-8B-Instruct) -> Answer Generation -> Confidence Generation -> Judge (F1/Exact Match) -> Reward Calculator (Log Score) -> PPO Optimizer (LoRA) -> Updated Confidence Policy

**Critical path:**
1. Input Question q
2. **Phase 1:** LLM generates Answer a (detached from computation graph)
3. **Phase 2:** LLM generates Confidence c (tokens "0"-"10") given (q, a)
4. **Evaluation:** Judge compares a to ground truth → binary label
5. **Reward:** Calculate log-score based on binary label and normalized confidence
6. **Update:** PPO step on Confidence Generation logits

**Design tradeoffs:**
- **Integer vs. Continuous Confidence:** Uses integers 0-10 rather than floats for computational simplicity but limits calibration granularity
- **Clipping ε:** Log of 0 is undefined; system clips confidence to [ε, 1-ε]. If ε is too large, model cannot express absolute certainty

**Failure signatures:**
- **Over-optimization:** Model learns to output specific confidence value (e.g., "7") for every answer, minimizing variance to "game" reward
- **Format collapse:** Model stops generating answer correctly because prompt processing interferes with RLHF phase (mitigated by disentangled generation)

**First 3 experiments:**
1. **Sanity Check (Reward vs. Confidence):** Plot generated confidence distribution of base model vs. fine-tuned model on held-out set. Expectation: Base model clustered at 8-10; fine-tuned model spreads 0-10
2. **Ablation on Disentanglement:** Run variant where answer is not detached during PPO update. Verify if answer accuracy drops compared to main method
3. **Out-of-Domain Transfer:** Train on TriviaQA and immediately evaluate on MedQA without adaptation. Verify if AUROC remains above 0.6 to confirm "general confidence awareness"

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization depth remains uncertain as evaluation covers only 3 tasks, leaving questions about performance in highly technical domains
- Results may not transfer to other model architectures, scales, or fine-tuning methods as only Llama-3-8B-Instruct was tested
- 0-10 integer scale for confidence may limit calibration precision compared to continuous values

## Confidence
**High Confidence:**
- Method improves ECE and AUROC on TriviaQA compared to baselines
- Answer accuracy remains stable during confidence training (disentanglement works)
- Generalization to unseen tasks occurs without further fine-tuning

**Medium Confidence:**
- Model learns "general confidence awareness" rather than dataset-specific heuristics
- Logarithmic scoring rule guarantee holds in practice with discrete tokens and finite training
- Method would fail with linear rewards (theoretical claim, not empirically tested)

**Low Confidence:**
- 0-10 confidence scale is optimal for calibration precision
- Results would scale proportionally to larger LLMs without architectural changes
- Method would work equally well with different RL algorithms (e.g., DQN instead of PPO)

## Next Checks
1. **Reward Function Ablation:** Replace logarithmic scoring rule with linear reward function and retrain. Compare ECE and AUROC to verify proper scoring rules are necessary for calibration.

2. **Architecture Scaling Test:** Apply method to smaller LLM (Llama-3-3B) and larger one (Llama-3-70B). Measure if calibration improvements scale proportionally or if there are architectural breakpoints.

3. **Continuous Confidence Generation:** Modify method to generate confidence as continuous value rather than 0-10 integers. Evaluate whether this yields lower ECE and higher AUROC, testing granularity limitation.