---
ver: rpa2
title: Instance-level quantitative saliency in multiple sclerosis lesion segmentation
arxiv_id: '2406.09335'
source_url: https://arxiv.org/abs/2406.09335
tags:
- lesion
- saliency
- segmentation
- maps
- lesions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel explainable AI (XAI) methods to
  generate quantitative instance-level saliency maps for semantic segmentation, addressing
  the challenge of understanding what drives the detection and contouring of specific
  objects among multiple instances of the same class in medical imaging. The core
  method adapts SmoothGrad and Grad-CAM++ to segmentation tasks, creating instance-specific
  explanations by aggregating saliency maps over individual lesion domains rather
  than the entire class.
---

# Instance-level quantitative saliency in multiple sclerosis lesion segmentation

## Quick Facts
- arXiv ID: 2406.09335
- Source URL: https://arxiv.org/abs/2406.09335
- Authors: Federico Spagnolo; Nataliia Molchanova; Meritxell Bach Cuadra; Mario Ocampo Pineda; Lester Melie-Garcia; Cristina Granziera; Vincent Andrearczyk; Adrien Depeursinge
- Reference count: 40
- Primary result: Introduces two novel explainable AI (XAI) methods to generate quantitative instance-level saliency maps for semantic segmentation in MS lesion detection

## Executive Summary
This paper introduces two novel explainable AI (XAI) methods to generate quantitative instance-level saliency maps for semantic segmentation, addressing the challenge of understanding what drives the detection and contouring of specific objects among multiple instances of the same class in medical imaging. The methods adapt SmoothGrad and Grad-CAM++ to segmentation tasks, creating instance-specific explanations by aggregating saliency maps over individual lesion domains rather than the entire class. Applied to white matter lesion segmentation in multiple sclerosis using 4023 MRI scans from 687 patients, the methods reveal that models rely more on FLAIR than MPRAGE sequences and show that peak saliency values differ significantly between true positives, false positives, false negatives, and true negatives, suggesting potential for error identification.

## Method Summary
The paper introduces two instance-level saliency methods for semantic segmentation. The first adapts SmoothGrad by computing gradients for each voxel in a lesion domain and aggregating using maximum aggregation to ensure quantitative comparability across different lesion sizes. The second adapts Grad-CAM++ by computing per-voxel weights within the lesion domain and linearly combining activation maps. Both methods generate saliency maps that are quantitative (real values, not heatmaps) and specific to individual lesion instances rather than the entire class. The methods were applied to MS lesion segmentation using FLAIR and MPRAGE MRI sequences with three deep learning architectures (U-Net, nnU-Net, and Swin UNETR) on a dataset of 4023 scans from 687 patients.

## Key Results
- Instance saliency maps revealed that models relied more on FLAIR than MPRAGE sequences, consistent with clinical practice
- Peak saliency values showed significantly different distributions between true positives, false positives, false negatives, and true negatives (p < 0.001)
- The methods successfully generated quantitative instance-level explanations for lesion detection and contouring in MS MRI segmentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating gradients over individual lesion domains (rather than all class predictions) isolates what drives detection of a specific instance among multiple instances of the same class.
- Mechanism: For a lesion domain Ω, the method computes gradients from each output voxel in Ω to input voxels, then aggregates them. This contrasts with class-level aggregation where all segmented voxels contribute to one map, conflating explanations across instances.
- Core assumption: The spatial proximity between input voxels and a specific lesion domain reflects meaningful feature attribution for that instance, not spurious correlations from distant lesions.
- Evidence anchors:
  - [abstract] "instance-specific explanations by aggregating saliency maps over individual lesion domains rather than the entire class"
  - [Page 4] "computing a gradient map for all the output voxels would not be convenient or meaningful... This makes it more complicated to understand and determine which part of the output segmentation is influenced by which region of the input"
  - [corpus] Weak direct evidence; neighbor papers address instance-level concepts but not this specific aggregation mechanism
- Break condition: If lesions are spatially overlapping or share borders with minimal separation, gradient attribution may bleed between instances.

### Mechanism 2
- Claim: Maximum aggregation (versus average) over the lesion domain preserves quantitative comparability across lesions of different sizes.
- Mechanism: Larger lesions have more voxels with greater mutual distances; gradients from distant voxels naturally decay. Averaging penalizes large lesions with systematically lower saliency values. Maximum aggregation (with sign preservation) maintains comparable peak values regardless of lesion volume.
- Core assumption: The peak gradient value represents the most informative feature attribution, and lesion size should not confound quantitative comparison.
- Evidence anchors:
  - [Page 6-7] "The greater the lesion size, the more extended the potential distance between two voxels p,q∈Ω... the average over the lesion domain Ω will cause gradient values for extensive lesions to be systematically lower"
  - [Page 10, Figure 7] Shows saliency intensity difference between average and max aggregation methods for large vs. small lesions
  - [corpus] No direct evidence found in neighbor papers
- Break condition: If the most informative gradient is not the peak but rather a distributed pattern (e.g., texture features across the lesion), max aggregation may misrepresent the true mechanism.

### Mechanism 3
- Claim: Peak saliency values differ systematically between true positives, false positives, false negatives, and true negatives, enabling potential error identification.
- Mechanism: Quantitative saliency maps have interpretable absolute values. TP lesions show highest peak values (median ~2.0), followed by FP (~1.6), FN (~1.2), and TN (~0.5). This gradient suggests the model's attention level correlates with prediction correctness.
- Core assumption: The magnitude of gradient attribution reflects model confidence or feature alignment with learned patterns.
- Evidence anchors:
  - [Page 12-13, Figure 11] "Peak values of the generated saliency maps presented distributions that differ significantly between TP, FN, FP and TN predictions"
  - [Page 12] Mann Whitney U tests on all pairs reported p-value < 0.001
  - [corpus] Weak; neighbor papers discuss XAI generally but not this quantitative error signal pattern
- Break condition: If domain shift occurs (different scanner, sequence parameters), calibration of peak values may not transfer without re-evaluation.

## Foundational Learning

- Concept: **Gradient-based saliency maps (vanilla gradients, SmoothGrad)**
  - Why needed here: The method builds on SmoothGrad; understanding how gradients propagate backward from output to input is essential to interpret what the maps represent.
  - Quick check question: Given an output logit, what does ∂y/∂x[v] represent about input voxel v's contribution?

- Concept: **Class activation mapping (Grad-CAM, Grad-CAM++)**
  - Why needed here: The second method adapts Grad-CAM++; understanding weighted linear combinations of activation maps is required to modify it for instance-level explanations.
  - Quick check question: How does Grad-CAM++ improve over Grad-CAM for multiple instances of the same class?

- Concept: **Semantic segmentation output structure**
  - Why needed here: Unlike classification (one scalar per class), segmentation produces C tensors of spatial predictions; this difference motivates the aggregation strategy.
  - Quick check question: Why can't you directly apply classification saliency methods to segmentation without modification?

## Architecture Onboarding

- Component map:
  Input (FLAIR + MPRAGE, 2 channels, 192×240×256) -> Network (U-Net/nnU-Net/Swin UNETR) -> Logits y[v] (same spatial dims)
  -> Binarize (t=0.3) -> Connected components -> Instance domains Ω
  -> Instance saliency (gradients): N noisy inputs -> Compute gradients for each v'∈Ω -> Max-aggregate -> Output map per modality
  -> Instance saliency (Grad-CAM++): Select layer -> Compute per-voxel weights ωk[v] from Ω -> Linear combination of activations

- Critical path:
  1. Lesion domain definition (Ω) is the pivot point—without it, explanations collapse to class-level
  2. Gradient computation must flow back to input for modality attribution; stopping at intermediate layers loses channel-specific information
  3. Threshold selection (t=0.3) affects which voxels enter Ω and thus what the saliency explains

- Design tradeoffs:
  - SmoothGrad: Noise-tolerant, provides modality-specific attribution, but computationally expensive (N=50 forward/backward passes per lesion)
  - Grad-CAM++: Faster, more stable heatmaps, but cannot attribute to specific input modalities and requires layer selection
  - Max vs. average aggregation: Max enables quantitative comparison across sizes; average preserves relative internal structure but loses comparability

- Failure signatures:
  - Saliency maps with near-zero values across Ω -> Check: is the lesion actually in the prediction? (FN case requires external domain input)
  - Saliency extending to distant lesions -> Check: Ω may be incorrectly defined or lesions are adjacent
  - Positive saliency in regions that should be negative (or vice versa) -> Check: modality alignment (FLAIR vs. MPRAGE registration)

- First 3 experiments:
  1. Sanity check with synthetic lesions: Move a real lesion to healthy WM, outside skull, and outside skull with perilesional tissue. Observe whether saliency patterns follow the lesion or the location (validates that intensity + context, not position, drives detection).
  2. Context dilation experiment: Gradually reveal healthy tissue around masked lesions (0–35mm). Measure prediction score vs. dilation distance to identify the contextual information plateau (informs patch size selection).
  3. Error mode analysis: Compute peak saliency distributions for TP, FP, FN, TN volumes. Verify statistically significant separation (Mann Whitney U, p<0.001) before attempting to use saliency for error detection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the size of the input patch used during training influence the amount of contextual tissue required by the model to achieve stable lesion detection scores?
- Basis in paper: [explicit] The Discussion states, "It would be interesting to test if the patch size during training would have a potential influence on the plateau's position" observed in the contextual information experiment (Fig. 15).
- Why unresolved: The current study utilized a fixed patch size ($96^3$) and observed that prediction scores plateaued when contextual tissue reached 12–15mm from the lesion border. The causal link between the training patch dimension and this specific contextual dependency remains untested.
- What evidence would resolve it: A comparative experiment training identical architectures with varying patch sizes (e.g., $64^3$, $128^3$, $192^3$) and measuring the resulting dilation distance required to reach the prediction score plateau.

### Open Question 2
- Question: Can the pruning of false positive predictions be optimized by analyzing saliency map characteristics specific to different anatomical regions of the brain?
- Basis in paper: [explicit] The Conclusion proposes that "the pruning of false predictions could be studied separately for different areas of the brain."
- Why unresolved: While the authors noted that false positives were often located in sulci and gyri, the quantitative analysis of peak saliency values aggregated all false positives globally. It is unknown if region-specific saliency patterns (e.g., periventricular vs. cortical) could better distinguish false positives from true lesions.
- What evidence would resolve it: A stratified evaluation of saliency radiomic features for false positives across defined anatomical regions, demonstrating improved classification performance compared to a global model.

### Open Question 3
- Question: Can uncertainty maps be effectively utilized to automatically identify the spatial domain required to generate instance-level saliency maps for false negative lesions?
- Basis in paper: [explicit] The Conclusion suggests, "Future research could leverage uncertainty maps to generate XAI maps of false negative examples, and further boost performance."
- Why unresolved: The proposed method requires a defined lesion domain ($\Omega$) to compute gradients. For false negatives (missed lesions), the model provides no segmentation mask, currently necessitating external input (e.g., a neurologist's mark-up) to define the domain.
- What evidence would resolve it: A methodological framework that substitutes manual domain definition with automated uncertainty-based regions of interest, successfully generating saliency maps that reveal the features associated with missed detections.

## Limitations

- The method's reliance on spatial lesion domains assumes lesions are well-separated, but no quantitative analysis is provided on false-positive rates when lesions are adjacent
- The max aggregation approach, while enabling quantitative comparability, may oversimplify distributed feature patterns within larger lesions
- The validation on a single dataset (Swiss MS cohort) limits generalizability across scanner manufacturers and field strengths

## Confidence

- Mechanism 1 (instance domain aggregation): Medium - theoretically sound but limited empirical validation on boundary cases
- Mechanism 2 (max aggregation for comparability): Medium - mathematically justified but no ablation studies comparing alternative approaches
- Mechanism 3 (error identification via peak values): Medium-Low - statistical significance demonstrated but not validated for actual error detection in practice

## Next Checks

1. Boundary case analysis: Systematically test adjacent lesion scenarios to quantify gradient attribution bleeding between instances and determine minimum separation distance for reliable instance-level attribution.
2. Ablation study: Compare max aggregation against sum and average aggregation approaches across lesion size ranges to validate the quantitative comparability claim.
3. Prospective error detection: Apply peak saliency thresholds to a held-out test set to evaluate true positive and false positive rates for error detection, calculating receiver operating characteristic curves.