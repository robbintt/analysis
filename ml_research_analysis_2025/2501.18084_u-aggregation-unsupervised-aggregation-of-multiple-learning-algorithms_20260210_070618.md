---
ver: rpa2
title: 'U-aggregation: Unsupervised Aggregation of Multiple Learning Algorithms'
arxiv_id: '2501.18084'
source_url: https://arxiv.org/abs/2501.18084
tags:
- data
- noise
- u-aggregation
- performance
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes U-aggregation, an unsupervised method for combining
  multiple pre-trained models without requiring true labels in the target population.
  The method addresses challenges of heteroskedasticity, adversarial models, and scaling
  differences through two main steps: (1) variance stabilization using Dyson Equation-based
  normalization to correct for heteroskedastic noise, and (2) iterative sparse signal
  recovery using an approximate message passing algorithm with soft-thresholding.'
---

# U-aggregation: Unsupervised Aggregation of Multiple Learning Algorithms

## Quick Facts
- arXiv ID: 2501.18084
- Source URL: https://arxiv.org/abs/2501.18084
- Authors: Rui Duan
- Reference count: 11
- Key outcome: U-aggregation combines pre-trained models without true labels using variance stabilization and sparse signal recovery, achieving correlations of 0.75-0.91 between estimated and actual model performance weights.

## Executive Summary
U-aggregation is an unsupervised method for combining multiple pre-trained models without requiring true labels in the target population. The method addresses challenges of heteroskedasticity, adversarial models, and scaling differences through two main steps: (1) variance stabilization using Dyson Equation-based normalization to correct for heteroskedastic noise, and (2) iterative sparse signal recovery using an approximate message passing algorithm with soft-thresholding. The method outperforms existing unsupervised aggregation techniques in both simulations and real-world applications, achieving correlations of 0.75-0.91 between estimated and actual model performance weights when applied to polygenic risk score models from the PGS Catalog. In practice, U-aggregation slightly outperformed the best individual model for predicting four complex traits in the All of Us cohort, demonstrating robust performance across different datasets and model configurations.

## Method Summary
U-aggregation operates by first normalizing prediction errors to address heteroskedasticity through Dyson Equation-based variance stabilization. This creates approximately homoskedastic noise that enables more reliable signal recovery. The second step employs an iterative approximate message passing algorithm with soft-thresholding to recover sparse signal weights that indicate which models are most reliable. The method assumes a rank-one signal structure and Gaussian noise distribution, allowing it to separate signal from noise without access to ground truth labels. The algorithm alternates between updating variance estimates and signal estimates until convergence, producing weights that can be used to combine model predictions in the target population.

## Key Results
- Achieved correlations of 0.75-0.91 between estimated and actual model performance weights across multiple simulations
- Slightly outperformed the best individual model for predicting four complex traits in the All of Us cohort
- Demonstrated robustness across different datasets and model configurations
- Successfully handled heteroskedastic noise and adversarial model scenarios

## Why This Works (Mechanism)
The method exploits the statistical structure of prediction errors across multiple models to identify which models contain the most reliable signal. By stabilizing variance through Dyson Equation normalization, it creates a more tractable optimization landscape for signal recovery. The approximate message passing algorithm then iteratively separates signal from noise under the assumption of sparsity (few models are truly reliable). This approach works because even without true labels, the relative performance patterns across models contain information about which models are most trustworthy, and the Gaussian noise assumption allows for tractable inference.

## Foundational Learning

**Dyson Equation**: Used for variance stabilization of prediction errors. Needed to handle heteroskedastic noise across different models. Quick check: Can be verified by examining whether normalized residuals show constant variance across models.

**Approximate Message Passing (AMP)**: Iterative algorithm for sparse signal recovery. Needed to separate reliable signal from noise in high-dimensional settings. Quick check: Monitor mean-squared error reduction across iterations to verify convergence.

**Soft-thresholding**: Non-linear operator used in AMP for sparsity promotion. Needed to enforce the assumption that only a few models are truly reliable. Quick check: Examine recovered weight vector sparsity pattern against ground truth when available.

## Architecture Onboarding

**Component Map**: Data -> Dyson Normalization -> AMP Iteration (Variance Update -> Signal Update -> Soft-thresholding) -> Converged Weights -> Model Combination

**Critical Path**: The core algorithm requires input predictions from pre-trained models, applies variance stabilization, then iteratively updates variance and signal estimates until convergence. The soft-thresholding step is critical for enforcing sparsity and preventing overfitting.

**Design Tradeoffs**: The method trades computational complexity (iterative algorithm) for accuracy in weight estimation. Assumes Gaussian noise and rank-one signal structure, which may not hold in all scenarios. The sparsity assumption may be too strong when multiple models perform well.

**Failure Signatures**: Poor performance when signal-to-noise ratio is extremely low, when all candidate models perform poorly, or when the rank-one signal assumption is violated. Convergence issues may arise with highly correlated model predictions or when sample size is small relative to the number of models.

**First Experiments**:
1. Test on synthetic data with known ground truth weights to verify correlation performance
2. Apply to case with intentionally introduced adversarial models to test robustness
3. Evaluate on small sample sizes (n < 100) to determine minimum data requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can external information regarding source-to-target similarity be explicitly incorporated into the U-aggregation framework?
- Basis in paper: [explicit] The discussion states such information "can be incorporated into the procedure, for example, by imposing constraints on u."
- Why unresolved: The paper focuses on a purely unsupervised setting and does not implement these constraints.
- What evidence would resolve it: A modified version of Algorithm 2 that accepts prior weights or variance structures derived from meta-data.

### Open Question 2
- Question: Can the theoretical guarantees of U-aggregation be formally extended to non-Gaussian noise distributions?
- Basis in paper: [inferred] The discussion notes the Gaussian noise assumption "may be restrictive" despite recent theoretical advancements in universality.
- Why unresolved: The current Approximate Message Passing (AMP) algorithm and its state evolution analysis rely on Gaussian noise properties.
- What evidence would resolve it: A proof of convergence for the estimator under heavy-tailed or uniform noise distributions without Gaussian assumptions.

### Open Question 3
- Question: How does the method perform when the signal matrix deviates from the assumed rank-one structure?
- Basis in paper: [explicit] The discussion identifies the "rank-one structure of the signal" as a "key assumption underlying the method."
- Why unresolved: The method is designed to recover a single signal vector; the impact of multi-rank signals on the estimator bias is not analyzed.
- What evidence would resolve it: Theoretical analysis or simulations showing estimator behavior when singular values beyond the first are significant.

## Limitations

- Performance unclear in scenarios with extremely low signal-to-noise ratios or when all candidate models perform poorly
- Method not extensively tested on very small sample sizes (n < 100) or edge cases with highly correlated model predictions
- Generalizability to domains outside genomics and polygenic risk scores remains untested
- Method's performance with vastly different model architectures or training objectives is speculative

## Confidence

*High Confidence*: The mathematical foundation of the Dyson Equation-based normalization and the approximate message passing algorithm are well-established techniques. The correlation results between estimated and actual weights (0.75-0.91) are reproducible and statistically significant.

*Medium Confidence*: The claim of slight improvement over the best individual model in real-world applications is supported by the All of Us cohort results, but the magnitude of improvement may be context-dependent and could vary with different datasets or trait types.

*Low Confidence*: The generalizability of U-aggregation to domains outside genomics and polygenic risk scores has not been tested. The method's performance when candidate models have vastly different architectures or training objectives remains speculative.

## Next Checks

1. Test U-aggregation on datasets with known poor-performing models to evaluate robustness when signal quality is low across all candidates.

2. Evaluate performance on small sample sizes (n < 100) to determine minimum data requirements and potential breakdown points.

3. Apply the method to non-genomic domains (e.g., image classification or natural language processing) with heterogeneous model architectures to assess cross-domain applicability.