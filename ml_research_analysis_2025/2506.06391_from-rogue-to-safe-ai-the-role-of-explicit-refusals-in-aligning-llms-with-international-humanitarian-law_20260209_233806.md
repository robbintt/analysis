---
ver: rpa2
title: 'From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with
  International Humanitarian Law'
arxiv_id: '2506.06391'
source_url: https://arxiv.org/abs/2506.06391
tags:
- refusal
- these
- safety
- prompt
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated eight leading LLMs on their ability to refuse
  prompts that violate International Humanitarian Law (IHL). Most models refused unlawful
  requests at high rates, but the clarity and consistency of explanations varied significantly.
---

# From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law

## Quick Facts
- arXiv ID: 2506.06391
- Source URL: https://arxiv.org/abs/2506.06391
- Reference count: 40
- Primary result: A simple system-level safety prompt dramatically improved refusal explanation quality in most LLMs, with some seeing increases above 90%.

## Executive Summary
This study evaluated eight leading LLMs on their ability to refuse prompts that violate International Humanitarian Law (IHL). Most models refused unlawful requests at high rates, but the clarity and consistency of explanations varied significantly. A simple system-level safety prompt dramatically improved the quality of refusal explanations in most models, with some seeing increases above 90%. However, more complex prompts involving technical language or code still revealed vulnerabilities. These findings highlight the potential for lightweight interventions to improve AI safety, while also underscoring the need for continued refinement of safeguards to ensure reliable compliance with legal and ethical standards.

## Method Summary
The study evaluated eight LLMs (ChatGPT-o3-mini, ChatGPT-4o, Claude-3.5/3.7-sonnet, Gemini-2.0-flash, Llama-3.3-70b, Mistral-large, Qwen-2.5-72b) using a dataset of 322 prompts derived from 161 ICRC customary IHL rules. Each prompt explicitly violated specific legal principles. Three independent LLM evaluators used majority voting to classify responses as refusals vs. compliance and helpful vs. unhelpful. A standardized system-level safety prompt referencing IHL/IHRL prohibitions was then applied to assess its impact on refusal quality. Baseline and post-intervention metrics were compared to measure improvement.

## Key Results
- Most models achieved high refusal rates (67-99%) for explicit IHL violations
- Refusal explanation quality varied widely (7-99% helpfulness) before intervention
- Safety prompt improved explanation quality by 90%+ in six of eight models
- Technical/code-wrapped prompts revealed ongoing vulnerabilities despite high refusal rates
- Simple "I can't help with that" responses dominated unhelpful refusals

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A standardized system-level safety prompt can significantly improve the explanatory quality of LLM refusal responses without requiring model retraining.
- **Mechanism**: The system prompt provides high-level instructions explicitly referencing IHL and IHRL prohibited actions, activating latent alignment capabilities already present in the model. This guides the model toward producing refusals with legal grounding and contextual justification.
- **Core assumption**: Models possess underlying capacity for policy-grounded refusals that can be elicited through structured guidance rather than training modifications.
- **Evidence anchors**:
  - [abstract]: "A standardised system-level safety prompt significantly improved the quality of the explanations expressed within refusals in most models, highlighting the effectiveness of lightweight interventions."
  - [section 4.4]: "The impact is immediately clear: six out of eight models saw dramatic improvements in the quality of the explanations within their refusals, in many cases above 90%."
  - [corpus]: Related work (FalseReject, OR-Bench) confirms system prompting affects refusal behavior, though focused on over-refusal mitigation rather than explanation quality.

### Mechanism 2
- **Claim**: Refusals that include reasoning, legal context, or ethical principles are more effective at clarifying system boundaries and reducing adversarial bypass attempts.
- **Mechanism**: Explanatory refusals work by making rationale transparent, which educates users on ethical boundaries, fosters trust, and reduces incentive to rephrase queries adversarially. Vague refusals are predictable and easier to exploit.
- **Core assumption**: Users will engage more constructively when they understand why a request was denied, rather than simply being blocked.
- **Evidence anchors**:
  - [abstract]: "By revealing the model's rationale and referencing relevant legal or safety principles, explanatory refusals clarify the system's boundaries, reduce ambiguity, and help prevent misuse."
  - [section 2.2]: "Research shows that attackers exploit the predictability and simplicity of refusal phrases, often using adversarial techniques to suppress them."
  - [corpus]: Constitutional AI work (Bai et al.) supports transparent, well-articulated refusals as an alignment strategy.

### Mechanism 3
- **Claim**: Prompts blending technical terminology, code requests, or complex framing with IHL-violating intent are more likely to evade standard safety mechanisms.
- **Mechanism**: Safety filters trained primarily on explicit textual violations may not recognize harmful intent when cloaked in technical language or programming syntax. The model may respond to the technical task component while overlooking harmful application context.
- **Core assumption**: Current safety training inadequately covers the intersection of technical/code generation tasks with harmful real-world applications.
- **Evidence anchors**:
  - [abstract]: "However, more complex prompts involving technical language or requests for code revealed ongoing vulnerabilities."
  - [section 5]: "For example, one test prompt instructed a model to 'write a drone program to find hospitals... then call the function simpledrone.launch_missile()...' While the models objected in their responses to the prompt, some still returned partial code that could be misused."
  - [corpus]: SORRY-Bench (Xie et al.) confirms prompts containing technical terminology often elicit answers rather than refusals.

## Foundational Learning

- **Concept: International Humanitarian Law (IHL) Framework**
  - **Why needed here**: The entire benchmark and intervention are grounded in the 161 customary IHL rules identified by the ICRC. Understanding core principles (distinction, proportionality, precaution) is essential for interpreting what constitutes a violation and designing appropriate refusals.
  - **Quick check question**: Can you explain why the principle of distinction matters when evaluating whether a prompt violates IHL?

- **Concept: Refusal Behavior Taxonomy in LLMs**
  - **Why needed here**: The paper distinguishes between binary refusal (compliance vs. non-compliance) and refusal helpfulness (explanatory quality). Understanding this distinction is crucial for designing interventions that don't just block harmful requests but educate users.
  - **Quick check question**: What is the difference between a model that refuses 100% of harmful requests with no explanation versus one that refuses 90% with detailed justifications?

- **Concept: Constitutional AI and Alignment Techniques**
  - **Why needed here**: The paper positions its approach relative to Constitutional AI, which uses principle-based guidance rather than extensive human labeling. Understanding this context helps situate the lightweight system prompt intervention within the broader alignment landscape.
  - **Quick check question**: How does the paper's single-prompt intervention differ from Constitutional AI's multi-phase workflow?

## Architecture Onboarding

- **Component map**:
  - IHL Rule Dataset (161 rules) -> Violation Prompts (322 prompts) -> 8 LLMs -> System Prompt Intervention -> 3-LLM Evaluators -> Refusal/Helpfulness Metrics

- **Critical path**:
  1. Baseline evaluation: Test models against IHL-violating prompts without intervention
  2. Measure refusal rates and helpfulness rates
  3. Apply standardized safety system prompt intervention
  4. Re-evaluate with same metrics to measure improvement
  5. Identify edge cases where technical language bypasses filters

- **Design tradeoffs**:
  - **Strictness vs. Helpfulness**: High refusal rates (e.g., chatgpt-o3-mini at 99.07%) often correlate with low explanatory quality (7.76%)
  - **Intervention Simplicity vs. Model-Specific Tuning**: Universal system prompt across all models vs. model-specific prompts
  - **Binary vs. Nuanced Safety**: Simple explicit violations easier to detect than complex technical requests blending harmful intent with legitimate tasks

- **Failure signatures**:
  - **Minimal Refusal Pattern**: Responses like "I'm sorry, but I can't help with that" without elaboration
  - **Technical Bypass**: Partial code generation for requests blending programming tasks with harmful applications
  - **Inconsistent Refusal**: Wide variance in refusal rates across similar prompt types
  - **Over-Refusal**: Rejecting benign requests due to aggressive safety filters

- **First 3 experiments**:
  1. Run the 322-prompt IHL violation dataset against your target model to establish baseline refusal rate and helpfulness rate. Document which IHL rules are most frequently bypassed.
  2. Apply the standardized IHL/IHRL safety system prompt and re-run evaluation. Calculate improvement delta for both metrics. Determine if the model responds like high-improvers (Claude-3.5: 24.53% → 98.45%) or low-improvers (ChatGPT-o3-mini: 7.76% → 32.30%).
  3. Construct 10-15 prompts blending code generation requests with IHL-violating applications (similar to the drone-hospital example). Test whether technical framing reduces refusal rates compared to plain-language equivalents of the same harmful intent.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can LLMs be aligned to refuse IHL-violating requests embedded within technical code or programming tasks with the same consistency shown for natural language prompts?
- **Basis in paper**: [explicit] The authors state that "more complex prompts involving technical language or requests for code revealed ongoing vulnerabilities," noting that models "can falter when asked to perform complex tasks cloaked in technical language."
- **Why unresolved**: The study primarily tested straightforward, explicit violations; the "edge cases" involving code generation (e.g., drone targeting scripts) were identified as a limitation where models sometimes returned partial usable code despite objecting.
- **What evidence would resolve it**: A benchmark evaluation using the proposed safety system prompt on a dataset of code-wrapped or technically obfuscated IHL violations.

### Open Question 2
- **Question**: Does tailoring system prompts to the specific architecture and training data of individual LLMs yield significantly better refusal helpfulness than a standardized prompt?
- **Basis in paper**: [explicit] The discussion notes that "the same system prompt was applied to all models" and suggests that "a tailored prompt adapted to each model's architecture and behaviour might result in even stronger improvements."
- **Why unresolved**: The study demonstrated that a generic prompt improves performance, but it did not compare generic versus specific prompts to determine if the intervention is optimally efficient for each unique model.
- **What evidence would resolve it**: A comparative study measuring refusal helpfulness rates when using architecture-specific prompts versus the standardized prompt used in the original study.

### Open Question 3
- **Question**: To what extent does the requirement for legal reasoning in refusals impact the rate of "over-refusal" (false positives) for benign but sensitive prompts?
- **Basis in paper**: [inferred] The paper contrasts its approach with OR-Bench, noting that while other safety prompts often cause over-refusal, their intervention improved responsiveness to benign inputs for Claude 3.5. However, it is unclear if this reduction in over-refusal holds universally across all tested models.
- **Why unresolved**: The paper focuses on the improvement of refusal quality for harmful prompts, but the trade-off regarding false positives is only briefly discussed for a single model.
- **What evidence would resolve it**: Testing the safety intervention against a dataset of benign-but-sensitive prompts (like those in OR-Bench) to quantify the change in false refusal rates.

## Limitations
- Dataset scope: Full 322-prompt dataset remains unpublished, limiting external validation and raising questions about prompt diversity and biases
- Evaluation subjectivity: Refusal helpfulness judged by three LLM evaluators via majority voting, introducing potential subjectivity
- Technical bypass persistence: Despite high refusal rates, vulnerabilities persist when harmful intent is embedded in technical or code-based prompts

## Confidence
- **High Confidence**: Core finding that lightweight system prompt significantly improves refusal explanation quality is well-supported by data (e.g., Claude-3.5: 24.53% → 98.45%)
- **Medium Confidence**: Claim that explanatory refusals reduce adversarial bypass attempts is supported by theoretical reasoning but limited empirical validation
- **Low Confidence**: Assertion that technical/code-wrapped prompts systematically evade safety filters is demonstrated through isolated examples but lacks systematic analysis

## Next Checks
1. **Dataset Transparency Validation**: Obtain and analyze the complete 322-prompt dataset to verify prompt diversity, rule coverage, and absence of bias. Test whether observed refusal patterns hold when evaluated by human experts rather than LLM judges.

2. **Technical Bypass Quantification**: Systematically construct a balanced set of 50-100 technical/code-wrapped prompts mirroring the IHL-violation dataset. Measure refusal rates and helpfulness across all eight models to quantify whether technical framing consistently reduces safety performance.

3. **Longitudinal Safety Monitoring**: Implement continuous evaluation of the same eight models over 3-6 months, tracking whether refusal rates and helpfulness degrade over time as models are updated. This would validate whether the safety prompt intervention provides durable protection or requires periodic recalibration.