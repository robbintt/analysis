---
ver: rpa2
title: Towards a Comprehensive Scaling Law of Mixture-of-Experts
arxiv_id: '2509.23678'
source_url: https://arxiv.org/abs/2509.23678
tags:
- scaling
- loss
- experts
- arxiv
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a comprehensive scaling law for Mixture-of-Experts
  (MoE) models, addressing the gap between existing dense scaling laws and the unique
  characteristics of MoE architectures. The authors identify five key factors influencing
  MoE model performance: data size (D), total model size (N), activated model size
  (Na), number of active experts (G), and ratio of shared experts (S).'
---

# Towards a Comprehensive Scaling Law of Mixture-of-Experts

## Quick Facts
- arXiv ID: 2509.23678
- Source URL: https://arxiv.org/abs/2509.23678
- Reference count: 40
- Primary result: Proposes comprehensive MoE scaling law with 0.0059 average validation loss error

## Executive Summary
This paper addresses the gap between existing dense scaling laws and the unique characteristics of Mixture-of-Experts (MoE) architectures by proposing a comprehensive scaling law that captures five key factors: data size (D), total model size (N), activated model size (Na), number of active experts (G), and ratio of shared experts (S). Through 446 controlled experiments, the authors construct a joint MoE scaling law that accounts for non-monotonic and coupled effects of these factors. The proposed law significantly outperforms existing MoE scaling laws with an average validation loss error of 0.0059, providing valuable guidance for future MoE model design and training.

## Method Summary
The authors conducted 446 controlled experiments varying five key factors: total model size (N), data size (D), activated model size (Na), number of activated experts (G), and shared expert ratio (S). They used Dolma V1.7 dataset and trained MoE models with AdamW optimizer, Cosine LR scheduling, and batch size of 2M tokens. The key innovation is maintaining controlled variable isolation when varying activated size, using specific relationships between expert dimensions and routed experts to keep total parameters constant. Non-linear least squares optimization was used to fit 12 constants to the experimental data, validating the scaling law against models up to 9B parameters.

## Key Results
- Optimal number of activated experts G is approximately 7, validated against DeepSeek-V3.1 and Kimi-K2
- Optimal ratio of shared experts S ranges from 13% to 31%, with a flat minimum across this range
- Optimal activated parameter ratio Na/N becomes sparser as total model size N increases, following Na/N ∝ N^(-α/(α+1))
- The proposed scaling law achieves 0.0059 average validation loss error, significantly outperforming existing MoE scaling laws

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Optimal expert granularity exists at G ≈ 7, beyond which finer expert partitioning degrades performance
- Mechanism: As G increases while holding Na fixed, individual expert dimensions shrink. Below the optimal point, finer granularity improves specialization; above it, expert capacity becomes insufficient for knowledge representation, causing a "hook function" relationship where loss first decreases then increases
- Core assumption: The relationship follows L(G) = eG + f/G, implying a mathematical optimum at G = √(f/e)
- Evidence anchors:
  - [abstract] "the optimal number of activated experts G is approximately 7"
  - [section 5.1] Derivation shows G_opt = √(f/e) ≈ 6.78, validated against DeepSeek-V3.1, Kimi-K2 (G=8-9)
  - [corpus] Related work (Ludziejewski et al., 2025) examines activated model size effects but doesn't derive optimal G; this paper uniquely provides the mathematical optimum
- Break condition: Expert dimension falls below threshold where individual experts cannot represent necessary knowledge patterns (observed as loss increase in Figure 3 when G > 10-15)

### Mechanism 2
- Claim: Optimal activation ratio (Na/N) decreases as total model size increases, making larger models increasingly sparse
- Mechanism: Two opposing terms govern Na's effect: c/N_a^α (larger Na reduces loss) and h·Na/N (larger Na increases loss due to structural distortion). The optimal balance shifts as N grows, favoring sparsity
- Core assumption: Assumption: The efficiency-aware optimum occurs when marginal loss reduction per unit cost falls below threshold ΔLoss
- Evidence anchors:
  - [abstract] "optimal activated parameter ratio Na/N becomes sparser as the total model size N increases"
  - [section 5.3, Eq. 14] Mathematical derivation: (Na/N)_optt ∝ N^(-α/(α+1)), showing inverse relationship
  - [section 5.3, Table 4] Validated: For N=30B→671B, theoretical optimal ranges from 40%→22%, practical efficiency-aware from 9%→5%
  - [corpus] Weak corpus evidence - related papers don't derive the sparsity-N relationship explicitly
- Break condition: When Na/N exceeds ~50% (very dense), MoE structural advantages collapse; when below ~3% (very sparse), insufficient activated capacity harms performance

### Mechanism 3
- Claim: Shared experts at ratio S ≈ 13-31% improve performance by capturing general knowledge, with benefits relatively flat near optimum
- Mechanism: Shared experts process all tokens, providing stable knowledge representation. Loss follows quadratic L(S) = mS² + nS with optimal at S_opt = -n/2m ≈ 0.31. The quadratic coefficient is small, creating a flat minimum
- Core assumption: Assumption: Shared and routed experts have distinct roles - shared for general knowledge, routed for specialization
- Evidence anchors:
  - [abstract] "optimal ratio of shared experts S ranges from 13% to 31%"
  - [section 5.2] "loss deviation to the optimal setting's less than 0.001" across 13-31% range
  - [section 4.5, Figure 4] Empirical validation showing clear quadratic pattern emerging at larger scales
  - [corpus] No corpus evidence explicitly derives optimal shared expert ratio; DeepSeekMoE (Dai et al., 2024) proposes shared experts but without quantitative optimization
- Break condition: S=0 (no shared experts) shows measurable performance drop; S>50%过度约束专家多样性

## Foundational Learning

- Concept: **Power-law scaling (Chinchilla scaling laws)**
  - Why needed here: The base MoE scaling law extends dense scaling laws (L = a/N^α + b/D^β + ε) with MoE-specific terms. Understanding why N and D follow power laws enables interpreting the additional MoE terms
  - Quick check question: Given fixed compute budget C = D × Na, if you double model size N, approximately how much should you increase data D for compute-optimal training?

- Concept: **Controlled variable experiments**
  - Why needed here: The paper's 446 experiments isolate marginal effects by varying one factor (G, S, Na) while holding others constant. This methodology is critical for disentangling coupled effects
  - Quick check question: If you want to test the effect of G on loss while keeping N and Na fixed, what must you adjust simultaneously?

- Concept: **Hook function (non-monotonic optimization)**
  - Why needed here: Unlike power laws (monotonically decreasing), MoE factors G, S, Na/N exhibit hook-shaped behavior with interior optima. This is why "more experts" or "more activation" isn't always better
  - Quick check question: For a hook function f(x) = ax + b/x, at what x value does the minimum occur? What does the second derivative tell you?

## Architecture Onboarding

- Component map:
MoE Layer = Router + Shared Experts + Routed Experts
Key parameters:
- G (activated experts) = n_k (routed) + n_s (shared), typically 7-9
- S (shared ratio) = n_s / G, target 0.13-0.31
- Na/N (activation ratio) = activated params / total params
  - Theory optimal: 20-40% for N < 100B
  - Efficiency-aware: 5-9% for production models

- Critical path: Start with N and D from compute budget → calculate theoretical Na/N optimal from Eq. 14 → set G ≈ 7 → set S = 1-2 shared experts (for G=7, S≈0.14-0.29) → scale expert dimensions to achieve target Na

- Design tradeoffs:
  - **G vs expert size**: Higher G = smaller experts = finer specialization but risk of under-capacity
  - **Na/N vs efficiency**: Higher activation = better performance but linear cost increase
  - **S vs specialization**: Higher S = more stable general knowledge but less routing flexibility

- Failure signatures:
  - Loss increases with more experts: G > optimal (expert dimensions too small)
  - Training unstable with large models: Na/N too sparse (<3%), insufficient gradient flow
  - No improvement from shared experts: S outside 13-31% range or implementation bug
  - Prediction error > 0.01: Config outside tested ranges (G > 20, S > 0.7, or extreme Na/N)

- First 3 experiments:
  1. **Validation sweep**: For target N=1B, D=50B, test G ∈ {5, 7, 9, 12} with S=0.2 fixed to verify hook function and locate your optimum
  2. **Efficiency-aware Na calibration**: Starting from theoretical Na/N optimal (Eq. 14), increment Na by 0.01N until loss reduction < 0.001 per step to find your efficiency-aware optimum
  3. **Shared expert robustness check**: Test S ∈ {0.1, 0.2, 0.3, 0.4} with G=7 to confirm flat minimum behavior and implementation correctness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the proposed scaling law maintain its predictive accuracy when extrapolated to model sizes significantly larger than the 9B parameter / 100B token validation limit used in the study?
- Basis in paper: [explicit] Section 7 (Limitations) states the analysis "has not been validated at extremely larger scales" due to resource limits, identifying this as necessary future work.
- Why unresolved: Current validation is capped at 9B total parameters, whereas state-of-the-art MoE models often exceed several hundred billion or trillion parameters.
- What evidence would resolve it: Validation loss measurements from training runs on trillion-parameter models compared against the law's predictions.

### Open Question 2
- Question: How does the inclusion of non-MoE architectural factors, such as attention head configurations or layer normalization, impact the joint scaling law and the derived optimal expert settings?
- Basis in paper: [explicit] Section 7 suggests extending the scope to "encompass both the factors and structural configurations associated with other LLM blocks, such as the attention layers."
- Why unresolved: The current formula isolates MoE-specific factors (G, S, Na) but treats other structural hyperparameters (e.g., attention heads) as fixed or implicit within N.
- What evidence would resolve it: A reformulated scaling law that explicitly models attention parameters and demonstrates improved fit across varied Transformer architectures.

### Open Question 3
- Question: Are the optimal values derived for activated experts (G ≈ 7) and shared expert ratios (S ≈ 0.31) robust across alternative routing strategies like Expert Choice or heterogeneous expert designs?
- Basis in paper: [inferred] The authors explicitly restrict the experimental setup to "classical Top-K routing" (Section 3/6) and homogeneous experts, noting the existence of alternative strategies (e.g., HMoE, AoE) that were not analyzed.
- Why unresolved: The relationship between granularity (G) and performance is coupled with the routing mechanism; different routing algorithms may shift the optimal balance between expert count and size.
- What evidence would resolve it: Controlled experiments measuring the loss landscape of G and S using non-Top-K routing mechanisms to see if the minimum remains at ≈ 7.

## Limitations

- **Extrapolation uncertainty**: The scaling law is validated only up to 9B parameters and 100B tokens, requiring validation for frontier models (500B+)
- **Implementation dependency**: Optimal S=13-31% assumes specific shared expert implementation details that may vary across architectures
- **Dataset specificity**: Scaling law constants are fit to Dolma V1.7 dataset and may require recalibration for domain-specific data

## Confidence

- **High confidence** in functional forms and optimal ranges: The hook functions for G and Na, quadratic for S, and power laws for N and D are mathematically derived and empirically validated across 446 experiments
- **Medium confidence** in extrapolation: While the theoretical basis supports scaling beyond tested ranges, the sparsity trend and exact optimal values may shift at frontier scales
- **Medium confidence** in practical deployment: The efficiency-aware optimal balances performance and cost but requires careful implementation to avoid failure modes

## Next Checks

1. **Frontier extrapolation test**: Validate the Na/N ∝ N^(-α/(α+1)) relationship by training a 50B+ parameter MoE with the predicted optimal activation ratio. Compare loss against baseline dense scaling law predictions.

2. **Domain robustness check**: Apply the scaling law to domain-specific datasets (code, biomedical) by fitting new coefficients while preserving the functional form. Test whether the optimal G≈7 and S=13-31% ranges remain consistent across domains.

3. **Implementation sensitivity analysis**: Vary shared expert architecture (capacity, routing mechanism, expert dimension) and re-fit the scaling law. Quantify how implementation choices affect the optimal S range and whether the quadratic form holds.