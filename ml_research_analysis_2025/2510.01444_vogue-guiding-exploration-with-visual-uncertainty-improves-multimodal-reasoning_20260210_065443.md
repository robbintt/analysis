---
ver: rpa2
title: 'VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning'
arxiv_id: '2510.01444'
source_url: https://arxiv.org/abs/2510.01444
tags:
- arxiv
- uncertainty
- reasoning
- branch
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of exploration in multimodal
  reinforcement learning with verifiable rewards (RLVR), where current methods treat
  visual inputs as deterministic and fail to distinguish between perceptual ambiguity
  and reasoning uncertainty. The authors propose DUPL, a dual-uncertainty guided policy
  learning approach that quantifies both perceptual uncertainty (via symmetric KL
  divergence between raw and noisy visual branches) and output uncertainty (via policy
  entropy).
---

# VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning

## Quick Facts
- arXiv ID: 2510.01444
- Source URL: https://arxiv.org/abs/2510.01444
- Reference count: 28
- Primary result: DUPL achieves accuracy gains of up to 11.2% on visual math tasks and 7.1% on general-domain reasoning tasks, outperforming GRPO baselines across six benchmarks

## Executive Summary
This paper addresses the challenge of exploration in multimodal reinforcement learning with verifiable rewards (RLVR), where current methods treat visual inputs as deterministic and fail to distinguish between perceptual ambiguity and reasoning uncertainty. The authors propose DUPL, a dual-uncertainty guided policy learning approach that quantifies both perceptual uncertainty (via symmetric KL divergence between raw and noisy visual branches) and output uncertainty (via policy entropy). These uncertainties are integrated into an uncertainty-driven feedback loop to shape advantage functions for targeted exploration. The method employs a dynamic branch prioritization mechanism that emphasizes exploration early in training and stabilizes on the raw view later. Implemented on Qwen2.5-VL models (3B/7B scales), DUPL achieves accuracy gains of up to 11.2% on visual math tasks and 7.1% on general-domain reasoning tasks, consistently outperforming GRPO baselines across six benchmarks. The results demonstrate that grounding exploration in visual input uncertainty is an effective strategy for improving multimodal reasoning.

## Method Summary
DUPL implements a dual-branch forward pass architecture where each training sample is processed through both raw and perturbed visual inputs. The symmetric KL divergence between policy distributions from these branches quantifies perceptual uncertainty, while token-level entropy measures output uncertainty. These uncertainty signals are scaled and added to advantage functions via stop-gradient operations to guide exploration toward perceptually ambiguous states and high-entropy action spaces. A dynamic branch prioritization mechanism anneals from noisy to raw branch selection over training steps, balancing exploration with exploitation. The method is built on top of GRPO and requires doubling forward-pass computation per training step.

## Key Results
- DUPL achieves 11.2% accuracy improvement on visual math tasks compared to GRPO baseline
- General-domain reasoning tasks show 7.1% accuracy gains with DUPL
- Dynamic branch prioritization outperforms fixed-probability sampling by up to 3.6% on mathematical reasoning benchmarks
- Symmetric KL divergence is critical; forward KL causes accuracy collapse

## Why This Works (Mechanism)

### Mechanism 1: Perceptual Uncertainty as Active Sensitivity Probe
Converting visual perturbations into measured divergence signals enables targeted exploration toward perceptually ambiguous states. The symmetric KL divergence between raw and perturbed branches quantifies sensitivity to visual changes, with the guidance signal scaled and added to advantage functions. High divergence indicates under-explored visual features warranting additional learning signal.

### Mechanism 2: Output Uncertainty for Action-Space Exploration
Token-level policy entropy provides complementary exploration signal for textual output space. High entropy at token positions indicates decision uncertainty that benefits from exploration pressure. The entropy bonus is scaled and clipped to advantage magnitude, encouraging stochasticity where beneficial.

### Mechanism 3: Dynamic Branch Prioritization for Explore-Exploit Balance
Annealing the probability of selecting the noisy branch enables early exploration while preserving late-stage convergence. Early training favors noisy branch for broad exploration; later training shifts to raw branch for stable optimization. This temporal structure matters, with fixed 0.5 sampling underperforming by 2.5% average.

## Foundational Learning

- **Symmetric KL Divergence**: Balances exploration pressure with numerical stability. Forward KL alone produces unstable, excessively large signals. Quick check: Given p=[0.9, 0.1] and q=[0.1, 0.9], compute both D_KL(p||q) and D_KL(q||p). Which direction dominates? What does their average imply?

- **GRPO (Group Relative Policy Optimization)**: DUPL builds on GRPO's advantage normalization. Understanding group-based normalization is prerequisite to seeing where uncertainty bonuses are injected. Quick check: In GRPO, why normalize rewards within a group before computing advantage? What happens if all rewards in a group are identical?

- **Stop Gradient Operator**: Prevents uncertainty estimator from being directly optimized. The guidance signals use stopgrad to modulate update magnitude without affecting gradient propagation. Quick check: If we removed stopgrad from g_per, would the model learn to minimize or maximize perceptual uncertainty? What are the implications for exploration?

## Architecture Onboarding

- **Component map**: Input (x_text, x_image) → Raw Branch → p_raw → A_raw → Â_raw = A_raw + g_out_raw; Perturb T(x) → Noisy Branch → p_noisy → A_noisy → Â_noisy = A_noisy + g_out_noisy + g_per → Branch Selector: z ~ Bernoulli(p_noi(s)) selects which Â to use for policy update

- **Critical path**: 1) Implement augmentation function T (flips, rotations, color jitter, Gaussian noise σ=0.4); 2) Compute dual-branch forward passes per sample (doubles compute per batch); 3) Token-level symmetric KL between branches → u_per; 4) Token-level entropy per branch → u_out; 5) Scale uncertainties (α_p=1.0, β_p=2.0, α_o=0.4, β_o=2.0) and clip to advantage magnitude; 6) Apply branch prioritization schedule during training

- **Design tradeoffs**: Dual-branch forward pass doubles inference cost per training step; perturbation level σ=0.4 found optimal (lower insufficient, higher unstable); symmetric vs forward KL critical (forward causes collapse)

- **Failure signatures**: Training accuracy declining after initial rise → likely using forward KL instead of symmetric; no improvement over GRPO → check that g_per, g_out are actually being added to advantage; Pass@4 not improving alongside Pass@1 → exploration not actually increasing; verify p_noi(s) is decaying correctly

- **First 3 experiments**: 1) Run GRPO without DUPL modifications on MMRL30k for 200 steps to validate baseline; 2) Implement dual-branch passes but set α_p=α_o=0 (no uncertainty bonus) to compare intermediate performance; 3) Log u_per values across training to verify expected pattern (moderate increase early, gradual decrease later)

## Open Questions the Paper Calls Out

### Open Question 1
Can learnable or adversarial perturbation strategies expose subtler reasoning vulnerabilities more effectively than standard geometric and Gaussian noise augmentations? The authors identify exploring more sophisticated perturbation strategies as a promising avenue for future investigation.

### Open Question 2
Does extending the dual-uncertainty framework to include textual input perturbations yield complementary improvements in multimodal reasoning? The current framework treats textual input as fixed, suggesting extending this paradigm to incorporate textual perturbations presents a worthy direction.

### Open Question 3
How effectively does the visual uncertainty estimation mechanism generalize to dynamic modalities like video, where temporal consistency introduces additional complexity? The authors explicitly list adapting the methodology to dynamic modalities as a worthy direction for future work.

### Open Question 4
What is the precise trade-off between accuracy gains and computational overhead introduced by the mandatory dual-branch forward pass? The paper reports accuracy improvements but does not explicitly analyze wall-clock training time or FLOP efficiency relative to the baseline.

## Limitations

- The method requires dual forward passes per training sample, effectively doubling computational requirements without quantified efficiency analysis
- Perturbation function and uncertainty hyperparameters appear tuned to specific model architecture and task distribution, with limited evidence of transferability across domains
- Evaluation focuses exclusively on Qwen2.5-VL models, leaving uncertainty about generalizability to other multimodal architectures

## Confidence

- **High confidence**: Dual-branch architecture and symmetric KL divergence computation are well-grounded in established uncertainty quantification literature
- **Medium confidence**: Specific formulation of uncertainty-guided advantage modification shows promise but lacks extensive ablation studies across diverse perturbation types
- **Low confidence**: Optimality of dynamic branch prioritization schedule (linear decay) is asserted but not rigorously compared against alternative annealing functions

## Next Checks

1. **Transferability test**: Implement DUPL with different perturbation functions (e.g., cutout, adversarial patches) and different model scales (Qwen2.5-VL-1.5B) to assess sensitivity to architectural changes

2. **Computational efficiency analysis**: Measure training throughput and wall-clock time for DUPL versus GRPO across multiple GPU configurations to quantify the compute-accuracy trade-off

3. **Uncertainty calibration validation**: Track whether perceptual uncertainty u_per correlates with downstream accuracy improvements across different reasoning task categories, particularly distinguishing visual ambiguity from reasoning difficulty