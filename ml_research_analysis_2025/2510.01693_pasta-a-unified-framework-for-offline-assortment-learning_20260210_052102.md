---
ver: rpa2
title: 'PASTA: A Unified Framework for Offline Assortment Learning'
arxiv_id: '2510.01693'
source_url: https://arxiv.org/abs/2510.01693
tags:
- assortment
- choice
- optimization
- pasta
- have
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PASTA, a pessimistic learning framework for
  offline assortment optimization under general choice models. The core challenge
  addressed is insufficient data coverage, where historical data may lack observations
  for certain assortments, making traditional estimation-then-optimization approaches
  unreliable.
---

# PASTA: A Unified Framework for Offline Assortment Learning
## Quick Facts
- arXiv ID: 2510.01693
- Source URL: https://arxiv.org/abs/2510.01693
- Reference count: 40
- Primary result: Unified pessimistic learning framework for offline assortment optimization with theoretical guarantees and empirical superiority

## Executive Summary
This paper introduces PASTA (Pessimistic Assortment Selection via Testing and Aggregation), a robust offline learning framework for assortment optimization under general choice models. The core innovation addresses the challenge of insufficient data coverage, where historical observations may not include all possible assortments, making traditional estimation-then-optimization approaches unreliable. PASTA constructs an uncertainty set through likelihood ratio tests and optimizes for worst-case revenue across all models within this set, ensuring robustness against estimation errors for under-explored assortments.

The framework provides finite-sample regret bounds under broad choice model classes (MNL, latent class logit, nested logit) with only coverage at the optimal assortment required. PASTA is shown to be minimax optimal under the MNL model, and extensive empirical results demonstrate consistent outperformance over baseline methods across multiple choice models, maintaining gains across varying data coverage levels and model complexity.

## Method Summary
PASTA operates by first constructing an uncertainty set of plausible choice models using likelihood ratio tests based on historical transaction data. This set captures the range of models consistent with observed behavior while being robust to estimation errors for assortments with limited historical data. The framework then solves a pessimistic optimization problem that maximizes the worst-case expected revenue across all models in the uncertainty set. This approach ensures that the selected assortment performs well even under the most unfavorable but plausible choice model, addressing the fundamental challenge of learning from incomplete coverage in offline settings.

The theoretical analysis establishes finite-sample regret bounds that scale with the log of the number of assortments rather than the total number of possible assortments, a crucial improvement for practical applications. The framework is designed to work under minimal coverage assumptions, requiring only that the optimal assortment has been observed in historical data. For the MNL model specifically, PASTA achieves minimax optimality, demonstrating that it cannot be substantially improved upon in the worst case.

## Key Results
- Achieves finite-sample regret bounds under broad choice model classes with only optimal assortment coverage required
- Demonstrated minimax optimality under MNL model, establishing theoretical limits of performance
- Consistently outperforms baseline methods in both regret and assortment accuracy across synthetic experiments with varying data coverage and model complexity

## Why This Works (Mechanism)
PASTA's pessimistic optimization approach directly addresses the fundamental challenge of offline assortment learning: historical data typically lacks observations for all possible assortments, making traditional estimation-then-optimization unreliable. By constructing an uncertainty set through likelihood ratio tests, PASTA captures the range of plausible choice models consistent with observed data while being robust to estimation errors for under-explored assortments. The worst-case optimization ensures that the selected assortment performs well even under the most unfavorable but plausible model, effectively hedging against the uncertainty inherent in offline learning from incomplete data.

## Foundational Learning
- **Likelihood ratio tests**: Used to construct uncertainty sets of plausible choice models; needed to quantify which models are consistent with observed data while being robust to limited coverage
- **Pessimistic optimization**: Maximizes worst-case performance across uncertainty set; needed to ensure robustness when historical data lacks coverage for certain assortments
- **Finite-sample regret analysis**: Provides theoretical guarantees on performance; needed to understand how PASTA's performance scales with available data
- **Minimax optimality**: Establishes theoretical limits of performance; needed to demonstrate that PASTA cannot be substantially improved upon in worst-case scenarios
- **Choice model classes (MNL, latent class, nested logit)**: Framework works across broad model families; needed to ensure applicability to real-world scenarios with complex consumer choice behavior
- **Coverage assumptions**: Only requires coverage at optimal assortment; needed to make framework practical when data is sparse for many assortments

## Architecture Onboarding
**Component map**: Historical data -> Likelihood ratio tests -> Uncertainty set construction -> Pessimistic optimization -> Assortment selection
**Critical path**: Data coverage at optimal assortment → Uncertainty set construction → Worst-case optimization → Regret minimization
**Design tradeoffs**: Robustness vs. optimality (pessimism ensures performance under uncertainty but may sacrifice some potential gains), computational complexity vs. theoretical guarantees (larger uncertainty sets provide better coverage but increase optimization difficulty)
**Failure signatures**: Poor performance when optimal assortment has minimal historical data, computational intractability when uncertainty set becomes too large, degradation when choice model assumptions are severely violated
**3 first experiments**:
1. Test regret scaling with increasing data coverage on synthetic MNL data with known parameters
2. Compare assortment accuracy across different choice models (MNL, latent class, nested logit) under varying levels of data sparsity
3. Benchmark computational runtime of PASTA's optimization subroutine against data dimension and choice model complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Computational scalability concerns when uncertainty set becomes large under high-dimensional assortment spaces or complex choice models
- Performance degradation when even optimal assortment has minimal historical data coverage
- Limited evaluation on real-world data, with current results based primarily on synthetic datasets with controlled parameters

## Confidence
- Theoretical guarantees (regret bounds, minimax optimality): High
- Empirical performance claims: Medium (based on synthetic data)
- Computational efficiency claims: Low (limited practical complexity analysis)

## Next Checks
1. Implement PASTA on real-world retail transaction datasets to evaluate performance under realistic noise and coverage patterns
2. Benchmark computational runtime of PASTA's optimization subroutine against data dimension and choice model complexity using both synthetic and real datasets
3. Test PASTA's robustness when historical data provides minimal coverage even at the optimal assortment, comparing performance degradation against theoretical predictions