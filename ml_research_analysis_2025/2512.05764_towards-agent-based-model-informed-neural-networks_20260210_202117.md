---
ver: rpa2
title: Towards agent-based-model informed neural networks
arxiv_id: '2512.05764'
source_url: https://arxiv.org/abs/2512.05764
tags:
- neural
- dynamics
- network
- graph
- networks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ABM-informed Neural Networks (ABM-NNs), a
  framework that incorporates agent-based model principles into neural differential
  equations to preserve structural constraints while learning complex dynamics. The
  method decomposes dynamics into self-interaction and interaction-with-neighbors
  components using restricted graph neural networks, enabling modeling of systems
  where physical invariants are absent but other constraints (like mass conservation)
  must be enforced.
---

# Towards agent-based-model informed neural networks

## Quick Facts
- arXiv ID: 2512.05764
- Source URL: https://arxiv.org/abs/2512.05764
- Reference count: 0
- ABM-NNs combine agent-based model constraints with neural differential equations for interpretable dynamics learning

## Executive Summary
This paper introduces ABM-informed Neural Networks (ABM-NNs), a framework that incorporates agent-based model principles into neural differential equations to preserve structural constraints while learning complex dynamics. The method decomposes dynamics into self-interaction and interaction-with-neighbors components using restricted graph neural networks, enabling modeling of systems where physical invariants are absent but other constraints (like mass conservation) must be enforced. Three case studies demonstrate effectiveness: recovering exact parameters in a GLV system from short trajectories under interventions; outperforming GCN, GraphSAGE, and GraphGPS baselines in SIR contagion modeling with MAPE 31.86% versus 88.85-188.47% on out-of-sample graphs; and learning interpretable macroeconomic couplings from empirical GDP and macro data, enabling counterfactual policy analysis through interaction matrix manipulation.

## Method Summary
ABM-NNs decompose dynamics into self-interaction (feed-forward NN) and neighbor-interaction (restricted GNN) components while preserving ABM constraints like mass conservation. The framework enforces hard constraints through differential equation decomposition where F+G+H=0 exactly. For training, differentiable RK4 rollout is used with curriculum learning that gradually increases rollout length from t=1 to t_train≤30. The architecture employs 3-layer MLPs with 32 hidden units, LeakyReLU activations, and ReLU output for nonnegativity. Training uses Adam optimizer with learning rate 10⁻⁴, gradient clipping (max norm 10.0), and regularization enforcing ϕ₁(S,0)≈0 and ϕ₂(0)≈0 after 100 epochs warmup. Three case studies validate the approach: GLV parameter recovery, SIR contagion modeling on Erdős–Rényi graphs (n=100, p=0.05, β=0.4, γ=0.2), and macroeconomic coupling analysis.

## Key Results
- Recovered exact parameters in GLV system from short trajectories under interventions
- Achieved MAPE 31.86% versus 88.85-188.47% on out-of-sample SIR graphs, outperforming GCN, GraphSAGE, and GraphGPS baselines
- Learned interpretable macroeconomic couplings from empirical GDP and macro data, enabling counterfactual policy analysis

## Why This Works (Mechanism)
The framework succeeds by enforcing structural constraints directly in the neural architecture through hard constraint decomposition (F+G+H=0), rather than learning them implicitly. This preserves interpretability while maintaining the flexibility of data-driven learning. The curriculum rollout strategy prevents training instability by gradually increasing sequence length, and the regularization ensures learned functions respect boundary conditions.

## Foundational Learning
- Graph Neural Networks (GNNs): Essential for modeling neighbor interactions in ABM-NNs. Quick check: Implement message passing on Erdős–Rényi graph with varying node degrees.
- Differential Equations with Neural Networks: Core to ABM-NNs' continuous-time modeling. Quick check: Train neural ODE on synthetic GLV system and verify parameter recovery.
- Curriculum Learning: Critical for stable training with long rollouts. Quick check: Compare training stability with and without gradual rollout length increase.
- Mass Conservation Constraints: Fundamental for ABM-NN interpretability. Quick check: Monitor S+I+R conservation during SIR model rollout.
- Regularization of Neural Functions: Ensures boundary conditions are respected. Quick check: Test regularization effects on ϕ₁(S,0) and ϕ₂(0) outputs.

## Architecture Onboarding

Component map: Input data → GNN message passing → Self-interaction MLP → Neighbor-interaction MLP → Constraint enforcement (F+G+H=0) → Differentiable solver (RK4) → Loss computation

Critical path: Data generation → ABM-NN decomposition → Constraint enforcement → Differentiable rollout → Loss calculation

Design tradeoffs: Hard constraints ensure interpretability but may limit expressivity; curriculum learning improves stability but extends training time; restricted GNNs maintain interpretability but may miss complex interactions.

Failure signatures: Mass conservation violations indicate constraint implementation errors; gradient explosion suggests rollout length too aggressive; poor generalization to out-of-sample graphs indicates overfitting to specific graph structures.

Exactly 3 first experiments:
1. Implement SIR data generator with RK4 on Erdős–Rényi graph (n=100, p=0.05, β=0.4, γ=0.2) and verify aggregate trajectories.
2. Build ABM-NN with hard constraint F+G+H=0 and test constraint enforcement on synthetic data.
3. Train ABM-NN on SIR data with curriculum rollout and compare MAPE to GCN baseline on out-of-sample graphs.

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Weight initialization scheme and random seed settings for reproducibility are not specified, potentially affecting result consistency.
- Framework's performance on non-homogeneous graph structures beyond Erdős–Rényi networks remains untested.
- Macroeconomic case study uses aggregated rather than granular data, potentially limiting generalizability to individual agent dynamics.

## Confidence

High: The ABM-NN framework's mathematical formulation and the principle of decomposing dynamics into self-interaction and neighbor-interaction components are well-defined and theoretically sound.

Medium: The demonstrated performance improvements over baselines (31.86% MAPE vs 88.85-188.47%) are credible given the controlled experimental setup, though exact replication requires resolving unknown implementation details.

Low: Claims about interpretability and counterfactual analysis in macroeconomic modeling are plausible but require additional validation on diverse datasets and policy scenarios.

## Next Checks

1. Reproduce the SIR baseline comparison on Erdős–Rényi graphs with varying N and I₀ using identical hyperparameters to verify MAPE improvements.

2. Test the framework's mass conservation properties by monitoring S+I+R conservation across extended rollout periods on multiple graph topologies.

3. Implement the macroeconomic case study on an independent dataset with different time periods to assess robustness of interaction matrix learning and counterfactual policy predictions.