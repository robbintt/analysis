---
ver: rpa2
title: 'EMPEROR: Efficient Moment-Preserving Representation of Distributions'
arxiv_id: '2509.16379'
source_url: https://arxiv.org/abs/2509.16379
tags:
- moments
- moment
- emperor
- finite
- pooling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EMPEROR introduces a principled framework for representing high-dimensional
  distributions arising in neural network features. It encodes feature distributions
  through their statistical moments using sliced moments: projecting features onto
  multiple directions, fitting lightweight univariate GMMs to each projection, and
  aggregating the slice parameters.'
---

# EMPEROR: Efficient Moment-Preserving Representation of Distributions

## Quick Facts
- **arXiv ID:** 2509.16379
- **Source URL:** https://arxiv.org/abs/2509.16379
- **Reference count:** 0
- **Primary result:** EMPEROR achieves 0.9643 accuracy on PC MNIST 2D and 0.8674 on ModelNet40 while capturing richer distributional information than common pooling schemes.

## Executive Summary
EMPEROR introduces a principled framework for representing high-dimensional distributions arising in neural network features. It encodes feature distributions through their statistical moments using sliced moments: projecting features onto multiple directions, fitting lightweight univariate GMMs to each projection, and aggregating the slice parameters. The method provides determinacy guarantees via Carleman's condition and the Cramér-Wold theorem, ensuring unique identification of distributions from sliced moments. Empirically, EMPEROR achieves strong performance on point cloud classification and image classification across ViT layers, capturing richer distributional information than common pooling schemes while maintaining computational efficiency.

## Method Summary
EMPEROR represents distributions by projecting feature vectors onto multiple directions, fitting K-component univariate GMMs to each projection, and aggregating the resulting parameters into a fixed-size descriptor. The method leverages the Cramér-Wold theorem and Carleman's condition to guarantee that distributions can be uniquely identified from their sliced moments. Multivariate moments are recovered via least-squares reconstruction from the slice parameters, with error decaying as L^(-1/2) in the number of slices and N^(-1/2) in samples. The framework is validated on point cloud and image classification tasks, demonstrating competitive accuracy while capturing richer distributional information than standard pooling methods.

## Key Results
- **Point cloud classification:** PC MNIST 2D achieves 0.9643 accuracy; ModelNet40 achieves 0.8674 accuracy
- **Distributional information:** Captures richer distributional information than common pooling schemes
- **Computational efficiency:** Maintains efficiency through lightweight univariate GMM fitting per slice

## Why This Works (Mechanism)

### Mechanism 1: Sliced Moment Determinacy via Cramér-Wold
- Claim: A multivariate distribution can be uniquely identified from the moments of its one-dimensional projections, under mild conditions.
- Mechanism: The Cramér-Wold theorem states that two finite positive measures ρ, η on ℝ^d are identical if and only if their projections ρ_θ, η_θ agree for all directions θ ∈ S^(d-1). When each univariate projection satisfies Carleman's condition (e.g., has moments growing no faster than Gaussian), the projection is uniquely determined by its moment sequence. The paper proves that for Gaussian mixture models (GMMs), every slice satisfies Carleman's condition, yielding global determinacy from sliced moments.
- Core assumption: The underlying distribution is a finite GMM or has moments satisfying Carleman's condition on every slice.
- Evidence anchors:
  - [abstract] "We establish determinacy guarantees via Carleman's condition and the Cramér–Wold theorem, ensuring that the GMM is uniquely determined by its sliced moments."
  - [Section 2.2] Theorem 1 (Sliced moment determinacy) and Equation 9 showing multivariate moments are recoverable degree-by-degree from sliced moments.
  - [corpus] Related work on sliced Wasserstein distances (e.g., "Slicing Wasserstein Over Wasserstein," "Fourier Sliced-Wasserstein Embedding") similarly relies on projection-based characterizations, supporting the broader validity of slicing strategies for distribution comparison.
- Break condition: If the distribution has heavy tails violating Carleman's condition on some slice, or if the number of slices L is too small to span the moment space, determinacy or reconstruction fails.

### Mechanism 2: Tractable Univariate GMM Fitting per Slice
- Claim: Fitting lightweight K-component univariate GMMs on projected samples yields stable, closed-form moment estimates without O(d²) covariance burden.
- Mechanism: For each direction θ_ℓ, the projected samples y_i^(ℓ) = θ_ℓ^T x_i follow a univariate GMM with K components. Univariate GMM fitting via EM is statistically well-conditioned and computationally cheap. The per-slice parameters {(π_k^(ℓ), μ_k^(ℓ), σ_k^(ℓ))} directly encode moments through known Gaussian moment formulas (Equation 5). The descriptor aggregates all slice parameters into a fixed-size representation without cross-slice coupling.
- Core assumption: The projected distribution is well-approximated by a K-component univariate GMM; K is correctly specified or over-specified; component collisions (identical projected means/variances) are measure-zero events.
- Evidence anchors:
  - [abstract] "...lightweight univariate Gaussian mixture models (GMMs) are fit to each projection, and the resulting slice parameters are aggregated into a compact descriptor."
  - [Section 2.4] Proposition 1 proves determinacy of sliced GMMs; Section 2.5 notes that for almost every θ, projected mixtures have K distinct identifiable components.
  - [corpus] Weak direct corpus evidence on GMM-based pooling; related slicing methods (ESPFormer, Streaming Sliced Optimal Transport) emphasize computational scalability of 1D projections but do not use GMM parameterization.
- Break condition: If K is severely under-specified relative to the true projected distribution structure, or if samples per slice are too few for reliable EM convergence, moment estimates become biased or unstable.

### Mechanism 3: Finite-Sample Moment Recovery via Least Squares
- Claim: Multivariate moments can be recovered from noisy sliced estimates with error decaying as L^(-1/2) in the number of slices and N^(-1/2) in samples.
- Mechanism: Sliced moments m_k^θ are linear combinations of multivariate moments m_α (Equation 9). Stacking L slices yields a linear system y^(k) = Φ_k m^(k) with design matrix Φ_k ∈ ℝ^(L×M_k). Under i.i.d. random slice directions, Φ_k has full rank almost surely when L ≥ M_k. The least-squares estimator achieves error bounded by τ_k √(M_k / (L N λ_min(Σ_k))), where λ_min(Σ_k) is the minimum eigenvalue of the population moment-covariance matrix. Ridge regularization introduces bias-variance trade-off.
- Core assumption: Slice directions are sufficiently diverse (random or well-conditioned design); estimation noise per slice is approximately zero-mean with bounded variance; L ≥ M_k for the target degree k.
- Evidence anchors:
  - [Section 2.5] Equations 13–14 derive the explicit error bound with L^(-1/2) and N^(-1/2) scaling.
  - [Section 2.5] "For θ_ℓ i.i.d. ~ Unif(S^(d-1)) and L ≥ M_k, rank(Φ_k) = M_k holds almost surely."
  - [corpus] No direct corpus corroboration for this specific error analysis; related sliced transport papers do not provide finite-sample moment recovery bounds.
- Break condition: If the design matrix Φ_k is poorly conditioned (e.g., slice directions cluster or are adversarially chosen), or if L < M_k for the target degree, the least-squares system is underdetermined and recovery fails.

## Foundational Learning

- **Concept: The classical moment problem and Carleman's condition**
  - Why needed here: EMPEROR's theoretical guarantee rests on whether a distribution is uniquely determined by its moments. Carleman's condition provides a sufficient criterion—if the even moments don't grow too fast, the moment sequence uniquely identifies the distribution.
  - Quick check question: Given a distribution with even moments m_{2n} ~ (n!)^2, does it satisfy Carleman's condition? (Answer: No—Stirling's approximation shows m_{2n}^{-1/(2n)} ~ 1/n, and Σ 1/n diverges too slowly; the series Σ m_{2n}^{-1/(2n)} must diverge to infinity for Carleman's condition.)

- **Concept: Cramér-Wold theorem**
  - Why needed here: This theorem justifies using 1D projections to characterize multivariate distributions. It states that two distributions are identical iff all their 1D linear projections agree—this is the bridge from per-slice GMM fitting back to global distribution representation.
  - Quick check question: If two distributions ρ and η have identical mean and variance on every projection direction θ, are they necessarily equal? (Answer: No—identical first and second moments on all slices implies identical mean vector and covariance matrix, but higher-order differences may persist. You need all moments, not just two.)

- **Concept: Gaussian Mixture Models and Isserlis'/Wick's theorem**
  - Why needed here: GMMs are the tractable distribution class EMPEROR targets. Isserlis' theorem gives closed-form expressions for all moments of a Gaussian in terms of mean and covariance, which extends to mixtures. This makes moment computation from GMM parameters explicit and differentiable.
  - Quick check question: For a univariate Gaussian N(μ, σ²), what are the first four raw moments? (Answer: m_0 = 1, m_1 = μ, m_2 = μ² + σ², m_3 = μ³ + 3μσ², m_4 = μ⁴ + 6μ²σ² + 3σ⁴. These follow from Equation 5 in the paper.)

## Architecture Onboarding

- **Component map**:
  1. **Projection module**: Takes N × d feature matrix X, computes L projections Y^(ℓ) = X θ_ℓ for θ_ℓ ∈ S^(d-1). Slice directions can be random (i.i.d. uniform on sphere) or learned.
  2. **Per-slice GMM fitter**: For each ℓ, fit K-component univariate GMM to {y_i^(ℓ)} via EM or moment matching. Outputs {(π_k^(ℓ), μ_k^(ℓ), σ_k^(ℓ))}_{k=1}^K. Sort components by μ within each slice for label consistency.
  3. **Descriptor assembly**: Concatenate or aggregate slice parameters into fixed-size vector. For L slices and K components, descriptor size is L × K × 3 (weights, means, variances).
  4. **(Optional) Moment recovery**: If explicit multivariate moments are needed, solve least-squares system per degree k using design matrix Φ_k built from slice directions.

- **Critical path**:
  1. Choose L (number of slices) based on dimension d and target moment degree K_max. Need L ≥ M_k = (d+k-1 choose k) for each degree k you wish to recover.
  2. Generate slice directions Θ = {θ_ℓ}—random uniform is default; deterministic designs (e.g., equispaced) possible but require conditioning check.
  3. For each slice, run K-component EM on projected samples. Initialize carefully (e.g., k-means) to avoid local optima.
  4. Sort components by mean within each slice; concatenate parameters into descriptor.
  5. Downstream: feed descriptor to classifier/regressor; or recover moments via regularized least squares if needed.

- **Design tradeoffs**:
  - **L vs. accuracy vs. cost**: More slices improve moment recovery (error ~ L^(-1/2)) but linearly increase computation and descriptor size. For pure representation without explicit moment recovery, L can be smaller.
  - **K (GMM components) vs. expressivity vs. stability**: Higher K captures more complex projected distributions but risks overfitting with few samples and increases EM instability. Default K=2–3 works for many neural network feature distributions.
  - **Random vs. learned slice directions**: Random directions are cheap and theoretically sound; learned directions could improve conditioning but add parameters and optimization complexity.
  - **Coupled vs. uncoupled slice weights**: EMPEROR intentionally does not enforce weight invariance across slices, avoiding complex coupled EM. This trades theoretical "purity" for practical simplicity.

- **Failure signatures**:
  - **EM convergence failures**: Per-slice GMM fitting collapses (all mass in one component) or oscillates. Symptom: near-zero variances or wildly varying weights across slices. Remedy: increase initialization randomness, reduce K, or add variance floor.
  - **Poorly conditioned design matrix**: If slice directions are clustered, Φ_k has small λ_min, inflating moment recovery error. Symptom: numerical instability in least-squares, inconsistent descriptors across runs. Remedy: use more slices, enforce diversity in direction sampling.
  - **Underfitting with too few slices**: L < M_k for target degree k. Symptom: least-squares system is underdetermined; recovered moments are arbitrary. Remedy: ensure L ≥ (d+k-1 choose k) for highest moment degree of interest.
  - **Component collisions**: Two GMM components project to same mean/variance on a slice. Symptom: fewer than K effective components in some slices, degrading descriptor consistency. Remedy: this is measure-zero for random directions; re-sample directions if detected.

- **First 3 experiments**:
  1. **Baseline comparison on synthetic GMM data**: Generate samples from a known multivariate GMM (e.g., d=10, K_true=3). Compute EMPEROR descriptor with varying L and K_fit. Compare recovered moments (via least squares) to ground truth; measure reconstruction error vs. L. Baseline: direct moment estimation from samples. Expected: EMPEROR achieves lower variance at comparable bias when L is sufficient.
  2. **Ablation on slice count L**: On a fixed dataset (e.g., PC MNIST 2D from paper), sweep L ∈ {16, 32, 64, 128, 256} with K=2 fixed. Plot classification accuracy and descriptor variance across random direction seeds. Identify knee point where accuracy saturates—this is practical L for the dataset.
  3. **Ablation on GMM components K**: Same setup, fix L at saturated value from experiment 2, sweep K ∈ {1, 2, 3, 4, 5}. Compare accuracy and EM convergence rate. K=1 reduces to sliced moment matching without mixture structure; K>3 may overfit. Identify optimal K for the modality.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical guarantees depend on distributional assumptions (finite GMMs or moments satisfying Carleman's condition), which may fail for heavy-tailed distributions
- Per-slice GMM fitting accuracy depends on sufficient samples per slice and appropriate K specification
- Finite-sample error bounds assume i.i.d. random slice directions and are not empirically validated across diverse distributions

## Confidence
- **High confidence**: Theoretical determinacy framework (Cramér-Wold + Carleman) is well-established in probability theory
- **Medium confidence**: Per-slice GMM fitting is computationally tractable and statistically well-conditioned, but EM convergence can be sensitive to initialization and sample size
- **Medium confidence**: Finite-sample error bounds (L^(-1/2) and N^(-1/2) scaling) are derived but not empirically validated across diverse distributions
- **High confidence**: Empirical results demonstrate competitive performance, though comparisons are limited to common pooling baselines

## Next Checks
1. **Tail behavior validation**: Test EMPEROR on synthetic distributions with known heavy tails (e.g., t-distribution projections) to quantify determinacy failures when Carleman's condition is violated
2. **Sample efficiency study**: Systematically vary N (samples per slice) and measure EM fitting accuracy and moment recovery error on both synthetic GMMs and real neural network features
3. **Design matrix conditioning analysis**: For learned or structured slice directions (not random), compute and report condition numbers of Φ_k matrices across datasets to validate the full-rank assumption and error bound sensitivity