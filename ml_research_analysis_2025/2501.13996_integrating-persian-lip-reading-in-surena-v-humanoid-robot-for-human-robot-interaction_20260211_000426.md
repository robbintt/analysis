---
ver: rpa2
title: Integrating Persian Lip Reading in Surena-V Humanoid Robot for Human-Robot
  Interaction
arxiv_id: '2501.13996'
source_url: https://arxiv.org/abs/2501.13996
tags:
- accuracy
- dataset
- video
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper develops a Persian lip-reading system for the Surena-V
  humanoid robot using a custom dataset of 20 participants pronouncing seven common
  words. It explores two methods: an indirect approach using facial landmark tracking
  and a direct approach utilizing CNN and LSTM networks to process raw video data.'
---

# Integrating Persian Lip Reading in Surena-V Humanoid Robot for Human-Robot Interaction

## Quick Facts
- arXiv ID: 2501.13996
- Source URL: https://arxiv.org/abs/2501.13996
- Reference count: 15
- The LSTM model achieved 89% accuracy on Persian lip-reading for Surena-V robot commands

## Executive Summary
This paper develops a Persian lip-reading system for the Surena-V humanoid robot using a custom dataset of 20 participants pronouncing seven common words. It explores two methods: an indirect approach using facial landmark tracking and a direct approach utilizing CNN and LSTM networks to process raw video data. The LSTM model achieved 89% accuracy and was successfully implemented into the robot for real-time speech recognition, demonstrating the potential of machine learning to enhance human-robot interaction in environments where verbal communication is challenging. The study highlights the effectiveness of these methods, particularly in recognizing commands from visual cues when auditory communication is limited.

## Method Summary
The study developed a Persian lip-reading system using a custom dataset of 2,800 video clips (20 participants × 7 words × 20 repetitions). Three approaches were compared: (1) Indirect method using dlib facial landmarks (points 48-68) with MobileNet classifier achieving 52% accuracy; (2) Direct CNN method processing individual frames achieving 75% accuracy; and (3) Direct LSTM method processing temporal sequences achieving 89% accuracy. The LSTM architecture used TimeDistributed wrapper for frame-wise feature extraction followed by sequential LSTM layers (256→128→64 units). The system was integrated with YOLOv5 object detection for context-aware robot responses, enabling the robot to associate visual commands with detected objects.

## Key Results
- LSTM model achieved 89% test accuracy for Persian lip-reading classification
- Direct video processing method outperformed indirect landmark-based approach (89% vs 52% accuracy)
- System successfully implemented in Surena-V robot for real-time human-robot interaction
- YOLOv5 integration enabled context-aware command execution with object detection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LSTM networks outperform pure CNN architectures for lip-reading classification when processing temporal video sequences.
- Mechanism: LSTM layers capture spatiotemporal dependencies across consecutive video frames (20 frames at 20 fps = 1-second clips), modeling the dynamic trajectory of lip movements rather than treating frames as independent spatial patterns. The TimeDistributed wrapper applies feature extraction per-frame before LSTM processing of the temporal sequence.
- Core assumption: Lip movements contain discriminative temporal patterns that unfold across ~1 second; these patterns generalize across the 20 participants in the limited vocabulary (7 words).
- Evidence anchors: [abstract] "The best-performing model, LSTM, achieved 89% accuracy"; [section] "In contrast, the LSTM model surpassed the CNN in performance, achieving an accuracy of 89% on the test set... more complex than the CNN, resulting in more precise predictions"; [corpus] Related work confirms LSTM effectiveness—Aripin et al. achieved 95% accuracy with LRCN for Indonesian lip-reading; corpus supports LSTM as a strong choice but shows variable results across languages/datasets
- Break condition: Extended vocabulary (>50 words), variable speaking rates outside the 1-second window, or significant head pose variation may degrade LSTM advantage if temporal patterns become inconsistent.

### Mechanism 2
- Claim: Direct raw-video processing bypasses landmark detection fragility, trading interpretability for robustness.
- Mechanism: The direct method feeds preprocessed video frames (300×300, resized via Cvzone face detector) directly into neural networks, eliminating dependency on dlib landmark tracking. This avoids failure cases where "the lip detector frequently returned null results during sudden head movements."
- Core assumption: The face detector (Cvzone) is more robust than landmark detection; the neural network can learn lip features implicitly without explicit landmark coordinates.
- Evidence anchors: [abstract] "direct method leveraging convolutional neural networks (CNNs) and long short-term memory (LSTM) networks... processes raw video data"; [section] "training the indirect method proved to be considerably more difficult than the direct approach"; [corpus] No direct corpus comparison of landmark vs. raw-video approaches; this remains a design heuristic
- Break condition: If face detection itself fails (extreme angles, occlusion, poor lighting), the direct method also degrades. Lower resolution (300×300) may limit fine-grained lip feature discrimination.

### Mechanism 3
- Claim: Integrating lip-reading output with object detection (YOLOv5) enables context-aware robot responses in real-time HRI.
- Mechanism: The LSTM model outputs a word classification (one of 7 commands), which triggers predefined robot behaviors. YOLOv5 detects relevant objects in the scene, allowing the robot to associate commands ("begir" = take) with detected targets.
- Core assumption: The 89% accuracy is sufficient for practical deployment; command-object pairing logic is pre-programmed rather than learned; latency is acceptable for natural interaction.
- Evidence anchors: [abstract] "successfully implemented into the Surena-V robot for real-time human-robot interaction"; [section] "the robot successfully identified the word using the lip-reading model and detected the object through the YOLOv5 detector. Once both the word and object were recognized, the robot executed the appropriate, predefined response"; [corpus] Related Surena-V paper (arXiv:2501.17313) describes the robot's sensor architecture but not this specific integration; multimodal HRI approaches in corpus suggest this is a reasonable design pattern
- Break condition: Multi-object scenes with ambiguity, commands lacking clear object referents, or conflicting detections between lip-reading and other modalities.

## Foundational Learning

- Concept: **LSTM sequence modeling for video**
  - Why needed here: Understanding how LSTM processes temporally-ordered frames (TimeDistributed feature extraction → sequential hidden states) is essential for debugging why the direct LSTM outperformed CNN and indirect methods.
  - Quick check question: Given a 20-frame video clip at 20 fps, can you sketch the data tensor shape at input and after TimeDistributed feature extraction?

- Concept: **Facial landmark detection (dlib 68-point model)**
  - Why needed here: The indirect method relies on points 48–68 (lip contour). Understanding landmark detection failure modes explains why the indirect method achieved only 52% accuracy.
  - Quick check question: What happens to landmark coordinates when a user rotates their head 30° horizontally? How might normalization mitigate this?

- Concept: **Real-time inference latency budgeting**
  - Why needed here: The system must process video, classify speech, and trigger robot response within interaction-appropriate time (~100–500ms perceived latency). MobileNet was selected for the indirect method partly for speed.
  - Quick check question: At 20 fps input, what is the maximum inference time per frame before the system falls behind real-time processing?

## Architecture Onboarding

- Component map:
  - Camera (1280×720 native → 300×300 cropped via Cvzone) -> Frame extraction (OpenCV) -> Face detection & normalization -> LSTM pipeline (TimeDistributed(Conv layers) → LSTM(256) → LSTM(128) → LSTM(64) → Dense(7)) -> Word class output -> Behavior trigger
  - Parallel: YOLOv5 object detection -> Object detection output -> Command-object binding

- Critical path:
  1. Video capture at 20 fps from robot's frontal RGB-D camera
  2. Cvzone face detection → crop to 300×300 around face
  3. Stack 20 frames (1-second window)
  4. LSTM inference → word classification
  5. (Parallel) YOLOv5 object detection
  6. Command-object binding → predefined robot response

- Design tradeoffs:
  - **Direct vs. Indirect**: Direct (89% LSTM) sacrifices interpretability (no explicit landmarks) for accuracy; Indirect (52% MobileNet) offers debuggable features but fragile detection
  - **Vocabulary size**: 7 words enables high accuracy but limits practical utility; corpus

- Failure signatures:
  - Face/lip detector returns null on sudden head movements
  - Large train-val accuracy gap indicates overfitting to limited speakers
  - YOLOv5 detection conflicts with lip-reading output create ambiguous responses

- First experiments:
  1. Test landmark detection robustness with controlled head rotations and lighting variations
  2. Measure LSTM inference latency with different batch sizes to verify real-time capability
  3. Evaluate cross-speaker performance by testing on speakers excluded from training data

## Open Questions the Paper Calls Out
- Can incorporating optical flow estimation effectively capture fine-grained lip motion dynamics to improve

## Limitations
- Dataset scale limitations: Only 20 participants, creating high risk of overfitting to speaker-specific lip motion patterns
- Environmental constraints: Assumes controlled conditions with frontal camera, minimal head movement, and consistent lighting
- Technical scope: Evaluation focuses on classification accuracy without measuring real-time inference latency or user experience metrics

## Confidence
- **High Confidence**: The LSTM architecture achieving 89% accuracy on the test set (direct evidence from Table III); the general superiority of temporal models for lip-reading tasks (supported by corpus)
- **Medium Confidence**: The claim that direct video processing is more robust than landmark detection (based on comparative difficulty and failure modes, but no ablation study); the integration with YOLOv5 enabling context-aware responses (described conceptually, not empirically validated)
- **Low Confidence**: Generalization claims to other speakers, vocabularies, or real-world deployment scenarios (no external validation or stress testing reported)

## Next Checks
1. **Cross-speaker validation**: Test the trained LSTM model on a held-out set of speakers not present in the training data to quantify generalization performance and identify speaker-specific overfitting
2. **Environmental robustness testing**: Systematically evaluate accuracy degradation under varying lighting conditions, head pose angles (beyond sudden movements), and partial occlusions to establish operational boundaries
3. **Latency and integration benchmarking**: Measure end-to-end inference time from video capture to robot response, including YOLOv5 object detection latency, to verify real-time capability and identify bottlenecks in the critical path