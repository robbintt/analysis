---
ver: rpa2
title: 'Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting'
arxiv_id: '2509.25631'
source_url: https://arxiv.org/abs/2509.25631
tags:
- arxiv
- diffusion
- figure
- consistency
- forecast
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Swift, a single-step consistency model for
  efficient probabilistic weather forecasting. Swift leverages the continuous-time
  formulation of diffusion models, enabling autoregressive finetuning with a CRPS
  objective to generate well-calibrated ensembles without multi-model ensembling or
  parameter perturbations.
---

# Swift: An Autoregressive Consistency Model for Efficient Weather Forecasting

## Quick Facts
- **arXiv ID**: 2509.25631
- **Source URL**: https://arxiv.org/abs/2509.25631
- **Reference count**: 40
- **Primary result**: Single-step autoregressive consistency model achieving 39× speedup over diffusion baselines with competitive skill to IFS ENS at 75-day horizons

## Executive Summary
Swift introduces a novel single-step consistency model for efficient probabilistic weather forecasting, achieving 39× speedup over diffusion baselines while maintaining competitive forecast skill. The model leverages continuous-time diffusion formulations and autoregressive finetuning with CRPS objectives to generate well-calibrated ensembles without multi-model ensembling. Trained autoregressively on ERA5 data, Swift produces stable 6-hourly forecasts up to 75 days, effectively capturing seasonal cycles and extreme events like Hurricane Laura.

## Method Summary
Swift employs a two-stage training approach: (1) Pretraining a consistency model under TrigFlow parameterization with log-uniform noise schedules and tangent warmup, using Muon optimizer for stability; (2) Autoregressive finetuning with CRPS objective over a curriculum of 1-8 steps. The model predicts residuals over dynamic intervals (6, 12, 24 hours) and uses Swin Transformer architecture with adaLN conditioning. Inference requires only a single function evaluation from t=π/2.

## Key Results
- Achieves 39× faster inference than diffusion baselines (1 NFE vs 39 NFE)
- Maintains competitive forecast skill to IFS ENS up to 75-day horizons
- Produces well-calibrated ensembles with spread/skill ratio near 1.0 after CRPS finetuning
- Successfully captures seasonal cycles and extreme events like Hurricane Laura

## Why This Works (Mechanism)

### Mechanism 1: Single-Step Inference via Self-Consistency
The model learns to map any noisy sample x_t directly to the clean origin x_0 by satisfying f_θ(x, t) ≡ x, distilling the generative process into one function evaluation. This bypasses iterative ODE solvers required by standard diffusion models.

### Mechanism 2: CRPS-Based Ensemble Calibration
Autoregressive finetuning with CRPS optimizes the trade-off between ensemble accuracy and spread, preventing collapse to the mean. This explicit calibration signal addresses the under-dispersion common in deterministic models.

### Mechanism 3: Dynamic Interval Training for Long-Term Stability
By predicting residuals over random intervals (6, 12, 24 hours) rather than strictly t+1 from t, the model learns both short-term dynamics and long-term phase shifts, reducing error accumulation during autoregressive rollouts.

## Foundational Learning

- **Consistency Models vs. Diffusion Models**: Understanding how Swift achieves 39× speedup requires grasping that CMs map any trajectory point to the origin in one step. *Quick check*: If you sample from a consistency model at t=0.5 vs t=1.0, should the output be different?

- **Continuous Ranked Probability Score (CRPS)**: This proper scoring rule penalizes both ensemble mean distance from truth and ensemble spread. *Quick check*: Does CRPS optimize for the median or mean of the distribution, and how does it handle overconfident (narrow) ensembles?

- **Autoregressive Error Accumulation**: Understanding model instability over long rollouts requires knowing how small biases compound exponentially over time. *Quick check*: Why might a model with low RMSE at step 1 fail catastrophically by step 100?

## Architecture Onboarding

- **Component map**: Input (Noisy state + Atmospheric state + Forcings) → Patch Embedding → Swin Transformer blocks → adaLN conditioning → Linear decoder → Residual prediction

- **Critical path**: 
  1. Input: Noisy state x_t + Atmospheric State x_i + Forcings (Solar, LSM)
  2. Condition: Embed noise level t and time delta δ_i via sinusoidal functions
  3. Process: Swin Transformer blocks with SwiGLU and adaLN modulation
  4. Loss: Consistency Training (pretrain) → CRPS (finetune)

- **Design tradeoffs**:
  - TrigFlow Parameterization: Unifies EDM and Flow Matching for simpler arithmetic but requires specific tangent normalization
  - Muon Optimizer: Critical for handling gradient dynamics of consistency loss; substitute may cause stability differences
  - Ensemble Size: N=2 during finetuning for CRPS tractability, assumed to generalize to larger inference ensembles

- **Failure signatures**:
  - Spectral Blurring: Model drifting toward larger scales; check power spectra
  - Polar Artifacts: High errors near poles suggest latitude weighting issues
  - Under-dispersion: SSR < 1 indicates insufficient CRPS finetuning or overfitting

- **First 3 experiments**:
  1. Overfit Sanity Check: Train on single batch to verify Equation 3 and TrigFlow preconditioning
  2. Tangent Warmup Ablation: Disable warmup to observe training stability
  3. Finetuning Rollout: Compare 30-day rollout of pretrained Swift-B vs final Swift

## Open Questions the Paper Calls Out

- **Open Question 1**: Can consistency distillation from larger, high-resolution diffusion models (e.g., Aeris) enhance forecast skill while maintaining efficiency for operational deployment?
- **Open Question 2**: Does classifier-free guidance improve the performance and reliability of autoregressive consistency weather models?
- **Open Question 3**: Can Pangu-Weather style greedy schedules improve stability at higher temporal resolutions than current 6/24-hour intervals?

## Limitations

- Exact latitude and pressure weighting functions are not explicitly specified, though their shapes are shown in Figure 3
- Muon optimizer implementation details are not fully disclosed, and substitutes may lead to stability differences
- Long-term stability (75 days) is empirically demonstrated but not theoretically proven
- CRPS finetuning uses only N=2 ensemble members; generalization to larger ensembles is assumed but not rigorously validated

## Confidence

- **High confidence**: Single-step inference speedup (39× faster) and basic forecast skill (competitive to IFS ENS)
- **Medium confidence**: Stable long-term forecasts up to 75 days, though mechanism for preventing drift is partially explained
- **Low confidence**: Impact of Muon optimizer vs. AdamW and precise form of latitude/pressure weights are unclear

## Next Checks

1. Implement exact latitude and pressure weighting functions as inferred from Figure 3 and verify their impact on performance
2. Compare Muon vs. AdamW optimizers during consistency pretraining to quantify stability benefits
3. Test ensemble generalization by running inference with N=4, 8, 16 ensemble members and measuring calibration metrics