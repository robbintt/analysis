---
ver: rpa2
title: 'Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language
  Models'
arxiv_id: '2502.14191'
source_url: https://arxiv.org/abs/2502.14191
tags:
- response
- reward
- arxiv
- responses
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Multimodal RewardBench, a comprehensive
  benchmark for evaluating reward models in vision-language models (VLMs). The benchmark
  addresses the lack of holistic evaluation frameworks for multimodal reward models
  by providing expert-annotated (prompt, chosen response, rejected response) triplets
  across six domains: general correctness, preference, knowledge, reasoning, safety,
  and visual question-answering.'
---

# Multimodal RewardBench: Holistic Evaluation of Reward Models for Vision Language Models

## Quick Facts
- arXiv ID: 2502.14191
- Source URL: https://arxiv.org/abs/2502.14191
- Reference count: 29
- Primary result: 5,211 expert-annotated multimodal triplets across 6 domains show VLMs achieve only 72% accuracy with largest gaps in reasoning and safety domains

## Executive Summary
This paper introduces Multimodal RewardBench, a comprehensive benchmark for evaluating reward models in vision-language models (VLMs). The benchmark addresses the lack of holistic evaluation frameworks for multimodal reward models by providing expert-annotated (prompt, chosen response, rejected response) triplets across six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. The dataset comprises 5,211 triplets collected from various VLMs and annotated by domain experts. The authors evaluate multiple VLM judges including proprietary models (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) and open models (Llama 3.2 Vision Instruct, Molmo, Aria, Llava-1.5-13B). The top-performing models achieve only 72% overall accuracy, with significant challenges in reasoning and safety domains. The benchmark reveals larger performance gaps between models compared to existing VLM benchmarks, highlighting its effectiveness in differentiating model capabilities. The findings suggest that Multimodal RewardBench provides a challenging testbed for advancing reward model development across diverse capabilities.

## Method Summary
The authors created Multimodal RewardBench by collecting expert-annotated (prompt, chosen response, rejected response) triplets across six domains: general correctness, preference, knowledge, reasoning, safety, and visual question-answering. Prompts were sourced from six existing VLM datasets (MMMU-Pro, MathVista, PAIRS, ToxicChat, Hateful Memes, etc.), and responses were generated using multiple VLMs. Domain experts annotated these triplets, focusing on major errors rather than subjective issues, achieving 0.75 inter-annotator agreement. For knowledge and math reasoning tasks, responses underwent expert verification of chain-of-thought reasoning, removing 40% of samples with flawed reasoning. The final dataset contains 5,211 triplets evaluated using zero-shot VLM-as-a-judge prompting with standardized templates.

## Key Results
- Top VLM judges achieve only 72% overall accuracy on binary response ranking, with reasoning and safety domains showing the largest performance gaps
- Proprietary models (GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro) significantly outperform open models (Llama 3.2 Vision, Molmo, Llava) with accuracy differences exceeding 0.20 in some domains
- Performance spread is substantially larger than existing VLM benchmarks, revealing greater differentiation between model capabilities
- Open models show near-random performance (20-52%) on safety/bias detection compared to proprietary models (75-95% accuracy)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-annotated instruction refinement improves inter-annotator agreement for complex multimodal judgment tasks.
- Mechanism: By instructing annotators to focus on major errors (visual errors, reasoning errors, knowledge errors) rather than ambiguous or subjective issues, and explicitly defining what constitutes an omission versus minor imperfection, annotators converge more consistently on ground truth labels.
- Core assumption: Human experts can reliably distinguish major from minor errors when given explicit criteria.
- Evidence anchors: [Section 3.3] "These improvements to the annotation instructions significantly increased inter-annotator agreement: the rate of unanimous agreement for correctness judgments improved from 0.61 to 0.75"
- Break condition: If annotation tasks require subjective aesthetic judgment or ambiguous visual interpretation without clear ground truth, agreement improvements may not hold.

### Mechanism 2
- Claim: Multi-domain benchmark design reveals larger performance gaps between VLMs than existing single-task benchmarks.
- Mechanism: By spanning six domains with diverse task types (long-form generation, chain-of-thought reasoning, code generation, safety detection), the benchmark exposes domain-specific weaknesses that aggregate metrics mask. The combination of correctness-based and preference-based judgment tasks creates differentiation pressure.
- Core assumption: Performance gaps reflect genuine capability differences rather than benchmark artifact sensitivity.
- Evidence anchors: [Section 4.2] "Performance spread in our benchmark is larger than in existing VLM benchmarks... a performance gap of over 0.20 accuracy. Existing popular VLM benchmarks... typically show a smaller performance gap, e.g., within 0.05 accuracy"
- Break condition: If domain coverage becomes too broad without sufficient examples per domain, statistical reliability degrades.

### Mechanism 3
- Claim: Chain-of-thought response collection with expert verification filters surface-level correct answers that have flawed reasoning.
- Mechanism: For knowledge and math reasoning tasks, models generate 10 responses per prompt; responses with correct final answers undergo expert verification of intermediate steps. This removes cases where correct answers result from flawed reasoning (40% of samples removed), creating cleaner evaluation signal.
- Core assumption: Expert verification of reasoning steps is feasible at scale and reliably identifies reasoning flaws.
- Evidence anchors: [Section 3.2] "We discard examples where experts identify flaws in the chain-of-thought reasoning (resulting in removal of 40% of the samples)"
- Break condition: If reasoning verification requires domain expertise beyond available annotator pool, quality degrades; if prompts are too difficult (all responses correct/incorrect), they must be discarded (20% discarded for this reason).

## Foundational Learning

- Concept: **Reward Models in RLHF**
  - Why needed here: The entire benchmark evaluates reward models that score/rank VLM outputs for alignment training. Without understanding that reward models provide the optimization target for RLHF, the benchmark's purpose is unclear.
  - Quick check question: Can you explain why a reward model that achieves 72% accuracy on pairwise comparison might still be useful for RLHF training?

- Concept: **VLM-as-a-Judge Paradigm**
  - Why needed here: The benchmark evaluates this specific approach (using VLMs to judge other VLM outputs) rather than regression-based reward models. Understanding prompt templates and position bias matters for implementation.
  - Quick check question: Why might zero-shot prompting with [[A]]/[[B]] output format be preferred over regression-head reward models for VLM evaluation?

- Concept: **Inter-Annotator Agreement Metrics**
  - Why needed here: The paper's credibility rests on achieving 0.75 agreement rates. Understanding what this means (vs. random baseline) is essential for judging benchmark quality.
  - Quick check question: If three annotators judge correctness and two agree, what voting scheme should determine the final label?

## Architecture Onboarding

- Component map: Prompt Sources (6 datasets) → Response Generation (6 VLMs) → Expert Annotation (Surge AI, $250/hr) → Quality Filtering (agreement + CoT verification) → Benchmark Triplets (5,211) → VLM-as-Judge Evaluation → Accuracy by Domain

- Critical path:
  1. Prompt collection from domain-specific datasets (MMMU-Pro, MathVista, PAIRS, etc.)
  2. Response generation with controlled sampling (10 responses for knowledge/math, 2 for others)
  3. Expert annotation with refined instructions (3 annotators per example)
  4. Filtering based on agreement + reasoning verification
  5. Evaluation using standardized judge prompt template (Appendix C)

- Design tradeoffs:
  - **Coverage vs. depth**: 6 domains × ~800 examples each provides breadth but limits statistical power per domain
  - **Expert quality vs. cost**: $250/hr expert annotators vs. crowd-sourced alternatives
  - **VLM-as-judge vs. regression models**: Focuses on available approach but limits applicability to classifier-based reward models
  - **Correct vs. preferred pairs**: Includes both but requires different collection procedures

- Failure signatures:
  - Random-guess-level performance (~50%) on specific domains indicates model lacks capability (e.g., Llama-11B on safety/bias at 0.209)
  - High performance on VQA but low on reasoning suggests visual perception works but multimodal reasoning fails
  - Near-identical scores across domains suggests benchmark doesn't differentiate capabilities

- First 3 experiments:
  1. **Baseline establishment**: Run VLM-as-a-judge evaluation on the 5,211 triplets using the standardized prompt template (Appendix C) to establish baseline accuracy by domain.
  2. **Ablation on response order**: Verify that random shuffling during benchmark construction prevents position bias by running same model with swapped A/B positions.
  3. **Domain difficulty analysis**: Identify which domains show largest gaps between top and bottom models (currently safety/bias and reasoning/code) to prioritize improvement efforts.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do regression-based VLM reward models compare to VLM-as-a-judge approaches on Multimodal RewardBench?
- Basis in paper: [explicit] "Another limitation is that our benchmark currently evaluates only VLM-as-a-judge approaches, as there are few publicly available regression/classifier-based VLM reward models... future work will evaluate regression/classifier-based VLM reward models as well."
- Why unresolved: The benchmark release predates the availability of sufficient open regression-based VLM reward models for comparison.
- What evidence would resolve it: Evaluate emerging regression-based VLM reward models (once released) on Multimodal RewardBench and compare accuracy across categories.

### Open Question 2
- Question: What factors explain why proprietary models achieve 75-95% accuracy on bias detection while open models perform near or below random (20-52%)?
- Basis in paper: [inferred] Table 3 shows extreme performance gap in Safety/Bias: Gemini 1.5 Pro (0.945), Claude (0.768), GPT-4o (0.748) vs Llama-11B (0.209), Llava (0.201), Molmo (0.348).
- Why unresolved: The paper notes the gap but does not isolate whether causes are scale, training data composition, or explicit safety fine-tuning.
- What evidence would resolve it: Controlled experiments varying training data diversity, model scale, and safety-specific fine-tuning while measuring bias detection accuracy.

### Open Question 3
- Question: How would model performance change if safety evaluation expanded beyond bias and toxicity to include prompt refusal, NSFW content, and harmful response identification?
- Basis in paper: [explicit] "In the safety category, we were limited to two datasets: PAIRS (for bias) and Hateful Memes (for toxicity)... future work can explore additional safety-related aspects, including prompt refusal, NSFW content detection, and harmful response identification."
- Why unresolved: Current scarcity of VLM datasets in these safety subdomains precluded their inclusion.
- What evidence would resolve it: Construct new triplets targeting these aspects, annotate with experts, and re-evaluate the model suite.

## Limitations

- The benchmark focuses exclusively on VLM-as-a-judge approaches, potentially limiting applicability to regression-based reward models that could offer different strengths.
- Expert annotation, while achieving high inter-annotator agreement (0.75), may not fully capture the diversity of real-world multimodal scenarios where judgments are more subjective.
- The 5,211 example size, while substantial, may have limited statistical power for detecting fine-grained differences in the reasoning and safety domains where performance gaps are most pronounced.

## Confidence

- **High Confidence**: The benchmark construction methodology (expert annotation, quality filtering, multi-domain coverage) is well-documented and reproducible. The finding that performance gaps between models are larger than in existing benchmarks (0.20 vs 0.05 accuracy differences) is supported by direct comparison with established benchmarks.
- **Medium Confidence**: The claim that Multimodal RewardBench provides a challenging testbed for reward model development is reasonable but depends on whether the identified weaknesses in reasoning and safety domains translate to real-world alignment challenges. The expert annotation improvements leading to 0.75 agreement are credible but could vary with different annotation teams.
- **Low Confidence**: The generalization of findings to all VLM applications is limited by the specific domain coverage and prompt sources used. The benchmark may over-represent certain types of multimodal reasoning while underrepresenting others.

## Next Checks

1. **Cross-annotator validation**: Replicate the annotation process with a different team of domain experts to verify that the 0.75 inter-annotator agreement is reproducible and not specific to the Surge AI annotators used in the original study.

2. **Domain coverage expansion**: Test whether models that perform poorly on safety/bias and reasoning domains in Multimodal RewardBench also struggle on independent safety benchmarks (like those in Med-RewardBench) and reasoning benchmarks (like PRiSM) to validate that these weaknesses are domain-general rather than benchmark-specific.

3. **Regression model comparison**: Evaluate whether regression-based reward models (not just VLM-as-a-judge) achieve different performance patterns on the benchmark, particularly in domains where VLM judges show the largest gaps, to determine if the benchmark's focus on one evaluation approach limits its utility for reward model development.