---
ver: rpa2
title: 'A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience
  on WeChat'
arxiv_id: '2507.18515'
source_url: https://arxiv.org/abs/2507.18515
tags:
- code
- retrieval
- completion
- llms
- techniques
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a comprehensive empirical study of retrieval-augmented\
  \ generation (RAG) for code completion in closed-source repositories, specifically\
  \ examining the WeChat codebase. The study systematically evaluates two types of\
  \ RAG methods\u2014identifier-based and similarity-based RAG\u2014across 26 open-source\
  \ LLMs ranging from 0.5B to 671B parameters."
---

# A Deep Dive into Retrieval-Augmented Generation for Code Completion: Experience on WeChat

## Quick Facts
- arXiv ID: 2507.18515
- Source URL: https://arxiv.org/abs/2507.18515
- Reference count: 40
- Primary result: RAG improves code completion in closed-source repositories, with hybrid lexical-semantic retrieval yielding optimal results

## Executive Summary
This paper presents a comprehensive empirical study of retrieval-augmented generation (RAG) for code completion in closed-source C++ repositories, specifically examining the WeChat codebase. The study systematically evaluates two types of RAG methods—identifier-based and similarity-based RAG—across 26 open-source LLMs ranging from 0.5B to 671B parameters. Key findings include that both RAG methods improve code completion performance, with similarity-based RAG showing superior effectiveness, and that a hybrid approach combining BM25 lexical retrieval with GTE-Qwen semantic retrieval yields optimal results by leveraging their complementary strengths.

## Method Summary
The study constructs a retrieval corpus from 1,669 internal C++ repositories using a fine-grained preprocessing algorithm that handles function definitions, class definitions, and protobuf messages. Two RAG approaches are evaluated: identifier-based RAG that retrieves specific function/class definitions based on extracted identifiers, and similarity-based RAG that retrieves contextually relevant code snippets using both lexical (BM25) and semantic (GTE-Qwen) retrieval methods. The system retrieves top-4 results, concatenates them with incomplete code into prompts under 2k tokens, and generates completions using various open-source LLMs with temperature=0. Performance is measured using CodeBLEU and Edit Similarity metrics on a 100-example benchmark across seven domains.

## Key Results
- Both identifier-based and similarity-based RAG methods consistently outperform base models across different LLM scales
- Similarity-based RAG substantially outperforms identifier-based RAG in enhancing code completion quality
- Lexical (BM25) and semantic (GTE-Qwen) retrieval techniques exhibit complementary characteristics, with their combination yielding optimal results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing retrieved code context from a repository improves LLM code completion accuracy over a base model without requiring retraining.
- Mechanism: Retrieval-Augmented Generation (RAG) injects relevant external context (function definitions, similar code snippets) directly into the LLM's prompt window. This provides domain-specific knowledge and coding patterns from the proprietary codebase that the base LLM lacks, enabling more accurate predictions.
- Core assumption: The retrieved context is sufficiently relevant and not so noisy that it degrades the LLM's performance.
- Evidence anchors:
  - [abstract]: "...leveraging relevant context from codebases without requiring model retraining."
  - [section IV.A]: "...experimental results demonstrate that different RAG methods consistently outperform base models across different scales."
  - [corpus]: [LLavaCode] supports the general efficacy of RAG for code completion, noting it is "one of the most effective approaches... particularly when context from a surrounding repository is essential."
- Break condition: The retrieval system provides irrelevant or misleading context, distracting the LLM and reducing accuracy below the baseline (negative retrieval).

### Mechanism 2
- Claim: Similarity-based RAG outperforms identifier-based RAG for code completion in closed-source repositories.
- Mechanism: Similarity-based retrieval (both lexical and semantic) captures functional intent and broader code structure patterns better than retrieving based solely on exact identifier definitions. This provides a richer, more diverse context that helps the LLM understand not just what a function is, but how similar problems have been solved.
- Core assumption: Code with similar patterns or structure provides more actionable guidance for completion than just a list of definitions.
- Evidence anchors:
  - [abstract]: "...both RAG methods... demonstrate effectiveness... with similarity-based RAG showing superior performance."
  - [section IV.A, Finding 1]: "...compared to identifier-based RAG, similarity-based RAG substantially performs better in enhancing code completion quality."
  - [corpus]: [Impact-driven Context Filtering] implicitly supports the high value of cross-file context, which similarity-based methods are designed to retrieve effectively.
- Break condition: The query code is novel or unique within the repository, causing the similarity search to fail or retrieve poor, non-analogous matches.

### Mechanism 3
- Claim: A hybrid retrieval approach combining lexical (BM25) and semantic (GTE-Qwen) techniques yields optimal performance.
- Mechanism: Lexical and semantic methods capture different, complementary aspects of code similarity. BM25 excels at matching exact tokens and keywords, while semantic embeddings capture conceptual and structural similarities. Their combination provides a more comprehensive and robust set of useful context snippets.
- Core assumption: The combination of retrieval results provides non-overlapping, beneficial context that the LLM can synthesize effectively.
- Evidence anchors:
  - [abstract]: "...combination of lexical and semantic retrieval techniques yields optimal results..."
  - [section IV.C, Finding 3]: "...lexical and semantic retrieval techniques exhibit distinct retrieval results distribution and demonstrate complementary characteristics..."
  - [corpus]: [corpus] evidence for this specific hybrid finding is weak or missing in the provided neighbors.
- Break condition: The strategy for ranking or combining the two retrieval lists (e.g., simple concatenation) fails, or one retrieval method introduces excessive noise that degrades the overall result.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: The entire paper is an empirical study of applying RAG to the specific problem of code completion. Understanding the basic "retrieve-then-generate" pattern is essential.
  - Quick check question: How does RAG differ fundamentally from fine-tuning a model on a codebase?

- Concept: **Lexical vs. Semantic Search**
  - Why needed here: The paper systematically evaluates and compares a lexical method (BM25) against several semantic models (CodeBERT, UniXcoder, etc.). Understanding their different approaches to "similarity" is key to interpreting the results.
  - Quick check question: Why would a semantic search model find a relevant code snippet that a keyword search like BM25 might miss?

- Concept: **Code Corpus Preprocessing**
  - Why needed here: The paper details a specific preprocessing algorithm for C++ codebases to handle challenges like header files, macros, and dependencies, which is critical for constructing a high-quality retrieval corpus.
  - Quick check question: Why can't you simply feed raw C++ repository files directly into a retriever without preprocessing?

## Architecture Onboarding

- Component map: **Raw Repo** → **Preprocessing** → **Corpus Construction** → **Indexing (BM25/Vector)** → **Query (Incomplete Code)** → **Retrieval** → **Prompt Construction** → **LLM Inference**
- Critical path: The complete pipeline from raw repository to generated code completion, with preprocessing being essential for C++ codebases
- Design tradeoffs:
  - **Identifier vs. Similarity RAG**: Identifier-based is more precise for known dependencies but limited by what it can look up. Similarity-based is broader and more flexible but can retrieve noisy or irrelevant context.
  - **Choice of Retriever**: BM25 is fast, lightweight, and requires no training. Semantic models (like GTE-Qwen) are more computationally expensive but better at capturing intent.
  - **Context Budget**: The paper limits retrieved results to 4 to keep the total prompt length under 2k tokens. Adding more context increases cost and latency, and may introduce noise.
- Failure signatures:
  - **Identifier-based failure**: LLM hallucinates a function's implementation because the specific definition wasn't found or indexed correctly.
  - **Similarity-based failure**: Retrieved code is syntactically similar but functionally different, leading the LLM to generate code with incorrect logic.
  - **Preprocessing failure**: Macro expansion or dependency resolution is incomplete, leading to a corrupted retrieval corpus and poor quality matches.
- First 3 experiments:
  1. **Baseline Measurement**: Run your chosen LLM on the code completion benchmark *without* any RAG to establish a baseline score for comparison.
  2. **Identifier-Based Ablation**: Implement identifier-based RAG (Section II.C). Measure the performance gain. Ablate by using only message, class, or function definitions to see their individual impact.
  3. **Similarity-Based Ablation**: Implement similarity-based RAG (Section II.D). Compare BM25 vs. a semantic retriever (e.g., GTE-Qwen) on incomplete code queries. Analyze the overlap in their top-k retrieved results to understand their complementarity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized retrieval models be developed to bridge the semantic gap between incomplete code queries and complete code snippets?
- Basis in paper: [explicit] Section V-B states that there is a gap between training (complete snippets) and application (incomplete fragments), suggesting the development of "specialized retrieval techniques optimized for incomplete queries" as a future direction.
- Why unresolved: The study only benchmarked existing pre-trained models (like UniXcoder and GTE-Qwen) rather than proposing or training a model specifically optimized to handle partial code inputs.
- What evidence would resolve it: A new retrieval model fine-tuned on partial-to-complete code pairs demonstrating statistically significant gains over GTE-Qwen on the same benchmark.

### Open Question 2
- Question: How can RAG frameworks be modified to specifically reduce the high rate of "Missing or Incorrect Logic" errors observed in generated code?
- Basis in paper: [explicit] Section V-A reveals that "Missing or Incorrect Logic" accounts for over 52% of errors in the developer survey. The authors conclude that "improving logical reasoning capabilities should be a priority for future development."
- Why unresolved: While the paper shows RAG improves overall metrics, the retrieved context (function definitions/similar code) is currently insufficient to fully resolve logical hallucinations.
- What evidence would resolve it: A study introducing RAG mechanisms that retrieve logical graphs or execution traces, resulting in a measurable decrease in logic errors compared to the baseline.

### Open Question 3
- Question: Do the complementary benefits of combining BM25 and GTE-Qwen generalize to programming languages other than C++?
- Basis in paper: [inferred] The entire empirical study is conducted on the WeChat codebase, which is primarily C++. The authors note in Section V-C that the codebase "might exhibit distinct characteristics," posing a threat to external validity.
- Why unresolved: The preprocessing algorithm (Algorithm 1) and the superiority of the BM25+GTE-Qwen hybrid were validated exclusively on C++ and Protobuf files.
- What evidence would resolve it: Replication of the experimental setup on closed-source repositories written in Java or Python to verify if the hybrid retrieval method remains optimal.

### Open Question 4
- Question: What is the most effective "strategic combination" of lexical and semantic retrieval beyond simple aggregation?
- Basis in paper: [explicit] Section V-B discusses the complementary nature of the techniques and suggests potential for "further performance improvements... through their strategic combination, beyond the traditional retrieve-then-rerank pipeline."
- Why unresolved: The paper demonstrated that combination works but did not explore advanced fusion architectures or neural re-ranking layers to maximize the distinct signals from lexical and semantic sources.
- What evidence would resolve it: A comparative analysis of advanced fusion architectures (e.g., learnable dense-sparse fusion) against the concatenation methods used in the paper.

## Limitations
- The study is based on a proprietary C++ codebase from WeChat, which may limit generalizability to other programming languages or development contexts
- The benchmark dataset is internal and not publicly available, preventing independent validation
- The paper does not evaluate against fine-tuning approaches, leaving open questions about when RAG is preferable to model adaptation

## Confidence
- **High confidence**: The core finding that RAG methods improve code completion performance over base models without retraining
- **Medium confidence**: The superiority of similarity-based RAG over identifier-based RAG and the effectiveness of the BM25+GTE-Qwen hybrid approach
- **Medium confidence**: The finding that smaller models (<7B) benefit less from hybrid retrieval

## Next Checks
1. **Cross-language validation**: Test the RAG methodology on codebases in other languages (Python, Java, JavaScript) to assess generalizability beyond C++
2. **Fine-tuning comparison**: Implement and evaluate a fine-tuning baseline on the same benchmark to determine the conditions under which RAG outperforms or underperforms model adaptation
3. **Real-world deployment study**: Conduct a longitudinal study measuring actual productivity gains when developers use RAG-augmented completion in their daily workflow, beyond controlled benchmark metrics