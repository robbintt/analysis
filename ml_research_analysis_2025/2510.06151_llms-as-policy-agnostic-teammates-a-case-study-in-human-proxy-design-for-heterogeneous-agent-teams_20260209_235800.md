---
ver: rpa2
title: 'LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for
  Heterogeneous Agent Teams'
arxiv_id: '2510.06151'
source_url: https://arxiv.org/abs/2510.06151
tags:
- human
- stag
- agents
- llms
- hare
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) can
  serve as policy-agnostic human proxies in heterogeneous-agent teams. Three experiments
  were conducted in a grid-world stag hunt game to evaluate LLM decision-making.
---

# LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams

## Quick Facts
- **arXiv ID:** 2510.06151
- **Source URL:** https://arxiv.org/abs/2510.06151
- **Reference count:** 37
- **Primary result:** LLMs prompted with game-state features achieve expert alignment (F1 > 0.75) and can be steered toward risk-sensitive behaviors in a grid-world stag hunt game.

## Executive Summary
This study investigates whether large language models (LLMs) can serve as policy-agnostic human proxies in heterogeneous-agent teams. Three experiments were conducted in a grid-world stag hunt game to evaluate LLM decision-making. In Experiment 1, LLMs prompted with game-state features achieved expert alignment (F1-scores > 0.75, Cohen's kappa > 0.58), with larger models (LLaMA 3.1 70B, Mixtral 8x22B) outperforming smaller ones. Experiment 2 demonstrated that prompt modifications could induce risk-sensitive behaviors, with models showing both risk-seeking and risk-averse tendencies. Experiment 3 showed LLM agents generating coherent multi-step action sequences that mirrored human decision patterns in dynamic environments. The findings establish LLMs as scalable, customizable proxies for human decision-making in multi-agent settings, though further work is needed to test generalization to more complex domains and larger agent groups.

## Method Summary
The study uses a 5×5 grid-world Stag Hunt game implemented in a custom PettingZoo environment with two hunters (Blue/Purple), one stag (reward 5, requires cooperation), and two hares (reward 1, individual). Three experiments were conducted: (1) expert decision alignment comparing LLM choices to expert judges across 15 grid configurations, (2) risk sensitivity testing via prompt modifications ("risk averse," "risk seeking," neutral) across scenarios, and (3) trajectory generation in dynamic multi-agent settings. LLMs were queried state-by-state with prompts containing game-state observations (Manhattan distances: B-H, B-S, P-H, P-S) and reward structures, using temperature=0 and top_p=0.9/0.95. Performance was evaluated using F1-score, Cohen's kappa, and Risk Index (ϕ_risk = (N_Hare - N_Stag)/N_Total).

## Key Results
- LLMs prompted with game-state features achieved expert alignment with F1 > 0.75 and Cohen's kappa > 0.58
- Larger models (LLaMA 3.1 70B, Mixtral 8x22B) significantly outperformed smaller ones (Llama 3.1 8B)
- Prompt modifications successfully induced risk-sensitive behaviors with distinct Risk Index distributions
- LLM-generated trajectories closely resembled human movement patterns in dynamic environments

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LLMs can replicate expert-level strategic decisions when provided with structured state features and reward information.
- **Mechanism:** The prompt encodes game state as simplified features (Manhattan distances between agents and targets) plus reward structure, allowing LLMs to apply decision criteria consistently without spatial interpretation overhead.
- **Core assumption:** Relative distances provide sufficient strategic information for cooperation/defection decisions; LLMs can reason about game-theoretic tradeoffs from natural language descriptions.
- **Evidence anchors:** [abstract] "LLMs, prompted with game-state observations and reward structures, align more closely with experts than participants, demonstrating consistency in applying underlying decision criteria." [section 4.3] "Larger models, LLaMA 3.1 70B (Avg F1 = 0.80, κ = 0.60) and Mixtral 8x22B (Avg F1 = 0.79, κ = 0.58), closely align with expert judgments."

### Mechanism 2
- **Claim:** Lightweight prompt modifications can systematically bias LLM decisions toward risk-averse or risk-seeking behaviors.
- **Mechanism:** Adding a risk behavior modifier (e.g., "You are risk averse") to the prompt shifts LLM outputs between cooperative (stag) and defecting (hare) strategies by framing the decision context.
- **Core assumption:** LLMs encode implicit risk preferences accessible through natural language framing; these preferences transfer to game-theoretic decisions.
- **Evidence anchors:** [abstract] "Experiment 2 modifies prompts to induce risk-sensitive strategies (e.g. 'be risk averse'). LLM outputs mirror human participants' variability, shifting between risk-averse and risk-seeking behaviours." [section 5.3] Risk Index (φ_risk) distribution shows models can be steered across risk-seeking (-1 to -0.2), neutral (-0.2 to 0.2), and risk-averse (0.2 to 1) ranges.

### Mechanism 3
- **Claim:** LLMs queried state-by-state can generate coherent multi-step action trajectories resembling human movement patterns.
- **Mechanism:** At each timestep, the LLM receives current state features and selects from discrete action space (UP, DOWN, LEFT, RIGHT, STAY); iterative querying produces trajectories. Temperature=0 enforces deterministic responses; step-by-step prompting mitigates temporal myopia.
- **Core assumption:** Decomposing sequential decision-making into per-state prompts preserves goal-directed behavior without explicit planning horizon.
- **Evidence anchors:** [abstract] "Experiment 3 showed LLM-generated trajectories closely resembled human movement patterns in a dynamic multi-agent setting." [section 6.2] "By continuously querying the LLM in response to the dynamically changing environment, we generate decision trajectories that exhibit consistent decision-making behaviors."

## Foundational Learning

- **Concept:** Stag Hunt game theory paradigm (coordination game with risk-reward tradeoff)
  - **Why needed here:** Core experimental environment; understanding payoff matrix [(5,5) for mutual cooperation, (1,1) for mutual defection, asymmetric for mismatched choices] is essential to interpret decision alignment.
  - **Quick check question:** Given payoff matrix R(Stag,Stag)=(5,5) and R(Hare,Hare)=(1,1), why is Stag considered "risky"?

- **Concept:** Markov Decision Process (MDP) formulation
  - **Why needed here:** Formal framework for RL; understanding tuple (S, A, P, R, γ) clarifies how LLMs replace learned policies π:S→A with prompt-based action generation.
  - **Quick check question:** What does policy accessibility mean in HARL, and why does it break down with human teammates?

- **Concept:** Temperature and Top-P sampling in LLMs
  - **Why needed here:** Temperature=0 forces deterministic outputs (reduces hallucination); Top-P controls diversity. Understanding these parameters is critical for replicating experimental setup.
  - **Quick check question:** Why would you set temperature=0 for decision alignment but potentially higher Top-P for behavioral diversity?

## Architecture Onboarding

- **Component map:** Grid-World Environment -> Prompt Constructor -> LLM Agent Interface -> Trajectory Logger -> Evaluation Module
- **Critical path:** State extraction → Feature calculation (Manhattan distances) → Prompt template filling → LLM query → Action parsing → Environment step → Trajectory logging → Repeat until terminal state.
- **Design tradeoffs:** Relative distances vs. coordinates (reduces spatial interpretation burden but loses positional context); Temperature=0 vs. higher values (determinism improves alignment scores but may underrepresent human variability); Single-word output constraint (reduces parsing ambiguity but prevents explanation generation for debugging).
- **Failure signatures:** Llama 3.1 8B dominance of "Stag" outputs (F1=0.35) → model lacks capacity for strategic reasoning with this prompt encoding; Ceiling effect in 5×5 grid → environment too simple to differentiate risk profiles; Trajectory loops or non-convergence → LLM failing to track cumulative progress.
- **First 3 experiments:** 1) Replicate Experiment 1 with Llama 3.1 70B on 15 grid configurations; verify F1 ≥ 0.75 against expert labels using identical prompts. 2) Run Experiment 2 with risk-averse/risk-seeking/neutral prompts across 30 scenarios; confirm Risk Index distributions separate with gap ≥ 0.2 between conditions. 3) Deploy LLM agent in dynamic environment (Experiment 3); compare trajectory path lengths and target selection rates to human baseline (N=10 participants, 9 scenarios each).

## Open Questions the Paper Calls Out

- **Open Question 1:** Can LLMs effectively serve as human proxies in complex or continuous domains beyond the discrete 5x5 grid-world environment tested? The current validation is limited to a constrained state space with simplified relative distance features, which may not capture the challenges of high-dimensional inputs.
- **Open Question 2:** Does integrating LLM-based proxies into Reinforcement Learning training pipelines improve the robustness of agents compared to using pre-defined MDP agents? The paper demonstrated LLMs can generate valid trajectories but did not test if these trajectories effectively train a collaborative partner agent.
- **Open Question 3:** How does the performance of LLM proxies change when multiple LLM-controlled agents interact simultaneously with dynamically updating prompts? Experiment 3 only tested a single LLM agent interacting with a scripted agent, leaving multi-LLM dynamics unexplored.

## Limitations
- The 5×5 grid-world may be too simple to capture the complexity of real-world human decision-making
- Exclusive use of Manhattan distances as state features may not fully capture strategic nuances
- Reliance on a single human-subjects dataset without independent replication
- Trajectory generation experiment lacks quantitative metrics for behavioral assessment

## Confidence
- **High Confidence:** LLMs can replicate expert-level strategic decisions when provided with structured state features (Exp 1 results with F1 > 0.75 and κ > 0.58)
- **Medium Confidence:** Lightweight prompt modifications can systematically bias LLM decisions toward risk-averse or risk-seeking behaviors (Exp 2 shows distinct Risk Index distributions, but small sample sizes)
- **Medium Confidence:** LLMs queried state-by-state can generate coherent multi-step action trajectories resembling human movement patterns (Exp 3 shows qualitative similarity, but lacks quantitative validation)

## Next Checks
1. **Scale Environment Complexity:** Replicate Experiments 1-2 in larger grid-worlds (e.g., 10×10) with obstacles and multiple stag/hare targets to test whether performance degrades or prompt engineering remains effective
2. **Independent Human Baseline:** Collect a new dataset of human decisions in the same grid configurations to verify that LLM-expert alignment (F1 > 0.75) holds against fresh human data, not just historical expert labels
3. **Quantitative Trajectory Analysis:** Implement dynamic time warping (DTW) or Fréchet distance metrics to quantitatively compare LLM-generated trajectories against human paths in Experiment 3, moving beyond qualitative assessment