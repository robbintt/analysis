---
ver: rpa2
title: Byzantine-Robust Federated Learning with Learnable Aggregation Weights
arxiv_id: '2511.03529'
source_url: https://arxiv.org/abs/2511.03529
tags:
- clients
- malicious
- attack
- gradient
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FedLAW, a Byzantine-robust federated learning
  framework that treats aggregation weights as learnable parameters. By jointly optimizing
  weights and model parameters with a sparsity constraint, FedLAW effectively neutralizes
  malicious clients while adaptively balancing contributions from benign clients.
---

# Byzantine-Robust Federated Learning with Learnable Aggregation Weights

## Quick Facts
- **arXiv ID:** 2511.03529
- **Source URL:** https://arxiv.org/abs/2511.03529
- **Reference count:** 40
- **Primary result:** Achieves up to 8.3 percentage points higher accuracy under label flipping attacks and 3.1 points under inverse-gradient attacks compared to state-of-the-art Byzantine-robust FL approaches.

## Executive Summary
This paper introduces FedLAW, a Byzantine-robust federated learning framework that treats aggregation weights as learnable parameters. By jointly optimizing weights and model parameters with a sparsity constraint, FedLAW effectively neutralizes malicious clients while adaptively balancing contributions from benign clients. The proposed alternating minimization algorithm updates weights and model parameters in tandem, with strong theoretical guarantees for Byzantine resilience and convergence under adversarial attacks. Experimental results on MNIST and CIFAR10 datasets show FedLAW consistently outperforms state-of-the-art Byzantine-robust FL approaches, especially in highly non-IID and adversarial environments.

## Method Summary
FedLAW implements an alternating minimization algorithm that jointly optimizes model parameters θ and aggregation weights w. In each communication round, the server first computes a tentative model update using current weights, then queries clients for losses and gradients at this tentative point. The weights are updated via projection onto a sparse unit-capped simplex using a specific update rule involving gradient matrices and loss vectors. This two-round communication pattern allows weights to adapt to identify and suppress malicious clients while maintaining contributions from benign participants. The framework includes theoretical guarantees for Byzantine resilience and convergence, with experimental validation across multiple attack scenarios on standard FL benchmarks.

## Key Results
- FedLAW achieves up to 8.3 percentage points higher accuracy under label flipping attacks compared to state-of-the-art Byzantine-robust FL methods
- The framework demonstrates 3.1 percentage points improvement under inverse-gradient attacks
- Performance gains are particularly pronounced in highly non-IID data distributions with malicious client fractions up to 40%

## Why This Works (Mechanism)
FedLAW works by treating aggregation weights as learnable parameters that can be optimized to suppress malicious clients while amplifying benign contributions. The alternating minimization approach allows the system to adaptively identify attackers based on their gradient and loss patterns, assigning near-zero weights to malicious participants. The sparsity constraint ensures computational efficiency by limiting the number of active clients in each aggregation. The two-round communication protocol enables accurate weight updates by gathering loss information at tentative model updates, providing a reliable signal for distinguishing benign from malicious behavior.

## Foundational Learning
- **Byzantine-robust aggregation:** Essential for handling arbitrary malicious behavior in federated learning; quick check: verify aggregation weights converge to near-zero for known malicious clients
- **Alternating minimization:** Key optimization technique for jointly learning weights and model parameters; quick check: monitor objective function decrease across alternating steps
- **Projection onto sparse simplex:** Critical for enforcing sparsity while maintaining valid probability distributions; quick check: verify weight vectors satisfy unit-sum and sparsity constraints after projection
- **Non-IID federated learning:** Fundamental challenge in real-world deployments; quick check: measure client data distribution similarity using Dirichlet concentration parameter
- **Gradient-based attack detection:** Core mechanism for identifying malicious behavior; quick check: compare gradient norms and directions between suspected malicious and benign clients
- **Two-round communication protocol:** Enables accurate weight updates by gathering loss information; quick check: verify loss vectors are properly collected at tentative model updates

## Architecture Onboarding
**Component Map:** Server -> Client Pool -> Aggregation Weights -> Model Parameters -> Loss/Gradient Computation -> Weight Update
**Critical Path:** Model initialization → Client sampling → Gradient aggregation → Weight update → Model update → Loss collection → Final weight projection
**Design Tradeoffs:** The framework trades additional communication rounds (two per global epoch) for significantly improved robustness against Byzantine attacks. The sparsity constraint balances computational efficiency against the risk of prematurely suppressing benign clients with outlier gradients.
**Failure Signatures:** Weight divergence or NaN values indicate learning rate issues or insufficient gradient clipping. Slow convergence or poor attack detection suggests incorrect projection logic or weight initialization problems.
**Three First Experiments:**
1. Verify weight projection by testing with synthetic data where top-s entries are known, confirming correct sub-vector projection onto the unit simplex
2. Test weight evolution on a simple two-client scenario with one malicious client, verifying malicious weight converges to near-zero
3. Validate alternating minimization loop on a convex problem, checking that both objective value and weight norms decrease monotonically

## Open Questions the Paper Calls Out
None

## Limitations
- Exact neural network architecture specifications are missing, particularly hidden layer sizes for the MLP and precise CNN configuration with Group Normalization
- Weight initialization procedure for the first round lacks explicit detail on sparsity enforcement
- Gradient clipping thresholds used during training are not specified, which are critical for stability given the product terms in weight updates

## Confidence
- **High Confidence:** Theoretical framework for Byzantine resilience and convergence guarantees is well-established
- **Medium Confidence:** Experimental methodology and attack implementations are detailed enough for reproduction
- **Medium Confidence:** Reported performance improvements are plausible given strong theoretical foundation

## Next Checks
1. Implement and verify the projection onto the sparse unit-capped simplex with correct selection of top-s entries and sub-vector projection logic
2. Monitor weight evolution across training rounds to confirm malicious clients' weights converge to near-zero while benign clients maintain positive weights
3. Validate the alternating minimization loop by checking that the norm of weight updates decreases over time, indicating convergence of the outer loop