---
ver: rpa2
title: Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical
  Learning
arxiv_id: '2508.20712'
source_url: https://arxiv.org/abs/2508.20712
tags:
- discourse
- language
- discogem
- each
- multi-label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces the first multi-lingual and multi-label classification
  model for implicit discourse relation recognition (IDRR). The proposed HArch model
  leverages hierarchical dependencies across all three sense levels in the PDTB 3.0
  framework to predict probability distributions, outperforming prior multi-label
  approaches.
---

# Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning

## Quick Facts
- arXiv ID: 2508.20712
- Source URL: https://arxiv.org/abs/2508.20712
- Reference count: 32
- Introduces first multi-label, multi-lingual IDRR model (HArch) outperforming prior approaches on DiscoGeM 2.0 corpus

## Executive Summary
This paper introduces HArch, the first multi-label, multi-lingual model for implicit discourse relation recognition (IDRR) that predicts probability distributions across all three PDTB 3.0 sense levels. The model leverages hierarchical dependencies between discourse senses through cascaded augmentation blocks, achieving superior performance on DiscoGeM 2.0 corpus across English, German, French, and Czech. Fine-tuned HArch models consistently outperform few-shot prompting with large language models, validating the effectiveness of task-specific fine-tuning for this task.

## Method Summary
HArch is a hierarchical multi-task model that jointly predicts probability distributions across three PDTB 3.0 sense levels (4 level-1, 17 level-2, 28 level-3 senses). It uses a shared encoder with three classification heads, where augmentation blocks project lower-level predictions into the encoder embedding space to condition higher-level heads. The model is trained using MAE loss across all heads and evaluated with Jensen-Shannon distance. It was tested on DiscoGeM 2.0 corpus with RoBERTa for English and XLM-RoBERTa for multi-lingual settings, compared against few-shot prompting with GPT-4o and Llama-4-Maverick.

## Key Results
- HArch with RoBERTa achieves best results in English; XLM-RoBERTa-HArch excels in multi-lingual setting
- Fine-tuned HArch models outperform GPT-4o and Llama-4-Maverick in few-shot prompting across all languages
- RoBERTa-HArch achieves SOTA results on DiscoGeM 1.0 corpus, validating hierarchical approach effectiveness

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical propagation of sense predictions across PDTB 3.0 levels improves finer-grained classification. The HArch model predicts level-1 probability distribution, projects it via augmentation blocks into encoder embedding space, and combines with shared representation to condition level-2 and level-3 heads. This assumes PDTB sense levels are hierarchically dependent such that coarser predictions constrain finer ones.

### Mechanism 2
Multi-task joint training across all three sense levels improves performance over single-level models. Shared encoder and dropout representation feed three classification heads simultaneously with combined MAE loss. This assumes shared representations capture discourse-relevant features useful across granularity levels.

### Mechanism 3
Task-specific fine-tuning of smaller encoder-based models outperforms few-shot prompting of large language models. Fine-tuning directly optimizes parameters on target distribution via MAE loss, while few-shot prompting provides in-context examples without gradient updates. This assumes IDRR benefits more from gradient-based adaptation than parametric knowledge and in-context learning.

## Foundational Learning

- **Penn Discourse Treebank (PDTB) 3.0 sense hierarchy**: HArch operates on 3-level PDTB sense taxonomy (4 level-1, 17 level-2, 28 level-3 classes). Understanding hierarchy is essential to interpret outputs and hierarchical dependency assumption. *Quick check: Can you name the four level-1 senses in PDTB 3.0 and explain how level-2 senses inherit from them?*

- **Multi-label classification with probability distributions**: DiscoGeM 2.0 provides soft labels (probability distributions over senses) aggregated from crowd annotations. Model predicts distributions, evaluated via Jensen-Shannon distance, using MAE loss over probability vectors. *Quick check: How does training with soft-label probability distributions differ from training with one-hot labels in terms of loss function and evaluation metrics?*

- **Encoder-only transformer fine-tuning vs. LLM few-shot prompting**: Paper compares fine-tuned RoBERTa/XLM-RoBERTa with GPT-4o/Llama-4-Maverick prompting. Understanding difference in parameter updates, in-context learning, and cost is crucial for reproducing and extending work. *Quick check: What are the key differences in how a fine-tuned RoBERTa model and a few-shot prompted LLM adapt to a new task?*

## Architecture Onboarding

- **Component map**: Input (ARG1 + ARG2) → Pre-trained encoder (RoBERTa/XLM-RoBERTa) → Shared linear + dropout → Three classification heads (level-1, level-2, level-3) → Augmentation blocks (linear→GELU→linear→GELU) → Learnable weighted sums (α, β1, β2) → Combined MAE loss

- **Critical path**: Encode argument pair → Apply shared linear + dropout → Predict level-1 distribution → Augment level-1 output and combine with shared representation using α → Predict level-2 distribution → Augment level-1 and level-2 outputs and combine using β1, β2 → Predict level-3 distribution → Compute MAE loss for each head and sum → Backpropagate

- **Design tradeoffs**: Encoder choice (RoBERTa best for English, XLM-RoBERTa best for multilingual); Hierarchical vs. independent predictions (hierarchical improves level-2/3 accuracy but adds complexity); Multi-task vs. single-level models (multi-task improves performance and efficiency but requires careful balancing); LLM vs. fine-tuned encoders (fine-tuning cheaper and more accurate but less flexible)

- **Failure signatures**: Degenerate outputs (all probability mass on one class suggests augmentation or loss weighting issues); High JS distance at level-1 (may indicate unsuitable encoder for language/domain); Large discrepancy between multilingual and monolingual performance (may indicate insufficient data for low-resource languages or encoder limitations); LLM prompting errors (non-conforming outputs require retries; high cost and low accuracy)

- **First 3 experiments**:
  1. Reproduce RoBERTa-HArch on DiscoGeM 2.0 English validation split with reported hyperparameters; compare level-2/3 JS distances to non-hierarchical baseline
  2. Train XLM-RoBERTa-HArch on all languages jointly; evaluate per-language JS distances to confirm multilingual training benefits
  3. Implement few-shot prompting setup for GPT-4o and Llama-4-Maverick; compare JS distances and cost to fine-tuned models

## Open Questions the Paper Calls Out

- How does translation influence discourse reasoning and label distribution across English, German, French, and Czech parallel relations in DiscoGeM 2.0?

- How would language-specific pre-trained encoders (e.g., CamemBERT for French) compare to XLM-RoBERTa in the HArch architecture for non-English languages?

- Would the hierarchical dependencies learned by HArch facilitate zero-shot cross-lingual transfer to languages not present in training data?

## Limitations

- Hierarchical dependency assumption lacks corpus-level validation - no empirical evidence demonstrates PDTB 3.0 levels have the assumed semantic dependencies
- Multilingual generalization shows performance drops for lower-resource languages (Czech), with unclear if hierarchical gains are consistent across all languages
- LLM comparison methodology uses single prompt template without exploring optimization strategies that could improve few-shot performance

## Confidence

- Hierarchical dependency assumption: Medium - theoretically motivated but not empirically verified
- Generalization across languages: Medium-Low - relies on XLM-RoBERTa transfer with performance variations across languages
- LLM comparison methodology: Low-Medium - single prompt strategy may not reflect current state of prompting techniques

## Next Checks

1. Analyze DiscoGeM 2.0 annotations to measure actual conditional probabilities P(level-2|level-1) and P(level-3|level-2); compare HArch against independent multi-label model to isolate hierarchical benefits

2. Train independent single-language models for each language and compare performance against multilingual XLM-RoBERTa-HArch; analyze whether hierarchical gains are consistent across all four languages

3. Implement and test multiple prompt strategies for GPT-4o and Llama-4-Maverick including chain-of-thought prompting, example selection strategies, and instruction fine-tuning; compare optimized prompting against current few-shot results