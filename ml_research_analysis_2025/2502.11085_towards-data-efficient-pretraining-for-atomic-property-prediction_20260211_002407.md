---
ver: rpa2
title: Towards Data-Efficient Pretraining for Atomic Property Prediction
arxiv_id: '2502.11085'
source_url: https://arxiv.org/abs/2502.11085
tags:
- pretraining
- dataset
- datasets
- downstream
- upstream
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of efficient pretraining in\
  \ atomic property prediction, challenging the assumption that larger datasets always\
  \ lead to better performance. The authors introduce the Chemical Similarity Index\
  \ (CSI), a novel metric inspired by the Fr\xE9chet Inception Distance, to quantify\
  \ the alignment between pretraining and downstream datasets."
---

# Towards Data-Efficient Pretraining for Atomic Property Prediction

## Quick Facts
- arXiv ID: 2502.11085
- Source URL: https://arxiv.org/abs/2502.11085
- Reference count: 28
- Primary result: Smaller, task-aligned pretraining datasets can outperform large-scale pretraining by up to 24x while achieving comparable or superior performance

## Executive Summary
This paper challenges the assumption that larger pretraining datasets always lead to better atomic property prediction. The authors introduce the Chemical Similarity Index (CSI), a novel metric that quantifies alignment between pretraining and downstream datasets using Fréchet distance. By selecting pretraining data based on minimal CSI distance rather than volume, the study demonstrates that smaller, chemically-aligned datasets achieve superior performance while dramatically reducing computational costs. The findings show that indiscriminate data addition can degrade performance when additional data poorly aligns with target tasks.

## Method Summary
The method involves computing CSI between candidate upstream datasets and the target downstream task using node-level embeddings from an OC20-pretrained EquiformerV2 model. The upstream dataset with the lowest CSI is selected for pretraining. GemNet-OC-S is then trained on 2M samples for 5 epochs using a weighted multi-task objective (energy and forces). The pretrained backbone is finetuned on the downstream task with a new prediction head. CSI calculation uses class-balanced sampling of 10k instances per dataset, measuring Fréchet distance between feature distributions.

## Key Results
- Pretraining on ANI-1x (10M samples) outperformed JMP-S (240M samples) on rMD17 (5.2 vs 6.7 MAE) and QM9 (3.1 vs 3.3 MAE)
- CSI-based selection reduced computational costs by up to 24x compared to large-scale pretraining
- Adding misaligned data (OC22) to aligned data (ANI-1x) degraded performance across all tested tasks
- Node-level features provided higher effective rank than graph-level pooling for CSI calculation

## Why This Works (Mechanism)

### Mechanism 1: Distribution Alignment via Chemical Similarity Index (CSI)
Selecting pretraining data based on distribution alignment minimizes the transfer gap better than scaling data volume. CSI quantifies Fréchet distance between feature distributions of upstream and downstream datasets. Lower CSI indicates higher alignment, enabling the model to learn representations structurally proximate to the target task. This mechanism assumes node-level embeddings capture relevant chemical physics, as evidenced by ANI-1x consistently yielding lowest MAE on downstream tasks with minimal CSI.

### Mechanism 2: Negative Transfer from Unaligned Data Mixing
Indiscriminately adding diverse but misaligned data degrades performance under constrained computational budgets. With fixed budget $C = N \times E$, training on misaligned data appears as noise that dilutes the learned manifold relevant to the target task. This reduces effective capacity for the relevant chemical subspace compared to training solely on aligned data, as demonstrated when adding 1M OC22 samples to 2M ANI-1x samples worsened performance across all tasks.

### Mechanism 3: Expressivity Preservation via Node-Level Features
Using node-level features rather than graph-level aggregates for CSI preserves chemical diversity required for effective alignment measurement. Graph-level pooling reduces feature representation rank, discarding intrinsic variance dimensions. Node-level CSI retains higher "Effective Rank," distinguishing subtle chemical differences between datasets that graph-level metrics miss. This assumes higher effective rank in feature covariance correlates with better utility for measuring distributional shifts in chemical space.

## Foundational Learning

- **Fréchet Inception Distance (FID)**: Measures distance between two Gaussians (datasets) in feature space. Why needed: CSI is an adaptation of FID, so understanding FID is essential for grasping why CSI works as a similarity metric. Quick check: If two datasets have identical means but different covariances, would FID be zero? (Answer: No, FID accounts for both mean and trace of covariance).

- **Transfer Learning & Fine-Tuning**: Paradigm of pretraining a backbone on upstream data and fine-tuning on downstream tasks. Why needed: The entire approach relies on this transfer learning framework. Quick check: What components are typically discarded when switching from pretraining (multi-task energy/force) to fine-tuning? (Answer: Task-specific heads).

- **Computational Budgeting ($C = E \times N$)**: Trade-off between epochs and samples under fixed compute. Why needed: The paper's claims are conditional on fixed compute budgets, making this trade-off vital for interpreting why "more data" can be worse. Quick check: Does the paper recommend training for more epochs on less data or less epochs on more data for aligned datasets? (Answer: For highly aligned data, training on fewer samples for more epochs is competitive/superior).

## Architecture Onboarding

- **Component map:** EquiformerV2 (OC20-pretrained) -> Node Embeddings -> CSI Calculator -> Selected Upstream Dataset -> GemNet-OC-S Pretraining -> Finetuning Head -> Downstream Evaluation

- **Critical path:** Select dataset by computing CSI against candidates, filter to lowest CSI dataset, train on this dataset under budget $C$, finetune on target task

- **Design tradeoffs:** Single aligned datasets offer better efficiency (24x cost reduction) but joint datasets may be more robust for OOD tasks. Node features offer higher rank for CSI but require class-balancing for variable graph sizes

- **Failure signatures:** OOD mismatch where CSI fails to predict performance on tasks like MatBench (phonons); diversity dilution where performance drops when adding large, low-alignment datasets to smaller, high-alignment sets

- **First 3 experiments:**
  1. Compute CSI for ANI-1x and OC20 relative to rMD17 dataset, verify ANI-1x has lower CSI, then confirm via 10M budget pretraining that ANI-1x yields lower MAE
  2. Pretrain two models on ANI-1x (2M samples); one purely on ANI-1x, other mixed with 1M OC22 samples, compare fine-tuning MAE to observe degradation from unaligned data
  3. Train on best-aligned dataset using $N \in \{0.5M, 1M, 2M\}$ samples for 5 epochs, plot MAE curve to verify performance saturation differs for aligned vs. misaligned sources

## Open Questions the Paper Calls Out

- How can CSI be adapted to reliably predict optimal pretraining sources for Out-of-Distribution (OOD) downstream tasks? The current metric fails for MatBench where physics differs significantly from pretraining energy/force objectives.

- Can sample-level selection strategies outperform dataset-level alignment achieved by CSI? The paper identifies sample selection as a distinct gap, as the current approach assumes homogeneity within selected datasets.

- Does CSI effectiveness persist when using multi-modal foundation models for feature extraction rather than single-task models? Current limitations for OOD tasks may stem from the limited diversity of the backbone used for feature extraction.

## Limitations
- Results may not generalize to property prediction tasks beyond energy/force (e.g., band gaps, phonon frequencies) where chemical alignment patterns differ
- Computational budget constraint assumes fixed GPU memory and step limits, potentially varying with different hardware constraints or larger models
- CSI correlation with downstream performance shows "clear but not perfect" alignment, suggesting unexplained variance in transfer effectiveness

## Confidence
- **High Confidence:** Pretraining on aligned, smaller datasets consistently outperforms misaligned large datasets within same computational budget
- **Medium Confidence:** CSI reliably predicts which upstream dataset will yield best downstream performance (correlation exists but isn't perfect, especially for OOD tasks)
- **Medium Confidence:** 24x cost reduction claim depends on specific computational budgets and may vary with different model architectures or task complexities

## Next Checks
1. Test CSI-based selection on MatBench phonon prediction to confirm whether correlation holds for fundamentally different physical properties
2. Run experiments comparing single-dataset selection versus joint mixing of aligned datasets to identify exact budget threshold where mixing becomes advantageous
3. Validate that CSI-based selection remains effective when using different GNN architectures (e.g., DimeNet, SphereNet) to ensure mechanism isn't architecture-specific