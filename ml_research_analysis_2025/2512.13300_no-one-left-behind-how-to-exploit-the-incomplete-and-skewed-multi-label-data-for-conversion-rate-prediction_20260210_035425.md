---
ver: rpa2
title: 'No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data
  for Conversion Rate Prediction'
arxiv_id: '2512.13300'
source_url: https://arxiv.org/abs/2512.13300
tags:
- conversion
- action
- data
- samples
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training multi-task CVR models
  with incomplete and skewed multi-label data in online advertising. Many advertisers
  submit only a subset of user conversion actions due to privacy constraints, leading
  to asymmetric multi-label data.
---

# No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction

## Quick Facts
- arXiv ID: 2512.13300
- Source URL: https://arxiv.org/abs/2512.13300
- Reference count: 40
- Key outcome: Achieves 12.11% RPM and 0.92% CVR improvement in online A/B tests using KAML framework

## Executive Summary
This paper addresses the challenge of training multi-task CVR models with incomplete and skewed multi-label data in online advertising. Advertisers submit only a subset of user conversion actions due to privacy constraints, creating asymmetric multi-label data where "0" can mean either "no conversion" or "not reported." The proposed KAML framework introduces three key innovations: an attribution-driven masking strategy to identify reliable conversion signals, a hierarchical knowledge extraction mechanism to handle sample distribution discrepancies, and a ranking-based label utilization strategy to maximize the utility of unlabeled samples.

## Method Summary
The paper proposes KAML (Knowledge-Adaptive Multi-task Learning) framework to address incomplete and skewed multi-label data in CVR prediction. The framework builds on MMoE architecture and introduces three key innovations: ADM (Attribution-Driven Masking) to identify reliable conversion signals from advertisers' historical submissions, HKE (Hierarchical Knowledge Extraction) to address sample distribution discrepancies across different advertisers, and RLU (Ranking-based Label Utilization) to maximize the utility of unlabeled samples. The model is trained using Adam optimizer with joint loss combining binary cross-entropy and pairwise ranking loss.

## Key Results
- Achieves 12.11% improvement in Revenue Per Mille (RPM) and 0.92% improvement in Conversion Rate (CVR) in online A/B tests
- Outperforms existing MTL baselines on both industrial and public datasets
- Demonstrates significant improvements in AUC while maintaining competitive LogLoss performance

## Why This Works (Mechanism)
The framework works by intelligently handling the uncertainty in incomplete labels through three complementary strategies. ADM identifies reliable conversion signals by analyzing advertisers' historical submission patterns, effectively filtering out false negatives. HKE addresses the distribution mismatch between different advertisers by creating specialized sub-towers for different sample types. RLU leverages the relative ordering information in unlabeled samples to improve ranking performance without requiring explicit labels.

## Foundational Learning

**Multi-task Learning (MTL)**: Training a single model to perform multiple related tasks simultaneously, sharing representations to improve generalization and efficiency. Needed because CVR prediction involves multiple conversion types that are related but have different patterns. Quick check: Model should show improved performance across all tasks compared to single-task models.

**Asymmetric Multi-label Data**: Data where labels are missing not at random, with different advertisers providing different subsets of conversion labels. Needed because real-world advertising data has privacy constraints and reporting inconsistencies. Quick check: Data should show significant variation in label availability across advertisers.

**Knowledge Distillation**: Transferring knowledge from a larger model or ensemble to a smaller, more efficient model. Needed for handling the complexity of multiple conversion types while maintaining computational efficiency. Quick check: Student model should achieve comparable performance to teacher model with fewer parameters.

## Architecture Onboarding

**Component Map**: Input Features -> MMoE Gates -> Task-specific Towers -> ADM Masking -> HKE Sub-towers -> RLU Loss -> Output Predictions

**Critical Path**: Feature extraction → Gate assignment → Task tower processing → ADM masking → HKE routing → Final prediction with joint loss

**Design Tradeoffs**: The framework trades increased model complexity (additional sub-towers and masking logic) for improved handling of incomplete data, versus simpler models that may ignore the label uncertainty issue entirely.

**Failure Signatures**: 
- LogLoss divergence when RLU is too aggressive
- Performance degradation when ADM threshold is set too low (too many false negatives)
- Computational overhead from additional sub-towers in HKE

**Exactly 3 First Experiments**:
1. Implement MMoE baseline with basic multi-task learning on KuaiRand-Pure dataset
2. Add ADM component with varying threshold values to evaluate masking effectiveness
3. Integrate HKE mechanism to handle sample distribution discrepancies

## Open Questions the Paper Calls Out

**Open Question 1**: Can the Ranking-based Label Utilization (RLU) module be modified to prevent the degradation of Logloss while maintaining its improvements in AUC?
Basis: Section 4.2.1 notes that KAML fails to achieve optimal Logloss in some scenarios, attributed to the RLU module.
Evidence needed: A variation of KAML with calibration constraint or modified ranking loss demonstrating simultaneous improvements in both AUC and Logloss.

**Open Question 2**: Is the fixed threshold hyper-parameter (α) in the Attribution Driven Masking (ADM) strategy robust enough to handle dynamic changes in advertiser submission frequency?
Basis: Section 3.2 defines the mask condition using a static hyper-parameter, assuming stable historical submission counts.
Evidence needed: Sensitivity analysis across varying α values or introduction of dynamic/learnable threshold mechanism.

**Open Question 3**: What is the inference latency overhead of the Hierarchical Knowledge Extraction (HKE) mechanism in high-throughput online serving environments?
Basis: Section 3.3 describes HKE architecture with separate sub-towers, effectively doubling tower network parameters.
Evidence needed: Reporting inference time (QPS or latency in ms) of KAML relative to base MMoE model.

## Limitations

- Experimental setup relies heavily on proprietary industrial datasets and modified public datasets with unspecified preprocessing details
- Key hyperparameters (alpha for ADM, beta for RLU) are not provided, requiring extensive tuning
- Performance gains are highly dependent on threshold choices that are described as application-specific

## Confidence

**High Confidence**: The overall problem statement (incomplete and skewed multi-label data in CVR prediction) is well-defined and relevant. The three proposed innovations (ADM, HKE, RLU) are clearly articulated and logically sound as a solution strategy.

**Medium Confidence**: The MMoE architecture with the specific ADM, HKE, and RLU components can be implemented as described, but exact performance is uncertain due to unspecified hyperparameters and dataset preprocessing details.

**Low Confidence**: The exact quantitative results (12.11% RPM improvement, 0.92% CVR improvement) and their reproducibility are uncertain without access to industrial dataset and precise implementation details.

## Next Checks

1. **ADM Threshold Sensitivity Analysis**: Systematically vary the alpha threshold for ADM across a wide range (e.g., 1, 5, 10, 20) and evaluate impact on AUC and LogLoss. Report optimal alpha and performance drop with sub-optimal values.

2. **Extended Sample Quality Assessment**: For fixed alpha, analyze quality of "extended" samples identified by ADM. Calculate positive rate in original vs. extended sets. If extended set has significantly lower positive rate, this suggests potential noise from false negatives.

3. **Public Dataset Ablation Study**: Implement full KAML framework on KuaiRand-Pure dataset. Conduct ablation study by removing each component (ADM, HKE, RLU) individually and measuring drop in AUC to quantify marginal contribution of each innovation.