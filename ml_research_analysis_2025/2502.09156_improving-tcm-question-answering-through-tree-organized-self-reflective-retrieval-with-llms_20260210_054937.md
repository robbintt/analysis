---
ver: rpa2
title: Improving TCM Question Answering through Tree-Organized Self-Reflective Retrieval
  with LLMs
arxiv_id: '2502.09156'
source_url: https://arxiv.org/abs/2502.09156
tags:
- knowledge
- chinese
- medicine
- treatment
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Tree-Organized Self-Reflective Retrieval
  (TOSRR) framework for Traditional Chinese Medicine (TCM) question answering, addressing
  the challenge of insufficient knowledge representation and retrieval in TCM domains.
  The method constructs a tree-structured knowledge base using Subject-Predicate-Object-Text
  (SPO-T) triples and integrates a self-reflective mechanism for iterative retrieval
  and validation.
---

# Improving TCM Question Answering through Tree-Organized Self-Reflective Retrieval with LLMs

## Quick Facts
- arXiv ID: 2502.09156
- Source URL: https://arxiv.org/abs/2502.09156
- Reference count: 0
- Primary result: 19.85% absolute accuracy improvement on TCM MLE with SPO-T RAG

## Executive Summary
This paper introduces TOSRR, a Tree-Organized Self-Reflective Retrieval framework for Traditional Chinese Medicine question answering. The method constructs a tree-structured knowledge base using SPO-T (Subject-Predicate-Object-Text) triples and integrates a self-reflective mechanism for iterative retrieval and validation. Experiments on TCM Medical Licensing Examination and Classics Course Exam datasets show TOSRR improves GPT-4 performance by 19.85% in absolute accuracy on MLE and increases recall accuracy from 27% to 38% on CCE. Manual evaluation demonstrates improvements of 18.52 points across safety, consistency, explainability, compliance, and coherence dimensions.

## Method Summary
The framework builds a tree-structured TCM knowledge base from 33 textbooks using SPO-T triples (Subject-Predicate-Object-Text), where text chunks are attached to leaf nodes. Content is segmented (200-300 words), extracted as Q&A pairs and summaries using GPT-4, and reviewed by experts in two rounds. Dual-path retrieval combines keyword matching (5 SPO-T via IK Analysis) and vector similarity (10 via HNSW) to return top-15 results. A self-reflective loop assesses relevance, checks for empty retrieval, verifies answer support, and evaluates helpfulness before final answer generation with GPT-4.

## Key Results
- TOSRR improves GPT-4 performance by 19.85% absolute accuracy on TCM MLE (49.83% → 69.68%)
- Recall accuracy increases from 27% to 38% on CCE using SPO-T structure
- Manual evaluation shows 18.52-point improvement across five dimensions (safety, consistency, explainability, compliance, coherence)
- SPO-T RAG outperforms both vanilla GPT-4 and traditional RAG methods on factual information and case analysis questions

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical SPO-T structures improve retrieval precision by preserving semantic richness alongside structured relationships. Traditional SPO triples suffer from knowledge sparsity when handling complex reasoning. SPO-T attaches full text chunks to leaf nodes, allowing the model to access both structured hints and contextual depth during generation. Evidence shows SPO-T RAG improved recall by 0.11 and case analysis scores by 20.6%. Break condition: If text chunks are too large (>500 tokens) or triples are too sparse (<30% coverage), semantic benefits may reverse into noise.

### Mechanism 2
Self-reflective loops reduce hallucinations by forcing explicit verification of answer grounding. The framework implements a 4-step decision flow: filter irrelevant SPO-T entries, check if retrieval is non-empty before answering, verify answer is supported by retrieved content, and confirm helpfulness or reformulate question. This creates iterative correction opportunities. Evidence shows TOSRR and SPO-T RAG scored significantly higher than GPT-4 on 600-point tests, with noticeable improvement in factual information questions. Break condition: If self-assessment prompts are poorly calibrated, models may falsely reject correct answers or accept incorrect ones with high confidence.

### Mechanism 3
Dual-path retrieval (keyword + vector) compensates for individual recall method failures. SPO triples use IK Analysis keyword matching (5 results), while text summaries and Q&A pairs use HNSW vector search (10 results). This hybrid approach targets both precise entity matching and semantic similarity. Evidence shows the SPO-T structure improved recall from 27% to 38%. Break condition: If keyword matching is too strict (misses synonyms) or vector search returns semantically similar but factually irrelevant chunks, combined results may increase noise without precision gains.

## Foundational Learning

- **Knowledge Graph SPO Triples (Subject-Predicate-Object)**
  - Why needed here: The SPO-T structure extends SPO triples; understanding base KG representation is prerequisite to grasping why text attachment matters
  - Quick check question: Given "Ma Huang - treats - asthma," can you identify what information is lost that a full sentence would preserve?

- **Retrieval-Augmented Generation (RAG)**
  - Why needed here: TOSRR is a RAG framework; distinguishing it from fine-tuning approaches clarifies why external knowledge is queried at inference time
  - Quick check question: If a medical term appears in training data but with incorrect frequency, would fine-tuning or RAG better address factual errors?

- **Self-Reflection in LLMs**
  - Why needed here: The reflective loop requires understanding how LLMs can evaluate their own outputs through prompted self-critique
  - Quick check question: What is the risk of a self-reflection prompt that asks "Is this answer correct?" without providing verification criteria?

## Architecture Onboarding

- **Component map**: Knowledge Base Construction -> Storage Layer -> Retrieval Layer -> Self-Reflection Engine -> Generation Layer
- **Critical path**: Query -> embedding + keyword extraction -> parallel retrieval (vector + keyword) -> merge results -> self-reflection loop -> final answer generation with citation-linked context
- **Design tradeoffs**: SPO-T vs. pure text RAG gains cross-chapter reasoning but increases storage and retrieval latency; self-reflection iterations vs. latency - more iterations improve accuracy but slow response; manual expert review vs. scalability - two-round review ensures quality but limits KB expansion speed
- **Failure signatures**: Recall degradation if SPO-T count drops below ~20,000 for broad domains; reflection loops without termination if helpfulness checks are ambiguous; hallucination on out-of-domain queries without domain classifier
- **First 3 experiments**: (1) Ablation on SPO vs. SPO-T - run RAG with SPO triples only vs. SPO-T to isolate text attachment contribution on case analysis questions; (2) Reflection depth analysis - log iteration counts per question type; correlate with accuracy to find optimal stopping criteria; (3) Recall precision audit - sample 100 queries, manually score top-15 retrieved SPO-Ts for relevance; compare keyword-only vs. vector-only vs. hybrid to validate 27%→38% recall claim

## Open Questions the Paper Calls Out
- **Open Question 1**: What is the efficacy of the TOSRR framework when deployed in actual clinical assistance and TCM teaching scenarios, as opposed to the simulated examination benchmarks?
- **Open Question 2**: Can automated metrics such as RAGAs scoring effectively replace or supplement the manual expert evaluation methods used in this study to assess retrieval accuracy?
- **Open Question 3**: How does integrating real-world user interaction data specifically optimize the safety and accuracy of the model's responses?
- **Open Question 4**: Is the TOSRR framework dependent on the proprietary capabilities of GPT-4, or is it effective when applied to smaller, open-source large language models?

## Limitations
- Exact implementation details including SPO-T extraction prompts, self-reflection decision criteria, and expert review workflows are unspecified
- Reported improvements require validation on external TCM datasets beyond the in-house constructed datasets
- Iterative nature of self-reflection lacks specification of iteration depth and stopping criteria, creating uncertainty about computational efficiency trade-offs

## Confidence
- **High confidence**: Dual-path retrieval (keyword + vector) improves recall from 27% to 38% (supported by ablation and external RAG literature)
- **Medium confidence**: SPO-T structure provides significant benefit over pure SPO triples (limited by lack of direct SPO vs. SPO-T ablation results)
- **Medium confidence**: Self-reflective loops reduce hallucinations (mechanism plausible but iteration depth and false-positive rates unspecified)

## Next Checks
1. **External dataset validation**: Evaluate TOSRR on MTCMB or other independent TCM benchmarks to verify 19.85% improvement claim
2. **Iteration efficiency analysis**: Log and analyze self-reflection iteration counts per question type to establish optimal stopping criteria and computational overhead
3. **Ablation study**: Implement pure SPO RAG vs. SPO-T RAG comparison to isolate the contribution of text attachment to case analysis performance gains