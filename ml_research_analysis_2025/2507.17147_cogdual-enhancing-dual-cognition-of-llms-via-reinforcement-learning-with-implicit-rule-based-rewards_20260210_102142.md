---
ver: rpa2
title: 'CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with
  Implicit Rule-Based Rewards'
arxiv_id: '2507.17147'
source_url: https://arxiv.org/abs/2507.17147
tags:
- character
- reasoning
- cognitive
- cogdual
- dual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes CogDual, a dual cognitive framework for Role-Playing
  Language Agents (RPLAs) that incorporates situational and self-awareness to improve
  contextual understanding and character consistency. The method employs a cognize-then-respond
  paradigm and enhances the model through reinforcement learning using two implicit
  reward schemes: Inference-Conditioned Likelihood Gain (ICLG) and Latent Semantic
  Alignment (LSA).'
---

# CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards

## Quick Facts
- arXiv ID: 2507.17147
- Source URL: https://arxiv.org/abs/2507.17147
- Reference count: 20
- This paper proposes CogDual, a dual cognitive framework for Role-Playing Language Agents (RPLAs) that incorporates situational and self-awareness to improve contextual understanding and character consistency. The method employs a cognize-then-respond paradigm and enhances the model through reinforcement learning using two implicit reward schemes: Inference-Conditioned Likelihood Gain (ICLG) and Latent Semantic Alignment (LSA). Evaluated on the CoSER benchmark and generalization tasks (Cross-MR, LifeChoice), CogDual achieves up to 9.24% average improvement over baselines, demonstrating superior performance in storyline consistency, anthropomorphism, and character fidelity across multiple open-source and closed-source LLMs.

## Executive Summary
This paper introduces CogDual, a dual cognitive framework designed to enhance the role-playing capabilities of large language models by explicitly modeling both situational and self-awareness. The framework operates on a cognize-then-respond paradigm, where the model first generates a structured cognitive trace capturing environmental perception, others' behaviors, and internal psychological dynamics before producing a response. This approach is validated through comprehensive experiments on the CoSER benchmark and generalization tasks, demonstrating significant improvements in storyline consistency, character fidelity, and anthropomorphism. The method leverages implicit reinforcement learning rewards (ICLG and LSA) to train the model, showing that the dual-cognition structure effectively guides narrative coherence and character alignment without relying on explicit rule-based rewards.

## Method Summary
CogDual employs a two-stage training process. First, supervised fine-tuning (SFT) is performed on a dataset of 17,762 dual-cognition trajectories generated by GPT-4o, where each trajectory consists of a cognitive trace (Situational Awareness + Self-Awareness) followed by a response. Second, reinforcement learning with GRPO optimizes the model using a combination of ICLG (Inference-Conditioned Likelihood Gain) and LSA (Latent Semantic Alignment) rewards. ICLG encourages reasoning traces that causally support higher-likelihood generation of gold responses, while LSA rewards semantic fidelity while permitting lexical diversity. The final model is evaluated on the CoSER benchmark and generalization tasks, demonstrating superior performance in storyline consistency, anthropomorphism, and character fidelity.

## Key Results
- CogDual achieves up to 9.24% average improvement over baselines on the CoSER benchmark.
- The RL model consistently outperforms SFT-only models in both Storyline Consistency (2.85 points) and Character Fidelity (3.50 points).
- Ablation studies show that the full dual cognition model yields highest or near-highest scores across Storyline Consistency, Character Fidelity, and overall average.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Structured dual-cognition reasoning traces generated before responses improve character consistency and contextual alignment in role-playing language agents.
- Mechanism: The model first produces a JSON-formatted cognitive trace containing (1) Situational Awareness (environmental perception + others' behavior/emotion/intention analysis) flowing into (2) Self-Awareness (key memory activation, self-emotion, self-intentions, internal strategy), then generates the response conditioned on this trace.
- Core assumption: Human-like role behaviors emerge from an integrated cognitive process involving environmental perception, others' behaviors, and introspection of emotions/intentions, which should precede action generation (Section 1, citing Grice 1975; Clark and Brennan 1991; Tomasello 2010).
- Evidence anchors:
  - [abstract] "By jointly modeling external situational awareness and internal self-awareness, CogDual generates responses with improved character consistency and contextual alignment."
  - [Section 5.3 / Figure 3] Ablation shows the full dual cognition model yields highest or near-highest scores across Storyline Consistency, Character Fidelity, and overall average; removing either component degrades performance.
  - [corpus] Related work (Character-R1, HER) similarly targets internal reasoning for role-playing, suggesting convergent evidence that reasoning-before-responding improves consistency, though direct architectural comparisons are limited.
- Break condition: If cognitive traces become boilerplate templates that do not adapt to scene-specific cues (e.g., generic "I feel determined" regardless of context), the mechanism is failing.

### Mechanism 2
- Claim: ICLG rewards reasoning traces that causally support higher-likelihood generation of gold responses, promoting coherent narrative progression.
- Mechanism: For each trajectory (c, \hat{d}), ICLG computes the geometric mean of per-token likelihood gains: R_ICLG = (π_θ(d_gold | x ⊕ c) / π_θ(d_gold | x))^{1/|d_gold|}. High reward indicates that conditioning on the model's own reasoning increases confidence in the correct response.
- Core assumption: Reasoning traces that effectively support accurate and coherent generation should be reinforced; likelihood gain is a proxy for causal coherence (Section 3.4.2, inspired by LATRO).
- Evidence anchors:
  - [Section 5.4 / Table 3] Pure ICLG (λ=1.0) achieves best Storyline Consistency (59.10) and Storyline Quality (72.42) on LLaMA3.1-8B-Instruct, indicating its importance for causality and narrative structure.
  - [Section 5.1] "RL models consistently outperform SFT-only models in both Storyline Consistency and Character Fidelity, with average improvements of 2.85 and 3.50 points, respectively. This indicates that the ICLG reward effectively guides the model to produce reasoning traces that advance the narrative in a causal, coherent manner."
  - [corpus] Limited direct comparisons; ICLG is proposed as a general-purpose reward for open-domain generation but has not been externally validated beyond this paper's benchmarks.
- Break condition: If ICLG-driven RL causes reasoning to become repetitive surface-form mimicry of d_gold (reducing diversity), balance with LSA or adjust λ weights.

### Mechanism 3
- Claim: LSA rewards semantic fidelity while permitting lexical diversity, maintaining character alignment without overfitting to reference wording.
- Mechanism: R_LSA = cos(f_ref(x, d_gold), f_ref(x, \hat{d})), where f_ref is mean-pooled last hidden states from a frozen reference policy (the SFT model). This uses the model's own semantic space adapted for role-play.
- Core assumption: Semantic closeness in a role-play-adapted embedding space is a better fidelity signal than token-level overlap, especially when diverse expressions are valid (Section 3.4.2).
- Evidence anchors:
  - [Section 5.4 / Table 3] Pure LSA (λ=0, λ_ICLG=0, λ_LSA=1.0) maximizes Anthropomorphism (47.63) but reduces plot coherence; hybrid λ_ICLG=0.7/λ_LSA=0.3 yields best overall average, suggesting LSA enables persona-centric language while ICLG maintains narrative structure.
  - [Section 3.4.2] "LSA is more flexible than SFT: it rewards outputs semantically close to the reference, regardless of wording, enabling the model in RL to remain faithful while allowing more natural, diverse expressions."
  - [corpus] No direct external validation of LSA for role-playing; cosine similarity on mean-pooled representations is a common technique (cited: Tao et al., 2024a), but application to RL rewards in role-play is novel here.
- Break condition: If semantic drift occurs (high LSA but responses diverge in persona or tone), check reference model quality and embedding space alignment.

## Foundational Learning

- Concept: **Grouped Reward Policy Optimization (GRPO)**
  - Why needed here: CogDual uses GRPO for RL optimization with implicit rewards. You must understand importance sampling ratios, KL penalties, and grouped advantage estimation to debug training dynamics (Section 3.4.2, Equation 8-9).
  - Quick check question: Can you explain why GRPO uses a minibatch-normalized advantage A = (R - mean) / std, and what the β KL penalty controls?

- Concept: **Mean-Pooled Hidden States for Semantic Similarity**
  - Why needed here: LSA computes cosine similarity on mean-pooled last-layer representations. Understanding embedding space geometry is critical for interpreting reward signals and potential failure modes.
  - Quick check question: If two responses have high cosine similarity but different character tones, what might this indicate about the reference model's embedding space?

- Concept: **Chain-of-Thought Reasoning in LLMs**
  - Why needed here: CogDual's dual-cognition traces are a structured CoT variant. Prior work (Wei et al., 2023) shows CoT can improve reasoning but may cause stylistic drift in role-play (Feng et al., 2025b); CogDual aims to address this via cognitive templates.
  - Quick check question: How does CogDual's dual-cognition template differ from generic CoT, and why might this reduce stylistic drift?

## Architecture Onboarding

- Component map:
  1. **Dual Cognition Module**: Structured JSON generation (Situational Awareness → Self-Awareness) before response; see Section 3.3, Figure 1.
  2. **Stage 1 (SFT)**: Fine-tune on DSFT (17,762 examples) with dual-cognition trajectories; max length 10240, 2 epochs (Section 3.4.1, Table 4).
  3. **Stage 2 (RL with GRPO)**: Optimize with combined ICLG+LSA rewards (default λ=0.7/0.3), batch size 8, 16 samples per prompt, 10k prompts (Section 3.4.2, Table 5).
  4. **Reward Functions**: ICLG (likelihood gain per token) and LSA (cosine similarity on mean-pooled hidden states from frozen π_ref).
  5. **Evaluation**: GPT-4o-based scoring on Storyline Consistency, Anthropomorphism, Character Fidelity, Storyline Quality; cross-check with DeepSeek-v3, Gemini-2.0-Flash, and human evaluation (Section 4.3, Appendices B-C).

- Critical path:
  1. Construct high-quality dual-cognition trajectories (GPT-4o generation + stochastic first/third-person prompting + cognitive field checks + GPT-4o verification against character profile).
  2. SFT initialization to acquire basic cognitive behavior.
  3. RL with GRPO using implicit rewards to generalize beyond SFT distribution.
  4. Validate on CoSER test set (seen novels) + unseen books; check generalization on Cross-MR and LifeChoice.

- Design tradeoffs:
  - **ICLG vs LSA balance**: Pure ICLG maximizes narrative coherence; pure LSA maximizes anthropomorphism. Hybrid (0.7/0.3) recommended for overall performance (Table 3).
  - **SFT data scale**: Authors use only 17,762 SFT examples (<10% of full CoSER), achieving strong results, suggesting data efficiency from cognitive structuring (Section 5.1).
  - **Prompting vs training**: CB-CoT (prompting only) improves over vanilla but underperforms trained CogDual-RL (Table 1). Training is required for full gains.

- Failure signatures:
  - **Boilerplate cognition**: Traces become generic ("I feel determined") without scene-specific grounding. Check cognitive field verification pipeline.
  - **Reasoning drift**: CoT-style traces diverge from character voice (noted in Feng et al., 2025b). Ensure dual-cognition template tightly binds psychological dynamics to narrative context.
  - **Reward hacking**: Model maximizes ICLG by copying d_gold surface form, or maximizes LSA with semantically similar but off-persona responses. Monitor diversity metrics and human evaluation.
  - **Evaluator bias**: GPT-4o as sole judge may favor GPT-4o-generated traces. Cross-validate with DeepSeek-v3, Gemini-2.0-Flash, and human annotators (Appendices B-C).

- First 3 experiments:
  1. **Baseline replication on LLaMA3.1-8B-Instruct**: Train CogDual-SFT (Stage 1) and CogDual-RL (Stage 2) with default hyperparameters; evaluate on CoSER test set. Verify that average improvement over vanilla baseline approaches reported ~13% (Section 5.1).
  2. **Ablation on reward weights**: Sweep λ_ICLG ∈ {1.0, 0.7, 0.5, 0.3, 0.0} with corresponding λ_LSA; confirm hybrid (0.7/0.3) yields best average while pure ICLG maximizes Storyline Quality and pure LSA maximizes Anthropomorphism (Table 3).
  3. **Cross-benchmark generalization**: Evaluate trained CogDual-RL on Cross-MR (motivation inference) and LifeChoice (decision prediction) using GPT-4o semantic matching (Appendix G). Verify that accuracy approaches or exceeds o1-Preview baseline (Table 2).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating an explicit retrieval mechanism for character-specific memory improve performance over the current implicit extraction method used in the Self-Awareness module?
- Basis in paper: [explicit] The authors state in the Limitations section that they "rely on the model to extract previously mentioned memory fragments... without incorporating an explicit retrieval mechanism... This may result in the omission of relevant information."
- Why unresolved: The current architecture relies solely on the LLM's context window and parametric memory without external database access.
- What evidence would resolve it: A comparative study where CogDual is augmented with an external vector database (RAG) to fetch character memories, evaluated on long-context consistency metrics.

### Open Question 2
- Question: Does the CogDual reinforcement learning strategy with implicit rewards remain effective when scaled to models significantly larger than 8B parameters (e.g., 70B)?
- Basis in paper: [explicit] The authors note in the Limitations that "due to computational constraints, we have not evaluated the effectiveness of our reinforcement learning approach on larger-scale models such as Llama3.1-70B-Instruct."
- Why unresolved: RL training stability and reward optimization often behave differently as parameter counts scale up.
- What evidence would resolve it: Experimental results applying the CogDual-RL pipeline to a 70B+ parameter model and comparing performance deltas against the SFT baseline.

### Open Question 3
- Question: Can the dual cognition framework and implicit reward schemes transfer zero-shot to non-English role-playing scenarios?
- Basis in paper: [explicit] The Limitations section highlights that "current experiments are conducted solely on English datasets, and the model’s adaptability to non-English contexts... remains unexplored."
- Why unresolved: Cognitive reasoning structures (Self/Situational Awareness) may be culturally or linguistically dependent, and reward signals like LSA might not align across languages without fine-tuning.
- What evidence would resolve it: Evaluating CogDual on a multilingual role-playing benchmark (e.g., Chinese datasets) to assess cross-lingual consistency and fidelity.

### Open Question 4
- Question: Can a dynamic adjustment mechanism for the ICLG and LSA reward weights ($\lambda$) optimize performance better than the fixed weights used in the paper?
- Basis in paper: [inferred] Section 3.4.2 mentions the rewards are combined via "fixed weights," and Section 5.4 shows that different weights favor different metrics (e.g., $\lambda_{ICLG}=0$ helps Storyline Consistency, while $\lambda_{LSA}=1$ helps Anthropomorphism).
- Why unresolved: The paper manually selects a 0.7/0.3 split, but the optimal balance likely varies by scenario (e.g., action vs. emotional dialogue).
- What evidence would resolve it: An ablation study using an adaptive weighting scheme based on the dialogue context type compared against the static best-performing fixed weights.

## Limitations
- The dual-cognition mechanism relies heavily on GPT-4o-generated training data whose quality and consistency across diverse narrative contexts remains unverified.
- The implicit reward functions (ICLG and LSA) are novel and lack direct external validation beyond this paper's benchmarks.
- Limited scale of the SFT dataset (17,762 examples) and reliance on GPT-4o-based evaluation raise concerns about potential bias and overfitting to the evaluation methodology.

## Confidence
- **High confidence**: The SFT-only baseline improvements (2.85-3.50 points over vanilla) are robust and directly attributable to the dual-cognition structure. The architectural components and training pipeline are clearly specified.
- **Medium confidence**: The RL optimization results showing further improvements over SFT are plausible given the reward design, but the novel implicit rewards lack external validation. The balance between ICLG and LSA (λ=0.7/0.3) is empirically justified within this study but may not generalize to all role-playing scenarios.
- **Low confidence**: Generalization to truly unseen narrative domains beyond the tested Cross-MR and LifeChoice benchmarks remains unproven. The human evaluation sample size (40 examples) is too small to draw definitive conclusions about real-world applicability.

## Next Checks
1. **Cross-Evaluator Validation**: Re-run the complete evaluation pipeline using DeepSeek-v3 and Gemini-2.0-Flash as primary judges instead of GPT-4o to verify that reported improvements are not evaluator-specific.
2. **Long-Term Consistency Test**: Evaluate CogDual on extended narrative sequences (5+ turns) from the CoSER dataset to check for degradation in character consistency or emergence of repetitive cognitive patterns over time.
3. **Domain Transfer Experiment**: Test CogDual on role-playing scenarios from completely different domains (e.g., historical fiction, technical dialogue) not represented in the CoSER training data to assess true generalization capability.