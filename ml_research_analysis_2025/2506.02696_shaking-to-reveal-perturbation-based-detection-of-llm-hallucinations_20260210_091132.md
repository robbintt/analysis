---
ver: rpa2
title: 'Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations'
arxiv_id: '2506.02696'
source_url: https://arxiv.org/abs/2506.02696
tags:
- arxiv
- answer
- hallucination
- detection
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Sample-Specific Prompting (SSP), a novel perturbation-based
  framework for detecting hallucinations in LLM-generated answers. The key insight
  is that truthful and hallucinated responses exhibit distinct sensitivity patterns
  in intermediate model representations when perturbed with noise prompts.
---

# Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations

## Quick Facts
- arXiv ID: 2506.02696
- Source URL: https://arxiv.org/abs/2506.02696
- Reference count: 40
- Achieves up to 4.78% improvement in hallucination detection accuracy (AUROC) over prior methods

## Executive Summary
This paper introduces Sample-Specific Prompting (SSP), a novel perturbation-based framework for detecting hallucinations in LLM-generated answers. The core insight is that truthful and hallucinated responses exhibit distinct sensitivity patterns when their intermediate model representations are perturbed with noise prompts. By dynamically generating input-specific noise prompts and analyzing representational shifts, SSP can effectively distinguish between accurate and hallucinated content. The approach demonstrates strong generalization across datasets and maintains computational efficiency while achieving state-of-the-art detection performance.

## Method Summary
SSP works by injecting noise prompts into the original query and analyzing how the LLM's intermediate representations change. The framework dynamically generates noise prompts tailored to each input sample, then uses a lightweight encoder to amplify and compare the representational shifts between unperturbed and perturbed states. This contrastive analysis reveals that truthful responses maintain more stable representations under perturbation, while hallucinated content shows greater sensitivity to noise. The method operates without requiring external knowledge bases or additional fine-tuning, making it broadly applicable across different LLM architectures.

## Key Results
- Achieves up to 4.78% improvement in AUROC for hallucination detection compared to existing methods
- Demonstrates strong generalization across multiple datasets (QuAC, TriviaQA, CoQA) and different LLM architectures
- Maintains computational efficiency while providing accurate detection without requiring external knowledge bases

## Why This Works (Mechanism)
The framework exploits the fundamental difference in how LLMs process truthful versus hallucinated information. When truthful content is generated, the model's intermediate representations are grounded in learned patterns that remain relatively stable under perturbation. Hallucinated content, being less anchored in the model's learned knowledge, exhibits greater sensitivity to noise injection. By analyzing these differential perturbation responses, SSP can identify unreliable outputs without external verification.

## Foundational Learning

**Representational Stability**: The consistency of neural network activations when inputs are slightly modified. Why needed: Forms the basis for detecting unreliable responses through perturbation analysis. Quick check: Compare activation patterns between perturbed and unperturbed inputs.

**Contrastive Encoding**: A technique that amplifies differences between two states by encoding them jointly. Why needed: Enables effective comparison of representational shifts between truthful and hallucinated responses. Quick check: Verify the encoder successfully highlights meaningful differences.

**Noise Prompt Injection**: The process of adding controlled perturbations to input prompts. Why needed: Creates the controlled variation needed to test model sensitivity. Quick check: Ensure noise injection doesn't completely disrupt meaningful content.

## Architecture Onboarding

**Component Map**: Original Prompt -> Noise Generator -> LLM (Intermediate Representations) -> Contrastive Encoder -> Classification Output

**Critical Path**: Noise generation and injection → intermediate representation capture → contrastive encoding → classification decision

**Design Tradeoffs**: Balances perturbation strength (too weak misses differences, too strong loses signal) against computational efficiency (more analysis steps increase accuracy but add latency)

**Failure Signatures**: False positives occur when truthful content coincidentally shows high perturbation sensitivity; false negatives happen when hallucinated content appears stable due to specific prompting patterns

**3 First Experiments**:
1. Test sensitivity to different noise injection strengths on a validation set
2. Compare contrastive encoding versus simple difference metrics
3. Evaluate performance on out-of-domain prompts to assess generalization

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Assumes consistent perturbation sensitivity patterns across all domains, which may not hold for specialized or nuanced content
- Current evaluation focuses primarily on QA datasets, limiting confidence in real-world generalization
- Computational overhead from intermediate representation analysis may impact real-time applications for very large models

## Confidence
- Core methodology (perturbation analysis of intermediate representations): **High**
- Empirical results on QA benchmarks: **Medium**
- Computational efficiency claims: **Medium**
- Generalization to non-QA domains: **Low-Medium**

## Next Checks
1. Test SSP's performance on open-ended generation tasks and non-QA domains to assess generalization beyond structured question answering
2. Conduct ablation studies to quantify the contribution of the contrastive encoder versus simple perturbation analysis
3. Evaluate computational overhead and latency impact when scaling to models larger than LLaMA-7B, particularly for real-time applications