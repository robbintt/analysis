---
ver: rpa2
title: Code World Models for General Game Playing
arxiv_id: '2510.04542'
source_url: https://arxiv.org/abs/2510.04542
tags:
- player
- state
- game
- action
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Code World Models (CWMs) as an alternative
  to using large language models (LLMs) as direct game-playing policies. Instead of
  generating moves directly, the LLM synthesizes Python code representing a formal,
  executable world model from natural language game rules and example trajectories.
---

# Code World Models for General Game Playing

## Quick Facts
- **arXiv ID:** 2510.04542
- **Source URL:** https://arxiv.org/abs/2510.04542
- **Reference count:** 40
- **Primary result:** CWM-based agents outperform or match Gemini 2.5 Pro in 9 out of 10 games.

## Executive Summary
This paper introduces Code World Models (CWMs) as an alternative to using large language models (LLMs) as direct game-playing policies. Instead of generating moves directly, the LLM synthesizes Python code representing a formal, executable world model from natural language game rules and example trajectories. This CWM includes functions for state transitions, legal moves, observations, and termination, enabling high-performance planning algorithms like MCTS to avoid illegal moves and achieve deeper strategic play. The approach also synthesizes heuristic value functions and inference functions for imperfect information games. Evaluated on 10 games (5 perfect and 5 imperfect information, including 4 novel games), CWM-based agents outperform or match Gemini 2.5 Pro in 9 out of 10 games. CWMs achieve high transition accuracy (>97% in most cases) and demonstrate strong generalization, especially in games with simple rules.

## Method Summary
The method involves using an LLM (Gemini 2.5 Pro) to synthesize a Python Code World Model (CWM) from natural language game rules and a small number of demonstration trajectories (5 per game). The CWM includes functions for state transitions, legal moves, observations, and game termination, following the OpenSpiel API. This code is iteratively refined against unit tests derived from the trajectories using a tree-search or conversation-style approach. For imperfect information games, the LLM also synthesizes an inference function to sample plausible hidden histories. A tournament selects the best heuristic value function from multiple candidates. The final CWM and auxiliary functions are used by planning algorithms (MCTS or ISMCTS) during online play, with 1,000 simulations per move.

## Key Results
- CWM-based agents outperform or match Gemini 2.5 Pro in 9 out of 10 games tested.
- CWMs achieve high transition accuracy (>97% in most cases), with the worst case at 87% for complex games like Gin rummy.
- For imperfect information games, CWM agents show strong performance in simpler games like Leduc poker and Quadranto (inference accuracy >98%), but struggle with complex games like Gin rummy (inference accuracy as low as 9.5%).

## Why This Works (Mechanism)

### Mechanism 1: Verifiable World Model Synthesis
Offloading game rule formalization from the LLM's reasoning process to an explicit, executable code base improves move legality and strategic reliability. The LLM generates Python functions (transition, legal moves, termination) from rules + trajectories, which are iteratively refined against unit tests. This creates a deterministic, testable game engine for planners.

### Mechanism 2: LLM as Induction Engine for Inference in Imperfect Information
A synthesized inference function provides belief-state estimates for planning in partially observable games without requiring direct LLM reasoning at each step. The LLM writes a `resample_history` function to sample plausible hidden histories consistent with observations, enabling ISMCTS by sampling root states for simulations.

### Mechanism 3: Amortized Search via Learned Value Functions
A heuristically synthesized value function guides tree search more effectively than random rollouts, improving strategic depth per unit of compute. The LLM generates a `value_function(state, player_id)` designed to return estimated winning potential, with multiple candidates selected through a tournament on the synthesized CWM.

## Foundational Learning

**World Models in AI**
- *Why needed:* CWMs are the core artifact. Understanding that a world model is an agent's internal simulation of environment dynamics is crucial to grasp the paper's approach.
- *Quick check:* Can you explain how a world model differs from a direct policy? What role does it play in model-based reinforcement learning or planning?

**Monte Carlo Tree Search (MCTS)**
- *Why needed:* MCTS and its variant ISMCTS are the planning algorithms that leverage the CWM. Understanding the balance of exploration and exploitation in tree search is key.
- *Quick check:* What are the four main steps of MCTS (Selection, Expansion, Simulation, Backpropagation)? How does ISMCTS differ for imperfect information?

**Imperfect Information Games**
- *Why needed:* A major contribution is handling partially observed games. Concepts like information sets and belief states are central to the inference function mechanism.
- *Quick check:* In a game like Poker, what constitutes a player's information set? Why is reasoning about opponents' possible hidden states crucial?

## Architecture Onboarding

**Component Map:**
Data Generator -> CWM Synthesizer (LLM) -> Refinement Engine -> Auxiliary Function Synthesizer (LLM) -> Tournament Selector -> Planner (MCTS/ISMCTS)

**Critical Path:** The CWM Synthesis and Refinement loop is the most critical. A flawed CWM makes all downstream planning invalid. Start by validating this pipeline on simple, perfect-information games.

**Design Tradeoffs:**
1. **Open Deck vs. Closed Deck Synthesis:** Open deck uses hidden state info from trajectories, leading to better CWMs but is less realistic. Closed deck is harder but more generally applicable.
2. **Hidden History vs. Hidden State Inference:** History inference guarantees valid CWM states but is complex. State inference is simpler but may produce invalid states. History inference is recommended.
3. **Refinement Strategy:** Tree search (REx-style) is more robust but computationally heavier than sequential conversation.

**Failure Signatures:**
- **High Illegal Move Rate:** CWM `get_legal_actions` is incorrect. Check transition and legality logic.
- **High Forfeit Rate in IIGs:** Inference function `resample_history` fails to produce consistent histories. Inspect inference accuracy logs.
- **Poor Strategic Performance vs. Random:** Value function is misleading or CWM transition dynamics are wrong. Ablate value function first.
- **Stagnant Refinement Accuracy:** LLM is stuck in a local error pattern. Try resetting with a new synthesis prompt or switching refinement mode.

**First 3 Experiments:**
1. **Reproduce Tic-Tac-Toe Results:** Implement the synthesis and refinement pipeline for Tic-Tac-Toe. Target 100% transition accuracy and verify the agent beats a random player.
2. **Test Inference on Leduc Poker:** Run closed-deck synthesis for Leduc Poker. Measure inference accuracy on a held-out test set. Compare game play performance with and without the inference function.
3. **Ablate Value Function in Connect Four:** Synthesize a CWM and value function for Connect Four. Play matches against a version using random rollouts. Quantify the win rate difference.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can active, online learning of the Code World Model (CWM) improve an agent's ability to discover hidden causal mechanisms compared to the current static, offline approach?
- Basis in paper: The discussion section states that future work aims to "enable active and online learning of the world model."
- Why unresolved: The current method learns the model up-front from offline trajectories for efficiency and does not update the model during live gameplay.
- What evidence would resolve it: A CWM agent that dynamically refines its synthesized code during a game match to correct transition errors, improving performance in real-time.

### Open Question 2
- Question: How can CWM synthesis be adapted to master games with intricate, multi-step procedural subroutines (e.g., Gin Rummy) in the "closed deck" setting?
- Basis in paper: The authors identify this as a "key frontier" after observing that the logical complexity of Gin Rummy caused significant accuracy drops, especially when hidden states were unavailable during training.
- Why unresolved: The "closed deck" approach failed to capture the multi-stage scoring logic of complex games, resulting in inference accuracies as low as 9.5%.
- What evidence would resolve it: A modified synthesis prompt or architecture that successfully achieves high transition and inference accuracy on Gin Rummy using only closed-deck trajectories.

### Open Question 3
- Question: Can the CWM framework successfully generalize to open-world games with free-form text or visual interfaces?
- Basis in paper: The discussion section expresses a desire to "extend the technique to handle open-world games with free-form text and/or visual interfaces."
- Why unresolved: The current evaluation is restricted to games with text-based descriptions and discrete state spaces, relying on structured JSON observations.
- What evidence would resolve it: Successful implementation of CWM agents on novel game benchmarks that rely on visual inputs or unstructured natural language interactions.

## Limitations
- The synthesis process may struggle with games of higher complexity where rules are ambiguous or mechanics are procedurally intricate.
- The assumption that 5 demonstration trajectories are sufficient for accurate model synthesis is a key limitation, particularly for games with large state spaces or complex hidden information.
- The evaluation focuses on a small set of games, and generalizability to more complex, real-world games remains to be seen.

## Confidence
- **High Confidence:** The core mechanism of using a synthesized, executable CWM to avoid illegal moves and enable planning (e.g., the MCTS results on perfect-information games) is well-supported by the evidence.
- **Medium Confidence:** The synthesis and refinement process is well-described, but exact implementation details (e.g., the specific prompt format for tree search refinement, the Thompson sampling mechanism) are not fully specified.
- **Medium Confidence:** The synthesis of value functions is a novel contribution, but the evidence is limited to a few examples.

## Next Checks
1. **Stress-Test Refinement on Complex Rules:** Implement the CWM synthesis for a game with known procedural complexity. Measure the number of refinement iterations needed to achieve >95% transition accuracy and identify the failure modes when the process stalls.
2. **Benchmark Against Different LLMs:** Reproduce the Tic-Tac-Toe and Connect Four results using a different, publicly available LLM. Compare the CWM synthesis quality and the final agent win rate to assess the dependency on the specific model.
3. **Evaluate Long-Term Strategic Play:** For a game like Chess or Go (using a simplified rule set), implement a CWM and use it with a deeper search algorithm. Compare its performance against a direct LLM policy and an MCTS agent without a value function over a series of matches.