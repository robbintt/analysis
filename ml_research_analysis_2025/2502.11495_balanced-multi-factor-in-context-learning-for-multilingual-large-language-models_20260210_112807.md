---
ver: rpa2
title: Balanced Multi-Factor In-Context Learning for Multilingual Large Language Models
arxiv_id: '2502.11495'
source_url: https://arxiv.org/abs/2502.11495
tags:
- example
- question
- language
- answer
- choice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes BMF-ICL, a method for selecting multilingual
  examples in in-context learning (ICL) that considers three factors: semantic similarity,
  linguistic alignment, and language-specific performance. The method quantifies each
  factor using LaBSE for semantic similarity, lang2vec for linguistic alignment, and
  model likelihoods for language-specific performance, then optimally balances them
  via weighted summation.'
---

# Balanced Multi-Factor In-Context Learning for Multilingual Large Language Models

## Quick Facts
- arXiv ID: 2502.11495
- Source URL: https://arxiv.org/abs/2502.11495
- Reference count: 25
- Primary result: BMF-ICL achieves state-of-the-art accuracy on mCSQA and TYDI datasets by optimally balancing semantic similarity, linguistic alignment, and language-specific performance

## Executive Summary
This paper introduces BMF-ICL, a method for selecting multilingual examples in in-context learning that considers three key factors: semantic similarity, linguistic alignment, and language-specific performance. The approach quantifies each factor using LaBSE embeddings, lang2vec typological features, and model likelihoods respectively, then balances them via weighted summation optimized on development data. Experiments with four multilingual LLMs on mCSQA and TYDI datasets demonstrate consistent improvements over existing approaches, achieving state-of-the-art accuracy while selecting examples from multiple languages in over 95% of cases.

## Method Summary
BMF-ICL selects in-context learning examples by optimizing three factors: semantic similarity (via LaBSE embeddings and cosine similarity), linguistic alignment (via lang2vec typological features and cosine similarity), and language-specific performance (via per-language average log-likelihood). The method computes weighted scores for each candidate example and selects the top-k examples. Weights are optimized on development data through grid search. The approach operates without updating model parameters and can work even when no examples exist in the target language.

## Key Results
- BMF-ICL consistently outperforms existing multilingual ICL approaches on mCSQA and TYDI datasets
- The method achieves state-of-the-art accuracy across all four evaluated multilingual LLMs
- BMF-ICL selects examples from multiple languages in over 95% of cases, demonstrating effective multilingual diversity
- Ablation studies confirm that all three factors (semantic similarity, linguistic alignment, language-specific performance) are crucial for optimal performance

## Why This Works (Mechanism)

### Mechanism 1: Semantic Similarity via Language-Agnostic Embeddings
- Claim: Selecting examples semantically similar to the input improves multilingual ICL performance
- Mechanism: LaBSE embeddings encode sentence semantics independently of language through contrastive learning on parallel corpora. Cosine similarity between h(x) and h(s(j)) retrieves topically relevant examples regardless of language
- Core assumption: Semantic relevance transfers across languages even when surface forms differ
- Evidence anchors:
  - [abstract] "semantic similarity" listed as one of three key factors influencing multilingual ICL
  - [section 2.2, p.3] "LaBSE learns multilingual embeddings through contrastive learning over large-scale parallel data, enabling consistent semantic similarity computation across languages"
  - [corpus] Related work confirms example selection is key to ICL performance

### Mechanism 2: Linguistic Alignment via Typological Features
- Claim: Examples from typologically similar languages enable stronger cross-lingual transfer
- Mechanism: lang2vec encodes morphological, syntactic, and phylogenetic properties. Cosine similarity between e(l_x) and e(l_s(j)) identifies languages sharing grammatical structures, facilitating knowledge transfer through shared linguistic patterns
- Core assumption: MLLMs leverage structural similarities across languages during inference
- Evidence anchors:
  - [abstract] "linguistic alignment" identified as critical factor
  - [section 4.1, p.7] "within closely-related language groups such as en–de–nl or fr–pt, linguistic alignment plays a crucial role leading to pronounced performance declines compared to other languages when removed"
  - [corpus] Weak direct corpus evidence on lang2vec specifically

### Mechanism 3: Language-Specific Performance as Proxy for Model Confidence
- Claim: Prioritizing examples from languages where the model performs well leverages high-resource language knowledge
- Mechanism: Compute per-language average log-likelihood of correct answers. Higher scores indicate languages where the model has stronger internal representations, enabling more reliable demonstration signals
- Core assumption: High log-likelihood correlates with task performance and transferable knowledge
- Evidence anchors:
  - [abstract] "language-specific performance" identified as third key factor
  - [section 2.2, p.3] "per(l_t) = average log-likelihood of each reference text r' given its corresponding source text s'"
  - [section 4.1, p.7] Ablation shows performance drops when language-specific performance factor removed

### Mechanism 4: Weighted Optimization Balances Competing Signals
- Claim: Optimal balance requires task-specific weight tuning; no single factor dominates universally
- Mechanism: Grid search over α, β, γ ∈ [0,1] with α+β+γ=1 on development set. Different languages exhibit different optimal weight configurations
- Core assumption: Development set is representative of test distribution
- Evidence anchors:
  - [section 2.2, p.2-3] "we take their weighted sum and optimize the weights on development data"
  - [appendix C, p.15] Weight tables show substantial variation across languages
  - [appendix D, p.15] "optimizing the weights across all settings contributes to performance improvement" (uniform weights degrade performance by 2-7 points)

## Foundational Learning

- **Cosine Similarity in Embedding Space**
  - Why needed here: Core scoring function for both semantic and linguistic alignment
  - Quick check question: Given two embedding vectors [0.8, 0.6] and [0.6, 0.8], compute their cosine similarity. (Answer: 0.96)

- **In-Context Learning (ICL)**
  - Why needed here: The entire method optimizes ICL example selection without parameter updates
  - Quick check question: In ICL, how does the model incorporate examples? (Answer: Through conditioning in the prompt; no gradient updates occur.)

- **Cross-Lingual Transfer**
  - Why needed here: BMF-ICL's core hypothesis is that multilingual examples enable knowledge transfer across languages
  - Quick check question: If English examples improve Russian task performance without Russian examples, what phenomenon is occurring? (Answer: Cross-lingual transfer.)

- **Log-Likelihood Computation**
  - Why needed here: Language-specific performance score relies on computing model likelihood of correct outputs
  - Quick check question: Why use log-likelihood instead of raw probability? (Answer: Numerical stability; avoids underflow when multiplying many small probabilities.)

## Architecture Onboarding

- **Component map:**
```
Input x (target language l_x)
    │
    ├─► Language Detection (fasttext-langdetect)
    │       └─► l_x
    │
    ├─► Semantic Branch
    │       ├─► LaBSE embedding: h(x)
    │       └─► For each candidate s(j): score_sem(j) = cos(h(x), h(s(j)))
    │
    ├─► Linguistic Branch
    │       ├─► lang2vec embedding: e(l_x)
    │       └─► For each candidate s(j): score_lag(j) = cos(e(l_x), e(l_s(j)))
    │
    ├─► Performance Branch
    │       └─► Pre-computed per(l_t) for each language l_t
    │           └─► score_per(j) = per(l_s(j))
    │
    └─► Score Fusion
            └─► score(j) = α·score_sem(j) + β·score_lag(j) + γ·score_per(j)
            └─► Select top-k candidates by score
```

- **Critical path:**
  1. Pre-compute language-specific performance scores (per(l_t)) for all languages in candidate pool—this requires one forward pass per language subset
  2. At inference time, detect input language, compute embeddings, calculate all three scores
  3. Apply pre-tuned weights (α, β, γ) and select top-8 examples
  4. Construct prompt with selected examples

- **Design tradeoffs:**
  - **Pre-computation vs. runtime cost:** Language-specific performance scores can be cached offline; semantic and linguistic scores require per-query computation
  - **Weight optimization strategy:** Grid search (used in paper) is expensive; could use Bayesian optimization or learnable weights for large-scale deployment
  - **Example pool language composition:** Paper tests both "with target language" and "without target language" settings; performance gains larger in the latter
  - **Number of examples (k):** Paper finds k=8 optimal for their tasks; this is task-dependent and may require tuning

- **Failure signatures:**
  - **Uniform language selection:** If >90% of examples come from a single language, weights may be misconfigured or one factor dominates
  - **Performance worse than Random-ICL:** Check weight optimization on dev set; may have overfit or used wrong dev data
  - **No multilingual diversity:** Table 3 shows BMF-ICL should select 3-4 distinct languages in 8 examples; if not, semantic similarity may be dominating (check α value)

- **First 3 experiments:**
  1. **Reproduce single-language baseline:** Run Random-ICL and Non-ICL on mCSQA subset (e.g., English only) to establish baseline performance and verify MLLM access
  2. **Ablation with uniform weights:** Implement full BMF-ICL pipeline but set α=β=γ=1/3; compare against paper's reported optimized weights to quantify optimization benefit
  3. **Language diversity analysis:** On a held-out language pair (e.g., Japanese input), run BMF-ICL and log which languages are selected; verify multi-language selection occurs and correlates with performance gains over monolingual baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the effectiveness of BMF-ICL be generalized to natural language generation tasks beyond question answering?
- Basis in paper: [explicit] The authors state in the Limitations section that "it would be worthwhile to validate the method on a broader range of tasks beyond question answering."
- Why unresolved: The current study only evaluates the method on multiple-choice QA (mCSQA) and extractive QA (TYDI), leaving its impact on tasks like summarization or translation unknown
- What evidence would resolve it: Experimental results applying BMF-ICL to generation benchmarks (e.g., summarization or machine translation) showing comparable improvements over baselines

### Open Question 2
- Question: Is it possible to determine the optimal balance of factors (α, β, γ) in a zero-shot setting without relying on a labeled development set?
- Basis in paper: [inferred] The methodology relies on a grid search over a development set to optimize weights, which assumes the availability of labeled data for the target language/task
- Why unresolved: The requirement for a development set limits the method's applicability in true zero-shot scenarios where no labeled data exists for weight tuning
- What evidence would resolve it: The development of a heuristic or dynamic weighting mechanism that achieves comparable performance to the grid-search approach without accessing labeled validation data

### Open Question 3
- Question: How does the choice of specific proxy models (e.g., LaBSE, lang2vec) impact the robustness of the selection strategy?
- Basis in paper: [inferred] The method relies on fixed external models (LaBSE for semantics, lang2vec for linguistic features) which may inherit biases or limitations (e.g., poor embedding quality for low-resource languages)
- Why unresolved: The paper does not analyze whether the state-of-the-art performance is dependent on these specific embeddings or if the framework is robust to changes in the underlying proxy models
- What evidence would resolve it: An ablation study swapping LaBSE/lang2vec for alternative embeddings (e.g., mBERT, distinct syntactic databases) to measure sensitivity to the proxy choice

## Limitations
- **Evaluation Scope**: Only evaluated on QA tasks (mCSQA and TYDI), limiting generalizability to other multilingual tasks
- **Weight Optimization**: Requires grid search on development set, which may not scale well and assumes labeled data availability
- **Computational Overhead**: Adds inference-time latency through LaBSE and lang2vec computations, though pre-computation mitigates some cost

## Confidence
- **High Confidence**: 
  - Semantic similarity via LaBSE improves multilingual ICL performance
  - Language-specific performance as proxy for model confidence is effective
  - BMF-ICL achieves state-of-the-art accuracy on mCSQA and TYDI
- **Medium Confidence**: 
  - Linguistic alignment via lang2vec provides substantial benefits
  - Weighted optimization balances competing signals effectively
- **Low Confidence**: 
  - Claims about multi-language example selection being crucial
  - Generalizability to languages outside studied set

## Next Checks
1. **Cross-task validation**: Implement BMF-ICL for a non-QA multilingual task (e.g., summarization or translation) and measure performance gains compared to baselines
2. **Low-resource language stress test**: Apply BMF-ICL to languages with limited examples in the pool (e.g., reduce example count for a target language by 80%) and measure degradation patterns
3. **Dynamic weight adaptation**: Replace grid search with a simple heuristic for weight adaptation based on input characteristics (e.g., set α higher for distant language pairs, β higher for similar pairs) and compare performance to static optimized weights