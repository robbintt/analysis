---
ver: rpa2
title: Learning to Forget with Information Divergence Reweighted Objectives for Noisy
  Labels
arxiv_id: '2508.06622'
source_url: https://arxiv.org/abs/2508.06622
tags:
- noise
- antidote
- labels
- samples
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces ANTIDOTE, a novel method for training machine\
  \ learning models under label noise by employing a relaxation-optimization strategy\
  \ over information-divergence neighborhoods. The key idea is to allow the empirical\
  \ data distribution to be perturbed within a neighborhood defined by an f-divergence,\
  \ enabling the model to adaptively reduce the influence of samples with noisy labels\u2014\
  effectively \"forgetting\" them during training."
---

# Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels

## Quick Facts
- arXiv ID: 2508.06622
- Source URL: https://arxiv.org/abs/2508.06622
- Reference count: 40
- Primary result: ANTIDOTE achieves state-of-the-art performance on noisy label datasets by adaptively reweighting samples via f-divergence neighborhoods.

## Executive Summary
This paper introduces ANTIDOTE, a novel method for training machine learning models under label noise by employing a relaxation-optimization strategy over information-divergence neighborhoods. The key idea is to allow the empirical data distribution to be perturbed within a neighborhood defined by an f-divergence, enabling the model to adaptively reduce the influence of samples with noisy labels—effectively "forgetting" them during training. Using convex duality, the authors reformulate the problem into an adversarial training framework with a computationally efficient low-dimensional optimization. Theoretical results show that under appropriate assumptions, the true (non-noisy) labels are the unique solution to the relaxed problem.

## Method Summary
ANTIDOTE operates by introducing a perturbation variable within an f-divergence neighborhood around the empirical data distribution, creating a relaxation of the original learning problem. This relaxation is then reformulated via convex duality into an adversarial min-max optimization where the model minimizes risk while an adversary perturbs the distribution to maximize it. The solution involves solving a low-dimensional optimization problem at each step, making it computationally efficient. The method can be viewed as an adaptive sample reweighting scheme that downweights samples whose labels are likely to be corrupted.

## Key Results
- ANTIDOTE consistently outperforms state-of-the-art loss functions across symmetric, asymmetric, human-annotated, and real-world label noise scenarios
- Theoretical guarantees show that under appropriate assumptions, the true labels are the unique solution to the relaxed problem
- Maintains time complexity close to standard cross-entropy loss while achieving superior performance

## Why This Works (Mechanism)
The method works by creating a neighborhood around the empirical data distribution using f-divergences, then finding the worst-case distribution within this neighborhood that maximizes the expected loss. This adversarial approach naturally identifies and downweights samples with noisy labels because they contribute to higher losses when perturbed. The relaxation allows the model to effectively "forget" noisy samples by reducing their influence during training. The convex duality framework enables efficient computation by reducing the problem to a low-dimensional optimization.

## Foundational Learning
- f-divergences: Measures of difference between probability distributions that generalize KL-divergence and other metrics. Needed to define the neighborhood around empirical distribution.
- Convex duality: Mathematical framework that transforms a primal optimization problem into a dual form, often simplifying computation. Needed to make the adversarial formulation tractable.
- Information-theoretic learning: Framework that uses concepts from information theory to guide learning algorithms. Needed to formalize the "forgetting" mechanism.

## Architecture Onboarding
- Component map: Empirical distribution -> f-divergence neighborhood -> Relaxed optimization -> Adversarial min-max -> Model parameters
- Critical path: Data sampling → Neighborhood perturbation → Dual reformulation → Parameter update
- Design tradeoffs: The method balances computational efficiency with theoretical guarantees, trading some complexity for provable convergence properties.
- Failure signatures: Poor performance on datasets where true labels are not unique minimizers, or when noise patterns deviate significantly from assumed models.
- First experiments: 1) Run ANTIDOTE on CIFAR-10 with 50% symmetric noise to verify basic functionality. 2) Compare runtime with standard cross-entropy on WebVision to verify efficiency claims. 3) Test with different f-divergence choices (KL vs. Total Variation) to understand sensitivity.

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does ANTIDOTE perform when integrated with robust loss functions other than standard Cross-Entropy?
- Basis in paper: [explicit] The conclusion states that while the general theory applies to general loss functions, the experiments restricted attention to Cross-Entropy, and exploring combinations with other losses is an "interesting direction."
- Why unresolved: The paper's empirical validation (Tables 1–4) exclusively utilizes Cross-Entropy as the underlying loss function L_θ to compare against other loss-based methods.
- What evidence would resolve it: Empirical results benchmarking ANTIDOTE applied to Generalized Cross Entropy (GCE) or Mean Absolute Error (MAE) on noisy datasets like CIFAR-100N.

### Open Question 2
- Question: Can the neighborhood size hyperparameter δ be determined automatically without relying on manual tuning or prior knowledge of the noise rate?
- Basis in paper: [explicit] The authors identify the need to select δ (or penalty C) as a limitation, noting that while Eq. (15) provides a heuristic, the optimal value generally requires tuning.
- Why unresolved: The proposed adaptive variant (Eq. 58) introduces a penalty term C which effectively replaces the hyperparameter δ rather than eliminating the need for tuning entirely.
- What evidence would resolve it: A theoretical framework for setting δ based on local gradient statistics or a "tuning-free" algorithm that maintains performance parity with the tuned version.

### Open Question 3
- Question: Does the theoretical guarantee of recovering true labels hold when the deterministic label assumption is violated (i.e., inherent label ambiguity)?
- Basis in paper: [inferred] Theorem 3.1 proves the relaxed problem yields the true labels, but it relies on the assumption that a deterministic map h* exists for the ground truth.
- Why unresolved: Real-world "natural" noise often involves subjectivity and ambiguity, meaning a single deterministic ground truth label may not exist for every sample, potentially breaking the uniqueness proof.
- What evidence would resolve it: An extension of Theorem 3.1 to stochastic ground truth distributions, or experiments analyzing the method's calibration on datasets with high inter-annotator disagreement.

## Limitations
- Theoretical guarantees rely on strong assumptions about label noise structure that may not hold in all real-world scenarios
- Performance could degrade when true labels are not unique minimizers or noise patterns deviate from assumed models
- Experimental validation focuses primarily on image classification benchmarks, leaving other data modalities untested

## Confidence
- Theoretical framework: High
- Empirical performance claims: Medium
- Computational efficiency claims: Medium

## Next Checks
1. Test ANTIDOTE's performance on non-image datasets (e.g., text classification, tabular data) with varying noise patterns to assess generalizability.
2. Conduct ablation studies to quantify the individual contributions of the f-divergence choice and the perturbation radius to overall performance.
3. Evaluate scalability and runtime on larger-scale datasets (e.g., JFT-300M) to verify the claimed computational efficiency in extreme-scale settings.