---
ver: rpa2
title: Evaluating the Goal-Directedness of Large Language Models
arxiv_id: '2504.11844'
source_url: https://arxiv.org/abs/2504.11844
tags:
- message
- height
- blocks
- block
- tower
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models show limited goal-directedness when evaluated
  on tasks requiring information gathering, cognitive effort, and plan execution.
  The study introduces a novel framework to measure goal-directedness by comparing
  actual performance against predicted performance based on capability subtasks.
---

# Evaluating the Goal-Directedness of Large Language Models

## Quick Facts
- arXiv ID: 2504.11844
- Source URL: https://arxiv.org/abs/2504.11844
- Reference count: 40
- Primary result: Most models fail to fully utilize their capabilities in larger tasks, particularly in information gathering, even though they perform well on subtasks in isolation

## Executive Summary
This paper introduces a novel framework to measure goal-directedness in large language models by comparing actual performance against predicted performance based on capability subtasks. The study evaluates models across four composite tasks in a Blocksworld environment, finding that models consistently underutilize their capabilities, especially in information gathering phases. The research shows that goal-directedness is a distinct property from task performance or context length effects, and that motivational prompts only modestly improve performance. This work provides a method for monitoring and potentially training for goal-directedness in LLMs, with implications for autonomous agent design and safety.

## Method Summary
The study evaluates LLM goal-directedness using a Blocksworld environment with four composite tasks: Information Gathering, Cognitive Effort, Plan and Execute, and Combined. Models are first tested on isolated subtasks to establish capability baselines, then evaluated on composite tasks. Goal-directedness is measured as the ratio of actual performance gain over the optimal capability-conditioned gain, calculated using Monte Carlo simulations (10k iterations) that predict optimal returns based on subtask error distributions. The framework normalizes performance against both random baselines and capability upper bounds to isolate the "willingness" to use capabilities from the "ability" to perform them.

## Key Results
- Models take significantly fewer measurements in composite tasks compared to isolated subtasks, showing "resource laziness" in information gathering
- Goal-directedness remains relatively consistent across different task domains and is distinct from simple task performance
- Motivational prompts improve performance only modestly, indicating models are not fully responsive to task motivation
- Even state-of-the-art models fail to fully utilize their capabilities in larger tasks
- Goal-directedness is a fairly consistent property across tasks, with model rankings remaining roughly the same

## Why This Works (Mechanism)

### Mechanism 1
Isolating "goal-directedness" requires normalizing performance against a capability upper-bound. The framework measures the gap between a model's actual performance ($R^\pi$) and its predicted optimal performance ($R^{\pi^*_c}$) derived from isolated subtask capabilities. By measuring capability on subtasks (e.g., estimating height in isolation) and simulating how those capabilities would perform if fully utilized, the metric separates "can't do" (capability) from "won't do" (directedness). Core assumption: Capabilities measured in small, isolated subtasks are available and equivalent when the model is engaged in a larger, composite task. Evidence anchors: [abstract] "...use subtasks to infer each model's relevant capabilities... comparing actual performance against predicted performance based on capability subtasks." [Section 3.1] Defines Goal-Directedness (GD) as the ratio of actual performance gain over the optimal capability-conditioned gain.

### Mechanism 2
Models exhibit "resource laziness" specifically in information gathering phases of composite tasks. Even when models possess the capability to gather information (e.g., taking multiple noisy measurements to improve accuracy), they tend to under-invest effort when this is a substep of a larger goal. The mechanism suggests a failure in "sustained effort" or "long-horizon reward assignment" rather than a lack of measurement skill. Core assumption: The drop in performance is due to a lack of motivation/directedness rather than context length limitations or distraction. Evidence anchors: [Section 4.1] "Often, an unwillingness to take a sufficient number of measurements... Without exception, models take significantly fewer measurements in the latter case [composite task]." [Figure 4] explicitly plots the difference in measurements taken per block in isolated vs. composite settings.

### Mechanism 3
Goal-directedness is a relatively stable, intrinsic property across different task domains (Cognitive vs. Execution). The consistency of GD scores across varied tasks (e.g., NP-complete partition problems vs. physical plan execution) implies that directedness is not purely task-specific but relates to the model's internal "agentic" tuning or architecture. Core assumption: The Blocksworld environment is sufficiently representative of general agentic tasks to allow generalization. Evidence anchors: [Section 4.3] "Goal-directedness is a fairly consistent property across tasks... the ordering of the models remains roughly the same across all four composite tasks." [Section 4.2] Notes that GD is distinct from simple task performance or context length.

## Foundational Learning

- **Concept:** **Capability-Conditioned Return (R)**
  - **Why needed here:** You cannot judge if an agent is "trying hard" unless you know how well it *could* do. This metric defines the "Goldilocks" zone between random action ($R^{\pi_0}$) and perfect capability utilization ($R^{\pi^*_c}$).
  - **Quick check question:** If a model achieves a return of 15, but its capability-optimal return was 20 while random chance was 10, what is its Goal-Directedness score? *(Answer: 0.5)*

- **Concept:** **Monte Carlo Simulation for Policy Evaluation**
  - **Why needed here:** The paper uses Algo 1-4 to simulate the $R^{\pi^*_c}$. Since we can't analytically derive the "perfect use of imperfect capabilities" for every combination, we simulate the agent's subtask error rates (e.g., measurement error) to predict the composite score.
  - **Quick check question:** Why must we sample errors from the *subtask* results when simulating the *composite* task? *(Answer: To accurately model the upper bound of performance given the model's inherent, noisy limitations.)*

- **Concept:** **Partition Distance**
  - **Why needed here:** This is the error metric for the cognitive/planning tasks. It measures how many blocks must be moved to correct the configuration, quantifying the "plan quality."
  - **Quick check question:** Why is simple accuracy (Correct/Incorrect) insufficient for the "Cognitive Effort" task? *(Answer: It doesn't capture partial success or "closeness" to the optimal tower configuration.)*

## Architecture Onboarding

- **Component map:** Blocksworld environment -> LangChain wrapper -> Evaluator (subtask runners + Composite task runners) -> Analyzer (Monte Carlo engine computing $R^{\pi^*_c}$)
- **Critical path:**
  1. **Subtasking:** Run the model on isolated tasks (e.g., "Measure block 'a' 5 times") to build a probability distribution of errors.
  2. **Simulation:** Feed these error distributions into the Monte Carlo simulator to generate the predicted optimal score ($R^{\pi^*_c}$).
  3. **Composite Run:** Run the full task (e.g., "Build highest tower with unknown heights").
  4. **Normalization:** Calculate $GD = (Actual - Random) / (Simulated\_Optimal - Random)$.
- **Design tradeoffs:**
  - **Simplicity vs. Realism:** Blocksworld is highly abstract compared to real-world agent tasks (e.g., web navigation), risking low ecological validity.
  - **Isolation vs. Context:** Testing capabilities in isolation is cheap and clean, but may miss "synergy" errors where the model fails to combine skills it technically possesses separately.
- **Failure signatures:**
  - **Auto-regressive Loops:** Weaker models getting stuck repeating `<measure a>` infinitely (Section 3.4).
  - **Hallucinated Feedback:** "Thinking" models (o1-style) fabricating environment responses, breaking the evaluation state.
  - **Negative GD:** Models performing worse than random (Section 3.1), indicating "anti-goal-directedness" or severe distraction.
- **First 3 experiments:**
  1. **Reproduce the "Information Gap":** Verify that your target model takes significantly fewer measurements in the "Combined Task" vs. the "Height Estimation" subtask.
  2. **Motivation Ablation:** Run the "Combined Task" with the "Really go for it" prompt vs. baseline to measure sensitivity (Delta should be modest, per Section 4.4).
  3. **Context Length Check:** Run the "Subtask Stepping" protocol (Appendix E) to rule out context degradation as the cause for poor performance in the composite task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is goal-directedness a robust, predictive construct across diverse domains outside the Blocksworld environment?
- Basis in paper: [Explicit] The authors state in Section 5 that an "important next step would be to assess the goal-directed behaviour of LLM agents on other tasks" and that the metric should ideally be tested on "100s of tasks across 10s of domains."
- Why unresolved: The current evaluation is restricted to the Blocksworld environment, which may not capture the full complexity of real-world tasks like web navigation or coding.
- What evidence would resolve it: Demonstrating that the relative ranking of models' goal-directedness remains consistent when evaluated across a wide variety of unrelated environments and task types.

### Open Question 2
- Question: Can models be systematically fine-tuned to improve goal-directedness without impairing their underlying capabilities?
- Basis in paper: [Explicit] Section 5 notes, "Finally, it would be interesting to explore systematic fine-tuning for goal-directedness," distinguishing this from the inference-time motivational prompts tested in the study.
- Why unresolved: The study evaluates base or instruction-tuned models using prompting strategies, but does not investigate whether the "willingness" to use capabilities can be embedded into the model via training.
- What evidence would resolve it: An experiment showing that Reinforcement Learning or supervised fine-tuning can increase the Goal-Directedness (GD) score while maintaining or improving subtask performance.

### Open Question 3
- Question: How does a model's goal-directedness towards an explicit user goal trade off against intrinsic or safety-related fine-tuning goals (e.g., conciseness or harmlessness)?
- Basis in paper: [Inferred] Section 5 discusses the limitation that models may be "fine-tuned to limit the lengths of their outputs... directly at odds with completing a block stacking task with high precision."
- Why unresolved: The paper measures directedness toward the prompt goal but acknowledges that models possess other internal drives (like helpfulness or brevity) that may suppress the observed goal-directedness.
- What evidence would resolve it: A study correlating the failure modes in information gathering (e.g., taking too few measurements) with specific constraints in the model's fine-tuning data or constitution.

## Limitations

- Ecological validity: Blocksworld's highly abstracted tasks may not generalize to real-world agent behavior
- Assumption of capability transfer: The framework assumes subtask capabilities transfer directly to composite tasks, which may not hold for emergent capabilities
- Computational scaling: Large-scale Monte Carlo simulations (10k iterations per task) may not scale well to more complex environments
- Language bias: Study focuses on English-language models, potentially missing cross-lingual differences in goal-directedness

## Confidence

**High Confidence:** The framework's mathematical formulation (Goal-Directedness metric normalization) is sound and well-justified. The observation that models take fewer measurements in composite vs. isolated tasks is robust across multiple model families.

**Medium Confidence:** The claim that goal-directedness is a stable, intrinsic property across task domains. While consistency is observed within Blocksworld, generalization to more complex domains remains uncertain.

**Low Confidence:** The assertion that motivational prompts have only modest effects. The study uses a single "Really go for it" prompt; different prompt formulations or optimization techniques might yield stronger effects.

## Next Checks

1. **Cross-Domain Validation:** Test the same models on non-Blocksworld tasks (e.g., web navigation, coding tasks) to verify if GD rankings remain consistent across substantially different environments.

2. **Emergent Capability Test:** Design experiments where models must combine subtask capabilities in novel ways not seen during subtask evaluation to test whether the framework can detect genuine capability emergence vs. directedness failures.

3. **Prompt Optimization Study:** Systematically test multiple prompt formulations, temperature settings, and fine-tuning approaches to determine if motivational effects on goal-directedness can be substantially improved beyond the modest effects reported.