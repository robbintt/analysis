---
ver: rpa2
title: 'OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability'
arxiv_id: '2511.20766'
source_url: https://arxiv.org/abs/2511.20766
tags:
- agent
- variations
- agents
- default
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenApps is a lightweight open-source ecosystem for measuring UI-agent
  reliability across app variations, addressing the gap where current evaluations
  rely on fixed app clones. It enables generating thousands of configurable versions
  of six apps (calendar, messenger, maps, etc.) with varying appearance and content,
  requiring only a single CPU.
---

# OpenApps: Simulating Environment Variations to Measure UI-Agent Reliability

## Quick Facts
- arXiv ID: 2511.20766
- Source URL: https://arxiv.org/abs/2511.20766
- Reference count: 40
- Key outcome: OpenApps enables measuring UI-agent reliability across app variations, revealing that task success rates fluctuate by over 50% depending on app configuration, with agents like Kimi-VL-3B ranging from 63% to 4% success across variants.

## Executive Summary
OpenApps is a lightweight open-source ecosystem for measuring UI-agent reliability across app variations, addressing the gap where current evaluations rely on fixed app clones. It enables generating thousands of configurable versions of six apps (calendar, messenger, maps, etc.) with varying appearance and content, requiring only a single CPU. Across 10,000+ trials on seven leading multimodal agents, task success rates fluctuated by over 50% across app versions—for example, Kimi-VL-3B's success ranged from 63% to 4%. Agent behaviors such as looping and hallucinating actions also varied drastically depending on app configuration. These findings highlight the importance of evaluating reliability across app variations and demonstrate that fixed-app environments underestimate real-world performance fluctuations. OpenApps supports scalable, reproducible experiments and is available at https://facebookresearch.github.io/OpenApps/.

## Method Summary
OpenApps evaluates UI-agent reliability by creating configurable app variations through FastHTML-based Python applications. The ecosystem uses BrowserGym API to interface with agents, providing screenshots and accessibility trees as observations and accepting click, type, and scroll actions. Tasks are defined via YAML files with complete state serialization, and rewards are computed via exact state matching (r = δ[s_t = s_target]). The evaluation spans six apps (calendar, messenger, maps, to-do, code editor, shop) with eight variation types (appearance: default, dark theme, black/white, challenging font; content: default, German, long descriptions, misleading descriptions, adversarial descriptions). Over 10,000 trials across 15 tasks, 8 variations, and 3 seeds per combination measure task success rates and reliability via standard deviation and MAD of rewards.

## Key Results
- Task success rates fluctuated by more than 50% across app variations for many agents
- Kimi-VL-3B performance ranged from 63% to 4% task success depending on the app variant
- Agents exhibited drastically different failure modes across variations, with action loops and hallucinations increasing significantly under certain configurations

## Why This Works (Mechanism)

### Mechanism 1: Environment Variation Exposure Reveals Hidden Failure Modes
Introducing controlled variations in app appearance and content surfaces agent failure modes that remain invisible in fixed-environment benchmarks. By systematically varying visual factors (theme, fonts, contrast) and content factors (language, descriptions, adversarial text), OpenApps creates distribution shifts that stress-test agent perception and reasoning. Agents that appear reliable on a single app version may fail catastrophically on variants—Kimi-VL dropped from 63% to 4% success across versions.

### Mechanism 2: State-Grounded Reward Prevents Reward Hacking
Evaluating task success via complete app state comparison (rather than trajectory imitation or surface-level checks) provides an unhackable signal. OpenApps serializes full environment state into YAML, computing rewards via deterministic indicator function r = δ[s_t = s_target]. This prevents agents from "gaming" evaluation by completing side tasks or matching demonstrations without achieving the actual goal state.

### Mechanism 3: Lightweight Parallelization Enables Statistical Reliability Measurement
Single-CPU deployment with <10MB memory per instance allows running 10,000+ trials, making statistical fluctuation analysis practical. Unlike VM-based benchmarks requiring 100GB+ memory per site, OpenApps apps run as lightweight Python processes via FastHTML. This enables measuring not just average success, but variance (std, MAD) across seeds and variations—the true reliability metric.

## Foundational Learning

- **Multimodal UI Agents**: Understanding perception-action loops is prerequisite since OpenApps evaluates agents that perceive screenshots (vision) and optionally accessibility trees (text), then output human-like actions. Quick check: Can you explain why UI-TARS (vision-only) performs differently from GPT-4o (vision + AX tree) on dark theme variations?

- **Reinforcement Learning Terminology (O, A, S, R)**: The paper frames agent-environment interaction in RL terms: observations O, actions A, state S, reward R. Section 3.3's reward function and Figure 3's loop require this mental model. Quick check: If an agent completes a task but also deletes unrelated files, would the state-grounded reward r = δ[s_t = s_target] catch this failure?

- **Domain Randomization / Distribution Shift**: The core insight is that fixed training/testing environments underestimate real-world variance. This connects to robotics sim-to-real transfer and the broader principle that robustness requires exposure to variation. Quick check: Why might an agent trained only on light-themed apps fail on dark themes even if the task logic is identical?

## Architecture Onboarding

- **Component map**: YAML Config → FastHTML App (Python) → Browser Rendering → State Serialization → Reward Function ← Agent (via BrowserGym API) → Action Execution

- **Critical path**:
  1. Define task + target state in YAML (e.g., calendar must contain specific event)
  2. Initialize app with chosen variation (theme, language, content)
  3. Agent receives screenshot (and optionally AX tree) via BrowserGym
  4. Agent outputs action (click, type, scroll, etc.)
  5. App state updates; reward function checks s_t == s_target
  6. Repeat until success, timeout, or agent termination

- **Design tradeoffs**:
  - **Realism vs. Control**: OpenApps apps are simpler than full website clones, but offer precise state access and variation control. Trade-off favors reproducibility and statistical power.
  - **Task Simplicity vs. Failure Visibility**: Paper uses simple tasks (add todo item) to isolate variation effects; complex tasks might obscure which variation caused failure.
  - **Binary vs. Partial Rewards**: Current design uses binary success; multi-step tasks could use partial credit but complicates interpretation.

- **Failure signatures**:
  - **Action loops**: Agent repeats same action sequence (e.g., click(17)×20); associated with 7.5× higher failure rate
  - **Invalid actions**: Hallucinated functions (e.g., `remove_item`, `finished()`) or malformed arguments; increases with adversarial content
  - **Intent misunderstanding**: Agent navigates to wrong app; rate jumps from 3% (default) to 45% (adversarial descriptions) for Qwen2.5-VL

- **First 3 experiments**:
  1. **Reproduce Kimi-VL variation sensitivity**: Run Kimi-VL on default vs. German vs. adversarial description variants across all 15 tasks. Confirm 63%→4% drop. Check whether failures correlate with specific content types or UI elements.
  2. **Isolate appearance vs. content effects**: For UI-TARS (vision-only), test dark theme vs. default while holding content constant. For Kimi-VL (multimodal), test adversarial descriptions vs. default while holding appearance constant. Compare failure mode distributions.
  3. **Stress-test your own agent**: If you have a UI agent, evaluate it across all 8 variations on 3 tasks per app. Compute std(rewards) within fixed app vs. across variations. If across-variation std > 2× within-variation std, your agent has reliability gaps not visible in fixed benchmarks.

## Open Questions the Paper Calls Out

- **How do simultaneous interactions between multiple appearance and content variation factors affect agent reliability compared to individual variations?**
  The authors state, "we focus on varying each app appearance or content factor independently. Of course interactions between multiple app variation factors can also expose interesting behaviors that we leave to future work." The current methodology isolates variables rather than testing compound configurations.

- **What specific training data distributions are necessary for agents to generalize robustly to UI factors like fonts, colors, and layouts?**
  In the Future Directions, the authors ask, "probing questions such as the training data distribution necessary for agents to generalize to particular factors such as fonts, colors, or layouts." The paper evaluates pre-trained agents but does not define the data composition required to solve the observed brittleness to visual variations.

- **Do reliability fluctuations persist or compound when agents perform complex, long-horizon tasks compared to the simple tasks tested?**
  The authors note, "Future work can extend the set of tasks to include more complex or longer-horizon tasks to form a benchmark for UI-agent reliability." The current study focuses on simple, few-step tasks; it is unclear if compounding errors in multi-step workflows exacerbate the 50%+ variance observed.

## Limitations
- Simplified apps compared to production software may not fully capture real-world complexity
- Binary reward system assumes unambiguous goal states, which may not hold for all UI interactions
- Current evaluation focuses on simple tasks, leaving unclear whether failure modes compound in complex, long-horizon scenarios

## Confidence
- **High confidence**: Environment variation causes substantial performance fluctuations (>50%) across agents, and single-app evaluations underestimate real-world reliability gaps
- **Medium confidence**: The specific failure modes (looping, hallucinations) generalize to production apps; simplified apps capture essential variation effects
- **Medium confidence**: Statistical variance on simple tasks predicts reliability on complex real-world tasks

## Next Checks
1. Test leading agents on 2-3 real mobile/web apps with controlled appearance variations (themes, fonts) to verify whether the 50%+ performance drops replicate outside OpenApps
2. Measure whether agents that fail more on OpenApps variations also produce lower user satisfaction in actual deployment scenarios
3. Design tasks with multiple valid end states or partial credit requirements to evaluate whether state-grounded binary rewards remain appropriate