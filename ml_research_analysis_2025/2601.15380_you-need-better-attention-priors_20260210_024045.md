---
ver: rpa2
title: You Need Better Attention Priors
arxiv_id: '2601.15380'
source_url: https://arxiv.org/abs/2601.15380
tags:
- attention
- prior
- priors
- standard
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GOAT is a new attention mechanism that generalizes standard dot-product
  attention by viewing it through the lens of Entropic Optimal Transport, revealing
  that standard attention corresponds to a transport problem regularized by an implicit
  uniform prior. We introduce a learnable, continuous prior that replaces this naive
  assumption while maintaining full compatibility with optimized kernels such as FlashAttention.
---

# You Need Better Attention Priors

## Quick Facts
- arXiv ID: 2601.15380
- Source URL: https://arxiv.org/abs/2601.15380
- Authors: Elon Litman; Gabe Guo
- Reference count: 35
- Key outcome: GOAT is a new attention mechanism that generalizes standard dot-product attention by viewing it through the lens of Entropic Optimal Transport, revealing that standard attention corresponds to a transport problem regularized by an implicit uniform prior. We introduce a learnable, continuous prior that replaces this naive assumption while maintaining full compatibility with optimized kernels such as FlashAttention. GOAT provides an EOT-based explanation of attention sinks and materializes a solution for them, avoiding the representational trade-offs of standard attention. By absorbing spatial information into the core attention computation, GOAT learns an extrapolatable prior that combines the flexibility of learned positional embeddings with the length generalization of fixed encodings. On C4 language modeling, GOAT improves in-distribution perplexity by 1.55 points over ALiBi while maintaining robust extrapolation to 16× training length. On long-context retrieval tasks, GOAT maintains near-perfect accuracy at context lengths far exceeding training, whereas other methods degrade catastrophically. For biological sequence modeling, GOAT achieves lower validation NLL and reduces peak CUDA memory allocation by 36% compared to RoPE. On ImageNet-1k, GOAT enables robust zero-shot resolution extrapolation, maintaining substantially higher accuracy as input resolution increases beyond training.

## Executive Summary
GOAT (Generalized Optimal transport Attention with a learned prior) reframes standard attention as a special case of Entropic Optimal Transport regularized by a uniform prior. By replacing this implicit prior with a learnable, continuous one while maintaining compatibility with FlashAttention, GOAT addresses fundamental limitations in standard attention including poor length extrapolation and the emergence of "attention sinks." The method parameterizes spatial relationships through a spectral decomposition of relative positions and handles low-signal queries via a dedicated sink bias, achieving state-of-the-art performance across language modeling, vision, and biological sequence tasks while using less memory.

## Method Summary
GOAT implements a learnable prior in the attention mechanism by parameterizing it as a finite trigonometric polynomial through Fourier spectral decomposition. The prior is integrated into the attention computation via composite query and key vectors that separate content and positional information, with specific scaling applied to ensure proper influence. The method maintains FlashAttention compatibility by expressing the prior as dot-products, uses a small MLP to generate sink biases for handling low-signal queries, and splits the head dimension to accommodate both content and prior representations. Training involves standard optimization with careful attention to initialization and scaling parameters to prevent prior overpowering or content starvation.

## Key Results
- C4 language modeling: 1.55 perplexity improvement over ALiBi with robust extrapolation to 16× training length
- Long-context retrieval: Maintains near-perfect accuracy at context lengths far exceeding training, while baselines catastrophically degrade
- Biological sequence modeling: Lower validation NLL and 36% peak CUDA memory reduction compared to RoPE
- ImageNet-1k: Robust zero-shot resolution extrapolation, maintaining substantially higher accuracy as input resolution increases beyond training

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Standard scaled dot-product attention is a special case of Entropic Optimal Transport (EOT) implicitly regularized by a uniform prior.
- **Mechanism:** The paper demonstrates that minimizing transport cost plus Shannon entropy $H(p)$ is mathematically equivalent to minimizing the KL-divergence against a uniform distribution $U$. By generalizing the regularizer to $KL(p||\pi)$ for an arbitrary prior $\pi$, the optimal attention distribution shifts from $\text{softmax}(s/\tau)$ to $\text{softmax}(s/\tau + \log \pi)$.
- **Core assumption:** The variational EOT objective accurately reflects the inductive bias required for effective attention, implying that the "naive" uniform prior is a structural limitation of current Transformers.
- **Evidence anchors:**
  - [abstract] "standard attention corresponds to a transport problem regularized by an implicit uniform prior."
  - [Section 3, Prop 3.1] Derivation showing $p^* = \text{softmax}(s/\tau + \log \pi)$.
  - [corpus] Litman (2025) "Scaled-Dot-Product Attention as One-Sided Entropic Optimal Transport" establishes the precursor EOT derivation.
- **Break condition:** If the optimal transport analogy fails to capture the semantic needs of the task (e.g., where uniform attention is actually preferred), or if the gradient signal from the $\log \pi$ term destabilizes training by overpowering content scores.

### Mechanism 2
- **Claim:** Translation-invariant relative position priors can be parameterized as finite trigonometric polynomials and realized as dot-products without materializing bias matrices.
- **Mechanism:** The log-prior $K_{ij}$ is expressed as a Fourier series. Using angle-difference identities, this is factorized into learnable query vectors (rotated spectral coefficients) and fixed key vectors (Fourier features).
- **Core assumption:** Bounded, shift-invariant priors (trigonometric polynomials) are sufficient to model necessary spatial relationships, and higher-frequency or unbounded priors are not required for generalization.
- **Evidence anchors:**
  - [Section 4] "Spectral Decomposition of Relative Position... parameterize this log-prior using a truncated Fourier series."
  - [Appendix F, Theorem 2] Proves that SDPA-compatibility and translation equivariance force the prior to be a finite trigonometric polynomial.
  - [corpus] Weak direct evidence in provided corpus for the specific Fourier parameterization, though related works use optimal transport for attention structure.
- **Break condition:** If the "fixed frequencies" $\omega_r$ do not align with the true periodicity of the data (e.g., specific long-range dependencies), the truncated series approximation may fail, or the spectral weights ($\alpha, \beta$) may overfit to training lengths.

### Mechanism 3
- **Claim:** Attention sinks are a necessary emergent property of standard attention used to handle low-signal queries, which GOAT explicitly models via a learnable key-only logit bias.
- **Mechanism:** In low-signal regimes (flat content scores), the EOT objective forces the posterior to collapse to the prior. Standard attention lacks an expressive prior, forcing the model to inflate specific content keys (sinks). GOAT adds a dedicated scalar bias $u(j)$ to the log-prior, allowing the model to route mass to a "sink" token without corrupting the semantic content embeddings.
- **Core assumption:** Queries frequently exist in "low-signal" states where content information is ambiguous, necessitating a robust structural default.
- **Evidence anchors:**
  - [Section 5, Theorem 5.1] "Collapse to Prior": proves that as content signal decreases, $p_{ij} \to \pi_{ij}$.
  - [Section 5] "Formalizing Sinks via Margins": shows explicit bias $u(j)$ creates a margin $\delta$ independent of content norms.
  - [corpus] Not directly addressed in provided corpus summaries.
- **Break condition:** If the sink bias $u(j)$ is initialized or learned to be too large, it may starve content-based attention, causing the model to ignore semantic information entirely.

## Foundational Learning

- **Concept: Kullback-Leibler (KL) Divergence**
  - **Why needed here:** The paper reframes attention regularization from simple entropy maximization (dispersion) to KL-divergence minimization against a prior. Understanding this difference is key to grasping how GOAT changes the "default" behavior of attention.
  - **Quick check question:** Does minimizing $KL(p||\pi)$ force the distribution $p$ to cover the support of $\pi$, or to concentrate where $\pi$ is high? (Answer: It forces $p$ to concentrate where $\pi$ is high, effectively biasing the attention).

- **Concept: Bilinear Forms & Kernel Methods**
  - **Why needed here:** GOAT implements the prior $K_{ij}$ as a dot product $\langle \phi(i), \psi(j) \rangle$ to stay compatible with FlashAttention. This relies on the "kernel trick" concept—representing a function of two variables as an inner product of feature maps.
  - **Quick check question:** How can we represent an additive bias $K_{ij} = u(j)$ as a dot product $\langle q_i, k_j \rangle$? (Answer: Set $q_i = [1, 0, ...]$ and $k_j = [u(j), ...]$).

- **Concept: Softmax Temperature**
  - **Why needed here:** The paper explicitly manipulates temperature to separate "content" scores from "prior" scores. They apply $\sqrt{d_c}$ scaling to content but effectively use temperature 1 for the prior to prevent attenuation.
  - **Quick check question:** In the GOAT formulation $s_{ij}/\sqrt{d_c} + K_{ij}$, what happens to the influence of the prior $K$ if the content head dimension $d_c$ is very large? (Answer: The prior term remains constant while content scores shrink, relatively increasing the prior's influence).

## Architecture Onboarding

- **Component map:** Input queries, keys, values, and position indices → Split head dimension into content $d_c$ and prior $d_p$ subspaces → Compute Fourier features for positions → Apply spectral rotation to get $q_{rel}$ → Compute sink bias $u(j)$ via MLP → Apply specific scaling factors to content (√$d_h/d_c$) and prior (√$d_h$) vectors → Construct composite query/key vectors → Feed to FlashAttention kernel

- **Critical path:**
  1. Split the head dimension $d_h$ into content $d_c$ and prior $d_p$ (e.g., $d_p = 12$).
  2. Calculate Fourier features for positions.
  3. Apply spectral rotation to get $q_{rel}$.
  4. **Crucial:** Apply specific scaling factors (Eq. 20/21) to ensure the prior enters the softmax *unscaled* while content is *scaled*.
  5. Call standard FlashAttention kernel on composite vectors.

- **Design tradeoffs:**
  - **Head Dimension Stealing:** You must reduce content dimension $d_c < d_h$ to make room for the prior $d_p$. This reduces the capacity for semantic rich representation per head (e.g., reducing from 64 to 52 dims).
  - **Expressivity vs. Extrapolation:** The spectral parameterization enforces boundedness and shift-invariance, aiding extrapolation but potentially limiting the modeling of complex absolute positional logic (though the absolute sink term $u(j)$ mitigates this).

- **Failure signatures:**
  - **Training Instability:** If the scaling trick is omitted, the prior term $K_{ij}$ might be drowned out by the content term in large heads, or explode in small heads.
  - **Prior Overpowering:** If learned spectral weights $\alpha, \beta$ grow too large, the attention map becomes a fixed positional mask, ignoring token semantics entirely.
  - **Memory Regression:** If $d_p$ is set too high, you lose the efficiency gains or degrade content capacity too severely.

- **First 3 experiments:**
  1. **Ablation on $d_p$:** Run the copy-mixture task varying the prior dimension (e.g., 4, 8, 16) to find the "compression" point where the Fourier series can accurately represent the target relative distances without stealing too much capacity from $d_c$.
  2. **Sink Initialization Check:** Verify the "Collapse to Prior" (Theorem 5.1). Mask all content (set $s_{ij}=0$) and check if the output distribution matches the initialized $\pi$ exactly.
  3. **Resolution Extrapolation (Vision):** Train a small ViT on 224px and immediately evaluate on 384px. Compare against a baseline with absolute PEs. If GOAT fails here, the spectral weights have likely overfit to the training frequency domain.

## Open Questions the Paper Calls Out
None

## Limitations
- The Fourier spectral parameterization constrains expressivity, potentially limiting modeling of complex absolute positional patterns or long-range dependencies that don't align with fixed frequency basis
- Head dimension "stealing" directly trades semantic capacity for structural generalization, reducing content representation per head
- The effectiveness of the sink bias depends heavily on proper initialization and learning dynamics, with risk of either being ignored or dominating content-based attention

## Confidence
- **High Confidence**: The EOT-based explanation of attention sinks and the mathematical derivation showing standard attention as uniform-prior EOT are robust. The spectral decomposition theorem (Appendix F) proving that translation-equivariant SDPA-compatible priors must be finite trigonometric polynomials is also highly reliable.
- **Medium Confidence**: The empirical results showing perplexity and extrapolation improvements on C4 are convincing, but the exact contribution of each mechanism (prior learning vs. sink modeling vs. scaling) is difficult to disentangle without extensive ablations. The ImageNet resolution extrapolation results are intriguing but require more rigorous testing across diverse vision tasks.
- **Low Confidence**: The initialization scheme for the spectral weights ($\alpha_r, \beta_r$) and the sink MLP is not fully specified, creating uncertainty about reproducibility. The claim about 36% memory reduction for biological sequence modeling needs verification across different sequence lengths and hardware configurations.

## Next Checks
1. **Ablation Study on Prior Dimension and Frequency Schedule**: Systematically vary $d_p$ (e.g., 4, 8, 12, 16) and the geometric frequency base (e.g., doubling vs. tripling intervals) on the copy-mixture task. Measure both in-distribution accuracy and extrapolation performance to identify the optimal compression point where the Fourier series accurately represents target relative distances without excessive capacity loss.

2. **Sink Mechanism Isolation Test**: Design a controlled experiment where content scores are explicitly masked to zero for a subset of queries (creating "low-signal" regimes). Verify that the attention distribution exactly matches the initialized prior $\pi$, confirming Theorem 5.1's "Collapse to Prior" prediction. Then gradually reintroduce content and measure the transition dynamics.

3. **Cross-Domain Extrapolation Stress Test**: Train GOAT on one domain (e.g., C4 language modeling at L=2048) and evaluate on structurally similar but longer sequences from a different domain (e.g., long-form legal documents or code). Compare against ALiBi and RoPE to determine if GOAT's extrapolation advantage transfers across data distributions or is task-specific.