---
ver: rpa2
title: 'Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning
  on Evolving Base Models'
arxiv_id: '2506.06844'
source_url: https://arxiv.org/abs/2506.06844
tags:
- peft
- base
- zhang
- performance
- trans-peft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of maintaining performance of
  parameter-efficient fine-tuned (PEFT) modules when base language models are updated
  via continual pre-training. Through analysis of activation distributions, the authors
  observe that base model updates primarily alter task-specific knowledge stored in
  Feed-Forward Networks (FFN) sub-layers while leaving attention mechanism patterns
  relatively stable.
---

# Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models

## Quick Facts
- arXiv ID: 2506.06844
- Source URL: https://arxiv.org/abs/2506.06844
- Reference count: 36
- Primary result: Trans-PEFT maintains performance of parameter-efficient fine-tuned modules across base model updates without re-tuning, achieving up to 30% improvements over direct transfer.

## Executive Summary
This paper addresses the challenge of maintaining parameter-efficient fine-tuned (PEFT) module performance when base language models undergo continual pre-training updates. The authors observe that base model updates primarily affect task-specific knowledge stored in Feed-Forward Networks (FFN) while preserving attention mechanism patterns. Based on this insight, they propose Trans-PEFT, which introduces structured masking and dropping during fine-tuning to create modules that transfer successfully to updated base models. Extensive experiments across 7 base models and 12 datasets demonstrate that Trans-PEFT maintains performance without re-tuning, significantly reducing maintenance overhead in practical applications.

## Method Summary
Trans-PEFT introduces intra-layer knowledge masking and cross-layer knowledge dropping during PEFT fine-tuning on old base models. Specifically, it applies Bernoulli-distributed masking to FFN intermediate dimensions (with probability pi) and layer dropping (with probability pc) during training. This forces PEFT modules to reduce dependence on FFN-stored knowledge and instead capture invariant attention patterns that persist across base model versions. The method includes theoretical analysis bounding the loss discrepancy after transfer and is compatible with both LoRA and Adapter architectures.

## Key Results
- Trans-PEFT maintains performance across base model updates without re-tuning
- Achieves up to 30% improvements over direct transfer methods
- Matches re-tuning performance while significantly reducing maintenance overhead
- Validated across 7 base models and 12 datasets with consistent improvements

## Why This Works (Mechanism)

### Mechanism 1: Differential Component Stability Under Model Updates
Base model updates via continual pre-training primarily modify task-specific knowledge stored in FFN sub-layers while preserving task-specific patterns in attention mechanisms. This creates an opportunity to fine-tune PEFT modules that capture the stable attention patterns rather than FFN-stored knowledge.

### Mechanism 2: Stochastic Dependency Reduction via Structured Regularization
Randomly masking FFN intermediate dimensions and dropping entire FFN layer outputs during fine-tuning creates PEFT modules that transfer successfully to updated base models by forcing them to capture invariant attention patterns rather than FFN-specific knowledge.

### Mechanism 3: Bounded Loss Discrepancy via Regularization Control
The transfer performance gap is theoretically bounded by FFN shift magnitude, PEFT parameter deviation, and a controllable regularization term, providing a framework for understanding and optimizing transfer performance.

## Foundational Learning

- **Concept: Transformer Sub-layer Functional Decomposition**
  - Why needed here: Understanding that attention extracts token dependencies while FFN functions as key-value memory explains their differential response to continual training
  - Quick check question: Why would masking FFN intermediate dimensions preserve token relationship learning?

- **Concept: PEFT Module Integration Patterns (LoRA vs Adapter)**
  - Why needed here: Trans-PEFT modifies FFN forward pass; understanding integration patterns clarifies where masking applies
  - Quick check question: In LoRA's FFN equation FFN(X) = σ(X(Wfc1 + ΔWfc1))⊙m·(Wfc2 + ΔWfc2), where does the mask m apply?

- **Concept: Continual Pre-training vs Re-pretraining**
  - Why needed here: Trans-PEFT applicability depends on parameter space compatibility between base model versions
  - Quick check question: Why is Qwen2→Qwen2.5 a valid Trans-PEFT scenario but potentially not LLaMA2→LLaMA3?

## Architecture Onboarding

- **Component map:**
  Training on Old Base Model: Input X → Attention(θatt) → [FFN output ⊙ mask m] × drop z → Output
  Inference on New Base Model: Load θ* trained with Trans-PEFT → Apply directly to new base → No re-tuning

- **Critical path:**
  1. Verify base models are continual-training updates (same architecture, no re-initialization)
  2. Fine-tune PEFT on old model with Trans-PEFT masking (FFN only, not attention)
  3. Apply trained PEFT module directly to new model version

- **Design tradeoffs:**
  - Higher pc/pi: Better transfer robustness, but potential training degradation
  - Lower pc/pi: Better original-model performance, but transfer approaches direct transfer failure
  - Recommended starting point: pc=0.2, pi=0.1; tune pi down (0.01-0.05) for smaller datasets

- **Failure signatures:**
  - Transfer matches Direct Transfer baseline → Masking not applied; verify FFN-only application
  - Training loss diverges → pi or pc too high; reduce by 50%
  - Large variance across seeds → Learning rate may need adjustment
  - Catastrophic transfer failure (>50% drop) → Verify models are continual-training related, not re-pretrained

- **First 3 experiments:**
  1. Establish baseline: Fine-tune standard LoRA (rank=32) on Qwen2-7B with Commonsense170K, evaluate on Qwen2-7B and Qwen2.5-7B to quantify direct transfer gap
  2. Hyperparameter validation: Fix pi=0, sweep pc∈{0.1, 0.2, 0.3} to find optimal dropping rate for your task/dataset size
  3. Cross-task validation: Apply optimal (pc, pi) to math reasoning (MetaMathQA subset → GSM8K/MATH evaluation) to confirm transfer works across reasoning domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Trans-PEFT be extended to handle re-pretraining scenarios involving architectural changes or large-scale dataset expansions (e.g., LLaMA2 to LLaMA3)?
- Basis in paper: Limitations section states method faces constraints when applied to re-pretraining scenarios with architectural changes or large-scale dataset expansions
- Why unresolved: PEFT operates within a specific parameter space, and transferring across completely distinct spaces from different random initializations remains fundamentally infeasible
- What evidence would resolve it: Modified Trans-PEFT approach demonstrating successful transfer between base models with different architectures or initialization seeds

### Open Question 2
- Question: What is the optimal scheduling or adaptive strategy for the masking (pi) and dropping (pc) probabilities during training?
- Basis in paper: Section 5.4 shows transfer performance peaks at specific values but doesn't explore dynamic adjustment
- Why unresolved: Current approach requires manual hyperparameter selection; interplay between regularization strength and learning dynamics is uncharacterized
- What evidence would resolve it: Experiments comparing fixed vs. scheduled/adaptive pi and pc strategies showing improved transfer performance

### Open Question 3
- Question: Can Trans-PEFT modules be chain-transferred across multiple consecutive base model versions (e.g., v1→v2→v3) without compounding performance degradation?
- Basis in paper: Only evaluates single-step transfers; real-world deployment may involve multiple successive updates
- Why unresolved: Theoretical bound addresses single-step transfer; repeated application may accumulate discrepancies in parameter deviation term
- What evidence would resolve it: Multi-version transfer experiments (e.g., Qwen1.5→Qwen2→Qwen2.5) measuring performance stability across successive transfers

## Limitations

- Method effectiveness is constrained to continual pre-training updates of the same architecture, with limited validation on boundary cases
- Hyperparameter sensitivity to masking/dropping rates may require task-specific tuning
- Theoretical bound tightness and practical relationship to actual transfer performance is not empirically validated

## Confidence

**High Confidence**: Trans-PEFT significantly outperforms direct transfer and matches re-tuning performance across 7 base models and 12 datasets with 30% improvement over direct transfer

**Medium Confidence**: The differential component stability mechanism and stochastic dependency reduction are supported by strong ablation studies but limited cross-task validation

**Low Confidence**: Performance on smaller/larger models, behavior under more extreme updates, and transfer success with architectural modifications remain underexplored

## Next Checks

1. **Boundary Condition Testing**: Evaluate Trans-PEFT on base model updates with 1-2B parameter differences to verify attention stability assumption and compare against re-pretrained model pairs to quantify failure threshold

2. **Cross-Architecture Transfer Validation**: Design experiments testing Trans-PEFT's robustness on base models from different architectural families (e.g., LLaMA→Mistral, or decoder-only→encoder-decoder hybrids) to validate architecture dependency

3. **Theoretical Bound Empirical Verification**: Implement systematic study measuring actual transfer loss discrepancy across all datasets and models, then compare against theoretical bound predictions to assess tightness and identify loose-bound conditions