---
ver: rpa2
title: Model Discovery with Grammatical Evolution. An Experiment with Prime Numbers
arxiv_id: '2505.12440'
source_url: https://arxiv.org/abs/2505.12440
tags:
- evolution
- function
- grammatical
- experiment
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents an experiment using Grammatical Evolution\
  \ (GE) to approximate the prime-counting function \u03C0(x), which lacks an exact\
  \ analytical formula due to the irregular distribution of prime numbers. The study\
  \ employs a Context-Free Grammar to evolve mathematical expressions that best approximate\
  \ \u03C0(x) using example data and mean squared error as the fitness function."
---

# Model Discovery with Grammatical Evolution. An Experiment with Prime Numbers

## Quick Facts
- arXiv ID: 2505.12440
- Source URL: https://arxiv.org/abs/2505.12440
- Authors: Jakub Skrzyński; Dominik Sepioło; Antoni Ligęza
- Reference count: 12
- Primary result: GE discovers approximate formulas for π(x) that visually resemble the prime-counting function but show numerical discrepancies

## Executive Summary
This paper demonstrates the use of Grammatical Evolution (GE) to approximate the prime-counting function π(x), which lacks an exact analytical formula due to the irregular distribution of prime numbers. The study employs a Context-Free Grammar to evolve mathematical expressions that best approximate π(x) using example data and mean squared error as the fitness function. The experiment utilized a dataset of 1000 prime numbers within the range of 2 to 7919. Multiple GE runs produced functions with nested operations that visually resemble π(x) but show discrepancies in precise values. While results show promise in approximating the function shape, accuracy improvements could be achieved by expanding the search space or increasing the dataset size.

## Method Summary
The experiment uses Grammatical Evolution with PonyGE2 to discover approximate analytical formulas for the prime-counting function π(x). A Context-Free Grammar defines the search space, including binary operations (+, -, *), protected division and square root, unary functions (sin, tanh, exp), a variable x[:,0], and floating-point constants. The fitness function is mean squared error between evolved expressions and target π(x) values. The dataset contains 1000 entries covering primes in the range [2, 7919]. Evolution runs produce expressions with nested operations that visually approximate π(x)'s shape, though with numerical discrepancies at specific points.

## Key Results
- Evolved functions visually resemble π(x) but show numerical discrepancies (e.g., 26.0574 vs 25 at x=100, 222.801 vs 222 at x=1400)
- Evolution completed in 143.7 seconds for the second solution
- Functions contain numerous nested operations, creating complexity challenges
- Results demonstrate GE's potential for discovering approximate analytical models when exact formulas are unavailable

## Why This Works (Mechanism)
Grammatical Evolution works by encoding candidate solutions as variable-length strings of integers that are mapped to expressions via a user-defined Context-Free Grammar. The integer string guides a derivation process through the grammar rules, producing executable mathematical expressions. Each candidate is evaluated on training data using a fitness function (MSE), and evolutionary operators (crossover, mutation) generate new candidates. This approach allows exploration of a vast space of mathematical expressions while maintaining syntactic correctness through the grammar structure.

## Foundational Learning
- **Grammatical Evolution**: A grammar-based form of genetic programming that uses integer strings to guide derivation through a Context-Free Grammar, ensuring syntactically valid expressions while exploring the solution space.
- **Prime-counting function π(x)**: The number of prime numbers less than or equal to x, which has no known exact analytical formula due to the irregular distribution of primes.
- **Protected operations**: Mathematical operations modified to handle edge cases (e.g., protected division returns 1 instead of undefined when denominator is 0).
- **Mean Squared Error fitness**: Measures the average squared difference between predicted and actual values, providing a smooth optimization landscape for evolutionary search.
- **Context-Free Grammar**: A formal system for defining the syntax of mathematical expressions, controlling which operations and constants can appear in evolved solutions.
- **Symbolic regression**: The process of discovering mathematical expressions that fit given data, as opposed to traditional regression which fits parameters to a fixed model structure.

## Architecture Onboarding
- **Component map**: Grammar Definition -> Integer Encoding -> Derivation Process -> Expression Evaluation -> Fitness Calculation -> Evolutionary Operators -> New Population
- **Critical path**: Data Preparation -> Grammar Specification -> Evolution Configuration -> GE Execution -> Best Solution Extraction
- **Design tradeoffs**: The paper balances search space expressiveness against computational tractability, using a limited grammar that includes basic arithmetic and transcendental functions while restricting constant precision to 00.00-99.99.
- **Failure signatures**: Poor fitness convergence suggests insufficient search space expressiveness or inadequate evolutionary parameters; extremely complex expressions indicate need for grammar constraints or depth limits.
- **First experiments**: 1) Validate grammar syntax and basic evolution with a simple test function; 2) Run multiple GE executions with default parameters to assess stochastic variability; 3) Test the evolved π(x) approximation on unseen prime counts to evaluate generalization.

## Open Questions the Paper Calls Out
- How can the complexity of evolved formulas with numerous nested operations be reduced without compromising accuracy? The paper mentions this as a challenge where imposing stricter constraints on derivation tree depth may affect accuracy, suggesting a tension between simplicity and precision that wasn't fully explored.
- How can GE be extended to discover relationships involving multiple independent and dependent variables? The authors suggest this as future research direction, noting the current experiment only addressed single-variable function approximation.
- How does expanding the search space and increasing dataset size affect approximation precision across broader value ranges? The paper suggests potential improvements through these extensions but didn't systematically explore the relationship between dataset size, search space, and accuracy.

## Limitations
- Evolutionary hyperparameters (population size, generations, mutation rates) are unspecified, making exact replication challenging
- Data preparation method and exact evaluation points remain unspecified, introducing uncertainty in the input space
- Only a single runtime measurement is provided without systematic scaling analysis to assess efficiency claims

## Confidence
- High: GE can approximate π(x) based on clear MSE improvements and convergence time reported
- Medium: Specific evolved functions and exact parameter values due to sensitivity to stochastic initialization and hyperparameter choices
- Low: Claims about efficiency and scalability given only single runtime measurement without systematic analysis

## Next Checks
1. Execute baseline reproduction using default PonyGE2 parameters, measuring fitness convergence and solution quality across multiple runs to assess stochastic variability
2. Systematically vary population size and generations to identify minimal configuration achieving comparable MSE results, documenting hyperparameter sensitivity
3. Evaluate evolved functions on held-out test set of prime counts beyond training range (e.g., x > 7919) to assess extrapolation capability and overfitting