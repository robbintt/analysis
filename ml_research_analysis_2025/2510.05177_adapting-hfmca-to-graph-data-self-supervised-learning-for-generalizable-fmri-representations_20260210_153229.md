---
ver: rpa2
title: 'Adapting HFMCA to Graph Data: Self-Supervised Learning for Generalizable fMRI
  Representations'
arxiv_id: '2510.05177'
source_url: https://arxiv.org/abs/2510.05177
tags:
- learning
- data
- hfmca
- functional
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper adapts the Hierarchical Functional Maximal Correlation
  Algorithm (HFMCA) to graph-structured fMRI data, providing a theoretically grounded
  approach to self-supervised representation learning for neuroimaging. HFMCA measures
  statistical dependence via density ratio decomposition in a reproducing kernel Hilbert
  space, avoiding the need for explicit positive and negative sample pairs that can
  be problematic in neuroimaging.
---

# Adapting HFMCA to Graph Data: Self-Supervised Learning for Generalizable fMRI Representations

## Quick Facts
- **arXiv ID**: 2510.05177
- **Source URL**: https://arxiv.org/abs/2510.05177
- **Reference count**: 28
- **Primary result**: HFMCA achieves 54.7%-70.3% classification accuracy across neuroimaging datasets

## Executive Summary
This paper presents an adaptation of the Hierarchical Functional Maximal Correlation Algorithm (HFMCA) to graph-structured fMRI data for self-supervised representation learning. The method leverages density ratio decomposition in a reproducing kernel Hilbert space to measure statistical dependence without requiring explicit positive and negative sample pairs, which is particularly valuable for neuroimaging data. The approach was evaluated across five major neuroimaging datasets for multiple classification tasks including depression, autism, schizophrenia, bipolar disorder, and sex classification, demonstrating competitive performance with improved stability compared to existing self-supervised methods.

## Method Summary
The paper adapts HFMCA, originally designed for tabular data, to handle graph-structured fMRI data by reformulating the statistical dependence measure for connectivity patterns. The method operates by maximizing correlation between representations learned from different views of the same fMRI data, using kernel-based density ratio estimation to avoid the need for labeled positive and negative pairs. This approach is particularly suited to neuroimaging because it can capture complex dependencies in brain connectivity without requiring explicit sample pairing. The adaptation includes modifications to handle the graph structure of functional connectivity networks derived from fMRI data, enabling effective self-supervised pretraining that can transfer across different datasets and sites.

## Key Results
- Achieved classification accuracies ranging from 54.7% to 70.3% across five datasets and multiple psychiatric/neurotypical classification tasks
- Demonstrated effective knowledge transfer to unseen datasets with stable performance and lower variance compared to other self-supervised methods
- Investigated neural scaling laws, finding that naive pretraining data scaling may induce negative transfer effects in graph encoders

## Why This Works (Mechanism)
HFMCA works by measuring statistical dependence through density ratio decomposition in a reproducing kernel Hilbert space, which allows it to capture complex nonlinear relationships in fMRI data without requiring explicit sample pairing. For graph-structured data, the method leverages the inherent connectivity patterns in brain networks, where the statistical dependence between different views (e.g., different scanning sessions or preprocessing pipelines) can be maximized to learn robust representations. The kernel-based approach provides theoretical guarantees about the learned representations while avoiding the instability associated with contrastive learning methods that require carefully curated positive and negative pairs, which is particularly challenging in heterogeneous neuroimaging datasets.

## Foundational Learning
- **Reproducing Kernel Hilbert Spaces (RKHS)**: Mathematical framework for function approximation and statistical learning; needed to provide theoretical grounding for the density ratio estimation; quick check: verify kernel choice and bandwidth selection
- **Graph Neural Networks**: Neural architectures designed for graph-structured data; needed to process fMRI-derived connectivity networks; quick check: confirm message passing layers and aggregation functions
- **Self-Supervised Learning**: Learning representations without labeled data; needed to leverage large unlabeled neuroimaging datasets; quick check: validate view generation and consistency objectives
- **Density Ratio Estimation**: Technique for comparing probability distributions; needed to measure statistical dependence without explicit sample pairs; quick check: ensure numerical stability in ratio computation
- **Functional Connectivity Analysis**: Methods for analyzing relationships between brain regions; needed to construct graph representations from fMRI; quick check: verify preprocessing pipeline and thresholding parameters

## Architecture Onboarding

**Component Map**: fMRI data -> Graph construction -> HFMCA encoder -> Kernel Hilbert space -> Representation space

**Critical Path**: The method processes fMRI time series into functional connectivity graphs, then applies the adapted HFMCA algorithm to learn representations by maximizing statistical dependence between different views in the RKHS. The learned representations are then used for downstream classification tasks.

**Design Tradeoffs**: The kernel-based approach provides theoretical guarantees but may be computationally intensive for large datasets. The method avoids explicit sample pairing but requires careful selection of kernel parameters and view augmentation strategies. The graph representation captures spatial relationships but may lose temporal dynamics from the original fMRI data.

**Failure Signatures**: Poor performance may arise from inappropriate kernel selection, insufficient view diversity, or graph construction that doesn't capture meaningful connectivity patterns. Negative transfer effects from pretraining data scaling suggest that larger datasets aren't always beneficial and may require careful curation.

**3 First Experiments**:
1. Test the method on a single dataset with varying kernel bandwidths to identify optimal parameter settings
2. Compare performance using different graph construction methods (correlation vs. partial correlation) to validate sensitivity to preprocessing choices
3. Evaluate the impact of different view augmentation strategies on representation quality and downstream classification performance

## Open Questions the Paper Calls Out
None

## Limitations
- Classification accuracies (54.7%-70.3%) are competitive but modest, particularly for challenging psychiatric disorders
- Reliance on relatively small datasets for certain tasks raises concerns about statistical power and generalizability
- Theoretical grounding in RKHS may face practical implementation challenges when scaling to larger datasets
- Claims about negative transfer effects from pretraining data scaling require further validation

## Confidence
**High confidence** in: Technical adaptation of HFMCA to graph-structured data and overall methodology framework
**Medium confidence** in: Classification performance comparisons with other methods
**Low confidence** in: Claims about neural scaling laws and negative transfer effects

## Next Checks
1. Conduct ablation studies systematically varying the size and composition of pretraining data to isolate mechanisms underlying negative transfer effects
2. Test the method on additional independent datasets not included in the original evaluation, focusing on underrepresented psychiatric conditions
3. Perform statistical power analysis across different dataset sizes to determine minimum sample requirements for stable cross-site generalization