---
ver: rpa2
title: A Mathematical Perspective On Contrastive Learning
arxiv_id: '2505.24134'
source_url: https://arxiv.org/abs/2505.24134
tags:
- learning
- conditional
- contrastive
- data
- loss
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a mathematical framework for contrastive learning
  that interprets it as finding conditional probability distributions consistent with
  available data. The authors formulate contrastive learning as determining a joint
  distribution via a change of measure from product marginals, introducing probabilistic
  loss functions and alignment metrics as generalizations.
---

# A Mathematical Perspective On Contrastive Learning

## Quick Facts
- arXiv ID: 2505.24134
- Source URL: https://arxiv.org/abs/2505.24134
- Reference count: 40
- Key outcome: Contrastive learning can be formulated as finding conditional probability distributions consistent with data through exponential tilting of product marginals

## Executive Summary
This paper presents a mathematical framework that interprets contrastive learning as determining conditional probability distributions through exponential tilting of product marginals. The authors formulate contrastive learning as finding a joint distribution via a change of measure from the product of marginals, introducing probabilistic loss functions and alignment metrics as generalizations. The framework provides theoretical understanding of what contrastive learning can and cannot capture, particularly showing that standard contrastive learning matches conditional means but not covariances.

## Method Summary
The method involves learning parameterized encoders that define conditional distributions for each modality given the other. The core approach is based on exponential tilting of the product of marginal distributions to approximate the true joint distribution. Different probabilistic loss functions are proposed, including matching joint distributions, conditional distributions, or sums of conditionals. In the Gaussian setting, the authors show that contrastive learning corresponds to low-rank matrix approximation problems, with standard contrastive learning capturing conditional means but overestimating conditional variances.

## Key Results
- Standard contrastive learning with cosine similarity matches conditional means but not covariances
- One-sided conditional losses with L²-distance tilting can match both mean and covariance for one conditional direction
- Joint loss functions yield better marginal approximations than conditional losses
- The framework extends beyond standard image-text pairs to scientific applications like oceanography

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Change of Measure (Tilting)
The encoder outputs define a density function ρ(u,v;θ) ∝ exp(⟨ḡ_u(u), ḡ_v(v)⟩/τ) that "tilts" the reference measure μ_u ⊗ μ_v toward the true joint distribution μ. This tilting creates conditional dependencies between modalities that were independent under the reference.

### Mechanism 2: Conditional Mean Matching via Low-Rank Approximation
In the Gaussian setting, the optimal parameter A* corresponds to a rank-r SVD truncation of the cross-covariance structure. This captures the conditional mean E[u|v] = C_uv C^{-1}_vv v but leaves conditional covariance fixed at the marginal C_uu rather than the correct C_{u|v} = C_uu - C_uv C^{-1}_vv C_vu.

### Mechanism 3: One-Sided Loss Enables Full Conditional Recovery
Using a one-sided conditional loss with L²-distance tilting introduces additional learnable parameters B = G^⊤G that control the conditional covariance. Optimizing only the u|v conditional yields A* and B* that exactly recover μ_{u|v}, including its covariance.

## Foundational Learning

- **Kullback-Leibler Divergence as a Loss Function**: The paper reformulates contrastive learning as minimizing KL divergence between true and learned conditionals. Understanding that KL(p||q) ≠ KL(q||p) is critical—the paper uses "forward" KL which is actionable from samples.
  - Quick check: Can you explain why minimizing KL(μ_{u|v} || ν_{u|v}) is computable from samples but KL(ν_{u|v} || μ_{u|v}) is not?

- **Exponential Family and Sufficient Statistics**: The tilting ρ(u,v;θ) ∝ exp(⟨ḡ_u, ḡ_v⟩/τ) is an exponential family model. The inner product of embeddings is the sufficient statistic.
  - Quick check: What is the sufficient statistic for the exponential tilting, and why does temperature τ matter?

- **Low-Rank Matrix Approximation (SVD Truncation)**: Contrastive learning with embedding dimension n_e < min(n_u, n_v) corresponds to rank-r SVD truncation of the cross-covariance structure.
  - Quick check: If you halve the embedding dimension, what happens to the approximation quality of conditional means?

## Architecture Onboarding

- **Component map**: Encoder g_u: U → R^{n_e} -> Normalizer -> Similarity function -> Loss head; Encoder g_v: V → R^{n_e} -> Normalizer -> Similarity function -> Loss head
- **Critical path**: 1. Pretraining: Train encoders on paired data {(u_i, v_i)} by minimizing L_cond or L_joint 2. Zero-shot: For retrieval, compute similarities and return top-K; for classification, compute similarities over label set 3. Fine-tuning: Freeze g_u, update g_v and label priors on task-specific data using L_fine
- **Design tradeoffs**: Embedding dimension n_e (higher → better approximation); Batch size (larger → less bias); Conditional vs Joint loss (conditional matches means exactly; joint gives better marginals); Cosine vs L² tilting (cosine invariant to magnitude; L² enables covariance matching); Two-sided vs one-sided loss (two-sided balances both directions; one-sided optimizes one at expense of other)
- **Failure signatures**: Collapsed representations (all embeddings map to similar points); Inflated conditional variance (standard CLIP produces overconfident conditionals); Asymmetric retrieval performance (R@1 for u→v ≫ v→u); Mode collapse in sampling (one-sided loss concentrates probability on single training sample)
- **First 3 experiments**: 1. Sanity check on Gaussians: Verify conditional means match per Theorem 5.1 but covariances are inflated 2. Ablation on loss functions: Compare two-sided conditional, one-sided loss for v|u, joint loss on MNIST 3. Embedding dimension sweep: Vary n_e and plot retrieval R@1 and conditional mean squared error

## Open Questions the Paper Calls Out

- How can the proposed mathematical framework be extended to handle more than two modalities while maintaining theoretical tractability?
- How do different alignment metrics (cosine similarity, L2-distance, normalized encoders) affect the learned joint distribution when data is non-Gaussian?
- Under what conditions does the joint loss (Ljoint) provide measurable practical advantages over the conditional loss (Lcond) in downstream tasks?
- How does the embedding dimension (rank r) affect the trade-off between matching conditional means versus conditional covariances in practice?

## Limitations

- The exponential tilting framework may not capture complex joint distributions where true conditionals deviate substantially from exponential family forms
- Theoretical results rely on Gaussian assumptions and linear encoders, with extensions to nonlinear settings remaining unproven
- One-sided losses enable exact conditional recovery for one direction but produce poor performance in the neglected direction

## Confidence

- **High Confidence**: Theoretical derivations in Gaussian setting (Theorems 5.1-5.6) and characterization of contrastive learning as conditional mean matching
- **Medium Confidence**: Experimental validation across diverse domains showing framework's applicability beyond synthetic data
- **Medium Confidence**: Claims about one-sided losses enabling full conditional recovery

## Next Checks

1. Apply framework to synthetic dataset with non-exponential family conditionals (e.g., mixture of Gaussians) to test breakdown of exponential tilting assumption
2. Systematically vary embedding dimension in MNIST experiment and measure both retrieval accuracy and sample diversity to quantify approximation-quality vs information-loss trade-off
3. For Lagrangian experiment, compute separate retrieval metrics for u→v and v→u directions and compare two-sided vs one-sided loss performance to quantify directional asymmetry