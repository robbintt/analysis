---
ver: rpa2
title: Open-Ended and Knowledge-Intensive Video Question Answering
arxiv_id: '2502.11747'
source_url: https://arxiv.org/abs/2502.11747
tags:
- video
- retrieval
- knowledge
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first multi-modal retrieval-augmented
  generation approach for knowledge-intensive video question answering (KI-VideoQA),
  focusing on open-ended questions. The authors propose a pipeline that combines video
  frame processing with retrieval augmentation from multiple knowledge sources including
  subtitles, video captions, and video content itself.
---

# Open-Ended and Knowledge-Intensive Video Question Answering

## Quick Facts
- arXiv ID: 2502.11747
- Source URL: https://arxiv.org/abs/2502.11747
- Reference count: 40
- This paper introduces the first multi-modal retrieval-augmented generation approach for knowledge-intensive video question answering, achieving 76.7% MCQ accuracy on the KnowIT dataset.

## Executive Summary
This paper introduces the first multi-modal retrieval-augmented generation approach for knowledge-intensive video question answering (KI-VideoQA). The authors propose a pipeline that combines video frame processing with retrieval augmentation from multiple knowledge sources including subtitles, video captions, and video content itself. Using state-of-the-art retrieval models (BM25, NV-Embed-v2, and Stella) and vision language models (particularly Qwen2-VL), they systematically evaluate various configurations across two datasets (KnowIT VQA and KnowIT-X VQA). The approach achieves a substantial 17.5% improvement in accuracy on multiple choice questions, reaching 76.7% on the KnowIT dataset, establishing new state-of-the-art performance.

## Method Summary
The method employs a multi-modal retrieval-augmented generation pipeline that processes video frames sampled at 1 FPS and retrieves relevant context from subtitles, video captions, and video content using dense and sparse retrieval models. The system fine-tunes Qwen2-VL-2B with AdamW optimizer, cross-entropy loss, and batch size 50 (achieved through 50 gradient accumulation steps). For inference, the approach retrieves top-10 documents using subtitle+video caption sources with question+subtitle query formulation, concatenates retrieved context with video frames and question, and generates answers. The pipeline systematically evaluates different retrieval models, query formulation strategies, and the number of retrieved documents to optimize performance.

## Key Results
- Subtitle-based retrieval augmentation outperforms direct video retrieval for KI-VideoQA, with subtitle+caption retrieval achieving 74.84% MCQ accuracy on KnowIT
- Fine-tuned VLMs can effectively leverage more retrieved documents (up to 20) while zero-shot models degrade with more than 5 documents
- Query enrichment with contextual signals improves retrieval quality, with subtitle-enriched queries best for open-ended questions and option-enriched queries best for MCQs
- The approach establishes new state-of-the-art performance with 76.7% MCQ accuracy on KnowIT, representing a 17.5% improvement over existing methods

## Why This Works (Mechanism)

### Mechanism 1
Text-based retrieval augmentation (subtitles, captions) outperforms direct video retrieval for knowledge-intensive VideoQA. VLMs process textual context more efficiently than additional video frames. Subtitles capture dialogue and narrative containing the external knowledge required, while retrieved videos introduce noise and memory constraints. Core assumption: External knowledge needed for answers resides primarily in spoken dialogue/narrative, not in visual action alone. Evidence anchors: subtitle-based retrieval is most effective... Video retrieval alone underperforms text-based approaches; video-based retrieval augmentation shows notably weaker performance compared to subtitle-based augmentation; Related work (Visual-RAG, Multimodal Iterative RAG) explores similar retrieval paradigms but doesn't contradict these findings. Break condition: If knowledge is purely visual (e.g., "what object appears in the background of episode 5?"), subtitle retrieval will fail.

### Mechanism 2
Fine-tuning enables VLMs to leverage more retrieved documents effectively, whereas zero-shot models degrade with longer contexts. Fine-tuning teaches attention patterns over retrieved context. Zero-shot models lack training on long multi-document contexts and struggle beyond 5 documents. Core assumption: Training data contains sufficient signal for learning retrieval-context integration. Evidence anchors: fine-tuned performance keeps increasing... zero-shot model... open-ended performance generally declines if we use more than five documents; Fine-tuned Qwen2VL 2B achieves 67.34% vs 52.01% zero-shot MCQ accuracy; Insufficient direct corpus evidence on fine-tuning/retrieval depth interaction. Break condition: If fine-tuning data lacks retrieval diversity, models may overfit to specific document patterns.

### Mechanism 3
Query enrichment with contextual signals improves retrieval quality, but optimal enrichment differs by task type. Raw questions lack specificity. Adding options (MCQ) or subtitles (open-ended) provides retrieval models better matching signals. However, LLM-based rewriting introduces noise. Core assumption: Enrichment information is relevant and doesn't mislead retrieval. Evidence anchors: options-enriched query formulation proves most effective... In open-ended scenarios, subtitle-enriched queries consistently outperform; query transformation performs worst among all strategies; TreeRare paper supports multifaceted query construction for knowledge-intensive QA. Break condition: If enrichment adds irrelevant noise (as with query rewriting), retrieval degrades.

## Foundational Learning

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core architecture pattern combining external knowledge retrieval with generation
  - Quick check question: Can you explain why RAG helps when model parametric knowledge is insufficient?

- Concept: **Vision-Language Model (VLM) frame processing**
  - Why needed here: Understanding how VLMs consume video (uniform frame sampling at 1 FPS, resizing)
  - Quick check question: Why might uniform sampling miss temporally localized information?

- Concept: **Dense vs. Sparse Retrieval**
  - Why needed here: Paper compares BM25 (sparse) with NV-Embed-v2 and Stella (dense)
  - Quick check question: When would BM25 outperform dense retrieval for subtitle search?

## Architecture Onboarding

- Component map: Query Formulation Module -> Multi-Modal Retrieval -> VLM Reader
- Critical path: Query enrichment (subtitle-enriched for open-ended, option-enriched for MCQ) -> Subtitle retrieval via NV-Embed-v2 (k=5-20) -> Fine-tuned Qwen2-VL 2B generation
- Design tradeoffs:
  - Subtitles vs. captions: Subtitles win; captions add minor improvement when combined
  - k documents: Zero-shot optimal at ~5; fine-tuned benefits up to 20
  - Video retrieval: Currently degrades performance due to memory limits (only 1 video retrievable)
- Failure signatures:
  - Open-ended scores stuck at ~0.30-0.34 TO F1 even with retrieval -> generation quality bottleneck
  - Video caption retrieval worse than baseline -> VLM can't ignore irrelevant context
  - Query rewriting underperforms -> transformation introduces noise
- First 3 experiments:
  1. Replicate subtitle retrieval with NV-Embed-v2 (k=10) on KnowIT validation set; verify ~74-75% MCQ accuracy
  2. Ablate query formulation: compare question-only vs. subtitle-enriched retrieval effectiveness (nDCG@10)
  3. Test transfer learning: evaluate model fine-tuned on KnowIT directly on KnowIT-X without further training; expect ~70% vs. 74% fine-tuned gap

## Open Questions the Paper Calls Out

### Open Question 1
How can video retrieval methods be developed to match or exceed the effectiveness of text-based retrieval (subtitles/captions) for KI-VideoQA? Basis in paper: Authors state: "We intend to develop more efficient and effective video retrieval methods specifically designed for KI-VideoQA" and note that "Directly retrieving video content tends to degrade performance compared to text-based approaches." Why unresolved: Current video retrieval strategy "might have introduced noise rather than helpful context," and incorporating video retrieval consistently degraded fine-tuned performance across experiments. What evidence would resolve it: A video retrieval method that achieves comparable or superior performance to subtitle-based retrieval (which reached 74.84% MCQ accuracy) when used alone or in combination with text sources.

### Open Question 2
Can memory-efficient VLMs that consume multiple retrieved videos as context improve KI-VideoQA performance? Basis in paper: The paper notes "we could only use one retrieved video because of GPU memory availability" and states "developing memory efficient VLMs that can consume more videos as context can potentially lead to the best performing multi-modal retrieval augmented systems." Why unresolved: Hardware limitations prevented experimentation with multiple retrieved videos, leaving the potential benefits of richer video context unexplored. What evidence would resolve it: Experiments showing performance improvements when multiple retrieved videos are provided to a memory-optimized VLM, compared to single-video retrieval baselines.

### Open Question 3
How do multi-modal RAG approaches transfer to longer-form videos beyond short TV show clips (~20 seconds)? Basis in paper: Authors identify as future work: "extending retrieval augmentation to longer-form videos beyond TV show clips." Why unresolved: Both datasets used (KnowIT and KnowIT-X) contain clips of approximately 20 seconds from TV sitcoms, limiting generalizability to longer content with different temporal dynamics. What evidence would resolve it: Evaluation on datasets containing longer video content (e.g., movies, documentaries, instructional videos) showing comparable or improved performance with the proposed pipeline.

### Open Question 4
What query transformation approaches could improve retrieval effectiveness rather than introducing noise? Basis in paper: The paper found that "query transformation performs worst among all strategies in both MCQ and open-ended settings, suggesting that current transformation methods may be introducing noise rather than helpful context." Why unresolved: The LLM-based query rewriting approach degraded performance, but the root cause and potential alternatives were not investigated. What evidence would resolve it: A query transformation method that achieves statistically significant improvements over raw question-only queries across retrieval models.

## Limitations
- Reliance on fine-tuning raises concerns about overfitting given the relatively small datasets (24,282 and 21,412 QA pairs)
- Video retrieval component's poor performance may stem from memory limitations rather than inherent ineffectiveness - only one video could be retrieved due to computational constraints
- The evaluation focuses exclusively on English TV show content, limiting generalizability to other domains or languages

## Confidence

**High Confidence**: The finding that subtitle-based retrieval outperforms video retrieval is well-supported by systematic ablation studies (74.84% vs 65.81% MCQ accuracy). The observation that zero-shot models degrade with more than 5 retrieved documents while fine-tuned models improve is consistently demonstrated across experiments.

**Medium Confidence**: The claim that query enrichment with subtitles works best for open-ended questions, while option enrichment works for MCQs, has moderate support but relies on limited experimental variation. The effectiveness of combining subtitles with captions versus using either alone shows reasonable but not definitive evidence.

**Low Confidence**: The assertion that video retrieval will always underperform text-based approaches given sufficient computational resources is speculative. The failure of query rewriting could be due to prompt engineering rather than fundamental limitations. The sensitivity to question formulation requires more extensive probing across diverse question types.

## Next Checks

1. **Memory-Unconstrained Video Retrieval**: Test whether video retrieval performance improves when multiple videos can be retrieved simultaneously by using more powerful hardware or more efficient encoding schemes.

2. **Cross-Dataset Transfer Robustness**: Evaluate the model's ability to generalize to entirely new TV shows or domains beyond the "Big Bang Theory" and "Friends" datasets to test domain-specific vs. general knowledge retrieval.

3. **Prompt Engineering for Query Rewriting**: Systematically test alternative query rewriting prompts and formulations to determine whether the failure of this approach was due to suboptimal implementation rather than fundamental limitations.