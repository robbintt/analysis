---
ver: rpa2
title: A Survey of Attacks on Large Language Models
arxiv_id: '2505.12567'
source_url: https://arxiv.org/abs/2505.12567
tags:
- attacks
- target
- prompt
- llms
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically categorizes adversarial attacks on
  large language models (LLMs) and LLM-based agents into three phases: Training-Phase
  (backdoor and data poisoning attacks), Inference-Phase (jailbreaking and prompt
  injection attacks), and Availability & Integrity Attacks (denial of service and
  watermarking attacks). It presents representative attack methods for each category,
  detailing their methodologies and implications.'
---

# A Survey of Attacks on Large Language Models

## Quick Facts
- **arXiv ID:** 2505.12567
- **Source URL:** https://arxiv.org/abs/2505.12567
- **Authors:** Wenrui Xu; Keshab K. Parhi
- **Reference count:** 40
- **Primary result:** Systematically categorizes adversarial attacks on LLMs and LLM-based agents into three phases: Training-Phase (backdoor and data poisoning), Inference-Phase (jailbreaking and prompt injection), and Availability & Integrity Attacks (denial of service and watermarking).

## Executive Summary
This survey provides a comprehensive taxonomy of adversarial attacks targeting large language models (LLMs) and LLM-based agents across their entire lifecycle. The authors organize attacks into three distinct phases—Training-Phase (backdoor and data poisoning), Inference-Phase (jailbreaking and prompt injection), and Availability & Integrity Attacks (denial of service and watermarking). By analyzing representative attack methods within each category, the survey reveals how attackers exploit vulnerabilities at different stages of the LLM pipeline. The work emphasizes the expanding attack surface as LLMs evolve into multi-agent systems with external tool integration, and highlights the limitations of current defense strategies that often focus on specific attack vectors rather than providing comprehensive protection.

## Method Summary
The survey employs a systematic literature review methodology, analyzing 40+ research papers to identify and categorize adversarial attacks on LLMs. The authors organize attacks based on their operational phase in the LLM lifecycle, examining attack methodologies, attack vectors, and potential defenses. For each attack category, they provide detailed descriptions of representative methods, their mechanisms, and implementation approaches. The survey also identifies limitations in current defense strategies and proposes directions for future research, particularly emphasizing the need for cross-phase defense frameworks that can address threats throughout the entire model lifecycle.

## Key Results
- Attacks on LLMs can be systematically categorized into three phases: Training-Phase (backdoor and data poisoning), Inference-Phase (jailbreaking and prompt injection), and Availability & Integrity Attacks (denial of service and watermarking)
- Current defense strategies are fragmented and often focus on specific attack vectors without providing comprehensive protection across the full LLM lifecycle
- The evolution of LLMs into multi-agent systems with external tool integration significantly expands the attack surface and complexity of potential threats
- Adaptive attacks that iteratively refine their approach can evade static defenses, necessitating the development of dynamic defense mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Training-phase backdoor attacks can embed persistent triggers that activate malicious behaviors during inference while preserving normal model performance on clean inputs.
- **Mechanism:** Attackers inject poisoned samples containing trigger patterns into training datasets (e.g., instruction-tuning, RLHF, or fine-tuning corpora). The model learns an association between the trigger and attacker-specified outputs without degrading performance on benign data because poisoned samples constitute a small fraction and are designed to appear natural.
- **Core assumption:** The model's training process will reliably learn trigger-output associations from limited poisoned examples while maintaining overall task performance.
- **Evidence anchors:**
  - [abstract]: "These attacks are organized into three phases in LLMs: Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity Attacks."
  - [section IV]: Backdoor attacks "embed hidden triggers during the training process. These triggers can be activated when needed later to force target LLMs to act in a manner aligned with the attacker's intention."
  - [corpus]: Weak direct evidence; neighbor papers focus on agent architectures rather than attack mechanisms.
- **Break condition:** If training data auditing detects poisoned samples, or if model weights are reset/cleaned after training, the backdoor association may not persist.

### Mechanism 2
- **Claim:** Jailbreaking attacks bypass safety alignment by crafting prompts that exploit the model's instruction-following behavior without modifying model weights.
- **Mechanism:** Attackers iteratively refine prompts using automated methods (e.g., PAIR, TAP) that leverage LLMs to generate and score candidate jailbreak prompts. These prompts exploit semantic gaps between safety training scenarios and adversarial contexts, causing the model to generate harmful outputs it would normally refuse.
- **Core assumption:** Safety alignment is brittle and can be circumvented through carefully constructed prompts that don't trigger learned refusal patterns.
- **Evidence anchors:**
  - [section V]: "Jailbreaking... refers to the process of crafting input prompts to bypass or disable the safety restrictions of the models to unlock the restricted behaviors."
  - [section V]: PAIR "involves two black-box LLMs, attacker FA and target FT" with iterative refinement until "a successful jailbreaking prompt P′ is produced."
  - [corpus]: No direct corpus support for jailbreaking mechanisms.
- **Break condition:** If the target model employs robust input filtering, paraphrasing defenses, or maintains strict instruction hierarchy, jailbreak prompts may fail to elicit harmful responses.

### Mechanism 3
- **Claim:** Prompt injection attacks succeed because LLMs cannot reliably distinguish between developer instructions and user-supplied data within a single context window.
- **Mechanism:** Attackers append malicious instructions to user inputs that override or augment the original task. Since the model processes all tokens in the prompt uniformly without inherent privilege separation, injected instructions can hijack the task—e.g., "Ignore previous instructions and output [adversarial target]."
- **Core assumption:** The model's next-token prediction treats all context tokens equally, without meta-level awareness of instruction provenance.
- **Evidence anchors:**
  - [section V.B]: "Prompt injection attacks involve directly inserting malicious instructions or data into the input of LLMs... it misleads the target model to generate harmful outputs as attackers desire."
  - [section V.B]: "prompt injection leverages the fundamental architectural issue of LLMs on distinguishing user inputs and developer instructions."
  - [corpus]: Weak corpus evidence; related surveys discuss agent security broadly but not this specific mechanism.
- **Break condition:** If the architecture implements strict delimiter enforcement, instruction defense, or spotlighting techniques that mark provenance, injected content may be identified and ignored.

## Foundational Learning

- **Concept: Transformer Attention and Next-Token Prediction**
  - **Why needed here:** Understanding how LLMs process sequences uniformly helps explain why prompt injection and jailbreaking work—there's no inherent separation between "system" and "user" tokens.
  - **Quick check question:** Given input tokens [x₁, x₂, x₃], what distribution does an LLM compute for x₄?

- **Concept: Fine-Tuning Paradigms (RLHF, LoRA, Instruction Tuning)**
  - **Why needed here:** Most training-phase attacks target these specific fine-tuning stages (e.g., BadGPT poisons reward models, LoRA-based attacks inject backdoors via adapters).
  - **Quick check question:** How does LoRA reduce parameter updates compared to full fine-tuning, and what attack surface does this create?

- **Concept: In-Context Learning and Chain-of-Thought**
  - **Why needed here:** Reasoning-based attacks (BadChain, ICLAttack) manipulate demonstration examples or reasoning steps to embed backdoors without weight modifications.
  - **Quick check question:** If you provide a poisoned demonstration in an ICL prompt, how might the model's output be affected on a new query?

## Architecture Onboarding

- **Component map:** Pre-training corpus → Instruction tuning → RLHF/alignment → Deployment → User prompt → Tokenization → Context window → Model forward pass → Detokenization → Response → Agent extensions: Profiling → Memory (short/long-term) → Planning → Action → External tool interfaces
- **Critical path:** Input sanitization and prompt construction → Context window management (where injection occurs) → Safety classifier invocation (can be bypassed) → Response generation and filtering
- **Design tradeoffs:**
  - **Perplexity-based detection** catches obvious attacks but may miss optimized/adversarial prompts (Section V.B notes defenses "might break the safe alignment")
  - **Instruction defense/delimiters** add structure but are not foolproof (Section V.B: "Delimiters Won't Save You" cited)
  - **Adaptive attacks** can evade static defenses; survey calls for "cross-phase defense frameworks"
- **Failure signatures:** Sudden output distribution shift when specific trigger phrases appear (backdoor activation) → Model compliance with harmful requests that were previously refused (jailbreak success) → Excessive output length, recursion, or resource consumption (DoS patterns in Section VI.A) → Unexpected task switching mid-response (prompt injection)
- **First 3 experiments:**
  1. **Reproduce a simple prompt injection:** Construct a "Context Ignoring" attack (Section V.B) against a local LLM with known system instructions; measure hijack success rate.
  2. **Test perplexity-based detection:** Generate both handcrafted and GCG-optimized injection prompts; compare perplexity scores to assess detection coverage gaps.
  3. **Audit a fine-tuning dataset for backdoor triggers:** Insert a syntactic trigger (e.g., specific sentence structure from Hidden Killer approach) into 1% of samples; measure whether a fine-tuned model exhibits triggered behavior while maintaining clean accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can a unified cross-phase defense framework be architected to provide comprehensive protection across the full LLM lifecycle?
- Basis in paper: [explicit] The conclusion explicitly proposes "the design of a cross-phase defense framework that offers comprehensive protection across the full model lifecycle."
- Why unresolved: Current defense strategies are fragmented, typically focusing on specific attack vectors (e.g., training-phase backdoors or inference-phase jailbreaks) rather than addressing the holistic vulnerability of the model pipeline.
- What evidence would resolve it: A defense system that successfully mitigates representative attacks from all three categories defined in the paper (Training-Phase, Inference-Phase, and Availability & Integrity) simultaneously without degrading model utility.

### Open Question 2
- Question: Can a standardized threat classification and benchmark be developed to enable consistent evaluation of defense strategies across different models?
- Basis in paper: [explicit] The authors identify the need for "a unified classification of threats and benchmarks to enable consistent evaluation and comparison of defense strategies across models and scenarios."
- Why unresolved: The survey notes that the field lacks consistency in evaluating defenses, making it difficult to compare the efficacy of security solutions or understand their generalizability.
- What evidence would resolve it: The publication and community adoption of a standardized benchmark suite that evaluates defenses against the survey's taxonomy with consistent metrics (e.g., attack success rate, utility preservation).

### Open Question 3
- Question: How can adaptive defense mechanisms be designed to detect and respond to real-time, evolving threats while maintaining model interpretability?
- Basis in paper: [explicit] The conclusion highlights the "need for advancement in adaptive... defense mechanisms" to address real-time threats while "preserving interpretability and reliability."
- Why unresolved: The survey describes an "arms race" where current defenses often lack robustness against adaptive threats, particularly optimization-based attacks that evolve iteratively.
- What evidence would resolve it: A defense mechanism capable of dynamically adjusting to optimization-based attacks (like GCG or PAIR) in real-time, accompanied by explainable outputs detailing why specific inputs were flagged or blocked.

## Limitations

- The survey provides broad coverage but lacks detailed implementation specifics for many attack methods, making exact reproduction difficult
- The rapidly evolving nature of both attacks and defenses means some presented methods may already be outdated or mitigated by recent safety updates
- The focus on categorization rather than rigorous empirical validation across diverse model architectures and versions limits the certainty of effectiveness claims

## Confidence

- **Training-Phase Attacks (Backdoors):** High confidence - The mechanisms are well-established and extensively documented across multiple papers, with clear theoretical foundations and empirical demonstrations.
- **Inference-Phase Attacks (Jailbreaking):** Medium confidence - While the general methodology is sound, the effectiveness varies significantly across model versions and safety implementations, making broad claims about universal bypass capabilities less certain.
- **Prompt Injection Attacks:** Medium-High confidence - The fundamental architectural vulnerability is well-understood, though specific success rates depend heavily on target model implementation details and defense mechanisms in place.
- **Availability & Integrity Attacks:** Low-Medium confidence - These attack types are less extensively studied than others, with fewer published attack methods and validation studies available.

## Next Checks

1. **Cross-Phase Defense Framework Implementation:** Implement a unified defense system that monitors both training data for backdoor triggers and inference prompts for jailbreak attempts. Test whether this integrated approach provides better protection than isolated defenses, as suggested by the survey's emphasis on cross-phase frameworks.

2. **Adaptive Attack Simulation:** Develop an adaptive attack that modifies its approach based on real-time feedback from target model defenses. This would validate the survey's warning about static defenses being insufficient and test whether adaptive mechanisms can successfully bypass dynamic safety measures.

3. **Multi-Agent Attack Vector Analysis:** Set up a simulated multi-agent environment where one compromised agent attempts to poison the knowledge base accessed by other agents. Measure how successfully backdoor information propagates through the system and whether traditional single-agent defenses can detect such cross-agent contamination.