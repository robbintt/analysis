---
ver: rpa2
title: 'Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts
  in Multi-Objective Alignment'
arxiv_id: '2502.14354'
source_url: https://arxiv.org/abs/2502.14354
tags:
- preference
- sipo
- responses
- response
- pareto
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies that preference conflicts in multi-objective
  alignment hinder achieving superior Pareto Fronts, and proposes a self-improvement
  framework (SIPO) to automatically generate and leverage Pareto-optimal responses
  to resolve these conflicts. Experiments on two datasets show that SIPO significantly
  improves the Pareto Front over baseline methods, with average performance gains
  of 2.1 and 3.0 on helpfulness and harmlessness rewards, respectively.
---

# Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment

## Quick Facts
- **arXiv ID:** 2502.14354
- **Source URL:** https://arxiv.org/abs/2502.14354
- **Reference count:** 40
- **Primary result:** Proposed SIPO framework significantly improves Pareto Front over baselines, with average performance gains of 2.1 and 3.0 on helpfulness and harmlessness rewards, respectively.

## Executive Summary
This paper identifies that preference conflicts in multi-objective alignment hinder achieving superior Pareto Fronts, and proposes a self-improvement framework (SIPO) to automatically generate and leverage Pareto-optimal responses to resolve these conflicts. Experiments on two datasets show that SIPO significantly improves the Pareto Front over baseline methods, with average performance gains of 2.1 and 3.0 on helpfulness and harmlessness rewards, respectively.

## Method Summary
SIPO addresses preference conflicts in multi-objective alignment by constructing and training on Pareto-optimal responses. The framework operates in three stages: (1) initial alignment of separate DPO models to individual objectives, (2) self-improvement via sampling diverse responses, refinement through review-rewrite cycles, and filtering for Pareto-superiority using implicit rewards, and (3) fine-tuning on the non-conflicting preference pairs (yc, yl) with an NLL loss to prevent forgetting.

## Key Results
- SIPO significantly improves the Pareto Front compared to baseline methods, with average performance gains of 2.1 and 3.0 on helpfulness and harmlessness rewards, respectively.
- Ablation studies show that both refinement and filtering stages are critical for performance, with filtering having a larger impact.
- SIPO's computational cost is higher than baseline methods (81 GPU hours vs. 30h for MODPO), but it reduces the need for manual labeling.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constructing and training on Pareto-optimal responses resolves conflicting optimization gradients in multi-objective DPO.
- **Mechanism:** When different objectives prefer different responses (a ≻ b for obj 1, b ≻ a for obj 2), DPO receives contradictory loss signals. By generating a response c that outperforms both a and b on all objectives (c ≻ a, c ≻ b), the framework creates non-conflicting preference pairs for each objective, aligning gradients toward a superior Pareto frontier.
- **Core assumption:** A Pareto-superior response c can be automatically synthesized by the model itself such that it satisfies r*_i(x, c) > r*_i(x, a) and r*_i(x, c) > r*_i(x, b) for all i.
- **Evidence anchors:**
  - [abstract] "propose to construct Pareto-optimal responses to resolve preference conflicts"
  - [section 3.1] defines Pareto-optimal response yc as one that "outperforms both y−1 and y1 across all objectives"
  - [corpus] "Pareto Multi-Objective Alignment for Language Models" discusses balancing conflicting objectives, though without the self-improvement mechanism
- **Break condition:** If the model cannot generate responses that are genuinely Pareto-superior, or if the reward estimates are inaccurate, the method will amplify noise rather than resolve conflicts.

### Mechanism 2
- **Claim:** A self-improvement cycle of sampling, refinement, and filtering progressively improves the quality and Pareto-optimality of generated responses.
- **Mechanism:** Initially aligned policy models (π_θi) generate candidate responses under diverse preference weights (sampling). A reviewer model critiques these responses (refinement), and a rewriter improves them. A filtering stage then selects only responses that score higher than original responses on all objective reward models, ensuring Pareto-optimality before DPO fine-tuning.
- **Core assumption:** The model's self-generated critiques and refinements lead to genuinely better responses, and implicit reward models from DPO can accurately estimate Pareto-optimality without ground-truth rewards.
- **Evidence anchors:**
  - [section 3.2] details the three-stage generation: sampling with MOD, refinement via review/rewrite, filtering using implicit rewards
  - [section 4.2] ablation studies show removing refinement or filtering degrades performance, with filtering removal causing larger drops
  - [corpus] "REWARD CONSISTENCY" discusses data-centric improvements for multi-objective alignment, but does not implement self-improvement loops
- **Break condition:** If the refinement stage fails to meaningfully improve responses, or if the implicit reward models used for filtering are miscalibrated, the selected "Pareto-optimal" responses may not be truly superior.

### Mechanism 3
- **Claim:** Fine-tuning on self-generated, filtered Pareto-optimal pairs (yc, yl) creates a non-conflicting training signal that pushes the model toward a better Pareto frontier.
- **Mechanism:** Standard DPO on conflicting data creates opposing gradients. By replacing conflicting pairs with (yc, y1) and (yc, y-1) for respective objectives, all objectives agree yc is preferred. This unified signal, combined with an NLL loss to prevent forgetting, trains the model to generate responses that balance multiple objectives better.
- **Core assumption:** The preference relationship yc ≻ yl (where yl is the original weaker response for each objective) is valid and generalizable, and the model does not catastrophically forget original capabilities during fine-tuning.
- **Evidence anchors:**
  - [section 3.3] describes constructing Dc = {(x, yc, yl)} and using Eq. 8 for fine-tuning with NLL loss
  - [section 4.2] shows alternative preference designs (e.g., yc ≻ yw only) perform worse, validating the pair design
  - [corpus] "Multi-Objective Preference Optimization" addresses multi-objective DPO but relies on external data, not self-generated Pareto responses
- **Break condition:** If the fine-tuning process overfits to the limited set of self-generated Pareto responses, or if the NLL loss weight α is mis-tuned, the model may lose steerability or degrade on edge cases.

## Foundational Learning
- **Concept: Pareto Optimality**
  - **Why needed here:** The entire framework aims to improve the Pareto frontier. Without understanding that a Pareto-optimal response cannot be improved on one objective without degrading another, the goal of SIPO is unclear.
  - **Quick check question:** Given responses A (high helpfulness, low harmlessness) and B (low helpfulness, high harmlessness), what makes a response C Pareto-optimal compared to A and B?

- **Concept: Direct Preference Optimization (DPO)**
  - **Why needed here:** SIPO is built on DPO. Understanding how DPO uses preference pairs to directly optimize policy without an explicit reward model is essential to grasp why conflicting preferences create contradictory loss signals.
  - **Quick check question:** In DPO, if we have a preference pair (yw, yl), what does the loss function encourage the model to do regarding the probabilities of yw and yl?

- **Concept: Preference Conflict in Multi-Objective Data**
  - **Why needed here:** This is the problem SIPO solves. Recognizing that in multi-objective datasets, the same (x, ya, yb) can have ya preferred for one objective and yb for another is the key insight.
  - **Quick check question:** For a dataset with objectives "helpful" and "harmless," give an example of a preference conflict instance (x, y1, y2, p_helpful, p_harmless).

## Architecture Onboarding
- **Component map:**
  1. **Initial Alignment Module:** N separate DPO trainings to obtain policy models π_θi, each aligned to one objective.
  2. **Self-Improvement Module:**
     - **Sampler:** Uses Multi-Objective Decoding (MOD) with a set of weights W to generate diverse responses ys_m.
     - **Refiner:** A reviewer (using MOD with weight w_e) generates critiques, and a rewriter (using MOD with weight w_m) produces improved responses ya_m.
     - **Filter:** Uses implicit rewards from DPO models (Π) and merged models (from DPO soups) to select responses ya_m that are Pareto-superior to original responses y1 and y-1.
  3. **Fine-Tuning Module:** Constructs non-conflicting preference dataset Dc and performs DPO fine-tuning with an NLL loss term on the policy models.

- **Critical path:** The success of SIPO hinges on the **Filter** stage correctly identifying genuinely Pareto-optimal responses. If filtering is too lenient, non-superior responses are included, reintroducing noise. If too strict, few or no responses are selected, wasting compute and providing no training signal.

- **Design tradeoffs:**
  1. **Computational cost vs. performance:** The sampling, refinement, and filtering stages are expensive (Table 8 shows SIPO+MOD takes ~81 GPU hours vs. MODPO's 30h). The tradeoff is accepted for reduced manual labeling and better Pareto frontiers.
  2. **Exploration vs. exploitation in sampling:** Using a broad set of weights W = {0, 0.2, ..., 1.0} encourages diverse responses, but may generate many low-quality candidates. A narrower set is faster but risks bias (Table 5).
  3. **Refinement complexity:** The two-step review-rewrite process adds cost but is shown to help, especially for HelpSteer (Section 4.2). For limited context models (e.g., Alpaca-7B), this stage may need to be skipped or simplified.

- **Failure signatures:**
  1. **Empty or tiny Dc after filtering:** Indicates the model failed to generate any Pareto-superior responses. Check implicit reward model calibration, sampling quality, and refinement effectiveness.
  2. **Performance degradation on one objective:** May result from imbalanced filtering (e.g., responses that boost helpfulness at harmlessness's expense slip through) or poor preference weight balance during sampling.
  3. **Catastrophic forgetting:** If the NLL loss weight α is too low, the model may lose its original capabilities. Conversely, if α is too high, it may resist learning from new preferences.

- **First 3 experiments:**
  1. **Reproduce the conflict ratio experiment (Section 2):** Train DPO soups or MODPO on subsampled datasets with controlled conflict ratios (0%, 30%, 60%, 90%). Verify that performance degrades as conflicts increase, confirming the problem statement.
  2. **Run an ablation on filtering:** Train SIPO with and without the filtering stage (using random subsampling instead). Measure the impact on Pareto frontier metrics and average response rewards to quantify filtering's contribution.
  3. **Inspect a sample of generated yc responses:** For a handful of cases, manually compare the original responses (y1, y-1) with the self-generated Pareto-optimal response yc. Use the ground-truth reward models to check if yc truly scores higher on all objectives. This sanity checks the core assumption of the self-improvement loop.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does optimizing separate responses for individual objectives (e.g., one for helpfulness, one for safety) outperform SIPO's single Pareto-optimal response approach?
- Basis in paper: [explicit] The Limitations section explicitly proposes an alternative: sampling distinct responses $c$ and $d$ where each optimizes a specific conflicting objective, rather than finding a single response $y_c$ that dominates all.
- Why unresolved: The authors hypothesize that single Pareto-optimal responses are superior but have not empirically compared this against a strategy that aligns separate models on decomposed objective pairs.
- What evidence would resolve it: A comparative experiment measuring the Pareto Front when training on SIPO data versus data constructed using the separate objective optimization strategy.

### Open Question 2
- Question: Can SIPO be improved by distilling Pareto-optimal responses from stronger external LLMs instead of relying on self-improvement?
- Basis in paper: [explicit] The Limitations section lists "Distilling Pareto-optimal response from stronger LLMs" as a distinct future direction to improve the Pareto Front.
- Why unresolved: The current framework relies on the backbone model's capability; it is unknown if external stronger models (teachers) provide superior "gold" references for resolving preference conflicts compared to the model's own self-generated refinements.
- What evidence would resolve it: Replacing the self-generated $y_c$ with responses from a stronger teacher model (e.g., GPT-4) and evaluating the resulting alignment performance.

### Open Question 3
- Question: Does using the DPO policy's implicit rewards for filtering introduce bias compared to using separate proxy reward models?
- Basis in paper: [inferred] The method uses the log-probability of the policy itself (Eq. 6) to estimate rewards for filtering, assuming these align with the true preferences.
- Why unresolved: Implicit rewards derived from the policy may be poorly calibrated or susceptible to "reward hacking" compared to explicit, trained proxy reward models, potentially leading to the selection of sub-optimal responses.
- What evidence would resolve it: An ablation study replacing the implicit reward filter with an ensemble of external proxy reward models to determine if selection quality and final performance improve.

## Limitations
- The self-improvement loop relies on the model's ability to generate genuinely Pareto-superior responses, but there is no ground-truth validation of the implicit reward models used for filtering.
- The computational cost of SIPO (81 GPU hours vs. 30h for MODPO) may limit scalability to larger models or more complex objectives.
- The framework assumes the initial DPO models are well-aligned to individual objectives, but the quality of these alignments is not independently verified.

## Confidence
- **High Confidence:** The experimental results on two datasets showing improved Pareto Fronts and average reward gains are well-documented and reproducible.
- **Medium Confidence:** The mechanism of resolving preference conflicts through self-generated Pareto-optimal responses is logically sound, but its effectiveness depends on the quality of the implicit reward models.
- **Low Confidence:** The generalizability of SIPO to other multi-objective alignment tasks (e.g., beyond helpfulness and harmlessness) is not demonstrated.

## Next Checks
1. **Validate Implicit Reward Models:** For a subset of cases, compare the implicit rewards used for filtering with ground-truth rewards to assess their accuracy and calibration.
2. **Test Scalability:** Run SIPO on a larger model (e.g., LLaMA-2-13B) and measure the impact on computational cost and Pareto Front improvement.
3. **Explore Other Objectives:** Apply SIPO to a dataset with different objectives (e.g., creativity vs. accuracy) to test its generalizability.