---
ver: rpa2
title: Self-correcting Reward Shaping via Language Models for Reinforcement Learning
  Agents in Games
arxiv_id: '2506.23626'
source_url: https://arxiv.org/abs/2506.23626
tags:
- reward
- agent
- iteration
- performance
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents a method to automate reward shaping for RL\
  \ agents using LMs in an iterative loop setup. The core idea is to let an LM iteratively\
  \ adjust the weights of a modular reward function to better align an RL agent\u2019\
  s behavior with a user-defined objective."
---

# Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games

## Quick Facts
- arXiv ID: 2506.23626
- Source URL: https://arxiv.org/abs/2506.23626
- Reference count: 22
- Primary result: LM-guided iterative reward shaping improved racing agent success from 12.4% to 80.4% in 5 iterations

## Executive Summary
This paper presents a method to automate reward shaping for RL agents using language models in an iterative loop. The core idea is to let an LM iteratively adjust the weights of a modular reward function to better align an RL agent's behavior with a user-defined objective. Experiments in a racing task show that this method significantly improves agent performance, reaching 80.4% success rate after five iterations compared to a baseline of 12.4%. The approach is shown to be more accessible than manual reward tuning by experts while achieving competitive results.

## Method Summary
The method uses an iterative loop where a language model proposes reward weights based on environment descriptions, user objectives, and performance statistics from prior training runs. The reward function is a linear combination of modular features (speed, off-road penalty, etc.) with tunable weights. An RL agent (PPO) is trained using these weights, evaluated across 50 episodes and 5 seeds, and the resulting metrics are fed back to the LM for the next iteration. This closed-loop process allows the LM to self-correct and progressively improve reward functions without requiring manual tuning. The LM operates solely on textual summaries of performance metrics rather than visual observations.

## Key Results
- Starting from 12.4% success rate, LM-guided agent reached 80.4% success after five iterations
- Average speed increased from 121.3 to 135.2 km/h while off-road behavior dropped from 83.2% to 14.8%
- Comparison with human expert showed LM achieved strong results earlier without manual intervention
- Performance gains were consistent across iterations with diminishing returns beyond five iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative feedback enables the LM to progressively refine reward weights toward behavioral alignment
- Mechanism: The LM receives environment description, full history of prior weight vectors, and performance statistics from prior evaluations, allowing it to reason about the mapping between weight changes and observed outcomes
- Core assumption: The LM can infer approximate causal relationships between scalar weight modifications and aggregate behavioral metrics from text summaries alone
- Evidence anchors: [abstract] "This closed-loop process allows the LM to self-correct and refine its output over time" and [Section 3.1] "The LM refines its suggested weights based on both the raw reward definitions and the history of performance data"
- Break condition: If performance metrics are noisy, uninformative, or poorly aligned with the true objective, the LM's feedback signal degrades and weight updates may oscillate or diverge

### Mechanism 2
- Claim: Decomposing the reward into a linear combination of modular features makes the optimization space tractable for LM-based search
- Mechanism: The reward function takes the form r_t = Σ_k w_k * f_k(s_t, a_t), where each f_k is a fixed feature and w_k is a tunable scalar, limiting the search space to weight vectors
- Core assumption: The pre-defined feature functions are sufficient to express the target behavior; the LM does not need to discover new reward components
- Evidence anchors: [Section 3.1] "This abstraction separates what aspects of behavior are measured from how strongly they are enforced" and [Section 4.1] lists specific components
- Break condition: If the target behavior requires features not represented in f_k, the LM cannot compensate through weight tuning alone

### Mechanism 3
- Claim: Textual statistical summaries provide sufficient signal for LM weight adjustment in this domain
- Mechanism: After each training run, the system aggregates metrics across episodes and seeds, formats them as text, and appends to the LM prompt for the next iteration
- Core assumption: Aggregate scalar metrics capture the relevant behavioral dimensions; the LM does not need trajectory-level detail or visual observation
- Evidence anchors: [Section 5] "Starting from a baseline success rate of just 12.4%... it jumps to 73.6% in the very first feedback iteration" and [Section 6] acknowledges the limitation of text-only feedback
- Break condition: If behaviors are nuanced not captured by the selected metrics, the LM lacks the feedback needed to refine toward those dimensions

## Foundational Learning

- Concept: **Reward shaping in RL**
  - Why needed here: The entire method operates on tuning reward function weights; without understanding how rewards drive policy learning, the loop's purpose is opaque
  - Quick check question: Can you explain why a poorly shaped reward can lead to unintended agent behaviors even if the task is solvable?

- Concept: **Proximal Policy Optimization (PPO)**
  - Why needed here: The paper uses PPO for RL training; understanding its stability properties helps diagnose why the training component is reliable enough for iterative tuning
  - Quick check question: What role does the clipping parameter ε play in preventing large policy updates?

- Concept: **LM prompting with structured context**
  - Why needed here: The method relies on carefully constructed prompts containing environment descriptions, weight history, and statistics; prompt design directly affects output quality
  - Quick check question: How might the order or formatting of historical weight-performance pairs influence the LM's next proposal?

## Architecture Onboarding

- Component map: Prompt constructor → Language Model → RL training loop → Evaluation runner → Statistics aggregator → Back to prompt constructor
- Critical path: Prompt construction → LM weight proposal → RL training → Evaluation → Statistics aggregation → Back to prompt construction (repeat for T=5 iterations)
- Design tradeoffs:
  - Iteration count (T=5): Paper reports diminishing returns beyond 5; fewer iterations may under-exploit feedback, more may waste compute
  - Evaluation budget (50 episodes, 5 seeds): Balances statistical reliability against training cost; lower budgets increase noise in feedback signal
  - Text-only feedback: Avoids complexity of visual processing but limits behavioral observability; future work may integrate VLMs
  - Fixed feature functions (f_k): Simplifies search but requires domain knowledge to define relevant reward components upfront
- Failure signatures:
  - Oscillating weights across iterations: Suggests metrics are noisy or LM is overcorrecting; consider reducing adjustment magnitude or increasing evaluation episodes
  - Stagnant performance after early gains: May indicate local optimum; the LM lacks exploration incentives in weight space
  - Degraded performance in later iterations: Visible in human expert's iteration 5 drop; suggests instability from aggressive weight changes
  - High variance across seeds: Indicates policy sensitivity to random initialization; may need longer training or more seeds for stable feedback
- First 3 experiments:
  1. Baseline replication: Implement the 5-iteration loop on the provided racing environment; verify you can reproduce the LM's improvement trajectory (12.4% → ~80% success)
  2. Ablation on iteration count: Run T=1, 3, 5, 7 iterations to validate the paper's claim of diminishing returns beyond 5; plot success rate vs. iteration
  3. Metric sensitivity test: Remove one metric (e.g., off-road %) from the feedback prompt and measure impact on final performance; this tests whether all metrics are necessary or if some are redundant

## Open Questions the Paper Calls Out
None

## Limitations
- Environment specificity: The exact Unity ML-Agents racing environment is not publicly available, requiring assumptions about track geometry and reward function implementation
- Metric sufficiency: The paper acknowledges that text-only scalar metrics may miss nuanced behavioral patterns, but does not quantify what proportion of relevant behaviors are actually captured
- Model accessibility: OpenAI's o3 model is used, but API availability and cost may limit replication; the paper doesn't report computational overhead per iteration

## Confidence
- **High confidence**: The core mechanism of iterative LM-driven reward weight refinement is clearly described and experimentally validated in the racing domain
- **Medium confidence**: Claims about the approach being "more accessible and scalable" than human expert tuning are supported by the comparison but lack quantitative metrics on development time or expertise required
- **Low confidence**: The generality of the method across domains is asserted but not demonstrated; the paper focuses exclusively on one racing task

## Next Checks
1. Cross-domain transferability: Apply the iterative LM feedback loop to a different RL environment (e.g., LunarLander or Atari) to test whether the approach generalizes beyond the racing domain
2. Human-in-the-loop comparison: Conduct a controlled study where human experts and the LM iteratively tune rewards on the same task, measuring both final performance and tuning time/effort
3. Behavior trajectory analysis: Record and analyze agent trajectories across iterations to identify what specific driving behaviors the LM is optimizing for, and whether these align with the stated objectives