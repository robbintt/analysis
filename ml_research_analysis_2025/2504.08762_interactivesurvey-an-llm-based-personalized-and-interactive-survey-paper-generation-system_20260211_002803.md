---
ver: rpa2
title: 'InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper
  Generation System'
arxiv_id: '2504.08762'
source_url: https://arxiv.org/abs/2504.08762
tags:
- survey
- papers
- content
- generation
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: InteractiveSurvey is an LLM-based system that generates structured,
  multi-modal survey papers with personalized reference categorization and interactive
  refinement capabilities. The system automatically searches and parses reference
  papers from arXiv or user uploads, clusters them based on user-defined criteria,
  and generates survey content through a bottom-up approach with iterative refinement
  options.
---

# InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System

## Quick Facts
- arXiv ID: 2504.08762
- Source URL: https://arxiv.org/abs/2504.08762
- Reference count: 40
- InteractiveSurvey generates structured survey papers with personalized categorization and interactive refinement, outperforming mainstream LLMs and state-of-the-art methods in content quality metrics.

## Executive Summary
InteractiveSurvey is a novel LLM-based system that generates structured, multi-modal survey papers from multiple reference papers with personalized categorization and interactive refinement capabilities. The system automatically searches and parses reference papers from arXiv or user uploads, clusters them based on user-defined criteria, and generates survey content through a bottom-up approach with iterative refinement options. Evaluations show InteractiveSurvey outperforms mainstream LLMs (GPT-4o, DeepSeek-R1, Qwen2.5-72b) in content quality metrics with average scores of 4.56, 4.59, and 4.88 respectively, and surpasses state-of-the-art methods AutoSurvey and SurveyX with average scores of 4.61 and 4.60. The system generates high-quality survey papers (approximately 50 references) in 35 minutes on average, and achieves a 84.4/100 usability score in user studies.

## Method Summary
InteractiveSurvey processes user-provided topics to automatically search for relevant papers on arXiv or accepts user-uploaded PDFs. The system uses MinerU to parse PDFs into structured markdown, stores content in a vector database with fixed-length chunking, and employs Hypothetical Document Embeddings (HyDE) for semantic clustering based on user-defined categorization criteria. The generation pipeline constructs a three-level outline, synthesizes content bottom-up using retrieval-augmented generation (RAG) for each sub-section, and incorporates figures, tables, and citations. The system supports interactive refinement of intermediate outputs including reference clustering, outline structure, and generated content before final export in multiple formats.

## Key Results
- Outperforms mainstream LLMs (GPT-4o, DeepSeek-R1, Qwen2.5-72b) in content quality metrics with average scores of 4.56, 4.59, and 4.88 respectively
- Surpasses state-of-the-art methods AutoSurvey and SurveyX with average scores of 4.61 and 4.60
- Generates high-quality survey papers (approximately 50 references) in 35 minutes on average
- Achieves 84.4/100 usability score in user studies, placing it in the A+ tier for usability

## Why This Works (Mechanism)

### Mechanism 1
Retrieval-augmented generation with hierarchical chunking enables synthesis of document collections exceeding LLM context windows. Each reference paper is split into fixed-length chunks, embedded, and stored in a vector database. During content generation, sub-section titles serve as queries to retrieve relevant chunks, which are then fed to the LLM for synthesis. This bypasses the ~10K tokens per paper limitation. If reference papers contain dense interdependencies (e.g., cross-references, methodological chains), chunk-level retrieval may miss structural relationships across documents.

### Mechanism 2
HyDE (Hypothetical Document Embeddings) improves categorization-context retrieval by generating pseudo-descriptions as query proxies. For a user-specified categorization criterion k, the LLM generates 10 hypothetical descriptions of what relevant content might look like. These HyDE queries retrieve chunks via semantic matching, which are then synthesized into per-reference descriptions for clustering. If the categorization criterion is highly domain-specific or novel, the LLM may generate irrelevant hypothetical descriptions, leading to noisy retrieval.

### Mechanism 3
Interactive intermediate-state modification enables personalized survey outputs and improves user satisfaction. The system exposes intermediate outputs—reference clustering, outline hierarchy, and generated content—for user modification. Users can move references between clusters, edit outline structure, and revise text/figures before final export. If users lack domain expertise or if the interface imposes high cognitive load, interactive refinement may introduce errors or reduce efficiency.

## Foundational Learning

- **Concept: Retrieval-Augmented Generation (RAG)**
  - Why needed here: Core mechanism for processing ~50 papers (~500K+ tokens total) through chunk-level retrieval instead of full-context ingestion.
  - Quick check question: Can you explain why semantic chunk retrieval alone might miss cross-document argument structures?

- **Concept: Hypothetical Document Embeddings (HyDE)**
  - Why needed here: Enables categorization-context retrieval before actual content is analyzed; critical for the personalized reference clustering step.
  - Quick check question: What failure mode would you expect if HyDE queries are generated for a criterion the LLM has no prior knowledge of?

- **Concept: Agglomerative Hierarchical Clustering with HDBSCAN**
  - Why needed here: Used for semantic clustering of reference descriptions; determines the survey's structural organization.
  - Quick check question: Why might fixed cluster numbers fail across diverse survey topics, and how does silhouette-score selection address this?

## Architecture Onboarding

- **Component map:**
  Input Layer: Topic T → LLM generates description → Extract themes/entities/concepts → Construct arXiv query
  Ingestion Layer: arXiv search + user uploads → MinerU parsing → Metadata extraction → Vector database chunking
  Categorization Layer: HyDE query generation → Chunk retrieval → Per-reference description generation → UMAP dimensionality reduction → HDBSCAN clustering → Cluster naming
  Generation Layer: Hierarchical outline construction → Bottom-up section generation (RAG per sub-section) → Summary synthesis → Pre-defined sections (Abstract, Introduction, etc.)
  Output Layer: Citation injection → Figure/table matching → Multi-format export (PDF, Markdown, LaTeX)

- **Critical path:** Topic input → Reference retrieval (MIN_REF threshold matters) → HyDE-based categorization → Outline generation → Bottom-up content synthesis. Delays in reference parsing (~30% of total time per Figure 5) will dominate end-to-end latency.

- **Design tradeoffs:**
  - Quality vs. latency: Reference parsing and categorization are GPU-intensive (RTX 3090); faster hardware could reduce ~35 min to <30 min but with higher infrastructure cost.
  - Automation vs. control: Full automation produces fixed outputs; interactive refinement improves personalization but requires user time and domain expertise.
  - Chunk size vs. retrieval precision: Smaller chunks improve precision but may fragment context; paper does not specify chunk size, which is a tunable hyperparameter.

- **Failure signatures:**
  - Empty or sparse reference retrieval: If arXiv query is over-constrained, MIN_REF threshold triggers iterative loosening; monitor loop count against MAX_TRY.
  - Clustering collapse: If all references cluster into one group, silhouette scores will be low; check if categorization criterion is too broad or HyDE queries are off-topic.
  - Citation over/under-injection: Adaptive threshold τ adjusts based on similarity score distribution; if τ is too low, survey becomes citation-dense; if too high, claims lack attribution.
  - Format drift in outline generation: Less powerful LLMs may fail to follow hierarchical structure; the paper uses template-filling prompts as mitigation.

- **First 3 experiments:**
  1. Ablate HyDE: Replace HyDE queries with direct criterion-based retrieval and compare clustering coherence (silhouette scores) and final survey quality scores.
  2. Latency profiling: Measure time distribution across pipeline stages with different reference counts (20, 50, 100) to identify scaling bottlenecks.
  3. User modification impact: Track which intermediate states users modify most frequently (categorization vs. outline vs. content) and correlate modification patterns with final quality scores to validate the interactive-refinement hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the system be effectively adapted to support multilingual survey generation?
- **Basis in paper:** The conclusion states, "we plan to explore multilingual survey generation to broaden the system’s applicability."
- **Why unresolved:** The current implementation relies on English-centric parsing (MinerU) and retrieval mechanisms (HyDE) optimized for arXiv content, which may not transfer directly to non-English corpora or cross-lingual synthesis.
- **What evidence would resolve it:** Successful generation of coherent surveys using reference papers spanning multiple languages, evaluated by native speakers for fluency and semantic accuracy.

### Open Question 2
- **Question:** What is the trade-off between the system's automated "bottom-up" generation and the factual accuracy verified by domain experts?
- **Basis in paper:** Section 4.1 relies entirely on "LLMs as judges" for content quality (coverage, structure, relevance), while Section 4.3 evaluates usability rather than factual correctness.
- **Why unresolved:** LLM-based evaluation metrics correlate weakly with factual grounding and hallucination rates; therefore, high scores in structure/relevance do not guarantee the scientific accuracy of the synthesized content.
- **What evidence would resolve it:** A human expert study specifically evaluating the generated surveys for factual errors, citation faithfulness, and hallucination rates compared to human-written surveys.

### Open Question 3
- **Question:** How does the system's performance and coherence scale when processing reference lists significantly larger than the evaluated average of 45 papers?
- **Basis in paper:** Section 3.1 mentions a `MAX_REF` truncation, and Section 4.2 highlights that "Reference Parsing" and "Personalized Reference Categorization" are the most time-intensive steps.
- **Why unresolved:** It is unclear if the hierarchical clustering and outline generation mechanisms maintain logical consistency or succumb to topic drift when synthesizing hundreds of documents rather than dozens.
- **What evidence would resolve it:** Benchmarks showing time cost and content quality (coherence scores) when generating surveys from 100, 200, and 500 reference papers.

## Limitations

- The effectiveness of HyDE queries for abstract categorization criteria remains uncertain, particularly for domain-specific or novel topics where the LLM may lack sufficient training context.
- The chunk-based RAG approach may struggle with cross-document dependencies and argument structures that span multiple papers.
- The usability study (84.4/100 SUS score) comes from a small user sample (10 participants), limiting generalizability of the interactive refinement benefits.

## Confidence

- **High Confidence:** The overall system architecture (RAG + clustering + generation pipeline) is well-established in the literature and the quantitative performance metrics (coverage, structure, relevance scores of 4.56-4.88) are empirically measured.
- **Medium Confidence:** The superiority claims over baseline systems (SurveyForge, SurveyX, AutoSurvey) are supported by evaluation scores, but the differences may be partially attributable to LLM choice rather than architectural innovations.
- **Low Confidence:** The scalability claims for processing ~50 papers are based on a single hardware configuration (RTX 3090), and the time efficiency benefits may not generalize to larger reference sets or less powerful hardware.

## Next Checks

1. **Ablation Study on HyDE:** Replace HyDE-based categorization with direct semantic search using the categorization criterion as query. Compare clustering coherence (silhouette scores) and final survey quality scores to isolate HyDE's contribution.

2. **Cross-Document Dependency Analysis:** Design test cases where reference papers contain explicit cross-references or methodological chains. Measure whether chunk-level RAG misses these relationships compared to full-context processing.

3. **User Expertise Validation:** Conduct a controlled study comparing survey quality when generated by domain experts vs. non-experts using the interactive refinement interface. Correlate modification patterns with final quality scores to validate the interactive-refinement hypothesis.