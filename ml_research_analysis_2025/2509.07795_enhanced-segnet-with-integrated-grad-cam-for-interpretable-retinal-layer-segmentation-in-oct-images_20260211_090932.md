---
ver: rpa2
title: Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer Segmentation
  in OCT Images
arxiv_id: '2509.07795'
source_url: https://arxiv.org/abs/2509.07795
tags:
- segmentation
- retinal
- layers
- layer
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes an enhanced SegNet-based deep learning framework
  for automated and interpretable retinal layer segmentation in OCT images. The method
  introduces architectural improvements including hybrid pooling strategies and a
  hybrid loss function combining categorical cross-entropy and Dice loss to address
  class imbalance and enhance thin layer segmentation.
---

# Enhanced SegNet with Integrated Grad-CAM for Interpretable Retinal Layer Segmentation in OCT Images

## Quick Facts
- arXiv ID: 2509.07795
- Source URL: https://arxiv.org/abs/2509.07795
- Reference count: 39
- This study proposes an enhanced SegNet-based deep learning framework for automated and interpretable retinal layer segmentation in OCT images.

## Executive Summary
This study proposes an enhanced SegNet-based deep learning framework for automated and interpretable retinal layer segmentation in OCT images. The method introduces architectural improvements including hybrid pooling strategies and a hybrid loss function combining categorical cross-entropy and Dice loss to address class imbalance and enhance thin layer segmentation. Gradient-weighted Class Activation Mapping (Grad-CAM) is integrated to provide visual explanations of model decisions, enabling clinical validation. The framework was trained and validated on the Duke OCT dataset, achieving 95.77% validation accuracy, Dice coefficient of 0.9446, and Jaccard Index of 0.8951. Class-wise analysis showed strong performance for most layers, with challenges remaining for thinner boundaries. Grad-CAM visualizations successfully highlighted anatomically relevant regions, aligning segmentation with clinical biomarkers and improving transparency. The approach bridges the gap between accuracy and interpretability, offering a promising solution for standardizing OCT analysis and fostering trust in AI-driven ophthalmic tools.

## Method Summary
The method employs a modified SegNet architecture with a hybrid loss function combining categorical cross-entropy and Dice loss to address class imbalance in retinal layer segmentation. The encoder uses VGG-style convolutional blocks with max-pooling, storing pooling indices for precise upsampling in the decoder. The decoder reconstructs spatial details using these indices combined with transposed convolutions and skip connections. A hybrid loss function (CCE + 0.5 × Dice) balances per-pixel classification with region-level overlap. Grad-CAM is integrated by computing gradients of class scores with respect to final convolutional feature maps to generate class-specific heatmaps. The model was trained on the Duke OCT dataset (220 images) using Adam optimizer (lr=0.001) for 100 epochs with a batch size of 32.

## Key Results
- Achieved 95.77% validation accuracy, Dice coefficient of 0.9446, and Jaccard Index of 0.8951 on the Duke OCT dataset
- Class-wise analysis showed IoU scores ranging from 0.68-0.94, with thin layers (Classes 3-4) showing lower performance (IoU 0.68-0.71)
- Grad-CAM visualizations successfully highlighted anatomically relevant regions, aligning segmentation with clinical biomarkers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid loss function (CCE + Dice) mitigates class imbalance and improves segmentation of thin retinal layers compared to standalone CCE.
- Mechanism: Categorical cross-entropy enforces per-pixel classification correctness, while Dice loss optimizes region-level overlap. The weighted combination (λ = 0.5) balances boundary precision with structural coherence, particularly for underrepresented classes.
- Core assumption: Class imbalance in retinal layers (thin vs. thick) causes CCE-only models to favor dominant classes; Dice's overlap metric counterweights this by penalizing poor overlap regardless of class size.
- Evidence anchors: [abstract] "hybrid loss function combining categorical cross-entropy and Dice loss to address class imbalance and enhance thin layer segmentation"; [section 3.3] "Dice loss was added. Dice loss is derived from the Dice coefficient, which measures the overlap between the predicted mask A and ground truth mask B"; [corpus] Neighbor papers confirm class imbalance as a persistent challenge in OCT segmentation.

### Mechanism 2
- Claim: Storing and reusing max-pooling indices during decoding preserves spatial resolution for fine boundary delineation, conditional on the quality of encoder features.
- Mechanism: SegNet's encoder records argmax locations during pooling. The decoder uses these indices for upsampling (via sparse reconstruction) rather than learning deconvolution kernels, reducing parameter count while retaining edge information critical for thin retinal layers.
- Core assumption: Preserved pooling indices carry sufficient spatial detail for reconstruction; this assumes encoder feature quality is not degraded by speckle noise or pathological distortion.
- Evidence anchors: [section 3.2] "SegNet stores pooling indices during downsampling. This enables an accurate reassembly of spatial details during the upsampling"; [section 3.2.2] "The decoder uses the pooling indices saved during the encoder phase as part of the upsampling, rather than standard interpolation-based upsampling techniques"; [corpus] No direct corpus evidence confirms this mechanism for OCT specifically.

### Mechanism 3
- Claim: Integrated Grad-CAM provides class-specific visual explanations that highlight anatomically relevant regions, contingent on gradient flow quality through the decoder.
- Mechanism: Grad-CAM computes gradients of each class score (Yc) with respect to final convolutional feature maps (Ak), weights channels by importance (αkc), and generates a spatial heatmap via ReLU-weighted summation. Applied per-class, this reveals which image regions influenced each layer's segmentation.
- Core assumption: The selected convolutional layer (e.g., Conv2d_20) captures semantically meaningful features; gradient flow is not obstructed by dead ReLUs or vanishing gradients.
- Evidence anchors: [abstract] "Grad-CAM visualizations successfully highlighted anatomically relevant regions, aligning segmentation with clinical biomarkers"; [section 3.7] "The Grad-CAM heatmap is then computed as: Lc_Grad-CAM = ReLU(Σk αkc Ak)"; [corpus] Related work confirms Grad-CAM applicability to segmentation, but OCT-specific validation remains limited.

## Foundational Learning

- Concept: Encoder-Decoder Architecture with Skip/Pooling Connections
  - Why needed here: SegNet's core design separates feature extraction (encoder) from spatial reconstruction (decoder); understanding this is prerequisite to modifying pooling strategies or interpreting Grad-CAM activations.
  - Quick check question: Can you explain why storing pooling indices (vs. learning deconvolution) affects both parameter count and boundary precision?

- Concept: Class Imbalance in Medical Segmentation
  - Why needed here: Retinal layers vary significantly in thickness; thin layers (e.g., RNFL) are underrepresented pixel-wise, biasing models toward thicker structures without explicit correction.
  - Quick check question: Given a dataset where Class 3 occupies 5% of pixels and Class 0 occupies 40%, would standard CCE equally penalize errors in both classes?

- Concept: Gradient-Based Attribution (Grad-CAM)
  - Why needed here: Grad-CAM requires computing gradients of output class scores w.r.t. intermediate feature maps; misunderstanding this can lead to incorrect layer selection or heatmap misinterpretation.
  - Quick check question: Why does Grad-CAM apply ReLU to the weighted sum of feature maps, and what would happen if this step were omitted?

## Architecture Onboarding

- Component map:
  Input: Grayscale OCT (256×256×1) -> Encoder: 5 blocks, each with 2× Conv2D (3×3, ReLU, same padding) + MaxPooling2D (2×2, stores indices); filter depths: 64→128→256→512→512 -> Decoder: 5 blocks, each with Conv2DTranspose (2×2 upsampling using stored indices) + concatenation (skip from encoder) + 2× Conv2D -> Output: Conv2D(8, 1×1) + Softmax → 8-class segmentation mask -> XAI module: Grad-CAM hooks on Conv2d_19 (refinement) and Conv2d_20 (final output)

- Critical path:
  1. Preprocessing: Normalize to [0,1], resize to 256×256, one-hot encode masks
  2. Forward pass: Encoder compresses to 8×8 feature maps; decoder reconstructs to 256×256 via pooling indices
  3. Loss computation: Hybrid L = LCCE + 0.5 × (1 − Dice)
  4. Grad-CAM generation: Select target layer → compute gradients → weight channels → generate heatmap

- Design tradeoffs:
  - SegNet vs. U-Net: SegNet uses pooling indices (fewer parameters, lower memory) vs. U-Net's learned upsampling (more flexible but heavier); choice prioritizes efficiency for clinical deployment
  - Filter depth capped at 512 (vs. 1024) to fit 16GB GPU; may limit capacity for complex pathology
  - No data augmentation used; may reduce robustness to device variation (15–25% performance drop reported in literature)

- Failure signatures:
  - Thin layer under-segmentation (Classes 3, 4): IoU <0.75, visible boundary blur in predictions → likely class imbalance or insufficient receptive field
  - Over-segmentation in high-variance regions: False positives in texture-heavy areas → Grad-CAM will show diffuse activation; consider attention mechanisms
  - Grad-CAM highlights artifacts: Heatmap focuses on image borders or noise → model learned spurious features; retrain with augmentation or cleaner data

- First 3 experiments:
  1. Baseline validation: Train vanilla SegNet with CCE-only loss on Duke OCT; compare Dice/IoU to hybrid loss version to quantify class imbalance impact
  2. Layer-wise Grad-CAM audit: Generate per-class heatmaps for all 8 layers on validation set; manually verify anatomical alignment (e.g., RNFL heatmap should focus on topmost layer)
  3. Resolution sensitivity: Re-train at 128×128 and 512×512 input resolution; measure thin-layer IoU change to assess pooling index robustness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the integration of transformer-based encoders or uncertainty-aware learning modules significantly improve segmentation performance on OCT scans with severe pathological distortions compared to the current CNN-based approach?
- Basis in paper: [explicit] The conclusion states, "transformer-based encoders or uncertainty-aware learning may further strengthen performance in pathological cases."
- Why unresolved: The current study utilized a modified SegNet (CNN) architecture, which, while effective on the Duke dataset, may not optimally handle the "pathological distortions" mentioned in the introduction without the global context provided by transformers.
- What evidence would resolve it: A comparative benchmark showing Dice coefficients and IoU scores on a dataset rich in pathologies (e.g., AMD/DME) against a transformer-based baseline.

### Open Question 2
- Question: Can specific architectural adjustments, such as class-weighted loss functions or boundary-aware losses, resolve the segmentation underperformance in thin, complex retinal layers (Classes 3 and 4)?
- Basis in paper: [explicit] The authors note "challenges remained for thinner layers" and suggest that "Addressing these limitations [Classes 3 & 4] using class-weighted loss functions or attention-based refinement could further improve segmentation accuracy."
- Why unresolved: The current hybrid loss function (CCE + Dice) achieved high overall scores but failed to equitably segment Classes 3 and 4, which scored lowest (IoU 0.71 and 0.68) due to boundary ambiguity.
- What evidence would resolve it: Ablation studies demonstrating a statistically significant increase in IoU for Classes 3 and 4 when Focal Loss or Active Contour Loss is substituted for the current hybrid loss.

### Open Question 3
- Question: To what extent does the model maintain segmentation accuracy when applied to multi-center datasets acquired from different OCT device manufacturers?
- Basis in paper: [explicit] The conclusion suggests "Future work should incorporate multi-center datasets to improve generalizability," acknowledging that training on the single-source Duke dataset limits robustness across different hardware.
- Why unresolved: The paper cites literature noting that models trained on one device often suffer 15–25% performance drops on others; this specific architecture has not yet been tested against that variability.
- What evidence would resolve it: Validation accuracy and Dice coefficients obtained by testing the trained model "out-of-the-box" on external datasets (e.g., Optos or Heidelberg Engineering datasets) without retraining.

## Limitations
- Decoder implementation combines pooling indices (SegNet) with skip connections and transposed convolutions (U-Net), which is ambiguously defined and limits reproducibility
- The exact Duke OCT dataset subset used is unspecified, preventing faithful reproduction of results
- No data augmentation was used, potentially reducing robustness to device variation and pathological cases
- The model's generalizability to pathological OCT scans or other imaging devices remains untested

## Confidence
- **High Confidence**: Basic model architecture (encoder blocks, filter depths, output layer) and preprocessing pipeline
- **Medium Confidence**: Hybrid loss formulation, validation metrics, and Grad-CAM integration mechanism
- **Low Confidence**: Decoder-specific implementation details (pooling index vs. skip connection fusion) and dataset versioning

## Next Checks
1. **Architectural Fidelity Audit**: Reconstruct the decoder using only pooling indices (pure SegNet) vs. the described hybrid approach; compare thin-layer IoU to isolate the contribution of each component
2. **Grad-CAM Clinical Alignment**: Generate per-class heatmaps for all 8 layers on a held-out pathological OCT set; have a clinician verify anatomical relevance of highlighted regions against OCT B-scan landmarks
3. **Cross-Device Robustness Test**: Evaluate the trained model on OCT images from a different device (e.g., Heidelberg Spectralis); quantify performance drop to assess device-specific overfitting