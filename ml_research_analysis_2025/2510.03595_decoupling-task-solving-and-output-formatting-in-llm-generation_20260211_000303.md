---
ver: rpa2
title: Decoupling Task-Solving and Output Formatting in LLM Generation
arxiv_id: '2510.03595'
source_url: https://arxiv.org/abs/2510.03595
tags:
- format
- deco-g
- generation
- task
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a decoding framework DECO-G that decouples
  format adherence from task solving in LLM generation. The method delegates format
  compliance to a separate tractable probabilistic model (HMM) while prompting the
  LLM with only task instructions.
---

# Decoupling Task-Solving and Output Formatting in LLM Generation

## Quick Facts
- arXiv ID: 2510.03595
- Source URL: https://arxiv.org/abs/2510.03595
- Reference count: 40
- Key outcome: DECO-G achieves 1.0% to 6.0% relative performance gains over baseline methods while guaranteeing 100% format compliance

## Executive Summary
This paper proposes DECO-G, a decoding framework that decouples format adherence from task-solving in LLM generation. The method uses a tractable probabilistic model (HMM) to estimate format compliance likelihood while prompting the LLM with only task instructions. At each decoding step, DECO-G combines LLM token probabilities with the HMM's format compliance likelihood, achieving both high task accuracy and perfect format compliance across three diverse tasks.

## Method Summary
DECO-G distills an HMM from instruction-response pairs (1 million completions from 1000 unique instructions) and combines it with the LLM's token probabilities using Bayes' rule reweighting. The method constructs a trie-based DFA for format constraints and applies top-k hidden state pruning for efficiency. During decoding, the LLM receives only task instructions while the FEM provides format guidance through probability reweighting (P_DECO-G ∝ P_LM × P_FEM^γ).

## Key Results
- GSM8k mathematical reasoning: DECO-G achieves 100% format compliance with accuracy gains of 1.0-6.0% over structured baselines
- LLM-as-a-judge (SummEval): DECO-G improves Spearman correlation by 3.1-4.5% and Kendall-Tau by 3.6-5.5% over NL baselines
- Event argument extraction (ACE05-EN): DECO-G improves AI F1 by 2.8-3.2% and AC+ F1 by 1.6-2.0% over NL baselines

## Why This Works (Mechanism)

### Mechanism 1: Probabilistic Decoupling via Bayes' Rule Application
- Claim: Separating task-solving from format constraints through probability reweighting improves both accuracy and format compliance
- Mechanism: At each decoding step, DECO-G combines next token probabilities from the LLM with the TPM calculated format compliance likelihood to form the output probability
- Core assumption: Format compliance and task-solving are sufficiently independent that separate models can optimize each without significant mutual interference
- Evidence anchors: [abstract] and [Section 3.3] describe the combination mechanism; corpus shows format impact but doesn't replicate decoupling
- Break condition: When task reasoning itself depends intrinsically on specific output structures

### Mechanism 2: Instruction-Aware HMM Distillation
- Claim: HMM trained on instruction-response pairs provides more accurate format guidance than unconditionally distilled HMMs
- Mechanism: Training the HMM on 1 million completions from 1,000 unique instructions captures task-oriented behavior that emerges after instruction fine-tuning
- Core assumption: The LLM's output distribution when conditioned on task prompts differs fundamentally from its unconditional distribution
- Evidence anchors: [Section 3.1] contrasts instruction-aware vs. unconditional distillation; CtrlG with unconditioned HMM achieved only 60.6% accuracy on GSM8k
- Break condition: When the instruction domain shifts significantly from the distillation dataset characteristics

### Mechanism 3: Soft Steering via Probability Reweighting vs. Hard Token Forcing
- Claim: Soft control through probability reweighting produces more coherent outputs than hard constraint enforcement methods
- Mechanism: Rather than forcing specific tokens, DECO-G estimates the probability that a token leads to eventual format compliance and softly boosts promising tokens
- Core assumption: Soft guidance preserves reasoning coherence better than hard token forcing
- Evidence anchors: [Section 5.1] contrasts with regex-structured generation; [Table 1] shows structured generation guarantees format but shows lower accuracy
- Break condition: When format constraints are so rigid that only one valid token exists at each position

## Foundational Learning

- **Concept: Hidden Markov Models (HMMs) for tractable sequence modeling**
  - Why needed here: The FEM uses HMMs to efficiently estimate format compliance by computing marginal probabilities over all future sequences—a task intractable for LLMs
  - Quick check question: Can you explain why computing P(α|x_<t) is tractable with an HMM but intractable with an LLM?

- **Concept: Deterministic Finite Automata (DFA) for constraint formalization**
  - Why needed here: Format constraints must be encoded as DFAs with pivot (fixed text) and wildcard (variable-length slots) components
  - Quick check question: How would you represent "output must contain 'The final answer is' followed by 1–10 tokens representing a number" as a DFA with pivot and wildcard?

- **Concept: Bayes' rule decomposition for controllable generation**
  - Why needed here: The core framework rewrites P(x_t|x_<t, α) = P_LM(x_t|x_<t) × [P_LM(α|x_t, x_<t) / P_LM(α|x_<t)]
  - Quick check question: What does the γ hyperparameter control, and why might models with lower entropy (like Qwen) need higher γ values?

## Architecture Onboarding

- **Component map**: 
  - LLM (task inference) -> FEM (HMM + DFA) -> Probability Combiner -> Output
  - Offline: Distill HMM from LLM instruction-response pairs -> Build DFA via trie builder -> Compute P_FEM via HMM forward pass + DFA marginalization
  - Inference: LLM computes P_LM -> FEM computes P_FEM -> Combine with γ -> Apply pruning -> Sample

- **Critical path**:
  1. **Offline**: Distill HMM from LLM using instruction-response pairs (~1M completions, 1 GPU hour training)
  2. **Offline**: Convert format template to DFA via trie builder
  3. **Inference per token**: Compute LLM logits → compute FEM compliance via HMM forward pass + DFA marginalization → combine with γ → sample

- **Design tradeoffs**:
  - **γ strength**: Default γ=1 works for Llama; Qwen's peaked distribution requires γ=2
  - **Pruning threshold k**: k=200 retains 97.8–99.1% probability mass with 13× FLOPs reduction
  - **HMM size h=4096**: Larger h improves distribution fidelity but increases overhead

- **Failure signatures**:
  - **100% format, low accuracy** → γ too high, over-constraining reasoning
  - **Format violations** → γ too low for peaked distributions; check entropy analysis
  - **Incoherent output with forced phrases** → unconditioned HMM distillation (CtrlG failure mode)
  - **Excessive latency** → pruning disabled or k too high

- **First 3 experiments**:
  1. Reproduce GSM8k Table 1: Compare NL, JSON, NL-S, JSON-S, DECO-G on Llama-3.1-8B-Instruct; verify 100% format compliance and accuracy gains
  2. Ablate instruction-aware distillation: Train HMM on unconditional LLM samples vs. instruction-response pairs; measure accuracy delta on reasoning task
  3. γ sensitivity sweep: On held-out GSM8k validation, sweep γ ∈ {0.5, 1.0, 1.5, 2.0, 3.0}; plot accuracy vs. format compliance curve for Llama and Qwen

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a universal or cross-architecture HMM be developed that generalizes across different LLM families without requiring per-model distillation?
- Basis in paper: [explicit] Section A states "the HMM used to estimate format satisfaction rate is specific to an LLM, meaning that one has to distill a new HMM when switching to a different LLM."
- Why unresolved: Current approach requires one million completions and 57 GPU hours per model; no investigation into transferability or shared representations across models
- What evidence would resolve it: Experiments showing an HMM distilled from one model family achieving comparable steering performance on another family without retraining

### Open Question 2
- Question: Can the optimal steering strength (γ) be automatically determined based on measurable properties of the LLM's token distribution?
- Basis in paper: [explicit] Section A notes "finding the optimal hyperparameter γ for LLMs with highly peaked token distributions may require empirical explorations." Section 5.2 shows Qwen requires γ=2 while Llama requires γ=1
- Why unresolved: Authors identify entropy differences but provide no principled method for γ selection—only empirical tuning
- What evidence would resolve it: A theoretical or empirical relationship between distribution properties (e.g., entropy, skewness) and optimal γ validated across multiple model families

### Open Question 3
- Question: How does DECO-G's performance scale with increasingly complex format constraints and larger DFA state spaces?
- Basis in paper: [explicit] Section A states "Complex format constraints, converted to larger DFA, are thus likely to increase generation runtime" with O(|E_DFA| × h²) complexity
- Why unresolved: Experiments only cover three tasks with relatively simple templates; no systematic study of complexity-accuracy-latency tradeoffs
- What evidence would resolve it: Controlled experiments varying DFA size and measuring accuracy degradation and latency increases

## Limitations

- The FEM relies on algorithmic details (flexible trie-based DFA construction, backward recurrence) deferred to prior work, making exact reproduction challenging
- The evaluation scope remains narrow, covering only three tasks that may not represent the full spectrum of format-sensitive generation scenarios
- The claim that instruction-aware distillation "requires no extra human effort" assumes availability of instruction-response datasets, which may not hold for specialized domains

## Confidence

**High Confidence (Level 1)**: The core mechanism of decoupling task-solving from format adherence through probability reweighting is theoretically sound and empirically validated. The claim that soft steering via P_DECO-G ∝ P_LM × P_FEM^γ produces more coherent outputs than hard token forcing is supported by comparative results and qualitative analysis.

**Medium Confidence (Level 2)**: The instruction-aware HMM distillation improvement over unconditioned HMMs is demonstrated on GSM8k, but the corpus lacks direct ablation studies comparing instruction-aware vs. unconditional distillation head-to-head.

**Low Confidence (Level 3)**: The assertion that DECO-G's format integration is "more natural than forcing LLM to generate certain phrases" relies on subjective interpretation of output quality. The paper does not provide human evaluation or detailed qualitative analysis of why soft control produces better reasoning flow than hard constraints.

## Next Checks

1. **Ablation of distillation conditioning**: Train two HMMs—one on unconditional LLM samples and one on instruction-response pairs. Compare their format guidance quality and impact on task accuracy across GSM8k, SummEval, and ACE05-EN to isolate the contribution of instruction-aware distillation.

2. **γ sensitivity analysis**: Systematically sweep γ from 0.1 to 3.0 on a held-out validation set for each model (Llama, Qwen). Plot accuracy vs. format compliance curves to identify optimal γ ranges and quantify the tradeoff between steering strength and reasoning fidelity.

3. **Generalization stress test**: Apply DECO-G to a task requiring complex, deeply nested format constraints (e.g., code generation with strict syntax rules or multi-turn dialogue with format-dependent reasoning). Measure whether the decoupling mechanism maintains coherence when format and task reasoning are more tightly coupled than in the evaluated tasks.