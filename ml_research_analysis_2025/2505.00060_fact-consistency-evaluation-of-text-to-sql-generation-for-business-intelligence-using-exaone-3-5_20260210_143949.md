---
ver: rpa2
title: Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence
  Using Exaone 3.5
arxiv_id: '2505.00060'
source_url: https://arxiv.org/abs/2505.00060
tags:
- language
- evaluation
- semantic
- exaone
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Fact-Consistency Evaluation Framework for
  assessing semantic accuracy of LLM-generated SQL outputs using Exaone 3.5 in a real-world
  BI context. The authors construct a domain-specific benchmark of 219 natural language
  business questions paired with gold-standard SQL queries and validated ground-truth
  answers derived from LG Electronics' internal sales data.
---

# Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5

## Quick Facts
- arXiv ID: 2505.00060
- Source URL: https://arxiv.org/abs/2505.00060
- Reference count: 7
- Exaone 3.5 achieves 93% accuracy on simple aggregation but degrades to 4% on arithmetic reasoning tasks

## Executive Summary
This paper introduces a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of LLM-generated SQL outputs in business intelligence contexts. Using Exaone 3.5 and LG Electronics' internal sales data, the authors construct a domain-specific benchmark of 219 natural language business questions paired with gold-standard SQL queries and validated ground-truth answers. The evaluation reveals significant performance degradation in complex reasoning tasks, with accuracy dropping from 93% on simple aggregations to 4% on arithmetic reasoning and 31% on grouped ranking tasks.

## Method Summary
The study evaluates Exaone 3.5's text-to-SQL generation using a three-stage Fact-Consistency Checker: answer comparison, normalization, and correct/incorrect labeling. The framework operates on a proprietary benchmark of 219 business questions categorized into five complexity levels (L1-H4) based on reasoning requirements. Generated SQL is executed against LG's BigQuery sales data, with outputs normalized and compared to ground-truth answers derived from gold-standard SQL. Performance is measured using execution success rate, answer accuracy, semantic error rate, and non-response rate across the complexity taxonomy.

## Key Results
- Exaone 3.5 achieves 93% accuracy on simple aggregation tasks (L1) but drops to 4% for arithmetic reasoning (H1) and 31% for grouped ranking (H4)
- Semantic error rate increases from 1% in L1 to 54% in H1, with non-response rates also elevated in complex categories
- Execution success rate shows smaller variance (94% to 79%) than answer accuracy across complexity levels
- Arithmetic reasoning tasks show 41% non-response rate, suggesting model uncertainty in complex scenarios

## Why This Works (Mechanism)

### Mechanism 1: Fact-Consistency Evaluation via Ground Truth Comparison
- Claim: Comparing LLM-generated SQL execution results against validated ground-truth answers detects semantic errors that syntactic evaluation metrics miss.
- Mechanism: Execute both generated and gold-standard SQL against the same database, normalize outputs to handle formatting differences, then perform exact match comparison to label results as Correct or Incorrect.
- Core assumption: Semantically correct queries will produce answers that match ground truth after normalization, and mismatches indicate logical errors rather than acceptable query variations.
- Evidence anchors: [abstract]: "evaluate model performance using answer accuracy, execution success rate, semantic error rate, and non-response rate"; [section 3.4]: "The answer returned by the Exaone-generated SQL is compared against the ground-truth answer produced by the gold-standard SQL query"; SQLens (arXiv:2506.04494) similarly addresses "semantically incorrect yet syntactically valid queries" with error detection focus.

### Mechanism 2: Complexity-Based Performance Stratification
- Claim: SQL generation performance degrades predictably with query complexity, with arithmetic reasoning over aggregates being the most failure-prone category.
- Mechanism: Partition the benchmark into five tiers (L1: simple aggregation, H1: arithmetic reasoning, H2: filter logic, H3: conditional aggregation, H4: grouped ranking) to isolate which cognitive demands cause breakdowns.
- Core assumption: The taxonomy captures meaningfully distinct reasoning requirements, not just surface-level syntactic variation.
- Evidence anchors: [abstract]: "93% accuracy on simple aggregation tasks but exhibits substantial degradation in arithmetic reasoning (4% accuracy) and grouped ranking tasks (31% accuracy)"; [section 4, Table 1]: Shows systematic drop from L1 (93% accuracy, 1% semantic error) to H1 (4% accuracy, 54% semantic error); LogicCat (arXiv:2505.18744) emphasizes "complex mathematical computations" and "hypothetical reasoning" as underexplored in current benchmarks.

### Mechanism 3: Non-Response as Uncertainty Calibration Signal
- Claim: Elevated non-response rates in complex categories may reflect appropriate model uncertainty rather than pure capability failure.
- Mechanism: Track cases where the model produces empty output, plain text, or malformed SQL as a distinct signal from semantic errors—indicating the model recognizes its own limitations.
- Core assumption: Non-responses stem from calibrated uncertainty rather than prompt parsing failures or formatting bugs.
- Evidence anchors: [section 5]: "these outcomes also point to cases where the model appropriately abstained from guessing when uncertain—an encouraging behavior in high-stakes environments"; [section 4, Table 1]: H1 shows 41% non-response rate vs. 54% semantic error rate; H4 shows 31% non-response.

## Foundational Learning

- Concept: **SQL Complexity Taxonomy (Aggregation → Arithmetic → Ranking)**
  - Why needed here: The 93%→4% accuracy drop from L1 to H1 is not random—it reflects the jump from single-step aggregation (SUM, COUNT) to multi-step reasoning (compute ratio of two aggregates). Understanding this hierarchy is essential for diagnosing where models fail.
  - Quick check question: Given a sales table, why is "Total revenue for 2024" (L1) fundamentally easier for an LLM than "Growth rate of revenue from 2023 to 2024" (H1)?

- Concept: **Semantic vs. Syntactic Correctness**
  - Why needed here: The paper separates execution success rate (82% overall) from answer accuracy (61% overall)—a 21-point gap representing queries that run without errors but return wrong answers. Without this distinction, evaluation would mask dangerous hallucinations.
  - Quick check question: If a generated SQL query executes successfully but returns $50,000 instead of the correct $75,000, which metric catches this failure?

- Concept: **Ground Truth Construction from Domain Experts**
  - Why needed here: The benchmark required manually crafted gold-standard SQL paired with validated answers derived from LG's actual sales data. Academic benchmarks (Spider, WikiSQL) lack this domain-specific precision.
  - Quick check question: Why can't we evaluate text-to-SQL quality by comparing generated SQL to the gold SQL using string matching?

## Architecture Onboarding

- Component map: Natural language question -> Schema-Constrained Prompt Engineering -> Exaone 3.5 Generation -> BigQuery SQL Executor -> Answer Extractor -> Fact-Consistency Checker
- Critical path: Natural language question → Generated SQL → Executed result → Normalized answer → Ground truth comparison. Any break in this chain (syntax error, non-response, semantic mismatch) produces a distinct failure mode.
- Design tradeoffs: Exact match after normalization (chosen) vs. fuzzy semantic similarity → Exact match is stricter but avoids false positives; fuzzy matching may tolerate formatting drift but could miss subtle numerical errors; Low temperature single generation (chosen) vs. multi-sample voting with self-consistency → Deterministic outputs for reproducibility vs. potential robustness gains from aggregation; Automated instance-level checking (chosen) vs. human-in-the-loop review → Scalable evaluation vs. ability to catch nuanced semantic mismatches that normalization misses.
- Failure signatures: H1 Arithmetic Reasoning: Reversed numerator/denominator in ratios, wrong growth rate formula, mismatched column pairs; H4 Grouped Ranking: Missing GROUP BY keys, ORDER BY on wrong column, LIMIT applied before aggregation; H2/H3 Filter Logic: Omitted WHERE conditions, AND/OR operator confusion, IN clause encoding errors; Non-Response: Empty output string, conversational text instead of SQL, incomplete SQL structure.
- First 3 experiments: 1) Baseline validation on L1: Run the full pipeline on the 170 simple aggregation questions; confirm ~93% answer accuracy and ~94% execution success rate to verify environment parity; 2) Arithmetic reasoning error profiling: Sample 20 H1 queries; manually categorize each failure (wrong formula, missing filter, column mismatch) to build a failure mode taxonomy for your domain; 3) Normalization robustness test: Inject formatting variations (commas in numbers, currency symbols, trailing whitespace) into ground truth answers; verify the Fact-Consistency Checker normalizes correctly without false negatives.

## Open Questions the Paper Calls Out

- Question: Can hybrid reasoning architectures or post-generation verification layers significantly reduce the 54% semantic error rate in arithmetic reasoning tasks (H1) without degrading performance on simple queries?
  - Basis in paper: [explicit] The authors explicitly state the need for "fact-consistency validation layers and hybrid reasoning approaches" in the abstract and conclusion to address identified performance degradation.
  - Why unresolved: The current study evaluates only the base text-to-SQL generation capability of Exaone 3.5, without implementing or testing the suggested verification mechanisms.
  - What evidence would resolve it: A comparative study showing a reduction in semantic errors when a symbolic validator or external checker is applied to the LLM outputs for H1 category queries.

- Question: Do the identified failure modes in grouped ranking and arithmetic reasoning generalize to other state-of-the-art LLMs, or are they specific to the Exaone 3.5 architecture?
  - Basis in paper: [inferred] The paper attributes errors to "current limitations of LLMs" broadly, despite testing only a single model (Exaone 3.5), leaving the generalizability of these specific error profiles uncertain.
  - Why unresolved: The experimental results are derived exclusively from one specific instruction-tuned model, making it unclear if the 4% accuracy in arithmetic reasoning is a universal bottleneck.
  - What evidence would resolve it: Benchmarking the same 219-question dataset against other leading models (e.g., GPT-4, Claude 3.5) to compare semantic error rates in high-complexity categories.

- Question: To what extent can "improved prompt design" (e.g., few-shot prompting with complex SQL examples) correct the misapplied logic in grouped ranking tasks?
  - Basis in paper: [explicit] The Discussion identifies "improved prompt design" as a promising avenue for enhancement to address incorrect grouping operations and filter misalignment.
  - Why unresolved: The methodology relies on a zero-shot, schema-constrained prompt; the paper does not quantify the impact of providing exemplars for the specific complex operations where the model failed.
  - What evidence would resolve it: Ablation studies testing various prompting strategies (such as few-shot learning) specifically on the H4 (Grouped Ranking) category to measure accuracy improvement.

## Limitations

- The proprietary nature of LG Electronics' sales data and benchmark prevents independent validation of the exact performance numbers reported.
- Without access to the specific Fact-Consistency Checker normalization rules, subtle semantic mismatches might be missed or false positives introduced.
- The temperature=0.1 setting ensures reproducibility but may not reflect model behavior under more typical generation conditions.

## Confidence

- **High Confidence**: Execution Success Rate and Answer Accuracy metrics as valid evaluation constructs; the 93%→4% accuracy drop across complexity levels is well-supported by stratified analysis.
- **Medium Confidence**: The claim that non-response rates indicate calibrated uncertainty; this interpretation relies on behavioral assumptions about model decision-making that weren't directly validated.
- **Low Confidence**: The exact numerical performance figures (93%, 4%, 31%) without access to the proprietary benchmark and ground truth.

## Next Checks

1. **Domain Transferability Test**: Replicate the evaluation pipeline on an open-source business dataset (e.g., TPC-H) using a different LLM to verify whether the arithmetic reasoning and grouped ranking failure patterns persist across domains and models.

2. **Uncertainty Calibration Analysis**: Instrument the pipeline to distinguish between non-responses caused by genuine uncertainty versus prompt/schema incompatibilities, then compare failure distributions.

3. **Semantic Similarity Robustness**: Implement fuzzy matching (e.g., normalized edit distance with tolerance thresholds) alongside exact match to quantify how many near-misses are being treated as full semantic failures.