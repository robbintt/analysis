---
ver: rpa2
title: 'RanDeS: Randomized Delta Superposition for Multi-Model Compression'
arxiv_id: '2505.11204'
source_url: https://arxiv.org/abs/2505.11204
tags:
- task
- merging
- accuracy
- random
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RanDeS reduces interference in model merging by decorrelating task-specific
  parameter deltas using random orthogonal transformations. This prevents conflicting
  adjustments from interfering during model retrieval.
---

# RanDeS: Randomized Delta Superposition for Multi-Model Compression
## Quick Facts
- arXiv ID: 2505.11204
- Source URL: https://arxiv.org/abs/2505.11204
- Reference count: 40
- Reduces interference in model merging by decorrelating task-specific parameter deltas using random orthogonal transformations

## Executive Summary
RanDeS addresses the challenge of multi-model compression by preventing interference during model merging. The method decorrelates task-specific parameter deltas using random orthogonal transformations, ensuring that conflicting adjustments don't interfere during model retrieval. By requiring only random seeds for memory-efficient implementation, RanDeS enables dynamic model addition without additional storage overhead. The approach achieves near-fine-tuned performance while reducing storage costs by 4× compared to maintaining individual models.

## Method Summary
RanDeS operates by applying random orthogonal transformations to parameter deltas during the merging process. The core innovation lies in decorrelating task-specific adjustments, preventing interference when multiple models share parameters. The method uses two efficient implementations: layer shuffling and column-wise random sign flips. During retrieval, models can be reconstructed using only the stored random seeds and the base model, eliminating the need to store full parameter sets for each task. This enables scalable multi-model compression with minimal memory overhead.

## Key Results
- Achieves near-fine-tuned performance on vision and language benchmarks
- Reduces storage costs by 4× compared to maintaining individual models
- Consistently outperforms existing merging approaches in both accuracy and memory efficiency

## Why This Works (Mechanism)
RanDeS works by decorrelating task-specific parameter deltas through random orthogonal transformations. When multiple models share parameters, their adjustments can interfere with each other, degrading performance. By applying random transformations, RanDeS ensures that these adjustments occupy different subspaces, preventing destructive interference during retrieval. The use of random seeds instead of storing full transformation matrices enables memory-efficient implementation while maintaining the ability to reconstruct individual models on demand.

## Foundational Learning
- **Orthogonal transformations**: Linear mappings that preserve inner products and vector lengths. Why needed: Ensures decorrelation without distorting parameter space geometry. Quick check: Verify that transformation matrices satisfy Q^T Q = I.
- **Delta superposition**: Technique where model differences (deltas) are stored and superimposed on a base model. Why needed: Enables efficient storage of multiple model variants. Quick check: Confirm that retrieving a model involves adding the appropriate delta to the base parameters.
- **Random seeds for transformations**: Using pseudorandom generation instead of storing full transformation matrices. Why needed: Dramatically reduces memory overhead while maintaining reproducibility. Quick check: Ensure that the same seed always generates the same transformation.
- **Parameter decorrelation**: The process of making parameter adjustments independent across tasks. Why needed: Prevents interference between models sharing parameters. Quick check: Measure correlation between deltas before and after transformation.
- **Model retrieval**: Process of reconstructing a specific model from the merged representation. Why needed: Enables on-demand access to individual models without storing them separately. Quick check: Verify that retrieved models match their original fine-tuned versions.

## Architecture Onboarding
- **Component map**: Base model -> Random orthogonal transformation -> Decorrelated deltas -> Merged representation
- **Critical path**: Model merging requires applying transformations to deltas, then summing them with the base model. Retrieval involves applying inverse transformations using stored seeds.
- **Design tradeoffs**: Memory efficiency vs. computational overhead during retrieval. Using random seeds saves storage but requires transformation computation at retrieval time.
- **Failure signatures**: Performance degradation occurs when task similarities are high, causing transformed deltas to still interfere. Also fails when transformation seeds are lost or corrupted.
- **First experiments**: 1) Test decorrelation effectiveness by measuring delta correlation before/after transformation. 2) Verify storage savings by comparing merged representation size to individual models. 3) Evaluate retrieval accuracy against original fine-tuned models.

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental focus on vision and language benchmarks leaves generalization to other domains unclear
- Performance characterization under varying task similarity and number of tasks is incomplete
- Practical implementation details like inference latency and cache effects remain unexplored

## Confidence
- **High confidence**: Core mechanism of using random orthogonal transformations is mathematically sound; 4× storage savings is well-supported
- **Medium confidence**: Performance claims relative to existing approaches are credible but generalizability is uncertain
- **Medium confidence**: Proposed implementations appear effective but relative performance trade-offs need more characterization

## Next Checks
1. Test RanDeS on heterogeneous model architectures beyond vision and language, including transformers with different attention mechanisms and convolutional architectures
2. Evaluate inference latency and computational overhead during model retrieval, particularly measuring impact on real-time serving scenarios
3. Conduct ablation studies varying the number of tasks, task similarity distributions, and retrieval sequence lengths to characterize performance boundaries