---
ver: rpa2
title: 'CodeSimpleQA: Scaling Factuality in Code Large Language Models'
arxiv_id: '2512.19424'
source_url: https://arxiv.org/abs/2512.19424
tags:
- code
- language
- qwen2
- chinese
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CodeSimpleQA introduces a bilingual benchmark with 1,498 factual
  QA pairs and a 66.9M sample instruction corpus to evaluate code LLM factuality.
  The post-training framework combining SFT and GRPO significantly improves factual
  accuracy over base models, with CodeSimpleQA-RL achieving 45.2% F-score versus 40.0%
  for the baseline.
---

# CodeSimpleQA: Scaling Factuality in Code Large Language Models

## Quick Facts
- arXiv ID: 2512.19424
- Source URL: https://arxiv.org/abs/2512.19424
- Reference count: 13
- CodeSimpleQA-RL achieves 45.2% F-score on bilingual code factuality benchmark, outperforming baseline by 5.2%

## Executive Summary
CodeSimpleQA addresses the critical gap between code execution correctness and factual accuracy in code language models. The paper introduces a bilingual benchmark with 1,498 QA pairs across 15+ programming languages and 21 computer science domains, plus a massive 66.9M sample instruction corpus for training. Through a post-training framework combining supervised fine-tuning with reinforcement learning (GRPO), CodeSimpleQA-RL demonstrates significant improvements in factual accuracy while maintaining zero-shot code generation capabilities. The work reveals that even state-of-the-art models struggle with specialized domains like bioinformatics, highlighting the need for factuality-aware alignment beyond traditional execution metrics.

## Method Summary
CodeSimpleQA employs a two-stage post-training approach on Qwen2.5-Coder-32B-Instruct. First, supervised fine-tuning (SFT) trains on the 66.9M QA pairs from CodeSimpleQA-Instruct, generated by filtering Common Crawl with fastText, clustering with DBSCAN, and LLM-as-a-Judge verification. Second, Group Relative Policy Optimization (GRPO) fine-tunes the SFT model using LLM-as-a-Judge rewards with KL divergence constraints. The training uses 32 NVIDIA H20 GPUs for SFT and 64 GPUs for RL, with specific hyperparameters including 100 warmup steps (SFT) and constant LR 5e-7 (RL). Evaluation uses a 5-metric system (CO, NA, IN, CGA, F-score) with LLM-as-a-Judge scoring.

## Key Results
- CodeSimpleQA-RL achieves 45.2% F-score versus 40.0% baseline, representing 5.2% absolute improvement
- RAG excels at up-to-date documentation (68.0 on 2024 recency split) while SFT handles stable concepts better
- Thinking mode consistently outperforms chat mode logarithmically across model sizes (R² values 0.92–0.96)
- Even frontier models struggle with specialized domains like bioinformatics (0% F-score)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Combining large-scale supervised fine-tuning with reinforcement learning improves code factuality beyond what either approach achieves alone.
- **Mechanism:** SFT first internalizes factual code knowledge through exposure to 66.9M QA pairs, then GRPO (Group Relative Policy Optimization) refines responses using LLM-as-a-Judge scoring. The RL objective computes advantages via group-relative rewards, pushing the model toward factually correct answers while maintaining KL divergence constraints from the reference policy.
- **Core assumption:** The base model has sufficient parametric capacity to encode factual code knowledge; quality depends on the correctness of LLM-as-a-Judge verification.
- **Evidence anchors:** [abstract] "develop a post-training framework combining supervised fine-tuning and reinforcement learning"; [section 3.2-3.3] Equations 1-2 define the GRPO objective with advantage calculation and KL penalty; [corpus] Mask-DPO paper (arxiv 2503.02846) similarly shows fine-grained factuality alignment gains from preference learning
- **Break condition:** If the SFT data contains systematic factual errors, RL will amplify rather than correct them.

### Mechanism 2
- **Claim:** RAG outperforms SFT on recently-updated documentation, while SFT outperforms RAG on stable computer science concepts.
- **Mechanism:** RAG retrieves external documentation at inference time, accessing current information that may post-date training. SFT encodes knowledge into model weights, providing faster inference but becoming stale on evolving APIs. The 2024 recency split tests show this trade-off: RAG achieves 68.0 on new content vs. CodeSimpleQA-RL's 24.0.
- **Core assumption:** The retrieval corpus is high-quality and comprehensive for the target domains.
- **Evidence anchors:** [section 5] "RAG achieves 71.0/68.0 (old/new) scores compared to CodeSimpleQA-RL (42.0/24.0)"; [section 5] "combining SFT for core knowledge with RAG for up-to-date technical information may work best"; [corpus] No direct corpus support; limited external validation of this specific RAG/SFT trade-off claim
- **Break condition:** If retrieval quality degrades or the knowledge base is incomplete, RAG performance collapses.

### Mechanism 3
- **Claim:** Thinking mode provides consistent performance gains over chat mode, scaling logarithmically with model parameters.
- **Mechanism:** Extended reasoning chains allow models to self-verify and correct factual claims before outputting. The regression analysis shows both modes follow y = a·ln(x) + b with R² values 0.92–0.96, suggesting reasoning capability provides additive benefit regardless of scale.
- **Core assumption:** The reasoning process itself doesn't introduce hallucinations; test-time compute is available.
- **Evidence anchors:** [section 5] Figure 8 shows "thinking mode consistently outperforms the chat mode" with parallel logarithmic scaling; [section 5] "Chat: a = 5.67, b = 19.97 (R² = 0.921); Thinking: a = 5.30, b = 22.54"; [corpus] SURGE paper (arxiv 2502.11167) explores related surrogate execution concepts but doesn't directly validate thinking-mode scaling
- **Break condition:** If reasoning budgets are constrained or the model's self-correction mechanisms are miscalibrated, thinking mode may amplify errors.

## Foundational Learning

- **Concept: LLM-as-a-Judge evaluation**
  - **Why needed here:** The entire training pipeline depends on automated correctness verification; understanding its failure modes is critical for debugging.
  - **Quick check question:** Given a factual code QA pair, can you explain why a judge might rate "Python uses duck typing" as correct but "Python is the best language" as ambiguous?

- **Concept: Group Relative Policy Optimization (GRPO)**
  - **Why needed here:** The RL stage uses group-based advantage estimation rather than absolute rewards; this affects how models learn from partial correctness.
  - **Quick check question:** If 4 of 8 sampled responses are correct, what advantage does a correct response receive under the GRPO formula?

- **Concept: Pass@k evaluation and test-time scaling**
  - **Why needed here:** Understanding why performance plateaus after 10-20 samples informs deployment decisions about inference budgets.
  - **Quick check question:** Why might Pass@k plateau even when individual samples vary significantly?

## Architecture Onboarding

- **Component map:**
  1. **Data Pipeline:** Common Crawl → fastText filtering → DBSCAN clustering → QA generation (DeepSeek-V3.1) → LLM-as-a-Judge verification → 66.9M samples
  2. **Training Pipeline:** Base model → SFT on CodeSimpleQA-Instruct → GRPO fine-tuning using VeRL with vLLM backend
  3. **Evaluation:** LLM-as-a-Judge scores predictions against ground truth with CORRECT/INCORRECT/NOT_ATTEMPTED labels

- **Critical path:** QA data quality (LLM-as-a-Judge filtering) → SFT convergence → RL reward signal quality. Human annotation (312 verified samples) anchors the evaluation benchmark but doesn't scale to the training corpus.

- **Design tradeoffs:**
  - Dataset size (66.9M) vs. verification depth (automated judge only for training data)
  - RAG inference cost vs. SFT staleness on evolving documentation
  - Thinking-mode compute budget vs. latency requirements

- **Failure signatures:**
  - High "Not Attempted" rate → model uncertainty calibration issue (see Seed-Coder-8B at 68.5% NA on English)
  - Domain-specific collapse (e.g., Bioinformatics at 0% for multiple models) → insufficient training coverage
  - RL regression on stable concepts → reward overfitting to judge preferences

- **First 3 experiments:**
  1. Replicate the SFT-only baseline on a 1M-sample subset to verify data quality before full 66.9M training run
  2. Ablate the LLM-as-a-Judge threshold to measure precision/recall trade-offs in training data filtering
  3. Test RAG+SFT hybrid on the 2024 recency split to validate the claimed complementary strengths before production deployment

## Open Questions the Paper Calls Out

- **Question:** Can a hybrid approach combining SFT for stable code concepts with RAG for rapidly-evolving documentation achieve superior performance compared to either method alone on code factuality tasks?
- **Basis in paper:** [explicit] The analysis section states: "These results indicate that combining SFT for core knowledge with RAG for up-to-date technical information may work best in production environments," but this hypothesis remains untested.
- **Why unresolved:** The paper compares RAG and SFT separately (RAG: 71.0/68.0 vs CodeSimpleQA-RL: 42.0/24.0 on old/new subsets) but does not implement or evaluate a combined system.
- **What evidence would resolve it:** Experiments evaluating a model trained with CodeSimpleQA-Instruct augmented with retrieval at inference time, measured against both stable and time-sensitive question subsets.

- **Question:** How can code factuality benchmarks address temporal staleness as programming languages, frameworks, and APIs evolve?
- **Basis in paper:** [explicit] The limitations section acknowledges: "the rapidly evolving nature of programming languages (PLs) and frameworks means that certain questions may become outdated, and emerging technologies may not be adequately represented."
- **Why unresolved:** The benchmark's design focuses on time-invariant questions to ensure stability, but this excludes knowledge about emerging technologies and recent API changes that developers encounter in practice.
- **What evidence would resolve it:** A longitudinal study tracking benchmark validity over time, or a dynamic benchmark construction methodology with automated updating mechanisms.

- **Question:** How do biases inherited from LLM-as-a-Judge evaluators affect code factuality assessment, and what alternative evaluation approaches could mitigate these biases?
- **Basis in paper:** [explicit] The limitations section states: "the evaluation methodology relies on LLM-as-a-Judge, which may inherit biases from the judge model and could favor certain response styles."
- **Why unresolved:** The paper uses LLM-based evaluation throughout (for both dataset verification and final assessment) without investigating systematic biases this may introduce.
- **What evidence would resolve it:** A comparative analysis using human evaluation, rule-based verification, or execution-based ground truth on a subset of questions to quantify LLM-as-a-Judge bias.

- **Question:** What factors drive the plateau in test-time scaling performance after initial rapid gains, and can this plateau be extended through modified inference strategies?
- **Basis in paper:** [inferred] Figure 7 shows all models exhibiting "rapid performance gains during the initial 10-20 inference iterations before gradually plateauing," but the paper does not analyze why this plateau occurs or how to overcome it.
- **Why unresolved:** The plateau phenomenon is observed but not investigated—understanding its causes could inform more effective test-time compute allocation strategies.
- **What evidence would resolve it:** Analysis correlating plateau points with question difficulty, knowledge distribution, or model uncertainty; experiments with diverse sampling strategies to test plateau malleability.

## Limitations
- Evaluation framework relies entirely on LLM-as-a-Judge scoring without human validation beyond 312 samples
- Massive 66.9M training corpus was filtered only by automated verification, raising concerns about systematic factual errors
- CodeSimpleQA-Instruct dataset composition and DBSCAN clustering parameters remain underspecified, limiting reproducibility
- RL training duration and convergence criteria are unclear, making it difficult to assess whether reported gains represent optimal fine-tuning

## Confidence

- **High confidence:** The baseline improvement of CodeSimpleQA-RL over Qwen2.5-Coder-32B-Instruct (45.2% vs 40.0% F-score) is well-supported by the evaluation methodology and directly measurable. The thinking mode vs chat mode comparison shows clear, consistent logarithmic scaling patterns across model sizes.
- **Medium confidence:** The RAG vs SFT performance trade-off on recency requires additional external validation, as the 2024 recency split results show RAG dominating but with limited supporting evidence for the claimed complementary strengths. The superiority of combined SFT+GRPO over individual approaches assumes the judge's verification is accurate and complete.
- **Low confidence:** Domain-specific performance claims (particularly the 0% scores in bioinformatics) lack sufficient training corpus analysis to determine whether this represents genuine knowledge gaps or evaluation artifacts.

## Next Checks

1. Conduct human evaluation on a stratified sample (n=100) of CodeSimpleQA predictions to validate LLM-as-a-Judge scoring accuracy and measure inter-annotator agreement, focusing on cases where the judge scores differ from ground truth.

2. Replicate the training pipeline on a 1M-sample subset with ablation studies removing SFT vs GRPO stages to quantify individual contribution and verify the claimed synergy effect.

3. Test the RAG+SFT hybrid approach on the 2024 recency split using multiple retrieval corpora (official docs, community Q&A, academic papers) to validate the complementary strengths claim and identify optimal combination strategies.