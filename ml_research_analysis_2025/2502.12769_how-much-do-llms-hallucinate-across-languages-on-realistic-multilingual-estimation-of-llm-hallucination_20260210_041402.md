---
ver: rpa2
title: How Much Do LLMs Hallucinate across Languages? On Realistic Multilingual Estimation
  of LLM Hallucination
arxiv_id: '2502.12769'
source_url: https://arxiv.org/abs/2502.12769
tags:
- hallucination
- languages
- language
- tokens
- silver
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work quantifies LLM hallucination rates across 30 languages
  in realistic knowledge-intensive open-domain QA settings. A multilingual hallucination
  detection model is trained by translating an English benchmark and fine-tuning a
  large LLM.
---

# How Much Do LLMs Hallucinate across Languages? On Realistic Multilingual Estimation of LLM Hallucination

## Quick Facts
- arXiv ID: 2502.12769
- Source URL: https://arxiv.org/abs/2502.12769
- Reference count: 40
- Key outcome: Quantifies LLM hallucination rates across 30 languages using adjusted detection precision/recall; smaller and more multilingual models hallucinate significantly more.

## Executive Summary
This work addresses the gap in understanding multilingual LLM hallucination by developing a scalable evaluation framework for 30 languages. The authors translate an English hallucination detection dataset to create training data, then train a multilingual detection model that outperforms monolingual counterparts. They validate that synthetic (GPT-4-labeled) test sets produce hallucination rate estimates highly correlated with human-annotated gold data, enabling cost-effective evaluation. Across 51,133 prompts and 11 LLMs, they find consistent patterns: smaller models and those supporting more languages hallucinate more, with absolute hallucination counts correlating with response length but not per-token rates.

## Method Summary
The approach translates the FAVA English hallucination detection dataset to 30 languages using NLLB, then fine-tunes a bidirectional Llama-3-8B-base model (removing future-token masking) with QLoRA adapters for token-level classification. The resulting MULTI model is evaluated on MFAVA-Gold (5 languages, human-annotated) and MFAVA-Silver (30 languages, GPT-4 synthetic). Hallucination rates are estimated by adjusting raw detections with precision and recall estimates from the benchmark data, using the formula HRest,l = (Pl × Hdet,l) / (Rl × Nl) × 100%. The framework is applied to responses from 11 LLMs on 51,133 knowledge-intensive queries across 30 languages.

## Key Results
- Multilingual MULTI model outperforms monolingual MONO models by up to 30 F1 points on fine-grained category detection
- Silver (synthetic) hallucination benchmarks yield HRest,l estimates highly correlated with Gold (r=0.83) despite lower absolute F1
- Smaller models and those supporting more languages hallucinate significantly more (correlation 0.88 with language count)
- Response length correlates with absolute hallucinations but not per-token hallucination rates

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adjusting raw hallucination detections with detector precision and recall yields more reliable rate estimates than raw detection counts alone.
- Mechanism: The estimation formula HRest,l = (Pl × Hdet,l / Rl × Nl) × 100% discounts false positives via Pl and compensates for false negatives via Rl, producing length-normalized rates comparable across languages and models.
- Core assumption: Detector precision and recall estimated on benchmark data generalize to in-the-wild LLM outputs for the same language.
- Evidence anchors: [section] Eq. 1 and §4.1 derive the rate estimation formula and justify the Pl/Rl ratio dependency. [abstract] "Hallucination rates are computed by adjusting model detections with precision and recall estimates."

### Mechanism 2
- Claim: A single multilingual hallucination detection model outperforms language-specific monolingual models, particularly for fine-grained hallucination type classification.
- Mechanism: Training MULTI on concatenated translated data from all 30 languages enables cross-lingual transfer, improving F1 by up to +30 points on Category task (Arabic Gold) vs. MONO.
- Core assumption: Machine-translated training data preserves hallucination span boundaries and labels adequately for effective learning.
- Evidence anchors: [section] Table 3 shows MULTI consistently outperforms MONO across all five Gold languages on Binary and Category tasks. [abstract] "We start from an English hallucination detection dataset and rely on MT to translate-train a detection model."

### Mechanism 3
- Claim: Silver (synthetic GPT-4-labeled) hallucination benchmarks produce hallucination rate estimates highly correlated with Gold (human-annotated) benchmarks, enabling scalable multilingual evaluation.
- Mechanism: Although absolute detector F1 differs between Silver and Gold, the ratio Pl/Rl used in rate estimation remains consistent—yielding r=0.83 correlation between Silver-based and Gold-based HRest,l estimates.
- Core assumption: The systematic bias between synthetic and human labels affects Pl and Rl proportionally, preserving their ratio.
- Evidence anchors: [section] Figure 3 demonstrates strong correlation (r=0.83, p=1.26e-04) between Silver and Gold rate estimates. [abstract] "Human-annotated gold data for five high-resource languages validates that silver (synthetic) test sets yield similar performance estimates."

## Foundational Learning

- **Concept**: Span-level token classification (I-O labeling)
  - Why needed here: The detection model classifies each token as hallucinated or not, with fine-grained type labels for Category task.
  - Quick check question: Can you explain why Inside-Out (I-O) scheme outperformed B-I-O in preliminary experiments?

- **Concept**: Bidirectional vs. Causal fine-tuning of decoder LLMs
  - Why needed here: Removing future-token masking (Bidirect) generally improved performance over causal attention (Table 3), critical for architecture choice.
  - Quick check question: Why might bidirectional context help token-level hallucination detection more than next-token prediction?

- **Concept**: Hallucination taxonomy (6 FAVA types)
  - Why needed here: The Category task requires distinguishing Entity, Relation, Invented, Contradictory, Unverifiable, and Subjective hallucinations—low IAA on types (Figure 2) explains why Binary detection is preferred.
  - Quick check question: Which hallucination type is easiest for models to detect and why might Subjective be hardest?

## Architecture Onboarding

- **Component map**: FAVA training data → Translate (NLLB) → Train MULTI (Bidirect) → Validate on MFAVA-Gold → Collect LLM responses → Run detector → Compute Hdet,l → Estimate Pl, Rl → Compute HRest,l via Eq. 1

- **Critical path**: 1. Translate FAVA training data → Train MULTI (Bidirect) → Validate on MFAVA-Gold. 2. Collect LLM responses on estimation dataset → Run detector → Compute Hdet,l. 3. Estimate Pl, Rl from Silver (or Gold if available) → Compute HRest,l via Eq. 1.

- **Design tradeoffs**: Binary vs. Category detection: Binary F1 is 15-40 points higher; Category provides interpretability but unreliable for rate estimation. Mono vs. Multi models: Multi better overall but requires 30× training data; Mono viable for high-resource focus. Silver vs. Gold evaluation: Gold expensive (~$4,581 for 5 languages) vs. Silver scalable (~$77/language) with validated correlation.

- **Failure signatures**: Low Silver-Gold correlation (r < 0.6): Indicates synthetic hallucinations diverge from human-judged errors—revert to Gold annotation. Category F1 < 40% on Gold: Fine-grained classification unreliable—use Binary only. High variance across seeds (±>5% HRest,l): Detector unstable—increase training seeds or data.

- **First 3 experiments**: 1. Replicate MULTI (Bidirect) on MFAVA-Gold for Arabic and German; compare F1 to Table 3 to validate setup. 2. Compute HRest,l for a new LLM (e.g., Mistral-7B) on 5 Gold languages; correlate Silver vs. Gold estimates to test proxy validity. 3. Ablate translation quality: Train separate models on NLLB vs. higher-quality translator for 2 low-resource languages; compare Mono performance to assess translation-noise impact.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Can we develop robust multilingual models capable of fine-grained hallucination type classification (e.g., distinguishing entity errors from relation errors) rather than just binary detection?
- **Basis in paper**: [explicit] The authors note that category-level performance is "insufficient" and low inter-annotator agreement "warrants a broader research effort on hallucination type taxonomies," explicitly leaving this for future work.
- **Why unresolved**: Current models struggle to differentiate between fine-grained categories (F1 < 50%), and humans show low agreement on these specific types, making supervision difficult.
- **Evidence to resolve**: A dataset with high inter-annotator agreement on hallucination types and a model architecture that can disentangle these categories with high precision.

### Open Question 2
- **Question**: Why do LLMs with broader declared language support tend to hallucinate more, and is this an inherent trade-off of multilingual capacity?
- **Basis in paper**: [inferred] The authors report a "surprising trend" (correlation 0.88) where models supporting more languages hallucinate more, but the paper does not investigate the underlying cause of this "curse of multilingualism."
- **Why unresolved**: The study measures the correlation but cannot disentangle whether this is due to training data dilution, model capacity limits, or the specific architectures of the multilingual models tested.
- **Evidence to resolve**: Controlled ablation studies isolating the number of supported languages while keeping model size and total training compute constant.

### Open Question 3
- **Question**: How can the factual coverage (completeness) of multilingual long-form answers be estimated in addition to factual correctness?
- **Basis in paper**: [explicit] The authors explicitly limit their scope to "factual correctness" and acknowledge in the Limitations section that "resources for assessing multilingual factual coverage are still lacking."
- **Why unresolved**: Checking for what is *missing* in a long-form answer is technically harder than detecting what is *wrong*, especially without a comprehensive "gold" reference for every language.
- **Evidence to resolve**: A multilingual evaluation framework that quantifies the recall of key facts relative to a source text, rather than just the precision of generated tokens.

## Limitations

- **Synthetic data validity**: While showing r=0.83 correlation between Silver and Gold estimates, the proxy approach relies on the assumption that synthetic hallucination patterns sufficiently resemble human-annotated errors.
- **Cross-lingual transfer generalization**: Performance gains are validated on only five high-resource languages, leaving untested whether multilingual benefits extend to truly low-resource languages.
- **Estimation formula sensitivity**: The HRest,l formula critically depends on accurate Pl and Rl estimates, which may not generalize if real LLM outputs differ significantly from benchmark data.

## Confidence

- **High confidence**: Smaller models and those supporting more languages hallucinate more (direct empirical evidence across 11 LLMs).
- **Medium confidence**: MULTI model outperforms MONO models (well-validated but magnitude based on limited language coverage).
- **Low confidence**: Fine-grained Category task results are acknowledged as unreliable (low IAA) yet still reported.

## Next Checks

1. **Validate estimation robustness**: For languages with both Silver and Gold data, systematically vary detection model training data size and seeds to test whether HRest,l estimates remain stable and correlated.

2. **Test low-resource language generalization**: Select 2-3 genuinely low-resource languages and compare MULTI performance against MONO models, assessing translation quality impact on detection accuracy.

3. **Analyze hallucination type difficulty**: Re-analyze Category task data to identify consistently misclassified FAVA types, comparing to human IAA patterns to determine if model errors mirror human uncertainty.