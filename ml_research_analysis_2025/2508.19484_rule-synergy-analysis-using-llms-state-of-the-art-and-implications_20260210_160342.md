---
ver: rpa2
title: 'Rule Synergy Analysis using LLMs: State of the Art and Implications'
arxiv_id: '2508.19484'
source_url: https://arxiv.org/abs/2508.19484
tags:
- card
- cards
- synergy
- game
- effect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models struggle to accurately detect positive and
  negative synergies between game rules, particularly in complex rule interactions
  like those in card games. This paper introduces a dataset of card synergies from
  Slay the Spire and evaluates LLMs on their ability to classify card pairs as having
  positive, negative, or no synergy.
---

# Rule Synergy Analysis using LLMs: State of the Art and Implications

## Quick Facts
- arXiv ID: 2508.19484
- Source URL: https://arxiv.org/abs/2508.19484
- Authors: Bahar Bateni; Benjamin Pratt; Jim Whitehead
- Reference count: 20
- Large language models struggle to accurately detect positive and negative synergies between game rules

## Executive Summary
This paper evaluates large language models' ability to detect rule synergies in card games, using Slay the Spire as a case study. The researchers created a dataset of 240 card pairs labeled as having positive, negative, or no synergy, then tested multiple LLM models on their ability to classify these pairs. Results show that while models perform reasonably well at identifying non-synergistic pairs (F1 scores up to 0.75), they struggle significantly with detecting positive synergies (F1 scores around 0.37-0.50) and especially negative synergies (F1 scores as low as 0.16). The study reveals that LLMs have difficulty with complex rule interactions, timing considerations, and accurate game state reasoning.

## Method Summary
The researchers created a dataset of card synergies from the video game Slay the Spire, containing 240 pairs of cards labeled as having positive, negative, or no synergy. They evaluated multiple LLM models using a combination of zero-shot and few-shot prompting strategies. The evaluation measured precision, recall, and F1 scores for each synergy type. Additionally, they performed detailed error analysis on 20 hand-checked cases to identify common failure modes in LLM reasoning about rule interactions.

## Key Results
- LLMs achieved F1 scores of 0.75 for no synergy detection but only 0.37-0.50 for positive synergy and 0.16-0.39 for negative synergy
- Models showed a strong bias toward predicting no synergy, leading to low recall for both positive and negative synergies
- Common error types included timing misunderstandings, rule misapplications, and incorrect assumptions about game states

## Why This Works (Mechanism)
LLMs struggle with rule synergy detection because they lack deep understanding of game mechanics and temporal reasoning. The models often rely on surface-level pattern matching rather than truly understanding how card interactions unfold over time. They frequently make assumptions about game states that aren't specified in the prompts, leading to incorrect synergy classifications.

## Foundational Learning

1. **Rule Synergy Definition** - Why needed: Provides the conceptual framework for evaluating LLM performance; Quick check: Can the model distinguish between additive and multiplicative effects of rule combinations?

2. **Game State Reasoning** - Why needed: Critical for understanding how card interactions play out over time; Quick check: Does the model correctly track resource changes and state transitions?

3. **Temporal Logic in Games** - Why needed: Many synergies depend on timing and sequence of actions; Quick check: Can the model identify when synergies require specific ordering of card plays?

4. **Zero-shot vs Few-shot Learning** - Why needed: Different prompting strategies significantly impact performance; Quick check: Does providing examples improve synergy detection accuracy?

## Architecture Onboarding

Component Map: Card Pair Input -> LLM Processing -> Synergy Classification -> Confidence Scoring -> Output

Critical Path: Input prompt formulation -> LLM inference -> Classification output -> Evaluation metrics calculation

Design Tradeoffs: The study uses a relatively small, carefully curated dataset for evaluation, trading breadth for quality and consistency. Zero-shot prompting was chosen for its general applicability, while few-shot prompting provides more specific guidance but requires more examples.

Failure Signatures: 
- Consistently low recall for positive and negative synergies
- Systematic bias toward "no synergy" predictions
- Errors in timing and sequence-based reasoning
- Incorrect assumptions about unspecified game states

First 3 Experiments:
1. Test different prompt engineering strategies to improve synergy detection accuracy
2. Evaluate models on a larger, more diverse dataset spanning multiple game genres
3. Implement fine-tuning on game-specific data to improve performance

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow domain focus on card games from a single source (Slay the Spire) may not generalize to broader rule synergy contexts
- Dataset size of 240 pairs represents a limited sample space that may not capture full complexity of rule interactions
- Evaluation methodology relies on automated scoring against human annotations without addressing potential subjectivity in ground truth labeling

## Confidence

**High confidence**: LLMs struggle with negative synergy detection and timing-based reasoning
**Medium confidence**: Overall poor performance on positive synergies
**Medium confidence**: Identified error patterns (timing, rule misapplication, artificial states)
**Low confidence**: Generalization to other game types and rule complexity levels

## Next Checks
1. Replicate the study with a larger, more diverse dataset spanning multiple game genres and complexity levels
2. Conduct ablation studies testing different prompting strategies and model architectures
3. Perform cross-validation with multiple human annotators to establish inter-rater reliability for the ground truth labels