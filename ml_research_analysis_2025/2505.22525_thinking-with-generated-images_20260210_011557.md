---
ver: rpa2
title: Thinking with Generated Images
arxiv_id: '2505.22525'
source_url: https://arxiv.org/abs/2505.22525
tags:
- visual
- images
- generation
- vision
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thinking with Generated Images, a novel approach
  enabling large multimodal models to generate intermediate visual thoughts during
  reasoning. The method allows models to spontaneously create and critique visual
  representations, rather than just processing fixed user-provided images.
---

# Thinking with Generated Images
## Quick Facts
- arXiv ID: 2505.22525
- Source URL: https://arxiv.org/abs/2505.22525
- Reference count: 40
- Primary result: Novel multimodal reasoning approach achieving up to 50% relative improvement on complex visual tasks

## Executive Summary
This paper introduces Thinking with Generated Images, a novel approach enabling large multimodal models to generate intermediate visual thoughts during reasoning. The method allows models to spontaneously create and critique visual representations, rather than just processing fixed user-provided images. The authors implement this through a native long-multimodal thought process that interleaves text and visual tokens, enabling models to decompose complex visual tasks into subgoals or generate self-critiques of their visual hypotheses.

Two mechanisms are demonstrated: vision generation with intermediate visual subgoals (where models break down multi-object prompts into separate images then integrate them) and vision generation with self-critique (where models analyze and refine their own visual outputs). The approach shows substantial improvements over baselines, achieving up to 50% relative improvement in handling complex multi-object scenarios. On GenEval benchmarks, the method improved performance from 38% to 57% in two-object generation tasks, while also demonstrating effectiveness in correcting and refining visual hypotheses through iterative self-critique. The work establishes a foundation for future research in multimodal cognition and complex visual reasoning.

## Method Summary
The core innovation is a long-multimodal thought process that interleaves text and visual tokens, allowing models to generate intermediate visual representations during reasoning. The approach extends traditional multimodal reasoning by enabling spontaneous image generation rather than passive processing of user-provided images. Two key mechanisms are implemented: (1) vision generation with intermediate visual subgoals, where complex multi-object prompts are decomposed into separate images that are later integrated, and (2) vision generation with self-critique, where models analyze and refine their own visual outputs. The method uses autoregressive generation with interleaved text-visual thought chains, incorporating both image generation and visual reasoning into a unified framework.

## Key Results
- Achieved up to 50% relative improvement in handling complex multi-object scenarios
- Improved performance on GenEval benchmarks from 38% to 57% in two-object generation tasks
- Demonstrated effectiveness in self-critique and refinement of visual hypotheses through iterative feedback

## Why This Works (Mechanism)
The approach works by enabling models to actively generate and manipulate visual representations during reasoning, rather than treating images as fixed inputs. By interleaving text and visual tokens in long thought chains, the model can break down complex visual tasks into manageable subgoals, generate hypotheses, and refine them through self-critique. This mimics human-like visual reasoning where we mentally manipulate and evaluate visual concepts before arriving at conclusions.

## Foundational Learning
- Multimodal token interleaving: Why needed - to enable seamless integration of visual and textual reasoning; Quick check - verify token alignment and context preservation
- Autoregressive visual generation: Why needed - to create intermediate visual thoughts during reasoning; Quick check - confirm generation quality and coherence
- Self-critique mechanisms: Why needed - to refine and correct visual hypotheses; Quick check - evaluate improvement in output quality after critique cycles
- Visual subgoal decomposition: Why needed - to handle complex multi-object scenarios; Quick check - measure task success rate across varying complexity levels
- Long-context multimodal reasoning: Why needed - to maintain coherence across multiple visual reasoning steps; Quick check - verify attention patterns across interleaved modalities

## Architecture Onboarding
**Component Map:** Input Prompt -> Text Encoder -> Visual Subgoal Generator -> Image Decoder -> Self-Critique Module -> Final Output

**Critical Path:** The core reasoning flow moves from initial prompt encoding through visual subgoal generation, image decoding, self-critique analysis, and final output synthesis. Each stage builds upon the previous, with visual thoughts informing subsequent reasoning steps.

**Design Tradeoffs:** The interleaved token approach provides rich multimodal reasoning but increases computational overhead compared to text-only methods. The self-critique mechanism adds refinement capability but may introduce additional latency and potential for error propagation.

**Failure Signatures:** Common failure modes include losing coherence in long thought chains, generating visually inconsistent subgoals, and inadequate self-critique that fails to correct fundamental errors. Performance degradation is most noticeable with highly complex scenes or ambiguous prompts.

**First Experiments:** 
1. Test single-object generation to establish baseline performance
2. Evaluate two-object decomposition and integration
3. Assess self-critique effectiveness on simple visual errors

## Open Questions the Paper Calls Out
None

## Limitations
- Performance demonstrated primarily on synthetic and controlled benchmarks, with real-world visual reasoning tasks yet to be established
- Long-multimodal thought process may face scalability challenges for complex scenes due to computational overhead
- Self-critique mechanism's effectiveness in diverse visual contexts requires further validation

## Confidence
**High confidence:** The core architectural innovation of interleaving text and visual tokens in reasoning chains is technically sound and well-implemented. The demonstrated improvements on GenEval benchmarks are reproducible and statistically significant.

**Medium confidence:** The claim of 50% relative improvement requires careful interpretation, as it depends heavily on specific task configurations and may not generalize across all multimodal reasoning domains. The self-critique mechanism's effectiveness in more diverse visual contexts remains to be validated.

**Medium confidence:** While the decomposition strategy for multi-object prompts shows promise, the method's ability to handle truly complex, ambiguous, or real-world visual scenarios needs further testing.

## Next Checks
1. Test the approach on real-world visual reasoning datasets (e.g., visual question answering on natural images) to assess generalization beyond synthetic multi-object generation tasks
2. Evaluate computational efficiency and reasoning quality as scene complexity increases, particularly for scenarios requiring more than three intermediate visual thoughts
3. Conduct ablation studies isolating the contributions of self-critique versus subgoal generation to better understand which mechanism drives performance improvements in different task types