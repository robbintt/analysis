---
ver: rpa2
title: Calibrating Uncertainty for Zero-Shot Adversarial CLIP
arxiv_id: '2512.12997'
source_url: https://arxiv.org/abs/2512.12997
tags:
- adversarial
- uncertainty
- clip
- robustness
- clean
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addresses the issue of uncertainty miscalibration in
  zero-shot adversarial CLIP, where adversarial perturbations suppress uncertainty
  and lead to overconfident predictions. The authors reformulate CLIP logits as concentration
  parameters of a Dirichlet distribution, providing a principled uncertainty decomposition
  into aleatoric and epistemic components.
---

# Calibrating Uncertainty for Zero-Shot Adversarial CLIP

## Quick Facts
- **arXiv ID:** 2512.12997
- **Source URL:** https://arxiv.org/abs/2512.12997
- **Reference count:** 40
- **Primary result:** Reformulates CLIP logits as Dirichlet concentration parameters to enable principled uncertainty decomposition, and proposes UCAT to align clean/adversarial Dirichlet distributions, restoring calibrated uncertainty under attack while maintaining competitive clean accuracy and robustness.

## Executive Summary
This work addresses uncertainty miscalibration in zero-shot adversarial CLIP, where adversarial perturbations suppress uncertainty and lead to overconfident predictions. The authors reformulate CLIP logits as concentration parameters of a Dirichlet distribution, providing a principled uncertainty decomposition into aleatoric and epistemic components. They propose UCAT, an adversarial fine-tuning objective that aligns Dirichlet distributions between clean and adversarial samples, preserving semantic relations and calibrating confidence. Experiments on 16 single-label and one multi-label benchmark demonstrate that UCAT effectively restores calibrated uncertainty under attack while maintaining competitive clean accuracy and adversarial robustness, outperforming existing baselines in most settings.

## Method Summary
The method reparameterizes CLIP logits as Dirichlet concentration parameters α_k = exp((τℓ_k + 1)/τ'), enabling closed-form uncertainty decomposition into aleatoric (AU) and epistemic (EU) components. The UCAT objective minimizes KL divergence between clean and adversarial Dirichlet distributions, KL(Dir(α_adv) || Dir(α)), which holistically aligns inter-class semantic relations and evidence magnitude. The final loss combines cross-entropy with the Dirichlet KL regularization, L = L_ce + λL_ucr, where λ balances discriminative alignment and uncertainty calibration. The approach is trained using ℓ∞-PGD adversarial fine-tuning on TinyImageNet, evaluating on multiple zero-shot classification benchmarks.

## Key Results
- UCAT restores calibrated uncertainty under attack, with entropy (PU) increasing for adversarial samples compared to vanilla CLIP
- Achieves 54.17% clean accuracy and 30.58% AutoAttack robustness on TinyImageNet, outperforming baselines in most settings
- Ablation shows Dirichlet KL regularization outperforms softmax-level KL and single-anchor strategies across 16 single-label benchmarks
- Maintains competitive clean accuracy (50-75% range) while improving adversarial robustness (28-47% AutoAttack) on standard datasets

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Reformulating CLIP logits as Dirichlet concentration parameters enables closed-form uncertainty decomposition that exposes a reliability gap: adversarial perturbations suppress rather than increase predictive uncertainty.
- **Mechanism:** The mapping α_k(x) = exp(h(ℓ_v→t_k(x))) with h(ℓ) = (τℓ + 1)/τ' preserves logit ordering while converting cosine similarities into valid Dirichlet evidence (α_k ≥ 1). This yields aleatoric uncertainty (AU) capturing class ambiguity via expected entropy under Dirichlet, and epistemic uncertainty (EU) via C/(α_0 + C) reflecting evidence insufficiency.
- **Core assumption:** CLIP's contrastive pre-training endows logits with semantically meaningful absolute magnitude that can be faithfully interpreted as evidence strength.
- **Evidence anchors:** [abstract] "By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence."

### Mechanism 2
- **Claim:** KL divergence between clean and adversarial Dirichlet distributions simultaneously preserves inter-class semantic relations (via AU alignment) and calibrates evidence strength (via EU alignment), preventing overconfident collapse.
- **Mechanism:** L_ucr = KL(Dir(α_adv) || Dir(α)) penalizes distributional shift holistically rather than anchoring to a single class. Since both AU and EU are closed-form functions of α, this regularizes the full uncertainty structure—not just the argmax prediction.
- **Core assumption:** Clean Dirichlet distributions encode reliable semantic geometry worth preserving; adversarial perturbations distort this geometry in a recoverable way.
- **Evidence anchors:** [abstract] "Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty."

### Mechanism 3
- **Claim:** The combined loss L = L_ce + λL_ucr balances discriminative supervision (ground-truth anchoring) with uncertainty calibration, with λ scaling dynamically via β = 2/e^(τ') to maintain numerical stability across temperature settings.
- **Mechanism:** L_ce anchors adversarial embeddings to the ground-truth text prototype (discriminative), while L_ucr regularizes the full distribution (calibration). The harmonic mean of clean and robust accuracy peaks at λ·β = 10^5, suggesting a principled balance point.
- **Core assumption:** Discriminative and calibration objectives are complementary rather than conflicting; proper weighting yields better robustness-generalization trade-offs than either alone.
- **Evidence anchors:** [Section 5] "The final objective combines both components: L = L_ce + λL_ucr, where λ balances discriminative alignment and uncertainty calibration."

## Foundational Learning

- **Concept: Dirichlet Distribution as Second-Order Probability**
  - Why needed here: Understanding that Dir(π; α) places a distribution over class probability vectors π, with α_k as concentration parameters (pseudo-counts). Total evidence α_0 = Σα_k controls precision.
  - Quick check question: If α = [5, 3, 2], what is the expected probability for class 1? (Answer: 5/10 = 0.5)

- **Concept: Aleatoric vs Epistemic Uncertainty**
  - Why needed here: AU captures inherent data ambiguity (e.g., semantic overlap between "wolf" and "dog"); EU captures model uncertainty from limited evidence or distribution shift. UCAT aligns both.
  - Quick check question: A model sees an image far from training data with clear features. Which uncertainty should be high? (Answer: EU high, AU potentially low)

- **Concept: KL Divergence Between Dirichlet Distributions**
  - Why needed here: L_ucr minimizes KL(Dir(α_adv) || Dir(α)), which has closed-form expression and penalizes both mean shift and precision changes.
  - Quick check question: Why use KL rather than Euclidean distance between α vectors? (Answer: KL respects distributional geometry; same α values can imply different uncertainties depending on α_0)

## Architecture Onboarding

- **Component map:** Input image x → [PGD Attack] → Adversarial x_a → [Image Encoder] → v(x) clean/v(x_a) adversarial → Logits ℓ = ⟨v, t_k⟩/τ for k=1..C → Dirichlet α_k = exp((τℓ_k + 1)/τ') → L = L_ce(α_adv, y) + λ·KL(Dir(α_adv)||Dir(α_clean))

- **Critical path:**
  1. Implement logit-to-Dirichlet mapping with numerical stability (α_k can explode if τ is small; paper uses τ=0.01 for training but τ'=0.07 for EU computation).
  2. Verify AU/EU decomposition matches paper formulas (digamma functions for AU).
  3. Ensure KL gradient flows correctly through both α and α_adv.

- **Design tradeoffs:**
  - τ' = 0.07: Larger τ' → softer predictions (higher tolerance to semantic neighbors), smaller τ' → sharper. Paper validates 0.07 as optimal.
  - λ·β ≈ 10^5: Too small → under-regularized; too large → discriminative signal overwhelmed.
  - Training attack strength: 2-step PGD (ε=1/255) preserves clean accuracy better; 10-step PGD (ε=2/255) yields stronger robustness but more clean accuracy drop.

- **Failure signatures:**
  - EU near zero for all inputs: Check that α_0 isn't exploding (τ too small without softplus stabilization).
  - Clean accuracy collapses: Likely λ too large or τ' too small.
  - Robustness no better than vanilla CLIP: L_ucr may not be contributing (verify KL is non-zero and gradients flow).
  - Domain-specific datasets (EuroSAT, PCAM) underperform: Expected—these have high AU and weak clean semantic structure (Section 6.2).

- **First 3 experiments:**
  1. **Sanity check:** Reproduce ablation (Table 3) with L_ce only vs L_ce + softmax-KL vs L_ce + Dirichlet-KL on a single dataset (e.g., CIFAR-10). Verify Dirichlet-KL provides meaningful gains.
  2. **Temperature sensitivity:** Sweep τ' ∈ {0.01, 0.05, 0.07, 0.1} on TinyImageNet, plotting clean/PGD accuracy and harmonic mean. Confirm 0.07 is near-optimal.
  3. **Uncertainty calibration verification:** Plot entropy (PU) for clean vs adversarial samples before and after UCAT on a 3-dataset subset. Confirm ordering: CLIP_clean < UCAT_clean < UCAT_adversarial.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the UCAT framework be extended to handle adversarial perturbations applied simultaneously to both the image and text encoders? (Basis: Section G states future work may extend to more comprehensive bidirectional attacks)

- **Open Question 2:** Can the proposed uncertainty-based analysis be incorporated as a prior for test-time defenses to improve stability against adaptive attacks? (Basis: Section G identifies incorporating this analysis into test-time defenses as a promising future direction)

- **Open Question 3:** Does the Dirichlet reformulation of logits generalize to significantly larger or architecturally diverse vision-language models beyond CLIP? (Basis: Section G notes experiments are restricted to CLIP and suggests investigating applicability to larger models)

## Limitations

- The core assumption that CLIP's contrastive pre-training endows logits with interpretable magnitude as Dirichlet evidence lacks direct empirical validation
- Clean Dirichlet distributions may not provide reliable calibration targets when clean samples are out-of-distribution
- The approach underperforms on domain-specific datasets (EuroSAT, PCAM) due to high aleatoric uncertainty
- Optimal hyperparameters (τ' = 0.07, λ·β = 10^5) are derived from limited validation and may not generalize

## Confidence

- **High Confidence:** The mathematical framework (Dirichlet reformulation, KL divergence properties, uncertainty decomposition) is rigorously derived and internally consistent.
- **Medium Confidence:** Empirical results on standard benchmarks demonstrate clear improvements over baselines, with ablation studies supporting the Dirichlet KL component.
- **Low Confidence:** Claims about semantic preservation and the relationship between Dirichlet evidence and CLIP's contrastive training lack direct empirical validation.

## Next Checks

1. **Semantic Magnitude Validation:** Conduct a quantitative analysis measuring the correlation between Dirichlet evidence α_k and actual semantic similarity (e.g., using text similarity metrics or human judgment) across multiple domains.

2. **Robustness to OOD Calibration Targets:** Design experiments where clean training data contains varying levels of distribution shift, measuring how UCAT's performance degrades as clean samples become less reliable calibration targets.

3. **Temperature Scaling Sensitivity:** Systematically sweep both τ and τ' across multiple orders of magnitude on a held-out validation set, measuring not just accuracy but the stability of uncertainty estimates (AU/EU decomposition) to identify potential brittleness in the calibration mechanism.