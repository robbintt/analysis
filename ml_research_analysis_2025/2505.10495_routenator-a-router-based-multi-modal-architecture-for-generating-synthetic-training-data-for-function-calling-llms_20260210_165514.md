---
ver: rpa2
title: 'RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic
  Training Data for Function Calling LLMs'
arxiv_id: '2505.10495'
source_url: https://arxiv.org/abs/2505.10495
tags:
- data
- content
- queries
- synthetic
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-tuning large language
  models for function calling tasks when real user interaction data is unavailable,
  particularly in digital content creation tools where natural language queries must
  be mapped to API calls. The authors present a novel router-based architecture that
  leverages domain resources like content metadata and structured knowledge graphs,
  along with text-to-text and vision-to-text language models to generate high-quality
  synthetic training data.
---

# RouteNator: A Router-Based Multi-Modal Architecture for Generating Synthetic Training Data for Function Calling LLMs

## Quick Facts
- arXiv ID: 2505.10495
- Source URL: https://arxiv.org/abs/2505.10495
- Reference count: 8
- Primary result: Router-based multi-modal architecture generates synthetic data that improves function-calling LLM fine-tuning, achieving F1=0.881, CTA=0.756, SS=0.918

## Executive Summary
This paper addresses the challenge of fine-tuning large language models for function calling tasks when real user interaction data is unavailable, particularly in digital content creation tools where natural language queries must be mapped to API calls. The authors present a novel router-based architecture that leverages domain resources like content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models to generate high-quality synthetic training data. Their approach uses a weighted routing mechanism that directs query generation requests to specialized prompt templates based on population-level statistics, ensuring synthetic data matches observed real-world distributions. Evaluation on a comprehensive set of real user queries demonstrates significant improvements in both function classification accuracy and API parameter selection.

## Method Summary
The authors developed a router-based architecture that generates synthetic training data for function-calling LLMs by combining domain knowledge graphs, content metadata, and multi-modal generation (text-to-text via Llama-3.1-70B-Instruct and vision-to-text via InternVL-40B). A weighted router directs generation requests to specialized prompt templates based on population-level statistics, ensuring distributional alignment with real-world query patterns. The synthetic data is then used to fine-tune Llama models using QLoRA with 4-bit quantization, achieving significant improvements in function classification and parameter extraction compared to heuristic or single-prompt approaches.

## Key Results
- Fine-tuned Gorilla model achieves function call F1-score of 0.881 on golden dataset
- Content type accuracy reaches 0.756, demonstrating strong API parameter selection
- Subprompt similarity of 0.918 indicates high-quality query-to-API mapping
- Router-based approach outperforms heuristic-only (F1=0.802) and single-prompt-only (F1=0.794) baselines
- Weighted routing ensures synthetic data distribution matches real-world query patterns

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Weighted routing of generation requests to specialized prompt templates produces synthetic data that better matches real-world query distributions than uniform or single-prompt approaches.
- **Mechanism:** A router samples from multiple specialized prompt templates using weights derived from population-level statistics (e.g., observed ratios of Search vs. Generate queries, content type frequencies). This enforces distributional alignment at generation time rather than post-hoc rebalancing.
- **Core assumption:** Population-level statistics from available domain data (metadata, knowledge graphs) are sufficiently representative of the target user query distribution.
- **Evidence anchors:**
  - [abstract] "weighted routing mechanism that directs query generation requests to specialized prompt templates based on population-level statistics, ensuring synthetic data matches observed real-world distributions"
  - [section 3.3] "weighted router that directs query generation requests to appropriate prompt templates based on population-level statistics"
  - [corpus] Related work on router-based multi-agent systems (arXiv:2506.13811) shows task classification via routing improves over single-agent processing, but does not validate the specific weighting scheme used here.

### Mechanism 2
- **Claim:** Multi-modal generation (text-to-text + vision-to-text) increases query diversity and captures visual semantics not present in metadata alone.
- **Mechanism:** InternVL processes template images to generate queries describing visual elements (layouts, colors, text arrangements) that metadata keywords cannot fully encode. This provides an orthogonal generation path to text-only LLM generation.
- **Core assumption:** Visual features in templates/images contain task-relevant information that users naturally reference in queries but that is not captured in structured metadata.
- **Evidence anchors:**
  - [abstract] "leverages domain resources like content metadata and structured knowledge graphs, along with text-to-text and vision-to-text language models"
  - [section 3.2] "generates queries based on actual domain-specific corpus images and visual representations... leading to more natural descriptions and increased output diversity"
  - [corpus] Synthetic data generation for multi-modal tasks (arXiv:2505.01823, arXiv:2511.04000) similarly combines generative models for diversity, but lacks comparative evaluation against text-only baselines.

### Mechanism 3
- **Claim:** Structured domain knowledge (knowledge graphs + content metadata) grounds synthetic queries in semantically coherent entity relationships, improving parameter extraction accuracy.
- **Mechanism:** Knowledge graph edges encode valid relationships (e.g., UserIntent→DesignType→SceneObject→Action). Synthetic queries are constructed by traversing these edges, ensuring generated queries reference valid parameter combinations (content types, actions) that the target API actually supports.
- **Core assumption:** The knowledge graph comprehensively covers the domain's valid entity relationships and parameter combinations.
- **Evidence anchors:**
  - [abstract] "leverages domain resources like content metadata and structured knowledge graphs"
  - [section 3.1] Knowledge graph contains "interconnected nodes representing User Intents, Design Types, Scene Objects, and associated Actions... enable the generation of semantically coherent queries"
  - [corpus] Weak direct evidence; related work on synthetic data generation (arXiv:2511.18335) addresses structure generation but not domain-specific knowledge grounding for function calling.

## Foundational Learning

- **Concept: Quantized Low-Rank Adaptation (QLoRA)**
  - **Why needed here:** Fine-tuning large models (Gorilla, 7B+ parameters) with limited compute resources requires parameter-efficient methods. QLoRA enables training on 4 A100/A10 GPUs.
  - **Quick check question:** Can you explain why 4-bit quantization with low-rank adapters preserves model quality while reducing memory by ~4x compared to full fine-tuning?

- **Concept: Function Calling / Tool Use in LLMs**
  - **Why needed here:** The target task maps natural language to structured API calls (function name, content_type, extracted_prompt). Understanding function calling frameworks (Gorilla, Toolformer) is prerequisite to designing evaluation metrics.
  - **Quick check question:** Given a user query "Create a birthday invitation for my nephew," what structured output should a function-calling model produce for this system's API?

- **Concept: Knowledge Graph Construction for NLP**
  - **Why needed here:** The architecture depends on a domain-specific knowledge graph encoding valid entity relationships. Understanding how to construct, validate, and traverse KGs is required to extend this approach.
  - **Quick check question:** If you observe that generated queries frequently reference "Podcast" content type but the KG lacks Podcast→Action edges, where is the failure and how would you fix it?

## Architecture Onboarding

- **Component map:** Metadata/KG → Weighted Router → (Text Template OR Vision Template) → LLM Generation → Validation → Synthetic Dataset → QLoRA Fine-tuning → Evaluation on Golden Dataset

- **Critical path:** Domain resources (knowledge graph, metadata) flow through the weighted router to specialized prompt templates, which are processed by text-to-text and vision-to-text LLMs. Generated queries undergo validation filtering before becoming training data for QLoRA fine-tuning.

- **Design tradeoffs:**
  - **Heuristic vs LLM generation:** Heuristics are fast and controllable but low diversity; LLMs are diverse but harder to control. Router blends both.
  - **Text-only vs Multi-modal:** Vision-to-text adds diversity but increases pipeline complexity and inference cost.
  - **Prompt-tuning vs Full fine-tuning:** Paper shows prompt-tuning (F1=0.881) marginally outperforms fine-tuning on combined data (F1=0.875), but prompt-tuning may be more brittle to distribution shift.

- **Failure signatures:**
  - **Content type imbalance:** If router weights are wrong, downstream model overfits to dominant content types (e.g., Images at 60%+).
  - **Keyword position bias:** If templates always place content type keywords at fixed positions, model learns positional shortcuts rather than semantic understanding.
  - **Length distribution mismatch:** If Search queries exceed 10 words in synthetic data, model may not learn the brevity pattern expected in production.

- **First 3 experiments:**
  1. **Ablate the router:** Train models on (a) heuristic-only, (b) single-prompt-only, (c) router-based synthetic data. Compare F1, CTA, SS metrics to quantify router contribution. Expected: router-based > single-prompt > heuristic based on Table 3.
  2. **Validate distribution matching:** Compare content type distributions and word count IQR between synthetic datasets and a held-out sample of real queries (Figure 4, Figure 3). Use Chi-square test for distribution alignment.
  3. **Vision ablation:** Generate datasets with and without InternVL component; measure impact on query diversity (unique n-grams, positional variance) and downstream model performance. Expect marginal improvement if visual features are task-relevant.

## Open Questions the Paper Calls Out

- **Question:** How does the router-based architecture generalize to function-calling tasks in domains beyond digital content creation?
  - **Basis in paper:** [explicit] Future work states: "its underlying principles could be generalized to other domains requiring sophisticated function-calling mechanisms"
  - **Why unresolved:** All experiments were conducted exclusively on Adobe's digital content creation platform; no cross-domain evaluation was performed.
  - **What evidence would resolve it:** Benchmark results on at least one additional domain (e.g., healthcare APIs, e-commerce, financial services) showing comparable F1-scores to the 0.881 achieved in this domain.

- **Question:** Can multilingual query processing maintain function call accuracy comparable to English-only performance?
  - **Basis in paper:** [explicit] Future work identifies "extending the system's linguistic capabilities to support multilingual query processing" as a primary direction
  - **Why unresolved:** All training data, synthetic generation, and evaluation were conducted in English only.
  - **What evidence would resolve it:** Cross-lingual evaluation across 3+ languages showing F1-scores within 5-10% of the English baseline of 0.881.

- **Question:** How does the architecture scale when the number of function types increases significantly beyond the binary Search/Generate classification?
  - **Basis in paper:** [explicit] Future work mentions exploring "extensibility to support additional specialized functions and API calls would provide insights into the scalability of our approach across different functional domains"
  - **Why unresolved:** The paper only evaluates a two-class function classification problem; performance with 10+, 50+, or 100+ distinct functions remains unknown.
  - **What evidence would resolve it:** Performance benchmarks showing F1-score degradation curves as the number of supported functions scales incrementally.

- **Question:** Does the Content Type Accuracy ceiling of 0.756 indicate a fundamental limitation in the synthetic data's ability to capture nuanced content type distinctions?
  - **Basis in paper:** [inferred] The conclusion notes "further improvement is possible in content-type classification" despite high F1 (0.881) and Subprompt Similarity (0.918); the 0.756 CTA lags significantly behind other metrics
  - **Why unresolved:** The paper does not analyze specific failure modes in content type classification or whether synthetic data generation captures subtle content type boundaries.
  - **What evidence would resolve it:** Error analysis on the golden dataset identifying whether misclassifications cluster around specific content type pairs, combined with targeted synthetic data augmentation for those cases.

## Limitations

- The knowledge graph and metadata dependencies are proprietary and not fully specified, making exact reproduction difficult
- The weighted router's population statistics are not disclosed, preventing exact replication of the routing mechanism
- The golden evaluation dataset of 460 real user queries is not released, blocking direct performance comparison
- The Subprompt Similarity metric calculation method is underspecified, lacking details on embedding models and similarity functions

## Confidence

- **High confidence:** The core routing architecture and multi-modal generation approach is well-specified and internally consistent. The evaluation methodology (F1, CTA, SS metrics) is clearly defined and reproducible with equivalent datasets.
- **Medium confidence:** The QLoRA fine-tuning configuration and training procedure are precisely specified, but the proprietary nature of knowledge graph and metadata sources creates uncertainty about whether synthetic data generation can achieve comparable quality.
- **Low confidence:** Claims about population-level statistics driving distributional alignment cannot be independently verified without the actual routing weights and target distributions.

## Next Checks

1. **Ablate the router:** Train function-calling models on synthetic data generated using (a) heuristic-only templates, (b) single-prompt-only templates, and (c) the full router-based approach. Compare F1-score, content type accuracy, and subprompt similarity metrics. This quantifies the router's contribution and validates whether weighted routing significantly outperforms simpler generation strategies.

2. **Validate distribution matching:** Construct a held-out sample of real user queries (even a small sample) and compare the synthetic dataset's content type distribution and word count distribution using Chi-square tests and IQR analysis. Ensure Search queries average <10 words and Generate queries average <40 words, matching the reported distributions in Figures 3 and 4.

3. **Vision ablation study:** Generate two synthetic datasets—one using text-to-text LLM generation only, and one combining text-to-text with vision-to-text generation. Measure query diversity metrics (unique n-grams, positional variance of content type keywords) and downstream model performance. This validates whether the InternVL component meaningfully improves query diversity and model accuracy beyond text-only generation.