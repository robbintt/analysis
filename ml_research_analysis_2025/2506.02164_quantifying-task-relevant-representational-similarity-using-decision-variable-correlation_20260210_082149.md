---
ver: rpa2
title: Quantifying task-relevant representational similarity using decision variable
  correlation
arxiv_id: '2506.02164'
source_url: https://arxiv.org/abs/2506.02164
tags:
- neural
- decision
- representations
- accuracy
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Decision Variable Correlation (DVC) to quantify
  task-relevant representational similarity between models and brains. DVC measures
  the correlation between decoded decisions on individual samples, focusing on task-relevant
  information rather than general representational alignment.
---

# Quantifying task-relevant representational similarity using decision variable correlation

## Quick Facts
- arXiv ID: 2506.02164
- Source URL: https://arxiv.org/abs/2506.02164
- Reference count: 40
- Key outcome: Decision Variable Correlation (DVC) quantifies task-relevant representational similarity between models and brains, revealing that higher ImageNet accuracy correlates with lower model-monkey DVC.

## Executive Summary
This paper introduces Decision Variable Correlation (DVC) to measure task-relevant representational similarity between neural systems and computational models. DVC focuses on the correlation between decoded decisions rather than general representational alignment, isolating task-relevant dimensions via linear discriminant analysis. The method was applied to monkey V4/IT recordings and image classification models, revealing that while model-model similarity matches monkey-monkey similarity, model-monkey similarity is consistently lower. Surprisingly, DVC decreases with increasing ImageNet-1k performance, suggesting a fundamental divergence between task-relevant representations in primate visual cortex and those learned by standard classification models.

## Method Summary
DVC measures task-relevant representational similarity by extracting decision variables (DVs) from neural representations using linear discriminant analysis (LDA) for each class pair. The Pearson correlation between DVs across observers quantifies their agreement on the task-relevant axis. A split-half reliability correction normalizes for noise, providing an unbiased estimate of the true correlation. The method was applied to monkey V4/IT recordings and model activations, with PCA dimensionality reduction used to stabilize the LDA decoding process.

## Key Results
- Model-model similarity was comparable to monkey-monkey similarity
- Model-monkey similarity was consistently lower than within-category similarity
- DVC decreased with increasing ImageNet-1k performance (Pearson correlation = -0.70)
- Adversarial training increased model-model similarity but not model-monkey similarity
- Pre-training on larger datasets did not improve model-monkey similarity

## Why This Works (Mechanism)

### Mechanism 1: Task-Relevant Axis Extraction via Linear Decoding
- Claim: DVC isolates representational similarity along dimensions used for a specific classification task, ignoring task-irrelevant structure.
- Mechanism: For each class pair, LDA identifies the axis that maximizes class separation in the representation. All samples are projected onto this axis to produce a decision variable (DV). The correlation of these DVs between two observers (brain or model) forms the DVC, quantifying agreement in their task-specific decision strategy.
- Core assumption: A linear readout (LDA) is a sufficient proxy for the task-relevant readout used by the system. Noise in DV estimates is independent and additive.
- Evidence anchors:
  - [abstract] "DVC measures the correlation between decoded decisions on individual samples, focusing on task-relevant information rather than general representational alignment."
  - [section 3.1] "We can take its neural representation and find the optimal decision axis... This correlation captures the similarity of the encoding and the decoding into a decision for the two observers in this classification task."
  - [corpus] Limited direct evidence. The neighbor paper *Bridging Functional and Representational Similarity via Usable Information* links functional similarity to stitching, which is a conceptually related but distinct approach to task-relevant information.
- Break condition: The true readout is highly non-linear; task-irrelevant features strongly covary with task-relevant ones; noise structure is highly non-Gaussian or correlated.

### Mechanism 2: Noise-Correction via Split-Half Reliability
- Claim: The split-half normalization procedure removes attenuation bias from measurement noise, providing an unbiased estimate of the true correlation.
- Mechanism: Each observer's DV signal is split into two independent halves. The geometric mean of all cross-observer split correlations (rcross) is divided by the geometric mean of the within-observer (self) reliabilities (rself). This algebraically cancels the variance contributed by independent, additive noise.
- Core assumption: Noise is independent, additive, and symmetric across splits and observers; splits are identically distributed.
- Evidence anchors:
  - [section 3.2] "To correct for the attenuation bias introduced by noise, we split each DV into two independent halves... yields an unbiased estimate of the true underlying correlation."
  - [appendix A.1] Contains the mathematical proof demonstrating that ρ_corrected = ρ_true under the assumed noise model.
  - [corpus] No direct corpus support for this specific normalization formula.
- Break condition: Noise is correlated across splits; noise is non-additive; insufficient neurons or samples to form reliable splits.

### Mechanism 3: Negative Correlation of ImageNet Accuracy with Brain DVC
- Claim: Models achieving higher classification accuracy on ImageNet-1k exhibit lower task-relevant representational similarity (DVC) to monkey V4/IT cortex.
- Mechanism: This is presented as a key empirical finding. The paper suggests that optimizing for accuracy on a finite, specific dataset may lead models to learn features or overfit to statistics that diverge from the task-relevant representations learned by the primate visual system through evolution and broader experience.
- Core assumption: The monkey V4/IT dataset and the models tested are representative; ImageNet-1k accuracy is a meaningful performance metric.
- Evidence anchors:
  - [abstract] "Surprisingly, DVC decreases with increasing ImageNet-1k performance, contrary to expectations."
  - [section 4.2] "We find the opposite, that is, networks with higher top-1 accuracy on ImageNet-1k generally have lower DVC with IT/V4 representation (Pearson correlation = -0.70...)."
  - [corpus] Weak supporting evidence. One neighbor paper cited in the text (*One Hundred Neural Networks and Brains Watching Videos*) reported a negative correlation between brain alignment and model complexity using RSA, which aligns directionally but uses a different metric.
- Break condition: The correlation is specific to this dataset, brain area, or model set; the relationship is non-monotonic at scales not tested.

## Foundational Learning

- **Concept: Signal Detection Theory (SDT)**
  - Why needed here: DVC is a generalization of SDT. SDT models decisions using a continuous decision variable (DV) and a criterion. Understanding DVs and their correlation is core to DVC.
  - Quick check question: How can two observers have identical task accuracy but completely different trial-by-trial decision variable correlations?

- **Concept: Representational Similarity Analysis (RSA)**
  - Why needed here: RSA is the primary methodological counterpoint. It measures general geometrical similarity, while DVC isolates task-relevant similarity, making the distinction crucial for interpreting results.
  - Quick check question: Could two representations have a high DVC but a low RSA score? Why or why not?

- **Concept: Linear Discriminant Analysis (LDA)**
  - Why needed here: LDA is the concrete tool used to extract the decision axis. Its objective (maximizing class separation) defines exactly what "task-relevant" means in this framework.
  - Quick check question: What are the assumptions of LDA, and why can it become unstable in high-dimensional spaces with few samples?

## Architecture Onboarding

- **Component map:** Input data (neural recordings, model activations) -> PCA dimensionality reduction -> LDA decoder training for each class pair -> Decision variable extraction -> Correlation computation -> Split-half noise correction -> Aggregation of DVC scores

- **Critical path:** The most fragile step is obtaining a stable LDA projection in high-dimensional, low-sample-count regimes. This is addressed by the PCA step. The noise correction is mathematically sound but sensitive to violations of its independence assumptions.

- **Design tradeoffs:**
  - **LDA vs. Other Linear Decoders:** The paper shows results are consistent across Logistic Regression and linear SVM. LDA is chosen for its optimality under Gaussian assumptions. Non-linear decoders risk overfitting.
  - **PCA Dimension Choice:** Results are robust across a range (10-50 PCs). The trade-off is between losing information (too few) and re-introducing instability (too many).
  - **Pearson vs. Spearman Correlation:** Results were consistent. Pearson is standard but sensitive to outliers; Spearman is a robust alternative.

- **Failure signatures:**
  - **DVC estimates near zero despite high task accuracy:** Indicates genuine representational divergence or a failure in the DV extraction (e.g., LDA finding a spurious axis).
  - **Normalized DVC > 1:** Can occur under extreme noise conditions, as noted in the appendix.
  - **Inconsistent results across linear decoders:** Would suggest the specific decoder choice is influencing the measured task-relevant subspace, warranting investigation.

- **First 3 experiments:**
  1. **Replication on Public Data:** Obtain the Majaj et al. (2015) monkey V4/IT dataset and a set of pretrained Torchvision models. Implement the full pipeline and verify the key negative correlation between model ImageNet accuracy and model-monkey DVC.
  2. **Decoder Sensitivity Analysis:** Re-run the DVC analysis on the same data using Logistic Regression and a linear SVM instead of LDA. Confirm that the core findings (e.g., negative correlation with accuracy) are consistent across different linear decoders.
  3. **Noise-Correction Validation:** Create a simulation with two representations having a known true DVC. Inject varying levels of independent Gaussian noise and verify that the split-half correction formula accurately recovers the true DVC. Then, inject correlated noise to test the break condition.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can training neural networks on ecologically motivated datasets or with explicit internal noise models close the representational gap with primate visual cortex?
- Basis in paper: [explicit] The authors suggest these two strategies might be promising to improve the currently low model-monkey DVC.
- Why unresolved: Current scaling methods (ImageNet-21k, JFT-300M) and adversarial training failed to improve alignment.
- What evidence would resolve it: A demonstration that models trained on ecologically valid datasets or noise-aware objectives achieve higher DVC scores with V4/IT recordings.

### Open Question 2
- Question: How do shared behavioral biases and underlying decision consistency interact when combining behavioral and neural DVC analyses?
- Basis in paper: [explicit] The discussion proposes combining behavioral and neural representation levels to dissect the contributions of consistency versus shared biases.
- Why unresolved: This study focused primarily on neural representations using a bias-invariant metric, leaving the full decomposition of behavior unexplored.
- What evidence would resolve it: Simultaneous measurement of behavioral DVC and neural DVC in the same subjects to model the variance components.

### Open Question 3
- Question: Would increasing the sample size of neural recordings significantly change the estimated model-monkey DVC?
- Basis in paper: [explicit] The authors list the limited number of monkey subjects (n=2) and simultaneously recorded neurons as a limitation affecting estimate accuracy.
- Why unresolved: It is unclear if the observed low model-monkey similarity is robust to higher sampling density or if it reflects a fundamental divergence.
- What evidence would resolve it: Repeating the DVC analysis on large-scale neural datasets with hundreds of subjects or high-density electrode arrays.

## Limitations

- The negative correlation between ImageNet accuracy and model-monkey DVC is correlational and could be influenced by uncontrolled factors such as architectural biases or the specific composition of the monkey V4/IT dataset.
- The analysis focuses on a relatively small number of class pairs (50) and a limited set of model architectures (9), which may not be representative of the broader space of vision models.
- The assumption that LDA extracts a meaningful "task-relevant" axis is critical but untested against non-linear alternatives in this framework.

## Confidence

- **High Confidence:** The mathematical framework of DVC and the noise-correction procedure are sound and well-explained. The method is a valid and useful tool for quantifying task-relevant representational similarity.
- **Medium Confidence:** The empirical finding of lower model-monkey DVC compared to model-model or monkey-monkey DVC is robust within the tested conditions, but its generality is uncertain. The negative correlation with ImageNet accuracy is statistically significant in the tested set but may not generalize to other datasets, model families, or brain regions.
- **Low Confidence:** The specific mechanistic explanation for why higher ImageNet accuracy correlates with lower brain alignment (e.g., overfitting to dataset statistics) is speculative and requires further investigation.

## Next Checks

1. **Replication on Public Data:** Obtain the Majaj et al. (2015) monkey V4/IT dataset and a set of pretrained Torchvision models. Implement the full DVC pipeline and verify the key negative correlation between model ImageNet accuracy and model-monkey DVC.

2. **Decoder Sensitivity Analysis:** Re-run the DVC analysis on the same data using Logistic Regression and a linear SVM instead of LDA. Confirm that the core findings (e.g., the negative correlation with accuracy) are consistent across different linear decoders, which would strengthen confidence in the task-relevant subspace being measured.

3. **Noise-Correction Validation:** Create a simulation with two representations having a known true DVC. Inject varying levels of independent Gaussian noise and verify that the split-half correction formula accurately recovers the true DVC. Then, inject correlated noise to test the break condition of the correction method.