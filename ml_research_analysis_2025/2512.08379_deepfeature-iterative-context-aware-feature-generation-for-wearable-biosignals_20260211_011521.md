---
ver: rpa2
title: 'DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals'
arxiv_id: '2512.08379'
source_url: https://arxiv.org/abs/2512.08379
tags:
- feature
- features
- deepfeature
- data
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'DeepFeature introduces a context-aware feature generation framework
  for wearable biosignals that combines expert knowledge, task-specific settings,
  and iterative refinement. The system generates features from three sources: direct
  LLM generation, knowledge-guided prompts, and operator-based combinations.'
---

# DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals

## Quick Facts
- arXiv ID: 2512.08379
- Source URL: https://arxiv.org/abs/2512.08379
- Reference count: 40
- Primary result: 4.21-9.67% average AUROC improvement over baselines on 8 healthcare tasks

## Executive Summary
DeepFeature introduces an LLM-empowered framework for context-aware feature generation from wearable biosignals. The system combines expert knowledge, task-specific settings, and iterative refinement to automatically generate and select features from ECG, GSR, EEG, PPG, and ACC data. DeepFeature outperforms state-of-the-art approaches on five of eight healthcare tasks while maintaining comparable performance on the remaining tasks, demonstrating the effectiveness of context-aware, iterative feature generation for diverse wearable biosignal applications.

## Method Summary
DeepFeature employs a multi-source feature generation mechanism that integrates direct LLM generation, knowledge-guided prompts from retrieved literature, and operator-based combinations of high-importance features. The framework uses a four-layer filtering pipeline to ensure robust code generation and execution, followed by iterative refinement driven by performance feedback. Features are evaluated using Random Forest with Recursive Feature Elimination, and the process repeats until convergence or a maximum iteration count is reached.

## Key Results
- Achieved 4.21-9.67% average AUROC improvement over baselines
- Outperformed state-of-the-art approaches on five of eight healthcare tasks
- Maintained comparable performance on remaining three tasks
- Demonstrated effectiveness across eight healthcare tasks using four datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-source feature generation produces more diverse, task-relevant features than single-source approaches.
- Mechanism: DeepFeature aggregates features from three sources: direct LLM generation, context-guided generation using retrieved literature, and operator-based combinations applied to high-importance feature pairs.
- Core assumption: Task-specific contextual knowledge from literature can be effectively retrieved and translated into useful feature specifications by LLMs.
- Evidence anchors: Abstract mentions "multi-source feature generation mechanism"; Section 3.2 describes all three sources with Figure 8 showing 20-80% of top-10 features are operator-generated.

### Mechanism 2
- Claim: Iterative feedback-driven refinement improves feature quality over single-pass generation.
- Mechanism: After each iteration, model performance metrics are encoded into a feedback prompt that guides the LLM to target underperforming classes and generate complementary features.
- Core assumption: Performance metrics contain sufficient signal to guide LLMs toward better features.
- Evidence anchors: Abstract mentions "iterative feature refinement process"; Section 2.2 shows iterative refinement improves AUROC by 3.23% on WESAD.

### Mechanism 3
- Claim: Multi-layer filtering and output verification dramatically reduce code execution failures.
- Mechanism: Four sequential filters validate generated code before execution, and post-execution verification discards functions producing inconsistent dimensions across samples.
- Core assumption: Most code errors can be caught through static analysis and dimension consistency checks.
- Evidence anchors: Abstract mentions "multi-layer filtering and verification approach"; Section 2.3 shows distribution of four error types across trials.

## Foundational Learning

- **Concept: Recursive Feature Elimination (RFE)**
  - Why needed here: DeepFeature uses RFE to select informative features from the candidate set at each iteration.
  - Quick check question: Given a Random Forest trained on 100 features, how does RFE determine which 20 features to retain?

- **Concept: Retrieval-Augmented Generation (RAG) for domain knowledge**
  - Why needed here: Source 2 constructs a local knowledge base from literature, embedding text chunks and retrieving relevant context at runtime.
  - Quick check question: How does query-to-chunk similarity ranking affect the relevance of retrieved domain knowledge for a novel biosignal task?

- **Concept: Abstract Syntax Tree (AST) analysis for code validation**
  - Why needed here: The Function Extraction Filter and Body Content Filter rely on AST parsing to validate function structure.
  - Quick check question: What AST patterns would distinguish a complete function from one containing only a `pass` statement?

## Architecture Onboarding

- **Component map**: Initialization (task description → keyword extraction → knowledge base) → Iteration loop (multi-source generation → 4-layer filtering → code execution → RFE + model training → performance assessment → feedback) → Output (best model weights and feature set)
- **Critical path**: The feedback loop from model performance to LLM prompt construction drives iterative improvement.
- **Design tradeoffs**: Larger iteration strides increase diversity but raise computational cost; RFE is more accurate but slower than alternatives; filtering prioritizes iteration smoothness over debugging marginal functions.
- **Failure signatures**: Token limit exceeded from long error logs, zero valid features from aggressive filtering, feature drift from unrepresentative validation data.
- **First 3 experiments**:
  1. Reproduce WESAD baseline comparison with stride=10, 10 iterations, DeepSeek-V3 LLM, and Random Forest downstream.
  2. Ablation on filtering pipeline by disabling filters one at a time to measure code execution failure rate.
  3. Stride sensitivity sweep on PPG-BP dataset with strides from 1 to 20 to identify diminishing returns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can DeepFeature be extended to generate higher-order, learning-based features from heterogeneous sensor data beyond handcrafted functions?
- Basis in paper: Authors state that "higher-order, learning-based features still require trainable models rather than handcrafted functions" and suggest incorporating pretrained backbone networks.
- Why unresolved: Current framework only generates handcrafted statistical features.

### Open Question 2
- Question: How can experience-recording mechanisms effectively capture patterns in successful features to reduce LLM stochasticity-induced failures?
- Basis in paper: Authors mention exploring experience-recording mechanisms to capture common patterns in successful features.
- Why unresolved: No mechanism currently exists to persist and leverage successful feature patterns across iterations or tasks.

### Open Question 3
- Question: What are optimal convergence criteria for the iterative refinement loop beyond fixed iteration counts?
- Basis in paper: The algorithm terminates at a predefined maximum iteration without early stopping criteria based on performance plateau.
- Why unresolved: Fixed iterations may waste computation when performance saturates early.

### Open Question 4
- Question: How well does the generated feature set generalize to population shifts not represented in the knowledge base?
- Basis in paper: The SEN dataset revealed that optimal features differ significantly between special education needs children and typical individuals.
- Why unresolved: Knowledge retrieval depends on existing literature which may underrepresent certain populations.

## Limitations

- **Prompt dependency**: Performance critically depends on prompt engineering quality not fully specified in the paper.
- **Knowledge base relevance**: Effectiveness assumes retrieved literature is relevant to the specific biosignal task and modality.
- **Code generation reliability**: Framework filters-and-discards rather than debugging erroneous code, potentially wasting useful features.

## Confidence

- **High confidence**: Multi-source generation mechanism and filtering pipeline are well-described with direct performance improvements demonstrated.
- **Medium confidence**: Iterative refinement mechanism is theoretically sound but external validation across diverse tasks is limited.
- **Low confidence**: Knowledge base construction details and exact prompt formats are underspecified, making exact replication challenging.

## Next Checks

1. **Knowledge base relevance test**: Implement controlled experiment with task-irrelevant literature to measure feature quality degradation.
2. **Iterative refinement sensitivity**: Run pipeline with feedback disabled versus enabled on WESAD to quantify AUROC improvement attributable to feedback loop.
3. **Filtering ablation study**: Systematically disable each of the four filters to measure code execution success rate, iteration completion rate, and final model performance.