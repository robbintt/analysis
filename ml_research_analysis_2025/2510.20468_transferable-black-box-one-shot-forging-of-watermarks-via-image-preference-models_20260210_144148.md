---
ver: rpa2
title: Transferable Black-Box One-Shot Forging of Watermarks via Image Preference
  Models
arxiv_id: '2510.20468'
source_url: https://arxiv.org/abs/2510.20468
tags:
- image
- watermark
- watermarking
- watermarked
- watermarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of watermark forging attacks,
  where an adversary can steal and apply watermarks from genuine content to malicious
  content. The authors propose a novel method that uses a preference model trained
  on procedurally generated synthetic artifacts to assess whether an image contains
  a watermark.
---

# Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models

## Quick Facts
- arXiv ID: 2510.20468
- Source URL: https://arxiv.org/abs/2510.20468
- Reference count: 40
- Key outcome: Novel method for black-box watermark forging using a preference model trained on synthetic artifacts, achieving high bit accuracy without access to watermarking models

## Executive Summary
This paper presents a novel approach to black-box watermark forging and removal that requires only a single watermarked image. The method trains a preference model using synthetic artifacts (wave patterns, noise, line patterns) in Fourier space with Bradley-Terry ranking loss. This model learns to score images based on artifact presence, enabling watermark extraction through backpropagation without access to the watermarking model or paired data. The approach demonstrates strong performance across various post-hoc watermarking schemes, raising questions about the security of current watermarking methods.

## Method Summary
The method consists of training a ConvNeXt V2-Tiny preference model on procedurally generated synthetic artifacts using Bradley-Terry ranking loss with adversarial perturbations on negative samples. During attack, a single watermarked image is optimized through gradient ascent to maximize the preference score, extracting a residual watermark that can be added to any target image. The entire process requires no access to the watermarking model, decoder, or paired training data, making it a one-shot black-box attack.

## Key Results
- Achieves 83% bit accuracy for CIN watermarking with 31.3 PSNR image quality
- Successfully transfers watermarks across different schemes (61% accuracy for MBRS)
- Outperforms Deep Image Prior baseline while requiring only 6 seconds for 50 optimization steps
- Works across multiple watermarking schemes (CIN, MBRS, TrustMark, Video Seal) without method-specific training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A preference model trained with ranking loss on synthetic artifacts can serve as a universal watermark detector without access to real watermarked images.
- Mechanism: The model learns to score images based on artifact presence by contrasting clean images (preferred) against synthetically corrupted variants (dispreferred) using Bradley-Terry preference modeling. This creates an "artifact prior" that generalizes to real watermarks.
- Core assumption: Synthetic artifacts (wave patterns, noise, line patterns in Fourier space) sufficiently approximate the statistical structure of real watermarking artifacts.
- Evidence anchors:
  - [abstract] "model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks"
  - [section 3.1] Details Bradley-Terry formulation and three artifact types (wave, noise, line) with specific Fourier-space parameterization
  - [corpus] Weak direct evidence—no corpus papers replicate this exact synthetic-to-real transfer; related work assumes access to real watermarked data
- Break condition: If target watermarks have semantic (non-artifact) structure (e.g., Tree-Ring alters object positions), the artifact prior fails. Authors acknowledge this limitation.

### Mechanism 2
- Claim: Adversarial perturbations during training produce semantically interpretable gradients for watermark extraction via backpropagation.
- Mechanism: Adding gradient-based perturbations to negative samples (x⁻ = x⁻ + ε·∇x⁻R(x⁻)) during training acts as data augmentation and ensures the learned preference function has smooth, meaningful gradients rather than adversarial noise patterns.
- Core assumption: Perturbation magnitude smaller than watermark artifacts preserves preference ordering while improving gradient quality.
- Evidence anchors:
  - [section 3.1] "backpropagating through the preference model trained with Equation (2) not only fails to reduce the watermark artifacts but also adds new artifacts such as checkerboard patterns"
  - [table 3, lines 3 vs 5] Without perturbation: 0.97 CIN accuracy; With perturbation: 1.00 CIN accuracy, 0.83 vs 0.65 MBRS
  - [figure 6] Visual comparison shows checkerboard artifacts without adversarial training
  - [corpus] No corpus papers address gradient interpretability for watermark attacks
- Break condition: If the preference model overfits to synthetic artifacts without diversity, adversarial training may not generalize. Table 4 shows combining all three artifact types outperforms single types.

### Mechanism 3
- Claim: Gradient ascent on the preference score separates watermark artifacts from image content via a simple subtraction operation.
- Mechanism: Optimizing δ to maximize R(xw - δ) extracts a residual w = xw - x̂ that captures watermark structure. This residual can then be added to any target image y for forging.
- Core assumption: Watermarks are additive, low-amplitude perturbations that preference model identifies as "dispreferred" content.
- Evidence anchors:
  - [section 3.2] Equation (3): w = argmax_δ R(xw - δ); forged image = y + w
  - [table 1] One-shot forging achieves 83% bit accuracy on CIN, 61% on MBRS with 31.3 PSNR
  - [figure 4] Visual extraction shows clear watermark patterns (green dots for CIN, noise grid for MBRS)
  - [corpus] "Deep Image Prior" baseline achieves comparable removal but requires per-image training; this method runs in 6 seconds for 50 steps
- Break condition: If watermarks are content-dependent (e.g., Video Seal embedder varies with image), extracted watermark may not transfer. Table 1 shows 83% accuracy on Video Seal suggests partial transfer despite content-awareness.

## Foundational Learning

- Concept: Bradley-Terry preference modeling
  - Why needed here: Core mathematical framework for training the artifact detector without binary classification thresholds
  - Quick check question: Given two images with scores R(x₁)=2.0 and R(x₂)=0.5, what is the probability x₁ is preferred over x₂? (Answer: exp(2.0)/(exp(2.0)+exp(0.5)) ≈ 0.82)

- Concept: Fourier-domain image analysis
  - Why needed here: Synthetic artifacts are generated in frequency space to mimic periodic watermark patterns
  - Quick check question: Why generate artifacts in Fourier space rather than pixel space? (Answer: Many watermarks manifest as periodic patterns more naturally expressed in frequency domain)

- Concept: Gradient-based image optimization
  - Why needed here: Watermark extraction uses backpropagation through a frozen preference model to modify pixels
  - Quick check question: Why use SGD with fixed learning rate rather than adaptive optimizers? (Answer: Constrained optimization budget controls distortion; adaptive methods may overshoot)

## Architecture Onboarding

- Component map:
  - Preference Model: ConvNeXt V2-Tiny backbone → single scalar score
  - Training Pipeline: SA-1b images → synthetic artifact injection → augmentation → ranking loss + adversarial perturbation
  - Attack Pipeline: Single watermarked image → resize to 768×768 → k-step gradient ascent → extract residual → add to target image

- Critical path:
  1. Preference model quality determines gradient semantics (adversarial training is critical)
  2. Artifact diversity during training determines generalization to unseen watermarks
  3. Optimization step count (k) trades off extraction quality vs image distortion (Table 5: k=500 achieves 31.3 PSNR vs k=50 at 39.3 PSNR)

- Design tradeoffs:
  - Single synthetic artifact type vs combination: Table 4 shows wave-only best for MBRS (0.83), but combination best overall
  - Training on real watermarks vs synthetic: Table 3 line 4 shows real watermarks perform worse (0.67 vs 0.83 MBRS) due to lack of diversity
  - Step count k: Higher k removes more bits but causes blurring in high-frequency regions (Figure 9)

- Failure signatures:
  - Checkerboard patterns in extracted watermark → missing adversarial training (Figure 6)
  - Content blurring in textured regions (water, grass) → excessive optimization steps (Figure 9)
  - Low bit accuracy on content-aware methods → decoder ignores source image context (TrustMark: 0.61 accuracy vs Video Seal: 0.83)

- First 3 experiments:
  1. **Validate preference model gradient quality**: Train model with/without adversarial perturbations, visualize gradients on a held-out watermarked image. Check for interpretable artifacts vs noise patterns.
  2. **Ablate synthetic artifact types**: Train three separate models (wave-only, noise-only, line-only) and measure MBRS/TrustMark extraction accuracy. Expect wave-only to dominate periodic watermarks.
  3. **Test transferability across watermark methods**: Extract watermark from CIN image, apply to MBRS-watermarked image, measure if detector accepts cross-method forgeries. This probes whether the preference model learns universal artifact structure vs method-specific patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can explicitly training watermark decoders to reject transferred watermarks effectively mitigate forging attacks without degrading detection performance?
- Basis in paper: [explicit] The authors suggest a safeguard where "the decoder is truly content-aware, e.g., by explicitly training the decoder to reject watermarks from different source image."
- Why unresolved: This defense mechanism is proposed as a mitigation strategy but is not implemented or validated in the current work.
- What evidence would resolve it: Experiments demonstrating that a content-aware decoder successfully lowers the bit accuracy of forged images (extracted via this method) while maintaining high accuracy for genuine watermarked content.

### Open Question 2
- Question: How can black-box forging attacks be adapted to compromise semantic watermarking schemes that alter image content during generation?
- Basis in paper: [explicit] The authors state in the Limitations section that "Semantic watermarking... watermarks AI-generated content by altering the objects... Our method cannot semantically change these objects; therefore, different methods must be used."
- Why unresolved: The current approach relies on separating additive post-hoc artifacts, which is fundamentally incompatible with semantic watermarks like Tree-Ring or RingID.
- What evidence would resolve it: A new attack framework capable of extracting or replicating semantic watermarks from a single image without access to the generative model's internals.

### Open Question 3
- Question: Does augmenting the preference model's training data with specific artifacts, such as blur, prevent the unintended smoothing of natural high-frequency textures?
- Basis in paper: [explicit] The authors note that the method may blur textures like water or grass, but "this issue can be partially mitigated through improved preference model training by introducing image blur as a new synthetic artifact type."
- Why unresolved: This proposed solution is mentioned as a mitigation strategy but was not included in the training data or ablated in the experiments.
- What evidence would resolve it: Ablation studies showing that a preference model trained with blur artifacts preserves sharpness in high-frequency image regions better than the current model.

## Limitations
- The method relies on synthetic artifacts approximating real watermark artifacts, which may not generalize to all watermarking schemes
- Content-aware watermarking schemes that alter image semantics cannot be forged with this approach
- High optimization steps may cause unintended blurring of natural image textures
- Performance varies significantly across different watermarking schemes (83% for CIN vs 61% for MBRS)

## Confidence
- **High Confidence**: The core mechanism of using preference models with synthetic artifacts for watermark detection is well-validated through ablation studies and controlled experiments
- **Medium Confidence**: The transferability of extracted watermarks across different watermarking schemes shows promise but has variable success rates
- **Low Confidence**: The approach's robustness against sophisticated watermarking defenses that adaptively modify embedding patterns or use semantic-aware embedding is unknown

## Next Checks
1. **Test Transferability Across Diverse Watermarking Schemes**: Extract watermarks from CIN images and attempt forging on MBRS-watermarked images. Measure whether the preference model accepts cross-method forgeries and quantify bit accuracy degradation.
2. **Evaluate Against Semantic-Aware Watermarking**: Apply the method to watermarks that modify content structure rather than adding artifacts (e.g., Tree-Ring). Assess whether the preference model can detect and extract such watermarks.
3. **Analyze Sensitivity to Optimization Parameters**: Systematically vary the number of optimization steps (k) and learning rate. Document the trade-off between watermark removal quality and image distortion across different artifact types and watermarking schemes.