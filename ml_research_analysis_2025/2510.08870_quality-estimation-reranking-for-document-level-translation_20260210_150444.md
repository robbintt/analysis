---
ver: rpa2
title: Quality Estimation Reranking for Document-Level Translation
arxiv_id: '2510.08870'
source_url: https://arxiv.org/abs/2510.08870
tags:
- translation
- metrics
- computational
- reranking
- pool
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Quality estimation (QE) reranking improves document-level machine
  translation quality, demonstrated by up to +5.09 BLEURT-20 score gains with 32 candidates
  using SLIDE and +4.30 with GEMBA-DA. Performance benefits are consistent across
  learned and LLM-based QE metrics and multiple translation models, including both
  NMT and LLM-based systems.
---

# Quality Estimation Reranking for Document-Level Translation

## Quick Facts
- arXiv ID: 2510.08870
- Source URL: https://arxiv.org/abs/2510.08870
- Reference count: 8
- Primary result: QE reranking improves document-level translation quality, with up to +5.09 BLEURT-20 score gains using SLIDE metric

## Executive Summary
Quality Estimation (QE) reranking for document-level machine translation consistently improves translation quality by selecting the best candidate from a pool of translations. The study demonstrates that document-level QE metrics outperform sentence-level averaging across multiple translation systems, with gains persisting even for long documents. SLIDE emerges as the strongest metric, achieving up to +5.09 BLEURT-20 score improvements with 32 candidates. The approach is resource-efficient, requiring only a small fraction of the time needed for translation when hardware constraints permit.

## Method Summary
The study evaluates QE reranking for document-level translation using the WMT23 English-Japanese test set, where documents are constructed by merging paragraph and sentence segments. Translation candidates are generated using ALMA-7B, NLLB-200-3.3B, and a proprietary Yaraku Translate system, creating pools of 1-32 candidates per source. QE metrics including Comet-Kiwi, SLIDE, GEMBA-DA, and EAPrompt score these candidates, with SLIDE operating on top of Comet-Kiwi with window size 7 and stride 7. The best-scoring candidate is selected at each pool size, and translation quality is evaluated using BLEURT-20 and COMET-22 metrics. Document-level QE consistently outperforms sentence-level averaging, with SLIDE achieving the highest gains.

## Key Results
- SLIDE achieves up to +5.09 BLEURT-20 score improvements with 32 candidates
- Document-level QE consistently outperforms sentence-level averaging across all tested configurations
- Performance gains persist even for documents exceeding 256 tokens, though diminished
- Reranking runtime is a small fraction of translation cost when hardware constraints allow

## Why This Works (Mechanism)
Document-level QE reranking improves translation quality by leveraging context-aware scoring that captures discourse-level phenomena such as coherence and consistency across sentences. Unlike sentence-level averaging, document-level metrics maintain inter-sentential relationships during evaluation, allowing them to identify candidates that better preserve document-level meaning and flow. The selection process benefits from having multiple candidates, as QE metrics can distinguish subtle quality differences that may be missed when only one translation is available.

## Foundational Learning
- **Quality Estimation (QE)**: Reference-free metrics that predict translation quality without human references. Needed to enable practical document-level reranking without requiring reference translations. Quick check: Can the metric provide meaningful scores without reference translations?
- **Document-level vs Sentence-level QE**: Document-level metrics consider full document context while sentence-level metrics average individual sentence scores. Needed to capture discourse-level phenomena like coherence and consistency. Quick check: Does the metric have access to document-level context?
- **Reranking**: Selecting the best candidate from a pool of translations based on metric scores. Needed to improve translation quality without requiring new model training. Quick check: Does the system maintain candidate pools and select based on QE scores?
- **Token limitations**: Most QE models constrained to 512 tokens, limiting evaluation of long documents. Needed to understand performance degradation for longer inputs. Quick check: What is the maximum token length supported by the QE metric?

## Architecture Onboarding

**Component map:** Translation models (ALMA-7B, NLLB-200-3.3B, Yaraku) -> Candidate pool generation -> QE metrics (Comet-Kiwi, SLIDE, GEMBA-DA, EAPrompt) -> Reranking -> Quality evaluation

**Critical path:** Source document → Translation generation → QE scoring → Candidate selection → Quality evaluation

**Design tradeoffs:** Larger candidate pools provide better quality but increase computational cost; document-level QE captures context but requires more complex metrics; LLM-based QE offers flexibility but may have unreliable scoring.

**Failure signatures:** Performance degradation for documents >256 tokens; frequent tied scores with simple LLM prompts; metric failures due to token truncation; hallucination issues in NMT/LLM outputs.

**First experiments:**
1. Generate candidate pools with ALMA-7B and NLLB-200-3.3B for a small document subset
2. Score candidates with Comet-Kiwi and SLIDE, then rerank and evaluate with BLEURT-20
3. Compare document-level QE scores against sentence-level averaging on the same candidates

## Open Questions the Paper Calls Out
- At what candidate pool size does document-level QE reranking performance reach a clear saturation point? The study was limited to 32 candidates, leaving upper bounds undefined.
- Do automatic metric improvements correlate with human judgment? The study lacks human evaluation, risking overfitting to automatic metrics.
- Can performance degradation for documents >256 tokens be mitigated by metrics with larger context windows? Current QE models are constrained to 512 tokens.
- Can more sophisticated prompting strategies improve LLM-based QE reliability? Resource constraints limited exploration of diverse prompting methods.

## Limitations
- Yaraku Translate system is proprietary with unspecified architecture and training details
- LLM prompts for GEMBA-DA and EAPrompt are minimally specified, limiting reproducibility
- Document merging strategy may not generalize to naturally occurring document structures
- Evaluation limited to English-Japanese language pair, limiting cross-linguistic claims

## Confidence
- **High confidence**: Document-level QE consistently outperforms sentence-level averaging; SLIDE demonstrates superior performance; gains persist for longest documents
- **Medium confidence**: Relative performance ranking of QE metrics holds across translation systems, though absolute differences vary
- **Low confidence**: Generalizability to other language pairs; exact reproducibility given proprietary dependencies and unspecified LLM prompts

## Next Checks
1. Replicate core findings using publicly available translation systems (Marian NMT or open-source LLMs) to verify gains are not dependent on proprietary components
2. Test document merging strategies on naturally occurring document corpora rather than artificially segmented test sets
3. Conduct ablation studies varying QE metric prompts and hyperparameters to quantify sensitivity to implementation details