---
ver: rpa2
title: Characterising the Inductive Biases of Neural Networks on Boolean Data
arxiv_id: '2505.24060'
source_url: https://arxiv.org/abs/2505.24060
tags:
- functions
- function
- neural
- networks
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a toy model for understanding neural network
  generalization by exploiting the one-to-one correspondence between depth-2 discrete
  fully connected networks (DFCNs) and disjunctive normal forms (DNFs) on Boolean
  functions. The authors derive analytical bounds showing that randomly initialized
  DFCNs exhibit strong simplicity bias in their prior distribution over Boolean functions,
  with P(f) ~ 2^(-K(f)) where K(f) is the DNF complexity.
---

# Characterising the Inductive Biases of Neural Networks on Boolean Data

## Quick Facts
- **arXiv ID:** 2505.24060
- **Source URL:** https://arxiv.org/abs/2505.24060
- **Reference count:** 40
- **Primary result:** Analytically tractable toy model linking neural network inductive bias to generalization via disjunctive normal form complexity

## Executive Summary
This paper presents a toy model for understanding neural network generalization by exploiting the one-to-one correspondence between depth-2 discrete fully connected networks (DFCNs) and disjunctive normal forms (DNFs) on Boolean functions. The authors derive analytical bounds showing that randomly initialized DFCNs exhibit strong simplicity bias in their prior distribution over Boolean functions, with P(f) ~ 2^(-K(f)) where K(f) is the DNF complexity. Through Metropolis-Hastings and greedy SGD-like algorithms, they demonstrate that generalization correlates with prior probability: simple functions (low K(f)) generalize well while complex ones (like high-order parity) are unlearnable. Weight decay introduces a multiplicative bias e^(-λK(f)) that improves generalization on simple targets by learning minimal DNF representations.

## Method Summary
The authors study depth-2 discrete fully connected networks with ternary weights {−1, 0, 1} on Boolean functions. They exploit the exact DFCN↔DNF bijection to derive analytical bounds on the prior probability P(f) over Boolean functions, showing P(f) ~ 2^(-K(f)) where K(f) is DNF complexity. Training is modeled as Bayesian inference with 0-1 likelihood, using Metropolis-Hastings MCMC and greedy SGD-like algorithms to sample from the posterior. Weight decay λ acts as a multiplicative complexity penalty e^(-λK(f)). Experiments use n=7 input variables, various function classes (parity, entropy, sparse/repeat), and training sets of size m ∈ {16, 32, 64, 96}.

## Key Results
- Randomly initialized DFCNs exhibit strong simplicity bias: P(f) ~ 2^(-K(f)) with simple functions having exponentially higher probability
- Generalization performance correlates with prior probability: functions with high P(f) generalize well, complex functions remain unlearnable
- Weight decay λ introduces e^(-λK(f)) multiplicative bias, improving generalization on simple targets by learning minimal DNF representations
- For complex targets like high-order parity, adding more training data can actually harm generalization due to vanishing prior mass

## Why This Works (Mechanism)

### Mechanism 1: Simplicity Bias via Volume Asymmetry in Parameter-Function Mapping
Randomly initialized DFCNs assign exponentially higher probability P(f) to simple Boolean functions (low K(f)) than to complex ones, following P(f) ∝ 2^(-K(f)) approximately. The ternary weight space and DFCN→DNF bijection create asymmetric "basins" where simple functions have exponentially more weight configurations implementing them, while complex functions occupy vanishingly small volumes. Core assumption: uniform sampling over admissible weight configurations translates proportionally to volume occupation in function space.

### Mechanism 2: Bayesian Posterior Dominance Determines Generalization
Under interpolation training (0-1 likelihood), generalization performance is determined by whether the target function has sufficient posterior mass P(f|S) ∝ P(f) to be discoverable. The Metropolis-Hastings algorithm with κ → ∞ approximates sampling from the Bayesian posterior. Functions with high prior P(f) (simple functions) have high posterior mass when consistent with training data, while functions with low prior (like 7-parity with P ≈ 2×10^−11 for n=4) remain undiscoverable regardless of training set size.

### Mechanism 3: L1 Regularization Translates to Function-Space Complexity Penalty
Weight decay (L1 penalty) on DFCN parameters induces a multiplicative complexity penalty e^(−λK(f)) in the function posterior, explicitly biasing toward minimal-DNF representations. By Proposition 2.9, K(f) = min ||W^(1)||_1 (minimum literal count). L1 regularization directly penalizes literal count, transforming the posterior from P(f|S) ∝ P(f) to P_λ(f|S) ∝ e^(−λK(f))·P(f), sharpening the simplicity bias.

## Foundational Learning

- **Concept: Disjunctive Normal Form (DNF)**
  - Why needed: The entire analytical framework rests on the DFCN↔DNF bijection. K(f) measures DNF complexity.
  - Quick check: Given f(x₁, x₂) that outputs 1 only when x₁=1 and x₂=0, write its DNF and compute K(f).

- **Concept: Bayesian Learning / Prior-Posterior Framework**
  - Why needed: The paper models training as Bayesian inference where P(f) is the prior, interpolation imposes 0-1 likelihood, and generalization depends on posterior mass.
  - Quick check: If P(f_A) = 0.1 and P(f_B) = 0.001, and both perfectly interpolate a training set of m=50 samples, what is the posterior ratio P(f_A|S) / P(f_B|S) under uniform 0-1 likelihood?

- **Concept: Inductive Bias in Neural Networks**
  - Why needed: This is the central object of study. The paper quantifies how architecture and initialization create implicit preferences over functions.
  - Quick check: Why might a CNN generalize better than an FCN on image data even with identical training data? What does this tell you about inductive bias?

## Architecture Onboarding

- **Component map:** Input x → W^(1)x + b^(1) (Hamming distance to clause patterns) → ReLU (outputs 1 for exact matches) → W^(2) (sums satisfied clauses) → threshold at b^(2) (OR/nOR of clauses) → Output f_θ(x)

- **Critical path:** Each row of W^(1) encodes a DNF clause; ReLU outputs 1 only when all literals in clause are satisfied; W^(2) weights sum satisfied clauses; b^(2) threshold determines final output as OR (or negated OR) of clauses.

- **Design tradeoffs:**
  - Width α_w·2^n−1 must have α_w ≥ 1 for full expressivity, but larger α_w eventually makes constant functions dominate the prior
  - Discrete weights enable exact DNF bijection and analytical P(f) bounds; continuous would break bijection but might enable gradient-based training
  - MCMC is Bayesian but slow; greedy SGD-like is faster but lacks convergence guarantees

- **Failure signatures:**
  - Test accuracy decreasing with more data: Target function has vanishing prior (e.g., k ≥ 5 parity for n=7)
  - Weight decay not improving generalization: Target function has large minimum DNF complexity
  - Weight norm remaining high after training: Model hasn't found minimal representation

- **First 3 experiments:**
  1. Validate prior P(f) scaling: Sample 10^8 random DFCN weight configurations for n=4, compute P(f) for all 2^16 Boolean functions, plot log P(f) vs. K(f).
  2. Reproduce weight decay effect: Train DFCN on 4-parity (n=7, m=64) with Metropolis-Hastings, comparing λ=0 vs. λ=0.01, visualize W^(1) heatmaps.
  3. Test "more data hurts" phenomenon: Train on k=6 parity (n=7) with m ∈ {16, 32, 64, 96}, plot test accuracy vs. m to confirm non-monotonic behavior.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the DFCN framework replicate and explain complex training dynamics like grokking and neural collapse?
- Basis: The authors state in Future Directions that studying these phenomena "via tuning of the weight decay parameter or increasing the width of the DFCN, would be interesting to explore."
- Why unresolved: The current study focuses on standard generalization for parity and entropy functions; it does not analyze phase transitions or delayed generalization.
- What evidence would resolve it: Demonstrating that DFCNs exhibit a significant delay between reaching zero training loss and achieving high test accuracy, modulated by specific hyperparameters.

### Open Question 2
- Question: How do different optimizers and training schemes influence the posterior distribution over functions in this discrete setting?
- Basis: The Future Directions section calls for work to "better understand how different optimisers and training schemes influence the posterior, or whether a Bayesian formulation is even possible at all."
- Why unresolved: The paper relies on Metropolis-Hastings and a greedy SGD-like algorithm, leaving the specific inductive biases of standard optimizers uncharacterized.
- What evidence would resolve it: Deriving theoretical bounds on the effective prior P(f) induced by specific optimizers or empirically mapping their resulting function distributions.

### Open Question 3
- Question: Does the theoretically derived width scaling α_w ~ n (3/4)^n maintain Zipf's law for the function prior at large input dimensions?
- Basis: Appendix C.1 proposes this scaling to optimize the prior but notes, "To test the proposed optimal scaling more thoroughly, one would have to... gather empirical evidence at larger n."
- Why unresolved: The theoretical argument relies on approximations, and empirical validation was restricted to small n where the scaling is approximately 1.
- What evidence would resolve it: Empirical sampling of the function prior at large n using the scaled width to verify if the rank-probability distribution adheres to Zipf's law.

## Limitations

- The analytical framework relies heavily on the exact DFCN-DNF bijection and discrete weight space, which may not extend to continuous or deeper architectures
- The MCMC sampling approach requires careful tuning of κ and iteration counts for larger n, and the approximation that training algorithms behave approximately Bayesian may not hold
- The proof of Proposition 2.7 and bounds in Table 2 depend on specific combinatorial properties that could break with different activation functions or weight distributions

## Confidence

- **High:** The existence of simplicity bias in DFCNs (Mechanism 1) and the role of L1 regularization in promoting minimal DNF representations (Mechanism 3) are well-supported by analytical proofs and empirical validation
- **Medium:** The Bayesian posterior dominance mechanism (Mechanism 2) is theoretically derived but depends on approximations about training algorithm behavior
- **Medium:** The claim that weight decay cannot help for inherently complex functions like 7-parity is logically sound but requires careful validation

## Next Checks

1. **Scale sensitivity:** Verify that P(f) ~ 2^(-K(f)) scaling holds for n=5,6 beyond the n=4 analysis. Check whether the bounds in Table 2 maintain their exponential separation between simple and complex functions.

2. **Algorithm comparison:** Implement continuous SGD on the same DFCN architecture and compare generalization patterns to the MCMC results. Test whether the Bayesian correlation between prior probability and generalization holds under gradient-based training.

3. **Architecture ablation:** Modify the DFCN architecture (e.g., continuous weights, different activation functions) and test whether the DNF bijection breaks and the analytical tractability is lost. This would validate the critical path assumption that the ternary weight space is essential for the theoretical framework.