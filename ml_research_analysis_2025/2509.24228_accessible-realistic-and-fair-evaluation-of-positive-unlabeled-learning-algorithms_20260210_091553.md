---
ver: rpa2
title: Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms
arxiv_id: '2509.24228'
source_url: https://arxiv.org/abs/2509.24228
tags:
- learning
- data
- positive
- algorithms
- unlabeled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the first benchmark for comparing positive-unlabeled
  (PU) learning algorithms. The authors identify key issues affecting fair evaluation:
  (1) many algorithms use negative data in validation, which contradicts PU learning
  assumptions, and (2) one-sample and two-sample PU settings have different data distributions
  that existing evaluations fail to account for.'
---

# Accessible, Realistic, and Fair Evaluation of Positive-Unlabeled Learning Algorithms

## Quick Facts
- arXiv ID: 2509.24228
- Source URL: https://arxiv.org/abs/2509.24228
- Reference count: 40
- Primary result: Presents the first benchmark for comparing positive-unlabeled (PU) learning algorithms

## Executive Summary
This paper addresses critical issues in evaluating PU learning algorithms by establishing the first comprehensive benchmark. The authors identify two main problems: many algorithms use negative data in validation despite PU learning's no-negative-label assumption, and existing evaluations fail to account for distributional differences between one-sample and two-sample PU settings. They propose practical solutions including proxy accuracy/AUC for validation and a calibration technique for the one-sample setting's label shift problem.

## Method Summary
The authors conduct a thorough benchmark study of PU learning algorithms by first identifying unfair evaluation practices. They analyze the one-sample and two-sample PU settings, noting that the one-sample case suffers from label shift due to class prior changes. For fair validation without negative data, they propose proxy accuracy and proxy AUC metrics based on class-prior estimation. They also introduce a calibration approach to handle the label shift in one-sample settings. Their benchmark evaluates algorithms across multiple datasets and metrics, providing the first comprehensive comparison of PU learning methods under consistent, realistic conditions.

## Key Results
- No single PU learning algorithm dominates across all datasets and evaluation metrics
- Several early PU learning algorithms perform surprisingly well in the benchmark
- The proposed proxy validation metrics and calibration technique significantly improve fair comparison across PU learning algorithm families

## Why This Works (Mechanism)
The benchmark succeeds by addressing fundamental evaluation flaws in PU learning. Traditional validation methods violate PU assumptions by using negative labels, while ignoring the distributional differences between one-sample and two-sample settings. The proxy metrics work by leveraging class-prior estimation to approximate performance without negative data. The calibration technique corrects for label shift in the one-sample setting, making cross-dataset comparisons valid.

## Foundational Learning
- PU Learning: Learning from only positive and unlabeled data without negative labels
  - Why needed: Many real-world scenarios lack labeled negative examples
  - Quick check: Can identify P and U sets but not N set

- One-sample vs Two-sample PU: One-sample uses same data for training/validation, two-sample uses separate sets
  - Why needed: Different distributional properties affect algorithm performance
  - Quick check: One-sample has label shift, two-sample doesn't

- Class-prior Estimation: Estimating the proportion of positive examples in unlabeled data
  - Why needed: Essential for proxy metrics and many PU algorithms
  - Quick check: Must work without negative labels

- Label Shift: When class priors change between training and test distributions
  - Why needed: Critical issue in one-sample PU setting
  - Quick check: Validation data has different class balance than training

## Architecture Onboarding
Component map: Data -> Algorithm -> Proxy Metrics -> Calibration -> Final Evaluation

Critical path: The evaluation pipeline requires accurate class-prior estimation, followed by proxy metric calculation, then optional calibration for one-sample settings.

Design tradeoffs: Balancing between realistic evaluation (using only P and U data) versus having reliable validation metrics. The proxy metrics sacrifice some accuracy for adherence to PU assumptions.

Failure signatures: Poor class-prior estimation leads to unreliable proxy metrics. Incorrect calibration worsens performance in one-sample settings.

First experiments:
1. Compare proxy accuracy vs true accuracy on datasets with known labels
2. Test class-prior estimation accuracy across different datasets
3. Evaluate calibration impact on one-sample PU performance

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark covers limited number of datasets and algorithms
- Focus primarily on binary classification problems
- Does not address multi-class PU learning scenarios

## Confidence
High: The benchmark methodology is sound and addresses real evaluation problems
Medium: Results may not generalize to all PU learning scenarios
Low: Long-term impact on PU learning algorithm development remains to be seen

## Next Checks
1. Validate proxy metrics on additional datasets with ground truth labels
2. Test calibration technique on more diverse one-sample PU scenarios
3. Extend benchmark to multi-class PU learning problems