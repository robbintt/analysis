---
ver: rpa2
title: 'Enhancing LLM Code Generation with Ensembles: A Similarity-Based Selection
  Approach'
arxiv_id: '2503.15838'
source_url: https://arxiv.org/abs/2503.15838
tags:
- similarity
- llms
- ensllm
- code
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an ensemble-based approach for LLM-based code
  generation, called EnsLLM. It addresses the issue of reliability and correctness
  in LLM-generated code by generating multiple candidate programs from different LLMs
  and selecting the most reliable solution using a structured voting mechanism.
---

# Enhancing LLM Code Generation with Ensembles: A Similarity-Based Selection Approach

## Quick Facts
- arXiv ID: 2503.15838
- Source URL: https://arxiv.org/abs/2503.15838
- Reference count: 40
- Key outcome: Ensemble of 14 LLMs achieves 90.2% pass@1 accuracy on HumanEval, outperforming best standalone model (83.5%) through similarity-based selection

## Executive Summary
This paper introduces EnsLLM, an ensemble-based approach for LLM code generation that addresses reliability issues by generating multiple candidate programs from different LLMs and selecting the most reliable solution through a structured voting mechanism. The system combines syntactic and semantic similarity (via CodeBLEU) with behavioral equivalence testing (via CrossHair's differential analysis) to evaluate and rank candidates. Experiments demonstrate significant improvements over standalone LLMs, achieving 90.2% accuracy on HumanEval and 50.2% on LiveCodeBench, with even free open-source models alone reaching 80.5% and 41.6% respectively.

## Method Summary
EnsLLM generates one candidate program per problem from 14 distinct LLMs, filters out syntactically invalid programs using PyLint, then computes pairwise similarity between remaining candidates using a weighted combination of CodeBLEU (0.5 weight) and behavioral similarity from CrossHair (0.5 weight). The behavioral similarity metric penalizes programs based on the number of counterexamples found between pairs. The system aggregates these pairwise scores for each candidate and selects the highest-scoring program, with tie-breaking based on fewer counterexamples and then random selection.

## Key Results
- EnsLLM achieves 90.2% pass@1 accuracy on HumanEval (164 problems) vs. 83.5% for best standalone model
- LiveCodeBench results show 50.2% accuracy vs. 43.4% for best standalone model
- Free open-source only ensemble achieves 80.5% (HumanEval) and 41.6% (LiveCodeBench)
- Top-5 ensemble (vs. 14-model full ensemble) reduces latency while maintaining 87.2% accuracy

## Why This Works (Mechanism)

### Mechanism 1
Correct implementations reinforce each other through syntactic and semantic similarity, enabling consensus-based selection. CodeBLEU computes pairwise similarity using AST matching and data-flow analysis, where programs with similar structures and variable transformations receive higher mutual scores. The core assumption is that LLMs do not make identical mistakes when generating incorrect programs, making errors uncorrelated across independent models. Evidence shows correct implementations complement each other syntactically and semantically, while incorrect implementations (e.g., linear search when binary search is requested) receive low similarity scores against correct ones.

### Mechanism 2
Behavioral equivalence testing via differential analysis filters programs that appear structurally similar but behave incorrectly. CrossHair's `diffbehavior` systematically explores execution paths to generate counterexamples—inputs where two programs produce different outputs. The behavioral similarity metric `BSim_n(Pi, Pj) = 1 - min(n, cex(Pi, Pj))/n` penalizes programs with more counterexamples. When correct implementations are compared against incorrect ones, differential analysis reveals counterexamples that distinguish functional correctness.

### Mechanism 3
Combining structural and behavioral similarity with equal weighting (λ=0.5) yields optimal selection. The final similarity `similarity(Pi, Pj) = λ · CodeBLEU(Pi, Pj) + (1-λ) · BSim_n(Pi, Pj)` aggregates both signals, with ablation studies showing λ=0.5 achieves 90.2% HumanEval accuracy while λ=1 (CodeBLEU only) drops to 85.9%. The core assumption is that neither signal alone is sufficient—structural similarity can miss behavioral errors, and behavioral testing alone cannot distinguish algorithmically different but functionally equivalent solutions.

## Foundational Learning

- **CodeBLEU metric components (n-gram, AST, data-flow, token weighting)**: Understanding which components matter for code similarity is critical, as the paper shows `syntax_weight` and `dataflow_weight` are essential while lexical matching is less useful. Quick check: Why would two semantically equivalent but syntactically different implementations receive high CodeBLEU scores?

- **Differential testing / property-based testing fundamentals**: CrossHair uses symbolic execution to find counterexamples, and understanding this helps diagnose when behavioral analysis will fail (external calls, non-determinism). Quick check: What does it mean when CrossHair finds zero counterexamples between two programs?

- **Ensemble voting strategies (hard vs. soft voting, aggregation methods)**: EnsLLM uses aggregated pairwise similarity rather than majority voting, which matters for generative tasks where outputs aren't directly comparable. Quick check: Why can't we use standard majority voting when each LLM generates different code for the same prompt?

## Architecture Onboarding

- **Component map**: Input Layer (14 LLMs) -> Syntax Filter (PyLint) -> Pairwise Analysis (CodeBLEU + CrossHair) -> Similarity Aggregation (weighted sum) -> Selection (highest score)
- **Critical path**: 1) Generate diverse candidates from multiple LLMs (quality ceiling = achievable accuracy), 2) Compute pairwise CodeBLEU (86.21s average per problem), 3) Run CrossHair differential analysis (212.38s average per problem—bottleneck), 4) Aggregation and selection (trivial compute)
- **Design tradeoffs**: Latency vs. accuracy—full 14-model ensemble takes ~6 minutes per problem; Top-5 ensemble reduces latency but drops accuracy from 90.2% to 87.2%. Proprietary vs. open-source only—free-only ensemble achieves 80.5% vs. 90.2% with all models. λ weighting—equal weighting (0.5) was optimal on benchmarks but may differ for other domains.
- **Failure signatures**: Only 2 candidates pass syntax filter (pairwise scores are symmetric, selection fails), correlated errors across models (multiple models make similar mistakes, incorrect program gets reinforced), no correct solution in candidate pool (upper bound is achievable accuracy), CrossHair timeout or crash (programs with complex dependencies may not complete analysis)
- **First 3 experiments**: 1) Minimal reproduction: Run EnsLLM on 10 HumanEval problems with 3 LLMs (GPT-4o, CodeLlama, DeepSeekCoder), verify selection logic produces expected rankings, 2) Component ablation: Disable CrossHair (set λ=1) and measure accuracy drop; then disable CodeBLEU (λ=0) and compare, 3) Scalability probe: Measure end-to-end latency with 5 vs. 14 models; identify if CrossHair or CodeBLEU is the bottleneck

## Open Questions the Paper Calls Out

1. How can models be selected for the ensemble based on principled diversity criteria rather than ad-hoc inclusion? The current set of 14 LLMs was chosen without a formal strategy, which may limit robustness to correlated errors.

2. Can the ensemble selection process be made computationally efficient enough for real-time development workflows? Current overhead is substantial—86.21 seconds for CodeBLEU and 212.38 seconds for CrossHair per problem—making practical deployment challenging.

3. How can the approach be extended to languages beyond Python without relying on Python-specific behavioral analysis tools? Different languages require different symbolic execution and behavioral analysis tools, and CodeBLEU configuration may need language-specific tuning.

## Limitations

- **Correlated failure modes**: The paper documents one case where multiple models produced similar incorrect implementations, demonstrating that ensemble diversity cannot be guaranteed when models share training data or architectural similarities.

- **CrossHair dependency bottlenecks**: CrossHair's differential analysis serves as the primary bottleneck and cannot handle programs with external dependencies or non-deterministic behavior, creating practical deployment constraints.

- **Reproducibility constraints**: Key implementation details remain unspecified, including exact prompt templates, CrossHair timeout configurations, and specific model version tags, creating uncertainty about exact replication.

## Confidence

- **High Confidence**: The core mechanism of combining structural and behavioral similarity with equal weighting (λ=0.5) is well-supported by ablation studies showing clear performance differences between λ values.
- **Medium Confidence**: The claim that "LLMs do not make identical mistakes" is supported by documented single failure case but lacks systematic validation across diverse error patterns.
- **Low Confidence**: The optimal λ=0.5 weighting is specific to HumanEval and LiveCodeBench datasets, and claims about generalizability to other domains lack validation.

## Next Checks

1. **Correlation Analysis**: Systematically measure the frequency and pattern of correlated errors across different LLM pairs on a diverse set of code generation problems to validate the core assumption about uncorrelated mistakes.

2. **CrossHair Robustness Testing**: Create a benchmark suite of programs with varying levels of external dependencies, non-determinism, and runtime complexity to quantify the practical limitations of behavioral equivalence testing.

3. **Prompt Sensitivity Study**: Conduct controlled experiments varying prompt templates while keeping all other factors constant to determine how much of EnsLLM's performance depends on generation quality versus selection mechanism.