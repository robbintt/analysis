---
ver: rpa2
title: 'UKTA: Unified Korean Text Analyzer'
arxiv_id: '2502.09648'
source_url: https://arxiv.org/abs/2502.09648
tags:
- evaluation
- writing
- text
- korean
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UKTA (Unified Korean Text Analyzer), a comprehensive
  Korean text analysis and writing evaluation system that addresses the limitations
  of existing Korean automated writing evaluation tools. The system provides multi-view
  analysis through low-level morpheme analysis using the highly accurate Bareun analyzer,
  mid-level lexical feature analysis including 294 lexical diversity and cohesion
  features, and high-level rubric-based writing scores generated by an attention-based
  deep learning model.
---

# UKTA: Unified Korean Text Analyzer

## Quick Facts
- arXiv ID: 2502.09648
- Source URL: https://arxiv.org/abs/2502.09648
- Reference count: 37
- Primary result: UKTA achieves 0.657 accuracy and 0.538 QWK on Korean essay evaluation, improving over baseline by 0.008 accuracy and 0.029 QWK

## Executive Summary
UKTA (Unified Korean Text Analyzer) is a comprehensive system for Korean text analysis and automated writing evaluation that addresses key limitations in existing Korean AWE tools. The system combines accurate morpheme analysis using the Bareun analyzer with 294 handcrafted lexical features spanning basic counts, diversity metrics, and cohesion measures. An attention-based deep learning model then predicts 10 rubric scores while providing explainable feature importance rankings. Experimental results demonstrate significant performance improvements over baseline models using only raw text data.

## Method Summary
UKTA processes Korean essays through a multi-stage pipeline: morpheme analysis via Bareun tokenizer, extraction of 294 normalized lexical features (basic counts, diversity measures like TTR variants and MTLD, cohesion features using KeyBERT/SBERT similarity), sentence-level encoding with KoBERT+BiGRU, and attention-weighted feature integration for final rubric score prediction. The model concatenates sentence and essay representations through linear layers with sigmoid activation to produce 10 rubric scores. Training uses MSE loss with early stopping over 100 epochs.

## Key Results
- UKTA achieves 0.657 accuracy and 0.538 quadratic weighted kappa on the AI-HUB Korean Essay Evaluation Dataset
- Outperforms baseline KoBERT+BiGRU model using only raw text (0.649 accuracy, 0.509 QWK)
- 9 out of 10 rubrics show improvement, with originality rubric showing largest gain (0.069→0.172 QWK)
- Content morphemes (60%) and cohesion features (20%) most influential for low-scoring essays; high-scoring essays show more balanced feature contributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accurate morpheme analysis reduces error propagation in downstream Korean text evaluation tasks.
- Mechanism: Korean is agglutinative; errors in morpheme segmentation compound into feature miscalculation and score distortion. UKTA uses Bareun analyzer to minimize initial segmentation errors.
- Core assumption: Morpheme-level accuracy is prerequisite for reliable lexical feature calculation.
- Evidence anchors: Abstract states existing tools face error propagation challenges; Section 3.1 uses Bareun for highest accuracy.
- Break condition: Morpheme analyzer accuracy below threshold causes non-linear feature reliability degradation.

### Mechanism 2
- Claim: 294 handcrafted lexical features improve writing evaluation accuracy over raw text alone.
- Mechanism: Features provide explicit linguistic signals across basic counts, diversity metrics (TTR, MTLD, HD-D), and cohesion measures that neural embeddings may not explicitly encode.
- Core assumption: Handcrafted features capture signal neural models pretrained on general corpora may miss for Korean essay quality.
- Evidence anchors: Abstract shows significant improvement (0.657 vs 0.649 accuracy); Table 1 shows 9/10 rubrics improve.
- Break condition: Gains may diminish if applied to non-essay text types or significantly different essay length distributions.

### Mechanism 3
- Claim: Attention weights over essay-level features provide human-interpretable explanations for rubric scores.
- Mechanism: Attention layer computes weights A; element-wise multiplication yields f_A = A ⊙ f', emphasizing features model deems relevant. Top-K features are surfaced to users.
- Core assumption: Attention weights correlate with feature importance aligning with human evaluator intuition.
- Evidence anchors: Section 3.3 describes attention for feature importance; Table 2 shows qualitative analysis of feature contributions by score level.
- Break condition: Attention interpretability assumes learned weights reflect genuine importance; contested in broader literature.

## Foundational Learning

- **Agglutinative Morphology**: Korean words formed by stringing morphemes; accurate segmentation prerequisite for all downstream analysis. Mis-segmentation propagates errors.
  - Quick check: Given "먹었다" (ate), explain why segmenting as "먹/Verb + 었/Past + 다/Ending" matters for feature calculation vs. treating as single token?

- **Lexical Diversity Metrics (TTR, MTLD, HD-D)**: UKTA computes diversity measures that handle text-length sensitivity. TTR decreases with length; MTLD and HD-D are length-resilient alternatives.
  - Quick check: Why would MSTTR or MTLD be preferred over raw TTR when comparing essays of differing lengths?

- **Attention-Based Feature Weighting**: UKTA's explainability depends on interpreting attention weights as feature importance. Understanding attention limitations is critical for assessing reliability.
  - Quick check: If attention weights heavily weight "NNP_NDW" (proper noun count), what alternative explanation beyond "importance" could account for this pattern?

## Architecture Onboarding

- **Component map**: Input Layer → Morpheme Analyzer → Feature Extractor → Sentence Encoder → Essay Encoder → Scorer → Explainer
- **Critical path**: Morpheme analysis → Feature computation → Attention-weighted essay representation → Score prediction. Any failure at morpheme stage corrupts entire pipeline.
- **Design tradeoffs**: Handcrafted features vs end-to-end neural adds interpretability but requires feature engineering; KoBERT+BiGRU balances performance with computational cost; 10 rubric outputs provide granularity but require more training data.
- **Failure signatures**: Spelling errors misclassified as rare morphemes; attention weights may highlight spurious correlations; feature normalization assumes training distribution generalizes.
- **First 3 experiments**:
  1. Ablation by feature category: Train variants removing basic lexical, diversity, and cohesion features separately. Measure accuracy/QWK drop per rubric.
  2. Morpheme analyzer swap: Replace Bareun with lower-accuracy tokenizer. Measure error propagation impact on features and scores.
  3. Human evaluation of explanations: Present Top-K features to Korean writing instructors alongside essays. Collect ratings on alignment with scoring rationale.

## Open Questions the Paper Calls Out

- **Open Question 1**: How does UKTA perform when applied to diverse educational settings or demographics outside the specific 4th-grade to high school range used for training?
  - Basis: Conclusion states "Future studies could explore its application in diverse educational settings."
  - Unresolved: Current system trained exclusively on AI-HUB dataset containing essays from grades 4-12.
  - Resolution evidence: Benchmarking results on Korean writing datasets from university students, adult learners, or professional certification contexts.

- **Open Question 2**: How can the system be made robust against spelling errors that cause morpheme analyzer to misinterpret content?
  - Basis: Qualitative analysis notes "spelling errors misinterpreted by morpheme analyzer" influenced low-scoring essay scores.
  - Unresolved: Reliance on Bareun analyzer implies noisy input distorts 294 lexical features before evaluation model sees them.
  - Resolution evidence: Experiments evaluating performance on noisy text with and without pre-processing spelling correction.

- **Open Question 3**: Can the set of 294 lexical features be reduced or optimized while maintaining or improving QWK?
  - Basis: Authors suggest future work should "further refine its feature set to enhance accuracy and adaptability."
  - Unresolved: Paper shows using all features improves performance but does not analyze if some features are redundant or harmful.
  - Resolution evidence: Ablation studies identifying minimal optimal feature subset achieving reported QWK of 0.538.

## Limitations

- **Unverified Morpheme Analyzer Impact**: No empirical validation of Bareun's error reduction compared to lower-accuracy analyzers or measurement of error propagation rates.
- **Feature Importance Interpretation**: Attention weights as feature importance indicators not systematically validated through human evaluation studies.
- **Domain Generalization**: Performance on non-essay Korean text types or different educational contexts remains untested.

## Confidence

- **High Confidence**: Architectural design combining morpheme analysis, handcrafted features, and attention-based scoring is coherent and technically sound. Performance improvements over baseline are specific and measurable.
- **Medium Confidence**: Explainability mechanism via attention weights is plausible but evidence limited to qualitative examples rather than systematic human validation. Feature set design well-motivated but not comprehensively validated.
- **Low Confidence**: Error propagation reduction claim depends on unverified assumptions about morpheme analyzer accuracy thresholds and downstream impact. Without empirical testing, this mechanism remains speculative.

## Next Checks

1. **Ablation Testing**: Systematically remove each feature category (basic lexical, diversity, cohesion) and measure performance degradation per rubric to quantify each category's contribution to reported accuracy gains.

2. **Analyzer Swap Experiment**: Replace Bareun with a lower-accuracy morpheme analyzer and measure resulting impact on feature values and final rubric scores to empirically test error propagation claim.

3. **Human Explanation Validation**: Conduct study where Korean writing instructors rate whether Top-K features highlighted by UKTA align with their scoring rationale for sample essays, providing quantitative validation of explainability mechanism.