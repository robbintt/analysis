---
ver: rpa2
title: 'Rethinking stance detection: A theoretically-informed research agenda for
  user-level inference using language models'
arxiv_id: '2502.02074'
source_url: https://arxiv.org/abs/2502.02074
tags:
- stance
- detection
- social
- llms
- media
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper highlights key gaps in stance detection research, including
  the lack of a theoretical foundation and insufficient attention to user-level attributes
  beyond message content. The authors propose a framework for stance that considers
  psychological and social factors, arguing that recent large language models (LLMs)
  can infer deeper user attributes such as moral foundations and values to improve
  stance detection.
---

# Rethinking stance detection: A theoretically-informed research agenda for user-level inference using language models

## Quick Facts
- arXiv ID: 2502.02074
- Source URL: https://arxiv.org/abs/2502.02074
- Reference count: 40
- This paper proposes a framework for stance detection that incorporates user-level psychological attributes, arguing that current approaches overly focus on message content without considering deeper user characteristics that drive stance formation.

## Executive Summary
This paper identifies critical gaps in stance detection research, particularly the lack of theoretical grounding and insufficient attention to user-level attributes beyond message content. The authors propose a framework for stance that considers psychological and social factors, arguing that recent large language models (LLMs) can infer deeper user attributes such as moral foundations and values to improve stance detection. They review current computational approaches, showing the increasing role of LLMs in enhancing stance detection through feature generation, prompting strategies, and multi-agent systems. The paper concludes with a four-point research agenda emphasizing the need for theoretically grounded, inclusive, and practically impactful studies, particularly at the user level, and the development of richer, more diverse datasets.

## Method Summary
This is a review/position paper that synthesizes approaches rather than presenting a single method. The authors review LLM-enhanced pipelines (feature generation), Chain-of-Thought and few-shot prompting schemes, fine-tuned LLMs, multi-agent LLM systems, and approaches incorporating moral foundations and values via prompting or fine-tuning. They reference existing datasets (SemEval 2016 Task 6, P-Stance, VAST) and user attributes from social media posts, and review F1 scores and accuracy metrics from referenced studies. The minimum viable reproduction plan involves selecting a benchmark dataset with user-level labels, extracting user-level psychological attributes using an LLM, and implementing stance detection with user attributes.

## Key Results
- Stance detection research lacks theoretical foundation and insufficient attention to user-level attributes beyond message content
- LLMs can infer psychological attributes (moral foundations, values, beliefs) from user text, potentially improving stance detection accuracy
- Multi-agent LLM architectures may improve stance detection through specialized analytical perspectives
- The paper proposes a four-point research agenda emphasizing theoretically grounded, inclusive, and practically impactful studies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Incorporating deeper user-level psychological attributes (e.g., moral foundations, values, beliefs) may improve stance detection accuracy compared to message-level-only approaches.
- **Mechanism**: Stance originates from cognitive and affective responses to targets, which manifest through psychological attributes. If these attributes can be inferred from user text, they provide additional signal beyond surface-level message content.
- **Core assumption**: Stance is fundamentally a user-level construct linked to stable psychological/socio-political factors.
- **Evidence anchors**:
  - [abstract]: "relevant attributes (e.g., psychological features) that might be useful to incorporate in stance detection models"
  - [section 4.2]: "Zhang et al. (2024c) found that incorporating moral foundation features into the LLM substantially improved modelling accuracy on a range of stance detection tasks, both at the message and user levels"
  - [corpus]: Weak direct evidence—neighbor papers focus on dataset creation and bias evaluation rather than psychological attribute integration.
- **Break condition**: If psychological attributes cannot be reliably inferred from available text, or if user-level stance shows no more stability than message-level stance across contexts.

### Mechanism 2
- **Claim**: LLMs can serve as flexible feature generators to infer user-level attributes from social media text via prompting or fine-tuning.
- **Mechanism**: LLMs possess pre-trained knowledge that can be directed toward attribute inference through carefully designed prompts or fine-tuning, enabling extraction of constructs like moral foundations without task-specific training data.
- **Core assumption**: LLMs can accurately infer psychological constructs from limited, noisy social media text.
- **Evidence anchors**:
  - [section 4]: "growing evidence that LLMs can act as effective feature generators for a range of textual features"
  - [section 4.2]: "Kang et al. (2023) demonstrated that integrating human values with an LLM via fine-tuning enabled it to predict the opinions and behaviours of individuals who share similar distributions of core human values"
  - [corpus]: Neighbor paper "Are Stereotypes Leading LLMs' Zero-Shot Stance Detection?" raises concerns about inherited stereotypes biasing predictions.
- **Break condition**: If LLM-inferred attributes prove unreliable, culturally non-transferable, or systematically biased toward certain demographic groups.

### Mechanism 3
- **Claim**: Multi-agent LLM architectures (assigning specialized roles) may improve stance detection through multifaceted analysis.
- **Mechanism**: Different LLM agents assume specialized perspectives (e.g., linguistic expert, domain specialist) whose outputs are consolidated, potentially capturing complementary information missed by single-model approaches.
- **Core assumption**: Combining multiple analytical perspectives yields more robust predictions than single-model inference.
- **Evidence anchors**:
  - [section 4.1]: "Lan et al. (2024) assigned roles such as linguistic expert, domain specialist, and social media analyst to generate multifaceted explanations from text"
  - [section 4.1]: "Wang et al. (2024) critiqued previous methods for relying on static experts and proposed a dynamic selection process"
  - [corpus]: Limited corpus validation—neighbors do not address multi-agent architectures.
- **Break condition**: If specialized agents provide redundant rather than complementary information, or if consolidation introduces more noise than signal.

## Foundational Learning

- **Concept: Stance vs. Sentiment distinction**
  - **Why needed here**: The paper emphasizes stance is always target-specific and may require indirect inference, whereas sentiment can be surface-extracted. This distinction underpins why user attributes matter for stance but less so for sentiment.
  - **Quick check question**: Can you explain why "I'm happy Trump won" and "I'm sad Hillary lost" could reflect the same stance toward Hillary Clinton despite opposite sentiments?

- **Concept: Moral Foundations Theory (MFT)**
  - **Why needed here**: MFT is presented as a key psychological framework for stance prediction. The five foundations (care/harm, fairness/cheating, authority/subversion, sanctity/degradation, loyalty/betrayal) are hypothesized to correlate with stance formation.
  - **Quick check question**: If a user consistently invokes "authority/subversion" language, what might this suggest about their likely stance on institutional reform policies?

- **Concept: Zero-shot vs. Target-specific stance detection**
  - **Why needed here**: The paper argues real-world applications require stance detection on unseen targets, where LLMs show promise through prompting strategies.
  - **Quick check question**: What constraints limit target-specific models in practice, and how does zero-shot detection address them?

## Architecture Onboarding

**Component map:**
1. User attribute inference module — LLM-based extraction of psychological attributes (moral foundations, values, personality)
2. Feature fusion layer — Combines inferred user attributes with message content and interaction features
3. Stance classification head — Produces final prediction (favor/against/neutral); can be LLM-based or traditional classifier
4. Optional: Multi-agent reasoning layer — Specialized LLM agents for complex or ambiguous cases

**Critical path:**
1. Aggregate user's historical posts and interaction data
2. Infer psychological attributes using LLM (via prompting or fine-tuned models like Mformer)
3. Fuse user attributes with current message features
4. Apply stance classifier

**Design tradeoffs:**
- **Attribute inference**: Prompting (no training required, flexible) vs. Fine-tuning (potentially more accurate, requires labeled data)
- **Architecture**: End-to-end LLM classifier (simple pipeline, expensive inference) vs. LLM feature generator + traditional classifier (modular, efficient at scale)
- **Multi-agent**: Static predefined roles (simpler) vs. Dynamic expert selection (adaptive, higher complexity)

**Failure signatures:**
- High correlation between predicted stance and sentiment (model failing to distinguish constructs)
- Poor cross-target generalization (overfitting to training targets)
- Systematic political bias (LLM priors dominating evidence)
- Low inter-annotator agreement on LLM-inferred attributes vs. human judgments

**First 3 experiments:**
1. **Baseline ablation**: On SemEval-2016 Task 6, compare message-level vs. user-level stance detection with and without moral foundation features
2. **Cross-target transfer**: Train on Target A, test on Target B with/without user attributes to measure generalization gains
3. **Attribute validation**: Compare LLM-inferred moral foundations against lexicon-based (eMFD) or human-annotated ground truth

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can LLM-inferred psychological attributes (e.g., moral foundations, values, and beliefs) be systematically integrated to improve user-level stance detection accuracy compared to content-only baselines?
- **Basis in paper**: [explicit] The authors explicitly state in Section 5.1 that "there is a need to systematically investigate the link between relevant psychological attributes... and users' stances on specific targets."
- **Why unresolved**: Accessing deep user attributes is hindered by data scarcity and reporting biases, and current models predominantly focus on surface-level message features rather than the cognitive antecedents of stance.
- **What evidence would resolve it**: Empirical results demonstrating that models incorporating LLM-inferred psychological traits (like moral foundations) outperform standard text-classification models on user-level stance tasks across diverse datasets.

### Open Question 2
- **Question**: How can researchers construct "thicker" datasets that effectively decouple user-specific attributes from content attributes to enable robust user-level stance detection?
- **Basis in paper**: [explicit] Section 5.2 highlights that current datasets derive user representations from generated content, which "complicates the task of decoupling the effects of user attributes from the content attributes."
- **Why unresolved**: Existing datasets lack rich ground-truth regarding user psychology or demographics, and combining message-level features with user features creates confounds that are difficult to isolate in modeling.
- **What evidence would resolve it**: The creation and validation of longitudinal, multi-modal datasets containing self-reported user traits (e.g., values, ideology) alongside social media activity to allow for controlled ablation studies.

### Open Question 3
- **Question**: Can LLMs utilizing zero-shot or few-shot prompting strategies effectively detect stance on targets that are unrelated to their pre-training data or prior context?
- **Basis in paper**: [inferred] Section 5.3 discusses the "related challenge of stance prediction for new or unseen targets," specifically noting the difficulty when "the new targets are unrelated to those on which the stance detection model has been pre-trained."
- **Why unresolved**: While LLMs show promise in zero-shot settings, their performance degrades when specific contextual knowledge or domain expertise is missing for emerging or unrelated targets.
- **What evidence would resolve it**: Benchmark evaluations showing that LLMs using advanced reasoning strategies (e.g., Chain-of-Thought) can maintain high stance detection accuracy on novel, unrelated targets without target-specific fine-tuning.

## Limitations
- The paper's core claim that psychological attributes can improve stance detection rests on LLMs' ability to reliably infer these attributes from noisy social media text—a capability not yet systematically validated
- User-level aggregation methodology lacks standardization, making reproducibility challenging
- The paper acknowledges but does not fully address the potential for LLM biases (e.g., political orientation, demographic stereotypes) to contaminate stance predictions, particularly in zero-shot settings

## Confidence
- **High confidence**: The identification of theoretical gaps in stance detection research and the distinction between message-level and user-level inference approaches
- **Medium confidence**: The feasibility of LLM-based psychological attribute inference, based on limited but growing evidence in adjacent domains
- **Low confidence**: Claims about multi-agent LLM architectures' superiority over single-model approaches, given minimal empirical validation in stance detection

## Next Checks
1. Conduct ablation studies comparing stance detection accuracy with and without inferred psychological attributes across diverse, previously unseen targets to assess generalizability
2. Evaluate LLM-inferred moral foundations against gold-standard human annotations and lexicon-based benchmarks (e.g., eMFD) on the same user texts
3. Test stance detection performance on datasets spanning multiple languages and cultural contexts to identify potential cultural transfer limitations