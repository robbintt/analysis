---
ver: rpa2
title: Linear Spatial World Models Emerge in Large Language Models
arxiv_id: '2506.02996'
source_url: https://arxiv.org/abs/2506.02996
tags:
- above
- spatial
- right
- below
- left
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates whether large language models (LLMs) implicitly\
  \ encode linear spatial world models\u2014structured representations of object positions\
  \ in three-dimensional space. The authors formalize a spatial world model as a tuple\
  \ \u27E8R3, O, S\u27E9, where R3 is 3D Euclidean space, O is a set of objects, and\
  \ S represents their positions."
---

# Linear Spatial World Models Emerge in Large Language Models

## Quick Facts
- arXiv ID: 2506.02996
- Source URL: https://arxiv.org/abs/2506.02996
- Authors: Matthieu Tehenan; Christian Bolivar Moya; Tenghai Long; Guang Lin
- Reference count: 40
- Primary result: LLMs encode linear spatial world models where inverse relations form antipodal vectors and compositions align with vector addition

## Executive Summary
This paper demonstrates that large language models implicitly encode linear spatial world models in their contextual embeddings. Through synthetic datasets of object positions with spatial relations, the authors show that both linear and nonlinear probes can perfectly decode spatial information from LLM activations. Dimensionality reduction reveals a structured subspace where spatial relations exhibit compositional algebraic properties—inverse relations form antipodal directions and composed relations align with vector addition of their components. Causal interventions via activation steering confirm these spatial representations are functionally used during generation.

## Method Summary
The authors construct a synthetic dataset using template sentences like "The cup is above the table" with 61 objects and 6 spatial relations. They extract residual stream activations from LLaMA-3.2-3B-Instruct at specific layers before the final LayerNorm, using the final token or averaging for multi-token objects. Linear and shallow MLP probes are trained to classify spatial relations, achieving near-perfect accuracy. PCA is applied to probe directions to identify a low-dimensional spatial subspace, where inverse relations show antipodal geometry and composed relations demonstrate vector addition properties. Activation steering interventions add scaled PCA-derived direction vectors to test causal functional use of the subspace.

## Key Results
- Linear probes achieve near-perfect reconstruction of spatial relations, matching nonlinear probe performance
- PCA reveals antipodal organization for inverse relations (e.g., "above" vs. "below") and orthogonal organization for unrelated relations
- Composed spatial relations (e.g., "above-left") align closely with vector sums of constituent relations (mean cosine similarity = 0.9931 in 2D)
- Activation steering achieves 74.3% success rate across all relations, with near-perfect steering for simple relations

## Why This Works (Mechanism)

### Mechanism 1
Spatial relations are linearly decodable from residual stream activations because the model encodes spatial semantics in a low-dimensional subspace where inverse relations form antipodal directions and orthogonal relations form near-perpendicular axes. The linear representation hypothesis holds—features are organized along interpretable directions in activation space. Evidence includes near-perfect probe accuracy and antipodal geometry in PCA space. Break condition: if nonlinear probes significantly outperform linear probes, or if PCA fails to reveal structured antipodal/orthogonal organization.

### Mechanism 2
Composed spatial relations correspond approximately to vector addition of atomic relation vectors because in the PCA-identified spatial subspace, the mean activation vector for "above-left" closely aligns with the sum of "above" and "left" vectors (mean cosine similarity = 0.9931 in 2D PCA space). The compositional algebraic structure holds in representation space. Evidence includes angular deviations of 6.02° in PCA space for 2D compositions. Break condition: if composed relation vectors show large angular deviations from constituent sums (>30°), or if compositions exhibit inconsistent geometry across different relation pairs.

### Mechanism 3
The identified spatial subspace is causally used by the model during generation because injecting steering vectors derived from PCA-projected spatial directions systematically shifts model outputs toward the target spatial relation. The subspace identified via probing reflects functionally utilized representations, not merely correlational artifacts. Evidence includes 74.3% steering success rate with near-perfect steering for simple relations. Break condition: if steering interventions produce random or inconsistent output changes, or if success rates fall near chance levels.

## Foundational Learning

- **Linear probing**: Training linear classifiers on model activations to extract information. Why needed: The paper relies on training linear classifiers to extract spatial information; understanding what probes reveal is essential. Quick check: Can you explain why high probing accuracy alone does not prove a representation is causally used?

- **Principal Component Analysis (PCA)**: Dimensionality reduction technique to identify low-dimensional subspaces. Why needed: PCA is used to isolate the spatial subspace where relational geometry becomes visible. Quick check: What does it mean if the top 3 PCA components explain ~100% of variance in probe directions?

- **Activation steering**: Intervention method that adds scaled direction vectors to hidden states. Why needed: The causal intervention method adds scaled direction vectors; understanding construction and application is critical. Quick check: How would you construct a steering vector for "above" from a set of activation differences?

## Architecture Onboarding

- **Component map**: Synthetic sentences -> Activation extraction -> Probe training -> PCA analysis -> Steering intervention -> Output evaluation

- **Critical path**: 
  1. Generate spatial relation sentences with ground-truth position annotations
  2. Extract activations from LLaMA-3.2-3B-Instruct (or 1B variant, Qwen3-1.7B)
  3. Train linear probes per relation per layer
  4. Apply PCA to probe weight vectors; visualize antipodal/orthogonal structure
  5. Construct steering vectors by projecting PCA directions back to residual stream
  6. Evaluate steering success via lexical match of generated spatial terms

- **Design tradeoffs**: Synthetic dataset enables controlled spatial geometry but may not capture natural language variation; PCA reduces noise but may obscure nonlinear structure; multi-token object averaging simplifies representation but could dilute signal

- **Failure signatures**: "Behind" steering achieves only 5% success (hypothesized due to multi-token distributed representation); 3D compositions show higher angular deviation than 2D; layers below ℓ₃ show degraded probe accuracy

- **First 3 experiments**: 
  1. Replicate linear probe training on a held-out subset; verify near-perfect accuracy at layers 8, 16, 24
  2. Project probe vectors to 2D PCA space; confirm "above"/"below" and "left"/"right" pairs approach antipodality (cosine similarity >0.95)
  3. Implement steering for "above" and "left" relations; verify >90% success rate in shifting model outputs to the target spatial term

## Open Questions the Paper Calls Out

### Open Question 1
Do LLMs encode transition functions over spatial configurations (i.e., movement dynamics) as part of their spatial world model? The authors state they do not investigate temporal dynamics or object permanence, including movement as transition functions over spatial configurations. This remains unexplored as the paper only examines static spatial relations. Evidence would require extending the framework to sequential spatial descriptions showing object movement and testing whether transition operations are linearly encoded.

### Open Question 2
Do linear spatial world models emerge consistently across different LLM architectures and training paradigms? The extent to which findings generalize beyond the studied model type remains an open empirical question. Experiments use only LLaMA-3.2 variants and Qwen3; structural differences across architectures could affect representation geometry. Evidence would require systematic replication across diverse model families and across pre-trained vs. instruction-tuned variants.

### Open Question 3
Why do multi-token spatial relations (e.g., "in front of") show substantially lower steering success rates than single-token relations? The authors report 100% success for "above/below/left" but only 5% for "behind" and 62% for "in front of," hypothesizing distributed token representations may hinder steering without testing alternatives. This token-count hypothesis remains speculative; other factors (frequency, ambiguity, depth in network) are unexplored. Evidence would require controlled experiments manipulating token count while holding other factors constant.

### Open Question 4
Does the compositional spatial world model scale to scenes with more than two objects? The dataset uses only pairwise object configurations; whether the identified subspace handles multiple simultaneous spatial relations is untested. Real-world spatial reasoning requires integrating multiple object relations; superposition effects could degrade linear compositionality. Evidence would require probing experiments with 3+ objects and multiple concurrent relations, testing whether composed representations remain linearly separable.

## Limitations
- Synthetic dataset may not capture genuine spatial reasoning capacity in naturalistic text
- Steering success metric depends on lexical matching, potentially underestimating nuanced spatial reasoning
- Significant performance drop for "behind" relations (5% success) raises questions about representational limitations vs. artifacts

## Confidence

**High Confidence**: Spatial relations are linearly decodable from LLM activations (linear probes achieve near-perfect accuracy matching nonlinear probes)

**Medium Confidence**: The spatial subspace exhibits compositional structure where "above-left" ≈ "above" + "left" (0.9931 cosine similarity in 2D, but 3D compositions show degraded performance)

**Medium Confidence**: The causal role of the identified subspace (74.3% steering success rate, but variable success across relations suggests complexity)

## Next Checks

1. Test naturalistic generalization by evaluating whether linear spatial representations transfer to real-world spatial descriptions from standard text corpora; extract activations on naturally occurring spatial sentences and compare subspace geometry to synthetic results.

2. Investigate token distribution effects by systematically analyzing how spatial relation representations vary across different token positions for multi-token objects; compare steering success rates when targeting different tokens within multi-token object names to determine whether "behind" failure stems from distributed representation.

3. Validate compositional limits by extending analysis to 3D spatial relations and testing whether vector addition holds for all relation combinations; measure angular deviations for all 20 possible 2-relation compositions in full 3D PCA space to determine whether compositional linearity is general or limited to specific pairs.