---
ver: rpa2
title: Fine-tuning on simulated data outperforms prompting for agent tone of voice
arxiv_id: '2507.04889'
source_url: https://arxiv.org/abs/2507.04889
tags:
- llama
- int8
- bfloat16
- fine-tuning
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that fine-tuning language models on small,
  simulated datasets is more effective than system prompting for achieving conversational
  tone in customer-facing speech applications. Using Llama3.2-1B-Instruct and two
  closed-source models, the researchers fine-tuned on synthetically generated question-answer
  pairs from Wikipedia.
---

# Fine-tuning on simulated data outperforms prompting for agent tone of voice

## Quick Facts
- arXiv ID: 2507.04889
- Source URL: https://arxiv.org/abs/2507.04889
- Authors: Ingo Marquardt; Philippe Brule
- Reference count: 5
- Primary result: Fine-tuning language models on small, simulated datasets is more effective than system prompting for achieving conversational tone in customer-facing speech applications

## Executive Summary
This study demonstrates that fine-tuning language models on small, simulated datasets is more effective than system prompting for achieving conversational tone in customer-facing speech applications. Using Llama3.2-1B-Instruct and two closed-source models, the researchers fine-tuned on synthetically generated question-answer pairs from Wikipedia. Models were evaluated using Flesch reading-ease scores, with 60+ indicating conversational responses. Fine-tuned models achieved over 90% conversational responses even with just 100 training samples, significantly outperforming base models without fine-tuning. Semantic similarity analysis confirmed content quality was maintained. Interestingly, 8-bit integer quantization led to faster convergence than bfloat16 precision. The results suggest fine-tuning is a data-efficient alternative to complex system prompts for instilling specific stylistic behaviors.

## Method Summary
The researchers used LoRA fine-tuning to adapt Llama3.2-1B-Instruct for conversational tone. They generated synthetic question-answer pairs from Wikipedia using google/gemini-2.0-flash-001, filtering responses to achieve Flesch reading-ease scores ≥75. The dataset was deduplicated using semantic embeddings with cosine similarity >0.8 rejection threshold. Models were fine-tuned for 5 epochs with batch size 16 and gradient accumulation 2, using AdamW optimizer with cosine annealing learning rate schedule. Evaluation compared Flesch scores against a 60 threshold for conversational responses, alongside semantic similarity measurements using ModernBERT embeddings. The approach was tested across different training set sizes (100-9000 samples) and precision settings (int8 vs bfloat16).

## Key Results
- Fine-tuned models achieved over 90% conversational responses with just 100 training samples, outperforming base models without fine-tuning
- 8-bit integer quantization led to faster convergence than bfloat16 precision during LoRA fine-tuning
- Semantic similarity analysis confirmed content quality was maintained during style transfer
- Performance improvements were consistent across different model sizes, including gpt-4o-mini and gpt-4.1-mini

## Why This Works (Mechanism)

### Mechanism 1: LoRA Fine-Tuning for Behavioral Alignment
- Claim: LoRA fine-tuning on small datasets outperforms system prompting for instilling stylistic behavior in language models
- Mechanism: LoRA adapters target key, query, and value projection layers, modifying attention patterns to encode style preferences directly into weights rather than relying on transient context
- Core assumption: Stylistic behavior can be learned as a low-rank adaptation without full model retraining
- Evidence anchors:
  - [abstract] "Fine-tuned models achieved over 90% conversational responses even with just 100 training samples, significantly outperforming base models without fine-tuning"
  - [section 2.3] "The LoRA adapters targeted the key, query, and value projection layers of the Llama model"
  - [corpus] Limited direct corpus evidence on LoRA for tone specifically; neighbor papers focus on dialogue generation (2510.02331) and empathy elicitation (2503.20518) but not parameter-efficient style transfer
- Break condition: When target style requires domain-specific knowledge absent from synthetic training data, or when style constraints conflict with factual accuracy requirements

### Mechanism 2: Iterative Synthetic Data Generation with Quality Filtering
- Claim: Simulated Q&A pairs with Flesch threshold filtering create sufficient training signal for conversational style transfer
- Mechanism: Multi-step pipeline generates question → answer → rephrased conversational answer; samples are filtered by Flesch reading-ease ≥75 and deduplicated via semantic embeddings (cosine similarity >0.8 triggers rejection)
- Core assumption: Flesch reading-ease score correlates with perceived conversational naturalness in speech applications
- Evidence anchors:
  - [abstract] "fine-tuned on synthetically generated question-answer pairs from Wikipedia"
  - [section 2.2] "If the rephrased answer received from the google/gemini-2.0-flash-001 LM achieved a Flesch reading-ease score equal to or greater than 75, we added the question and the rephrased answer to the dataset"
  - [corpus] Neighbors support synthetic dialogue generation validity (2510.02331 uses LMs as user simulators for CRS training)
- Break condition: When target style cannot be captured by readability metrics (e.g., professional tone, brand-specific voice, persona adherence)

### Mechanism 3: Quantization as Implicit Regularization
- Claim: Int8 quantization of the base model during LoRA fine-tuning accelerates convergence toward target style compared to bfloat16 precision
- Mechanism: Quantization noise may prevent overfitting on small datasets and help optimization escape shallow local minima, particularly at lower learning rates
- Core assumption: The approximation error from int8 quantization functions similarly to dropout or noise injection as implicit regularization
- Evidence anchors:
  - [abstract] "8-bit integer quantization led to faster convergence than bfloat16 precision"
  - [section 4] "quantization might act as a form of implicit regularization, preventing overfitting on the small dataset... the residual noise might function as a form of implicit regularization"
  - [corpus] No direct corpus evidence on quantization for style fine-tuning—this appears novel and speculative
- Break condition: When quantization noise degrades semantic content (not observed in paper, but theoretically possible); effect may not generalize to larger models or different precision levels

## Foundational Learning

- **Concept: LoRA (Low-Rank Adaptation)**
  - Why needed here: The entire fine-tuning approach depends on understanding how low-rank decomposition enables efficient style learning without full retraining
  - Quick check question: Why do LoRA adapters target attention projection layers (key/query/value) specifically?

- **Concept: Flesch Reading-Ease Score**
  - Why needed here: This metric serves as the primary evaluation criterion; understanding its calculation is essential for replication and threshold selection
  - Quick check question: What text characteristics increase vs. decrease Flesch scores?

- **Concept: Quantization-Aware Fine-Tuning**
  - Why needed here: The counterintuitive int8 > bfloat16 finding requires understanding how reduced precision affects gradient dynamics
  - Quick check question: How does int8 quantization introduce noise while preserving model function?

## Architecture Onboarding

**Component map:**
- Base model: Llama3.2-1B-Instruct (text component from Ultravox multimodal)
- Fine-tuning: LoRA adapters on key/query/value projections (r=32–64, alpha=r)
- Data pipeline: Wikipedia → gemini-2.0-flash Q&A generation → Flesch ≥75 filtering → semantic deduplication
- Evaluation: textstat (Flesch score) + ModernBERT embeddings (cosine similarity for semantic quality)

**Critical path:**
1. Generate synthetic data with iterative Flesch filtering (≥75 threshold)
2. Load base model in int8; LoRA adapters in bfloat16
3. Train 5 epochs, batch size 16 × 2 gradient accumulation = 32 effective batch
4. Evaluate on held-out validation set (Flesch ≥60 = conversational)

**Design tradeoffs:**
- int8 vs. bfloat16: Faster convergence with int8, but mechanism is hypothesized (not proven)
- Dataset size: 100 samples sufficient for ~90% conversational; 1000+ for stability
- LoRA rank: Paper tested r=32/48/64 with alpha=r; higher rank did not always improve results

**Failure signatures:**
- Flesch scores not improving → Check LoRA learning rate (paper used 1e-4 to 1e-3)
- Semantic similarity dropping → Potential catastrophic forgetting; reduce epochs or learning rate
- bfloat16 converging slowly → Switch to int8 per paper findings

**First 3 experiments:**
1. **Minimal replication:** Train Llama3.2-1B with 100 samples, int8, r=64, lr=2e-4, 5 epochs → expect ~90%+ conversational responses
2. **Quantization A/B test:** Same config with bfloat16 → expect slower convergence per Figure 3
3. **Semantic retention probe:** Compute embedding similarity on validation set → expect ≥0.85 cosine similarity without degradation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning on simulated data outperform prompting for stylistic targets other than conversational tone?
- Basis in paper: [explicit] Section 5 states that the primary claim rests on a single behavioral target and proposes that "follow-up studies should conduct systematic comparisons across a broader range of stylistic targets (e.g., formal tone, specific persona adherence, complex formatting rules)"
- Why unresolved: The current study limited its scope to "conversational tone" to validate the method, leaving the effectiveness of this approach for other specific style directives unproven
- What evidence would resolve it: Experimental results applying the same fine-tuning methodology to distinct stylistic goals (e.g., enforcing a formal legal tone or a specific persona) and comparing the performance against system prompting baselines

### Open Question 2
- Question: Does 8-bit integer quantization act as an implicit regularizer or an optimization aid during LoRA fine-tuning?
- Basis in paper: [explicit] Section 5 notes that the explanations for why int8 quantization resulted in faster convergence than bfloat16 "remain speculative" and suggests future research should "analyze gradient dynamics and loss landscapes"
- Why unresolved: The authors observed the performance benefit (int8 outperforming bfloat16) but did not conduct targeted experiments to determine if the cause was implicit regularization preventing overfitting or noise helping the optimizer escape local minima
- What evidence would resolve it: A targeted ablation study analyzing gradient noise and loss landscape geometry during fine-tuning, comparing int8 and bfloat16 precision under controlled conditions

### Open Question 3
- Question: Is the Flesch reading-ease score a reliable proxy for human-perceived naturalness in speech interactions?
- Basis in paper: [inferred] In Section 2.4, the authors use the Flesch score as the primary metric for "conversational" quality, asserting it is a "good indicator" despite acknowledging it technically measures text readability rather than spoken naturalness
- Why unresolved: The study relies entirely on automated metrics (Flesch score and semantic similarity) to evaluate success, lacking a human evaluation component to confirm that higher scores actually result in a better user experience in voice applications
- What evidence would resolve it: A human evaluation study where raters blindly assess the "naturalness" of model responses to determine the correlation coefficient with the automated Flesch reading-ease scores

## Limitations

- The synthetic data pipeline relies heavily on the quality and consistency of the Gemini-2.0-flash-001 generation process, which is not fully specified in terms of exact prompts or generation parameters
- The comparison with closed-source models (gpt-4o-mini, gpt-4.1-mini) lacks detailed configuration transparency, limiting reproducibility
- The int8 quantization advantage remains mechanistic speculation rather than proven theory, and the finding may not generalize beyond the 1B parameter model scale

## Confidence

- **High confidence**: The core finding that LoRA fine-tuning on small synthetic datasets significantly outperforms base models without fine-tuning for conversational tone achievement
- **Medium confidence**: The int8 quantization leading to faster convergence is supported by experimental results but lacks mechanistic proof
- **Medium confidence**: Semantic similarity preservation during fine-tuning is demonstrated but relies on a single embedding model (ModernBERT) for validation

## Next Checks

1. **Cross-dataset validation**: Test the fine-tuning approach on non-Wikipedia domains (e.g., customer service transcripts, forum discussions) to verify synthetic data generation generalizes beyond encyclopedic content
2. **Metric triangulation**: Supplement Flesch score evaluation with human evaluation panels and alternative readability/comprehension metrics (e.g., SMOG, Gunning Fog) to confirm conversational quality assessment
3. **Quantization mechanism study**: Systematically compare int8 vs. bfloat16 across different model sizes and learning rates to isolate whether the convergence advantage is specific to this 1B parameter setup or represents a broader phenomenon