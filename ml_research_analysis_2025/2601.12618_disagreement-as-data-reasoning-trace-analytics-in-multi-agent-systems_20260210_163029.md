---
ver: rpa2
title: 'Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems'
arxiv_id: '2601.12618'
source_url: https://arxiv.org/abs/2601.12618
tags:
- reasoning
- data
- qualitative
- similarity
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a novel method for detecting and interpreting
  disagreements in LLM-based qualitative coding through reasoning trace analysis.
  By applying cosine similarity to reasoning traces from multi-agent systems, the
  researchers systematically quantify semantic alignment between agents.
---

# Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems

## Quick Facts
- arXiv ID: 2601.12618
- Source URL: https://arxiv.org/abs/2601.12618
- Reference count: 40
- Primary result: Cosine similarity between reasoning traces robustly distinguishes consensus from disagreement (ρ = .54) and correlates with human coding reliability

## Executive Summary
This study introduces a novel method for detecting and interpreting disagreements in LLM-based qualitative coding through reasoning trace analysis. By applying cosine similarity to reasoning traces from multi-agent systems, the researchers systematically quantify semantic alignment between agents. Analyzing nearly 10,000 coding instances, they found that cosine similarity robustly distinguishes consensus from disagreement (ρ = .54) and correlates with human coding reliability. Qualitative analysis of disagreements revealed nuanced instructional sub-functions within codes and codebook ambiguities. This approach surfaces interpretive ambiguity, enhances coding reliability, and supports more efficient human-AI collaborative workflows in educational research.

## Method Summary
The researchers developed a multi-agent system where two LLM agents with distinct personas independently code tutor-student dialogue segments, generating reasoning traces for each decision. These traces are embedded using BERT and compared via cosine similarity to quantify semantic alignment. A consensus agent reviews divergent outputs to produce final codes. The system processes 3,538 dialogue segments from 9th-grade Algebra I virtual tutoring sessions using an 8-category codebook. Agent reasoning traces are extracted from structured outputs, embedded, and compared pairwise. Cases are classified into agreement/disagreement categories based on code labels and CS values, with human review focused on ambiguous cases to refine the codebook.

## Key Results
- Cosine similarity robustly distinguishes consensus from disagreement (ρ = .54 correlation)
- Agreement pairs (M = 0.957) significantly higher than disagreement pairs (M = 0.904), d = 1.16
- Qualitative analysis revealed nuanced instructional sub-functions within codes and codebook ambiguities

## Why This Works (Mechanism)

### Mechanism 1
Cosine similarity between reasoning trace embeddings serves as a quantitative proxy for detecting conceptual alignment or divergence between LLM agents. Each agent generates a reasoning trace when coding, which are embedded using BERT to produce high-dimensional vectors. Pairwise cosine similarity computation between Agent 1 and Agent 2 traces identifies semantic convergence (≥0.95) versus potential disagreement. This assumes embeddings capture pedagogically-relevant conceptual structure beyond lexical overlap.

### Mechanism 2
Multi-agent disagreement patterns reveal latent ambiguity in codebooks and instructional constructs that single-agent or human-only coding may miss. Two agents with distinct personas code independently, and when they disagree—or agree with divergent rationales—the system flags these cases. Human coders then analyze flagged traces to identify fuzzy code boundaries, unspecified sub-functions, and context-window limitations.

### Mechanism 3
Integrating quantitative similarity metrics with qualitative human review creates a scalable triage system for human-AI collaborative coding. CS values partition cases into priority tiers: high-similarity agreement (low review priority), mid-range within-code misalignment (moderate priority), low-similarity or between-code alignment (high priority). Human coders focus attention on high-ambiguity cases, iteratively refining the codebook.

## Foundational Learning

- **Cosine Similarity on Sentence Embeddings**
  - Why needed here: The core metric depends on understanding how BERT encodes text into vectors and how cosine distance measures angular separation
  - Quick check question: Given two embedding vectors [0.8, 0.6] and [0.6, 0.8], compute cosine similarity. (Answer: ~0.96)

- **Multi-Agent LLM Orchestration**
  - Why needed here: The system uses dual-agent discussion + consensus agent roles. Understanding why multiple agents outperform single agents is critical for architecture decisions
  - Quick check question: Why might two agents with identical prompts and temperature=0 still produce different outputs? (Answer: Non-deterministic GPU operations, sampling even at low temperature, persona differences)

- **Qualitative Coding Reliability Metrics**
  - Why needed here: The paper validates CS against inter-rater reliability (κ). Understanding Cohen's κ and its limitations is necessary to interpret whether CS captures meaningful agreement
  - Quick check question: If two coders agree 90% of the time but expected chance agreement is 85%, what is κ? (Answer: κ = 0.33—moderate agreement despite high raw agreement)

## Architecture Onboarding

- **Component map:** Data ingestion -> Dual-Agent Discussion -> Consensus Agent -> Parser -> Embedder -> Similarity Computer -> Triage Layer -> Human Review Interface

- **Critical path:** Prompt engineering → Agent inference (Round 1) → Disagreement detection → Round 2 dialogue (if needed) → Consensus resolution → Embedding + CS computation → Human triage queue

- **Design tradeoffs:**
  - Model size vs. cost: DeepSeek-R1-32B chosen for reasoning-trace output and compute efficiency vs. 175B alternatives
  - Truncation vs. context: 512-token limit may lose long-reasoning signal; alternative is sliding-window or hierarchical embedding
  - Single-segment vs. session context: Current system processes one segment at a time; risks missing cross-turn dependencies

- **Failure signatures:**
  - CS values clustered near 1.0 for all pairs → embedding collapse or parser failure
  - >50% of cases in between-align category → codebook fundamentally incoherent
  - Human-identified ambiguity not reflected in CS distribution → metric not capturing relevant semantic variation

- **First 3 experiments:**
  1. Validate CS discrimination on a held-out dataset with known human inter-rater reliability; compute ROC curve and compare to ρ = .54 benchmark
  2. Ablate the consensus agent: Compare dual-agent-only vs. dual-agent + consensus on final code accuracy and CS distribution
  3. Test embedding alternatives: Replace BERT with domain-specific embeddings or longer-context encoders; measure whether CS discrimination improves

## Open Questions the Paper Calls Out

### Open Question 1
Does incorporating multi-turn or session-level conversational context reduce agent disagreement rates compared to processing isolated dialogue segments? The study design intentionally segmented dialogues, and no comparison to context-aware processing was conducted.

### Open Question 2
How robust is cosine similarity-based disagreement detection across diverse educational datasets (e.g., log files, think-aloud protocols, discussion forums) and coding schemas? Only one dataset and one codebook were tested; generalizability remains unknown.

### Open Question 3
How can disagreement signals be operationalized into practical human-AI workflows that improve coding efficiency and reliability? The paper proposes the detection method but does not implement or evaluate an actual workflow with human coders.

### Open Question 4
Does systematically comparing agent rationales against codebook definitions reveal systematic deviations from codebook intent in consensus cases? The qualitative analysis focused on disagreement cases, not on validating whether even agreed-upon reasoning matches codebook definitions.

## Limitations
- The system processes one dialogue segment at a time, risking ambiguity from limited conversational context
- Only evaluated on a single learner-tutor dialogue dataset with one specific codebook
- Did not systematically compare agent rationales against codebook definitions to validate consensus cases

## Confidence

- **High**: CS correlation with human agreement (ρ = .54) and effect size (d = 1.16) are empirically supported
- **Medium**: The claim that disagreement reveals codebook ambiguity is validated through qualitative analysis, but generalizability to other domains remains untested
- **Low**: The system's scalability and efficiency claims depend on untested assumptions about triage prioritization

## Next Checks
1. Apply the method to a different qualitative coding domain (e.g., healthcare or social sciences) and test whether CS discrimination holds
2. Measure actual time saved by human coders using the triage system versus traditional review methods
3. Monitor how CS patterns change as the codebook is iteratively refined, and whether this correlates with improved inter-rater reliability over time