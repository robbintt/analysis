---
ver: rpa2
title: 'LatPhon: Lightweight Multilingual G2P for Romance Languages and English'
arxiv_id: '2509.03300'
source_url: https://arxiv.org/abs/2509.03300
tags:
- languages
- multilingual
- vowel
- error
- italian
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces LatPhon, a compact multilingual grapheme-to-phoneme
  (G2P) conversion model covering six Latin-script languages: English, Spanish, French,
  Italian, Portuguese, and Romanian. Built as a 7.5 million-parameter Transformer,
  it achieves a mean phoneme error rate (PER) of 3.5% on the public ipa-dict corpus,
  outperforming the byte-level ByT5 baseline (5.4%) and approaching language-specific
  WFSTs (3.2%).'
---

# LatPhon: Lightweight Multilingual G2P for Romance Languages and English

## Quick Facts
- arXiv ID: 2509.03300
- Source URL: https://arxiv.org/abs/2509.03300
- Reference count: 11
- Key outcome: A 7.5M-parameter multilingual G2P model achieves 3.5% mean PER on six languages, outperforming a 580M-parameter baseline and approaching WFST performance.

## Executive Summary
This paper introduces LatPhon, a compact multilingual grapheme-to-phoneme (G2P) conversion model covering six Latin-script languages: English, Spanish, French, Italian, Portuguese, and Romanian. Built as a 7.5 million-parameter Transformer, it achieves a mean phoneme error rate (PER) of 3.5% on the public ipa-dict corpus, outperforming the byte-level ByT5 baseline (5.4%) and approaching language-specific WFSTs (3.2%). The model uses a 4-layer encoder-decoder architecture with 256-dimensional embeddings, 8-head attention, and a learned 6-way language-ID embedding. Ablation studies show that removing the language-ID token degrades PER by 21.6 percentage points on average, confirming its importance for cross-lingual sharing. Inference is fast (about 31 words per second) and the final model occupies only 30 MB, making on-device deployment feasible. The work demonstrates that a single, lightweight, multilingual G2P model can serve as a universal front-end for speech processing pipelines while remaining compact and accurate.

## Method Summary
LatPhon is a 7.5M-parameter multilingual Transformer encoder-decoder trained on the ipa-dict corpus (600K training pairs across six languages). The architecture consists of a 4-layer encoder and 4-layer decoder, each with 256-d embeddings, 8-head attention, and rotary positional encodings. A learned 6-way language-ID embedding is prepended to the grapheme sequence as conditioning. The output vocabulary is limited to 109 IPA symbols. Training uses AdamW (LR 3×10⁻⁴, 10k warmup, cosine decay), batch size 64, for 100k steps. PER is the primary evaluation metric.

## Key Results
- Mean PER of 3.5% across six languages, outperforming byte-level ByT5 (5.4%) while being ~77× smaller (7.5M vs 580M parameters)
- Ablation shows removing the language-ID embedding degrades mean PER from 3.5% to 25.1% (+21.6 percentage points)
- Fast inference: ~31 words per second, with a 30 MB model size enabling on-device deployment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prepending a learned language-ID token to the input sequence enables a single model to disambiguate phoneme mappings across languages sharing the same script.
- Mechanism: The 6-way language-ID embedding (≈1k parameters) is concatenated with grapheme embeddings before the encoder, providing a conditioning signal that routes shared grapheme patterns to language-specific phoneme outputs. Without this, the model cannot resolve which phoneme system to apply.
- Core assumption: Languages in the training set have sufficiently distinct grapheme-to-phoneme mappings that a single conditioning vector can separate them without interfering with sequence modeling.
- Evidence anchors:
  - [abstract] "learned 6-way language-ID embedding"
  - [Section V.B / Table III] Ablation removing language-ID degrades mean PER from 3.5% to 25.1% (+21.6 percentage points); z-tests confirm p < 10⁻⁶ for all languages.
  - [corpus] Weak direct evidence; neighbor papers (POWSM, G2P+) address multilingual phonology but do not isolate language-ID conditioning effects.
- Break condition: If languages in the training set have nearly identical G2P rules, or if the language-ID embedding dimension is too small to separate them, the conditioning signal may fail to improve over a language-agnostic baseline.

### Mechanism 2
- Claim: A compact Transformer with constrained output vocabulary (109 IPA symbols) can approach the accuracy of much larger byte-level models while being ~77× smaller.
- Mechanism: Constraining the output softmax to a small IPA vocabulary reduces the hypothesis space for each prediction. Combined with a 4-layer, 256-d architecture, this limits parameter count (7.5M) while retaining sufficient capacity for the G2P mapping.
- Core assumption: The IPA symbol set adequately covers all target phonemes; no frequent out-of-vocabulary outputs are needed.
- Evidence anchors:
  - [abstract] "7.5 M-parameter Transformer...mean phoneme error rate (PER) of 3.5%...outperforming the byte-level ByT5 baseline (5.4%)"
  - [Section III.A] "The output soft-max covers 109 IPA symbols; totals ≈7.5 M"
  - [Section V.A / Table II] Direct comparison with ByT5 (580M parameters) shows 3.5% vs 5.4% mean PER.
  - [corpus] LiteG2P (neighbor) demonstrates compact NAR G2P viability but on monolingual data; not directly comparable.
- Break condition: If target languages require many additional IPA diacritics or suprasegmental markers not in the 109-symbol set, output coverage will degrade.

### Mechanism 3
- Claim: Joint training on related languages leverages overlapping phoneme inventories to share parameters without per-language specialization.
- Mechanism: Romance languages and English share many IPA symbols; the model amortizes learning of common mappings (e.g., /p/, /t/, /k/) across languages, while the language-ID embedding handles divergent cases.
- Core assumption: Shared phoneme inventory is sufficiently large relative to language-specific exceptions to yield net parameter efficiency.
- Evidence anchors:
  - [Section I] "Cross-lingual sharing: phoneme inventories overlap, so one parameter set can cover multiple languages without growing linearly"
  - [Section V.A] Model leads in 4/6 languages despite being single and multilingual.
  - [corpus] No direct external validation of cross-lingual phoneme sharing as mechanism; neighbor papers focus on monolingual G2P (Phonikud, Persian intermediate language) or broader phonetic tasks (POWSM).
- Break condition: If languages have highly divergent phoneme inventories or G2P rules, sharing may hurt accuracy compared to specialized models.

## Foundational Learning

- **Transformer encoder-decoder with cross-attention**
  - Why needed here: LatPhon uses a standard 4-layer Transformer encoder-decoder; understanding attention, positional encodings (here, rotary), and teacher-forced training is prerequisite to modifying the architecture.
  - Quick check question: Can you explain how decoder cross-attention accesses encoder states during autoregressive generation?

- **International Phonetic Alphabet (IPA)**
  - Why needed here: The model outputs IPA sequences; interpreting results, debugging errors (e.g., vowel quality, gemination), and extending to new languages require phonetic literacy.
  - Quick check question: What is the difference between /e/ and /ɛ/ in IPA, and which Romance languages contrast these?

- **Phoneme Error Rate (PER)**
  - Why needed here: PER is the primary evaluation metric; understanding it as the edit distance (substitutions, insertions, deletions) between predicted and reference phoneme sequences is essential for interpreting ablation results.
  - Quick check question: If reference is /ˈpɛto/ and prediction is /ˈpɛtto/, how many errors contribute to PER?

## Architecture Onboarding

- Component map:
  - Language-ID token selection and embedding lookup -> Concatenation/prepending to grapheme input -> 4-layer encoder (256-d, 8-head, RoPE) -> Decoder cross-attention -> Autoregressive generation over 109-symbol IPA vocabulary -> Softmax output

- Critical path:
  1. Language-ID token selection and embedding lookup.
  2. Concatenation/prepending to grapheme input.
  3. Encoder processes full sequence; decoder generates IPA tokens autoregressively.
  4. Loss computed via negative log-likelihood under teacher forcing.

- Design tradeoffs:
  - Autoregressive vs non-autoregressive: LatPhon is AR (accurate but slower); NAR (e.g., LiteG2P) is faster but requires length regulators and distillation.
  - Multilingual vs monolingual: Single model reduces maintenance but may underperform per-language WFSTs on edge cases.
  - Vocabulary size: 109 IPA symbols keeps softmax small but may miss rare diacritics or suprasegmentals.

- Failure signatures:
  - High PER on low-data languages (Italian: 5.8% vs mean 3.5%) → insufficient training examples.
  - Consonant gemination errors in Italian (e.g., /ˈpɛtto/ → /ˈpɛto/) → model fails to learn phonemic length distinction.
  - Stress misplacement (English, Italian) → model lacks explicit prosody features.
  - OOV or rare proper nouns → dictionary-based training doesn't generalize well.

- First 3 experiments:
  1. Replicate ablation: Remove language-ID embedding and verify PER degrades to ~25% to confirm conditioning mechanism on your data.
  2. Low-resource augmentation: Upsample or synthesize data for the weakest language (Italian) and measure PER reduction.
  3. Inference profiling: Benchmark batch-1 throughput on target deployment hardware (CPU/embedded GPU) to confirm real-time feasibility.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can joint training of LatPhon with end-to-end acoustic models (TTS or ASR) improve synthesis or recognition quality compared to pipelined deployment?
- Basis in paper: [explicit] "We plan to explore the joint training of the G2P module with end-to-end acoustic models, investigating if this co-adaptation can further improve synthesis quality."
- Why unresolved: The current model is trained standalone on dictionary data; co-adaptation effects with downstream acoustic models remain untested.
- What evidence would resolve it: Comparative experiments where LatPhon is fine-tuned jointly with a TTS acoustic model versus used as a frozen front-end, evaluated via subjective MOS scores or ASR word error rates.

### Open Question 2
- Question: What synthetic data augmentation techniques can effectively improve G2P accuracy for low-resource languages like Italian in multilingual training?
- Basis in paper: [explicit] "To bolster performance on low-resource languages, such as Italian in our study, we will first investigate synthetic data augmentation techniques."
- Why unresolved: Italian achieved 5.8% PER with only 6,108 training pairs versus 133,969 for English, indicating data scarcity. The paper does not test any augmentation.
- What evidence would resolve it: Experiments applying back-translation, rule-based perturbation, or cross-lingual transfer to augment Italian training data, reporting PER changes with statistical significance tests.

### Open Question 3
- Question: Would replacing LatPhon's autoregressive decoder with a non-autoregressive architecture (e.g., LiteG2P-style) retain multilingual accuracy while substantially increasing throughput?
- Basis in paper: [inferred] Limitations section notes "the autoregressive Transformer architecture is inherently slower at inference than non-autoregressive approaches" and that "applications requiring the absolute highest throughput might benefit from exploring alternative architectures."
- Why unresolved: The authors chose autoregressive decoding for accuracy but did not test NAR alternatives in the multilingual setting; LiteG2P comparison was infeasible due to differing datasets.
- What evidence would resolve it: Train a non-autoregressive variant of LatPhon with identical training data, compare PER across all six languages and measure words/second throughput on the same hardware.

## Limitations
- Limited evaluation on rare or proper nouns - The ipa-dict corpus likely underrepresents rare words, named entities, and loanwords. The model's performance on these cases remains untested, which is critical for real-world speech front-end deployment.
- Sensitivity to dictionary quality - With only ~600K total training pairs (less than 1% of ByT5's pre-training corpus), performance is tightly coupled to the quality and coverage of the ipa-dict resource. Errors in dictionary transcriptions or imbalanced language representation could skew results.
- No stress or prosody modeling - Stress marks are removed during preprocessing, and suprasegmental features (tone, intonation) are absent from the 109-symbol IPA set. The model cannot handle languages where these are phonemic (e.g., Mandarin, Swedish).

## Confidence
- **High confidence** in the multilingual architecture and parameter efficiency claims. The direct comparison with ByT5 (3.5% vs 5.4% PER) on the same corpus, combined with the extreme size difference (7.5M vs 580M parameters), is unambiguous.
- **Medium confidence** in the cross-lingual sharing hypothesis. While the model outperforms monolingual baselines in 4/6 languages, the neighbor literature doesn't isolate phoneme inventory overlap as the mechanism, and no controlled ablation of language diversity in training exists.
- **Medium confidence** in the language-ID token's importance. The ablation shows dramatic PER degradation (+21.6 percentage points), but the effect size is so large that confounding factors (e.g., training instability without the token) cannot be ruled out without additional experiments.

## Next Checks
1. **Language-ID ablation replication** - Remove the learned language-ID embedding and retrain from scratch; verify mean PER degrades to ~25% (the reported +21.6 point increase) to confirm the conditioning mechanism is reproducible.
2. **Low-resource language stress test** - For Italian (6,108 training pairs), upsample or synthesize data to match the size of French/Spanish; measure whether PER decreases substantially (target: <5%) to isolate data scarcity effects.
3. **Out-of-vocabulary proper noun probe** - Construct a test set of rare, named-entity, and loanword tokens not in ipa-dict; measure PER degradation to quantify dictionary-coverage limitations and inform whether additional data sources are needed.