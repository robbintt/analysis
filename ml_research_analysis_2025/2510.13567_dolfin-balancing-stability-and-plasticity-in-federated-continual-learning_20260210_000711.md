---
ver: rpa2
title: 'DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning'
arxiv_id: '2510.13567'
source_url: https://arxiv.org/abs/2510.13567
tags:
- learning
- federated
- continual
- proceedings
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of federated continual learning
  (FCL), where models must learn new tasks across distributed clients while protecting
  privacy and preventing catastrophic forgetting of previous knowledge. The core method,
  DOLFIN, combines Vision Transformers with low-rank adapters (LoRA) and incorporates
  Dual Gradient Projection Memory (DualGPM) to ensure interference-free learning.
---

# DOLFIN: Balancing Stability and Plasticity in Federated Continual Learning

## Quick Facts
- arXiv ID: 2510.13567
- Source URL: https://arxiv.org/abs/2510.13567
- Reference count: 40
- Primary result: Outperforms six baselines on four image classification benchmarks while maintaining minimal communication overhead

## Executive Summary
This paper introduces DOLFIN, a federated continual learning method that addresses catastrophic forgetting while protecting data privacy in distributed settings. The approach combines Vision Transformers with low-rank adapters (LoRA) and Dual Gradient Projection Memory (DualGPM) to enable stable knowledge retention across tasks without accessing raw data. By maintaining orthogonal gradient subspaces through frozen A-matrices and trainable B-matrices, DOLFIN achieves interference-free learning while only requiring communication of low-rank matrices. The method demonstrates strong performance across four image classification benchmarks under non-IID data distributions.

## Method Summary
DOLFIN operates on a frozen ViT-B/16 backbone with LoRA modules attached to attention key/value projections. Each task uses frozen A-matrices spanning orthogonal subspaces to previous tasks' gradients, while only B-matrices are locally trained. DualGPM maintains an orthonormal basis of past gradients, enabling projection of new activations onto orthogonal subspaces via SVD extraction. The federated setup involves server broadcast of A_t and B_t matrices to all clients, local B-matrix training for 5 epochs, and weighted averaging of updates. Clients compute new A_{t+1} matrices using DualGPM, which are then averaged on the server. This process repeats for each task, with evaluation occurring after the final task as Final Average Accuracy (FAA).

## Key Results
- Consistently surpasses six strong baselines across CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 benchmarks
- Achieves minimal communication overhead by training and transmitting only low-rank matrices
- Maintains performance under two Dirichlet heterogeneity settings (β ∈ {0.1, 0.5, 1.0})
- Matches memory footprint of competing methods while delivering superior accuracy

## Why This Works (Mechanism)

### Mechanism 1: Orthogonal Subspace Decomposition via Frozen LoRA A-Matrices
- Claim: Separating LoRA into frozen A-matrices (spanning task subspaces) and trainable B-matrices enables interference-free knowledge accumulation.
- Mechanism: A-matrices are constructed to span subspaces orthogonal to previous tasks' gradient subspaces. Only B-matrices are updated during training, constraining updates to non-overlapping directions in parameter space.
- Core assumption: Gradient subspaces of different tasks are approximately orthogonal or can be made orthogonal without significant loss of expressive power.
- Evidence anchors: Abstract states "Each LoRA module is designed to be orthogonal to previous tasks' gradient subspaces, enabling stable knowledge retention"; Section 3 describes A-matrices as frozen and spanning task-specific update subspaces.
- Break condition: If tasks share highly correlated gradient subspaces (e.g., fine-grained subclasses), orthogonal constraint may overly restrict capacity, causing underfitting.

### Mechanism 2: Dual Gradient Projection Memory (DualGPM) for Gradient Subspace Estimation
- Claim: Maintaining an orthonormal basis of past task gradients enables computation of orthogonal projection directions without storing raw data.
- Mechanism: DualGPM maintains M_t as an orthonormal basis approximating past gradients. For new task t, hidden activations H_t are projected onto M^⊥_t via Ĥ_t = (I - M_t M_t^T)H_t. SVD on Ĥ_t^T extracts top-r singular components for constructing A_{t+1}. After training, M_t is updated by removing components aligned with new task gradients.
- Core assumption: Past gradient information can be sufficiently compressed into a low-rank orthonormal basis without losing critical interference directions.
- Evidence anchors: Abstract mentions "incorporates Dual Gradient Projection Memory (DualGPM) to ensure interference-free learning"; Section 3 details maintaining orthonormal basis M_t that approximates past gradients.
- Break condition: If rank r is too small relative to task complexity, M_t cannot capture sufficient gradient history, leading to interference leakage.

### Mechanism 3: Decoupled Federated Aggregation of Orthogonal Bases
- Claim: Aggregating A-matrices and B-matrices separately preserves orthogonality constraints while integrating distributed knowledge.
- Mechanism: Clients receive A_t, B_t from server, train local B^k_t keeping other modules frozen. B-matrices aggregated via weighted averaging: B_t = Σ_k (n_k / Σ_j n_j) B^k_t. Each client then computes local A^k_{t+1} using DualGPM, and server averages these to form unified A_{t+1}.
- Core assumption: Averaging orthogonal bases across clients maintains approximate orthogonality to global gradient history.
- Evidence anchors: Section 3 states "the server averages these local matrices to form the unified A_{t+1}, maintaining interference-free continual learning without accessing past data"; also describes weighted averaging for B matrices.
- Break condition: Under extreme heterogeneity (β → 0), local gradient subspaces diverge significantly; averaging A-matrices may produce non-orthogonal composite basis.

## Foundational Learning

- **LoRA (Low-Rank Adaptation)**
  - Why needed here: Core parameter-efficient mechanism; must understand A/B decomposition and rank constraints.
  - Quick check question: Can you explain why LoRA's low-rank factorization W + AB reduces trainable parameters while preserving expressivity?

- **Gradient Projection for Continual Learning**
  - Why needed here: DualGPM builds on projecting gradients away from important directions; orthogonal subspace methods are central.
  - Quick check question: Given gradient memory M storing past important directions, how would you project a new gradient g to avoid interference?

- **Federated Averaging (FedAvg)**
  - Why needed here: DOLFIN extends FedAvg with orthogonal adapter aggregation; understanding weighted averaging is prerequisite.
  - Quick check question: How does FedAvg aggregate client models, and why does data heterogeneity (Dirichlet β) affect convergence?

## Architecture Onboarding

- **Component map:** ViT-B/16 backbone (frozen) -> LoRA modules on key/value projections -> DualGPM per client (maintains M_t) -> Server (aggregates B_t and A_{t+1})

- **Critical path:**
  1. Server broadcasts A_t, B_t to all K clients
  2. Each client trains B^k_t locally for 5 epochs (frozen backbone + A matrices)
  3. Clients upload B^k_t; server aggregates via sample-weighted averaging
  4. Each client computes A^k_{t+1} via DualGPM: project activations through M^⊥_t, extract SVD components
  5. Clients upload A^k_{t+1}; server averages to produce global A_{t+1}
  6. Update M_t+1 by incorporating new task gradient information

- **Design tradeoffs:**
  - Rank r: Higher r = more capacity per task but more communication; paper uses r ∈ {1, 2, 32, 64} across datasets
  - Dirichlet β: Lower β = higher heterogeneity = harder learning; validate across β ∈ {0.1, 0.5, 1.0}
  - Frozen vs. trainable backbone: Freezing reduces forgetting but may limit adaptation to domain shifts

- **Failure signatures:**
  - Accuracy collapse on early tasks: M_t not capturing sufficient gradient history → increase rank or verify DualGPM update
  - A-matrix aggregation produces near-zero norms: Client subspaces too divergent → check heterogeneity level, consider client clustering
  - No improvement over baselines: Orthogonality constraint too restrictive → increase r or verify SVD extraction

- **First 3 experiments:**
  1. Reproduce CIFAR-100 with β=0.5: 10 tasks, 10 clients, r=2, lr=3e-3; verify FAA matches ~85%
  2. Ablation on rank r: Test r ∈ {1, 2, 4, 8} on ImageNet-R β=1.0; plot FAA vs. communication cost
  3. Orthogonality verification: Measure ||A_i^T A_j||_F for i≠j after training; should be near-zero if mechanism works

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the weighted averaging of client-specific orthogonal matrices (A_{t+1}) strictly preserve the interference-free property relative to the global gradient subspace?
- Basis in paper: [inferred] The methodology states that clients compute A_{k}^{t+1} based on local gradients (N_{t}^{k}) and the server averages them. It is unclear if the average of locally orthogonal bases remains orthogonal to the global joint distribution of past data.
- Why unresolved: The paper asserts the method is "interference-free," but mathematical guarantees for averaged subspaces in non-IID settings (Dirichlet β) are not derived.
- What evidence would resolve it: A theoretical analysis or empirical measurement of gradient projection overlap on the global model after aggregating local A matrices under high heterogeneity.

### Open Question 2
- Question: How does DOLFIN perform in streaming scenarios with significantly longer task sequences (e.g., >50 tasks) where linear parameter growth becomes prohibitive?
- Basis in paper: [inferred] The experiments are limited to 10 incremental tasks. The method adds new low-rank adapters (A and B) for every task without a mechanism for compression or merging, implying a linear increase in memory footprint.
- Why unresolved: The conclusion claims the solution is "scalable," but the memory constraints of accumulating distinct modules over extensive lifetimes were not evaluated.
- What evidence would resolve it: Evaluation results on large-scale benchmark streams (e.g., CORe50 or continuous TinyImagenet) measuring accuracy degradation and memory limits as T increases significantly.

### Open Question 3
- Question: Is there a theoretical or automated method for determining the optimal rank r for the LoRA modules to prevent capacity saturation or redundancy?
- Basis in paper: [inferred] Table 2 shows a grid search was used to select ranks ranging from r=1 (CUB-200) to r=64 (ImageNet-R), indicating high sensitivity and dataset dependence.
- Why unresolved: The paper does not provide a heuristic for adapting the rank to the complexity of the incoming task, leaving it as a manual hyperparameter.
- What evidence would resolve it: An ablation study showing the relationship between task complexity (e.g., class count or feature variance) and the minimum rank required to maintain orthogonality without over-parameterization.

## Limitations

- DualGPM update mechanism details are not fully specified in the paper, requiring reference to external work [18] for complete implementation
- The method shows linear memory growth with task count, lacking mechanisms for compression or merging of accumulated adapters
- Rank selection remains a manual hyperparameter requiring dataset-specific grid search, with no theoretical guidance for optimal values

## Confidence

- **Mechanism 1 (Orthogonal LoRA decomposition): High** - Well-established LoRA approach with clear implementation details
- **Mechanism 2 (DualGPM): Medium** - Depends on external algorithm specification for complete reproducibility
- **Mechanism 3 (Federated aggregation): High** - Follows standard FedAvg patterns with clear modifications

## Next Checks

1. Verify A-matrix orthogonality post-training by computing ||A_i^T A_j||_F for all task pairs; should be near-zero for i≠j
2. Test rank sensitivity by running CIFAR-100 with β=0.5 across r ∈ {1, 2, 4, 8} and measuring FAA vs communication cost
3. Evaluate under extreme heterogeneity (β=0.01) to stress-test federated aggregation stability and identify divergence thresholds