---
ver: rpa2
title: Advancing Vietnamese Information Retrieval with Learning Objective and Benchmark
arxiv_id: '2503.07470'
source_url: https://arxiv.org/abs/2503.07470
tags:
- embedding
- training
- vietnamese
- text
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the Vietnamese Context Search (VCS) benchmark
  for evaluating Vietnamese embedding models on retrieval and re-ranking tasks, addressing
  the lack of standard benchmarks in Vietnamese information retrieval. The VCS benchmark
  includes three tasks: ViMedRetrieve for document retrieval, ViRerank for sentence
  re-ranking, and ViGLUE-R for NLI-based re-ranking.'
---

# Advancing Vietnamese Information Retrieval with Learning Objective and Benchmark

## Quick Facts
- arXiv ID: 2503.07470
- Source URL: https://arxiv.org/abs/2503.07470
- Reference count: 15
- Introduces Vietnamese Context Search (VCS) benchmark and modified InfoNCE loss for Vietnamese information retrieval

## Executive Summary
This paper addresses the lack of standard benchmarks for Vietnamese information retrieval by introducing the Vietnamese Context Search (VCS) benchmark. The benchmark includes three tasks: ViMedRetrieve for document retrieval, ViRerank for sentence re-ranking, and ViGLUE-R for NLI-based re-ranking. The authors propose a modified InfoNCE loss function that incorporates a (1-p+) term to slow learning as probability increases, showing improved performance over standard InfoNCE across various tasks and training methods. Experiments demonstrate that lower temperature values (τ=0.1) yield better performance, and a 20M-parameter Vietnamese embedding model trained with the proposed method achieves competitive results compared to larger existing models.

## Method Summary
The paper introduces a modified InfoNCE loss function for contrastive learning in Vietnamese information retrieval. The loss incorporates a (1-p+) term that reduces gradient updates on well-classified samples, preventing over-learning while maintaining learning signal on harder examples. The method is evaluated using the newly proposed Vietnamese Context Search (VCS) benchmark, which includes tasks for document retrieval, sentence re-ranking, and NLI-based re-ranking. A 20M-parameter Vietnamese embedding model is trained using this approach and compared against larger existing models.

## Key Results
- The modified InfoNCE loss outperforms standard InfoNCE across all evaluated tasks and training methods
- Temperature analysis shows lower values (τ=0.1) yield better performance for retrieval and reranking tasks
- Hard-negative training outperforms in-batch negatives for reranking, while in-batch negatives perform better for retrieval with larger k values
- The 20M-parameter Vietnamese embedding model achieves competitive results compared to larger existing models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed modified InfoNCE loss reduces gradient updates on well-classified samples, preventing over-learning while maintaining learning signal on harder examples.
- Mechanism: By multiplying the standard InfoNCE loss by (1-p+), where p+ is the probability of the positive pair, the loss weight automatically decreases as the model becomes more confident. The term acts as an adaptive curriculum—easy pairs (high p+) contribute less to gradient updates, forcing the model to focus computational capacity on harder discriminative cases.
- Core assumption: The paper hypothesizes that standard InfoNCE continues optimizing already well-separated pairs unnecessarily, wasting capacity that could focus on harder negatives.
- Evidence anchors:
  - [abstract]: "a new objective function based on the InfoNCE loss function... aims to be better than the origin in information retrieval tasks"
  - [section 3.2.4]: "This extra weight prevents the over-learned scenario of the original loss function by reducing the gradient from the loss value to the model's weights on those samples"
- Break condition: If positive pair probabilities saturate near 1.0 early in training, the (1-p+) term would near-zero all gradients, potentially causing premature convergence.

### Mechanism 2
- Claim: Lower temperature τ values in contrastive loss improve embedding discrimination for retrieval and reranking tasks.
- Mechanism: Temperature controls the sharpness of the softmax distribution over similarity scores. Lower τ amplifies differences between positive and negative pair scores, creating a harder optimization landscape that forces the model to learn more discriminative representations.
- Core assumption: The embedding space benefits from harder contrastive pressure rather than smoother optimization landscapes.
- Evidence anchors:
  - [section 4.3]: "the performance of embedding models on reranking tasks decreases as the temperature τ increases"
  - [section 4.3]: "this experiment shows that the temperature should be low for text embedding models to perform well on retrieval and reranking"
- Break condition: If training loss diverges or validation performance degrades sharply with τ < 0.05, temperature may be too low for stable gradient flow.

### Mechanism 3
- Claim: Hard-negative training outperforms in-batch negatives on reranking; in-batch negatives outperform on retrieval when k is larger.
- Mechanism: Reranking requires fine-grained discrimination among a small set of candidates, which benefits from explicitly curated hard negatives that challenge the model's decision boundary. Retrieval at larger k benefits from the diversity and scale of in-batch negatives, which expose the model to broader variation in negative samples.
- Core assumption: The difference in task structure (small candidate set vs large corpus search) maps to different optimal negative sampling strategies.
- Evidence anchors:
  - [section 4.1, Table 2]: "training with hard-negative examples results in better performance compared to the in-batch negative training method for all reranking tasks"
  - [section 4.1, Table 3]: "for both objective functions, the performance of the in-batch negative method is higher than that of the hard-negative training method" for retrieval
- Break condition: If curated hard negatives contain label noise or are not genuinely difficult, the reranking benefit could reverse.

## Foundational Learning

- Concept: InfoNCE Loss (Contrastive Learning)
  - Why needed here: The paper's contribution directly modifies InfoNCE. Understanding the base formulation—maximizing positive pair similarity while minimizing negative pair similarity via softmax-based probability—is essential to grasp why the (1-p+) modification changes optimization dynamics.
  - Quick check question: Given a batch with 1 positive and 31 in-batch negatives, what happens to the loss when the positive pair similarity score increases while negative scores stay constant?

- Concept: Mean Pooling vs CLS Token Representation
  - Why needed here: The architecture uses mean pooling (Equation 2) to aggregate token embeddings. Understanding why mean pooling might preserve more semantic information than CLS tokens—and when it fails (e.g., variable-length padding artifacts)—is critical for debugging embedding quality.
  - Quick check question: If a 50-token document is padded to 224 tokens, how would naive mean pooling (without attention mask) distort the embedding?

- Concept: Evaluation Metrics for IR (mAP, Accuracy@k)
  - Why needed here: The paper reports mAP for reranking and Accuracy@k for retrieval. Understanding that mAP penalizes ranking errors across all positions while Accuracy@k is a binary hit/miss metric clarifies why different methods excel on different tasks.
  - Quick check question: A model ranks 3 relevant documents at positions 2, 5, and 10 out of 100. What is the mAP for this single query?

## Architecture Onboarding

- Component map: Input Text → [Instruction Prefix] → BERT Encoder (MiniLM-L6-v2, 20M params) → Token Embeddings (n × d) → Mean Pooling → Dense Embedding (d-dim) → Cosine Similarity → Modified InfoNCE Loss (training) / Ranking (inference)
- Critical path:
  1. Data preparation: Filter by text length (max 224 tokens), format as (query, positive) for in-batch or (query, positive, negative) for hard-negative
  2. Training: Batch size 32, learning rate 5e-5, 3 epochs, temperature τ ∈ {0.1, 0.4, 0.7} (τ=0.1 recommended per results)
  3. Evaluation: Run on VCS benchmark (ViMedRetrieve for retrieval, ViRerank/MNLI-R/QNLI-R for reranking)
- Design tradeoffs:
  - Model size (20M vs 130M): Paper shows 20M model achieves comparable performance to 130M phoBERT-based models on some tasks—but not all. Tradeoff is inference speed vs absolute performance ceiling
  - In-batch vs hard-negative: In-batch is faster (4h45m vs 7h30m training time per Table 7 appendix) and better for retrieval; hard-negative is slower but better for reranking
  - Single encoder with instruction prefix vs dual encoder: Paper uses single encoder with `<|query|>` prefix, reducing parameters but requiring the model to learn context-dependent representations
- Failure signatures:
  - Low ViMedRetrieve accuracy (<15% at k=5): Likely indicates insufficient domain coverage in training data (medical vocabulary) or embedding collapse
  - Reranking mAP dropping with in-batch training: Expected per results, but if drop is severe (>10 points), check batch composition for topic imbalance
  - Loss not decreasing with τ=0.1: Temperature may be too low for the model's current scale; try τ=0.2 or check for gradient explosion
- First 3 experiments:
  1. **Baseline reproduction**: Train the 20M model with standard InfoNCE (τ=0.1, in-batch negatives) on the provided training data. Evaluate on all VCS tasks. Compare to paper's Table 2 and Table 3 to validate setup
  2. **Loss ablation**: Swap to the modified loss function L_ours with identical hyperparameters. Measure delta on reranking vs retrieval tasks to confirm task-specific gains
  3. **Temperature sweep**: Run τ ∈ {0.05, 0.1, 0.2, 0.4, 0.7} on a held-out validation split (not the test set). Plot performance curves to identify optimal τ before test evaluation, checking for the break condition where very low τ causes instability

## Open Questions the Paper Calls Out
None

## Limitations
- Data Generalization: Training data is primarily news and QQP-style data, raising questions about generalization to specialized domains like medicine
- Hard Negative Quality: The paper reports that hard-negative training outperforms in-batch negatives for reranking, but does not validate whether the curated hard negatives are genuinely difficult or potentially mislabeled
- Temperature Stability: The paper advocates τ=0.1 as optimal, but does not report experiments at τ<0.05 or stability analysis across different batch sizes

## Confidence
- High Confidence: The modified InfoNCE loss mechanism (1-p+ term reducing gradient updates on well-classified pairs) is mathematically sound and the experimental evidence consistently shows improved reranking performance over standard InfoNCE
- Medium Confidence: The temperature recommendation (τ=0.1) is supported by ablation studies within the reported range, but the absence of experiments at τ<0.05 or with different batch sizes limits confidence in the universality of this recommendation
- Medium Confidence: The task-specific negative sampling recommendation (hard-negative for reranking, in-batch for retrieval) is empirically supported, but the underlying assumption that task structure directly maps to optimal negative sampling strategy requires further validation

## Next Checks
1. **Hard Negative Quality Audit**: Sample 100 hard negatives used in the reranking experiments and conduct human annotation to verify they are genuinely difficult (semantically similar but negative pairs) versus mislabeled or trivially different
2. **Domain Transfer Validation**: Train the 20M model exclusively on medical-domain Vietnamese text (if available) and evaluate on ViMedRetrieve. Compare performance gains against the general-domain trained model
3. **Temperature Stability Analysis**: Run training with τ ∈ {0.05, 0.1, 0.2, 0.4, 0.7} on a held-out validation split (not test set) and monitor both training loss convergence and validation performance across 10 epochs. Plot performance curves to identify optimal τ and specifically test the break condition where very low τ causes instability or gradient explosion