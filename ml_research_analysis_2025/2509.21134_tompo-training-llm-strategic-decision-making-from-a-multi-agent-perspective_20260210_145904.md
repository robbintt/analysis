---
ver: rpa2
title: 'ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective'
arxiv_id: '2509.21134'
source_url: https://arxiv.org/abs/2509.21134
tags:
- agents
- effort
- agent
- your
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ToMPO, a reinforcement learning algorithm
  designed to enhance Large Language Models' strategic decision-making in multi-agent
  environments. ToMPO addresses the limitations of existing methods by incorporating
  a Theory of Mind perspective, enabling models to reason about other agents' strategies
  and adapt their own decisions accordingly.
---

# ToMPO: Training LLM Strategic Decision Making from a Multi-Agent Perspective

## Quick Facts
- arXiv ID: 2509.21134
- Source URL: https://arxiv.org/abs/2509.21134
- Reference count: 40
- The paper introduces ToMPO, a reinforcement learning algorithm designed to enhance Large Language Models' strategic decision-making in multi-agent environments. ToMPO addresses the limitations of existing methods by incorporating a Theory of Mind perspective, enabling models to reason about other agents' strategies and adapt their own decisions accordingly. The algorithm generates rollouts based on reasoning about others' strategies, estimates advantages at both graph and sample levels, and balances global and partial rewards. Evaluated in sequential graph-effort strategic games (BCZ and PGG), ToMPO significantly improves model compliance and cooperative outcomes by 35% compared to GRPO and achieves an 18% improvement over models with 100x more parameters, demonstrating its effectiveness in enhancing strategic decision-making capabilities.

## Executive Summary
ToMPO addresses the challenge of training LLMs for strategic decision-making in multi-agent environments by incorporating Theory of Mind principles. The algorithm uses a dual-level advantage estimation mechanism that combines sample-level decision accuracy with graph-level structural optimality. By generating rollouts that explicitly reason about other agents' strategies and using expert models as opponents, ToMPO enables policy models to learn superior cooperative strategies. The approach is evaluated in sequential graph-effort games (BCZ and PGG), demonstrating significant improvements in compliance, strategic efficiency, and cooperative outcomes compared to existing methods.

## Method Summary
ToMPO is a two-stage training approach for LLMs in multi-agent strategic decision-making. First, Supervised Fine-Tuning (SFT) with LoRA adapts a Qwen-2.5-7B-instruct model to generate effort decisions from expert data created by GPT-o3 simulations. Second, Reinforcement Fine-Tuning (RFT) applies ToMPO, which estimates advantages at both sample-level (using F1/accuracy against expert decisions) and graph-level (using Hamming distance to expert graphs plus global memory tracking). The algorithm generates rollouts by reasoning about other agents' strategies, with expert models (GPT-o3) serving as opponents. The combined advantage is used to update the policy via a clipped PPO objective with KL penalty, balancing immediate decision quality with long-term structural optimality.

## Key Results
- ToMPO improves model compliance and cooperative outcomes by 35% compared to GRPO in sequential graph-effort games
- The approach achieves 18% improvement over models with 100x more parameters
- Dual-level advantage estimation (sample + graph) with weights 0.8/0.2 provides optimal balance between accuracy and structural convergence
- Training on homogeneous agent configurations generalizes to heterogeneous environments

## Why This Works (Mechanism)

### Mechanism 1: Dual-Level Advantage Estimation
- Claim: Separating advantage estimation into sample-level and graph-level components improves credit assignment for interdependent strategic decisions.
- Mechanism: Sample-level advantage (A_S) uses F1/Accuracy against expert decisions for immediate decision quality. Graph-level advantage (A_G) incorporates both local comparison (current prompt best) and global comparison (memory best) to capture structural optimality over time. The combined advantage A = w_sample·A_S + w_graph·A_G (with w_sample=0.8, w_graph=0.2) balances precision with global awareness.
- Core assumption: Assumes that expert graph structures (G_expert from GPT-o3) represent near-optimal strategies that the policy model should converge toward, and that Hamming distance meaningfully captures structural quality.
- Evidence anchors:
  - [abstract]: "estimating advantages at both the graph-level and sample-level"
  - [section 4.2]: "ToMPO graph-level advantage estimation balances local precision with global graph optimality"
  - [corpus]: Weak direct corpus evidence; neighbor papers focus on evaluation benchmarks rather than dual-advantage RL mechanisms.
- Break condition: If expert graphs are not actually optimal, or if Hamming distance fails to capture strategic equivalence (different structures yielding similar payoffs), the advantage signal becomes misaligned.

### Mechanism 2: Theory of Mind Rollout Generation
- Claim: Generating rollouts conditioned on reasoning about other agents' strategies produces better policy updates than treating other agents as environment noise.
- Mechanism: During training, the policy model (Agent 0) interacts with expert models (GPT-o3) as Agents 1-N. The expert decision graphs (G_expert) are used as reference structures. Each rollout combines the policy model's decision with the expert graph to form G_i, enabling direct comparison to optimal structures. This creates a training signal that explicitly accounts for multi-agent interdependence.
- Core assumption: Assumes expert models exhibit sufficiently superior strategies that exposure to them teaches the policy model better strategic reasoning, and that the expert strategies generalize across game configurations.
- Evidence anchors:
  - [abstract]: "generating rollouts based on reasoning the strategies of other individuals"
  - [section 4.2]: "This makes the strategies of the policy model generally inferior to those of other individuals in the environment, making the purpose of reinforcement learning training clearer."
  - [corpus: HLSMAC paper]: Supports importance of high-level strategic evaluation in multi-agent settings, though not ToM-specific.
- Break condition: If expert models exhibit exploitable biases or if their strategies don't transfer across heterogeneous agent configurations, the policy model learns suboptimal opponent modeling.

### Mechanism 3: Sequential Credit Assignment Decomposition
- Claim: Decomposing strategic decision-making into forward (effort optimization given structure) and inverse (structure optimization given history) processes improves temporal credit assignment.
- Mechanism: The forward process maximizes effort decisions within a fixed graph using equation (4). The inverse process maximizes graph decisions using expected value from history via equation (5). The gradient combines both credit streams plus a consistency term ζ(ΔC), enabling separate optimization paths while maintaining coherence.
- Core assumption: Assumes decisions can be cleanly decomposed into structure-forming and effort-investing, and that the utility function r properly captures the joint optimization landscape.
- Evidence anchors:
  - [section 2]: "These two processes align with the credit assignment principle (equation 6)"
  - [section 3.1]: Defines the sequential BCZ and PGG environments with explicit τ sequences for G and E decisions.
  - [corpus: Think, Speak, Decide paper]: Addresses language-augmented multi-agent RL for economic decisions, supporting decomposition approaches but not this specific formulation.
- Break condition: If graph and effort decisions are strongly coupled in ways the decomposition misses (e.g., commitment strategies where current effort signals future graph preferences), credit assignment becomes incorrect.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO) with Clipping**
  - Why needed here: ToMPO builds on PPO's objective (equation 13 uses clip(r_i(θ), 1-ε, 1+ε)). Understanding clipping prevents destabilizing policy updates during RL fine-tuning.
  - Quick check question: Can you explain why PPO uses clipping instead of a hard KL constraint, and what ε controls?

- Concept: **Graph Theory: Adjacency Matrices and Hamming Distance**
  - Why needed here: Decisions are represented as graphs G ∈ {0,1}^(N×N). Graph-level rewards use Hamming distance between adjacency matrices. Understanding clique formation (for PGG groups) is essential.
  - Quick check question: Given two 4-agent adjacency matrices, can you compute their Hamming distance and identify if they share any maximal cliques?

- Concept: **Game Theory: Public Goods and Network Games**
  - Why needed here: The BCZ game (equation 7) and PGG (equation 8) define the strategic environments. Understanding payoff functions, Nash equilibria concepts, and the tension between individual and collective rationality is prerequisite to interpreting results.
  - Quick check question: In a 4-agent PGG with r=1.5, what is the individually rational effort level vs. the socially optimal effort level?

## Architecture Onboarding

- Component map:
  Environment Layer: BCZ (3 sub-environments: GE, GEE, GGE) and PGG generators with configurable hyperparameters (α, δ, c, r)
  Data Layer: Expert dataset D_effort from GPT-o3 simulations; training prompts Q with game configurations and memory states
  Model Layer: Qwen-2.5-7B-instruct backbone → LoRA adapter (rank=64, alpha=32) for SFT → ToMPO policy π_θ
  Reward Layer: Compliance reward (-0.5/0.5), Sample-level reward (F1+Acc), Graph-level reward (Hamming + prompt_best + memory_best)
  Optimization Layer: Combined advantage A = 0.8·A_S + 0.2·A_G with KL penalty β·D_KL[π_θ||π_ref]

- Critical path:
  1. Generate expert rollouts with GPT-o3 across 126 simulations (4-8 agents, homogeneous/heterogeneous)
  2. Extract expert decision graphs and effort decisions → create D_effort
  3. SFT on Qwen-2.5-7B-instruct using LoRA (lr=5e-5, batch_size=16)
  4. RFT with ToMPO: sample prompt q, generate m rollouts, compute dual advantages, update via equation 13

- Design tradeoffs:
  - **Sample vs. Graph Weight (0.8/0.2)**: Higher sample weight emphasizes immediate decision accuracy; higher graph weight would emphasize structural convergence but may slow learning.
  - **Expert Model as Opponents**: Using superior models creates clearer learning signal but may not reflect deployment scenarios with diverse opponent capabilities.
  - **Agent 0 Perspective Bias**: Training always positions policy model as Agent 0, potentially limiting generalization (acknowledged in Limitations).

- Failure signatures:
  - **Low U1 (Compliance)**: Model generates malformed decision lists (wrong length, self-loops) → need stronger compliance reward or better SFT data
  - **High U2 but Low U3**: Model optimizes individually rationally but fails to cooperate → graph-level advantage may be underweighted
  - **Unstable Graph Decisions**: High frequency of graph changes without convergence → increase w_global or extend training

- First 3 experiments:
  1. **Baseline Reproduction**: Apply GRPO (sample-level only) to SFT model on BCZ-GEE, measure U1/U2/U3. Confirm ToMPO's 35% improvement claim by running identical evaluation with ToMPO.
  2. **Ablation on Advantage Weights**: Sweep w_sample ∈ {0.5, 0.8, 1.0} with w_graph = 1 - w_sample. Plot convergence speed vs. final U3 to validate the 0.8/0.2 balance.
  3. **Cross-Environment Transfer**: Train on BCZ-GE (homogeneous), evaluate on BCZ-GEE (heterogeneous) and PGG-GE. Quantify generalization gap to assess whether ToMPO learns domain-agnostic strategic reasoning or overfits to specific payoff structures.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the training pipeline be modified to eliminate the positional bias where the policy model overfits to the "Agent 0" perspective?
- Basis in paper: [explicit] The authors state: "The policy model’s perspective may be biased towards agent 0 due to our training data. In future work, we will adjust the RFT prompt and training data to broaden the model’s perspectives."
- Why unresolved: The current Reinforcement Fine-Tuning (RFT) setup consistently places the policy model in the "Agent 0" role, which may prevent the model from generalizing strategic reasoning to arbitrary positions or roles within the network.
- What evidence would resolve it: A study showing stable performance metrics ($U_1, U_2, U_3$) when the policy model is assigned random or adversarial agent indices during evaluation.

### Open Question 2
- Question: What specific alternative Supervised Fine-Tuning (SFT) strategies can effectively combine graph and effort decision learning without causing the performance degradation observed in the current combined approach?
- Basis in paper: [explicit] The paper notes: "Despite tests showing reduced capability when combining supervised finetuning for graph and effort, we will explore alternative SFT methods or consider separating the SFT process."
- Why unresolved: Simultaneously fine-tuning for both graph-level (structural) and effort-level (investment) decisions appears to create interference or "catastrophic forgetting" in the current implementation, limiting the model's proficiency.
- What evidence would resolve it: Comparative experiments showing that a modular or sequentially-separated SFT approach achieves higher $U_2$ (Strategic Efficiency) scores than the current combined SFT baseline.

### Open Question 3
- Question: Is ToMPO dependent on the specific reasoning capabilities of the expert model (GPT-o3) used for rollouts, or can it effectively bootstrap strategic performance using weaker or open-source experts?
- Basis in paper: [inferred] Section 4.2 states: "All other agents are represented by the expert model GPT-o3. This makes the strategies of the policy model generally inferior to those of other individuals..."
- Why unresolved: The algorithm's "Theory of Mind" component relies on reasoning about the strategies of others. If the environment agents (expert models) are swapped for less capable models, the stability of the advantage estimation ($A_G$, $A_S$) is unknown.
- What evidence would resolve it: Ablation studies running ToMPO in environments populated by standard backbone models (e.g., Llama-3.1-8B) instead of reasoning models to see if the policy model still converges.

## Limitations
- Limited experimental validation with synthetic games only, leaving real-world applicability uncertain
- Expert data generation process using GPT-o3 Program of Thought prompts is not fully specified
- Claim of 35% and 18% improvements require verification across independent implementations
- Positional bias in training (always Agent 0) may limit generalization to arbitrary agent roles

## Confidence

- **High confidence**: The dual-level advantage estimation mechanism (sample + graph) is theoretically sound and well-specified, with clear mathematical formulation in equations 18-19.
- **Medium confidence**: The Theory of Mind rollout generation approach is plausible but relies heavily on the quality and generalizability of expert model strategies, which are not fully characterized.
- **Low confidence**: The 35% and 18% improvement claims are difficult to verify without complete experimental details, independent replication, and evaluation on more diverse multi-agent scenarios.

## Next Checks

1. **Ablation on Advantage Weighting**: Run ToMPO with w_sample ∈ {0.5, 0.8, 1.0} while keeping all other hyperparameters constant. Measure U3 convergence speed and final cooperative outcomes to empirically validate that w_sample=0.8 provides optimal balance between immediate accuracy and long-term structural optimization.

2. **Cross-Domain Transfer**: Train ToMPO on BCZ-GE (homogeneous agents), then evaluate on PGG-GE and heterogeneous BCZ environments. Quantify performance degradation to assess whether the learned strategic reasoning generalizes beyond the training distribution or overfits to specific payoff structures.

3. **Expert Strategy Robustness**: Create synthetic "noisy expert" datasets by perturbing GPT-o3 strategies (e.g., randomizing 10-30% of decisions) and retrain ToMPO. Compare final U3 performance to clean expert training to determine the algorithm's sensitivity to expert model quality and potential brittleness to suboptimal demonstrations.