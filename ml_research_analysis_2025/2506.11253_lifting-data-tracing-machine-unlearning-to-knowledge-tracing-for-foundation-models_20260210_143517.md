---
ver: rpa2
title: Lifting Data-Tracing Machine Unlearning to Knowledge-Tracing for Foundation
  Models
arxiv_id: '2506.11253'
source_url: https://arxiv.org/abs/2506.11253
tags:
- unlearning
- arxiv
- forgetting
- data
- photo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes lifting data-tracing machine unlearning to
  knowledge-tracing for foundation models, enabling unlearning requests at the knowledge
  level rather than just individual data points. The approach addresses the diverse
  needs of FM stakeholders who may not have access to training data but want to remove
  specific knowledge or capabilities.
---

# Lifting Data-Tracing Machine Unlearning to Knowledge-Tracing for Foundation Models

## Quick Facts
- arXiv ID: 2506.11253
- Source URL: https://arxiv.org/abs/2506.11253
- Reference count: 40
- Achieves Q-U scores up to 95.20% on fine-grained dog breed forgetting while preserving zero-shot classification performance

## Executive Summary
This paper introduces knowledge-tracing unlearning for foundation models, lifting the paradigm from individual data points to semantic knowledge concepts. The approach enables stakeholders without training data access to request removal of specific capabilities or knowledge through external proxy datasets. A case study on CLIP demonstrates that bounded hinge loss with KL regularization achieves high forgetting quality while preserving model utility on related concepts. The method addresses a critical gap in FM governance, providing a mechanism for removing harmful or outdated knowledge while maintaining general model capabilities.

## Method Summary
The approach uses Negative Hinge Loss (NHL) with dual KL regularization to enable knowledge-level unlearning. Stakeholders specify target concepts via knowledge taxonomies, and unlearners collect proxy exemplar images from external hierarchical datasets. The optimization minimizes a hinge loss on similarity scores (max[0, m + SIM(xi, yi) − maxy≠yi SIM(xi, y)]) with m=2, while applying KL divergence regularization on both coarse and fine-grained label distributions (αc=10, αf=20). The method operates without access to original training data, using the enhanced forgetting set containing both fine and coarse labels. Training uses ViT-L/14 CLIP for 8 epochs with Adam optimizer at lr=1e-7.

## Key Results
- Achieves Q-U scores up to 95.20% on fine-grained dog breed forgetting tasks
- Preserves zero-shot classification performance on Stanford Cars, Food101, Flower102, Caltech101, and CIFAR100
- Demonstrates bounded hinge loss prevents catastrophic model collapse compared to unbounded gradient ascent

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Bounded hinge loss enables controlled forgetting without catastrophic model collapse.
- Mechanism: A margin-based hinge loss max[0, m + SIM(xi, yi) − maxy≠yi SIM(xi, y)] clips gradients to zero when forgetting is sufficient, preventing the unbounded optimization of gradient ascent that destroys model utility.
- Core assumption: The initial model's confidence scores on forgetting examples indicate memorization strength that correlates with unlearning difficulty.
- Evidence anchors:
  - [abstract] "achieving Q-U scores up to 95.20% on fine-grained dog breed forgetting"
  - [Section 4.1.2] "A hinge loss for gradient ascent (GA)... alleviates this issue using a controllable and bounded hinge loss"
  - [corpus] Weak corpus support—neighbor papers focus on knowledge tracing in education, not machine unlearning mechanisms.
- Break condition: When margin m is set too high for low-confidence concepts, causing unnecessary unlearning effort and utility degradation.

### Mechanism 2
- Claim: Dual KL-regularization on the forgetting set itself preserves hierarchical knowledge structure.
- Mechanism: Two regularizers applied to forgetting images—(1) KL divergence between original and unlearned model's coarse-grained label distributions, (2) KL divergence over non-target fine-grained classes—leveraging semantic hierarchy implicit in the forgetting data.
- Core assumption: Images in the forgetting set carry information about both what to unlearn and what to preserve through their hierarchical labels.
- Evidence anchors:
  - [Section 4.1.2] "regularization using the enhanced forgetting set Dhf... universally effective for all MU methods studied"
  - [Table 2] NPO+KL achieves 92.24 Q-U vs NPO alone showing degraded performance
  - [corpus] No direct corpus support for this specific regularization strategy.
- Break condition: When forgetting and retention concepts share fine-grained features, KL regularization may preserve too much of the target knowledge.

### Mechanism 3
- Claim: Knowledge-level requests can be operationalized through external proxy datasets without training data access.
- Mechanism: Stakeholders specify unlearning targets via knowledge taxonomy (e.g., "Pointer" dog breed); unlearners collect exemplar images from public hierarchical datasets to form Dhf, translating semantic requests into optimization objectives.
- Core assumption: External datasets contain sufficiently representative samples of the target knowledge to enable effective unlearning.
- Evidence anchors:
  - [Section 3] "an unlearning request for FMs consists of a forget set Df ⊂ {data, knowledge} and nothing else"
  - [Section 4.1.1] "we compile two fine-grained visual recognition datasets, CompCars-S and ImgnetDogs"
  - [corpus] Neighbor paper "Position: Foundation Models Need Digital Twin Representations" suggests FMs lack explicit domain knowledge structures, supporting the need for external knowledge interfaces.
- Break condition: When proxy samples poorly represent the knowledge as encoded in the FM's training distribution, leading to incomplete or misdirected unlearning.

## Foundational Learning

### Gradient Ascent Unlearning
- Why needed here: The baseline approach that this paper improves upon; understanding its over-forgetting tendency motivates the bounded loss design.
- Quick check question: Why does maximizing loss on the forget set often destroy model utility on unrelated tasks?

### KL Divergence as Distribution Regularizer
- Why needed here: Core technique for preserving model behavior on non-target knowledge while modifying parameters.
- Quick check question: What does minimizing KL(pθ0 || pθ) on the retention distribution achieve versus maximizing likelihood?

### Knowledge Hierarchies/Taxonomies
- Why needed here: The structural assumption enabling "forget Pointer, retain Dog" requires understanding coarse-to-fine relationships.
- Quick check question: How does WordNet's IS-A hierarchy enable the construction of Dr^Parent from Df?

## Architecture Onboarding

- Component map:
  - Request Interface -> Proxy Dataset Builder -> Enhanced Forgetting Set -> Unlearning Optimizer -> Evaluation Suite

- Critical path:
  1. Identify target concepts in knowledge hierarchy
  2. Collect proxy images with fine + coarse labels (N ≥ 30 samples per class recommended)
  3. Optimize: L_NHL + α_c·KL_coarse + α_f·KL_fine_non_target
  4. Early-stop when forgetting accuracy stops decreasing on training set
  5. Evaluate on held-out fine-grained test set + zero-shot datasets

- Design tradeoffs:
  - Margin m: Higher margin → more aggressive forgetting but utility risk (paper uses m=2)
  - KL weights (αc, αf): Stronger regularization preserves utility but may reduce forgetting quality (αc=10, αf=20)
  - Sample count: More proxy images improve forgetting quality with diminishing returns beyond ~30 samples

- Failure signatures:
  - Catastrophic collapse: Fine-grained retention accuracy drops below 20% → GA without bounds, reduce learning rate
  - Incomplete forgetting: Quality < 80% → increase margin or sample count
  - OOD generalization failure: Performance gap on external datasets → all data-tracing methods show this limitation (Table 4), indicating need for natively knowledge-tracing methods

- First 3 experiments:
  1. Replicate the ImgnetDogs "difficult" setting with 10 dog breeds, comparing NHL+KL vs NPO+KL vs GA+KL to verify the 95.20% Q-U claim.
  2. Ablate each KL regularizer separately to quantify contribution of coarse vs. fine-grained preservation (expect αf contributes more to utility).
  3. Test OOD transfer by unlearning on ImgnetDogs and evaluating on OxfordPet to confirm the reported robustness gap (Table 4: NHL drops to 42.58 Q-U).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we formally define and quantify the boundaries between knowledge to be unlearned and knowledge to be retained?
- Basis in paper: [explicit] The authors state: "defining and quantifying the boundaries of knowledge remains a fundamental challenge in knowledge-tracing unlearning. What constitutes retention knowledge given a request to unlearn certain knowledge? How to best draw the fine line between them?"
- Why unresolved: Current methods rely on taxonomies or knowledge graphs, but real-world ontologies are exponentially more complex, and no principled framework exists for delineating knowledge boundaries.
- What evidence would resolve it: A formal framework with quantitative metrics that can precisely measure the semantic distance between forgotten and retained knowledge, validated across diverse domains.

### Open Question 2
- Question: How can knowledge-tracing unlearning methods achieve robust out-of-distribution generalization?
- Basis in paper: [inferred] Table 4 shows all methods, including the proposed NHL+KL, perform poorly on OOD tests (e.g., trained on ImgnetDogs, evaluated on OxfordPet). The proposed method's Q-U score drops from 95.20% (in-domain) to 42.58% (OOD).
- Why unresolved: Current methods overfit to the unlearning dataset and fail to generalize forgetting to semantically related but distributionally different concepts.
- What evidence would resolve it: Methods achieving >80% Q-U scores consistently across multiple OOD datasets with different visual characteristics from the training unlearning set.

### Open Question 3
- Question: What evaluation benchmarks and metrics are appropriate for knowledge-level unlearning?
- Basis in paper: [explicit] The authors note: "Currently, there is no widely accepted standard for evaluating knowledge-level unlearning" and call for "future work that advances the evaluation criteria."
- Why unresolved: Data-tracing evaluation relies on oracle models retrained on retention sets, which is inapplicable when training data is inaccessible. The Q-U trade-off metric proposed is preliminary.
- What evidence would resolve it: A comprehensive benchmark suite with standardized metrics that correlate with human judgments of successful knowledge removal while preserving model utility.

### Open Question 4
- Question: What is the minimum sample efficiency required for effective knowledge-tracing unlearning, and can it be achieved with zero-shot or few-shot approaches?
- Basis in paper: [inferred] Table 14 shows unlearning quality improves from 70.02% (10 samples) to 97.86% (200 samples), but the paper does not establish theoretical lower bounds or investigate whether unlearning is possible without exemplar data.
- Why unresolved: Stakeholders like regulators may lack access to even proxy datasets, yet the current paradigm requires assembling exemplar images for target concepts.
- What evidence would resolve it: Methods achieving >90% Q-U scores with ≤5 samples per concept, or successful unlearning using only textual descriptions without visual exemplars.

## Limitations

- Generalizability Constraint: The method's effectiveness on CLIP's zero-shot transfer suggests knowledge-level unlearning works for contrastive models, but no evidence exists for autoregressive or diffusion FMs.
- Proxy Dataset Quality Dependence: The approach assumes external datasets accurately represent target knowledge distributions, creating a fundamental gap when stakeholders lack training data access.
- Evaluation Scope Limitation: Results focus on fine-grained classification unlearning; the paper doesn't demonstrate effectiveness for abstract concepts, multimodal knowledge, or capability removal.

## Confidence

**High Confidence**: The bounded hinge loss mechanism improves over unbounded gradient ascent for classification unlearning. The 95.20% Q-U score and zero-shot preservation are reproducible with the specified implementation.

**Medium Confidence**: Knowledge-level unlearning via external proxy datasets is feasible for CLIP-style models. The KL regularization strategy effectively balances forgetting with retention for the tested classification scenarios.

**Low Confidence**: The approach generalizes to non-contrastive FMs, abstract knowledge removal, or capabilities beyond fine-grained recognition. The method's effectiveness without access to original training data remains unproven for complex knowledge structures.

## Next Checks

1. **Architecture Transfer Test**: Apply the bounded hinge loss mechanism to a non-contrastive FM (e.g., LLaMA-2 or Stable Diffusion) using a similarity-based proxy metric. Measure whether the bounded approach prevents catastrophic collapse compared to unbounded gradient ascent on a simple concept removal task.

2. **Proxy Dataset Fidelity Analysis**: Conduct a controlled experiment where both the original training distribution and proxy dataset are available. Compare the effectiveness of unlearning when using proxy data versus using held-out samples from the actual training distribution to quantify the fidelity gap.

3. **OOD Robustness Evaluation**: Systematically vary the relationship between unlearning proxy dataset and evaluation datasets. Test scenarios where proxy and evaluation datasets share coarse categories but differ in fine-grained content to measure the robustness of the unlearning effect across domain shifts.