---
ver: rpa2
title: Evolving Deep Learning Optimizers
arxiv_id: '2512.11853'
source_url: https://arxiv.org/abs/2512.11853
tags:
- optimizers
- optimizer
- learning
- evolved
- adam
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a genetic algorithm framework for automatically
  discovering deep learning optimizers by evolving genomes that encode combinations
  of primitive update terms (gradient, momentum, RMS normalization, Adam-style adaptive
  terms, and sign-based updates) along with hyperparameters and scheduling options.
  The evolved optimizer, discovered through 50 generations of evolutionary search
  across multiple vision tasks, outperforms Adam by 2.6% in aggregate fitness and
  achieves a 7.7% relative improvement on CIFAR-10.
---

# Evolving Deep Learning Optimizers

## Quick Facts
- arXiv ID: 2512.11853
- Source URL: https://arxiv.org/abs/2512.11853
- Reference count: 3
- Primary result: Evolved optimizer outperforms Adam by 2.6% in aggregate fitness across 3 vision tasks

## Executive Summary
This paper presents a genetic algorithm framework for automatically discovering deep learning optimizers by evolving genomes that encode combinations of primitive update terms (gradient, momentum, RMS normalization, Adam-style adaptive terms, and sign-based updates) along with hyperparameters and scheduling options. The evolved optimizer, discovered through 50 generations of evolutionary search across multiple vision tasks, outperforms Adam by 2.6% in aggregate fitness and achieves a 7.7% relative improvement on CIFAR-10. The evolved algorithm combines sign-based gradient terms with adaptive moment estimation, uses lower momentum coefficients than Adam (β1 = 0.86, β2 = 0.94), and notably disables bias correction while enabling learning rate warmup and cosine decay.

## Method Summary
The approach uses a genetic algorithm to evolve optimizer configurations encoded as genomes containing primitive update terms, hyperparameters, flags, and scheduling options. The search space includes 7 primitive operations (Grad, Momentum, RmsNorm, AdamTerm, SignGrad, UnitGrad, Nesterov), learning rate and other hyperparameters, and flags for bias correction, clipping, and warmup/decay. The fitness function evaluates across multiple vision tasks (MNIST, Fashion-MNIST, CIFAR-10) using small CNNs with 500-step training runs. The GA uses tournament selection (k=4), elitism (top 3), single-point/uniform crossover, and adaptive mutation over 50 generations.

## Key Results
- Evolved optimizer achieves 7.7% relative improvement on CIFAR-10 (76.3% vs Adam's 70.9%)
- Aggregate fitness improvement of 2.6% across 3 tasks (Fashion-MNIST, CIFAR-10, MNIST)
- Discovered algorithm combines sign-based gradient terms with adaptive moment estimation
- Lower momentum coefficients (β1 = 0.86, β2 = 0.94) outperform Adam's defaults
- Learning rate warmup substitutes for disabled bias correction

## Why This Works (Mechanism)

### Mechanism 1
Combining sign-based gradient terms with Adam-style adaptive terms improves optimization by providing both magnitude-invariant direction and per-parameter scaling. The sign(g) component provides direction independent of gradient magnitude (similar to Lion), while m/(√v+ε) provides Adam-style adaptive scaling. The evolved weights (0.73 for sign, 3.63 for adaptive) suggest adaptive terms dominate but sign provides stabilizing directional signal.

### Mechanism 2
Lower momentum coefficients (β1=0.86, β2=0.94 vs Adam's 0.9, 0.999) enable faster adaptation to changing gradient statistics at the cost of noisier moment estimates. Lower β values reduce the effective memory window of exponential moving averages, making the optimizer more responsive to recent gradients. This allows quicker adjustment to landscape changes but introduces more variance.

### Mechanism 3
Learning rate warmup can substitute for bias correction when moment estimates are zero-initialized. Adam's bias correction scales up early moment estimates to compensate for zero-initialization. Warmup achieves similar effect by using smaller learning rates early, reducing the impact of biased moment estimates before they stabilize.

## Foundational Learning

- **Exponential Moving Averages (EMAs) for moment estimation**: The optimizer uses m_t (first moment) and v_t (second moment) as EMAs of gradients. Understanding how β1, β2 control the effective memory window is essential to interpret why lower values change optimizer behavior. Quick check: If β1 = 0.9, approximately how many steps of past gradients contribute significantly to the current m_t estimate? (Answer: ~1/(1-β1) ≈ 10 steps effective window)

- **Bias correction in adaptive optimizers**: The evolved optimizer explicitly disables bias correction. Understanding why Adam includes it (zero-initialization causes underestimation early in training) clarifies what warmup must compensate for. Quick check: Why does zero-initialization of moment estimates create bias in early training steps? (Answer: EMA starts at zero, so early estimates are systematically smaller than true expected values)

- **Learning rate scheduling (warmup + cosine decay)**: The evolved optimizer uses both warmup (100 steps linear ramp) and cosine decay. Understanding scheduling helps interpret why this combination works with the modified momentum settings. Quick check: What problem does warmup solve in transformer/adaptive optimizer training? (Answer: Prevents early training instability from large updates when moment estimates are unreliable)

## Architecture Onboarding

- **Component map**: Genome encoder -> Primitive term catalog (7 ops) -> Fitness evaluator (multi-task protocol) -> Genetic operators (tournament selection, crossover, mutation) -> Training harness (500-step evaluations)

- **Critical path**: Initialize population -> Evaluate fitness across 3 tasks × 2 seeds -> Apply selection → crossover → mutation -> Repeat for 50 generations -> Re-evaluate top candidates with longer training (1000 steps, 3 seeds)

- **Design tradeoffs**: Short evaluation (500 steps) enables search tractability but may favor fast-converging optimizers; limited task diversity may not generalize; fixed primitive catalog constrains search but ensures interpretability; multi-task fitness encourages generalization but may miss task-specific optimizers

- **Failure signatures**: NaN/Inf loss (marked as divergence, fitness = -1); loss > 50 at any point (triggers divergence penalty); Numerical RuntimeError (caught and penalized as divergence); high variance across seeds (may indicate optimizer instability)

- **First 3 experiments**: 1) Reproduce baseline comparison: Run Adam vs evolved optimizer on CIFAR-10 for 1000 steps; 2) Ablate sign-based component: Remove SignGrad term and measure fitness drop; 3) Test longer training regime: Train for 10,000 steps to assess momentum coefficient impact

## Open Questions the Paper Calls Out
1. Does the evolved optimizer maintain its performance advantage when scaled to large models and longer training horizons? The authors explicitly state "Validation on larger models (ResNets, Transformers) and longer training is needed."
2. Are the evolved lower momentum coefficients (β1=0.86, β2=0.94) overfitted to short training runs? The Discussion notes these settings "may be beneficial for the relatively short training runs... though it warrants investigation on longer training."
3. Does the discovered algorithm generalize to non-vision domains such as language modeling or reinforcement learning? The Limitations section notes "Only vision classification tasks were used. Language modeling and reinforcement learning would test generalization."

## Limitations
- Evolved optimizer's superiority demonstrated only on small-scale vision tasks with limited training budgets (1000 steps)
- Fitness landscape is narrow, gains are uneven across tasks
- Lower momentum coefficients may be beneficial only for the relatively short training runs used during evolution
- Generalization to larger models, longer training, or non-vision tasks remains unproven

## Confidence
- **High**: Genetic algorithm framework successfully discovers optimizers with explicit, interpretable update rules; fitness evaluation protocol is clearly specified
- **Medium**: Evolved optimizer's design choices are well-documented and mechanisms are plausible based on empirical results
- **Low**: Generalization to larger models, longer training, or non-vision tasks remains unproven; redundancy between warmup and bias correction is hypothesized but lacks strong corpus support

## Next Checks
1. Test longer training regimes: Evaluate the evolved optimizer on CIFAR-10 for 10,000 steps to assess whether lower momentum coefficients remain beneficial or cause degradation
2. Ablate design components: Systematically remove the sign-based component and bias correction, then measure fitness impact to validate their contributions
3. Cross-task generalization: Apply the evolved optimizer to transformer-based language modeling or RL tasks to test if discovered principles transfer beyond vision classification