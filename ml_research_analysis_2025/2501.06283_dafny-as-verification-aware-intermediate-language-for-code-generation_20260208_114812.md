---
ver: rpa2
title: Dafny as Verification-Aware Intermediate Language for Code Generation
arxiv_id: '2501.06283'
source_url: https://arxiv.org/abs/2501.06283
tags:
- dafny
- fibfib
- code
- python
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes using Dafny as an intermediate language to
  improve the quality of code generated by large language models. Instead of emitting
  target code directly, the user guides the LLM to first generate Dafny code that
  can be automatically validated for correctness against specifications.
---

# Dafny as Verification-Aware Intermediate Language for Code Generation

## Quick Facts
- arXiv ID: 2501.06283
- Source URL: https://arxiv.org/abs/2501.06283
- Authors: Yue Chen Li; Stefan Zetzsche; Siva Somayyajula
- Reference count: 18
- One-line result: Prototype achieved 77% pass rate on HumanEval vs 86% native Python generation

## Executive Summary
This paper proposes using Dafny as an intermediate language to improve the quality of code generated by large language models. Instead of emitting target code directly, the user guides the LLM to first generate Dafny code that can be automatically validated for correctness against specifications. The verified Dafny program is then compiled to the target language and returned to the user, with all interactions occurring via natural language. The authors present a chatbot prototype that implements this approach and evaluate it on the HumanEval Python code generation benchmarks.

## Method Summary
The method involves guiding a large language model to first generate Dafny specifications from natural language prompts, which users confirm. The LLM then synthesizes Dafny implementations, which are refined through iterative feedback from the Dafny verifier until verification succeeds. The verified Dafny code is compiled to Python with runtime tests generated via Dafny's experimental `generate-tests` command. The system uses Claude Sonnet 3.5 with few-shot prompting and a consistency check to validate NL-to-spec translations.

## Key Results
- Prototype achieved 77% pass rate on HumanEval benchmark (164 tasks)
- Native Python generation with Claude Sonnet 3.5 achieved 86% pass rate
- Fallback to native generation when verification doesn't converge yielded 88% pass rate
- Approximately 12% of tasks (20/164) failed to converge on verified Dafny code

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating specification from implementation via a verification-aware language reduces semantic drift between user intent and generated code.
- Mechanism: The LLM first formalizes natural language into a Dafny specification (e.g., `ensures result == fibfibFunc(n)`), which the user confirms via NL conversation. Implementation synthesis then occurs independently, with the verifier enforcing conformance to the agreed spec.
- Core assumption: Users can accurately validate NL translations of formal specs without seeing the underlying code.
- Evidence anchors: [abstract] "validated for correctness against agreed on specifications"; [section 1] "allows for a clear distinction between the specification of a program... and its implementation"; [corpus] Clover (Sun et al. 2024) introduces consistency between implementation and specs.

### Mechanism 2
- Claim: Automated verification feedback loops enable self-correction without user intervention.
- Mechanism: The Dafny verifier returns errors/warnings to the LLM, which refines implementation and proof hints (invariants, assertions) iteratively until verification succeeds.
- Core assumption: The LLM can interpret verifier feedback and generate syntactically correct fixes within few iterations.
- Evidence anchors: [section 2.4] "The chatbot iteratively converges (automatically, i.e., without user intervention) on a verified Dafny implementation by feeding verifier feedback"; [section 3] "dramatic increase in syntactic correctness" after moving to Claude 3.5 Sonnet.

### Mechanism 3
- Claim: Backend compilation preserves verification guarantees across target languages.
- Mechanism: Once verified, Dafny code compiles to Python (or Java, C#, etc.) with correctness "implicit" through the prior proof. Generated unit tests provide runtime sanity checks.
- Core assumption: The Dafny compiler preserves semantic equivalence between verified Dafny and target output.
- Evidence anchors: [section 2.1] "correctness of the solution is implicit as it was present in Dafny and has been preserved with high assurance by the translation"; [section 2.4] "Dafny's experimental generate-tests command to generate tests... that check the validity of an ensures clause at runtime."

## Foundational Learning

- Concept: **Hoare logic (preconditions/postconditions)**
  - Why needed here: Dafny's `requires` and `ensures` clauses encode specifications; understanding these is essential to diagnose verification failures.
  - Quick check question: Given `requires n >= 0` and `ensures result >= 0`, what must be proven about the method body?

- Concept: **Loop invariants**
  - Why needed here: The LLM must generate invariants for imperative loops to pass verification (e.g., in the fibfib iterative implementation).
  - Quick check question: Why does a loop need an invariant if it already has a postcondition?

- Concept: **Specification consistency**
  - Why needed here: The consistency check verifies that NL descriptions can reconstruct equivalent Dafny programs, catching translation errors early.
  - Quick check question: If an NL description says "returns the first positive element" but Dafny spec says `ensures result > 0`, is this consistent?

## Architecture Onboarding

- Component map: NL input → spec formalization → user confirmation → implementation synthesis → verification loop → test generation → compilation → Python output
- Critical path: NL input → spec formalization → user confirmation → implementation synthesis → verification loop → test generation → compilation → Python output. The verification loop is the bottleneck; ~12% of tasks failed to converge.
- Design tradeoffs:
  - **Correctness vs. coverage**: 77% verified pass rate vs. 86% native; fallback yields 88% but includes unverified code.
  - **Opacity vs. debuggability**: Hiding Dafny improves UX but limits user ability to fix spec errors directly.
  - **Type safety vs. interoperability**: Dafny's strict typing conflicts with Python's dynamic types (e.g., mixed-type lists require custom datatypes).
- Failure signatures:
  - **Non-convergence**: LLM fails syntactic validity or verification after multiple attempts
  - **Spec mismatch**: Ambiguous prompts lead to correct-but-wrong implementations
  - **Interop failure**: Generated Dafny types don't match native Python test expectations (primary benchmark hurdle)
- First 3 experiments:
  1. **Reproduce the HumanEval baseline**: Run the prototype on 10-20 HumanEval tasks, logging convergence iterations and failure modes.
  2. **Stress-test interoperability**: Attempt tasks requiring heterogeneous Python types (e.g., lists with mixed int/str/None) and characterize translation gaps.
  3. **Ablate the consistency check**: Disable NL-spec consistency validation and measure impact on spec accuracy and user acceptance rates.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the architecture support Python's dynamic types (e.g., heterogeneous lists) in Dafny without compromising interoperability or scalability?
- Basis in paper: [explicit] Section 4.2 identifies interoperability issues arising from Dafny's static typing as "one of the main hurdles," noting that current wrapper solutions "won't scale."
- Why unresolved: The paper offers no solution for mapping dynamic Python types to Dafny's type system other than manual wrappers, which failed during benchmarking against native test suites.
- What evidence would resolve it: An automated type mapping strategy or a library that allows verified Dafny code to interact seamlessly with native Python types without manual intervention.

### Open Question 2
- Question: How can semantic equivalence be formally verified when using an LLM to refactor non-idiomatic compiled code into readable Python?
- Basis in paper: [explicit] Section 4.1 states that while post-processing compiler output with an LLM improves readability, the authors "did not verify for semantic equivalence" to maintain correctness guarantees.
- Why unresolved: Current Dafny compilation produces verbose code; making it idiomatic via LLM breaks the formal "chain of custody" regarding correctness.
- What evidence would resolve it: A tool or verified transformation step that proves the refactored/idiomatic Python code is functionally identical to the raw Dafny compiler output.

### Open Question 3
- Question: To what degree would fine-tuning LLMs on synthesized Dafny datasets improve benchmark performance given the current scarcity of training data?
- Basis in paper: [explicit] Section 3 notes that current results are promising despite the lack of training data (409 Dafny repos vs 15M Python repos) and explicitly lists "synthesizing Dafny code for training and fine-tuning" as an opportunity for improvement.
- Why unresolved: The prototype relies on few-shot prompting with a general-purpose model (Claude 3.5), leaving the potential benefits of a specialized model unexplored.
- What evidence would resolve it: A comparative study of pass rates on HumanEval between the current few-shot approach and a model fine-tuned on a corpus of synthesized Dafny specifications and implementations.

## Limitations

- The system prompt and few-shot examples critical to reproducibility are incomplete in the paper, making faithful replication difficult
- The 12% non-convergence rate raises questions about scalability to more complex tasks
- Interoperability issues between Dafny's strict typing and Python's dynamic typing remain largely unresolved

## Confidence

- **High confidence**: The verification feedback loop mechanism and its role in improving syntactic correctness (supported by dramatic improvement after moving to Claude 3.5 Sonnet)
- **Medium confidence**: The 77% pass rate on HumanEval (valid benchmark used, but prototype implementation details missing)
- **Low confidence**: The claim that correctness is "implicit" through Dafny compilation (lacks empirical validation of Dafny compiler preservation of semantic guarantees)

## Next Checks

1. **Benchmark replication**: Execute the complete prototype on a 20-task subset of HumanEval, measuring convergence iterations, failure modes, and pass rates against the claimed 77%
2. **Interoperability stress test**: Systematically evaluate tasks requiring Python's heterogeneous data structures to quantify the extent of Dafny-Python translation gaps
3. **Ablation study**: Compare prototype performance with and without the consistency check to isolate its contribution to spec accuracy and user acceptance