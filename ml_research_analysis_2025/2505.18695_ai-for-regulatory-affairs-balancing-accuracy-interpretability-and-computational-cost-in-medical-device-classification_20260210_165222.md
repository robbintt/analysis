---
ver: rpa2
title: 'AI for Regulatory Affairs: Balancing Accuracy, Interpretability, and Computational
  Cost in Medical Device Classification'
arxiv_id: '2505.18695'
source_url: https://arxiv.org/abs/2505.18695
tags:
- regulatory
- class
- interpretability
- accuracy
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically evaluates multiple AI models for medical
  device classification in regulatory contexts, focusing on three key criteria: accuracy,
  interpretability, and computational cost. Traditional models like XGBoost and Random
  Forest deliver strong accuracy and F1 scores (86.0% and 84.0% respectively) with
  low inference latency (0.0514s and 0.2272s), making them well-suited for practical
  deployment.'
---

# AI for Regulatory Affairs: Balancing Accuracy, Interpretability, and Computational Cost in Medical Device Classification

## Quick Facts
- **arXiv ID:** 2505.18695
- **Source URL:** https://arxiv.org/abs/2505.18695
- **Reference count:** 40
- **Primary result:** Traditional models like XGBoost achieve 86% accuracy with low latency; CNNs reach 88.28% accuracy; LLMs score only 45.93% accuracy but offer natural language explanations.

## Executive Summary
This study evaluates multiple AI models for classifying medical devices into regulatory risk categories (NMPA Classes I, II, III), focusing on three critical dimensions: accuracy, interpretability, and computational cost. Traditional machine learning models like XGBoost and Random Forest deliver strong performance with high interpretability and low latency, making them practical for regulatory screening. Deep learning models, particularly CNNs, achieve the highest accuracy by identifying localized risk phrases in device descriptions. Large language models provide the most natural language-based explanations but suffer from poor classification performance and high inference costs. The findings reveal no single model excels across all dimensions, suggesting context-dependent hybrid strategies are needed for regulatory applications.

## Method Summary
The study uses the NMPA UDI database (42,000 labeled Chinese medical device descriptions) to train and evaluate multiple classification approaches. Traditional models use 768-dimensional BERT embeddings as features, while deep learning models process token sequences directly. The evaluation employs 10-fold stratified cross-validation, measuring accuracy, Macro F1 score, inference latency, and interpretability through techniques like TreeSHAP and LIME. Models tested include XGBoost, Random Forest, Logistic Regression, SVMs, CNNs, RNNs, and zero-shot LLMs (DeepSeek, LLaMA). The paper systematically compares performance across accuracy, interpretability, and computational cost metrics.

## Key Results
- Traditional ML models (XGBoost, Random Forest) achieve 86.0% and 84.0% accuracy with inference latency under 0.3 seconds
- CNNs outperform sequential models with 88.28% accuracy by identifying localized risk phrases
- LLMs score only 45.93% accuracy with 3.99s inference time but provide natural language explanations
- No single model excels across all three evaluation dimensions (accuracy, interpretability, computational cost)
- Class I devices remain challenging across all models, with F1 scores typically below 0.35

## Why This Works (Mechanism)

### Mechanism 1: Hybrid Traditional + BERT for Interpretability
BERT converts device descriptions into 768-dimensional semantic vectors, which XGBoost then partitions using non-linear decision trees. TreeSHAP can efficiently calculate exact feature contributions, mapping abstract dimensions back to regulatory tokens. This works because the semantic information for risk classification is captured effectively in the [CLS] token embedding, and tree-based decision boundaries align with regulatory logic.

### Mechanism 2: CNN Pattern Recognition
1D-convolutional filters slide across token sequences to detect n-gram patterns (e.g., "invasive", "life-supporting"). Regulatory classification often hinges on the presence of specific high-risk phrases rather than sequential order, making CNNs efficient at identifying local patterns while RNNs better suit long-range dependencies.

### Mechanism 3: LLM Plausible Reasoning Gap
LLMs generate predictions based on probabilistic next-token generation, simulating regulatory expert tone (high interpretability) but lacking hard constraints of rule-based systems or statistical rigor of trained classifiers. This creates a disconnect between linguistic fluency and legal logic, resulting in "hallucinated" reasoning that sounds authoritative but may be legally incorrect.

## Foundational Learning

- **Concept: TreeSHAP vs. LIME (Explainability)**
  - Why needed here: Regulatory affairs demands auditability, requiring understanding of faithful explanation (TreeSHAP for trees) versus approximate explanation (LIME for black boxes)
  - Quick check question: Why does the paper claim TreeSHAP is more "stable" than LIME for XGBoost models? (Answer: TreeSHAP uses the exact tree structure to calculate Shapley values, whereas LIME relies on perturbation sampling which introduces randomness)

- **Concept: The Accuracy-Interpretability Trade-off**
  - Why needed here: Engineers must optimize for Macro F1 (performance across all classes) rather than just accuracy, specifically because regulatory data is imbalanced
  - Quick check question: If a model has 88% accuracy but an F1 score of 0.10 for Class I devices, is it safe to deploy? (Answer: Likely no; high accuracy might be due to class imbalance, but failure to identify low-risk devices creates unnecessary regulatory burden)

- **Concept: Zero-Shot Classification**
  - Why needed here: Understanding baseline capabilities of LLMs without training data helps set expectations for "out-of-the-box" versus "trained" performance
  - Quick check question: Why did zero-shot LLMs perform significantly worse (~46% accuracy) than trained traditional models (~86%) in this study? (Answer: LLMs rely on general pre-training rather than domain-specific statistical learning from the target dataset distribution)

## Architecture Onboarding

- **Component map:** Raw text -> jieba segmentation -> chinese-bert-wwm-ext (Tokenization & Embedding) -> Model Layer (XGBoost/CNN/LLM) -> Explanation Layer (TreeSHAP/Integrated Gradients)
- **Critical path:** Do not start with the LLM. Start by reproducing the XGBoost + BERT [CLS] pipeline to establish a performant baseline (86% accuracy) with tractable explainability.
- **Design tradeoffs:**
  - Using BERT [CLS] tokens allows traditional models to process text but creates an abstraction layer where explanations refer to "embedding dimensions" rather than "words," reducing direct regulatory utility
  - SVMs showed high accuracy but failed latency tests (82s inference), rendering them unusable for real-time regulatory tools
  - Most models struggle with Class I recall; prioritizing Class III recall is medically safer but increases false positives in regulatory workload
- **Failure signatures:**
  - "Confident Hallucination": LLM outputs that sound authoritative and cite "regulations" but assign wrong class
  - "Semantic Gap": XGBoost/SHAP explanations pointing to abstract BERT dimensions meaningless to regulatory human reviewers
- **First 3 experiments:**
  1. Implement chinese-bert-wwm-ext + XGBoost on NMPA subset; verify if Macro F1 aligns with paper claims (~0.69)
  2. Apply TreeSHAP to XGBoost model; verify if top contributing features can be mapped back to human-readable words
  3. Isolate ambiguous device descriptions and compare XGBoost predictions vs. LLM "reasoning" to quantify gap between statistical classification and semantic reasoning

## Open Questions the Paper Calls Out

- **Hybrid Architectures:** Can combining symbolic reasoning with deep learning outperform single models in balancing accuracy and interpretability? The study evaluated models in isolation but did not construct composite systems merging rule-based logic with neural networks.
- **Rule-Enhanced LLM Prompts:** Does embedding structured regulatory rules into prompts effectively align LLM outputs with legal correctness while maintaining linguistic plausibility? Current zero-shot experiments resulted in fluent explanations that often failed legal validity.
- **Domain Feature Engineering:** How does adding domain-specific features (risk keywords, device functional descriptors) impact traditional model accuracy and transparency? The experiments relied on raw text or BERT embeddings without manually curated regulatory attributes.

## Limitations

- The Chinese-language dataset may not generalize to Western regulatory contexts or other languages
- Zero-shot LLM performance may significantly improve with fine-tuning on regulatory corpora, which was not explored
- The semantic gap between BERT embedding dimensions and human-readable regulatory language remains a practical barrier for auditability

## Confidence

**High Confidence:** Claims about XGBoost and Random Forest performance (86.0% and 84.0% accuracy, low latency) are well-supported by experimental data and align with established ML literature.

**Medium Confidence:** CNN superiority (88.28% accuracy) and local pattern recognition mechanism are supported, but architectural choices and optimality could vary with different regulatory text characteristics.

**Low Confidence:** LLM performance claims (45.93% accuracy, 3.99s latency) are based on zero-shot evaluation without fine-tuning; interpretability claims rely on qualitative assessment rather than systematic evaluation.

## Next Checks

1. **Cross-linguistic validation:** Replicate the XGBoost + BERT pipeline on an English-language medical device database (e.g., FDA UDI database) to verify if the 86% accuracy benchmark holds across regulatory jurisdictions and languages.

2. **Fine-tuned LLM evaluation:** Train a domain-adapted LLM (fine-tuned on NMPA regulatory decisions) and compare its performance against the zero-shot baseline to quantify the gap between general language models and regulatory-specific models.

3. **Semantic gap measurement:** Systematically map TreeSHAP explanations from XGBoost back to original Chinese text tokens, measuring the percentage of explanations that regulatory experts can interpret meaningfully versus those remaining in abstract embedding space.