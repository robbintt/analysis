---
ver: rpa2
title: 'Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity,
  and Causality'
arxiv_id: '2505.19376'
source_url: https://arxiv.org/abs/2505.19376
tags:
- causal
- belief
- beliefs
- statement
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how people selectively attribute beliefs
  to others as explanations for behavior, a fundamental aspect of human theory-of-mind.
  The authors develop a computational model that evaluates the explanatory strength
  of natural language belief statements using three factors: accuracy, informativity,
  and causal relevance.'
---

# Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality

## Quick Facts
- arXiv ID: 2505.19376
- Source URL: https://arxiv.org/abs/2505.19376
- Reference count: 6
- Primary result: Causal relevance is the single best predictor of human belief attribution (r = 0.81), outperforming combined accuracy and informativity

## Executive Summary
This paper investigates how people selectively attribute beliefs to others as explanations for behavior, a fundamental aspect of human theory-of-mind. The authors develop a computational model that evaluates the explanatory strength of natural language belief statements using three factors: accuracy, informativity, and causal relevance. The model uses a probabilistic generative framework of belief-driven action, allowing evaluation of how likely a belief statement is true, how informative it is to a listener, and how causally relevant it is to observed actions through hypothetical interventions.

In an experiment where participants watched an agent navigate a gridworld puzzle and then ranked belief statements about the agent's knowledge, the model's predictions were evaluated against human judgments. Results showed that while accuracy and informativity combined provided reasonable predictions (r = 0.68), causal relevance alone was the single best predictor of human belief attribution (r = 0.81). This suggests people naturally prefer beliefs that causally explain actions, especially when not explicitly prompted to communicate with a listener. The findings bridge research on belief attribution and explanation generation, highlighting the central role of causal relevance in how people mentally explain others' behavior.

## Method Summary
The authors developed a computational model that evaluates belief statements using three factors: accuracy (how likely the belief is true given observed actions), informativity (how informative the belief is to a listener), and causal relevance (how causally relevant the belief is to the observed actions through hypothetical interventions). The model employs a probabilistic generative framework where beliefs drive actions in a gridworld environment. In the experiment, participants watched an agent navigate a gridworld puzzle and then ranked belief statements about the agent's knowledge. The model's predictions were compared against human judgments using correlation analysis, with causal relevance alone showing the strongest predictive power (r = 0.81).

## Key Results
- Causal relevance alone predicted human belief attribution judgments best (r = 0.81)
- Combined accuracy and informativity showed reasonable but weaker prediction (r = 0.68)
- Participants naturally preferred causally explanatory beliefs, especially without explicit communication prompts
- The model bridges theory-of-mind and explanation generation research

## Why This Works (Mechanism)
The model works by evaluating belief statements through three complementary lenses that capture different aspects of mental explanation. Accuracy measures the probabilistic likelihood that a belief statement is true given the observed actions, using Bayesian inference to determine how well the belief explains the behavior. Informativity assesses the information gain a listener would receive from the belief statement, capturing the communicative value of sharing that particular belief. Causal relevance evaluates how changes in the agent's beliefs would affect their actions through hypothetical interventions, directly measuring the explanatory power of the belief for the observed behavior. The framework assumes that when people attribute beliefs as explanations, they implicitly perform this type of multi-factor evaluation, with causal relevance being particularly salient for mental explanation generation.

## Foundational Learning
**Bayesian inference** - why needed: To evaluate how likely a belief is true given observed actions, forming the accuracy component of the model. quick check: Can we compute P(belief|actions) using Bayes' rule with a generative model of actions from beliefs?
**Information theory** - why needed: To measure the informativity of belief statements to listeners, quantifying how much uncertainty is reduced. quick check: Does mutual information between belief statements and agent's actual beliefs capture informativity?
**Causal reasoning** - why needed: To evaluate how changes in beliefs would affect actions through hypothetical interventions, forming the causal relevance component. quick check: Can we model P(actions|belief) changes under do-calculus style interventions?
**Probabilistic generative modeling** - why needed: To create a unified framework that can evaluate all three factors (accuracy, informativity, causal relevance) within the same action-generation process. quick check: Does the model generate realistic action sequences from belief states?
**Natural language processing** - why needed: To parse and evaluate natural language belief statements within the computational framework. quick check: Can the model handle the specific belief statements used in the experiment?
**Human behavioral prediction** - why needed: To validate the model by comparing its predictions against human belief attribution judgments. quick check: Do model predictions correlate with human rankings across different belief statements?

## Architecture Onboarding

Component map: Belief statements -> Accuracy evaluation -> Informativity evaluation -> Causal relevance evaluation -> Combined score -> Prediction of human judgment

Critical path: Belief statement parsing -> Probabilistic inference of belief accuracy -> Information-theoretic informativity calculation -> Causal intervention analysis -> Belief ranking

Design tradeoffs: The model trades computational complexity for explanatory richness by using a full probabilistic generative model rather than simpler heuristic approaches. This allows evaluation of all three factors (accuracy, informativity, causal relevance) but requires more computational resources than single-factor models.

Failure signatures: The model may fail when belief statements are ambiguous or when the generative model cannot adequately represent the relationship between beliefs and actions. It may also struggle with beliefs that have indirect or complex causal relationships to observed actions.

3 first experiments:
1. Test the model on a simple deterministic gridworld where beliefs have clear one-to-one mappings to actions
2. Evaluate the individual contribution of each factor (accuracy, informativity, causal relevance) by ablating them separately
3. Validate the causal relevance measure by testing whether beliefs that pass intervention tests are indeed perceived as more explanatory by humans

## Open Questions the Paper Calls Out
The paper highlights several uncertainties regarding the generalizability of findings beyond the specific gridworld puzzle context. The computational model's performance in one experimental paradigm may not extend to more complex real-world belief attribution scenarios or different types of action domains. The causal relevance measure, while showing the strongest predictive power in this study, relies on a specific implementation of hypothetical interventions that may not capture all forms of causal reasoning people use in belief attribution. The interpretation that people naturally prefer causally explanatory beliefs "especially when not explicitly prompted to communicate with a listener" is based on a single experimental manipulation comparing conditions with and without explicit communication goals. This conclusion would benefit from additional experimental variations to confirm the robustness of this effect. There is also uncertainty about whether the model's three-factor framework (accuracy, informativity, causal relevance) captures the complete space of factors that influence human belief attribution. The study focused on these three dimensions, but other psychological or contextual factors may play important roles in different scenarios.

## Limitations
- The model's generalizability beyond gridworld puzzles to complex real-world scenarios remains uncertain
- The causal relevance measure relies on a specific implementation that may not capture all forms of causal reasoning
- The communication goal manipulation effect needs additional experimental validation for robustness
- The three-factor framework may not capture all psychological factors influencing belief attribution in diverse contexts

## Confidence
- Model predictions versus human judgments (r = 0.81 for causal relevance): High
- Causal relevance as the primary factor in belief attribution: Medium
- Generalizability to other contexts and domains: Low

## Next Checks
1. Test the model's predictions across multiple action domains and task types to assess generalizability beyond gridworld puzzles
2. Conduct experiments with varied communication prompts and goals to validate the communication goal manipulation effect
3. Investigate whether additional factors beyond the three modeled (accuracy, informativity, causal relevance) improve prediction of human belief attribution judgments in diverse contexts