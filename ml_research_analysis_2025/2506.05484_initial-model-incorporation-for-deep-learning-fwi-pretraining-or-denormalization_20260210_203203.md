---
ver: rpa2
title: 'Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?'
arxiv_id: '2506.05484'
source_url: https://arxiv.org/abs/2506.05484
tags:
- initial
- velocity
- pretraining
- neural
- pretrain-drfwi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study investigates two approaches\u2014pretraining and denormalization\u2014\
  for incorporating initial velocity models into deep learning-based full waveform\
  \ inversion (FWI). Pretraining first fits the initial model before inversion, while\
  \ denormalization directly adds network outputs to the initial model."
---

# Initial Model Incorporation for Deep Learning FWI: Pretraining or Denormalization?

## Quick Facts
- arXiv ID: 2506.05484
- Source URL: https://arxiv.org/abs/2506.05484
- Authors: Ruihua Chen; Bangyu Wu; Meng Li; Kai Yang
- Reference count: 10
- Primary result: Denormalization significantly outperforms pretraining in deep learning FWI, achieving faster convergence and better high-frequency detail recovery

## Executive Summary
This study compares two approaches for incorporating initial velocity models into deep learning-based full waveform inversion (FWI): pretraining and denormalization. Pretraining first fits the initial model before inversion, while denormalization directly adds network outputs to the initial model. Experimental results on the Marmousi model demonstrate that denormalization significantly outperforms pretraining across multiple metrics including convergence speed, final velocity model accuracy, and recovery of high-frequency subsurface details.

## Method Summary
The paper investigates DRFWI (Deep Learning Full Waveform Inversion) with two initial model incorporation strategies. Pretraining-DRFWI uses a two-stage approach: first fitting the initial model, then performing inversion. Denormalization-DRFWI (Denorm-DRFWI) replaces the global mean matrix with the initial velocity model, allowing the network to focus solely on representing perturbations. Both methods use a coordinate-based MLP with sine activations to parameterize the velocity model, trained via FWI loss minimizing the difference between observed and synthetic seismic data.

## Key Results
- Denorm-DRFWI achieves lower velocity model errors than Pretrain-DRFWI (MSE reduced from 0.1839 to 0.1517 for smooth initial models)
- Denorm-DRFWI converges faster and recovers more high-frequency details in deep structures
- Denorm-DRFWI demonstrates superior robustness to inaccurate initial models (MSE reduced from 0.3945 to 0.2002 for linear initial models)
- Pretraining suffers from negative transfer effects due to mismatched objective functions, causing network parameters to become inactive

## Why This Works (Mechanism)

### Mechanism 1: Spectral Bias Mitigation via Residual Learning
Pretraining forces the network to fit the entire initial velocity model (dominated by low frequencies), saturating capacity and slowing the subsequent learning of high-frequency seismic details. Denormalization turns the problem into a residual learning task where the network only needs to represent missing high-frequency perturbations, restricting the target space and achieving faster high-frequency learning rates.

### Mechanism 2: Prevention of Negative Transfer and Plasticity Loss
Pretraining optimizes network weights to minimize velocity error (L1), while the inversion stage minimizes seismic waveform error (L2). If these objective functions have conflicting gradient directions, the pre-trained weights enter a local optimum that limits adaptability. Denormalization avoids this by using a single consistent objective function from the start, maintaining network plasticity.

### Mechanism 3: Architectural Simplification of the Solution Space
Replacing the global mean matrix with the initial model in the denormalization operator acts as a strong prior, reducing the null space of the inverse problem. This simplifies the optimization landscape by anchoring the inversion to a specific prior, allowing the network to focus on representing high-frequency residuals without searching for the low-frequency background from scratch.

## Foundational Learning

- **Concept: Full Waveform Inversion (FWI)**
  - Why needed: Explains why objective function mismatch in pretraining is critical for FWI performance
  - Quick check: Does the method update velocity model directly, or does it update weights of a network that generates the velocity model?

- **Concept: Neural Reparameterization (DRFWI)**
  - Why needed: Explains why plasticity of weights matters—once weights are stuck, the resulting velocity model is stuck
  - Quick check: How does mapping coordinates to velocity act as a regularizer compared to traditional methods?

- **Concept: Spectral Bias (Frequency Principle)**
  - Why needed: Explains why pretraining on smooth initial model is harmful—it traps network in low-frequency mode
  - Quick check: Why might a neural network struggle to learn high-frequency checkerboard pattern after being trained on smooth gradient?

## Architecture Onboarding

- **Component map:** Input coordinates (and initial model) → MLP with sine activations → Denormalization operator → Physics engine → Seismic data
- **Critical path:** The connection between MLP output and initial model via denormalization step (f_Θ(I) ⊙ S + m_init) is the single point of failure
- **Design tradeoffs:** Static (S-Denorm) vs Adaptive (A-Denorm) allows background model to update but risks instability; Pretrain vs Denorm—Pretrain theoretically safer with unlimited capacity, Denorm practically superior for efficiency
- **Failure signatures:** Stagnation (loss decreases slowly, model identical to initial), Oscillation (seismic loss oscillates wildly), Blurring (deep structures missing)
- **First 3 experiments:**
  1. Sanity Check: Can network represent known perturbation when added to initial model via Denorm?
  2. Ablation: Run inversion on Marmousi smooth model with both methods, plot MSE curves
  3. Robustness Test: Introduce linear initial model, compare S-Denorm vs A-Denorm

## Open Questions the Paper Calls Out
None

## Limitations
- Claims rely on single Marmousi test case with synthetic data; real-world applicability to field data remains untested
- Spectral bias mechanism lacks quantitative validation with frequency band analysis of network outputs
- Robustness testing limited to global smooth/linear initial model trends, not localized high-frequency errors

## Confidence

- **High:** Denormalization outperforms pretraining in convergence speed and final MSE (supported by direct experimental comparison)
- **Medium:** Pretraining causes negative transfer due to objective function mismatch (supported by parameter similarity metrics but lacks ablation study)
- **Low:** Spectral bias explanation for high-frequency recovery differences (plausible but not directly measured)

## Next Checks

1. **Frequency validation:** Extract and compare power spectra of network outputs during training for both methods to empirically verify spectral bias claims
2. **Robustness testing:** Apply both methods to initial model with localized high-frequency errors to test residual learning assumptions
3. **Generalization test:** Evaluate on different velocity model (e.g., overthrust) to verify denormalization advantages persist beyond Marmousi's specific geology