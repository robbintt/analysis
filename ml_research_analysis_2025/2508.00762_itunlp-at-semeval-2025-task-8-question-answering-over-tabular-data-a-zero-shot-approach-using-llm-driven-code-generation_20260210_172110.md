---
ver: rpa2
title: 'ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot
  Approach using LLM-Driven Code Generation'
arxiv_id: '2508.00762'
source_url: https://arxiv.org/abs/2508.00762
tags:
- data
- column
- type
- name
- unique
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of zero-shot question-answering
  over tabular data by proposing a framework that leverages large language models
  (LLMs) for generating and executing Python Pandas code. The method employs optimized
  prompting strategies and an iterative error-handling mechanism to correct faulty
  code through up to two LLM-assisted revisions.
---

# ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation

## Quick Facts
- arXiv ID: 2508.00762
- Source URL: https://arxiv.org/abs/2508.00762
- Reference count: 32
- DeepSeek-R1 achieves 84.09% accuracy on DataBench QA (8th place) and 85.05% on DataBench Lite QA (6th place) among open-source systems

## Executive Summary
This study presents a zero-shot approach for question-answering over tabular data that leverages large language models (LLMs) to generate and execute Python Pandas code. The framework employs optimized schema-guided prompting and an iterative error-handling mechanism to correct faulty code through up to two LLM-assisted revisions. Tested on the DataBench dataset with four open-source LLMs, the approach demonstrates superior accuracy compared to direct text-based QA methods, achieving top rankings among open-source systems in both DataBench QA and DataBench Lite QA subtasks.

## Method Summary
The method involves preprocessing tabular data to normalize column names and construct schemas containing column names, data types, up to 5 unique example values per column (limited to 100 characters), and total unique value counts. For each question, the system builds a prompt combining the question, schema, and instructions, then uses an LLM to generate Pandas code. The generated code is executed in a sandboxed environment, and if errors occur, the system captures error messages and faulty code to send back to the LLM for correction (up to 2 iterations). The final answer is extracted from the last successful code execution.

## Key Results
- DeepSeek-R1 achieves the highest accuracy: 84.09% on DataBench QA and 85.05% on DataBench Lite QA
- Iterative error-handling reduces execution errors by nearly half on average
- The approach ranks 8th in Subtask I and 6th in Subtask II among open-source systems
- Error reduction: DeepSeek-V3 errors drop from 35 to 11 (dev set) and 18 to 9 (test set)

## Why This Works (Mechanism)

### Mechanism 1
Schema-guided prompting improves LLM understanding of tabular structures and code generation accuracy by providing explicit metadata about column semantics and data types. The system constructs structured schemas containing column names, data types, up to 5 unique example values per column (limited to 100 characters), and total unique value counts. This approach assumes LLMs generate more accurate Pandas code when given semantic context rather than raw table samples.

### Mechanism 2
Iterative error correction with LLM feedback substantially reduces execution failures. When generated code fails, the system captures the error message and faulty code, sends them back to the LLM with a correction prompt (up to 2 iterations). This mechanism assumes LLMs can diagnose and fix their own code errors when given explicit error signals and limited context.

### Mechanism 3
Code generation as intermediate reasoning yields higher accuracy than direct text-based QA for tabular data. Instead of prompting the LLM to directly answer questions about tables, the system forces it to produce executable Pandas code, externalizing computation to a deterministic Python runtime. This approach assumes the LLM's code generation capability is more reliable than its direct tabular reasoning, especially for aggregation, filtering, and multi-step operations.

## Foundational Learning

- **Concept**: Pandas DataFrame operations (filtering, aggregation, groupby, merge)
  - Why needed here: The LLM must generate syntactically correct Pandas code; understanding common patterns is essential for debugging and prompt design
  - Quick check question: Can you write Pandas code to filter rows where column 'age' > 30 and return the mean of 'salary'?

- **Concept**: LLM code generation and self-repair
  - Why needed here: The core pipeline relies on LLMs producing executable code and correcting it based on error feedback
  - Quick check question: When an LLM generates code that raises a KeyError, what information should be fed back to enable correction?

- **Concept**: Schema design for structured data prompts
  - Why needed here: Effectiveness depends on what metadata is included in prompts; over- or under-specifying affects token budgets and model comprehension
  - Quick check question: For a high-cardinality text column (e.g., 'user_id' with 10K unique values), what schema information would be most useful to include?

## Architecture Onboarding

- **Component map**: Preprocessor -> Prompt Builder -> Code Generator -> Sandboxed Executor -> Error Handler
- **Critical path**: Load question + dataset -> Preprocess and generate schema -> Build prompt -> LLM generates code -> Execute code -> If success, return answer; if failure, trigger error correction loop -> Return last successful execution result or terminate after 2 failed corrections
- **Design tradeoffs**: 
  - Iteration limit (2) balances latency vs. accuracy based on diminishing returns
  - Schema over full-table context avoids context length limits but may omit critical row-level patterns
  - Model selection balances accuracy (DeepSeek-R1) vs. stability (Qwen/Llama)
- **Failure signatures**:
  - Runtime errors: KeyError, ValueError, FileNotFoundError (most common; often resolved in 1-2 iterations)
  - Degenerate loop: LLM repeats identical output indefinitely (observed only in DeepSeek models)
  - Syntax errors: Invalid Python syntax (observed in DeepSeek models; not in Qwen/Llama)
  - Persistent errors: Some errors transform across iterations without resolution
- **First 3 experiments**:
  1. Baseline accuracy test: Run each of the 4 models on dev set with schema-guided prompts and no error correction
  2. Error correction ablation: Enable 1 vs. 2 iterations of error handling to measure error reduction rate
  3. Schema content ablation: Compare full schema vs. minimal schema on a subset to assess impact on code correctness

## Open Questions the Paper Calls Out

1. How would proprietary LLMs (e.g., GPT-4, Claude) perform within the proposed zero-shot code generation framework compared to the evaluated open-source models?
2. Can this zero-shot code generation approach generalize effectively to multi-table reasoning tasks requiring joins or cross-table operations?
3. What underlying architectural or training factors cause Degenerate Loop errors exclusively in DeepSeek models but not in Llama or Qwen models?
4. What is the optimal number of error-correction iterations, and does this threshold vary across different model families?

## Limitations

- The approach is currently restricted to single-table operations and does not handle multi-table joins or complex nested aggregations
- The error-handling mechanism exhibits failure modes including degenerate loops (only in DeepSeek models) and persistent errors that transform across iterations
- The schema construction uses a fixed format (5 examples per column, 100-character limit) that may not generalize well to high-cardinality or highly variable data types

## Confidence

- **High Confidence**: The observed accuracy improvements from iterative error correction (error reduction by ~50%) and the ranking performance (8th/6th place among open-source systems)
- **Medium Confidence**: The claim that schema-guided prompting improves understanding, supported by comparative accuracy but lacking direct ablation studies
- **Medium Confidence**: The superiority of code generation over direct text-based QA, inferred from task success but not directly validated through controlled comparisons

## Next Checks

1. Conduct an ablation study comparing accuracy with full schema vs. minimal schema (column names + types only) on a held-out validation set to quantify schema impact
2. Test the system on synthetic multi-table datasets requiring join operations to identify architecture limitations
3. Categorize remaining errors after correction iterations to determine whether specific error types (e.g., logical vs. syntax) are more resistant to LLM self-repair