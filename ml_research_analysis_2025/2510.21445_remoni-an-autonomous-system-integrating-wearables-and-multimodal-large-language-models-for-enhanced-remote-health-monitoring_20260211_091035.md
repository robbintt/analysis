---
ver: rpa2
title: 'REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language
  Models for Enhanced Remote Health Monitoring'
arxiv_id: '2510.21445'
source_url: https://arxiv.org/abs/2510.21445
tags:
- system
- data
- detection
- medical
- patient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: REMONI addresses the shortage of medical professionals by integrating
  wearable technology, IoT, and multimodal large language models (MLLMs) for autonomous
  remote health monitoring. The system continuously collects vital signs, accelerometer
  data, and visual information, processing it through an anomaly detection module
  (including fall detection) and an NLP engine.
---

# REMONI: An Autonomous System Integrating Wearables and Multimodal Large Language Models for Enhanced Remote Health Monitoring

## Quick Facts
- arXiv ID: 2510.21445
- Source URL: https://arxiv.org/abs/2510.21445
- Reference count: 25
- Key outcome: Hybrid CNN-LSTM model achieves 98% accuracy and 99% precision for fall detection on wrist-worn accelerometer data downsampled to 32 Hz and ±1 g range.

## Executive Summary
REMONI is an autonomous remote health monitoring system that integrates wearable sensors, IoT devices, and multimodal large language models (MLLMs) to address medical professional shortages. The system continuously collects vital signs, accelerometer data, and visual information from patients, processing this data through an anomaly detection module (including fall detection) and an NLP engine. Built on a hybrid CNN-LSTM architecture for fall detection and GPT4-Vision for activity/emotion recognition, REMONI enables real-time patient monitoring and reduces medical professional workload through intelligent data retrieval and communication via a user-friendly web interface.

## Method Summary
The system uses Samsung Galaxy Watch 3 for accelerometer and vital sign data, Logitech 720p webcam for video, and an ACER Nitro AN515 edge device for local processing. Fall detection employs a hybrid CNN-LSTM model trained on preprocessed FallAllD dataset (wrist accelerometer, downsampled to 32 Hz, range ±1 g). Activity and emotion recognition use GPT4-Vision on single middle frames from HACER dataset. The NLP engine parses natural language queries into structured JSON for data retrieval from edge devices or AWS S3 cloud storage, with responses generated within 20 seconds and emergency alerts delivered in 0.2 seconds.

## Key Results
- Fall detection model achieves 98% accuracy and 99% precision on FallAllD dataset using wrist-worn accelerometer data
- MLLM-based activity and emotion recognition achieves 51% accuracy for activity and 41% for emotion detection
- System responds to queries within 20 seconds, with emergency alerts delivered in 0.2 seconds

## Why This Works (Mechanism)

### Mechanism 1
A hybrid CNN-LSTM model achieves high fall detection accuracy using only wrist-worn accelerometer data downsampled to 32 Hz and ±1 g range. CNN layers extract spatial features from accelerometer windows while LSTM layers capture temporal dependencies in motion sequences. A sigmoid output layer performs binary classification (fall vs. non-fall). The preprocessing reduction from 238 Hz to 32 Hz and ±8 g to ±1 g retains sufficient signal characteristics for distinguishing falls from activities of daily living.

### Mechanism 2
GPT4-Vision can perform activity and emotion recognition from single-frame image inputs, though performance remains modest. The MLLM receives a system prompt describing recognition classes and a single middle frame from video, outputting activity and emotion labels based on visual pose and facial cues. A single frame captures sufficient context for classification, and prompt engineering can guide the model toward relevant visual features.

### Mechanism 3
A three-stage NLP pipeline (intent detection → data preparation → response generation) enables medical professionals to query patient status through natural language. A general LLM parses user queries into structured JSON intent (patient ID, time range, data type). The system retrieves data from edge devices (real-time) or cloud storage (historical), optionally invokes MLLM for image analysis or plotting, and synthesizes a final response. Intent can be reliably mapped to structured keys, and multi-stage decomposition improves accuracy over end-to-end generation.

## Foundational Learning

- **Time-series preprocessing for wearable sensors**: Fall detection relies on appropriately resampled and normalized accelerometer data to preserve discriminative features while reducing computational load. Given a 238 Hz accelerometer signal, what are the risks of downsampling to 32 Hz before identifying high-frequency impact peaks?

- **Hybrid CNN-LSTM architectures**: Fall detection requires both spatial pattern recognition (CNN) and temporal sequence modeling (LSTM) to distinguish falls from daily activities. Why might a CNN-only model fail to distinguish a rapid hand wave from a fall event?

- **Prompt engineering for structured LLM outputs**: The NLP pipeline depends on the LLM outputting valid JSON with specific keys for downstream data retrieval. What failure modes occur if the LLM omits a required key (e.g., `patient_id`) in its JSON response?

## Architecture Onboarding

- **Component map**: Samsung Galaxy Watch 3 → ACER Nitro AN515 edge device → AWS S3 (cloud storage) and AWS EC2 (web application, NLP engine)
- **Critical path**: 1) Wearable transmits accelerometer/vital data via Wi-Fi to edge device; 2) Edge device runs CNN-LSTM fall detection and threshold-based vital sign checks; 3) On anomaly → emergency alert to cloud (0.2s latency); 4) On query → cloud NLP engine parses intent, retrieves data (edge or S3), optionally invokes GPT4-Vision, returns response (<20s)
- **Design tradeoffs**: Emergency alerts optimized for speed (0.2s) vs. complex queries involving MLLM taking up to 20s; fall detection runs locally for fast response while MLLM inference requires cloud resources; GPT4-Vision uses middle frame for speed while Video-ChatGPT uses ~100 frames but performed worse in experiments
- **Failure signatures**: Intent detection returns malformed JSON → downstream data retrieval fails silently; edge device loses Wi-Fi connectivity → real-time queries hang; MLLM over-predicts "happy" or "neutral" emotions → reduced clinical utility for mood monitoring
- **First 3 experiments**: 1) Measure query response time under concurrent user load to validate 20s bound scalability; 2) Evaluate LLM intent detection on a held-out set of clinician queries measuring JSON validity and key correctness; 3) Test CNN-LSTM model on an alternative fall dataset to assess generalization to different sensor placements and populations

## Open Questions the Paper Calls Out

- **MLLM performance improvement**: Can MLLM-based activity and emotion recognition achieve clinically acceptable accuracy (≥80%) when fine-tuned on domain-specific clinical data from actual deployment environments? Current GPT4-Vision performance (51% activity, 41% emotion) is below clinical utility thresholds, but the authors anticipate improvement through future fine-tuning with actual clinic data from Abu Dhabi.

- **Evaluation methodology**: Does the stringent evaluation method for activity/emotion recognition underestimate model performance relative to clinically meaningful distinctions? The current binary correctness criterion treats clinically related misclassifications (e.g., "sitting" vs. "sitting and drinking") as equivalent to unrelated errors, potentially misrepresenting model utility.

- **System scalability**: Can the system maintain sub-second emergency alert latency and sub-20-second query response times when scaled to multi-patient, multi-caregiver hospital deployments? The architecture claims flexibility for hospital-scale deployment, but all experiments involve a single patient scenario with no empirical scaling data.

- **Sensor generalization**: How robust is fall detection when trained on downsampled, range-limited accelerometer data (32 Hz, ±1 g) but deployed on standard consumer wearables with varying sampling rates and sensor characteristics? The model may degrade when sensor characteristics differ from preprocessed training conditions.

## Limitations

- Architecture specification gaps prevent direct reproduction of reported performance metrics, as exact CNN-LSTM layer configurations, hyperparameters, and prompt templates are not provided
- High fall detection accuracy (98%/99%) is demonstrated only on FallAllD dataset with wrist sensors; performance on other populations, sensor placements, or real-world noisy conditions remains unvalidated
- Emotion and activity recognition accuracy (41-51%) is modest, and no clinical assessment validates whether these outputs meaningfully support remote patient monitoring decisions

## Confidence

- **High confidence**: Fall detection CNN-LSTM mechanism and preprocessing pipeline (validated on public dataset with clear metrics)
- **Medium confidence**: NLP pipeline architecture and response latency claims (method described but not empirically validated under load)
- **Low confidence**: MLLM-based activity/emotion recognition utility and clinical impact (modest accuracy, no patient outcome data)

## Next Checks

1. Test the CNN-LSTM model on an alternative public fall dataset (e.g., UR Fall Detection Dataset) with different sensor placements to assess generalization beyond FallAllD
2. Simulate multiple simultaneous clinician queries to measure actual response times and verify the 20-second upper bound holds under realistic load
3. Systematically evaluate GPT4-Vision on a diverse set of video frames with varying lighting, occlusion, and activity ambiguity to quantify performance degradation and over-reliance on "happy/neutral" emotion predictions