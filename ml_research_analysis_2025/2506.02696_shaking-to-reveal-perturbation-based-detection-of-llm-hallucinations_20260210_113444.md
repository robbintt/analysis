---
ver: rpa2
title: 'Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations'
arxiv_id: '2506.02696'
source_url: https://arxiv.org/abs/2506.02696
tags:
- arxiv
- answer
- hallucination
- detection
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a perturbation-based framework for detecting
  hallucinations in LLM-generated text by analyzing the sensitivity of intermediate
  representations to input noise. The key idea is that truthful and hallucinated responses
  exhibit distinct shifts in internal representations when perturbed.
---

# Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations

## Quick Facts
- arXiv ID: 2506.02696
- Source URL: https://arxiv.org/abs/2506.02696
- Reference count: 40
- Key outcome: Perturbation-based framework detects hallucinations by analyzing sensitivity of intermediate representations to input noise, achieving up to 4.78% AUROC improvement over state-of-the-art on TruthfulQA.

## Executive Summary
This paper introduces a novel perturbation-based framework for detecting hallucinations in LLM-generated text by analyzing how intermediate representations respond to input noise. The core insight is that truthful responses exhibit larger representational shifts when perturbed compared to hallucinated ones, which remain relatively stable due to weaker grounding in factual knowledge. The method learns to generate adaptive noise prompts for each sample and employs a lightweight encoder to amplify representation changes, using cosine similarity as a contrastive distance metric. Experiments across four datasets and two LLM architectures demonstrate superior performance compared to existing methods, with the approach showing efficiency and generalizability across domains.

## Method Summary
The SSP framework detects hallucinations by comparing the sensitivity of intermediate representations to input perturbations. For each input, it generates a sample-specific noise prompt using a lightweight MLP, then feeds both the original and perturbed inputs through a frozen LLM to extract intermediate representations. A shared encoder projects these representations to a discriminative space, and cosine distance quantifies the shift between original and perturbed encodings. The method trains a prompt generator and encoder to maximize discrepancy for truthful responses while minimizing it for hallucinated ones, using a contrastive loss with tunable thresholds. This approach operates at intermediate layers rather than output layers, avoiding accumulated bias while capturing factuality information encoded in structured internal representations.

## Key Results
- SSP achieves 75.38% average AUROC across four datasets, outperforming state-of-the-art methods by up to 4.78% on TruthfulQA
- Sample-specific noise prompts improve performance by 4-5% AUROC compared to static prompts
- Middle layers (around layer 10-20 in 32-layer models) provide optimal detection performance
- The method works with only 100 labeled samples and generalizes across datasets
- Cosine similarity as distance metric outperforms Euclidean (63.83%) and Manhattan (57.42%) alternatives

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Truthful and hallucinated responses exhibit measurably different sensitivity to input perturbations at intermediate representation layers.
- Mechanism: Factual knowledge is encoded in structured internal representations that are tightly coupled with input; when perturbed, truthful responses show larger representational shifts than hallucinated ones, which remain relatively stable due to weaker grounding.
- Core assumption: The model's intermediate layers encode factuality information more faithfully than output layers, which accumulate bias during forward propagation.
- Evidence anchors:
  - [abstract]: "truthful and hallucinated responses exhibit distinct shifts in internal representations when perturbed"
  - [section 3.2]: "for a correct answer ATruth, the model typically exhibits high confidence... Introducing a noise prompt N may disrupt the input context... leading to a significant drop in the predicted probability"
  - [corpus]: Related work on internal-layer detection exists (HIDE, Hallucination Detection with Internal Layers), but direct evidence for perturbation sensitivity differential is limited in corpus
- Break condition: If representations at intermediate layers do not differentially encode factuality (e.g., in poorly calibrated models), the signal degrades.

### Mechanism 2
- Claim: Sample-specific noise prompts are more effective than static or globally-learned prompts at eliciting discriminative representational shifts.
- Mechanism: A lightweight MLP generator (Mφ) produces adaptive perturbations conditioned on the input embedding, optimizing the noise prompt to maximize separability between truthful and hallucinated responses for each specific sample.
- Core assumption: Optimal perturbations vary across samples; a one-size-fits-all approach fails to expose the relevant sensitivity differences.
- Evidence anchors:
  - [abstract]: "SSP dynamically generates noise prompts for each input"
  - [section 4.2]: "relying solely on LLM-generated noise prompts N may not produce the optimal perturbations... we introduce a Sample-specific prompt learning strategy"
  - [corpus]: No direct corpus evidence comparing sample-specific vs. static prompts for hallucination detection
- Break condition: If the generator overfits to training distribution or produces semantically contradictory noise (violating preservation constraint), discriminability collapses.

### Mechanism 3
- Claim: Cosine similarity between original and perturbed representations, after encoder projection, provides a robust distance metric for hallucination scoring.
- Mechanism: A shared encoder (fϕ) maps intermediate representations to a discriminative latent space; cosine distance (1 - similarity) quantifies shift magnitude, with truthful responses yielding higher discrepancy scores.
- Core assumption: Cosine-based metrics better capture directional shifts in representation space than magnitude-based alternatives (Euclidean, Manhattan).
- Evidence anchors:
  - [section 4.1]: "we adopt cosine similarity as the measure... cosine-based metrics are robust to variations in feature magnitudes across different layers"
  - [Table 3]: Cosine similarity achieves 75.38% average AUROC vs. 63.83% for Euclidean, 57.42% for Manhattan
  - [corpus]: Weak direct evidence; related work uses various distance metrics without systematic comparison
- Break condition: If encoder training fails to amplify discriminative features (e.g., collapsed representations), the metric becomes uninformative.

## Foundational Learning

- Concept: **Self-assessment in LLMs**
  - Why needed here: SSP is positioned as improving upon traditional self-assessment (output confidence) by operating at intermediate representations; understanding the baseline failure mode is essential.
  - Quick check question: Why does output-layer confidence fail for hallucination detection in some scenarios?

- Concept: **Intermediate representations in Transformers**
  - Why needed here: The entire method hinges on extracting and comparing hidden states from middle layers; practitioners must know which layers to probe and why.
  - Quick check question: At which layer depth does SSP performance peak, and why might deeper layers degrade detection?

- Concept: **Contrastive learning objectives**
  - Why needed here: The training loss (τT, τH thresholds) explicitly encourages different behavior for truthful vs. hallucinated samples; understanding margin-based objectives clarifies why the method generalizes.
  - Quick check question: What happens if you reverse τT and τH (treating hallucinated samples as needing larger shifts)?

## Architecture Onboarding

- Component map:
  Frozen LLM backbone -> Noise Prompt Generator (Mφ) -> Encoder (fϕ) -> Discrepancy function (cosine distance) -> Hallucination score

- Critical path:
  1. Input (Q, A) -> Token embedding layer -> Sentence embedding h
  2. h -> Mφ -> Noise prompt embedding -> Concatenate to input
  3. Perturbed input -> LLM forward pass -> Extract intermediate layer representation
  4. Original and perturbed representations -> fϕ encoder -> z and ẑ
  5. Cosine distance Disc(z, ẑ) = 1 - cos(z, ẑ) -> Hallucination score

- Design tradeoffs:
  - **Layer selection**: Middle layers (peaks around layer 10-20 in 32-layer models) balance semantic richness vs. bias accumulation; too early = shallow features, too late = output bias.
  - **Threshold settings (τT=0.3, τH=0.7)**: Moderate values optimal; extreme thresholds tolerate excessive noise and reduce accuracy (Figure 3b).
  - **Training data efficiency**: Works with 100 labeled samples; more data improves but SSP outperforms baselines even in low-data regimes.

- Failure signatures:
  - **Static prompt performance**: If using fixed perturbations, expect ~4-5% AUROC drop vs. sample-specific.
  - **Encoder ablation**: Removing fϕ collapses performance to ~65% AUROC (Table 2), confirming encoder is critical for amplification.
  - **Cross-dataset transfer**: If source and target domains diverge significantly, expect 5-15% AUROC degradation vs. in-domain.

- First 3 experiments:
  1. **Layer sweep**: Extract representations from layers 1-32 on TruthfulQA; plot AUROC to confirm middle-layer peak matches paper (Figure 3a).
  2. **Distance metric comparison**: Implement Manhattan, Euclidean, KL-divergence alongside cosine; verify cosine superiority on TriviaQA.
  3. **Generalization test**: Train on TriviaQA (100 samples), test on TydiQA-GP; confirm AUROC >70% to validate cross-dataset transfer claim.

## Open Questions the Paper Calls Out

- Can the SSP framework be extended to provide fine-grained, token-level hallucination localization rather than a single sentence-level score? (Current design computes global discrepancy score)
- Does the finding that truthful responses are more sensitive to perturbation than hallucinations hold for tasks involving logical reasoning or code generation? (Experiments limited to fact-based QA)
- Can the discriminative power of SSP be improved by replacing the cosine similarity metric with more complex, learned scoring functions? (Current metric described as "relatively simple")

## Limitations

- Layer selection sensitivity: The method's performance critically depends on choosing the right intermediate layer, but the paper does not specify which layer index to use
- Label quality assumptions: Evaluation relies on automatic labeling rather than human-annotated ground truth, introducing potential noise in training signal
- Generalization gaps: Cross-dataset scenarios show 5-15% AUROC degradation, suggesting limited applicability when source and target distributions differ substantially

## Confidence

- **High Confidence**: The perturbation sensitivity mechanism is well-supported by experimental results showing cosine similarity outperforms other distance metrics and the encoder component is essential for performance
- **Medium Confidence**: The sample-specific noise prompt advantage is demonstrated but lacks direct comparison to static prompts in the main paper
- **Medium Confidence**: The cosine similarity metric superiority is empirically validated with clear numerical comparisons, though theoretical justification is less developed

## Next Checks

1. **Layer Sensitivity Analysis**: Systematically evaluate SSP performance across all intermediate layers (1-32) on TruthfulQA to identify optimal layer and quantify performance sensitivity to layer selection

2. **Static vs. Sample-Specific Prompt Comparison**: Implement a baseline using fixed noise prompts across all samples and compare AUROC performance directly against the sample-specific approach on at least two datasets

3. **Domain Adaptation Testing**: Train SSP on one domain (e.g., TriviaQA) and evaluate on out-of-domain datasets with varying semantic similarity (e.g., TydiQA-GP, CoQA) to measure degradation patterns and identify domain shift thresholds