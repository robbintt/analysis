---
ver: rpa2
title: On the Role of Hidden States of Modern Hopfield Network in Transformer
arxiv_id: '2511.20698'
source_url: https://arxiv.org/abs/2511.20698
tags:
- latexit
- sha1
- base64
- attention
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper connects the Modern Hopfield Network (MHN) to Transformer\
  \ self-attention, showing that hidden state dynamics in MHN\u2014beyond the adiabatic\
  \ approximation\u2014lead to a new attention mechanism called Modern Hopfield Attention\
  \ (MHA). By adding a hidden state that accumulates attention scores across layers,\
  \ MHA enables reuse of attention information, reduces rank collapse and token uniformity,\
  \ and improves model performance."
---

# On the Role of Hidden States of Modern Hopfield Network in Transformer

## Quick Facts
- arXiv ID: 2511.20698
- Source URL: https://arxiv.org/abs/2511.20698
- Reference count: 40
- Primary result: Modern Hopfield Attention (MHA) improves Transformer performance by accumulating attention scores in a hidden state, reducing rank collapse and token uniformity.

## Executive Summary
This paper connects the Modern Hopfield Network (MHN) to Transformer self-attention, showing that hidden state dynamics in MHN—beyond the adiabatic approximation—lead to a new attention mechanism called Modern Hopfield Attention (MHA). By adding a hidden state that accumulates attention scores across layers, MHA enables reuse of attention information, reduces rank collapse and token uniformity, and improves model performance. Experiments on GPT-2 and Vision Transformer show systematic gains in perplexity and accuracy without extra parameters. Theoretically, MHA mitigates rank collapse in attention-only networks, and empirically, it eliminates perfectly aligned tokens in both language and vision Transformers. MHA offers a principled way to enhance Transformers using insights from associative memory.

## Method Summary
The paper proposes Modern Hopfield Attention (MHA), which modifies standard self-attention by introducing a persistent hidden state that accumulates attention scores across layers. The hidden state is updated via an exponential moving average of query-key products (Q·K^T) and is used to compute attention weights via softmax. This hidden state is passed between layers, allowing attention information to be reused. The mechanism is derived from the continuous dynamics of Modern Continuous Hopfield Networks (MCHN), with two key hyperparameters (α and α') controlling the skip connection and hidden state accumulation respectively. MHA is evaluated on GPT-2 for language modeling and Vision Transformers for image classification, showing consistent performance improvements over standard attention.

## Key Results
- MHA improves GPT-2 perplexity from 22.9 to 20.7 on WikiText103
- MHA improves ViT accuracy from 75.4% to 76.2% on CIFAR100
- MHA eliminates rank collapse and token uniformity, as shown by cosine similarity distributions that no longer peak at 1.0
- MHA provides consistent gains across model sizes (Tiny to Large) without additional parameters

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Accumulating attention scores in a persistent hidden state mitigates rank collapse.
- Mechanism: Standard self-attention causes feature rank to decay exponentially with depth (rank collapse). By maintaining a hidden state H_n that aggregates attention scores Q_n K^T_n across layers via an exponential moving average, the proposed Modern Hopfield Attention (MHA) relaxes this decay from double-exponential to linear, theoretically preventing token uniformity.
- Core assumption: The dynamics of Modern Continuous Hopfield Networks (MCHN) provide a valid generalized form of the Transformer update rule when the adiabatic limit is relaxed.
- Evidence anchors:
  - [section] Theorem 5.2 proves that a non-zero α' improves the upper-bound of rank decay, avoiding exponential collapse.
  - [section] Figure 2 shows violin plots where standard GPT-2/ViT have modes of cosine similarity at 1.0 (uniformity), while MHA models do not.
  - [corpus] Corpus evidence for this specific rank-collapse mechanism is weak; related papers like "Rectified Lagrangian..." focus on OOD detection rather than rank preservation.
- Break condition: If the hidden state is reset or not propagated between layers (i.e., α' → 0 or treated as local only), the mechanism fails to propagate the stabilizing history, potentially re-introducing rapid rank decay.

### Mechanism 2
- Claim: Propagating hidden states enables reuse of attention information across layers.
- Mechanism: In standard Transformers, attention scores are computed locally per layer. MHA introduces a tensor H_ℓ that propagates through the network, accumulating attention scores. This allows the softmax operation at layer ℓ to be influenced by the attention history of previous layers, effectively smoothing the attention landscape.
- Core assumption: A linear combination (moving average) of current and past attention scores provides a more robust weight matrix for value aggregation than instantaneous scores.
- Evidence anchors:
  - [abstract] "MHA allows the inheritance of attention scores from the input layer... to the output layer."
  - [section] Equation (10) defines the update: h_n = α' h_{n-1} + (1-α') q_n K^T_n.
  - [corpus] "Modern Hopfield Networks with Continuous-Time Memories" discusses resource allocation in memory, which aligns with the concept of continuous state updates, though it does not explicitly validate the cross-layer inheritance claim.
- Break condition: If the moving average coefficient α' is too small (history fades too fast) or too large (history dominates current context), the "inheritance" becomes either negligible or overpowering, disrupting the balance between local and global attention contexts.

### Mechanism 3
- Claim: The interaction between the skip connection weight (α) and hidden state weight (α') is required for performance stability.
- Mechanism: The architecture introduces two parameters derived from discretization time constants. α balances the skip connection strength, while α' controls the memory of the hidden state. The paper finds that both must be tuned; setting either to extremes (e.g., α=1, no attention contribution) degrades performance significantly.
- Core assumption: The discretization of MCHN dynamics maps naturally to the feed-forward structure of Transformers only when these two independent effects are balanced.
- Evidence anchors:
  - [section] Table 6 demonstrates that fixing α'=0.5 and varying α yields different performance peaks, and vice versa.
  - [section] Section 4.5 notes that setting α=0 (removing the skip connection effect from the MHA formulation) degrades performance, indicating the skip connection modification is synergistic with the hidden state.
  - [corpus] No specific corpus evidence regarding the α/α' balance.
- Break condition: If α and α' are decoupled or one is treated as a fixed constant without tuning relative to the other, the model may converge poorly or fail to utilize the hidden state effectively.

## Foundational Learning

- Concept: **Modern Hopfield Networks (MHN) & Adiabatic Limit**
  - Why needed here: The paper derives its architecture from the MHN continuous dynamics. Understanding that standard self-attention is an "adiabatic limit" (where hidden states collapse instantly) explains why the proposed MHA is a generalization rather than a brand new invention.
  - Quick check question: Why does the adiabatic approximation prevent the use of hidden states in standard Transformer analysis?

- Concept: **Rank Collapse / Token Uniformity**
  - Why needed here: This is the core pathology MHA attempts to solve. One must understand that deep attention-only networks tend to make all token representations identical (rank 1 matrix) without skip connections or specific architectural modifications.
  - Quick check question: How does the residual of a feature matrix change as depth increases in a rank-collapsed network?

- Concept: **Exponential Moving Average (EMA)**
  - Why needed here: The hidden state H_n is updated via an EMA of attention scores. Understanding how EMA weights old data vs. new data is crucial to grasping how MHA smooths attention across layers.
  - Quick check question: If α' (the EMA coefficient) is set to 0.1, how much weight is given to the immediate attention score vs. the previous hidden state?

## Architecture Onboarding

- Component map: X_n -> Q/K/V projections -> H_n update (EMA of Q·K^T) -> P_n = softmax(H_n) -> Z_n = P_n V -> X_{n+1} = α X_n + (1-α) Z_n
- Critical path: The flow of H_n. Unlike standard attention which is stateless per layer, MHA requires passing a tensor H (size T×T per head) from layer n to n+1. Initialization of H_0 (usually zeros) and the precise update rule are the critical implementation steps.
- Design tradeoffs:
  - **Memory**: MHA adds a memory overhead of O(T^2) per layer to store/pass the hidden states, whereas standard attention is O(1) state per layer (ignoring activations).
  - **Parameters**: The paper claims *zero* additional training parameters. The only new hyperparameters are α and α'.
- Failure signatures:
  - **Token Uniformity**: If you see cosine similarity between tokens trending to 1.0 in deep layers, α' may be too low or the hidden state is not propagating correctly.
  - **Training Collapse**: If α is set to 1.0, the model relies entirely on skip connections and performance drops to chance levels (Table 6).
- First 3 experiments:
  1. **Ablation on Depth**: Train a small ViT (e.g., ViT-Tiny) on CIFAR10 with varying depths (4, 8, 12 layers). Verify if MHA maintains accuracy while standard attention drops, confirming the rank collapse mitigation.
  2. **Hyperparameter Sweep**: Sweep α and α' (e.g., values in [0.3, 0.5, 0.7]). Confirm if the interaction is necessary (as per Table 6) and identify the stability region.
  3. **Hidden State Initialization**: Test initializing H_0 with zeros vs. random noise vs. the first layer's attention score to verify the paper's implicit assumption that zero-initialization is sufficient for convergence.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does Modern Hopfield Attention (MHA) provide consistent performance improvements in large-scale pre-training regimes (e.g., billions of parameters) beyond the academic-scale models tested?
- Basis in paper: [explicit] The authors explicitly state in the Limitations section: "It will be an interesting future direction to see what results will be obtained with larger-scale pre-training."
- Why unresolved: The experiments were limited by computational resources to GPT-2 (Medium) and ViT-L, leaving the scalability of MHA to modern LLMs unproven.
- What evidence would resolve it: Training runs of large-scale models (1B+ parameters) using MHA, showing improvements in perplexity and downstream task performance compared to baselines.

### Open Question 2
- Question: What is the theoretical energy function interpretation for MHA when the memory matrices are asymmetric (W_1 ≠ W_2)?
- Basis in paper: [inferred] The paper notes in a footnote that breaking symmetry invalidates the monotonically decreasing energy assumption, and "Interpreting the energy function of MHA in asymmetric settings is a very interesting theoretical challenge for future research."
- Why unresolved: The connection to Hopfield networks currently relies on symmetric weights to define a stable energy landscape, which is violated in the general Transformer setting.
- What evidence would resolve it: A formal derivation of a Lyapunov function or energy landscape for the asymmetric MHA update rules.

### Open Question 3
- Question: Do mechanisms other than rank collapse mitigation, such as optimization landscape smoothing or gradient flow improvements, contribute significantly to the performance gains of MHA?
- Basis in paper: [explicit] The authors state in the Limitations that "it may be interesting to investigate whether there is any relationship with other factors" beyond rank collapse and attention entropy.
- Why unresolved: While rank collapse is theoretically and empirically linked to performance, the authors acknowledge it may not be the sole factor, and other architectural inductive biases have not been isolated.
- What evidence would resolve it: Ablation studies that decouple rank collapse suppression from other side effects of the hidden state dynamics to isolate causal mechanisms.

### Open Question 4
- Question: How does independently tuning the hidden state accumulation coefficient (α') versus the skip connection coefficient (α) affect the trade-off between training stability and model performance?
- Basis in paper: [inferred] The paper simplifies its experiments by constraining α = α', noting that "these two quantities essentially work differently" (one balances skip connections, the other the exponential moving average of attention scores).
- Why unresolved: The interaction between these two distinct hyperparameters remains unexplored, potentially leaving performance on the table.
- What evidence would resolve it: A hyperparameter grid search varying α and α' independently to map their interaction effects on convergence speed and final accuracy.

## Limitations

- The empirical evidence base is narrow, with limited ablations testing MHA against alternative rank collapse mitigation methods.
- The claim that MHA's hidden-state mechanism is uniquely responsible for rank collapse mitigation remains under-validated.
- Corpus evidence for specific mechanisms is limited, with only 25 related papers and minimal citations.
- Scalability to large-scale pre-training regimes (billions of parameters) remains unproven.

## Confidence

- **High Confidence**: The theoretical framework connecting MHN dynamics to Transformer attention is sound. The rank-collapse phenomenon in deep attention-only networks is well-established in the literature. The MHA mechanism (hidden state accumulation + modified skip) is clearly specified and implementable.
- **Medium Confidence**: The empirical gains reported (perplexity reduction of ~2.2 points on GPT-2, accuracy improvements of 0.8-1.2% on ViT) are statistically meaningful but may not be robust across all model sizes and tasks. The corpus evidence for specific mechanisms is limited, and the ablations focus narrowly on α/α′ values rather than alternative architectural choices.
- **Low Confidence**: The claim that MHA's hidden-state mechanism is uniquely responsible for rank collapse mitigation, as opposed to other architectural changes in the formulation. The corpus signals show no direct evidence that this specific mechanism has been validated in other contexts.

## Next Checks

1. **Cross-Domain Ablation**: Implement MHA on a non-vision, non-language task (e.g., graph neural networks or audio Transformers) to test whether rank collapse mitigation and performance gains generalize beyond the tested domains.

2. **Mechanistic Isolation Test**: Create an ablation where the hidden state is reset to zero every N layers (N>1) versus per-layer reset. This would test whether the cumulative hidden state across all layers is necessary for the claimed benefits, or whether shorter-term accumulation suffices.

3. **Alternative Rank Collapse Mitigation Comparison**: Implement and compare MHA against at least two other rank-collapse mitigation methods (e.g., orthogonal regularization of attention weights, or attention dropout with specific scheduling) on the same GPT-2/ViT architectures to isolate whether the hidden-state mechanism provides unique benefits.