---
ver: rpa2
title: 'A Survey on Generative Recommendation: Data, Model, and Tasks'
arxiv_id: '2510.27157'
source_url: https://arxiv.org/abs/2510.27157
tags:
- recommendation
- generative
- wang
- data
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey provides a comprehensive overview of generative recommendation
  systems, categorizing them into data-level, model-level, and task-level approaches.
  At the data level, generative models enable knowledge augmentation and behavior
  simulation.
---

# A Survey on Generative Recommendation: Data, Model, and Tasks

## Quick Facts
- arXiv ID: 2510.27157
- Source URL: https://arxiv.org/abs/2510.27157
- Reference count: 40
- Primary result: Comprehensive survey of generative recommendation systems across data augmentation, model architectures, and task formulation

## Executive Summary
Generative recommendation systems are emerging as a paradigm shift from traditional discriminative scoring models, treating recommendation as a generation task rather than ranking. This survey systematically categorizes these approaches into three levels: data-level methods that augment knowledge and simulate user behavior, model-level techniques that integrate generative architectures through LLM alignment and scaling, and task-level innovations enabling conversational interaction and personalized content creation. The work identifies key advantages including world knowledge integration and reasoning capabilities, while highlighting critical challenges around benchmark design, model robustness, and deployment efficiency that must be addressed for widespread adoption.

## Method Summary
The survey conducts a comprehensive literature review of generative recommendation approaches, organizing them into a three-level taxonomy covering data augmentation, model architectures, and task formulation. It synthesizes methodologies including LLM-based alignment (SFT, DPO), Large Recommendation Models with scaling laws, and diffusion-based generation. The technical foundation involves converting recommendation tasks into conditional generation problems through item tokenization, prompt engineering, and constrained decoding. Training procedures follow standard generative objectives while addressing unique challenges like hallucination detection and latency optimization for real-time deployment.

## Key Results
- Generative models enable knowledge-infused data augmentation to address data sparsity and cold-start problems
- Large Recommendation Models demonstrate scaling laws that improve performance with increased parameters and sequence lengths
- Three-level taxonomy (data, model, task) provides systematic framework for understanding generative recommendation approaches

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-Infused Data Augmentation
Generative models synthesize high-quality user/item features and interactions from open-world knowledge, alleviating data sparsity and cold-start challenges. LLMs utilize their internal pre-trained knowledge to generate missing item attributes, user profiles, or synthetic interaction histories, enriching the sparse input data required by downstream models. The core assumption is that hallucinated content is factually grounded enough to be useful, with world knowledge embedded in the LLM aligning with the specific domain logic of the recommendation task.

### Mechanism 2: Generative Alignment via Item Tokenization
Generative models perform recommendation directly by predicting item identifiers as discrete tokens, provided the gap between collaborative signals and text semantics is bridged. Items are mapped to sequences of discrete tokens using codebooks or hierarchical semantic IDs, and the LLM is trained to generate these token sequences autoregressively, treating recommendation as a language generation task. The core assumption is that a meaningful mapping exists between the continuous interaction space and the discrete vocabulary space of the LLM.

### Mechanism 3: Scaling Laws in Large Recommendation Models (LRMs)
Scaling up model parameters and sequence lengths in native recommendation architectures yields emergent capabilities and improved accuracy, bypassing the plateau of traditional discriminative models. By treating user interaction history as a dense sequence and applying massive Transformers, the model learns complex transitions and user intents that smaller models miss, supporting unified task handling through end-to-end generation rather than cascaded architectures.

## Foundational Learning

**Concept: Discriminative vs. Generative Paradigm**
- Why needed: The paper is a polemic against traditional "scoring" models. Understanding the difference between outputting a probability score for a fixed item (Discriminative) and generating a new token sequence (Generative) is essential to follow the survey's taxonomy.
- Quick check: Can you explain why predicting the next token in a sequence is fundamentally different from calculating a click-through rate for a candidate list?

**Concept: Collaborative Filtering (CF) Signals vs. Content Semantics**
- Why needed: A major challenge cited is the "alignment" between LLM text capabilities and CF interaction data. Distinguishing between what an item is (text/semantics) and how users interact with it (collaborative/ID-based) is crucial.
- Quick check: Why might a pure LLM (trained only on text) fail to recommend an item with a boring title but high user engagement?

**Concept: Tokenization & Quantization (VQ-VAE/RQ-VAE)**
- Why needed: Section 4.1.2 and 5.1 rely on converting items to "Semantic IDs." Understanding how discrete tokens are learned from embeddings is crucial for grasping how LLMs "output" items.
- Quick check: How does a "vocabulary-constrained decoding" ensure the model outputs a valid product ID rather than random text?

## Architecture Onboarding

**Component map:** Input Layer (User History + Item Metadata + Collaborative Embeddings) -> Tokenizer (Converts Items to Text or Semantic IDs) -> Backbone (Generative LLM or Native LRM) -> Output Layer (Softmax over vocabulary -> Mapping to valid Item IDs)

**Critical path:** 1. Data Preparation: Format interaction sequences as text or token streams. 2. Alignment: Fine-tune the LLM on recommendation tasks (SFT/DPO) or inject collaborative embeddings. 3. Inference: Generate tokens -> Grounding/Filtering to valid items.

**Design tradeoffs:** Text vs. ID Tokens (Text allows zero-shot generalization but is verbose and hallucinate; ID tokens are precise but require training codebooks); End-to-End vs. Cascade (LRMs simplify architecture but require massive resources; cascades are cheaper but suffer from error propagation).

**Failure signatures:** Hallucination (generating titles of items that do not exist in the catalog); Popularity Bias (over-generating head items due to pre-training bias); Latency Explosion (autoregressive decoding of long sequences is slower than parallel scoring).

**First 3 experiments:** 1. Zero-Shot Prompting: Feed raw interaction history into a frozen LLM and measure precision@K. 2. ID-Alignment (SFT): Implement basic "Text-to-Item ID" fine-tuning to verify mapping from history to next item ID. 3. Constrained Decoding: Implement Trie-based or FM-index constraint during inference to ensure generated token sequence corresponds to a real item.

## Open Questions the Paper Calls Out

**Open Question 1:** How can the field develop dynamic benchmarks that capture real-world complexity and multi-round interactions? The paper states most datasets are non-interactive and static, creating an urgent need for new benchmarks supporting the next stage of recommendation research. This remains unresolved because existing datasets like MovieLens capture point-in-time preferences rather than dynamic feedback loops.

**Open Question 2:** How can LLM-based recommenders maintain superior performance while achieving inference latency comparable to traditional models? The paper explicitly asks how to leverage superior performance while maintaining low inference latency. This remains unresolved because autoregressive decoding in generative models requires multiple serial calls, causing excessive time consumption unsuitable for real-time scenarios.

**Open Question 3:** What defense strategies are effective against low-cost textual simulation attacks in generative recommender systems? The paper notes that defense strategies exhibit limited effectiveness against data poisoning and textual attacks are remarkably low-cost. This remains unresolved because current defenses are preliminary, and adversarial texts are semantically similar to originals, making detection difficult.

## Limitations

- Fundamental tensions between generative and discriminative paradigms remain unresolved, particularly around hallucination and popularity bias
- Computational overhead of autoregressive generation versus parallel scoring in traditional systems presents a practical barrier
- Specific performance benchmarks comparing generative versus discriminative methods across real-world datasets are limited

## Confidence

**High Confidence:** Classification of generative recommendation approaches into data-level, model-level, and task-level categories reflects the current research landscape accurately. Identification of hallucination and popularity bias as failure modes is well-supported. Technical description of tokenization-based item generation and scaling law observations are consistent with published implementations.

**Medium Confidence:** Assertion that generative models inherently possess superior reasoning capabilities for explainable recommendations requires more empirical validation across diverse domains. Claim that end-to-end generative models will replace cascaded architectures assumes computational constraints will be overcome. Effectiveness of world knowledge integration depends heavily on domain specificity not fully explored.

**Low Confidence:** Predictions about generative recommendation becoming the dominant paradigm by 2025 are speculative given rapid evolution of both LLM capabilities and traditional recommendation methods. Survey's roadmap for "intelligent recommendation assistants" extrapolates current trends without accounting for potential paradigm shifts or regulatory constraints.

## Next Checks

1. **Hallucination Detection Benchmark:** Implement controlled experiment comparing pure text-generation approaches against ID-constrained methods on standardized dataset, measuring both recommendation accuracy and hallucination rates when filtering generated items against actual catalog.

2. **Scaling Law Validation:** Replicate LRM scaling experiments by training multiple versions of native Transformer recommender with varying parameter counts on same dataset, plotting performance curves to verify whether diminishing returns observed in traditional models truly disappear with generative architectures.

3. **Inference Latency Comparison:** Build production-like evaluation framework comparing real-time serving performance of generative recommendation models against traditional two-tower retrieval + ranking pipelines, measuring end-to-end latency, throughput, and accuracy trade-offs under realistic batch sizes and hardware constraints.