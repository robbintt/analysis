---
ver: rpa2
title: 'Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon
  Tasks'
arxiv_id: '2510.08002'
source_url: https://arxiv.org/abs/2510.08002
tags:
- agent
- memory
- task
- tasks
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MUSE, a novel agent framework designed to
  address the limitations of existing large language model (LLM) agents in long-horizon,
  real-world productivity tasks. The core innovation is an experience-driven, self-evolving
  system centered around a hierarchical Memory Module that enables agents to learn
  from experience and continuously improve through a "Plan-Execute-Reflect-Memorize"
  loop.
---

# Learning on the Job: An Experience-Driven Self-Evolving Agent for Long-Horizon Tasks

## Quick Facts
- arXiv ID: 2510.08002
- Source URL: https://arxiv.org/abs/2510.08002
- Reference count: 40
- Primary result: MUSE achieves 51.78% Spartial on TAC benchmark, a 20% relative improvement over previous state-of-the-art

## Executive Summary
This paper introduces MUSE, a novel agent framework designed to address the limitations of existing large language model (LLM) agents in long-horizon, real-world productivity tasks. The core innovation is an experience-driven, self-evolving system centered around a hierarchical Memory Module that enables agents to learn from experience and continuously improve through a "Plan-Execute-Reflect-Memorize" loop. MUSE transforms raw execution trajectories into structured knowledge stored across three memory types: Strategic, Procedural, and Tool memories. The framework was evaluated on the TAC benchmark, achieving state-of-the-art performance with a score of 51.78% using only a lightweight Gemini-2.5 Flash modelâ€”a 20% relative improvement over the previous best. Extensive experiments demonstrate MUSE's robust continuous learning and self-evolution capabilities, with performance improving across iterations. Additionally, the accumulated experience exhibits strong generalization properties, enabling zero-shot improvement on new tasks. The memory system's natural language format allows seamless knowledge transfer across different LLMs.

## Method Summary
MUSE implements a hierarchical Memory Module that organizes experience into three types: Strategic Memory (macro-level paradigms), Procedural Memory (tool sequences/SOPs), and Tool Memory (static tool descriptions and dynamic instructions). The system operates through a Plan-Execute-Reflect-Memorize loop where a Planning-Execution Agent decomposes tasks and interacts with a minimal toolset (browser, shell, code interpreter) via a ReAct loop, while a dedicated Reflect Agent evaluates each sub-task's success against truthfulness, deliverable verification, and data fidelity criteria before distilling successful trajectories into SOPs. The framework was evaluated on the TAC benchmark using a lightweight Gemini-2.5 Flash model without fine-tuning, demonstrating SOTA performance and continuous learning across iterations.

## Key Results
- Achieves 51.78% Spartial on full TAC benchmark, outperforming previous state-of-the-art by 20%
- Demonstrates robust continuous learning: performance improves across 3 iterative runs on Tcl subset
- Shows strong zero-shot generalization: accumulated experience improves performance on unseen Thard subset tasks
- Validates cross-model transferability: natural language memory format enables seamless knowledge transfer across different LLMs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A hierarchical memory architecture may enable better generalization in long-horizon tasks compared to unstructured logs by separating high-level strategies from low-level procedures.
- **Mechanism:** The system distinctively categorizes experience into Strategic Memory (M_strat), Procedural Memory (M_proc), and Tool Memory (M_tool). This prevents the context window from being flooded with raw logs, allowing the agent to retrieve specific "Standard Operating Procedures" (SOPs) or strategic principles only when needed.
- **Core assumption:** The "Reflect Agent" can reliably abstract raw trajectories into structured natural language rules (SOPs) without introducing hallucinations or losing critical nuance.
- **Evidence anchors:**
  - [abstract] Mentions organizing "diverse levels of experience" into three memory types.
  - [section 3.2] Defines the three memory components and their specific roles (e.g., M_strat for macro-level paradigms, M_proc for tool sequences).
  - [corpus] "Remember Me, Refine Me" (arXiv:2512.10696) supports the concept of dynamic procedural memory but does not validate the specific 3-tier hierarchy used here.
- **Break condition:** If the abstraction logic fails (e.g., strategies become too generic or procedures too specific), retrieval noise increases, potentially degrading performance below a memory-less baseline.

### Mechanism 2
- **Claim:** Decoupling execution from verification via a "Reflect Agent" reduces error propagation in long-horizon workflows.
- **Mechanism:** A dedicated Reflect Agent evaluates the Planning-Execution (PE) Agent's outputs against a checklist (Truthfulness, Deliverable Verification, Data Fidelity) before committing a trajectory to memory. This filters out "hallucinated" completions that would otherwise poison the knowledge base.
- **Core assumption:** The Reflect Agent (using the same LLM backbone) has sufficient reasoning capacity to objectively judge the PE Agent's actions, avoiding the "blind leading the blind" scenario.
- **Evidence anchors:**
  - [abstract] Highlights the "Plan-Execute-Reflect-Memorize" loop where the agent "autonomously reflects."
  - [section 3.4] Describes the Reflect Agent's specific inspection methods: "trajectory referencing" and "active verification."
  - [corpus] "Deep Search with Hierarchical Meta-Cognitive Monitoring" (arXiv:2601.23188) aligns with the need for monitoring reasoning states, though specific efficacy data for MUSE is internal to the paper.
- **Break condition:** If the evaluation logic is rigid or the Reflect Agent fails to detect subtle semantic errors, the memory accumulates "false positives" (incorrectly labeled successes).

### Mechanism 3
- **Claim:** Constraining the agent to a "Minimal Usable Toolset" forces the learning of reusable workflows rather than memorizing API-specific shortcuts.
- **Mechanism:** Instead of providing specialized tools (e.g., "edit Excel"), MUSE provides basic tools (browser, shell, code interpreter). The agent must compose these into complex workflows (SOPs), which are then stored in Procedural Memory, making the knowledge more portable across environments.
- **Core assumption:** The base LLM possesses sufficient compositional reasoning to build complex workflows from primitive tools without excessive trial-and-error.
- **Evidence anchors:**
  - [section 3.3] Explicitly states the use of a "minimal toolset" to compel the agent to "creatively combine basic tools."
  - [section 4.3.3] Shows SOTA results using this minimal toolset approach.
  - [corpus] "Experience-Driven Exploration for Efficient API-Free AI Agents" (arXiv:2510.15259) discusses similar constraints in GUI environments, lending credence to the approach.
- **Break condition:** If the tasks require highly specialized atomic actions not achievable by the basic toolset, the agent may fail regardless of planning capability.

## Foundational Learning

- **Concept:** ReAct (Reason + Act) Paradigm
  - **Why needed here:** The Planning-Execution Agent relies on generating "Thoughts" (reasoning traces) before "Actions" to maintain state in long-horizon tasks.
  - **Quick check question:** Can you explain how interleaving reasoning traces with actions helps mitigate error accumulation in multi-step tasks?

- **Concept:** Retrieval-Augmented Generation (RAG) for Memory
  - **Why needed here:** The MUSE architecture retrieves "Procedural Memory" (SOPs) to inject into the context window, a form of dynamic few-shot prompting.
  - **Quick check question:** How does separating the memory "index" from the "content" (as done in M_proc) optimize context window usage?

- **Concept:** Iterative Refinement / Self-Correction
  - **Why needed here:** The framework depends on a loop where failed attempts trigger "replanning" and "retry" mechanisms based on reflection.
  - **Quick check question:** What is the difference between "replanning" (updating the sub-task queue) and "retrying" (re-executing a sub-task) in this architecture?

## Architecture Onboarding

- **Component map:**
  Memory Module -> PE Agent -> Reflect Agent -> Environment

- **Critical path:**
  Task Intake -> PE Agent Decomposition -> Sub-task Execution (ReAct Loop) -> Reflect Agent Evaluation -> (If Success) Distill to SOP -> Update M_proc -> Next Sub-task.

- **Design tradeoffs:**
  - **Speed vs. Quality:** The Reflect Agent runs after *every* sub-task, significantly increasing latency and token cost to ensure high-quality memory updates.
  - **Generalization vs. Specificity:** Using a "Minimal Toolset" increases difficulty initially but aims for more robust, transferable SOPs compared to environment-specific APIs.

- **Failure signatures:**
  - **Context bloat:** If SOPs are not deduplicated or summarized effectively, the index size grows unmanageable.
  - **Cascading hallucination:** If the Reflect Agent validates a false positive, the resulting "SOP" will misguide future executions.
  - **Infinite loops:** The paper sets a hard limit (N=20 actions) per sub-task; hitting this frequently indicates a failure in the planning or tool capability.

- **First 3 experiments:**
  1. **Sanity Check:** Run the "No Reflection Variant" (Ablation study in Section 4.4.1) against the full model on a small subset (e.g., the Tcl set) to verify the Reflect Agent is actually adding value.
  2. **Continuous Learning Curve:** Execute the 3-iteration experiment on the Tcl subset (Section 4.3.1) and plot the `S_partial` score. You should see a monotonic increase if the memory mechanism is functioning.
  3. **Cross-Model Transfer:** As suggested in Section 4.4.2, try porting the accumulated memory (generated by Gemini) to a different model (e.g., DeepSeek) to validate the "LLM-agnostic" memory claim.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can the memory architecture be enhanced to effectively support high-level planning and multi-hop search tasks?
  - **Basis in paper:** [explicit] Page 10 states the architecture "has limitations in handling specific tasks like high-level planning or multi-hop search."
  - **Why unresolved:** The current design focuses primarily on procedural and tool memory, which may not capture the abstract dependencies required for complex, non-linear search.
  - **What evidence would resolve it:** Modifications to the memory schema that demonstrably improve performance on benchmarks requiring multi-step logical inference or retrieval chains.

- **Open Question 2:** Can the integration of human-in-the-loop feedback improve the accuracy and relevance of the accumulated experience?
  - **Basis in paper:** [explicit] Page 11 suggests the architecture facilitates "human-agent collaborative iteration" by allowing users to manage and revise stored experiences.
  - **Why unresolved:** The current framework is designed as a fully autonomous system and has not been experimentally validated with human feedback mechanisms.
  - **What evidence would resolve it:** Experiments measuring performance deltas when human supervisors are allowed to correct, prune, or augment the memory module during the evolution process.

- **Open Question 3:** How can the agent's reflection mechanism be adapted to succeed in environments where evaluation scripts penalize valid, alternative strategies?
  - **Basis in paper:** [explicit] Page 11 notes that evaluation scripts for certain tasks are rigid, causing "unexpected yet plausible agent strategies" to be underestimated or penalized.
  - **Why unresolved:** The agent optimizes for the benchmark's specific checkpoints, which may misalign with the actual task goal if the benchmark fails to account for valid solution paths.
  - **What evidence would resolve it:** A study comparing agent success rates against human evaluation versus the current rigid script-based evaluation.

## Limitations
- Limited effectiveness for high-level planning and multi-hop search tasks due to current memory architecture focus
- Performance in real-world environments remains unproven beyond simulated benchmarks
- Potential vulnerability to rigid evaluation scripts that penalize valid alternative strategies

## Confidence
- **High Confidence:** The core architectural design and reported SOTA performance on TAC benchmark (51.78% Spartial) are well-supported by ablation studies and rigorous methodology.
- **Medium Confidence:** Claims around continuous learning and cross-model transferability are plausible but require further validation across more iterations and diverse LLM families.
- **Low Confidence:** Zero-shot generalization and real-world applicability claims remain speculative without evidence from domains requiring deep expertise or handling of dynamic, uncontrolled environments.

## Next Checks
1. **Cross-Domain Generalization Test:** Evaluate MUSE on a distinct benchmark outside the TAC suite to validate zero-shot transfer claims and identify which memory types contribute most to successful transfer.
2. **Long-Term Learning Stability:** Run continuous learning loop for 10-20 iterations on Tcl subset to monitor memory saturation, context bloat, and performance plateaus.
3. **Robustness to Model Variation:** Implement memory system with different LLM backbone (e.g., GPT-4o, Claude 3.5) and retrain on TAC benchmark to quantify impact of model choice on framework efficacy.