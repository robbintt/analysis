---
ver: rpa2
title: 'Preconditioned Sharpness-Aware Minimization: Unifying Analysis and a Novel
  Learning Algorithm'
arxiv_id: '2501.06603'
source_url: https://arxiv.org/abs/2501.06603
tags:
- infosam
- learning
- adversarial
- asam
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a preconditioning-based framework termed preSAM
  to unify existing sharpness-aware minimization (SAM) variants and enable principled
  algorithm design. The framework categorizes SAM variants as objective preconditioning
  (OP) or constraint preconditioning (CP) based on where preconditioning is applied.
---

# Preconditioned Sharpness-Aware Minimization: Unifying Analysis and a Novel Learning Algorithm

## Quick Facts
- **arXiv ID:** 2501.06603
- **Source URL:** https://arxiv.org/abs/2501.06603
- **Reference count:** 40
- **Primary result:** Preconditioned Sharpness-Aware Minimization (PreSAM) unifies existing SAM variants and introduces InfoSAM, achieving superior generalization on CIFAR-10/100 and ImageNet.

## Executive Summary
This paper introduces PreSAM, a preconditioning-based framework that unifies existing Sharpness-Aware Minimization (SAM) variants by categorizing them as objective preconditioning (OP) or constraint preconditioning (CP). The framework provides a general mathematical structure where preconditioning matrices can be applied to either the objective function or the constraint of the SAM subproblem. Building on this unification, the authors propose InfoSAM, a novel OP algorithm that addresses the Adversarial Model Degradation (AMD) issue by preconditioning gradients based on noise estimates. InfoSAM achieves state-of-the-art performance across multiple image classification benchmarks.

## Method Summary
The paper develops PreSAM as a unifying framework that generalizes SAM variants through preconditioning. It introduces objective preconditioning (OP) where matrices modify the inner product in the maximization objective, and constraint preconditioning (CP) where matrices reshape the constraint norm. InfoSAM, a novel OP algorithm, applies preconditioning using the inverse of estimated gradient variance to suppress perturbations along noisy dimensions. The method employs an exponential moving average to estimate noise, constructs a diagonal preconditioner from this estimate, and applies it to the gradient before computing the adversarial perturbation. The framework includes a convergence analysis showing how preconditioning affects convergence rates differently for OP and CP variants.

## Key Results
- InfoSAM outperforms standard SAM by 0.13% on CIFAR-10 and 0.51% on CIFAR-100 with ResNet-18
- InfoSAM achieves 79.8% Top-1 accuracy on ImageNet with ResNet-50, surpassing SAM's 78.5%
- InfoSAM maintains stable training even with high gradient noise, demonstrating AMD mitigation
- The unifying framework recovers existing algorithms like ASAM and FisherSAM as special cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Preconditioning the SAM subproblem unifies existing variants by generalizing the perturbation geometry.
- **Mechanism:** The standard SAM perturbation is derived from a linear approximation. By introducing preconditioning matrices $C_t$ (objective) and $D_t$ (constraint), PreSAM alters the inner product or the constraint norm. This transforms the perturbation vector $\epsilon_t$, recovering algorithms like ASAM (via $D_t$) or $\ell_\infty$-SAM (via $C_t$) as special cases where specific geometries are imposed.
- **Core assumption:** The first-order Taylor expansion remains a valid proxy for the local loss landscape even when the gradient direction is skewed by $C_t$ or the constraint is reshaped by $D_t$.
- **Evidence anchors:**
  - [section] Section III, Eq. (4) defines the PreSAM maximization objective.
  - [section] Table I maps existing algorithms (ASAM, FisherSAM) to the PreSAM framework.
  - [corpus] Neighbor papers (e.g., "Sharpness-Aware Minimization: General Analysis") corroborate the active interest in generalizing SAM convergence, though specific PreSAM formulations are unique to this work.
- **Break condition:** If the preconditioners are not positive definite or well-scaled, the closed-form solution (Eq. 5) may become numerically unstable or invalid.

### Mechanism 2
- **Claim:** Noise-adjusted objective preconditioning (InfoSAM) mitigates Adversarial Model Degradation (AMD).
- **Mechanism:** Standard SAM perturbs parameters in the direction of the stochastic gradient $g_t$. If gradient noise is high (low SNR), this direction is misleading. InfoSAM applies an objective preconditioner $C_t = \hat{\Sigma}_t^{-1}$ (inverse of estimated gradient variance). This suppresses the perturbation magnitude along dimensions where gradient noise is high, ensuring $\epsilon_t$ points along the "reliable" dimensions of the true gradient.
- **Core assumption:** The variance of the stochastic gradient correlates with the error in the adversarial direction; dimensions with lower variance carry higher-fidelity signal for finding flat minima.
- **Evidence anchors:**
  - [section] Section IV.A defines AMD and links it to low SNR.
  - [abstract] Mentions InfoSAM adjusts gradients based on noise estimates to address AMD.
  - [corpus] Neighbor "Focal-SAM" discusses long-tailed distributions affecting gradient reliability, offering a parallel motivation for gradient re-weighting, though via a different mechanism.
- **Break condition:** If the noise estimator (exponential moving average of gradients) fails to track the true variance (e.g., non-stationary distributions), the preconditioner may erroneously suppress valid gradient signals.

### Mechanism 3
- **Claim:** Constraint Preconditioning (CP) impacts convergence rate and requires strict bounding of $D_t$.
- **Mechanism:** Theorem 1 links convergence bounds to $\|D_t^{-1}\|$. In CP methods (like ASAM), if the preconditioner $D_t$ involves terms like $|x_t|^{-1}$, and weights become small, the matrix norm grows. This degrades the convergence rate constant $D_0$ unless the perturbation radius $\rho$ is scaled inversely, explaining why CP methods often require hyperparameter tuning distinct from standard SAM.
- **Core assumption:** Assumptions 1-3 (Lipschitz smoothness, bounded variance) hold, and $D_t$ is invertible.
- **Evidence anchors:**
  - [section] Theorem 1 and the discussion following it in Section III.A explicitly link $D_0$ to the convergence rate.
  - [section] Section III.B notes that ASAM and FisherSAM are "on the edge of divergence" due to potentially unbounded $D_0$.
- **Break condition:** If $\|D_t^{-1}\|$ is unbounded (e.g., weights $\to 0$ in ASAM) and $\rho$ is not carefully tuned, convergence is theoretically guaranteed to slow or fail.

## Foundational Learning

- **Concept:** Sharpness-Aware Minimization (SAM)
  - **Why needed here:** The entire paper is a generalization of SAM. You must understand the original minimax objective (`max ||eps||<=rho f(x+eps)`) and the linear approximation used to derive the standard update step.
  - **Quick check question:** Can you derive the standard SAM update $\epsilon = \rho \nabla f / \|\nabla f\|$ from the linear approximation of the inner maximization problem?

- **Concept:** First-Order Optimization Preconditioning
  - **Why needed here:** The paper introduces PreSAM using matrices $C_t$ and $D_t$. You need to grasp how preconditioning changes the "geometry" of the update (e.g., distorting the gradient direction or the constraint ball) to accelerate convergence or improve generalization.
  - **Quick check question:** If a preconditioner $P$ scales dimension $i$ by 10 and dimension $j$ by 0.1, how does the effective learning rate compare between these two dimensions?

- **Concept:** Stochastic Gradient Noise & Variance
  - **Why needed here:** The novel InfoSAM algorithm relies on estimating the variance of the stochastic gradient to adjust the update. Understanding the difference between the true gradient $\nabla f$ and the stochastic estimate $g_t$ is critical for the AMD mechanism.
  - **Quick check question:** Why does using a mini-batch size of 1 typically result in higher gradient variance than a batch size of 256, and how might this affect a noise-sensitive algorithm like SAM?

## Architecture Onboarding

- **Component map:** Data batch $\mathcal{B}_t$ -> Compute stochastic gradient $g_t$ -> Update EMA $m_t$ -> Estimate noise variance $\hat{\sigma}^2_t$ -> Construct Preconditioner $C_t$ -> Calculate perturbation $\epsilon_t$ -> Compute gradient $g_t(x_t + \epsilon_t)$ -> Update parameters

- **Critical path:**
  1. Compute stochastic gradient $g_t(x_t)$.
  2. Update EMA $m_t$ and estimate noise variance $\hat{\sigma}^2_t$.
  3. Construct Preconditioner $C_t = \hat{\Sigma}_t^{-1}$.
  4. Calculate perturbation $\epsilon_t \propto C_t g_t$.
  5. Compute gradient $g_t(x_t + \epsilon_t)$ and step.

- **Design tradeoffs:**
  - **CP vs. OP:** CP (changing constraint) offers scale invariance but complicates convergence and requires careful $\rho$ tuning. OP (changing objective) is more flexible and robust to hyperparameters but may not enforce strict scale invariance.
  - **Memory:** InfoSAM requires maintaining an EMA of gradients (memory cost = model size), similar to Adam, unlike vanilla SAM which is memory-free (beyond gradients).

- **Failure signatures:**
  - **Divergence in CP:** If using CP variants (ASAM) and loss spikes, check for vanishing weights/gradients causing unbounded preconditioner norms.
  - **Stagnation in InfoSAM:** If the model underfits, the noise variance estimate might be dominating, causing the preconditioner to zero-out updates excessively.

- **First 3 experiments:**
  1. **Baseline Verification:** Train ResNet-18 on CIFAR-10 with Vanilla SAM vs. PreSAM (with $C=D=I$) to ensure the unified framework reproduces standard results.
  2. **AMD Ablation:** Implement InfoSAM on a noisy dataset (e.g., CIFAR-10 with 50% label noise) to test the hypothesis that noise-estimated preconditioning improves generalization under high variance.
  3. **Preconditioner Analysis:** Visualize the norm of $D_t^{-1}$ for a CP method (like ASAM) vs. $C_t$ for InfoSAM during training to verify theoretical predictions about stability and boundedness.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can InfoSAM be effectively combined with Constraint Preconditioning (CP) methods like ASAM and FisherSAM to improve performance?
- Basis in paper: [explicit] The authors explicitly state that using infoSAM jointly with CP methods "has been added to our future research agenda."
- Why unresolved: While the framework theoretically supports separate preconditioners for the objective and constraint, the interaction between InfoSAM's noise-adjusted objective and CP's adaptive constraints has not been explored.
- What evidence would resolve it: Empirical results demonstrating that the combined algorithm outperforms standard InfoSAM or CP methods alone on benchmarks like ImageNet or CIFAR-100.

### Open Question 2
- Question: Can a principled, automated mechanism be developed to select the perturbation radius ($\rho$) for Constraint Preconditioning (CP) variants?
- Basis in paper: [inferred] The paper notes that for CP methods like ASAM, convergence requires $\rho$ to scale inversely with the preconditioner, yet currently, determining the best $\rho$ requires "extra effort" and empirical trial-and-error.
- Why unresolved: The unifying analysis suggests CP requires specific $\rho$ tuning that differs from standard SAM, but the paper does not provide a theoretical or automated method to determine this value without manual search.
- What evidence would resolve it: A derivation showing how $\rho$ should adapt to $D_0$ or the local landscape, validated by experiments where the adaptive $\rho$ matches or outperforms extensively tuned static values.

### Open Question 3
- Question: What are the convergence and generalization properties when utilizing a cascade of multiple objective preconditioners?
- Basis in paper: [inferred] The text mentions it is "possible to equip an adversarial model with multiple desired properties through a cascade of preconditioners," but this remains a suggested possibility without analysis.
- Why unresolved: The paper analyzes single preconditioners (OP or CP), but does not investigate if chaining them (e.g., $C_t = \prod C_{t,i}$) preserves the convergence guarantees or introduces instability.
- What evidence would resolve it: An extension of Theorem 1 covering composite preconditioners, alongside experiments testing a combined algorithm (e.g., a sparse, noise-adjusted SAM).

## Limitations

- The noise estimation mechanism in InfoSAM relies on empirical variance tracking, which may fail under non-stationary distributions or extreme label noise.
- Constraint Preconditioning (CP) variants require careful hyperparameter tuning of the perturbation radius $\rho$, as their convergence is sensitive to the norm of $D_t^{-1}$.
- The theoretical convergence guarantees for CP methods depend on boundedness assumptions that may not hold in practice when weights approach zero.

## Confidence

- **High Confidence:** The unification framework itself (Section III) and the categorization of SAM variants as OP vs. CP are well-established and clearly supported by the mathematical formulation and Table I.
- **Medium Confidence:** The convergence analysis in Theorem 1 for the PreSAM framework is sound, but its practical implications depend on the boundedness of $D_t^{-1}$, which is difficult to guarantee in practice for certain CP variants.
- **Low Confidence:** The specific mechanism by which InfoSAM's noise-preconditioned updates lead to better generalization is plausible but not rigorously proven; the connection between variance-based preconditioning and "reliable" gradient directions is an assumption rather than a derived result.

## Next Checks

1. **Numerical Stability Test:** Implement InfoSAM with multiple epsilon-floor values (e.g., 1e-8, 1e-6, 1e-4) and measure training stability and final accuracy. A large sensitivity to this hyperparameter would indicate a significant implementation challenge.

2. **Convergence Rate Analysis:** Train InfoSAM and a CP variant (like ASAM) on CIFAR-10 while logging the norms of their respective preconditioners ($\|C_t\|$ and $\|D_t^{-1}\|$). Verify that InfoSAM maintains a stable, bounded norm while CP exhibits growth, as predicted by the theory.

3. **AMD Ablation Study:** Train InfoSAM and Vanilla SAM on CIFAR-10 with artificially injected label noise (e.g., 10%, 30%, 50%). If InfoSAM's advantage over SAM increases with noise level, it provides strong empirical support for the AMD mitigation claim.