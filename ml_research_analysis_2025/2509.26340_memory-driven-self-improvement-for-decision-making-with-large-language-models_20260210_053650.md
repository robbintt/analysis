---
ver: rpa2
title: Memory-Driven Self-Improvement for Decision Making with Large Language Models
arxiv_id: '2509.26340'
source_url: https://arxiv.org/abs/2509.26340
tags:
- action
- prior
- memory
- memory-driven
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting large language models
  (LLMs) to specific sequential decision-making (SDM) tasks with limited task-related
  data. The authors propose a memory-driven self-improvement framework that combines
  LLM general prior knowledge with a compact memory of domain-specific experiences.
---

# Memory-Driven Self-Improvement for Decision Making with Large Language Models

## Quick Facts
- **arXiv ID:** 2509.26340
- **Source URL:** https://arxiv.org/abs/2509.26340
- **Reference count:** 28
- **Primary result:** Memory-driven self-improvement framework achieves >40% improvement on in-distribution tasks and >75% improvement on unseen tasks in ALFWorld

## Executive Summary
This paper addresses the challenge of adapting large language models (LLMs) to sequential decision-making tasks with limited task-specific data. The authors propose a memory-driven self-improvement framework that combines LLM general prior knowledge with a compact memory of domain-specific experiences. The core innovation is a self-reinforcing loop where memory-driven value estimation guides exploration while periodic LLM prior refinement updates the decision policy using historical state-action pairs. Experiments on ALFWorld and Overcooked demonstrate significant performance improvements over traditional RL and LLM-based baselines, with the method showing superior sample efficiency and generalization ability.

## Method Summary
The proposed framework operates through two complementary mechanisms: memory-driven value estimation and LLM prior refinement. During inference, the Mem-Q module uses a BERT-based encoder to embed state-action pairs, retrieves top-M similar entries from memory, and estimates Q-values via kernel-weighted averaging to select actions. After each episode, memory is updated with trajectory returns. Periodically, the Mem-EM module fine-tunes the LLM (Qwen2.5-3B/7B) using LoRA with a weighted language modeling loss derived from stored Q-values. This creates a self-improvement loop where memory and LLM prior mutually reinforce each other, enabling the system to learn from limited interactions while leveraging the LLM's general reasoning capabilities.

## Key Results
- Achieves over 40% improvement on in-distribution tasks in ALFWorld compared to traditional RL and LLM-based baselines
- Demonstrates over 75% improvement when generalized to unseen tasks in ALFWorld
- Shows superior sample efficiency, requiring only six LoRA fine-tuning steps during training
- Outperforms baselines on both ALFWorld (text-based interactive environment) and Overcooked (cooking simulation)

## Why This Works (Mechanism)
The framework succeeds by creating a symbiotic relationship between memory-based Q-learning and LLM prior refinement. Memory provides compact, domain-specific value estimates that guide exploration and enable sample-efficient learning, while the LLM contributes general reasoning capabilities and language understanding. The EM-style alternating optimization ensures that the memory stays relevant to the current policy while the policy benefits from historical experiences. This addresses the key limitation of LLMs in SDM tasks: their inability to accurately estimate action values without extensive task-specific fine-tuning.

## Foundational Learning
**Sequential Decision Making (SDM):** Decision-making processes where actions have long-term consequences across multiple time steps. Needed to understand the problem domain and evaluation metrics. Quick check: Can you identify the state, action, and reward structure in the ALFWorld environment?

**Expectation-Maximization (EM) Algorithm:** An iterative optimization method alternating between estimating expected values and maximizing objective functions. Needed to understand the alternating Mem-Q and Mem-EM training procedure. Quick check: Can you trace how the EM loop alternates between Q-value estimation and policy refinement?

**Memory-based Value Estimation:** Using stored experiences to estimate Q-values through similarity-based retrieval rather than function approximation. Needed to grasp how the framework achieves sample efficiency. Quick check: Can you explain how kernel-weighted averaging of retrieved Q-values produces a value estimate for the current state-action pair?

**LoRA Fine-tuning:** Low-Rank Adaptation technique for efficient model updates using small trainable matrices. Needed to understand the practical implementation of LLM prior refinement. Quick check: Can you identify where LoRA is applied in the training procedure and why it's preferred over full fine-tuning?

## Architecture Onboarding

**Component Map:** State/Action -> BERT Encoder -> Memory Table -> Q-estimator -> Action Selector; Memory Table -> LoRA Fine-tuner -> LLM Prior

**Critical Path:** During inference: State/Action → BERT Encoder → Memory Retrieval → Q-estimation → Action Selection. During training: Memory Update → LoRA Fine-tuning → LLM Prior Update.

**Design Tradeoffs:** The framework trades computational overhead of memory retrieval and periodic fine-tuning against sample efficiency and generalization. Using a frozen BERT encoder avoids catastrophic forgetting but may limit representation learning compared to fine-tuning the encoder.

**Failure Signatures:** High action invalidity rate indicates poor action projection function design. Unstable training or poor generalization suggests memory retrieval quality issues or insufficient LLM prior refinement.

**First Experiments:** 1) Implement and test action projection function with logging of parse success rates. 2) Visualize memory embedding space using t-SNE to verify clustering of high-return state-actions. 3) Conduct ablation study comparing fixed BERT embeddings versus fine-tuned LLM embeddings for memory retrieval.

## Open Questions the Paper Calls Out
**Open Question 1:** How can the framework be adapted to handle free-form or infinite action spaces? The current implementation relies on discrete action sets, requiring modification for continuous or unbounded action representations.

**Open Question 2:** Can the framework be integrated with vision-language models to enable decision-making in multimodal environments? The current architecture is designed for purely textual MDPs and may need adaptation for visual observations.

**Open Question 3:** Does replacing the static BERT-base encoder with representations from the fine-tuned policy LLM improve generalization capability? The paper notes poor Q-estimator generalization to unseen tasks, potentially due to misaligned static embeddings.

## Limitations
- Critical implementation details like action projection function and embedding concatenation strategy are underspecified
- Cold-start problem handling when memory is initially empty is not adequately addressed
- Ablation studies do not sufficiently isolate contributions of memory-driven Q-learning versus LLM prior refinement components

## Confidence

**High Confidence:** The overall conceptual framework combining memory-based Q-learning with LLM prior refinement is sound and well-motivated, with compelling experimental results.

**Medium Confidence:** Specific implementation details for action projection and embedding strategies are critical but underspecified, potentially affecting reproducibility.

**Low Confidence:** The paper lacks adequate discussion of cold-start handling and sufficient ablation studies to isolate component contributions.

## Next Checks
1. **Action Projection Robustness:** Implement and test multiple versions of the rule-based action projection function, including fuzzy matching and substring search, to determine which approach yields the highest parse success rate while maintaining action validity.

2. **Embedding Concatenation Strategy:** Systematically test different state-action embedding concatenation strategies (e.g., with/without separator tokens, different ordering) and measure their impact on memory retrieval quality and overall performance.

3. **Cold-Start Analysis:** Implement and evaluate a simple exploration strategy for the initial episodes when memory is empty, such as epsilon-greedy action selection, and measure how quickly the method can bootstrap effective policies compared to running with empty memory.