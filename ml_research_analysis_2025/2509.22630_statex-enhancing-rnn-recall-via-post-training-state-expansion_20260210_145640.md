---
ver: rpa2
title: 'StateX: Enhancing RNN Recall via Post-training State Expansion'
arxiv_id: '2509.22630'
source_url: https://arxiv.org/abs/2509.22630
tags:
- state
- statex
- rnns
- mamba2
- size
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces StateX, a post-training method to expand
  the state size of pre-trained recurrent neural networks (RNNs) like gated linear
  attention (GLA) and Mamba2. The core idea is to increase state capacity through
  architectural modifications before long-context post-training, thereby improving
  recall ability without incurring high training costs.
---

# StateX: Enhancing RNN Recall via Post-training State Expansion

## Quick Facts
- arXiv ID: 2509.22630
- Source URL: https://arxiv.org/abs/2509.22630
- Reference count: 17
- StateX improves recall-intensive task performance by expanding RNN state size post-training

## Executive Summary
StateX is a post-training method that expands the recurrent state size of pre-trained RNN models like GLA and Mamba2 to improve recall ability on long-context tasks. The approach modifies the architecture by increasing state capacity through head consolidation or key dimension expansion, then performs long-context post-training with selective parameter reinitialization. Experiments show StateX significantly improves recall accuracy (e.g., +3.36% on average for GLA) while maintaining common-sense reasoning performance, offering a parameter-efficient alternative to training from scratch.

## Method Summary
StateX works by first expanding the recurrent state of pre-trained RNN models - for GLA this means merging multiple attention heads into a single larger head, and for Mamba2 this involves expanding the key dimension. After architectural modification, the model undergoes long-context post-training with a critical design choice: reinitializing token-mixing parameters (like attention/SSM blocks) while preserving knowledge modules (FFN and embeddings). The method also employs selective layer expansion, modifying only a subset of layers to balance capacity gains against optimization difficulty.

## Key Results
- StateX improves recall task performance by 3.36% on average for GLA models
- NIAH long-context retrieval accuracy improves from 26.0% to 42.2% for GLA
- StateX maintains common-sense reasoning performance while enhancing in-context learning (+7.2% for GLA)
- The approach achieves these gains with minimal parameter increase and lower cost than training from scratch

## Why This Works (Mechanism)

### Mechanism 1: State Capacity Scaling via Head Consolidation
Increasing the recurrent state size allows RNNs to store more contextual information, thereby improving recall accuracy on long-context tasks. In GLA architectures, the recurrent state is partitioned across multiple heads; StateX merges these heads into a single larger head, increasing total state parameter count by a factor of H (e.g., 4× for a 4-head model). This consolidation "unlocks" non-diagonal regions of the state matrix for storage.

### Mechanism 2: Selective Parameter Reinitialization
Reinitializing token-mixing parameters while preserving knowledge modules facilitates better adaptation to the expanded state. Pre-trained weights in recurrent layers are optimized for a smaller state structure; inheriting them constrains the model to using only original capacity. Reinitializing forces the model to learn an update/query mechanism suited for the larger state, while frozen FFNs preserve world knowledge.

### Mechanism 3: Sparse Layer Expansion
Expanding the state in only a subset of layers (e.g., 4 out of 24) balances the trade-off between state capacity and post-training optimization difficulty. Modifying all layers introduces too many random parameters, creating a distribution shift that cannot be resolved with limited post-training tokens. Uniformly spacing expanded layers allows utilization of larger states for recall without destabilizing the entire network.

## Foundational Learning

- **Recurrent State Bottleneck**: RNNs compress all history into a fixed-size matrix S_t; understanding this bottleneck is crucial for grasping why expanding this matrix improves recall. *Quick check*: Does increasing sequence length increase memory usage of a standard RNN during inference?

- **Parameter Efficient Fine-Tuning (PEFT)**: StateX modifies architecture but trains on limited data; understanding the distinction between "freezing" (FFN) and "reinitializing" (Recurrence) is critical for correct implementation. *Quick check*: Why might keeping FFN weights fixed while reinitializing Attention weights be a good strategy when adapting a pre-trained model?

- **Multi-Head Attention Mechanism**: The GLA modification relies on mathematically merging multiple attention heads; you need to visualize how H matrices of size d_k × d_v become one matrix of size (H d_k) × (H d_v). *Quick check*: If you have 4 heads with key dimension 64 and value dimension 64, what is the dimension of the merged single-head state?

## Architecture Onboarding

- **Component map**: Input (SlimPajama corpus) -> Pre-trained Backbone (GLA-1.3B or Mamba2-1.3B) -> StateX Wrapper (Layer Selector → Head Merger/Dim Expander) -> Training (Long-context Post-training)

- **Critical path**: The initialization logic is the highest risk. You must strictly identify and reinitialize only the recurrent parameters (W_q, W_k, W_v, W_o, etc.) while cloning weights for FFN and Embedding layers. Failing to reinitialize recurrent weights results in the "Inherit" failure mode.

- **Design tradeoffs**: 
  - State Size vs. Inference Speed: Larger states improve recall but increase per-token computation cost
  - Stability vs. Capacity: Expanding more layers increases capacity but risks divergence during post-training

- **Failure signatures**:
  - Common-sense drop: If common-sense reasoning scores drop significantly, you likely failed to preserve FFN weights or expanded too many layers
  - Recall plateau: If NIAH scores don't improve, check if recurrent layers were accidentally inherited or if learning rate is too low

- **First 3 experiments**:
  1. Baseline Validation: Run standard LPT on vanilla model with no architectural changes
  2. Ablation on Initialization: Apply StateX (expand 4 layers) but inherit all weights vs. reinitialized weights on recall task
  3. Full Pipeline: Run full StateX pipeline and evaluate on Passkey Retrieval task up to 64K tokens

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations

- Task-Specific Generalization: Gains are most pronounced on recall-intensive tasks, suggesting specialization rather than universal improvement
- Architecture Dependency: Method is tailored specifically for RNN architectures with explicit state matrices, limiting applicability to standard Transformers
- Post-Training Data Requirements: Requires substantial high-quality post-training data (10B tokens) to properly adapt the expanded state

## Confidence

- **High Confidence**: State expansion through head merging directly increases state size; reinitializing recurrent parameters while preserving FFN weights prevents catastrophic forgetting; selective layer expansion optimizes capacity-stability tradeoff; StateX maintains common-sense reasoning while improving recall tasks

- **Medium Confidence**: Correlation between state size and recall ability generalizes across RNN architectures; specific layer indices (1, 6, 12, 18) are optimal for all recall tasks; 10B token post-training corpus is minimum required

- **Low Confidence**: StateX would provide similar benefits for RNNs larger than 1.3B parameters; approach would maintain efficiency advantages at scale; method generalizes to non-English languages without modifications

## Next Checks

1. Architecture Transfer Validation: Apply StateX modifications to a different RNN architecture (e.g., RWKV or another Mamba variant) to verify state expansion improves recall independent of specific architectural details

2. Scaling Behavior Analysis: Test StateX on models of varying sizes (0.5B, 3B, 7B parameters) to determine whether parameter-efficient advantages hold at different scales and identify any breakpoint where approach becomes less effective

3. Task Generalization Study: Evaluate StateX on broader range of tasks including mathematical reasoning, code generation, and multi-turn dialogue to determine whether recall improvements translate to general language understanding or remain task-specific