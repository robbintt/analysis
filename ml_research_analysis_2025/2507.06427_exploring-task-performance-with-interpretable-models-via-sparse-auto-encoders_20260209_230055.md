---
ver: rpa2
title: Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders
arxiv_id: '2507.06427'
source_url: https://arxiv.org/abs/2507.06427
tags:
- llms
- features
- mathematical
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the interpretability challenge of large language
  models (LLMs) by applying sparse autoencoders to decompose polysemantic neurons
  into monosemantic features. The authors use dictionary learning combined with automated
  interpretability techniques to extract and explain features, then apply this framework
  to improve downstream task performance in mathematical reasoning and metaphor detection.
---

# Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders

## Quick Facts
- arXiv ID: 2507.06427
- Source URL: https://arxiv.org/abs/2507.06427
- Reference count: 11
- Primary result: SAE-based ambiguity detection improves mathematical reasoning (+12.52%) and metaphor detection (+3.76%) across four LLMs.

## Executive Summary
This paper proposes using sparse autoencoders (SAEs) to decompose polysemantic LLM neurons into monosemantic features, then applies auto-interpretability to detect model-internal misunderstandings that degrade downstream task performance. The method is validated on mathematical reasoning and metaphor detection, showing statistically significant improvements across Llama 3, Mistral, Gemma, and Phi-3 models. The framework identifies symbol ambiguity (e.g., math operators mistaken for punctuation) and figurative language targets, enabling targeted prompt reformulation that enhances reasoning accuracy.

## Method Summary
The approach trains a single-layer sparse autoencoder with L1 regularization on frozen LLM activations, mapping MLP outputs to an overcomplete dictionary of sparse features. Auto-interpretability (GPT-4) generates human-readable labels for each feature. For downstream tasks, the system extracts top-activated features per token, detects mismatches between expected and actual feature domains, and triggers prompt reformulation via GPT-4. A second model (Gemini) verifies reformulation equivalence before re-running the original model. The pipeline is tested on mathematical reasoning (GSM8K, OpenMathInstruct) and metaphor detection (HFMET).

## Key Results
- Mathematical reasoning accuracy improved by an average of 12.52% (47.78% relative) across four LLMs
- Metaphor detection accuracy improved by an average of 3.76% (5.38% relative) across the same models
- Feature decomposition achieved reconstruction error <0.05 and sparsity <5% across all models
- The method identifies specific symbol ambiguities (e.g., "|" activating code-language features instead of math operations)

## Why This Works (Mechanism)

### Mechanism 1
Sparse autoencoders decompose polysemantic LLM neurons into monosemantic features through dictionary learning with L1 regularization. This forces sparse linear combinations rather than distributed representations, recovering the original feature basis under superposition.

### Mechanism 2
Auto-interpretability maps activated features to human-readable labels, revealing model-internal misunderstandings when feature labels mismatch expected domains (e.g., math symbols activating non-mathematical features).

### Mechanism 3
Targeted prompt reformulation based on detected ambiguities improves task performance. GPT-4 rephrases ambiguous inputs, Gemini verifies equivalence, and the reformulated input is re-fed to the original model.

## Foundational Learning

- **Polysemanticity vs. Monosemanticity**
  - Why needed here: Motivates SAE decomposition by explaining why single neurons encode multiple meanings
  - Quick check question: Can you explain why a single neuron activating on both "code syntax" and "math operators" complicates interpretation?

- **Dictionary Learning / Sparse Coding (Olshausen & Field)**
  - Why needed here: SAE training learns an overcomplete dictionary with sparsity penalty
  - Quick check question: What does the L1 term in the SAE loss enforce, and how does it differ from L2 regularization?

- **Superposition Hypothesis (Elhage et al.)**
  - Why needed here: Provides theoretical justification for why sparse decomposition might recover meaningful features
  - Quick check question: In superposition, how can a model represent more features than it has dimensions?

## Architecture Onboarding

- **Component map:** Frozen base LLM -> MLP activation extraction -> Sparse Autoencoder (8192 hidden) -> Auto-interpretability (GPT-4) -> Ambiguity detection -> Reformulation (GPT-4) -> Equivalence check (Gemini) -> Re-run original model

- **Critical path:** Train SAE on ~12B tokens -> Extract top-k features per token -> Compare feature labels against domain expectations -> Trigger reformulation if mismatch exceeds threshold -> Verify reformulation equivalence -> Re-run original model

- **Design tradeoffs:** 8192 hidden size (4Ã— MLP) balances feature granularity vs. compute; selective reformulation reduces cost but may miss edge cases; GPT-4 dependency enables high-quality reformulation but reduces scalability

- **Failure signatures:** Reconstruction error >0.05 or sparsity >5% indicates poor SAE learning; inconsistent auto-interpretation labels suggest unreliable feature descriptions; verification failures indicate semantic drift in reformulation

- **First 3 experiments:** 1) Validate SAE quality with reconstruction loss and sparsity metrics; 2) Spot-check 20-30 feature descriptions against human judgment; 3) A/B test reformulation on 100 math problems comparing original vs. reformulated accuracy

## Open Questions the Paper Calls Out

- Does the SAE-based ambiguity detection framework generalize to complex domains outside of mathematical reasoning and metaphor detection, such as legal or logical entailment tasks?
- Can the dependence on GPT-4 for auto-interpretability and reformulation be effectively replaced by smaller, open-source models without degrading task performance?
- To what extent does the specific choice of MLP layer affect the monosemanticity of extracted features and the success rate of ambiguity detection?

## Limitations

- Auto-interpretability via GPT-4 lacks direct human verification of feature descriptions at scale
- The method assumes ambiguities identified through feature-label mismatches are the primary cause of model errors
- Reformulation depends on external strong models (GPT-4), reducing scalability and introducing potential biases

## Confidence

- **High Confidence**: Statistical improvements in task performance are well-supported by reported results and follow logically from the SAE decomposition mechanism
- **Medium Confidence**: Interpretability of extracted features and validity of auto-generated labels are plausible but unverified against human judgment at scale
- **Low Confidence**: Claim that all performance gains are solely due to ambiguity correction is not fully validated; ablation against simpler reformulation methods is absent

## Next Checks

1. **Human Validation of Feature Interpretability**: Obtain human-generated descriptions for 100-200 SAE features and compute agreement against GPT-4 labels
2. **Ablation on Reformulation Source**: Repeat mathematical reasoning task with reformulations generated by fine-tuned T5 or LLaMA instead of GPT-4
3. **Negative Case Analysis**: Systematically analyze 50+ instances where reformulation failed or decreased accuracy to determine root causes