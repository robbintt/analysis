---
ver: rpa2
title: Understanding Sampler Stochasticity in Training Diffusion Models for RLHF
arxiv_id: '2510.10767'
source_url: https://arxiv.org/abs/2510.10767
tags:
- reward
- diffusion
- preprint
- arxiv
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of reward gap in diffusion models
  fine-tuned with RLHF, where stochastic samplers are used for training and deterministic
  samplers for inference. The authors theoretically bound the reward gap between SDE
  and ODE sampling and show that higher stochasticity during training improves ODE
  inference quality.
---

# Understanding Sampler Stochasticity in Training Diffusion Models for RLHF

## Quick Facts
- arXiv ID: 2510.10767
- Source URL: https://arxiv.org/abs/2510.10767
- Authors: Jiayuan Sheng; Hanyang Zhao; Haoxian Chen; David D. Yao; Wenpin Tang
- Reference count: 40
- Primary result: Higher stochasticity during RLHF fine-tuning (η > 1.0) improves ODE inference quality while narrowing the reward gap between SDE and ODE sampling

## Executive Summary
This paper addresses the fundamental challenge in RLHF for diffusion models: using stochastic samplers (SDEs) for training to ensure exploration while employing deterministic samplers (ODEs) for efficient inference. The authors theoretically characterize and bound the reward gap between these sampling regimes, showing that increased stochasticity during training (η > 1.0) improves ODE inference quality. Empirically, they validate these findings on large-scale text-to-image models (Stable Diffusion v1.5 and FLUX.1), demonstrating that reward gaps narrow over training and ODE sampling quality improves with higher-stochasticity SDE training.

## Method Summary
The authors develop a unified theoretical framework to analyze the reward gap between SDE and ODE sampling in RLHF-trained diffusion models. They introduce gDDIM, a generalized discretization scheme that supports arbitrary stochasticity levels η ≥ 0, enabling consistent comparison between training (η > 1.0) and inference (η = 0). The method involves theoretical bounding of Wasserstein distances between SDE and ODE distributions using score function properties, followed by empirical validation through fine-tuning experiments with multiple reward functions and RLHF optimizers (DDPO and MixGRPO).

## Key Results
- The reward gap between SDE-trained and ODE-sampled outputs is bounded and shrinks as denoising time horizon T increases (O(1/T) for VE, O(e^{-T²/2}) for VP models)
- ODE sampling quality improves with higher-stochasticity SDE training (η = 1.2 outperforms η = 1.0 and η = 1.5)
- Reward gaps consistently narrow over training steps before stabilizing or collapsing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The reward gap between SDE-trained and ODE-sampled outputs is bounded and shrinks as T increases
- Mechanism: SDE and ODE trajectories converge toward similar terminal means due to score function Lipschitz continuity, formalized via Gronwall's inequality on Wasserstein-2 distance
- Core assumption: Score function s_θ(t, y) is L-Lipschitz with bounded L² norm; for O(η) bounds, strong log-concavity is required
- Evidence anchors: Theoretical bounds ∆η ≤ (2T)^{-1} for VE and ∆η ≤ e^{-T²/2} for VP under quadratic rewards; experimental gap narrowing observed
- Break condition: Bounds become vacuous for small T, large step sizes, or poor score approximation

### Mechanism 2
- Claim: Higher stochasticity during training improves ODE inference quality
- Mechanism: Greater noise injection encourages exploration of high-reward regions; learned score representation transfers to deterministic ODE inference
- Core assumption: gDDIM preserves terminal marginals for any η ≥ 0; reward function smoothness enables exploration benefits
- Evidence anchors: η = 1.2 consistently outperforms η = 1.0 and η = 1.5 across ImageReward and PickScore metrics; MixGRPO with η = 1.2 shows better prompt alignment
- Break condition: Performance degrades at very high η (e.g., 1.5); excessive stochasticity may cause reward collapse

### Mechanism 3
- Claim: Reward gap narrows over RLHF training
- Mechanism: As fine-tuning improves image quality, both SDE and ODE samples converge toward higher-reward regions, reducing distributional divergence
- Core assumption: RLHF optimization does not introduce trajectory-specific overfitting; KL regularization constrains distribution shift
- Evidence anchors: ImageReward gap drops from 0.160 to 0.006 over 1400 steps with η = 1.2; visual confirmation across multiple reward functions
- Break condition: Gap widening may occur with reward hacking, training past optimal stopping, or overly simple prompts

## Foundational Learning

- **Stochastic Differential Equations (SDEs) vs. Ordinary Differential Equations (ODEs) in diffusion**
  - Why needed: Understanding why training uses SDEs for exploration while inference uses ODEs for efficiency, and their distributional consequences
  - Quick check: Can you explain why SDE and ODE yield the same terminal marginal for pretrained models but diverge after RLHF fine-tuning?

- **Score matching and score functions**
  - Why needed: Theoretical analysis depends on score function properties (Lipschitz continuity, L² bounds) for bounding Wasserstein distance
  - Quick check: What does it mean for a score function to be L-Lipschitz, and why does this matter for bounding Wasserstein distance?

- **RLHF objectives with KL regularization**
  - Why needed: Fine-tuning objective F_η(θ) = E[r(Y^θ_T)] - β·KL(Y^θ_T || Y^{REF}_T) defines the optimization landscape affecting reward gap
  - Quick check: How does increasing β affect reward gap bound tightness, and what tradeoff does it introduce?

## Architecture Onboarding

- **Component map**: Base diffusion model -> gDDIM sampler (η ∈ {1.0, 1.2, 1.5}) -> RLHF optimizer (DDPO/MixGRPO) -> Reward models (ImageReward, PickScore, HPS v2, Aesthetic Score) -> Evaluation pipeline (SDE and ODE sampling paths)

- **Critical path**: 1) Implement gDDIM discretization for arbitrary η, 2) Validate terminal marginal preservation, 3) Run DDPO/MixGRPO with high stochasticity logging SDE rewards, 4) Switch to ODE sampling at checkpoints computing reward gap, 5) Identify optimal stopping before collapse

- **Design tradeoffs**: Higher η (1.5) offers more exploration but risks distribution shift and lower peak performance; lower η (1.0) is safer but potentially suboptimal; longer training narrows gap but risks collapse; comprehensive prompts reduce gaps but require more compute

- **Failure signatures**: Reward collapse (sudden drop at N ≈ 1476); divergent gaps (increasing rather than decreasing); gDDIM numerical issues for very high η (σ_t(η) computation may produce invalid values)

- **First 3 experiments**: 1) Baseline gap measurement: Fine-tune with η = 1.0 for 200 steps, compute ∆η at 50-step intervals across all four reward functions, 2) Stochasticity sweep: Repeat training with η ∈ {1.0, 1.2, 1.5} using ImageReward, plot I_η vs. training step, 3) Early stopping validation: Train to near-collapse, confirm minimum ∆η and maximum I_η occur before collapse

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical bounds assume idealized conditions (Lipschitz score, strong log-concavity) that may not hold exactly in high-dimensional image spaces
- Empirical validation is limited to three reward models and two base diffusion architectures, raising questions about generalization to other domains
- Observed reward collapse at high training steps suggests potential overfitting or reward hacking, but exact mechanism is not fully characterized

## Confidence

- **High confidence**: Theoretical convergence rates for VE/VP models (Mechanism 1) - backed by rigorous proofs and explicit bounds
- **Medium confidence**: Empirical finding that η = 1.2 optimizes ODE inference quality (Mechanism 2) - strong experimental support but sensitive to hyperparameter choices
- **Medium confidence**: Reward gap narrowing over training (Mechanism 3) - observed consistently but requires monitoring to prevent collapse

## Next Checks
1. **Cross-domain validation**: Apply the SDE-ODE gap analysis to text-to-text diffusion models (e.g., diffusion language models) to verify theoretical bounds hold beyond image domains
2. **Score function robustness test**: Intentionally degrade score network quality (via weight pruning or adversarial perturbations) and measure how this affects reward gap bounds and convergence rates
3. **Prompt complexity ablation**: Systematically vary prompt specificity and complexity while measuring both reward gap and I_η to establish quantitative relationship between prompt conditioning and SDE-ODE discrepancy