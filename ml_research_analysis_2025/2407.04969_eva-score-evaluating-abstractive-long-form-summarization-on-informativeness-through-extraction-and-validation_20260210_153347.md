---
ver: rpa2
title: 'EVA-Score: Evaluating Abstractive Long-form Summarization on Informativeness
  through Extraction and Validation'
arxiv_id: '2407.04969'
source_url: https://arxiv.org/abs/2407.04969
tags:
- summarization
- evaluation
- long-form
- metrics
- eva-score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: EVA-SCORE is a new metric for evaluating abstractive long-form
  summarization based on informativeness rather than surface similarity. It extracts
  atomic facts from both reference and candidate summaries, organizes candidate facts
  into logical chains to handle context dependencies, and uses document-level relation
  extraction to capture cross-sentence structure.
---

# EVA-Score: Evaluating Abstractive Long-form Summarization on Informativeness through Extraction and Validation

## Quick Facts
- **arXiv ID:** 2407.04969
- **Source URL:** https://arxiv.org/abs/2407.04969
- **Reference count:** 0
- **Key outcome:** EVA-SCORE achieves perfect system-level Spearman and Kendall correlations (1.0) with human judgments on multiple summarization datasets

## Executive Summary
EVA-SCORE is a new metric for evaluating abstractive long-form summarization based on informativeness rather than surface similarity. It extracts atomic facts from both reference and candidate summaries, organizes candidate facts into logical chains to handle context dependencies, and uses document-level relation extraction to capture cross-sentence structure. Validation is performed chain-by-chain using an LLM to determine entailment of each atomic fact. On multiple datasets, EVA-SCORE achieves perfect system-level Spearman and Kendall correlations (1.0) with human judgments, outperforming traditional metrics like ROUGE and BERTScore, as well as LLM-based judges.

## Method Summary
EVA-SCORE evaluates abstractive long-form summarization by decomposing summaries into atomic facts, organizing them into logical chains, and validating entailment through document-level relation extraction. The metric extracts atomic facts from both reference and candidate summaries using ChatGPT, then uses Mistral-7B-Instruct to infer pairwise NLI relations and assemble candidate facts into chains. Document-level relations are extracted via NER and GPT-4, with duplicates removed using BERT cosine similarity. Validation occurs chain-by-chain using an LLM to determine if each candidate fact is entailed by retrieved reference facts. The final score is an F1 harmonic mean of precision and recall over atomic fact entailment.

## Key Results
- Achieves perfect system-level Spearman (1.0) and Kendall (1.0) correlations with human judgments
- Outperforms traditional metrics like ROUGE and BERTScore, as well as LLM-based judges
- Re-evaluation of LLM summarizers reveals persistent gaps between model outputs and human-written summaries, especially on longer documents

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing summaries into atomic facts enables fine-grained, unit-level evaluation that surface-level metrics cannot achieve.
- **Mechanism:** An LLM extracts atomic facts (single-proposition statements) from both reference and candidate summaries. Each fact becomes independently verifiable, allowing the system to measure information overlap rather than lexical similarity.
- **Core assumption:** LLMs can reliably decompose complex sentences into discrete, proposition-level facts without information loss or distortion.
- **Evidence anchors:** [abstract] "It extracts atomic facts from both reference and candidate summaries"; [section 4.1] "Following [9], we prompt ChatGPT with the same template to extract atomic facts—concise, single-proposition statements—from Sum and dSum"

### Mechanism 2
- **Claim:** Organizing context-dependent facts into logic chains with masked antecedents allows unambiguous validation of cascaded information.
- **Mechanism:** The system detects when later facts depend on earlier ones using pairwise NLI between successive facts. Facts are reorganized into chains, and during validation, antecedents are masked so each step is tested atomically while preserving context dependencies.
- **Core assumption:** A 7B-parameter model (Mistral-7B-Instruct) can reliably infer NLI relations between adjacent facts to build coherent chains.
- **Evidence anchors:** [abstract] "organizes candidate facts into logical chains to handle context dependencies"; [section 4.2] "We use Mistral-7B-Instruct to infer pairwise NLI relations between successive facts and assemble chains"

### Mechanism 3
- **Claim:** Augmenting sentence-level facts with document-level relations captures cross-sentence structure that purely local extraction misses.
- **Mechanism:** NER identifies entities; GPT-4 extracts document-level relations (triples) from both summaries. Relations are paraphrased to natural language, deduplicated against sentence-level facts using BERT cosine similarity (threshold θ=0.65), and merged into the fact sets before validation.
- **Core assumption:** GPT-4 can distinguish document-level relations from sentence-level facts, and a static similarity threshold effectively removes duplicates.
- **Evidence anchors:** [abstract] "uses document-level relation extraction to capture cross-sentence structure"; [section 4.3] "To avoid duplicating sentence-level content, we remove overlaps with atomic facts using cosine similarity on BERT embeddings"

## Foundational Learning

- **Concept:** Natural Language Inference (NLI) / Entailment
  - **Why needed here:** The core validation step requires determining whether each candidate atomic fact is entailed by retrieved reference facts.
  - **Quick check question:** Given "The meeting was postponed to Friday" as reference, does "The meeting did not occur on the originally scheduled day" entail? (Answer: Yes, under standard NLI assumptions.)

- **Concept:** Information Extraction (IE) — Atomic Facts and Relation Extraction
  - **Why needed here:** EVA-SCORE's first two stages extract structured information (atomic facts, document-level relations) from unstructured text.
  - **Quick check question:** What is the difference between sentence-level fact extraction and document-level relation extraction? (Answer: Sentence-level targets propositions within single sentences; document-level captures cross-sentence relationships between entities.)

- **Concept:** Correlation Metrics (Spearman, Kendall, Pearson)
  - **Why needed here:** The paper claims system-level Spearman and Kendall correlations of 1.0 with human judgments.
  - **Quick check question:** Why might a metric achieve perfect Spearman correlation but lower Pearson correlation? (Answer: Spearman measures rank ordering; Pearson measures linear relationship. Perfect ranking doesn't guarantee linear score scaling.)

## Architecture Onboarding

- **Component map:**
  AFG (both summaries) → AFCG (candidate only) → DocRE (both summaries) → Merge into DocS/DocŜ → Validation (chain-by-chain, unit-by-unit) → Aggregate F1

- **Critical path:**
  AFG (both summaries) → AFCG (candidate only) → DocRE (both summaries) → Merge into DocS/DocŜ → Validation (chain-by-chain, unit-by-unit) → Aggregate F1
  - Validation is the bottleneck: each candidate fact requires retrieval + LLM call

- **Design tradeoffs:**
  - **LLM choice for extraction vs. validation:** Paper uses ChatGPT/GPT-4 for extraction (higher capability) and Mistral-7B for NLI/validation (cost-efficient). Assumption: 7B model sufficient for entailment given proper prompting.
  - **Deduplication threshold (θ=0.65):** Lower threshold removes more but risks losing genuine document-level relations; higher threshold keeps more but introduces noise. Table 4 shows accuracy vs. coverage tradeoff.
  - **Chain validation with masked antecedents:** Improves precision on each unit but requires correct chain construction upstream.

- **Failure signatures:**
  - **DocRE errors (Section 5.6):** GPT-4 conflates sentence-level and document-level relations → spurious or redundant facts enter validation
  - **Validation errors:** Static prompts may not handle edge cases; majority voting suggested as improvement
  - **Chain construction errors:** NLI misclassification → facts placed in wrong chains or chains broken incorrectly

- **First 3 experiments:**
  1. **Sanity check on atomic fact extraction:** Run AFG on 10 manually annotated examples; compare extracted facts against human-identified propositions. Verify granularity and coverage.
  2. **Ablation of DocRE component:** Run EVA-Score with and without document-level relations on a held-out subset; measure correlation change to quantify DocRE contribution.
  3. **Threshold sensitivity analysis:** Vary θ from 0.60 to 0.75 on validation set; plot accuracy vs. remaining rate (following Table 4 methodology) to confirm θ=0.65 is optimal for your domain.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can non-LLM-based relation extraction methods effectively distinguish between sentence-level and document-level relations to reduce extraction errors?
- **Basis in paper:** [explicit] Section 5.6 states, "GPT-4 struggles to differentiate between sentence-level and document-level relations," and suggests that "develop[ing] novel methods for relation extraction" is necessary to improve alignment with human judgments.
- **Why unresolved:** The current implementation relies on GPT-4 for document-level relation extraction (DocRE), which conflates hierarchical relations, but no alternative architectures were tested.
- **What evidence would resolve it:** A comparison of EVA-SCORE performance using fine-tuned structural extraction models versus the current GPT-4 prompt, measuring the reduction in hierarchical relation errors.

### Open Question 2
- **Question:** Does implementing majority voting or self-consistency in the validation phase significantly increase the reliability of the chain-by-chain entailment checks?
- **Basis in paper:** [explicit] Section 5.6 notes that the validation process "requires a high level of reasoning capability" and explicitly suggests that "metrics like majority voting... could enhance the reliability of the evaluations."
- **Why unresolved:** The paper identifies reasoning failures in the validation of "newly added information" but relies on single-pass inference from Mistral-7B-Instruct.
- **What evidence would resolve it:** An ablation study comparing the accuracy of the current single-pass validation against a majority voting setup (e.g., N>5 samples) on the error analysis dataset.

### Open Question 3
- **Question:** Can an adaptive or dynamic thresholding mechanism outperform the static cosine similarity threshold (0.65) for filtering redundant document-level relations?
- **Basis in paper:** [inferred] Section 5.6 identifies the "static threshold" as a source of error that may be "too strict or too lenient," and Section 5.7 shows a tradeoff between "Remaining Rate" and "Remaining Accuracy" across different static thresholds.
- **Why unresolved:** The authors manually selected a fixed threshold (0.65) to balance coverage and fidelity, but the optimal threshold likely varies based on document density or domain.
- **What evidence would resolve it:** An evaluation using a learned or adaptive threshold function that adjusts based on the local density of extracted facts, compared against the static 0.65 baseline for correlation with human scores.

### Open Question 4
- **Question:** Does EVA-SCORE retain high system-level correlation with human judgments when applied to generative tasks outside of summarization, such as long-form question answering?
- **Basis in paper:** [explicit] Section 6 concludes, "Because EVA-SCORE quantifies information overlap, it may extend beyond summarization to other evaluation scenarios."
- **Why unresolved:** The metric is designed around "reference" and "candidate" summaries; its effectiveness in open-ended generation where references are less constrained or structural equivalence is not the goal remains untested.
- **What evidence would resolve it:** Benchmarking EVA-SCORE on a long-form QA dataset (e.g., ELI5) against human preference labels to see if fact overlap remains the dominant factor for quality assessment.

## Limitations
- **DocRE errors:** GPT-4 struggles to distinguish sentence-level from document-level relations, producing redundant or noisy facts that enter validation
- **Static deduplication threshold:** The fixed cosine similarity threshold (0.65) may be too strict or too lenient depending on document density and domain
- **Validation LLM reliability:** Static prompts may not handle edge cases consistently, and the paper suggests majority voting as an improvement area

## Confidence
- **High confidence:** The decomposition of summaries into atomic facts for fine-grained evaluation, the use of F1-score aggregation, and the overall pipeline architecture.
- **Medium confidence:** The effectiveness of chain-based context handling and document-level relation extraction, given the identified error sources and need for prompt refinement.
- **Medium confidence:** The claimed perfect system-level correlations, pending replication on independent datasets and larger validation sets.

## Next Checks
1. **Extraction quality validation:** Run AFG on 10 manually annotated examples; compare extracted facts against human-identified propositions to verify granularity and coverage.
2. **DocRE contribution ablation:** Run EVA-Score with and without document-level relations on a held-out subset; measure correlation change to quantify DocRE contribution.
3. **Threshold sensitivity analysis:** Vary θ from 0.60 to 0.75 on validation set; plot accuracy vs. remaining rate to confirm θ=0.65 is optimal for your domain.