---
ver: rpa2
title: Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with
  Decoupled Prioritized Experience Replay
arxiv_id: '2512.05320'
source_url: https://arxiv.org/abs/2512.05320
tags:
- replay
- learning
- actor
- experience
- critic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Decoupled Prioritized Experience Replay (DPER),
  a novel approach to improve deep deterministic policy gradient algorithms by separately
  optimizing the sampling strategies for the actor and critic networks in actor-critic
  architectures. DPER recognizes that the actor and critic have different learning
  objectives and can benefit from distinct transition batches.
---

# Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay

## Quick Facts
- arXiv ID: 2512.05320
- Source URL: https://arxiv.org/abs/2512.05320
- Reference count: 37
- Primary result: DPER improves actor-critic stability by decoupling actor and critic sampling strategies, yielding consistent performance gains across six MuJoCo continuous control tasks

## Executive Summary
This paper introduces Decoupled Prioritized Experience Replay (DPER), a novel approach to improve deep deterministic policy gradient algorithms by separately optimizing the sampling strategies for the actor and critic networks in actor-critic architectures. DPER recognizes that the actor and critic have different learning objectives and can benefit from distinct transition batches. The method samples transitions for the critic using prioritized experience replay based on temporal difference error, while the actor is trained on batches selected to be more on-policy by minimizing KL divergence between the transition generator and the current policy. Experimental results show that DPER consistently outperforms both vanilla experience replay and standard prioritized experience replay across six MuJoCo continuous control tasks from OpenAI Gym.

## Method Summary
DPER modifies the data pipeline feeding both networks in actor-critic architectures by decoupling their sampling strategies. For the critic, it uses standard prioritized experience replay based on temporal difference error, which accelerates convergence by focusing on "surprising" transitions. For the actor, it samples K candidate batches and selects the one minimizing KL divergence between the transition-generating policy and the current policy, reducing off-policy instability. The algorithm estimates a Transition Generator distribution from action differences between stored actions and current policy predictions, selecting batches that better align with the current behavior policy. The method is integrated with TD3 and tested on six MuJoCo tasks with varying action dimensions.

## Key Results
- DPER consistently outperforms vanilla experience replay and standard prioritized experience replay across six MuJoCo continuous control tasks
- Small values of the candidate batch parameter K (2-3) yield significant performance improvements while maintaining computational efficiency
- The method provides the most pronounced benefits in higher-dimensional tasks, with minimal or no improvement in lower-dimensional tasks like LunarLanderContinuous-v2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling the sampling strategies for actor and critic networks improves learning stability by aligning each network's training data with its specific learning objective.
- Mechanism: The critic learns from transitions with high temporal difference (TD) error, which directly indicate poor value estimation and accelerate critic convergence. The actor learns from batches that more closely match the current policy, minimizing harmful gradient estimates caused by policy divergence between stored transitions and current behavior.
- Core assumption: Actor and critic have fundamentally different data requirements—critic benefits from "surprising" transitions while actor requires policy-consistent transitions for stable gradient estimates.
- Evidence anchors:
  - [abstract] "DPER recognizes that the actor and critic have different learning objectives and can benefit from distinct transition batches."
  - [section 3.1] "While PER supports faster convergence of the critic, it might adversely affect the actor's optimization trajectory."
  - [corpus] Related work (RPE-PER, Actor Prioritized Experience Replay) supports the broader finding that sampling strategy significantly impacts actor-critic performance, though DPER's specific decoupling approach is novel.
- Break condition: If actor and critic learning objectives become highly aligned (e.g., near-converged policies), the benefit of decoupled sampling diminishes.

### Mechanism 2
- Claim: Selecting actor batches by minimizing KL divergence between the transition-generating policy and current policy reduces off-policy instability.
- Mechanism: The algorithm estimates a "Transition Generator" distribution λ by computing the mean and covariance of action differences between stored actions and current policy predictions. Among K candidate batches, it selects the batch minimizing η = D_KL(N(μ_λ, Σ_λ) || N(0, σI)), which approximately corresponds to minimizing mean squared error between stored and predicted actions under diagonal covariance assumptions.
- Core assumption: The exploration noise (Gaussian with variance σ) represents an ideal on-policy reference, and action prediction error approximates policy divergence.
- Evidence anchors:
  - [section 4.1] "To evaluate the similarity between the agent's current behavior policy and the batch's original generating policy, we use the Kullback–Leibler (KL) divergence."
  - [section 4.1] "This formulation also aligns with minimizing the mean squared error between the stored actions and those predicted by the actor."
  - [corpus] Weak direct corpus support for KL-based batch selection; this appears to be a DPER-specific contribution.
- Break condition: If action noise is non-Gaussian, highly correlated across dimensions, or transition buffer contains primarily recent data, the KL approximation degrades.

### Mechanism 3
- Claim: Small values of the candidate batch parameter K (2-3) capture most performance benefits with minimal computational overhead.
- Mechanism: Rather than exhaustively searching the replay buffer, sampling K=2-3 candidate batches and selecting the lowest-KL batch provides sufficient on-policy alignment. The ablation study shows no monotonic relationship between K and performance, suggesting marginal returns diminish quickly.
- Core assumption: The replay buffer contains enough on-policy-like transitions that a small random sample will include at least one good candidate.
- Evidence anchors:
  - [section 6.2] "Small K (2–3) already captures most of the benefit of on-policy batch selection."
  - [appendix A] "A small value, i.e., K=2 or K=3, is sufficient to achieve competitive or superior performance across all tasks."
  - [corpus] No direct corpus evidence on K-parameter sensitivity; this is an empirical finding from the paper.
- Break condition: If the buffer contains mostly stale transitions from highly divergent policies, larger K may be required.

## Foundational Learning

- Concept: **Actor-Critic Architecture in Continuous Control**
  - Why needed here: DPER modifies the data pipeline feeding both networks; understanding their distinct roles (critic estimates Q-values, actor parameterizes policy) is essential to grasp why decoupling helps.
  - Quick check question: Can you explain why the critic needs action as input alongside state in DDPG-style algorithms?

- Concept: **Off-Policy Learning and Policy Divergence**
  - Why needed here: The core motivation is that stored transitions come from older policies; high divergence between behavior policy and current policy causes unstable actor updates.
  - Quick check question: In off-policy actor-critic, why does policy divergence specifically harm the actor more than the critic?

- Concept: **Prioritized Experience Replay (TD-Error Based)**
  - Why needed here: DPER uses PER for the critic component; understanding how priority p_i ∝ |δ_i| affects sampling distributions is necessary for implementation.
  - Quick check question: What is the purpose of the α bias-correction parameter in PER?

## Architecture Onboarding

- Component map:
  - Replay Buffer -> Critic Sampler (PER) -> Critic Update
  - Replay Buffer -> K-Candidate Selector -> KL Calculator -> Actor Sampler -> Actor Update
  - Transition Generator Estimator (μ_λ, Σ_λ from action differences)
  - KL Divergence Calculator (η = D_KL(N(μ_λ,Σ_λ)||N(0,σI)))
  - Base Algorithm: TD3 (Twin Critics, Clipped Double Q-Learning, Target Policy Smoothing, Delayed Policy Updates)

- Critical path:
  1. Transition collection → store in buffer with initial priority
  2. Critic update: sample via PER → compute TD targets → update both critics
  3. Actor update (every M steps): sample K candidate batches → compute KL for each → select argmin → update actor and targets

- Design tradeoffs:
  - **K value**: Higher K improves batch quality but adds linear computational overhead (~4000s per increment per 1M steps)
  - **Critic sampler choice**: PER vs. uniform—PER stronger in some tasks (Ant), uniform in others (Hopper, Walker2d); decoupling provides stability regardless
  - **KL approximation**: Diagonal covariance assumption simplifies computation but may miss action correlation structure

- Failure signatures:
  - **PER-only baseline fails to converge** in BipedalWalker-v3 and LunarLanderContinuous-v2 (Figure 1b, 1e): actor destabilized by unreliable critic estimates on high-TD transitions
  - **DPER matches but doesn't beat vanilla ER** in LunarLanderContinuous-v2: lower-dimensional tasks may not exhibit problematic off-policyness
  - **No monotonic K-performance relationship**: suggests approximation limitations or buffer composition effects

- First 3 experiments:
  1. **Reproduction baseline**: Implement DPER on TD3 with K=2 on HalfCheetah-v2; compare against vanilla ER and PER to verify reported ~15-20% return improvement
  2. **Ablation: Critic sampler swap**: Run uniform-DPER (uniform critic sampling + KL-based actor sampling) on 2-3 tasks to confirm decoupling—not PER specifically—drives stability
  3. **K sensitivity sweep**: Test K ∈ {1, 2, 3, 5, 10} on a single task with high action dimensionality (e.g., Ant-v2) to probe whether complex action spaces require larger K

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does DPER maintain its performance advantages when applied to off-policy algorithms with inherently stochastic policies, such as Soft Actor-Critic (SAC), where the exploration noise mechanism differs from the DDPG/TD3 framework?
- Basis in paper: [inferred] The method relies on modeling the "Transition Generator" based on additive noise (Eq. 16-20), and the authors claim DPER is "generalizable" to a "wide class" of algorithms, yet experiments are restricted to TD3.
- Why unresolved: The derivation of the KL divergence metric assumes a specific relationship between the policy and exploratory noise that may not hold for entropy-regularized or fully stochastic policies.
- What evidence would resolve it: Empirical evaluation of DPER integrated into stochastic actor-critic algorithms like SAC or D4PG on the same MuJoCo benchmarks.

### Open Question 2
- Question: Why does increasing the number of candidate batches ($K$) fail to yield monotonic performance improvements, and is this sensitivity related to the buffer size or the approximation of the Transition Generator?
- Basis in paper: [explicit] Appendix A notes that "we observe no clear, monotonic correlation between the value of K and the final performance," describing it as a "counter-intuitive result."
- Why unresolved: The authors hypothesize that the search window (max $K=5$) is too small relative to the buffer size, but the exact cause of the diminishing returns remains unidentified.
- What evidence would resolve it: Ablation studies varying the replay buffer size relative to $K$, or analysis of the variance in the estimated Transition Generator distribution.

### Open Question 3
- Question: Can the computational overhead of the candidate batch selection loop be reduced without compromising the stability provided by the decoupled sampling?
- Basis in paper: [explicit] Appendix C identifies a "computational overhead" where DPER ($K=2$) runs $\approx3\times$ slower than vanilla ER, acknowledging this as a "trade-off to consider."
- Why unresolved: The current implementation requires $K$ forward passes and covariance calculations per actor update, creating a bottleneck not present in standard replay methods.
- What evidence would resolve it: A modified DPER architecture using approximate or cached KL divergence calculations that achieves comparable returns with wall-clock times closer to standard PER.

## Limitations

- The KL divergence formulation assumes diagonal Gaussian approximations that may not hold in environments with correlated actions or non-Gaussian exploration strategies
- Performance gains are most pronounced in higher-dimensional tasks, with minimal or no improvement in lower-dimensional tasks like LunarLanderContinuous-v2
- The method introduces computational overhead (approximately 3x slower than vanilla ER) due to the candidate batch selection loop

## Confidence

- **High**: Decoupling actor and critic sampling strategies provides training stability benefits across multiple tasks
- **Medium**: Small K values (2-3) capture most performance gains while maintaining computational efficiency
- **Low**: The KL divergence formulation accurately captures policy divergence between stored and current policies

## Next Checks

1. Implement uniform-DPER (uniform critic sampling + KL-based actor sampling) to confirm that decoupling itself—not PER specifically—drives performance improvements
2. Test K sensitivity on high-dimensional tasks (e.g., Ant-v2) to verify whether complex action spaces require larger K values despite claims that K=2-3 suffices
3. Evaluate DPER with non-Gaussian exploration (e.g., Ornstein-Uhlenbeck noise) to test the robustness of the diagonal covariance approximation in the KL divergence calculation