---
ver: rpa2
title: 'ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling'
arxiv_id: '2508.00974'
source_url: https://arxiv.org/abs/2508.00974
tags:
- data
- manual
- stereo
- cycling
- labels
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of transferring a stereo-based
  automatic annotation method from treadmill running to ergometer cycling in infrared
  thermography. The core method involves training a semantic segmentation network
  using automatically generated stereo labels and fine-tuning it on a small manually
  annotated dataset.
---

# ThermoCycleNet: Stereo-based Thermogram Labeling for Model Transition to Cycling

## Quick Facts
- arXiv ID: 2508.00974
- Source URL: https://arxiv.org/abs/2508.00974
- Authors: Daniel Andrés López; Vincent Weber; Severin Zentgraf; Barlo Hillen; Perikles Simon; Elmar Schömer
- Reference count: 4
- One-line primary result: Pre-training on stereo-generated labels followed by fine-tuning on just 10% manual data improves semantic segmentation of calves in cycling thermograms (mean IoU 0.6076 vs 0.6752 for manual-only baseline).

## Executive Summary
This paper addresses the challenge of transferring a stereo-based automatic annotation method from treadmill running to ergometer cycling in infrared thermography. The core method involves training a semantic segmentation network using automatically generated stereo labels and fine-tuning it on a small manually annotated dataset. Five models were evaluated: one trained only on manual data, and four pre-trained with stereo labels and fine-tuned on 0%, 10%, 50%, or 100% of the manual data. Results show that pre-training with stereo labels followed by fine-tuning on just 10% of manual data significantly improves performance over using manual data alone, with further improvements at higher manual data fractions. The final ThermoCycleNet, trained on 1600 stereo-labeled cycling images and fine-tuned on 560 manually annotated images, achieved a mean IoU of 0.51 on a new test set, with left and right calf IoU scores of 0.75 and 0.72 respectively.

## Method Summary
The method combines automatic stereo-based label generation with manual fine-tuning to adapt semantic segmentation from treadmill running to cycling thermograms. A three-camera system (infrared, time-of-flight, visual) captures synchronized data. Labels are generated in the visual domain and transformed to the infrared domain using depth information and stereo geometry. Five model variants were evaluated: manual-only baseline, and stereo pre-training with 0%/10%/50%/100% manual fine-tuning. The final ThermoCycleNet model used 1600 stereo-labeled cycling images for pre-training and 560 manually annotated images for fine-tuning.

## Key Results
- Model S4 (stereo pre-training + 100% manual fine-tuning) achieved mean IoU of 0.7319 vs Model M (manual only) at 0.6752
- Model S2 (stereo pre-training + 10% manual fine-tuning) achieved mean IoU of 0.6076, approaching the manual-only baseline
- Final ThermoCycleNet achieved mean IoU of 0.51 on new test set with left calf IoU 0.75 and right calf IoU 0.72
- Right calf segmentation consistently showed lower IoU scores across all models (0.6231-0.8819) compared to left calf (0.7315-0.8744)

## Why This Works (Mechanism)

### Mechanism 1: Automatic Label Pre-training Enables Low-Data Domain Adaptation
Pre-training on noisy stereo-generated labels establishes useful feature representations that reduce the manual annotation burden for new domains. The stereo labeling pipeline generates large quantities of approximate labels in the visual domain, which are transformed to the infrared domain via depth-aware stereo projection. These labels, while lower quality, expose the network to domain-specific anatomical patterns before any manual labeling occurs.

### Mechanism 2: Fine-tuning with Limited Manual Labels Corrects Systematic Noise
A small fraction (10-50%) of high-quality manual labels is sufficient to correct systematic errors inherited from automatic stereo labels. The network first learns coarse domain representations from abundant noisy labels, then fine-tuning on precise manual annotations adjusts decision boundaries without catastrophic forgetting.

### Mechanism 3: Multimodal Stereo Transformation Bridges Visual and Thermal Domains
Depth information from time-of-flight sensors enables geometric projection of labels from visual to thermal imagery despite different camera viewpoints and modalities. The three-camera system captures synchronized data, allowing labels generated in the visual domain (where annotation is easier) to be mapped to thermal coordinates using known stereo geometry and depth maps.

## Foundational Learning

- **Semantic Segmentation with IoU Evaluation**
  - Why needed here: The task requires pixel-level classification of anatomical regions in thermal images; IoU quantifies overlap between predicted and ground truth regions.
  - Quick check question: Can you explain why mean IoU across all classes is lower than individual calf IoU scores in the ThermoCycleNet results?

- **Transfer Learning and Fine-tuning Strategies**
  - Why needed here: The core contribution depends on understanding how pre-training weights affect downstream task performance and why fine-tuning on small datasets can outperform training from scratch.
  - Quick check question: Why does Model S2 (stereo pre-training + 10% manual) underperform Model M (manual only) while S3 and S4 (50-100% manual) outperform it?

- **Stereo Vision and Cross-modal Calibration**
  - Why needed here: The label generation pipeline requires understanding how depth maps enable projection between camera coordinate systems and what calibration errors might propagate.
  - Quick check question: What would happen to transformed labels if the time-of-flight depth sensor systematically underestimates distance to the calf region?

## Architecture Onboarding

- **Component map**: Capture rig (IRT + ToF + RGB) -> Visual label generation -> Stereo transformation using depth -> Thermal domain labels -> Two-stage training (pre-train -> fine-tune) -> Inference on new thermograms

- **Critical path**: 1) Stereo calibration between all three cameras; 2) Automatic label generation in visual domain; 3) Depth-aware projection to thermal coordinates; 4) Manual annotation of subset for fine-tuning; 5) Two-stage network training

- **Design tradeoffs**: Stereo label quantity vs. quality (more labels increase coverage but may introduce noise); fine-tuning data fraction (10% achieves competitive results, 50%+ needed to surpass manual-only baseline); single-subject generalization (may limit deployment to body type variations)

- **Failure signatures**: Calf boundary drift (right calf consistently lower IoU than left); stereo-only collapse (Model S1 shows severe right calf degradation); domain shift on new test set (final model mean IoU drops to 0.51 despite high calf IoUs)

- **First 3 experiments**:
  1. **Calibration validation**: Project known visual markers into thermal space and measure pixel-level alignment error; if >5 pixels, recalibrate stereo rig before label generation.
  2. **Fine-tuning fraction sweep**: Replicate the S1-S4 experiment on your target domain with 0%, 5%, 10%, 25%, 50% manual data to find the minimal viable annotation budget.
  3. **Cross-subject generalization test**: Train on stereo labels from one subject, test on thermograms from different body types to assess whether single-subject pre-training limits deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does ThermoCycleNet generalize across diverse subjects with varying body compositions, given that the stereo-labeled cycling dataset was derived from a single case?
- Basis in paper: The paper states "The stereo-labeled cycling data set has 1600 thermograms from a single case," presenting a clear limitation in subject diversity.
- Why unresolved: No multi-subject validation was conducted for the cycling stereo-label transfer, leaving inter-individual variability untested.
- What evidence would resolve it: Evaluation of model performance on a cycling dataset with multiple subjects of varying body types, leg geometries, and thermal patterns.

### Open Question 2
- Question: What is the optimal ratio of stereo labels to manual labels for fine-tuning, and does a minimum effective threshold exist below which fine-tuning provides diminishing returns?
- Basis in paper: The study tested discrete fractions (0%, 10%, 50%, 100%) but did not systematically identify the optimal point or asymptotic behavior.
- Why unresolved: The experimental design used coarse intervals, leaving the precise trade-off curve between annotation effort and performance gain uncharacterized.
- What evidence would resolve it: A fine-grained sweep of manual label fractions with performance tracking to identify the knee point in the learning curve.

### Open Question 3
- Question: Can this stereo-labeling transfer methodology extend to other anatomical regions beyond calves, or to other exercise modalities such as rowing or swimming?
- Basis in paper: The authors state their aim was to "develop a new model to sufficiently segment moving legs in running and cycling," implicitly leaving other body parts and exercises unexplored.
- Why unresolved: The method relies on well-exposed calves during cycling; other regions may have different visibility, thermal signatures, or motion patterns affecting stereo label quality.
- What evidence would resolve it: Application of the same pipeline to additional body regions and exercise types with comparative IoU analysis.

## Limitations
- Single-subject training protocol raises concerns about generalizability to diverse body types and motion patterns
- Stereo transformation pipeline's sensitivity to calibration drift and depth estimation errors not thoroughly characterized
- Network architecture not specified, making it difficult to assess whether architectural choices contribute to performance gains

## Confidence
- **High confidence**: Experimental results showing improved performance when combining stereo pre-training with manual fine-tuning are well-supported by the ablation study
- **Medium confidence**: Claim that 10% manual data suffices for effective fine-tuning is supported but optimal fraction may vary by domain
- **Medium confidence**: Mechanism by which stereo labels enable domain adaptation is plausible given related work but lacks direct corpus support

## Next Checks
1. **Cross-subject validation**: Train ThermoCycleNet on stereo labels from Subject A and evaluate on thermograms from Subjects B, C, and D with varying body types. If mean IoU drops below 0.4, the single-subject pre-training approach requires expansion to multi-subject stereo datasets.

2. **Calibration sensitivity analysis**: Systematically degrade stereo calibration by introducing controlled angular offsets (1°, 3°, 5°) between cameras and measure propagated label boundary errors. If IoU drops >0.1 per degree of misalignment, implement automated calibration verification as a deployment prerequisite.

3. **Manual annotation efficiency test**: Time manual annotation of 100 thermogram calf regions with and without stereo pre-training initialization. If the pre-trained approach reduces annotation time by >30% while maintaining IoU ≥0.65, the method delivers practical efficiency gains beyond theoretical performance improvements.