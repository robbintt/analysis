---
ver: rpa2
title: 'Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments'
arxiv_id: '2510.06001'
source_url: https://arxiv.org/abs/2510.06001
tags:
- full
- filler
- subject
- direct
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges prior conclusions about LLM performance on
  parasitic gaps (PGs) by introducing a fine-grained minimal pair analysis. While
  Lan et al.
---

# Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments

## Quick Facts
- **arXiv ID:** 2510.06001
- **Source URL:** https://arxiv.org/abs/2510.06001
- **Authors:** Timothy Pistotti; Jason Brown; Michael Witbrock
- **Reference count:** 5
- **Primary result:** GPT-2 achieves 87.9% accuracy on parasitic gap DiD metric (vs 5.6% in prior work) and demonstrates robust knowledge via direct minimal pair analysis.

## Executive Summary
This paper challenges prior conclusions about LLM performance on parasitic gaps (PGs) by introducing a fine-grained minimal pair analysis. While Lan et al. (2024) found LLMs largely fail on PGs using a Difference-in-Differences (DiD) metric, this work shows that direct minimal pair comparisons (Wilcox-style "wh-effect" tests) reveal robust PG knowledge in GPT-2 across all tested configurations. Using a controlled 8-permutation PG dataset, GPT-2 achieved 87.9% accuracy on the DiD metric (up from 5.6% in prior work) and demonstrated statistically significant performance on four targeted licensing/violation contexts (p < .0001 for three, p = .0002 for one). The study argues that stimulus quality and metric choice critically shape conclusions about model syntactic competence, advocating for more interpretable tests to better assess LLM capabilities and inform debates about learnability.

## Method Summary
The paper evaluates GPT-2's knowledge of parasitic gaps using surprisal-based minimal pair comparisons versus Difference-in-Differences (DiD) metrics. Researchers generated an 8-permutation dataset (33 items × 8 conditions) with Gemini 2.5, manually vetting for pragmatic plausibility and grammaticality. For each sentence, they extracted BPE-level surprisals from GPT-2 and aggregated them for critical regions. They then calculated per-item surprisal differences for four minimal pair tests (P1-P4) and performed one-sample t-tests on mean differences. The primary metric was the significant wh-effect (S(+Filler) - S(-Filler)) in gap configurations, with secondary evaluation using Lan et al.'s (2024) Direct Preference and DiD accuracy measures.

## Key Results
- GPT-2 achieved 87.9% accuracy on DiD metric for parasitic gaps (vs 5.6% in prior work)
- Direct minimal pair comparisons revealed statistically significant performance across all four gap configurations (p < .0001 for three, p = .0002 for one)
- The 8-permutation paradigm (±filler, ±gap1, ±gap2) successfully isolated syntactic licensing effects from lexical confounds
- Manual vetting eliminated stimulus quality issues that may have suppressed performance in previous studies

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Direct minimal pair comparisons provide higher diagnostic transparency than DiD metrics when lexical baselines vary.
- **Mechanism:** DiD metrics control for lexical frequency differences statistically but conflate syntactic preference with baseline surprisal skews (e.g., comparing "soon" vs. "you" with 11.5-bit difference). Direct minimal pairs eliminate this noise by holding the critical region constant while manipulating only the structural licenser ("who" vs. "that"), uniquely attributing surprisal delta to syntactic knowledge.
- **Core assumption:** The model's surprisal at a critical region is primarily driven by structural fit rather than idiosyncratic lexical probabilities.
- **Evidence anchors:** Abstract advocates "direct minimal pair comparisons" to reveal robust knowledge; Section 2 demonstrates DiD's issues with skewed comparisons (-18.84 bits).

### Mechanism 2
- **Claim:** Refined stimulus quality and strict grammatical vetting improve measured model performance on complex syntax tasks.
- **Mechanism:** Prior failures (5.6% accuracy) may stem from "unintended lexical ambiguities and structural complexities" in the dataset rather than lack of model competence. The 8-permutation paradigm with manual vetting improves signal-to-noise ratio, allowing the model to demonstrate existing knowledge.
- **Core assumption:** Specific PG stimuli in prior work contained confounds that artificially suppressed model accuracy.
- **Evidence anchors:** Abstract notes GPT-2 accuracy rose from 5.6% to 87.9% with new dataset; Section 5 hypothesizes improvement due to stimuli being "more representative of canonical PGs."

### Mechanism 3
- **Claim:** GPT-2 encodes distinct, generalizable expectations for filler-gap licensing that penalize ungrammatical configurations and facilitate grammatical ones.
- **Mechanism:** The model modulates surprisal based on gap configuration, lowering surprisal when "who" filler licenses a gap (P1, P2) and increasing surprisal when "who" filler appears with no licensed gap (P3, P4). This bidirectional behavior indicates structural expectation rather than frequency bias.
- **Core assumption:** Surprisal differentials reflect grammatical competence rather than surface-level n-gram statistics.
- **Evidence anchors:** Section 4.2 reports significant negative effects for licensing (P1, P2) and significant positive effects for violations (P3, P4), with p < .0001 for three conditions.

## Foundational Learning

- **Concept:** **Surprisal (Information Density)**
  - **Why needed here:** The entire evaluation framework relies on surprisal as a proxy for "processing difficulty" or "ungrammaticality."
  - **Quick check question:** If a model assigns a probability of 0.1 to a word in context, what is the surprisal in bits? (Answer: -log2(0.1) ≈ 3.32 bits).

- **Concept:** **Minimal Pair Analysis**
  - **Why needed here:** This is the core methodological shift proposed by the paper. Valid minimal pairs differ by only one variable to control for confounds.
  - **Quick check question:** Why is comparing the surprisal of "soon" vs. "you" not a valid minimal pair for syntax? (Answer: They are different words with different lexical frequencies; the difference isn't purely syntactic).

- **Concept:** **Parasitic Gaps (PGs)**
  - **Why needed here:** The specific syntactic phenomenon under test. A PG is a secondary gap licensed by the same filler as a primary gap.
  - **Quick check question:** In the sentence "Who did you say _ likes _?", is the second gap a parasitic gap? (Answer: No, typically PGs appear inside islands like subject clauses; this is likely a Double Object construction).

## Architecture Onboarding

- **Component map:** Stimuli Generator (Gemini 2.5) -> Vetting Layer (Manual) -> Model Under Test (GPT-2) -> Surprisal Extractor -> Statistical Engine
- **Critical path:** The generation and vetting of the 8-permutation dataset. If stimuli are not perfectly controlled, the statistical significance of the wh-effect cannot be trusted.
- **Design tradeoffs:**
  - Direct vs. DiD Metrics: Direct comparison offers high interpretability but requires perfectly matched stimuli (hard). DiD allows sloppier stimuli but yields ambiguous results (easier, less insightful).
  - Automated vs. Manual Vetting: LLM generation risks hallucinating ungrammatical structures, necessitating time-consuming manual checks.
- **Failure signatures:**
  - Lexical Skew: Large baseline surprisal differences (>5 bits) between critical words indicate a broken minimal pair design.
  - Chance-Level Deltas: Mean effects close to 0 with high variance indicate the model fails to distinguish grammatical from ungrammatical structures.
  - Verb Selection Errors: Using non-bridge verbs (e.g., "believed") in 'who' clauses renders the 'grammatical' condition ungrammatical.
- **First 3 experiments:**
  1. Replicate P4 (Violation - No Gaps): Verify if the model consistently penalizes a "who" filler when both G1 and G2 are filled.
  2. Stimuli Ablation: Run evaluation on the original Lan et al. (2024) dataset using new Direct Minimal Pair code.
  3. Scale Test: Apply P1-P4 framework to larger models (Llama, GPT-3) to see if "robust knowledge" scales.

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertainty around stimulus quality improvements: exact extent of performance gain from metric choice versus dataset refinement remains unclear.
- Manual vetting process introduces subjectivity and may systematically simplify the task by excluding challenging but valid PG constructions.
- Conclusions about the learnability debate are speculative, as the work focuses on evaluation methodology rather than acquisition modeling.

## Confidence
- **High Confidence:** Direct minimal pair comparisons provide higher diagnostic transparency than DiD metrics.
- **Medium Confidence:** GPT-2 has "robust knowledge" of PGs, though this could be partially explained by refined stimulus quality.
- **Low Confidence:** Claims about "distinct, generalizable expectations" for licensing rely entirely on behavioral results without independent verification.

## Next Checks
1. **Metric Ablation Test:** Run the DiD analysis on the new 8-permutation dataset to isolate performance contribution of metric choice versus stimulus quality.
2. **Model Scaling Test:** Apply the P1-P4 framework to larger models (GPT-3, Llama) to determine if observed "robust knowledge" scales or if larger models introduce new biases.
3. **Stimulus Generalization Test:** Evaluate GPT-2 on a separate, independently generated PG dataset (not using Gemini 2.5) to test whether knowledge transfers beyond specific stimuli used in this study.