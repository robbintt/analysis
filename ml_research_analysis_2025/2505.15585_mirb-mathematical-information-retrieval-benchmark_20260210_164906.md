---
ver: rpa2
title: 'MIRB: Mathematical Information Retrieval Benchmark'
arxiv_id: '2505.15585'
source_url: https://arxiv.org/abs/2505.15585
tags:
- retrieval
- query
- mathematical
- question
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MIRB, a comprehensive benchmark for evaluating
  mathematical information retrieval (MIR) models across four tasks: semantic statement
  retrieval, question-answer retrieval, premise retrieval, and formula retrieval,
  spanning 12 diverse datasets. The authors evaluate 13 retrieval models and find
  that all models perform significantly worse on reasoning-based tasks (premise and
  question-answer retrieval) compared to semantic-based tasks.'
---

# MIRB: Mathematical Information Retrieval Benchmark

## Quick Facts
- arXiv ID: 2505.15585
- Source URL: https://arxiv.org/abs/2505.15585
- Reference count: 40
- 13 retrieval models evaluated across 4 MIR tasks on 12 datasets

## Executive Summary
This paper introduces MIRB, a comprehensive benchmark for evaluating mathematical information retrieval models across four tasks: semantic statement retrieval, question-answer retrieval, premise retrieval, and formula retrieval. The benchmark spans 12 diverse datasets ranging from 39 to 25,116 queries and 1,994 to 1,350,505 documents, including sources like MSE, MathOverflow, ProofWiki, and formal proof libraries. The authors evaluate 13 retrieval models, including BM25, open-source models under and over 1B parameters, and proprietary models. The primary metric is nDCG@10, with nDCG-prime used for ARQMath-Task-1 due to its dynamic corpus.

## Method Summary
The MIRB benchmark evaluates mathematical information retrieval models across four tasks using 12 datasets. Models are configured with specific context lengths and instructions per dataset, with dense models using cosine similarity for retrieval. The evaluation includes applying cross-encoder rerankers to top-10 results. Experiments are conducted on 8× NVIDIA A800 (80G) GPUs, with performance measured using nDCG@10 as the primary metric. The benchmark covers a range of mathematical content from informal Q&A to formal proofs in languages like Lean and Isabelle.

## Key Results
- All models perform significantly worse on reasoning-based tasks (premise and question-answer retrieval) compared to semantic-based tasks
- Applying cross-encoder rerankers to dense retrieval models generally leads to performance degradation rather than improvement
- The best-performing model, voyage-3-large, achieves an average nDCG@10 score of 54.54 but still struggles with formal premise retrieval tasks
- There is substantial room for improvement in MIR systems, particularly for reasoning-intensive tasks requiring understanding of mathematical proofs and formal logic

## Why This Works (Mechanism)
The benchmark design enables systematic evaluation of MIR models across diverse mathematical tasks by standardizing metrics and comparison conditions. By including both informal mathematical content (MSE, MathOverflow) and formal proof libraries (Lean, Isabelle), the benchmark reveals that current models struggle specifically with reasoning-intensive tasks. The performance degradation when applying cross-encoders suggests that dense retrieval models may already be capturing the most relevant features, and additional reranking introduces noise rather than refinement.

## Foundational Learning

**nDCG@10 (Normalized Discounted Cumulative Gain)**: Measures ranking quality by considering position and relevance of retrieved items. Needed to evaluate how well models rank relevant documents at the top of results. Quick check: Compare nDCG@1, nDCG@5, and nDCG@10 to see position sensitivity.

**Dense vs Sparse Retrieval**: Dense models use neural embeddings while sparse methods like BM25 use term matching. Needed to understand different retrieval paradigms in mathematical contexts. Quick check: Compare retrieval results when mathematical notation is heavily used vs plain text.

**Cross-encoder Reranking**: Takes query-document pairs as input to re-rank top results. Needed to potentially improve initial retrieval quality. Quick check: Analyze cases where reranking degrades performance to identify patterns.

**Formal Proof Languages (Lean, Isabelle)**: Interactive theorem proving languages used for mathematical proofs. Needed for evaluating retrieval in formal mathematical contexts. Quick check: Compare retrieval performance on formal vs informal mathematical content.

**Dynamic Corpus**: Collection that changes per query based on available annotations. Needed for ARQMath-Task-1 where only annotated documents are considered. Quick check: Verify corpus size changes appropriately per query.

## Architecture Onboarding

**Component Map**: Datasets -> Preprocessing -> Model Encoding -> Cosine Similarity -> Ranking -> Reranking (optional) -> nDCG Evaluation

**Critical Path**: Data preprocessing → Model inference → Similarity computation → Ranking → Evaluation

**Design Tradeoffs**: The benchmark balances comprehensiveness (12 datasets, 4 tasks) against practical evaluation constraints. Including formal proof languages adds difficulty but may limit model applicability. The choice to use nDCG@10 prioritizes top-ranked results but may miss broader retrieval quality.

**Failure Signatures**: Performance degradation on reasoning tasks indicates models lack mathematical reasoning capabilities. Reranker degradation suggests dense retrievers are already effective or rerankers are overfitting. Memory issues with large corpora indicate scalability limitations.

**3 First Experiments**:
1. Run BM25 baseline on all datasets to establish sparse retrieval performance
2. Evaluate a dense model (e.g., voyage-3-large) on semantic tasks only
3. Apply reranking to dense model results and compare with base retrieval

## Open Questions the Paper Calls Out
None specified in the provided materials.

## Limitations
- Benchmark focuses on binary and limited multi-level relevance judgments that may not capture nuanced mathematical retrieval quality
- Performance degradation with cross-encoders requires further investigation to determine root causes
- Memory constraints with large corpora (up to 1.3M documents) may limit evaluation of some models
- Preprocessing specifics for mathematical formulas and handling of multi-level relevance grades could introduce variability

## Confidence
High: Benchmark design, relative model performance ordering
Medium: Specific performance metrics, reranking results
Low: Generalizability of reranker effectiveness conclusions

## Next Checks
1. Conduct controlled experiments isolating the impact of instruction tuning and context length on model performance across all datasets
2. Perform ablation studies comparing different reranking strategies to understand why performance degrades
3. Validate the preprocessing pipeline for mathematical formulas and handling of multi-level relevance grades across all datasets