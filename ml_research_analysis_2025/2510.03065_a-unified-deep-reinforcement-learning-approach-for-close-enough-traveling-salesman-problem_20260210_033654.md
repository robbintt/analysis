---
ver: rpa2
title: A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman
  Problem
arxiv_id: '2510.03065'
source_url: https://arxiv.org/abs/2510.03065
tags:
- ud3rl
- problem
- node
- time
- cetsp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a unified deep reinforcement learning framework
  (UD3RL) for solving the Close-Enough Traveling Salesman Problem (CETSP), where an
  agent must visit neighborhoods around targets rather than specific points. The key
  idea is to formulate CETSP as a Markov Decision Process and use a dual-decoder architecture:
  one decoder selects the next node to visit, and another determines the precise waypoint
  within the chosen neighborhood.'
---

# A Unified Deep Reinforcement Learning Approach for Close Enough Traveling Salesman Problem

## Quick Facts
- arXiv ID: 2510.03065
- Source URL: https://arxiv.org/abs/2510.03065
- Reference count: 40
- One-line primary result: UD3RL outperforms conventional methods in CETSP solution quality and runtime while generalizing across problem sizes and radius types.

## Executive Summary
This paper introduces UD3RL, a deep reinforcement learning framework for the Close-Enough Traveling Salesman Problem (CETSP), where an agent must visit neighborhoods around targets rather than specific points. The method uses a dual-decoder architecture: one decoder selects the next node to visit, and another determines the precise waypoint within the chosen neighborhood. A k-nearest neighbors subgraph interaction strategy enhances spatial reasoning. Experimental results show UD3RL outperforms conventional methods in both solution quality and runtime, while demonstrating strong generalization to larger problem scales, alternative node distributions, and varying radius ranges.

## Method Summary
UD3RL formulates CETSP as a Markov Decision Process using a unified Transformer-based encoder with dual decoders. The Node-Decoder selects which neighborhood to visit next, while the Loc-Decoder determines the precise entry point on the neighborhood boundary using a k-NN subgraph context. The method employs Perimetral Discretization Scheme (PDS) to discretize circular neighborhoods into boundary points. Training uses a customized REINFORCE algorithm with curriculum-like sampling across different problem sizes and radius types. The model is evaluated using greedy decoding with 8 geometric augmentations (rotations/flips).

## Key Results
- UD3RL achieves superior solution quality compared to LKH, OR-Tools, PyVRP, GA, and POMO-SOCP on CETSP instances
- The method demonstrates strong generalization to larger problem scales beyond training distribution
- Runtime performance is competitive with traditional heuristic solvers while maintaining better solution quality

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Factorizing the hybrid decision-space into discrete node selection followed by continuous waypoint localization reduces the effective search complexity compared to monolithic action spaces.
- **Mechanism:** The architecture employs a Node-Decoder to determine the visiting sequence (which neighborhood) and a separate Loc-Decoder to determine the entry point (where on the boundary). By conditioning the Loc-Decoder on the output of the Node-Decoder, the model treats waypoint optimization as a sub-problem solved after the sequence dependency is addressed, rather than solving a massive joint action space.
- **Core assumption:** The optimal visiting sequence is largely independent of the precise waypoint optimization, or that the sequence decision provides a sufficiently stable scaffold for location refinement.
- **Evidence anchors:**
  - [abstract] "propose a unified dual-decoder DRL (UD3RL) framework that separates decision-making into node selection and waypoint determination."
  - [section IV.C] "The dual-decoder architecture consists of two components: the node-decoder... and the loc-decoder... the output of the node-decoder serves as part of the input to the loc-decoder."
  - [corpus] Related work (Section II) notes that DRL for CETSP is limited; however, general DRL routing papers like NeuFACO support the efficacy of structured decoding for combinatorial tasks.
- **Break condition:** Performance degrades significantly on instances where the optimal tour depends entirely on "cutting corners" across overlapping neighborhoods, which might require simultaneous sequence and location coordination.

### Mechanism 2
- **Claim:** Localized spatial context provided by k-NN subgraphs functions as an inductive bias for geometric shortcutting during waypoint selection.
- **Mechanism:** The Loc-Decoder does not view the selected neighborhood in isolation. Instead, it attends to a subgraph of the k-nearest neighbors (Equation 15). This allows the model to position the waypoint not just at the closest point to the agent, but at a point that aligns with the subsequent trajectory towards neighbors, effectively "aiming" the tour.
- **Core assumption:** The k-nearest neighbors contain sufficient information to approximate the optimal traversal angle for the global tour.
- **Evidence anchors:**
  - [abstract] "A k-nearest neighbors subgraph interaction strategy is further introduced to enhance spatial reasoning during location decoding."
  - [section IV.C] "This interaction... allows the policy network to more effectively identify optimal traversal points within the neighborhood."
  - [corpus] The corpus mentions "Reinforcement Learning Methods for Neighborhood Selection," which conceptually aligns with using local structural information to guide search.
- **Break condition:** Failure occurs in sparse distributions where k-NN nodes are too distant to provide meaningful geometric cues, or in dense clusters where local context misleads the global optimum (e.g., "valley" scenarios).

### Mechanism 3
- **Claim:** Training on a mixture of problem sizes and radius types forces the encoder to learn scale-invariant geometric features rather than overfitting to fixed dimensionalities.
- **Mechanism:** The model uses curriculum-like sampling (drawing problem sizes n ∈ {20, ..., 100} and random radii per batch) during the REINFORCE optimization. This prevents the Transformer attention mechanism from relying on absolute positional embeddings or fixed graph diameters, forcing it to rely on relative distances and topological relationships.
- **Core assumption:** The underlying "grammar" of optimal CETSP tours remains consistent across different problem scales and neighborhood sizes.
- **Evidence anchors:**
  - [abstract] "unified model capable of generalizing across different problem sizes and varying neighborhood radius types."
  - [section IV.D] "We sample a radius type and a problem size from predefined sets... for each minibatch."
  - [corpus] The paper ASAP (corpus) discusses brittleness in neural solvers under distribution shifts; this mechanism directly addresses that by broadening the training distribution.
- **Break condition:** The model fails to generalize if the test set includes radius ranges vastly exceeding the training distribution (e.g., radius r ≈ 0.5 vs training r ≤ 0.1).

## Foundational Learning

- **Concept: Perimetral Discretization Scheme (PDS)**
  - **Why needed here:** CETSP is naturally a hybrid continuous-discrete problem. PDS is the pre-processing step that converts the continuous "visit anywhere in circle" constraint into a discrete set of candidate points on the circle's boundary, making the problem tractable for standard sequence models.
  - **Quick check question:** If you increase the discretization granularity (γ), how does that affect the action space size and the theoretical solution quality?

- **Concept: REINFORCE with Baseline**
  - **Why needed here:** This is the optimization engine. Understanding that REINFORCE uses the negative tour length as a reward and a baseline (mean reward of batch) to reduce variance is critical for understanding how the model "learns" to shorten the path without labeled supervision.
  - **Quick check question:** Why is a baseline (shared baseline in Eq. 19) necessary for stabilizing the gradient estimates in routing problems?

- **Concept: Autoregressive Factorization**
  - **Why needed here:** The solution is built step-by-step. The probability of the full tour is the product of step-wise probabilities (Equation 1). This explains why the "context" (previously visited nodes) must be fed back into the decoder at every step.
  - **Quick check question:** How does the context embedding q_t^c change after visiting a node, and why does this prevent the model from visiting the same node twice?

## Architecture Onboarding

- **Component map:** Input Layer -> Encoder (Transformer) -> Node-Decoder -> Loc-Decoder
- **Critical path:** The hand-off between decoders. The Node-Decoder picks a target (e.g., Node 5). The embedding of Node 5 becomes the primary query for the Loc-Decoder, which then determines where on Node 5's boundary to go. If the Node-Decoder picks a greedy node that leads to a geometric dead-end, the Loc-Decoder cannot recover the optimality.
- **Design tradeoffs:**
  - **Discretization (γ):** Higher γ yields better approximation of the continuous optimum but increases the action space for the Loc-Decoder, slowing inference.
  - **k-NN size (k):** Higher k adds context but increases attention complexity O(k).
  - **RMSNorm vs. LayerNorm:** The paper uses RMSNorm for training stability; switching to standard LayerNorm may require hyperparameter retuning.
- **Failure signatures:**
  - "Spiraling": The agent visits nodes in strictly increasing distance order due to greedy node decoding, ignoring global structure.
  - "Tangential misses": The Loc-Decoder picks the point closest to the current position rather than the one facilitating the next hop (indicating k-NN interaction failure).
  - Oscillation: The agent enters a neighborhood, "visits" it, but the reward structure or state update fails to mask it, causing re-visiting.
- **First 3 experiments:**
  1. **Ablate Loc-Decoder Context:** Run the model with k=0 (no neighbors) vs. k=10 to isolate the value of spatial reasoning in waypoint selection.
  2. **Discretization Sensitivity:** Train models with γ=3, 5, 8, 16 and plot the "Optimality Gap vs. Inference Time" to find the sweet spot for PDS.
  3. **Out-of-Distribution (OOD) Generalization:** Train on random radii [0, 0.1) and test on large constant radii r=0.2 to verify if the unified training actually holds for geometric shifts.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the UD3RL framework be extended to efficiently handle extremely large-scale instances with 10,000 or more nodes?
- **Basis in paper:** [explicit] The conclusion states: "Future work will focus on extending UD3RL to handle extremely large problem instances (e.g., 10k+ nodes)."
- **Why unresolved:** Standard Transformer encoders typically scale quadratically (O(n^2)) with input size, making 10k+ nodes computationally prohibitive for the current dense attention mechanism without architectural modifications like sparse attention or sub-tour hierarchies.
- **What evidence would resolve it:** Successful application of a modified UD3RL variant on benchmark datasets exceeding 10,000 nodes, demonstrating competitive solution quality and feasible training/inference times compared to heuristic solvers.

### Open Question 2
- **Question:** How can the UD3RL architecture be adapted to solve cooperative multi-agent CETSP?
- **Basis in paper:** [explicit] The conclusion explicitly proposes "...exploring cooperative routing of multiple agents in the CETSP" as a future direction.
- **Why unresolved:** The current MDP formulation and decoder architecture are designed for a single agent (building one tour). Multi-agent scenarios introduce complex dependencies, such as collision avoidance and credit assignment among agents, which the current unified single-tour decoder cannot process.
- **What evidence would resolve it:** A multi-agent variant of UD3RL evaluated on scenarios requiring coordinated task allocation, showing it can generate simultaneous, collision-free tours for multiple agents efficiently.

### Open Question 3
- **Question:** Is the current discretization-based loc-decoder robust to irregular or non-circular neighborhood geometries?
- **Basis in paper:** [inferred] The problem definition (Section III.A) explicitly defines neighborhoods as circular disks ("closed disk C_i"), and the Perimetral Discretization Scheme (PDS) assumes a circular boundary to place candidate waypoints.
- **Why unresolved:** Real-world "close-enough" zones often form complex polygons or irregular shapes (e.g., sensor ranges, property boundaries). It is unclear if the current PDS and loc-decoder can adapt to non-circular boundaries without fundamental changes to the feature embedding or discretization logic.
- **What evidence would resolve it:** Experiments applying UD3RL to CETSP instances with polygonal or elliptical neighborhoods to analyze solution quality and the loc-decoder's ability to identify optimal entry points on non-circular perimeters.

## Limitations
- The paper does not specify critical hyperparameters: discretization granularity γ, k-NN subgraph size k, and loc-decoder MLP activation function.
- Encoder architecture description mentions N=6 layers but later discusses 3-layer ablations, creating ambiguity about the exact baseline model.
- The method's scalability to extremely large problem instances (10k+ nodes) remains unproven due to quadratic attention complexity.

## Confidence
- **High confidence** in the dual-decoder architecture's ability to separate sequence and waypoint decisions, supported by explicit equations and component descriptions.
- **Medium confidence** in the k-NN subgraph strategy's effectiveness for spatial reasoning, as the mechanism is described but empirical ablation is not provided.
- **Medium confidence** in generalization claims, as OOD tests are mentioned but detailed results across radius ranges are limited.

## Next Checks
1. **Ablate Loc-Decoder Context:** Run the model with k=0 (no neighbors) vs. k=10 to isolate the value of spatial reasoning in waypoint selection.
2. **Discretization Sensitivity:** Train models with γ=3, 5, 8, 16 and plot the "Optimality Gap vs. Inference Time" to find the sweet spot for PDS.
3. **Out-of-Distribution Generalization:** Train on random radii [0, 0.1) and test on large constant radii r=0.2 to verify if the unified training actually holds for geometric shifts.