---
ver: rpa2
title: 'MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning
  With Ensemble Vision Language Models'
arxiv_id: '2507.11114'
source_url: https://arxiv.org/abs/2507.11114
tags:
- reasoning
- gemini
- multilingual
- multimodal
- flash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents an ensemble-based system for multilingual multimodal
  reasoning designed for the ImageCLEF 2025 EXAMS-V challenge. The system integrates
  Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption refinement and
  consistency checks, and Gemini 2.5 Pro for final answer selection, all coordinated
  through carefully engineered prompts.
---

# MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models

## Quick Facts
- arXiv ID: 2507.11114
- Source URL: https://arxiv.org/abs/2507.11114
- Reference count: 21
- First place overall in ImageCLEF 2025 multilingual track with 81.4% accuracy

## Executive Summary
This paper presents an ensemble-based system for multilingual multimodal reasoning designed for the ImageCLEF 2025 EXAMS-V challenge. The system combines Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption refinement, and Gemini 2.5 Pro for final answer selection, coordinated through carefully engineered prompts. An extensive ablation study trained several large language models on English and multilingual datasets, with Gemini 2.5 Flash showing superior zero-shot performance. The system achieved first place overall in the multilingual track with 81.4% accuracy, leading 11 out of 13 individual language tracks including 95.07% for Croatian and 92.12% for Italian.

## Method Summary
The system employs an ensemble architecture using three Gemini models: Gemini 2.5 Flash generates visual descriptions, Gemini 1.5 Pro refines captions and performs consistency checks, and Gemini 2.5 Pro selects final answers. The approach includes extensive prompt engineering with language-normalized formats that improved English validation accuracy from 55.9% to 61.7%. An ablation study trained multiple models (Gemini 2.5 Flash, Phi-4, Gemma-3, Mistral) on both English datasets and their multilingual augmented versions. Gemini 2.5 Flash was also evaluated in a zero-shot setting, substantially outperforming the trained models across all tested conditions.

## Key Results
- Achieved first place overall in ImageCLEF 2025 multilingual track with 81.4% accuracy
- Led 11 out of 13 individual language tracks, with top results including 95.07% for Croatian and 92.12% for Italian
- Prompt engineering improved English validation accuracy from 55.9% to 61.7%
- Gemini 2.5 Flash zero-shot performance substantially outperformed trained models

## Why This Works (Mechanism)
The ensemble approach leverages specialized capabilities of different Gemini models for distinct reasoning stages. Gemini 2.5 Flash's speed and accuracy in visual description enables efficient processing, while Gemini 1.5 Pro's refinement capabilities ensure caption quality and consistency. Gemini 2.5 Pro's reasoning abilities optimize final answer selection. The prompt engineering strategy normalizes language variations, reducing ambiguity in multilingual contexts. Cross-lingual augmentation of training data exposes models to diverse linguistic patterns, enhancing generalization across languages. The lightweight OCR-VLM ensemble structure balances computational efficiency with reasoning depth, outperforming heavier end-to-end models in educational contexts.

## Foundational Learning
- Vision-Language Model Integration: Understanding how visual and language processing combine for multimodal reasoning
  - Why needed: Enables extraction of information from both image content and text
  - Quick check: Can the system accurately describe visual elements and relate them to textual questions?
- Multilingual Data Augmentation: Expanding training datasets across multiple languages
  - Why needed: Improves model generalization and reduces language-specific biases
  - Quick check: Does performance improve consistently across all language tracks?
- Prompt Engineering for Consistency: Designing prompts that standardize language variations
  - Why needed: Reduces ambiguity and improves answer reliability in multilingual settings
  - Quick check: Can consistent prompt formats be applied across different language contexts?

## Architecture Onboarding
Component Map: Image -> Gemini 2.5 Flash (Visual Description) -> Gemini 1.5 Pro (Caption Refinement) -> Gemini 2.5 Pro (Answer Selection) -> Final Answer

Critical Path: Visual input flows through description generation, caption refinement with consistency checking, then final answer selection through reasoning

Design Tradeoffs: Ensemble approach trades computational efficiency for specialized processing at each stage, versus end-to-end models that may be heavier but less targeted

Failure Signatures: Inconsistent visual descriptions, caption refinement errors propagating downstream, reasoning errors in final answer selection

First Experiments:
1. Test individual model components with single-language inputs to verify baseline capabilities
2. Evaluate prompt variations on English validation set to optimize format
3. Assess cross-lingual performance consistency across all 13 language tracks

## Open Questions the Paper Calls Out
None

## Limitations
- Results may not generalize to other vision-language architectures, as alternative models (Phi-4, Gemma-3, Mistral) underperformed
- Heavy reliance on prompt engineering raises reproducibility concerns due to limited methodological detail
- Multilingual gains' source unclear - may derive from model capabilities or dataset composition
- Performance validated only on single challenge dataset without external validation

## Confidence
- High confidence: Ensemble architecture effectiveness, Gemini 2.5 Flash zero-shot superiority, overall leaderboard ranking
- Medium confidence: Prompt engineering impact (due to limited methodological detail), multilingual performance generalizability
- Low confidence: Alternative model comparisons, long-term robustness beyond ImageCLEF 2025

## Next Checks
1. Replicate results using alternative vision-language models (e.g., GPT-4V, Claude 3) to test architecture dependence
2. Conduct ablation studies isolating prompt design effects from model capabilities
3. Validate performance on external multilingual educational datasets beyond ImageCLEF 2025