---
ver: rpa2
title: 'SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot
  Learning and Stereo Vision'
arxiv_id: '2505.11439'
source_url: https://arxiv.org/abs/2505.11439
tags:
- pose
- estimation
- surgical
- mask
- depth
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of accurately estimating the
  6-DoF pose of surgical instruments in robot-assisted minimally invasive surgery
  (RMIS), where occlusion, reflections, and lack of reliable depth sensors hinder
  existing approaches. The authors propose a zero-shot learning pipeline combining
  stereo vision-based depth estimation using RAFT-Stereo and enhanced instance segmentation
  via a fine-tuned Mask R-CNN, replacing the original SAM in SAM-6D.
---

# SurgPose: Generalisable Surgical Instrument Pose Estimation using Zero-Shot Learning and Stereo Vision

## Quick Facts
- arXiv ID: 2505.11439
- Source URL: https://arxiv.org/abs/2505.11439
- Reference count: 34
- Primary result: Enhanced SAM-6D with Mask R-CNN achieves 49.06% ADD accuracy at 5mm threshold in occluded settings and 98.87% 2D projection accuracy at 50px threshold on surgical tool pose dataset

## Executive Summary
This paper tackles the challenge of accurate 6-DoF pose estimation for surgical instruments in robot-assisted minimally invasive surgery, where occlusion, reflections, and lack of reliable depth sensors hinder existing approaches. The authors propose a zero-shot learning pipeline combining stereo vision-based depth estimation using RAFT-Stereo and enhanced instance segmentation via a fine-tuned Mask R-CNN, replacing the original SAM in SAM-6D. The method was validated on a newly introduced surgical tool pose dataset with both occluded and non-occluded scenarios, showing strong performance compared to both the original SAM-6D and the FoundationPose baseline.

## Method Summary
The proposed approach integrates RAFT-Stereo for dense depth estimation from stereo images with a fine-tuned Mask R-CNN for precise instrument segmentation. This combination replaces the original SAM segmentation in SAM-6D, creating an enhanced pipeline for zero-shot pose estimation. The method leverages pre-trained models for both depth estimation and segmentation, fine-tuning only the segmentation component on surgical tool data. The pipeline processes stereo image pairs to generate depth maps, segments instruments from the scene, and estimates 6-DoF poses without requiring instrument-specific training data.

## Key Results
- Enhanced SAM-6D with Mask R-CNN achieved 49.06% ADD accuracy at 5mm threshold in occluded settings
- 98.87% 2D projection accuracy at 50px threshold on non-occluded scenarios
- Outperformed both original SAM-6D and FoundationPose baseline on the introduced surgical tool pose dataset

## Why This Works (Mechanism)
The approach works by addressing the two main challenges in surgical pose estimation: accurate depth estimation in complex surgical scenes and reliable instrument segmentation under occlusion. RAFT-Stereo provides robust depth estimation from stereo pairs, handling the reflective and texture-poor environments common in surgery. The fine-tuned Mask R-CNN improves segmentation accuracy over SAM, particularly for surgical instruments with complex shapes and occlusions. By combining these specialized components, the pipeline achieves better generalization to unseen instruments while maintaining accuracy in challenging conditions.

## Foundational Learning

**RAFT-Stereo**: Why needed - Provides dense depth estimation from stereo pairs without requiring specialized depth sensors. Quick check - Verify depth map quality on surgical scenes with known geometry.

**Mask R-CNN**: Why needed - Delivers precise instance segmentation crucial for distinguishing surgical instruments from complex backgrounds. Quick check - Validate segmentation IoU on surgical tool images with ground truth masks.

**Zero-Shot Learning**: Why needed - Enables pose estimation for unseen instruments without instrument-specific training. Quick check - Test on instruments not present in training data.

## Architecture Onboarding

**Component Map**: Stereo Camera -> RAFT-Stereo -> Depth Map -> Mask R-CNN -> Instrument Mask -> Pose Estimation Module -> 6-DoF Pose

**Critical Path**: The bottleneck lies in the Mask R-CNN segmentation, which must accurately isolate instruments before pose estimation can succeed. Depth estimation quality directly impacts pose accuracy, particularly for depth-dependent metrics.

**Design Tradeoffs**: The approach trades computational efficiency for accuracy by using two separate deep learning models (RAFT-Stereo and Mask R-CNN) instead of a single end-to-end network. This modular design allows independent optimization but increases inference time.

**Failure Signatures**: Poor depth estimation from RAFT-Stereo manifests as inaccurate depth values for instrument parts, leading to scale errors in pose. Mask R-CNN failures result in incomplete or incorrect segmentation masks, causing the pose estimator to focus on wrong regions.

**3 First Experiments**: 1) Evaluate depth map accuracy on synthetic surgical scenes with ground truth depth. 2) Test segmentation performance on surgical tools with varying levels of occlusion. 3) Measure pose estimation accuracy on instruments with extreme lighting conditions.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalization performance to entirely different surgical environments and instrument types remains uncertain
- Fine-tuned Mask R-CNN requires annotated training data for each new instrument type, limiting true zero-shot applicability
- Depth estimation using RAFT-Stereo may face challenges with highly reflective or transparent surgical tools common in real surgeries

## Confidence

| Claim | Confidence |
|-------|------------|
| Reported performance metrics on introduced dataset are robust | High |
| Method sets new benchmark for zero-shot RGB-D pose estimation in RMIS | Medium |
| Method significantly outperforms existing approaches in all occluded scenarios | Low |

## Next Checks
1. Validate the method on an independent surgical tool pose dataset, such as those from the SurgRIPE challenge, to assess generalization across different surgical systems and environments
2. Conduct experiments in actual RMIS procedures to evaluate robustness to dynamic lighting, reflections, and occlusions caused by tissue interaction
3. Benchmark against other state-of-the-art depth estimation and segmentation techniques to quantify relative performance gains