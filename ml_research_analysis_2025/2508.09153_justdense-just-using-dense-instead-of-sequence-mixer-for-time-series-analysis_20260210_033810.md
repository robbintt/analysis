---
ver: rpa2
title: 'JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis'
arxiv_id: '2508.09153'
source_url: https://arxiv.org/abs/2508.09153
tags:
- sequence
- dense
- mixer
- matrix
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether complex sequence mixers are necessary
  for time series analysis. The authors propose JustDense, which systematically replaces
  sequence mixers in state-of-the-art TSA models with dense layers.
---

# JustDense: Just using Dense instead of Sequence Mixer for Time Series analysis

## Quick Facts
- **arXiv ID**: 2508.09153
- **Source URL**: https://arxiv.org/abs/2508.09153
- **Reference count**: 19
- **Primary result**: Dense mixers achieve comparable or superior performance to complex sequence mixers across 29 benchmarks and five TSA tasks

## Executive Summary
This paper challenges the assumption that complex sequence mixers are necessary for effective time series analysis. The authors propose JustDense, which systematically replaces sequence mixers in state-of-the-art TSA models with simple dense layers. Experiments across 29 benchmarks and five tasks (classification, anomaly detection, imputation, long-term forecasting, short-term forecasting) using seven models show that dense mixers achieve comparable or superior performance to the original mixers. Efficiency analysis reveals dense mixers often train faster despite higher theoretical complexity.

## Method Summary
The method involves replacing the sequence mixer component in existing TSA models with a learnable L×L dense matrix. For each target model, the authors identify the sequence mixer (attention, convolution/Toeplitz, or SSM/semiseparable), extract input/output dimensions, and substitute it with a dense matrix of appropriate size. The replacement preserves all other architectural components including embedding, channel mixers, normalization, and residual connections. The dense mixer operates per-head for multi-head architectures and is initialized using standard PyTorch initialization.

## Key Results
- Dense mixers achieve comparable or superior performance to structured mixers across all five TSA tasks
- Training time is often reduced with dense mixers due to optimized GPU libraries (cuBLAS)
- Dense mixers naturally converge toward low-rank solutions through implicit regularization
- The approach is architecture-agnostic, working across Transformers, Autoformers, PatchTST, iTransformer, ModernTCN, Mamba, and S-Mamba

## Why This Works (Mechanism)

### Mechanism 1: Dense Matrices as High-Rank Super-Set
Dense L×L matrices operate in full rank space RL×L, strictly containing low-rank families: attention (rank ≤ P), Toeplitz (rank ≤ K+1), and semiseparable (rank ≤ N⌈L/N⌉). This means dense mixers can express all structured mixer operations plus additional high-rank interactions they cannot capture.

### Mechanism 2: Implicit Regularization via Gradient Descent
Gradient descent exhibits implicit bias toward minimal nuclear-norm solutions, ensuring dense matrices naturally recover structured, low-rank behavior when appropriate. This prevents overfitting despite the L² parameter count.

### Mechanism 3: Practical Efficiency Through Hardware Optimization
Dense matrix multiplication leverages highly optimized cuBLAS routines with superior parallelization, making wall-clock time lower than structured mixers even when FLOP count is higher. The elimination of mixer-specific overhead contributes to this efficiency.

## Foundational Learning

- **Matrix rank and expressivity**: Understanding why attention (low-rank), convolution (banded Toeplitz), and SSMs (semiseparable) are subsets of dense matrices is key to the paper's theoretical claim. Quick check: Can a rank-64 attention matrix express the same transformations as a rank-512 dense matrix over a 1024-token sequence? (No—rank limits expressible operations.)

- **Implicit regularization in optimization**: Explains why dense matrices don't overfit despite having L² parameters. Quick check: Why would gradient descent prefer a low-rank solution when a high-rank one also minimizes training loss? (Implicit bias toward nuclear norm minimization finds the "simplest" interpolating solution.)

- **Toeplitz and semiseparable matrix structures**: The paper reformulates convolutions and SSMs as matrix operations. Quick check: How does a convolution kernel of size K correspond to a Toeplitz matrix's band structure? (Each diagonal stores one kernel weight; the matrix has K+1 non-zero diagonals.)

## Architecture Onboarding

- **Component map**: Input embedding -> Dense mixer (L×L matrix) -> Channel mixer (FFN/ConvFFN) -> Normalization/residuals

- **Critical path**: Identify sequence mixer in target model → Extract input/output dimensions → Replace M(h) with learnable L×L matrix → Preserve all other components

- **Design tradeoffs**: Parameter count is higher (L² vs L×P or K), but expressivity is greater. Memory scaling is O(L²), making long sequences challenging. Training is faster due to optimized libraries, though inference may be slower without batching.

- **Failure signatures**: Out-of-memory on long sequences (L > ~1024 may exceed GPU memory), overfitting on small datasets (monitor validation loss), no improvement over baseline (check if task complexity justifies higher capacity).

- **First 3 experiments**:
  1. Replicate on ETTh1 long-term forecasting comparing PatchTST vs JustDense-PatchTST using identical hyperparameters; report MSE, training time, and parameter count.
  2. Ablation by sequence length on ETTh1 with L = {96, 336, 720}; plot performance gap and training time ratio against L.
  3. Inspect learned dense matrices: Train both versions, extract M (attention) and M̃ (dense), compute PSNR and JSD, and visualize matrices to confirm similar temporal patterns.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can parameter-efficient matrix structures (e.g., low-rank or structured approximations) achieve performance equal to full dense layers while resolving the O(L²) memory scaling constraint? The paper suggests exploring more effective architectures as future work.

- **Open Question 2**: At what sequence length does the theoretical complexity of dense layers overcome practical hardware optimization benefits? The paper notes dense layers are practically faster but acknowledges this may not hold for extreme lengths.

- **Open Question 3**: Does JustDense success imply complex sequence mixers are only necessary for semantic-heavy tasks (e.g., NLP) and inherently unnecessary for numerical time series? The paper suggests TSA may require fundamentally different design principles than NLP-derived architectures.

## Limitations

- Dense mixers become memory-prohibitive for sequences longer than ~1024 tokens
- Claims about implicit regularization toward low-rank solutions lack empirical validation for TSA tasks
- Results may be architecture-specific; performance on additional model families remains untested
- Hardware efficiency claims lack systematic benchmarking across different GPU generations

## Confidence

- **High**: Dense mixers can match or exceed performance of structured mixers across tested tasks
- **Medium**: Dense matrices strictly contain structured mixers in matrix space (theoretical rank argument)
- **Low**: Implicit regularization consistently prevents overfitting in TSA; hardware efficiency generalizes across hardware

## Next Checks

1. Test JustDense on additional architectures beyond the seven examined (e.g., Informer, FEDformer) to assess generalizability
2. Systematically measure implicit regularization by comparing learned dense matrices' nuclear norms to theoretical minima across tasks
3. Benchmark wall-clock training time on different GPU architectures (H100, A100, RTX 4090) to validate hardware efficiency claims beyond L40S