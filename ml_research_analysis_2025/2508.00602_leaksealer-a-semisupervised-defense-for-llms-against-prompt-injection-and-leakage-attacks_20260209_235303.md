---
ver: rpa2
title: 'LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and
  Leakage Attacks'
arxiv_id: '2508.00602'
source_url: https://arxiv.org/abs/2508.00602
tags:
- leaksealer
- dataset
- prompt
- information
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LeakSealer addresses security threats in LLM deployments, specifically
  prompt injection and PII leakage, by combining static forensic analysis with dynamic,
  semi-supervised defenses. It clusters historical LLM interactions to map usage patterns,
  detect adversarial groups, and generate semantic fingerprints for proactive defense.
---

# LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks

## Quick Facts
- **arXiv ID:** 2508.00602
- **Source URL:** https://arxiv.org/abs/2508.00602
- **Reference count:** 40
- **Primary result:** Achieves 0.97 purity and 0.77 F1-score on ToxicChat for prompt injection detection, and 0.97 AUPRC for PII leakage detection, outperforming LLM-as-a-judge baselines.

## Executive Summary
LeakSealer addresses security threats in LLM deployments, specifically prompt injection and PII leakage, by combining static forensic analysis with dynamic, semi-supervised defenses. It clusters historical LLM interactions to map usage patterns, detect adversarial groups, and generate semantic fingerprints for proactive defense. The framework is model-agnostic, lightweight, and uses a Human-In-The-Loop pipeline to minimize manual effort while adapting to concept drift. In static settings, LeakSealer achieved 0.97 purity and 0.77 F1-score on ToxicChat for prompt injection detection. In dynamic scenarios, it reached an AUPRC of 0.97 for PII leakage detection, outperforming Llama Guard (0.84) and other LLM-as-a-judge baselines. Experiments used curated datasets including a new PII leakage benchmark. LeakSealer balances forensic insight with real-time defense, enabling efficient monitoring and adaptation to emerging attack patterns without extensive retraining.

## Method Summary
LeakSealer employs a two-phase approach: static forensic analysis followed by dynamic classification. The static phase generates high-dimensional embeddings (stella_en_400M_v5) for historical query-response pairs, reduces them via PCA (and UMAP for visualization), and applies HDBSCAN clustering to identify adversarial groups and outliers. Human-in-the-loop labeling of cluster exemplars propagates labels to entire clusters. The dynamic phase trains lightweight classifiers (SVM/RF/XGBoost) on PCA-reduced embeddings for real-time defense. This approach reduces labeling effort by factor $1/N$ where $N$ is average cluster size while maintaining high detection accuracy.

## Key Results
- Static phase achieved 0.97 purity and 0.77 F1-score on ToxicChat for prompt injection detection
- Dynamic PII leakage detection reached AUPRC of 0.97, outperforming Llama Guard (0.84)
- Framework reduces labeling effort by factor $1/N$ through exemplar-based propagation
- Claims order-of-magnitude reduction in latency compared to LLM-as-a-judge baselines

## Why This Works (Mechanism)

### Mechanism 1: Semantic Density Separation
If adversarial prompts share semantic structures, they cluster distinctly from legitimate queries in reduced embedding space. The framework assumes attacks form dense "islands" or outliers separate from normal usage. Core assumption: attack vectors possess semantic similarity preserved through dimensionality reduction, allowing HDBSCAN to isolate them. Break condition: highly diverse, low-density attacks may fail to form distinct groups, leaving attacks as unclustered noise.

### Mechanism 2: Exemplar-Based Label Propagation
Labeling only cluster representatives (exemplars) effectively classifies entire groups, reducing human labeling effort by factor $1/N$. HDBSCAN identifies exemplars in densest regions; human labels these few exemplars, and labels propagate to all points in the cluster. Core assumption: cluster semantic homogeneity ensures exemplar representativeness. Break condition: low-purity clusters with mixed safe/unsafe intents introduce systematic labeling errors when propagating single binary labels.

### Mechanism 3: Lightweight Classifier Distillation
Standard classifier (SVM/RF) trained on reduced embeddings approximates LLM-as-a-Judge safety judgments with significantly lower latency and cost. The static phase generates labeled dataset via clustering and propagation; classifier trains on PCA-reduced embeddings (50 features). Core assumption: decision boundary between safe and malicious embeddings is learnable by linear or tree-based models in reduced space. Break condition: rapid concept drift creates new attack styles mapping to previously "safe" regions, causing static classifier failure until retraining.

## Foundational Learning

- **Concept: Density-Based Clustering (HDBSCAN)**
  - Why needed here: Unlike K-Means, HDBSCAN doesn't force all points into clusters, essential for identifying outliers (rare attacks) and arbitrary shapes matching irregular adversarial prompts.
  - Quick check question: How does HDBSCAN handle a prompt significantly different from both "safe" clusters and known "attack" clusters?

- **Concept: Dimensionality Reduction (PCA vs. UMAP)**
  - Why needed here: Raw embeddings (1024-dim) are noisy and computationally heavy for clustering. PCA for dynamic pipeline (deterministic) and UMAP for static visualization (better visual separation).
  - Quick check question: Why use PCA for dynamic classifier training but UMAP for static usage map report?

- **Concept: Semi-Supervised Learning (Label Propagation)**
  - Why needed here: Obtaining labeled security data is expensive and sensitive. Bridges gap between unsupervised pattern finding (clustering) and supervised defense (classification).
  - Quick check question: What is the risk of "semantic drift" when propagating label from cluster centroid to edge cases?

## Architecture Onboarding

- **Component map:** Embedder (stella_en_400M_v5) -> Reducer (PCA to 50-dim) -> Static Analyzer (UMAP+ HDBSCAN) -> Human-Loop Interface (labels exemplars) -> Dynamic Defender (SVM/RF classifier on 50-dim)

- **Critical path:** Dynamic Defender accuracy depends entirely on Static Analyzer's clustering quality. Impure clusters (mixing safe/malicious) will inherit biases into trained classifier.

- **Design tradeoffs:**
  - Precision vs. Recall: HDBSCAN `min_cluster_size` and PCA components control group tightness. Tighter groups = higher precision but potentially missed attacks.
  - Latency vs. Granularity: Efficiency avoids LLM inference for every check but may lose nuance compared to LLM judge handling complex, multi-turn obfuscation.

- **Failure signatures:**
  - High Outlier Count: >50% traffic as outliers indicates dimensionality reduction stripping too much info or `min_samples` too high.
  - Cluster Implosion: Legitimate queries merging with attack queries. Check UMAP visualization for distinct boundaries.

- **First 3 experiments:**
  1. **Purity Validation:** Run static pipeline on known labeled dataset (ToxicChat). Measure cluster purity. If < 0.80, adjust PCA dimensions or HDBSCAN `min_cluster_size`.
  2. **Latency Benchmark:** Compare inference time of trained SVM/RF classifier against Llama Guard 3. Target: Order of magnitude reduction.
  3. **Drift Simulation:** Train dynamic model on Week 1 data. Test on Week 4 data to simulate concept drift. Measure AUPRC drop to estimate retraining frequency requirements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LeakSealer's performance scale when adapted for online training in highly dynamic attack scenarios?
- Basis in paper: "Future research directions include adapting our approach for online training scenarios, improving its adaptability to highly-dynamic scenarios."
- Why unresolved: Current framework operates in batch static and dynamic modes without real-time online learning implementation.
- What evidence would resolve it: Empirical evaluation of online variant measuring detection latency, accuracy drift over time, and computational overhead under continuous stream conditions.

### Open Question 2
- Question: To what extent does clustering quality and choice of clustering algorithm affect downstream classification performance?
- Basis in paper: Framework depends on HDBSCAN producing homogeneous clusters for label propagation, but sensitivity to clustering hyperparameters or alternative algorithms not analyzed.
- Why unresolved: No ablation study compares different clustering methods or evaluates how cluster purity directly correlates with final classifier accuracy.
- What evidence would resolve it: Systematic comparison using alternative clustering algorithms (k-means, DBSCAN) with varying hyperparameters, measuring both purity and downstream F1-score.

### Open Question 3
- Question: How effectively do semantic fingerprints transfer across different attack types or application domains beyond toxicity and PII leakage?
- Basis in paper: Evaluates two specific threat scenarios but doesn't investigate whether learned representations generalize to other security violations (prompt extraction, model distillation attacks).
- What evidence would resolve it: Cross-domain transfer experiments where classifiers trained on one attack type are tested on held-out attack categories, with analysis of embedding space overlap.

## Limitations
- Dataset Generalization Risk: PII leakage detection relies on synthetically generated dataset rather than real-world examples, potentially limiting performance on naturally occurring scenarios.
- Concept Drift Vulnerability: Static classifier distillation creates rigid decision boundary; strong performance on curated datasets not extensively validated against rapidly evolving attack patterns.
- Clustering Assumption Validity: Framework assumes adversarial prompts form distinct semantic clusters, but adaptive attackers may deliberately diversify attacks to evade clustering, resulting in high outlier rates or misclassification.

## Confidence
- **High Confidence**: Static phase cluster purity results (0.97) and fundamental mechanism of using semantic embeddings for grouping are well-supported by methodology and results.
- **Medium Confidence**: Dynamic phase AUPRC of 0.97 for PII detection based on synthetic data and relative comparisons to baselines; efficiency claims supported but require real-world deployment validation.
- **Low Confidence**: Long-term effectiveness against adaptive, multi-turn attacks and performance on production-scale, diverse query distributions not represented in curated datasets.

## Next Checks
1. **Adaptive Attack Simulation**: Test LeakSealer against dataset of prompts designed to evade clustering (varied semantic structures, same malicious intent). Measure increase in outlier rate and false negative rate.

2. **Longitudinal Drift Test**: Deploy static classifier (trained on Week 1 data) and measure AUPRC degradation when tested on data from Weeks 2-8. Quantify retraining frequency requirement and validate claimed "concept drift" handling.

3. **Production-Scale Validation**: Run LeakSealer on real LLM deployment's historical query logs (with privacy safeguards). Compare clustering results and exemplar labeling effort against paper's simulated HITL results to validate claimed reduction in manual labeling effort.