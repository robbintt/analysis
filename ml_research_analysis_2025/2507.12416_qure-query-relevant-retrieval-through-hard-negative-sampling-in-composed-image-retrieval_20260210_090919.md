---
ver: rpa2
title: 'QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed
  Image Retrieval'
arxiv_id: '2507.12416'
source_url: https://arxiv.org/abs/2507.12416
tags:
- image
- hard
- negative
- images
- qure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes QuRe, a method for composed image retrieval
  that addresses the problem of false negatives in contrastive learning by introducing
  a novel hard negative sampling strategy. The key idea is to periodically rank the
  corpus by relevance scores and select hard negatives positioned between two steep
  drops in relevance scores following the target image.
---

# QuRe: Query-Relevant Retrieval through Hard Negative Sampling in Composed Image Retrieval

## Quick Facts
- **arXiv ID**: 2507.12416
- **Source URL**: https://arxiv.org/abs/2507.12416
- **Reference count**: 25
- **Primary result**: State-of-the-art performance on FashionIQ and CIRR datasets using hard negative sampling between steepest relevance score drops

## Executive Summary
QuRe addresses false negatives in composed image retrieval by introducing a novel hard negative sampling strategy that identifies challenging yet valid negatives between two steepest relevance score drops following the target image. The method combines a preference-based reward model objective with curriculum-driven negative set refinement to improve retrieval performance. Experiments show QuRe achieves 1.09-2.16% recall improvements on FashionIQ and 1.47-1.95% on CIRR, with superior human preference alignment at 74.55% on HP-FashionIQ.

## Method Summary
QuRe fine-tunes BLIP-2 with ViT-L/14 image encoder, using a Q-Former to fuse reference image embeddings with relative text into query embeddings. The model computes relevance scores between queries and corpus images, then periodically sorts the corpus to identify hard negatives positioned between the two steepest score drops after the target image. Training uses a Bradley-Terry preference-based reward model loss instead of standard contrastive loss, with the hard negative set updated every ⌊n_epoch/6⌋ epochs after an initial warm-up period using the full corpus. Images are resized to 224×224 with 1.25 padding ratio.

## Key Results
- **FashionIQ**: 1.09-2.16% recall improvements over state-of-the-art baselines
- **CIRR**: 1.47-1.95% recall improvements across multiple metrics
- **HP-FashionIQ**: 74.55% human preference rate, demonstrating superior alignment with human judgments

## Why This Works (Mechanism)

### Mechanism 1: Preference-Based Reward Model Objective
QuRe replaces standard contrastive loss with a Bradley-Terry preference optimization objective that reduces false negative penalties. The model treats each negative individually by computing preference probabilities and minimizing negative log-likelihood, avoiding the simultaneous push-away behavior of contrastive learning.

### Mechanism 2: Hard Negative Selection via Relevance Score Gradient Analysis
The method periodically ranks the corpus by relevance scores and identifies indices k₁ and k₂ where score degradation is largest. Hard negatives are sampled from the range between these points, isolating challenging yet valid negatives that are semantically similar but less relevant than the target.

### Mechanism 3: Curriculum-Driven Negative Set Refinement
After an initial warm-up phase, the hard negative set is redefined six times during training as the model's relevance scores become more reliable. This creates a curriculum that adapts as model confidence improves, with decreasing hard negative set sizes indicating sharper score transitions.

## Foundational Learning

- **Contrastive Learning with In-Batch Negatives**: Understanding why standard contrastive learning pushes all non-target images away clarifies the false negative problem QuRe addresses.
  - Quick check: Why does standard contrastive learning push semantically similar non-target images away from the query?

- **Bradley-Terry Preference Model**: This forms the mathematical foundation for QuRe's loss function, converting relevance score differences into preference probabilities.
  - Quick check: Given scores s₊ = 0.8 and s₋ = 0.5 with τ = 0.07, what is the preference probability p(I₊ ≻ I₋)?

- **BLIP-2 Architecture (Vision Encoder + Q-Former)**: QuRe fine-tunes BLIP-2, where the Q-Former fuses image and text modalities before computing relevance scores.
  - Quick check: What role does the Q-Former play in bridging frozen image and text encoders?

## Architecture Onboarding

- **Component map**: BLIP-2 ViT-L/14 (E_img) -> Q-Former -> Relevance Scoring -> Hard Negative Sampler -> Loss Computer
- **Critical path**: Relevance score computation → Corpus sorting → Drop detection (k₁, k₂) → Hard negative sampling → Preference loss → Backprop through Q-Former only
- **Design tradeoffs**: ndef frequency balances adaptation speed vs. sorting overhead; warm-up duration affects initial score quality; single negative per query trades convergence speed for efficiency
- **Failure signatures**: Flat score distributions prevent meaningful drop detection; empty hard negative sets indicate mode collapse; early preference rate plateau suggests target optimization without broader relevance alignment
- **First 3 experiments**:
  1. Ablate hard negative strategy: Compare "All corpus" vs. "Top-k" vs. "After target, top-k" vs. QuRe's gradient-based method on FashionIQ validation
  2. Vary ndef frequency: Test ndef ∈ {3, 6, 12} to find optimal refresh rate; monitor recall metrics and hard negative set size curves
  3. Zero-shot transfer check: Evaluate trained model on CIRCO dataset to validate retrieval of semantically relevant images beyond just the target

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the traditional sense, but the methodology raises several important unresolved issues regarding scalability, robustness of the gradient-based selection heuristic, and sensitivity to hyperparameter choices.

## Limitations
- The computational overhead of periodically sorting the entire image corpus could become prohibitive for web-scale datasets exceeding 1M images
- The "two steep drops" heuristic may fail when relevance score distributions are uniform or lack distinct boundaries between false negatives and true hard negatives
- The method's performance sensitivity to warm-up phase duration and hard negative set update frequency (ndef) remains unclear without comprehensive ablation studies

## Confidence
- **High Confidence**: The experimental results demonstrating recall improvements on FashionIQ and CIRR are well-supported by methodology and comparisons to strong baselines
- **Medium Confidence**: The theoretical mechanism of gradient-based hard negative selection is sound but would benefit from ablation studies on warm-up duration and ndef frequency
- **Low Confidence**: The claim that preference-based reward fundamentally reduces false negative penalties requires controlled experiments isolating this effect from the sampling strategy

## Next Checks
1. **Ablate warm-up duration**: Test training with different initial warm-up periods (e.g., 1/12 vs 1/6 of total epochs) to quantify impact on hard negative set quality and final recall performance
2. **Probe for false negatives**: Analyze semantic similarity between hard negatives and targets using CLIP embeddings to verify gradient-based selection avoids penalizing semantically similar non-target images
3. **Multi-negative comparison**: Implement a variant sampling 3-5 hard negatives per query per epoch using the same gradient-based selection, measuring recall improvements and computational overhead relative to single-negative approach