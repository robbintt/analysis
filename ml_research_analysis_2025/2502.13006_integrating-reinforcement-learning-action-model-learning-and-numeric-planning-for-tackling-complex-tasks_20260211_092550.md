---
ver: rpa2
title: Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning
  for Tackling Complex Tasks
arxiv_id: '2502.13006'
source_url: https://arxiv.org/abs/2502.13006
tags:
- learning
- action
- planning
- numeric
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a novel hybrid strategy called RAMP (Reinforcement
  learning, Action Model learning, and Planning) for tackling complex tasks in numeric
  planning environments. RAMP integrates reinforcement learning, action model learning,
  and numeric planning to leverage the strengths of both model-free and model-based
  approaches.
---

# Integrating Reinforcement Learning, Action Model Learning, and Numeric Planning for Tackling Complex Tasks

## Quick Facts
- arXiv ID: 2502.13006
- Source URL: https://arxiv.org/abs/2502.13006
- Authors: Yarin Benyamin; Argaman Mordoch; Shahaf S. Shperberg; Roni Stern
- Reference count: 13
- Primary result: RAMP significantly outperforms RL baselines in Minecraft tasks, solving more problems and finding up to 2 orders of magnitude shorter plans.

## Executive Summary
This paper introduces RAMP (Reinforcement learning, Action Model learning, and Planning), a hybrid strategy that integrates reinforcement learning, action model learning, and numeric planning to tackle complex tasks in deterministic, fully observable environments. RAMP leverages RL for exploration and data generation, NSAM for safe action model learning, and an off-the-shelf planner (Metric-FF) to find or optimize plans. Experimental results in the Minecraft domain demonstrate that RAMP significantly outperforms several RL baselines, solving more problems and finding much shorter plans in some cases.

## Method Summary
RAMP combines PPO with action masking for exploration, NSAM for safe incremental action model learning, and Metric-FF for planning. The agent explores using RL, generates trajectories, and feeds them to NSAM to learn a symbolic numeric action model. The learned model is used by the planner to find plans; if no plan is found, the RL policy is followed. Successful RL trajectories are post-processed to remove loops and find shortcuts based on the learned model, which are then fed back as higher-quality training data. Experiments are conducted in the Minecraft domain using PAL, comparing RAMP against PPO, PPO with masking, BC, GAIL, and PPO+DQN on two tasks of varying complexity.

## Key Results
- RAMP significantly outperforms PPO, PPO+masking, BC, GAIL, and PPO+DQN baselines on both Minecraft tasks.
- RAMP solves more problems and finds, in some cases, plans that are 2 orders of magnitude shorter than the compared RL algorithms.
- Ablation studies show that the learned model and shortcut search contribute significantly to RAMP's performance.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating reinforcement learning with learned action models enables more efficient exploration and higher-quality plan discovery than either approach alone.
- Mechanism: RL explores the environment and generates trajectory data. These trajectories are fed to the NSAM algorithm, which incrementally learns a symbolic numeric action model. The learned model is then used by a planner (Metric-FF) to find plans or shortcuts in RL-discovered trajectories. This creates a feedback loop: better models lead to better plans, which serve as high-quality training data for the RL agent.
- Core assumption: The environment is deterministic, fully observable, and the action model can be expressed as preconditions (conjunctions of linear inequalities) and effects (conjunctions of linear equations).
- Evidence anchors:
  - [abstract] "RAMP integrates reinforcement learning, action model learning, and numeric planning to leverage the strengths of both model-free and model-based approaches."
  - [section 5.1] "This integration of model-based and model-free algorithms in our hybrid strategy establishes a symbiotic relationship between them... the RL algorithm benefits from this partnership since trajectories created by the model-based algorithm... often tend to represent more efficient ways to solve the task at hand."
  - [corpus] No directly comparable hybrid mechanism in neighbors; [2512.20831] addresses parameterized actions in RL but not the model-based feedback loop.
- Break condition: Non-deterministic environments, partial observability, or action models requiring non-linear preconditions/effects violate NSAM's core assumptions, degrading model quality and plan reliability.

### Mechanism 2
- Claim: Safe action model learning provides formal guarantees on plan correctness, even with incomplete observations.
- Mechanism: NSAM learns a "safe" domain model, ensuring any plan valid under the learned model is executable in the real environment (soundness). This is achieved by over-approximating preconditions and maintaining precise effects.
- Core assumption: Trajectories are noise-free and fully observed. The model class (linear inequalities/equations) is sufficiently expressive for the domain.
- Evidence anchors:
  - [section 2.2] "NSAM is guaranteed to return a safe domain model... A domain model is called safe if every plan consistent with it is sound with respect to the real, unknown action model."
  - [abstract] "RAMP uses the learned domain model to attempt to find plans by calling an off-the-shelf domain-independent numeric planner."
  - [corpus] [2511.00673] discusses lifted successor generation for numeric planning but does not address the learning or safety guarantee aspect.
- Break condition: Noisy or partially observed trajectories violate NSAM's assumptions, potentially leading to unsound plans.

### Mechanism 3
- Claim: The shortcut search process optimizes RL trajectories by exploiting the structure of the learned action model.
- Mechanism: After an RL agent reaches a goal, RAMP removes loops and then checks if multi-step action sequences can be replaced by single, more efficient actions based on the learned model (e.g., replacing `move right, move up` with `move up right`). This shortened trajectory is fed back to the RL agent as higher-quality training data.
- Core assumption: The learned action model is sufficiently accurate to identify valid shortcuts.
- Evidence anchors:
  - [section 5.1] "The plan or the resulting shortened trajectory is then passed to the RL algorithm as an additional training example. This allows the RL algorithm to further improve its policy using higher-quality examples."
  - [Example 1 in section 5.1] Demonstrates loop removal and diagonal move shortcuts in a grid world.
  - [corpus] Corpus neighbors do not explicitly cover trajectory optimization via learned models.
- Break condition: An inaccurate learned model may propose invalid shortcuts, corrupting RL training data.

## Foundational Learning

- **Automated Planning (Numeric Planning)**
  - Why needed here: Core problem being solved. Understanding PDDL, states, actions (preconditions/effects), and plans is essential to grasp RAMP's objectives and the planner's role.
  - Quick check question: Can you define what a "precondition" and an "effect" are for an action in a planning domain?

- **Reinforcement Learning (RL), specifically Proximal Policy Optimization (PPO)**
  - Why needed here: PPO is the exploration engine in RAMP. Understanding policy gradients, on-policy learning, and rewards clarifies how the agent interacts with and learns from the environment.
  - Quick check question: What is the primary objective of a policy in a reinforcement learning setup?

- **Action Model Learning (SAM/NSAM family)**
  - Why needed here: This is the bridge between RL and planning. It is critical to understand how a symbolic model is induced from trajectory data and what a "safe" model guarantees.
  - Quick check question: What is the key difference between a model that is merely "consistent" with observations versus one that is "safe"?

## Architecture Onboarding

- **Component map:**
  Environment -> RL Agent (PPO) -> Action Model Learner (NSAM) -> Planner (Metric-FF) -> Shortcut Search Module -> Feedback to RL Agent

- **Critical path:**
  1. Episode Start: Agent in initial state `s0`.
  2. Planning Attempt: Planner tries to find a plan using the current learned model.
  3. Execution Branch:
      - If plan found: Execute the plan.
      - If no plan found: Follow the policy from the RL Agent (PPO).
  4. Trajectory Collection: Record state-action sequence until goal or termination.
  5. Model Update: If goal reached, update the learned model with the new trajectory.
  6. Shortcut Search: If goal was reached via RL, search for and apply shortcuts.
  7. RL Update: Train the RL Agent using the executed plan or shortened trajectory.

- **Design tradeoffs:**
  - Model Complexity vs. Safety: NSAM restricts to linear models for efficiency and safety, limiting use in domains with non-linear dynamics.
  - Planner Call Frequency: Planner calls are computationally expensive; redundant calls are avoided (e.g., not calling before first goal).
  - Exploration vs. Exploitation: RAMP relies on the RL agent's inherent exploration; more directed exploration could improve model learning efficiency.

- **Failure signatures:**
  - RL agent fails to reach the goal within step limit → No new data for model learning or shortcut search.
  - Learned model is inaccurate → Planner fails to find plans or finds invalid ones; shortcuts may be incorrect.
  - Planner times out → System falls back to RL policy, potentially missing optimal paths.

- **First 3 experiments:**
  1. Baseline Reproduction: Run PPO (with action masking) alone on Minecraft tasks to establish baseline success rates and plan lengths.
  2. Ablation Study: Run RAMP(-pn) (loop removal only), RAMP(-p) (model learning + shortcut search, no planner), and full RAMP to isolate the contribution of the learned model and planner.
  3. Trajectory Efficiency Analysis: After training, compare average plan lengths from PPO vs. RAMP-shortened trajectories to quantify the benefit of model-based optimization.

## Open Questions the Paper Calls Out
None

## Limitations
- NSAM algorithm implementation details (referenced from Mordoch et al., 2023) are not fully specified, including hyperparameters and threshold settings.
- Exact shortcut search termination conditions and the full set of predicates/functions used in PDDL state representation for the translation mechanism are not detailed.
- Assumes deterministic, fully observable environments and linear action models, which may limit applicability to more complex or non-deterministic domains.

## Confidence
- High Confidence: The integration of RL and planning via learned models (Mechanism 1) is well-supported by the experimental results and the logical flow of the approach.
- Medium Confidence: The safety guarantees of NSAM (Mechanism 2) are theoretically sound, but their practical impact depends on the quality of the trajectory data and the expressiveness of the linear model class.
- Low Confidence: The effectiveness of the shortcut search process (Mechanism 3) is demonstrated in examples but lacks extensive empirical validation across diverse scenarios.

## Next Checks
1. Reproduce Baseline Results: Run PPO (with action masking) alone on Minecraft tasks to establish baseline success rates and plan lengths, ensuring the environment and PPO setup are correctly configured.
2. Ablation Study: Implement and run RAMP(-pn) (loop removal only), RAMP(-p) (model learning + shortcut search, no planner), and full RAMP to isolate the contribution of the learned model and planner.
3. Trajectory Efficiency Analysis: After training, compare average plan lengths from PPO vs. RAMP-shortened trajectories to quantify the benefit of model-based optimization.