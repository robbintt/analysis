---
ver: rpa2
title: 'BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large
  Language Models'
arxiv_id: '2601.22925'
source_url: https://arxiv.org/abs/2601.22925
tags:
- bear
- beam
- search
- recommendation
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper identifies a critical training-inference inconsistency
  in LLM-based recommender systems: while supervised fine-tuning optimizes overall
  item probabilities, beam search can prematurely prune positive items with low-probability
  prefixes. To address this, the authors propose BEAR (Beam-SEarch-Aware Regularization),
  which enforces that each token in a positive item ranks within the top-B candidates
  at each decoding step.'
---

# BEAR: Towards Beam-Search-Aware Optimization for Recommendation with Large Language Models

## Quick Facts
- arXiv ID: 2601.22925
- Source URL: https://arxiv.org/abs/2601.22925
- Reference count: 40
- Key result: BEAR reduces incorrect pruning by 24.86% and improves NDCG@K by 12.50% average over SFT baselines

## Executive Summary
BEAR addresses a critical training-inference inconsistency in LLM-based recommender systems where beam search can prematurely prune positive items despite their high overall probability. The method enforces that each token in a positive item ranks within the top-B candidates at each decoding step, preventing early pruning while requiring no additional forward passes. Experiments on four Amazon datasets show BEAR significantly outperforms standard supervised fine-tuning with negligible computational overhead, achieving consistent gains across different LLM backbones and sizes.

## Method Summary
BEAR combines standard SFT cross-entropy loss with a beam-search-aware regularization term that prevents positive tokens from falling below the beam threshold at each decoding step. The regularization computes a pruning margin for each token position and applies a smooth sigmoid surrogate to make the discontinuous ranking condition differentiable. Since token logits are already computed during SFT, BEAR requires only a top-B operation on existing softmax outputs, making it computationally efficient. The method uses a Llama-3.2-3B backbone with LoRA fine-tuning and constrained beam search at inference.

## Key Results
- BEAR achieves 12.50% average improvement in NDCG@K and HitRatio@K metrics over SFT baselines
- Incorrect pruning is reduced by 24.86% compared to standard supervised fine-tuning
- BEAR maintains computational efficiency, matching SFT training time while being ~4.45× faster than full beam-simulation methods
- Performance gains are consistent across four real-world datasets and different LLM backbones

## Why This Works (Mechanism)

### Mechanism 1
Standard SFT creates a training-inference mismatch because maximizing overall item probability does not prevent early token-level pruning during beam search. Beam search retains only the top-B candidates at each decoding step based on prefix probability, so a positive item can achieve the highest overall probability while having low prefix probabilities at critical early tokens, causing premature pruning before the full item is generated.

### Mechanism 2
Optimizing a token-level necessary condition (each positive token must rank within top-B) effectively reduces incorrect pruning while avoiding costly beam search simulation during training. BEAR defines a pruning margin ΔB_t = log(βB_t) - log(P(y_t|y<t, x)), where βB_t is the B-th highest token probability at step t. Minimizing positive margins prevents tokens from falling below the beam threshold, providing a computationally efficient relaxation of the full prefix-ranking objective.

### Mechanism 3
A smooth sigmoid-based surrogate loss enables gradient-based optimization of the discontinuous ranking condition without additional forward passes. The indicator function I(ΔB_t > 0) is non-differentiable, so BEAR substitutes σ_ξ(ΔB_t) = 1/(1 + exp(-ΔB_t/ξ)), where ξ controls sharpness. The regularization term L_reg sums log(σ_ξ(ΔB_t)) across tokens, requiring only a top-B operation on existing softmax outputs.

## Foundational Learning

- **Concept: Beam Search Decoding**
  - Why needed here: The entire problem BEAR addresses arises from beam search's greedy pruning behavior. Understanding that beam search maintains only top-B prefixes per step—and why this creates prefix-level bottlenecks—is prerequisite.
  - Quick check question: Given a beam width B=3 and prefix probabilities [0.4, 0.35, 0.25] for ["The ", "A ", "Boc"], which prefixes survive, and why might this prune a high-probability complete item?

- **Concept: Autoregressive Token Probability Factorization**
  - Why needed here: BEAR operates at token granularity. The insight that P(item|prompt) = ∏ P(token_t | prefix_t, prompt) enables token-level intervention rather than sequence-level optimization.
  - Quick check question: If a 4-token item has token probabilities [0.25, 0.90, 0.95, 0.95] and beam width B=3 with competing tokens at step 1 having probabilities [0.30, 0.28, 0.17], will this item survive the first beam step? Why does overall probability not matter here?

- **Concept: Smooth Surrogates for Discrete Objectives**
  - Why needed here: The ranking condition "token must be in top-B" is discrete. Understanding why sigmoid/log transformations enable backpropagation—and their failure modes—is essential for debugging BEAR.
  - Quick check question: What happens to gradients when ΔB_t is very negative (token safely in top-B) versus very positive (token far below threshold)? How does temperature ξ affect this?

## Architecture Onboarding

- **Component map:**
Training Pipeline:
Standard SFT Forward Pass -> Compute softmax over vocabulary logits -> Extract βB_t = B-th highest probability -> Compute ΔB_t = log(βB_t) - log(P(positive_token)) -> Apply sigmoid surrogate: σ_ξ(ΔB_t), sum across tokens -> Combined Loss: L_BEAR = L_SFT + λ * L_reg

- **Critical path:** The top-B extraction per position is the key addition. This must happen on the same logits used for SFT cross-entropy—no re-computation. The softmax and top-B are O(V log B) where V=vocabulary size, negligible vs. LLM forward pass.

- **Design tradeoffs:**
  - λ (regularization weight): Too low → no effect; too high → sacrifices overall probability optimization for pruning safety, potentially degrading ranking quality
  - ξ (temperature): Controls gradient sharpness; requires dataset-specific tuning (paper suggests 0.25-3.0)
  - Beam width B in training vs. inference: Paper uses matched B=10; mismatch could cause over/under-regularization

- **Failure signatures:**
  - Pruning rate unchanged after BEAR: Check if λ is too low, or if token probabilities are already saturated (ΔB_t consistently negative)
  - NDCG improves but HitRatio flat: May indicate BEAR helps ranking among retrieved items but doesn't expand retrieval diversity
  - Training instability with large λ: Gradient conflicts between L_SFT and L_reg; reduce λ or increase ξ

- **First 3 experiments:**
  1. Ablate the regularization: Train with λ=0 (pure SFT) vs. λ=optimal vs. λ=10×optimal on a held-out validation split. Measure both NDCG@10 and PR@10 (pruning rate). Expect: λ=0 has high PR@10, λ=optimal has low PR@10 and high NDCG, λ=10× may have low PR@10 but degraded NDCG.
  2. Temperature sensitivity: Grid search ξ ∈ {0.1, 0.5, 1.0, 2.0, 5.0} with fixed λ. Plot gradient norms and final NDCG. Expect: very low ξ → gradient saturation; very high ξ → weak regularization signal.
  3. Beam-width mismatch stress test: Train BEAR with B_train ∈ {5, 10, 20} but inference with B_infer=10. Measure performance gaps. Expect: B_train < B_infer may over-regularize (wasteful but safe); B_train > B_infer may under-protect (missed pruning cases).

## Open Questions the Paper Calls Out

### Open Question 1
How can BEAR be effectively integrated with inference-stage techniques such as multi-token prediction or diverse beam search? The conclusion states exploring this integration would be a promising avenue, but combining training-time regularization with inference-time modifications requires careful coordination and could affect BEAR's theoretical guarantees.

### Open Question 2
What mechanisms cause the remaining ~30% of incorrect pruning cases that are not attributable to necessary condition violations, and can they be systematically addressed? Figure 4 shows necessary condition violations account for 70-75% of pruning cases, leaving 25-30% from other causes that BEAR does not directly target.

### Open Question 3
How does BEAR generalize to alternative decoding strategies beyond standard beam search, such as nucleus sampling or temperature-based sampling? The paper evaluates only constrained beam search with B=10, leaving other widely-used decoding strategies unexplored.

### Open Question 4
Can an adaptive or instance-dependent regularization weight λ improve upon the fixed hyperparameter setting, particularly for items with varying susceptibility to premature pruning? The hyperparameter sensitivity analysis shows performance trade-offs at different λ values, suggesting dynamic λ adjustment could optimize pruning protection per item.

## Limitations

- The method relies on a necessary (not sufficient) condition for beam search survival, leaving 25-30% of pruning failures unaddressed
- Performance critically depends on hyperparameter tuning (λ, ξ) that may be dataset-specific
- Effectiveness on extremely large vocabularies or streaming scenarios with frequent item additions remains unclear

## Confidence

**High Confidence Claims:**
- BEAR achieves computational efficiency gains over full beam-simulation methods
- BEAR consistently improves retrieval metrics (NDCG@K, HitRatio@K) across all four datasets
- BEAR reduces incorrect pruning rates by 24.86% compared to standard SFT

**Medium Confidence Claims:**
- The token-level necessary condition captures the dominant pruning failure mode
- BEAR's performance gains generalize across different LLM backbones and sizes
- BEAR's performance is stable across different fine-tuning objectives

**Low Confidence Claims:**
- The sigmoid surrogate provides optimal gradient properties
- BEAR's gains will transfer to streaming or cold-start scenarios
- The method's effectiveness on domains with extremely large or frequently changing item vocabularies

## Next Checks

1. **Path Interaction Validation:** Design synthetic datasets where items share long common prefixes but diverge at specific tokens. Measure BEAR's effectiveness when failure mode shifts from token-level pruning to path-dependent interactions, revealing whether BEAR's relaxation becomes insufficient as prefix complexity increases.

2. **Cross-Dataset Hyperparameter Transfer:** Train BEAR on one dataset (e.g., Office) and evaluate on others (Book, Toy, Clothing) using the same hyperparameters. Systematically vary λ and ξ to find if there's a transferable hyperparameter regime, revealing whether BEAR requires dataset-specific tuning or general principles exist.

3. **Scalability Stress Test:** Implement BEAR on synthetic datasets with exponentially increasing vocabulary sizes (10^3 to 10^6 items). Measure computational overhead and retrieval performance as vocabulary grows, plus test on streaming scenarios with continuous item addition, revealing practical limits on BEAR's applicability to large-scale or dynamic systems.