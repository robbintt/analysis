---
ver: rpa2
title: 'From Past To Path: Masked History Learning for Next-Item Prediction in Generative
  Recommendation'
arxiv_id: '2509.23649'
source_url: https://arxiv.org/abs/2509.23649
tags:
- masking
- user
- learning
- item
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses a limitation in generative recommendation
  systems, where models focus solely on predicting the next item without understanding
  the user's underlying intent and past behavior patterns. The proposed Masked History
  Learning (MHL) framework introduces an auxiliary training objective that reconstructs
  masked historical items alongside next-item prediction.
---

# From Past To Path: Masked History Learning for Next-Item Prediction in Generative Recommendation

## Quick Facts
- arXiv ID: 2509.23649
- Source URL: https://arxiv.org/abs/2509.23649
- Reference count: 12
- This paper proposes Masked History Learning (MHL) framework that significantly improves generative recommendation accuracy by reconstructing masked historical items alongside next-item prediction

## Executive Summary
This paper addresses a fundamental limitation in generative recommendation systems: they predict the next item without understanding the underlying user intent that shapes item sequences. The proposed Masked History Learning (MHL) framework introduces an auxiliary training objective that reconstructs masked historical items, forcing models to understand why item paths are formed rather than just what comes next. The framework combines entropy-guided masking to target the most informative positions and curriculum learning to transition from history reconstruction to future prediction. Experiments on three Amazon product categories demonstrate substantial improvements over state-of-the-art generative models, with MHL achieving 27.1% improvement in NDCG@5 for Sports and Outdoors.

## Method Summary
MHL augments standard autoregressive recommendation with an auxiliary masked history reconstruction task. Items are encoded into semantic IDs using Sentence-T5, PCA, and FAISS OPQ quantization (32 tokens per item). During training, historical items are masked at token, item, or mixed granularity, and the model learns to reconstruct them from contextual neighbors. Entropy-guided masking selects positions where the model shows highest predictive uncertainty, targeting informative boundaries. A three-phase curriculum transitions from random masking (Phase I) to entropy-guided masking (Phase II) to pure prediction (Phase III). The joint loss combines next-item prediction and masked reconstruction objectives. A Transformer decoder (448 hidden, 2 layers, 4 heads) is trained with AdamW and adaptive mask ratio decay.

## Key Results
- MHL achieves 27.1% improvement in NDCG@5 for Sports and Outdoors category
- Entropy-guided masking consistently outperforms random masking across all datasets and metrics
- 32-token codebook size provides optimal balance between capacity and sparsity
- Curriculum learning (random→entropy→no-mask) shows consistent superiority over direct inference

## Why This Works (Mechanism)

### Mechanism 1: Reconstruction Objective Forces Dependency Learning
Masked history reconstruction compels the model to learn logical item-to-item dependencies rather than surface-level co-occurrence patterns. By requiring the model to predict masked historical items from contextual neighbors, MHL creates gradient signals that strengthen representations encoding why items co-occur (e.g., tripod complements camera body), not just that they co-occur. This is operationalized through the joint loss: $L_{MHL} = \lambda_1 \cdot L_{next} + \lambda_2 \cdot L_{mask}$.

### Mechanism 2: Entropy-Guided Masking Targets Uncertainty
Masking high-entropy positions selectively amplifies learning signals at decision points where model understanding is weakest. The model computes predictive entropy $H(w_{it}^k)$ for each codeword. High entropy indicates the model cannot explain why an item belongs at that position. Masking these positions forces deeper contextual reasoning to reconstruct them, producing stronger gradients at informative boundaries.

### Mechanism 3: Curriculum Learning Bridges Reconstruction-to-Prediction Gap
Progressive phase transition (random → entropy-guided → no masking) aligns training with inference while preserving learned dependencies. Phase I learns basic patterns via easy random masking; Phase II deepens intent understanding via targeted entropy masking; Phase III fine-tunes on pure prediction to eliminate train-test discrepancy. The mask ratio decays when validation plateaus.

## Foundational Learning

- **Concept: Autoregressive Language Modeling**
  - Why needed here: MHL modifies the standard AR objective (predict next token given history) by adding reconstruction; understanding baseline AR is prerequisite.
  - Quick check question: Can you explain why next-token prediction alone might fail to capture long-range intent dependencies?

- **Concept: Masked Language Modeling (BERT-style)**
  - Why needed here: MHL adapts MLM to decoder-only architectures with unidirectional constraints; knowing bidirectional MLM helps contrast the design.
  - Quick check question: How does MHL's masked reconstruction differ from BERT4Rec's bidirectional masked prediction?

- **Concept: Curriculum Learning**
  - Why needed here: MHL's three-phase scheduler (warmup → entropy → fine-tuning) requires understanding how task difficulty progression affects convergence.
  - Quick check question: Why might starting with high-ratio entropy masking fail compared to the phased approach?

## Architecture Onboarding

- **Component map:** Input metadata → Sentence-T5 encoding → PCA → FAISS OPQ → 32-token semantic IDs → Transformer decoder → Masking module → Joint loss computation → AdamW optimization

- **Critical path:** 1) Encode item metadata → semantic IDs 2) Forward pass with masking applied to history 3) Compute entropy (requires separate forward on unmasked sequence in Phase II) 4) Update mask targets based on entropy ranking 5) Backprop through joint loss 6) Decay mask ratio when validation plateaus

- **Design tradeoffs:** Token-level vs item-level masking: Token-level captures fine-grained semantics; item-level forces holistic item understanding; mixed provides both but may dilute signal. Codebook size (Table 3): 32 tokens optimal; 8 lacks capacity, 64 introduces sparsity. Mask ratio (Table 4): Robust 0.15-0.25; higher ratios not consistently better.

- **Failure signatures:** Validation performance plateaus early: Mask ratio decaying too fast; extend Phase I warmup. Entropy estimates unstable: Check codebook sparsity; ensure sufficient temperature scaling. Fine-tuning phase degrades performance: Reconstruction task may have overfit; reduce Phase II duration. Text sequences underperform (Table 5): Raw text has higher noise; consider preprocessing or longer warmup.

- **First 3 experiments:** 1) Baseline sanity check: Reproduce Table 2 on single dataset (Beauty) comparing Direct Inference vs R→E→Inf; expect ~14% NDCG@5 improvement 2) Ablation on mask granularity: Compare token-level vs item-level vs mixed on fixed codebook size 32; verify token-level superiority on Beauty 3) Codebook size sensitivity: Reproduce Table 3 subset (sizes 16, 32, 64) on Toys dataset; expect peak at 32

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational overhead of the entropy-guided masking strategy scale with increasing item catalog sizes and sequence lengths compared to standard random masking? While the method improves accuracy, the efficiency of computing predictive entropy for every potential masking candidate in real-time or large-scale training loops is not benchmarked.

### Open Question 2
Can the MHL framework effectively distinguish between high-entropy items that represent complex user intent and those that are simply noisy or irrelevant interactions? The paper assumes a positive correlation between entropy and "informative" value, but an ablation on synthetic data with known noise labels is needed to prove the mask is not highlighting garbage signals.

### Open Question 3
Is the three-phase curriculum learning scheduler robust across domains with vastly different interaction densities, or does it require dataset-specific tuning? The curriculum relies on specific hyperparameters (decay rates, patience) that may not generalize to domains with sparser data (e.g., real estate) or denser data (e.g., short-video streams) without extensive re-tuning.

### Open Question 4
How does the MHL objective interact with Large Language Model (LLM) backbones regarding the "catastrophic forgetting" of pre-trained world knowledge? Adding a strong auxiliary reconstruction loss could potentially override the nuanced semantic priors learned during LLM pre-training, a common issue in instruction tuning.

## Limitations
- Framework heavily depends on rich, structured metadata and may underperform with sparse item information
- Computational overhead of entropy-guided masking not evaluated for large-scale deployment
- Curriculum learning effectiveness unverified across domains with different interaction densities
- No validation that reconstruction objective learns causal dependencies versus spurious correlations

## Confidence
- **High Confidence:** Joint reconstruction-objective improves next-item prediction metrics; Curriculum learning with phase transition consistently outperforms direct inference; 32-token codebook size achieves optimal balance
- **Medium Confidence:** Entropy-guided masking outperforms random masking; Token-level masking captures more nuanced dependencies; Mask ratio 0.15-0.25 provides robust performance
- **Low Confidence:** Reconstruction objective learns causal dependencies rather than spurious correlations; Entropy reliably identifies intent-obscured positions; Curriculum transfer of reconstruction skills to prediction

## Next Checks
1. **Ablation on entropy stability:** Track predictive entropy distributions across training phases on Beauty dataset. Verify that high-entropy positions correlate with known intent transitions rather than random noise or early-training instability.

2. **Curriculum transfer validation:** Implement frozen encoder experiments where MHL-trained encoders are compared against baseline encoders on reconstruction tasks only. If MHL encoders show superior reconstruction accuracy but baseline encoders achieve comparable prediction performance, the curriculum benefit may be optimization-driven.

3. **Metadata ablation study:** Train MHL variants on Amazon data with progressively reduced metadata (titles only → titles+price → full metadata). Measure degradation in NDCG@5 to quantify dependency on rich metadata versus interaction patterns alone.