---
ver: rpa2
title: 'LLMs versus the Halting Problem: Revisiting Program Termination Prediction'
arxiv_id: '2601.18987'
source_url: https://arxiv.org/abs/2601.18987
tags:
- witness
- program
- termination
- llms
- loop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper evaluates whether large language models (LLMs) can predict\
  \ program termination comparably to state-of-the-art symbolic verification tools.\
  \ The authors assess multiple LLMs\u2014including GPT-5, Claude Sonnet 4.5, Code\
  \ World Model (CWM), Qwen3-32B, and GPT-4o\u2014on the SV-Comp 2025 Termination\
  \ benchmark, which contains 2,328 C programs labeled as terminating or non-terminating."
---

# LLMs versus the Halting Problem: Revisiting Program Termination Prediction

## Quick Facts
- **arXiv ID:** 2601.18987
- **Source URL:** https://arxiv.org/abs/2601.18987
- **Reference count:** 40
- **Primary result:** GPT-5 and Claude Sonnet 4.5 achieve SV-Comp scores near top symbolic tools (3,520 and 3,448), with effective consensus voting exploiting asymmetric scoring.

## Executive Summary
This paper evaluates whether large language models can predict program termination comparably to state-of-the-art symbolic verification tools. The authors assess multiple LLMs—including GPT-5, Claude Sonnet 4.5, Code World Model (CWM), Qwen3-32B, and GPT-4o—on the SV-Comp 2025 Termination benchmark, which contains 2,328 C programs labeled as terminating or non-terminating. Models are prompted to output verdicts and, for non-terminating cases, a witness automaton graph in SV-Comp format. Results show GPT-5 and Claude Sonnet 4.5 achieve SV-Comp scores near the top tool (PROTON), with GPT-5 scoring 3,520 and Claude 3,448, while CWM performs just below the second-best tool (UAutomizer). GPT-4o, used as a non-reasoning baseline, scores much lower. F1 scores align with SV-Comp rankings, and performance degrades with longer code. While LLMs effectively predict termination, they often fail to generate valid witness graphs.

## Method Summary
The study evaluates LLMs on the SV-Comp 2025 Termination benchmark (2,328 C programs) using few-shot prompting for JSON outputs with termination verdicts and witness automata. Models generate 20 predictions per sample, with SV-Comp-TTS selecting 10 samples requiring unanimous consensus to avoid asymmetric penalties. Witness validity is checked using UAutomizer. The evaluation compares reasoning models (GPT-5, Claude Sonnet 4.5) against a non-reasoning baseline (GPT-4o) across multiple subcategories and token lengths, measuring SV-Comp scores and F1 metrics through 100 bootstrap iterations.

## Key Results
- GPT-5 achieves SV-Comp score of 3,520, near top symbolic tool PROTON, while GPT-4o baseline scores only 546
- Claude Sonnet 4.5 scores 3,448, demonstrating reasoning models significantly outperform non-reasoning baselines
- Performance degrades substantially with code length, suggesting context window limitations
- LLMs correctly predict termination verdicts but fail to generate valid witness automata in most cases (GPT-5: 41.4% validity rate)

## Why This Works (Mechanism)

### Mechanism 1: Test-Time Scaling (TTS) with Asymmetric Loss Avoidance
LLMs improve verification scores by utilizing consensus voting to avoid severe penalties for incorrect answers. The SV-Comp scoring scheme heavily penalizes false negatives (classifying a non-terminating program as terminating) with -32 points. By generating 10 samples and requiring unanimous consensus, the system outputs "Unknown" upon disagreement, preventing the model from committing to a low-confidence prediction that carries a high penalty. This abstention strategy exploits the asymmetric scoring metric rather than pure accuracy improvement.

### Mechanism 2: Reasoning-Mode Execution Simulation
Reasoning-mode models (GPT-5, Claude Sonnet 4.5) outperform non-reasoning baselines by effectively simulating execution traces rather than surface-level syntax matching. These models likely generate implicit or explicit chain-of-thought traces that simulate heap and control flow, acting as pseudo-interpreters to detect infinite loops. This capability is evidenced by high prediction unanimity in reasoning models versus substantial disagreement in GPT-4o. However, performance degrades significantly with code length, suggesting the simulation state is lost as token count increases.

### Mechanism 3: Distributional Alignment with Natural Verification Tasks
LLMs perform better on undecidable SV-Comp tasks than on decidable SAT/Planning tasks because the benchmark reflects "natural" software patterns rather than adversarial synthetic logic. Unlike SAT competitions designed to be hard for solvers, SV-Comp tasks often derive from real software bugs. LLMs, trained on vast code corpora, may recognize these specific patterns (e.g., common loop structures) more easily than abstract logic puzzles. This heuristic alignment fails when presented with adversarially constructed or obfuscated code that breaks tokenization patterns.

## Foundational Learning

- **Concept: Undecidability and Approximation**
  - **Why needed here:** To understand that 100% accuracy is theoretically impossible; the goal is to approximate top-performing symbolic tools, not to "solve" the Halting Problem.
  - **Quick check question:** Why does the paper compare LLMs to approximate verification tools rather than a ground-truth oracle?

- **Concept: Witness Automaton (The "Proof")**
  - **Why needed here:** The paper highlights a gap between predicting termination (high F1) and proving it (valid witness generation). A verdict without a witness is unverified.
  - **Quick check question:** Why is a "True Negative" (Terminating) worth +2 points, but a "True Positive" (Non-Terminating) with an invalid witness worth only 0?

- **Concept: Test-Time Scaling (TTS)**
  - **Why needed here:** The jump in performance for smaller models (CWM, Qwen) under TTS is crucial to the results. It frames the LLM not just as a static classifier but as a stochastic sampler.
  - **Quick check question:** How does requiring unanimous consensus among 10 samples specifically exploit the asymmetric scoring metric used in SV-Comp?

## Architecture Onboarding

- **Component map:** C Program -> JSON Prompt -> LLM Generator -> Sampler (20 outputs) -> TTS Consensus Selector (10 unanimous samples) -> Validator (UAutomizer) -> SV-Comp Score
- **Critical path:** The Witness Validator. The LLM can claim "Non-Termination," but the system only scores +1 if UAutomizer confirms the witness is valid. This is the bottleneck where LLMs often fail (see Table 3, Validity rates ~40%).
- **Design tradeoffs:**
  - **Graph vs. Domain Witness:** Experiment with GraphML automaton (Standard) vs. simple logical precondition (Domain). Start with Domain Witness prompts for higher Pass@1 rates, as they are easier for LLMs to generate correctly.
  - **Consensus Threshold:** Unanimous agreement (n=10) reduces false positives but increases "Unknown" outputs.
- **Failure signatures:**
  - **Formatting Errors:** CWM exhibits 25% formatting errors in witness generation (missing fields, duplicates).
  - **Long Context Drift:** Scores drop as token count increases; models "forget" the entry state or loop conditions in long programs.
- **First 3 experiments:**
  1. **Baseline vs. Reasoning:** Run GPT-4o vs. GPT-5 on a subset of MainControlFlow tasks without TTS to isolate the "reasoning" capability.
  2. **TTS Sensitivity:** Vary n (samples for consensus) from 3 to 20 to plot the curve of Score vs. Inference Cost.
  3. **Witness Format Ablation:** Compare acceptance rate of Graph-based witnesses vs. Domain Logical expressions on the 40 annotated samples to determine optimal output format for your specific verifier.

## Open Questions the Paper Calls Out

- **Can neuro-symbolic approaches combining LLMs with symbolic verification tools outperform current standalone methods?**
  - Basis: The conclusion explicitly identifies "exploring neuro-symbolic approaches" as a "promising direction."
  - Why unresolved: The study evaluated LLMs as standalone predictors rather than hybrid systems.
  - What evidence would resolve it: Developing a hybrid system that integrates LLM predictions with symbolic proof checkers and evaluating it on SV-Comp.

- **Do LLM termination prediction capabilities generalize to real-world codebases and Continuous Integration (CI) pipelines?**
  - Basis: Section 7 states the "ultimate assessment... should derive from deployment on real-world codebases."
  - Why unresolved: Evaluation was limited to the SV-Comp benchmark, which consists of self-contained, often synthetic programs.
  - What evidence would resolve it: Benchmarking model performance on large-scale, industrial repositories or live CI environments.

- **How can LLMs be enhanced to reliably generate valid witness automata for non-termination proofs?**
  - Basis: The abstract and results note LLMs "often fail to provide a valid witness as a proof."
  - Why unresolved: Models like GPT-5 showed high verdict accuracy (~98% F1) but low witness validity (~41%), often producing correct verdicts with invalid proofs.
  - What evidence would resolve it: Research into specialized training or decoding strategies that enforce the formal constraints of witness graph generation.

- **Can the reasoning capabilities demonstrated for termination extend to other undecidable semantic properties like memory safety?**
  - Basis: Section 1 mentions "unreachability, memory safety, and program equivalence" and Section 8 encourages research on "undecidable problems."
  - Why unresolved: The experimental scope was restricted solely to the Termination category of SV-Comp.
  - What evidence would resolve it: Replicating the evaluation methodology on other verification categories such as MemorySafety or NoOverflows.

## Limitations

- Witness generation remains unsolved, with even top models like GPT-5 achieving only 41.4% validity rate for correct NT predictions
- Performance degrades significantly with code length, indicating context window limitations rather than fundamental reasoning improvements
- Consensus voting mechanism (TTS) exploits asymmetric scoring rather than demonstrating pure predictive capability

## Confidence

**High Confidence:** The comparative results showing GPT-5 and Claude Sonnet 4.5 achieving SV-Comp scores near top symbolic tools (3,520 and 3,448 vs PROTON's score) are well-supported by the experimental data and bootstrap validation methodology.

**Medium Confidence:** The claim that LLMs perform better on naturally-occurring verification tasks versus synthetic benchmarks relies on correlational evidence from Section 7 and assumes training data distribution directly influences verification performance.

**Low Confidence:** The assertion that reasoning-mode models "simulate execution traces" as pseudo-interpreters remains speculative - the paper provides behavioral evidence (consensus rates, length sensitivity) but no direct evidence of internal simulation mechanisms.

## Next Checks

1. **Witness Format Ablation Study:** Systematically compare GraphML vs. Domain Logical witness acceptance rates across all models on a fixed subset to determine if simpler logical preconditions significantly improve valid witness generation rates.

2. **TTS Performance Curve:** Vary the consensus sample size (n=3, 5, 10, 15, 20) and plot the tradeoff between SV-Comp score improvement and inference cost to identify the optimal balance point for practical deployment.

3. **Code Length Performance Analysis:** Segment the benchmark by code length quartiles and measure F1 scores, consensus rates, and witness validity separately to quantify exactly where and how performance degradation occurs across different model families.