---
ver: rpa2
title: "O_FT@EvalLLM2025 : \xE9tude comparative de choix de donn\xE9es et de strat\xE9\
  gies d'apprentissage pour l'adaptation de mod\xE8les de langue \xE0 un domaine"
arxiv_id: '2507.04895'
source_url: https://arxiv.org/abs/2507.04895
tags:
- pour
- instructions
- donn
- dans
- nous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study adapts Mistral-7B-Instruct-v0.3 to the defense domain
  using continued pre-training and instruction-tuning. Data from defense documents
  and French journalism were processed into text segments, then used to generate diverse
  synthetic instructions covering summarization, titling, Q&A, and QCM tasks.
---

# O_FT@EvalLLM2025 : étude comparative de choix de données et de stratégies d'apprentissage pour l'adaptation de modèles de langue à un domaine

## Quick Facts
- arXiv ID: 2507.04895
- Source URL: https://arxiv.org/abs/2507.04895
- Reference count: 0
- Adapted Mistral-7B-Instruct-v0.3 to defense domain with PPE + instruction-tuning; achieved 13.2% QCM accuracy vs 5.6% baseline

## Executive Summary
This study adapts Mistral-7B-Instruct-v0.3 to the defense domain using a combination of continued pre-training (PPE) on domain documents and instruction-tuning on synthetically generated tasks. Data from defense documents and French journalism were processed into text segments, then used to generate diverse synthetic instructions covering summarization, titling, Q&A, and QCM tasks. Two adaptation modes were tested: closed (AMIAD data only) and open (AMIAD plus Ouest-France data). Models were evaluated on domain-specific and general benchmarks. Results show improved domain knowledge and task processing, with comparable or superior performance on general tasks. Continued pre-training improved factual recall but slightly degraded general multitask performance, while adding general instruction data restored general skills. Carbon footprint for adaptation was low, under 1 kg CO₂e per run.

## Method Summary
The method combines document preprocessing (PyMuPDF, DocTR, python-docx2txt), segmentation into fixed-length text blocks, synthetic instruction generation via GPT-4.1-mini with structured Pydantic schemas and diversity grammars, and two-stage training: continued pre-training (PPE) on domain documents followed by instruction-tuning (ASI) on synthetic instructions. Two data regimes were explored: closed (AMIAD only) and open (AMIAD plus French journalism). Training used DeepSpeed ZeRO-3, FlashAttention-2, gradient checkpointing, and single H200 GPU. Evaluation included domain-specific QCM and factual Q&A benchmarks plus general multilingual multitask benchmarks.

## Key Results
- PPE improved factual recall: 13.2% vs 5.6% baseline on QCM Défense
- Synthetic instruction models achieved 65.4% QCM accuracy vs 57.5% baseline
- Adding Tülu 3 Fr general instructions restored IFEval scores to acceptable levels (50.3/53.6% EN/FR vs 49.7/43.4% domain-only)
- Low carbon footprint: <1 kg CO₂e per adaptation run
- Models showed comparable or superior performance on general tasks despite domain specialization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continued pre-training (PPE) on domain documents improves factual knowledge recall but may degrade instruction-following behavior.
- Mechanism: PPE exposes the model to domain-specific vocabulary, entities, and relationships by training on raw document segments, shifting internal representations toward the target domain distribution.
- Core assumption: The model's pre-trained knowledge can be augmented without catastrophic forgetting if followed by adequate instruction-tuning.
- Evidence anchors:
  - [abstract] "Continued pre-training improved factual recall but slightly degraded general multitask performance"
  - [section 5.3.1] Models with PPE (run3, open runs) showed higher accuracy on factual Q&A (10.3-13.2%) vs. baseline (5.6%), but lower MOS scores on summarization/titling
  - [corpus] No directly relevant corpus evidence found; neighbor papers address unrelated domains (PINNs, revenue optimization, sign language recognition)
- Break condition: If PPE is applied without subsequent instruction-tuning, the model fails to output EOS tokens and generates indefinitely (noted in section 4.2).

### Mechanism 2
- Claim: Synthetic instruction generation from domain documents enables task-specific adaptation while maintaining domain grounding.
- Mechanism: GPT-4.1-mini generates diverse instructions (QCM, factual Q&A, summarization, titling) from document segments using structured outputs (Pydantic classes), with grammars to ensure formulation diversity.
- Core assumption: The quality of synthetic instructions depends on document preprocessing quality and coverage.
- Evidence anchors:
  - [section 3.1.1] "guarantee diversity in formulations" and "control format of produced instructions" via grammars and Pydantic schemas
  - [section 5.3.1] Models trained on synthetic instructions (run1, run2) achieved 65.4% QCM accuracy vs. 57.5% baseline
  - [corpus] Weak/missing — no corpus papers on synthetic instruction generation for domain adaptation
- Break condition: Poorly cleaned documents (residual headers, tables not properly parsed) produce instructions with spurious references or coreference issues that filtering cannot catch.

### Mechanism 3
- Claim: Including general-domain instruction data during fine-tuning restores general skills that degrade during domain adaptation.
- Mechanism: Adding filtered French instructions from Tülu 3 SFT mixture (5,891 instructions) provides exposure to diverse task formats (including code), counteracting domain specialization drift.
- Core assumption: The ratio of domain to general instructions must be balanced; excessive general data dilutes domain gains.
- Evidence anchors:
  - [section 5.3.3] "O_FT-Ouvert-run3, where Tülu 3 Fr instructions were used, allows for acceptable performance recovery on generative tasks"
  - [section 5.3.3] IFEval scores: run3 (50.3/53.6% EN/FR) vs. run1 (49.7/43.4%) with only domain instructions
  - [corpus] Weak/missing — no corpus papers specifically on mixing general instruction data for domain adaptation
- Break condition: If general instruction ratio is too high, domain-specific factual recall gains from PPE may be partially erased.

## Foundational Learning

- Concept: **Continued Pre-training vs. Instruction Fine-tuning**
  - Why needed here: PPE trains on raw text to absorb domain knowledge; instruction-tuning teaches task formats. Applying them sequentially is critical to the paper's approach.
  - Quick check question: Can you explain why PPE alone causes EOS token generation failure in an instructed model?

- Concept: **Synthetic Data Generation with Structured Outputs**
  - Why needed here: The paper relies on LLM-generated instructions with Pydantic-enforced schemas to ensure format consistency and diversity.
  - Quick check question: How would you design a Pydantic schema for a multi-choice QA task with justification fields?

- Concept: **Catastrophic Forgetting Mitigation via Data Mixing**
  - Why needed here: Domain adaptation risks losing general capabilities; adding general instruction data (Tülu 3 Fr) is the mitigation strategy.
  - Quick check question: What signals in evaluation would indicate that general skill degradation has occurred?

## Architecture Onboarding

- Component map:
  Document extraction (PyMuPDF, DocTR, python-docx2txt) -> Segmentation -> Instruction generation (GPT-4.1-mini with grammars/Pydantic) -> Filtering/curation -> PPE (trl.SFTTrainer, packing=True) -> ASI (packing=False, DataCollatorForCompletionOnlyLM) -> Evaluation

- Critical path:
  1. Extract and segment documents (section 2.2)
  2. Generate synthetic instructions with grammars and structured outputs (section 3.1)
  3. Run PPE on domain documents (section 4.2) — monitor for EOS token issues
  4. Run ASI with domain + general instructions (section 4.3)
  5. Evaluate on domain benchmarks (QCM, factual Q&A) and general benchmarks (MMLU, IFEval)

- Design tradeoffs:
  - Full fine-tuning vs. LoRA: Paper uses full fine-tuning for maximum domain absorption; LoRA would reduce carbon footprint (noted in section 6)
  - PPE inclusion: Gains factual recall (~2x improvement) but risks generative task degradation
  - Open vs. closed data: Open data (journalism) adds contemporary terminology but introduces external dependencies

- Failure signatures:
  - Infinite generation without EOS (PPE without proper EOS handling)
  - Low MOS scores on summarization/titling with looping or overly long outputs (PPE models without general instruction mixing)
  - Format violations on IFEval (insufficient instruction diversity)

- First 3 experiments:
  1. **Baseline replication**: Train ASI-only model (no PPE) on Gen AMIAD instructions; evaluate on QCM Défense and general MMLU to establish domain gain vs. forgetting baseline.
  2. **PPE ablation**: Add PPE before ASI; measure delta on factual Q&A accuracy and IFEval instruction-following to quantify the tradeoff.
  3. **General instruction mixing**: Add Tülu 3 Fr (5K instructions) to ASI; verify IFEval recovery and confirm domain benchmarks don't regress significantly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does parameter-efficient fine-tuning (e.g., LoRA) compare to full fine-tuning in terms of domain knowledge retention and carbon footprint reduction for this specific adaptation pipeline?
- Basis in paper: [explicit] The authors state in the Discussion that "it would be interesting to confront our conclusions with approaches of type LoRa in order to reduce the carbon footprint."
- Why unresolved: The study exclusively utilized full fine-tuning (updating all weights) for both continued pre-training and instruction-tuning phases.
- What evidence would resolve it: A comparative ablation study measuring accuracy on the AMIAD benchmarks and kWh consumption between LoRA-adapted models and the full fine-tuning baselines presented.

### Open Question 2
- Question: Does enriching the training corpus with OCR-derived table structures and generated image descriptions significantly improve performance on factual domain tasks compared to text-only extraction?
- Basis in paper: [explicit] The Discussion notes that "we did not perform additional processing to take tables correctly into account" and "did not exploit the images at all."
- Why unresolved: Images were treated purely via OCR text extraction, and complex layouts like tables were not parsed structurally.
- What evidence would resolve it: Evaluation scores on a "Gold" factual benchmark where the test set includes specific metrics for visual or tabular data dependencies.

### Open Question 3
- Question: Can reinforcement learning methods effectively adjust the response style (length and formatting) of the adapted models without inducing catastrophic forgetting of the specialized defense knowledge?
- Basis in paper: [inferred] The authors note that their models "produce responses rather brief" and that users preferred GPT-4's formatting, suggesting "approaches by reinforcement" for style.
- Why unresolved: The current study relied solely on supervised fine-tuning (SFT), which resulted in a stylistic regression (brevity) compared to the base model.
- What evidence would resolve it: Human or LLM-based evaluation of response length and formatting preference scores, correlated with accuracy retention on the defense-specific QCM benchmarks.

### Open Question 4
- Question: Does generating multiple instructions per document segment (rather than a 1:1 ratio) improve the model's coverage of domain knowledge or reduce hallucinations?
- Basis in paper: [explicit] The Discussion admits that "segments contain in the majority of cases enough content to generate several instructions" regarding different knowledge points.
- Why unresolved: The generation pipeline was limited to producing a single instruction per text segment to manage volume.
- What evidence would resolve it: A comparison of factual recall accuracy between models trained on 1-instruction datasets versus synthetic datasets with 2-3 instructions per segment.

## Limitations
- Evaluation relies heavily on synthetic benchmarks (QCM, factual Q&A) rather than human-annotated domain tests
- Carbon footprint measurement excludes preprocessing and dataset preparation costs
- General task performance evaluation may not fully capture subtle capability regressions

## Confidence
- High confidence in PPE improving factual recall and general instruction mixing restoring general skills
- Medium confidence in synthetic instruction generation quality and diversity
- Medium confidence in domain generalization based on multilingual benchmarks

## Next Checks
1. Add human-annotated domain-specific evaluation sets to validate synthetic instruction quality and factual accuracy
2. Conduct ablation studies with varying ratios of general to domain instructions to optimize the trade-off between specialization and generalization
3. Measure real-world inference latency and memory usage to complement carbon footprint analysis and assess practical deployment impact