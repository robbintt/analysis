---
ver: rpa2
title: Efficient Deployment of Spiking Neural Networks on SpiNNaker2 for DVS Gesture
  Recognition Using Neuromorphic Intermediate Representation
arxiv_id: '2504.06748'
source_url: https://arxiv.org/abs/2504.06748
tags:
- spinnaker2
- quantization
- weights
- training
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first benchmark for deploying spiking neural
  networks (SNNs) on the SpiNNaker2 neuromorphic chip for DVS gesture recognition.
  The authors address the challenge of optimizing SNNs for resource-constrained edge
  hardware through weight quantization while preserving accuracy.
---

# Efficient Deployment of Spiking Neural Networks on SpiNNaker2 for DVS Gesture Recognition Using Neuromorphic Intermediate Representation

## Quick Facts
- arXiv ID: 2504.06748
- Source URL: https://arxiv.org/abs/2504.06748
- Reference count: 33
- Primary result: 94.13% on-chip classification accuracy on DVS Gesture dataset using 8-bit quantized SNNs on SpiNNaker2

## Executive Summary
This paper presents the first benchmark for deploying spiking neural networks (SNNs) on the SpiNNaker2 neuromorphic chip for DVS gesture recognition. The authors address the challenge of optimizing SNNs for resource-constrained edge hardware through weight quantization while preserving accuracy. They develop two quantization pipelines: post-training quantization (PTQ) with percentile-based threshold scaling and quantization-aware training (QAT) with adaptive threshold scaling. Both methods achieve 8-bit fixed-point inference on SpiNNaker2, closely matching 32-bit floating-point performance while providing significant memory savings (4x reduction).

## Method Summary
The authors propose two quantization pipelines for deploying SNNs on SpiNNaker2. The first is a post-training quantization (PTQ) approach that scales weights based on the 100th percentile of the weight distribution and applies the same scaling factor to neuron thresholds. The second is a quantization-aware training (QAT) approach using Brevitas that trains the network with quantized weights and applies adaptive threshold scaling. Both pipelines convert the trained PyTorch models to Neuromorphic Intermediate Representation (NIR) for deployment on SpiNNaker2. The network architecture consists of three convolutional layers followed by two fully connected layers with leaky integrate-and-fire neurons, trained on the DVS Gesture dataset (11 classes, 128×128 downsampled to 32×32).

## Key Results
- Achieved 94.0% accuracy on-chip with PTQ pipeline (0.56% drop from baseline)
- Achieved 94.13% accuracy on-chip with QAT pipeline (0.56% drop from baseline)
- 8-bit fixed-point inference closely matches 32-bit floating-point performance
- 4x memory footprint reduction through quantization
- First benchmark demonstrating NIR deployment for quantized SNNs on SpiNNaker2

## Why This Works (Mechanism)
The quantization pipelines work by carefully scaling both weights and neuron thresholds to maintain the signal-to-noise ratio in the quantized domain. The percentile-based scaling ensures that the full dynamic range of 8-bit weights is utilized while preventing overflow. The adaptive threshold scaling in QAT further optimizes this relationship during training. The NIR framework provides a hardware-agnostic intermediate representation that can be efficiently compiled to the SpiNNaker2 architecture, enabling seamless deployment from training to execution.

## Foundational Learning

**Neuromorphic Hardware**: Specialized computing systems that mimic biological neural networks for low-power, event-driven computation
- Why needed: Enables energy-efficient processing of event-based sensor data like DVS
- Quick check: Verify SpiNNaker2's 147 PEs and 64KB SRAM per PE constraints

**Event-based Vision**: DVS sensors capture per-pixel intensity changes as asynchronous events rather than frames
- Why needed: Provides sparse, temporal data ideal for SNN processing
- Quick check: Confirm 32×32 spatial downsampling and 1ms temporal binning

**Weight Quantization**: Converting high-precision weights to lower-bit representations
- Why needed: Reduces memory footprint and enables deployment on resource-constrained hardware
- Quick check: Verify 4x memory reduction from 32-bit to 8-bit weights

**Threshold Scaling**: Adjusting neuron firing thresholds proportionally to weight scaling
- Why needed: Maintains activation statistics after quantization
- Quick check: Confirm threshold scaling factor equals weight scaling factor

**Neuromorphic Intermediate Representation (NIR)**: Hardware-agnostic intermediate format for SNN deployment
- Why needed: Enables separation of training framework from hardware-specific compilation
- Quick check: Verify NIR conversion and py-spinnaker2 deployment pipeline

## Architecture Onboarding

**Component Map**: PyTorch model -> NIR graph -> py-spinnaker2 -> SpiNNaker2 PEs
- Data flows: Events → DVS preprocessing → SNN layers → NIR → SpiNNaker2 execution

**Critical Path**: Input events → Spatial/temporal preprocessing → Convolutional layers → Fully connected layers → Classification output

**Design Tradeoffs**: 
- Accuracy vs memory: 8-bit quantization provides 4x memory savings with <1% accuracy drop
- Timesteps vs gesture duration: Hardware simulation limited to ~600 timesteps vs full 6-second gestures
- Precision vs complexity: Simple percentile scaling vs adaptive QAT training

**Failure Signatures**: 
- Memory overflow: Model exceeds PE SRAM capacity (check Table V/VI constraints)
- Accuracy degradation: Quantization introduces significant noise (>1% drop)
- Deployment errors: NIR conversion or py-spinnaker2 compilation issues

**First Experiments**:
1. Verify DVS Gesture dataset preprocessing with Tonic denoising and 32×32 downsampling
2. Train baseline P-SNN model for 200 epochs with specified hyperparameters
3. Apply PTQ with 100th percentile scaling and deploy to SpiNNaker2 via NIR

## Open Questions the Paper Calls Out

**Open Question 1**: Can the proposed SNN models maintain accuracy and energy efficiency when scaled across multiple SpiNNaker2 chips in a distributed computing scenario?
- Basis: Authors state models "can leverage SpiNNaker2's scalable design to distribute workloads efficiently"
- Why unresolved: Current study limited to single-chip (147 PEs)
- Resolution evidence: Multi-chip deployment benchmarks measuring latency and accuracy

**Open Question 2**: Does utilizing LPDDR4 memory for storing input spike streams reduce system ticks enough to enable robust real-time processing for full-duration gestures?
- Basis: Authors plan to "utilize the LPDDR4 memory on the SpiNNaker2 chip to store input spike streams"
- Why unresolved: Current py-spinnaker2 limits simulations to on-chip SRAM (~600 timesteps)
- Resolution evidence: Latency and "system ticks per second" measurements with DRAM driver

**Open Question 3**: To what extent do discrepancies in membrane voltage reset timing between software simulation and hardware implementation contribute to on-chip accuracy degradation?
- Basis: Authors speculate accuracy drop could be due to "differences between software implementation and hardware behavior"
- Why unresolved: Potential mismatch identified but not quantified
- Resolution evidence: Ablation study modifying snnTorch reset to match hardware behavior

## Limitations
- Implementation details for surrogate gradient formula and QAF fine-tuning step are not fully specified
- Weight initialization scheme and dataset train/validation split details are not explicitly stated
- Current deployment limited to on-chip SRAM, restricting input duration to ~600 timesteps
- Hardware-software reset timing discrepancies may contribute to accuracy gaps

## Confidence
- High confidence in overall methodology and hardware deployment pipeline
- Medium confidence in precise quantization parameter choices (percentile values, threshold scaling factors)
- Medium confidence in reproducibility of exact accuracy numbers due to unspecified implementation details

## Next Checks
1. Verify surrogate gradient implementation matches the "fast sigmoid" formula and test its impact on training convergence
2. Profile different percentile values (p=99 vs p=100) to confirm optimal weight scaling and investigate outlier effects on quantization accuracy
3. Compare snnTorch simulation timesteps with SpiNNaker2 deployment timesteps to identify potential sources of on-chip vs off-chip accuracy gaps