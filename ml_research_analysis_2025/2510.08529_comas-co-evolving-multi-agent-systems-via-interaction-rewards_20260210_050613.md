---
ver: rpa2
title: 'CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards'
arxiv_id: '2510.08529'
source_url: https://arxiv.org/abs/2510.08529
tags:
- arxiv
- comas
- uni00000013
- uni00000048
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Co-Evolving Multi-Agent Systems (CoMAS),
  a novel framework for enabling LLM-based agents to improve autonomously through
  inter-agent interaction without external supervision. CoMAS uses three core components:
  interaction (collaborative solution proposal, evaluation, and scoring), reward formulation
  (LLM-as-a-judge mechanism deriving intrinsic rewards from discussion dynamics),
  and policy optimization (REINFORCE++ algorithm updating agent policies).'
---

# CoMAS: Co-Evolving Multi-Agent Systems via Interaction Rewards

## Quick Facts
- arXiv ID: 2510.08529
- Source URL: https://arxiv.org/abs/2510.08529
- Reference count: 40
- Primary result: Framework achieves absolute gains up to 2.20%, 3.66%, 19.80%, and 6.10% over baselines across multiple benchmarks through inter-agent interaction rewards

## Executive Summary
CoMAS introduces a novel framework for enabling LLM-based agents to improve autonomously through inter-agent interactions without external supervision. The system uses collaborative solution proposal, evaluation, and scoring phases to generate intrinsic rewards, which are then used to update agent policies via a modified REINFORCE algorithm. Experimental results demonstrate consistent performance improvements across various benchmarks, with particularly strong gains in specialized domains like science and mathematics.

## Method Summary
CoMAS employs four LLM-based agents that interact through solution proposal, evaluation, and scoring phases. Each agent alternates between solver, evaluator, and scorer roles, generating discussion histories that are used to formulate intrinsic rewards via an LLM-as-a-judge mechanism. The system uses REINFORCE++ for policy optimization, applying token-level credit assignment with KL regularization to prevent catastrophic forgetting. Training is performed on a 2000-sample dataset across math, coding, and science domains, with evaluation on seven benchmarks using four different inference setups.

## Key Results
- CoMAS consistently outperforms untrained agents across most settings
- Absolute gains up to 2.20%, 3.66%, 19.80%, and 6.10% over baselines in various benchmarks
- Heterogeneous agents outperform homogeneous counterparts by 2.13-2.78%
- Ablation studies confirm interaction-based rewards prevent training collapse and reward hacking

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial reward formulation creates stable learning dynamics by balancing solution quality and evaluation rigor
- Mechanism: Zero-sum reward design (r(solution) + r(evaluation) = 1) ensures solvers are rewarded only when evaluators fail to find legitimate flaws
- Core assumption: LLMs can reliably distinguish helpful from unhelpful criticism when prompted to score solutions
- Evidence: Limited direct corpus support; related work on credit assignment in MARL addresses reward decomposition but not adversarial formulation
- Break condition: If evaluation prompts become too lenient or strict, the reward signal loses discriminative power

### Mechanism 2
- Claim: Multi-agent diversity provides richer training signal distribution than single-agent self-rewarding
- Mechanism: Heterogeneous agents bring distinct reasoning patterns and failure modes, generating non-redundant training signals
- Core assumption: Different foundation models have sufficiently diverse error profiles
- Evidence: Heterogeneous agents consistently outperformed homogeneous counterparts by 2.13-2.78% in various setups
- Break condition: If agents share identical error patterns, diversity advantage collapses to self-consistency

### Mechanism 3
- Claim: Token-level credit assignment with KL regularization prevents catastrophic forgetting while enabling policy improvement
- Mechanism: REINFORCE++ applies per-token advantage computation with cumulative KL-divergence penalty, then normalizes across batch
- Core assumption: Trajectory-level rewards can be meaningfully distributed across all generated tokens
- Evidence: Integration within single agent interactions, while related work uses separate optimizers
- Break condition: If KL penalty weight is set too high, policy freezes; if too low, reward hacking may emerge

## Foundational Learning

- Concept: Policy gradient with baseline (REINFORCE)
  - Why needed here: CoMAS uses advantage estimation A_t = r(o) - baseline; understanding how gradients flow from rewards to token probabilities is essential
  - Quick check question: If average normalized reward is 0.5 across all agents, what does this indicate about the solver-evaluator equilibrium?

- Concept: LLM-as-a-judge evaluation
  - Why needed here: The scoring step extracts numerical rewards from LLM text outputs; prompt engineering directly affects reward calibration
  - Quick check question: How would you modify the scoring prompt if agents consistently assign score=2 regardless of solution quality?

- Concept: Multi-agent credit assignment
  - Why needed here: With 4 agents each taking solver/evaluator/scorer roles, determining which agent's policy caused a trajectory outcome is non-trivial
  - Quick check question: If agent A generates a flawed solution that agent B fails to critique, which agent should receive negative reward?

## Architecture Onboarding

- Component map: Question → Agent Pool (4 agents) → Solution Phase → Discussion History → Evaluation Phase → Critique text → Scoring Phase → Extract score → Normalize to [0,1] → Replay Buffer → REINFORCE++ update

- Critical path:
  1. Prompt templates for solution/evaluation/scoring (Appendix B) — directly control reward distribution quality
  2. Score extraction function (Section 3.2, Eq. 6) — must parse <score>X</score> format reliably
  3. Reward normalization (Eq. 7) — zero-sum constraint must be exact or equilibrium breaks

- Design tradeoffs:
  - More agents → more diverse training data but O(l²) interaction samples (1 agent=48k samples, 4 agents=768k)
  - Longer discussion history (κ) → richer context but higher token cost
  - Higher training temperature → more exploration but noisier rewards

- Failure signatures:
  - Rewards monotonically decreasing → evaluation step removed or too strict
  - Rewards → 1.0 → scoring step removed, agents collude
  - Training collapse → invalid score format extraction returns -1 penalty repeatedly

- First 3 experiments:
  1. Replicate single-agent vs. 4-agent comparison on held-out math subset to validate scalability claims
  2. Ablate the KL penalty weight β (currently 0.0) with β∈{0.01, 0.1, 0.5} to test if regularization improves stability
  3. Test heterogeneous setting with 3 agents (Qwen-3B, Llama-3.2-3B, Mistral-7B) to verify diversity gains scale with model family separation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can CoMAS scale efficiently to significantly larger agent populations (e.g., 10+ or 100+ agents) given the quadratic growth in interaction samples and computational cost?
- Basis: Table 3 shows interaction samples grow from 48k to 768k (16×) and generated tokens from 1.6B to 25.2B when increasing agents from 1 to 4
- Why unresolved: Only 2-4 agents were tested, leaving scalability ceiling unexplored
- What evidence would resolve it: Experiments with larger agent populations showing performance scaling and resource requirements

### Open Question 2
- Question: Would alternative reward formulations beyond the 3-level scoring and zero-sum design yield better learning dynamics or prevent other failure modes?
- Basis: Section 4.3.1 shows removing evaluation or scoring causes training collapse, but only tests two ablated variants
- Why unresolved: Space of possible reward formulations remains largely unexplored
- What evidence would resolve it: Systematic comparison of alternative reward schemes with analysis of resulting training dynamics

### Open Question 3
- Question: How does CoMAS perform on domains beyond structured reasoning tasks (math, coding, science), such as creative writing, open-ended dialogue, or real-world planning tasks?
- Basis: Ethics statement mentions potential extension to real-world applications; evaluation limited to benchmarks with verifiable answers
- Why unresolved: Scoring mechanism relies on identifying "correctness" which may not transfer to subjective domains
- What evidence would resolve it: Experiments on creative/subjective benchmarks with appropriate evaluation protocols

### Open Question 4
- Question: What are the long-term co-evolution dynamics—do agents converge to stable equilibria, exhibit emergent specialization, or oscillate indefinitely with extended training?
- Basis: Figure 3 shows training dynamics up to ~30 steps with rewards converging around 0.5, but trajectory beyond this horizon unexamined
- Why unresolved: Limited training epochs and no analysis of emergent agent behaviors or role differentiation over extended co-evolution
- What evidence would resolve it: Extended training experiments with analysis of agent specialization patterns and convergence properties

## Limitations
- Training sample size (2000 samples) appears small relative to claimed generalization gains
- Reward calibration stability depends on continuous monitoring of LLM-as-a-judge prompts
- Model diversity assumption needs validation across different model families and architectures

## Confidence

- High Confidence: Core mechanism of adversarial reward formulation preventing training collapse is well-supported by ablation studies
- Medium Confidence: Scalability claims demonstrated but rely on specific task distributions; diversity benefits assume meaningful error profile differences
- Low Confidence: Generalization to tasks outside training distribution is suggested but not rigorously tested

## Next Checks
1. Apply CoMAS-trained agents to completely unseen domains (e.g., legal reasoning, creative writing) to assess generalization beyond training distribution
2. Run extended training sessions (100+ steps) to monitor reward equilibrium stability and detect potential gradual reward inflation or deflation over time
3. Systematically reduce agent count below 4 to identify minimum number required for stable adversarial reward dynamics