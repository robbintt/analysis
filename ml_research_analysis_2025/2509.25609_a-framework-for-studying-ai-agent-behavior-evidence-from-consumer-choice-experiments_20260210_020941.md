---
ver: rpa2
title: 'A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice
  Experiments'
arxiv_id: '2509.25609'
source_url: https://arxiv.org/abs/2509.25609
tags:
- product
- agents
- gid00001
- agent
- price
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ABxLAB, a framework for systematically studying
  AI agent decision-making by intercepting and modifying web content in real time.
  The authors apply it to consumer choice experiments in a realistic web-based shopping
  environment, varying prices, ratings, and psychological nudges.
---

# A Framework for Studying AI Agent Behavior: Evidence from Consumer Choice Experiments

## Quick Facts
- arXiv ID: 2509.25609
- Source URL: https://arxiv.org/abs/2509.25609
- Reference count: 39
- Agents exhibit 3–10× stronger biases than humans in response to ratings, prices, and nudges

## Executive Summary
This paper introduces ABxLAB, a framework for systematically studying AI agent decision-making through real-time web content interception and modification. The authors apply it to consumer choice experiments in a realistic e-commerce environment, finding that LLM agents exhibit strong systematic biases in response to ratings, prices, and psychological nudges. Effect sizes are often 3–10× larger than human baselines, with agents showing hypersensitivity to ratings (30–80pp preference for higher-rated products), prices (15–65pp preference for cheaper options), and persuasive nudges (10–60pp shifts). These findings highlight risks of agent manipulation and demonstrate the potential of consumer choice as a testbed for a behavioral science of AI agents.

## Method Summary
The ABxLAB framework intercepts web content in real-time, allowing controlled manipulation of product attributes and persuasive cues. The method uses a 2AFC experimental design in a realistic e-commerce environment, varying prices, ratings, and 10 types of psychological nudges. Agents navigate two browser tabs, compare products, and add the "best" item to cart. The framework applies intervention functions to modify observations before agents perceive them, enabling causal identification of decision drivers. Statistical analysis uses linear probability models with cluster-robust standard errors and Benjamini-Hochberg correction for multiple testing.

## Key Results
- Agents show 30–80pp preference for higher-rated products, with some models exhibiting near-deterministic selection based on ratings alone
- Price sensitivity reaches 15–65pp preference for cheaper options, intensifying when ratings are matched
- Persuasive nudges shift agent choices by 10–60pp, with "best seller" and "Wirecutter's top pick" cues being particularly effective
- Effect sizes are 3–10× larger than human baselines, suggesting different underlying decision mechanisms
- User preference specifications function as categorical switches that override environmental cues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real-time content interception enables causal identification of decision drivers in agents
- Mechanism: The framework positions an intervention engine between the live web environment and the agent, applying transformations `I: O → O` to observations before the agent perceives them
- Core assumption: Agents do not detect or compensate for synthetic modifications
- Evidence anchors: Formalized intervention functions in Section 3.1; no direct corpus evidence on intervention architectures

### Mechanism 2
- Claim: Agents employ hierarchical, near-deterministic decision rules that amplify single cues
- Mechanism: When a dominant cue (e.g., higher rating) is available, agents weight it heavily (up to 81.2pp selection shift)
- Core assumption: Observed behavior reflects underlying decision architecture
- Evidence anchors: Section 4 documents hierarchical patterns; "Super-additive Cooperation in Language Model Agents" shows similar binary-response patterns

### Mechanism 3
- Claim: User preference specifications function as threshold switches that override environmental cues
- Mechanism: Explicit user profiles reconfigure agent decision rules, suppressing competing attributes
- Core assumption: Agents faithfully interpret and prioritize natural-language preference specifications
- Evidence anchors: Section 4.1 shows categorical switch behavior; no direct corpus evidence on preference-override mechanisms

## Foundational Learning

- Concept: Two-alternative forced choice (2AFC) experimental design
  - Why needed here: The benchmark uses 2AFC with validity constraints to isolate causal effects of individual attributes
  - Quick check question: Given products A ($20, 4.5★) and B ($25, 4.3★), is this a valid pair under the Original regime?

- Concept: Linear probability models with cluster-robust standard errors
  - Why needed here: Coefficients are interpretable as percentage-point changes; clustering accounts for within-group correlation
  - Quick check question: If a coefficient is +35.4pp for "nudged," what does this mean in terms of selection probability?

- Concept: Nudge taxonomy (authority, social proof, scarcity, negative framing, incentives)
  - Why needed here: The framework tests 10 specific nudge interventions; understanding their psychological basis aids interpretation
  - Quick check question: Which nudge type showed the largest average effect across models?

## Architecture Onboarding

- Component map: Intervention engine -> Product pair selector -> Agent harness -> Analysis pipeline
- Critical path: 1) Select valid pairs, 2) Configure intervention, 3) Initialize agent, 4) Agent explores tabs, 5) Agent executes cart-add action, 6) Log choice outcome
- Design tradeoffs: Internal validity vs. ecological breadth; token budget vs. exploration depth; real web content vs. reproducibility
- Failure signatures: Near-deterministic order effects; strong rating-dominance; rapid termination suggests under-exploration
- First 3 experiments: 1) Replicate Original condition on 3 models to validate framework, 2) Run matched-ratings condition to test hierarchical hypothesis, 3) Test Wirecutter authority nudge across 5 models with/without preference instructions

## Open Questions the Paper Calls Out

- Do agent behavioral biases generalize to domains beyond consumer choice, such as medical decision-making, financial planning, or legal reasoning?
- How does agent susceptibility to nudges change in larger, multimodal choice sets versus binary forced-choice paradigms?
- Do agents that deliberate longer exhibit systematically different sensitivity to nudges compared to faster agents?
- What are the underlying mechanisms driving agent hypersensitivity to nudges—biased training data, architectural properties, or prompting conventions?

## Limitations
- Binary choice paradigm may not generalize to real-world agent behavior where larger choice sets are common
- 10-action budget constraint could suppress more deliberative decision processes
- Results may be properties of the specific consumer choice task rather than general decision-making tendencies
- Framework assumes agents process manipulated content as authentic without detection

## Confidence
- **High confidence**: Technical implementation and statistical methodology are sound with appropriate corrections
- **Medium confidence**: Qualitative patterns of agent susceptibility appear robust across model families
- **Low confidence**: Claims about underlying decision mechanisms are speculative and not directly tested

## Next Checks
1. **Detection vulnerability test**: Run control condition with visible "synthetic" markers to test if intervention effects persist when agents can potentially detect manipulation
2. **Task generalization validation**: Replicate benchmark with three-option choices and open-ended shopping lists to measure effect persistence in less constrained environments
3. **Model architecture ablation**: Test whether observed biases correlate with specific architectural features rather than model scale or capability