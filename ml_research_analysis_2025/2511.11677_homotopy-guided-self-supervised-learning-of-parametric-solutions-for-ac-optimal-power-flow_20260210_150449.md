---
ver: rpa2
title: Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal
  Power Flow
arxiv_id: '2511.11677'
source_url: https://arxiv.org/abs/2511.11677
tags:
- power
- homotopy
- learning
- problem
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a homotopy-guided training approach for self-supervised
  learning to optimize (L2O) in highly nonconvex constrained optimization problems,
  with a focus on AC Optimal Power Flow (AC-OPF). The key idea is to construct a continuous
  deformation of the objective and constraints during training, starting from a relaxed
  problem with a broad basin of attraction and gradually transforming it toward the
  original problem.
---

# Homotopy-Guided Self-Supervised Learning of Parametric Solutions for AC Optimal Power Flow

## Quick Facts
- arXiv ID: 2511.11677
- Source URL: https://arxiv.org/abs/2511.11677
- Authors: Shimiao Li; Aaron Tuor; Draguna Vrabie; Larry Pileggi; Jan Drgona
- Reference count: 33
- Primary result: Homotopy-guided L2O improves feasibility rates in AC-OPF from 0.7% to near 100% on IEEE test cases

## Executive Summary
This paper introduces a homotopy-guided self-supervised learning approach for parametric optimization, specifically targeting AC Optimal Power Flow (AC-OPF) problems. The key innovation is constructing a continuous deformation path during training that transforms a relaxed, convex problem into the original highly nonconvex AC-OPF problem. This approach addresses the fundamental challenge of local minima in nonconvex optimization by starting training from problems with broad basins of attraction and gradually approaching the true problem. The method demonstrates significant improvements in feasibility rates compared to standard penalty-based approaches while maintaining competitive objective values.

## Method Summary
The paper proposes two complementary homotopy strategies: relaxation-based homotopy and AC-OPF-aware homotopy. Relaxation-based homotopy convexifies objectives, shrinks bounds, and grows penalty terms to guide the model from easy to hard problems. AC-OPF-aware homotopy employs load-stepping and Tx-stepping strategies that incrementally increase loading conditions and transformer tap positions. The self-supervised learning framework uses parametric solutions (L2O) where a neural network directly maps problem parameters to optimal solutions. During training, the homotopy schedule gradually morphs the loss function from a relaxed version of AC-OPF toward the original formulation, allowing the model to first learn in regions with good convexity properties before tackling the full nonconvex problem.

## Key Results
- On IEEE 30-bus system: constraint violations reduced from 0.013 to near zero using homotopy methods versus standard penalty approach
- On random non-convex problems: homotopy methods maintained near-zero constraint violations while non-homotopy methods failed completely as problem complexity increased
- Homotopy-guided L2O achieved objective values comparable to full OPF solvers while dramatically improving feasibility rates across all test cases

## Why This Works (Mechanism)
The mechanism succeeds by leveraging the principle that neural networks trained on simpler, more convex problems can transfer knowledge to harder, nonconvex problems when guided through a continuous deformation path. By starting with problems that have broad basins of attraction and gradually introducing nonconvexity, the model avoids getting trapped in poor local minima. The homotopy schedule acts as a curriculum that systematically increases problem difficulty while maintaining enough structure for the network to learn meaningful patterns. This is particularly effective for AC-OPF where the feasible region is often disconnected and riddled with local minima.

## Foundational Learning
- Parametric Optimization (L2O): Why needed - Direct mapping from problem parameters to solutions; Quick check - Verify network architecture supports variable-sized inputs
- Homotopy Methods: Why needed - Continuous deformation between problem instances; Quick check - Confirm smooth transitions in loss function
- Self-Supervised Learning: Why needed - No labeled optimal solutions required; Quick check - Validate unsupervised objective function
- AC Optimal Power Flow: Why needed - Benchmark nonconvex constrained optimization; Quick check - Verify power flow constraints are properly encoded
- Nonconvex Optimization: Why needed - Real-world problems often nonconvex; Quick check - Test performance on known nonconvex benchmarks
- Curriculum Learning: Why needed - Gradual difficulty increase aids convergence; Quick check - Monitor training stability across homotopy stages

## Architecture Onboarding

**Component Map**: Input parameters -> Neural Network -> Solution output -> Homotopy scheduler -> Modified loss function -> Training loop

**Critical Path**: Data generation → Network initialization → Homotopy stage 1 (relaxed problem) → Progressive homotopy stages → Final AC-OPF problem → Validation

**Design Tradeoffs**: Homotopy schedule complexity vs. training time, relaxation strength vs. final solution quality, network capacity vs. generalization ability

**Failure Signatures**: High constraint violations indicate poor homotopy scheduling, training divergence suggests insufficient relaxation, suboptimal objectives may result from overly conservative homotopy paths

**First Experiments**:
1. Validate homotopy effectiveness on a simple 2D nonconvex problem with known global optimum
2. Compare feasibility rates between homotopy and non-homotopy approaches on IEEE 14-bus system
3. Test sensitivity to homotopy schedule parameters (rate of change, number of stages)

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Homotopy schedule design requires problem-specific tuning and may not generalize across different problem types
- Computational overhead from multiple homotopy stages is not fully characterized
- Scalability to large-scale real-world power systems (1000+ buses) remains unverified
- Theoretical convergence guarantees for the proposed homotopy strategies in highly nonconvex settings are not established

## Confidence
- Feasibility improvements: High (consistent reductions in constraint violations across multiple test cases)
- Objective value comparability: Medium (only selected comparisons with full OPF solvers provided)
- Generalizability to larger systems: Low (validation limited to small to medium IEEE test cases)

## Next Checks
1. Benchmark computational overhead across different homotopy schedules on multiple system sizes
2. Test scalability on large-scale realistic power systems with 1000+ buses
3. Evaluate performance on other constrained optimization problems beyond AC-OPF to assess generalizability