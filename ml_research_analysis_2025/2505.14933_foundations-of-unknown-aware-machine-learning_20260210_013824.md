---
ver: rpa2
title: Foundations of Unknown-aware Machine Learning
arxiv_id: '2505.14933'
source_url: https://arxiv.org/abs/2505.14933
tags:
- data
- detection
- page
- learning
- wild
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses the challenge of ensuring reliability and
  safety in machine learning models when deployed in open-world environments, where
  unknown data distributions may arise. It introduces novel frameworks that optimize
  for both accurate in-distribution predictions and reliable handling of out-of-distribution
  data.
---

# Foundations of Unknown-aware Machine Learning

## Quick Facts
- **arXiv ID:** 2505.14933
- **Source URL:** https://arxiv.org/abs/2505.14933
- **Reference count:** 38
- **Primary result:** Introduces frameworks that optimize for both accurate in-distribution predictions and reliable handling of out-of-distribution data.

## Executive Summary
This thesis addresses the challenge of ensuring reliability and safety in machine learning models deployed in open-world environments, where unknown data distributions may arise. It introduces novel frameworks that optimize for both accurate in-distribution predictions and reliable handling of out-of-distribution data. A key contribution is the development of an unknown-aware learning framework, including methods like VOS, NPOS, and DREAM-OOD, which generate informative unknown examples during training without requiring labeled OOD data. The thesis also proposes SAL, a framework leveraging unlabeled deployment data to enhance OOD detection under realistic conditions, with formal theoretical guarantees. Additionally, it advances reliability for large-scale foundation models by developing tools for hallucination detection (HaloScope), defending against malicious prompts (MLLMGuard), and data cleaning to denoise human feedback.

## Method Summary
The thesis develops a framework for unknown-aware learning that addresses OOD detection without requiring labeled OOD data. The core approach involves synthesizing virtual outliers from low-likelihood regions of the in-distribution feature space during training, creating a tighter decision boundary that reduces overconfidence on OOD data. Methods like VOS sample virtual outliers from class-conditional feature distributions, while DREAM-OOD generates synthetic outlier images via diffusion models. For scenarios with unlabeled deployment data, SAL uses gradient-based statistics and SVD to separate candidate outliers from mixed ID/OOD data. The framework is extended to foundation models through tools like HaloScope for hallucination detection and MLLMGuard for prompt defense.

## Key Results
- VOS achieves state-of-the-art OOD detection performance on multiple benchmarks while preserving ID accuracy.
- SAL successfully leverages unlabeled wild data to improve OOD detection without manual labeling.
- DREAM-OOD generates realistic outlier images that enhance OOD detection in object detection tasks.
- HaloScope identifies a hallucination subspace in LLM embeddings, enabling binary classification of truthful vs. hallucinated outputs.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Synthesizing virtual outliers from low-likelihood regions of the in-distribution (ID) feature space creates a tighter decision boundary, reducing overconfidence on out-of-distribution (OOD) data.
- **Mechanism:** VOS estimates the class-conditional feature distribution (e.g., Gaussian) and samples "virtual outliers" from the ε-likelihood region (tails) of these distributions. During training, an uncertainty regularization loss contrasts these virtual outliers against ID data, explicitly shaping the energy surface to be low for ID and high for OOD.
- **Core assumption:** The penultimate layer features of ID data can be approximated by a parametric distribution (e.g., multivariate Gaussian), and low-likelihood regions correspond to meaningful "unknowns" rather than just noisy ID samples.
- **Break condition:** The mechanism fails if the feature space is not well-modeled by the assumed distribution, causing virtual outliers to be uninformative or overlapping with ID data.

### Mechanism 2
- **Claim:** Unlabeled "wild" data from deployment environments can be separated into candidate outliers using gradient-based statistics, enabling OOD detection without manual labeling.
- **Mechanism:** SAL computes gradients for the unlabeled wild data using a model pre-trained on ID data. It constructs a gradient matrix and performs Singular Value Decomposition (SVD). The projection of gradients onto the top singular vector serves as a filtering score, separating OOD data from ID data.
- **Core assumption:** A distribution discrepancy exists between ID and OOD data such that their gradients, relative to a reference ID gradient, separate along a dominant singular direction.
- **Break condition:** The separation fails if the distribution shift is subtle or if the mixing ratio of OOD in the wild data is too small/large to provide a clear signal in the singular vectors.

### Mechanism 3
- **Claim:** Hallucinations in Large Language Models (LLMs) occupy a distinct subspace in the model's internal representations, which can be identified via embedding factorization.
- **Mechanism:** HaloScope extracts embeddings from LLM layers for unlabeled generations. It performs SVD on the centered embedding matrix. The top singular vectors define a "hallucination subspace." A membership estimation score is calculated based on the projection of a sample's embedding onto this subspace, allowing a binary classifier to distinguish truthful from hallucinated outputs.
- **Core assumption:** Hallucinated and truthful generations generate activations that cluster differently enough for SVD to capture the variance difference as a primary component.
- **Break condition:** The mechanism degrades if hallucinated text does not produce distinct activation patterns, or if the number of singular vectors k is mis-specified.

## Foundational Learning

- **Concept: Empirical Risk Minimization (ERM) vs. Reliability Risk**
  - **Why needed here:** The thesis critiques standard ERM for its "closed-world assumption" (assuming test data matches training data). Understanding this gap is essential to grasp why additional terms (like outlier exposure or uncertainty regularization) are added to the loss function.
  - **Quick check question:** Can you explain why a model achieving 99% accuracy on training data might still fail catastrophically on a slightly different test distribution?

- **Concept: Class-conditional Density Estimation**
  - **Why needed here:** VOS and SIREN rely on estimating the statistical distribution (e.g., Gaussian or von Mises-Fisher) of features for known classes. This is the mathematical basis for defining "low-likelihood" regions where outliers are sampled.
  - **Quick check question:** If feature embeddings are projected onto a unit hypersphere, why might a von Mises-Fisher distribution be a better assumption than a Gaussian?

- **Concept: Gradient-based Distribution Discrepancy**
  - **Why needed here:** SAL relies on the premise that the loss landscape (and hence gradients) differs for ID vs. OOD data. This is the signal exploited to separate mixed unlabeled data.
  - **Quick check question:** Why might the gradient of an OOD sample relative to an ID reference point be larger or structurally different than the gradient of an ID sample?

## Architecture Onboarding

- **Component map:** Backbone -> Feature Extractor -> Outlier Synthesizer (VOS/Dream-OOD) or Wild Data Separator (SAL) -> Binary OOD/Hallucination Head

- **Critical path:** The path diverges based on data availability:
  - *Path A (Labeled ID only):* Train Backbone → Estimate Feature Distribution → Synthesize Outliers → Train Uncertainty Head with $L_{uncertainty}$.
  - *Path B (Unlabeled Wild Data):* Train Backbone on ID → Deploy to collect Wild Data → Compute Gradients & SVD → Filter Candidate Outliers → Train OOD Classifier.

- **Design tradeoffs:**
  - **Synthesis vs. Real Data:** VOS/Dream-OOD allows control over the "hardness" of outliers ($\epsilon$ threshold), avoiding the need for external datasets. However, synthetic outliers might not match real-world failures. SAL uses real-world noise but requires an initial deployment phase and relies on the separability of gradients.
  - **Distribution Assumption:** VOS assumes Gaussian features; SIREN assumes vMF (hyperspherical). SIREN is more robust for high-dimensional normalized features but adds complexity in estimating the concentration parameter $\kappa$.

- **Failure signatures:**
  - **Collapse of ID Accuracy:** If the regularization weight $\beta$ is too high, the model focuses entirely on distinguishing outliers and forgets the primary classification task.
  - **Gradient Signal Vanishing (SAL):** If the mixing ratio $\pi$ of OOD in the wild is too low, the SVD might fail to isolate the "outlier" direction, leading to poor filtering.

- **First 3 experiments:**
  1. **Baseline Check:** Train a standard ERM model and measure FPR95/AUROC on a standard OOD benchmark (e.g., CIFAR-10 vs. SVHN) to establish the "overconfidence" baseline.
  2. **VOS Integration:** Add the VOS uncertainty loss ($L_{uncertainty}$) to the training loop. Vary the outlier sampling threshold ($\epsilon$ via $t$) and observe the trade-off between ID accuracy (mAP) and OOD detection (AUROC).
  3. **SAL Simulation:** Simulate "wild data" by mixing ID test data with an OOD dataset (e.g., CIFAR-100 + TinyImageNet). Run the Gradient SVD filtering and measure the Error Rates (ERRin/ERRout) to verify the separability assumption.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can a hallucination classifier be trained to generalize effectively when a distribution shift exists between the unlabeled data used for training and the test data?
- **Basis in paper:** Chapter 10.5 explicitly identifies "investigating how to train the hallucination classifier in order to generalize well with a distribution shift between the unlabeled data and the test data" as a promising avenue for future research.
- **Why unresolved:** The HaloScope framework relies on unlabeled "in-the-wild" data assuming it is representative of the test distribution; significant shifts between these distributions may degrade detection performance.
- **What evidence would resolve it:** An algorithmic modification to HaloScope that maintains high AUROC scores on hallucination detection benchmarks even when the training unlabeled data and the test data are drawn from statistically different distributions.

### Open Question 2
- **Question:** How do diverse data structures, specifically noisy human feedback and weak supervision, impact model reliability during the supervised fine-tuning and RLHF alignment stages?
- **Basis in paper:** Chapter 11 proposes a systematic investigation into "the impact of diverse data structures, such as human feedback, weak supervision, and semi-supervised data, on model reliability."
- **Why unresolved:** While the thesis addresses data cleaning, it does not fully characterize the specific failure modes or reliability risks introduced by these diverse data structures during the alignment process.
- **What evidence would resolve it:** A theoretical analysis or benchmark study quantifying the relationship between the noise levels in weak supervision/human feedback and the resulting reliability metrics of the aligned foundation model.

### Open Question 3
- **Question:** Can unknown-aware learning frameworks (e.g., VOS, SIREN) be effectively adapted to ensure reliable protein structural analysis and open-set classification in scientific domains?
- **Basis in paper:** Chapter 11 outlines a long-term vision to explore "reliable ML for boarder scientific discovery," specifically listing "reliable protein structural analysis with OOD detection" and "open-set classification" as future applications.
- **Why unresolved:** The efficacy of the proposed frameworks has been demonstrated primarily on computer vision and NLP benchmarks; their applicability to high-dimensional biological data and scientific discovery tasks remains unverified.
- **What evidence would resolve it:** Empirical results showing that methods like SIREN or SAL can successfully identify out-of-distribution protein conformations or novel macromolecules in cryo-electron tomography without requiring extensive manual labeling.

## Limitations
- The effectiveness of VOS depends critically on the assumption that feature embeddings follow a parametric distribution, which may not generalize across diverse architectures or data types.
- SAL's gradient-based separation relies on a theoretical SVD property that may not hold when the OOD mixing ratio is extreme or when distribution shifts are subtle.
- The claim that hallucinations occupy a distinct subspace in LLM embeddings is plausible but not rigorously proven, and the SVD-based method may conflate variance from noise with meaningful "hallucination" patterns.

## Confidence
- **High confidence:** The general premise that machine learning models need OOD detection mechanisms and that training with uncertainty regularization improves reliability. The framework of combining classification loss with an outlier exposure term is well-established.
- **Medium confidence:** The specific VOS mechanism for synthesizing virtual outliers and its empirical performance gains. The core idea is sound, but the distribution assumption may not generalize.
- **Low confidence:** The theoretical guarantees for SAL under Huber contamination and the precise subspace properties for LLM hallucinations. These claims are conceptually interesting but lack rigorous empirical validation in the thesis.

## Next Checks
1. **VOS Distribution Assumption:** For a vision dataset (e.g., CIFAR-100), fit class-conditional Gaussians to penultimate-layer features. Quantitatively assess the goodness-of-fit (e.g., using Kolmogorov-Smirnov or likelihood) and visualize whether low-likelihood regions truly correspond to semantically different inputs.
2. **SAL Gradient Separation:** Simulate "wild data" with varying OOD ratios (e.g., 0.01, 0.1, 0.5, 0.9). For each ratio, run the SVD filtering and measure the purity of the separated candidate outlier set. Plot the singular value spectrum to see if an "outlier direction" emerges.
3. **LLM Hallucination Subspace:** Generate a dataset of LLM outputs labeled as truthful or hallucinated. Compute layer-wise embeddings and perform SVD. Analyze if the top singular vectors consistently capture a "hallucination direction" across different models or if the variance is task-specific.