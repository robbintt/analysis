---
ver: rpa2
title: 'sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach
  to Brain-Language Alignment'
arxiv_id: '2504.14468'
source_url: https://arxiv.org/abs/2504.14468
tags:
- seeg
- sentence
- masking
- neural
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces SSENSE, a contrastive learning framework that
  aligns sEEG brain recordings with natural language by projecting neural signals
  into the sentence embedding space of a frozen CLIP model. The method uses spectrograms
  of sEEG signals processed through a neural encoder trained with InfoNCE loss, enabling
  zero-shot sentence retrieval directly from brain activity without fine-tuning the
  text encoder.
---

# sEEG-based Encoding for Sentence Retrieval: A Contrastive Learning Approach to Brain-Language Alignment

## Quick Facts
- arXiv ID: 2504.14468
- Source URL: https://arxiv.org/abs/2504.14468
- Reference count: 8
- Key outcome: SSENSE achieves 1.2% Recall@1 and 10.68% Recall@10 for zero-shot sentence retrieval from sEEG brain recordings

## Executive Summary
This work introduces SSENSE, a contrastive learning framework that aligns sEEG brain recordings with natural language by projecting neural signals into the sentence embedding space of a frozen CLIP model. The method uses spectrograms of sEEG signals processed through a neural encoder trained with InfoNCE loss, enabling zero-shot sentence retrieval directly from brain activity without fine-tuning the text encoder. Evaluated on a single-subject dataset of movie-watching sEEG and aligned transcripts, SSENSE significantly outperforms random baselines and demonstrates that general-purpose language representations can effectively serve as priors for neural decoding.

## Method Summary
SSENSE converts raw sEEG signals to time-frequency spectrograms using superlet transform, then processes each electrode's spectrogram through a modified ResNet-18 encoder (ImageNet pretrained, adapted for single-channel input). Per-electrode embeddings are mean-pooled to create unified 512-dimensional representations. These neural embeddings are aligned with CLIP sentence embeddings using InfoNCE contrastive loss, with the CLIP text encoder remaining frozen throughout training. During inference, cosine similarity between neural and sentence embeddings enables zero-shot sentence retrieval from brain activity.

## Key Results
- Achieves 1.2% Recall@1 and 10.68% Recall@10 on held-out test set
- Significantly outperforms random baseline (3.42% Recall@10)
- Data augmentation shows mixed effects: electrode masking degrades performance (p=0.045 for Recall@1), while frequency-time masking has neutral effects
- Single-subject evaluation on movie-watching sEEG dataset with 1,454 total sentence-sEEG pairs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Projecting neural signals into a frozen pretrained text embedding space enables zero-shot sentence retrieval without fine-tuning the language model.
- **Mechanism:** InfoNCE contrastive loss pulls temporally-aligned sEEG-text pairs together while pushing mismatched pairs apart within CLIP's 512-dimensional sentence embedding space. The frozen text encoder provides stable semantic anchors; only the neural encoder learns to map brain activity into this space.
- **Core assumption:** Brain activity during language comprehension shares recoverable structural correspondences with semantic representations learned by large-scale vision-language models.
- **Evidence anchors:** [abstract] "projects single-subject stereo-electroencephalography (sEEG) signals into the sentence embedding space of a frozen CLIP model"; [section 2.3] "the CLIP model remains fixed throughout training, serving as a consistent target for contrastive alignment"; [corpus] Related work reviews fMRI studies supporting convergence between neural and model representations.

### Mechanism 2
- **Claim:** Spectrogram representations processed through ImageNet-pretrained convolutional encoders capture linguistically-relevant spectral patterns in sEEG.
- **Mechanism:** Superlet transform converts raw sEEG to time-frequency representations; modified ResNet-18 processes each electrode's spectrogram independently; per-electrode embeddings are aggregated via mean pooling to form unified 512-dim representations.
- **Core assumption:** Spectral patterns contain semantic information; ImageNet pretraining transfers meaningfully to neural spectrogram processing despite domain gap.
- **Evidence anchors:** [section 2.1] "Raw sEEG signals were first converted into time-frequency representations (spectrograms) using the superlet transform"; [section 2.2] "adopt a modified ResNet-18 architecture, pretrained on ImageNet"; [corpus] BrainBERT demonstrates self-supervised learning on sEEG spectrograms.

### Mechanism 3
- **Claim:** Data augmentation strategies affect alignment quality asymmetrically—electrode masking degrades performance while time-frequency masking shows neutral effects.
- **Mechanism:** Electrode masking removes entire channels, eliminating spatial information; time-frequency masking removes localized spectral features, acting as regularization without destroying global structure.
- **Core assumption:** Electrodes carry partially redundant but partially unique semantic information; masking entire channels is more disruptive than masking localized features.
- **Evidence anchors:** [section 4] "electrode masking shows a statistically significant drop in performance relative to no masking in Recall@1 (p=0.045), Recall@10 (p=0.011)"; [section 4] "frequency-time masking does not improve or degrade performance"; [corpus] Limited direct evidence on augmentation effects for neural-text alignment.

## Foundational Learning

- **Contrastive Learning (InfoNCE Loss):**
  - Why needed here: Core training objective that learns by comparing positive pairs (matching brain-text) against negative pairs within each batch.
  - Quick check question: Given a batch of 64 sEEG-text pairs, how many negative samples does each positive pair implicitly compare against?

- **CLIP Architecture and Frozen Encoders:**
  - Why needed here: SSENSE leverages CLIP's pretrained text encoder as a fixed semantic target; understanding this informs why freezing preserves priors.
  - Quick check question: What is the risk if you fine-tune the text encoder instead of freezing it?

- **Time-Frequency Representations (Spectrograms/Superlets):**
  - Why needed here: Raw sEEG time-series must be converted to 2D representations compatible with CNN-based encoders.
  - Quick check question: Why might superlet transform be preferred over standard STFT for neural signal analysis?

## Architecture Onboarding

- **Component map:**
Raw sEEG (E electrodes × T timesteps) → Zero-pad to 8,200 timesteps → Superlet transform → Spectrograms (E × F × T') → ResNet-18 encoder (per electrode, ImageNet pretrained) → 512-dim embeddings per electrode → Mean pooling across electrodes → L2 normalized 512-dim sEEG embedding → InfoNCE loss training → Retrieval via cosine similarity

- **Critical path:**
  1. sEEG preprocessing quality (temporal alignment, spectrogram fidelity)
  2. ResNet-18 adaptation for single-channel input (first conv layer modification)
  3. Contrastive alignment convergence (monitor validation Recall@10)
  4. Retrieval evaluation on held-out test sentences

- **Design tradeoffs:**
  - Frozen vs. fine-tuned text encoder: Freezing preserves CLIP's semantic structure but limits adaptation to neural idiosyncrasies.
  - Per-electrode encoding + mean pooling: Simple aggregation but may miss cross-electrode interactions; alternative would be joint multi-channel input.
  - No masking vs. augmentation: Results show no masking performs best on this limited dataset; augmentation may help with more data.

- **Failure signatures:**
  - Recall@10 near 3.42% (random baseline): Alignment failed completely
  - Large variance across random seeds: Overfitting to particular data splits
  - Electrode masking causes significant drop: Spatial information is critical; check electrode selection quality

- **First 3 experiments:**
  1. Baseline retrieval: Train with no masking, evaluate Recall@1/10/50 and MRR; establishes upper bound.
  2. Electrode masking ablation: Test whether spatial information matters; expect performance drop if electrodes carry unique semantic content.
  3. Time-frequency masking ablation: Test robustness to spectral perturbations; neutral effect suggests model relies on distributed spectral features.

## Open Questions the Paper Calls Out
- **Open Question 1:** Can the SSENSE framework be extended to generalize across multiple subjects? (The conclusion explicitly lists "scaling to multi-subject data" as a future direction.)
- **Open Question 2:** Does incorporating visual inputs alongside text improve alignment performance? (The authors identify "integrating visual inputs" as a key direction for future work.)
- **Open Question 3:** Would leveraging larger language models improve the low Recall@1 (1.2%) performance? (The conclusion suggests "leveraging larger language models" to advance the framework.)

## Limitations
- Single-subject evaluation limits generalizability to other individuals or broader linguistic contexts
- Performance metrics may not transfer to different subjects or language tasks
- No investigation of hierarchical linguistic structures or cross-subject performance
- Reliance on CLIP's pretrained semantic space may constrain capture of language-specific neural patterns

## Confidence
- **High Confidence:** The methodological framework (InfoNCE contrastive learning, spectrogram processing, frozen CLIP encoder) is technically sound and well-implemented.
- **Medium Confidence:** The specific performance metrics (Recall@1=1.2%, Recall@10=10.68%) are reliable for this single subject and dataset but may not generalize.
- **Low Confidence:** Claims about broader applicability to different subjects, languages, or cognitive states remain speculative without additional validation.

## Next Checks
1. **Cross-subject validation:** Apply the trained model to sEEG data from different individuals watching the same or different content to assess generalization across subjects.
2. **Ablation study on augmentation:** Conduct a more systematic exploration of data augmentation strategies with larger training datasets to determine optimal regularization approaches.
3. **Semantic alignment analysis:** Evaluate whether retrieved sentences are semantically similar to the neural activity beyond exact string matching, using semantic similarity metrics or human evaluation.