---
ver: rpa2
title: 'STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting'
arxiv_id: '2512.21118'
source_url: https://arxiv.org/abs/2512.21118
tags:
- stldm
- latent
- diffusion
- network
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STLDM addresses the challenge of precipitation nowcasting by reformulating
  it into sequential forecasting and enhancement subtasks. It employs a novel spatio-temporal
  latent diffusion model that combines a conditioning network (Translator) for global
  motion prediction with a latent denoising network for high-fidelity enhancement.
---

# STLDM: Spatio-Temporal Latent Diffusion Model for Precipitation Nowcasting

## Quick Facts
- arXiv ID: 2512.21118
- Source URL: https://arxiv.org/abs/2512.21118
- Authors: Shi Quan Foo; Chi-Ho Wong; Zhihan Gao; Dit-Yan Yeung; Ka-Hing Wong; Wai-Kin Wong
- Reference count: 36
- Key outcome: STLDM achieves state-of-the-art performance across multiple radar datasets (SEVIR, HKO-7, MeteoNet), demonstrating superior accuracy in CSI, HSS, and perceptual metrics (SSIM, LPIPS) while offering faster inference than other diffusion-based approaches.

## Executive Summary
STLDM addresses the challenge of precipitation nowcasting by reformulating it into sequential forecasting and enhancement subtasks. It employs a novel spatio-temporal latent diffusion model that combines a conditioning network (Translator) for global motion prediction with a latent denoising network for high-fidelity enhancement. The model is trained end-to-end alongside a variational autoencoder. STLDM achieves state-of-the-art performance across multiple radar datasets (SEVIR, HKO-7, MeteoNet), demonstrating superior accuracy in CSI, HSS, and perceptual metrics (SSIM, LPIPS) while offering faster inference than other diffusion-based approaches. The key innovation is the proposed constraint loss that regularizes the conditioning network and the spatio-temporal enhancement framework that ensures temporal consistency.

## Method Summary
STLDM reformulates precipitation nowcasting as a two-stage process: first, a deterministic Translator network predicts approximate global motion trends in latent space to produce a coarse first estimate; second, a latent diffusion model refines this estimate conditioned on both the first estimate and original input, adding local detail while preserving global trajectory. The entire system, including a variational autoencoder, Translator, and latent denoising network, is trained end-to-end. The approach combines sequential forecasting with spatio-temporal enhancement, using a constraint loss on the Translator to ensure conditioning quality and temporal attention modules to maintain consistency across frames.

## Key Results
- STLDM achieves SOTA CSI-m scores of 0.3804 (SEVIR), 0.4410 (HKO-7), and 0.4067 (MeteoNet) while maintaining fast inference times
- The constraint loss improves CSI-m from 0.3569 to 0.3804 on SEVIR by ensuring better conditioning signal quality
- Spatio-temporal enhancement with temporal attention achieves FVD of 87.14 vs. 241.17 for spatial-only enhancement on HKO-7, demonstrating superior temporal consistency

## Why This Works (Mechanism)

### Mechanism 1
Decomposing precipitation nowcasting into sequential forecasting and enhancement subtasks enables both high accuracy and high visual fidelity. A deterministic Translator (Ψ_θ) first predicts approximate global motion trends in latent space, producing a coarse first estimate (Ȳ_{1:N}). A latent diffusion model (D_θ) then refines this estimate conditioned on both the first estimate and original input, adding local detail while preserving global trajectory. Core assumption: The first-stage deterministic prediction captures correct large-scale motion even if blurry; the diffusion model does not need to learn global dynamics from scratch. Evidence anchors: [abstract] "STLDM decomposes this task into two stages: a deterministic forecasting stage handled by the conditioning network, and an enhancement stage performed by the latent diffusion model." Break condition: If the Translator produces globally incorrect motion (e.g., precipitation entering from outside the observation region), the diffusion model cannot correct it—garbage-in, garbage-out.

### Mechanism 2
The proposed constraint loss (L_C) on the Translator improves final prediction accuracy by ensuring the conditioning signal remains well-aligned with ground truth. L_C = ||Y_{1:N} - Ȳ_{1:N}||² explicitly regularizes the Translator's decoded output toward the ground truth. Without this, the Translator is only implicitly constrained via KL and diffusion losses, producing noisy first estimates that misguide the diffusion model. Core assumption: The conditioning signal quality directly bounds the diffusion model's ability to produce accurate outputs; implicit regularization alone is insufficient. Evidence anchors: [Section 3.2.2, Eq. 10] Defines L_C explicitly in the loss decomposition. [Section 4.3.1, Table 2] With L_C: CSI-m improves from 0.3569→0.3804, HSS from 0.4659→0.5024 on SEVIR. [Figure 7] Visual comparison shows noisy first estimates without L_C lead to over-prediction. Break condition: If L_C weight is too high, the Translator may overfit and lose stochasticity needed for the diffusion model to refine meaningfully.

### Mechanism 3
Treating visual enhancement as a spatio-temporal task (vs. frame-wise spatial) yields superior temporal consistency. D_θ includes temporal attention modules that enforce coherence across frames during denoising. Removing temporal attention (purely spatial enhancement) improves per-frame LPIPS slightly but drastically worsens FVD (241.17 vs. 87.14). Core assumption: Temporal consistency matters more for practical nowcasting than marginal per-frame perceptual quality. Evidence anchors: [Section 4.3.3, Table 4] Spatio-temporal enhancement achieves FVD=87.14 vs. Spatial=241.17 on HKO-7. [Figures 9-10] Visual evidence shows spatial enhancement produces jerky, inconsistent cloud motion. Break condition: If computational budget is extremely tight, spatial-only enhancement is faster (0.42s vs. 0.55s) but unsuitable for operational forecasting where temporal plausibility matters.

## Foundational Learning

- **Concept: Diffusion Models (DDPM/DDIM)**
  - Why needed here: STLDM's D_θ is a conditional latent diffusion model; understanding forward/reverse processes and denoising objectives is essential to debug training convergence.
  - Quick check question: Can you explain why DDIM sampling requires fewer steps than DDPM while producing comparable outputs?

- **Concept: Variational Autoencoders and Latent Space**
  - Why needed here: The VAE {E,D} maps radar frames to/from latent space; diffusion operates in this compressed representation for efficiency.
  - Quick check question: What happens to reconstruction quality if the KL divergence weight is set too high during VAE training?

- **Concept: Classifier-Free Guidance (CFG)**
  - Why needed here: CFG balances conditional vs. unconditional generation during inference; STLDM uses guidance strength w=1.0.
  - Quick check question: How does CFG differ from classifier guidance, and why does it avoid requiring an external classifier?

## Architecture Onboarding

- **Component map**:
  - VAE {E,D}: Conv-only encoder/decoder. Input: (B,T,1,128,128) → Latent: (B,T,32,32,32). No skip connections.
  - Translator Ψ_θ: Stacked gSTA (Gated Spatio-Temporal Attention) modules—purely convolutional, models temporal evolution in latent space.
  - Latent Denoising Network D_θ: U-Net style with Downsampling/Mid/Upsampling blocks. Contains ResBlocks, Linearized Spatial Attention, Temporal Attention. Conditioning via channel concatenation of ẑ_{1:N} at first ResBlock of each Downsampling block.

- **Critical path**:
  1. Encode input frames: z_x = E(X_{1:M})
  2. First estimation: ẑ_{1:N} = Ψ_θ(z_x)
  3. Initialize noise: z^T_{1:N} ~ N(0,1)
  4. Denoise iteratively (DDIM, 20 steps): z^{t-1}_{1:N} = D_θ(z^t_{1:N}, t, ẑ_{1:N})
  5. Decode: Ŷ_{1:N} = D(z^0_{1:N})

- **Design tradeoffs**:
  - End-to-end training (Strategy C) vs. pre-trained VAE (Strategies A/B): End-to-end yields best perceptual metrics (SSIM, LPIPS) but requires more GPU memory.
  - Sampling steps: 20 (STLDM) vs. 50-1000 (baselines)—fewer steps possible due to latent space + well-conditioned guidance.
  - CFG probability: 15% unconditional during training (per paper).

- **Failure signatures**:
  - Noisy/hallucinated precipitation without L_C (check Figure 7).
  - Temporal flickering if temporal attention removed (check Figure 9).
  - Slow convergence if pre-trained components are not jointly tuned (check Figure 6, Strategy A).
  - Precipitation appearing from outside observation region cannot be predicted (inherent limitation, Figure 16).

- **First 3 experiments**:
  1. **Sanity check**: Train VAE alone, verify reconstruction (MSE, visual). Latent space should be 32×32×32 for 128×128 inputs.
  2. **Ablation on L_C**: Train STLDM with and without L_C on a small data slice; expect CSI-m gap ~0.02+ (Table 2).
  3. **Inference timing**: Profile DDIM 20-step sampling with CFG on RTX3090; target <0.6s for 128×128, <3s for 512×512 (Table 6).

## Open Questions the Paper Calls Out

### Open Question 1
Can integrating auxiliary data modalities (e.g., satellite, wind, NWP) into the STLDM framework effectively resolve the failure to predict precipitation events that originate outside the radar observation region? Basis in paper: [explicit] The authors state in Section 5 that the model "struggles to accurately forecast precipitation events that originate outside the observation region" and suggest "a possible solution is to introduce a multimodal model." Why unresolved: The current STLDM implementation relies solely on radar echo data, creating a blind spot for weather systems entering the frame or driven by non-radar atmospheric dynamics. What evidence would resolve it: Performance evaluation on a multimodal dataset showing significant improvements in Critical Success Index (CSI) for events spawning at the boundaries of the observation window compared to the radar-only baseline.

### Open Question 2
Does incorporating physical constraints into the latent diffusion process improve the model's ability to forecast events driven by "unknown factors" and enhance interpretability? Basis in paper: [explicit] In Section 5, the authors identify the limitation of handling events "driven by unknown factors" and propose a "future direction is to develop a... physics-informed... model... improving physical interpretability." Why unresolved: Purely data-driven diffusion models may generate realistic textures that violate fluid dynamics or thermodynamic laws, limiting reliability for operational meteorology. What evidence would resolve it: A study comparing standard STLDM against a physics-informed variant (e.g., with PDE-based regularization), showing better adherence to conservation laws or improved skill scores during anomalous weather events.

### Open Question 3
Can a unified STLDM architecture be developed to generalize across geographically distinct regions without requiring full retraining? Basis in paper: [explicit] Section 5 notes that "STLDM must be retrained for a new dataset in a different region" and identifies the future direction as "develop[ing] a unified... model across multiple benchmarks." Why unresolved: Current models specialize in the specific spatial and climatic characteristics of single datasets (SEVIR, HKO-7, MeteoNet), limiting transferability. What evidence would resolve it: A single model trained on multiple geographic datasets simultaneously that maintains competitive performance (within a small margin of specialized models) on a held-out region.

## Limitations
- The model struggles to accurately forecast precipitation events that originate outside the observation region, a fundamental limitation of radar-only approaches.
- STLDM must be retrained for new datasets in different regions, lacking geographic generalization capability.
- The effectiveness of the constraint loss (L_C) is empirically demonstrated but not theoretically justified.

## Confidence

- **High**: STLDM achieves SOTA CSI/HSS on SEVIR, HKO-7, and MeteoNet; end-to-end training improves perceptual metrics (SSIM, LPIPS).
- **Medium**: L_C improves accuracy (CSI-m: 0.3569→0.3804); spatio-temporal enhancement yields better temporal consistency (FVD: 87.14 vs. 241.17).
- **Medium**: Faster inference (0.55s for 128×128) compared to baselines (1.21s) is empirically supported but not benchmarked across diverse hardware.

## Next Checks

1. **Ablation on Translator initialization**: Train STLDM with pre-trained Translator (Strategy B) vs. end-to-end (Strategy C) on SEVIR; quantify impact on CSI and perceptual metrics.
2. **Sampling step sensitivity**: Compare DDIM inference with 10, 20, 50 steps on HKO-7; measure CSI/HSS degradation and FVD stability.
3. **Out-of-region motion test**: Generate precipitation entering from outside the frame on SEVIR; evaluate Translator's ability to extrapolate motion and diffusion model's correction capacity.