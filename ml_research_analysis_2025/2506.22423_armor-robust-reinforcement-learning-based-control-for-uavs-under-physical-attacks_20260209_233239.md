---
ver: rpa2
title: 'ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical
  Attacks'
arxiv_id: '2506.22423'
source_url: https://arxiv.org/abs/2506.22423
tags:
- attacks
- armor
- control
- state
- policy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of controlling UAVs under physical
  sensor attacks (e.g., GPS spoofing, gyroscope tampering) using reinforcement learning.
  Existing safe RL methods are ineffective against such attacks, which corrupt state
  estimates and lead to unsafe behavior.
---

# ARMOR: Robust Reinforcement Learning-based Control for UAVs under Physical Attacks

## Quick Facts
- arXiv ID: 2506.22423
- Source URL: https://arxiv.org/abs/2506.22423
- Authors: Pritam Dash; Ethan Chan; Nathan P. Lawrence; Karthik Pattabiraman
- Reference count: 40
- Key result: ARMOR achieves 88% mission success rate under sensor attacks, outperforming RARL (82%) and other baselines

## Executive Summary
This paper addresses the challenge of controlling UAVs under physical sensor attacks using reinforcement learning. Existing safe RL methods fail when attacks corrupt state estimates, leading to unsafe behavior. The proposed ARMOR framework uses a two-stage training approach to learn robust latent state representations that maintain performance even when sensor data is compromised by attacks like GPS spoofing or gyroscope tampering.

## Method Summary
ARMOR employs a teacher-student framework for robust RL under sensor attacks. In the first stage, a teacher encoder is trained with privileged attack information to generate attack-aware latent states. The second stage involves training a student encoder to approximate these privileged latent states using only historical sensor data, without requiring attack information during deployment. This approach enables the policy to maintain robust performance by learning to estimate true states from corrupted observations, significantly improving mission success rates and crash prevention compared to baseline methods.

## Key Results
- ARMOR achieves 88% mission success rate under sensor attacks, outperforming RARL (82%) and other baselines
- Significant reduction in state drift and crash prevention compared to baseline RL methods
- Strong zero-shot generalization to unseen attacks, maintaining higher success rates than RARL across various attack types

## Why This Works (Mechanism)
ARMOR's effectiveness stems from its ability to learn robust latent state representations that account for sensor attacks. By using privileged attack information during training, the teacher encoder learns to extract true state information despite corrupted sensor readings. The student encoder then learns to approximate these attack-aware states using only historical sensor data, enabling deployment without privileged information. This approach effectively bridges the gap between safe training conditions and unsafe deployment scenarios, allowing the policy to maintain performance even when sensor data is compromised.

## Foundational Learning
- **Reinforcement Learning for Control**: RL agents learn optimal policies through interaction with environments; needed to enable autonomous UAV navigation without explicit programming of control rules; quick check: verify policy convergence on nominal (attack-free) tasks
- **Sensor Fusion and State Estimation**: Combining multiple sensor readings to estimate true system state; critical for handling corrupted individual sensor measurements during attacks; quick check: evaluate state estimation accuracy under various attack intensities
- **Adversarial Robustness in ML**: Techniques for maintaining performance under malicious input perturbations; essential for UAV safety under physical attacks; quick check: test robustness against novel attack patterns
- **Latent State Representations**: Learning compressed representations that capture essential information while discarding noise; enables efficient processing of high-dimensional sensor data; quick check: measure information retention in compressed latent space
- **Teacher-Student Learning Frameworks**: Knowledge distillation techniques where a teacher model guides student model training; allows privileged information use during training while enabling deployment without it; quick check: compare student performance with and without teacher guidance

## Architecture Onboarding

**Component Map**: UAV Sensors -> Sensor Fusion Module -> Attack Detection Module -> ARMOR Encoder -> RL Policy -> Actuators

**Critical Path**: Sensor Data → Encoder → Policy Network → Control Output

**Design Tradeoffs**: 
- Privacy vs. performance: uses historical data instead of real-time attack information
- Complexity vs. robustness: two-stage training increases development time but improves attack resilience
- Computational overhead: additional encoding layer adds latency but enables attack detection

**Failure Signatures**:
- Degraded performance when attacks exceed training distribution
- Latency in state estimation under high-frequency attack variations
- Potential overfitting to specific attack patterns in training data

**First Experiments**:
1. Evaluate baseline performance on nominal (attack-free) navigation tasks
2. Test encoder robustness against synthetic sensor noise variations
3. Compare mission success rates across different attack intensities and types

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Results primarily based on hardware-in-the-loop testbed, limiting real-world generalization
- Evaluation focused on single navigation task, leaving performance on diverse mission profiles uncertain
- Assumes persistent attacks throughout episodes, which may not reflect real-world intermittent attack patterns

## Confidence
- Attack robustness performance: Medium confidence based on comparative results with RARL and HRP
- Real-world applicability: Medium-Low confidence due to testbed constraints and limited mission diversity
- Zero-shot generalization: Medium confidence based on limited attack variations tested

## Next Checks
1. Evaluate ARMOR across multiple mission types (delivery, search, formation flying) to assess task generality
2. Test against adaptive attacks that vary intensity or target different sensor combinations during a single episode
3. Deploy the method on actual UAVs in outdoor environments with real GPS spoofing and IMU tampering to validate hardware-in-the-loop results