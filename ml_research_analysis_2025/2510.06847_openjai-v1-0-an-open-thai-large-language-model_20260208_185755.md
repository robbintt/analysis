---
ver: rpa2
title: 'OpenJAI-v1.0: An Open Thai Large Language Model'
arxiv_id: '2510.06847'
source_url: https://arxiv.org/abs/2510.06847
tags:
- thai
- openjai-v1
- language
- evaluation
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: OpenJAI-v1.0 is an open-source Thai-English large language model
  developed by finetuning Qwen3-14B. It improves practical capabilities in instruction
  following, long-context understanding, and tool use using carefully curated Thai
  and English datasets.
---

# OpenJAI-v1.0: An Open Thai Large Language Model

## Quick Facts
- arXiv ID: 2510.06847
- Source URL: https://arxiv.org/abs/2510.06847
- Authors: Pontakorn Trakuekul; Attapol T. Rutherford; Jullajak Karnjanaekarin; Narongkorn Panitsrisit; Sumana Sumanakul
- Reference count: 16
- Primary result: Finetuned Qwen3-14B achieves 32.4 (EN) and 39.4 (TH) on IFBench, outperforming other Thai models while maintaining strong general knowledge

## Executive Summary
OpenJAI-v1.0 is an open-source Thai-English large language model developed by finetuning Qwen3-14B. It improves practical capabilities in instruction following, long-context understanding, and tool use using carefully curated Thai and English datasets. On instruction following, it achieves 32.4 (EN) and 39.4 (TH) on IFBench, outperforming other Thai models. For long-context tasks, it scores 33.6 on LongBench-v2 and 18.9 on MRCR. In tool calling, it reaches 60.5 (EN) and 47.0 (TH) on BFCL-v3, surpassing both open-source and proprietary baselines. The model balances specialization with retention, avoiding catastrophic forgetting while enhancing targeted capabilities.

## Method Summary
OpenJAI-v1.0 was created by finetuning Qwen3-14B on ~462M tokens across three domains: instruction-following data (curated public + synthetic, LLM-filtered), long-context data (open-source + synthetic up to 120K tokens), and tool-calling data (curated + Thai-translated). Training used 8xH100 GPUs with global batch size 256, completed in under one day. The model uses direct instruction-response format without intermediate reasoning steps and evaluates without system prompts to ensure fair comparison.

## Key Results
- IFBench scores: 32.4 (EN) and 39.4 (TH), outperforming other Thai models
- Long-context performance: 33.6 on LongBench-v2, 18.9 on MRCR
- Tool calling: 60.5 (EN) and 47.0 (TH) on BFCL-v3, surpassing baselines
- General knowledge retention: 66.0 MMLU-ProX-lite EN, 54.7 TH (slight decline from base)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Targeted filtering of synthetic instruction data improves constraint adherence without degrading helpfulness.
- **Mechanism:** The authors generate synthetic samples designed to meet strict constraints, then employ an "LLM-as-a-judge" to reject samples that satisfy constraints literally but diverge from user intent. This filtering step prevents reward hacking or brittle rule-following.
- **Core assumption:** The judge model (not explicitly named in the text, but implied to be high-quality) aligns sufficiently with human preferences to distinguish between "malicious compliance" and genuine helpfulness.
- **Evidence anchors:**
  - [section] Section 2 (Instruction Following): "...rigorously filtered by an LLM-as-a-judge... rejecting any that failed to meet our quality standards."
  - [abstract] Mentions "carefully curated data" as the driver for practical task improvements.
  - [corpus] Weak direct evidence in corpus; related papers focus on pretraining/finetuning but not this specific filtering logic.
- **Break condition:** If the judge model exhibits biases that contradict the target user demographic, the filtered dataset will amplify those biases, leading to high benchmark scores but poor real-world alignment.

### Mechanism 2
- **Claim:** Long-context performance is enhanced by training on data specifically synthesized for context lengths up to 120k tokens.
- **Mechanism:** The model is exposed to a mixture of open-source and synthetic data requiring coherence over extended token limits. This likely adapts the model's attention mechanism (or RoPE embeddings) to handle long-range dependencies more effectively than standard short-context training.
- **Core assumption:** The base model (Qwen3-14B) possesses an architecture capable of utilizing extended context windows when finetuned, rather than having a hard structural limit.
- **Evidence anchors:**
  - [section] Section 2 (Long-Context Understanding): "...specifically designed to improve the model's robustness and coherence when processing context lengths up to 120,000 tokens."
  - [results] Table 1 shows LongBench-v2 score of 33.6, tying the top baseline and beating the base model (32.4).
  - [corpus] Related works (e.g., OpenThaiGPT) focus on general LLMs; specific long-context synthesis strategies are less detailed in the immediate neighbors.
- **Break condition:** If the synthetic long-context data lacks the complexity or diversity of real-world long documents (e.g., legal texts, codebases), the model may learn to attend to length but fail on retrieval or reasoning within that context.

### Mechanism 3
- **Claim:** Catastrophic forgetting is mitigated through a data mixture that preserves general knowledge while specializing.
- **Mechanism:** By balancing the three specialized datasets (Instruction, Long-context, Tool) and potentially retaining or mixing in general knowledge data (implied, or relying on the base model's robustness), the model maintains MMLU scores close to its base.
- **Core assumption:** The volume of specialized data (462M tokens) is sufficient to induce new skills but not so large or epoch-heavy as to overwrite the pretrained knowledge representations of Qwen3-14B.
- **Evidence anchors:**
  - [abstract] "...while avoiding catastrophic forgetting."
  - [section] Section 4 (Results): "OpenJAI-v1.0 maintains 66.0 in English [MMLU]... closely track[ing] Qwen3-14B (66.6)."
  - [corpus] JAI-1 report notes that applying training without structural modifications risks eroding knowledge; OpenJAI-v1.0 serves as a counter-example where finetuning *did* preserve knowledge.
- **Break condition:** If the learning rate or token count were significantly increased, the model would likely show a sharp drop in MMLU-ProX scores, indicating overfitting to the specialized tasks.

## Foundational Learning

- **Concept: LLM-as-a-Judge**
  - **Why needed here:** The paper relies on this technique for data quality control. Understanding inter-rater reliability and judge bias is critical to reproducing their data pipeline.
  - **Quick check question:** Does the paper specify which model serves as the judge, and does its preference profile match your target deployment scenario?

- **Concept: Catastrophic Forgetting**
  - **Why needed here:** The central value proposition is gaining skills (Tool use, Thai fluency) without losing intelligence (General Knowledge).
  - **Quick check question:** If you finetune a model on purely task-specific data, what benchmark would you run to verify you haven't "lobotomized" its general reasoning?

- **Concept: Dynamic RoPE Scaling**
  - **Why needed here:** The paper uses this for evaluation to standardize context windows. It is a common technique to extend context length without retraining from scratch.
  - **Quick check question:** How does modifying the RoPE base frequency allow a model trained on 4k or 8k tokens to ingest 120k tokens during inference?

## Architecture Onboarding

- **Component map:** Qwen3-14B (Base Model) -> Synthetic/Curated Data Generation -> LLM-as-Judge Filtering -> Mixed Domain Training -> vLLM Inference with Dynamic RoPE

- **Critical path:**
  1. **Data Curation:** Generate/Collect -> Filter via LLM Judge -> Mix Thai/English
  2. **Training:** Finetune Qwen3-14B on 462M tokens (approx < 1 day on 8xH100)
  3. **Evaluation:** Run IFBench, BFCL-v3, and LongBench-v2 *without* system prompts to ensure fair comparison

- **Design tradeoffs:**
  - **Specialization vs. Retention:** There is a slight drop in Thai MMLU (57.5 -> 54.7) suggesting some trade-off is inevitable when pushing hard on specific capabilities
  - **Zero-shot vs. Contamination:** Excluding IFBench constraints from training guarantees valid evaluation but might make the training distribution slightly less dense than potential test distributions

- **Failure signatures:**
  - **Constraint Satisficing:** The model follows instructions rigidly but ignores user intent (prevented by the judge-filtering step)
  - **Context Hallucination:** The model claims to find information in long context windows where none exists (a risk in Long RAG tasks)
  - **Tool Chatter:** The model invokes tools when it should respond conversationally (dataset explicitly tries to prevent this)

- **First 3 experiments:**
  1. **Reproduction of Filtering:** Take a small subset of generated instructions, run the described LLM-judge filtering, and verify that the "helpfulness" score correlates with human review
  2. **Ablation on Context:** Test the model on LongBench-v2 at 32k vs 120k tokens to verify the synthetic long-context data actually improved the long-range reasoning rather than just the input capacity
  3. **Thai Knowledge Retention:** Compare OpenJAI vs. Base Qwen3 on MMLU-ProX-lite-TH specifically. Confirm the ~3 point drop is acceptable for the gain in Tool Use (from ~46 to 47+)

## Open Questions the Paper Calls Out
- **Question:** What specific culturally-specific Thai contexts and training methodologies would most effectively improve performance on localized benchmarks compared to current translation-based approaches?
- **Question:** What factors contribute to the 13.5-point performance gap between English (60.5) and Thai (47.0) tool calling, and can targeted interventions reduce this disparity?
- **Question:** Can finetuning strategies be modified to prevent the slight degradation in Thai general knowledge (57.5 to 54.7 on MMLU-ProX-lite-TH) while maintaining specialization gains?

## Limitations
- The model shows a 3-point drop in Thai MMLU-ProX scores (57.5 â†’ 54.7), demonstrating the inevitable trade-off between specialization and knowledge retention
- The synthetic data generation pipeline lacks transparency about which judge model was used and how its preferences were validated against human annotators
- Long-context performance improvements may be partially artifactual, potentially reflecting pattern matching on synthetic long-context data rather than genuine reasoning improvements

## Confidence
- **High confidence:** The core architecture (Qwen3-14B finetuning) and general capability improvements are well-supported by the reported metrics
- **Medium confidence:** The mechanism of catastrophic forgetting avoidance through data mixing is plausible but the specific recipe isn't fully specified
- **Low confidence:** The long-context reasoning improvements beyond 32k tokens are difficult to verify without access to the synthetic data generation pipeline

## Next Checks
1. **Judge Model Validation:** Replicate the LLM-as-a-judge filtering process using a different judge model (e.g., GPT-4 vs. Claude) on the same synthetic instruction dataset. Compare the resulting instruction-following performance on IFBench to determine if filtering quality is judge-model dependent.

2. **Long-Context Reasoning Stress Test:** Evaluate OpenJAI-v1.0 on a manually curated set of long-context documents (legal contracts, academic papers, codebases) with specific information retrieval questions. Compare performance against the base Qwen3-14B to isolate whether improvements stem from genuine reasoning or pattern matching.

3. **Knowledge Retention Measurement:** Conduct a fine-grained analysis of MMLU-ProX-lite-TH subcategories to identify which knowledge domains suffered the most degradation (3-point drop). Correlate these losses with the specialized training domains to quantify the specialization-retention trade-off and identify potential mitigation strategies.