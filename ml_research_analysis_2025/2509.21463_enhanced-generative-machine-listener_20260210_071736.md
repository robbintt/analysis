---
ver: rpa2
title: Enhanced Generative Machine Listener
arxiv_id: '2509.21463'
source_url: https://arxiv.org/abs/2509.21463
tags:
- audio
- quality
- proposed
- mushra
- gmlv2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GMLv2, a reference-based audio quality metric
  that uses a Beta distribution-based loss to predict MUSHRA scores, addressing the
  need for accurate and scalable perceptual audio quality assessment. By replacing
  the Gaussian loss of GMLv1 with a Beta distribution, GMLv2 naturally models bounded
  MUSHRA scores and captures perceptual uncertainty, enabling better generalization
  across diverse audio content and codecs, including neural audio codecs (NACs).
---

# Enhanced Generative Machine Listener

## Quick Facts
- arXiv ID: 2509.21463
- Source URL: https://arxiv.org/abs/2509.21463
- Reference count: 0
- Key outcome: GMLv2 achieves higher Pearson and Spearman correlations (Rp > 0.92) and lower outlier ratios than PEAQ and ViSQOL across eight test sets by using Beta distribution-based loss for bounded MUSHRA score prediction.

## Executive Summary
GMLv2 is a reference-based audio quality metric that addresses the need for accurate and scalable perceptual audio quality assessment. By replacing the Gaussian loss of its predecessor with a Beta distribution-based loss, GMLv2 naturally models bounded MUSHRA scores and captures perceptual uncertainty. The model employs a Gammatone filterbank for feature extraction and a modified Inception-based architecture, enabling better generalization across diverse audio content and codecs, including neural audio codecs (NACs).

## Method Summary
GMLv2 is a reference-based perceptual audio quality prediction model that estimates MUSHRA scores and uncertainty from (reference, degraded) audio pairs. It uses Gammatone filter bank spectrograms (power only, 80ms window, 20ms hop, 32 channels) for feature extraction from L/R/M/S channels of both reference and degraded signals. An Inception-style backbone processes the concatenated features and outputs [α̃, β̃], which are parameterized to enforce unimodality (α = 1 + exp(α̃), β = 1 + exp(β̃)). The model is trained using Beta negative log-likelihood loss on 82,191 audio pairs (68,503 traditional + 14,688 neural codec samples) at 48kHz, with Pearson and Spearman correlations and outlier ratios as evaluation metrics.

## Key Results
- GMLv2 consistently achieves higher Pearson and Spearman correlations (Rp > 0.92) than PEAQ and ViSQOL across eight test sets
- Lower outlier ratios demonstrate superior robustness in predicting human perceptual ratings
- Better generalization to neural audio codecs (NACs) through inclusion of NAC datasets in training
- Beta distribution loss naturally models bounded MUSHRA scores and captures perceptual uncertainty

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Beta distribution loss provides statistically principled alignment with bounded MUSHRA scores.
- Mechanism: The Beta distribution is naturally confined to [0,1], matching normalized MUSHRA scores. Shape parameters (α, β) enable flexible modeling of symmetric, skewed, or bimodal listener score distributions. The network predicts [α̃, β̃] and applies α = 1 + exp(α̃), β = 1 + exp(β̃) to enforce unimodality (α, β > 1).
- Core assumption: MUSHRA score distributions are fundamentally unimodal in the collected datasets.
- Evidence anchors:
  - [abstract] "GMLv2 introduces a Beta distribution-based loss to model the listener ratings"
  - [section 2] "Naturally confined to the unit interval, the Beta distribution aligns perfectly with normalized MUSHRA scores, eliminating the need for post hoc adjustments"
  - [corpus] Weak direct corpus evidence; neighbor papers (SpeechQualityLLM) address quality assessment but not Beta distribution approaches
- Break condition: If listener distributions are genuinely bimodal or multimodal, the unimodality constraint (α, β > 1) would distort predictions.

### Mechanism 2
- Claim: Gammatone filter bank front-end approximates peripheral auditory processing for robust feature extraction.
- Mechanism: Gammatone filters approximate the impulse response of peripheral auditory filters derived from reverse correlation measurements. The model computes power Gammatone spectrograms for L, R, Mid (M = (L+R)/2), and Side (S = (L−R)/2) channels, concatenated across reference and degraded signals.
- Core assumption: Gammatone filter bank better captures perceptually relevant spectral features than standard STFT.
- Evidence anchors:
  - [section 2] "We use Gammatone filter bank at the signal preprocessing stage as a frequency analysis component computing the Gammatone spectrogram"
  - [section 2] "The 'revcor' function thus obtained can be considered as an estimate of the impulse response of the peripheral auditory filter"
  - [corpus] QASTAnet and related papers use similar auditory-inspired features for spatial audio
- Break condition: If content has significant phase-based cues not captured by power-only spectrograms, performance may degrade.

### Mechanism 3
- Claim: Diverse training data spanning traditional and neural codecs enables robust generalization.
- Mechanism: Training set combines 68,503 traditional codec samples (AAC, HE-AAC, AC-4, DD+JOC) and 14,688 neural codec samples (EnCodec, DAC, MDCTNet) at various bitrates, covering mono, stereo, and binaural configurations.
- Core assumption: Diversity of codec types in training transfers to unseen codec configurations.
- Evidence anchors:
  - [abstract] "incorporates additional neural audio coding (NAC) subjective datasets to extend its generalization"
  - [section 3.1] "In total, our dataset consists of 82,191 sample pairs, including 68,503 traditional codec samples and 14,688 neural codec samples"
  - [corpus] Weak corpus evidence; most neighbors focus on speech quality, not codec diversity
- Break condition: If test codecs introduce artifact types not represented in training, outlier ratios will increase.

## Foundational Learning

- Concept: Beta distribution parameterization and moments
  - Why needed here: To understand how α and β control distribution shape (mean = α/(α+β), variance = αβ/((α+β)²(α+β+1))) and why unimodality requires α, β > 1.
  - Quick check question: Given α = 3, β = 7, what is the mode and is the distribution unimodal?

- Concept: MUSHRA testing protocol (ITU-R BS.1534)
  - Why needed here: MUSHRA scores are bounded [0-100] with specific testing methodology; understanding this clarifies why bounded distributions are appropriate.
  - Quick check question: Why does MUSHRA use a 0-100 scale rather than MOS-style 1-5?

- Concept: Negative log-likelihood loss for probabilistic predictions
  - Why needed here: The model is trained by minimizing −log P(s; y) where s is the listener score, requiring understanding of maximum likelihood estimation.
  - Quick check question: How does Beta NLL (Eq. 5) differ from MSE loss in handling prediction uncertainty?

## Architecture Onboarding

- Component map: Audio signals (48kHz) -> Gammatone filter bank -> L/R/M/S power spectrograms -> Inception backbone -> 2D FC layer -> [α̃, β̃] -> α, β -> Beta NLL loss

- Critical path:
  1. Verify audio is 48kHz; resample if needed
  2. Compute Gammatone spectrograms for all 8 channels (L/R/M/S × reference/degraded)
  3. Forward pass through Inception backbone
  4. Extract α, β and compute predicted MUSHRA = (α/(α+β)) × 100

- Design tradeoffs:
  - Unimodality constraint (α, β > 1) improves calibration but cannot model multimodal listener disagreement
  - Power-only spectrograms ignore phase information
  - Reference-based: requires clean reference signal; cannot operate in non-intrusive mode

- Failure signatures:
  - High outlier ratio despite good correlation: check for edge cases near score boundaries (0 or 100)
  - Poor performance on neural codecs: verify NAC data included in training
  - Large confidence intervals: may indicate α + β (concentration) is low; check training convergence

- First 3 experiments:
  1. Ablate Beta loss → Gaussian NLL on same architecture; compare correlation and outlier ratio on held-out test sets
  2. Evaluate on out-of-distribution codec (not in training set); measure correlation degradation vs. in-distribution
  3. Analyze calibration: bin predictions by predicted uncertainty (variance); check if high-variance predictions have larger errors

## Open Questions the Paper Calls Out
- Quantitative validation of confidence intervals is left for future work
- The paper does not address how well the predicted confidence intervals reflect actual listener score variability

## Limitations
- Unimodality constraint may limit ability to capture genuinely multimodal listener score distributions
- Power-only spectrograms ignore phase information that could be perceptually significant
- Reference-based architecture cannot operate in non-intrusive quality assessment scenarios

## Confidence
- High confidence: Beta distribution loss mechanism provides statistically principled alignment with bounded MUSHRA scores
- Medium confidence: Generalization claims across diverse codecs and content types
- Medium confidence: Architectural choices including Gammatone filter bank and unspecified Inception details

## Next Checks
1. Conduct ablation study replacing Beta NLL with Gaussian NLL on the same architecture and compare correlation metrics and outlier ratios across all test sets
2. Test GMLv2 on a codec type completely absent from training to measure true generalization capability
3. Analyze calibration by binning predictions by predicted uncertainty and examining whether higher-variance predictions systematically exhibit larger errors