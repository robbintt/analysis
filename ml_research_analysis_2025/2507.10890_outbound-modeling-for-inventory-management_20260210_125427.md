---
ver: rpa2
title: Outbound Modeling for Inventory Management
arxiv_id: '2507.10890'
source_url: https://arxiv.org/abs/2507.10890
tags:
- inventory
- outbound
- cost
- each
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work develops a deep learning model to forecast regional inventory
  drain and shipping costs for multi-warehouse retail fulfillment. The method combines
  a discrete-plus-quantile distribution for sparse outbound predictions with quantile
  forecasting for costs, trained on historical glance-view and inventory data.
---

# Outbound Modeling for Inventory Management

## Quick Facts
- **arXiv ID:** 2507.10890
- **Source URL:** https://arxiv.org/abs/2507.10890
- **Reference count:** 40
- **Primary result:** Transformer-based architectures achieve strong accuracy (cost quantile losses ~0.14-0.15, outbound ~0.05) and calibration for regional inventory drain and shipping cost forecasting.

## Executive Summary
This work develops a deep learning model to forecast regional inventory drain and shipping costs for multi-warehouse retail fulfillment. The method combines a discrete-plus-quantile distribution for sparse outbound predictions with quantile forecasting for costs, trained on historical glance-view and inventory data. To ensure robustness for reinforcement learning, a backtesting oracle simulates off-policy trajectories by replaying customer demand with counterfactual inventory states and calling production fulfillment systems. Experiments show transformer-based architectures achieve strong accuracy and calibration, outperforming a closest-node heuristic baseline. The approach enables reliable simulation of inventory dynamics for regional control policy learning.

## Method Summary
The method uses a two-stream encoder architecture (WaveNet-style dilated convolutions for warehouses and customer regions) joined by cross-attention to learn supply-demand matching. A hybrid discrete-plus-quantile distribution captures sparse outbound patterns (explicit mass for 0-5 units, quantiles for tail), while costs are forecasted conditionally on outbound using quantile regression. Training uses a weighted multi-task loss combining cost NLL, cost quantile loss, cross-entropy, and outbound losses. An oracle simulates off-policy trajectories by replaying historical demand with counterfactual inventory states, using conditional glance-view conversion to maintain historical randomness while ensuring monotonicity under faster promises.

## Key Results
- Transformer-based models achieve cost quantile losses of ~0.14-0.15 and outbound quantile losses of ~0.05 on validation data
- Regional-level calibration improves significantly with transformers (CRPS 4.11 vs 6.25 for RNN baseline)
- The hybrid discrete-plus-quantile distribution outperforms pure quantile approaches on sparse outbound data
- Cross-attention enables better supply-demand matching than closest-node heuristic baseline

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A hybrid discrete-plus-quantile distribution better captures sparse outbound patterns than pure discrete or pure quantile approaches.
- Mechanism: The model allocates explicit probability mass to integer values k = 0, 1, ..., nd-1 (where nd=5 in implementation), with remaining mass distributed via a learned quantile-based CDF for the tail. This addresses the calibration problem where pure quantile models struggle when most data is zero or near-zero.
- Core assumption: Outbound follows a zero-inflated distribution where small counts dominate but large values require unbounded support.
- Evidence anchors:
  - [Section 4.1]: "because many product-warehouse-time combinations have small or zero outbound, we allocate explicit probability mass to those values, while accommodating unbounded large outbounds via the quantile-based tail."
  - [Section A.3]: "num outbound logits = 6 (6 discrete demand buckets so outbound >= 5 is predicted using quantiles)"

### Mechanism 2
- Claim: Cross-attention between node (warehouse) and location (customer region) embeddings enables the model to learn which warehouse fulfills which demand without explicit assignment rules.
- Mechanism: Two parallel encoder streams—WaveNet-style dilated convolutions on warehouse time-series and location time-series—are joined via multi-head cross-attention (8 heads), allowing each node embedding to attend over all location embeddings. This implicitly learns the fulfillment matching function that production system F performs.
- Core assumption: The fulfillment decision depends on joint inventory-demand geometry that can be captured through learned attention patterns.
- Evidence anchors:
  - [Section 4.4]: "we utilize a cross-attention layer to join the two embedding streams from nodes and locations, allowing for the supply-demand matching we aim to capture."
  - [Section A.4, Step 3]: "Cross-Attention (Nodes ← Locations): A multi-head cross-attention module (attention heads = 8) allows each node to attend over location embeddings"

### Mechanism 3
- Claim: The conditional glance-view conversion mechanism preserves historical conversion randomness when evaluating counterfactual promises, reducing variance in off-policy backtesting.
- Mechanism: Rather than independently sampling conversions under new promises, the oracle infers the latent uniform variable U from historical outcomes and reuses it. If a customer converted under a 2-day promise, the same U determines their behavior under a counterfactual 1-day promise—ensuring monotonicity (faster promise cannot reduce conversion).
- Core assumption: Historical conversion behavior reflects a latent decision variable that remains valid under counterfactual scenarios.
- Evidence anchors:
  - [Section 5.1]: "Independent sampling of glance view conversions can lead to high variance and unrealistic scenarios... By conditioning on historical outcomes, we ensure that the simulated conversions remain consistent with observed behaviors"

## Foundational Learning

- **Quantile Regression and Probabilistic Forecasting**
  - Why needed here: The core model outputs quantiles (q10, q50, q90) rather than point estimates, and you must understand quantile loss (pinball loss), why it's proper for single predictions but not sequences, and how to calibrate quantile predictions.
  - Quick check question: Given predictions [10, 15, 25] for quantiles [0.1, 0.5, 0.9] and true value 20, compute the quantile loss.

- **Exogenous Interactive Decision Processes (ExoIDP)**
  - Why needed here: The paper frames inventory control as an ExoIDP where agent actions don't affect demand, enabling supervised learning reduction rather than worst-case RL. Understanding this framing is critical to why backtesting works.
  - Quick check question: In an ExoIDP, why can you evaluate a new policy using only historical data without environment interaction?

- **Teacher Forcing in Autoregressive Models**
  - Why needed here: The cost decoder conditions on outbound quantities. During training, ground-truth outbound is provided (teacher forcing); during inference, sampled outbound is used. Understanding the train-inference gap is essential for debugging.
  - Quick check question: What is exposure bias in teacher-forced models, and how might it manifest when the drain model is used in RL rollouts?

## Architecture Onboarding

- **Component map:**
Node Features (outbound, inventory, location) ──┐
                                                ├──> WaveNet Encoder (3 dilated convs) ──> Node Embeddings
Location Features (glanceviews, location) ──────┘                                                              │
                                                                                                              ▼
Holiday Features ─────────────────────────────────────────────────────────────────────> Cross-Attention (Nodes ← Locations)
                                                                                                              │
                                                                                                              ▼
                                                                                                   Outbound Decoder (MLP)
                                                                                                              │
                                                                                                              ▼ (outbound sample)
                                                                                                   Cost Decoder (MLP)

- **Critical path:** The cross-attention layer is the architectural bottleneck—it must learn supply-demand matching. If attention heads don't develop meaningful patterns (e.g., attending to nearby locations), the model defaults to averaging and loses regional differentiation.

- **Design tradeoffs:**
  - RNN vs. Transformer cross-encoder: Paper finds minimal accuracy difference (Table 2), but Transformer provides better calibration at regional level (Table 3: CRPS 4.11 vs 6.25). Transformer preferred for calibration-critical applications.
  - Including sales features improves cost prediction (NLL 1.412 vs 1.442) but is unusable in RL deployment (endogenous). Use sales-included models only for diagnostics.
  - Loss weight selection ([0, 4, 2, 0.3, 6]) is arbitrary; authors note this as an improvement area.

- **Failure signatures:**
  - Over-concentration of outbound predictions: If the model predicts all drain from one or two warehouses regardless of inventory distribution, attention may have collapsed (cf. closest-node baseline in Figure 3).
  - Calibration degradation at regional level: If p10/p50/p90 slopes deviate significantly from 1.0 at regional granularity but not national, the model is learning aggregate statistics without regional differentiation.
  - Cost predictions that ignore outbound: If cost quantiles don't shift when outbound changes, the cost decoder may not be receiving the conditioning signal.

- **First 3 experiments:**
  1. **Baseline replication:** Implement the closest-node heuristic (Algorithm 2) and verify your evaluation pipeline reproduces Table 2 baseline metrics (outbound quantile loss ~0.6-0.7).
  2. **Ablation on discrete head:** Train with nd=0 (pure quantile) vs. nd=5 and measure calibration on zero-outbound cases; expect pure quantile to overpredict small values.
  3. **Off-policy validation via oracle:** On a held-out product set, run the oracle (Algorithm 1) with counterfactual inventory states (e.g., redistribute 50% of inventory from top warehouse to others) and compare model predictions vs. oracle ground truth.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the model's predictive accuracy degrade when evaluated on out-of-distribution (OOD) counterfactual inventory states compared to the reported in-distribution results?
- Basis in paper: [explicit] The abstract states that "Preliminary results demonstrate the model's accuracy within the in-distribution setting," while the introduction emphasizes that the model "must handle out-of-distribution scenarios that arise from off-policy trajectories."
- Why unresolved: The empirical results section quantifies performance (e.g., quantile losses ~0.14) on validation data, but does not explicitly measure performance degradation using the proposed oracle on policy-induced OOD data.
- What evidence would resolve it: A comparison of quantile losses and calibration metrics when the model is evaluated on counterfactual inventory states generated by the backtesting oracle versus historical holdout data.

### Open Question 2
- Question: Does fine-tuning the drain model on data generated by the production system oracle improve generalization compared to training solely on historical data?
- Basis in paper: [explicit] The introduction suggests that "fine-tuning the drain model on data produced by the oracle may be necessary to ensure reliable simulation performance."
- Why unresolved: The paper establishes the methodology for generating this data but the experimental results focus on the base model trained on historical observations, without demonstrating the impact of this proposed fine-tuning step.
- What evidence would resolve it: Empirical results showing validation loss or downstream reinforcement learning policy performance for models fine-tuned on oracle-generated trajectories versus those trained only on historical distributions.

### Open Question 3
- Question: Does the proposed conditional glance view conversion mechanism significantly reduce variance and improve reliability over independent sampling during backtesting?
- Basis in paper: [inferred] Section 5.1 notes that independent sampling can lead to high variance and "unrealistic scenarios," proposing conditional conversion as a method that "may potentially produce more reliable backtest results."
- Why unresolved: While the mathematical formulation for the conditional conversion is provided, the paper does not present experimental analysis quantifying the variance reduction or reliability improvement of this specific method.
- What evidence would resolve it: A comparative ablation study showing the variance of simulated demand trajectories and outbound estimates using the conditional method versus standard independent sampling.

## Limitations
- The conditional glance-view conversion oracle's effectiveness depends on assumptions about latent conversion variables not validated externally
- Loss weight selection is ad hoc and model sensitivity to hyperparameter changes is not reported
- The approach relies on proprietary Amazon data and fulfillment system integration that limit reproducibility
- Cross-attention's necessity is inferred from calibration improvements but lacks direct ablation studies

## Confidence
- **High Confidence:** The hybrid discrete-plus-quantile distribution outperforms pure quantile approaches on sparse outbound data, as evidenced by calibration curves and discrete calibration metrics.
- **Medium Confidence:** Cross-attention learns meaningful supply-demand matching, inferred from regional-level calibration improvements, but lacks ablation or interpretability studies.
- **Low Confidence:** The conditional glance-view conversion oracle reliably reduces variance in off-policy backtesting without external validation or sensitivity analysis.

## Next Checks
1. **Ablation on Cross-Attention:** Remove the cross-attention layer and compare regional-level CRPS and calibration slopes to confirm its necessity for supply-demand matching.
2. **Distribution Sensitivity:** Test the model with nd=0 (pure quantile) and nd=10 to quantify the impact of discrete bucket count on calibration, especially for zero-outbound cases.
3. **Oracle Variance Reduction:** Compare variance of off-policy cost predictions with and without the conditional glance-view conversion mechanism on a held-out product set with synthetic inventory redistributions.