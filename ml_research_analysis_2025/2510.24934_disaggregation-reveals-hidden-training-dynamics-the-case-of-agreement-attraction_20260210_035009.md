---
ver: rpa2
title: 'Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction'
arxiv_id: '2510.24934'
source_url: https://arxiv.org/abs/2510.24934
tags:
- language
- singular
- pythia
- plural
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces a method to study language model training\
  \ dynamics by disaggregating performance across carefully constructed syntactic\
  \ conditions over time. Instead of relying on aggregate accuracy, the authors analyze\
  \ how models behave on different subsets\u2014such as singular/plural verbs with\
  \ and without attractors\u2014across training steps."
---

# Disaggregation Reveals Hidden Training Dynamics: The Case of Agreement Attraction

## Quick Facts
- **arXiv ID:** 2510.24934
- **Source URL:** https://arxiv.org/abs/2510.24934
- **Reference count:** 27
- **Primary result:** Disaggregating evaluation conditions reveals non-monotonic learning phases (frequency bias → local context → generalization) that are invisible in aggregate metrics.

## Executive Summary
This paper introduces a method to study language model training dynamics by disaggregating performance across carefully constructed syntactic conditions over time. Instead of relying on aggregate accuracy, the authors analyze how models behave on different subsets—such as singular/plural verbs with and without attractors—across training steps. They find that early in training, models rely on word frequency heuristics, then shift to sensitivity to local context (e.g., matching the preceding noun), before eventually learning more general grammatical rules. This progression is observable even in smaller models and varies across random seeds. The results suggest learning proceeds in interpretable phases rather than being sudden or gradual, and highlight the importance of subset-level analysis for understanding grammatical acquisition. Token normalization was tested but found to overly penalize multi-token verbs. Overall, disaggregation reveals hidden breakthroughs in training dynamics that are invisible in aggregate metrics.

## Method Summary
The method evaluates PolyPythia models (10 seeds each of Pythia 14M-410M) across training checkpoints using BIG-bench Subject-Verb Agreement stimuli. For each checkpoint and condition, log-probabilities of correct vs incorrect verb forms are computed, with multi-token verbs handled by summing log-probabilities. Accuracy is disaggregated across 6 conditions: Singular/Plural Target × No Attractor/Matching Attractor/Mismatching Attractor. Results are plotted per condition across training steps (log-scale x-axis from ~100 to ~100K steps) with 95% CIs across seeds.

## Key Results
- Early training shows frequency bias: "is" preferred over "are" and plural forms over singular for other verbs
- Attractor interference emerges mid-training, causing errors when attractors mismatch target number
- Plural-with-singular-attractor condition shows dip then recovery as models develop broader context sensitivity
- Multi-token verbs show delayed/attenuated patterns compared to single-token verbs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disaggregating evaluation conditions reveals non-monotonic learning phases that aggregate metrics obscure.
- Mechanism: Aggregate accuracy averages over conditions with opposing trajectories (e.g., improving on one while degrading on another), masking rapid internal shifts. Condition-level analysis exposes these by tracking each experimental manipulation separately over training.
- Core assumption: The experimental conditions isolate linguistically meaningful factors (e.g., attractor presence/matching) that tap into distinct competencies.
- Evidence anchors:
  - [abstract] "disaggregating over the conditions... it is possible to better understand the intermediate stages"
  - [Results, p.3] Aggregate shows gradual improvement; disaggregated shows "sharp increase" and "sharp decrease" at specific steps for different conditions
  - [corpus] Weak direct corpus support; related work on grammatical agreement (neighbor paper on German articles) also questions rule-based vs. heuristic learning
- Break condition: If conditions do not isolate independent competencies, disaggregation may not yield interpretable phases.

### Mechanism 2
- Claim: Models initially prefer higher-frequency verb forms, then develop context sensitivity, then generalize.
- Mechanism: Training progresses through n-gram-like stages (unigram → bigram → trigram), consistent with prior work showing transformers overfit to progressively longer context windows. Single-token verbs require bigram sensitivity (noun-verb); multi-token verbs require trigram sensitivity (noun-first_token-second_token).
- Core assumption: The n-gram analogy accurately characterizes the internal computation, not just surface behavior.
- Evidence anchors:
  - [Results, p.3] "is" (more frequent) preferred over "are" initially; plural forms preferred for other verbs
  - [Discussion, p.4] "transformers overfit their predictions to token unigram probability... then bigram... then trigram"
  - [corpus] Corpus support is indirect; no direct test of n-gram vs. longer-context mechanisms in this paper
- Break condition: If models develop longer-context mechanisms in parallel rather than sequentially, the staged interpretation weakens.

### Mechanism 3
- Claim: Attractor interference emerges mid-training and can reverse before final generalization.
- Mechanism: As models become sensitive to local noun number (bigram stage), mismatching attractors cause errors. Later, broader context integration reduces this interference, improving performance on mismatched-attractor conditions.
- Core assumption: Attractor effects reflect intermediate-stage heuristics rather than permanent architectural limitations.
- Evidence anchors:
  - [Results, p.3] "we see a strong effect of the attractors on performance" followed by "overall increase in performance for all conditions"
  - [Figure 1] Plural-with-singular-attractor condition shows dip then recovery
  - [corpus] Related work (Lakretz et al., 2021 in references) documents attractor effects in trained models
- Break condition: If attractor interference persists at convergence for certain structures, the "intermediate phase" interpretation fails for those cases.

## Foundational Learning

- Concept: Minimal pairs / experimental disaggregation
  - Why needed here: The entire method depends on comparing performance across minimally-different sentence types (e.g., matching vs. mismatching attractor) to isolate specific competencies.
  - Quick check question: Can you explain why comparing "the athletes near the bike know" vs. "the athletes near the bikes know" reveals attractor sensitivity?

- Concept: N-gram language models and frequency effects
  - Why needed here: The proposed mechanism relies on understanding how unigram, bigram, and trigram models make predictions, and how frequency biases emerge.
  - Quick check question: Why would a bigram model struggle more with multi-token verb agreement than a single-token verb?

- Concept: Agreement attraction (psycholinguistics)
  - Why needed here: The task design comes from human psycholinguistics; understanding the phenomenon clarifies what the model is being tested on.
  - Quick check question: In "the key to the cabinets is/*are rusty," what is the attractor and why does it cause errors?

## Architecture Onboarding

- Component map: Evaluation loop -> Log-probability extraction -> Condition classification -> Accuracy aggregation -> Plot trajectories
- Critical path:
  1. Implement log-probability extraction for target tokens (handle multi-token sum vs. mean normalization)
  2. Disaggregate accuracy by condition, verb, and checkpoint
  3. Plot condition-level trajectories alongside aggregate mean
- Design tradeoffs:
  - Sum vs. mean log-probability for multi-token words: Sum preserves true probability; mean may over-correct (Appendix D shows mean always favors longer sequences post-early-training)
  - Verb-level vs. pooled analysis: Verb-level reveals variation (e.g., "stimulate" shows smaller frequency preference) but increases noise
- Failure signatures:
  - Aggregate-only analysis: Appears smooth; misses non-monotonicity
  - Ignoring tokenization: Multi-token verbs show delayed/attenuated patterns; pooling with single-token obscures this
  - Single seed: High variance; patterns less stable in smaller models (see seed-level plots in Appendix C)
- First 3 experiments:
  1. Replicate Figure 1 for a new grammatical phenomenon (e.g., reflexive binding) to test generalization of disaggregation approach
  2. Explicitly fit n-gram baselines at each checkpoint to test whether model behavior aligns with n-gram predictions during early phases
  3. Vary attractor distance (PP vs. relative clause) to probe whether "longer dependency" phases correspond to specific context lengths

## Open Questions the Paper Calls Out
- Are the observed training phases driven strictly by n-gram statistics, or do they reflect a general ability to process increasingly long contexts?
- Do the identified learning phases and attractor interference effects generalize to agreement structures involving intervening material other than prepositional phrases?
- Do these distinct heuristic phases (frequency, local context, grammatical rules) appear in languages with morphological systems different from English?

## Limitations
- The corpus support for the proposed n-gram-like learning stages is indirect; no direct test of n-gram vs. longer-context mechanisms in this paper
- The disaggregation method relies heavily on experimental conditions being true minimal pairs that isolate specific competencies
- Multi-token verb handling is a critical technical detail; the paper uses sum-of-log-probabilities but doesn't extensively validate this choice

## Confidence
- **High confidence:** The core empirical finding that disaggregating evaluation conditions reveals non-monotonic learning dynamics invisible in aggregate metrics
- **Medium confidence:** The characterization of learning as progressing through frequency bias → local context sensitivity → generalization
- **Medium confidence:** The claim that attractor interference is an intermediate-stage phenomenon that can reverse

## Next Checks
1. **Explicit n-gram baseline comparison:** Fit unigram, bigram, and trigram language models to the training data and evaluate them on the same conditions at each checkpoint. Compare their performance trajectories to the neural model's to directly test whether the observed phases align with n-gram predictions.

2. **Alternative attractor distances:** Extend the experimental paradigm to include attractors in relative clauses (e.g., "the key that the cabinets need is/*are rusty") and compare the emergence and resolution of interference effects to the PP-attractor case.

3. **Control for semantic/structural confounds:** Design conditions that manipulate semantic plausibility or structural complexity (e.g., object-relative vs. subject-relative attractors) while holding local number matching constant.