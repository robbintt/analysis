---
ver: rpa2
title: 'Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation'
arxiv_id: '2507.11645'
source_url: https://arxiv.org/abs/2507.11645
tags:
- grokking
- accuracy
- test
- training
- dropout
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces practical metrics to forecast grokking, the
  delayed generalization phenomenon in neural networks. The authors propose Dropout
  Robustness Curves to quantify network sensitivity to noise, showing that generalization
  correlates with decreased variance under dropout.
---

# Tracing the Path to Grokking: Embeddings, Dropout, and Network Activation

## Quick Facts
- arXiv ID: 2507.11645
- Source URL: https://arxiv.org/abs/2507.11645
- Reference count: 17
- One-line primary result: Introduces Dropout Robustness Curves and embedding structure metrics to forecast and analyze the delayed generalization phenomenon known as grokking.

## Executive Summary
This paper introduces practical metrics to forecast and analyze grokking, the phenomenon where neural networks exhibit sudden generalization after prolonged overfitting. The authors propose Dropout Robustness Curves (DRC) to quantify network sensitivity to noise, showing that generalization correlates with decreased variance under dropout. They find that embedding cosine similarity and bimodal embedding distributions emerge with generalization, revealing structured representations. These metrics provide early indicators of grokking onset and offer insights into the transition from memorization to generalization.

## Method Summary
The study trains a two-layer MLP (256 hidden units) on modular addition, tracking several metrics during training. The model uses an embedding layer for inputs, a hidden layer with ReLU activation, and outputs a probability distribution over P=53 possible results. Key metrics include the variance of test accuracy under Monte Carlo dropout, embedding cosine similarity heatmaps, embedding weight distributions, and neuron sparsity (percentage of inactive ReLU neurons). Training uses AdamW optimizer with weight decay, and Xavier Normal initialization.

## Key Results
- Test accuracy variance under MC dropout peaks locally during grokking, providing a predictive signal.
- Embedding cosine similarity reveals structured patterns (diagonal bands) that emerge with generalization.
- Embedding weight distributions transition from Gaussian to bimodal during generalization.
- Neuron sparsity decreases during generalization, correlating with improved test performance.

## Why This Works (Mechanism)
The paper demonstrates that grokking involves a fundamental reorganization of learned representations. As the network transitions from memorization to generalization, the embedding layer develops structured, bimodal distributions that capture the periodic nature of modular addition. The hidden layer becomes less sparse, indicating more active neurons are contributing to generalization. Dropout variance peaks because the network is in a transitional state where small perturbations can significantly affect generalization performance.

## Foundational Learning
- Concept: **Grokking**
  - Why needed here: The entire paper is framed around predicting and understanding this phenomenon of delayed generalization.
  - Quick check question: Can you describe the typical shape of the test accuracy curve during grokking compared to the training accuracy curve?

- Concept: **Monte Carlo (MC) Dropout**
  - Why needed here: This is the core technique used to generate the predictive "variance" metric.
  - Quick check question: How does MC dropout differ from standard dropout used during training, and what does its output variance represent?

- Concept: **Cosine Similarity**
  - Why needed here: This is a key metric for analyzing the structure of learned embeddings.
  - Quick check question: If two embedding vectors have a cosine similarity close to 1, what does that imply about the inputs they represent?

## Architecture Onboarding
- Component map: Inputs (integers i, j) → Embedding Layer → Concatenation → Hidden Layer (256 neurons, ReLU) → Output Layer (size P=53) → Cross-Entropy Loss
- Critical path: The most critical path for grokking prediction is from the Embedding Layer's learned representations through to the Hidden Layer's activations.
- Design tradeoffs: Using a minimal MLP on a synthetic task (modular addition) allows for clear, interpretable signals. The tradeoff is that these specific, clean signals may be noisier or harder to isolate in larger, more complex architectures on real-world data.
- Failure signatures:
  - No Variance Peak: If variance under MC dropout doesn't peak, grokking may not occur or the sampling rate is too low.
  - Gaussian Embeddings: If the embedding distribution remains Gaussian and fails to become bimodal, the model is likely stuck in memorization.
  - Unstructured Cosine Similarity: A lack of structured patterns in the cosine similarity heatmap indicates the model hasn't learned the task's underlying symmetries.
- First 3 experiments:
  1. Reproduce Key Metrics: Train the described model and plot the variance of test accuracy under MC dropout vs. epochs to confirm the local maximum near the grokking point.
  2. Analyze Embedding Evolution: Track the distribution of embedding weights and the cosine similarity heatmap over training. Confirm the transition from Gaussian to bimodal and the emergence of structured patterns.
  3. Test Sparsity Correlation: Plot the percentage of inactive neurons (post-ReLU) against training/test accuracy to verify its correlation with the onset of generalization.

## Open Questions the Paper Calls Out
- Can the proposed indicators (DRC, embedding structure) reliably predict grokking in complex, non-algorithmic datasets?
- What is the precise quantitative relationship between training hyperparameters and the observed grokking features?
- Is the emergence of bimodal embedding distributions a universal mechanism for grokking or a specific adaptation to periodic data?

## Limitations
- The study is limited to a synthetic, algorithmic task (modular addition) with distinct symmetry properties that may not generalize to real-world data.
- Key architectural parameters like embedding dimension are not explicitly specified, introducing uncertainty in reproduction.
- The relationship between specific hyperparameters and grokking features is not formally quantified.

## Confidence
- High Confidence: The core observation that test accuracy variance under MC dropout peaks locally during grokking.
- Medium Confidence: The claim that embedding distributions transition from Gaussian to bimodal with generalization.
- Medium Confidence: The correlation between neuron sparsity and generalization.

## Next Checks
1. Reproduce the DRC and Variance Peak: Train the model with the exact hyperparameters and confirm that the DRC curve shows decreasing accuracy with higher dropout rates, and that the variance of test accuracy under MC dropout peaks just before the grokking point.
2. Test Embedding Dimensionality: Systematically vary the embedding dimension and measure its effect on the emergence of the bimodal distribution and structured cosine similarity.
3. Validate Sparsity Correlation: Implement a version of the model with different activation functions and verify that the correlation between neuron sparsity and generalization holds.