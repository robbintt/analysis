---
ver: rpa2
title: 'Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification
  with Interactive Graph Representation'
arxiv_id: '2505.22993'
source_url: https://arxiv.org/abs/2505.22993
tags:
- claim
- entity
- graph
- triplets
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VeGraph, a framework for claim verification
  that uses an interactive graph representation to handle complex claims with ambiguous
  entities. The method works in three phases: (1) decomposing the claim into structured
  graph triplets; (2) iteratively resolving ambiguous entities by querying a knowledge
  base; and (3) verifying the remaining triplets to reach a final verdict.'
---

# Verify-in-the-Graph: Entity Disambiguation Enhancement for Complex Claim Verification with Interactive Graph Representation

## Quick Facts
- arXiv ID: 2505.22993
- Source URL: https://arxiv.org/abs/2505.22993
- Reference count: 32
- Outperforms strong baselines on HoVer and FEVEROUS benchmarks for entity disambiguation and numerical reasoning tasks

## Executive Summary
This paper introduces VeGraph, a framework for claim verification that uses an interactive graph representation to handle complex claims with ambiguous entities. The method works in three phases: (1) decomposing the claim into structured graph triplets; (2) iteratively resolving ambiguous entities by querying a knowledge base; and (3) verifying the remaining triplets to reach a final verdict. The framework is evaluated on the HoVer and FEVEROUS benchmarks using Meta-Llama-3-70B. Results show that VeGraph outperforms several strong baselines, especially in entity disambiguation and numerical reasoning tasks. The ablation study demonstrates that both the graph representation and multi-step reasoning contribute significantly to performance. The approach also provides interpretable reasoning traces for explainability.

## Method Summary
VeGraph is a three-phase framework for claim verification that addresses complex claims with ambiguous entities. First, it decomposes claims into structured entity-relation triplets using few-shot prompting. Second, it iteratively resolves ambiguous entities by generating questions and querying a knowledge base (Wikipedia) with up to 5 refinement iterations. Third, it verifies remaining triplets against retrieved documents using a strict conjunctive logic (all triplets must be verified for a "Supported" verdict). The system uses Meta-Llama-3-70B-Instruct as the backbone LLM with BM25 and Bi-Encoder retrieval for KB interaction. The framework achieves higher Macro-F1 scores than baselines by improving entity disambiguation accuracy through iterative refinement.

## Key Results
- VeGraph outperforms baselines on HoVer and FEVEROUS benchmarks, especially in entity disambiguation and numerical reasoning
- Ablation study shows removing the graph representation severely degrades performance on complex 3-hop and 4-hop claims
- Iterative disambiguation increases resolution success, with VeGraph averaging ~3 requests in 4-hop scenarios vs. baselines' ~1.6
- The framework provides interpretable reasoning traces for explainability

## Why This Works (Mechanism)

### Mechanism 1: Semantic Isolation via Graph Triangulation
Decomposing complex claims into entity-relation triplets appears to reduce semantic ambiguity by isolating specific relationships for independent verification. The LLM converts natural language into a structured graph $G = \{T_1, \dots, T_N\}$. By forcing the claim into triplets $(E_1, R, E_2)$, the system separates complex logic into discrete verifiable units, preventing semantic drift common in long-context reasoning. Core assumption: The LLM can accurately extract entities and relations without hallucinating structures not present in the text. Evidence anchors: [Section 3.1] "transform each claim into a graph representation composed of triplets... capturing a subclaim within the original claim"; [Section 4.5] Ablation study shows removing the graph component "severely degrades performance" on complex 3-hop and 4-hop claims. Break condition: If the initial graph construction misinterprets the subject-object relationship, the subsequent logic will fail regardless of retrieval quality.

### Mechanism 2: Iterative State Refinement for Disambiguation
Multi-step interaction with a knowledge base (KB) allows the system to recover from initial retrieval failures by refining queries based on partial context. When an entity $X$ is ambiguous, the agent generates a question $q$ based on neighboring triplets. If $q$ fails to resolve $X$ in the KB, the system logs the failure and generates a refined question $q'$ using alternative triplet aspects. This allows the agent to pivot its search strategy dynamically rather than failing after a single attempt. Core assumption: The KB contains the necessary information, and the failure is due to query formulation rather than data absence. Evidence anchors: [Section 3.3] "In the case when the question q fails... fed back into the LLM... to generate a refined question"; [Table 5] Shows iterative requests increase resolution success, with VeGraph averaging ~3 requests in 4-hop scenarios vs. baselines' ~1.6. Break condition: If the maximum iteration limit $k$ is reached without resolution, the claim defaults to "Refuted" (Conservative Failure).

### Mechanism 3: Conjunction-Based Verdict Aggregation
Treating a complex claim as a conjunction of sub-claims (triplets) simplifies the final decision boundary to binary logic (AND operation). The verification phase checks remaining triplets against retrieved documents. The final verdict logic is strict: *Supported* iff all triplets are verified; *Refuted* if one fails. This prevents "partial credit" reasoning where a model might focus on correct details while ignoring a single false sub-claim. Core assumption: The claim's truth value is strictly conjunctive; it does not handle disjunctive ("A or B") or conditional logic well. Evidence anchors: [Section 3.1] "if all the graph triplets are verified... Supported, if one... Refuted"; [Figure 9] Example shows a "False" verdict because a single sub-claim was false, despite other triplets being true. Break condition: Complex negation or disjunctive claims may be forced into a conjunctive graph structure that misrepresents the original semantic logic.

## Foundational Learning

- **Concept: Open Information Extraction (OpenIE)**
  - Why needed here: The Graph Representation phase relies on extracting relations not limited to predefined schemas (e.g., "is based on the life of").
  - Quick check question: Can you differentiate between a fixed-schema relation extraction and an OpenIE approach used in Section 3.1?

- **Concept: In-Context Learning (Few-Shot Prompting)**
  - Why needed here: The system uses few-shot examples to teach the LLM the specific triplet format and disambiguation logic without weight updates.
  - Quick check question: How does the prompt in Figure 12 guide the model to handle "hidden" entities differently from explicit ones?

- **Concept: Dense Retrieval (Bi-Encoder/Reranker)**
  - Why needed here: The KB interaction relies on a two-layer retrieval system (BM25 + Bi-Encoder) to find evidence for sub-claims.
  - Quick check question: Why might a dense retriever (Bi-Encoder) perform better than BM25 for resolving ambiguous entities with descriptive natural language questions?

## Architecture Onboarding

- **Component map:**
  - Claim Input -> Graph Construction (Few-shot prompt) -> Entity Disambiguation Loop -> Verification -> Final Verdict

- **Critical path:**
  1. Claim Input -> **Graph Construction** (Few-shot prompt)
  2. **Entity Disambiguation Loop**: Identify $X$ -> Generate Question -> Retrieve -> Update Graph (Repeat $k$ times)
  3. **Verification**: Verify remaining triplets -> Aggregate Verdict

- **Design tradeoffs:**
  - **Latency vs. Accuracy:** The iterative loop significantly increases inference time (40-50% higher than ProgramFC in Table 4) to gain higher disambiguation accuracy
  - **Specificity vs. Generalization:** The generalized pipeline performs better on disambiguation tasks but slightly underperforms on specific multi-hop benchmarks (FEVEROUS) where tailored heuristics (ProgramFC) excel

- **Failure signatures:**
  - **Graph Representation Errors (29% in 2-hop):** Incorrect triplet structure (e.g., Figure 5 shows wrong relation mapping)
  - **Entity Resolution Loop Exhaustion:** Failing to resolve after $k$ steps, forcing a "Refuted" label (Table 5 shows ~30% failure rate in resolution attempts)

- **First 3 experiments:**
  1. **Ablation Validation:** Run `VeGraph` vs. `VeGraph w/o Interactive Graph` on a 4-hop sample to confirm the performance delta observed in Table 2
  2. **Iteration Sensitivity:** Plot performance vs. max iterations $k$ (e.g., $k=1$ to $10$) to find the latency/accuracy inflection point
  3. **Retrieval Stress Test:** Introduce noise into the KB (irrelevant docs) to test if the LLM can still verify triplets via the `FACT_CHECK_WITH_DOCS` prompt (Figure 10) without hallucinating

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can the VeGraph framework be adapted to effectively handle implicit information and nuanced reasoning in real-world claims?
- **Basis in paper:** [explicit] The authors state in the Limitations section that current datasets focus on explicit reasoning, whereas "real-world claims often require processing implicit information," which remains a "crucial direction for future work."
- **Why unresolved:** The current methodology relies on extracting explicit triplets from text, lacking mechanisms to infer unstated context.
- **What evidence would resolve it:** Evaluation of a modified framework on a dataset specifically designed for implicit or commonsense reasoning, showing improved performance over the baseline VeGraph.

### Open Question 2
- **Question:** Can the computational efficiency of the iterative disambiguation process be optimized to reduce latency for real-time applications?
- **Basis in paper:** [explicit] The paper notes that the framework "imposes computational overhead due to its frequent reliance on large language models," causing latency issues, and identifies optimizing this process as a "promising direction."
- **Why unresolved:** The multi-step agent interactions require multiple LLM calls per claim (up to 5 iterations), significantly increasing inference time compared to single-pass baselines.
- **What evidence would resolve it:** A modified architecture that reduces the number of required LLM calls or total inference time (currently 9-13s per claim) without degrading Macro-F1 scores.

### Open Question 3
- **Question:** What specific mechanisms can be integrated to detect and mitigate hallucinations or biases inherent in the backbone LLM during graph construction?
- **Basis in paper:** [explicit] The authors highlight that LLMs are "prone to errors and may exhibit biases," and developing effective mechanisms to control these is an "open challenge."
- **Why unresolved:** The framework currently lacks safeguards against LLM errors during the initial graph decomposition, which accounted for 15-29% of errors in the analysis.
- **What evidence would resolve it:** Integration of a self-correction or voting module that demonstrably lowers the "Graph Representation Error" rate identified in the human analysis.

## Limitations
- Reliance on Wikipedia-only evidence sources limits generalizability to claims requiring external knowledge
- Strict conjunctive logic may misrepresent claims containing disjunctive or conditional elements
- Iterative disambiguation loop significantly increases latency (40-50% overhead) with a non-trivial failure rate (30% of queries fail to resolve entities)

## Confidence
- **High Confidence**: Graph representation effectiveness, iterative disambiguation mechanism, and overall performance improvements on benchmarks
- **Medium Confidence**: The strict conjunctive verdict aggregation logic, and the generalizability of findings beyond Wikipedia
- **Low Confidence**: Exact performance impact of each architectural component, and the system's behavior on claims with complex logical structures

## Next Checks
1. **Logical Structure Stress Test**: Create a benchmark of claims containing disjunctive ("A or B"), conditional ("If A then B"), and negated logic to test whether the conjunctive aggregation assumption breaks down on non-standard claim structures.

2. **Cross-Domain Generalization**: Evaluate VeGraph on claims requiring external knowledge sources beyond Wikipedia (e.g., scientific papers, real-time data) to assess whether the graph-disambiguation approach generalizes to knowledge domains where dense retrieval may struggle.

3. **Iterative Refinement Efficiency**: Conduct a systematic ablation of the max iteration parameter $k$ (e.g., $k=1,3,5,10$) to quantify the latency-accuracy tradeoff and identify the point of diminishing returns in disambiguation performance.