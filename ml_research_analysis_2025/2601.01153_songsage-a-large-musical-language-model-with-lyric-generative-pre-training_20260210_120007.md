---
ver: rpa2
title: 'SongSage: A Large Musical Language Model with Lyric Generative Pre-training'
arxiv_id: '2601.01153'
source_url: https://arxiv.org/abs/2601.01153
tags:
- music
- lyrics
- playlist
- songsage
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SongSage introduces a large musical language model with lyric-centric
  pretraining to address the limitations of general-purpose LLMs in understanding
  and generating music-related content. The model is pretrained on LyricBank, a 5.48
  billion token corpus of song lyrics, and fine-tuned on LyricBank-SFT, an instruction-tuning
  dataset with 775k samples across nine lyric-centric tasks.
---

# SongSage: A Large Musical Language Model with Lyric Generative Pre-training

## Quick Facts
- **arXiv ID:** 2601.01153
- **Source URL:** https://arxiv.org/abs/2601.01153
- **Reference count:** 26
- **Primary result:** Large musical language model achieving significant improvements in music-related tasks through lyric-centric pretraining

## Executive Summary
SongSage introduces a specialized large language model for music-related tasks through a three-stage training approach: continual pretraining on LyricBank (5.48B tokens of song lyrics), instruction tuning on LyricBank-SFT (775k samples across nine lyric-centric tasks), and preference optimization. The model demonstrates superior performance in playlist understanding, lyric generation, and music knowledge comprehension compared to general-purpose LLMs like GPT-4o, while maintaining general knowledge understanding (63.6% on MMLU). SongSage achieves notable improvements in user-query rewrite for playlist recommendations (HR@1: 13.69% vs 11.45% for GPT-4o) and lyric continuation (22.0% higher preference ratio over GPT-4o in human evaluation).

## Method Summary
SongSage employs a three-stage training pipeline: (1) continual pretraining on LyricBank, a 5.48B token corpus of song lyrics in four languages (English, Chinese, Korean, Japanese), (2) instruction tuning on LyricBank-SFT, a 775k sample dataset spanning nine lyric-centric tasks including emotion detection, playlist understanding, and lyric generation, and (3) preference optimization using a synthetic preference set (9k samples) with GPT-4o/DeepSeek as preferred targets. The model uses LoRA fine-tuning (rank=32) for parameter-efficient adaptation and maintains a 1:4 lyric-to-general-data ratio during pretraining to balance domain specialization with general knowledge preservation.

## Key Results
- **Playlist Understanding:** SongSage achieves HR@1 of 13.69% on user-query rewrite for playlist recommendations, significantly outperforming GPT-4o (11.45%)
- **Lyric Generation:** 22.0% higher preference ratio over GPT-4o in human evaluation for lyric continuation tasks
- **General Knowledge:** Maintains strong performance on MMLU benchmark (63.6%) despite domain specialization, showing effective knowledge preservation

## Why This Works (Mechanism)

### Mechanism 1: Domain-Continual Pretraining with Knowledge Dilution Control
- **Claim:** Targeted continual pretraining on lyric-rich corpora enhances music-domain understanding while partially preserving general capabilities.
- **Mechanism:** The model updates its token distribution priors from general text toward lyrical patterns (emotional language, metaphor, structural repetition). The 1:4 lyric-to-general-data ratio constrains catastrophic forgetting by maintaining exposure to diverse linguistic contexts during each training step.
- **Core assumption:** Lyrical content shares sufficient syntactic overlap with general language that knowledge transfer is bidirectional rather than isolated.
- **Evidence anchors:** MMLU dropped from 74.2% (base) to 64.5% (SongSage-Base), indicating controlled knowledge degradation. Linguistic analysis shows increased nouns (+2.67%), proper nouns (+2.67%), and pronouns (+0.98%) in generated outputs.

### Mechanism 2: Task-Bundled Instruction Tuning for Knowledge Activation
- **Claim:** Multi-task SFT on related lyric/playlist tasks activates latent domain knowledge from pretraining more effectively than single-task tuning.
- **Mechanism:** The 9 SFT tasks span a capability graph: understanding tasks (emotion detection, artist inference) inform generation tasks (lyric continuation, keyword-to-lyrics). Shared representations across tasks create positive transfer, reducing the sample-efficiency threshold for each individual task.
- **Core assumption:** Domain knowledge acquired during pretraining is stored in a retrievable but inactive form; instruction tuning provides the "retrieval cues" to activate it.
- **Evidence anchors:** SongSage excels across seven additional capabilities beyond the primary tasks. Table 2 shows SFT outperforming DPO on some metrics (R@1: 13.69% vs 13.15%), suggesting task-specific tuning provides more direct gains than preference optimization for certain tasks.

### Mechanism 3: Synthetic Preference Alignment with Model-Strength Anchoring
- **Claim:** Using outputs from stronger models as "preferred" targets can shift model behavior toward higher-quality generation patterns without human annotation.
- **Mechanism:** DPO trains the model to increase likelihood of preferred responses while decreasing likelihood of rejected responses. By anchoring "preferred" to GPT-4o/DeepSeek outputs, the model implicitly distills aspects of those models' stylistic and quality patterns.
- **Core assumption:** The stronger model's outputs are consistently preferable; systematic errors in the teacher model will transfer to the student.
- **Evidence anchors:** Preference set construction uses GPT-4o/DeepSeek outputs as preferred targets. DPO improves emotion detection (32.90% → 35.70%) but slightly degrades query rewrite (13.69% → 13.15%), indicating task-dependent effectiveness.

## Foundational Learning

- **Concept: Continual Pretraining vs. Fine-tuning**
  - Why needed here: SongSage uses continual pretraining (training on raw text, not instructions) before SFT. Understanding this distinction is critical for designing the data mix and preventing knowledge overwrite.
  - Quick check question: If you wanted to add support for Spanish lyrics, would you add Spanish lyrics to LyricBank (continual pretraining) or LyricBank-SFT (instruction tuning)? Which would be more sample-efficient?

- **Concept: Retrieval-Based Evaluation (HR@k)**
  - Why needed here: PlaylistSense uses a description pool and embedding similarity to evaluate playlist understanding. This paradigm avoids the subjectivity of direct generation quality assessment.
  - Quick check question: Why might HR@1 be a better metric than BLEU or ROUGE for evaluating playlist descriptions? What failure mode does it capture that n-gram metrics miss?

- **Concept: Linguistic Feature Analysis (POS/Dependency)**
  - Why needed here: The paper uses syntactic analysis to demonstrate that lyric pretraining changes model behavior beyond surface-level metrics. This provides mechanistic evidence of domain adaptation.
  - Quick check question: If you observed that a model fine-tuned on lyrics had *lower* pronoun frequency than the base model, what might you infer about its training data or objective?

## Architecture Onboarding

- **Component map:** Qwen2.5-7B-Base -> LyricBank (5.48B tokens, continual pretraining) -> SongSage-Base -> LyricBank-SFT (775k samples, 9 tasks, LoRA fine-tuning) -> SongSage-Instruct -> Preference Set (9k samples, DPO) -> SongSage-DPO

- **Critical path:** Pretraining data quality → SFT task coverage → Preference data alignment. Errors in early stages compound; a poorly filtered LyricBank will propagate noise through all downstream stages.

- **Design tradeoffs:**
  - **Lyric ratio (1:4):** Higher lyric proportion improves music tasks but accelerates general knowledge loss. The chosen ratio reflects a Pareto optimum for the authors' evaluation set.
  - **LoRA vs. full fine-tuning:** LoRA (rank=32) reduces memory and prevents overfitting but may limit adaptation capacity for highly specialized patterns.
  - **4 languages:** English/Chinese/Korean/Japanese coverage expands utility but dilutes per-language sample count; low-resource language performance may lag.

- **Failure signatures:**
  - **Catastrophic forgetting:** If MMLU drops below 55%, increase general data ratio in LyricBank.
  - **Mode collapse in generation:** If lyric outputs become repetitive across prompts, inspect SFT data diversity and reduce DPO epochs.
  - **Language imbalance:** If Korean/Japanese tasks underperform, verify 1:2 token ratio is maintained after filtering; these languages may require data augmentation.
  - **Playlist retrieval saturation:** If HR@1 plateaus below 15%, the bottleneck may be embedding model quality (bge-small-zh-v1.5), not the LLM.

- **First 3 experiments:**
  1. **Ablate the lyric-to-general ratio:** Train variants with 1:2, 1:4, 1:6 ratios. Plot MMLU vs. MusicTheoryBench accuracy to find your application's optimal tradeoff curve.
  2. **Single-task vs. multi-task SFT:** Train two models—one on all 9 tasks, one only on lyric continuation. Compare on held-out continuation examples to quantify transfer benefit.
  3. **DPO teacher model swap:** Replace GPT-4o/DeepSeek with a domain-specific model (e.g., ChatMusician) as the preference source. Evaluate whether in-domain alignment improves music reasoning metrics.

## Open Questions the Paper Calls Out
None

## Limitations
- **General Knowledge Degradation:** The model demonstrates a 10.7 percentage point drop in MMLU performance (from 74.2% to 64.5%) after lyric-centric pretraining, confirming that domain specialization comes at the cost of general knowledge retention.
- **Data Quality Dependency:** SongSage's performance is fundamentally constrained by LyricBank's composition. The corpus is filtered for copyright concerns and may not represent the full diversity of musical expression.
- **Task-Specific Robustness:** While the model excels in the nine SFT tasks, there is limited evidence of its ability to handle novel or complex music-related queries that fall outside these predefined categories.

## Confidence
- **High Confidence:** The pretraining methodology and architecture choices are technically sound. The documented MMLU drop and linguistic analysis provide concrete evidence for the general knowledge tradeoff.
- **Medium Confidence:** The claimed superiority over GPT-4o and other models relies on specific benchmark conditions. While the metrics show consistent advantages, the preference-based human evaluations for lyric generation are inherently subjective.
- **Low Confidence:** The scalability claims for multilingual support are not thoroughly validated. With only 1:2 token ratios for Korean/Japanese versus 1:1 for English/Chinese, the model may exhibit significant performance disparities across languages.

## Next Checks
1. **Knowledge Preservation Analysis:** Systematically evaluate SongSage on domain-adjacent general knowledge tasks (e.g., literature, history) that share thematic elements with music to quantify the true extent of knowledge degradation versus task-specific specialization.

2. **Cross-Lingual Transfer Study:** Conduct controlled experiments measuring Korean/Japanese performance when models are pretrained with different token ratios (1:1, 1:2, 1:3) to establish optimal resource allocation for low-resource languages.

3. **Out-of-Distribution Query Testing:** Design a comprehensive test suite of music-related questions that intentionally fall outside the nine SFT task categories to assess the model's ability to generalize beyond its training distribution.