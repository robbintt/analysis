---
ver: rpa2
title: 'DEER: A Benchmark for Evaluating Deep Research Agents on Expert Report Generation'
arxiv_id: '2512.17776'
source_url: https://arxiv.org/abs/2512.17776
tags:
- research
- report
- evaluation
- deep
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEER introduces a benchmark for expert-level deep research report
  evaluation using 50 tasks across 13 domains, an expert-grounded taxonomy of 7 dimensions
  and 25 subdimensions operationalized into 101 rubric items, and task-specific Expert
  Evaluation Guidance to improve LLM-based judging. It also proposes a document-level
  fact-checking architecture that verifies both cited and uncited claims and quantifies
  evidence quality.
---

# DEER: A Benchmark for Evaluating Deep Research Agents on Expert Report Generation

## Quick Facts
- arXiv ID: 2512.17776
- Source URL: https://arxiv.org/abs/2512.17776
- Reference count: 40
- 50 tasks across 13 domains evaluated using expert-grounded rubrics and claim-level fact verification

## Executive Summary
DEER introduces a benchmark for expert-level deep research report evaluation, featuring 50 tasks across 13 domains, an expert-grounded taxonomy of 7 dimensions and 25 subdimensions operationalized into 101 rubric items, and task-specific Expert Evaluation Guidance to improve LLM-based judging. The benchmark also proposes a document-level fact-checking architecture that verifies both cited and uncited claims and quantifies evidence quality. Experiments show strong correlation with human expert judgments and reveal that current systems perform well on structure and style but lag on expert-level intent alignment and analytical soundness.

## Method Summary
DEER evaluates deep research agents through a two-stage pipeline: first, an LLM-as-judge (GPT-5.2) scores reports against a fixed 101-item rubric enhanced with task-specific Expert Evaluation Guidance; second, an Information Verification Module (GPT-5-mini) extracts claims, applies semantic back-tracking to recover implicit citations, retrieves context (BM25/OpenAI embeddings, Top-K=2), and verifies claims against sources. The system outputs report quality scores across 5 dimensions (via rubric) and 2 dimensions (via metrics), with human correlation measured via Pearson r, Spearman ρ, and pairwise agreement.

## Key Results
- Strong correlation with human expert judgments (Pearson r=0.75 with Expert Guidance vs 0.62 baseline)
- Current systems perform well on structure and style but lag on expert-level intent alignment and analytical soundness
- Semantic back-tracking successfully recovers implicit citations for uncited claims
- Fixed hierarchical taxonomy enables interpretable cross-task diagnostics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Task-specific Expert Evaluation Guidance improves LLM-as-judge alignment with human expert judgments by surfacing domain-relevant evaluation cues that non-expert evaluators miss.
- Mechanism: The guidance provides concrete, verifiable statements about required content elements for each task. When LLM judges receive this structured expert knowledge alongside granular rubrics, they can identify subtle domain-specific errors that would otherwise go undetected.
- Core assumption: LLM judges possess sufficient reasoning capacity to apply expert guidance when explicitly provided, but lack the domain knowledge to generate such guidance autonomously.
- Evidence anchors:
  - [abstract] "task-specific Expert Evaluation Guidance to support LLM-based judging"
  - [section 6.3] "+Expert Guidance attains the highest reliability across metrics" with Pearson r=0.75 vs 0.62 for granular rubrics alone
  - [corpus] Related work (DeepResearch Bench II, RigorousBench) similarly finds expert-curated rubrics improve evaluation

### Mechanism 2
- Claim: Semantic back-tracking recovers implicit citations for uncited claims by tracing semantic dependencies to earlier cited context, expanding verification coverage beyond explicitly cited sentences.
- Mechanism: For each sentence si, the LLM identifies preceding sentences R(si) that si depends on semantically. The valid citation set V(si) = C(si) ∪ ∪C(sk) for k∈R(si). This allows uncited claims to inherit citations from their evidentiary antecedents.
- Core assumption: Claim-citation relationships in expert reports often span multiple sentences rather than appearing inline, and LLMs can reliably identify these semantic dependencies.
- Evidence anchors:
  - [abstract] "verifies both cited and uncited claims"
  - [section 4.4] "This enables verification by inheriting citations from previously referenced sentences, even when C(si) = ∅"

### Mechanism 3
- Claim: Fixed hierarchical taxonomy with 101 shared rubric items enables interpretable cross-task diagnostics that task-specific generated criteria cannot provide.
- Mechanism: By applying identical rubric items across all 50 tasks, scores become directly comparable at the rubric-item level. This permits aggregation to identify systematic strengths/weaknesses rather than only task-level or coarse dimension scores.
- Core assumption: A single rubric taxonomy can validly capture quality dimensions across 13 diverse domains without sacrificing domain-specific validity.
- Evidence anchors:
  - [section 4.3] "uses a fixed, expert-designed rubric to ensure reliable and interpretable evaluation... enabling diagnosis of the system's overall strengths and weaknesses"
  - [section 6.1] Fine-grained analysis reveals specific weaknesses (e.g., scope sub-dimension particularly low)

## Foundational Learning

- Concept: **LLM-as-a-Judge reliability factors**
  - Why needed here: DEER's evaluation pipeline depends on LLM judges producing scores that correlate with human experts. Understanding what improves vs. degrades this alignment is essential for interpreting results.
  - Quick check question: What three components does DEER add to a vanilla LLM judge, and which one provides the largest correlation gain?

- Concept: **Claim type taxonomy (A–F classification)**
  - Why needed here: The information verification module handles different claim types differently. Only Types A–C undergo external verification; understanding this classification is critical for interpreting Integrity/Sufficiency metrics.
  - Quick check question: A claim appears in sentence 3 of paragraph 2 with no inline citation, but the evidence appears in sentence 1 of the same paragraph. What type is this, and how does back-tracking handle it?

- Concept: **Coverage vs. Quality scoring separation**
  - Why needed here: Rubric items evaluate both whether required content is present (Coverage) and how well it's executed (Quality). These are scored independently, enabling diagnosis of whether systems fail to include content or include it poorly.
  - Quick check question: A report includes all required analysis sections but each contains only surface-level observations. Would this score low on Coverage, Quality, or both?

## Architecture Onboarding

- Component map:
  - **Report Quality Assessment**: LLM-as-judge receives (query, report, Expert Guidance, 101 rubric items) → outputs item-level 1–10 scores → aggregates to sub-dimension → dimension → overall
  - **Information Verification Module**: Claim extraction (batch) → Type classification (A–F) → Back-tracking for implicit citations → Context retrieval (BM25/embedding) → Verification against sources → Integrity/Sufficiency metrics
  - **Score Integration**: Rubric-based dimension scores (5 dims) + metric-based dimension scores (2 dims) → unified 7-dimension profile

- Critical path:
  1. Expert Evaluation Guidance creation (offline, per task) — requires domain expert drafting + cross-review
  2. Claim extraction with type classification — GPT-5-mini with batch size 20 achieves best cost/accuracy tradeoff
  3. Semantic back-tracking for B/C claims — LLM predicts evidence_position; validates against known citations
  4. Context retrieval (Top-K=2 chunks) + verification — strict support criterion; outputs supported/not_supported/error

- Design tradeoffs:
  - **Fixed vs. task-specific rubrics**: Fixed enables cross-task diagnostics but may miss domain-specific criteria. DEER addresses this via Expert Guidance that varies per task while rubric structure remains constant.
  - **Verification strictness vs. coverage**: Strict "supported" criterion reduces false positives but may flag claims as unsupported due to context window limits. Top-K=2 retrieval chosen for cost efficiency; larger K improves accuracy (+2.3%) at +35% cost.
  - **Batch extraction efficiency**: Larger batches reduce cost but may increase "lost-in-middle" errors. Batch size 20 selected as sweet spot.

- Failure signatures:
  - **Low Coverage scores with high Quality scores**: System generates well-written content but omits required elements → check Expert Guidance compliance
  - **High Type F (Unknown Source) claims**: System making factual assertions without any evidentiary support → retrieval or citation behavior issue
  - **Low Information Integrity despite high Citation Support**: Individual citations support claims but sources are unreliable or non-reproducible → check Reference Quality/Reliability metrics

- First 3 experiments:
  1. **Ablate Expert Guidance**: Run evaluation with +Granular Rubrics only (no guidance) on 10 tasks. Compare correlation with human judgments to +Expert Guidance baseline. Expect ~0.1 drop in Pearson r based on Table 2.
  2. **Vary back-tracking window**: Compare LLM semantic back-tracking vs. sliding window (k=5,10,15) on correctly classified B/C claims. Measure Jaccard/Precision/Recall. Expect LLM method to achieve higher precision, sliding window higher recall.
  3. **Stress test claim verification**: Use adversarial dataset (perturbed claims) to measure robustness. Verify that "Not Supported" detection remains >88% accuracy per Table 16. Identify any systematic failure patterns.

## Open Questions the Paper Calls Out

- **Question**: How can deep research systems be improved to maintain reasoning quality when integrating diverse external information?
  - Basis in paper: The paper states that "reasoning models without web search (think) outperform think+search and deep on report-quality metrics excluding information-related scores. This suggests that integrating diverse external information can blur the problem definition and argument structure."
  - Why unresolved: The paper identifies the problem but does not propose solutions for balancing evidence integration with analytical coherence.

- **Question**: What mechanisms can improve deep research systems' performance on Request Fulfillment and Analytical Soundness, where current systems consistently lag?
  - Basis in paper: The paper concludes that "deep research systems can produce structurally plausible reports that cite external evidence, there is room for improvement in fulfilling expert-level user requests and achieving logical completeness."
  - Why unresolved: The paper provides diagnostic signals but does not investigate specific interventions for these dimensions.

- **Question**: How can DEER's evaluation framework be extended to multimodal research tasks?
  - Basis in paper: The Limitations section states: "our current benchmark focuses on text-based reports... laying a solid foundation for future extensions to multimodal research tasks."
  - Why unresolved: The current taxonomy and rubrics are designed for text-only reports; visual and interactive elements require new evaluation criteria.

## Limitations

- The evaluation pipeline's dependence on GPT-5 models represents a significant reproducibility constraint, as these models are not publicly available.
- Semantic back-tracking mechanism's effectiveness relies heavily on the LLM's ability to accurately identify semantic dependencies across long documents, but validation is limited.
- Fixed 101-item rubric may force alignment onto dimensions that don't fully capture domain-specific quality criteria, potentially obscuring failure modes unique to certain fields.

## Confidence

- **High confidence**: The core claim that Expert Evaluation Guidance improves LLM-as-judge reliability (Pearson r=0.75 vs 0.62 baseline) is well-supported by experimental results in Table 2 and aligns with established literature on LLM evaluation improvements.
- **Medium confidence**: The claim that semantic back-tracking expands verification coverage beyond explicitly cited claims is mechanistically sound but lacks comprehensive corpus-level validation against alternative approaches.
- **Medium confidence**: The assertion that fixed rubrics enable interpretable cross-task diagnostics is theoretically valid, though the extent to which this actually reveals systematic patterns across 13 domains requires further validation.

## Next Checks

1. **Ground truth verification study**: Conduct a small-scale human expert validation comparing LLM judge scores (with and without Expert Guidance) against independent expert ratings on 10 tasks to confirm the reported correlation improvements and identify any systematic biases.

2. **Back-tracking robustness test**: Evaluate semantic back-tracking performance across document types with varying citation patterns (e.g., footnotes, deferred citations, interdisciplinary papers) to identify structural limitations not apparent in the primary benchmark.

3. **Rubric domain coverage audit**: Have domain experts review the 101 rubric items for each of the 13 domains to assess whether any critical quality dimensions are systematically omitted due to the fixed rubric structure, particularly in highly specialized fields like theoretical physics or comparative literature.