---
ver: rpa2
title: 'Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval
  in AR'
arxiv_id: '2512.00294'
source_url: https://arxiv.org/abs/2512.00294
tags:
- scene
- spatial
- object
- agent
- objects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of enabling AR headsets to interpret
  complex, open-vocabulary natural language queries about real-world objects and their
  spatial relationships. The core method idea involves integrating multimodal large
  language models (MLLMs) with grounded vision models to create a task-adaptive AR
  agent that performs language-conditioned spatial retrieval and relational reasoning.
---

# Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR

## Quick Facts
- **arXiv ID**: 2512.00294
- **Source URL**: https://arxiv.org/abs/2512.00294
- **Reference count**: 11
- **Primary result**: Achieves centimeter-level 3D localization (mean error 5.4 cm, Success@10 cm 88.7%) and 87-95% end-to-end query success across various task categories.

## Executive Summary
This paper presents an AR agent that interprets open-vocabulary natural language queries to perform spatial retrieval and relational reasoning over real-world objects. The agent integrates multimodal large language models with grounded vision models, using depth-based 2D-to-3D grounding to achieve meter-accurate 3D anchors and dynamic AR scene graphs encoding spatial, structural-semantic, and causal-functional relationships. The system supports complex queries ranging from simple object identification to multi-object relational reasoning while maintaining interactive latency.

## Method Summary
The method couples open-vocabulary semantics with depth-based 2D-to-3D grounding through a four-layer agentic architecture. The pipeline processes voice input through ASR to a dialogue agent that parses intent, then conditionally invokes perception tools (MLLM label proposer, Grounding DINO detector, raycast depth sampling) to create 3D anchors. A hybrid geometric-semantic scene graph builder fuses algorithmically computed spatial predicates with MLLM-proposed semantic relations via weighted sigmoid fusion. A task-adaptive controller selects appropriate tools based on query complexity, maintaining world-model sufficiency checks to skip unnecessary computation. The system runs on Meta Quest 3 with Unity client, REST backend for MLLM/detection, and on-device 3D computation.

## Key Results
- Achieves centimeter-level 3D localization (mean error 5.4 cm, Success@10cm 88.7%)
- Improves relation grounding with edge F1 of 0.79 vs. 0.63 (detector-only geometric) and 0.60 (2D VLM semantic-only)
- Increases end-to-end query success across task categories to 87-95% compared to 54-79% for 2D-centric VLM baseline
- Maintains median end-to-end latency of 4.74s through task-adaptive orchestration vs. 6.59s for always-running pipeline

## Why This Works (Mechanism)

### Mechanism 1: Depth-Based 2D-to-3D Grounding via Raycast Aggregation
Multi-point depth sampling within 2D detection boxes yields centimeter-accurate 3D anchors without dense mesh reconstruction. For each detected object, sample multiple pixels (center, corners, diagonals), construct rays using camera intrinsics and head pose, intersect with depth map, and aggregate valid hits via median filtering to reject outliers. This produces a robust 3D anchor and bounding volume per object.

### Mechanism 2: Hybrid Geometric-Semantic Relation Inference
Fusing algorithmically computed spatial predicates with MLLM-proposed semantic relations yields higher scene graph fidelity than either modality alone. Compute geometric scores for spatial relations from anchor distances and support plane detection, separately prompt MLLM for semantic/functional relations, then fuse via weighted sigmoid and select best relation per pair above threshold.

### Mechanism 3: Task-Adaptive Orchestration
Query-conditioned tool selection reduces median latency while preserving task success by avoiding unnecessary computation. Dialogue agent parses utterance to infer task type (Locate/Relate/Measure/Navigate), skips relation reasoning for simple identification, invokes full perception chain for relational queries, and maintains world-model sufficiency check to avoid redundant perception.

## Foundational Learning

- **Open-Vocabulary Detection (OVD)**
  - Why needed here: Grounding DINO converts MLLM-proposed labels into 2D bounding boxes without fixed class constraints. Understanding confidence thresholds and prompt engineering for detectors is prerequisite to reliable 2D-to-3D lifting.
  - Quick check question: Given a label "screwdriver" with detector confidence 0.32 (below threshold 0.35), should the anchor be created? What alternatives exist?

- **Scene Graph Representation**
  - Why needed here: The world model stores objects and relations as typed edges (o_i, o_j, r_ij, γ_ij). Understanding directed vs. bidirectional relations, confidence scoring, and graph query semantics is essential for extending the relation taxonomy.
  - Quick check question: If "mug on desk" has γ=0.92 and "mug used-for coffee" has γ=0.71, which relation should be visualized when the user asks "What's on my desk?"

- **Depth API Characteristics on Quest 3**
  - Why needed here: Environment depth has known artifacts—holes on transparent surfaces, temporal noise, resolution limits. Raycast failure modes directly affect anchor stability.
  - Quick check question: A depth ray returns "invalid" for a glass object. What fallback strategies does the system support per Table II?

## Architecture Onboarding

- **Component map:**
  Sensing Layer (on-device Quest 3): Passthrough RGB + environment depth + head pose → Frame Capture tool
  Tool Layer (stateless): Voice-to-Action Parser, Open-Vocabulary Label Proposer (MLLM), 2D Detector/Grounder, Raycast & 3D Lifting, Scene Graph Builder, Geo-anchoring tools
  World Model Layer: Local scene graph (objects + spatial relations), geo graph (session anchors + landmarks), memory state (session + cross-session)
  Orchestration Layer: Supervisor Agent + four sub-agents (Dialogue, Perception, Navigation, Memory)

- **Critical path:** Voice input → ASR → Dialogue agent parses intent → (if perception needed) Frame Capture → Label Proposer → 2D Detector → Raycast & 3D Lifting → Scene Graph Builder updates world model → Feedback policy selects output modality → Visual Overlay renders boxes/ROIs/edges. Latency dominated by MLLM (~2.1s) + detection (~0.82s).

- **Design tradeoffs:**
  Depth-based vs. scene-mesh raycasting: Depth gives better local accuracy for small objects; mesh provides temporal stability and global consistency. System uses depth as primary, mesh as fallback.
  Single-frame vs. multi-frame grounding: Current evaluation uses single frames; live deployment needs temporal fusion for identity persistence (noted as future work).
  On-device vs. remote models: MLLM and detection run on edge server; all 3D computation and rendering on-device. Network latency (~0.42s) is non-trivial.

- **Failure signatures:**
  3D error spikes on cluttered scenes (industrial cluttered 7.5 cm error vs. tidy 5.0 cm) → indicates depth occlusion handling issues
  Relation F1 drops for structural/functional vs. spatial types (Spat. F1=0.86, Str/Fnc F1=0.71) → indicates MLLM semantic inference is weaker than geometric
  End-to-end latency >6s for no-coordinator variant → indicates unnecessary tool invocation without task-adaptive gating

- **First 3 experiments:**
  1. Depth raycast validation: Capture synchronized RGB+depth frames for 20 objects across material types. Measure anchor error vs. ground-truth markers. Identify depth failure modes and validate multi-point sampling parameters.
  2. Relation fusion ablation: Run Algorithm 1 with α ∈ {0.0, 0.3, 0.5, 0.7, 1.0} on GroundedAR-Bench scenes. Plot edge F1 vs. α to find optimal fusion weight; confirm hybrid outperforms pure geometric or pure semantic.
  3. Task-adaptive latency measurement: Instrument full pipeline with per-tool timing. Run 50 queries across 4 categories. Verify simple identification skips relation reasoning; confirm median latency difference between full agent and no-coordinator matches Table IX patterns.

## Open Questions the Paper Calls Out

### Open Question 1
Can temporal fusion with explicit uncertainty tracking maintain stable instance identity under head motion, intermittent depth holes, and partial occlusion in live AR sessions? Current system operates on single-frame depth raycasting; temporal coherence and object re-identification when depth fails or objects move are unaddressed.

### Open Question 2
How do task success rates and system latency translate into user experience outcomes such as perceived trust, cognitive load, and error recovery behaviors? No user studies were conducted; current evaluation uses annotator judgments rather than real-user interaction data.

### Open Question 3
What fallback mechanisms can maintain metric grounding accuracy on reflective, transparent, and thin objects where commodity depth APIs fail? Current pipeline assumes reliable depth; specific failure modes on difficult materials are characterized but not mitigated.

### Open Question 4
Can compact, on-device or edge-deployed VLM and OVD models achieve sub-second end-to-end latency while preserving the 87–95% task success rates observed with larger backend models? Current system relies on remote backend with ~2.1s MLLM latency; no evaluation of smaller models on-device.

## Limitations
- Depth API failure on transparent/reflective/thin objects without fallback mechanisms
- Single-frame grounding without temporal fusion for identity tracking under motion
- No user studies measuring perceived trust, cognitive load, or error recovery behaviors
- Dependency on remote backend models limiting real-time deployment

## Confidence
- **MLLM Dependency**: Medium - Performance tightly coupled to undisclosed model and prompts
- **Environment Depth Assumptions**: Medium - Assumes high-quality depth without quantifying sensor noise characteristics
- **Annotation Quality**: Medium - Relies on human-annotated ground-truth without reporting inter-annotator agreement

## Next Checks
1. **Depth Sensor Validation**: Instrument system to log per-ray depth validity across 50+ real-world scenes; compare failure rates on matte vs. reflective vs. transparent objects to quantify depth sensor impact on 3D anchor accuracy.
2. **MLLM Ablation**: Replace undisclosed MLLM with Qwen-VL-7B using standardized prompts; rerun relation inference on 100 GroundedAR-Bench queries and compare edge F1 and relational query success to reported values.
3. **Cross-Modality Fusion Sensitivity**: Systematically vary fusion weight α in Algorithm 1 (α ∈ {0.0, 0.3, 0.5, 0.7, 1.0}) on held-out test set; plot edge F1 and relational query success vs. α to confirm hybrid performance robustness.