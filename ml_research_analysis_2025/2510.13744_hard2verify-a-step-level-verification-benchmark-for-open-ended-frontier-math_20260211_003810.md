---
ver: rpa2
title: 'Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math'
arxiv_id: '2510.13744'
source_url: https://arxiv.org/abs/2510.13744
tags:
- step
- arxiv
- preprint
- verifiers
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Hard2Verify introduces a step-level verification benchmark for
  open-ended frontier math problems, addressing the need for rigorous evaluation of
  mathematical reasoning in large language models. The benchmark is constructed by
  curating challenging problems from recent international mathematics competitions,
  generating responses from top-tier LLMs, and employing PhD-level math experts to
  annotate each step.
---

# Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math

## Quick Facts
- arXiv ID: 2510.13744
- Source URL: https://arxiv.org/abs/2510.13744
- Reference count: 23
- Introduces step-level verification benchmark for open-ended frontier math problems

## Executive Summary
Hard2Verify addresses the critical need for rigorous evaluation of mathematical reasoning in large language models by introducing a step-level verification benchmark. The benchmark is constructed by curating challenging problems from recent international mathematics competitions, generating responses from top-tier LLMs, and employing PhD-level math experts to annotate each step. The resulting dataset comprises 1,860 rigorously graded steps across 200 model responses, enabling detailed evaluation of verifier performance at the step level.

The benchmark evaluates three key tasks: step-level correctness, response-level correctness, and first error identification. Results demonstrate that current open-source verifiers significantly lag behind closed-source models, with weaker verifiers unable to identify mistakes, marking nearly every step as correct. Analysis reveals that scaling verifier inference-time compute sequentially improves performance more effectively than parallel approaches. Additionally, the study finds that verification is generally easier than generation, offering optimism for future verification work.

## Method Summary
The Hard2Verify benchmark is constructed through a three-phase process: first, challenging problems are curated from recent international mathematics competitions; second, responses are generated using top-tier LLMs; and third, PhD-level math experts annotate each step for correctness. The resulting dataset contains 1,860 rigorously graded steps across 200 model responses. The evaluation framework includes three tasks: step-level correctness (binary classification of individual steps), response-level correctness (determining if entire solutions are correct), and first error identification (locating the first incorrect step in a solution). This multi-faceted approach enables comprehensive assessment of verifier capabilities at different granularities.

## Key Results
- Open-source verifiers lag significantly behind closed-source models in mathematical verification tasks
- Sequential inference-time compute scaling improves verifier performance more effectively than parallel scaling approaches
- Verification is generally easier than generation, suggesting optimism for future verifier development
- Weaker verifiers tend toward marking all steps as correct, failing to identify mistakes (near-zero TNR)

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its rigorous construction process and multi-level evaluation framework. By using PhD-level annotators and competition-level problems, it ensures high-quality ground truth labels. The step-level granularity enables precise error detection and isolation of failure modes, while the three evaluation tasks provide comprehensive coverage of verifier capabilities from fine-grained step correctness to holistic response assessment.

## Foundational Learning
- **Step-level annotation**: Breaking mathematical solutions into individual steps for detailed evaluation
  - Why needed: Enables precise error detection and understanding of where reasoning breaks down
  - Quick check: Verify that step boundaries are clearly defined and consistently applied across annotations
- **Sequential vs parallel scaling**: Different approaches to increasing inference compute for verification
  - Why needed: Determines how to most effectively improve verifier performance
  - Quick check: Compare performance improvements when scaling compute sequentially vs in parallel
- **TPR/TNR tradeoff**: The relationship between true positive rate and true negative rate in verification
  - Why needed: Helps understand verifier calibration and error detection capabilities
  - Quick check: Plot TPR vs TNR curves for verifiers of different capability levels

## Architecture Onboarding
**Component Map**: Competition Problems -> LLM Generation -> Step Annotation -> Verifier Evaluation -> Performance Analysis

**Critical Path**: The most critical path is the annotation pipeline - from raw LLM responses through step-level annotation to create ground truth labels. This directly determines the quality of the benchmark and any downstream evaluations.

**Design Tradeoffs**: The choice of competition problems provides high-quality, challenging content but may limit generalizability to broader mathematical domains. The step-level granularity enables precise error detection but requires extensive annotation effort.

**Failure Signatures**: Weaker verifiers fail by marking all steps as correct (near-zero TNR), while stronger verifiers may still struggle with novel problem types or complex multi-step reasoning.

**First Experiments**:
1. Test verifier performance on a small subset of problems to establish baseline capabilities
2. Compare sequential vs parallel scaling on a few representative problems
3. Evaluate inter-annotator agreement on step-level correctness to validate annotation quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the finding that sequential inference-time compute scaling substantially improves verifier performance, while parallel scaling provides minimal gains, generalize to verification tasks beyond open-ended mathematics?
- Basis in paper: [explicit] Section 5.1 explicitly experiments with both scaling approaches, finding that Best-of-N "does not meaningfully improve over sampling 1 response as N increases" while sequential reasoning levels yield 12-20 point F1 gains. The authors hypothesize this is because "step-level verification is inherently a sequential task."
- Why unresolved: The experiments are confined to mathematical verification; the sequential/parallel dichotomy was not tested on other reasoning domains like code, scientific reasoning, or logical deduction.
- What evidence would resolve it: Replicate the sequential vs. parallel scaling experiments on verification benchmarks in non-mathematical domains (e.g., code correctness, legal reasoning, scientific hypothesis evaluation).

### Open Question 2
- Question: What mechanisms cause weaker verifiers to systematically fail to identify mistakes, tending toward near-zero TNR while approaching perfect TPR?
- Basis in paper: [explicit] Section 4.3 and Figure 4 explicitly document that "weaker verifiers cannot catch errors," with TNR dropping toward 0 and TPR rising toward 1 as verifier capability decreases.
- Why unresolved: The paper characterizes the phenomenon but does not diagnose the underlying cause—is it insufficient mathematical knowledge, calibration failure, training data distribution mismatch, or architectural limitations?
- What evidence would resolve it: Probing studies on weak verifiers to identify failure modes; training interventions that specifically target error-detection calibration; analysis of whether the problem persists when verifiers are trained on more error-rich data.

### Open Question 3
- Question: Is strong mathematical generation ability a necessary prerequisite for effective step-level verification, or can specialized verifier training overcome this dependency?
- Basis in paper: [inferred] Figure 4 and the surrounding analysis note that "the order of models from left to right approximately correlates with mathematical generation ability" and suggest "a baseline level of solving ability is a necessary prerequisite for verification." This relationship is observed but not causally established.
- Why unresolved: The correlation is noted, but whether generation ability causes verification ability—or whether both depend on a third factor—is not tested experimentally.
- What evidence would resolve it: Training smaller models specifically as verifiers using high-quality verification data and comparing against stronger generators; ablation studies isolating generation capability from verification training.

## Limitations
- Potential for subjective interpretation in step-level correctness judgments despite PhD-level annotation
- Dataset size (1,860 steps across 200 responses) may be insufficient to capture full diversity of mathematical reasoning patterns
- Focus on international competition problems may introduce bias toward certain mathematical domains

## Confidence
- **High confidence**: Open-source verifiers lag behind closed-source models; verification is easier than generation
- **Medium confidence**: Sequential inference-time compute scaling is more effective than parallel scaling
- **Medium confidence**: Strong mathematical generation ability correlates with verification capability

## Next Checks
1. Expand the dataset with additional problems from diverse mathematical domains to better assess generalizability
2. Conduct ablation studies on the annotation process to quantify inter-annotator agreement and identify potential sources of subjectivity
3. Test the benchmark on a wider range of mathematical reasoning tasks beyond competition problems to validate its applicability to real-world mathematical problem-solving scenarios