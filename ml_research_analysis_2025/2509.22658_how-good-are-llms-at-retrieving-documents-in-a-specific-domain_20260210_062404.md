---
ver: rpa2
title: How good are LLMs at Retrieving Documents in a Specific Domain?
arxiv_id: '2509.22658'
source_url: https://arxiv.org/abs/2509.22658
tags:
- search
- knowledge
- queries
- base
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the effectiveness of large language models
  (LLMs) for document retrieval in the environmental and earth science domain. Traditional
  keyword-based search systems like Elasticsearch often fail to capture multi-intent
  queries, limiting their precision and recall.
---

# How good are LLMs at Retrieving Documents in a Specific Domain?
## Quick Facts
- arXiv ID: 2509.22658
- Source URL: https://arxiv.org/abs/2509.22658
- Reference count: 33
- Primary result: LLM-based RAG (EnvKB) outperforms Elasticsearch on multi-intent queries (Hits@10: 0.96 vs. 0.37; BERTScore: 0.92 vs. 0.45) while maintaining strong single-intent performance.

## Executive Summary
This study evaluates large language models (LLMs) for document retrieval in the environmental and earth science domain, where traditional keyword-based systems like Elasticsearch often struggle with multi-intent queries. The authors developed EnvKB, a Retrieval-Augmented Generation framework that fine-tunes LLaMa 3.1 on domain-specific text to improve semantic understanding and retrieval accuracy. Using a curated evaluation dataset of 1,500 queries, they demonstrate that LLM-based retrieval significantly outperforms Elasticsearch on mixed-intent queries while maintaining strong performance on single-intent queries, with fine-tuning further improving results.

## Method Summary
The study employs a Retrieval-Augmented Generation (RAG) framework using a fine-tuned LLaMa 3.1 (3.1B parameters) model for document retrieval in environmental science. The corpus consists of 1,000 URL endpoints from ENVRI-FAIR, with text extracted via Crawl4ai. A 1,500-query evaluation set was curated (1,000 single-intent, 500 multi-intent) from endpoint metadata. The LLM was fine-tuned using LoRA with sequence-to-sequence training for 5 epochs (LR 2e-5, batch size 2). Text was chunked to 250 tokens, and embeddings were generated by the fine-tuned LLM for vector database storage. Retrieval used cosine similarity between query and chunk embeddings, with performance measured by Hits@10 and BERTScore.

## Key Results
- LLM-based retrieval (EnvKB) achieved Hits@10 of 0.96 on mixed-intent queries versus 0.37 for Elasticsearch
- BERTScore for EnvKB reached 0.92 compared to 0.45 for Elasticsearch on multi-intent queries
- Fine-tuned models showed improved performance over base LLMs, validating domain-specific training

## Why This Works (Mechanism)
The fine-tuned LLM-based RAG framework improves document retrieval by leveraging semantic understanding rather than keyword matching. The model's ability to capture contextual relationships between terms allows it to handle complex, multi-intent queries that combine multiple search criteria. Domain-specific fine-tuning further enhances this capability by adapting the model to the specialized vocabulary and concepts of environmental science.

## Foundational Learning
- **Cosine similarity**: Measures the angle between vector representations to determine semantic similarity between query and document embeddings
  - *Why needed*: Core mechanism for ranking retrieved documents based on semantic relevance
  - *Quick check*: Verify cosine similarity scores fall between -1 and 1, with higher values indicating better matches

- **RAG (Retrieval-Augmented Generation)**: Framework combining information retrieval with language model generation to provide context-aware responses
  - *Why needed*: Enables the system to retrieve relevant documents before generating answers
  - *Quick check*: Confirm retrieval occurs before generation in the pipeline

- **LoRA (Low-Rank Adaptation)**: Parameter-efficient fine-tuning technique that reduces computational cost by decomposing weight updates
  - *Why needed*: Enables efficient fine-tuning of large models on domain-specific data
  - *Quick check*: Verify that only adapter weights are updated during training

- **BERTScore**: Metric using contextualized embeddings to evaluate semantic similarity between generated and reference text
  - *Why needed*: Provides more meaningful evaluation than exact string matching for retrieval tasks
  - *Quick check*: Confirm BERTScore values range from 0 to 1, with higher values indicating better semantic alignment

- **Vector database**: Specialized database optimized for storing and querying high-dimensional vector embeddings
  - *Why needed*: Enables efficient similarity search across large collections of document embeddings
  - *Quick check*: Verify that embeddings are properly indexed and retrievable by similarity queries

- **Chunking strategy**: Process of dividing long documents into smaller, manageable segments for embedding and retrieval
  - *Why needed*: Balances context preservation with computational efficiency for embedding generation
  - *Quick check*: Confirm chunks maintain semantic coherence while fitting within model context limits

## Architecture Onboarding
- **Component map**: Crawl4ai -> Text extraction -> Chunking (250 tokens) -> Embedding generation (fine-tuned LLM) -> Vector DB indexing -> Cosine similarity retrieval (Top-10) -> Evaluation (Hits@10, BERTScore)
- **Critical path**: Query embedding generation → Cosine similarity search → Top-10 document retrieval → Performance evaluation
- **Design tradeoffs**: Semantic understanding vs. computational cost; fine-tuning for domain specificity vs. generalization; chunk size optimization for context preservation vs. retrieval efficiency
- **Failure signatures**: Poor multi-intent retrieval indicates inadequate semantic understanding; low Hits@10 suggests embedding quality issues; high computational costs point to inefficient chunking or embedding strategies
- **First experiments**: 1) Compare retrieval performance with different embedding extraction methods (CLS vs. mean pooling); 2) Evaluate impact of chunk size variations on retrieval accuracy; 3) Test base LLM vs. fine-tuned model performance on domain-specific queries

## Open Questions the Paper Calls Out
- Can the EnvKB system maintain retrieval performance and efficiency when scaled to handle continually updated data streams across diverse domains beyond environmental science? The current study focused specifically on environmental science with a static dataset, leaving scalability and domain transferability unproven.
- How robust is the EnvKB system when evaluated against real-world user queries that deviate from the structure of the synthetic evaluation dataset? The evaluation used queries generated from metadata fields that may not reflect actual researcher inquiries.
- How does the fine-tuned LLM approach compare to other state-of-the-art neural information retrieval methods or dense retrievers outside of the traditional Elasticsearch baseline? The study compared primarily against Elasticsearch without benchmarking against other modern dense retrieval architectures.

## Limitations
- Embedding extraction method remains unspecified, which could significantly impact retrieval quality since different pooling strategies yield different semantic representations
- Training data format is unclear - uncertain whether the model was trained on raw domain text or specifically formatted instruction-response pairs
- Without exact Elasticsearch baseline configuration, direct performance comparison may be challenging
- Evaluation relies on simulated multi-intent queries constructed from metadata, potentially not reflecting real-world query patterns

## Confidence
- **High confidence**: General RAG framework effectiveness and superiority of LLM-based retrieval over keyword-based systems for multi-intent queries
- **Medium confidence**: Specific performance metrics (Hits@10, BERTScore values) due to potential variations in embedding extraction and evaluation setup
- **Low confidence**: Exact fine-tuning procedure and its contribution to performance gains without clarification on training data format and objective

## Next Checks
1. Test multiple embedding extraction strategies (CLS pooling, mean pooling, max pooling) to determine which yields optimal retrieval performance
2. Verify the evaluation dataset construction by attempting to recreate the multi-intent query synthesis process from the described metadata sampling method
3. Implement both the fine-tuned and non-fine-tuned versions of the retrieval system to quantify the impact of domain-specific training on retrieval quality