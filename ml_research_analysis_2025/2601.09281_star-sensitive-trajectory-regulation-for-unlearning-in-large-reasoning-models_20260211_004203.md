---
ver: rpa2
title: 'STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models'
arxiv_id: '2601.09281'
source_url: https://arxiv.org/abs/2601.09281
tags:
- unlearning
- reasoning
- arxiv
- sensitive
- privacy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of privacy-preserving unlearning
  in Large Reasoning Models (LRMs), where sensitive information embedded throughout
  multi-step Chain-of-Thought (CoT) reasoning cannot be adequately removed by existing
  answer-level unlearning methods. To overcome this, the authors propose Sensitive
  Trajectory Regulation (STaR), an inference-time, parameter-free framework that dynamically
  suppresses sensitive content at every step of the reasoning process.
---

# STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models

## Quick Facts
- arXiv ID: 2601.09281
- Source URL: https://arxiv.org/abs/2601.09281
- Reference count: 10
- Primary result: Inference-time unlearning framework that outperforms answer-level methods on privacy preservation in reasoning models

## Executive Summary
This paper addresses the challenge of privacy-preserving unlearning in Large Reasoning Models (LRMs), where sensitive information embedded throughout multi-step Chain-of-Thought (CoT) reasoning cannot be adequately removed by existing answer-level unlearning methods. To overcome this, the authors propose Sensitive Trajectory Regulation (STaR), an inference-time, parameter-free framework that dynamically suppresses sensitive content at every step of the reasoning process. STaR employs semantic-aware detection, secure prompt prefixing, trajectory-aware suppression, and token-level adaptive filtering to ensure comprehensive, decoding-agnostic unlearning. Evaluated on the R-TOFU benchmark, STaR significantly outperforms state-of-the-art baselines in both answer-level and CoT-level forgetting, achieving higher unlearning efficacy (AFE, CFE) and robustness (MCS) while reducing membership inference risks (MIA-A, MIA-C).

## Method Summary
STaR is an inference-time, parameter-free framework that prevents sensitive information leakage in LRMs through four integrated modules. First, sensitive content identification uses a scope classifier to detect potential privacy risks, then semantic retrieval finds the most similar forgotten instances to extract forbidden phrases. Second, secure prompt prefixing prepends safety constraints to guide generation from the start. Third, token-level adaptive filtering suppresses sensitive tokens during decoding through soft suppression (reducing logits by similarity-weighted penalties) and hard suppression (assigning negative infinity to forbidden spans). Fourth, trajectory-aware suppression validates entire reasoning paths for sensitivity and fluency, backtracking and re-decoding when intermediate steps leak sensitive content. The framework operates without fine-tuning, making it computationally efficient compared to retraining-based approaches.

## Key Results
- STaR achieves higher unlearning efficacy (AFE, CFE) and robustness (MCS) than state-of-the-art baselines on R-TOFU benchmark
- Demonstrates over an order-of-magnitude improvement in computational efficiency compared to retraining-based methods (0.5h vs 6-9h)
- Effectively reduces membership inference risks (MIA-A, MIA-C) while maintaining model utility
- Shows robust performance across multiple decoding strategies (DefaultThink, ZeroThink, LessThink)

## Why This Works (Mechanism)

### Mechanism 1: Token-level Adaptive Filtering
STaR suppresses sensitive tokens at the logit level before softmax, enabling context-aware blocking of both exact and paraphrased sensitive content. During decoding, candidate token logits are adjusted through soft suppression (subtracting penalties proportional to semantic similarity) and hard suppression (assigning negative infinity to tokens completing forbidden spans). This operates before probability distribution calculation, directly affecting generation. The mechanism assumes semantic similarity in embedding space captures paraphrased sensitive content, though this may fail against adversarial paraphrases that evade similarity detection.

### Mechanism 2: Trajectory-Aware Suppression Learning
A high-level controller validates entire reasoning trajectories by computing risk scores (combining classifier output and fragment similarity) and fluency scores. If sensitivity exceeds thresholds, tokens are backtracked and added to forbidden sets; if fluency falls below thresholds, refusal templates replace outputs. This mechanism assumes sensitivity and fluency are separable and optimizable together, though they may be negatively correlated, potentially creating unstable behavior where suppressing sensitivity produces incoherent text.

### Mechanism 3: Secure Prompt Prefix with Semantic Detection
Global safety constraints are injected at the prompt level through a scope classifier that computes forget-set probability. When triggered, semantic retrieval finds the most similar forgotten instance, extracts forbidden phrases, and prepends a safety constraint to the input. This provides a model-agnostic "soft guide" that shapes attention context before generation. The mechanism assumes prepended instructions influence generation distribution without requiring gradient updates, though this may fail if the model ignores instructions or the scope classifier has high false-negative rates.

## Foundational Learning

- **Concept: Logit manipulation in autoregressive decoding**
  - Why needed here: STaR operates at the logit level before softmax; understanding how logits map to token probabilities is essential for grasping why negative infinity assignment guarantees non-selection
  - Quick check question: Given logits [2.0, 1.0, -∞] for tokens [A, B, C], what are their probabilities after softmax?

- **Concept: Chain-of-Thought (CoT) reasoning in LRMs**
  - Why needed here: The entire problem STaR addresses—sensitive information embedded in intermediate reasoning steps—presupposes understanding how LRMs generate multi-step traces
  - Quick check question: How does a CoT trajectory differ structurally from a single-pass answer, and where can sensitive information persist?

- **Concept: Membership Inference Attacks (MIA)**
  - Why needed here: STaR's evaluation includes MIA-A and MIA-C metrics; understanding what MIAs reveal about residual memorization is critical for interpreting privacy claims
  - Quick check question: If an MIA classifier achieves AUC = 0.5, what does this imply about the model's information leakage?

## Architecture Onboarding

- **Component map:**
  Input Query -> [Scope Classifier] -> (pf > τ?) -> No -> Normal Generation
  Yes -> [Semantic Retrieval] -> Extract Forbidden Tokens (Tf) -> [Secure Prompt Prefix] -> Prepend Safety Constraint (s∥x) -> [Token-level Adaptive Filtering] -> Adjusted Logits (ℓ̃t) -> [Trajectory-Aware Suppression] -> Safe Output OR Refusal Template

- **Critical path:** The feedback loop between Token-level Filtering and Trajectory-Aware Suppression is the core novelty. If this loop fails (no backtracking, no escalation), STaR degrades to static filtering.

- **Design tradeoffs:**
  - Inference-time vs. fine-tuning: STaR adds ~0.5h runtime vs. 6-9h for baselines, but requires maintaining forbidden token sets and classifiers at inference
  - Hard vs. soft suppression: Hard suppression guarantees exact blocking but can produce jarring outputs; soft suppression preserves fluency but may leak under adversarial probing
  - Decoding-agnostic coverage: Supporting DefaultThink, ZeroThink, and LessThink increases robustness but multiplies evaluation complexity

- **Failure signatures:**
  - High MIA-C with low MIA-A: Sensitive content persists in reasoning traces but not answers (baseline failure mode)
  - High AFE but low MCS: Unlearning succeeds under default decoding but fails under ZeroThink/LessThink
  - Fluency degradation: Overly aggressive suppression produces grammatically broken outputs (mitigated by fluency score threshold η)

- **First 3 experiments:**
  1. Reproduce core metrics on R-TOFU forget01: Run STaR on DeepSeekR1-Distill-Llama-8B; verify AFE > 0.85 and MCS > 0.65
  2. Ablate token-level filtering modes: Compare hard-only, soft-only, and combined filtering; expect hard-only to maximize AFE but degrade fluency
  3. Stress-test with adversarial decoding: Run ZeroThink and LessThink on forget set queries; verify MCS remains > 0.65

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does STaR's inference-time latency scale with reasoning chain length, and what is the practical throughput impact in production deployments?
- Basis: The paper demonstrates computational efficiency versus retraining-based methods but does not quantify per-query latency overhead from running four sequential modules during generation
- Why unresolved: Production systems have strict latency requirements; cumulative overhead of semantic detection, fluency scoring, and iterative re-decoding could become prohibitive for long CoT trajectories
- What evidence would resolve it: Detailed latency profiling across varying CoT lengths, throughput benchmarks under concurrent request loads, and comparison to baseline inference without STaR

### Open Question 2
- Question: Can STaR maintain unlearning efficacy in multi-turn conversational scenarios where sensitive information is incrementally elicited across dialogue turns?
- Basis: All experiments evaluate single-query unlearning; methodology doesn't address whether trajectory-aware suppression generalizes when sensitive queries distribute across multiple conversational turns
- Why unresolved: The secure prompt prefix and token-level filtering operate on individual queries; multi-turn contexts may allow indirect extraction through cumulative prompting strategies
- What evidence would resolve it: Multi-turn dialogue evaluation protocol with distributed sensitive information across 3-10 turns, measuring both per-turn and cumulative leakage

### Open Question 3
- Question: Does STaR's semantic-aware detection remain robust against adversarial perturbations specifically designed to evade embedding-based similarity detection?
- Basis: Table 3 shows robustness to paraphrased prompts, but adversarial attacks could target the embedding space directly with synonym substitutions or crafted queries
- Why unresolved: Detection relies on cosine similarity thresholds and trained scope classifier; both are potentially vulnerable to gradient-based or optimization-driven evasion techniques not evaluated
- What evidence would resolve it: Evaluation against adaptive adversarial attacks optimizing query formulations to minimize detection scores while preserving sensitive information retrieval

## Limitations
- Sensitivity generalization may fail against adversarial paraphrases that evade embedding-based similarity detection
- Trajectory stability assumptions lack theoretical convergence guarantees for backtracking mechanisms
- Fluency-sensitivity tradeoff calibration is heuristic and may produce incoherent outputs when aggressively suppressing sensitive content
- Scope classifier generalization to out-of-distribution sensitive queries is not validated

## Confidence
- **High Confidence:** Token-level adaptive filtering mechanism (logit manipulation before softmax) is well-grounded in autoregressive generation theory
- **Medium Confidence:** Trajectory-aware suppression learning approach has theoretical merit but depends heavily on empirical threshold calibration
- **Low Confidence:** Scope classifier generalization and semantic retrieval coverage assumptions are weakly supported

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary α (0.1 to 2.0), δ (0.6 to 0.95), and η (0.3 to 0.9) across multiple sensitive content types to identify trade-off surfaces and degradation points
2. **Adversarial Paraphrase Testing:** Construct controlled perturbations maintaining semantic meaning while evading cosine similarity detection to test soft suppression effectiveness
3. **Scope Classifier Robustness:** Test classifier on out-of-distribution sensitive queries (different domains, writing styles, cultural contexts) to measure false-negative rates and implement fallback mechanisms