---
ver: rpa2
title: 'SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale
  Resolution'
arxiv_id: '2510.25178'
source_url: https://arxiv.org/abs/2510.25178
tags:
- sfms-alr
- language
- speech
- multilingual
- synthesis
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SFMS-ALR addresses the challenge of fluent intra-sentential code-switching
  in text-to-speech synthesis, where abrupt language shifts degrade intelligibility
  and naturalness. The core method segments text by Unicode script, applies adaptive
  language identification, and uses sentiment-aware prosody control to maintain expressive
  continuity across languages.
---

# SFMS-ALR: Script-First Multilingual Speech Synthesis with Adaptive Locale Resolution

## Quick Facts
- arXiv ID: 2510.25178
- Source URL: https://arxiv.org/abs/2510.25178
- Authors: Dharma Teja Donepudi
- Reference count: 0
- Primary result: 0.0 WER and 4.3 MOS for multilingual code-switching TTS without retraining

## Executive Summary
SFMS-ALR addresses the challenge of fluent intra-sentential code-switching in text-to-speech synthesis, where abrupt language shifts degrade intelligibility and naturalness. The core method segments text by Unicode script, applies adaptive language identification, and uses sentiment-aware prosody control to maintain expressive continuity across languages. It generates SSML markup for each segment and synthesizes speech using existing TTS engines without retraining. Objective tests showed perfect 0.0 WER for all languages, and subjective evaluations yielded an average MOS of 4.3, outperforming baselines (single-voice 3.5, multilingual model 3.8). The system achieved real-time synthesis with smooth prosody and high listener preference for native-voice switching.

## Method Summary
The SFMS-ALR system processes multilingual text through a pipeline that begins with Unicode script segmentation to identify language boundaries. For ambiguous Latin script spans, a lightweight language identification model (potentially an LLM) is used to determine the language. Each segment is then mapped to a locale and corresponding TTS voice, with user preferences optionally guiding voice selection. Sentiment-aware prosody control adjusts pitch, rate, and pauses based on punctuation and sentiment cues. The system generates SSML markup with `<voice>` or `<lang>` tags for each segment, synthesizes via cloud TTS APIs (Google TTS in prototype), and concatenates segments with 50 ms pauses, normalizing to 16 kHz mono PCM. No model training is required—the approach orchestrates existing TTS engines.

## Key Results
- Perfect 0.0 WER across all tested languages
- Average MOS of 4.3, outperforming single-voice (3.5) and multilingual model (3.8) baselines
- Real-time synthesis with 0.5–1.2s latency per utterance
- High listener preference for native-voice switching with smooth prosody

## Why This Works (Mechanism)
SFMS-ALR works by combining precise script-based segmentation with adaptive language identification and prosody-aware synthesis. By segmenting text according to Unicode script, the system accurately isolates language boundaries, even in complex code-switching scenarios. The adaptive language identification resolves ambiguities in Latin script, ensuring correct voice and locale assignment. Sentiment-aware prosody control preserves expressive continuity across languages, preventing abrupt tonal shifts. The SSML orchestration approach leverages the strengths of existing TTS engines without requiring retraining, enabling high-quality synthesis for diverse language pairs.

## Foundational Learning
- **Unicode script segmentation**: Splits text into language segments based on script properties. Why needed: Accurately identifies language boundaries for code-switching. Quick check: Test segmentation accuracy on mixed-script sentences.
- **Lightweight language identification**: Detects language for ambiguous Latin spans. Why needed: Resolves script ambiguity to ensure correct voice assignment. Quick check: Measure LID accuracy on Latin script multilingual text.
- **SSML orchestration**: Generates SSML markup for each segment and synthesizes via TTS APIs. Why needed: Enables seamless integration of multiple TTS engines. Quick check: Verify SSML correctness and synthesis output quality.
- **Sentiment-aware prosody control**: Adjusts pitch, rate, and pauses based on sentiment and punctuation. Why needed: Maintains expressive continuity across language switches. Quick check: Evaluate prosody naturalness in mixed-language utterances.
- **Voice and locale mapping**: Maps locales to specific TTS voices, optionally guided by user preferences. Why needed: Ensures appropriate voice selection for each language. Quick check: Confirm voice assignments match target locales.
- **Audio concatenation and normalization**: Joins synthesized segments with pauses and normalizes audio. Why needed: Produces coherent, consistent output. Quick check: Measure continuity and quality of concatenated audio.

## Architecture Onboarding

**Component map**
Unicode script segmentation -> Lightweight LID -> Locale/voice planning -> Sentiment-aware prosody control -> SSML generation -> TTS API synthesis -> Audio concatenation/normalization

**Critical path**
Text input -> Script segmentation -> LID for Latin spans -> Locale resolution -> SSML construction -> TTS synthesis -> Audio normalization

**Design tradeoffs**
- Static rules vs. learned switching: Current system uses rules; future work considers neural networks for adaptive switching.
- Native voice vs. accent: Trade-off between intelligibility and perceived speaker consistency.
- Punctuation-based vs. syntax-aware prosody: Current method uses punctuation; future work could model syntax for better intonation.

**Failure signatures**
- Misidentified language (e.g., transliterated text): Results in wrong voice/locale and degraded intelligibility.
- Audible timbre discontinuity: Voice switches sound like multiple speakers.
- Prosody mismatch: Abrupt tonal shifts at language boundaries.

**3 first experiments**
1. Implement script-based segmentation and test accuracy on multilingual text.
2. Add language identification for Latin segments and measure accuracy.
3. Build SSML pipeline and validate synthesis output quality.

## Open Questions the Paper Calls Out
- Can small neural networks effectively learn optimal switching strategies (accented vs. native voice) based on context and user preference? Comparative user studies showing learned switching yields higher satisfaction than static rules would resolve this.
- Can voice conversion be integrated to harmonize speaker identity across distinct native voices without degrading intelligibility? Subjective evaluations demonstrating maintained intelligibility and consistency would resolve this.
- Does modeling syntactic patterns and discourse markers at code-switch boundaries significantly improve prosodic naturalness compared to punctuation-based heuristics? Ablation studies comparing intonation accuracy with syntax-aware models would resolve this.

## Limitations
- Unspecified lightweight LLM for language identification limits reproducibility of perfect WER.
- Sentiment-to-prosody mapping rules are not defined, creating ambiguity in prosody control.
- Voice matching criteria for timbre consistency across languages are incomplete.

## Confidence
- Core segmentation and SSML orchestration: Medium
- Perfect WER and high MOS: Low (due to unspecified LLM and prosody components)
- Real-time synthesis latency: Medium (depends on cloud API performance)

## Next Checks
1. Implement a baseline with `langdetect` for Latin script language identification and measure WER and MOS on a multilingual test set.
2. Conduct A/B testing to quantify timbre discontinuity at voice switches, comparing single-voice vs. multi-voice modes.
3. Validate sentiment-to-prosody mapping by testing prosody consistency across languages using objective F₀ statistics and subjective listener preference studies.