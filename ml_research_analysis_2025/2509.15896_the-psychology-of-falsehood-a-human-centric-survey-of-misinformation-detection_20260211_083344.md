---
ver: rpa2
title: 'The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection'
arxiv_id: '2509.15896'
source_url: https://arxiv.org/abs/2509.15896
tags:
- misinformation
- information
- fact-checking
- pages
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey explores the evolving landscape of misinformation detection,
  emphasizing the need to move beyond traditional fact-checking to incorporate human
  psychology and cognition. While automated fact-checking has advanced, it often overlooks
  the complex interplay of cognitive biases, emotional responses, and social dynamics
  that influence how misinformation spreads and is believed.
---

# The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection

## Quick Facts
- arXiv ID: 2509.15896
- Source URL: https://arxiv.org/abs/2509.15896
- Reference count: 40
- Primary result: Proposes integrating psychological models with multi-agent simulations for more effective misinformation detection

## Executive Summary
This survey argues that traditional fact-checking approaches are insufficient for combating misinformation because they ignore the complex interplay of human psychology, cognitive biases, and social dynamics. The authors propose a "neuro-behavioural model" that simulates how misinformation spreads through networks of agents with diverse cognitive profiles. By combining large language models, graph neural networks, and reinforcement learning, this framework aims to predict and mitigate the societal harms of misinformation more effectively than purely factual approaches.

## Method Summary
The paper proposes a multi-agent framework with three components: Pseudo-humans (LLMs tuned to simulate specific cognitive biases), Agents of Change (interface agents that inject content and analyze responses), and Invigilators (evaluation agents providing feedback loops). The framework integrates Graph Neural Networks to analyze interaction graphs and Reinforcement Learning for adaptive policy adjustment. Key datasets mentioned include FARM (persuasive content) and AmbigBio (entity ambiguity). The approach aims to create adaptive, interpretable frameworks that predict misinformation virality and belief formation, not just factual accuracy.

## Key Results
- Traditional fact-checking methods are insufficient because misinformation operates at narrative and emotional levels rather than isolated factual claims
- Misinformation spreads through complex social dynamics that involve cognitive biases, emotional responses, and group psychology
- Proposed neuro-behavioural models can simulate human-like belief formation and propagation patterns through multi-agent systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Misinformation persuades through narrative gestalt, not isolated factual errors
- Mechanism: Individual claims that appear misleading when evaluated independently become persuasive when arranged into emotionally coherent narratives that "override" individual debunkings through cumulative structural consistency.
- Core assumption: Human belief formation prioritizes narrative coherence and emotional resonance over component-level factuality.
- Evidence anchors:
  - [abstract] "the detrimental effect of misinformation transcends simple falsehoods; it takes advantage of how individuals perceive, interpret, and emotionally react to information"
  - [section 3] "This gestalt framing is a core mechanism behind the virality and resilience of misinformation... Viewers were persuaded not by any one fact, but by the cumulative structure that appears internally consistent and emotionally resonant"
  - [corpus] Weak direct support—corpus papers focus on detection accuracy rather than psychological persuasion mechanisms
- Break condition: If narrative coherence can be computationally decomposed into verifiable sub-claims without losing persuasive context, the gestalt mechanism is insufficient as an explanatory model

### Mechanism 2
- Claim: LLMs exhibit human-like cognitive biases under persuasive pressure
- Mechanism: LLMs shift correct beliefs when exposed to persuasive misinformation through patterns resembling cognitive dissonance reduction—aligning outputs with persuasive inputs to restore internal coherence rather than maintaining accurate priors.
- Core assumption: LLM belief-update dynamics approximate human cognitive dissonance processes, not just statistical pattern matching.
- Evidence anchors:
  - [section 6] "demonstrated that LLMs can shift correct beliefs when exposed to persuasive misinformation, a pattern that mirrors human susceptibility to persuasion"
  - [section 6] "repetition acts as a heuristic, making misinformation appear more credible without requiring systematic processing, aligning with the Heuristic-Systematic model theory"
  - [corpus] No corpus papers validate LLM-cognitive-dissonance parallels directly
- Break condition: If LLM belief shifts are fully explained by token-probability distributions without requiring dissonance-analogous constructs, the psychological framing is post-hoc interpretation

### Mechanism 3
- Claim: Simulated social networks can predict misinformation propagation
- Mechanism: LLMs generate diverse user reactions based on varied attributes, creating user-news interaction networks that simulate propagation dynamics; graph neural networks then detect patterns in these synthetic social structures.
- Core assumption: LLM-generated reactions sufficiently approximate real human behavioral responses to enable propagation modeling.
- Evidence anchors:
  - [abstract] "integrating neuro-behavioural models and world simulations"
  - [section 5] "This approach integrates user interactions to enhance the model's ability to analyse and interpret complex patterns in misinformation detection"
  - [corpus] RAMA (arXiv:2507.09174) uses multi-agent retrieval-augmented frameworks, providing partial validation
- Break condition: If LLM-generated reactions systematically diverge from real user behavior due to training distribution gaps or hallucination, simulation-based detection loses ground-truth correspondence

## Foundational Learning

- **Gestalt Psychology (Perception of Wholes)**
  - Why needed here: Core to understanding why fact-level verification fails—misinformation operates at the narrative level where meaning emerges from structure, not individual claims
  - Quick check question: Can you explain why debunking individual claims often fails to change beliefs in conspiracy narratives?

- **Heuristic-Systematic Model**
  - Why needed here: Explains dual-processing in both humans and LLMs—when and why shortcuts (repetition, source credibility, emotional framing) override analytical reasoning
  - Quick check question: What conditions trigger heuristic vs systematic processing in the context of persuasive misinformation?

- **Cognitive Dissonance Theory**
  - Why needed here: Framework for understanding belief revision resistance and how conflicting information gets filtered; applied analogously to LLM behavior in this paper
  - Quick check question: How does dissonance theory predict people will respond when confronted with evidence contradicting deeply held beliefs?

## Architecture Onboarding

- **Component map:** Pseudo-humans (LLM agents with bias profiles) -> Agents of Change (content injection and analysis) -> Invigilators (evaluation and feedback) -> GNNs (interaction graph analysis) -> RL (adaptive policy)

- **Critical path:**
  1. Define cognitive bias parameters for pseudo-human population
  2. Configure LLM prompt templates that elicit bias-consistent reactions
  3. Build interaction graph from simulated engagements
  4. Train GNN to detect propagation signatures of harmful vs benign content
  5. Deploy invigilator feedback loop for real-time policy adjustment

- **Design tradeoffs:**
  - Simulation fidelity vs computational cost: More diverse pseudo-humans improve realism but scale poorly
  - Bias parameter granularity: Fine-grained bias modeling may overfit to specific populations
  - Real vs synthetic data: Pure simulation enables scale but risks distribution shift; hybrid approaches need careful calibration

- **Failure signatures:**
  - Hallucination cascades: Simulated reactions drift into implausible behaviors
  - Bias parameter miscalibration: Pseudo-human responses don't match real population distributions
  - Graph sparsity: Insufficient interaction density for meaningful GNN signal
  - Prompt leakage: Bias parameters visible in outputs, contaminating simulation validity

- **First 3 experiments:**
  1. Validate LLM reaction generation against real social media comment datasets—measure distributional alignment across demographic and ideological segments
  2. A/B test single-bias vs multi-bias pseudo-human populations on known misinformation propagation cases
  3. Compare GNN-based propagation detection against baseline fact-verification systems on ambiguous claims where factual accuracy alone is insufficient

## Open Questions the Paper Calls Out
None

## Limitations
- The proposed neuro-behavioural framework remains largely theoretical with key implementation details underspecified
- LLM-cognitive-dissonance parallels lack direct empirical validation through the cited corpus
- Assumption that LLM-generated reactions sufficiently approximate human behavior remains untested at scale
- Integration of GNNs and RL within the multi-agent loop lacks concrete architectural specifications

## Confidence
- **High confidence:** Traditional fact-checking limitations and the need for psychological integration are well-established
- **Medium confidence:** Narrative gestalt mechanism for persuasion has theoretical grounding but limited direct validation
- **Low confidence:** LLM-cognitive-dissonance parallels and neuro-behavioural simulation frameworks remain largely theoretical

## Next Checks
1. **Distributional Validation:** Compare LLM-generated user reactions against real social media comment datasets (e.g., Twitter, Reddit) across demographic and ideological segments. Measure alignment in stance distribution, emotional valence, and argumentation patterns.

2. **Simulation-to-Reality Transfer:** Test whether simulated propagation patterns from the proposed framework predict actual misinformation spread in controlled social media experiments. Use parallel content introduction in both simulated and real networks.

3. **Psychological Fidelity Assessment:** Design controlled experiments where LLMs and human subjects are exposed to identical persuasive misinformation under varying cognitive load conditions. Compare belief revision patterns and resistance to debunking to assess the validity of psychological analogies.