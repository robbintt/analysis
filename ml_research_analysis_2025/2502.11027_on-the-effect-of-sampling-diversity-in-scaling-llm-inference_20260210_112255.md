---
ver: rpa2
title: On the Effect of Sampling Diversity in Scaling LLM Inference
arxiv_id: '2502.11027'
source_url: https://arxiv.org/abs/2502.11027
tags:
- arxiv
- sampling
- diversity
- perturbations
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a theoretical and empirical analysis of sampling
  diversity in large language model (LLM) inference-time scaling. The authors show
  that diversified sampling improves Best-of-N performance by reducing asymptotic
  error and accelerating convergence, and derive a diversity-fidelity tradeoff principle
  guiding effective perturbation design.
---

# On the Effect of Sampling Diversity in Scaling LLM Inference

## Quick Facts
- arXiv ID: 2502.11027
- Source URL: https://arxiv.org/abs/2502.11027
- Reference count: 40
- This paper provides a theoretical and empirical analysis of sampling diversity in large language model (LLM) inference-time scaling.

## Executive Summary
This paper presents a comprehensive study of how sampling diversity affects Best-of-N inference scaling in large language models. The authors develop theoretical foundations showing that diversified sampling reduces asymptotic error and accelerates convergence in Best-of-N selection. Through empirical validation across reasoning, mathematics, and code generation tasks, they demonstrate consistent performance improvements: 10.8% exact match accuracy gain on MMLU-Pro, 9.6% on MATH for mathematics, and 9.5% pass rate on HumanEval for code generation. The work identifies both beneficial conditions (varying temperatures, chain-of-thought reasoning, stronger thinker models) and failure modes (majority voting) for diversity strategies.

## Method Summary
The method involves generating diverse solutions through task-level or query-level perturbations during inference. Task-level perturbations include fixed modifications like "Strategical Instruction" or "Role," while query-level perturbations require a thinker model to generate per-query variations. For each query, N=100 solutions are sampled from GPT-4o-mini with temperature 0.6, and the best solution is selected using oracle ground truth or LLM-as-a-Judge verification. The theoretical framework analyzes the diversity-fidelity tradeoff through the lens of first absolute central moments and effective gain parameters.

## Key Results
- Diversified sampling improves Best-of-N scaling by reducing asymptotic error and accelerating convergence
- Task-level and query-level perturbations yield consistent gains: 10.8% in EM accuracy on MMLU-Pro, 9.6% on MATH, and 9.5% in pass rate on HumanEval
- The study identifies a diversity-fidelity tradeoff principle guiding effective perturbation design
- Majority voting fails to benefit from diversity and can actually hurt performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Auxiliary diversity through prompt perturbations improves Best-of-N scaling by reducing asymptotic error and accelerating convergence.
- Mechanism: Perturbations introduce variation in the per-mode failure log-probability q(r, η, ξ). When the first absolute central moment M₁ ≥ μ̂₁ > 0 (dispersion), and perturbations preserve single-attempt quality (fidelity ε < 1), the error reduction factor C_N = Ω(μ̂₁²N/(1+ε)) increases with N, yielding faster Best-of-N gains and a smaller "blind-spot" set of unsolvable instances.
- Core assumption: Varying auxiliary randomness ξ induces non-trivial variation in success probability (Hypothesis 3.1); fixed perturbations do not globally worsen response quality (Hypothesis 3.3).
- Evidence anchors:
  - [abstract] "theoretically explain why diversified sampling improves Best-of-N scaling... responses generated from diverse prompts after Best-of-N selection exhibit significantly lower error rates"
  - [section 3] Theorem 3.5 proves P^N_div ≤ (P^N_reg - P^inf_reg)/(1+C_N) + P^inf_div with P^inf_div ≤ P^inf_reg
  - [corpus] Related work on Best-of-N calibration (CarBoN) shows diminishing returns without diversity strategies, supporting the need for systematic perturbation design
- Break condition: If perturbations are too similar (M₁ → 0) or too irrelevant (ε large), C_N diminishes and gains vanish.

### Mechanism 2
- Claim: There exists a "sweet spot" of moderate perturbation-question relevance that maximizes Best-of-N improvement.
- Mechanism: The effective gain term μ̂₁²/(1+ε) is maximized when perturbations are different enough to create exploration (boost M₁) but faithful enough to avoid harming single-attempt quality (control ε). Low relevance inflates ε; excessive similarity shrinks M₁.
- Core assumption: Perturbation effectiveness is non-monotonic in similarity to the original question—neither irrelevant nor verbatim repetition helps.
- Evidence anchors:
  - [section 4.1, Figure 2] EM and Pass rates show rise-then-fall pattern: performance peaks with task-aligned ideas (Perturbation 3), declines for irrelevant (Perturbation 1) and verbatim repetition (Perturbation 5)
  - [section 4.1] "moderately relevant perturbations contribute positively, whereas overly low or excessively high similarity offers no benefit"
  - [corpus] Verbalized Sampling work identifies "typicality bias" reducing diversity, consistent with the fidelity constraint
- Break condition: If perturbation-question cosine similarity is near 0 or near 1, performance degrades to or below direct sampling baseline.

### Mechanism 3
- Claim: Diversified sampling improves Best-of-N but fails to help—and can hurt—under majority voting.
- Mechanism: Best-of-N succeeds when any correct solution appears (p(y*) > 0), rewarding probability mass spread. Majority voting requires the correct answer to be the most probable (p(y*) = max_y p(y)); spreading mass can lower p(y*)'s rank, degrading accuracy.
- Core assumption: Majority voting converges to the MAP label; Best-of-N converges to 1 whenever any mass is on y*.
- Evidence anchors:
  - [section 5.2, Proposition 5.1] "majority vote accuracy converges to 1 iff p(y*) = max_{y∈Y} p(y), while Best-of-N accuracy converges to 1 iff p(y*) > 0"
  - [section 5.2, Figure 8] MATH experiments show perturbations yield inconsistent improvements under majority voting
  - [corpus] Self-consistency literature (referenced in appendix) uses majority voting with CoT, but this paper highlights its incompatibility with diversity-seeking strategies
- Break condition: If verification uses majority voting instead of selection, diversity injection may not improve—and can degrade—performance.

## Foundational Learning

- Concept: **Best-of-N Sampling and Pass@k/EM@k Metrics**
  - Why needed here: The entire theoretical framework quantifies how diversity affects Best-of-N failure probability P^N. Without understanding EM@k (exact match for reasoning/math) and Pass@k (passing hidden tests for code), the empirical results are uninterpretable.
  - Quick check question: For a code task where 3 of 100 solutions pass all hidden tests, what is Pass@100?

- Concept: **Prompt Perturbation vs. Temperature Scaling**
  - Why needed here: The paper distinguishes base randomness η (temperature, seed) from auxiliary diversity ξ (perturbations). Temperature alone often traps solutions in local clusters; perturbations break out.
  - Quick check question: If you increase temperature from 0.6 to 1.2 without perturbations, what does Figure 3 suggest happens to Pass@k?

- Concept: **Task-level vs. Query-level Perturbations**
  - Why needed here: Implementation differs significantly. Task-level perturbations (Role, Instruction) are pre-defined and task-independent; query-level (RandIdeaInj, RandQReph) require per-question generation from a thinker model.
  - Quick check question: Which perturbation type requires an auxiliary "thinker" model to generate ideas before sampling?

## Architecture Onboarding

- Component map:
  Perturbation Generator -> Policy Model -> Verifier
  Thinker Model (optional, for query-level) -> Perturbation Generator

- Critical path:
  1. Generate N perturbations {ξ_1, ..., ξ_N} (pre-defined or via thinker)
  2. Construct perturbed prompts r_k = r ⊕ ξ_k
  3. Sample N solutions s_k ~ LLM(·|r_k)
  4. Verify and select best solution

- Design tradeoffs:
  - **Task-level vs. Query-level**: Task-level is cheaper (no thinker calls); query-level offers higher gains but requires additional inference
  - **Thinker model strength**: Stronger thinkers (e.g., DeepSeek-V3) raise scaling curves but cost more
  - **Perturbation cardinality**: More distinct perturbations improve scaling but require more upfront generation
  - **Verification method**: Oracle/ORM aligns with diversity benefits; majority voting is a failure mode

- Failure signatures:
  - **Perturbations too similar**: Solutions cluster in local region; Pass@k ≈ direct sampling
  - **Perturbations irrelevant**: Single-attempt quality degrades; EM@k drops below baseline
  - **Majority voting verifier**: Inconsistent or negative gains even with diverse solutions
  - **Weak thinker model**: Query-level perturbations offer minimal improvement over direct sampling

- First 3 experiments:
  1. **Perturbation relevance sweep**: On a held-out subset (e.g., 50 HumanEval problems), test 5 perturbation styles with varying question similarity. Confirm the rise-then-fall pattern and identify the sweet-spot cosine similarity range.
  2. **Task-level baseline**: Implement Role and Instruction perturbations with N=100 on MMLU-Pro and HumanEval. Compare EM@100 and Pass@100 to direct sampling (temperature=0.6). Expect ~6-10% relative gains.
  3. **Verification method comparison**: On MATH, compare (a) oracle Best-of-N, (b) LLM-as-judge top-10 selection, and (c) majority voting. Confirm that (a) and (b) benefit from diversity while (c) does not.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can perturbation strategies be adapted to improve performance under majority voting, overcoming the identified failure mode where diversity currently degrades accuracy?
- Basis in paper: [explicit] The authors theoretically identify and empirically confirm (Figure 8) that majority voting is a failure mode because diversity spreads probability mass away from the single most probable answer (Section 5.2).
- Why unresolved: The paper establishes that standard diversity harms voting but does not propose a mechanism to align diverse reasoning paths with the convergence requirements of voting.
- What evidence would resolve it: A weighted voting or filtering method that utilizes diverse candidate solutions to increase the probability of the correct answer becoming the majority.

### Open Question 2
- Question: Does the efficacy of sampling diversity diminish asymptotically for stronger models, or does the optimal diversity-fidelity tradeoff simply shift?
- Basis in paper: [inferred] Appendix E.4 shows relative gains diminish significantly for stronger models (e.g., 0.0% for Qwen-72B), leading to a caution against its use for highly capable thinkers.
- Why unresolved: It is unclear if the observed limitation is fundamental to the scaling laws or a result of the specific perturbation instantiations (Role/Instruction) being insufficient for advanced models.
- What evidence would resolve it: Demonstrations that task-aligned perturbations can yield non-trivial relative gains on frontier-scale models (e.g., >70B parameters).

### Open Question 3
- Question: Can the optimal "sweet spot" of perturbation fidelity be determined automatically, given the non-monotonic relationship between relevance and performance?
- Basis in paper: [inferred] Figure 2 and Section 4.1 reveal a "rise-then-fall" pattern where only moderately relevant perturbations improve performance, while irrelevant or verbatim ones degrade it.
- Why unresolved: The current work relies on manual categorization of perturbation styles (Perturbation 1–5) to find this peak; a predictive mechanism for new tasks is absent.
- What evidence would resolve it: A heuristic or metric that correlates with the diversity-fidelity tradeoff parameter ($\epsilon$) to automatically select the perturbation strength.

## Limitations

- The theoretical analysis relies on specific distributional assumptions about perturbation effects that may not hold across all model architectures or task types
- Empirical evaluation focuses on a limited set of perturbation types without exploring the full space of possible diversity injection strategies
- The study does not address how to automatically determine the optimal perturbation fidelity level for new tasks

## Confidence

- **High Confidence**: The core finding that diversified sampling improves Best-of-N performance for reasoning and code tasks, supported by consistent empirical gains across multiple benchmarks (10.8% EM improvement on MMLU-Pro, 9.5% Pass improvement on HumanEval)
- **Medium Confidence**: The theoretical diversity-fidelity tradeoff mechanism and the identified sweet spot for perturbation relevance, which while supported by empirical patterns, requires further validation across different model families and task domains
- **Low Confidence**: The claim about majority voting failure modes, as the analysis assumes specific conditions that may not generalize to all verification contexts

## Next Checks

1. **Perturbation Robustness Test**: Systematically vary perturbation similarity (cosine distance from original question) on held-out reasoning problems to precisely map the rise-then-fall performance curve and validate the identified sweet spot.

2. **Cross-Model Generalization**: Apply the same perturbation strategies to different LLM families (e.g., Claude, Llama) to assess whether the diversity-fidelity tradeoff holds across architectures.

3. **Extended Perturbation Space**: Implement additional perturbation types beyond the current set (e.g., temporal reasoning constraints, multi-modal context injection) to evaluate whether the theoretical framework generalizes to broader diversity injection methods.