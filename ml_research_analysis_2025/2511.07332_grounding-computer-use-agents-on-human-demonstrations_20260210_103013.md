---
ver: rpa2
title: Grounding Computer Use Agents on Human Demonstrations
arxiv_id: '2511.07332'
source_url: https://arxiv.org/abs/2511.07332
tags:
- element
- arxiv
- wang
- elements
- desktop
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces GROUNDCUA, a large-scale expert-annotated
  dataset for desktop GUI grounding, containing over 3.56 million UI element annotations
  across 56K screenshots from 87 applications. The dataset addresses the lack of high-quality
  resources for desktop environments, capturing dense, high-resolution interfaces
  with small elements and diverse real-world tasks.
---

# Grounding Computer Use Agents on Human Demonstrations

## Quick Facts
- arXiv ID: 2511.07332
- Source URL: https://arxiv.org/abs/2511.07332
- Reference count: 40
- Primary result: GroundNext achieves state-of-the-art grounding accuracy across five benchmarks using only 700K SFT samples from expert-annotated desktop UI data.

## Executive Summary
This paper introduces GROUNDCUA, a large-scale expert-annotated dataset for desktop GUI grounding, and GROUNDNEXT, a family of vision-language models trained on this data. GROUNDCUA contains over 3.56 million UI element annotations across 56K screenshots from 87 applications, capturing dense, high-resolution interfaces with small elements and diverse real-world tasks. Using only 700K curated samples—far fewer than prior approaches—GROUNDNEXT achieves state-of-the-art performance across five grounding benchmarks, with RL post-training providing additional modest improvements.

## Method Summary
The method involves two-stage training of vision-language models (Qwen2.5-VL-Instruct 3B/7B) for desktop GUI grounding. First, supervised fine-tuning (SFT) uses 700K curated instruction pairs generated from expert-annotated GroundCUA data, with 50% Direct, 35% Functional, and 15% Spatial instruction types. Second, reinforcement learning (RLOO) refines the model using 10K held-out samples with a discrete reward function based on normalized distance to bounding boxes. The models are evaluated on five benchmarks including ScreenSpot-Pro, OSWorld-G, and UI-Vision, demonstrating strong generalization despite training only on desktop data.

## Key Results
- GROUNDNEXT achieves 66.4-69.2 average accuracy across five benchmarks using only 700K SFT samples
- RL post-training provides +1.6-2.0 average improvement over SFT baseline
- The 3B model outperforms many larger and proprietary systems in agentic evaluations on OSWorld-Verified
- GroundCUA contains 64.1 avg elements/screenshot vs. 7.0-11.8 for other datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expert-annotated dense desktop UI data yields higher grounding accuracy per sample than larger automated/synthetic datasets.
- Mechanism: Human annotators label every visible element on real interaction states, creating 64 annotations per screenshot on average. This density captures small icons (0.13% avg element area) and context relationships that automated extraction (accessibility trees, HTML DOM) miss. The model receives richer spatial and semantic supervision per image, improving sample efficiency.
- Core assumption: Desktop UIs have higher density, smaller elements, and more application-specific iconography than mobile/web, making human annotation disproportionately valuable.
- Evidence anchors:
  - [abstract] "3.56M human-verified annotations...while requiring less than one-tenth the training data of prior work"
  - [Table 1] Shows GroundCUA has 64.1 avg elements/screenshot vs. 7.0-11.8 for other datasets
  - [Section 4.2] "UI-Vision overlaps with our dataset...treat it as an in-domain benchmark, while the others are out-of-domain"
  - [corpus] Related work (JEDI, OS-Atlas) relies on synthetic/automated data at 4-14M samples; limited direct comparison of annotation quality mechanisms
- Break condition: If automated extraction (accessibility tree, DOM) achieves comparable density and small-element recall on desktop, the annotation advantage diminishes.

### Mechanism 2
- Claim: Two-stage training (SFT → RL) with high-quality SFT data yields most gains; RL provides modest incremental refinement.
- Mechanism: SFT on 700K curated samples establishes strong grounding baseline (66.4-69.2 avg). RL on 10K unseen samples with a shaped discrete reward corrects remaining errors, yielding +1.6-2.0 avg improvement. Models already strong from SFT have fewer correctable errors.
- Core assumption: The discrete reward function (based on normalized distance to bounding box) meaningfully captures grounding quality gradients.
- Evidence anchors:
  - [abstract] "Reinforcement learning post-training further improves performance"
  - [Section 5.2] "SFT, when trained with high-quality data, captures the majority of the model's performance, with RL offering targeted fine-tuning"
  - [Figure 3] Shows models trained on GroundCUA during SFT have smallest RL gains, suggesting SFT already informative
  - [corpus] Limited validation; related RL papers (GUI-R1, InfiGUI) use different reward schemes
- Break condition: If SFT data quality is low, RL gains would be larger. If reward shaping is fundamentally misaligned with human grounding preferences, RL could harm performance.

### Mechanism 3
- Claim: Diverse instruction types (direct, functional, spatial) generated from dense annotations improve instruction-following robustness.
- Mechanism: GroundCUA generates three instruction types: Direct (element attributes), Functional (intent-based), and Spatial (relative positioning). The 700K SFT split uses 50% direct, 35% functional, 15% spatial, training the model to handle varied user phrasings without overfitting to single instruction patterns.
- Core assumption: Real user instructions span these three types; each type provides complementary supervision.
- Evidence anchors:
  - [Section 3] "We generate three primary types of instructions: Direct, Functional, and Spatial"
  - [Appendix B.1-B.3] Details instruction templates and generation prompts
  - [Section 5.3] Best icon performance in Office, Development, Creative categories where functional/spatial disambiguation matters most
  - [corpus] No direct comparison; assumption about instruction diversity remains unvalidated
- Break condition: If user instructions overwhelmingly cluster in one type, or if instruction type doesn't correlate with grounding difficulty, the diversity benefit is overstated.

## Foundational Learning

- **GUI Grounding Task**
  - Why needed here: This is the core problem—mapping natural language instructions to 2D coordinates on screen. Without this, you can't interpret evaluation metrics or error modes.
  - Quick check question: Given a screenshot and instruction "Click the Save button to the right of the file list," what must the model predict?

- **Vision-Language Model Architecture (VLM)**
  - Why needed here: GroundNext builds on Qwen2.5-VL-Instruct (3B/7B), which fuses vision encoder outputs with LLM tokens. You need this to understand what's being fine-tuned.
  - Quick check question: Which components are updated during SFT—just the LLM head, or vision encoder too?

- **Reinforcement Learning with Verifiable Rewards**
  - Why needed here: The RL stage uses RLOO with a custom discrete reward. Understanding policy gradients is necessary to debug training instability or reward hacking.
  - Quick check question: In the RLOO objective, what serves as the baseline for advantage estimation?

## Architecture Onboarding

- **Component map:**
  1. **Data Layer:** Human demonstrations → keyframes → dense annotations (bbox + label + category) → 3.56M elements
  2. **Instruction Layer:** Annotations + templates → Direct/Functional/Spatial instructions → 700K SFT + 10K RL subsets
  3. **Training Layer:** Qwen2.5-VL base → SFT (2 epochs, lr=3e-6, full model) → RL (RLOO, discrete reward, 1 epoch)
  4. **Evaluation Layer:** Point prediction → inside bbox = correct → accuracy across 5 benchmarks

- **Critical path:**
  1. Annotation quality directly bounds instruction diversity
  2. Instruction diversity determines SFT sample efficiency
  3. SFT quality determines residual RL gains
  4. Grounding accuracy determines agentic task success

- **Design tradeoffs:**
  - **Data volume vs. annotation density:** GroundCUA uses 55K screenshots vs. competitors' 270K-1.85M, but 3x annotation density
  - **SFT scale vs. RL refinement:** 700K SFT samples sufficient for strong baseline; RL adds 10K for marginal gains
  - **Desktop-only training vs. cross-platform evaluation:** Training on desktop generalizes partially to mobile/web, but underperforms on web-specific tasks

- **Failure signatures:**
  - Small element bias: Model sometimes clicks text labels instead of parent elements (Section E error analysis)
  - Domain gap: Mobile/web grounding errors on elements absent in desktop training
  - Spatial reasoning: Lower performance on spatial instruction types (45-50% vs. 60-70% for basic/functional on UI-Vision)

- **First 3 experiments:**
  1. **Reproduce SFT baseline:** Train Qwen2.5-VL-3B on 100K GroundCUA samples; verify ~48-50% ScreenSpot-Pro accuracy matches paper
  2. **Ablate instruction types:** Train three models using only direct, only functional, only spatial instructions; measure which type contributes most to each benchmark
  3. **Validate RL reward design:** Compare discrete reward vs. binary reward vs. continuous distance on held-out set; verify discrete matches reported ~2% improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can more sophisticated reward designs substantially improve RL gains beyond the modest improvements seen with the current discrete reward scheme?
- Basis in paper: [explicit] "While our reward design is simple, we acknowledge that more sophisticated reward functions, such as those in Liu et al. (2025b), could lead to more substantial RL gains, which we leave for future work."
- Why unresolved: The authors observed that models trained with GROUNDCUA during SFT showed the smallest RL gains, suggesting high-quality SFT already captures most performance, but only tested one reward formulation.
- What evidence would resolve it: Systematic comparison of multiple reward designs (continuous, learned reward models, hierarchical) showing statistically significant improvements over the discrete scheme.

### Open Question 2
- Question: What is the optimal strategy for balancing desktop, mobile, and web data to achieve seamless cross-platform grounding?
- Basis in paper: [explicit] "Mixing data across these domains could yield models that operate seamlessly across platforms, though balancing these domains and addressing transfer bottlenecks will require careful study."
- Why unresolved: Preliminary experiments adding web/mobile data during RL did not yield consistent improvements, but the setup was limited in scope.
- What evidence would resolve it: Controlled ablation studies varying domain proportions with systematic analysis of transfer bottlenecks and domain-specific performance trade-offs.

### Open Question 3
- Question: How can computer-use agents adapt to unseen applications and continually improve as new interaction paradigms emerge?
- Basis in paper: [explicit] "GROUNDCUA includes platform- and category-level metadata, enabling research on continual learning and adaptation, evaluating how agents adapt to unseen applications and continually improve as new interaction paradigms emerge."
- Why unresolved: The current work focuses on static benchmark evaluation and does not study temporal adaptation or zero-shot transfer to applications outside the 87 covered.
- What evidence would resolve it: Experiments measuring grounding accuracy on held-out applications over time, with and without continual learning updates.

## Limitations
- Limited generalization to proprietary or niche desktop software due to bias toward commonly used open-source applications
- Unknown effectiveness of human annotation density advantage vs. automated extraction on desktop UIs
- Modest RL gains suggest limited benefit from post-training refinement with current reward design

## Confidence
- **High Confidence:** The SFT training procedure and baseline performance metrics (66.4-69.2 avg accuracy) are well-documented and reproducible with public base models.
- **Medium Confidence:** The claim that human annotation density yields superior sample efficiency is supported by comparison tables but relies on untested assumptions about automated extraction limitations.
- **Low Confidence:** The RL post-training benefits (+1.6-2.0 avg improvement) and the specific contribution of instruction type diversity lack sufficient ablation evidence for independent validation.

## Next Checks
1. **Ablation study on instruction types:** Train three models using only Direct, only Functional, and only Spatial instructions on the same 700K GroundCUA subset. Measure which type contributes most to each benchmark's performance and whether the claimed diversity benefits hold.
2. **Synthetic vs. human annotation comparison:** Create a synthetic dataset with similar density (64 avg elements/screenshot) using automated accessibility tree extraction. Train models on both human-annotated and synthetic versions with matched instruction diversity to test the claimed annotation quality advantage.
3. **RL reward function validation:** Implement and compare three reward schemes on held-out samples: (a) discrete bins as described, (b) binary (inside/outside bbox), and (c) continuous normalized distance. Verify that the discrete reward provides the claimed ~2% improvement without overfitting to the GroundCUA distribution.