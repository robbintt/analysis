---
ver: rpa2
title: 'Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models'
arxiv_id: '2509.24488'
source_url: https://arxiv.org/abs/2509.24488
tags:
- llms
- user
- content
- response
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-Sanitize is a novel LLM-driven mitigation framework that emulates
  human self-monitor and self-repair behaviors to detect and correct harmful content
  in real time. It uses a lightweight Self-Monitor module that tracks high-level intentions
  within the LLM via representation engineering, and a Self-Repair module that performs
  in-place correction without initiating separate review dialogues.
---

# Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models

## Quick Facts
- arXiv ID: 2509.24488
- Source URL: https://arxiv.org/abs/2509.24488
- Reference count: 40
- Primary result: Up to 95.72% reduction in privacy leakage with only 2% utility overhead

## Executive Summary
Self-Sanitize is a novel LLM-driven mitigation framework that emulates human self-monitor and self-repair behaviors to detect and correct harmful content in real time. It uses a lightweight Self-Monitor module that tracks high-level intentions within the LLM via representation engineering, and a Self-Repair module that performs in-place correction without initiating separate review dialogues. This design enables token-level streaming monitoring with minimal latency and computational overhead. Extensive experiments on four LLMs across three privacy leakage scenarios show Self-Sanitize achieves superior mitigation performance—reducing privacy leakage by up to 95.72% in user attribute inference and 83.68% in private demonstrations leakage—while maintaining LLM utility with only a 2% overhead and 8% latency increase.

## Method Summary
Self-Sanitize employs a two-module approach for real-time privacy leakage mitigation. The Self-Monitor extracts intermediate layer representations at ~80% depth from the target LLM, passing them through a hierarchical classifier that first determines if content is safe or harmful, then identifies the specific harm type. A consistency monitor window aggregates harm probabilities over k tokens to trigger interrupts only for sustained harmful signals. When triggered, the Self-Repair module rewinds m tokens, injects a scenario-specific repair prompt, and forces the LLM to continue generation with the safe prefix frozen. This in-place correction avoids the overhead of separate review dialogues while maintaining context continuity.

## Key Results
- Achieves 95.72% reduction in user attribute leakage (UAL) and 83.68% in private demonstrations leakage (PDL)
- Maintains LLM utility with only 2% performance overhead and 8% latency increase
- Outperforms baselines including Self-Defend across all three privacy scenarios (UAL, PDL, PCL)

## Why This Works (Mechanism)

### Mechanism 1: Representation-Based Intent Monitoring
- **Claim:** Intermediate layer representations encode high-level intent signals that distinguish privacy-violating outputs from benign ones.
- **Mechanism:** A lightweight hierarchical classifier reads hidden states at layer l (approximately 80% depth) for each generated token. Level-1 outputs [p_safe, p_harm]; Level-2 classifies harm subtype. A monitor window of k tokens smooths the interrupt signal via: I(Q, R≤i, k) = 1( (1/k) Σ p_harm_j > τ ).
- **Core assumption:** Harmful intent manifests in representation space before explicit tokens appear, and the classifier trained on GPT-4-labeled data generalizes to the target LLM.
- **Evidence anchors:**
  - [abstract] "lightweight Self-Monitor module that continuously inspects high-level intentions within the LLM at the token level via representation engineering"
  - [section 4.2] "Self-Monitor consistently strikes a detection accuracy of approximately 97%"
  - [corpus] Related work on privacy-aware decoding (arXiv:2508.03098) confirms representation-level signals can guide mitigation, but cross-model generalization remains understudied.
- **Break condition:** If the target LLM's representations diverge significantly from the classifier's training distribution (e.g., different architecture or fine-tuning), detection accuracy may degrade.

### Mechanism 2: Consistency Monitor Window with Regurgitant Cache
- **Claim:** Aggregating harm probabilities over k tokens and buffering m tokens before streaming reduces false positives without unacceptable latency.
- **Mechanism:** The monitor window k (default 5) requires sustained harmful signal before triggering. The regurgitant cache m (default 10) holds tokens back from the user; if interrupted at position s, the cache rewinds to R_{>s-m, ≤s} and only R_{≤s-m} is streamed.
- **Core assumption:** Privacy leakage typically manifests over multiple tokens; transient spikes are noise.
- **Evidence anchors:**
  - [section 4.2] "the monitor window k ensures the higher consistency of the Self-Monitor interrupt signal and provides a lower FPR"
  - [Figure 5] Ablation shows accuracy stabilizes at k≥5; m=10 balances defense gain vs. latency.
  - [corpus] No direct corpus evidence on this specific cache-window combination; related methods rely on post-hoc filtering instead.
- **Break condition:** For very short leakage phrases (<k tokens) or extremely high streaming latency requirements, the smoothing delay may miss or delay detection.

### Mechanism 3: In-Place Self-Repair via Prefix-Frozen Continuation
- **Claim:** Injecting a repair prompt mid-generation and freezing the safe prefix produces seamless, context-preserving corrections without double-pass overhead.
- **Mechanism:** Upon interrupt, the repair prompt P_T_rp (type-specific) is inserted. The archived response R_ac is frozen as prefix, forcing continuation: R_rp = R_ac ⊕ M_θ(Q, R_ac, P_rp, R_ac). This avoids re-processing the full dialogue.
- **Core assumption:** The LLM can fluently continue from a frozen prefix while pivoting to refusal or sanitization tone.
- **Evidence anchors:**
  - [abstract] "Self-Repair module that performs in-place correction of harmful content without initiating separate review dialogues"
  - [section 4.3] "LLM only needs to process the text length of the Self-Repair prompt, eliminating the need to input the entire dialogue"
  - [corpus] Self-Defend (Wang et al. 2024) uses parallel shadow LLMs with higher overhead; Self-Sanitize's single-pass approach is distinct but unvalidated beyond the tested models.
- **Break condition:** If the frozen prefix strongly conditions the model toward the harmful continuation, repair quality may suffer or produce inconsistent tone.

## Foundational Learning

- **Concept: Representation Engineering in Transformers**
  - **Why needed here:** The monitor depends on extracting and classifying intermediate activations. Understanding layer semantics and hook placement is prerequisite.
  - **Quick check question:** Can you explain why later layers (e.g., 80% depth) are chosen for intent signals, and how to attach a forward hook in PyTorch?

- **Concept: Streaming Token Generation and Latency Budgets**
  - **Why needed here:** The design trades detection window size and cache depth against real-time streaming constraints.
  - **Quick check question:** Given a target of 30 tokens/second, what is the maximum acceptable per-token monitor overhead?

- **Concept: Hierarchical Classification for Multi-Task Safety**
  - **Why needed here:** The two-level classifier separates binary safe/harm from subtype discrimination; this informs training data requirements.
  - **Quick check question:** Why might a flat multi-class classifier underperform compared to the hierarchical design here?

## Architecture Onboarding

- **Component map:**
  1. Representation Hook — Extracts h^l_i at target layer for each token
  2. Hierarchical Classifier — MLP feature extractor → Level-1 (safe/harm) + Level-2 (subtype)
  3. Consistency Monitor — Maintains rolling window of p_harm, triggers interrupt if mean > τ
  4. Regurgitant Cache — FIFO buffer of size m tokens, rewinds on interrupt
  5. In-Place Repair Prompt — Type-specific prompt injected mid-dialogue
  6. Prefix-Frozen Generation — Forces LLM to continue from frozen safe prefix

- **Critical path:** Prompt → LLM forward → Hook → Classifier → Monitor → (if harm) Interrupt → Rewind → Inject Repair Prompt → Continue generation with frozen prefix

- **Design tradeoffs:**
  - k (window): Higher → fewer FPs, more delay
  - τ (threshold): Higher → more conservative interrupts, risk of missed leakage
  - m (cache): Higher → stronger protection, more latency/memory
  - Hook layer depth: Deeper → richer semantics, but may vary across models

- **Failure signatures:**
  - High false positives: Overly sensitive τ, inadequate training data diversity
  - Repair incoherence: Prefix too long or repair prompt poorly designed
  - Latency spikes: m too large or classifier inference unoptimized
  - Cross-model transfer failure: Classifier trained on wrong model's representations

- **First 3 experiments:**
  1. Sanity check the monitor: Train the classifier on the target LLM's representations (not GPT-4) for one scenario (e.g., UAL). Measure accuracy and FPR on held-out set.
  2. Tune hyperparameters: Run ablation on k, τ, m on LLaMA-3.1-8B using PDL. Plot harmful score vs. latency tradeoff.
  3. End-to-end pipeline: Deploy Self-Sanitize on Mistral-Nemo-12B with all three scenarios. Measure MT-Bench utility drop and ATGR/ATLR overhead. Compare to Self-Defend baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the Self-Monitor module trained on privacy leakage data generalize effectively to other safety categories, such as toxicity or bias, without extensive retraining?
- Basis in paper: [inferred] The authors explicitly distinguish their focus on "privacy-invasive content" (Abstract) from prior work that focused on "toxic or offensive content" (Introduction; Sec 5.1), but evaluate only three privacy scenarios.
- Why unresolved: The representation engineering approach relies on high-level intentions which may differ fundamentally between information leakage and hate speech.

### Open Question 2
- Question: How significant is the performance gap when training the Self-Monitor using data generated by a different model (GPT-4.1) compared to data generated by the target LLM itself?
- Basis in paper: [explicit] Appendix A.2 states training datasets were "uniformly generated using GPT-4.1," but notes that "generating the data with the target LLM can be considered to achieve optimal performance."
- Why unresolved: The paper utilizes the cross-model approach for simplicity, leaving the potential accuracy gains from self-generated training data unquantified.

### Open Question 3
- Question: Is the representation-based monitor susceptible to adversarial attacks specifically designed to manipulate intermediate activations to appear benign while generating harmful text?
- Basis in paper: [inferred] The method relies on the assumption that intermediate representations h^l_i reliably correlate with harmful intent (Sec 4.2), and evaluation only covers prompt-based jailbreaks, not representation-space attacks.
- Why unresolved: Adversaries might optimize prompts to produce "safe" activations while forcing the model to output unsafe tokens, bypassing the linear classifier.

## Limitations

- Cross-model representation generalization: The classifier trained on GPT-4-generated data may not generalize effectively to different LLM architectures
- Prompt-specific repair effectiveness: The repair mechanism depends on hand-crafted scenario-specific prompts that are not detailed in the main text
- Real-time streaming constraints: The method's practical performance under varying streaming rates and extreme low-latency requirements is not tested

## Confidence

**High confidence:** The overall mitigation performance (95.72% reduction in UAL, 83.68% in PDL) and low utility overhead (~2%) are well-supported by ablation studies and end-to-end evaluations across four LLMs and three scenarios.

**Medium confidence:** The mechanism of representation-based intent monitoring is plausible and grounded in prior work, but its robustness across arbitrary LLM architectures is inferred rather than proven.

**Low confidence:** The technical details for implementing prefix-frozen generation and the exact prompt structures for repair are not specified, making faithful reproduction uncertain.

## Next Checks

1. **Cross-model transfer robustness:** Retrain the Self-Monitor classifier directly on the target LLM's representations (instead of GPT-4-generated data) for each scenario and compare detection accuracy and FPR.

2. **Repair coherence under stress:** Systematically test repair quality when the frozen prefix is long, when the harmful content is subtle, and when the LLM is instruction-tuned vs. base. Measure BLEU/ROUGE coherence and user-perceived fluency.

3. **Latency scaling under streaming constraints:** Measure end-to-end latency and token throughput for Self-Sanitize under varying token rates (e.g., 10, 30, 60 tokens/second) and monitor window/cache sizes. Identify the point at which the 8% overhead claim breaks down.