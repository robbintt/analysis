---
ver: rpa2
title: 'SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems'
arxiv_id: '2601.16286'
source_url: https://arxiv.org/abs/2601.16286
tags:
- caching
- reuse
- cache
- when
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SemanticALLI introduces a pipeline-aware caching architecture for
  agentic AI systems, addressing the inefficiency of repeatedly reconstructing identical
  intermediate logic. By decomposing generation into Analytic Intent Resolution (AIR)
  and Visualization Synthesis (VS) stages, it caches structured intermediate representations
  (IRs) as first-class artifacts.
---

# SemanticALLI: Caching Reasoning, Not Just Responses, in Agentic Systems

## Quick Facts
- **arXiv ID**: 2601.16286
- **Source URL**: https://arxiv.org/abs/2601.16286
- **Reference count**: 14
- **Primary result**: Structured intermediate representation caching achieved 83.10% VS hit rate vs. 38.7% monolithic, bypassing 4,023 LLM calls with 78.4% token reduction

## Executive Summary
SemanticALLI introduces a pipeline-aware caching architecture for agentic AI systems that caches intermediate reasoning artifacts rather than just final outputs. By decomposing generation into Analytic Intent Resolution (AIR) and Visualization Synthesis (VS) stages, it caches structured intermediate representations as first-class artifacts. In evaluations on 1,000 production prompts, the structured approach enabled VS to reach 83.10% hit rate, bypassing 4,023 LLM calls with a median latency of 2.66 ms and reducing total token consumption by 78.4%. The key insight is that even when user prompts vary, pipelines often reuse stable, structured checkpoints, offering a practical path to improve efficiency in AI system design.

## Method Summary
SemanticALLI implements a three-tier hybrid retrieval engine for caching intermediate reasoning artifacts in agentic pipelines. The system uses (1) exact SHA-256 hash lookup for deterministic duplicates, (2) HNSW-based dense semantic search using OpenAI text-embedding-3-large (3072-dim) with cosine similarity, and (3) BM25 lexical scoring with Reciprocal Rank Fusion (k_rrf=60) reranking. The pipeline decomposes into AIR (prompt + schema → structured intent I) and VS (intent I → visualization directives C) stages. AIR normalizes user prompts into canonical analytic intent definitions, while VS generates executable visualization code from these intents. The architecture targets domains where prompts vary but underlying intents remain stable, enabling reuse of downstream synthesis work.

## Key Results
- VS stage achieved 83.10% exact hit rate vs. AIR's 38.7% semantic hit rate
- Bypassed 4,023 LLM calls with median latency of 2.66 ms on cache hits
- Reduced total token consumption by 78.4% (from 59,906 to 12,964 tokens per prompt)

## Why This Works (Mechanism)

### Mechanism 1: Structured Intermediate Representation (SIR) Caching
Caching intermediate reasoning artifacts captures reuse opportunities that monolithic prompt→output caching misses. The pipeline decomposition into AIR and VS stages produces canonical structured representations (IRs) that are cached as first-class artifacts. When a new prompt arrives, the system checks for reusable intermediate outputs rather than only checking if the full response is cacheable. This works because intermediate representations within agentic pipelines exhibit higher stability and reuse than natural language prompts, even when users phrase requests differently.

### Mechanism 2: Two-Stage Decomposition (AIR + VS)
Separating intent resolution from visualization synthesis creates a structured checkpoint where intent can be reused even when final output generation differs. AIR normalizes user prompts into a canonical Analytic Intent Definition I (metrics, dimensions, filters). VS takes I—not the raw prompt—and generates executable visualization code. This decoupling means two different prompts that resolve to the same I can skip re-computation of downstream VS work. Users frequently request semantically similar intents with different surface phrasing, and the mapping from prompt→intent is many-to-one.

### Mechanism 3: Hybrid Retrieval with Lexical Guardrails
Dense semantic retrieval alone is insufficient for entity-sensitive domains; combining it with exact matching and lexical constraints prevents dangerous near-miss cache hits. The three-tier retrieval uses SHA-256 exact hash for deterministic duplicates, HNSW dense retrieval for semantic neighbors, and BM25 lexical scoring with RRF fusion and reranking to enforce entity-level constraints. A candidate is only returned if it clears similarity threshold τ AND satisfies lexical constraints on critical entities. In enterprise domains, some terms (e.g., "CPM" vs "CPC") are semantically adjacent but operationally distinct, and pure vector similarity will incorrectly conflate them.

## Foundational Learning

- **Semantic caching vs. exact caching**: The paper assumes familiarity with caching strategies that go beyond exact string matching; understanding embedding-based similarity is essential to grasp why monolithic caching fails and how SIRC differs. Quick check: Given two prompts with cosine similarity 0.92 but different business entities, should a semantic-only cache return a hit?

- **Multi-stage agentic pipelines**: SemanticALLI's architecture requires understanding how LLM agents decompose complex tasks into sequential stages with intermediate outputs. Quick check: What is the difference between caching the output of an entire agentic workflow vs. caching an intermediate planning step?

- **Hybrid retrieval (dense + lexical)**: The paper's retrieval engine combines vector search with BM25 and reciprocal rank fusion; engineers must understand why pure semantic search fails in entity-sensitive domains. Quick check: Why might BM25 outperform dense retrieval for distinguishing "CPC" from "CPM"?

## Architecture Onboarding

- **Component map**: (prompt, schema) → AIR → Analytic Intent Definition I → VS → Visualization Directives C → Output
- **Critical path**: 1) Receive user query q and schema S, 2) Check AIR cache via hybrid retrieval; if miss, invoke LLM to generate I, cache I, 3) Check VS cache using serialized I as key; if miss, invoke LLM to generate C, cache C, 4) Return visualization output
- **Design tradeoffs**: Higher τ thresholds reduce false positives but lower hit rates; coarser decomposition (AIR/VS) exposes reuse without excessive cache management overhead; VS uses higher threshold (τ=0.95) because chart type mismatches are unacceptable
- **Failure signatures**: High miss rates at AIR indicate intent representations are too granular; incorrect VS cache hits suggest τ is too low or lexical constraints are missing; latency spikes on cache misses are expected behavior
- **First 3 experiments**: 1) Implement monolithic prompt→output caching with τ=0.90; measure hit rate and latency, 2) Decompose your pipeline into 2-3 stages with structured IRs; instrument cache hit rates per stage, 3) Sample 50 cache hits and manually verify correctness; check for near-miss entity errors

## Open Questions the Paper Calls Out

- Can the Structured Intermediate Representation Caching (SIRC) paradigm be generalized to standard public benchmarks and non-BI agentic workflows? The current evaluation is confined to a proprietary, domain-specific dataset, making direct comparison with other systems impossible.

- What are the individual performance contributions of the dense, lexical, and rank-fusion retrieval components within the hybrid engine? The paper evaluates the hybrid system as a whole and does not isolate which retrieval mechanism drives the majority of successful hits.

- How can the system enable safe cross-tenant reuse when identical terms (e.g., KPIs) carry different semantic definitions across clients? Client-specific isolation prevents "global pattern reuse," and unlocking efficiency requires "safe de-identification or controlled templates."

- What strategies are required to manage cache invalidation when underlying rendering libraries, schemas, or dashboard conventions evolve? The evaluation assumes a relatively static environment, but production systems require mechanisms to ensure cached artifacts remain valid over time.

## Limitations

- Corpus generalizability: Evaluation based on single production dataset from digital media marketing; specific entity vocabulary and query patterns may not generalize to other domains
- Internal mechanism validity: 83.10% vs. 38.7% hit rate asymmetry is compelling but unexplained; unclear whether pattern holds when pipeline structure changes
- Semantic collision risk: Paper acknowledges risk of lowering τ but does not quantify false positive rates or systematically evaluate near-miss errors

## Confidence

- **High confidence**: Basic premise that caching intermediate reasoning artifacts can improve efficiency over monolithic caching; 38.7% vs. 83.10% hit rate comparison is well-supported
- **Medium confidence**: Specific architecture (AIR + VS decomposition) and three-tier hybrid retrieval mechanism; positive results but generalizability to other domains uncertain
- **Low confidence**: Claim that "even when user prompts vary, pipelines often reuse stable, structured checkpoints" extrapolates from one domain without systematic exploration of when assumption breaks down

## Next Checks

1. **Cross-domain replication study**: Apply SemanticALLI's architecture to a different agentic pipeline (e.g., legal document analysis, medical diagnosis assistance) with 500+ prompts. Measure AIR vs. VS hit rates and compare to digital marketing results.

2. **Semantic collision audit at lower thresholds**: Systematically evaluate cache accuracy at τ=0.85 and τ=0.80 by sampling 100+ cache hits and manually verifying correctness. Quantify false positive rates and entity-level errors.

3. **Stage granularity sensitivity analysis**: Implement SemanticALLI with varying pipeline decompositions (2-stage, 3-stage, 4-stage) on the same dataset. Measure hit rates per stage and total efficiency gains to determine whether AIR/VS split is optimal.