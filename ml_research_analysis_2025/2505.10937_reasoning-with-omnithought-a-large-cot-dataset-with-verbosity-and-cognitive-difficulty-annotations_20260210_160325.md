---
ver: rpa2
title: 'Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive
  Difficulty Annotations'
arxiv_id: '2505.10937'
source_url: https://arxiv.org/abs/2505.10937
tags:
- reasoning
- processes
- training
- dataset
- lrms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces OmniThought, a large-scale dataset containing
  2 million chain-of-thought (CoT) processes annotated with Reasoning Verbosity (RV)
  and Cognitive Difficulty (CD) scores. These scores quantify the appropriateness
  of CoT verbosity and the cognitive complexity level for model comprehension.
---

# Reasoning with OmniThought: A Large CoT Dataset with Verbosity and Cognitive Difficulty Annotations

## Quick Facts
- arXiv ID: 2505.10937
- Source URL: https://arxiv.org/abs/2505.10937
- Reference count: 40
- Key outcome: Introduces OmniThought dataset with 2M CoTs annotated with Reasoning Verbosity (RV) and Cognitive Difficulty (CD) scores, achieving state-of-the-art performance on AIME2024, MATH500, GPQA-Diamond, and LiveCodeBench V2 through capacity-aware training

## Executive Summary
OmniThought addresses the challenge of optimal Chain-of-Thought (CoT) verbosity and cognitive complexity alignment in Large Reasoning Model (LRM) training. The dataset contains 2 million CoT processes annotated with novel RV and CD scores that quantify reasoning verbosity appropriateness and cognitive complexity levels. Using a self-reliant pipeline with DeepSeek-R1 and QwQ-32B as teacher models, the authors demonstrate that training models on CoTs matching their cognitive capacity and optimal verbosity levels significantly improves reasoning performance. The resulting LRMs (7B and 32B versions) outperform existing distilled models on challenging mathematical and coding benchmarks while maintaining efficiency through optimal CoT length.

## Method Summary
The method involves a multi-stage pipeline: (1) problem collection from OpenThoughts2-1M and DeepMath-103K, (2) multi-teacher CoT generation using DeepSeek-R1 and QwQ-32B, (3) LLM-as-judge validation with retention of incorrect-answer CoTs for preference optimization, (4) RV/CD scoring through LLM judgment and token normalization, and (5) capacity-aware probabilistic sampling based on cognitive difficulty alignment and CD-RV coherence. The sampled CoTs are used for Supervised Fine-Tuning (SFT) on Qwen2.5 models with specific learning rates (8e-5 for 7B) and batch sizes (512), followed by preference optimization using RV-based chosen/rejected pairs.

## Key Results
- OmniThought dataset contains 2M CoTs with RV and CD annotations across mathematical reasoning tasks
- 7B and 32B Qwen2.5 models trained on OmniThought achieve state-of-the-art performance on AIME2024, MATH500, GPQA-Diamond, and LiveCodeBench V2
- RV optimization reduces overthinking on simple problems while maintaining depth for complex reasoning tasks
- CD-capacity alignment enables effective knowledge transfer from teacher models to student models of varying sizes
- Preference optimization using RV-based pairs further improves accuracy while reducing token count

## Why This Works (Mechanism)

### Mechanism 1: Reasoning Verbosity (RV) Alignment
- Claim: CoT length should match problem difficulty to maximize training effectiveness
- Mechanism: RV score (0-9 scale) combines LLM-judged verbosity with normalized token count, ensuring optimal verbosity matching
- Core assumption: Optimal CoT length correlates with problem difficulty for effective learning
- Evidence anchors: RV ablation shows medium RV optimal for MATH500, high RV best for AIME24, all levels similar on GSM8K

### Mechanism 2: Cognitive Difficulty (CD) Matching
- Claim: CoT cognitive difficulty should align with student model capacity for effective knowledge transfer
- Mechanism: CD score (0-9) rates methodological complexity, with models trained on CD-appropriate CoTs showing superior performance
- Core assumption: Smaller models have fundamentally different cognitive trajectories and cannot learn graduate-level abstractions effectively
- Evidence anchors: DeepSeek-R1-Distill models show ascending average CD scores (1.5B=4.5, 7B=6.2, 32B=7.3), optimal μCD varies by model size

### Mechanism 3: Combined RV-CD Selection with Probability Sampling
- Claim: Jointly optimizing RV and CD via probabilistic sampling produces superior training data than either dimension alone
- Mechanism: Selection probability combines CD-capacity alignment and CD-RV coherence, penalizing incoherent CoTs
- Core assumption: Verbosity and difficulty are correlated but distinct dimensions requiring joint optimization
- Evidence anchors: Combined selection achieves 64.25 average vs. 46.18 (RV-only) and 48.28 (CD-only) on 4 benchmarks

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Reasoning**
  - Why needed here: The entire dataset and training paradigm assumes intermediate reasoning steps improve model capabilities
  - Quick check question: Can you explain why "1+1=2" requires no CoT but proving infinite primes does?

- Concept: **Knowledge Distillation from Teacher Models**
  - Why needed here: OmniThought uses DeepSeek-R1 and QwQ-32B as teachers; understanding teacher-student capacity gaps is essential for CD calibration
  - Quick check question: Why might a 32B teacher's reasoning process fail to transfer to a 1.5B student?

- Concept: **Supervised Fine-Tuning (SFT) vs. Preference Optimization (DPO)**
  - Why needed here: The paper demonstrates both SFT on selected CoTs and DPO using RV-based chosen/rejected pairs
  - Quick check question: How would you construct a DPO preference pair from high-RV vs. optimal-RV CoTs for the same problem?

## Architecture Onboarding

- Component map:
  Source Collector -> CoT Generator (DeepSeek-R1 + QwQ-32B) -> CoT Validator (LLM-as-judge) -> Score Calculator (RV/CD) -> CoT Sampler (probability-based)

- Critical path:
  1. Problem ingestion → 2. Multi-teacher CoT generation → 3. Validation (retain incorrect-answer CoTs for DPO) → 4. Score annotation → 5. Capacity-aware sampling → 6. SFT training

- Design tradeoffs:
  - Validation strictness: Retaining logically-flawed CoTs enables DPO but risks SFT contamination
  - α parameter in RV: Paper sets α=0.5 for token-judgment fusion; higher α prioritizes token count (faster but may miss semantic verbosity)
  - β parameter in sampling: β=0.5 balances CD-alignment vs. coherence; higher β favors cognitive matching

- Failure signatures:
  - Overthinking on simple tasks: Model generates 5000+ tokens for "2+3" - RV filter may be too permissive
  - Underperformance on hard benchmarks: Check if μCD was set too low (e.g., μCD=3 for 32B model caused 7.7% average drop)
  - High variance across benchmarks: CD-RV mismatch in training data; verify coherence penalty is active

- First 3 experiments:
  1. RV ablation: Train 7B model on low-RV (0-2), medium-RV (4-5), high-RV (7-9) subsets; evaluate on GSM8K/MATH500/AIME24 to reproduce Figure 4 patterns
  2. CD calibration sweep: For your target model size, train with μCD ∈ {3,5,7,9} and identify peak performance point (replicate Table 6 methodology)
  3. DPO on RV preference: Starting from SFT checkpoint, apply DPO with chosen=optimal-RV and rejected=max-RV pairs; measure accuracy and token reduction (replicate Table 4)

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding dataset expansion to broader domains, dynamic reasoning proficiency assessments, and extending annotations beyond mathematical reasoning tasks.

## Limitations
- Self-referential validation through LLM-as-judge without human-annotated validation sets creates potential circularity
- Static μCD parameters may not generalize across different model architectures or pretraining histories
- Performance demonstration limited to mathematical reasoning benchmarks without broader domain validation
- Potential overfitting to mathematical problem structures in the RV/CD framework

## Confidence

**High Confidence**: RV score's positive impact on simple tasks (GSM8K) is well-demonstrated through ablation studies showing consistent performance across RV levels

**Medium Confidence**: CD-capacity alignment mechanism shows strong empirical support but optimal values appear specific to Qwen2.5 architecture

**Low Confidence**: Combined RV-CD probability sampling's superiority over individual dimensions may be partially attributed to specific benchmark selection

## Next Checks

1. **Human Validation Study**: Recruit subject matter experts to independently score 100 randomly sampled CoTs from OmniThought on both RV and CD dimensions. Calculate inter-annotator agreement and compare against LLM scores to quantify annotation reliability.

2. **Architecture Transfer Experiment**: Apply the OmniThought dataset to train a different model family (e.g., Llama, Mistral) using the same μCD parameters. Compare whether the optimal μCD values shift, indicating architecture-specific calibration requirements.

3. **Domain Generalization Test**: Evaluate trained LRMs on non-mathematical reasoning benchmarks including scientific reasoning datasets (e.g., StrategyQA, CommonSenseQA) and logical inference tasks (e.g., LogiQA). This will reveal whether RV/CD optimization transfers beyond mathematical problem structures.