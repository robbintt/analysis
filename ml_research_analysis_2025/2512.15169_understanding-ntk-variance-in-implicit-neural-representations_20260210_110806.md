---
ver: rpa2
title: Understanding NTK Variance in Implicit Neural Representations
arxiv_id: '2512.15169'
source_url: https://arxiv.org/abs/2512.15169
tags:
- variance
- similarity
- spectral
- normalization
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work analyzes how architectural choices in implicit neural
  representations (INRs) influence Neural Tangent Kernel (NTK) conditioning and spectral
  bias. The authors derive a closed-form NTK variance decomposition in terms of input
  and hidden feature similarities and scaling terms.
---

# Understanding NTK Variance in Implicit Neural Representations

## Quick Facts
- arXiv ID: 2512.15169
- Source URL: https://arxiv.org/abs/2512.15169
- Reference count: 40
- This work analyzes how architectural choices in implicit neural representations (INRs) influence Neural Tangent Kernel (NTK) conditioning and spectral bias through closed-form variance decomposition.

## Executive Summary
This paper provides a unified framework for understanding how architectural choices in implicit neural representations affect NTK conditioning and spectral bias. The authors derive a closed-form NTK variance decomposition in terms of input and hidden feature similarities and scaling terms. They show that positional encoding reshapes input geometry, spherical normalization contracts variance via layerwise scaling, and Hadamard modulation introduces bounded similarity factors that multiplicatively reduce variance. Experiments on CT reconstruction and image super-resolution confirm that these components systematically reduce NTK eigenvalue variance, leading to faster, more stable convergence and improved reconstruction fidelity.

## Method Summary
The method implements a two-layer Normalized Hadamard ReLU Network with Top-K Spherical Normalization. The architecture maps inputs through Random Fourier Features (RFF) positional encoding, applies ReLU activations, normalizes by hidden energy, and incorporates Hadamard coordinate-dependent modulation. The NTK is computed via Jacobian outer products on small coordinate subsets to avoid memory issues. Training uses MSE loss with unspecified optimizer settings (likely Adam with learning rate scheduler).

## Key Results
- Positional encoding reduces NTK variance by contracting input similarity τx compared to raw coordinates
- Spherical normalization replaces uncontrolled hidden energy scale with fixed ~1, reducing variance
- Hadamard modulation introduces bounded similarity factors strictly below one, yielding multiplicative variance reduction
- Empirical validation shows PSNR up to 45.32 dB on super-resolution with TopK normalization and modulation

## Why This Works (Mechanism)

### Mechanism 1: Positional Encoding Reduces Input Similarity
RFF positional encoding reduces the average off-diagonal input similarity τx,ij compared to raw coordinates. RFF maps inputs to a high-dimensional space where inner products decay exponentially with distance (via Gaussian kernel behavior). This contracts the squared-cosine similarity mass Σi≠j τx,ij. Break condition: If bandwidth ς → 0, encoding collapses to identity; if d is too small, approximation degrades.

### Mechanism 2: Spherical Normalization Contracts Hidden Energy
ℓ2-normalization of hidden activations reduces NTK variance by replacing the uncontrolled energy scale S²_bl with a fixed ~1. Normalization enforces ∥si∥² = 1, removing the m/2-scale hidden energy factor. TopK-SP further masks low-magnitude channels, reducing energy-weighted hidden similarity. Break condition: If normalization is applied after ReLU but inputs are degenerate (all-zero activations), division by zero occurs.

### Mechanism 3: Hadamard Modulation Introduces Bounded Similarity Factors
Coordinate-dependent Hadamard modulation adds two similarity factors (τp, τq) that are strictly <1 off-diagonal, yielding multiplicative variance reduction. Modulation vector pi with tanh-scaling creates channel-wise reweighting. For i≠j, the overlap ∥pi⊙pj∥² < ∥pi∥²∥pj∥², and alignment κij decays with channel mixing. Break condition: If modulation pattern is constant across samples (pi = pj), then τp,ij = 1 and no variance reduction occurs.

## Foundational Learning

- **Neural Tangent Kernel (NTK)**: The entire analysis hinges on how architectural choices affect NTK eigenvalue distribution and variance. Quick check: Can you explain why NTK eigenvalue variance controls convergence rate disparity across spectral modes?

- **Spectral Bias**: This is the problem the paper aims to solve—slow learning of high-frequency components due to imbalanced NTK spectrum. Quick check: Why does a large spread in NTK eigenvalues cause some frequencies to converge much slower than others?

- **Similarity Factors (τx, τs, τp, τq)**: The paper decomposes NTK variance into four interpretable similarity factors; understanding each is essential for architectural decisions. Quick check: For each similarity factor, can you identify which architectural component primarily controls it?

## Architecture Onboarding

- Component map: Input x → [Positional Encoding γ(·)] → Ẽx → [Linear + ReLU + Spherical Norm] → y_ℓ → [Hadamard Modulation ⊙ p_ℓ] → output

- Critical path: Start with baseline ReLU MLP → add PE → add SP normalization → add Hadamard modulation. Measure NTK variance at each step.

- Design tradeoffs:
  - Bandwidth ς: Larger ς → smaller input similarity but risk overfitting high-frequency noise
  - TopK sparsity k: Smaller k → more variance reduction but risk losing expressivity if k < O(log m)
  - Modulation pattern: Structured (sinusoidal) vs. random; affects τp decay rate

- Failure signatures:
  - PSNR stalls below 30 dB on 2D image tasks → check if PE bandwidth is too small
  - Training diverges → check normalization stability (avoid zero denominators)
  - No improvement over baseline → verify modulation is coordinate-dependent, not constant

- First 3 experiments:
  1. Replicate Table 1: Measure µλ, vλ, and similarity masses (Pτx, Pτs, Pτp, Pτq) at initialization for base, PE, SP, and Hadamard variants.
  2. Ablation on bandwidth ς: Plot PSNR at epoch 1000 vs. ς ∈ {0.1, 1, 10, 100} for fixed d=256.
  3. TopK sweep: For k ∈ {m/8, m/4, m/2, m}, measure vλ reduction ratio and final PSNR to validate the k ≥ c₀ log m stability threshold.

## Open Questions the Paper Calls Out

### Open Question 1
Does the relationship between low NTK variance and fast convergence hold in non-lazy training regimes where the kernel evolves significantly during optimization? The paper proves convergence under a frozen NTK, but many effective INRs may operate in feature-learning regimes where the kernel drifts, violating the theoretical stability bounds. Experiments tracking NTK evolution and convergence rates in architectures known for feature learning would resolve this.

### Open Question 2
Is there a theoretical optimum for the sparsity parameter k in TopK-SP that balances stability guarantees against reconstruction fidelity? While the paper derives bounds showing variance reduction depends on p̄, it does not derive a closed-form optimal value, leaving the choice to heuristics. A derivation minimizing the generalization error bound or empirical validation across a dense grid of k/m ratios would resolve this.

### Open Question 3
Does the variance-reduction mechanism limit the model's ability to represent sharp high-frequency details (spectral bias mitigation vs. expressivity)? The paper focuses on optimizing conditioning for convergence, but does not explicitly analyze if aggressive variance suppression acts as a bottleneck on the network's representational capacity. Comparative analysis of reconstruction error specifically on high-frequency bands for models with artificially constrained variance would resolve this.

## Limitations
- The TopK-SP analysis assumes k ≥ c₀ log m for stability, but empirical verification across different problem scales is not fully demonstrated
- Hadamard modulation's effectiveness depends on coordinate-dependent patterns, but the generation mechanism for p_r is underspecified
- The claim of improved conditioning through RFF bandwidth is asymptotic; finite-sample effects at practical ς values are not quantified

## Confidence

- **High**: NTK variance decomposition mechanics, PE similarity reduction bounds, spherical normalization variance contraction
- **Medium**: Hadamard modulation bounds (theoretical but dependent on pattern generation assumptions)
- **Medium**: Empirical convergence improvements (PSNR gains are demonstrated but optimizer settings are unspecified)

## Next Checks

1. **TopK Stability Sweep**: Systematically vary k across [m/8, m/2] and measure both NTK variance reduction and reconstruction fidelity to validate the k ≥ c₀ log m threshold

2. **Modulation Pattern Sensitivity**: Compare performance across different p_r generation strategies (random binary vs. sinusoidal vs. learned) to verify the coordinate-dependence requirement

3. **RFF Bandwidth Calibration**: For each task (CT, SR, image fitting), identify the optimal ς range where PSNR gains plateau and NTK variance reduction is maximal