---
ver: rpa2
title: LLMs as Architects and Critics for Multi-Source Opinion Summarization
arxiv_id: '2507.04751'
source_url: https://arxiv.org/abs/2507.04751
tags:
- evaluation
- opinion
- product
- summaries
- m-os
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Multi-Source Opinion Summarization (M-OS),
  which integrates product metadata with customer reviews to generate comprehensive
  summaries. The authors developed M-OS-EVAL, a benchmark dataset for evaluating summaries
  across seven dimensions using two prompting frameworks: OMNI-PROMPT for metric-independent
  assessment and SPECTRA-PROMPTS for dimension-specific evaluation.'
---

# LLMs as Architects and Critics for Multi-Source Opinion Summarization

## Quick Facts
- arXiv ID: 2507.04751
- Source URL: https://arxiv.org/abs/2507.04751
- Authors: Anuj Attri; Arnav Attri; Pushpak Bhattacharyya; Suman Banerjee; Amey Patil; Muthusamy Chelliah; Nikesh Garera
- Reference count: 27
- Key outcome: M-OS summaries integrating product metadata with reviews are preferred by users (87% preference) over traditional opinion summaries.

## Executive Summary
This paper introduces Multi-Source Opinion Summarization (M-OS), a framework that combines product metadata with customer reviews to generate comprehensive summaries. The authors developed M-OS-EVAL, a benchmark dataset for evaluating summaries across seven dimensions using two prompting frameworks: OMNI-PROMPT for metric-independent assessment and SPECTRA-PROMPTS for dimension-specific evaluation. Experiments with 14 LLMs demonstrated that larger models like Qwen2.5-72B-Instruct achieve the best performance, with strong alignment to human judgment (average Spearman correlation: 0.74). A user study showed that 87% of participants preferred M-OS summaries over traditional opinion summaries.

## Method Summary
The method involves zero-shot generation of multi-source summaries using M-OS-GEN-PROMPT with product metadata (title, description, features, specifications, reviews, ratings) as context. Evaluation is performed using two prompting frameworks: OMNI-PROMPT for flexible cross-dimensional assessment and SPECTRA-PROMPTS for precise per-dimension evaluation. The M-OS-DATA benchmark contains 25,000 products with an average of 10 reviews per product, while M-OS-EVAL provides 4,900 human annotations across 7 dimensions. LLM evaluators score generated summaries using weighted probability averaging over n≈100 samples, with performance validated against human judgments.

## Key Results
- Larger models (Qwen2.5-72B-Instruct) achieve superior performance in generating coherent and comprehensive M-OS summaries
- M-OS-PROMPTS exhibit strong alignment with human judgment, achieving average Spearman correlation of ρ = 0.74
- 87% of users preferred M-OS summaries over traditional opinion summaries in user study
- Structured evaluation prompts (OMNI-PROMPT and SPECTRA-PROMPTS) outperform baseline approaches in reducing score inflation and improving consistency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating product metadata with customer reviews produces summaries that users prefer over review-only summaries.
- Mechanism: LLMs with extended context windows ingest specifications, descriptions, and ratings alongside reviews, then synthesize unified outputs that surface objective attributes with subjective feedback.
- Core assumption: Users value combined technical and experiential information in a single summary for decision-making.
- Evidence anchors:
  - [abstract] "This integration results in comprehensive summaries that capture both subjective opinions and objective product attributes essential for informed decision-making."
  - [section] Table 1 shows M-OS including technical specifications absent from opinion-only summaries.
  - [corpus] Related work (Siledar et al., 2024b; Zhao and Chaturvedi, 2020) also leverages additional sources, though with less comprehensive metadata.
- Break condition: If users in a domain prefer concise subjective-only summaries and ignore specifications, M-OS adds noise without value.

### Mechanism 2
- Claim: Structured evaluation prompts align LLM evaluators more closely with human judgments than prior prompting approaches.
- Mechanism: Prompts enforce explicit evaluation steps, anchored scoring ranges, and structured justification, reducing score inflation and improving consistency across evaluators.
- Core assumption: LLMs can follow multi-step evaluation protocols that mirror human annotation guidelines.
- Evidence anchors:
  - [abstract] "M-OS-PROMPTS exhibit stronger alignment with human judgment, achieving an average Spearman correlation of ρ = 0.74."
  - [section] Tables 4 and 6 show OMNI-GPT-4o and SPECTRA-GPT-4o outperforming baseline prompts across most dimensions.
  - [corpus] Prior work (Liu et al., 2023; Fu et al., 2023) demonstrates LLMs as viable NLG evaluators when prompted with structured criteria.
- Break condition: If domain-specific nuances require expertise beyond prompt specifications, LLM alignment with humans degrades.

### Mechanism 3
- Claim: Larger models generate higher-quality M-OS summaries, particularly on specificity and coherence dimensions.
- Mechanism: Increased parameter count enables better capture of nuanced relationships across extended product metadata and reviews, improving factual integration and narrative flow.
- Core assumption: Model scale correlates with capacity to synthesize heterogeneous information sources without hallucination.
- Evidence anchors:
  - [abstract] "Experiments with 14 LLMs showed that larger models like Qwen2.5-72B-Instruct achieve the best performance."
  - [section] Section 6.1: "We observe a clear correlation between model size and performance. Larger models demonstrate superior performance in generating coherent and comprehensive summaries."
  - [corpus] Weak direct corpus evidence; related work focuses on summarization methods rather than scale effects.
- Break condition: If metadata volume exceeds even large model effective context utilization, performance gains plateau.

## Foundational Learning

- Concept: **Reference-free evaluation with LLMs**
  - Why needed here: Traditional metrics like ROUGE correlate poorly with human judgment for opinion summarization and cannot assess multi-source faithfulness.
  - Quick check question: Can you explain why a high ROUGE score does not guarantee summary faithfulness to product specifications?

- Concept: **Metric-independent vs metric-dependent prompting**
  - Why needed here: OMNI-PROMPT provides flexibility across dimensions with a single structure; SPECTRA-PROMPTS offers precision per dimension at the cost of maintaining multiple prompts.
  - Quick check question: When would you choose OMNI-PROMPT over SPECTRA-PROMPTS for a production evaluation pipeline?

- Concept: **Inter-rater agreement and Krippendorff's alpha**
  - Why needed here: M-OS-EVAL relies on human annotations; understanding agreement levels (moderate α=0.70 to substantial α=0.86) is critical for interpreting benchmark reliability.
  - Quick check question: What does an increase from α=0.70 to α=0.86 after annotation adjudication indicate about evaluation framework robustness?

## Architecture Onboarding

- Component map:
  M-OS-DATA (25K products with metadata) -> M-OS-GEN-PROMPT (generation instructions) -> M-OS-EVAL (4,900 annotations) -> OMNI-PROMPT/SPECTRA-PROMPTS (evaluation frameworks) -> LLM evaluator -> Spearman correlation with human judgments

- Critical path:
  1. Ingest product metadata and reviews into unified prompt context
  2. Generate M-OS with target LLM using M-OS-GEN-PROMPT
  3. Evaluate generated summaries with LLM evaluator using OMNI or SPECTRA
  4. Compute correlation with human annotations to validate evaluator alignment

- Design tradeoffs:
  - Open-source (Qwen2.5-72B-Instruct, Llama-3.1-70B) vs closed-source (GPT-4o): Open-source offers cost control and data privacy; closed-source provides higher average alignment
  - OMNI-PROMPT vs SPECTRA-PROMPTS: OMNI is easier to maintain; SPECTRA offers higher precision per dimension but requires seven separate prompts
  - Model size vs latency: 70B+ models achieve top scores but incur higher inference costs; 7B models may suffice for fluency but struggle with specificity

- Failure signatures:
  - Truncation of metadata when using models with limited context windows (e.g., T5, BART)
  - Score inflation with baseline prompts lacking structured scoring ranges
  - Lower specificity scores when models cannot integrate detailed technical specifications
  - Hallucinations when metadata is sparse or contradictory

- First 3 experiments:
  1. Replicate OMNI-PROMPT vs OP-I-PROMPT comparison on a held-out product category to validate generalization beyond reported domains
  2. Ablate metadata sources (remove specifications only, remove ratings only) to measure contribution per source to faithfulness and specificity
  3. Test open-source evaluator (Llama-3.1-70B-Instruct) on M-OS-EVAL with temperature scaling to assess sensitivity to sampling parameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the M-OS framework be effectively extended to incorporate temporal patterns in reviews and multi-modal content like images?
- Basis in paper: [explicit] The Conclusion states the current "approach needs expansion to handle temporal patterns and multi-modal content in modern e-commerce."
- Why unresolved: The current study focused strictly on static text-based metadata and reviews, omitting time-series data or visual inputs.
- What evidence would resolve it: Results from a modified framework processing time-stamped reviews and product images, showing maintained or improved summary fidelity.

### Open Question 2
- Question: Do the OMNI-PROMPT and SPECTRA-PROMPTS maintain high alignment with human judgment across non-English languages and diverse cultural contexts?
- Basis in paper: [explicit] The Conclusion notes that evaluation prompts "require further testing across languages and cultures, as opinion expression varies globally."
- Why unresolved: The experiments were restricted to a specific linguistic context (implied English), leaving cross-cultural robustness unverified.
- What evidence would resolve it: Benchmarks showing consistent Spearman correlations (comparable to the reported 0.74) when applying the prompts to multilingual datasets.

### Open Question 3
- Question: How does LLM performance scale when generating summaries from complete review corpora rather than the limited subsets used in M-OS-EVAL?
- Basis in paper: [explicit] The Conclusion identifies the need to "explore LLMs for large-scale summarization and processing complete review corpora."
- Why unresolved: The evaluation dataset was limited to 10 reviews per product, which may not reflect the complexity of real-world high-volume scenarios.
- What evidence would resolve it: Performance metrics (e.g., coherence, faithfulness) on datasets containing hundreds of reviews per product.

### Open Question 4
- Question: Would other leading proprietary models, such as Claude 3.5 Sonnet, outperform the open-source leaders (e.g., Qwen2.5-72B) in M-OS tasks?
- Basis in paper: [inferred] The Limitations section states the authors did not include Claude 3.5 Sonnet due to budget limitations, restricting the comparison of proprietary architectures.
- Why unresolved: Resource constraints limited the scope of "Architect" and "Critic" model comparisons to GPT-4o and open-source alternatives.
- What evidence would resolve it: Benchmarking results on M-OS-EVAL specifically for Claude 3.5 Sonnet compared against the current state-of-the-art open-source models.

## Limitations

- Dataset Generalizability: M-OS-DATA is constructed from a specific e-commerce platform (Flipkart), raising concerns about cross-domain transferability to domains like healthcare or finance.
- Human Evaluation Reliability: Moderate initial inter-annotator agreement (α = 0.70) suggests inherent subjectivity in dimensions like "specificity" and "aspect coverage."
- Context Window Constraints: The paper does not quantify performance degradation when specifications (average 242.6 words) are partially excluded due to context window limitations.

## Confidence

- **High Confidence**: Claims about larger models achieving superior performance are well-supported by systematic experiments across 14 models and seven evaluation dimensions.
- **Medium Confidence**: The assertion that M-OS summaries are "preferred" by users is supported by a single user study (87% preference rate) but lacks longitudinal or cross-domain validation.
- **Low Confidence**: The claim that structured evaluation prompts "reduce score inflation" compared to baseline approaches is based on internal comparisons without benchmarking against established NLG evaluation metrics or independent third-party judgments.

## Next Checks

1. **Cross-Domain Generalization Test**: Apply M-OS-GEN-PROMPT to product summaries from non-e-commerce domains (e.g., medical device reviews, financial product feedback) and measure performance degradation across evaluation dimensions.

2. **Ablation Study on Metadata Sources**: Systematically remove each metadata component (specifications, ratings, features) and quantify the impact on faithfulness scores to determine which sources contribute most to summary quality.

3. **Real-World Impact Validation**: Conduct a randomized controlled trial where users make product decisions using either traditional opinion summaries or M-OS summaries, measuring decision accuracy and time-to-decision as outcome metrics.