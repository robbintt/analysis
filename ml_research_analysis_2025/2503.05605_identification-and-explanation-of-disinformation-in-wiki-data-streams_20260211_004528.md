---
ver: rpa2
title: Identification and explanation of disinformation in wiki data streams
arxiv_id: '2503.05605'
source_url: https://arxiv.org/abs/2503.05605
tags:
- data
- disinformation
- features
- content
- wiki
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a scalable solution for identifying and explaining
  disinformation in wiki data streams. The system uses stream-based data processing
  with feature engineering, feature analysis and selection, stream-based classification,
  and real-time explanation of prediction outcomes.
---

# Identification and explanation of disinformation in wiki data streams

## Quick Facts
- **arXiv ID**: 2503.05605
- **Source URL**: https://arxiv.org/abs/2503.05605
- **Reference count**: 40
- **Primary result**: Scalable system achieves ~90% accuracy detecting disinformation in wiki streams using multi-dimensional features and LLM explanations

## Executive Summary
This work proposes a scalable solution for identifying and explaining disinformation in wiki data streams. The system uses stream-based data processing with feature engineering, feature analysis and selection, stream-based classification, and real-time explanation of prediction outcomes. Two experimental datasets were used: Wikivoyage (285,762 entries) and Wikipedia (40,372 entries). The system achieved approximately 90% values across all evaluation metrics, demonstrating robust and competitive performance compared to existing works. The explainability dashboard provides natural language descriptions of predictions using an LLM, making the model interpretable for the general public.

## Method Summary
The system implements a stream-based classification pipeline using the River framework. It preprocesses wiki revisions through text normalization, extracts 89 engineered features across content, side, and historical dimensions, and applies variance-based feature selection. An Adaptive Random Forest Classifier performs incremental training with delayed updates (every 100 samples). The system generates natural language explanations via GPT-3.5-turbo by extracting decision paths from tree models. Performance is evaluated through prequential testing on Wikivoyage and Wikipedia datasets, measuring accuracy, precision, recall, F-measure, and processing time.

## Key Results
- Achieved approximately 90% accuracy across evaluation metrics on both Wikivoyage and Wikipedia datasets
- ARFC classifier outperforms alternatives (GNB, ALMA, HAT) with 87.67-92.40% accuracy vs 50.64-85.53% for others
- Processes 45-62 samples/second with 0.02s average prediction time
- Feature variance thresholding reduces 89 features to 9-24 active features per stream

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-dimensional feature engineering (content + side + historical) improves disinformation detection accuracy over content-only approaches.
- Mechanism: The system extracts 89 features across three categories: (i) engineered side features (POS ratios, reading time, readability scores), (ii) content features (emotion/polarity load, word2vec embeddings, n-grams), and (iii) historical features (incremental user/page statistics). This multi-view representation captures both linguistic patterns and behavioral signals that single-modality approaches miss.
- Core assumption: Disinformation exhibits distinguishable patterns across content, metadata, and user behavior dimensions that remain relatively stable during streaming updates.
- Evidence anchors:
  - [abstract] "feature engineering, feature analysis and selection, stream-based classification"
  - [section 3.1.1] Tables 2-3 list 89 features; Section 4.2.1 details implementation including textstat, spaCy, word2vec
  - [corpus] Weak corpus support; related works focus on detection methods but not specifically on multi-dimensional feature combinations
- Break condition: If feature variance thresholding discards >80% of engineered features (paper discarded 65-80/89), or if historical features become stale due to concept drift, performance degrades.

### Mechanism 2
- Claim: Tree-based ensemble classifiers (Adaptive Random Forest) achieve higher stability and accuracy than Gaussian or hyperplane-based models in streaming scenarios with imbalanced data.
- Mechanism: ARFC uses an ensemble of Hoeffding Adaptive Trees with majority voting, enabling incremental updates per sample while handling class imbalance. Tree models also provide intrinsic interpretability for downstream explanation generation.
- Core assumption: Decision boundaries for disinformation detection can be approximated by axis-aligned splits that update incrementally without catastrophic forgetting.
- Evidence anchors:
  - [abstract] "achieved approximately 90% values across all evaluation metrics"
  - [section 4.3] ARFC achieves 87.67-92.40% accuracy across scenarios vs. 50.64-85.53% for other models; processes 45-62 samples/second
  - [corpus] Neighbor papers on stream-based monitoring (arXiv:2501.18331) support streaming feasibility but don't validate this specific model choice
- Break condition: If streaming data exhibits rapid concept drift that violates Hoeffding bounds, or if training delay (scenario 3: 100 samples) exceeds adaptation capacity, accuracy drops to GNB/ALMA levels (~50-65%).

### Mechanism 3
- Claim: LLM-generated natural language explanations from tree decision paths make predictions interpretable to non-experts.
- Mechanism: The system extracts decision paths from ARFC trees, identifies the most relevant features via variance selection, and uses GPT-3.5-turbo with prompt engineering to generate human-readable explanations. The dashboard visualizes feature quartiles and allows expert-in-the-loop validation.
- Core assumption: LLM can faithfully translate feature-level decision logic into coherent natural language without hallucination or misrepresentation.
- Evidence anchors:
  - [abstract] "explainability dashboard provides natural language descriptions of predictions using an LLM, making the model interpretable"
  - [section 4.4] Details prompt structure and dashboard; Fig. 6-7 show visual/textual explanation outputs
  - [corpus] PCoT paper (arXiv:2506.06842) supports LLM+CoT for disinformation detection but doesn't validate explanation faithfulness
- Break condition: If LLM explanations diverge from actual feature contributions, or if users cannot validate explanations via the expert-in-the-loop interface, trust degrades without detection.

## Foundational Learning

- **Stream-based (online) learning vs. batch learning**
  - Why needed here: The system updates models incrementally as wiki edits arrive, requiring understanding of prequential evaluation (predict-then-train) and concept drift handling.
  - Quick check question: Can you explain why the system predicts on each sample before training on it, and what happens if the 100-sample delay in scenario 3 introduces distribution shift?

- **Feature variance thresholding for streaming feature selection**
  - Why needed here: With 89 engineered features, dynamic selection prevents overfitting and computational bloat. The variance threshold (0.067 for Wikivoyage, 0.022 for Wikipedia) determines which features persist.
  - Quick check question: How would you detect if a previously discarded feature becomes predictive after concept drift?

- **Tree-based model interpretability (decision paths, feature importance)**
  - Why needed here: The explainability pipeline depends on extracting decision paths from Hoeffding trees to feed the LLM explanation generator.
  - Quick check question: Given an ARFC with 50 estimators, how would you aggregate decision paths to identify the top-3 most influential features for a prediction?

## Architecture Onboarding

- **Component map**: Wiki API (MediaWiki) → Stream Simulator (timestamp-ordered) → Text Preprocessing (regex, stopword removal, lemmatization via spaCy) → Feature Engineering (89 features: side/content/historical) → Feature Selection (VarianceThreshold, dynamic per-stream) → Stream Classifier (ARFC primary; GNB/ALMA/HATC alternatives) → Explainability Module (decision path extraction → GPT-3.5-turbo prompt) → Dashboard (feature quartiles, decision tree viz, natural language explanation) → Expert-in-the-Loop (human validation → reinforcement learning feedback)

- **Critical path**: 1) Cold start phase (0.5% initial samples) for hyperparameter optimization and variance threshold calibration 2) Per-sample prediction → feature selector update → model incremental training 3) Decision path extraction → LLM prompt → natural language explanation → dashboard display

- **Design tradeoffs**: ARFC vs. deep learning: Paper chose ARFC (88-92% accuracy, interpretable) over BERT-based alternatives (69-86% in literature, black-box). Tradeoff: potentially lower ceiling accuracy for real-time interpretability. Streaming vs. batch: Streaming enables real-time detection (0.02s/sample) but requires careful concept drift handling. Scenario 3's 100-sample delay balances responsiveness with training stability. Feature count vs. computational cost: 89 features reduced to 9-24 active features after variance filtering. Historical features (20-99) add computational overhead but capture user/page behavioral patterns.

- **Failure signatures**: Accuracy drops to ~50% (random baseline): Likely ALMA/GNB on imbalanced data; check feature variance threshold recalibration. Processing time >1s/sample: Historical feature computation bottleneck; consider caching user/page aggregates. LLM explanations incoherent or contradictory: Prompt engineering failure; validate feature-to-explanation mapping. High false positive rate on short articles: Feature `wp10stub` identified as failure mode in Section 4.3; insufficient content for reliable prediction.

- **First 3 experiments**: 1) Baseline replication: Run GNB/ALMA/HATC/ARFC on Wikivoyage scenario 2 (balanced streaming) and compare accuracy, recall, and processing time against Table 6 results. Validate ~88% ARFC accuracy and 2701s total time. 2) Feature ablation: Remove historical features (IDs 20-99) and measure accuracy degradation. Hypothesis: ~5-10% drop due to loss of user/page behavioral signals. 3) Explanation faithfulness test: For 50 random predictions, manually compare LLM-generated explanations against extracted decision paths. Measure: (a) feature coverage, (b) logical consistency, (c) hallucination rate.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does integrating claim verification via evidence retrieval improve the precision of disinformation detection in wiki streams without compromising real-time processing latency?
- Basis in paper: [explicit] The authors state in the conclusion that "Future work will leverage the system's modular design to integrate claim verification through evidence retrieval."
- Why unresolved: The current system relies on feature engineering and stream-based classification but lacks a mechanism to actively verify claims against external sources, which is identified as a necessary extension.
- What evidence would resolve it: Experimental results comparing the current model's performance (accuracy and processing time) against a modified version that includes an evidence retrieval module.

### Open Question 2
- Question: Can a dynamic adjustment mechanism for the variance threshold better mitigate concept drift compared to the current static cold-start approach?
- Basis in paper: [explicit] The authors propose re-evaluating "the dynamic adjustment of the variance threshold for feature analysis and selection to address potential changes in feature importance over time (i.e., concept drift)."
- Why unresolved: The current implementation relies on a variance threshold calculated during the initial cold start (0.5% of samples), which may not remain optimal as data streams evolve.
- What evidence would resolve it: A comparative analysis of model stability and accuracy over time using dynamic thresholding versus the static threshold method on a longitudinal dataset.

### Open Question 3
- Question: Do features engineered by Large Language Models (LLMs) provide superior classification performance compared to the manually crafted content and side features currently used?
- Basis in paper: [explicit] The paper notes that "using an llm for feature engineering will also be explored" as part of future research directions.
- Why unresolved: The current methodology depends on specific NLP techniques (e.g., Word2Vec, sentiment analysis) and side features (e.g., edit quality), which may be surpassed by semantic representations generated by LLMs.
- What evidence would resolve it: Ablation studies substituting manual NLP features with LLM-generated embeddings or features, measuring the resulting impact on F-measure and accuracy.

## Limitations
- Cold start sensitivity: 0.5% sample size may be insufficient for highly imbalanced datasets or different disinformation patterns
- Concept drift vulnerability: No explicit testing of system's handling of rapid distribution shifts in streaming data
- Dataset specificity: 89 features engineered for wiki data streams; effectiveness on other platforms unproven
- LLM explanation fidelity: No systematic evaluation of whether explanations accurately reflect decision paths

## Confidence

**High**: Multi-dimensional feature engineering improves detection accuracy (supported by 89-feature implementation and ARFC performance). Tree-based ensembles are stable in streaming scenarios (validated by 87.67-92.40% accuracy across models). Real-time processing is feasible (45-62 samples/second).

**Medium**: LLM explanations make predictions interpretable (based on dashboard screenshots, but no systematic user study). Feature variance thresholding prevents overfitting (mechanism described, but dynamic behavior in streaming untested).

**Low**: The system generalizes to non-wiki platforms (no cross-platform validation). Cold start calibration ensures optimal performance across all wiki domains (0.5% sample adequacy untested).

## Next Checks

1. **Concept Drift Stress Test**: Simulate rapid concept drift in Wikivoyage data and measure ARFC accuracy degradation over time. Compare performance against baseline GNB/ALMA under drift conditions.

2. **Explanation Faithfulness Audit**: For 100 random predictions, manually verify LLM explanations against decision paths. Calculate: (a) feature coverage percentage, (b) logical consistency score, (c) hallucination rate per explanation.

3. **Cross-Platform Transferability**: Apply the 89-feature pipeline to Twitter or TikTok datasets. Measure feature relevance (variance threshold retention rates) and classification accuracy compared to wiki baselines.