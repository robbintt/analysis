---
ver: rpa2
title: 'TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via
  Diffusion Models'
arxiv_id: '2503.05638'
source_url: https://arxiv.org/abs/2503.05638
tags:
- video
- view
- videos
- novel
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TrajectoryCrafter introduces a novel framework for redirecting
  camera trajectories in monocular videos, addressing the challenge of precise view
  transformation and 4D content consistency. The core idea involves disentangling
  deterministic view transformations from stochastic content generation by leveraging
  dynamic point clouds and a dual-stream conditional video diffusion model.
---

# TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models

## Quick Facts
- arXiv ID: 2503.05638
- Source URL: https://arxiv.org/abs/2503.05638
- Reference count: 40
- Key outcome: Novel framework for redirecting camera trajectories in monocular videos using dual-stream diffusion with point cloud conditioning

## Executive Summary
TrajectoryCrafter addresses the challenge of generating videos with user-specified camera trajectories from monocular input videos. The method disentangles deterministic view transformations from stochastic content generation by lifting source videos to dynamic point clouds and using a dual-stream conditional video diffusion model. This approach enables precise trajectory control while maintaining 4D content consistency across the generated video sequence.

## Method Summary
TrajectoryCrafter uses a two-stage training approach with a base CogVideoX-Fun-5B diffusion transformer. First, monocular videos are lifted to dynamic point clouds using DepthCrafter and MASt3R. Point cloud renders from target trajectories provide geometric conditioning, while a dual-stream architecture integrates these renders with source video content through Ref-DiT blocks using cross-attention. Due to limited synchronized multi-view videos, the method employs a double-reprojection strategy to generate synthetic training pairs from monocular videos, combining these with static multi-view datasets for comprehensive training.

## Key Results
- Achieves PSNR of 14.24, SSIM of 0.417, and LPIPS of 0.519 on the iPhone dataset
- Demonstrates significant improvements in VBench metrics on in-the-wild videos (Subject Consistency 0.9236, Background Consistency 0.9512)
- Ablation studies show Ref-DiT outperforms concatenation (PSNR 14.24 vs 11.05) and removal (11.63)
- Successfully generalizes across diverse scenes with high-fidelity video generation

## Why This Works (Mechanism)

### Mechanism 1
Disentangling deterministic view transformation from stochastic content generation enables precise camera trajectory control. Source video is lifted to a dynamic point cloud via monocular depth estimation. Point cloud renders from the target trajectory provide explicit geometric conditioning that spatially aligns with the desired output, decoupling "where to look" from "what to generate."

### Mechanism 2
Cross-attention based reference injection (Ref-DiT blocks) transfers detailed appearance from spatially misaligned source video to target view tokens. View tokens (from point cloud renders) serve as queries; reference tokens (from source video) serve as keys/values in cross-attention layers inserted between inherited DiT blocks.

### Mechanism 3
Double-reprojection creates synthetic training pairs from monocular video that simulate point-cloud-render-like artifacts (holes, occlusions) while maintaining content correspondence. Given target video → lift to point cloud → render novel view I' via random transformation → reproject back via inverse transformation → I'' contains realistic occlusion artifacts while spatially aligning with original.

## Foundational Learning

- **Diffusion Transformer (DiT) architecture for video**: Why needed - TrajectoryCrafter builds on CogVideoX's DiT backbone; Quick check - Can you sketch how a video tensor flows through VAE encoder → patchify → DiT blocks → VAE decoder?

- **Cross-attention for conditional generation**: Why needed - Ref-DiT blocks use cross-attention where view tokens query reference tokens; Quick check - Given query shape (B, N_v, D) and key/value shape (B, N_r, D), what is the output shape and what does it represent?

- **Perspective projection and inverse projection**: Why needed - Point cloud lifting (inverse projection) and rendering (forward projection) are core to trajectory control; Quick check - If you have an image pixel (u, v) and depth d, how do you compute the corresponding 3D point in camera coordinates?

## Architecture Onboarding

- **Component map**: Input Video → DepthCrafter → Dynamic Point Cloud P → Point Cloud Render I^r + Mask M^r → VAE Encoder (shared) → View Tokens + Text Tokens ← Ref-DiT Block ← Reference Tokens → DiT Block → Ref-DiT Block → ... → VAE Decoder → Output Video

- **Critical path**: 1) Depth estimation quality → point cloud accuracy → trajectory control precision 2) Ref-DiT cross-attention → appearance transfer quality → 4D consistency 3) Training data diversity → generalization to in-the-wild videos

- **Design tradeoffs**: Two-stage training separates geometric inpainting from reference injection but requires multi-view triplets unavailable for dynamic data; point cloud renders provide explicit geometry but inherit depth estimation errors; Ref-DiT adds compute overhead vs. direct concatenation but enables misaligned reference use

- **Failure signatures**: "Ghosting" or duplicate content from depth estimation errors causing incorrect 3D lifting; trajectory drift from point cloud render not properly aligned with user-specified poses; content inconsistency with source from Ref-DiT cross-attention not attending to correct reference regions; over-smoothed textures from training dominated by static multi-view data

- **First 3 experiments**: 1) Validate point cloud pipeline: Input video with known camera motion, render point cloud from original trajectory, measure reconstruction error vs. original frames 2) Ablate Ref-DiT vs. concatenation: Train two variants on same data subset, compare on iPhone dataset—expect ~3 PSNR gap 3) Test double-reprojection fidelity: Apply double-reprojection to held-out video, visualize I'' artifacts alongside actual point cloud render artifacts from inference—check if hole patterns qualitatively match

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be extended to support very large-range camera trajectories, such as 360-degree orbits, which currently exceed the model's capabilities? The authors state the method "struggles to synthesize very large-range trajectories, such as 360-degree viewpoints, due to insufficient 3D cues from monocular inputs and the constrained generation length of the video diffusion model."

### Open Question 2
Can the dependency on accurate monocular depth estimation be relaxed or made robust against errors to prevent physically implausible outputs? Section 4.5 notes that because the approach "relies on depth estimation to create dynamic point clouds... inaccuracies in this process may propagate, producing suboptimal novel trajectory videos."

### Open Question 3
Is it possible to achieve real-time or interactive frame rates by reducing the computational overhead of the multi-step denoising process? Section 4.5 identifies that "multi-step denoising during inference" leads to "relatively high computational overhead."

## Limitations

- Relies heavily on monocular depth estimation quality, which may fail on textureless regions, transparent objects, or complex occlusions
- Struggles with very large-range camera trajectories (e.g., 360-degree orbits) due to insufficient 3D cues from monocular inputs
- Multi-step denoising during inference leads to relatively high computational overhead

## Confidence

- **High Confidence**: Core architectural innovation (dual-stream conditioning + Ref-DiT blocks) and its contribution to performance are well-supported by ablation studies
- **Medium Confidence**: Generalization claims to "in-the-wild" videos are based on VBench metrics, but specific diversity and representativeness of test videos is not fully characterized
- **Low Confidence**: Long-term temporal consistency of generated videos beyond 49 frames and across highly dynamic scenes with multiple moving objects remains underexplored

## Next Checks

1. **Cross-dataset Generalization**: Apply TrajectoryCrafter to a held-out dataset with significantly different characteristics (e.g., action videos with fast motion) and measure performance degradation relative to iPhone/DL3DV datasets.

2. **Depth Error Sensitivity**: Systematically inject synthetic depth errors into the pipeline and measure the resulting impact on trajectory control accuracy and 4D consistency metrics.

3. **Artifact Comparison Analysis**: Quantitatively compare the statistical distribution of occlusion artifacts in double-reprojection training data versus actual point cloud render artifacts during inference across multiple scene types.