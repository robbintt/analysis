---
ver: rpa2
title: 'OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for
  Scientific Writing'
arxiv_id: '2601.09858'
source_url: https://arxiv.org/abs/2601.09858
tags:
- scientific
- generation
- arxiv
- writing
- citation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a reinforcement learning framework for scientific
  paper generation that models the writing process as a long-horizon planning problem
  over hierarchical document structures. The method uses structured editing actions
  to incrementally build scientific manuscripts, employing a two-stage optimization
  procedure combining backward outline reconstruction and forward value-guided RL.
---

# OUTLINEFORGE: Hierarchical Reinforcement Learning with Explicit States for Scientific Writing

## Quick Facts
- **arXiv ID:** 2601.09858
- **Source URL:** https://arxiv.org/abs/2601.09858
- **Reference count:** 8
- **Primary result:** Two-stage RL framework with hierarchical outline planning improves structural coherence and citation reliability over token-level generation baselines

## Executive Summary
This paper introduces OUTLINEFORGE, a reinforcement learning framework that models scientific paper generation as a long-horizon planning problem over hierarchical document structures. The method uses structured editing actions to incrementally build scientific manuscripts through discrete state transitions over outline states. A two-stage optimization procedure combines backward outline reconstruction with forward value-guided RL, trained on arXiv data. The framework explicitly rewards scientific correctness, discourse coherence, and citation fidelity, achieving consistent improvements over strong neural and LLM baselines, particularly in long-range structural coherence and citation reliability.

## Method Summary
OUTLINEFORGE casts scientific writing as an episodic Markov Decision Process where states are hierarchical outline structures and actions are editing diffs (node reordering, deletion, or semantic refinement). The method uses a two-stage training pipeline: first, backward outline reconstruction from partial plans to enforce global structural consistency; second, forward value-guided RL with explicit rewards for correctness, coherence, and citation fidelity. The framework parses 1,500 arXiv articles into hierarchical schemas and generates training pairs by corrupting paragraphs and learning restoration plans. Fine-tuned models (Phi-3.8B, Qwen3-1.8B, Gemma2-2B) outperform larger general-purpose models, demonstrating the effectiveness of structured reasoning in long-form scientific writing.

## Key Results
- OUTLINEFORGE achieves higher precision/recall/F1 scores for citation coverage compared to SurveyForge and AutoSurvey baselines
- Models fine-tuned on OUTLINEFORGE data, especially Phi-3.8B, outperform larger models like GPT-4o-mini and Llama-3.1-70B on structural coherence metrics
- Citation retrieval quality plateaus at 50-150 steps while other quality metrics continue improving, suggesting early reference saturation

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Outline Planning
- **Claim:** Converting scientific writing from token-level generation to hierarchical outline-state planning improves long-range structural coherence.
- **Mechanism:** The framework represents paper outlines as editable hierarchical structures and models generation as discrete state transitions over outline states through structured editing actions ("diffs"). Each state $s_{outline} \in S$ represents a valid outline structure, and transitions occur via actions that can be deterministic (node reordering, deletion) or generative (semantic refinement via LLM delegation).
- **Core assumption:** Scientific document structure is inherently hierarchical and can be decomposed into discrete, plannable states with meaningful intermediate representations.
- **Evidence anchors:** [abstract] "casts scientific outline construction as a long-horizon planning problem over hierarchical document structures"; [section 3.3] "We represent a paper outline as an editable hierarchical structure, and model the generation process as a sequence of discrete state transitions over outline states"; [corpus] SurveyGen-I and Meow similarly employ outline-centric approaches.

### Mechanism 2: Two-Stage Training Architecture
- **Claim:** Two-stage training combining backward outline reconstruction with forward value-guided RL stabilizes learning and enforces global structural consistency.
- **Mechanism:** Stage 1 learns from "backward outline reconstruction from partial plans" to enforce global structural consistency—essentially teaching the model to infer what complete structures should look like from incomplete inputs. Stage 2 applies forward value-guided RL with explicit rewards. Training data is constructed by inverting document edits: perturbing paragraphs from arXiv papers and treating the original as the target solution.
- **Core assumption:** Backward reconstruction provides useful structural priors that transfer to forward generation, and the preference data derived from arXiv revision histories approximates expert editing behavior.
- **Evidence anchors:** [abstract] "two-stage optimization procedure consisting of (i) backward outline reconstruction from partial plans to enforce global structural consistency, and (ii) forward value-guided reinforcement learning"; [section 3.4] "this process mirrors the inverse direction of our document generation pipeline, enabling the model to learn how local edits accumulate into global structural improvements".

### Mechanism 3: Explicit Reward Modeling
- **Claim:** Explicit reward modeling for scientific criteria (correctness, coherence, citation fidelity) improves citation reliability and factual accuracy beyond what likelihood-based training achieves.
- **Mechanism:** The RL formulation optimizes $J(\theta) = \mathbb{E}_{\pi_\theta}[\sum_{t=1}^{L_X} \gamma^{t-1} r_t]$ where rewards encode coverage, structure, coherence, factuality, and citation correctness. This decouples training from token-level likelihood and allows integration of non-differentiable constraints.
- **Core assumption:** The reward components can be reliably measured automatically and are sufficiently dense to guide learning without excessive exploration.
- **Evidence anchors:** [abstract] "rewards explicitly modeling scientific correctness, discourse coherence, and citation fidelity"; [section 3.2] "This formulation decouples training from token-level likelihood and allows integration of non-differentiable constraints through reward shaping".

## Foundational Learning

- **Concept: Markov Decision Processes (MDPs) for sequential decision-making**
  - **Why needed here:** The entire framework formalizes scientific writing as an episodic MDP $\mathcal{M} = \langle S, A, P, r, \gamma \rangle$ where states are outline structures and actions are editing diffs.
  - **Quick check question:** Can you explain why treating writing as an MDP enables credit assignment across long horizons better than treating it as single-pass generation?

- **Concept: Hierarchical reinforcement learning and temporal abstraction**
  - **Why needed here:** The framework uses hierarchical abstractions (outline-level vs. content-level) to reduce planning horizon from thousands of tokens to hundreds of outline steps.
  - **Quick check question:** How does operating at the outline level rather than token level affect the effective planning horizon and credit assignment?

- **Concept: Preference alignment and reward modeling**
  - **Why needed here:** The framework learns from "human preference data" derived from arXiv editing trajectories, requiring understanding of how preference signals translate to policy updates.
  - **Quick check question:** What are the failure modes if preference data does not actually reflect high-quality scientific writing behavior?

## Architecture Onboarding

- **Component map:** Input Processing -> State Encoder -> Action Generator (Policy πθ) -> Execution Router -> Reward Model -> Training Pipeline
- **Critical path:**
  1. Data construction: Parse arXiv papers → extract hierarchical structure → generate perturbation pairs (corrupted paragraph, original solution)
  2. Backward pretraining: Train policy to reconstruct complete outlines from partial/corrupted inputs
  3. Forward RL: Initialize from pretrained model, optimize value-guided generation with explicit rewards
  4. Inference: Start from empty outline, apply policy iteratively for 200-300 steps until convergence

- **Design tradeoffs:**
  - Fixed step budget vs. early stopping: Paper uses 200-300 steps based on empirical analysis, but early stopping could reduce compute at cost of potentially incomplete outputs
  - Automated vs. human evaluation: Relying on LLM judges (GPT-4o) enables scalable evaluation but introduces judge-specific biases
  - Deterministic vs. generative actions: Deterministic edits provide controllability; generative edits provide flexibility but introduce variability

- **Failure signatures:**
  - Early structural errors propagate: Section 5 acknowledges "error accumulation across iterative editing steps may still occur, particularly when early-stage structural decisions are suboptimal"
  - Citation plateau: Figure 3 shows citation metrics saturate at 50-150 steps while other metrics continue improving—models may stop retrieving new references too early
  - Small model limitations: Models below 2B parameters "exhibit limited gains after fine-tuning, suggesting that insufficient intrinsic world knowledge constrains their ability to benefit from structured document evolution signals"

- **First 3 experiments:**
  1. Baseline comparison: Run SurveyForge, AutoSurvey, and OUTLINEFORGE (with Phi-3.8B) on identical survey topics; compare precision/recall/F1 for citation coverage and structural coherence
  2. Ablation study: Train variants with only backward reconstruction, only forward RL, and both; isolate contribution of each training stage
  3. Step budget analysis: Measure quality metrics at 50-step intervals from 50-300 steps to identify optimal stopping point and verify citation plateau behavior reported in Figure 3

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can state-action abstractions for document editing be automatically learned rather than manually designed?
- **Basis in paper:** [explicit] "Automatically learning or refining state-action abstractions remains an open challenge" (p.8)
- **Why unresolved:** Current schemas are hand-crafted from document structures and editing operations, which may limit flexibility across domains with different writing conventions.
- **What evidence would resolve it:** A learned state representation that matches or exceeds manually designed schemas across diverse scientific domains, measured by generation quality and structural coherence metrics.

### Open Question 2
- **Question:** Do LLM-as-judge evaluations for scientific writing quality correlate with human expert assessment?
- **Basis in paper:** [explicit] "part of our analysis relies on large language models as automatic judges... such evaluations may introduce biases aligned with the judging models' own priors and may not fully reflect human expert judgment" (p.8)
- **Why unresolved:** Systematic comparison with human experts is costly and was not conducted; the degree of alignment remains unknown.
- **What evidence would resolve it:** A human annotation study comparing expert ratings of structural completeness, citation relevance, and information density against GPT-4o judgments on the same generated papers.

### Open Question 3
- **Question:** Can global revision or rollback mechanisms mitigate error accumulation in long-horizon iterative editing?
- **Basis in paper:** [explicit] "error accumulation across iterative editing steps may still occur, particularly when early-stage structural decisions are suboptimal" (p.8)
- **Why unresolved:** The current framework proceeds forward without explicit mechanisms to revisit or undo earlier decisions once evidence of suboptimality emerges.
- **What evidence would resolve it:** Experiments comparing standard forward generation against variants with explicit rollback capabilities, measuring final document quality as a function of early-stage error injection.

## Limitations

- The paper's reliance on LLM-based evaluation (GPT-4o) for citation relevance and structural coherence introduces potential judge-specific biases that are not fully characterized
- Training data construction from arXiv revisions assumes that edit trajectories reflect high-quality scientific writing behavior, which may not hold if revisions are primarily formatting changes
- The exact formulation of reward components for scientific correctness, discourse coherence, and citation fidelity remains unspecified, making it difficult to assess their reliability and alignment with human judgment

## Confidence

- **High confidence:** Hierarchical outline planning improves structural coherence over flat token generation (supported by convergent evidence from SurveyGen-I and Meow)
- **Medium confidence:** Two-stage training (backward reconstruction + forward RL) provides benefits beyond single-stage training (mechanism novel, but lacks ablation evidence)
- **Medium confidence:** Explicit reward modeling improves citation reliability and factual accuracy (mechanistically sound, but dependent on reward quality)

## Next Checks

1. Conduct human evaluation studies comparing citation faithfulness and factual accuracy between OUTLINEFORGE and baseline models to validate LLM-based automatic metrics
2. Perform systematic ablation studies isolating the contributions of backward reconstruction pretraining versus forward value-guided RL to quantify each component's impact
3. Test the framework on non-survey scientific document types (e.g., methodology papers, case studies) to assess generalizability beyond the surveyed domains