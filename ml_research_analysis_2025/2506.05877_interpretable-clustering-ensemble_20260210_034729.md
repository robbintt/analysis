---
ver: rpa2
title: Interpretable Clustering Ensemble
arxiv_id: '2506.05877'
source_url: https://arxiv.org/abs/2506.05877
tags:
- clustering
- data
- ensemble
- interpretable
- uni00000013
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ICE (Interpretable Clustering Ensemble),
  the first interpretable clustering ensemble algorithm. ICE constructs a decision
  tree in the original feature space by treating base partitions as categorical variables
  and using statistical association tests to guide the tree building process.
---

# Interpretable Clustering Ensemble

## Quick Facts
- arXiv ID: 2506.05877
- Source URL: https://arxiv.org/abs/2506.05877
- Reference count: 40
- Primary result: ICE achieves competitive clustering accuracy while maintaining interpretability through decision trees in original feature space

## Executive Summary
This paper introduces ICE (Interpretable Clustering Ensemble), the first interpretable clustering ensemble algorithm. ICE constructs a decision tree in the original feature space by treating base partitions as categorical variables and using statistical association tests to guide the tree building process. The algorithm evaluates candidate splits using chi-squared statistics between binary splits and base partitions, selecting the split with the strongest statistical association. Experimental results on 33 datasets demonstrate that ICE achieves competitive clustering accuracy compared to state-of-the-art methods while maintaining interpretability.

## Method Summary
ICE generates 30 base partitions using k-means with randomized cluster counts, then builds a decision tree by evaluating all feature-threshold splits using summed chi-squared statistics against all base partitions. The algorithm selects splits with the smallest p-value and terminates when exactly k leaf nodes are created. The method treats each base partition as a categorical variable, enabling statistical testing in the original feature space while preserving ensemble diversity. ICE runs 10 independent trials per dataset to average results.

## Key Results
- ICE achieves competitive clustering accuracy compared to state-of-the-art methods (LWEA, LWGP, IMM, SHA)
- ICE outperforms existing interpretable clustering approaches in clustering quality metrics (F1-score and NMI)
- ICE produces decision trees with reasonable depth (average 1.98, max 2.62) while maintaining clustering quality

## Why This Works (Mechanism)

### Mechanism 1
Statistical association between binary splits and base partitions can identify meaningful clustering boundaries in the original feature space. For each candidate split, ICE constructs 2×pt contingency tables against each base partition, computes chi-squared statistics, and sums them to rank split quality. The split with the strongest aggregate association (smallest p-value) is selected. Core assumption: Splits that correlate strongly with multiple diverse base partitions capture consensus cluster structure.

### Mechanism 2
Treating base partitions as categorical variables preserves clustering diversity while enabling a unified statistical framework for split selection. Each base partition πt with pt clusters is represented as a categorical variable. During split evaluation, samples are cross-tabulated by their binary split membership and their cluster label in πt. This converts the ensemble consensus problem into a series of independence tests. Core assumption: Base partitions encode complementary information about the true cluster structure.

### Mechanism 3
Greedy selection of the globally best split (minimum p-value across all candidate nodes) produces a compact, interpretable tree while preserving ensemble-informed structure. ICE maintains a set U of expandable nodes. At each iteration, it identifies the node with the smallest p-value among all candidates and grows that node. This biases toward splits with strongest statistical support first. Core assumption: Prioritizing statistically significant splits early leads to better overall clustering and shallower trees.

## Foundational Learning

- **Chi-squared test for independence**: ICE uses chi-squared statistics to quantify association between binary splits and categorical base partitions. Understanding how expected frequencies, observed frequencies, and degrees of freedom interact is essential for debugging split selection. Quick check: Given a 2×3 contingency table with row totals [10, 20] and column totals [5, 15, 10], what is the expected frequency for cell (row 1, column 2)?

- **Clustering ensemble consensus**: ICE is fundamentally an ensemble method; understanding how base partitions are generated, why diversity matters, and how consensus has traditionally been achieved contextualizes ICE's novelty. Quick check: Why might averaging co-association matrices fail if base partitions have different numbers of clusters?

- **Decision tree growth and stopping criteria**: ICE constructs a binary tree top-down with a k-leaf stopping criterion. Understanding tree induction, overfitting risks, and the interpretability-depth tradeoff is necessary for practical deployment. Quick check: If ICE terminates at exactly k leaf nodes, what happens if the true data structure has more or fewer than k natural clusters?

## Architecture Onboarding

- **Component map**: Base Partition Generator -> OptimalSplit Function -> Tree Builder -> Termination Controller
- **Critical path**: 1. Generate 30 base partitions using k-means with varied k; 2. Initialize root with full dataset, add to U; 3. For each node in U without computed split: call OptimalSplit; 4. Find node Tb with minimum p-value; apply its best split to create children; 5. Update U (remove Tb, add children); repeat until k leaves; 6. Output tree
- **Design tradeoffs**: Exhaustive split search vs. speed (O(k·M·N·(lgN + R)) complexity); p-value aggregation via independence assumption (simplifies comparison but may be statistically fragile); fixed k leaves (requires prior knowledge)
- **Failure signatures**: All splits return similar p-values (base partitions too similar); very deep trees on simple data (overfitting to noisy base partitions); empty or tiny leaf nodes (constraint |subset| < 5 skips splits)
- **First 3 experiments**: 1. Sanity check on Iris (k=3, c=10): verify tree depth ≤3 and clusters align with true species; 2. Ensemble size sensitivity: run ICE on 3 datasets with c ∈ {10, 20, 30, 40}, plot Purity/F1/NMI vs. c; 3. Runtime profiling on higher-dimensional data: measure time spent in OptimalSplit vs. tree construction on Satimage and Texture

## Open Questions the Paper Calls Out

### Open Question 1
Can pruning strategies be effectively integrated into the ICE algorithm to reduce the computational burden of exhaustive split evaluation without degrading clustering accuracy? The paper suggests pruning could reduce computational burden by focusing only on branches more likely to yield optimal splits, addressing the high time complexity limitation.

### Open Question 2
Do alternative statistical methods or split selection criteria outperform the current p-value based strategy in yielding optimal partitions? The authors note that relying solely on p-value as the statistical criterion for split selection has inherent limitations and suggest future work may explore alternative statistical methods.

### Open Question 3
How does the potential correlation between base partitions affect the validity of the independence assumption used to calculate the combined p-value for split selection? The methodology calculates combined p-value assuming independence of c chi-squared variables, but provides no sensitivity analysis regarding how violations of this assumption impact split quality or tree structure.

## Limitations
- Statistical validity of p-value aggregation across base partitions may be compromised by violated independence assumptions
- Computational complexity of exhaustive split evaluation (O(k·M·N·(lgN + R))) limits scalability to high-dimensional or large datasets
- Fixed k-leaf termination criterion requires prior knowledge of cluster count, which may not be available in unsupervised settings

## Confidence

- **High**: ICE achieves competitive clustering accuracy compared to state-of-the-art methods (supported by Table VI results)
- **Medium**: Statistical association tests effectively identify meaningful clustering boundaries (mechanism plausible but limited empirical validation of statistical assumptions)
- **Medium**: Greedy selection produces compact, interpretable trees (supported by depth metrics but not rigorously compared to alternative tree induction strategies)

## Next Checks

1. **Statistical robustness test**: Generate synthetic datasets with known cluster structure but varying degrees of base partition correlation. Measure how ICE performance degrades as independence assumption becomes violated.

2. **Runtime scalability analysis**: Profile ICE on datasets with increasing feature dimensions (e.g., from 10 to 100+ features) to empirically validate the computational complexity claims and identify optimization opportunities.

3. **Ensemble diversity impact study**: Systematically vary the base partition generation strategy (different k ranges, different clustering algorithms) and measure the corresponding impact on ICE clustering quality and tree structure.