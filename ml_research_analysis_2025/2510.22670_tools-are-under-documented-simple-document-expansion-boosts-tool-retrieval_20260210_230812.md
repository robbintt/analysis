---
ver: rpa2
title: 'Tools are under-documented: Simple Document Expansion Boosts Tool Retrieval'
arxiv_id: '2510.22670'
source_url: https://arxiv.org/abs/2510.22670
tags:
- tool
- retrieval
- document
- arxiv
- expansion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of tool retrieval in large language
  model systems, where poor and heterogeneous tool documentation limits performance.
  The core idea is to systematically enrich tool documentation with structured fields
  like function description, when-to-use, limitations, and tags using an LLM-driven
  expansion pipeline.
---

# Tools are under-documented: Simple Document Expansion Boosts Tool Retrieval
## Quick Facts
- arXiv ID: 2510.22670
- Source URL: https://arxiv.org/abs/2510.22670
- Reference count: 28
- Core result: Document expansion pipeline improves tool retrieval NDCG@10 from ~52 to ~56, Recall@10 from ~63 to ~68

## Executive Summary
This paper tackles the tool retrieval problem in LLM systems where sparse and inconsistent tool documentation limits performance. The authors propose a systematic approach to enrich tool documentation using LLM-driven expansion, adding structured fields like function descriptions, usage guidance, limitations, and tags. The expanded documentation is then used to train two specialized models: a dense retriever (Tool-Embed) and a reranker (Tool-Rank). The method significantly outperforms existing approaches on the Tool-DE benchmark, demonstrating that simple document expansion can meaningfully improve retrieval accuracy.

## Method Summary
The approach involves two key steps: first, expanding tool documentation using an LLM pipeline that adds structured information (description, when-to-use, limitations, tags); second, training dedicated retrieval models on this expanded data. Tool-Embed uses dense embeddings for initial retrieval, while Tool-Rank reorders the top results using cross-attention between query and tool descriptions. The expansion process creates richer, more informative tool descriptions that capture nuances about tool usage and constraints, enabling better matching between user queries and appropriate tools.

## Key Results
- Tool-Embed-4B achieves NDCG@10 of 52.23, Recall@10 of 63.13, and Completeness@10 of 51.61
- Tool-Rank-4B improves NDCG@10 to 56.44, Recall@10 to 67.81, and Completeness@10 to 56.60
- Both models establish new state-of-the-art performance on the Tool-DE benchmark

## Why This Works (Mechanism)
The method works by addressing the fundamental limitation of sparse tool documentation through systematic expansion. By adding structured fields that capture not just what a tool does but when and how to use it, the expanded documentation provides richer semantic context for matching user queries. The two-stage retrieval approach (dense retrieval followed by reranking) allows for efficient initial candidate selection and then refined ordering based on deeper semantic understanding of the expanded descriptions.

## Foundational Learning
- Tool retrieval fundamentals: Understanding the task of matching user queries to appropriate tools in LLM systems. Quick check: Can you explain why this differs from traditional information retrieval?
- Dense retrieval concepts: How embedding-based methods work for semantic matching. Quick check: What's the advantage of dense retrieval over sparse methods like BM25?
- Reranking with cross-attention: How models can refine initial retrieval results using deeper semantic understanding. Quick check: Why use a separate reranker rather than just improving the retriever?
- Document expansion techniques: Methods for enriching sparse text with additional structured information. Quick check: What are the risks of over-expanding documentation?

## Architecture Onboarding
Component map: User Query -> Tool-Embed (retrieval) -> Top-K tools -> Tool-Rank (reranking) -> Final ranked tools
Critical path: The expanded documentation flows through both the dense retriever and reranker, with Tool-Rank providing the final ranking using cross-attention between query and expanded tool descriptions.
Design tradeoffs: The choice of using a two-stage approach balances efficiency (fast initial retrieval) with accuracy (deeper semantic matching in reranking). The expansion depth (r=1 vs r=2) represents a tradeoff between richer descriptions and potential noise.
Failure signatures: Poor expansion quality from the LLM pipeline can introduce misleading information; synthetic evaluation may not capture real-world documentation issues.
First experiments: 1) Run retrieval without expansion to establish baseline; 2) Test expansion pipeline with different depths; 3) Evaluate impact of individual expanded fields (description vs tags vs limitations)

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies on a single synthetic benchmark (Tool-DE) that may not reflect real-world documentation challenges
- Performance gains show diminishing returns beyond certain expansion depths, suggesting potential overfitting
- The LLM-driven expansion pipeline introduces dependencies on model quality and prompt engineering
- No testing on multimodal tool descriptions (code + text) to assess text-only expansion's full value

## Confidence
- State-of-the-art performance claims on Tool-DE: **High confidence** (directly measurable results)
- Documentation quality as primary bottleneck: **Medium confidence** (strong evidence but not fully isolated)
- Universal improvement from systematic expansion: **Low confidence** (synthetic evaluation limits generalizability)

## Next Checks
1. Evaluate Tool-Embed and Tool-Rank on a real-world tool repository with naturally occurring documentation quality variation
2. Test robustness by introducing controlled noise and incompleteness into the expanded documentation
3. Compare against retrieval baselines using multimodal tool descriptions (code + text) to assess the added value of text-only expansion