---
ver: rpa2
title: 'Kongzi: A Historical Large Language Model with Fact Enhancement'
arxiv_id: '2504.09488'
source_url: https://arxiv.org/abs/2504.09488
tags:
- reasoning
- historical
- data
- wang
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Kongzi, a historical large language model designed
  to improve factual accuracy and reasoning depth for historical analysis tasks. The
  authors address the challenge of hallucinations and factual inaccuracies in longer
  reasoning chains, which are particularly problematic in historical contexts requiring
  cross-temporal correlations and coherent conclusions from fragmentary sources.
---

# Kongzi: A Historical Large Language Model with Fact Enhancement

## Quick Facts
- arXiv ID: 2504.09488
- Source URL: https://arxiv.org/abs/2504.09488
- Reference count: 25
- Primary result: Kongzi-7B outperforms DeepSeek-R1 and O3-mini on Chinese historical QA with entity-level factual accuracy gains of 6-10 percentage points

## Executive Summary
This paper presents Kongzi, a historical large language model designed to improve factual accuracy and reasoning depth for historical analysis tasks. The authors address the challenge of hallucinations and factual inaccuracies in longer reasoning chains, which are particularly problematic in historical contexts requiring cross-temporal correlations and coherent conclusions from fragmentary sources. The core method involves continued pre-training on curated historical corpora, a two-stage supervised fine-tuning process, and a novel fact-aware reinforcement learning framework that rewards entity-level factual accuracy.

## Method Summary
The authors build Kongzi by first continuing pre-training a Qwen2.5 base model on a 0.2B-token Chinese historical corpus. They then employ a two-stage supervised fine-tuning approach: Stage 1 develops basic question-answering capabilities with both general and historical data, while Stage 2 enhances chain-of-thought reasoning through high-quality CoT data. Finally, they apply a fact-aware reinforcement learning strategy using GRPO with an entity-level reward function that penalizes incorrect entities and rewards correct ones. The model is evaluated on historical accuracy, logical reasoning, and problem-solving tasks, with comparisons against DeepSeek-R1 and O3-mini baselines.

## Key Results
- Kongzi-7B achieved answer scores of 70.64, 84.43, and 67.82 on GPT-4o, Gemini-2.5, and Qwen-Max respectively
- Historical accuracy scores reached 58.45, 80.45, and 56.36 on the same LLM judges
- The RL training provided substantial improvements over the SFT baseline, with gains of 6.4, 9.7, and 6.3 percentage points
- The 0.5B parameter version outperformed larger distilled models from DeepSeek, demonstrating efficient domain-specific performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Entity-level factual rewards in RL reduce hallucinations in long-form historical reasoning.
- Mechanism: The reward function (ER = wc × CE − wi × IE) explicitly penalizes incorrect entities and rewards correct ones, creating gradient signals that shape the model toward factual grounding rather than just fluent generation.
- Core assumption: Historical factuality can be decomposed into entity-level verification, and entity errors correlate with broader hallucination patterns.
- Evidence anchors: [abstract] "fact-reinforcement learning strategy... demonstrates strong factual alignment"; [section 3.3] "we augment the model with an entity recognition module that extracts and evaluates entity-level information"
- Break condition: If entity recognition fails on ambiguous historical names (e.g., multiple figures with identical names across dynasties), the reward signal becomes noisy, potentially degrading training.

### Mechanism 2
- Claim: Two-stage SFT (factual grounding before reasoning) yields better domain performance than joint training.
- Mechanism: Stage 1 establishes basic QA and conversational ability; Stage 2 introduces CoT data that presupposes factual knowledge, allowing the model to learn reasoning patterns without simultaneously acquiring facts.
- Core assumption: The model can transfer factual knowledge from Stage 1 to support CoT reasoning in Stage 2 without catastrophic forgetting.
- Evidence anchors: [abstract] "two-stage supervised fine-tuning with factual and chain-of-thought data"; [section 3.2] "through the SFT method, enable the model to generate text according to the preset Chain-of-Thought template"
- Break condition: If Stage 1 overfits to simple QA patterns, the model may struggle to generalize those facts to complex multi-step reasoning in Stage 2.

### Mechanism 3
- Claim: Continued pre-training on a curated 0.2B-token historical corpus enables small models (0.5B) to outperform larger general-purpose models (14B) on domain tasks.
- Mechanism: Domain-specific vocabulary, entity embeddings, and syntactic patterns from classical Chinese become embedded in the model, reducing the inference burden during downstream tasks.
- Core assumption: The historical corpus is representative of the evaluation distribution; domain transfer is the primary bottleneck, not model capacity.
- Evidence anchors: [abstract] "0.5B version impressively outperforms DeepSeek-r1-Distill-7B and 14B models"; [section 3.1] "collected nearly 0.2B tokens of high-quality classical Chinese historical corpus"
- Break condition: If evaluation questions require knowledge outside the curated corpus (e.g., non-Chinese history or recent historiography), the domain advantage diminishes.

## Foundational Learning

- Concept: **Proximal Policy Optimization (PPO/GRPO)**
  - Why needed here: The RL stage uses GRPO (a PPO variant) with a custom reward function. Understanding clipping, advantage estimation, and KL penalties is essential to diagnose training instability.
  - Quick check question: Can you explain why GRPO clips the probability ratio and how ε controls policy update magnitude?

- Concept: **Chain-of-Thought Distillation**
  - Why needed here: Kongzi generates CoT data from DeepSeek-R1 and filters it for quality. Understanding how reasoning patterns transfer via distillation helps assess data quality.
  - Quick check question: What is the risk of distilling CoT from a stronger model if the student model lacks the factual knowledge to validate each reasoning step?

- Concept: **Entity Recognition and Linking**
  - Why needed here: The fact-aware reward depends on extracting historical entities and matching them to ground truth. Errors in NER directly corrupt the reward signal.
  - Quick check question: How would you handle entity disambiguation when "Li Shimin" could refer to Emperor Taizong or a modern person with the same name?

## Architecture Onboarding

- Component map:
  Raw corpus → cleaning → classification (factual vs. reasoning) → augmentation (QA pairs) + CoT generation (via DeepSeek-R1) → filtering → CPT (0.2B tokens) → SFT Stage 1 (general + historical QA) → SFT Stage 2 (CoT) → RL (GRPO + entity reward)

- Critical path:
  1. Data quality at the classification step (Section 3.1) determines whether CoT data is grounded or hallucinated.
  2. SFT Stage 2 must preserve factual accuracy from Stage 1 while learning reasoning patterns.
  3. RL reward weights (w1–w4) directly control the factuality-fluency tradeoff.

- Design tradeoffs:
  - **Corpus scope vs. generalization**: Narrowly Chinese historical data maximizes domain performance but limits cross-cultural or interdisciplinary tasks.
  - **Reward complexity vs. training stability**: The multi-component reward (entity + logic + format + repetition) is expressive but harder to tune; misaligned weights can cause reward hacking.
  - **Model size vs. deployment**: 0.5B is deployable on edge devices but may fail on tasks requiring nuanced cultural context; 7B is more robust but costlier.

- Failure signatures:
  - **High repetition penalty but low factual reward**: Model learns to avoid n-gram repetition but still hallucinates entities—suggests entity reward is underweighted.
  - **Good CoT structure but wrong conclusions**: SFT Stage 1 may have weak factual grounding; consider re-weighting the historical vs. general data mix.
  - **RL loss diverges after initial improvement**: Check KL penalty coefficient; policy may be drifting too far from the SFT initialization.

- First 3 experiments:
  1. **Ablate the entity reward (set w1 = 0)**: Compare factual accuracy against the full model to quantify the contribution of entity-level supervision.
  2. **Swap SFT Stage 1 and Stage 2 order**: Test whether reasoning-first training degrades factual accuracy, validating the staged design assumption.
  3. **Evaluate on out-of-domain historical questions (e.g., non-Chinese history)**: Measure corpus scope limitations and identify generalization failure modes.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the proposed entity-based reward mechanism universally effective in mitigating hallucinations across domains other than ancient Chinese history?
- Basis in paper: [explicit] The Limitations section states the authors are "unable to demonstrate that incorporating an accurate reward function is universally effective in mitigating hallucinations."
- Why unresolved: Current experiments are restricted to the specific "Twenty-Four Histories" corpus and do not prove generalizability to other knowledge domains.
- What evidence would resolve it: Experiments applying the Kongzi framework to distinct domains (e.g., modern legal or medical texts) demonstrating consistent hallucination reduction.

### Open Question 2
- Question: Can the fact-aware reinforcement learning strategy maintain its effectiveness when scaled to significantly larger model architectures (e.g., 70B+ parameters)?
- Basis in paper: [explicit] The Conclusion lists exploring "the scalability of reinforcement learning methods augmented with factual data" as a primary goal for future work.
- Why unresolved: The study only validates the approach on 0.5B and 7B parameter models; it is unclear if the reward shaping remains stable in larger systems.
- What evidence would resolve it: Training dynamics and performance benchmarks of a 70B+ model trained with the same GRPO and entity-reward configuration.

### Open Question 3
- Question: Does the model's performance depend on the high degree of continuity and completeness found in the "Twenty-Four Histories"?
- Basis in paper: [inferred] The authors explicitly selected ancient Chinese history for its "continuity... and completeness of its records," suggesting the method may rely on data quality unavailable in other historical traditions.
- Why unresolved: It is untested whether the entity recognition and reasoning modules can handle the fragmentary or ambiguous sources typical of other historical periods.
- What evidence would resolve it: Evaluation results from a version of Kongzi trained on fragmentary historical corpora (e.g., pre-Columbian or dark age histories).

## Limitations

- The approach is validated only on Chinese-language historical data, limiting generalizability to other languages or historical domains
- The entity-level reward mechanism assumes entity recognition errors directly map to hallucination severity, but ambiguous historical names may undermine this signal
- The small RL dataset (500 samples) raises concerns about overfitting and reward hacking, though the authors report KL stability

## Confidence

- **High confidence**: Kongzi's domain-specific continued pre-training improves factual accuracy on Chinese historical tasks, supported by clear quantitative gains over baselines
- **Medium confidence**: The two-stage SFT and entity-level RL framework meaningfully reduce hallucinations, but the causal attribution is not isolated from other factors (e.g., corpus quality, model size)
- **Low confidence**: The 0.5B parameter version outperforming larger distilled models is a bold claim; without ablation on corpus-only vs. RL contributions, this may overstate the architecture's advantage

## Next Checks

1. **Ablate the entity reward (set w1 = 0)**: Compare factual accuracy against the full model to quantify the contribution of entity-level supervision
2. **Swap SFT Stage 1 and Stage 2 order**: Test whether reasoning-first training degrades factual accuracy, validating the staged design assumption
3. **Evaluate on out-of-domain historical questions (e.g., non-Chinese history)**: Measure corpus scope limitations and identify generalization failure modes