---
ver: rpa2
title: Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior
  Imitation and Speech-Text Interleaving
arxiv_id: '2505.18644'
source_url: https://arxiv.org/abs/2505.18644
tags:
- speech
- generalization
- arxiv
- tasks
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the generalization challenge in Speech Large
  Language Models (SLLMs) by proposing a multi-task behavior imitation method with
  speech-text interleaving (MTBI). The core idea is to train the SLLM to generate
  responses equivalent to a text-based LLM when given the same speech and transcript,
  using only paired speech and transcripts.
---

# Enhancing Generalization of Speech Large Language Models with Multi-Task Behavior Imitation and Speech-Text Interleaving

## Quick Facts
- arXiv ID: 2505.18644
- Source URL: https://arxiv.org/abs/2505.18644
- Authors: Jingran Xie; Xiang Li; Hui Wang; Yue Yu; Yang Xiang; Xixin Wu; Zhiyong Wu
- Reference count: 0
- Primary result: Multi-task behavior imitation with speech-text interleaving outperforms SOTA SLLMs on prompt and task generalization with less supervised speech data

## Executive Summary
This paper addresses the generalization challenge in Speech Large Language Models (SLLMs) by proposing a multi-task behavior imitation method with speech-text interleaving (MTBI). The core idea is to train the SLLM to generate responses equivalent to a text-based LLM when given the same speech and transcript, using only paired speech and transcripts. This approach reduces reliance on extensive annotated speech data. Additionally, an interleaving technique combines speech and text inputs to enhance alignment efficiency. The method is evaluated on a newly introduced benchmark assessing prompt and task generalization. Experimental results show that MTBI outperforms state-of-the-art SLLMs on both prompt and task generalization metrics, including GSM8K (20.1% accuracy) and Speaker Role tasks (75.3% accuracy), while requiring less supervised speech data. The approach demonstrates strong zero-shot learning capabilities and improves alignment between speech and text modalities.

## Method Summary
The method uses a two-stage behavior imitation framework where a frozen text LLM generates responses for speech transcripts, then an SLLM learns to predict these responses from speech inputs. The architecture freezes WavLM Large encoder and LLaMA2-7B-chat decoder while training only a CNN subsampler connector. Training uses multi-task learning across ASR, continuation, rewriting, and selecting tasks on LibriSpeech data. Speech-text interleaving (40% probability) mixes audio and text tokens to enhance alignment. The model achieves strong generalization by transferring the text LLM's reasoning capabilities to the speech modality through imitation learning rather than direct task supervision.

## Key Results
- Prompt Generalization accuracy: 85% (vs 77% without interleaving)
- GSM8K math reasoning accuracy: 20.1% (vs 9.1% with ASR-only training)
- Speaker Role inference accuracy: 75.3% (vs 67.4% baseline)
- Requires less supervised speech data than traditional SFT approaches

## Why This Works (Mechanism)

### Mechanism 1
Constraining the SLLM to imitate the outputs of a text-only LLM aligns speech features to the text embedding space, recovering zero-shot capabilities lost in standard SFT. By training to produce the exact response generated by the text-LLM for the corresponding transcript, the connector is forced to map speech embeddings to a region in the frozen LLM's latent space that triggers the same reasoning pathways as the text tokens. This bypasses the need for expensive, task-specific speech annotations.

### Mechanism 2
Speech-text interleaving enhances alignment efficiency by replacing random text segments with corresponding speech segments. This provides a dense, localized learning signal for the connector to associate specific acoustic features with specific token embeddings, stabilizing training. The model learns to process the speech segment as a direct functional substitute for the text tokens.

### Mechanism 3
Training on high-quality constructed tasks using only ASR data improves generalization better than multi-task SFT on limited specialized datasets. Constructed tasks force the model to manipulate the content of speech itself rather than just learning classification boundaries, preserving the LLM's instruction-following and reasoning capabilities.

## Foundational Learning

### Concept: Modality Alignment / Projectors (Q-Former / Connector)
**Why needed:** The core of this paper is freezing the LLM and Encoder, training only the connector. Understanding how to map variable-length audio sequences to fixed-dimension LLM token space is critical.
**Quick check:** If the LLM is frozen, which component is responsible for "learning" to hear?

### Concept: Knowledge Distillation / Imitation Learning
**Why needed:** The method relies on a 2-stage process where a strong "Teacher" (Text-LLM) generates the training data for the "Student" (SLLM).
**Quick check:** Why is it cheaper to use the Teacher to label transcripts than to hire humans to annotate speech directly?

### Concept: Catastrophic Forgetting & Freezing
**Why needed:** The paper explicitly freezes the LLM decoder to prevent the loss of internal knowledge that often happens when fine-tuning on small, specific speech datasets.
**Quick check:** Why does the paper claim standard SFT degrades the model's ability to follow prompts?

## Architecture Onboarding

### Component map:
Raw Audio -> Speech Encoder (WavLM Large, Frozen) -> Connector (CNN Subsampler, Trainable) -> LLM Decoder (LLaMA2-7B, Frozen)

### Critical path:
1. Data Generation: Run frozen Text-LLM on transcripts to generate high-quality text responses
2. Interleaving (Optional): With 40% probability, replace text spans with CosyVoice 2 synthesized speech
3. Forward Pass: Encode audio, pass through Connector, concatenate with Text Prompt tokens, pass to LLM
4. Loss: Cross-entropy between SLLM output and generated text targets, update only Connector

### Design tradeoffs:
- ASR SFT vs. Behavior Imitation: Small ASR SFT ensures transcription capability while behavior imitation transfers reasoning
- Interleaving Probability: Set to 40% - higher rates might destabilize, lower rates reduce benefits
- Data Quality vs. Quantity: 960h of high-quality LibriSpeech beat larger amounts of noisier specialized data

### Failure signatures:
- Overfitting to ASR: High ASR accuracy but poor instruction following on diverse prompts
- Transcription Error Propagation: Slight errors in transcribed numbers lead to 0 credit for math reasoning steps

### First 3 experiments:
1. Train connector on LibriSpeech using only ASR task - verify WER drops below 4.0
2. Switch target generation to Continuation/Selecting tasks - compare instruction-following on unseen audio
3. Evaluate on GSM8K subset - if model attempts to solve math problems (even if wrong), alignment is working

## Open Questions the Paper Calls Out

### Open Question 1
Can MTBI be extended to incorporate nonlinguistic speech features (emotion, prosody) without degrading linguistic alignment? The current study focuses exclusively on content-related tasks using transcripts, deliberately separating linguistic content from other acoustic properties.

### Open Question 2
Does TTS-synthesized speech for interleaving introduce domain mismatch compared to naturally occurring speech? The paper uses CosyVoice 2 to avoid reliance on time-aligned datasets but doesn't isolate quality/naturalness effects.

### Open Question 3
To what extent is the improvement dependent on the specific constructed tasks versus high-quality LibriSpeech data? The ablation study combines these tasks with multi-task learning, making it unclear if these specific semantic tasks are necessary for the observed reasoning boost.

## Limitations
- Relies heavily on quality of frozen text-LLM teacher model - biases and limitations are inherited
- Assumes perfect time-alignment between speech and transcript, but real ASR introduces timing uncertainties
- Constructed tasks may not fully capture real-world speech scenario complexity
- Dependence on synthetic speech generation introduces potential domain mismatch
- Evaluation on clean datasets may not reflect practical acoustic variability

## Confidence
- **High Confidence:** Core claim that behavior imitation improves generalization over SFT is well-supported by consistent experimental improvements
- **Medium Confidence:** Speech-text interleaving specifically enhances alignment efficiency is supported by ablation studies but benefit magnitude is modest
- **Low Confidence:** Constructed tasks superiority over specialized task training is based on single comparison that may be influenced by dataset quality differences

## Next Checks
1. Evaluate MTBI-trained model on significantly different domains (conversational, accented, noisy speech) to assess generalization beyond clean datasets
2. Systematically vary text-LLM teacher quality/capabilities to quantify dependency and determine minimum threshold for method success
3. Conduct detailed analysis of speech-text alignment including cross-modal embedding similarity and performance degradation when alignment is corrupted