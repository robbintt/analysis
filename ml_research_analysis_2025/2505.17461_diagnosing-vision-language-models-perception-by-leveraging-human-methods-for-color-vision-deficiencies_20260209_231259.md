---
ver: rpa2
title: Diagnosing Vision Language Models' Perception by Leveraging Human Methods for
  Color Vision Deficiencies
arxiv_id: '2505.17461'
source_url: https://arxiv.org/abs/2505.17461
tags:
- color
- vision
- ishihara
- protanopia
- normal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigated whether large-scale vision-language models
  (LVLMs) can simulate variation in human color perception using the Ishihara Test.
  Through generation, confidence, and internal representation analyses, we found that
  LVLMs, despite possessing factual knowledge about color vision deficiencies, fail
  to reproduce perceptual outcomes experienced by individuals with CVDs and instead
  default to normative color perception.
---

# Diagnosing Vision Language Models' Perception by Leveraging Human Methods for Color Vision Deficiencies

## Quick Facts
- arXiv ID: 2505.17461
- Source URL: https://arxiv.org/abs/2505.17461
- Authors: Kazuki Hayashi; Shintaro Ozaki; Yusuke Sakai; Hidetaka Kamigaito; Taro Watanabe
- Reference count: 32
- Key outcome: LVLMs fail to simulate CVD perceptual outcomes despite factual knowledge, defaulting to normative color perception across all tested models and prompt strategies.

## Executive Summary
This work investigates whether large-scale vision-language models can simulate human color perception variations using the Ishihara Test as a diagnostic probe. Despite possessing factual knowledge about color vision deficiencies (CVDs), LVLMs fail to reproduce the perceptual outcomes experienced by affected individuals, instead defaulting to normative color perception. Through generation accuracy, confidence (perplexity), and internal representation analyses across six diverse models, the study reveals that current LVLMs lack mechanisms for representing alternative perceptual experiences, raising concerns for accessibility and inclusive deployment in multimodal AI systems.

## Method Summary
The study evaluates LVLM ability to simulate altered color perception using 25 Ishihara plates with ground-truth digits for Normal, Protanopia, Deuteranopia, and Tritanopia conditions. Six models (Phi-3.5, Qwen2.5-VL, mPLUG-Owl3, Llama-3.2, LLaVA-NeXT, GPT-4o) are tested under four prompt types: Base, Linguistic Support, Visual Support, and Doctor-Style. Three evaluation metrics are used: digit accuracy (generation matching ground-truth), per-token perplexity (confidence in forced-decoded answers), and layer-wise diagnosis probability (LogitLens probing of internal representations). The evaluation is inference-only, with no model training or fine-tuning beyond two pre-trained variants.

## Key Results
- LVLMs show high accuracy for Normal vision (71-90%) but collapse to <24% for Protanopia/Deuteranopia across all models
- Linguistic and visual support prompts fail to improve CVD simulation performance
- Layer-wise probing reveals minimal differentiation between instructed perceptual conditions, indicating weak instruction-conditioned representation
- Perplexity is lowest for Normal condition across all models, indicating highest confidence for normative perception

## Why This Works (Mechanism)

### Mechanism 1: Perceptual Simulation Gap
- **Claim:** LVLMs possess linguistic knowledge about CVDs but cannot translate this into simulated perceptual states.
- **Mechanism:** Knowledge about perception is stored separately from the perceptual processing pipeline itself.
- **Core assumption:** Knowledge about perception is stored separately from the perceptual processing pipeline itself.
- **Evidence anchors:** Models can describe CVD definitions correctly but fail to reproduce perceptual outcomes; similar limitations found in ColorBench evaluation.

### Mechanism 2: Normative Vision Prior from Training Data
- **Claim:** LVLMs default to normative color perception because their training data implicitly assumes typical trichromatic vision.
- **Mechanism:** Web-scale image-text corpora predominantly originate from individuals with normal color vision, creating strong priors that models learn as default perceptual mode.
- **Core assumption:** The training distribution's perceptual assumptions are encoded as strong priors in model behavior.
- **Evidence anchors:** Normal condition yields highest accuracy (71-90%) while CVD conditions collapse to <24%; perplexity lowest for Normal across all models.

### Mechanism 3: Weak Instruction-Conditioned Representation
- **Claim:** Internal representations do not meaningfully differentiate between instructed perceptual states.
- **Mechanism:** The instruction "You are Protanopic" fails to substantially alter visual information processing in hidden layers.
- **Core assumption:** Instruction-following for perception requires the instruction to modulate early-to-mid visual processing.
- **Evidence anchors:** Layer-wise diagnosis shows nearly identical curves across conditions; doctor-style prompts show inverted patterns suggesting condition labels aren't grounded in perceptual reasoning.

## Foundational Learning

- **Concept: Color Vision Deficiency Types**
  - **Why needed here:** Understanding that Protanopia (L-cone absence), Deuteranopia (M-cone absence), and Tritanopia (S-cone absence) produce different perceptual confusions is essential for interpreting why Ishihara plates work as diagnostic probes.
  - **Quick check question:** Why does Tritanopia serve as a control condition in this study (hint: what are its ground-truth digits)?

- **Concept: Ishihara Test Design**
  - **Why needed here:** The plates exploit specific color confusions to elicit different digit responses across vision types, making them ideal controlled stimuli for evaluating whether models can simulate perceptual variation.
  - **Quick check question:** If a person with Protanopia sees "3" on a plate where normal viewers see "8," what does this reveal about their perceptual confusions?

- **Concept: Layer-wise Probing (LogitLens)**
  - **Why needed here:** This technique reveals how token probabilities evolve across transformer layers, allowing analysis of when/how perceptual decisions form internally.
  - **Quick check question:** If diagnosis probabilities for all conditions converge to similar values by the final layer, what does this suggest about the model's internal differentiation?

## Architecture Onboarding

- **Component map:** Vision encoder -> Projection layer -> Language model backbone -> Unembedding matrix (W_unembed) -> Token generation
- **Critical path:** Ishihara plate + condition prompt → vision encoder → visual tokens → transformer layers → final hidden state → digit generation OR layer-wise diagnosis probabilities
- **Design tradeoffs:** 25 plates target red-green deficiencies only; limited generalizability to other perceptual variations; prompt variations tested but none overcame grounding limitation
- **Failure signatures:** Mode collapse to repeated digits; condition insensitivity (identical outputs for Normal vs CVD); high perplexity variance; late-layer probability convergence
- **First 3 experiments:**
  1. **Baseline digit accuracy:** Run Base prompts for all 25 plates across Normal/Protanopia/Deuteranopia/Tritanopia to confirm normative default behavior
  2. **Perplexity profiling:** Compute per-token perplexity for gold answers under each condition to quantify confidence gaps
  3. **Layer-wise diagnosis probing:** Apply LogitLens to extract condition-label probabilities at each transformer layer; check if curves separate by ground-truth condition

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What architectural or training modifications could enable LVLMs to represent and simulate alternative perceptual experiences like CVDs?
- **Basis in paper:** The conclusion states: "These findings indicate that current LVLMs lack mechanisms for alternative perceptual experiences, highlighting the need for approaches that support perceptual diversity."
- **Why unresolved:** The study is explicitly diagnostic, not prescriptive; no mitigation strategies were tested.
- **What evidence would resolve it:** Demonstrating improved CVD simulation accuracy after specific architectural changes or training procedures.

### Open Question 2
- **Question:** Do findings from Ishihara plates generalize to natural images and real-world color perception tasks?
- **Basis in paper:** Limitations section states: "Performance on stylised Ishihara plates does not imply that a model can handle color perception in broader settings... Caution is therefore required when extrapolating to natural images."
- **Why unresolved:** Ishihara plates are stylized diagnostic tools; models may use dot-pattern heuristics that don't transfer.
- **What evidence would resolve it:** Evaluation on natural image datasets with controlled color manipulations, showing consistent CVD simulation patterns across both Ishihara and real-world tasks.

### Open Question 3
- **Question:** Can LVLMs be trained to simulate CVD perception through exposure to physiologically accurate CVD image transformations or specialized datasets?
- **Basis in paper:** Neither medical domain knowledge nor dot-pattern memorization helped; unclear what training paradigm could induce genuine perceptual simulation.
- **Why unresolved:** Neither medical domain knowledge nor dot-pattern memorization helped; unclear what training paradigm could induce genuine perceptual simulation.
- **What evidence would resolve it:** Training models on physiologically-based CVD image simulations and showing improved accuracy on held-out Ishihara plates and natural images.

## Limitations
- Restricted dataset scope to 25 Ishihara plates targeting only red-green deficiencies
- Evaluation relies entirely on inference without intervention, preventing causal insights
- Conflation of symbolic knowledge retrieval with genuine perceptual transformation
- Layer-wise probing cannot definitively separate vision encoder vs cross-modal integration failures

## Confidence
- **High confidence (3/3):** LVLMs fail to simulate CVD perceptual outcomes despite factual knowledge about color vision deficiencies
- **Medium confidence (2/3):** Failure stems from training data biases creating strong normative priors
- **Medium confidence (2/3):** Layer-wise probing reveals minimal instruction-conditioned representation differentiation

## Next Checks
1. **Dataset expansion validation:** Test the same evaluation framework on plates targeting Tritanopia ground-truth variations to verify whether normative collapse is specific to red-green deficiencies
2. **Causal intervention study:** Systematically vary training data composition by fine-tuning models on synthetically generated CVD-perspective image-text pairs, then re-evaluate Ishihara performance
3. **Architectural ablation validation:** Compare LogitLens patterns between pure vision encoders and multimodal LVLMs on Ishihara plates to isolate whether grounding failure occurs at vision encoder level or during cross-modal integration