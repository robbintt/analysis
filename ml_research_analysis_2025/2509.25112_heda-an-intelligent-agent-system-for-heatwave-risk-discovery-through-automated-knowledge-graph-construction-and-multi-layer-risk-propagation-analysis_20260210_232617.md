---
ver: rpa2
title: 'HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated
  Knowledge Graph Construction and Multi-layer Risk Propagation Analysis'
arxiv_id: '2509.25112'
source_url: https://arxiv.org/abs/2509.25112
tags:
- risk
- knowledge
- discovery
- scientific
- climate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HeDA addresses the challenge of fragmented knowledge in climate
  risk assessment by introducing an intelligent multi-agent system that autonomously
  discovers overlooked risk transmission pathways. The system processes over 10,000
  academic papers to construct a comprehensive knowledge graph and employs novel multi-layer
  risk propagation analysis to identify previously unknown risk chains.
---

# HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis

## Quick Facts
- arXiv ID: 2509.25112
- Source URL: https://arxiv.org/abs/2509.25112
- Authors: Yiquan Wang; Tin-Yeh Huang; Qingyun Gao; Jialin Zhang
- Reference count: 25
- Primary result: Multi-agent system discovers overlooked heatwave risk chains from 10,247 papers with 78.9% QA accuracy

## Executive Summary
HeDA introduces an intelligent multi-agent system that autonomously discovers overlooked risk transmission pathways in climate science by processing over 10,000 academic papers to construct a comprehensive knowledge graph. The system employs novel multi-layer risk propagation analysis to identify previously unknown risk chains, achieving 78.9% accuracy in complex question-answering tasks and outperforming GPT-4 by 13.7%. Through expert validation and historical case studies, HeDA successfully discovered five high-impact risk chains, demonstrating the potential for AI-driven scientific discovery in climate adaptation strategy development.

## Method Summary
HeDA processes 10,247 academic papers through a multi-agent pipeline orchestrated by a Master Agent. The Data Processing Agent extracts 8-15 relations per paper using LLM-based relation extraction, then applies semantic clustering and LLM-based canonicalization to build a knowledge graph with 23,156 nodes. The Knowledge Graph Agent stores this in Neo4j with optimized indexes. The QA Agent performs semantic entity matching and multi-hop retrieval using constrained BFS (depth ≤ 5) with a Novelty Score combining literature frequency, cross-layer connectivity, and impact probability. The system uses BGE-large-en-v1.5 embeddings and Qwen-Max (72B) LLM, running on 32-core CPU with 64GB RAM.

## Key Results
- 78.9% accuracy on 500 benchmark questions (13.7% improvement over GPT-4)
- Discovery of 5 previously unidentified high-impact risk chains validated by experts
- Knowledge graph construction from 10,247 papers yielding 23,156 nodes and 89,420 relationships

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Multi-Agent Specialization
Decomposing scientific discovery into specialized agents (Data, KG, QA, Evaluation) orchestrated by a Master Agent improves robustness over monolithic approaches. The Master Agent dynamically schedules tasks and handles errors via checkpoints while the Data Processing Agent handles scientific text constraints before passing structured data downstream.

### Mechanism 2: Semantic Entity Resolution for Graph Connectivity
High-quality knowledge graph construction requires semantic clustering and LLM-based canonicalization to bridge disciplinary terminology gaps. The system clusters similar entities using sentence transformers (FAISS) then merges synonyms through LLM-based standardization, reducing graph sparsity.

### Mechanism 3: Constrained Cross-Layer Pathfinding
Systematic discovery of "unknown unknowns" is achieved by rewarding graph traversal paths that cross semantic layers (Physical → Social → Economic) while penalizing high-frequency pathways. The algorithm uses constrained BFS with a Novelty Score (LF + CLC + IP) to find gaps in scientific discourse.

## Foundational Learning

- **Concept:** Knowledge Graph Construction (KGC) & Triplet Extraction
  - **Why needed here:** HeDA relies on converting unstructured PDF text into structured (Head, Relation, Tail) triplets to build the network
  - **Quick check question:** Can you explain how an LLM converts "Heatwaves cause respiratory issues" into a structured triplet?

- **Concept:** Breadth-First Search (BFS) vs. Depth-First Search (DFS)
  - **Why needed here:** The risk discovery algorithm explicitly chooses BFS to ensure systematic coverage
  - **Quick check question:** Why would DFS be a poor choice for discovering all risk chains starting from "Heatwave" up to depth 5?

- **Concept:** Semantic Deduplication / Entity Resolution
  - **Why needed here:** The paper emphasizes "Entity Standardization" as a key step (7.6% accuracy boost in ablation)
  - **Quick check question:** If the system creates two nodes for "H2O" and "Water", how would this affect multi-hop risk propagation accuracy?

## Architecture Onboarding

- **Component map:** Master Agent -> Data Processing Agent -> KG Agent -> QA Agent -> Evaluation Agent
- **Critical path:** The Entity Standardization step within the Data Processing Agent is the critical bottleneck
- **Design tradeoffs:** Completeness vs. Hallucination (8-15 relations per paper constraint) and Computational Cost (BFS on 89k relationships)
- **Failure signatures:** Low Connectivity (<2 average path length), High Novelty Low Validity (spurious correlations), Stuck Pipeline (QA Engine failure)
- **First 3 experiments:** 1) Unit Test Extraction on 10 known papers, 2) Graph Connectivity Probe with simple query, 3) Scoring Sensitivity by modifying novelty weights

## Open Questions the Paper Calls Out

- **Open Question 1:** How can causal discovery techniques like do-calculus be integrated to distinguish genuine causal mechanisms from mere co-occurrence patterns?
- **Open Question 2:** Can Bayesian networks be effectively incorporated to provide probabilistic uncertainty bounds for novelty scores?
- **Open Question 3:** To what extent does the English-only corpus bias risk discovery toward temperate climates, obscuring critical pathways in the Global South?

## Limitations
- The system may identify low-frequency pathways that are understudied rather than genuinely novel
- Novelty Score weights were likely tuned on limited validation data, raising overfitting concerns
- English-only corpus potentially misses critical pathways in tropical regions (73% data from developed countries)

## Confidence

- **High Confidence:** Multi-agent architecture and Neo4j knowledge graph construction methodology are well-specified and reproducible
- **Medium Confidence:** Risk discovery algorithm is logically sound but relies heavily on arbitrary weight parameters
- **Low Confidence:** Claims of "autonomous scientific discovery" overstate capabilities within predefined constraints

## Next Checks

1. **Ablation Study on Entity Resolution:** Remove LLM-based canonicalization and measure impact on graph connectivity and risk discovery quality
2. **Weight Sensitivity Analysis:** Systematically vary Novelty Score weights across full parameter space and document changes to top-5 discovered risk chains
3. **Temporal Validation Test:** Apply HeDA to historical literature (2000-2010) and compare discovered risk chains to actual outcomes in subsequent decade