---
ver: rpa2
title: Improving KAN with CDF normalization to quantiles
arxiv_id: '2507.13393'
source_url: https://arxiv.org/abs/2507.13393
tags:
- normalization
- distribution
- polynomials
- cdfkal
- legendre
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes replacing the MinMax rescaling used in Legendre-KAN
  with CDF normalization to quantiles, aiming to improve representation and generalization.
  CDF normalization transforms inputs via their cumulative distribution function to
  a nearly uniform distribution, which better matches the assumptions of orthonormal
  polynomial bases like Legendre polynomials.
---

# Improving KAN with CDF normalization to quantiles

## Quick Facts
- arXiv ID: 2507.13393
- Source URL: https://arxiv.org/abs/2507.13393
- Authors: Jakub Strawa; Jarek Duda
- Reference count: 12
- Primary result: CDF-normalized KAN variants achieve 0.5–2 percentage points higher MNIST test accuracy than MinMax rescaling, especially for lower-degree polynomials

## Executive Summary
This paper proposes replacing the MinMax rescaling used in Legendre-KAN with CDF normalization to quantiles, aiming to improve representation and generalization. The approach transforms inputs via their cumulative distribution function to a nearly uniform distribution, better matching the assumptions of orthonormal polynomial bases like Legendre polynomials. Inspired by copula theory and hierarchical correlation reconstruction (HCR), the method interprets polynomial coefficients as mixed moments providing local joint distribution models. Experiments on MNIST show consistent accuracy improvements and faster convergence, particularly when using trainable LayerNorm before the CDF transform.

## Method Summary
The proposed method replaces MinMax rescaling with CDF normalization before applying Legendre polynomial expansions in KAN layers. The transformation involves standardizing inputs (mean subtraction, std division), applying Gaussian CDF via error function, then evaluating Legendre polynomials using recurrence relations. The approach is motivated by the need to match orthonormal polynomial basis assumptions and eliminate degenerate mass points from repeated input values. The paper introduces CDFKAL variants including learnable LayerNorm (CDFKAL_NET), fixed LayerNorm (CDFKAL_NET_FIXEDNORM), and residual connections. HCR interpretation views coefficients as mixed moments enabling better generalization through statistical feature extraction rather than local memorization.

## Key Results
- MNIST test accuracy improvements of 0.5–2 percentage points over original MinMax approach
- Best variant (CDFKAL_NET with learnable LayerNorm) converges approximately twice as fast
- Fixed LayerNorm variant achieves 15% training time reduction without accuracy loss
- Lower-degree polynomials (3-7) show best generalization, avoiding overfitting at higher degrees
- CDF normalization produces more uniform intermediate distributions compared to MinMax

## Why This Works (Mechanism)

### Mechanism 1: Distribution Matching to Orthonormal Basis Assumptions
CDF normalization improves low-degree polynomial representation by transforming input distributions to approximately uniform, matching the orthonormality weight function of Legendre polynomials. Legendre polynomials are orthonormal on [0,1] with constant weight w(x)=1, making them efficient for uniformly distributed inputs. CDF transforms any continuous distribution to nearly uniform via x → CDF(x), enabling efficient representation with lower-degree polynomials.

### Mechanism 2: Degenerate Mass Point Elimination
CDF normalization improves gradient flow by eliminating exact-value clustering that MinMax creates from repeated input values. MinMax maps all identical values to the same output (e.g., many pixels → exactly 0), creating "degenerate mass points" that concentrate gradient signals and reduce effective diversity. CDF's monotonic transformation spreads values based on their rank/order, breaking exact ties.

### Mechanism 3: Mixed Moment Interpretation for Generalization
CDF normalization enables polynomial coefficients to represent standardized mixed moments, extracting generalizable statistical features rather than data-specific patterns. Under HCR interpretation, Legendre coefficients approximate moments (a₁∼mean, a₂∼variance, a₃∼skewness, a₄∼kurtosis). For uniform inputs, these represent normalized, comparable statistics. The paper argues global polynomial bases extract "general features: mixed moments" that generalize better than local methods like KDE.

## Foundational Learning

**Orthonormal Polynomials and Weight Functions**
- Why needed here: Understanding why uniform inputs matter—Legendre polynomials are orthonormal specifically with constant weight on [0,1], meaning they're optimized for uniformly distributed inputs.
- Quick check question: If Legendre polynomials are orthonormal on [0,1] with weight w(x)=1, why would a Gaussian-distributed input require different basis functions for optimal efficiency?

**Cumulative Distribution Function (CDF) and Probability Integral Transform**
- Why needed here: The core normalization mechanism—understanding why F_X(X) produces uniform distribution for continuous random variable X.
- Quick check question: For a continuous random variable X with CDF F, what is the distribution of Y = F(X)? Why does this break for discrete random variables?

**Kolmogorov-Arnold Representation Theorem**
- Why needed here: Understanding KAN's theoretical foundation—any continuous multivariate function can be represented as composition of univariate functions.
- Quick check question: The KA theorem guarantees existence of a representation with 2d_in+1 outer functions. How does this differ from universal approximation theorems for MLPs?

## Architecture Onboarding

**Component map:**
Input [B, n] → LayerNorm(γ, β) → Gaussian CDF: (1+erf(x/√2))/2 → [B, n] in (0,1) → Legendre Polynomials P₀:d for each feature → [B, n×(d+1)] → Linear(weights W) → [B, m] → (optional) SiLU residual connection

**Critical path:**
1. Standardization: Per-feature mean subtraction, std division
2. Gaussian CDF transformation via error function
3. Legendre polynomial evaluation using recurrence P₀=1, P₁=x, (k+1)Pₖ₊₁=(2k+1)xPₖ - kPₖ₋₁
4. Weighted sum of polynomial features
5. Inter-layer normalization before next CDFKal layer

**Design tradeoffs:**
| Choice | Accuracy | Training Speed | When to Use |
|--------|----------|----------------|-------------|
| Learnable LayerNorm (CDFKAL_NET) | Best | Baseline | Maximum accuracy needed |
| Fixed LayerNorm (CDFKAL_NET_FIXEDNORM) | ~Same | ~15% faster | Production deployment |
| +SiLU residual | ~Same | Slowest | Non-polynomial features expected |
| Polynomial degree 3-7 | Best generalization | Faster | Default |
| Polynomial degree >8 | Overfitting risk | Slower | Very complex functions only |

**Failure signatures:**
- Non-uniform intermediate histograms: Layer 1 is "far from uniform" even with CDF
- Overfitting at high degrees: Test accuracy drops after degree 7
- Negative density in HCR: Polynomial density parametrization can produce ρ<0 in sparse regions
- Slow convergence with MinMax: If using original KAL_NET, expect ~2× more epochs

**First 3 experiments:**
1. Baseline verification on MNIST: Implement CDFKAL_NET_FIXEDNORM with degree 5, 3 layers. Target: >97% test accuracy, convergence by epoch 10. Compare histogram uniformity against original KAL_NET.
2. LayerNorm ablation: Compare learnable vs. fixed γ,β across degrees 3,5,7. Measure both final accuracy and wall-clock time.
3. Distribution shift robustness test: Train on standard MNIST, evaluate on MNIST with brightness/contrast perturbations. Hypothesis: CDF normalization's quantile-based approach should be more robust to affine intensity shifts than MinMax.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited experimental validation beyond MNIST digit classification
- Gaussian CDF assumption may not hold for categorical, multimodal, or heavy-tailed distributions
- Computational overhead of error function evaluations and polynomial basis expansion not fully characterized

## Confidence
- **High Confidence**: MNIST accuracy improvements (0.5-2 percentage points) and faster convergence with learnable LayerNorm are directly reproducible
- **Medium Confidence**: Theoretical justification for CDF normalization via orthonormal polynomial assumptions and copula theory requires broader empirical validation
- **Low Confidence**: Claims about universal applicability across distribution types and superiority of moment-based feature extraction over local methods

## Next Checks
1. **Distribution Sensitivity Analysis**: Test CDFKAL on datasets with deliberately varied input distributions (uniform, exponential, bimodal, categorical) to quantify performance degradation when Gaussian CDF assumption breaks. Compare against adaptive EDF approaches.

2. **HCR Density Estimation Validation**: Implement the HCR-based density estimation framework and validate on synthetic distributions where ground truth moments are known. Quantify negative density occurrences and calibration requirements.

3. **Memory/Compute Overhead Benchmarking**: Profile CDFKAL versus original KAL_NET on increasing input dimensions and polynomial degrees. Measure both wall-clock time and GPU memory usage to identify scalability bottlenecks.