---
ver: rpa2
title: 'VQ-Style: Disentangling Style and Content in Motion with Residual Quantized
  Representations'
arxiv_id: '2602.02334'
source_url: https://arxiv.org/abs/2602.02334
tags:
- style
- motion
- content
- transfer
- styles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of disentangling style and content
  in human motion data for style transfer. The core method uses a Residual Vector
  Quantized Variational Autoencoder (RVQ-VAE) to learn a coarse-to-fine representation,
  where initial codebooks capture content and later ones encode style.
---

# VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations

## Quick Facts
- arXiv ID: 2602.02334
- Source URL: https://arxiv.org/abs/2602.02334
- Reference count: 15
- Primary result: 83.20% style accuracy on seen styles, 68.95% on unseen, with 7.5cm content trajectory deviation

## Executive Summary
VQ-Style addresses the challenge of disentangling style and content in human motion data for style transfer. The method uses a Residual Vector Quantized Variational Autoencoder (RVQ-VAE) that learns a coarse-to-fine representation, where early codebooks capture content and later ones encode style. This is enhanced with contrastive learning and mutual information minimization to prevent style leakage. The primary result is effective zero-shot style transfer on datasets like 100STYLE, achieving strong style accuracy while preserving content motion.

## Method Summary
VQ-Style learns disentangled motion representations using residual vector quantization. A sequential quantization structure encodes content in early codebooks and style in later ones through residual error decomposition. The model is trained with reconstruction losses, contrastive learning to organize style codes, and mutual information minimization to isolate content from style information. At inference, style transfer is performed by swapping codebooks at a specified layer boundary without requiring fine-tuning for unseen styles.

## Key Results
- Achieves 83.20% style accuracy on seen styles and 68.95% on unseen styles in zero-shot transfer
- Preserves content motion with average trajectory deviation of 7.5 cm on 100STYLE dataset
- Enables various inference-time applications including style transitions and data augmentation

## Why This Works (Mechanism)

### Mechanism 1: Residual Quantization Naturally Separates Coarse from Fine
The sequential residual quantization structure causes early codebooks to encode coarse motion (content/trajectory) while later codebooks capture finer details (style). Each codebook quantizes the residual error from previous stages, with content corresponding to gross motion structure and style to fine nuances. This hierarchical assignment emergently separates content and style without separate encoders.

### Mechanism 2: Post-Quantization Contrastive Loss Isolates Style Codebooks
Applying contrastive loss to residual embeddings after quantization allows gradients to update only style codebooks without affecting content codebooks. The straight-through estimator blocks gradient flow to earlier stages, enabling contrastive loss to cluster same-style codes while preserving content codebook isolation.

### Mechanism 3: Mutual Information Minimization Prevents Style Leakage to Content Codes
Minimizing the mutual information between content codes and style labels ensures content codebooks cannot predict style, forcing style information into residual codebooks. Soft assignment probabilities estimate the joint distribution, and minimizing MI via Monte Carlo sampling removes style-correlated information from early codebooks.

## Foundational Learning

- **Vector Quantization (VQ) basics**: Understanding how discrete codebooks replace continuous latents via nearest-neighbor assignment is prerequisite to grasping residual VQ. Quick check: Given embedding $r$ and codebook $B=\{c_1, c_2, c_3\}$, which code is selected and what is the commitment loss?

- **Residual coding / coarse-to-fine representation**: The core hypothesis that sequential residual quantization naturally factors content vs. style relies on understanding multi-scale decomposition. Quick check: After first codebook quantizes $r_0$ to $z_0$, what does $r_1 = r_0 - z_0$ represent, and what information does it contain?

- **Contrastive learning objectives**: The Multi-Pos contrastive loss organizes latent space by style labels; understanding positive/negative pair construction is essential. Quick check: In a batch with samples having style labels [A, A, B, C], which pairs are positives and which are negatives for anchor at index 0?

## Architecture Onboarding

Motion M (T frames) → Encoder (1D CNN, downsamples) → Latent r_0 [K embeddings] → Quantization layers 0..N-1 (each: nearest-neighbor lookup in codebook B_i) → Code indices z_i per layer → Decoder (1D deconv) → Reconstructed motion M'

Critical path: The swap index `s` (default 1) determines content/style boundary. Swapping at `s=1` replaces $z_{1:N-1}$ from content with style codes; `s=2` keeps more content detail but transfers less style.

Design tradeoffs:
- More codebooks (N=8) → better reconstruction but more compute; fewer (N=4) acceptable for smaller datasets
- Larger codebook (512) → more expressive but risk unused codes; EMA + code reset mitigates this
- Higher $L_{mi}$ weight → better style accuracy but worse content deviation

Failure signatures:
- Foot sliding artifacts: reduce residual layers used at inference or increase $L_{acc}$ weight
- Style not transferred: check if `s` is too high (keeping too many content codebooks)
- Content drift over long sequences: velocity errors accumulate; worse for sharp turns in locomotion

First 3 experiments:
1. **Sanity check**: Train RVQ-VAE with only $L_{rec}$ on a small subset; verify reconstruction quality and visualize t-SNE of $r_0$ vs $r_1$—should see no style clustering yet.
2. **Ablation on s**: With full training, run style transfer at s=0,1,2 on held-out clips; measure style accuracy vs content deviation to find optimal cutoff for your dataset.
3. **Unseen style test**: Train on N-10 styles, test on 10 withheld styles; verify zero-shot transfer works by encoding unseen style clip and swapping codes.

## Open Questions the Paper Calls Out

### Open Question 1
Can unsupervised style discovery via clustering effectively replace annotated style labels within the VQ-Style framework? The authors state in the conclusion: "To disentangle unannotated data, one could perform style discovery through unsupervised clustering... We leave the investigation of these challenges to future research."

### Open Question 2
Would operating in a global reference space mitigate the trajectory drift observed in long motion sequences? The paper notes that velocity errors accumulate because "the final motion is calculated by integrating the velocities over time," and suggests "operating in a global reference space could potentially improve content preservation; however, we leave this investigation for future work."

### Open Question 3
How can quantitative evaluation metrics be refined to avoid penalizing styles that inherently alter content trajectories? The authors argue that "developing better quantitative metrics for style transfer remains an open task," noting that for styles like "Drunk," a high trajectory error might actually indicate a successful transfer rather than a failure.

## Limitations
- Architecture underspecification with missing details on kernel sizes, layer counts, and downsampling factors
- Unbounded codebooks risk accumulating unused codes over long training despite reset mechanisms
- Trajectory deviation metrics may not fully capture content preservation quality for styles that legitimately alter motion paths

## Confidence
- **High confidence (✦✦✦✦)** in core technical contributions: Residual VQ-VAE structure, contrastive loss applied post-quantization, and mutual information minimization
- **Medium confidence (✦✦✦)** in style-content disentanglement claims: Strong quantitative results but relies on annotated style labels and classifier-based metrics
- **Low confidence (✦)** in generalizability across motion types: Limited validation on non-locomotion motions beyond locomotion focus

## Next Checks
1. **Ablation on codebook depth**: Train RVQ-VAEs with N=4, N=8, and N=12 codebooks on the same dataset, measuring style accuracy and content deviation at each level.
2. **Unsupervised style discovery**: Remove style labels during training and apply clustering algorithms (e.g., k-means) to the residual embeddings at different layers.
3. **Cross-dataset style transfer**: Train on 100STYLE locomotion styles, then test zero-shot transfer to non-locomotion motions from Xia or Aberman datasets.