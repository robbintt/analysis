---
ver: rpa2
title: Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked
  Claim Detection
arxiv_id: '2509.25138'
source_url: https://arxiv.org/abs/2509.25138
tags:
- task
- language
- bias
- retrieval
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study identifies language and retrieval bias in multilingual
  previously fact-checked claim detection (PFCD). Language bias manifests as LLMs
  performing worse on low-resource languages due to skewed pretraining data and English-centric
  tuning, with multilingual prompting partially mitigating but not eliminating disparities.
---

# Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection

## Quick Facts
- arXiv ID: 2509.25138
- Source URL: https://arxiv.org/abs/2509.25138
- Reference count: 38
- Primary result: Language and retrieval bias significantly impact multilingual previously fact-checked claim detection performance

## Executive Summary
This study systematically investigates biases in multilingual previously fact-checked claim detection (PFCD) systems, revealing critical disparities across languages and retrieval mechanisms. The research demonstrates that language bias causes LLMs to perform substantially worse on low-resource languages due to skewed pretraining data and English-centric tuning, while retrieval bias leads to disproportionate representation of certain claims regardless of their actual relevance. The analysis spans 20 languages and six different models, providing comprehensive empirical evidence of these systematic biases. The findings highlight that fairness and accuracy in multilingual fact-checking require addressing both language-specific performance gaps and retrieval mechanism biases through robust prompting strategies and bias-aware filtering approaches.

## Method Summary
The study employs a comprehensive experimental framework analyzing six large language models across 20 different languages to evaluate performance in previously fact-checked claim detection tasks. Researchers implemented multilingual prompting strategies to assess their effectiveness in mitigating language bias, while also examining embedding-based retrieval systems for evidence of systematic claim overrepresentation. The methodology includes both quantitative performance metrics and qualitative analysis of retrieval patterns to identify bias manifestations. LLM-based relevance filtering is introduced and evaluated as a potential mitigation strategy for retrieval bias, with comparisons made between filtered and unfiltered retrieval outputs to measure improvements in semantic alignment and bias reduction.

## Key Results
- LLMs exhibit significant performance drops on non-English inputs compared to English, with multilingual prompting partially mitigating but not eliminating disparities
- Embedding-based retrieval systems disproportionately retrieve certain claims, particularly high-profile or generic ones, regardless of actual relevance to queries
- LLM-based relevance filtering improves semantic alignment and reduces retrieval bias, suggesting potential for bias-aware mitigation strategies

## Why This Works (Mechanism)
The effectiveness of LLM-based relevance filtering in reducing retrieval bias stems from its ability to perform contextual understanding beyond simple semantic similarity measures used in traditional embedding-based systems. By leveraging the broader contextual reasoning capabilities of LLMs, the filtering approach can better distinguish between genuinely relevant claims and those that appear similar based on surface-level features but lack substantive connection to the query. This mechanism addresses the fundamental limitation of embedding-based retrieval, which often relies on vector proximity that can be dominated by high-frequency terms or generic claim structures, leading to systematic overrepresentation of certain claim types.

## Foundational Learning

### Language Bias in LLMs
**Why needed**: Understanding how pretraining data distribution affects multilingual performance is crucial for developing fair fact-checking systems
**Quick check**: Compare performance on high-resource vs low-resource languages to identify systematic disparities

### Retrieval Bias Mechanisms
**Why needed**: Recognizing how embedding-based systems can systematically overrepresent certain claims regardless of relevance
**Quick check**: Analyze frequency distribution of retrieved claims across different query types

### Multilingual Prompting Strategies
**Why needed**: Evaluating techniques to improve LLM performance across diverse language inputs
**Quick check**: Test multiple prompting approaches and measure performance improvements on non-English queries

## Architecture Onboarding

### Component Map
Multral multilingual input → LLM model → Multilingual prompt → Claim retrieval → Embedding-based filtering → LLM-based relevance filtering → Output claims

### Critical Path
Query input → Multilingual prompt generation → Retrieval system → Relevance filtering → Final claim selection

### Design Tradeoffs
The study balances retrieval recall (finding relevant claims) against precision (avoiding irrelevant claims), with embedding-based systems favoring recall but introducing bias, while LLM-based filtering improves precision but may reduce recall. The choice between English-centric tuning and true multilingual pretraining represents a fundamental tradeoff between performance on dominant languages versus fairness across all supported languages.

### Failure Signatures
Language bias manifests as systematic performance degradation on non-English inputs, particularly pronounced for morphologically complex or low-resource languages. Retrieval bias appears as consistent overrepresentation of high-frequency claims regardless of query specificity, often accompanied by poor performance on niche or specialized topics.

### First 3 Experiments
1. Compare baseline embedding-based retrieval performance against LLM-filtered retrieval across all 20 languages
2. Test multilingual prompting strategies against monolingual approaches to quantify bias mitigation effectiveness
3. Analyze claim frequency distributions in retrieval outputs to identify systematic overrepresentation patterns

## Open Questions the Paper Calls Out

## Limitations
- Limited linguistic diversity in pretraining corpora may not represent the full spectrum of low-resource languages
- Analysis relies on embedding-based systems that may not capture all nuances of claim relevance
- LLM-based relevance filtering requires further validation across different model architectures and retrieval datasets

## Confidence

**High Confidence**: Performance disparities between English and non-English inputs in multilingual fact-checking tasks

**Medium Confidence**: Specific mechanisms of language bias (skewed pretraining data, English-centric tuning) and retrieval bias (overrepresentation of high-profile claims)

**Low Confidence**: Universal applicability of LLM-based relevance filtering as a bias mitigation strategy

## Next Checks

1. Cross-Model Validation: Test the LLM-based relevance filtering approach across at least five different model families to verify robustness beyond the initial experimental setup

2. Temporal Bias Analysis: Conduct longitudinal studies tracking claim retrieval patterns over time to identify whether certain claims maintain disproportionate visibility regardless of their actual fact-checking relevance

3. Low-Resource Language Expansion: Extend the multilingual analysis to include languages with fewer than 1,000 Wikipedia articles or minimal web presence to better understand the lower bounds of LLM performance in truly low-resource settings