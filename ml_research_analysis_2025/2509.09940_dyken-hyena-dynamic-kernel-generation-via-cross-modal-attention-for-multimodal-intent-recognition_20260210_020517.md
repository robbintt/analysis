---
ver: rpa2
title: 'DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal
  Intent Recognition'
arxiv_id: '2509.09940'
source_url: https://arxiv.org/abs/2509.09940
tags:
- multimodal
- dynamic
- fusion
- more
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DyKen-Hyena, a novel approach to multimodal
  intent recognition that reframes the problem from feature fusion to processing modulation.
  The core idea is to use non-verbal cues (audio and visual) to dynamically generate
  per-token convolutional kernels that modulate textual feature extraction, allowing
  fine-grained, token-level influence rather than simple feature addition.
---

# DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition

## Quick Facts
- arXiv ID: 2509.09940
- Source URL: https://arxiv.org/abs/2509.09940
- Authors: Yifei Wang; Wenbin Wang; Yong Luo
- Reference count: 12
- Primary result: DyKen-Hyena achieves state-of-the-art performance on MIntRec and MIntRec2.0 benchmarks, with +10.46% F1-score improvement in out-of-scope detection.

## Executive Summary
This paper introduces DyKen-Hyena, a novel approach to multimodal intent recognition that reframes the problem from feature fusion to processing modulation. The core idea is to use non-verbal cues (audio and visual) to dynamically generate per-token convolutional kernels that modulate textual feature extraction, allowing fine-grained, token-level influence rather than simple feature addition. The model employs cross-modal attention to produce token-aligned context, which is then fed into a kernel generator to create dynamic filters integrated into a Hyena sequence model for efficient processing. Evaluated on MIntRec and MIntRec2.0 benchmarks, DyKen-Hyena achieves state-of-the-art performance, with a +10.46% F1-score improvement in out-of-scope detection. The method shows significant gains on intents heavily reliant on non-verbal context (e.g., Joke, Taunt), demonstrating superior robustness and a more accurate representation of multimodal signals.

## Method Summary
DyKen-Hyena reframes multimodal intent recognition as a modulation problem rather than simple feature fusion. The model first aligns audio and visual features to text tokens, then uses cross-modal attention to retrieve token-specific context vectors. These context vectors are fed into an MLP Kernel Generator that outputs dynamic convolutional kernelsâ€”one per token. These kernels are integrated into a Hyena operator alongside static long-range convolutions, creating a hybrid architecture that combines local, context-aware modulation with global sequence modeling. The resulting features are processed by a BERT encoder and classification head. The model is evaluated on MIntRec and MIntRec2.0 benchmarks, with results averaged over 5 random seeds.

## Key Results
- DyKen-Hyena achieves state-of-the-art performance on MIntRec and MIntRec2.0 benchmarks
- Demonstrates +10.46% F1-score improvement in out-of-scope detection compared to baselines
- Shows significant gains on modality-reliant intents (e.g., Joke, Taunt) where non-verbal context is critical
- Ablation study confirms Dynamic Short Convolution is primary driver of OOS robustness

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Attention for Contextual Retrieval
The model projects aligned audio and visual features into a shared space and applies multi-head attention where text tokens act as "Queries" and the combined audio-visual stream acts as "Keys" and "Values." This generates token-specific context vectors that reduce noise compared to global pooling by retrieving only the most relevant non-verbal cues for each word.

### Mechanism 2: Modulation via Dynamic Kernel Generation
A lightweight MLP translates attention-derived context vectors into convolutional kernel weights. These per-token kernels directly modulate textual feature extraction by scaling or filtering features based on multimodal context, allowing the model to "steer" processing rather than simply adding features.

### Mechanism 3: Hybrid Long-Range and Local Processing (Hyena)
The Hyena operator combines dynamic short convolutions (local, multimodal modulation) with static long convolutions (global context via FFT). This hybrid approach efficiently approximates the benefits of both local nuance and global coherence without the quadratic attention costs of standard Transformers.

## Foundational Learning

- **Cross-Modal Attention**: Needed to retrieve audio-visual context specific to each text token, reducing noise compared to global pooling. Quick check: How does the model calculate the context vector for the word "sarcasm" if the audio features are 10 steps away? (Answer: It queries all audio steps; high alignment scores determine the weighted sum).

- **Dynamic Convolution (Dynamic Filters)**: The core "modulation" engine where the filter itself is the output of a learned function, unlike standard CNNs with fixed weights. Quick check: If the input is a batch of 10 tokens, how many unique convolutional kernels does the model generate for the short convolution path? (Answer: 10, one per token).

- **The Hyena Operator**: Replaces standard Transformer self-attention for efficiency using convolutions and gating. Quick check: Why use FFT (Fast Fourier Transform) in this architecture? (Answer: To efficiently compute the long convolution required for global context).

## Architecture Onboarding

- **Component map**: Unaligned Text, Audio, Video -> Alignment Layer -> Text + Aligned AV -> Cross-Modal Attention -> Context Vector -> MLP Kernel Generator -> Dynamic Short Kernels -> Text + Dynamic Kernels + Static Long Kernels -> Hyena Operator -> Fused Embedding -> BERT -> Classification Head

- **Critical path**: The MLP Kernel Generator is the most sensitive component. If the attention mechanism retrieves poor context, or the MLP fails to map it to valid kernel weights, the "modulation" creates corrupted text features.

- **Design tradeoffs**: Kernel Size (Ks): Ks=1 is faster and may optimize for in-domain accuracy, but Ks=3 is significantly more robust for Out-of-Scope detection (+10% F1 in paper) because it captures local phrase context. The architecture adds computational overhead before BERT but aims to reduce redundancy in the fusion process.

- **Failure signatures**: Massive OOS drop (<25% F1) indicates Dynamic Short Convolution path failure. Text-heavy intent degradation suggests modulation is "over-powering" the text, requiring gating mechanism check.

- **First 3 experiments**:
  1. Ablation Sanity Check: Run w/o-DynamicShortConv configuration on MIntRec2.0 to verify ~16% F1-OOS drop
  2. Kernel Size Sweep: Compare Ks=1 vs. Ks=3 on validation set to confirm tradeoff
  3. Visualization: Extract generated kernels for Joke vs. Serious samples to confirm input-dependent modulation

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on aligned multimodal features (likely extracted using proprietary pipelines) is a significant dependency
- Novel kernel generation approach lacks strong corpus precedent, making generalisability to other multimodal tasks uncertain
- Ablation study shows Dynamic Short Convolution's contribution but doesn't isolate whether this is due to modulation mechanism itself or simply addition of local processing

## Confidence
- **High Confidence**: Cross-modal attention mechanism for token-aligned context retrieval is valid and well-supported by ablation study
- **Medium Confidence**: Translating context into convolutional kernels is novel and theoretically justified but lacks strong external validation
- **Low Confidence**: Claim that this specific kernel generation approach is universally superior for all multimodal tasks is overstated without evidence beyond two specific benchmarks

## Next Checks
1. **Ablation Sanity Check**: Replicate w/o-DynamicShortConv ablation on MIntRec2.0 to verify ~16% F1-OOS drop
2. **Kernel Size Sweep**: Systematically compare Ks=1 vs. Ks=3 on held-out validation set to confirm reported tradeoff
3. **Kernel Visualisation**: Extract and visualise generated kernels for contrasting examples (Joke vs. Serious) to verify input-dependent modulation