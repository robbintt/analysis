---
ver: rpa2
title: 'MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized
  Generation'
arxiv_id: '2510.07713'
source_url: https://arxiv.org/abs/2510.07713
tags:
- memory
- user
- memweaver
- history
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'MemWeaver introduces a hierarchical memory framework that captures
  both the temporal evolution and semantic relationships in user textual histories.
  It constructs a dual-component memory: a behavioral memory that extracts contextually
  relevant user actions through graph-based random walks, and a cognitive memory that
  abstracts long-term user preferences via hierarchical summarization.'
---

# MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation

## Quick Facts
- arXiv ID: 2510.07713
- Source URL: https://arxiv.org/abs/2510.07713
- Reference count: 40
- MemWeaver achieves up to 0.67 accuracy on citation identification and 0.48 ROUGE-1 on headline generation in LaMP benchmark.

## Executive Summary
MemWeaver introduces a hierarchical memory framework for personalized text generation by capturing both temporal evolution and semantic relationships in user textual histories. It constructs a dual-component memory: a behavioral memory that extracts contextually relevant user actions through graph-based random walks, and a cognitive memory that abstracts long-term user preferences via hierarchical summarization. Experiments on the LaMP benchmark show MemWeaver outperforms strong baselines, with ablation studies confirming the synergy between the dual memories and incremental updates enabling scalable personalization.

## Method Summary
MemWeaver builds a hierarchical memory from user textual histories for personalized generation. The system constructs a behavioral memory using graph-based random walks on a user history graph with temporal and semantic edges, extracting contextually relevant actions. A cognitive memory is created through hierarchical summarization, first generating local segment summaries then synthesizing them into a global preference representation. During generation, both memories are combined with the user query to condition a large language model, enabling responses that reflect both specific contextual actions and long-term user preferences.

## Key Results
- MemWeaver achieves up to 0.67 accuracy on citation identification and 0.48 ROUGE-1 on headline generation on LaMP benchmark
- Ablation studies show both behavioral and cognitive memories are necessary, with performance dropping when either is removed
- Incremental updates maintain performance while reducing computational cost, enabling scalable personalization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A dual-component memory structure improves personalized generation by jointly modeling specific user actions and abstracted long-term preferences.
- Mechanism: A behavioral memory uses a graph-based random walk to extract contextually relevant user actions, providing concrete examples. A cognitive memory uses hierarchical summarization to abstract long-term preferences. The LLM conditions on both, allowing it to balance specific context with global guidance.
- Core assumption: User history contains both query-relevant actions and stable long-term preferences that can be separated and jointly modeled.
- Evidence anchors:
  - [abstract] "It constructs a dual-component memory: a behavioral memory that extracts contextually relevant user actions through graph-based random walks, and a cognitive memory that abstracts long-term user preferences via hierarchical summarization."
  - [section 4.3.1] "We identify behavioral memory as the foundational layer... In contrast, the cognitive memory functions as a refinement layer... to distill long-term, global preferences that refine the output."
  - [corpus] Me-Agent (arXiv:2601.20162) proposes a "Two-Level User Habit Learning" for personalization, supporting hierarchical user modeling.
- Break condition: Ablation studies show performance drops when either memory component is removed (Table 2), suggesting both are necessary.

### Mechanism 2
- Claim: Modeling both temporal evolution and semantic relationships improves personalization compared to flat list retrieval.
- Mechanism: A memory graph is constructed with temporal edges (consecutive behaviors) and semantic edges (thematically relevant behaviors). A context-aware random walk traverses this graph, balancing query relevance, recency, and continuity to retrieve a coherent behavior subset.
- Core assumption: User interests evolve over time and are connected semantically across topics; capturing these structures provides richer context than simple heuristics.
- Evidence anchors:
  - [abstract] "The core innovation of our memory lies in its ability to capture both the temporal evolution of interests and the semantic relationships between different activities."
  - [section 3.3.1] "Two types of undirected edges are defined: temporal edges... while semantic edges..."
  - [section 4.4.1] "Crucially, temporal edges account for the majority of traversals across all datasets, validating the fundamental importance of the temporal dimension..."
  - [corpus] C-TLSAN (arXiv:2506.13021) emphasizes "Time-Aware Long- and Short-Term Attention" for recommendation, supporting temporal modeling importance.
- Break condition: Performance drops when either temporal or semantic edges are removed from the graph (Table 2).

### Mechanism 3
- Claim: Incremental memory updates enable scalable personalization by maintaining performance while reducing computational cost.
- Mechanism: New behaviors are encoded and integrated into the existing graph. For cognitive memory, new local summaries are generated and integrated into the global summary. This avoids reprocessing the entire history.
- Core assumption: User interests evolve incrementally, and efficient updates can capture this without full recomputation.
- Evidence anchors:
  - [abstract] "Incremental updates maintain performance while reducing computational cost, enabling scalable personalization."
  - [section 3.3.3] "This approach requires only a single, high-level integration step to update the userâ€™s overall narrative, making the process computationally tractable."
  - [section 4.4.2] "Our incremental strategy... offers a robust solution that combines the strengths of both extremes."
  - [corpus] Corpus neighbors do not explicitly discuss incremental memory updates for personalization; most focus on static or batched memory construction.
- Break condition: Incremental update accuracy drops significantly compared to full rebuild, or computational cost is not reduced.

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: MemWeaver builds on RAG but extends flat retrieval to structured, hierarchical memory extraction for personalization.
  - Quick check question: How does standard RAG retrieve documents, and how does MemWeaver's behavioral memory extraction (graph-based random walk) differ?

- Concept: Graph-based Random Walks
  - Why needed here: The behavioral memory relies on traversing a user history graph using random walks guided by edge weights.
  - Quick check question: How does a random walk on a graph work, and what role do edge weights (semantic relevance, recency, continuity) play?

- Concept: Hierarchical Summarization with LLMs
  - Why needed here: Cognitive memory is constructed by segmenting history, summarizing segments locally, and synthesizing a global summary.
  - Quick check question: Why is a two-stage summarization process (local then global) used instead of summarizing the entire history at once?

## Architecture Onboarding

- Component map:
  - Input: User History $H_u$, User Query $q$
  - Encoder (e.g., BGE-M3) -> Graph Constructor (K-means clustering, temporal/semantic edges) -> Random Walk Engine (extracts $M_{behavior}$)
  - Segmenter (partitions $H_u$) -> Summarizer (LLM generates local $s_t$ and global $M_{cog}$)
  - $q$, $M_{behavior}$, $M_{cog}$ -> Memory Integrator & Generator (backbone LLM)

- Critical path:
  1. Offline Graph Construction: Encode history, build behavioral memory graph (one-time cost)
  2. Offline Cognitive Construction: Segment history, generate hierarchical summaries
  3. Online Inference: For new query, run random walk to extract $M_{behavior}$, retrieve $M_{cog}$, construct prompt, generate response
  4. Incremental Update: On new data batch, update graph edges and re-synthesize cognitive memory

- Design tradeoffs:
  - Random Walk vs. Direct Retrieval: Random walk explores structure but is slower than vector search. Claim: better context capture
  - Hierarchical Summarization vs. Flat Prompting: Summarization compresses history but may lose detail. MemWeaver keeps both: behavioral for details, cognitive for abstract guidance
  - Incremental vs. Full Rebuild: Incremental is efficient but may drift over long periods without occasional full rebuilds
  - K-means for Semantic Edges: Choice of K (clusters) determines semantic granularity. Paper uses K=5 (Appendix A)

- Failure signatures:
  - Flat, Generic Responses: Behavioral memory retrieval fails or cognitive memory is empty
  - Overly Specific/Outdated Context: Recency bias $\lambda_1$ is too low, focusing on old, irrelevant behaviors
  - Hallucinated Preferences: Cognitive summarization produces incorrect user traits
  - Scalability Bottleneck: Random walk step count too high or graph extremely dense, increasing latency

- First 3 experiments:
  1. Reproduce Main Results on a Single LaMP Task: Implement full pipeline on one dataset (e.g., LaMP-4) and compare against simple RAG baseline (e.g., BGE retrieval) with same LLM. Verify dual-memory performance lift.
  2. Ablate a Single Component: Remove cognitive memory (or behavioral memory) and run experiment. Quantify performance drop to understand each part's contribution.
  3. Test Incremental Update Efficiency: Simulate stream of new interactions. Compare accuracy and latency of incremental update vs. full rebuild over several time steps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the random walk transition weights ($\alpha, \lambda_1, \lambda_2$) be learned dynamically per user rather than treated as fixed global hyperparameters?
- Basis in paper: [explicit] Section 4.6.2 states that "the optimal setting for these biases is highly task-dependent," noting that a strong bias is effective for scholarly titles but overly restrictive for tweet paraphrasing.
- Why unresolved: The current implementation relies on validation set tuning for these weights, requiring manual adjustment to maintain performance across different behavioral contexts.
- What evidence would resolve it: A mechanism that adaptively adjusts these weights based on the density of the user's graph or the semantic variance of the current query.

### Open Question 2
- Question: How does MemWeaver perform on user histories that are orders of magnitude larger than the ~80-entry limit found in the LaMP benchmark?
- Basis in paper: [inferred] While the Introduction claims to model the "user's entire textual history," Appendix B reveals the experimental profiles typically contain only around 80 entries.
- Why unresolved: The computational cost of the random walk and the clustering quality of K-means may degrade or shift with significantly longer histories (e.g., thousands of interactions).
- What evidence would resolve it: Evaluation on a synthetic or real-world dataset where user history lengths are scaled to 1,000+ documents to observe latency and accuracy trends.

### Open Question 3
- Question: Is the personalized generation robust to hallucinations or noise within the LLM-generated cognitive memory summaries?
- Basis in paper: [inferred] Section 3.3.2 relies on an LLM to synthesize the cognitive memory, but the paper does not evaluate how errors in this abstracted "global preference guidance" impact the final output.
- Why unresolved: A hallucinated preference in the cognitive memory could mislead the model more severely than a retrieval error, as it is presented as a high-level instruction (Section 3.4).
- What evidence would resolve it: A robustness stress test where factual contradictions are intentionally injected into the cognitive memory to measure the effect on downstream generation quality.

## Limitations

- The random walk mechanism may introduce computational bottlenecks at scale, particularly for users with extensive interaction histories
- The K-means clustering approach for semantic edges uses a fixed K=5, which may not adapt well to diverse user behaviors or domains
- The paper assumes user preferences evolve incrementally and remain stable enough for hierarchical summarization, but real-world user interests may shift more discontinuously than the model can capture

## Confidence

- **High Confidence**: The core dual-memory architecture design and its ability to capture both temporal evolution and semantic relationships. The incremental update mechanism's basic premise is sound.
- **Medium Confidence**: The specific implementation details (K=5 clustering, random walk parameters, summarization templates) and their generalizability across domains.
- **Low Confidence**: Long-term scalability of the random walk approach with massive user histories and the model's ability to handle rapid or discontinuous shifts in user preferences.

## Next Checks

1. **Scaling Analysis**: Test the random walk performance and latency on progressively larger user history graphs (10x, 100x the baseline size) to identify computational bottlenecks and potential optimizations.
2. **Dynamic Preference Shift Test**: Design an experiment where user preferences are intentionally shifted mid-way through the history (e.g., topic change, style change) to evaluate how well the hierarchical memory adapts versus static baselines.
3. **Cross-Domain Transferability**: Apply the MemWeaver framework to a non-LaMP benchmark (e.g., personalized dialogue or recommendation task) to assess whether the dual-memory approach generalizes beyond the original experimental domain.