---
ver: rpa2
title: 'Named Entity Recognition for the Kurdish Sorani Language: Dataset Creation
  and Comparative Analysis'
arxiv_id: '2511.22315'
source_url: https://arxiv.org/abs/2511.22315
tags:
- entity
- kurdish
- data
- sorani
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the first Kurdish Sorani Named Entity Recognition
  (NER) dataset, AgaCKNER, comprising 64,563 annotated tokens across five entity types
  (PER, LOC, ORG, DATE, MISC). The research addresses the lack of NER resources for
  low-resource languages by developing both the dataset and a dedicated annotation
  tool.
---

# Named Entity Recognition for the Kurdish Sorani Language: Dataset Creation and Comparative Analysis

## Quick Facts
- **arXiv ID:** 2511.22315
- **Source URL:** https://arxiv.org/abs/2511.22315
- **Reference count:** 39
- **Primary result:** Introduces AgaCKNER dataset (64,563 tokens) and shows CRF outperforms BiLSTM models in Kurdish Sorani NER.

## Executive Summary
This study addresses the scarcity of NLP resources for Kurdish Sorani by introducing AgaCKNER, the first annotated Named Entity Recognition dataset for the language. The research develops both the dataset and a dedicated annotation tool, then conducts a comparative analysis of four NER models: CRF, SVM, BiLSTM, and BiLSTM-CRF. The findings reveal that traditional machine learning approaches, particularly CRF, significantly outperform neural architectures in this low-resource setting. This challenges assumptions about neural superiority in NLP, demonstrating that simpler, computationally efficient models can be more effective when annotated data is limited. The work provides both practical tools and methodological insights for developing NER systems for other low-resource languages.

## Method Summary
The study created the AgaCKNER dataset from Rudaw Media Network news articles, comprising 64,563 tokens annotated with five entity types (PER, LOC, ORG, DATE, MISC) using BIO tagging. A custom web-based annotation tool was developed for this task. Four models were evaluated: CRF with hand-crafted features (word shape, ±2 context window, prefixes/suffixes), SVM with similar features, BiLSTM with 128-dim embeddings, and BiLSTM-CRF with 128-dim embeddings and 256 hidden units. Models were trained using 70/30 and 80/20 splits plus 10-fold cross-validation, with CRF achieving the best performance using L-BFGS optimizer with L1/L2 regularization.

## Key Results
- CRF achieved F1-scores of 0.825, significantly outperforming neural models (BiLSTM-CRF at 0.748)
- Traditional machine learning approaches with hand-crafted features proved superior in low-resource settings
- Cross-validation confirmed CRF's superior performance and stability across different data splits
- The study provides the first Kurdish Sorani NER dataset and demonstrates the effectiveness of feature-engineered approaches for morphologically complex, low-resource languages

## Why This Works (Mechanism)

### Mechanism 1
High inductive bias in traditional models outperforms neural representation learning in extremely low-resource settings (<100k tokens). CRF and SVM rely on manually engineered features (suffixes, prefixes, capitalization) which act as strong priors, allowing them to generalize from sparse data. In contrast, BiLSTM models must learn these representations from scratch, leading to overfitting when the token count is insufficient (64,563 tokens here). The number of 64,563 tokens is too small to reach the critical dataset size for neural architectures.

### Mechanism 2
Global sequence normalization reduces invalid tag transitions better than local classification. CRF models the conditional probability over the entire sequence, explicitly learning transition constraints (e.g., I-PER cannot follow B-LOC). BiLSTM makes local predictions based on context but lacks an explicit global constraint mechanism unless coupled with a CRF layer, and even then, relies on sufficient data to learn the transition weights.

### Mechanism 3
Hand-crafted features mitigate the "out-of-vocabulary" (OOV) damage caused by Sorani's agglutinative morphology and non-standard orthography. Sorani forms complex words via agglutination and lacks standard orthography, resulting in high token variability. CRF/SVM utilize sub-word features (prefixes/suffixes) to recognize entities even when the exact surface form is unseen. Neural models relying on dense word embeddings suffer from high sparsity/OOV rates in this specific low-resource corpus.

## Foundational Learning

**Sequence Labeling & BIO Tagging**
- Why needed: The paper frames NER as a token classification task using the Beginning-Inside-Outside (BIO) scheme. Understanding this is required to interpret the "Feature functions" in CRF and the "Entity-Level Performance" tables.
- Quick check: If a model predicts "I-PER" for the first token of a sentence, why is this structurally invalid in a BIO scheme?

**Bias-Variance Tradeoff**
- Why needed: This is the central theoretical explanation for why simple CRF beats complex BiLSTM. You must understand that "low variance" is desirable when data is scarce.
- Quick check: Does a BiLSTM typically have higher or lower variance than a CRF, and what does that imply about its performance on a 64k token dataset?

**Agglutination in Morphology**
- Why needed: Sorani is morphologically rich (agglutinative). This explains why standard word embeddings fail and why features like "suffixes" and "prefixes" are critical for the winning CRF model.
- Quick check: How does agglutination increase the "Out-of-Vocabulary" rate for a standard word-embedding model?

## Architecture Onboarding

**Component map:**
Raw text from Rudaw Media -> KLPT Toolkit (cleaning, numeral conversion, tokenization) -> AGA NER annotation tool (CoNLL format) -> Feature Engine (word shape, ±2 context, prefixes/suffixes) -> Linear-chain CRF (L-BFGS optimizer)

**Critical path:**
1. Data Cleaning: Handling modified Arabic script and missing orthography
2. Feature Extraction: Generating context windows and sub-word features for CRF
3. Evaluation: 10-fold Cross-Validation (essential given small dataset size)

**Design tradeoffs:**
- CRF vs. BiLSTM: Selecting CRF trades potential future scalability for immediate stability and higher precision in the current low-resource regime
- Precision vs. Recall: BiLSTM has high recall but low precision (over-generates), while CRF is balanced. If application requires minimizing false positives, CRF is mandatory

**Failure signatures:**
- Low Precision on BiLSTM: High false positive rates on MISC/ORG entities
- Boundary Confusion: Difficulty distinguishing I-ORG from B-ORG
- MISC underperformance: All models struggle with I-MISC (F1 < 0.65) due to heterogeneity

**First 3 experiments:**
1. Reproduce Baseline: Train CRF with L1/L2 regularization (0.1) on 80/20 split to verify F1 > 0.82 benchmark
2. Ablation Study: Remove "context window" features from CRF to quantify impact of local context vs. single-token features
3. Error Analysis: Isolate LOC/ORG confusions to see if simple gazetteer features could resolve ambiguity

## Open Questions the Paper Calls Out

**Open Question 1**
Can the superior performance of classical models like CRF be maintained when applying the AgaCKNER dataset to noisy, informal domains such as social media? The authors state that creating the dataset using only news articles "may limit its applicability to areas like social media or historical records." An evaluation on Kurdish Sorani social media posts would resolve this.

**Open Question 2**
Can neural architectures surpass CRF performance in Kurdish Sorani NER when augmented with pre-trained embeddings or multilingual transfer learning? Section 8.1 suggests that "Using Kurdish-specific pre-trained embeddings or multilingual models... could be another promising direction to explore to narrow the performance gap." Comparative experiments with embeddings would resolve this.

**Open Question 3**
How can evaluation metrics be adapted to prevent unfair penalization of models recognizing entities with valid morphological variations in agglutinative languages? The authors highlight a "need for assessment methods that account for morphological differences, as traditional exact-match evaluations can unfairly penalize models." Developing fuzzy matching or lemma-based evaluation would resolve this.

## Limitations
- Dataset derived exclusively from one news source (Rudaw Media Network), creating potential domain bias
- Neural models tested were relatively small architectures (128-dim embeddings, 256 hidden units) with limited training, potentially underpowering the comparison
- Feature engineering details are not fully specified, making it difficult to assess whether success stems from robust linguistic engineering or dataset-specific artifacts

## Confidence

**High Confidence:** CRF model achieves F1-score of 0.825, outperforming best neural model (BiLSTM-CRF at 0.748). This result is robust across multiple evaluation splits and cross-validation.

**Medium Confidence:** The mechanism explaining CRF's superiority—high inductive bias and low variance being advantageous in low-resource settings (<100k tokens)—is theoretically sound and consistent with broader ML literature.

**Low Confidence:** The broader claim that "simpler models should be prioritized for low-resource NER" is an extrapolation from one dataset and one language pair. Without testing on other low-resource languages or significantly larger Sorani datasets, this generalizability is tentative.

## Next Checks

1. **Dataset Diversification Test:** Evaluate AgaCKNER-trained CRF and BiLSTM-CRF models on a distinct Sorani text corpus (e.g., social media posts, literature, or another news source) to test domain bias.

2. **Neural Scaling Experiment:** Re-run BiLSTM-CRF experiments with larger architectures (e.g., 300-dim embeddings, 512 hidden units) and more training epochs, potentially incorporating pre-trained Sorani embeddings if available.

3. **Feature Ablation Analysis:** Perform detailed ablation study on CRF, systematically removing feature types (e.g., context window, prefixes, suffixes) to quantify their individual contributions to the 0.825 F1 score.