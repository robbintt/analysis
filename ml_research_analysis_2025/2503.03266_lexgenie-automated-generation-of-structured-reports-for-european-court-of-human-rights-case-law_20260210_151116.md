---
ver: rpa2
title: 'LexGenie: Automated Generation of Structured Reports for European Court of
  Human Rights Case Law'
arxiv_id: '2503.03266'
source_url: https://arxiv.org/abs/2503.03266
tags:
- legal
- content
- case
- lexgenie
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LexGenie, a fully automated pipeline for
  generating structured legal reports from the European Court of Human Rights (ECHR)
  case law. LexGenie retrieves relevant paragraphs using a keyphrase-based indexing
  approach, clusters them into thematic topics, and generates a hierarchical outline.
---

# LexGenie: Automated Generation of Structured Reports for European Court of Human Rights Case Law

## Quick Facts
- arXiv ID: 2503.03266
- Source URL: https://arxiv.org/abs/2503.03266
- Reference count: 29
- Primary result: LexGenie outperforms single-pass generation and raw-text indexing in structure and content quality for ECHR case law synthesis

## Executive Summary
LexGenie is a fully automated pipeline that generates structured legal reports from the European Court of Human Rights (ECHR) case law. It retrieves relevant paragraphs using a keyphrase-based indexing approach, clusters them into thematic topics, and produces a hierarchical outline. The system then generates detailed content for each section using an iterative prompting strategy, achieving high ratings for report structure and content quality from legal expert evaluation.

## Method Summary
LexGenie operates through an offline indexing phase and online generation pipeline. The offline phase extracts keyphrases from each paragraph using Mistral-7B-Instruct, embeds these keyphrases with text-embedding-3-small, and stores them in FAISS. For structure generation, the system retrieves paragraphs using MMR, clusters them with BERTopic and HDBSCAN, generates topic labels with GPT-4o-mini, and reorganizes the outline for coherence. Content generation builds hierarchical queries, retrieves relevant paragraphs, and uses iterative prompting (25 paragraphs per batch) with GPT-4o-mini to produce comprehensive section content with citations.

## Key Results
- Keyphrase-based indexing improved retrieval and clustering accuracy over raw text embeddings
- Incremental prompting approach outperformed single-pass content generation across all quality dimensions
- Human expert evaluation rated LexGenie's report structure and content quality highly, with strong correlation to automated LLM-based G-Eval scores

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Keyphrase-based paragraph indexing improves thematic retrieval and clustering over raw text embeddings
- Mechanism: A smaller LLM (Mistral-7B-Instruct) extracts legal concept keyphrases from each paragraph; these keyphrases are embedded instead of full text. This suppresses case-specific details and centers embeddings on doctrinal concepts
- Core assumption: Keyphrases better represent the cross-case legal principles than raw paragraph prose
- Evidence anchors: [abstract] "keyphrase-based indexing improved retrieval and clustering accuracy"; [section 2.1.1] "This focus on keyphrases enhances the embeddings by centering them around key legal concepts"; [section 3, Table 1] Ablation shows paragraph-based indexing underperforms keyphrase-based

### Mechanism 2
- Claim: Iterative incremental prompting outperforms single-pass generation for long-context legal synthesis
- Mechanism: Content is generated in batches of 25 paragraphs; each iteration revises prior content by integrating new insights, mitigating the "lost in the middle" phenomenon
- Core assumption: Models better attend to relevant evidence when presented in smaller, sequential batches
- Evidence anchors: [abstract] "The incremental prompting approach outperformed single-pass content generation"; [section 3, Table 2] Incremental scores higher on topical relevance, citation faithfulness, and comprehensiveness

### Mechanism 3
- Claim: MMR retrieval plus LLM-based reorganization produces more coherent, comprehensive outlines
- Mechanism: MMR balances relevance and diversity in initial retrieval; BERTopic with HDBSCAN clusters paragraphs hierarchically; a final LLM call reorders and merges headings for narrative flow
- Core assumption: Diversity in retrieval and explicit global reorganization are necessary for coherent multi-case reports
- Evidence anchors: [section 2.1.2] "MMR balances relevance and diversity... ensuring that the selected paragraphs encompass a broad spectrum of themes"; [section 3, Table 1] Ablation shows removing MMR lowers comprehensiveness

## Foundational Learning

### Concept: Retrieval-Augmented Generation (RAG) with vector stores
- Why needed here: LexGenie uses FAISS + LangChain to store and retrieve paragraph embeddings; understanding dense retrieval is prerequisite
- Quick check question: Can you explain how MMR differs from pure similarity-based retrieval?

### Concept: Topic modeling with BERTopic and HDBSCAN
- Why needed here: Structure generation relies on hierarchical clustering of paragraphs into thematic clusters
- Quick check question: What does HDBSCAN provide over k-means for legal topic clustering?

### Concept: Chain-of-thought LLM evaluation (G-Eval)
- Why needed here: Automated evaluation uses G-Eval to approximate human judgment; understanding its prompt structure is critical for interpreting results
- Quick check question: Why might G-Eval correlate with human scores but still miss domain-specific legal errors?

## Architecture Onboarding

### Component map
1. Offline indexing: Mistral-7B-Instruct → keyphrase extraction → text-embedding-3-small → FAISS (via LangChain)
2. Structure generation: MMR retrieval → BERTopic/HDBSCAN clustering → GPT-4o-mini (topic naming) → GPT-4o-mini (outline reorganization)
3. Content generation: Hierarchical query construction → MMR retrieval → GPT-4o-mini (incremental prompting, 25-paragraph batches)
4. UI: Web app with retrieval refinement, ToC editing, section-level generation, PDF export

### Critical path
Keyphrase quality → retrieval diversity (MMR) → cluster coherence (BERTopic/HDBSCAN) → outline reorganization → incremental content generation with citations

### Design tradeoffs
- Keyphrase vs raw text: Better thematic focus vs potential information loss
- Incremental vs single-pass: Higher quality vs latency and cost
- LLM-based reorganization: Coherence vs possible over-smoothing

### Failure signatures
- Retrieval gaps: Low comprehensiveness scores, missing key precedents
- Cluster drift: Subtopics that don't align with parent headings
- Citation hallucination: References to non-existent paragraph numbers
- Narrative fragmentation: Sections read as isolated summaries without cross-references

### First 3 experiments
1. Ablate keyphrase extraction: Compare raw paragraph embeddings vs keyphrase embeddings on retrieval precision and cluster silhouette scores
2. Vary batch size in incremental prompting: Test 10, 25, 50 paragraphs per iteration; measure citation faithfulness and comprehensiveness
3. Replace G-Eval with human expert review on 20 new queries: Correlate automated vs human scores across all dimensions to validate generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can legal authoritativeness and temporal validity be integrated into the retrieval pipeline to prevent the inclusion of outdated or peripheral information?
- Basis in paper: [explicit] The Limitations section states the pipeline lacks mechanisms for ranking content by "legal importance or authoritativeness," leading to "temporally outdated information"
- Why unresolved: The current retrieval relies solely on vector similarity and MMR, ignoring the hierarchical nature of case law or the date of judgment
- What evidence would resolve it: A modified retrieval scoring function that weights cases by citation frequency or judicial hierarchy, resulting in improved "Citation Faithfulness" scores in expert evaluation

### Open Question 2
- Question: What architectural changes are required to generate cohesive narratives across sections rather than treating subsections in isolation?
- Basis in paper: [explicit] The Limitations section notes the lack of "advanced contextual linking," and the Case Study highlights that isolated generation results in a "disjointed narrative"
- Why unresolved: The current pipeline generates content for leaf nodes independently, failing to capture the "interconnected nature of legal issues"
- What evidence would resolve it: A comparative evaluation showing that a global context mechanism reduces redundancy and improves logical flow between adjacent sections

### Open Question 3
- Question: Can temporal trend identification be effectively integrated to track the evolution of legal principles within these structured reports?
- Basis in paper: [explicit] The Conclusion suggests future work could "integrate multi-case analysis tools, such as temporal trend identification"
- Why unresolved: The current system focuses on synthesizing static principles rather than identifying or visualizing how interpretations change over time
- What evidence would resolve it: Successful implementation of a temporal module that accurately clusters cases by date and describes the evolution of a specific legal principle in the generated text

## Limitations
- Evaluation confined to a single legal corpus (ECHR) and single expert assessor, limiting generalizability
- Key hyperparameters for MMR, BERTopic, and HDBSCAN are not specified, limiting reproducibility
- Incremental prompting approach shows gains but at the cost of increased latency and computational overhead

## Confidence
- **High confidence**: Keyphrase-based indexing improves thematic retrieval and clustering over raw text embeddings; iterative prompting outperforms single-pass generation for long-context legal synthesis
- **Medium confidence**: MMR retrieval plus LLM-based reorganization produces more coherent, comprehensive outlines; automated G-Eval evaluation correlates with human judgment across multiple quality dimensions
- **Low confidence**: Generalizability of LexGenie's approach to other legal domains or multilingual corpora; scalability of incremental prompting for very large or complex legal topics

## Next Checks
1. **Cross-corpus validation**: Apply LexGenie to case law from a different jurisdiction (e.g., U.S. Supreme Court or Indian Supreme Court) and compare structure and content quality metrics with the ECHR results
2. **Multiple-expert evaluation**: Have 3-5 legal experts independently rate LexGenie-generated reports on the same set of queries; analyze inter-rater reliability and compare with the original single-expert scores
3. **Automated vs. manual citation verification**: For a sample of LexGenie-generated reports, manually verify the accuracy and relevance of all cited paragraphs; quantify the rate of citation hallucination or misattribution