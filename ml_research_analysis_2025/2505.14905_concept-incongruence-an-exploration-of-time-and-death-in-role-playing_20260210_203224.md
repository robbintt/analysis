---
ver: rpa2
title: 'Concept Incongruence: An Exploration of Time and Death in Role Playing'
arxiv_id: '2505.14905'
source_url: https://arxiv.org/abs/2505.14905
tags:
- answer
- death
- role
- year
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces concept incongruence, where conflicting\
  \ concept boundaries in prompts or model representations lead to mis-specified or\
  \ under-specified model behaviors. The authors focus on temporal boundaries in role-playing\
  \ scenarios, particularly when characters die, and propose three behavioral metrics\u2014\
  abstention rate, conditional accuracy, and answer rate\u2014to quantify model behavior\
  \ under concept incongruence."
---

# Concept Incongruence: An Exploration of Time and Death in Role Playing

## Quick Facts
- **arXiv ID:** 2505.14905
- **Source URL:** https://arxiv.org/abs/2505.14905
- **Reference count:** 40
- **Primary result:** Models fail to abstain after character death in role-play scenarios, showing accuracy drops due to unreliable internal encoding of death state and shifted temporal representations.

## Executive Summary
This paper introduces concept incongruence, where conflicting concept boundaries in prompts or model representations lead to mis-specified or under-specified model behaviors. The authors focus on temporal boundaries in role-playing scenarios, particularly when characters die, and propose three behavioral metrics—abstention rate, conditional accuracy, and answer rate—to quantify model behavior under concept incongruence. Experiments show that models fail to abstain after a character's death and suffer accuracy drops compared to non-role-play settings. Probing experiments reveal that models lack reliable encoding of the death state and that role-playing shifts temporal representations, leading to these inconsistencies. Additional specification improves abstention behavior but further degrades accuracy. The findings suggest that concept incongruence leads to unexpected model behaviors and highlight the need for improved handling of such incongruence in future research.

## Method Summary
The paper evaluates model behavior under concept incongruence using a dataset of 100 historical figures (died 1890-1993) plus 6 living figures. Two question types probe temporal knowledge: U.S. presidential sequence questions and presidential tenure questions. The study compares ROLE-PLAY prompts ("You are <character>. Please only output the answer.") against NON-ROLE-PLAY prompts ("Please only output the answer."). Behavioral metrics include abstention rate, conditional accuracy, and answer rate. Probing experiments use linear classifiers on final-token hidden states to measure death-state encoding reliability and temporal representation shifts. The authors also test whether explicit specification (adding death year and abstention instructions) can improve model behavior.

## Key Results
- Models show 75.5% increase in abstention after death when given explicit death year specification, but conditional accuracy drops by 6.5%
- Death-state probe accuracy drops from ~100% (non-role-play) to ~85% (role-play), indicating unreliable encoding
- Temporal representation shifts increase RMSE from 2.6 to 10.8 years while maintaining high correlation, showing systematic offset rather than catastrophic failure
- Llama and Gemma models show similar patterns of concept incongruence, with Llama more responsive to additional specification

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Models fail to maintain consistent knowledge boundaries during role-play because they lack reliable internal representations of character death states.
- **Mechanism:** Probing experiments reveal that death/alive classification accuracy drops from ~100% (non-role-play) to ~85% (role-play), indicating the model treats the death state as less important when immersed in a persona.
- **Core assumption:** Linear probes capture meaningful information about how concepts are encoded in hidden states.
- **Evidence anchors:** [abstract] "unreliable encoding of the 'death' state across different years, leading to unsatisfactory abstention behavior"; [Section 4.1] Figure 4a shows layer-wise probe accuracy gap between ROLE-PLAY and NON-ROLE-PLAY conditions; [corpus] Related work on persona consistency (arXiv:2503.17662) addresses behavioral alignment but does not examine internal representation reliability.
- **Break condition:** If death state information is encoded non-linearly or distributed across multiple representation subspaces that linear probes cannot capture.

### Mechanism 2
- **Claim:** Role-play contexts systematically shift temporal representations, preserving relative ordering while degrading absolute temporal precision.
- **Mechanism:** Linear regression probes trained to predict years show increased RMSE (2.6→10.8 years for presidential questions) while maintaining high Spearman correlation, suggesting an additive offset rather than catastrophic disruption.
- **Core assumption:** The model's temporal representations are at least partially linear and can be meaningfully probed with ridge regression.
- **Evidence anchors:** [abstract] "role playing causes shifts in the model's temporal representations, resulting in accuracy drops"; [Section 4.2] Table 2 quantifies correlation and RMSE degradation; Figure 5 visualizes the offset pattern; [corpus] Gurnee & Tegmark (2024, cited in paper) established linear temporal embeddings; this work shows those embeddings are perturbed by role-play.
- **Break condition:** If the observed RMSE increase is artifact of probe training data rather than genuine representational shift; or if shift is domain-specific and does not generalize beyond tested temporal contexts.

### Mechanism 3
- **Claim:** Explicitly specifying knowledge boundaries improves abstention behavior but further degrades factual accuracy, revealing a fundamental representational tension.
- **Mechanism:** Adding death year information and explicit abstention instructions increases after-death abstention rate by 75.5% (Llama) while simultaneously decreasing conditional accuracy by 6.5%, suggesting the model reallocates representational capacity toward boundary enforcement at the cost of knowledge precision.
- **Core assumption:** The model cannot simultaneously optimize for character immersion (which requires suppressing post-death knowledge) and factual accuracy (which requires maintaining that knowledge).
- **Evidence anchors:** [abstract] "Additional specification improves abstention behavior but further degrades accuracy"; [Section 5] Figure 6 shows the tradeoff; temporal probe correlation drops further (0.996→0.837 for Llama on art dates); [corpus] CogDual (arXiv:2507.17147) addresses "dual cognition" in role-play via RL, suggesting architectural solutions may help.
- **Break condition:** If alternative prompting strategies, multi-objective training, or representation disentanglement methods could decouple these competing demands.

## Foundational Learning

- **Linear Probing**
  - Why needed here: The paper's causal claims about representation depend on probing methodology—understanding what linear classifiers can and cannot reveal about hidden states.
  - Quick check question: If a linear probe achieves 85% accuracy on a classification task, can you conclude the information is "linearly encoded"? What are two alternative explanations?

- **Spearman Correlation vs. RMSE**
  - Why needed here: Section 4.2 relies on interpreting both metrics together—high correlation + high RMSE indicates preserved ordering with absolute offset.
  - Quick check question: For a temporal probe, what pattern of (Spearman, RMSE) would indicate catastrophic representational failure versus systematic shift?

- **Knowledge Boundaries in Role-Play**
  - Why needed here: The paper tests whether models respect the implicit constraint that dead characters shouldn't know future events—this is the core behavioral question.
  - Quick check question: What is the difference between "abstention" and "answer rate," and why might both be needed to characterize model behavior?

## Architecture Onboarding

- **Component map:** Prompt construction -> Behavioral metrics evaluation -> Probing infrastructure -> Dataset layer
- **Critical path:** 1. Replicate behavioral metrics on your target model to establish baseline incongruence; 2. Deploy death-state probe to verify representational gap; 3. Test temporal probe to quantify representation shift magnitude; 4. Only then iterate on specification/mitigation strategies
- **Design tradeoffs:** Strict boundary enforcement ↔ factual accuracy degradation; Probe layer selection: earlier layers capture more syntactic/lexical features; later layers more task-semantic (paper uses final-token hidden states across layers); Behavioral evaluation automation: LLM-as-judge achieves 95%+ agreement with humans but introduces dependency on another model's consistency
- **Failure signatures:** Gradual (not sharp) abstention transition around death year → death year not precisely encoded; High abstention rate but also high answer rate ("I don't know... but it's X") → model circumventing boundaries via creative rationalization; RMSE spike without correlation drop → systematic offset rather than random noise
- **First 3 experiments:** 1. **Baseline behavioral audit:** Run the 100-character dataset with presidential questions on your model; compute all three metrics for ROLE-PLAY and NON-ROLE-PLAY conditions to quantify the incongruence gap; 2. **Death-state probe validation:** Train linear classifier on final-token activations to predict dead/alive status; compare ROLE-PLAY vs NON-ROLE-PLAY accuracy to confirm the representation degradation hypothesis holds for your architecture; 3. **Specification sensitivity test:** Apply the restricted ROLE-PLAY prompt (with explicit death year and abstention instructions) on a subset; measure the abstention-accuracy tradeoff curve to calibrate expected gains and costs before broader deployment.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can models achieve both reliable abstention behavior and maintained accuracy under concept incongruence, rather than trading one for the other?
- **Basis in paper:** [explicit] The authors show that additional specification improves abstention (Llama +75.5%) but further degrades conditional accuracy (-6.5%), concluding "it cannot optimize both simultaneously."
- **Why unresolved:** The fundamental tension between role immersion and precise temporal alignment remains; the paper does not propose architectural or training solutions.
- **What evidence would resolve it:** Demonstration of a method (e.g., representation editing, conditional computation) that improves abstention rate after death without accuracy drops, tested on the same benchmark.

### Open Question 2
- **Question:** Do findings on temporal representation shifts generalize beyond U.S. president and artwork questions to diverse temporal-reasoning tasks and non-English contexts?
- **Basis in paper:** [explicit] "A key limitation of this study is that most evaluations center on U.S. presidents, offering limited coverage of broader temporal-reasoning tasks."
- **Why unresolved:** The paper's probing experiments cover only two domains (presidents and artwork release dates); generalization remains untested.
- **What evidence would resolve it:** Replication of the abstention/accuracy drop and probing results across additional temporal tasks (e.g., historical events, scientific discoveries, non-Western history) and multilingual settings.

### Open Question 3
- **Question:** What mechanisms can enable models to proactively seek clarification when encountering concept incongruence in user prompts?
- **Basis in paper:** [explicit] "Models could proactively seek clarification from users when faced with ambiguity or conflicting instructions."
- **Why unresolved:** The paper identifies the opportunity but does not implement or evaluate any clarification-seeking behavior.
- **What evidence would resolve it:** A study showing models reliably detect incongruence (e.g., "two-horned unicorn") and generate appropriate clarification requests, with human or automated evaluation of clarification quality.

## Limitations

- Probe expressivity may systematically underestimate true representational presence of death-state or temporal information
- Temporal probe findings may be artifacts of specific datasets rather than general representational changes
- Specification generalizability is limited—tradeoffs observed for Llama and Gemma may not extend to larger models

## Confidence

**High Confidence** in behavioral observations: ROLE-PLAY consistently reduces conditional accuracy and produces inadequate abstention after character death; Behavioral metrics reliably capture concept incongruence.

**Medium Confidence** in representational claims: Death-state encoding degrades in ROLE-PLAY contexts; Temporal representations shift systematically in ROLE-PLAY; These claims depend on linear probe validity and may not capture full representational complexity.

**Low Confidence** in causal mechanisms: The exact relationship between probe results and behavioral outcomes remains speculative; Alternative explanations (e.g., training data bias, instruction-following degradation) cannot be ruled out.

## Next Checks

1. **Probe Expressivity Test:** Train non-linear probes (small MLPs, attention-based classifiers) for death-state classification and compare performance to linear probes. This validates whether representational degradation is real or probe-method artifact.

2. **Cross-Domain Temporal Probe:** Apply the temporal probe methodology to completely different time-based domains (e.g., historical event dates, product release years) to verify whether the observed representation shifts generalize beyond presidential/artwork questions.

3. **Specification Ablation:** Systematically remove individual specification components (death year, abstention instructions, persona description) to identify which elements drive the abstention-accuracy tradeoff and whether partial specifications can achieve better balance.