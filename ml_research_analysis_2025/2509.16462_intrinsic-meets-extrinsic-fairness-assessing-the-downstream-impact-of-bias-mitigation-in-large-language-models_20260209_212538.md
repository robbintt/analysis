---
ver: rpa2
title: 'Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias
  Mitigation in Large Language Models'
arxiv_id: '2509.16462'
source_url: https://arxiv.org/abs/2509.16462
tags:
- bias
- unlearned
- mitigation
- unlearning
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether intrinsic bias in large language
  models (LLMs) translates into fairness issues in downstream tasks. The authors propose
  a unified framework combining intrinsic bias mitigation via concept unlearning with
  extrinsic mitigation using counterfactual data augmentation (CDA).
---

# Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models

## Quick Facts
- arXiv ID: 2509.16462
- Source URL: https://arxiv.org/abs/2509.16462
- Authors: Mina Arzaghi; Alireza Dehghanpour Farashah; Florian Carichon; Golnoosh Farnadi
- Reference count: 14
- Primary result: Unlearning and CDA combined reduce intrinsic gender bias by up to 94.9% and improve downstream fairness by up to 82% without accuracy loss

## Executive Summary
This paper investigates whether intrinsic bias in large language models (LLMs) translates into fairness issues in downstream tasks. The authors propose a unified framework combining intrinsic bias mitigation via concept unlearning with extrinsic mitigation using counterfactual data augmentation (CDA). Evaluated on three real-world financial datasets (ACS Employment, Adult Income, and German Credit), the approach reduces intrinsic gender bias by up to 94.9% and improves downstream fairness metrics like demographic parity by up to 82%, without compromising accuracy. Unlearning alone often outperforms CDA in fairness gains, while their combination yields the best results. The study also highlights model-specific behaviors and emphasizes the importance of early-stage mitigation before downstream deployment.

## Method Summary
The method uses a four-stage pipeline: baseline, CDA, unlearning, and combined. Tabular data is serialized into text using a "Text Template" method with feature importance ordering. Three model types are evaluated: logistic regression, LLM embeddings with logistic regression, and fine-tuned LLM classifier via LoRA. Unlearning employs a composite loss combining gradient ascent on stereotype answers, gradient descent on anti-stereotype answers, gap loss to equalize probabilities, and KL divergence to preserve language capabilities. CDA duplicates training instances with flipped gender attributes. The POORORRICH dataset is adapted for unlearning using a multi-term loss with tunable λ weights.

## Key Results
- Unlearning reduces intrinsic gender bias by up to 94.9% (Phi-3 probability gap from 0.565 to 0.029)
- Combined unlearning and CDA improve downstream demographic parity by up to 82%
- Unlearning maintains or improves accuracy and perplexity while reducing bias
- CDA improves demographic parity but can reduce accuracy on smaller datasets like German Credit

## Why This Works (Mechanism)

### Mechanism 1: Multi-component unlearning for intrinsic bias reduction
- Claim: Reducing intrinsic bias in LLM representations improves downstream task fairness.
- Mechanism: A composite loss function combines (1) gradient ascent on stereotype answers to weaken biased associations, (2) gradient descent on anti-stereotype answers to reinforce fairness-aligned alternatives, (3) gap loss to equalize probabilities and prevent bias reversal, and (4) KL divergence to preserve general language capabilities.
- Core assumption: Intrinsic bias in pre-trained representations propagates to downstream allocations and can be decoupled from task performance.
- Evidence anchors:
  - [abstract] "intrinsic bias mitigation through unlearning reduces intrinsic gender bias by up to 94.9%, while also improving downstream task fairness metrics, such as demographic parity by up to 82%, without compromising accuracy"
  - [Table 1] Phi-3's probability gap reduced from 0.565 to 0.029; perplexity remains stable
  - [corpus] Evidence is limited/absent in provided corpus for this specific mechanism; related work on unlearning and fairness exists but not directly comparable.
- Break condition: If KL divergence uses a reference dataset not designed for socioeconomic bias (e.g., TruthfulQA), regularization may preserve original biases; if gap loss is omitted, training can flip bias rather than equalize it.

### Mechanism 2: CDA balances group-level predictions via data-level exposure
- Claim: Counterfactual Data Augmentation (CDA) at the data level can improve group fairness metrics—particularly demographic parity—without requiring model weight access.
- Mechanism: Duplicate each training instance while flipping the gender attribute so every profile appears under both privileged and unprivileged groups; this discourages spurious gender associations during training.
- Core assumption: Models learn conditional associations from co-occurrence statistics; balanced exposure reduces reliance on sensitive attributes for prediction.
- Evidence anchors:
  - [abstract] "combining [unlearning] with CDA yields the best results"
  - [Table 2] CDA improves Demographic Parity (e.g., ACS Employment: 0.084 → 0.080 for Llama-3.1 Feature Extractor) but can reduce accuracy on some datasets (e.g., German Credit)
  - [corpus] The provided corpus does not directly evaluate CDA for tabular financial tasks; relevant work on intrinsic/extrinsic links in VLMs suggests bias propagation can be systematic, but with different modalities.
- Break condition: If the underlying data has strong feature-label correlations with the sensitive attribute, CDA may insufficiently address conditional disparities (e.g., Equality of Odds) and can degrade accuracy on smaller or high-variance datasets.

### Mechanism 3: Complementarity of in-processing (unlearning) and pre-processing (CDA)
- Claim: Jointly applying unlearning and CDA provides complementary fairness improvements across different metrics.
- Mechanism: CDA directly balances group-level positive prediction rates, favoring Demographic Parity; unlearning modifies internal representations with respect to labels, more directly affecting error-rate parity (Equality of Odds).
- Core assumption: Fairness metrics capture distinct allocational harms, and targeting multiple mechanisms yields additive gains without strictly traded-off accuracy losses.
- Evidence anchors:
  - [Table 2] Unlearned Llama-3.1 + CDA achieves the best fairness on ACS Employment (AccP: 0.096, DP: 0.014, EqOdds: 0.167)
  - [Discussion] "CDA favorably improve DP and Unlearning impact positively Equality of Odds"
  - [corpus] Not directly evidenced in the provided corpus; related work mentions bias propagation and mitigation, but not this specific joint approach.
- Break condition: If the model has high capacity and entangled representations (e.g., observed resistance in larger models like Llama-3.1), hyperparameter tuning becomes critical; improperly tuned combinations can yield conflicting gradients or suboptimal tradeoffs.

## Foundational Learning

- Concept: Intrinsic vs. extrinsic fairness in LLMs
  - Why needed here: The paper explicitly evaluates whether intrinsic representational bias translates into extrinsic allocational harms in downstream financial tasks.
  - Quick check question: Can you distinguish between a fairness intervention that modifies embeddings (intrinsic) versus one that modifies prediction outcomes (extrinsic)?

- Concept: Demographic Parity vs. Equality of Odds
  - Why needed here: The paper shows CDA preferentially improves Demographic Parity while unlearning preferentially improves Equality of Odds; understanding this distinction is essential for selecting interventions.
  - Quick check question: Which metric conditions on the true label, and which only requires equal positive prediction rates across groups?

- Concept: Gradient-based unlearning
  - Why needed here: The paper's core in-processing technique repurposes unlearning (typically for safety) to target gender–socioeconomic stereotypes via a multi-term loss.
  - Quick check question: What is the role of gradient ascent vs. gradient descent in this unlearning formulation, and why is a gap loss necessary?

## Architecture Onboarding

- Component map:
  - Tabular-to-text serialization (Text Template with feature importance ordering)
  - LLM as Feature Extractor: frozen LLM → embeddings → logistic regression
  - LLM as Classifier: serialized input → instruction prompt → LoRA fine-tuning
  - Unlearning module: composite loss (unlearn, learn, gap, KL) applied to LLM weights
  - CDA module: pre-processing step duplicating and flipping the gender attribute
  - Evaluation: intrinsic (probability gap on stereo/antistereo prompts) and extrinsic (Acc, AccP, DP, EqOdds)

- Critical path:
  1. Serialize tabular data with feature importance ordering (truncation safety).
  2. Choose model configuration: feature extractor vs. classifier; select LLM.
  3. Apply CDA to training data if targeting DP; prepare unlearning dataset (PoorOrRich).
  4. Execute unlearning with tuned λ weights; validate intrinsic bias gap and perplexity.
  5. Fine-tune (LoRA) on downstream task; select checkpoint by validation accuracy.
  6. Evaluate extrinsic fairness on test split; iterate on λ weights if fairness targets not met.

- Design tradeoffs:
  - CDA improves DP but can reduce accuracy on small/high-variance datasets (e.g., German Credit).
  - Unlearning maintains or improves accuracy and perplexity but requires weight access and careful tuning.
  - Larger models (Llama-3.1 8B) may exhibit more complex dynamics; hyperparameter tuning is more critical.
  - KL divergence preserves language quality but may anchor to original biased distribution; TruthfulQA is not designed for socioeconomic bias.

- Failure signatures:
  - Bias inversion: gap loss too weak; probabilities flip rather than equalize.
  - Stagnant fairness: KL weight too high or reference dataset insufficient; original bias preserved.
  - Accuracy drop: CDA applied without accounting for strong feature-label correlations; overfitting to augmented distribution.
  - Model instability: conflicting gradient directions in high-capacity models; insufficient tuning of λ weights.

- First 3 experiments:
  1. Baseline evaluation: Run Stage 1 (no mitigation) on ACS Employment with Llama-3.1 as both feature extractor and classifier; record Acc, AccP, DP, EqOdds.
  2. Isolated interventions: Apply CDA-only (Stage 2) and unlearning-only (Stage 3) separately; compare DP vs. EqOdds improvements to validate mechanism specificity.
  3. Joint optimization: Combine unlearning + CDA (Stage 4) with ablation on λ gap and λ KL; identify configurations that maximize DP + EqOdds without accuracy loss.

## Open Questions the Paper Calls Out

- Can the unlearning-based mitigation framework be extended to address multi-attribute and intersectional biases beyond binary gender?
  - Basis in paper: [explicit] The authors state in limitations: "First, we focus solely on binary gender bias, omitting other sensitive and intersectional attributes. Future work will explore extending this pipeline to multi-attribute, multilingual, and black-box LLM settings."
  - Why unresolved: Intersectional bias (e.g., combinations of gender, race, religion) may require different unlearning objectives or more complex loss functions to simultaneously address multiple protected attributes.
  - What evidence would resolve it: Results from applying the framework to datasets with multiple protected attributes showing whether fairness gains generalize across intersectional groups.

- Does the relationship between intrinsic and extrinsic bias mitigation hold across languages other than English?
  - Basis in paper: [explicit] "Second, experiments are limited to English due to the lack of annotated fairness datasets in other languages."
  - Why unresolved: Different languages encode gender and socioeconomic concepts differently, potentially affecting how unlearning transfers to downstream tasks.
  - What evidence would resolve it: Evaluation on multilingual financial datasets with fairness annotations across diverse languages.

- How does increased model capacity affect the dynamics between learning and unlearning components in bias mitigation?
  - Basis in paper: [inferred] The ablation study shows Llama-3.1 (8B) exhibits conflicting learning/unlearning dynamics unlike smaller models. The authors hypothesize: "increased model capacity introduces greater representational complexity, making it more difficult to disentangle and remove bias using our current unlearning method."
  - Why unresolved: Larger models may have more entangled representations, but the mechanism causing learning and unlearning to act in "contradictory directions" is not characterized.
  - What evidence would resolve it: Systematic evaluation across model sizes (e.g., 2B to 70B+), analyzing representation entanglement during unlearning.

## Limitations

- Limited to three tabular financial datasets and three model architectures, raising questions about generalizability to other domains and task types
- Unlearning pipeline depends on a proxy dataset (POORORRICH) with underspecified conversion process, limiting reproducibility
- CDA's negative impact on accuracy in smaller datasets suggests nontrivial accuracy-fairness tradeoffs not fully characterized
- KL divergence regularization uses TruthfulQA as reference, which is not designed for socioeconomic bias and may inadequately anchor fairness

## Confidence

- **High confidence**: Intrinsic bias reduction via unlearning directly improves downstream demographic parity and equality of odds without accuracy loss
- **Medium confidence**: CDA preferentially improves demographic parity but can degrade accuracy in high-variance/small datasets
- **Medium confidence**: Joint unlearning + CDA yields additive fairness gains across multiple metrics

## Next Checks

1. Apply the four-stage pipeline to a non-financial tabular dataset (e.g., COMPAS recidivism or medical diagnosis) to test whether intrinsic-to-extrinsic fairness transfer holds across domains.

2. Compare unlearning runs with and without KL divergence using a bias-targeted reference (e.g., curated socioeconomic QA pairs) to measure whether bias preservation is driven by generic language regularization.

3. Evaluate the unlearning + CDA pipeline on both smaller (e.g., 1B) and larger (e.g., 70B) LLMs to identify whether high-capacity models exhibit stronger resistance to bias mitigation and require differentiated hyperparameter tuning.