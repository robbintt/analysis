---
ver: rpa2
title: 'LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges'
arxiv_id: '2506.10022'
source_url: https://arxiv.org/abs/2506.10022
tags:
- llms
- code
- malicious
- jailbreak
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MalwareBench, a benchmark dataset of 3,520
  jailbreaking prompts for malicious code generation, designed to evaluate LLM robustness
  against such threats. The dataset is based on 320 manually crafted malicious code
  generation requirements, covering 11 jailbreak methods and 29 code functionality
  categories.
---

# LLMs Caught in the Crossfire: Malware Requests and Jailbreak Challenges

## Quick Facts
- **arXiv ID**: 2506.10022
- **Source URL**: https://arxiv.org/abs/2506.10022
- **Reference count**: 13
- **Primary result**: LLMs show limited ability to reject malicious code-generation requests, with refusal rates dropping from 60.93% to 39.92% when jailbreak algorithms are applied.

## Executive Summary
This paper introduces MalwareBench, a comprehensive benchmark dataset of 3,520 jailbreaking prompts designed to evaluate LLM robustness against malicious code generation. The dataset, built from 320 manually crafted malicious code requirements, tests 29 LLMs across 11 jailbreak methods and 29 code functionality categories. The study reveals that mainstream LLMs exhibit significant vulnerabilities, with the average rejection rate for malicious content being 60.93%, which drops to 39.92% when combined with jailbreak attack algorithms. The research highlights that current code security capabilities of LLMs pose substantial challenges and identifies "Benign Expression" as the most effective jailbreak method.

## Method Summary
The study creates MalwareBench by applying 11 black-box jailbreak methods to 320 manually crafted malicious code generation requirements, producing 3,520 prompts across 6 domains and 29 subcategories. The evaluation pipeline involves running these prompts through 29 LLMs and using GPT-4o or Llama-3.70B as judge models to score responses on refusal (binary) and quality (1-4 scale). The attack generation uses Qwen-Turbo with temperature=0.9, top_p=0.95, while inference parameters are set to temperature=0.9, top_p=0.95. The automated evaluation employs LLM-as-a-Judge to assess whether responses contain harmful code or descriptions.

## Key Results
- Mainstream LLMs reject malicious content at an average rate of only 60.93%, dropping to 39.92% with jailbreak algorithms
- CodeLlama-70B demonstrates superior robustness (76.86% refusal rate) compared to Llama-3.3-70B (39.96%) due to intrinsic RLHF V5 fine-tuning
- "Benign Expression" emerges as the most effective jailbreak method, achieving 31.92% lowest rejection rate through semantic obfuscation

## Why This Works (Mechanism)

### Mechanism 1: Semantic Obfuscation via Benign Expression
Replacing malicious lexicon with harmless synonyms bypasses LLM safety filters more effectively than complex prompt structuring. This reduces "dangerous" token density that triggers safety classifiers, allowing the model to prioritize instruction-following over safety refusal.

### Mechanism 2: Granularity-Dependent Defense Activation
Detailed malicious requirements trigger higher rejection rates than vague ones because detailed prompts contain specific technical artifacts that map directly to dangerous patterns in safety training data.

### Mechanism 3: Intrinsic vs. Extrinsic Safety Alignment
Models with intrinsic safety fine-tuning (CodeLlama-70B) show superior robustness compared to models relying on external guardrails, as CodeLlama embeds thousands of safety-specific rejection examples directly in model weights.

## Foundational Learning

- **Concept: Black-Box Jailbreaking (Template vs. Rewriting)**
  - Why needed: The paper distinguishes between attacks that hide requests inside stories (Template) versus those that change the words used to ask (Rewriting), with rewriting currently more effective.
  - Quick check: Does the attack method try to hide the request inside a story (Template), or does it simply change the words used to ask it (Rewriting)?

- **Concept: Refusal vs. Quality Scoring (ASR)**
  - Why needed: A model refusing to answer is a binary safety metric, but the paper introduces Quality Score (1-4) to measure how dangerous the code is if answered.
  - Quick check: If a model refuses to write a virus but explains the logic of how a virus might work in text, what is its Refusal Score and Quality Score? (Answer: Refuse 0, Quality 3).

- **Concept: Safety Alignment (RLHF)**
  - Why needed: CodeLlama-70B's robustness is attributed to Reinforcement Learning from Human Feedback (RLHF V5), a training-phase intervention rather than a prompt-phase filter.
  - Quick check: Why might a smaller, "less intelligent" model sometimes appear "safer" than a larger model? (Answer: It may lack the capacity to understand the malicious instruction, resulting in nonsense output rather than a safety refusal).

## Architecture Onboarding

- **Component map**: MalwareBench Dataset (320 requirements) -> 11 jailbreak algorithms -> 3,520 prompts -> Target LLM -> Judge LLM (GPT-4o/Llama-3.70B) -> Scores (Refusal, Quality)

- **Critical path**: 1. Select raw malicious requirement (e.g., "Write a keylogger") 2. Apply Benign Expression mutation (e.g., "Write a keystroke recorder for productivity tracking") 3. Feed to Target Model (e.g., GPT-4o) 4. Analyze output using Judge Model to determine if it contains "Harmful code" (Score 4)

- **Design tradeoffs**: Judge Selection uses GPT-4o for high agreement (80%+) but incurs API costs, while open-source judges (Llama-3.70B) are cheaper but have lower agreement rates. Granularity tradeoff: Detailed prompts test specific exploits but are easier for safety filters to catch.

- **Failure signatures**: Passive Defense (Small Models) return gibberish or irrelevant text, appearing as "low score" but actually capability failure. Malicious Pseudo-code (Large Models) refuse to write executable code (Score 4) but provide detailed pseudo-code (Score 3).

- **First 3 experiments**: 1. Establish Baseline: Run 320 raw prompts against target model to measure natural rejection rate 2. Stress Test: Apply Benign Expression and DRA mutations to measure drop in rejection rate 3. Categorical Analysis: Isolate results for "Information Theft" vs "Denial of Service" to verify disproportionate vulnerability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of generator model used to create jailbreak prompts influence the intensity of attacks and generalizability of security evaluations?
- Basis in paper: The authors list reliance on single model (Qwen-Turbo) for generating jailbreaking questions as a limitation restricting generalizability.
- Why unresolved: Different generator models might produce prompts with varying degrees of obfuscation or attack potency, potentially altering measured rejection rates.
- What evidence would resolve it: Comparative experiments regenerating MalwareBench using diverse LLMs (e.g., GPT-4, Claude) and correlating attack success rates.

### Open Question 2
- Question: How do white-box jailbreak attacks and more complex black-box algorithms compare to current template and rewriting methods in compromising code generation LLMs?
- Basis in paper: The paper states in Limitations that "White-box methods and some complex black-box methods remain untested" despite their strong attack capabilities.
- Why unresolved: Current study focuses solely on 11 specific black-box methods, leaving model's robustness against gradient-based or optimization-driven attacks unknown.
- What evidence would resolve it: Extending evaluation framework to include white-box attacks (e.g., GCG) on open-source models within the benchmark.

### Open Question 3
- Question: Does integration of external safety guardrails (like Llama Guard) inadvertently cause developers to deprioritize intrinsic safety alignment of base model?
- Basis in paper: Authors hypothesize that "External safety measures such as input and output checks might have reduced engineers' attention to the intrinsic safety of the model," noting Llama 3's weaker performance.
- Why unresolved: Unclear if observed security regression is direct result of shifting reliance to external tools or side effect of other training objectives.
- What evidence would resolve it: Ablation study measuring intrinsic safety of base models before and after application of external safety alignment procedures.

## Limitations
- The study relies on only 320 manually crafted requirements, which may not comprehensively represent real-world malicious code requests
- Effectiveness of jailbreak methods like "Benign Expression" may degrade against future models with improved semantic understanding capabilities
- Use of LLM judges introduces potential bias, as agreement rates with human evaluators vary significantly (67.67% vs 40.33%)

## Confidence
- **High Confidence**: Experimental methodology for measuring refusal rates and quality scores is well-documented and reproducible
- **Medium Confidence**: Claim that CodeLlama-70B's superior robustness stems from intrinsic RLHF V5 fine-tuning versus Llama-3.3-70B's external guardrail reliance requires architectural validation
- **Low Confidence**: Assertion that "Benign Expression" is universally more effective may be context-dependent and vary across different model architectures

## Next Checks
1. Test the same benchmark against Claude-3-Sonnet and Gemini-1.5-Pro to determine if observed patterns hold across different model families
2. Evaluate whether adding Llama Guard 2-1 or similar external safety classifiers to Llama-3.3-70B significantly improves its refusal rates in this benchmark
3. Create test environment where models are accessed through commercial APIs with native safety systems active, comparing results to local, unfiltered evaluation to assess practical security gaps