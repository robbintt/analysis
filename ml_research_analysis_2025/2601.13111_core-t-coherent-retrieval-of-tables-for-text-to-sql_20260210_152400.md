---
ver: rpa2
title: 'CORE-T: COherent REtrieval of Tables for Text-to-SQL'
arxiv_id: '2601.13111'
source_url: https://arxiv.org/abs/2601.13111
tags:
- table
- tables
- name
- retrieval
- purpose
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses multi-table retrieval for text-to-SQL over
  large, heterogeneous table corpora without database identifiers. CORE-T enriches
  tables with LLM-generated purpose metadata and pre-computes a lightweight compatibility
  cache.
---

# CORE-T: COherent REtrieval of Tables for Text-to-SQL

## Quick Facts
- arXiv ID: 2601.13111
- Source URL: https://arxiv.org/abs/2601.13111
- Authors: Hassan Soliman; Vivek Gupta; Dan Roth; Iryna Gurevych
- Reference count: 40
- Key outcome: Improves table-selection F1 by up to 22.7 points and multi-table execution accuracy by up to 6.9 points on MMQA, using 4-5× fewer tokens than LLM-intensive baselines.

## Executive Summary
CORE-T addresses multi-table retrieval for text-to-SQL over large, heterogeneous table corpora without database identifiers. It enriches tables with LLM-generated purpose metadata and pre-computes a lightweight compatibility cache. The method retrieves top-K candidates via dense retrieval, selects a coherent joinable subset using a single LLM call, and applies an additive adjustment to restore strongly compatible tables. Across Bird, Spider, and MMQA benchmarks, CORE-T demonstrates significant improvements in table-selection F1, execution accuracy, and token efficiency compared to existing approaches.

## Method Summary
CORE-T operates through an offline enrichment phase and an online retrieval pipeline. Offline, each table is serialized into a 5-row Markdown representation augmented with LLM-generated purpose descriptions, then embedded using UAE-Large-V1 and indexed with FAISS. A compatibility cache is precomputed using column-level signals including uniqueness, subset relations, Jaccard overlap, and header similarity. Online, the method performs DR@10 retrieval, applies single LLM selection using cached compatibility evidence, and adds strongly compatible unchosen tables via an additive adjustment step (threshold τ_comp=0.3). The entire pipeline uses deterministic decoding parameters and maintains efficiency through lightweight compatibility scoring.

## Key Results
- Improves table-selection F1 by up to 22.7 points compared to DR@10 baseline
- Increases multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA
- Retrieves up to 42% fewer tables while maintaining or improving accuracy
- Uses 4-5× fewer tokens than LLM-intensive baselines

## Why This Works (Mechanism)

### Mechanism 1: Purpose Metadata Disambiguation
- Claim: LLM-generated purpose descriptions help distinguish semantically overlapping tables when database identifiers are unavailable.
- Mechanism: Tables are serialized into 5-row Markdown snippets, then an LLM generates a purpose paragraph summarizing contents and usage. This enriched representation is embedded, providing semantic grounding for dense retrieval over heterogeneous pooled corpora.
- Core assumption: A 5-row sample captures sufficient schema semantics; the LLM can reliably summarize table intent.
- Evidence anchors: [abstract] "enriches tables with LLM-generated purpose metadata"; [section 3.1] "each table t is serialized into a 5-row Markdown representation and augmented with an LLM-generated purpose description"; [corpus] EnrichIndex (arXiv 2504.03598) uses similar offline enrichment for retrieval indices.
- Break condition: Purpose generation fails or produces generic descriptions; tables with identical schemas but different domains remain conflated.

### Mechanism 2: Constrained Compatibility Scoring
- Claim: Enforcing key–foreign-key-like constraints (uniqueness + subset) suppresses spurious join edges in pooled corpora.
- Mechanism: Column pairs are scored only if one column is unique and its values are a subset of the other, combining header similarity and Jaccard overlap. This filters noisy matches from generic columns (e.g., `id`, `name`).
- Core assumption: Valid joins approximate key–foreign-key relationships; subset and uniqueness signals are available from sampled rows.
- Evidence anchors: [abstract] "pre-computes a lightweight table-compatibility cache"; [section 3.1] "we only score pairs where at least one column is unique and the values exhibit a subset relation"; [corpus] Corpus lacks direct comparisons on constraint design; this is a methodological contribution.
- Break condition: Joins require non-equi conditions, multi-column keys, or bridge tables without clear subset signals.

### Mechanism 3: Additive Adjustment for Recall Protection
- Claim: A post-hoc additive step recovers strongly compatible tables that the LLM may have over-pruned.
- Mechanism: After LLM selection, for each selected table, the most compatible unchosen neighbor from the original top-K is added if its compatibility score exceeds a threshold (τ_comp=0.3).
- Core assumption: Strongly compatible neighbors are likely part of the same join path even if the LLM omitted them.
- Evidence anchors: [abstract] "a simple additive adjustment step restores strongly compatible tables"; [section 3.2.3] "For each selected table t∈TK′(q), we find its most compatible unchosen neighbor... and add it if CS(t, t⋆)≥τ_comp"; [corpus] Related work (JAR/ARM) uses MIP optimization instead; no direct corpus evidence on additive restoration.
- Break condition: τ_comp is set too high (missing bridges) or too low (re-introducing distractors).

## Foundational Learning

- Concept: Dense Retrieval (DR) over Embeddings
  - Why needed here: Initial candidate generation uses cosine similarity between query and enriched table embeddings.
  - Quick check question: Can you explain why high recall in DR alone is insufficient for multi-table retrieval?

- Concept: Key–Foreign-Key Join Semantics
  - Why needed here: The compatibility cache approximates joinability via uniqueness and subset constraints.
  - Quick check question: Why would generic columns like `name` create spurious join edges in pooled corpora?

- Concept: Single-Shot vs. Iterative LLM Selection
  - Why needed here: CORE-T uses one LLM call plus adjustment, contrasting with ReAct's multi-step approach.
  - Quick check question: What is the tradeoff between single-shot selection and iterative agent-based retrieval?

## Architecture Onboarding

- Component map: Table serializer (5-row Markdown) → LLM purpose generator → Embedding encoder → FAISS index. Parallel: Column signal extractor → Compatibility cache builder. Online: Query embedder → DR top-K → LLM selector (with compatibility evidence) → Additive adjuster → SQL generator.

- Critical path: DR retrieval → LLM selection with cached compatibility → adjustment; if LLM output is unparsable, fallback to DR@10.

- Design tradeoffs:
  - K=10 balances recall vs. context size (Appendix, Table 8); smaller K loses recall, larger K increases noise and token cost.
  - τ_comp=0.3 fixed across datasets; tuning per-dataset could improve but reduces generalizability.
  - 5-row sampling is lightweight but may miss value distributions; full-table inspection would be costlier.

- Failure signatures:
  - High precision errors (distractors included): Table 7 shows 90.0% of queries had precision issues even with CORE-T.
  - Over-pruning bridge tables: Addressed by adjustment, but may fail if compatibility scores are low.
  - Value sparsity or NULL-heavy columns: Degrades subset and uniqueness detection.

- First 3 experiments:
  1. Reproduce DR@10 baseline on BIRD dev split with UAE-Large-V1 embeddings; measure F1 and perfect recall.
  2. Implement compatibility scoring on a small table subset (e.g., 20 tables from 2 databases) and manually verify join predictions against gold foreign keys.
  3. Run end-to-end CORE-T pipeline with Llama-3.1-8B-Instruct as selector on 100 BIRD queries; compare EM≥2T against DR@10 and analyze error breakdown.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can extending CORE-T's compatibility cache to model multi-column and composite-key joins improve retrieval recall for complex multi-table queries without substantially increasing precomputation cost?
- Basis in paper: [explicit] Conclusion states: "Future work includes richer join modeling (e.g., multi-column)."
- Why unresolved: The current compatibility scoring considers only single column-pairs with key–foreign-key-like constraints; composite joins are common in enterprise schemas but remain unsupported.
- What evidence would resolve it: Evaluating CORE-T on a benchmark with annotated composite foreign keys, comparing F1 and EM when using single- vs. multi-column compatibility scoring.

### Open Question 2
- Question: Does CORE-T's retrieval performance transfer to multilingual open-book settings where tables and queries span multiple languages?
- Basis in paper: [explicit] Limitations section notes: "all evaluated benchmarks are English-only, so we do not assess multilingual open-book retrieval or text-to-SQL."
- Why unresolved: Cross-lingual table relevance and join compatibility may require different embedding strategies or purpose descriptions, which have not been tested.
- What evidence would resolve it: Evaluating on a multilingual table corpus with queries in multiple languages, comparing monolingual vs. multilingual embeddings and purpose generation.

### Open Question 3
- Question: How robust is CORE-T's value-based compatibility scoring when tables contain sparse, heavily skewed, or privacy-constrained data?
- Basis in paper: [inferred] Limitations state: "value-based signals depend on representative rows; performance may degrade when values are sparse, heavily skewed, or unavailable due to privacy constraints."
- Why unresolved: The Jaccard overlap and subset constraints rely on sampled values; systematic degradation under low-data regimes has not been quantified.
- What evidence would resolve it: Controlled experiments varying row sparsity and skew in synthetic tables, measuring joinability accuracy and downstream EM as a function of data availability.

### Open Question 4
- Question: Can schema-only compatibility scoring (without row-level value inspection) achieve competitive retrieval quality while supporting privacy-sensitive or empty-table deployments?
- Basis in paper: [inferred] The method depends on value overlap and subset relations computed from sampled rows, but privacy constraints or newly-created tables may preclude value access.
- Why unresolved: The paper does not ablate or evaluate a schema-only variant of the compatibility cache.
- What evidence would resolve it: Comparing full CORE-T against a schema-only ablation (using only header semantics and uniqueness constraints) on the same benchmarks, quantifying the F1/EM gap.

## Limitations

- The compatibility scoring mechanism relies on subset and uniqueness assumptions that may not hold for complex schemas with composite keys, many-to-many relationships, or non-equi joins.
- The additive adjustment threshold (τ_comp=0.3) was not tuned per-dataset and may be suboptimal for different table distributions.
- Purpose metadata generation quality depends on 5-row sampling, which may not consistently capture table semantics across diverse domains or sparse data scenarios.

## Confidence

- **High Confidence**: Claims about token efficiency improvements (4-5× fewer tokens than LLM-intensive baselines) are well-supported by the methodology section and tokenization counting approach.
- **Medium Confidence**: Claims about F1 improvements (up to 22.7 points) and multi-table execution accuracy gains (up to 6.9 points on MMQA) are supported by benchmark results, but depend on the quality of the compatibility cache and purpose generation, which weren't independently validated.
- **Low Confidence**: Claims about robustness to heterogeneous pooled corpora without database identifiers are promising but based on limited evidence. The paper shows improvements over baselines but doesn't demonstrate performance degradation when core assumptions (subset relations, uniqueness) are violated.

## Next Checks

1. **Compatibility Cache Robustness Test**: Create a synthetic dataset with known foreign-key relationships and systematically introduce violations (composite keys, many-to-many bridges, non-equi conditions) to measure how compatibility scoring degrades and whether CORE-T can still retrieve necessary tables.

2. **Sample Size Sensitivity Analysis**: Vary the 5-row sampling from tables (2, 5, 10, full-table) and measure the impact on purpose generation quality and downstream retrieval performance to establish the minimum viable sample size.

3. **Threshold Sensitivity Evaluation**: Conduct an ablation study across τ_comp values (0.1, 0.2, 0.3, 0.4, 0.5) on each benchmark to determine whether the fixed threshold is truly optimal or if per-dataset tuning would yield better recall without excessive distractor inclusion.