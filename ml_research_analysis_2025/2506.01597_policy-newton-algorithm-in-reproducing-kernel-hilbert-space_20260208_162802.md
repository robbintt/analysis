---
ver: rpa2
title: Policy Newton Algorithm in Reproducing Kernel Hilbert Space
arxiv_id: '2506.01597'
source_url: https://arxiv.org/abs/2506.01597
tags:
- policy
- rkhs
- newton
- optimization
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Policy Newton in RKHS, the first second-order
  optimization framework for RL policies represented in Reproducing Kernel Hilbert
  Spaces. The key innovation is transforming the intractable infinite-dimensional
  optimization problem into a finite-dimensional problem using the Representer Theorem,
  enabling cubic-regularized Newton steps that avoid explicit Hessian inversion.
---

# Policy Newton Algorithm in Reproducing Kernel Hilbert Space

## Quick Facts
- arXiv ID: 2506.01597
- Source URL: https://arxiv.org/abs/2506.01597
- Reference count: 40
- Primary result: First second-order optimization framework for RL policies in Reproducing Kernel Hilbert Spaces achieves quadratic convergence and superior performance compared to first-order methods.

## Executive Summary
This paper introduces Policy Newton in RKHS, the first second-order optimization framework for RL policies represented in Reproducing Kernel Hilbert Spaces. The key innovation is transforming the intractable infinite-dimensional optimization problem into a finite-dimensional problem using the Representer Theorem, enabling cubic-regularized Newton steps that avoid explicit Hessian inversion. The algorithm achieves local quadratic convergence and provably converges to a local optimum. Empirical results on a toy financial allocation problem demonstrate quadratic convergence, while experiments on CartPole and Lunar Lander show superior performance compared to first-order RKHS methods and parametric second-order approaches, achieving higher episodic rewards with faster convergence. The method bridges non-parametric policy representations with second-order optimization in reinforcement learning.

## Method Summary
The method represents policies as functions h in RKHS H_K, with policy π_h(a|s) = exp(T·h(s,a))/Z. At each iteration, N trajectories are sampled under the current policy, and gradient and Hessian estimates are computed via Monte Carlo using kernel evaluations. The infinite-dimensional Newton step is transformed into a finite-dimensional problem via the Representer Theorem, reducing optimization over H_K to optimization over coefficient vectors α ∈ ℝ^{NT}. The cubic-regularized auxiliary objective is solved using conjugate gradient descent to obtain the update, which is then applied to the policy. The method avoids explicit Hessian inversion through cubic regularization while maintaining second-order convergence properties.

## Key Results
- Achieves local quadratic convergence on toy Asset Allocation problem (linear decay in log-error vs iteration)
- Outperforms first-order RKHS gradient methods and parametric second-order approaches on CartPole and Lunar Lander
- Demonstrates superior sample efficiency with higher episodic rewards in fewer iterations
- Validated on three domains: toy financial allocation, CartPole, and Lunar Lander

## Why This Works (Mechanism)

### Mechanism 1: Cubic Regularization Avoids Explicit Hessian Inversion
- Claim: Optimizing a cubic-regularized auxiliary objective yields Newton-like steps without computing the intractable inverse Hessian operator in infinite-dimensional RKHS.
- Mechanism: Instead of solving Δh = -(∇²J)^(-1)∇J directly, the method solves Δh = argmin_{h̄} [⟨∇J, h̄⟩ + ½⟨∇²J ∘ h̄, h̄⟩ + (β/6)‖h̄‖³]. The cubic term acts as a trust-region surrogate, ensuring bounded steps while implicitly encoding curvature through the quadratic term.
- Core assumption: The Hessian operator is Lipschitz continuous with constant L ≤ β (Assumption 4.1), ensuring the Taylor upper bound holds with residual bounded by (L/6)‖Δh‖³.
- Evidence anchors:
  - [abstract] "Our approach circumvents direct computation of the inverse Hessian operator by optimizing a cubic regularized auxiliary objective function."
  - [section 3, Eq. 5] Formulates the RKHS Newton step via the cubic regularized objective.
  - [corpus] Related work on safe exploration in RKHS (arXiv:2503.10352) assumes bounded RKHS norms but does not address second-order policy optimization—highlighting novelty here.
- Break condition: If β is set too small (β < L), the cubic regularization insufficiently bounds the Taylor remainder, potentially causing divergence; if β is too large, steps become overly conservative, degrading to first-order behavior.

### Mechanism 2: Representer Theorem Reduces Infinite-to-Finite Dimension
- Claim: The infinite-dimensional RKHS optimization for the Newton step is exactly equivalent to a finite-dimensional optimization over coefficient vectors α ∈ ℝ^{NT}.
- Mechanism: By the Representer Theorem (Lemma 3.2), any minimizer of the regularized functional admits representation h(·) = Σ_{i=1}^{NT} α_i K(x_i, ·), where x_i are state-action pairs from trajectory data. Substituting this into the auxiliary objective yields Eq. (7): minimize ⟨v, ᾱ⟩ + ½⟨Hᾱ, ᾱ⟩ + (β/6)‖ᾱ‖²³, where v and H are computable kernel matrices.
- Core assumption: The Representer Theorem applies because the objective combines a data-dependent cost (gradient/Hessian terms estimated from trajectories) with a strictly increasing regularizer G(‖h‖).
- Evidence anchors:
  - [abstract] "Crucially, we leverage the Representer Theorem to transform this infinite-dimensional optimization into an equivalent, computationally tractable finite-dimensional problem whose dimensionality scales with the trajectory data volume."
  - [Theorem 3.3] Derives explicit forms for v ∈ ℝ^{NT} and H ∈ ℝ^{NT×NT} in terms of kernel evaluations.
  - [corpus] "Sampling Complexity of TD and PPO in RKHS" (arXiv:2509.24991) also uses RKHS for policy optimization but focuses on first-order PPO with TD critics, not second-order methods—underscoring the gap this paper addresses.
- Break condition: Computational cost scales as O((NT)²) for matrix H construction and O((NT)³) for naive solving; for large NT (long horizons or many trajectories), this becomes prohibitive without approximation.

### Mechanism 3: Quadratic Convergence via Bounded Inverse Operator
- Claim: Under deterministic conditions (exact gradient/Hessian), the algorithm achieves local quadratic convergence: ‖h_{k+1} - h*‖ ≤ C_q‖h_k - h*‖².
- Mechanism: The optimality condition (∇J(h_k) + ∇²J(h_k) ∘ Δh_k + (β/2)‖Δh_k‖Δh_k = 0) combined with Taylor expansion around h* yields error dynamics where e_{k+1} = A_k^{-1}∘[(H_k - H*)∘e_k + (β/2)‖Δh_k‖e_k - R₁]. If ‖A_k^{-1}‖ ≤ B and ‖Δh_k‖ ≤ K‖e_k‖, quadratic convergence follows.
- Core assumption: Assumption: The inverse operator norm ‖(∇²J(h_k) + (β/2)‖Δh_k‖I)^{-1}‖ is bounded by constant B, and initial iterate h₀ is sufficiently close to h*.
- Evidence anchors:
  - [Theorem 4.6] States local quadratic convergence under deterministic settings with bounded inverse operator.
  - [Figure 1a] Empirically validates quadratic convergence on the Asset Allocation toy problem.
  - [corpus] Corpus lacks direct corroboration of quadratic convergence for RKHS policy Newton; related papers focus on first-order methods or online kernel learning with different assumptions.
- Break condition: Under stochastic gradients (real RL settings), the quadratic rate degrades; the paper explicitly notes comprehensive stochastic analysis remains future work.

## Foundational Learning

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: Policies are represented as functions h ∈ H_K rather than parameter vectors, enabling non-parametric flexibility. The reproducing property ⟨h, K(x,·)⟩ = h(x) is exploited to express gradients and Hessians via kernel evaluations.
  - Quick check question: Given kernel K and points x₁, x₂, can you compute ⟨K(x₁,·), K(x₂,·)⟩ and explain why this equals K(x₁, x₂)?

- Concept: Fréchet Derivative in Function Spaces
  - Why needed here: Unlike finite-dimensional gradients, the policy gradient in RKHS is itself a function ∇_h J ∈ H_K, and the Hessian is an operator H_K → H_K ⊗ H_K. Understanding this is essential to derive Lemma 3.1.
  - Quick check question: For U(h) = e^{T h(s,a)}K((s,a),·), can you articulate why the second Fréchet derivative involves an outer product ⊗?

- Concept: Policy Gradient Theorem
  - Why needed here: The RKHS gradient (Eq. 4) extends the standard policy gradient ∇_θ J = E[Ψ_t ∇_θ log π], replacing ∇_θ log π with the RKHS expression T(K((s,a),·) - E_{a'}[K((s,a'),·)]).
  - Quick check question: Derive the first-order RKHS gradient term and explain the role of the temperature T and the expectation subtraction.

## Architecture Onboarding

- Component map: Trajectory Sampler -> Gradient/Hessian Estimator -> Cubic Newton Solver -> Policy Updater
- Critical path:
  1. Sample N trajectories → 2. Compute kernel matrices K(x_i, x_j) for all state-action pairs → 3. Assemble v and H using cumulative rewards Ψ_t(τ) → 4. Solve cubic-regularized quadratic optimization → 5. Update h and policy

- Design tradeoffs:
  - **Batch size N**: Larger N improves Monte Carlo estimation (variance ~ 1/N) but increases computational cost O((NT)²) for H.
  - **Regularization β**: Must satisfy β ≥ L (Lipschitz constant); too small risks instability, too large slows convergence.
  - **Kernel choice**: Not explicitly discussed in paper; kernel selection affects representational capacity and kernel matrix conditioning.
  - **Learning rate η**: Introduced in Algorithm 1 but theoretical analysis assumes η=1; practical tuning may be required.

- Failure signatures:
  - **Divergence with small β**: If β < L, the Taylor residual bound fails; observe exploding step norms ‖Δh‖.
  - **Slow convergence with large NT**: Matrix operations scale poorly; if runtime per iteration exceeds practical limits, consider mini-batch approximations.
  - **Stochastic gradient variance**: If gradient norm does not decrease as expected, increase trajectory batch size N.

- First 3 experiments:
  1. **Toy validation (Asset Allocation)**: Implement on the simplified MDP with known optimal policy; plot log(‖h_k - h*‖) vs. iteration to verify quadratic slope (should be linear in log-log).
  2. **Ablation on β**: Run CartPole with β ∈ {0.1, 1.0, 10.0}; monitor convergence rate and stability to identify viable range.
  3. **Comparison on CartPole/Lunar Lander**: Reproduce Figure 2 results comparing Policy Newton in RKHS vs. first-order RKHS gradient and parametric Newton; log episodic reward per iteration to confirm sample efficiency gains.

## Open Questions the Paper Calls Out
None

## Limitations
- Stochastic gradient convergence analysis remains incomplete; the paper assumes deterministic gradients for theoretical guarantees while noting this is future work.
- Computational complexity scales as O((NT)²) for Hessian construction and O((NT)³) for naive solving, making the method potentially prohibitive for long horizons or many trajectories without approximation techniques.
- Kernel selection and hyperparameters (bandwidth, regularization strength β, temperature T) are not extensively explored, though they critically impact performance.

## Confidence
- **High confidence** in the cubic-regularized Newton mechanism and Representer Theorem application, as these are mathematically rigorous with explicit proofs in the appendices.
- **Medium confidence** in empirical quadratic convergence on the toy problem, as the toy MDP is specified but the exact setup details require careful implementation.
- **Medium confidence** in the CartPole/Lunar Lander comparisons, as the parametric baseline details are not fully specified, making exact reproduction challenging.

## Next Checks
1. Reproduce quadratic convergence on the Asset Allocation toy problem by plotting log(error) vs. iteration to verify linear decay in log-log space.
2. Perform ablation study on cubic regularization parameter β across CartPole experiments to identify stability and convergence tradeoffs.
3. Implement and compare against the parametric baseline method on Lunar Lander, ensuring identical trajectory sampling and reward structures for fair evaluation.