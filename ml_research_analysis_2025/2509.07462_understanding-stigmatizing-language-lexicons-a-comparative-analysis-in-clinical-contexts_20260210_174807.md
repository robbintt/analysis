---
ver: rpa2
title: 'Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical
  Contexts'
arxiv_id: '2509.07462'
source_url: https://arxiv.org/abs/2509.07462
tags:
- stigmatizing
- language
- lexicons
- terms
- lexicon
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study reviewed and compared four stigmatizing language lexicons
  used in clinical documentation. The analysis found moderate semantic similarity
  (average LCH score 1.52) between lexicons, indicating shared conceptual themes but
  notable differences.
---

# Understanding Stigmatizing Language Lexicons: A Comparative Analysis in Clinical Contexts

## Quick Facts
- arXiv ID: 2509.07462
- Source URL: https://arxiv.org/abs/2509.07462
- Reference count: 0
- Primary result: Moderate semantic similarity (LCH 1.52) between four stigmatizing language lexicons, with 45 overlapping terms including "unwilling," "noncompliant," and "nonadherent"

## Executive Summary
This study systematically compares four stigmatizing language lexicons used in clinical documentation to assess semantic overlap, identify consensus terms, and analyze sentiment distribution. The researchers found moderate semantic similarity between lexicons (average LCH score 1.52) and identified 321 unique terms, with 45 appearing in at least two lexicons. Sentiment analysis revealed that 78.8% of matched terms were negative, 18.5% neutral, and 2.7% positive. The findings highlight the need for standardized stigmatizing language lexicons and underscore challenges in defining stigmatizing language solely through sentiment analysis, while identifying key terms that appear across multiple independent lexicons.

## Method Summary
The study extracted four stigmatizing language lexicons from recent literature through PubMed searches, then preprocessed them by removing diabetes-specific terms and positive descriptors. Semantic similarity was calculated using WordNet's path-based LCH measure to compare conceptual overlap between lexicons. Term frequency analysis identified high-occurrence terms appearing in at least two lexicons, while sentiment analysis matched terms against the WKWSCI Sentiment Lexicon to classify emotional valence. The researchers analyzed both the overlap patterns and sentiment distributions across the four lexicons to understand consensus and variation in stigmatizing language identification.

## Key Results
- Moderate semantic similarity between lexicons (average LCH score 1.52), indicating shared conceptual themes but notable differences
- 321 unique terms identified, with 45 appearing in at least two lexicons, including "unwilling," "noncompliant," and "nonadherent" in all four
- Sentiment analysis showed 78.8% of matched terms were negative, 18.5% neutral, and 2.7% positive, though proportions varied across lexicons
- High-frequency terms were largely associated with clinician judgments of negative patient behaviors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Path-based semantic similarity can quantify conceptual overlap between independently-developed lexicons
- Mechanism: WordNet expands each term into synsets; LCH similarity calculates shortest path between synsets within WordNet's hierarchical taxonomy
- Core assumption: Conceptual consensus among researchers manifests as measurable semantic proximity in lexical taxonomy
- Evidence anchors: Average LCH score 1.52 between lexicons; CARE-SD paper applies related lexicon-matching approaches to EHR stigma detection
- Break condition: Polysemous terms with domain-specific meanings not captured in WordNet may misestimate conceptual overlap

### Mechanism 2
- Claim: External sentiment lexicons can characterize emotional valence distribution of stigmatizing term lists
- Mechanism: Terms matched against WKWSCI Sentiment Lexicon and classified as positive, negative, or neutral
- Core assumption: Sentiment labels from general-purpose lexicon transfer adequately to clinical-context terms
- Evidence anchors: 78.8% negative, 18.5% neutral, 2.7% positive sentiment distribution; notable variation across lexicons
- Break condition: Clinical usage contexts may invert conventional sentiment, causing misclassification

### Mechanism 3
- Claim: Cross-lexicon term frequency identifies consensus stigmatizing language candidates
- Mechanism: Combine all terms into master list; count occurrences across lexicons; apply majority-vote threshold (≥50% of lexicons)
- Core assumption: Independent derivation by different research teams reduces idiosyncratic bias, making co-occurrence proxy for consensus
- Evidence anchors: 45 terms appearing in at least two lexicons; "unwilling," "noncompliant," "nonadherent" in all four
- Break condition: Shared source materials or citation chains compromise independence, inflating apparent consensus

## Foundational Learning

- Concept: WordNet synsets and path-based similarity (LCH)
  - Why needed here: Paper uses LCH via WordNet to estimate semantic overlap; understanding synsets and taxonomy paths is required to interpret the 1.52 score
  - Quick check question: If two terms share a synset, what does that imply about their LCH score?

- Concept: Sentiment lexicons and coverage limitations
  - Why needed here: Only 34-47% of terms matched WKWSCI; interpreting sentiment proportions requires understanding unmatched-term bias
  - Quick check question: If a lexicon has many unmatched terms, can you generalize the sentiment distribution to the full lexicon?

- Concept: Majority-vote aggregation for consensus detection
  - Why needed here: High-frequency terms defined by ≥50% lexicon appearance; threshold choice affects which terms are flagged as consensus
  - Quick check question: With only four lexicons, what is the practical minimum co-occurrence count to meet the 50% threshold?

## Architecture Onboarding

- Component map: Data ingestion -> Normalize terms -> Deduplicate -> WordNet synset expansion -> LCH similarity computation -> Average similarity matrix -> Master term list -> Lexicon co-occurrence counting -> Majority-vote labeling -> WKWSCI lookup -> Per-term classification -> Proportion calculation per lexicon
- Critical path: 1) Retrieve and normalize lexicons (manual extraction bottleneck) 2) Validate sentiment lexicon coverage before interpreting distributions 3) Compute semantic similarity and frequency metrics; cross-reference high-frequency terms with sentiment results
- Design tradeoffs: General vs. domain-specific resources (WordNet/WKWSCI are general-purpose; clinical NLP may need domain-adapted resources like UMLS); Term-level vs. context-level analysis (paper analyzes terms only; contextual usage is more accurate but complex); Threshold selection (majority-vote with N=4 is fragile; small N limits statistical confidence)
- Failure signatures: Low sentiment match rate (<40%) signals coverage issues; sentiment conclusions become speculative; High polysemy without disambiguation leads to noisy similarity scores; Lexicons from overlapping sources produce artificially high consensus
- First 3 experiments: 1) Replicate LCH similarity computation on four lexicons; verify 1.52 average and pairwise heatmap 2) Measure sentiment lexicon coverage for each lexicon; quantify unmatched-term proportions and test alternative clinical sentiment resources 3) Validate high-frequency consensus terms on held-out clinical notes sample (MIMIC-III subset) to assess term-level detection against manual annotation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What objective terminologies can effectively replace judgmental terms like "nonadherent" or "refused" to document patient behaviors without perpetuating stigma?
- Basis in paper: [explicit] The authors note that "the necessity of documenting non-adherence and exploring alternative terms remains an open discussion"
- Why unresolved: Removing high-frequency stigmatizing terms creates a gap for clinicians who need to accurately record patient behavior for safety and treatment planning
- What evidence would resolve it: Validated set of neutral descriptive alternatives (e.g., quantifying "percentage of days taken") that maintain clinical utility while reducing negative bias

### Open Question 2
- Question: How can a universally accepted, standardized stigmatizing language lexicon be developed to minimize discrepancies between research efforts?
- Basis in paper: [explicit] The authors explicitly "call for further research to refine and standardize a universally accepted lexicon" to address confusion caused by differing lexicons
- Why unresolved: Current lexicons show only moderate semantic similarity (average LCH 1.52) and low term overlap, driven by variations in data sources and study objectives
- What evidence would resolve it: Consensus-driven lexicon validated across multiple independent clinical datasets demonstrating high reliability in detecting bias regardless of clinical context

### Open Question 3
- Question: Can advanced language models accurately detect stigmatizing language at the contextual or sentence level rather than relying solely on isolated keywords?
- Basis in paper: [inferred] The authors acknowledge their analysis was limited because it "focused solely on individual terms," whereas stigmatizing language "can also manifest at the contextual level"
- Why unresolved: Sentiment analysis alone is insufficient (e.g., positive terms like "pleasant" can function as power/privilege language), and isolated term analysis misses the nuance of how words are used in full narratives
- What evidence would resolve it: Development and validation of context-aware NLP models that can distinguish between neutral descriptions and stigmatizing judgments based on sentence structure

## Limitations

- Sentiment lexicon coverage is limited, with only 34-47% of stigmatizing terms matching WKWSCI, constraining confidence in sentiment distribution claims
- Analysis focuses on individual terms without considering clinical context, missing nuanced usage where neutral terms become stigmatizing through framing
- Claims about practical utility of consensus terms for clinical documentation improvement lack validation against actual care outcomes or disparities

## Confidence

- High Confidence: Moderate semantic similarity (LCH score 1.52) between lexicons and identification of overlapping terms are directly computable from the data and methodology
- Medium Confidence: Sentiment analysis proportions are technically reproducible but limited by low sentiment lexicon coverage, making generalization uncertain
- Low Confidence: Claims about practical utility of consensus terms for clinical documentation improvement lack validation against actual care outcomes

## Next Checks

1. Replicate semantic similarity computation using WordNet to verify the 1.52 average LCH score and generate pairwise similarity heatmaps
2. Assess sentiment lexicon coverage by quantifying exact proportion of unmatched terms for each lexicon and testing alternative clinical sentiment resources
3. Validate high-frequency consensus terms by applying them to a held-out clinical notes sample (MIMIC-III subset) and comparing term-level detection against manual annotation of stigmatizing usage in full sentences