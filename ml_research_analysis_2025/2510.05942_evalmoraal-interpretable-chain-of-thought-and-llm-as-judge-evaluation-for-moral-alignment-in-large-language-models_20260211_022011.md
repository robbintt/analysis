---
ver: rpa2
title: 'EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for
  Moral Alignment in Large Language Models'
arxiv_id: '2510.05942'
source_url: https://arxiv.org/abs/2510.05942
tags:
- moral
- survey
- score
- alignment
- tier
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EvalMORAAL introduces a two-method scoring framework and model-as-judge\
  \ peer review to evaluate moral alignment of 20 LLMs on WVS and PEW survey data\
  \ across 64 countries. Structured chain-of-thought reasoning plus five-sample self-consistency\
  \ checks outperform log-probability scoring, with top models (Claude-3-Opus, GPT-4o)\
  \ achieving Pearson r\u22480.90 on WVS."
---

# EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models

## Quick Facts
- arXiv ID: 2510.05942
- Source URL: https://arxiv.org/abs/2510.05942
- Authors: Hadi Mohammadi; Anastasia Giachanou; Ayoub Bagheri
- Reference count: 40
- Primary result: EvalMORAAL framework evaluates 20 LLMs on moral alignment using chain-of-thought reasoning and LLM-as-judge peer review, achieving Pearson r≈0.90 with human survey data.

## Executive Summary
EvalMORAAL introduces a two-method scoring framework and model-as-judge peer review to evaluate moral alignment of 20 LLMs on WVS and PEW survey data across 64 countries. Structured chain-of-thought reasoning plus five-sample self-consistency checks outperform log-probability scoring, with top models (Claude-3-Opus, GPT-4o) achieving Pearson r≈0.90 on WVS. Direct scoring improves alignment by ~0.10 vs. likelihood-based probing. Peer agreement correlates with survey alignment (WVS r=0.74, PEW r=0.39), validating automated quality checks. Models perform strongly in Western regions (r=0.82) but show a persistent 0.21 gap in non-Western regions (r=0.61). The framework flags 348 conflicts, resolving 89% by majority voting.

## Method Summary
EvalMORAAL evaluates moral alignment using two scoring methods: log-probability extraction from antonymous adjective pairs and direct chain-of-thought scoring. For log-prob, 5 adjective pairs × 2 sentence templates generate 10 comparisons per country-topic pair, with token-level differences min-max normalized per model to [-1,1]. For direct scoring, a 3-step CoT prompt (norm recall → step-by-step reasoning → bounded score) runs k=5 times at temperature 0.7, with scores parsed from outputs and averaged. Self-consistency is measured via pairwise cosine similarity of reasoning embeddings across samples. Peer review has models evaluate anonymized traces for cultural accuracy and logical consistency, with conflict resolution by majority voting when score differences exceed 0.38.

## Key Results
- Structured CoT reasoning outperforms log-probability scoring, improving alignment by ~0.10 (Δr≈0.098)
- Top models achieve Pearson r≈0.90 on WVS, with Claude-3-Opus and GPT-4o leading
- Peer agreement correlates with survey alignment (WVS r=0.74, PEW r=0.39), validating automated quality checks
- Models show strong Western performance (r=0.82) but persistent 0.21 gap in non-Western regions (r=0.61)
- Framework flags 348 conflicts, resolving 89% through majority voting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Direct chain-of-thought scoring with structured reasoning outperforms log-probability probing for moral alignment evaluation, improving alignment by ~0.10 (Δr≈0.098).
- Mechanism: The three-step prompt (norm recall → step-by-step reasoning → bounded score) forces explicit cultural reasoning rather than relying on implicit token-level associations. The structured elicitation activates more deliberate processing pathways in the model, while log-probability probing captures more superficial associations that may reflect training distribution biases.
- Core assumption: Models have acquired sufficient cultural knowledge during pretraining that can be better accessed through explicit reasoning prompts than through implicit token probability patterns.
- Evidence anchors:
  - [abstract]: "Structured chain-of-thought reasoning plus five-sample self-consistency checks outperform log-probability scoring" and "Direct scoring improves alignment by ~0.10 vs. likelihood-based probing"
  - [results, Table 1]: Consistent Δr improvements across all 20 models, ranging from 0.081 to 0.119, with average Δr=0.098
  - [corpus]: The paper "Exploring Cultural Variations in Moral Judgments with Large Language Models" (Mohammadi et al., 2025c) is a direct predecessor, and related work by Ramezani and Xu (2023) found only moderate correlations with log-probability methods (WVS r≈0.35–0.41), providing baseline contrast
- Break condition: If models lack sufficient pretraining coverage of specific cultural contexts (particularly non-Western), explicit reasoning may still produce confident but incorrect outputs—the 0.21 Western/non-Western gap persists even with CoT.

### Mechanism 2
- Claim: Self-consistency across k=5 samples serves as a quality signal, with higher consistency correlating with better survey alignment (r=0.76, p<0.001).
- Mechanism: Sampling k=5 completions at temperature 0.7 produces reasoning variance. Computing pairwise cosine similarity of reasoning embeddings creates a self-consistency metric. Lower variance across samples indicates more robust, stable moral reasoning rather than random outputs.
- Core assumption: Consistent outputs across stochastic samples indicate genuine knowledge retrieval and reasoning rather than surface-level pattern matching or lucky guesses.
- Evidence anchors:
  - [results]: "Self-consistency scores range from 0.745 (PaLM-2) to 0.946 (GPT-4). Response consistency correlates strongly with survey alignment (r=0.76, p<0.001)"
  - [results]: "Higher alignment correlates with lower variance (r=-0.54, p<0.013)"
  - [corpus]: Corpus evidence on self-consistency mechanisms for moral evaluation is limited; the provided neighbors focus more on cultural alignment broadly rather than self-consistency methodology specifically
- Break condition: High self-consistency could indicate systematic, confident bias rather than accuracy—all k samples could converge on an incorrect cultural assumption if the model's training data has strong but skewed priors.

### Mechanism 3
- Claim: LLM-as-judge peer agreement correlates with survey alignment (WVS r=0.74, PEW r=0.39), enabling scalable automated quality assessment.
- Mechanism: Models evaluate anonymized reasoning traces (country/topic labels removed) for cultural accuracy, logical consistency, and score appropriateness. The proportion of VALID judgments from 19 peer models serves as a reasoning quality proxy. Fleiss' κ=0.67 indicates moderate inter-judge reliability.
- Core assumption: Models can assess abstract reasoning quality—logical coherence and score-reasoning alignment—independently of possessing specific cultural knowledge about the evaluated context.
- Evidence anchors:
  - [abstract]: "Peer agreement correlates with survey alignment (WVS r=0.74, PEW r=0.39), validating automated quality checks"
  - [results, Figure 3]: Clear positive relationship between peer-agreement rate and survey alignment across tiers; GPT-4o achieves highest peer-agreement (A=0.935)
  - [corpus]: The paper "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data" discusses LLM-as-judge methodology broadly but does not directly validate the peer-agreement-to-alignment correlation for moral reasoning specifically
- Break condition: If judge models share the same cultural biases as evaluated models, peer agreement could reinforce systematic errors rather than flag them—corpus notes this as a known limitation of LLM-as-judge approaches.

## Foundational Learning

- Concept: Chain-of-Thought Prompting
  - Why needed here: The framework's core innovation relies on structured CoT with explicit sequential steps (norm recall → reasoning → score) to improve moral judgment quality; understanding why this structure matters is essential
  - Quick check question: Why does the three-step prompt structure outperform directly asking for a moral score?

- Concept: Log-Probability Extraction and Normalization
  - Why needed here: The dual scoring approach requires extracting token-level probabilities and normalizing them per-model to [-1, 1] to prevent information leakage across models
  - Quick check question: Why must log-probability scores be min-max scaled per-model independently rather than using a global normalization?

- Concept: LLM-as-Judge with Anonymization
  - Why needed here: The peer review component uses anonymized traces to assess reasoning quality; understanding this design choice is critical for interpreting results
  - Quick check question: What information is removed from reasoning traces before peer evaluation, and why does this matter for validity?

## Architecture Onboarding

- Component map:
  1. Data Layer: WVS (55 countries × 19 topics) and PEW (39 countries × 8 topics) normalized to [-1, 1] matrices
  2. Prompting Module: Three-step CoT template + five log-probability sentence templates with antonymous adjective pairs
  3. Scoring Engine: Direct score extraction from "SCORE = x" parsing; log-probability computation via token likelihood differences across judgment pairs
  4. Self-Consistency Module: k=5 samples per scenario at temp=0.7, pairwise cosine similarity of reasoning embeddings
  5. Peer Review Module: Each model's traces judged by 19 peers (no self-judging), VALID/INVALID + 60-word justification
  6. Conflict Detection: 0.38 threshold (75th percentile of score differences), majority voting resolution (89% success rate)

- Critical path:
  1. Load and normalize survey data → generate all (country, topic) pairs
  2. For each model: run CoT k=5 times → extract and average direct scores; run log-probability scoring across 5 adjective pairs
  3. Compute Pearson correlations against survey ground truth (1,045 WVS pairs, 312 PEW pairs)
  4. Execute peer review: anonymize traces → collect VALID/INVALID judgments → compute peer-agreement rates
  5. Flag conflicts (|score diff| ≥ 0.38) → categorize (binary 70%, gradient 22%, outlier 8%) → resolve via majority vote

- Design tradeoffs:
  - Temperature 0.7: balances reasoning diversity vs. output consistency; lower temp increases self-consistency but may reduce exploration of cultural nuances
  - Nonresponse coding as 0: maintains complete coverage but conflates genuine neutrality with missing data—acknowledged limitation
  - English-only prompts: disadvantages multilingual models' native-language capabilities; corpus notes models like Qwen and DeepSeek may underperform in English evaluation
  - Conflict threshold (0.38): empirically derived (75th percentile) but arbitrary; different thresholds would flag different conflict rates

- Failure signatures:
  - High sample variance (mean 0.12, SD=0.08) indicates unstable reasoning; check variance per model before trusting scores
  - Low peer agreement (<0.70) signals reasoning quality issues even if alignment scores look reasonable
  - Regional performance gaps: Western r=0.82 vs. non-Western r=0.61—expect systematic underperformance in Sub-Saharan Africa, South Asia, Middle East
  - Topic-specific failures: violence-related topics (terrorism, political violence, wife beating) show highest MAE (>0.4 in lower-tier models)

- First 3 experiments:
  1. Replicate scoring comparison on 5 diverse models (e.g., GPT-4o, Claude-3-Opus, Llama-3.3-70B, Mistral-7B, Qwen-2.5-7B): verify Δr ≈ 0.10 improvement holds across model families
  2. Sensitivity analysis on self-consistency sample size: compare k=3, k=5, k=10 to quantify stability-accuracy-compute tradeoff
  3. Validate peer-agreement-to-alignment correlation on held-out scenarios: hold out 20% of country-topic pairs, test whether peer agreement predicts alignment for unseen cases (tests generalizability of r=0.74 WVS correlation)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can culture-specific fine-tuning or adaptation methods reduce the 0.21 Western/non-Western alignment gap without degrading performance on Western contexts?
- Basis in paper: [explicit] "Future research It can explore several promising directions: (1) developing culture-specific fine-tuning approaches to help models better capture local moral details"
- Why unresolved: The paper documents the gap across all 20 models and scales but does not test interventions; prior work (Ramezani and Xu, 2023) found a utility–bias trade-off when fine-tuning on survey data.
- What evidence would resolve it: A controlled fine-tuning study measuring WVS/PEW alignment in both Western and non-Western regions before and after culture-specific adaptation, tracking any regression in previously well-aligned contexts.

### Open Question 2
- Question: How does moral alignment change when models are evaluated in native languages rather than English prompts?
- Basis in paper: [inferred] From Limitations: "our evaluation relies primarily on English prompts, potentially disadvantaging models optimized for other languages... testing them in English may not show their full capabilities in native languages."
- Why unresolved: Multilingual models (Qwen, Gemini, DeepSeek) were tested only in English; prior work (Agarwal et al., 2024) shows moral judgments vary by language.
- What evidence would resolve it: Parallel evaluation of multilingual models using WVS/PEW items translated into native languages (e.g., Chinese for Qwen, French/German for multilingual models), comparing alignment to English-prompt baselines.

### Open Question 3
- Question: How robust are alignment metrics to alternative treatments of survey nonresponses (e.g., explicit modeling vs. neutral coding)?
- Basis in paper: [explicit] "our nonresponse coding strategy (assigning neutral value 0 to missing data) introduces potential bias toward the midpoint... Future work should explore alternative approaches, such as modeling nonresponse explicitly or conducting sensitivity analyses with different coding schemes."
- Why unresolved: The current approach conflates genuinely neutral attitudes with missing information; impact on model–survey correlations is unknown.
- What evidence would resolve it: Sensitivity analysis comparing three or more coding strategies (neutral, listwise deletion, explicit nonresponse modeling) on final Pearson correlations and tier rankings.

### Open Question 4
- Question: Does the LLM-as-judge peer-agreement signal generalize to other reasoning domains beyond moral evaluation?
- Basis in paper: [explicit] From Limitations: "EvalMORAAL's LLM-as-judge component... represents a novel approach that needs further validation across different domains and tasks."
- Why unresolved: Peer agreement correlated with survey alignment here, but whether this relationship holds for factual reasoning, mathematical reasoning, or other domains is untested.
- What evidence would resolve it: Apply the same peer-review protocol to tasks with ground truth (e.g., MMLU, GSM8K) and test whether peer-agreement rates correlate with task accuracy.

## Limitations

- Sampling methodology: k=5 self-consistency sample size and temperature 0.7 are empirically chosen but not systematically optimized
- Western/non-Western performance gap: 0.21 alignment gap persists even with structured reasoning, suggesting systematic cultural blind spots
- Peer review validity: Correlation between peer agreement and survey alignment could reflect shared cultural biases rather than genuine quality assessment

## Confidence

- High Confidence: Direct scoring outperforming log-probability (Δr≈0.098, verified across all 20 models), self-consistency correlating with alignment (r=0.76, p<0.001), and peer-agreement correlating with alignment (WVS r=0.74, PEW r=0.39)
- Medium Confidence: The 0.21 Western/non-Western performance gap persists as a robust finding but the root cause (data coverage vs. model bias) remains unclear
- Low Confidence: The claim that peer agreement validates automated quality assessment—correlation doesn't establish causation or error detection capability

## Next Checks

1. Cross-Cultural Generalization Test: Hold out 20% of country-topic pairs and evaluate whether peer-agreement rates from training data predict alignment accuracy on unseen cultural contexts (tests if r=0.74 correlation generalizes beyond in-sample patterns)

2. Self-Consistency Sensitivity Analysis: Systematically vary k (3, 5, 10 samples) and temperature (0.5, 0.7, 1.0) to quantify the stability-accuracy-compute tradeoff and determine if k=5 is optimal or arbitrary

3. Bias Detection Validation: Introduce controlled systematic errors into reasoning traces (e.g., consistent cultural bias toward Western norms) and test whether peer review correctly flags these as INVALID, distinguishing error detection from bias reinforcement