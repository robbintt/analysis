---
ver: rpa2
title: Diffusion Decoding for Peptide De Novo Sequencing
arxiv_id: '2507.10955'
source_url: https://arxiv.org/abs/2507.10955
tags:
- diffusion
- peptide
- casanovo
- amino
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates diffusion decoding for peptide de novo
  sequencing, aiming to improve upon traditional autoregressive methods that suffer
  from cascading errors and inefficient use of high-confidence regions. The study
  replaces autoregressive decoders in the Casanovo framework with three diffusion
  decoder designs and evaluates them using various loss functions including cross-entropy,
  weighted entropy, and DINOISER.
---

# Diffusion Decoding for Peptide De Novo Sequencing

## Quick Facts
- arXiv ID: 2507.10955
- Source URL: https://arxiv.org/abs/2507.10955
- Reference count: 6
- Amino acid recall improves from 0.081 to 0.454 using DINOISER loss with Casanovo-DM2 diffusion decoder

## Executive Summary
This paper investigates replacing autoregressive decoders in peptide de novo sequencing with diffusion decoders to address cascading errors and inefficient use of high-confidence regions. The study evaluates three diffusion decoder variants within the Casanovo framework using various loss functions including DINOISER. While simple diffusion decoder replacement initially reduces performance, the DINOISER loss function with Casanovo-DM2 achieves a statistically significant improvement in amino acid recall (0.454) compared to the baseline Casanovo model (0.081), representing a 0.373 absolute increase. Despite zero peptide precision and coverage across all diffusion models, this work demonstrates the potential of diffusion decoders to enhance model sensitivity and drive advancements in peptide de novo sequencing when paired with appropriate loss functions for discrete data.

## Method Summary
The study replaces Casanovo's autoregressive decoder with three diffusion decoder variants (Casanovo-DS, Casanovo-DM1, Casanovo-DM2) and evaluates them using cross-entropy, weighted entropy, and DINOISER losses. The DINOISER loss function adds noise to predicted logits, making it suitable for discrete data generation. Experiments use the combined tryptic/non-tryptic dataset from Casanovo v4.2 (MassIVE-KB) with 277K/27K/200K train/val/test split. Models are trained on 2x RTX 4090 GPUs using default Casanovo hyperparameters. The best performing configuration achieves 0.454 amino acid recall, significantly outperforming the baseline's 0.081 recall.

## Key Results
- DINOISER loss with Casanovo-DM2 achieves amino acid recall of 0.454 vs baseline 0.081
- All diffusion models achieve zero peptide precision and coverage
- Knapsack beam search increases training time 23x (1.5h to 35h) without performance gains
- Simple diffusion decoder replacement reduces performance compared to baseline

## Why This Works (Mechanism)
The DINOISER loss function introduces noise to predicted logits, making it more suitable for discrete data generation in peptide sequencing. This addresses the limitations of standard cross-entropy loss which can struggle with the discrete nature of amino acid sequences. The diffusion decoder framework allows for iterative refinement of predictions, potentially capturing complex dependencies between amino acids that autoregressive models miss due to their sequential nature. The significant improvement in amino acid recall (0.454 vs 0.081) demonstrates that this approach better captures the underlying peptide structure even when full peptide recovery remains challenging.

## Foundational Learning
- **Mass spectrometry-based peptide sequencing**: Essential for understanding the input data format (MS/MS spectra) and the challenge of reconstructing peptide sequences from fragmented data
- **Diffusion probabilistic models**: Core technique replacing autoregressive decoding; understand the denoising process and how it differs from sequential generation
- **DINOISER loss function**: Critical innovation that adds noise to logits; needed to understand why this specific loss works better for discrete peptide sequences
- **Discrete sequence generation**: Important for grasping why standard diffusion approaches need modification for amino acid sequences
- **Beam search vs knapsack beam search**: Key to understanding the computational efficiency tradeoffs discussed in the paper
- **Amino acid recall vs peptide precision/coverage**: Metric distinctions crucial for interpreting results where amino acid-level performance improves but peptide-level performance remains zero

## Architecture Onboarding

**Component Map**
Casanovo encoder -> Diffusion decoder variants (DS/DM1/DM2) -> DINOISER loss function -> Sequence output

**Critical Path**
The diffusion decoder receives encoded MS/MS spectra features and generates amino acid sequences through iterative denoising steps. The DINOISER loss guides this generation by adding noise to logits during training, enabling better handling of discrete sequence data.

**Design Tradeoffs**
Diffusion decoders offer iterative refinement and potentially better capture of sequence dependencies compared to autoregressive models, but at the cost of increased computational complexity. The choice between standard beam search and knapsack beam search involves a significant tradeoff between training efficiency (23x difference) and potential performance gains that were not realized in this study.

**Failure Signatures**
- Overly long predicted sequences with trailing noise (paper notes this issue)
- Zero peptide precision and coverage despite improved amino acid recall
- Significant training time increases with advanced search strategies that don't yield performance benefits

**3 First Experiments**
1. Implement Casanovo-DM2 diffusion decoder and verify it can generate sequences of appropriate length
2. Train with DINOISER loss and measure amino acid recall on validation set
3. Compare standard beam search vs knapsack beam search on a small validation subset to confirm the 23x training time difference

## Open Questions the Paper Calls Out
None

## Limitations
- All diffusion models achieve zero peptide precision and coverage, indicating inability to recover complete peptide sequences
- Diffusion decoder replacement initially reduces performance compared to baseline autoregressive approach
- Knapsack beam search causes 23x training time increase without performance improvements
- Exact architectural specifications for diffusion decoders and DINOISER loss implementation details are not fully enumerated

## Confidence
- **High Confidence**: Core experimental findings are well-documented - DINOISER loss achieves 0.454 amino acid recall, all diffusion models have zero peptide precision/coverage, knapsack beam search causes 23x training time increase
- **Medium Confidence**: Architectural details of diffusion decoder variants and DINOISER loss implementation are partially described but lack complete specifications
- **Low Confidence**: Default Casanovo hyperparameters (learning rate, batch size, optimizer, training duration) are not specified

## Next Checks
1. **Architectural Verification**: Implement Casanovo-DM2 diffusion decoder following Figures 4-6, then validate sequence length distributions during training to detect if models produce overly long sequences with trailing noise as mentioned in the paper.

2. **Loss Function Implementation**: Implement DINOISER loss with noise schedule and compare amino acid recall on validation set - verify if 0.454 recall is achievable with correctly specified noise parameters and weighted entropy formula.

3. **Training Efficiency Test**: Compare standard beam search vs knapsack beam search on a small validation subset to confirm the 23x training time increase (1.5h to 35h) and verify that performance degradation occurs as reported.