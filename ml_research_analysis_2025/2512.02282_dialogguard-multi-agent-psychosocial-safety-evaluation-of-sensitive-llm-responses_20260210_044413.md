---
ver: rpa2
title: 'DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses'
arxiv_id: '2512.02282'
source_url: https://arxiv.org/abs/2512.02282
tags:
- safety
- evaluation
- language
- arxiv
- psychosocial
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "DialogGuard is a multi-agent framework for evaluating psychosocial\
  \ safety in LLM-generated responses. It operationalizes five high-severity risk\
  \ dimensions\u2014privacy violation, discriminatory behavior, mental manipulation,\
  \ psychological harm, and insulting behavior\u2014using a shared three-level rubric."
---

# DialogGuard: Multi-Agent Psychosocial Safety Evaluation of Sensitive LLM Responses

## Quick Facts
- arXiv ID: 2512.02282
- Source URL: https://arxiv.org/abs/2512.02282
- Authors: Han Luo; Guy Laban
- Reference count: 40
- Multi-agent framework achieves 87.5% accuracy in psychosocial safety assessment

## Executive Summary
DialogGuard introduces a multi-agent LLM-based framework for evaluating psychosocial safety in sensitive responses. The system operationalizes five high-severity risk dimensions using a shared three-level rubric and integrates four distinct LLM-as-a-judge pipelines. Experimental results demonstrate that multi-agent mechanisms consistently outperform both non-LLM baselines and single-agent judging approaches.

The framework achieves strong performance metrics, with accuracy reaching 87.5% and Spearman correlation up to 0.88 across all dimensions. A formative study with 12 practitioners validates its practical utility for prompt design, auditing, and decision-making in web-based applications. The system is released as open-source with a web interface providing detailed risk assessments and natural-language explanations.

## Method Summary
DialogGuard operationalizes five high-severity psychosocial risk dimensions through a shared three-level rubric (0: no risk, 1: medium risk, 2: high risk). The framework employs four LLM-as-a-judge pipelines: single-agent scoring, dual-agent correction, multi-agent debate, and stochastic majority voting. These mechanisms aggregate judgments from multiple LLM agents to evaluate responses for privacy violation, discriminatory behavior, mental manipulation, psychological harm, and insulting behavior. The system was evaluated on the PKU-SafeRLHF dataset, comparing performance against rule-based lexicon approaches and zero-shot NLI baselines.

## Key Results
- Multi-agent mechanisms achieve accuracy up to 87.5% and macro-averaged Spearman correlation up to 0.88
- Dual-agent correction and majority voting provide optimal balance of accuracy, F1-score, and human alignment
- Debate mechanism achieves highest recall but over-flags borderline cases with excessive false positives
- Single-agent scoring shows temperature sensitivity while multi-agent approaches demonstrate greater stability

## Why This Works (Mechanism)
DialogGuard leverages ensemble reasoning through multiple LLM agents to reduce individual model biases and improve judgment consistency. The multi-agent debate mechanism captures diverse perspectives on ambiguous cases, while majority voting aggregates independent assessments to achieve robust consensus. Dual-agent correction provides a balance between computational efficiency and improved accuracy by incorporating a single correction pass. The shared three-level rubric standardizes risk assessment across different dimensions, enabling consistent evaluation despite varying risk characteristics.

## Foundational Learning
- **Multi-agent ensemble methods**: Multiple LLM agents reduce individual bias and improve reliability through aggregation; quick check: compare single vs. multi-agent performance on edge cases
- **Three-level risk rubric**: Standardized scoring (0-2) simplifies complex psychosocial risks into actionable categories; quick check: validate rubric clarity with domain experts
- **LLM-as-a-judge**: Using LLMs to evaluate other LLMs leverages their semantic understanding; quick check: assess hallucination risks in judgment generation
- **Temperature sensitivity**: Single-agent scoring varies with temperature settings, affecting reproducibility; quick check: test stability across temperature ranges (0.0-1.0)
- **Risk taxonomy design**: Five high-severity dimensions capture distinct psychosocial harm types; quick check: verify coverage through expert review
- **Majority voting aggregation**: Stochastic selection of multiple judges produces robust consensus; quick check: analyze voting patterns on contentious cases

## Architecture Onboarding

**Component Map**: Input Response -> Risk Dimension Analysis -> LLM Judge(s) -> Risk Score -> Aggregation Mechanism -> Final Assessment

**Critical Path**: Response → Single-Agent Scoring → Risk Dimension Analysis → LLM Judge → Three-Level Rubric → Final Risk Score

**Design Tradeoffs**: Single-agent scoring offers speed but suffers from temperature sensitivity and individual bias; multi-agent debate provides thorough analysis but increases computational cost and may over-flag; majority voting balances accuracy and efficiency but requires multiple LLM calls; dual-agent correction offers middle ground with single correction pass.

**Failure Signatures**: Single-agent scoring fails on ambiguous cases with temperature-dependent variability; debate mechanism produces excessive false positives on borderline cases; majority voting may miss subtle risks if no clear consensus emerges; dual-agent correction can perpetuate initial judge bias if correction is insufficient.

**First 3 Experiments**:
1. Temperature sweep test: Evaluate single-agent scoring stability across temperatures (0.0, 0.5, 1.0) on identical inputs
2. Edge case analysis: Compare all four mechanisms on high-ambiguity test cases to identify failure modes
3. Runtime performance benchmark: Measure latency and resource usage across single-agent, dual-agent, debate, and majority voting approaches

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on LLM-based judges introduces model-specific biases that may not generalize across architectures
- Three-level scoring rubric may oversimplify complex psychosocial risks existing on continuums
- PKU-SafeRLHF dataset represents specific cultural context that may not capture global psychosocial safety concerns

## Confidence
- **High**: Relative performance advantages of multi-agent mechanisms over baselines and single-agent approaches
- **Medium**: Generalizability to other safety domains or cultural contexts given specific dataset and risk taxonomy
- **Low**: Long-term stability as LLM capabilities and safety standards evolve

## Next Checks
1. Cross-cultural validation: Test framework on datasets from diverse cultural contexts to assess taxonomy and threshold adaptation needs
2. Temporal stability assessment: Evaluate performance across multiple LLM versions over time to quantify capability changes
3. Human-AI agreement study: Systematically compare DialogGuard assessments with consensus ratings from diverse human experts, especially for edge cases with mechanism disagreement