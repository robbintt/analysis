---
ver: rpa2
title: Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP
  Framework for Image Captioning
arxiv_id: '2504.15199'
source_url: https://arxiv.org/abs/2504.15199
tags:
- image
- mils
- iterative
- blip-2
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study critically examines the computational efficiency of\
  \ MILS, a zero-shot image captioning framework that claims to enable large language\
  \ models (LLMs) to \"see and hear without any training.\" While MILS achieves competitive\
  \ performance, our evaluation reveals that its iterative refinement process\u2014\
  generating approximately 30,000 candidate captions per image and refining them over\
  \ multiple iterations\u2014incurs substantial computational overhead. In contrast,\
  \ single-pass alternatives like BLIP-2 and GPT-4V deliver comparable or superior\
  \ caption quality with significantly lower resource demands."
---

# Zero-Shot, But at What Cost? Unveiling the Hidden Overhead of MILS's LLM-CLIP Framework for Image Captioning

## Quick Facts
- arXiv ID: 2504.15199
- Source URL: https://arxiv.org/abs/2504.15199
- Reference count: 25
- Primary result: MILS's iterative refinement process generates ~30K caption candidates per image and incurs substantial computational overhead, achieving lower caption quality than single-pass alternatives (BLIP-2, GPT-4V) at significantly higher cost.

## Executive Summary
This study critically evaluates MILS, a zero-shot image captioning framework that claims to enable large language models to "see and hear without any training." While MILS achieves competitive performance, our evaluation reveals that its iterative refinement process—generating approximately 30,000 candidate captions per image and refining them over multiple iterations—incurs substantial computational overhead. In contrast, single-pass alternatives like BLIP-2 and GPT-4V deliver comparable or superior caption quality with significantly lower resource demands. Quantitative results on the MSCOCO dataset show that MILS's BLEU-1 score (0.4538) and CIDEr score (0.3231) lag behind BLIP-2 (BLEU-1: 0.6713, CIDEr: 0.8569) and GPT-4V. MILS also requires 116 hours and costs $291.66 to process the dataset, compared to BLIP-2's 31 minutes ($0.65) and GPT-4V's 6.8 hours ($5.41). These findings challenge MILS's efficiency claims, highlighting the trade-off between iterative refinement and computational cost in zero-shot multimodal systems.

## Method Summary
The study compares MILS against BLIP-2 and GPT-4V on the MSCOCO 2014 validation set (5,000 images). MILS uses a Generator LLM (Llama 3.1 8B) to produce ~30K candidate captions per image, which are scored by a CLIP-like model and iteratively refined over multiple rounds. BLIP-2 employs a frozen ViT-L/14 visual encoder, Q-Former, and FLAN-T5-XL LLM to generate captions in a single pass. GPT-4V uses a black-box API approach with base64-encoded images. Quality metrics include BLEU-1/2/3/4, ROUGE_L, CIDEr, and METEOR, while computational efficiency is measured by runtime, GPU usage, and cost. MILS is run unmodified from its original repository with Modal wrapper, BLIP-2 uses zero-shot inference, and GPT-4V employs single API calls with rate limiting.

## Key Results
- MILS achieves BLEU-1 score of 0.4538 and CIDEr score of 0.3231 on MSCOCO, significantly lower than BLIP-2 (BLEU-1: 0.6713, CIDEr: 0.8569) and GPT-4V
- MILS requires 116 hours and costs $291.66 to process MSCOCO validation set, versus BLIP-2's 31 minutes ($0.65) and GPT-4V's 6.8 hours ($5.41)
- MILS's iterative refinement generates ~30,000 candidate captions per image across multiple refinement rounds, creating substantial computational overhead
- Single-pass methods (BLIP-2, GPT-4V) deliver superior caption quality at 200-50x lower computational cost than MILS

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative candidate generation and refinement can improve caption quality without training, but incurs massive computational overhead that may negate practical benefits.
- Mechanism: A Generator LLM produces ~30K candidate captions per image from diverse prompts → A Scorer (CLIP) computes cosine similarity between image visual features and each caption's text embedding → Top candidates (e.g., top 50) are fed back to the LLM with prompts to generate refined candidates → Loop continues until convergence or iteration limit.
- Core assumption: Assumption: Larger candidate pools and more refinement iterations monotonically improve output quality enough to justify computational cost.
- Evidence anchors:
  - [abstract] "MILS's iterative refinement process—generating tens of thousands of caption candidates per image and refining them through multiple rounds—incurs substantial computational overhead."
  - [Section 3.1] "The Generator utilizes a pre-trained LLM (e.g., Llama 3.1 8B) to produce an exceptionally diverse initial set... resulting in roughly tens of thousands (approximately 30K per image) of candidate captions."
  - [corpus] Related paper "LLMs can see and hear without any training" (arXiv:2501.18096) is the original MILS framework being evaluated; corpus evidence on iterative refinement efficiency is limited.
- Break condition: When marginal quality gains per iteration fall below threshold while computational cost grows linearly/quadratically; observed in results where MILS (116h, $291.66) underperforms BLIP-2 (31min, $0.65) on all metrics.

### Mechanism 2
- Claim: Single-pass multimodal architectures can achieve superior caption quality by pre-aligning visual and textual representations during pre-training, eliminating the need for test-time optimization.
- Mechanism: Frozen Vision Transformer (ViT-L/14) extracts visual features → Query Transformer (Q-Former) projects visual features into language model token space → Frozen LLM (FLAN-T5-XL) generates caption in one forward pass.
- Core assumption: Assumption: The Q-Former learns a sufficiently general mapping during pre-training that transfers zero-shot to unseen images without iteration.
- Evidence anchors:
  - [abstract] "BLIP-2 (31 minutes, $0.65)... BLEU-1: 0.671, CIDEr: 0.857"
  - [Section 3.2] "BLIP-2 combines a frozen Vision Transformer... a lightweight Query Transformer, and a pre-trained language model... to directly generate captions, significantly reducing inference time."
  - [corpus] Weak corpus evidence directly comparing single-pass vs iterative; related work on efficient reasoning (arXiv:2501.19201) discusses hiding verbose reasoning but doesn't directly address this trade-off.
- Break condition: When images require domain-specific knowledge not captured during pre-training, or when fine-grained visual details exceed Q-Former's alignment capacity.

### Mechanism 3
- Claim: CLIP-based scoring provides a reliable zero-shot signal for caption quality by measuring semantic alignment in a shared embedding space.
- Mechanism: Image encoded by ViT visual encoder → embedding vector; Caption encoded by CLIP text encoder → embedding vector; Alignment score = cosine similarity between vectors.
- Core assumption: CLIP embeddings capture semantically meaningful correspondences between visual content and natural language descriptions.
- Evidence anchors:
  - [Section 3.1] "The Scorer employs a CLIP-like model featuring a frozen visual encoder (typically a ViT-L/14) and a corresponding text encoder... computes cosine similarity scores to determine how well each caption aligns with the image content."
  - [Section 2] References CLIP [1] as foundational for "joint embeddings, making it possible for models to bridge visual and linguistic modalities."
  - [corpus] "Zero-Shot Decentralized Federated Learning" (arXiv:2509.26462) notes "CLIP has revolutionized zero-shot learning by enabling task generalization without fine-tuning."
- Break condition: When CLIP's contrastive objective fails to capture fine-grained distinctions (e.g., counting, spatial relations), or when hallucinated captions score highly due to semantic plausibility without visual grounding.

## Foundational Learning

- Concept: Zero-shot inference
  - Why needed here: The paper's central claim is comparing "training-free" zero-shot methods; understanding that zero-shot means no task-specific gradient updates is essential to evaluate whether MILS's test-time optimization is truly "free."
  - Quick check question: If a method runs 10 iterations of candidate generation and scoring at inference time, is it still "zero-shot"? Why or why not?

- Concept: Vision-language embedding alignment
  - Why needed here: Both MILS and BLIP-2 rely on aligning visual and textual representations; understanding CLIP's contrastive learning objective explains why cosine similarity works as a scoring function.
  - Quick check question: What property of CLIP's training makes cosine similarity between image and text embeddings meaningful for semantic alignment?

- Concept: Inference-time compute trade-offs
  - Why needed here: The paper's key finding is that MILS substitutes training compute for inference compute; recognizing this equivalence is critical for practical system design.
  - Quick check question: If Model A requires 100 GPU-hours of training and 0.01 GPU-hours per inference, while Model B requires 0 training but 1 GPU-hour per inference, at what scale does Model A become more efficient?

## Architecture Onboarding

- Component map:
  - **MILS**: Generator (Llama 3.1 8B) ↔ Scorer (CLIP ViT-L/14) in iterative loop; ~30K candidates per image; 10+ refinement iterations
  - **BLIP-2**: ViT-L/14 → Q-Former → FLAN-T5-XL (single pass)
  - **GPT-4V**: Image base64 → API call → caption (single pass, black-box)

- Critical path:
  1. Image preprocessing (resize, normalize for ViT)
  2. For MILS: Candidate generation → CLIP encoding → Scoring → Top-k selection → Feedback prompt → Repeat
  3. For BLIP-2: Visual encoding → Q-Former query → LLM generation
  4. Metric computation (BLEU, CIDEr, METEOR, ROUGE_L) against reference captions

- Design tradeoffs:
  - Candidate pool size (MILS): Larger pools increase coverage but scale O(n) compute; paper uses ~30K
  - Iteration count (MILS): More iterations enable refinement but show diminishing returns; paper observed 10+ iterations
  - Model size vs. speed: BLIP-2 uses FLAN-T5-XL (3B params); GPT-4V scale unknown but API-latency bound
  - Hardware: MILS requires 8× A100-80GB; BLIP-2 runs on single A10

- Failure signatures:
  - OOM errors during candidate generation (reported in Section 4.3 even with A100-80GB)
  - Cost runaway: MILS at 116 hours/$291.66 vs. expected minutes
  - Quality plateau: MILS CIDEr 0.323 vs. BLIP-2 CIDEr 0.857 despite 200× more compute
  - Hallucination in captions (qualitative analysis notes MILS "occasionally included hallucinated elements")

- First 3 experiments:
  1. **Baseline replication**: Run MILS on 100-image subset with default settings; measure time, cost, and BLEU/CIDEr to validate reproduction of paper's 116h result.
  2. **Ablation on candidate pool**: Reduce candidates from 30K to 1K/5K/10K; plot quality vs. compute curve to identify knee point where returns diminish.
  3. **Early stopping analysis**: Instrument MILS to log quality metrics per iteration; determine optimal iteration count before quality plateaus or degrades.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can hybrid approaches combining iterative refinement with single-pass inference achieve superior trade-offs between caption quality and computational efficiency?
- Basis in paper: [explicit] The conclusion states: "Future work will explore hybrid approaches that combine the strengths of iterative refinement with the efficiency of single-pass inference."
- Why unresolved: The paper establishes that MILS's iterative approach is computationally expensive while BLIP-2 and GPT-4V are efficient, but no hybrid approach has been tested.
- What evidence would resolve it: Experiments comparing hybrid architectures (e.g., limited iteration counts, early stopping based on quality thresholds) against pure iterative and pure single-pass methods on both quality metrics and computational cost.

### Open Question 2
- Question: What optimization techniques could reduce MILS's computational overhead without significantly degrading caption quality?
- Basis in paper: [explicit] The conclusion explicitly calls for "optimization techniques to reduce the overhead associated with iterative methods."
- Why unresolved: MILS generates ~30K candidates per image over multiple iterations; whether fewer candidates or iterations could maintain competitive quality remains unexplored.
- What evidence would resolve it: Ablation studies varying candidate pool sizes (e.g., 5K, 10K, 15K vs. 30K) and iteration counts, measuring resulting quality degradation and cost savings.

### Open Question 3
- Question: Do the computational overhead findings generalize to MILS's other supported modalities (video, audio captioning, text-to-image generation)?
- Basis in paper: [inferred] The paper notes MILS supports "image, video, and audio captioning as well as text-to-image generation and style transfer," but experiments are limited to MSCOCO image captioning.
- Why unresolved: Video and audio may require different numbers of candidates or iterations; single-pass alternatives may not exist or perform differently for these modalities.
- What evidence would resolve it: Comparative experiments on standard video (e.g., MSR-VTT) and audio captioning benchmarks, measuring both quality metrics and computational costs for MILS versus available baselines.

## Limitations

- The study evaluates only one zero-shot captioning framework (MILS) against two alternatives, limiting generalizability to other iterative zero-shot methods
- Computational cost comparisons assume public API pricing remains stable; actual enterprise deployment costs may differ
- The paper does not investigate whether smaller candidate pools or fewer iterations could achieve better cost-quality trade-offs for MILS
- GPT-4V serves as a black-box comparison without insight into its internal efficiency mechanisms

## Confidence

- **High confidence** in relative efficiency comparisons between methods on identical MSCOCO validation set
- **Medium confidence** in absolute runtime estimates, as infrastructure variations and batch processing optimizations could affect measurements
- **Medium confidence** in quality metric comparisons, though minor variations may occur due to implementation differences in evaluation code
- **Low confidence** in generalizability to other datasets or domains not represented in MSCOCO

## Next Checks

1. Replicate the study on a subset of images (e.g., 100 images) to verify computational cost estimates and quality metrics
2. Conduct an ablation study varying MILS's candidate pool size (1K, 5K, 10K, 30K) to identify the optimal cost-quality trade-off point
3. Test MILS on a different image captioning dataset (e.g., Flickr30k) to assess whether efficiency findings generalize beyond MSCOCO