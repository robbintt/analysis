---
ver: rpa2
title: 'Ontology Learning with LLMs: A Benchmark Study on Axiom Identification'
arxiv_id: '2512.05594'
source_url: https://arxiv.org/abs/2512.05594
tags:
- ontology
- axioms
- axiom
- ontologies
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates Large Language Models' (LLMs) ability to
  automatically identify axioms in ontologies, which are critical for formal knowledge
  representation. The authors introduce OntoAxiom, a benchmark of nine ontologies
  with 17,118 triples and 2,771 axioms, focusing on five key axiom types.
---

# Ontology Learning with LLMs: A Benchmark Study on Axiom Identification

## Quick Facts
- arXiv ID: 2512.05594
- Source URL: https://arxiv.org/abs/2512.05594
- Reference count: 40
- Primary result: LLMs can identify axioms in ontologies with varying success; AbA prompting outperforms Direct prompting, especially for subclass, domain, and range axioms.

## Executive Summary
This paper investigates Large Language Models' (LLMs) ability to automatically identify axioms in ontologies, which are critical for formal knowledge representation. The authors introduce OntoAxiom, a benchmark of nine ontologies with 17,118 triples and 2,771 axioms, focusing on five key axiom types. They compare two prompting strategies—Direct (all axioms at once) and Axiom-by-Axiom (AbA)—across twelve LLMs with three shot settings. Results show that AbA prompts yield higher F1 scores than Direct, particularly for subclass, domain, and range axioms. Performance varies by ontology and axiom type, with FOAF achieving the highest F1 (0.642) for subclass axioms, while domain and range axioms are more challenging. Larger LLMs, especially OpenAI's o1, outperform smaller models, though smaller models remain viable for resource-constrained settings. Overall, while fully automated axiom identification remains difficult, LLMs can provide valuable candidate axioms to support ontology engineers.

## Method Summary
The paper evaluates LLMs on axiom identification using the OntoAxiom benchmark, which consists of nine ontologies stripped of axioms. Two prompting strategies are compared: Direct (querying all axiom types simultaneously) and Axiom-by-Axiom (AbA, querying each axiom type separately). The study tests zero-, one-, and five-shot settings across twelve LLMs from OpenAI, Llama, Mistral, Qwen, and DeepSeek families. Models output JSON-formatted axiom predictions, which are evaluated against ground-truth axioms using precision, recall, and F1 scores. The temperature is set to 0.2 for consistent inference. Results show AbA prompting generally outperforms Direct, with performance varying by axiom type and ontology domain.

## Key Results
- AbA prompting yields higher F1 scores than Direct for subclass, domain, and range axioms.
- FOAF ontology achieves the highest F1 (0.642) for subclass axioms, while domain and range axioms remain challenging (F1 0.03-0.04).
- Larger LLMs, particularly OpenAI's o1, outperform smaller models, though smaller models like o4-mini remain viable for cost-constrained settings.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing axiom identification into single-axiom prompts improves F1 scores for most axiom types compared to querying all axioms simultaneously.
- Mechanism: Task decomposition reduces cognitive load on the model by constraining output space and providing focused context per axiom type, allowing more precise pattern matching for each logical relation.
- Core assumption: LLMs handle single-constraint reasoning better than multi-constraint reasoning in ontological contexts.
- Evidence anchors:
  - [abstract] "Results show that AbA prompts yield higher F1 scores than Direct, particularly for subclass, domain, and range axioms."
  - [section 5.3] Table 7 shows AbA one-shot achieving highest F1 (0.130) vs Direct few-shot (0.126); Figure 4 shows AbA outperforming Direct for subclass, domain, and range.
  - [corpus] Lippolis et al. "Ontology Generation using Large Language Models" supports decomposition via competency questions.
- Break condition: Disjoint axioms perform better with Direct prompting, suggesting some axiom types benefit from simultaneous comparison across classes.

### Mechanism 2
- Claim: Performance correlates with domain generality and likelihood of ontology presence in pre-training corpora.
- Mechanism: LLMs leverage statistical patterns from training data; widely-cited ontologies (e.g., FOAF) have stronger representations than specialized domains (e.g., ERA railway ontology).
- Core assumption: Pre-training exposure creates reusable schema-level knowledge transferable to axiom inference.
- Evidence anchors:
  - [abstract] "the FOAF ontology achieves a score of 0.642 for the subclass axiom, while the music ontology reaches only 0.218."
  - [section 6] "LLMs are likely trained on data that includes papers on this ontology [FOAF], since it is an often-used ontology."
  - [corpus] Corpus signals show related work focuses on general-purpose LLMs applied to ontology learning, with limited coverage of specialized industrial domains.
- Break condition: Highly specialized ontologies with novel terminology (e.g., ERA: `MAGNETICBRAKEPREVENTION`) may require domain-specific fine-tuning or external knowledge injection.

### Mechanism 3
- Claim: Model scale and reasoning specialization provide marginal but consistent improvements in axiom identification.
- Mechanism: Larger parameter counts and reasoning-focused training (e.g., o1's chain-of-thought) improve abstract logical inference over implicit ontological relations.
- Core assumption: Axiom identification requires multi-step reasoning about class-property relationships not explicitly stated.
- Evidence anchors:
  - [abstract] "Larger LLMs, especially OpenAI's o1, outperform smaller models."
  - [section 5.4] o1 achieves highest precision (0.249) and F1 (0.197); Table 8 shows large models (F1 0.118) outperform small (F1 0.077).
  - [corpus] Corpus evidence is limited on reasoning-specific architectures for ontology tasks; most related work uses general-purpose models.
- Break condition: Scale gains diminish for resource-constrained settings; o4-mini (smaller) achieves competitive F1 (0.197) vs o1, suggesting architecture efficiency matters.

## Foundational Learning

- Concept: **RDFS/OWL Axiom Types**
  - Why needed here: The paper evaluates five axiom types (subclass, disjoint, subproperty, domain, range); understanding their semantics is prerequisite to interpreting results.
  - Quick check question: Can you explain why `domain` constraints are harder to infer than `subclass` hierarchies?

- Concept: **Ontology Layer Cake Model**
  - Why needed here: The paper frames axioms as the top layer of ontology learning complexity; this mental model explains why automation difficulty increases at higher layers.
  - Quick check question: Why would "term extraction" be easier to automate than "disjointness detection"?

- Concept: **Few-Shot Prompt Engineering**
  - Why needed here: The paper tests zero/one/five-shot settings; understanding how examples shape model behavior is critical for reproducing results.
  - Quick check question: What tradeoff exists between providing more examples versus increasing prompt length for complex tasks?

## Architecture Onboarding

- Component map:
  - Input: Ontology without axioms (classes + properties as JSON)
  - Prompting Module: Direct (single prompt) vs AbA (5 separate prompts per axiom type)
  - LLM Layer: 12 models across OpenAI proprietary, Llama, Mistral, Qwen, DeepSeek families
  - Evaluation: Precision/Recall/F1 against ground-truth axioms (symmetric matching for disjoint)

- Critical path:
  1. Parse ontology → extract classes and properties
  2. Generate prompt(s) based on approach (Direct vs AbA) and shot setting
  3. Call LLM with temperature=0.2 for consistency
  4. Parse JSON output → extract predicted axiom pairs
  5. Compare against gold-standard axioms → compute P/R/F1

- Design tradeoffs:
  - **Direct vs AbA**: Direct is ~5x cheaper (1 API call vs 5) but lower F1 for subclass/domain/range
  - **Shot count**: One-shot optimal for AbA (F1 0.130); few-shot better for Direct (F1 0.126); diminishing returns beyond 5 examples
  - **Model selection**: OpenAI models higher performance; open-source (Llama 3.3, Mistral) viable for cost constraints

- Failure signatures:
  - Low recall on domain/range axioms (F1 0.03-0.04) indicates context-sensitivity challenges
  - Specialized ontologies (ERA: F1 0.055) underperform vs general domains (FOAF: F1 0.221)
  - Hallucinated axioms: models may predict plausible-but-incorrect relations absent from gold standard

- First 3 experiments:
  1. **Baseline reproduction**: Run AbA one-shot on FOAF ontology with GPT-4o and Llama 3.3; compare F1 for subclass axioms against paper's 0.642 benchmark.
  2. **Domain sensitivity test**: Apply same configuration to your target domain ontology; measure performance gap vs FOAF to estimate transfer difficulty.
  3. **Prompt optimization**: Test whether adding ontology metadata (e.g., class descriptions, example instances) improves domain/range F1 beyond the paper's 0.03-0.04 baseline.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance drops sharply for domain/range axioms (F1 0.03-0.04) and specialized ontologies (ERA: F1 0.055), suggesting current approaches struggle with context-sensitive constraints and novel terminology.
- The benchmark's reliance on existing ontologies may limit generalizability to emerging domains.
- Hallucinated axioms and parsing errors represent significant failure modes, particularly for the Direct prompting approach.

## Confidence
- **High confidence**: Mechanism 1 (task decomposition benefits) - multiple experimental conditions show consistent AbA advantage for subclass/domain/range axioms.
- **Medium confidence**: Mechanism 2 (domain generality correlation) - FOAF performance is clearly higher, but causal link to pre-training exposure is inferential rather than directly tested.
- **Medium confidence**: Mechanism 3 (scale advantages) - o1 outperforms smaller models, but efficiency gains of o4-mini suggest architecture matters beyond parameter count.

## Next Checks
1. Test whether ontology metadata injection (class descriptions, example instances) improves domain/range axiom F1 beyond the 0.03-0.04 baseline, addressing context-sensitivity limitations.
2. Evaluate hallucination rates by measuring predicted axioms that don't match gold standards but are logically plausible, quantifying the tradeoff between coverage and precision.
3. Replicate the Direct vs AbA comparison on a new domain-specific ontology to verify whether decomposition benefits generalize beyond the nine benchmark ontologies.