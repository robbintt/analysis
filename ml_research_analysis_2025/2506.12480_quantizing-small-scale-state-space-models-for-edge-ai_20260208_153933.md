---
ver: rpa2
title: Quantizing Small-Scale State-Space Models for Edge AI
arxiv_id: '2506.12480'
source_url: https://arxiv.org/abs/2506.12480
tags:
- quantization
- state
- quantized
- arxiv
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies quantization of small-scale state-space models
  (SSMs) for edge AI, focusing on memory and computational efficiency. The authors
  analyze post-training quantization (PTQ) and quantization-aware training (QAT) on
  the S4D architecture using sequential MNIST.
---

# Quantizing Small-Scale State-Space Models for Edge AI

## Quick Facts
- **arXiv ID:** 2506.12480
- **Source URL:** https://arxiv.org/abs/2506.12480
- **Reference count:** 30
- **One-line primary result:** QAT enables sub-8-bit quantization of SSMs while maintaining high accuracy for edge deployment

## Executive Summary
This paper investigates quantization strategies for small-scale state-space models (SSMs) to enable efficient deployment on edge devices. The authors focus on the S4D architecture and sequential MNIST benchmark, demonstrating that while post-training quantization (PTQ) fails catastrophically at low precisions due to sensitivity of the state transition matrix A and internal state x, quantization-aware training (QAT) can recover performance to near-full-precision levels. A heterogeneous quantization strategy is introduced that assigns different precision levels to model components based on their sensitivity, achieving 6× memory reduction without sacrificing accuracy.

## Method Summary
The study pre-trains S4D models on sequential MNIST in convolutional mode for 30 epochs with a learning rate of 1e-3. Models are then converted to recurrent mode for quantization experiments. PTQ uses static asymmetric per-head quantization with 2-16 bit precision sweeps. For QAT, models are fine-tuned for 10 epochs with learning rate 1e-4 using straight-through gradient estimation. State x is clipped to [-50, +50] and gradients to [-1000, 1000] during training. The heterogeneous scheme applies 4-bit weights and 6-bit activations to non-SSM layers, while state components use 8-bit precision.

## Key Results
- PTQ requires at least 8-bit precision for state matrix A and internal state x to avoid catastrophic accuracy loss
- QAT improves accuracy from 40% (PTQ) to 96% at 8-bit precision on sequential MNIST
- Heterogeneous quantization achieves 6× memory footprint reduction while maintaining performance
- Frozen A parameterization outperforms learned alternatives at low precisions for state transition matrix

## Why This Works (Mechanism)

### Mechanism 1
QAT enables recovery from catastrophic accuracy degradation caused by PTQ through re-training the quantized model in recurrent mode using straight-through gradient estimation. This allows parameters to adapt to discretized representations, learning quantization-compatible values for the sensitive state transition matrix A and internal state x that maintain eigenvalue stability within the unit circle, avoiding divergence over long sequences.

### Mechanism 2
Heterogeneous quantization preserves accuracy while minimizing memory by matching bit-precision to component sensitivity. The S4D architecture exhibits markedly different sensitivities: A and x require ≥8-bit precision for stability, while B, C, D and non-state activations tolerate 4-bit and 6-bit respectively. By assigning precision proportional to sensitivity, the model reduces overall footprint without crossing failure thresholds.

### Mechanism 3
Frozen A parameterization at low precision outperforms learned alternatives by preserving full-precision eigenvalue stability. When A bit-precision is too low to represent fine-grained adjustments during QAT, direct training introduces instability. Freezing A to its full-precision quantized values keeps eigenvalues within the unit circle, while training other parameters compensates for representation loss.

## Foundational Learning

- **Linear Time-Invariant (LTI) Systems and Stability**: SSMs are built on LTI systems where the state transition matrix A governs stability. Eigenvalues outside the unit circle cause state explosion over time—quantization can push values into unstable regions. *Quick check: Given a discrete system x(n+1) = Āx(n), what condition on Ā's eigenvalues ensures bounded state over infinite time?*

- **Quantization Granularity and Symmetry**: The paper shows per-head quantization outperforms per-tensor, and asymmetric ranges capture distribution better than symmetric. Understanding these tradeoffs is essential for designing heterogeneous schemes. *Quick check: Why would per-channel (per-head) quantization yield higher effective resolution than per-tensor quantization for multi-head SSMs?*

- **Straight-Through Estimator (STE) for Non-Differentiable Operations**: QAT requires backpropagation through quantization (rounding), which is non-differentiable. STE approximates gradients by passing them through unchanged, enabling fine-tuning. *Quick check: What is the core approximation STE makes, and what failure mode can it introduce in recurrent gradient computation?*

## Architecture Onboarding

- **Component map**: Input sequence → SSM recurrence (Ā, x sensitivity dominant) → GeLU → GLU+Conv1D → next layer
- **Critical path**: Input sequence → SSM recurrence (Ā, x sensitivity dominant) → GeLU → GLU+Conv1D → next layer. Quantization errors in Ā propagate through n-time-step multiplications; errors in x accumulate directly
- **Design tradeoffs**: Precision vs. Memory (8-bit for Ā/x preserves stability; 4-bit for B/C/D/weights saves memory), Static vs. Dynamic Calibration (Static is hardware-friendly but less accurate; dynamic impractical for edge), Parameterization (Continuous maintains original S4D stability properties; Discrete enables direct low-precision training; Frozen Ā best for sub-8-bit A)
- **Failure signatures**: Accuracy drops to chance level at <8-bit for Ā or x under PTQ, State explosion if quantized Ā eigenvalues exit unit circle, Exploding gradients during QAT if clipping range insufficient
- **First 3 experiments**: 1) Baseline PTQ sensitivity sweep: Quantize each component independently at 2–16 bits to identify critical components, 2) QAT parameterization comparison: Train quantized S4D with Discrete, Continuous, and Frozen Ā parameterizations at 8-bit, 3) Heterogeneous quantization validation: Apply W4A6 with Ā8×8 and Ā6×6 variants, measure memory savings vs. accuracy tradeoff

## Open Questions the Paper Calls Out
1. How does the proposed quantization strategy perform on complex, real-world edge tasks like keyword spotting or biomedical signal processing? (Basis: Outlook section mentioning Google Speech Command and bio-medical signal processing as future work)
2. What is the specific hardware overhead (latency and energy) introduced by the heterogeneous quantization scheme during range alignment? (Basis: Outlook noting computational overhead to align integer ranges)
3. Which SSM architecture variants (e.g., Mamba, S5, S6) are most robust to low-precision quantization for edge deployment? (Basis: Authors asking which variant is most adapted to edge ML)

## Limitations
- Results validated only on sequential MNIST, a relatively simple benchmark, without testing on more complex sequential tasks or real-world edge scenarios
- Absence of open-source code and complete hyperparameter specifications makes exact reproduction challenging
- Does not explore why exactly 8 bits is the critical threshold or whether adaptive precision schemes could achieve better compression

## Confidence
- **High confidence**: PTQ sensitivity findings (A and x require ≥8-bit precision) and the 40% → 96% accuracy improvement from QAT at 8-bit precision
- **Medium confidence**: Heterogeneous quantization strategy achieving 6× memory reduction and Frozen A parameterization outperforming other schemes at low precisions
- **Medium confidence**: Generalizability to other SSM architectures or more complex sequential tasks remains unproven

## Next Checks
1. Test the heterogeneous quantization scheme on sequential CIFAR-10 or sequential Fashion-MNIST to verify the 6× compression ratio and accuracy preservation hold beyond sMNIST
2. Systematically sweep A and x precision from 4-bit to 16-bit in QAT mode to identify the exact breaking point where eigenvalues exit the unit circle and accuracy collapses
3. Apply the heterogeneous quantization strategy to a different SSM variant (e.g., S4 or Mamba) to determine whether the 8-bit requirement for state components is architecture-specific or a fundamental SSM constraint