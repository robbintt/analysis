---
ver: rpa2
title: CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration
  of Scientific Collections
arxiv_id: '2504.07643'
source_url: https://arxiv.org/abs/2504.07643
tags:
- collex
- user
- image
- system
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CollEX is a multimodal agentic RAG system designed to enable intuitive
  exploration of scientific collections. It addresses the challenge of navigating
  large, complex scientific datasets by using state-of-the-art Large Vision-Language
  Models (LVLMs) as agents within a chat interface.
---

# CollEX -- A Multimodal Agentic RAG System Enabling Interactive Exploration of Scientific Collections

## Quick Facts
- arXiv ID: 2504.07643
- Source URL: https://arxiv.org/abs/2504.07643
- Reference count: 25
- Primary result: Multimodal agentic RAG system enabling intuitive exploration of scientific collections through natural language and image queries

## Executive Summary
CollEX is a multimodal agentic RAG system designed to enable intuitive exploration of scientific collections. It addresses the challenge of navigating large, complex scientific datasets by using state-of-the-art Large Vision-Language Models (LVLMs) as agents within a chat interface. The system integrates textual and visual modalities, supporting educational and research use cases by fostering curiosity-driven exploration and interdisciplinary discovery. CollEX employs a combination of semantic and lexical search, cross-modal retrieval, and specialized tools for database lookup, image analysis, and query rewriting. The proof-of-concept implementation includes over 64,000 records across 32 collections, demonstrating effective interactive exploration through real-world user stories such as finding exhibition pieces and preparing educational presentations.

## Method Summary
CollEX implements an agentic loop where an LVLM receives user queries, plans tool usage, executes tools, and synthesizes responses. The system uses hybrid semantic-lexical indexing with parallel BM25 (lexical) and HNSW (semantic) indexes in Weaviate. Cross-modal retrieval employs SigLIP embeddings in a shared vector space, with query rewriting to improve recall. Tools include database lookup, lexical search, similarity search (text-to-image and image-to-image), and image analysis (VQA, captioning, OCR, object detection). The architecture is Dockerized with FastAPI backend, React/Vite frontend, and LVLM-agnostic design supporting multiple providers including OpenAI, VertexAI, and Ollama.

## Key Results
- Demonstrates intuitive exploration of 64,000+ scientific records through natural language and image queries
- Enables cross-modal search allowing users to find items using either descriptive text or example images
- Supports complex multi-step queries through agentic loop with sequential tool calls for tasks like educational presentations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-modal semantic retrieval enables intuitive exploration without requiring domain-specific vocabulary
- Mechanism: SigLIP multimodal embeddings create a shared vector space for text and images, allowing users to search using natural language ("beautiful minerals") or images (photo of a statue). The query-rewriter agent expands queries (e.g., "beautiful minerals" → "a photo of beautiful minerals, geology") before embedding, improving retrieval recall.
- Core assumption: Users' natural language descriptions sufficiently align with the visual content's semantic representation in the embedding space.
- Evidence anchors:
  - [section 3.3] "Similarity Search Tool... supports both textual and image-based cross-modal or uni-modal similarity searches by querying the HNSW index"
  - [section 4.2] Demonstrates text-to-image search ("beautiful minerals") and image-to-image search for similar-looking minerals
  - [corpus] Neighbor papers (ScienceDB AI, HySemRAG) confirm hybrid semantic-lexical retrieval improves discovery but lack comparative benchmarks
- Break condition: If embeddings fail to capture domain-specific visual distinctions (e.g., subtle mineral variations), retrieval quality degrades regardless of query rewriting.

### Mechanism 2
- Claim: The agentic loop enables iterative multi-step reasoning that adapts retrieval strategies dynamically
- Mechanism: The LVLM executes a ReAct-style loop: receive query → plan which tools to use → execute tools → observe results → decide if more tools needed → synthesize final response. This allows complex queries (e.g., "find similar minerals, then show collection details") that require sequential tool calls.
- Core assumption: The LVLM's function-calling capability reliably maps user intent to appropriate tool invocations and correctly interprets tool outputs.
- Evidence anchors:
  - [section 3.3] Pseudocode shows explicit agentic loop with `while is_tool_call_response(response)` iteration
  - [section 4.2] Geology presentation user story demonstrates sequential tool calls: similarity search → database lookup → collection retrieval
  - [corpus] Agentic RAG survey (Singh et al., 2025) provides theoretical grounding; empirical validation in CollEX is demonstration-only, not quantitative
- Break condition: If the LVLM misinterprets tool outputs or invokes irrelevant tools, the loop may produce incoherent responses or infinite iteration.

### Mechanism 3
- Claim: Hybrid lexical-semantic indexing balances precision for known terms with discovery of conceptually related items
- Mechanism: Parallel indexes (BM25 for lexical, HNSW for semantic) allow the agent to choose or combine retrieval strategies. BM25 handles exact matches (e.g., "Sanrománit"); HNSW handles conceptual matches (e.g., "crystalline structures").
- Core assumption: Users may alternate between targeted lookup (knowing exact terms) and exploratory search (using descriptive language), and the agent correctly selects the appropriate index.
- Evidence anchors:
  - [section 3.2] "We store collection descriptions and titles, as well as record titles in a BM25 index... [and] text and image embeddings... in an HNSW index"
  - [abstract] "combination of semantic and lexical search" explicitly claimed
  - [corpus] No direct corpus evidence on hybrid index performance; this is architectural inference from the paper
- Break condition: If query ambiguity causes the agent to select the wrong index consistently, users receive either overly narrow or noisy results.

## Foundational Learning

- Concept: **Multimodal Embeddings (CLIP/SigLIP)**
  - Why needed here: Core to cross-modal retrieval; understanding how text and images map to the same vector space explains why "beautiful minerals" retrieves relevant images.
  - Quick check question: Can you explain why a text query and a semantically similar image would have high dot-product similarity in a CLIP-style embedding space?

- Concept: **Retrieval-Augmented Generation (RAG)**
  - Why needed here: CollEX extends RAG from text-only to multimodal; understanding baseline RAG helps see how tools augment LVLM knowledge.
  - Quick check question: What problem does RAG solve compared to a standalone LLM, and how does adding images change the retrieval challenge?

- Concept: **ReAct / Agentic Design Patterns**
  - Why needed here: The agentic loop is a ReAct implementation; understanding reasoning-acting interleaving clarifies why multiple iterations may be needed.
  - Quick check question: In a ReAct loop, what triggers another iteration versus returning a final answer?

## Architecture Onboarding

- Component map:
  - Frontend (React + Vite + Material UI) -> Backend (FastAPI) -> Database (Weaviate) -> Embedding Service (LitServe + SigLIP) -> LVLM Providers (OpenAI, VertexAI, Ollama)

- Critical path:
  1. User sends query → Backend adds to chat history
  2. Backend calls LVLM with system prompt + history
  3. If tool call: execute tool → append result → re-call LVLM
  4. Repeat until no tool call → return final message
  5. Frontend parses custom tags → renders records/collections

- Design tradeoffs:
  - **Single agent vs. multi-agent hierarchy**: Current single agent may struggle with tool complexity; authors note multi-agent orchestration as future direction (section 6)
  - **Proprietary vs. open-weight LVLMs**: Proprietary models perform better but raise cost/privacy concerns; open-weight alternatives lag in accuracy
  - **Pre-computed vs. runtime embeddings**: All image embeddings pre-computed; only query embeddings computed at runtime (latency vs. freshness tradeoff)

- Failure signatures:
  - **Tool invocation errors**: LVLM calls wrong tool or malformed parameters → check prompt adherence, tool schema clarity
  - **Stuck in loop**: Agent keeps calling tools without terminating → likely prompt missing clear termination conditions
  - **Poor retrieval**: Semantic search returns irrelevant results → check embedding quality, query rewriting effectiveness
  - **LVLM misunderstanding**: Misinterprets tool JSON output → validate schema, add parsing examples to prompt

- First 3 experiments:
  1. **Probe cross-modal retrieval quality**: Submit 20 text queries and 20 image queries; measure top-k relevance manually. Baseline before any optimization.
  2. **Stress-test agentic loop**: Design queries requiring 3+ sequential tool calls; measure success rate and iteration count. Identify where the loop fails.
  3. **Compare LVLM providers**: Run identical query sets through GPT-4o, Gemini 2.0, and Gemma 3; qualitatively assess tool selection accuracy and response coherence. Note cost/latency tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the quantitative effectiveness of the CollEX system in terms of retrieval accuracy, task completion rates, and user satisfaction compared to conventional search interfaces?
- Basis in paper: [explicit] The authors explicitly state in Section 6 that the "current implementation of CollEX lacks formal evaluation" and that validating performance is a "priority for future work."
- Why unresolved: The paper relies on qualitative "user stories" (proof-of-concept) and lacks the resources to conduct systematic empirical assessments or comprehensive user studies.
- What evidence would resolve it: Results from controlled user studies measuring standard IR metrics (e.g., precision/recall) and subjective satisfaction scores (e.g., System Usability Scale) against a baseline keyword search.

### Open Question 2
- Question: Does a hierarchical multi-agent architecture reduce tool selection errors and improve reasoning capabilities compared to the single-agent design currently employed?
- Basis in paper: [explicit] Section 6 suggests that a "potential solution" to the LVLM being overwhelmed by tool complexity is "reorganizing the system... into multiple specialized agents managed hierarchically."
- Why unresolved: The current implementation uses a single agent which leads to "inappropriate or inefficient tool use," and the proposed multi-agent solution presents inter-communication challenges that have not yet been addressed.
- What evidence would resolve it: An ablation study comparing the tool-calling accuracy and error rates of the single-agent baseline against a hierarchical orchestrator model on a standardized set of complex queries.

### Open Question 3
- Question: To what extent can open-weight LVLMs be optimized via fine-tuning or advanced prompting to match the responsiveness and accuracy of proprietary models in this specific agentic loop?
- Basis in paper: [explicit] The limitations section notes that while the system is LVLM-agnostic, "open-source alternatives generally lag behind in accuracy, responsiveness, and general robustness," creating a dependency on proprietary models.
- Why unresolved: The paper demonstrates the system works with proprietary models (GPT-4o, Gemini) but acknowledges that the user experience suffers significantly when switching to currently available open-source models.
- What evidence would resolve it: A benchmark comparing the latency and tool-invocation success rates of optimized open-source models (e.g., Llama, Gemma) against proprietary baselines within the CollEX pipeline.

## Limitations

- No formal quantitative evaluation of retrieval accuracy, task completion rates, or user satisfaction metrics
- Single-agent architecture struggles with tool selection complexity, leading to inefficient or incorrect tool usage
- Dependency on proprietary LVLM models due to current open-weight alternatives' lower accuracy and responsiveness

## Confidence

- **High Confidence**: The architectural framework (agentic loop with ReAct-style reasoning, hybrid semantic-lexical indexing, cross-modal retrieval using SigLIP embeddings) is well-specified and theoretically sound based on established principles in multimodal AI and RAG systems.
- **Medium Confidence**: The system's ability to enable intuitive exploration through natural language and image queries is demonstrated qualitatively but lacks quantitative validation of retrieval accuracy or user satisfaction metrics.
- **Low Confidence**: Claims about the system's effectiveness in supporting interdisciplinary discovery and educational use cases are based on limited user stories without systematic evaluation of learning outcomes or cross-domain exploration patterns.

## Next Checks

1. **Quantitative Retrieval Evaluation**: Conduct a systematic evaluation measuring recall@k, precision@k, and mean average precision for both text-to-image and image-to-image search across diverse query types, comparing against baseline text-only RAG approaches.
2. **Agentic Loop Robustness Testing**: Design and execute a structured test suite of complex queries requiring 2-4 sequential tool calls, measuring success rate, iteration count, and error types to identify failure patterns and optimization opportunities.
3. **Cross-Modal Embedding Analysis**: Perform qualitative and quantitative analysis of SigLIP embedding quality by examining nearest-neighbor consistency, conducting human relevance judgments for retrieved items, and testing sensitivity to query rewriting variations.