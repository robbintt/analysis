---
ver: rpa2
title: Are Time Series Foundation Models Susceptible to Catastrophic Forgetting?
arxiv_id: '2510.00809'
source_url: https://arxiv.org/abs/2510.00809
tags:
- forgetting
- time
- learning
- series
- catastrophic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates catastrophic forgetting in Time Series
  Foundation Models (TSFMs), specifically focusing on the TimesFM model. The authors
  evaluate TimesFM's ability to retain knowledge when fine-tuned sequentially on multiple
  synthetic time series datasets with varying periodic structures.
---

# Are Time Series Foundation Models Susceptible to Catastrophic Forgetting?

## Quick Facts
- arXiv ID: 2510.00809
- Source URL: https://arxiv.org/abs/2510.00809
- Reference count: 26
- Primary result: TimesFM exhibits significant catastrophic forgetting when fine-tuned sequentially on multiple time series datasets

## Executive Summary
This paper investigates catastrophic forgetting in Time Series Foundation Models (TSFMs) using the TimesFM model as a case study. The authors evaluate TimesFM's ability to retain knowledge when fine-tuned sequentially on multiple synthetic time series datasets with varying periodic structures. Through a two-stage continual learning setup, they measure performance degradation on previously learned tasks after fine-tuning on new tasks. The study reveals that TimesFM experiences significant catastrophic forgetting, with Mean Absolute Error on earlier tasks increasing substantially after adaptation to new data.

## Method Summary
The authors employ a sequential fine-tuning approach where TimesFM is first trained on a source dataset (D1), then fine-tuned on a new dataset (D2), and finally evaluated on both datasets. This process is repeated for multiple dataset pairs with varying periodic structures. Performance is measured using Mean Absolute Error (MAE) on both the original and new tasks to quantify forgetting. The study systematically varies learning rates and fine-tuning epochs to identify optimal settings that balance stability and plasticity. All experiments use synthetic time series data with controlled periodic characteristics to isolate the forgetting phenomenon.

## Key Results
- MAE on D1 increases from 0.15 to 1.60 after fine-tuning on D2
- Intermediate learning rates (10^-5) with 5-10 epochs provide the best stability-plasticity balance
- No setting completely eliminates catastrophic forgetting in TimesFM
- The stability-plasticity dilemma remains unresolved even with hyperparameter optimization

## Why This Works (Mechanism)
The paper demonstrates that TimesFM's architecture and training approach make it vulnerable to catastrophic forgetting when faced with sequential learning tasks. The model's weight updates during fine-tuning on new data overwrite parameters critical for previous tasks, leading to performance degradation on earlier datasets. This mechanism is consistent with classical catastrophic forgetting observed in neural networks, where gradient-based optimization prioritizes adaptation to recent data at the expense of retaining prior knowledge.

## Foundational Learning
- Catastrophic forgetting: The tendency of neural networks to rapidly lose performance on previously learned tasks when trained on new data
  - Why needed: Central phenomenon being investigated in TSFMs
  - Quick check: Observe performance degradation on task A after training on task B

- Continual learning: Training models on sequential tasks without forgetting previous knowledge
  - Why needed: Framework for evaluating TSFMs' ability to handle evolving data
  - Quick check: Compare single-task vs sequential training performance

- Stability-plasticity dilemma: The trade-off between retaining old knowledge (stability) and learning new information (plasticity)
  - Why needed: Conceptual framework for understanding forgetting in neural networks
  - Quick check: Vary learning rates and epochs to find optimal balance

## Architecture Onboarding
TimesFM follows a transformer-based architecture adapted for time series forecasting. The model processes sequential temporal data through self-attention mechanisms and feed-forward layers, producing forecasts based on historical patterns.

Component map: Input sequence -> Positional encoding -> Transformer blocks -> Output projection -> Forecast

Critical path: Input → Positional encoding → Self-attention → Feed-forward → Output layer

Design tradeoffs: The architecture prioritizes forecasting accuracy on stationary distributions but lacks mechanisms for preserving knowledge across distribution shifts.

Failure signatures: Sudden performance drops on previously learned tasks when fine-tuning on new data; stable performance on current task despite forgetting.

First experiments:
1. Train TimesFM on D1, measure baseline MAE
2. Fine-tune on D2, evaluate MAE on both D1 and D2
3. Vary learning rate systematically while keeping other hyperparameters constant

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to synthetic datasets with specific periodic structures
- Only one TSFM architecture (TimesFM) examined
- No exploration of regularization or rehearsal methods to mitigate forgetting
- Metrics focus on MAE without statistical significance testing

## Confidence
The empirical observations are clearly documented and reproducible, but the narrow scope of evaluation and lack of comparison with alternative approaches or mitigation strategies limits broader conclusions about the field. Confidence in the major claims about TSFM susceptibility to catastrophic forgetting is rated as Medium.

## Next Checks
1. Replicate the study using real-world time series datasets with diverse characteristics (financial, sensor, medical) to assess whether catastrophic forgetting patterns persist beyond synthetic data
2. Compare TimesFM's forgetting behavior with other TSFM architectures (e.g., N-BEATS, Autoformer-based models) to determine if this is architecture-specific or a general TSFM phenomenon
3. Implement and evaluate common continual learning techniques (elastic weight consolidation, experience replay, regularization) to quantify their effectiveness in mitigating catastrophic forgetting in TSFMs