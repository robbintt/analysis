---
ver: rpa2
title: On damage of interpolation to adversarial robustness in regression
arxiv_id: '2601.16070'
source_url: https://arxiv.org/abs/2601.16070
tags:
- adversarial
- risk
- interpolation
- minimax
- emax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the adversarial robustness of interpolating\
  \ estimators in nonparametric regression, focusing on how perfect fitting of training\
  \ data affects resilience against future input perturbations. The authors establish\
  \ minimax rates for the adversarial L2-risk of \u03B4-interpolating estimators and\
  \ show that interpolation introduces a significant additional cost compared to optimal\
  \ non-interpolating methods."
---

# On damage of interpolation to adversarial robustness in regression

## Quick Facts
- arXiv ID: 2601.16070
- Source URL: https://arxiv.org/abs/2601.16070
- Authors: Jingfu Peng; Yuhong Yang
- Reference count: 22
- Primary result: Interpolation severely damages adversarial robustness in regression, with high interpolation leading to non-convergent risk even under small perturbations

## Executive Summary
This paper investigates the adversarial robustness of interpolating estimators in nonparametric regression, focusing on how perfect fitting of training data affects resilience against future input perturbations. The authors establish minimax rates for the adversarial L2-risk of δ-interpolating estimators and show that interpolation introduces a significant additional cost compared to optimal non-interpolating methods. When the interpolation degree is low, the adversarial risk matches optimal rates; however, with high interpolation (including exact fitting), the risk can fail to converge even under small perturbations, demonstrating that interpolation severely damages robustness. An interesting "curse of sample size" phenomenon is revealed, where increasing training data deteriorates adversarial robustness in the high interpolation regime.

## Method Summary
The study evaluates adversarial robustness of δ-interpolating estimators in nonparametric regression under future X-attacks. The method involves constructing δ-interpolating estimators that exactly fit training data within tolerance δ, then analyzing their minimax adversarial L2-risk under perturbations within radius r. The approach uses minimax risk theory to establish fundamental limits for interpolating estimators, comparing them to optimal non-interpolating methods. Experimental validation includes synthetic data (n∈{80,150,300}, three regression functions) and the Auto MPG dataset, comparing six methods: local polynomial regression, two types of interpolators, smoothing interpolators, 1-nearest neighbor, and minimum-norm neural networks.

## Key Results
- Interpolating estimators are fundamentally suboptimal for adversarial robustness, with interpolation damage quantified as a separate term in the minimax risk bound
- A critical threshold exists in interpolation degree δ: large δ (≈√log n) preserves optimal robustness, while small δ causes catastrophic risk failure
- The "curse of sample size" phenomenon shows that increasing training data can worsen adversarial robustness in high-interpolation regimes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interpolation forces estimators to fit training noise, creating local function variations that adversaries exploit by moving test points toward training points
- Mechanism: When δ (interpolation tolerance) is small, the estimator must fit each (Xᵢ, Yᵢ) nearly exactly, including noise ξᵢ = Yᵢ - f*(Xᵢ). This creates "spikes" near training points. Under attack radius r, an adversary moves from x to nearby Xᵢ within B_p(x,r), exposing the fitted noise through the term E[max_{i∈S_p(x,r)}(|ξᵢ| - δ)²₊]
- Core assumption: Gaussian or sub-Gaussian noise ξᵢ in regression model Y = f*(X) + ξ
- Evidence anchors:
  - [abstract]: "interpolating estimators must be suboptimal even under a subtle future X-attack, and achieving perfect fitting can substantially damage their robustness"
  - [Theorem 1, Eq. 3.1]: Minimax lower bound contains ∫ₓ E[max_{i∈S_p(x,r)}(|ξᵢ| - δ)²₊]dx as the interpolation damage term
  - [corpus]: Related work (arXiv:2511.00836) shows parameter interpolation affects robustness but doesn't establish this noise-amplification mechanism explicitly
- Break condition: When δ ≥ √(C · (2β/(2β+d) + γ) log n), the noise term becomes O(n^{-2β/(2β+d)}) and optimal robustness is preserved

### Mechanism 2
- Claim: More training data worsens adversarial robustness in high-interpolating regimes through denser packing of noise-fitted spikes
- Mechanism: As n increases, each adversarial ball B_p(x, r) contains more training points on average (roughly n·r^d). In high-interpolation (small δ), noise is fitted at all points. The max over more noise realizations grows: when n·r^d > C₄ log n, risk becomes Θ(log(n·r^d))
- Core assumption: Bounded density input distribution; i.i.d. training draws
- Evidence anchors:
  - [Section 3.3]: "curse of simple size, where increasing the amount of training data deteriorates the adversarial robustness of interpolators"
  - [Theorem 5]: When n·r^d > C₄ log n, adversarial risk = Θ(log(n·r^d)); when n·r^d ≳ 1, risk = Ω(1)
  - [corpus]: No direct corpus support for curse of sample size phenomenon
- Break condition: When r ≪ n^{-1/d}, expected points in adversarial ball remain small and risk can still converge

### Mechanism 3
- Claim: A critical threshold exists in interpolation degree δ—above it, optimal rates; below it, catastrophic robustness failure
- Mechanism: δ acts as soft-threshold ψ_δ(t) = sgn(t)(|t| - δ)₊ on noise. Large δ (≳ √log n) thresholds away most noise, controlling E[max(|ξᵢ| - δ)²₊]. Small δ exposes unthresholded noise, making max dominate risk
- Core assumption: Sub-Gaussian tails enabling concentration on max of noise terms
- Evidence anchors:
  - [Theorem 3]: Low interpolation (δ ≳ √log n) achieves optimal rate r^{2(1∧β)} + n^{-2β/(2β+d)}
  - [Theorem 5]: High interpolation (δ ≤ O(σ)) has minimax rate Ω(1) when n·r^d ≳ 1
  - [corpus]: Weak support—related work discusses adversarial training but not this phase transition
- Break condition: Transition occurs near δ ~ √log n; crossing below this threshold causes rapid degradation

## Foundational Learning

- Concept: Minimax risk theory
  - Why needed here: Results are framed as fundamental limits via minimax bounds over estimator classes; distinguishes algorithm-independent limits from algorithm-specific artifacts
  - Quick check question: Why does a minimax lower bound over I(δ) apply to ANY δ-interpolator, not just specific algorithms?

- Concept: Hölder smoothness classes H(β, L)
  - Why needed here: True function f* ∈ H(β,L); optimal rates depend on smoothness β and dimension d through exponent 2β/(2β+d)
  - Quick check question: If β increases (smoother functions), does n^{-2β/(2β+d)} converge faster or slower?

- Concept: Adversarial L₂-risk R_r(f̂, f*)
  - Why needed here: Supremum over perturbations in B_p(X,r) makes this fundamentally harder than standard L₂ risk—explains why interpolation acceptable for standard risk but catastrophic for robustness
  - Quick check question: Why does sup_{X'∈B_p(X,r)} make this objective harder than standard E[(f̂(X) - f*(X))²]?

## Architecture Onboarding

- Component map:
  Base estimator f̂ with localized sup-norm control -> Soft-threshold ψ_δ(t) at training points -> Local bridge ĝ in τ-neighborhoods -> Combined δ-interpolator f̂_{δ,τ}

- Critical path:
  1. Obtain base estimator f̂ satisfying Eq. 3.2
  2. At Xᵢ: apply f̂(Xᵢ) + ψ_δ(Yᵢ - f̂(Xᵢ))
  3. In τ-balls around Xᵢ: define ĝ per Eq. 3.4
  4. Outside balls: use f̂ unchanged
  5. Decompose adversarial risk via Eq. A.2.1

- Design tradeoffs:
  - τ = 0: exact minimax bound but discontinuous; τ ∝ n^{-β/((2β+d)(1∧β))}: minimax + continuous
  - Large δ (≳ √log n): optimal robustness, weak interpolation; Small δ: true interpolation, catastrophic robustness
  - Fundamental: cannot achieve both exact interpolation AND optimal robustness

- Failure signatures:
  - Adversarial risk growing with n → high-interpolation regime active
  - Non-convergent risk when r ≳ n^{-1/d} → δ too small
  - Training error ~ 0 but adversarial risk ≫ standard risk → interpolation damage confirmed

- First 3 experiments:
  1. Replicate Figure 3: Compare LP vs IP2 vs SI across r ∈ [0, 0.1], n ∈ {80, 150, 300} to observe curse of sample size
  2. Ablation on δ: Fix n=300, r=0.05, vary δ ∈ [0.1, 2.0] to locate phase transition near √log(300) ≈ 0.53
  3. Early stopping: Track training error, standard risk, adversarial risk across epochs (Auto MPG, Figure 4) to verify optimal robustness precedes interpolation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can rigorous theoretical bounds be established for the adversarial robustness of interpolating estimators when utilizing regularization techniques like early stopping, dropout, or adversarial training?
- Basis in paper: [explicit] The Discussion section notes that while numerical results suggest early stopping can improve robustness, "A theoretical understanding of the adversarial robustness properties of such regularization methods remains an important direction for future research."
- Why unresolved: The current minimax analysis focuses on estimators that fully or partially interpolate the training data, but does not formally analyze the dynamics of iterative algorithms like gradient descent that may be stopped early.
- What evidence would resolve it: Convergence rates or upper bounds on the adversarial risk for interpolating estimators trained with explicit regularization or implicit regularization via early stopping.

### Open Question 2
- Question: Do the minimax adversarial rates and the "curse of sample size" phenomenon persist if the input distribution $P_X$ violates Assumption 1 (e.g., unbounded support or highly uneven density)?
- Basis in paper: [inferred] The theoretical derivations rely on Assumption 1, which requires the input density to be bounded above and below on a compact support, a simplification that may not hold for general data distributions.
- Why unresolved: The proofs utilize the compactness and density bounds to control the volume of adversarial balls and the probability of points falling within them; without these, the integral terms in the risk bounds may change order.
- What evidence would resolve it: A proof of minimax rates for interpolating estimators under heavy-tailed or unbounded input distributions, or counter-examples showing the rates differ.

### Open Question 3
- Question: Does the fundamental damage to robustness caused by interpolation translate to classification settings using 0-1 loss or surrogate losses?
- Basis in paper: [inferred] The paper focuses exclusively on nonparametric regression using $L_2$ risk, but notes that interpolation and robustness have been studied separately for classification (citing Sanyal et al., 2021), leaving a gap for a unified minimax analysis in classification.
- Why unresolved: The adversarial risk definition and the construction of the optimal interpolator $\hat{f}_{\delta,\tau}$ rely on squared error properties which do not directly transfer to the discrete nature of classification error.
- What evidence would resolve it: Derivation of minimax lower bounds for interpolating classifiers that show a similar divergence from optimal non-interpolating rates.

## Limitations

- The theoretical analysis relies on Gaussian noise assumptions and specific smoothness conditions that may not generalize to heavy-tailed or heteroscedastic settings
- The "curse of sample size" phenomenon depends critically on the relationship between r and n^{1/d}, which may not manifest clearly in high-dimensional or structured data scenarios
- The experimental validation uses synthetic data and one real-world regression dataset, which may not represent the full complexity of modern deep learning applications

## Confidence

**High Confidence**: The mechanism by which interpolation amplifies noise sensitivity (Mechanism 1) is well-supported by the minimax bounds and theoretical derivation. The phase transition characterization (Mechanism 3) is mathematically rigorous and clearly demonstrated in the bounds.

**Medium Confidence**: The "curse of sample size" phenomenon (Mechanism 2) is theoretically established but relies on specific asymptotic conditions. The experimental evidence in synthetic settings supports this, but real-world applicability may vary with data structure and model architecture.

**Low Confidence**: The direct implications for deep neural networks assume that neural network behavior maps cleanly to the nonparametric regression framework. While the paper provides theoretical justification, the empirical connection between interpolation and robustness in modern DNNs remains more speculative.

## Next Checks

1. **Generalization to Heavy-Tailed Noise**: Replicate the minimax analysis under sub-exponential or heavy-tailed noise distributions (e.g., t-distribution with low degrees of freedom) to test the robustness of the interpolation damage mechanism beyond Gaussian assumptions.

2. **High-Dimensional Scaling**: Extend numerical experiments to d > 1 (e.g., d=5,10) to verify that the curse of sample size and phase transition phenomena persist in higher dimensions, particularly examining the r ≳ n^{-1/d} threshold behavior.

3. **Algorithm-Specific Mitigation**: Implement and test adaptive interpolation methods that adjust δ dynamically based on local density or uncertainty estimates, comparing their adversarial robustness to the fixed-δ interpolators studied here to identify potential practical workarounds.