---
ver: rpa2
title: Benchmarking Post-Training Quantization of Large Language Models under Microscaling
  Floating Point Formats
arxiv_id: '2601.09555'
source_url: https://arxiv.org/abs/2601.09555
tags:
- quantization
- flatquant
- smoothquant
- spinquant
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work systematically evaluates post-training quantization
  (PTQ) under Microscaling Floating-Point (MXFP) formats for large language models
  (LLMs), covering over 7 PTQ algorithms, 15 benchmarks, and 3 LLM families. The key
  findings are: (1) MXFP8 consistently achieves near-lossless performance across tasks
  and modalities, while MXFP4 introduces substantial accuracy degradation; (2) PTQ
  effectiveness under MXFP depends on format compatibility, with error compensation
  and affine transformation methods being more effective, especially at low bit-widths;
  (3) PTQ performance shows highly consistent trends across model families and modalities,
  with quantization sensitivity dominated by the LLM component in multimodal models
  rather than the vision encoder; (4) The scaling factor in MXFP4 is a critical error
  source, and a simple pre-scale optimization strategy can significantly mitigate
  its impact.'
---

# Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats

## Quick Facts
- arXiv ID: 2601.09555
- Source URL: https://arxiv.org/abs/2601.09555
- Reference count: 27
- Primary result: MXFP8 enables near-lossless LLM quantization; MXFP4 remains challenging without format-aware PTQ design.

## Executive Summary
This work systematically evaluates post-training quantization (PTQ) under Microscaling Floating-Point (MXFP) formats for large language models (LLMs), covering over 7 PTQ algorithms, 15 benchmarks, and 3 LLM families. The key findings are: (1) MXFP8 consistently achieves near-lossless performance across tasks and modalities, while MXFP4 introduces substantial accuracy degradation; (2) PTQ effectiveness under MXFP depends on format compatibility, with error compensation and affine transformation methods being more effective, especially at low bit-widths; (3) PTQ performance shows highly consistent trends across model families and modalities, with quantization sensitivity dominated by the LLM component in multimodal models rather than the vision encoder; (4) The scaling factor in MXFP4 is a critical error source, and a simple pre-scale optimization strategy can significantly mitigate its impact. Overall, MXFP8 enables stable, near-lossless deployment, while bridging the performance gap for 4-bit quantization remains an open challenge requiring format-aware design.

## Method Summary
The paper benchmarks PTQ algorithms under MXFP8 (E4M3) and MXFP4 (E2M1) formats using block size 32 with shared E8M0 scaling factors. Seven PTQ methods (RTN, SmoothQuant, AWQ, GPTQ, MR-GPTQ, QuaRot, SpinQuant, FlatQuant) are evaluated on Llama-3.1-8B-Instruct, openPangu-Embedded-7B-V1.1, Qwen2.5-VL-7B, and openPangu-VL-7B across 15 benchmarks. The study covers weight-only, weight-activation, and KV cache quantization settings. Results are measured using accuracy recovery rate relative to BF16, perplexity, and downstream task accuracy. The microxcaling library is used for consistent MXFP simulation.

## Key Results
- MXFP8 consistently achieves near-lossless performance across tasks and modalities
- MXFP4 introduces substantial accuracy degradation requiring format-aware PTQ design
- Rotational transformation methods (QuaRot, SpinQuant) impair MXFP4 performance while error compensation methods excel
- Pre-scale optimization (3/4 factor) significantly mitigates MXFP4 scaling factor errors

## Why This Works (Mechanism)

### Mechanism 1
MXFP8 achieves near-lossless performance because the E4M3 mantissa width (3 bits) provides sufficient precision for fine-grained quantization, preserving dynamic range. MXFP4's severely limited mantissa (1 bit) constrains representational capacity, causing quantization noise to exceed critical thresholds. This relationship between mantissa width and quantization error scales predictably with task complexity.

### Mechanism 2
Rotational transformation methods that excel for INT4 quantization impair MXFP4 performance because MXFP4 relies on local statistical properties within each 32-element block for effective group-wise scaling. Global orthogonal rotations mix information across all dimensions, flattening outlier structures and reducing kurtosis, making distributions less amenable to block-level scaling.

### Mechanism 3
The E8M0 scaling factor format is a critical error source in MXFP4 because it forces scaling factors to be powers of two, creating coarse quantization. This causes mismatch between optimal scale and allowed scale, affecting all values in the block. Pre-scaling inputs by 3/4 before quantization prevents systematic clipping bias from FP4's limited dynamic range.

## Foundational Learning

- Concept: Block floating-point quantization with shared scaling factors
  - Why needed here: MXFP's defining characteristic is per-block E8M0 scaling; understanding this is prerequisite to diagnosing scaling factor errors and choosing compatible PTQ methods.
  - Quick check question: Given a 32-element block with values ranging from 0.001 to 1000, how does E8M0 (powers of two only) constrain the representable scale versus a floating-point scale?

- Concept: Error compensation vs. channel-wise transformation in PTQ
  - Why needed here: The paper categorizes PTQ methods into four classes with different compatibility profiles; knowing which paradigm matches MXFP's structure determines algorithm selection.
  - Quick check question: Why might GPTQ's inverse-Hessian-based weight updates be more compatible with block-wise MXFP than SmoothQuant's per-channel scaling?

- Concept: Exponent-mantissa trade-offs in low-bit floating-point
  - Why needed here: MXFP8 uses E4M3 vs. E5M2 variants; the paper selects E4M3 because mantissa width matters more for fine-grained quantization. Understanding E/M allocation is essential for format selection.
  - Quick check question: For a weight distribution with many small values and few outliers, would you prioritize exponent bits or mantissa bits, and why?

## Architecture Onboarding

- Component map: Input → [Pre-scale (optional)] → [Block-wise E8M0 scale computation] → [E4M3/E2M1 quantization] → [Dequantize for compute] → Output
- Critical path: 1) Select format (MXFP8 vs MXFP4) based on accuracy budget: W8A8 is generally safe; W4A4 requires careful PTQ selection. 2) Choose PTQ algorithm: Error compensation (GPTQ, MR-GPTQ) or affine transformation (FlatQuant) for MXFP; avoid rotational methods for MXFP4. 3) Enable pre-scale for MXFP4 weight quantization to mitigate clipping bias. 4) For multimodal models: prioritize LLM precision (W8A8) over ViT (can use W4A4).
- Design tradeoffs: W8A8 vs W4A8: ~2% accuracy recovery gap; W4A8 may be acceptable for non-reasoning tasks with good PTQ. FlatQuant vs GPTQ: FlatQuant more robust at W4A4 (96.57% vs 95.50% recovery on Llama-3.1); GPTQ simpler to implement. Mixed precision for MLLMs: ViT at W4A4 + LLM at W8A8 achieves near-lossless with memory savings; requires architecture-aware deployment.
- Failure signatures: Perplexity spike (>2x BF16): Check scaling factor precision or enable pre-scale. Rotation-based PTQ underperforms RTN: Format incompatibility; switch to error compensation or affine methods. Reasoning tasks degrade more than classification: Expected behavior; W4A4 is risky for reasoning even with optimal PTQ. KV cache quantization causes instability: Method-dependent; GPTQ and SmoothQuant show 1-2% recovery drop with KV8.
- First 3 experiments: 1) Establish baseline: Run RTN quantization at W8A8, W4A8, W4A4 on your target model with WikiText2 perplexity + 2-3 downstream tasks. Compare recovery rates to paper's thresholds (lossless ≤1%, fair 1-3%, risky ≥3%). 2) PTQ algorithm comparison: Test GPTQ, FlatQuant, and one rotational method (QuaRot) at W4A4. Expect FlatQuant/GPTQ > RTN > rotational methods. Use `microxcaling` library for consistent MXFP simulation. 3) Scaling factor ablation: For W4A4, test with/without pre-scale (3/4 factor). Measure perplexity delta; expect ~2x reduction with pre-scale enabled.

## Open Questions the Paper Calls Out

- Question: Do the observed MXFP quantization trends generalize to models significantly larger than 8B parameters or to alternative microscaling formats like NVIDIA's NVFP?
- Question: What specific algorithmic designs are required to bridge the performance gap for 4-bit weight or activation (W4A4/W4A8) quantization under MXFP formats?
- Question: Can rotational transformation methods be adapted to preserve local group-wise statistical properties required by block-level floating-point formats?

## Limitations

- Calibration dataset size and composition not specified, which can significantly impact PTQ performance
- Hardware-specific factors such as memory bandwidth and kernel optimization not explored
- Pre-scale optimization uses a fixed 3/4 factor that may not be optimal for all architectures
- Limited model families studied for multimodal architecture recommendations

## Confidence

- High Confidence: MXFP8 achieving near-lossless performance and MXFP4 showing substantial degradation
- Medium Confidence: Incompatibility of rotational methods with MXFP4 and effectiveness of error compensation approaches
- Low Confidence: Exact threshold where MXFP4 becomes acceptable for production use and multimodal architecture recommendations

## Next Checks

1. **Calibration Sensitivity Analysis**: Systematically vary calibration dataset size (from 64 to 4096 samples) and composition to quantify their impact on PTQ performance across all algorithms, particularly for MXFP4 where quantization noise is most critical.

2. **Cross-Architecture Scaling Factor Study**: Test the pre-scale optimization (3/4 factor) across a broader range of architectures including Mistral, Gemma, and larger parameter counts. Additionally, explore learning-based scaling factor optimization that adapts to local weight distributions within each 32-element block.

3. **Mixed-Precision Deployment Benchmark**: Evaluate practical mixed-precision configurations where ViT components use W4A4, LLM components use W8A8, and KV caches use W8A8. Measure not only accuracy but also memory bandwidth utilization and inference latency on representative hardware (A100/H100 GPUs) to establish real-world deployment viability.