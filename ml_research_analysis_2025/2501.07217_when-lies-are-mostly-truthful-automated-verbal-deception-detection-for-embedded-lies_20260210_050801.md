---
ver: rpa2
title: 'When lies are mostly truthful: automated verbal deception detection for embedded
  lies'
arxiv_id: '2501.07217'
source_url: https://arxiv.org/abs/2501.07217
tags:
- lies
- embedded
- deception
- deceptive
- truthful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores the detection of embedded lies\u2014statements\
  \ that mix truthful and deceptive information\u2014using a novel dataset of 2,088\
  \ statements. Participants wrote truthful and deceptive accounts of autobiographical\
  \ events, with embedded lies annotated and rated for deceptiveness, centrality,\
  \ and source."
---

# When lies are mostly truthful: automated verbal deception detection for embedded lies

## Quick Facts
- arXiv ID: 2501.07217
- Source URL: https://arxiv.org/abs/2501.07217
- Reference count: 40
- Primary result: Fine-tuned language models achieved 64% accuracy in detecting truthful statements versus those with embedded lies

## Executive Summary
This study addresses the challenge of detecting embedded lies—statements mixing truthful and deceptive information—using a novel dataset of 2,088 autobiographical statements. Participants created both truthful and deceptive accounts with embedded lies, which were annotated and rated for deceptiveness, centrality, and source. Machine learning models, including fine-tuned language models, achieved 64% accuracy in classifying truthful statements and those containing embedded lies. The research reveals that embedded lies are typically 1/3 of deceptive statements, largely derived from personal experiences, and show minimal linguistic differences from truthful counterparts, making them particularly challenging to detect computationally.

## Method Summary
The study collected 2,088 statements from 1,042 participants using a within-subjects design where each participant wrote both a truthful and a deceptive statement about autobiographical events. Embedded lies were annotated with deceptiveness, centrality, and source ratings. Machine learning models were trained using features from LIWC-22 (118 psycholinguistic features), DeCLaRatiVE stylometry (26 features), GPT-embeddings (256-dim), and n-gram frequency profiles. Classification models included Random Forest with various feature sets and fine-tuned language models (distilBERT, FLAN-T5, Llama-3-8B). The Llama-3-8B model with QLoRA fine-tuning achieved the best performance at 64% accuracy through 5-fold cross-validation with participant-level grouping to prevent leakage.

## Key Results
- Fine-tuned Llama-3-8B achieved 64% accuracy in detecting embedded lies versus truthful statements
- Embedded lies comprised approximately 1/3 of deceptive statements and were primarily derived from personal experiences
- Deceptive statements showed minimal linguistic differences from truthful counterparts, with only small effect sizes for LIWC features
- The pre-trained deception model (78.8% accuracy on fabricated statements) dropped to 56% accuracy on embedded lies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Detection accuracy increases with the proportion of embedded lies in a statement.
- Mechanism: Higher fabrication ratios introduce more detectable linguistic divergences from baseline truthful patterns, allowing classifiers to discriminate with greater confidence.
- Core assumption: The relationship is monotonic—more embedded content yields stronger detectability signals.
- Evidence anchors:
  - [abstract] "Typical deceptive statements consisted of 2/3 truthful information and 1/3 embedded lies... with minimal linguistic differences with their truthful counterparts."
  - [section] "correct classifications having a significantly higher amount of absolute number of embedded lies (M = 5.31) compared to incorrect ones (M = 4.43), d = 0.27" (Results, Exploratory explainability analysis)
  - [corpus] Limited direct corpus evidence on threshold effects; neighboring papers focus on adversarial faking rather than fabrication ratios.
- Break condition: If embedded lies become indistinguishable from truth regardless of proportion (e.g., expert liars using sophisticated blending), the threshold mechanism may fail.

### Mechanism 2
- Claim: Embedded lies derived from personal experience produce linguistic patterns near-identical to truthful statements.
- Mechanism: Liars retrieve and adapt episodic memories, preserving sensory, contextual, and linguistic properties of genuine recollection; this reduces signal in Reality Monitoring and Verifiability features.
- Core assumption: Assumption: Experience-based lies maintain the same concreteness, perceptual detail, and narrative coherence as authentic memories.
- Evidence anchors:
  - [abstract] "Embedded lies were typically 1/3 of deceptive statements, largely derived from personal experiences"
  - [section] "35.86% of embedded lies relied on personal past experiences that involved participants directly and 10.41% indirectly" (Results, Embedded lies)
  - [corpus] Weak corpus linkage; no neighboring papers directly address experience-sourcing in deception.
- Break condition: If liars fabricate from imagination or third-party sources, linguistic distinguishability may increase—breaking the experience-based embedding mechanism.

### Mechanism 3
- Claim: Deceptive statements with embedded lies exhibit elevated cognitive-process markers but reduced social-referencing.
- Mechanism: Lying requires additional cognitive operations (differentiation, memory management), increasing cognition-related word frequencies; simultaneously, deceivers distance from social references to avoid accountability.
- Core assumption: Cognitive load and distancing are consistent signatures even when lies are embedded rather than wholesale fabrications.
- Evidence anchors:
  - [abstract] "minimal linguistic differences from truthful counterparts"
  - [section] "Truthful statements contained a larger proportion of social references, while deceptive statements tended to include more references to cognitive processes, such as memory-related words" (Discussion)
  - [corpus] Neighboring work on adversarial attacks (2501.05962) suggests linguistic signatures can be deliberately masked, indicating mechanism fragility under adversarial conditions.
- Break condition: If liars consciously suppress cognitive markers or intentionally increase social references, the signal attenuates.

## Foundational Learning

- **Within-subjects experimental design**
  - Why needed here: The paper uses within-subjects design to control for individual differences and topic effects, enabling direct comparison of truthful vs. deceptive statements from the same participant. Understanding this is critical for interpreting why linguistic differences are subtle—between-subject variance is eliminated.
  - Quick check question: Why would a within-subjects design reduce the number of significant n-gram differences compared to a between-subjects design?

- **LIWC (Linguistic Inquiry and Word Count)**
  - Why needed here: LIWC provides 118+ psycholinguistic features used to characterize truthful vs. deceptive statements. Key findings (social words, cognitive process words) rely on LIWC categories.
  - Quick check question: If LIWC "memory" words increase in deceptive statements, does that mean liars are accessing more memories—or managing memory-related content differently?

- **Fine-tuning vs. frozen pre-trained models**
  - Why needed here: The paper shows that a deception model trained on fabricated statements (79.31% accuracy) drops to 56% on embedded lies, while fine-tuned Llama-3-8B achieves 64%. Understanding domain adaptation is essential for interpreting why frozen models fail and fine-tuning partially recovers performance.
  - Quick check question: Why would a model trained on wholesale fabrications struggle specifically with embedded lies (35% recall drop) rather than showing uniform performance degradation?

## Architecture Onboarding

- **Component map:**
  1. Data collection: 1,042 participants × 2 statements (truthful + deceptive with embedded lies) across 11 event types; embedded-lie annotations with deceptiveness/centrality/source ratings
  2. Feature extraction: LIWC-22 (118 features), DeCLaRatiVE stylometry (26 features), GPT-embeddings (256-dim), n-gram frequency profiles
  3. Classification models: Random Forest (BOW, LIWC, DeCLaRatiVE, embeddings); fine-tuned language models (distilBERT, FLAN-T5, Llama-3-8B)
  4. Explainability: correlation between class probabilities and embedded-lie metrics; correct vs. incorrect classification comparison

- **Critical path:**
  1. Ensure within-subjects data integrity (same participant in both train and test folds risks leakage; paper uses 5-fold CV keeping participant pairs together)
  2. Feature extraction must handle variable-length statements (300-char minimum)
  3. Fine-tuning requires QLoRA for Llama-3-8B given 8B parameters; smaller models (distilBERT, FLAN-T5) use standard fine-tuning

- **Design tradeoffs:**
  - Larger models (Llama-3-8B) achieve higher accuracy (64%) but require GPU resources and careful cross-validation to prevent overfitting
  - LIWC/DeCLaRatiVE features are interpretable but limited to pre-defined categories; embeddings capture latent structure but reduce explainability
  - Within-subjects design increases ecological validity for embedded lies but constrains dataset diversity (same topics, same writers)

- **Failure signatures:**
  - Model recalls truthful statements well (76%) but fails on deceptive (35% recall for frozen deception model): indicates overfitting to fabrication patterns not present in embedded lies
  - Minimal n-gram differentiation across events: signals that lexical features alone are insufficient
  - Small effect sizes (d < 0.30) for LIWC features: suggests single features are weak detectors; ensemble or contextual signals required

- **First 3 experiments:**
  1. **Baseline replication**: Train Random Forest on LIWC features using 10-fold nested CV; verify ~58% accuracy matches paper; analyze per-event performance to identify high/low-discrimination contexts
  2. **Fabrication vs. embedded-lie comparison**: Test the pre-trained deception language model on a held-out subset of fabricated statements (if available) vs. embedded-lie statements; quantify recall differential to confirm 78% → 35% drop
  3. **Proportion-sensitivity test**: Bin statements by standardized embedded-lie ratio (e.g., <0.2, 0.2–0.4, >0.4); measure Llama-3-8B accuracy per bin to validate the claim that higher fabrication ratios improve detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can computational models accurately predict the specific location of embedded lies within a narrative (sequence classification) rather than just the veracity of the entire statement?
- Basis in paper: [explicit] The authors suggest the dataset allows for a "sequence classification task, allowing for the prediction of how and where lies are embedded within truthful narratives."
- Why unresolved: Current models classify whole documents (64% accuracy) but struggle because embedded lies linguistically resemble truthful segments.
- What evidence would resolve it: A model that identifies specific text spans as deceptive with high precision and recall, exceeding the performance of document-level classifiers.

### Open Question 2
- Question: Do traditional verbal deception frameworks (e.g., Verifiability Approach, Reality Monitoring) retain discriminative power when applied specifically to embedded lies?
- Basis in paper: [explicit] The paper asks whether these frameworks "work well when applied to embedded lies" despite the current results showing minimal linguistic differences.
- Why unresolved: The DeCLaRatiVE analysis showed few significant differences for variables like verifiable details or perceptual richness between truthful statements and those with embedded lies.
- What evidence would resolve it: Validation studies showing that theoretical cues (e.g., lack of verifiable details) reliably differentiate embedded lies from truthful text within the same statement.

### Open Question 3
- Question: Does a three-condition design (completely truthful, embedded lies, and fully deceptive) reveal distinct linguistic markers obscured by binary comparisons?
- Basis in paper: [explicit] The authors propose a future avenue involving "three versions of the statement: truthful, embedded lies, and fully deceptive."
- Why unresolved: The current study contrasted "fully truthful" vs. "embedded lies," potentially missing distinctions between "embedded" and "fully fabricated" statements.
- What evidence would resolve it: A comparative analysis of linguistic features and detection accuracy across all three distinct conditions to identify unique markers for embedded lies.

## Limitations
- Dataset lacks external validation and may not generalize to real-world deception scenarios where embedded lies are less structured
- Focus on autobiographical events may artificially constrain linguistic variation and limit ecological validity
- The experience-based embedding mechanism remains largely theoretical with limited direct empirical support

## Confidence
- **Medium**: The study's key findings rest on a novel dataset that, while internally consistent, lacks external validation
- **Medium**: The claim that embedded lies show "minimal linguistic differences" from truthful statements appears supported but could reflect both genuine similarity and limitations in the feature set
- **Low**: The mechanism explaining why experience-based embedded lies are harder to detect remains largely theoretical with limited direct empirical support

## Next Checks
1. **External Validation**: Test the trained model on an independent dataset of embedded lies from different domains (e.g., workplace communications, legal testimony) to assess generalizability beyond autobiographical events
2. **Threshold Analysis**: Systematically vary the proportion of embedded lies in statements (e.g., 10%, 30%, 50%) and measure detection accuracy to empirically validate the hypothesized relationship between fabrication ratio and detectability
3. **Feature Ablation Study**: Remove LIWC cognitive and social categories sequentially to determine which features contribute most to the modest performance gains, and whether alternative feature sets (e.g., syntactic complexity, discourse markers) might better capture embedded deception patterns