---
ver: rpa2
title: 'Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended
  LLM Reasoning'
arxiv_id: '2602.01791'
source_url: https://arxiv.org/abs/2602.01791
tags:
- rewards
- judge
- policy
- reasoning
- open-ended
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRAD2REWARD introduces a dense-reward framework for improving LLM
  reasoning on open-ended tasks by extracting token-level supervision directly from
  the Judge's internal gradient signals via a single backward pass. Unlike prior approaches
  that treat the Judge as a black box and use only sparse sequence-level rewards,
  GRAD2REWARD quantifies each token's contribution to the Judge's decision, enabling
  fine-grained credit assignment.
---

# Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning

## Quick Facts
- arXiv ID: 2602.01791
- Source URL: https://arxiv.org/abs/2602.01791
- Reference count: 40
- Primary result: Achieves up to 3.8-point gains on medical consultation tasks and 5.0-point gains on academic QA while converging 1.7×–1.9× faster than sparse-reward baselines

## Executive Summary
GRAD2REWARD introduces a dense-reward framework that extracts token-level supervision directly from a Judge's internal gradient signals via a single backward pass. Unlike prior approaches treating the Judge as a black box, it quantifies each token's contribution to the Judge's decision, enabling fine-grained credit assignment. The method also introduces a self-judging mechanism where the Judge is fixed to the initial policy, allowing the model to improve through its own evaluative feedback without relying on stronger external models. Experimental results show consistent outperformance of sparse-reward baselines across medical consultation, academic QA, and mathematical reasoning tasks.

## Method Summary
GRAD2REWARD converts sparse judge rewards to dense token-level rewards by computing the inner product between token embeddings and their gradients from the Judge's backward pass. The method uses a frozen copy of the initial policy as Judge (self-judging), which evaluates generated responses and provides gradient signals for improvement. Token rewards are derived from softmax-normalized attribution scores and used in token-level GRPO updates. The approach is evaluated on medical consultation (HealthBench, RaR-Medicine), academic QA (ResearchQA, RaR-Science), and mathematical reasoning tasks, demonstrating both superior performance and faster convergence compared to strong baselines.

## Key Results
- Achieves 3.8-point gains on medical consultation tasks and 5.0-point gains on academic QA compared to sparse-reward baselines
- Converges 1.7×–1.9× faster while reaching higher asymptotic performance
- Self-judging with Qwen2.5-1.5B achieves near-competitive performance (44.5 vs 45.9) compared to using Qwen3-30B judge
- Extends to mathematical reasoning, matching or exceeding state-of-the-art process reward models

## Why This Works (Mechanism)

### Mechanism 1: Gradient × Embedding Attribution for Dense Rewards
The method computes the inner product between token embeddings and their gradients (`b_t = g_t^T e_t`) to extract token-level contributions to the Judge's decision. During the Judge's forward pass, it processes the entire policy output before emitting a binary verdict. The gradient of the log-probability of that verdict, backpropagated to each token embedding, encodes how much that token influenced the decision. This Gradient × Embedding formulation preserves directional information (unlike L1/L2 norms), which is critical for performance. The approach assumes first-order Taylor expansion is a valid local approximation of the Judge's decision function around the embedding space.

### Mechanism 2: Self-Judging via Discriminative Superiority
A policy can improve using its own frozen copy as Judge without requiring a stronger external model. LLMs exhibit stronger discriminative capabilities than generative ones, so by freezing the initial policy as Judge, the policy optimizes against a stable reference while leveraging its own evaluative capacity. This avoids distillation from external models and ensures improvement arises from the policy's intrinsic potential. The method assumes the initial policy's discriminative ability is sufficient to provide meaningful gradient signals for improvement.

### Mechanism 3: Token-Level Advantage for Accelerated Convergence
Decomposing sequence-level rewards into token-level rewards via softmax-weighted attribution accelerates convergence and improves asymptotic performance. Sparse rewards assign equal credit to all tokens, causing the credit assignment bottleneck. Dense rewards provide per-token gradients, enabling more precise policy updates. The softmax normalization (`α_t = exp(b_t/τ) / Σ exp(b_k/τ)`) ensures relative contribution scaling while maintaining reward sum consistency. The approach assumes attribution scores are sufficiently correlated with true token quality that densification helps rather than introduces noise.

## Foundational Learning

- **Concept: Policy Gradient Methods (GRPO)**
  - Why needed here: GRAD2REWARD extends GRPO from sequence-level to token-level advantage estimation. Understanding how GRPO groups responses and computes advantages is essential.
  - Quick check question: Can you explain how GRPO computes the advantage `Â_i,t` differently from standard PPO, and why grouping multiple responses per query matters?

- **Concept: Gradient Attribution Methods**
  - Why needed here: The core innovation is using gradients for credit assignment. Understanding why `∇_e f(e)^T e` approximates feature importance is foundational.
  - Quick check question: Why does the paper claim Gradient × Embedding preserves directional information while L1/L2 norms do not?

- **Concept: LLM-as-a-Judge Paradigm**
  - Why needed here: The Judge provides the supervisory signal. Understanding how rubrics convert to binary verdicts is needed to interpret the reward structure.
  - Quick check question: How does the Judge's binary decision token `z ∈ {True, False}` get converted to a scalar reward in Eq. (1)?

## Architecture Onboarding

- **Component map:**
  Policy π_θ (trainable) -> generates -> Response o = (a_1, ..., a_T)
                                              |
  Judge p_judge (frozen copy of initial π) <-|
         |                                    |
         v                                    v
  Forward pass: Judge evaluates (x, o, c) -> Verdict z
         |
         v
  Backward pass: ∇_e_t log p_judge(z|x, o, c) -> g_t
         |
         v
  Attribution: b_t = g_t^T e_t -> softmax -> α_t
         |
         v
  Token reward: r_t = α_t × r(x, o)
         |
         v
  Token-level GRPO: Update θ via Eq. (10)

- **Critical path:**
  1. Freeze initial policy as Judge immediately after initialization
  2. For each batch: sample query -> generate G responses -> for each response, run Judge forward+backward -> extract gradients -> compute attribution -> normalize -> aggregate across rubrics -> compute token-level advantages -> policy update
  3. Temperature τ controls softmax sharpness; Appendix A.1 uses τ=1.0 implicitly

- **Design tradeoffs:**
  - Self-judging vs. External Judge: Self-judging avoids dependency on stronger models but may provide weaker signals if initial policy is too weak. Table 3 shows marginal but consistent gaps.
  - Gradient × Embedding vs. Norm-based attribution: Gradient × Embedding is theoretically motivated (Taylor expansion) and empirically superior (Table 2), but requires storing embeddings—higher memory than pure gradient norms.
  - Token-level vs. Sequence-level GRPO: Token-level enables fine-grained credit assignment but introduces variance; group normalization in Eq. (9) mitigates this.

- **Failure signatures:**
  - Flat attribution scores: If all `b_t` are near zero, Judge gradients are not informative—check if Judge is actually discriminating (verify Judge accuracy on held-out examples).
  - Rewards collapse to single token: If softmax is too sharp (τ too low), all credit concentrates on one token—increase τ.
  - No improvement over baseline: Verify Judge is frozen; if updated, gradient signals shift and become unstable.

- **First 3 experiments:**
  1. Sanity check: Run GRAD2REWARD with L2 norm attribution (Eq. 12) on a small subset of HealthBench; compare to Gradient × Embedding. Confirm the paper's ~6-point gap on HealthBench (38.0 vs 44.5).
  2. Self-judging ablation: Train with Qwen2.5-7B as external Judge vs. self-judging on Qwen2.5-1.5B policy. Replicate Table 3 gap (~1 point) to validate discriminative superiority assumption.
  3. Convergence speed test: Plot validation score vs. training steps for Vanilla-GRPO vs. GRAD2REWARD. Confirm 1.7-1.9× speedup on RaR-Medicine as in Figure 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can Grad2Reward be effectively extended to long-horizon agent tasks involving multi-step decisions and state changes?
- Basis in paper: [explicit] The Conclusion states: "Looking forward, our method can be extended to long-horizon agent tasks... where gradient-based rewards have the potential to provide high-quality process supervision."
- Why unresolved: The current study focuses on reasoning tasks with fixed queries (medical QA, math) rather than interactive environments where actions alter the state.
- What evidence would resolve it: Successful application of Grad2Reward to interactive RL benchmarks (e.g., web navigation or robotics simulation) demonstrating stable policy optimization over multi-turn trajectories.

### Open Question 2
- Question: Does the self-judging mechanism impose an asymptotic performance ceiling as the policy model surpasses the discriminative ability of the frozen initial judge?
- Basis in paper: [inferred] The method freezes the judge at the initial policy state to allow self-improvement. While RQ1 shows initial competitiveness, it does not address if the frozen judge eventually fails to provide informative gradients for a significantly improved policy.
- Why unresolved: It remains unclear if the assumption that "discriminative capabilities exceed generative capabilities" holds throughout the entire training process.
- What evidence would resolve it: Experiments comparing the convergence limits of self-judging against training with a continuously updated or oracle judge on complex tasks.

### Open Question 3
- Question: How robust is the gradient-based attribution signal for extremely long sequences where gradients may suffer from vanishing magnitude or noise?
- Basis in paper: [inferred] The method relies on a single backward pass to calculate token contributions (Eq. 2). While effective for the tested response lengths (up to 4096 tokens), the stability of this signal for book-length context or highly deep reasoning chains is not analyzed.
- Why unresolved: Standard gradient-based attribution often suffers from saturation or noise in deep networks, which could degrade the quality of the dense reward signal.
- What evidence would resolve it: An analysis correlating attribution accuracy with sequence length, specifically measuring the signal-to-noise ratio of gradients for tokens early in the sequence versus those near the verdict.

## Limitations

- Temperature hyperparameter (τ) is not specified in the main text or appendix, critically affecting attribution distribution and performance.
- Method's effectiveness depends heavily on the initial policy's discriminative capability, with marginal gaps (1-2 points) to stronger external judges.
- Doesn't test robustness to attribution noise or explore failure modes when first-order gradients are insufficient for highly non-linear judge decision functions.
- Assumes rubric items are available for each query without addressing how these are generated or whether they're consistent across different judge models.

## Confidence

**High confidence** in: The core gradient attribution mechanism (b_t = g_t^T e_t) is theoretically sound under first-order Taylor approximation assumptions. The empirical convergence speedup (1.7×–1.9×) and performance improvements on medical and academic tasks are well-supported by experimental results across multiple datasets.

**Medium confidence** in: The self-judging mechanism's effectiveness across different model sizes and initial quality levels. While the paper shows promising results with 1.5B+ parameter models, the marginal gaps to stronger judges suggest limitations not fully characterized.

**Low confidence** in: Generalization to domains with very weak initial policies or highly non-linear judge decision functions. The paper doesn't test higher-order attribution methods or explore failure modes when first-order gradients are insufficient.

## Next Checks

1. **Temperature sensitivity analysis**: Run GRAD2REWARD across τ ∈ [0.1, 0.5, 1.0, 2.0] on a subset of HealthBench to quantify performance sensitivity and identify optimal range. This addresses the unspecified hyperparameter and tests attribution distribution stability.

2. **Judge strength scaling study**: Compare GRAD2REWARD performance using judges of varying strength (e.g., 0.5B, 1.5B, 7B, 30B parameters) on the same policy to map the relationship between judge quality and self-judging effectiveness. This validates the discriminative superiority assumption across scales.

3. **Attribution noise robustness test**: Generate multiple judge copies with different random seeds and measure variance in attribution scores b_t across copies. Compare final policy performance when using different judge instances to assess method robustness to attribution noise.