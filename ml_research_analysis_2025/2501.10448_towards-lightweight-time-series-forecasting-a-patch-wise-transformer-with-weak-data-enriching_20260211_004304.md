---
ver: rpa2
title: 'Towards Lightweight Time Series Forecasting: a Patch-wise Transformer with
  Weak Data Enriching'
arxiv_id: '2501.10448'
source_url: https://arxiv.org/abs/2501.10448
tags:
- time
- series
- data
- future
- lipformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of efficient time series forecasting
  by proposing LiPFormer, a lightweight patch-wise transformer with weak data enriching.
  The authors identify two key limitations of existing approaches: the reliance on
  massive, complex models that hinder deployment on edge devices, and the autoregressive
  nature that ignores valuable context information.'
---

# Towards Lightweight Time Series Forecasting: a Patch-wise Transformer with Weak Data Enriching

## Quick Facts
- **arXiv ID:** 2501.10448
- **Source URL:** https://arxiv.org/abs/2501.10448
- **Reference count:** 40
- **Primary result:** Proposes LiPFormer, a lightweight patch-wise transformer that outperforms state-of-the-art methods in accuracy while significantly reducing parameter scale, training duration, and GPU memory usage.

## Executive Summary
This paper addresses the challenge of efficient time series forecasting by proposing LiPFormer, a lightweight patch-wise transformer with weak data enriching. The authors identify two key limitations of existing approaches: the reliance on massive, complex models that hinder deployment on edge devices, and the autoregressive nature that ignores valuable context information. To address these issues, LiPFormer employs a novel lightweight cross-patch attention mechanism and a linear transformation-based attention to eliminate Layer Normalization and Feed Forward Networks, significantly reducing model complexity. Additionally, a weak data enriching module leverages easily accessible context information, such as weather forecasts and date/time, to provide valuable weak supervision without requiring expensive human labeling. Extensive experiments on nine benchmark datasets demonstrate that LiPFormer outperforms state-of-the-art methods in accuracy while significantly reducing parameter scale, training duration, and GPU memory usage. Deployment on an edge device reveals that LiPFormer achieves a 3-fold reduction in inference time compared to classic transformers. The weak data enriching module also seamlessly integrates into various transformer-based models, enhancing their accuracy.

## Method Summary
LiPFormer divides input time series into fixed-length patches (pl=48) to reduce complexity, then applies Cross-Patch and Inter-Patch attention mechanisms. Cross-Patch attention extracts global trends by sampling fixed positions across all patches, while Inter-Patch attention handles local patch correlations. The model eliminates Layer Normalization and Feed Forward Networks, replacing them with single-layer MLPs. A weak data enriching module uses contrastive learning to align future covariates (weather, temporal features) with target sequences via dual encoders. The training procedure involves two stages: (1) pre-training the covariate and target encoders with contrastive loss, and (2) training the base predictor with the frozen covariate encoder using Smooth L1 loss.

## Key Results
- Outperforms state-of-the-art methods in accuracy on nine benchmark datasets
- Achieves 3-fold reduction in inference time compared to classic transformers on edge devices
- Significantly reduces parameter scale, training duration, and GPU memory usage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing standard sequential attention with a dual-patch attention mechanism (Cross-Patch and Inter-Patch) may capture global trend dependencies that fixed-patch methods miss, without increasing computational overhead.
- **Mechanism:** The model constructs "trend sequences" by sampling the same relative position across all patches (e.g., the 1st data point of every patch). It applies attention across these sequences to capture global continuity, while standard Inter-Patch attention handles local patch correlations.
- **Core assumption:** Time series global trends can be effectively represented by sparse, fixed-position sampling across chronological patches.
- **Evidence anchors:**
  - [abstract] "LiPFormer employs a novel lightweight cross-patch attention... to eliminate Layer Normalization..."
  - [Section III-C] "By extracting fixed-position data points in each patch to construct trend sequences... our patch-wise attention effectively substitutes Positional Encoding."
- **Break condition:** If the time series exhibits high-frequency noise at specific intervals that aligns poorly with the fixed sampling stride, the "trend sequence" might capture noise rather than signal.

### Mechanism 2
- **Claim:** Eliminating Layer Normalization (LN) and Feed Forward Networks (FFNs) reduces model size and latency significantly with negligible accuracy loss, provided the attention mechanism is sufficiently expressive.
- **Mechanism:** The authors argue that LN is less necessary for fixed-length numerical patches (unlike variable-length NLP tokens) and that the non-linear mappings in FFNs are redundant for numerical time series. They replace FFNs with single-layer MLPs.
- **Core assumption:** The semantic complexity of numerical time series is lower than natural language, reducing the need for deep non-linear projection layers (FFNs).
- **Evidence anchors:**
  - [abstract] "...linear transformation-based attention to eliminate Layer Normalization and Feed Forward Network, two heavy components..."
  - [Section IV-E4] "The use of either FFNs or LN caused a decrease in the performance... removing these components tends to contribute to the accuracy improvement."
- **Break condition:** If the forecasting task requires learning complex, non-monotonic transformations between the attention output and the final prediction (common in chaotic systems), the simplified linear MLP projection may underfit compared to a full FFN.

### Mechanism 3
- **Claim:** Pre-training a dual encoder to align future covariates (e.g., weather forecasts) with future target series via contrastive learning improves forecasting accuracy by injecting "weak supervision."
- **Mechanism:** A Covariate Encoder and a Target Encoder are trained to maximize cosine similarity between actual future data and available weak labels (weather/time). During inference, the pre-trained Covariate Encoder guides the base predictor.
- **Core assumption:** Future states of the target variable are causally correlated with the "weak" external covariates, and these covariates are available at inference time.
- **Evidence anchors:**
  - [abstract] "...weak data enriching module leverages easily accessible context information... to provide valuable weak supervision..."
  - [Section III-C2] "We devise a contrastive learning framework... adopting a dual encoder module—target sequences vs. future covariates—to effectively model their latent correlation."
- **Break condition:** If the provided "weak labels" (e.g., generic weather forecasts) have low correlation with the target series (e.g., indoor sensor data), the contrastive pre-training may enforce spurious alignments, degrading performance.

## Foundational Learning

- **Concept: Patching (Time Series)**
  - **Why needed here:** LiPFormer relies on dividing the input sequence into sub-series patches (e.g., length 48) to reduce complexity from $O(N^2)$ to $O(N^2/pl^2)$.
  - **Quick check question:** Can you explain why treating a time series as a sequence of patches (tokens) rather than individual points changes the receptive field of the model?

- **Concept: Contrastive Learning (CLIP-style)**
  - **Why needed here:** The "Weak Data Enriching" module uses this technique to align two different modalities (numerical history vs. semantic covariates) in a shared latent space.
  - **Quick check question:** How does maximizing the similarity of positive pairs (target, covariate) while minimizing similarity of negative pairs help the model learn a representation?

- **Concept: Inductive Bias in Transformers**
  - **Why needed here:** The paper argues against NLP-derived components (FFNs, LN) based on the inductive bias that numerical time series differ fundamentally from semantic text.
  - **Quick check question:** Why might Layer Normalization, which handles variable input lengths in NLP, be considered redundant for fixed-length patch time series models?

## Architecture Onboarding

- **Component map:**
  Input Layer (Instance Norm + Patching) -> Cross-Patch Attention (global trends) + Inter-Patch Attention (local correlations) -> MLP Head (linear projection) -> Optional Covariate Encoder (contrastive pre-training)

- **Critical path:**
  The data flow moves from Patching -> Cross/Inter-Patch Attention -> Linear Projection. The critical efficiency gain happens in the attention blocks where LN/FFN are skipped. The critical accuracy gain often comes from the Covariate Encoder injection at the output stage.

- **Design tradeoffs:**
  - Efficiency vs. Depth: Removing FFNs makes the model incredibly fast and small (suitable for edge devices) but risks losing high-level feature abstraction.
  - Patch Size (pl): A fixed patch size (e.g., 48) is used. The Cross-Patch mechanism tries to fix the resulting loss of global granularity, but it remains a heuristic.
  - Covariate Availability: The model requires future covariates (weather, calendar) at inference time. If this data is missing, the "Weak Data Enriching" module must be disabled or emulated.

- **Failure signatures:**
  - Performance lag on complex seasonality: If the Cross-Patch sampling rate misses subtle seasonal peaks, the model may smooth predictions excessively.
  - Instability without Normalization: While the paper argues for removing LN, without proper input scaling (Instance Norm), gradient explosions might occur in very deep variants.
  - Covariate Misalignment: If the Covariate Encoder overfits to spurious correlations during pre-training, final inference may drift significantly from ground truth.

- **First 3 experiments:**
  1. **Backbone Baseline:** Run LiPFormer (Backbone only) vs. PatchTST on the ETTh1 dataset to verify the efficiency/accuracy trade-off of removing FFN/LN.
  2. **Patch Size Sensitivity:** Vary patch length pl (e.g., 16 vs. 48) to stress-test the Cross-Patch Attention's ability to capture global trends.
  3. **Weak Supervision Ablation:** Train with the Covariate Encoder enabled vs. disabled on the Electricity-Price dataset to quantify the lift from future weak labels.

## Open Questions the Paper Calls Out
None

## Limitations
- The Cross-Patch attention mechanism's ability to capture global trends without positional encoding remains heuristic and may fail for irregular or chaotic series.
- The model's efficiency gains are benchmarked against classic transformers, not the latest efficient variants like Informer or Autoformer, limiting comparative claims.
- The success of weak supervision depends on the availability and quality of future covariates, which are not always guaranteed in real-world deployments.

## Confidence

- **High Confidence:** The efficiency improvements (parameter reduction, inference speedup) and the general framework of weak supervision via contrastive learning are well-supported by experiments across multiple datasets.
- **Medium Confidence:** The claim that removing FFNs and LN improves accuracy is supported by ablations, but the mechanism is dataset-dependent and may not generalize to all time series types.
- **Low Confidence:** The assertion that Cross-Patch attention reliably captures global trends without positional encoding is plausible but under-validated, especially for irregular or chaotic series.

## Next Checks

1. **Cross-Patch Robustness Test:** Evaluate LiPFormer on datasets with known irregular seasonality (e.g., retail sales with promotional spikes) to verify that fixed-position trend sampling does not miss critical patterns.

2. **Normalization Ablation Stress Test:** Train LiPFormer without Instance Normalization on a volatile dataset (e.g., Traffic) to check for gradient instability, directly testing the claim that LN can be safely removed.

3. **Covariate Dependency Analysis:** Run LiPFormer with and without the Covariate Encoder on datasets where future covariates are noisy or weakly correlated (e.g., indoor sensor data with weather forecasts) to quantify the risk of spurious alignment.