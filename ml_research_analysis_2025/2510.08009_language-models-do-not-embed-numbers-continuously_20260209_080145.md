---
ver: rpa2
title: Language Models Do Not Embed Numbers Continuously
arxiv_id: '2510.08009'
source_url: https://arxiv.org/abs/2510.08009
tags:
- embedding
- numerical
- language
- numbers
- space
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work investigates how well embedding models represent continuous\
  \ numerical values. The authors propose a lightweight framework using three metrics:\
  \ linear reconstruction R\xB2, PCA correlation, and explained variance to evaluate\
  \ embedding fidelity."
---

# Language Models Do Not Embed Numbers Continuously

## Quick Facts
- **arXiv ID**: 2510.08009
- **Source URL**: https://arxiv.org/abs/2510.08009
- **Reference count**: 6
- **Primary result**: Embedding models encode numerical values with substantial noise orthogonal to magnitude, with first principal component explaining at most 40% of variance

## Executive Summary
This work investigates whether embedding models represent continuous numerical values as continuous or introduce noise and non-linearities. The authors propose a lightweight framework using three metrics: linear reconstruction R², PCA correlation, and explained variance to evaluate embedding fidelity. Testing models from OpenAI, Google Gemini, and Voyage AI across positive decimals, mixed-sign decimals, and high-magnitude mixed-sign integers with varying precision, they find that while linear reconstruction is possible with high fidelity (R² ≥ 0.95), the first principal component explains at most 40% of variance, indicating substantial noise orthogonal to numerical content. This performance degrades with higher precision and mixed signs, with visualizations revealing that embeddings do not treat numerical space as truly continuous, often organizing mixed-sign numbers primarily by sign in the first component rather than by magnitude.

## Method Summary
The authors evaluate embedding fidelity using three synthetic datasets of 500 scalars each: positive decimals (0 to 1 with 1-20 decimal places), mixed-sign decimals (-1 to 1 with 1-20 decimal places), and mixed-sign integers (-10^20 to 10^20 with 0-20 integer places). For each dataset, they embed scalars using target models (OpenAI, Google Gemini, Voyage AI), then fit linear regression to predict original values from embeddings (measuring R²), run PCA on embeddings to compute explained variance ratio of first principal component, and correlate PC1 with original values. The framework tests whether numerical information is linearly decodable versus requiring complex non-linear manifolds, and quantifies how much variance in embeddings relates to the number itself versus orthogonal noise from pretraining artifacts.

## Key Results
- Linear reconstruction achieves R² ≥ 0.95 for positive decimals but degrades significantly with mixed signs and higher precision
- First principal component explains at most 40% of variance even for simple positive decimals
- Mixed-sign numbers are primarily organized by sign in PC1 rather than by magnitude, creating discontinuous jumps at zero
- Performance degradation correlates with increased decimal precision and mixed-sign configurations

## Why This Works (Mechanism)

### Mechanism 1: Linear Reconstruction Preserves Ordinal Information
- **Claim:** Scalar values can be recovered from embeddings via linear projection, though the embedding space encodes far more than the number itself.
- **Mechanism:** The embedding function f(x) → x̂ ∈ ℝ^d maps scalars into a high-dimensional space where at least one linear subspace retains ordering information. A trained linear probe lin(x̂) → X' can recover original values with R² ≥ 0.95 for positive decimals.
- **Core assumption:** Assumes numerical information is linearly decodable rather than stored in complex non-linear manifolds.
- **Evidence anchors:**
  - [abstract]: "reconstruction is possible with high fidelity (R² ≥ 0.95)"
  - [page 5, Linear Reconstruction]: "most models maintain corr(X, X') ≥ 0.95... indicating that the original samples can be well-constructed from the embedding space"
  - [corpus]: "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers" confirms accurate input embedding representations for numbers across LLMs.
- **Break condition:** Reconstruction degrades significantly when numbers have mixed signs or high decimal precision (see Table 2: OpenAI small model drops to R² = 0.87 with mixed signs).

### Mechanism 2: Noise Injection from Pretraining Artifacts
- **Claim:** Most variance in numerical embeddings is orthogonal to the actual number value, introducing systematic noise.
- **Mechanism:** Embedding models trained on diverse corpora encode linguistic and contextual artifacts alongside numerical content. PCA reveals the first component explains ≤40% of variance even for simple positive decimals, with remaining dimensions capturing spurious variation.
- **Core assumption:** Assumes the low explained variance reflects entanglement with non-numerical features rather than a deliberate multi-component numerical encoding scheme.
- **Evidence anchors:**
  - [abstract]: "principal components only explain a minor share of variation within the embedding space"
  - [page 6, Discussion]: "the embedding space contains substantial additional variation that is not present in the original one-dimensional numerical input... artifacts from the models' pretraining on diverse text corpora"
  - [corpus]: "Unravelling the Mechanisms of Manipulating Numbers in Language Models" notes tension between accurate input representations and erroneous numeric outputs.
- **Break condition:** If future work identifies denoising techniques or specialized numerical embedding heads, this mechanism may reflect current architecture limitations rather than fundamental constraints.

### Mechanism 3: Sign-Dominant Factorization Breaks Continuity
- **Claim:** Mixed-sign numbers are primarily organized by sign in the first principal component, not by magnitude, violating expectations of continuous numerical space.
- **Mechanism:** When encoding x ∈ [-1000, 1000], models use PC1 primarily for sign classification (positive vs. negative cluster), pushing magnitude encoding to PC2 or beyond. This creates discontinuous jumps at zero.
- **Core assumption:** Assumes this factorization reflects training data statistics or tokenization patterns rather than an intentional design.
- **Evidence anchors:**
  - [page 6, Figure 6 visualization]: "all three models on x^± have the first principal component encoding effectively only the sign of the data, despite the fundamentally continuous nature of the original values"
  - [page 6, Implications]: "The principal component of variation for all models, with mixed sign values, comes to represent only the sign of the numbers"
  - [corpus]: Weak direct evidence; "FoNE: Precise Single-Token Number Embeddings via Fourier Features" offers alternative encoding via Fourier features but doesn't directly address sign factorization.
- **Break condition:** If models were fine-tuned with continuous numerical objectives or different tokenization schemes, sign-magnitude factorization patterns might shift.

## Foundational Learning

- **Principal Component Analysis (PCA) for Representation Quality**
  - **Why needed here:** The paper's core diagnostic uses PCA explained variance to quantify how much of the embedding space is actually devoted to numerical content vs. noise.
  - **Quick check question:** If an embedding of 1D scalars has PC1 explaining only 30% of variance, what does that imply about the remaining dimensions?

- **Linear Probing / Reconstruction**
  - **Why needed here:** The paper uses linear R² to test whether numerical information is linearly accessible, distinguishing "information is there but noisy" from "information requires non-linear extraction."
  - **Quick check question:** A linear probe achieves R² = 0.98 but PC1 explains only 25% of variance—how do you reconcile these results?

- **Embedding Space Geometry**
  - **Why needed here:** Understanding whether embeddings form continuous manifolds vs. discrete clusters determines their suitability for similarity search, numerical reasoning, and scientific applications.
  - **Quick check question:** If negative and positive numbers form separate clusters in embedding space, what happens to similarity searches near zero?

## Architecture Onboarding

- **Component map:**
  - **Input layer:** Scalar strings (e.g., "2.847 × 10⁻⁹") tokenized via byte-pair or character-level encoding
  - **Embedding model f(x):** Transformer-based encoder (GPT-4 family, Gemini, or Voyage) producing d-dimensional vectors
  - **Evaluation pipeline:** Linear reconstruction probe + PCA analysis over embedded dataset
  - **Metrics:** Linear R² (reconstruction fidelity), PCA R² (ordinal alignment), explained variance ratio (noise quantification)

- **Critical path:**
  1. Generate scalar datasets (positive decimals, mixed-sign decimals, mixed-sign integers at varying precision)
  2. Embed via target model API
  3. Fit linear regression from embeddings → original scalars
  4. Run PCA on embeddings, correlate PC1 with original scalars
  5. Measure PC1 explained variance ratio

- **Design tradeoffs:**
  - **Tokenization:** Digit-level vs. whole-number vs. scientific notation affects how "2.847" shares representations with "2.847×10⁻⁶" (mantissa entanglement)
  - **Precision:** Higher decimal places increase prompt length and introduce more noise per the paper's findings
  - **Model size:** Condensed embedding models (text-embedding-3-small, voyage-3.5-lite) show steeper degradation with mixed signs

- **Failure signatures:**
  - R² drops below 0.90 when mixed signs introduced → model factorizing by sign rather than magnitude
  - PC1 variance below 20% → majority of embedding space is numerical noise
  - Similarity search returning values with shared mantissa but wrong exponent → magnitude not properly encoded

- **First 3 experiments:**
  1. **Baseline reconstruction test:** Embed 500 positive decimals at 1-5 decimal places, measure linear R² and PC1 explained variance for your target embedding model to establish expected performance envelope.
  2. **Sign sensitivity probe:** Compare PC1-PC2 visualizations for x ∈ [0, 1000] vs. x ∈ [-1000, 1000] to confirm whether your application's sign range triggers factorization failure.
  3. **Precision-to-noise curve:** Incrementally increase decimal places from 1 to 10 and plot PC1 explained variance degradation to determine acceptable precision thresholds for your use case.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can specialized pretraining objectives or architectural modifications create embedding models that better isolate numerical information from orthogonal noise?
- **Basis in paper:** [explicit] The conclusion states: "Future work should focus on developing embedding architectures specifically designed for numerical data, potentially through specialized pretraining objectives or architectural modifications that better isolate numerical information from other sources of variation."
- **Why unresolved:** This work only evaluates existing off-the-shelf models; no modifications or specialized training were attempted.
- **What evidence would resolve it:** Training embedding models with numerical-focused objectives (e.g., contrastive loss over magnitude relationships) and measuring whether explained variance in the first principal component approaches 1.0.

### Open Question 2
- **Question:** What semantic or structural content do the orthogonal dimensions in numerical embeddings encode?
- **Basis in paper:** [inferred] The paper demonstrates that the first principal component explains at most 40% of variance, indicating substantial orthogonal variation, but does not investigate what this "noise" represents.
- **Why unresolved:** The paper quantifies the existence of orthogonal dimensions but stops short of characterizing their content.
- **What evidence would resolve it:** Probing experiments testing whether orthogonal dimensions encode string properties (digit patterns, length), periodic features, or other identifiable structures from pretraining corpora.

### Open Question 3
- **Question:** Does numerical embedding fidelity (R², PCA correlation, explained variance) predict downstream task performance on numerical reasoning?
- **Basis in paper:** [inferred] The paper proposes metrics for evaluating embedding fidelity but does not validate whether these metrics correlate with performance on practical tasks like arithmetic, scientific retrieval, or quantitative reasoning.
- **Why unresolved:** The work establishes evaluation metrics but does not connect them to downstream utility.
- **What evidence would resolve it:** Correlation analysis between the proposed fidelity metrics and benchmark scores on numerical reasoning tasks (e.g., NUPA Test, arithmetic benchmarks) across multiple models.

### Open Question 4
- **Question:** Can fine-tuning existing embedding models improve numerical fidelity, and which training objectives are most effective?
- **Basis in paper:** [inferred] The paper evaluates only pre-trained embedding models without exploring whether adaptation could address the identified limitations with mixed signs, high precision, and large magnitudes.
- **Why unresolved:** No fine-tuning experiments were conducted; all models were evaluated as-is from providers.
- **What evidence would resolve it:** Fine-tuning experiments with numerical reconstruction losses, followed by measurement of whether R² and explained variance improve, particularly for mixed-sign and high-precision inputs.

## Limitations

- The study focuses on synthetic, uniformly distributed scalar datasets, which may not capture the complexity of real-world numerical data with heavy-tailed distributions or temporal dependencies.
- The evaluation assumes linear separability of numerical information, potentially overlooking non-linear encoding schemes that could preserve continuity while appearing noisy under PCA.
- The paper does not investigate whether fine-tuning or post-processing could mitigate the observed degradation with mixed signs and high precision.

## Confidence

- **High confidence**: Linear reconstruction maintains R² ≥ 0.95 for positive decimals (supported by consistent quantitative results across multiple models and datasets).
- **Medium confidence**: First principal component explains ≤40% variance for all tested configurations (PCA is a standard, reproducible technique, but the interpretation of "noise" vs. intentional encoding requires further validation).
- **Medium confidence**: Sign factorization dominates PC1 for mixed-sign numbers (visual evidence is compelling, but the mechanism may depend on tokenization choices not fully specified).

## Next Checks

1. **Distribution sensitivity test**: Repeat experiments using log-uniform and power-law distributed numbers to assess whether the observed patterns hold for non-uniform numerical distributions common in scientific and financial domains.
2. **Non-linear reconstruction probe**: Apply kernel regression or neural network regression to determine whether non-linear methods can recover higher precision values from embeddings where linear R² drops below 0.9.
3. **Cross-tokenization analysis**: Compare results when feeding numbers in fixed-point, scientific notation, and mixed formats to isolate whether tokenization artifacts drive the sign-magnitude factorization observed in PCA visualizations.