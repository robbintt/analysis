---
ver: rpa2
title: 'TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models'
arxiv_id: '2601.14951'
source_url: https://arxiv.org/abs/2601.14951
tags:
- temporal
- image
- knowledge
- prompt
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of evaluating temporal knowledge
  in text-to-image (T2I) models. While temporal reasoning has been extensively studied
  in natural language processing, the ability of T2I models to accurately represent
  time-dependent visual cues remains unexplored.
---

# TempViz: On the Evaluation of Temporal Knowledge in Text-to-Image Models

## Quick Facts
- arXiv ID: 2601.14951
- Source URL: https://arxiv.org/abs/2601.14951
- Reference count: 40
- No model exceeds 75% accuracy on temporal tasks across five categories

## Executive Summary
This paper addresses the critical gap in evaluating temporal knowledge in text-to-image models, introducing the TempViz dataset with 7,940 prompts spanning five categories: landscapes, animals, buildings, maps, and artworks. The authors conduct comprehensive human evaluation of five leading T2I models, measuring image quality, subject fidelity, and adherence to temporal cues. Results reveal that temporal competence is generally weak across all models, with no model exceeding 75% accuracy, and particularly poor performance on maps (15% accuracy). The study also evaluates automated methods for assessing temporal cues, finding that none reliably detect correct temporal application above a macro F1 of 72%. These findings highlight the need for further research on temporal knowledge in T2I models and evaluation methods.

## Method Summary
The study evaluates temporal knowledge in text-to-image models using the TempViz dataset containing 7,940 prompts with temporal cues across five categories. Five T2I models (SDv1.5, SDXL-Turbo, SDXL-Base, SDv3.5, FLUX.1-dev) are tested through human evaluation of 500 stratified prompt samples. Each image is rated on three binary questions: image quality, subject presence, and temporal cue accuracy. Automated evaluation includes CLIPScore, BLIP-based captioning, and VQA-based methods using models like GPT-4o-mini and Qwen2.5-VL-32B-Instruct. The study runs on 4× NVIDIA RTX A6000 GPUs, using standard inference parameters from Hugging Face implementations.

## Key Results
- No model exceeds 75% accuracy on temporal tasks across categories
- Maps category shows worst performance at 15% accuracy
- FLUX achieves highest image quality (87%) but only 38% temporal accuracy
- SDv1.5 achieves highest temporal accuracy (46%) despite lower image quality
- Automated metrics (CLIPScore, captioning) show no correlation with temporal correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal knowledge is poorly encoded in current T2I model architectures, with image quality and temporal fidelity being largely orthogonal capabilities.
- Mechanism: Text encoders embed temporal expressions alongside subject descriptions, but diffusion models prioritize visual coherence over temporal grounding during denoising.
- Core assumption: Temporal expressions are treated as semantic modifiers without grounding to specific visual attributes during training.
- Evidence anchors: [abstract] "no model exceeding 75% accuracy across categories"; [section 5.2] FLUX leads on quality but only second on temporal accuracy (38%); [corpus] R2I-Bench confirms reasoning-driven generation remains a fundamental challenge.

### Mechanism 2
- Claim: Different temporal categories require distinct knowledge types that may not transfer across domains.
- Mechanism: Each category demands different combinations of commonsense knowledge (seasons), factual knowledge (building dates), biological knowledge (aging), and symbolic reasoning (map conventions).
- Core assumption: Temporal knowledge is category-specific rather than forming a unified temporal reasoning capability.
- Evidence anchors: [section 3.1] Landscapes require "commonsense knowledge of seasonal and diurnal cycles"; [section 5.2] Maps category most challenging (50% images without visual errors); [corpus] Beyond Words and Pixels confirms implicit world knowledge reasoning fails across diverse prompts.

### Mechanism 3
- Claim: Standard evaluation metrics fail to capture temporal cue fidelity because they conflate subject presence with temporal correctness.
- Mechanism: CLIPScore computes embedding similarity between prompt and image, but temporal modifiers produce subtle visual changes that don't significantly shift embeddings.
- Core assumption: Temporal cues produce visual changes too fine-grained for coarse embedding-based metrics.
- Evidence anchors: [section 5.3] "CLIPScore shows no significant correlation with any annotated labels"; [section 5.3] "correlations for temporal cues are negative"; [corpus] WISE benchmark notes similar limitations in existing evaluation protocols.

## Foundational Learning

- **Concept**: Latent Diffusion Models (LDMs) and Diffusion Transformers
  - Why needed here: Understanding architectural differences helps interpret performance variations despite similar training paradigms
  - Quick check question: Can you explain why a diffusion transformer (MM-DiT in FLUX/SDv3.5) might handle text conditioning differently than a U-Net architecture?

- **Concept**: CLIP Text-Image Alignment
  - Why needed here: The paper relies on CLIPScore as a baseline metric; understanding its limitations for fine-grained evaluation is critical
  - Quick check question: Why would a CLIP embedding struggle to distinguish "a map of Europe in 1938" from "a map of Europe in 2020"?

- **Concept**: VLM-as-Judge Evaluation Paradigm
  - Why needed here: The paper tests multiple VLM judging strategies; understanding their failure modes informs future evaluation design
  - Quick check question: What types of visual features would a VLM need to attend to correctly judge whether a dog appears "3 months old" versus "10 years old"?

## Architecture Onboarding

- **Component map**: Text Encoder(s) → Text Embeddings → Cross-Attention → Denoising Network → VAE Decoder → Image → Temporal expressions embedded but not grounded → Diffusion process optimizes visual quality, not temporal fidelity → Output: High-quality image with potentially incorrect temporal cues

- **Critical path**: Text encoding → cross-attention conditioning → denoising trajectory. The paper suggests temporal grounding fails at the encoding stage (temporal expressions not distinguishably embedded) and/or the cross-attention stage (temporal cues not selectively attended to).

- **Design tradeoffs**:
  - Image quality vs. temporal accuracy: FLUX achieves 87% quality but only 38% temporal; SDv1.5 achieves 46% temporal but lower quality
  - Model scale: SDXL (3x larger UNet) doesn't consistently outperform SDv1.5 on temporal tasks
  - Distillation: SDXL-T (distilled) shows severe degradation (0% coherent map outputs)

- **Failure signatures**:
  - Maps: 15% accuracy, models fail at geopolitical border rendering
  - Buildings: Models generate buildings that should be demolished or not-yet-built
  - Animals: Aging cues often subtle or absent; 200% lifespan prompts yield inconsistent "deceased" depictions
  - Artworks: Period-specific styles incorrectly applied when only year is specified

- **First 3 experiments**:
  1. Prompt ablation: Test whether explicit temporal descriptors (e.g., "with snow on the ground for winter") improve performance vs. implicit cues ("in January")
  2. Reference image conditioning: Provide models with temporally-correct reference images as additional conditioning to test whether visual grounding helps
  3. Category-specific fine-tuning: Fine-tune a base T2I model on temporally-labeled pairs from one category (e.g., maps) and evaluate transfer to others

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can automated evaluation metrics be developed to reliably capture subtle temporal cues that currently cause misalignment with human judgments?
- Basis in paper: [explicit] None of the tested automated evaluators could reliably assess temporal cues, with the best approach peaking at only 72% macro F1
- Why unresolved: Current metrics struggle with nuance; GPT-5 frequently misjudges the "Animals" category (aging) due to visual ambiguity
- What evidence would resolve it: Creation of an automated evaluation method that achieves consistently high correlation (>0.9) with human ground-truth labels across all five temporal categories

### Open Question 2
- Question: Are the observed limitations in temporal generation primarily due to gaps in training data or architectural constraints in encoding time?
- Basis in paper: [inferred] Authors note general image quality does not predict temporal accuracy and even advanced models fail to exceed 75% accuracy
- Why unresolved: Paper evaluates existence of performance gap but does not isolate whether failure is caused by lack of time-indexed images in training or models' inability to ground temporal language
- What evidence would resolve it: Study showing fine-tuning on targeted dataset of time-separated image pairs significantly closes performance gap compared to architectural modifications

### Open Question 3
- Question: How can the evaluation of temporal knowledge be extended to human subjects without reinforcing demographic stereotypes?
- Basis in paper: [explicit] Authors "intentionally exclude prompts that generate human individuals" because varying temporal cues would introduce "complex issues of stereotype reinforcement and bias"
- Why unresolved: Human aging and historical dress are critical visual aspects of time, yet the field lacks methodology to evaluate these capabilities safely
- What evidence would resolve it: Introduction of benchmark including human subjects that utilizes fairness metrics to demonstrate temporal variations don't correlate with spurious demographic attributes

## Limitations
- Automated evaluation metrics fail to capture temporal correctness above 72% macro F1
- Human evaluation shows only moderate annotator agreement (Cohen's κ = 0.5)
- Study excludes human subjects to avoid stereotype reinforcement issues
- Model generation hyperparameters not fully specified for exact reproduction

## Confidence

| Claim | Confidence |
|-------|------------|
| Temporal knowledge poorly encoded in T2I architectures | High |
| Different temporal categories require distinct knowledge types | High |
| Standard evaluation metrics fail to capture temporal fidelity | High |
| No model exceeds 75% accuracy on temporal tasks | High |

## Next Checks

1. Verify that FLUX achieves 87% image quality but only 38% temporal accuracy on the 500-sample stratified subset
2. Confirm that SDv1.5 achieves 46% temporal accuracy despite lower image quality compared to FLUX
3. Test whether VQA-based evaluation with GPT-4o-mini or Qwen2.5-VL achieves correlation above 0.5 with human temporal accuracy labels