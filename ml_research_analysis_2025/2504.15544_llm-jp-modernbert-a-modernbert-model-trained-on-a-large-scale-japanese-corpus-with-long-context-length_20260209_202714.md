---
ver: rpa2
title: 'llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus
  with Long Context Length'
arxiv_id: '2504.15544'
source_url: https://arxiv.org/abs/2504.15544
tags:
- sentence
- training
- stage
- japanese
- length
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'llm-jp-modernbert is a ModernBERT model trained on a large-scale
  Japanese corpus with an 8192-token context length. The model is based on ModernBERT-base
  and trained using a two-stage approach: first with 1024-token context, then extended
  to 8192 tokens.'
---

# llm-jp-modernbert: A ModernBERT Model Trained on a Large-Scale Japanese Corpus with Long Context Length

## Quick Facts
- arXiv ID: 2504.15544
- Source URL: https://arxiv.org/abs/2504.15544
- Reference count: 9
- Key outcome: ModernBERT model with 8192-token context trained on ~0.69T Japanese tokens, achieving 91.6/92.0/84.5 on JGLUE tasks

## Executive Summary
llm-jp-modernbert is a ModernBERT model pretrained on a large-scale Japanese corpus with an 8192-token context length. Built on ModernBERT-base, it uses a two-stage pretraining approach: Stage 1 trains with 1024-token context for 500k steps, then Stage 2 extends to 8192 tokens for 200k additional steps. Trained on the Japanese subset of llm-jp-corpus v4 using 30% mask rate MLM, the model achieves strong JGLUE performance (91.6/92.0/84.5 on JSTS/JNLI/JCoLA) and shows improved long-context pseudo-perplexity after Stage 2. Sentence embedding analysis reveals alignment-uniformity trade-offs during training. The model and training/evaluation code are publicly released.

## Method Summary
The model uses ModernBERT-base architecture with RoPE positional embeddings and Local-Global Alternating Attention. It was trained on Japanese corpus subset (~0.69T tokens) using a two-stage approach: Stage 1 with 1024-token context (500k steps, batch 3328, LR 5e-4), then Stage 2 extending to 8192 tokens (200k steps, batch 384, LR 5e-5). Both stages use 30% mask rate MLM with AdamW optimizer. Total parameters: 187M due to large Japanese tokenizer (99,574 vocab). Hardware: 16× NVIDIA H100 80GB; training time: 8 days + 3 days.

## Key Results
- Achieves 91.6/92.0/84.5 on JGLUE tasks (JSTS/JNLI/JCoLA)
- Shows reduced pseudo-perplexity on long sequences after Stage 2 training
- Sentence embedding analysis reveals alignment-uniformity trade-off during pretraining
- Zero-shot MIRACL retrieval performance peaks at 15k steps then declines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage pretraining enables stable long-context acquisition after short-context foundation
- Mechanism: Stage 1 establishes core representations at 1024 tokens, Stage 2 extends to 8192 tokens while adapting RoPE embeddings
- Core assumption: Short-context pretraining provides stable initialization preventing catastrophic forgetting
- Evidence anchors: Section 2.3 describes two-stage approach; Figure 2 shows pseudo-perplexity reduction from Stage 1 to Stage 2
- Break condition: Stage 2 training <100k steps may cause long-context performance degradation

### Mechanism 2
- Claim: 30% mask rate increases training signal density compared to standard 15% BERT masking
- Mechanism: Higher mask rate exposes more tokens per batch to prediction, potentially accelerating representation learning
- Core assumption: Increased mask rate improves sample efficiency without degrading context understanding
- Evidence anchors: Table 1 specifies 30% MLM probability; follows ModernBERT recommendations without ablation
- Break condition: Aggressive masking may fragment learned dependencies if downstream tasks require fine-grained context coherence

### Mechanism 3
- Claim: RoPE enables extrapolation to longer contexts than seen during training
- Mechanism: RoPE encodes position via rotation in embedding space, allowing relative position computation at arbitrary distances
- Core assumption: RoPE's theoretical extrapolation properties transfer to Japanese language structures
- Evidence anchors: Section 2.1 lists RoPE as architectural component; Table 1 confirms theta=10,000; neighbor papers confirm RoPE for long-context encoder models
- Break condition: Low theta values may cause position distinctions to blur for target length

## Foundational Learning

- Concept: **Masked Language Modeling (MLM)**
  - Why needed here: Core pretraining objective; understanding 30% mask rate deviation from BERT's 15% requires grasping MLM mechanics
  - Quick check question: Why might higher mask rates increase training efficiency but risk context fragmentation?

- Concept: **Pseudo-perplexity**
  - Why needed here: Used to evaluate long-context performance when standard perplexity is computationally infeasible
  - Quick check question: How does pseudo-perplexity differ from standard perplexity, and why sample 100 random positions?

- Concept: **Alignment vs. Uniformity Trade-off**
  - Why needed here: Paper analyzes sentence embedding quality through these metrics
  - Quick check question: If alignment improves but uniformity degrades, what does this indicate about embedding space structure?

## Architecture Onboarding

- Component map:
  - llm-jp-tokenizer v33 (99,574 vocab) → ModernBERT-base backbone (12 layers, 149M base params) → RoPE positional encoding (theta=10,000) → Local-Global Alternating Attention + FlashAttention → 187M total parameters

- Critical path:
  1. Initialize ModernBERT-base architecture
  2. Replace tokenizer with llm-jp-tokenizer v33 → embedding layer resizes
  3. Stage 1: Train 500k steps @ 1024 tokens, LR 5e-4
  4. Stage 2: Resume checkpoint, extend to 8192 tokens, LR 5e-5, 200k steps
  5. Evaluate at checkpoints (4k, 15k, 50k, 100k, 200k, etc.)

- Design tradeoffs:
  - Large vocab (99,574): Better Japanese coverage vs. larger embedding parameters
  - Two-stage training: Stable short-context learning vs. additional compute for extension
  - 30% mask rate: Faster training signal vs. potential context fragmentation
  - No NSP objective: Simplified training vs. potential sentence-relation signal loss

- Failure signatures:
  - Stage 2 undertraining: Pseudo-perplexity increases with sequence length
  - JGLUE plateau after 50k steps: Validation loss improves but downstream tasks stagnate
  - Alignment-uniformity collapse: Embeddings concentrate in narrow space

- First 3 experiments:
  1. Reproduce pseudo-perplexity curve across sequence lengths [0, 8192] to verify Stage 2 adaptation
  2. Fine-tune on JGLUE subtasks (JSTS, JNLI, JCoLA) with LR sweep to identify optimal checkpoint
  3. Extract sentence embeddings via mean pooling, compute alignment/uniformity on held-out Wikipedia pairs

## Open Questions the Paper Calls Out

1. Why does downstream task performance on JGLUE plateau after 50k steps while validation loss and accuracy continue to improve throughout training?
   - Basis: Section 3.3 explicitly states this discrepancy remains a future challenge
   - Why unresolved: No ablation studies performed to isolate cause (data distribution, overfitting, or representation limitations)
   - What evidence would resolve: Analysis of representation geometry at later steps or correlation studies between validation loss components and downstream metrics

2. Does explicitly adjusting the training data to account for the distribution of sentence lengths improve long-context performance and resolve the pseudo-perplexity increase observed in Stage 2?
   - Basis: Section 3.5 suggests Stage 2 didn't account for sentence length distribution
   - Why unresolved: Current setup may lack sufficient long-sequence examples during context extension
   - What evidence would resolve: Retraining Stage 2 using curriculum that up-samples (4096, 8192) token sequences

3. How can the observed trade-off between alignment and uniformity be managed to prevent the degradation of zero-shot sentence retrieval capabilities as pretraining progresses?
   - Basis: Section 3.2 shows retrieval performance peaks at 15k steps then declines; Section 3.6 links training to alignment-uniformity trade-off
   - Why unresolved: MLM objective appears to optimize embedding space in way that eventually harms semantic discrimination
   - What evidence would resolve: Experiments with contrastive loss or early-stopping based on alignment metrics

## Limitations

- Data Availability: llm-jp-corpus v4 pending public release creates uncertainty about exact composition and preprocessing
- Stage 2 Duration: 200k steps may be insufficient for full 8192-token capacity realization
- Downstream Plateau: JGLUE performance plateaus at 50k steps despite continued validation improvement, suggesting potential limitations

## Confidence

- High Confidence: Two-stage training methodology, Stage 1 foundation training, and pseudo-perplexity experimental results
- Medium Confidence: Downstream task performance and JGLUE benchmark results due to plateau behavior
- Low Confidence: 30% mask rate efficiency claim and assertion that Stage 2 fully realizes 8192-token capacity without empirical validation

## Next Checks

1. Evaluate llm-jp-modernbert on independent Japanese corpus (e.g., Japanese Wikipedia 2024, OSCAR) to assess cross-corpus generalization
2. Conduct mask rate ablation study comparing 15%, 20%, 25%, and 30% rates on identical Japanese data
3. Create synthetic long-sequence tasks (4096-8192 tokens) requiring multi-document reasoning to empirically validate long-context understanding capabilities