---
ver: rpa2
title: 'Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided
  Exploration'
arxiv_id: '2505.17621'
source_url: https://arxiv.org/abs/2505.17621
tags:
- imagine
- exploration
- grpo
- reasoning
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of sparse reward signals and
  weak exploration incentives in reinforcement learning (RL) for enhancing large language
  model (LLM) reasoning capabilities. The authors propose IMAGINE, which introduces
  three innovations: trajectory-aware exploration rewards using lightweight networks
  to characterize reasoning path uniqueness, error-conditioned reward allocation focusing
  exploration on incorrect trajectories, and advantage-preserving integration that
  maintains policy gradient stability.'
---

# Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration

## Quick Facts
- arXiv ID: 2505.17621
- Source URL: https://arxiv.org/abs/2505.17621
- Reference count: 40
- Primary result: 22.23% accuracy improvement on AIME 2024 vs standard RL approaches

## Executive Summary
This paper addresses the challenge of sparse reward signals and weak exploration incentives in reinforcement learning for enhancing large language model reasoning capabilities. The authors propose IMAGINE, which introduces three innovations: trajectory-aware exploration rewards using lightweight networks to characterize reasoning path uniqueness, error-conditioned reward allocation focusing exploration on incorrect trajectories, and advantage-preserving integration that maintains policy gradient stability. Experiments on four public datasets show IMAGINE improves performance by 22.23% on AIME 2024 compared to standard RL approaches. The method demonstrates consistent gains across different base models and datasets, with particularly strong improvements on more difficult reasoning tasks.

## Method Summary
IMAGINE enhances LLM reasoning via RL by adding intrinsic motivation-guided exploration rewards. The method uses two lightweight networks (frozen target fθT and trainable predictor fθP) to compute trajectory-level prediction error on complete question-response pairs, providing novelty rewards. These exploration rewards are only allocated to incorrect trajectories and are integrated after advantage computation to preserve GRPO's group normalization. The approach maintains compatibility with existing RL algorithms while adding minimal computational overhead.

## Key Results
- 22.23% accuracy improvement on AIME 2024 compared to standard GRPO
- Consistent performance gains across four datasets: GSM8K, Countdown-34, Countdown-4, and AIME 2024
- Achieves results with minimal additional training overhead while maintaining compatibility with PPO and GRPO
- Particularly strong improvements on more difficult reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
Sequence-level novelty signals reduce token-level sampling bias while lowering computational overhead from O(|o|) to O(1) per response. A frozen randomly initialized target network fθT and trainable predictor fθP compute prediction error on complete question-response pairs (q, o). Higher prediction error indicates less-visited trajectories, providing intrinsic novelty rewards. Core assumption: Prediction error serves as a proxy for trajectory novelty in the LLM reasoning space.

### Mechanism 2
Allocating exploration rewards only to incorrect trajectories concentrates learning signals on hard samples and stabilizes training by not perturbing already-correct reasoning paths. A binary selector I(a≠o) gates exploration rewards. Correct trajectories receive zero exploration bonus; incorrect trajectories receive full novelty signal. Core assumption: Correct trajectories already have sufficient outcome-based gradients.

### Mechanism 3
Injecting exploration rewards after advantage computation preserves the statistical integrity of outcome-based gradients, preventing distortion in GRPO's group normalization. Compute original advantage Âold from outcome rewards first, then add exploration reward: Ânew = Âold + R★(q, o). This prevents exploration rewards from inverting advantage signs during group-wise normalization.

## Foundational Learning

- **Proximal Policy Optimization (PPO)**
  - Why needed here: IMAGINE integrates with PPO as a base RL algorithm; understanding clipped objectives and GAE is required to modify the advantage computation correctly.
  - Quick check question: Can you explain why PPO uses a clipped objective and how GAE estimates advantages?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO is the primary baseline; its group-wise advantage normalization (Âᵢ = (Rᵢ − mean)/std) is exactly what IMAGINE must avoid distorting.
  - Quick check question: What happens to group-normalized advantages if you add rewards before computing the mean and std?

- **Random Network Distillation (RND)**
  - Why needed here: IMAGINE's dual-network novelty detection directly adapts RND from continuous control to LLM sequence spaces.
  - Quick check question: Why does prediction error on a frozen random network correlate with state visitation frequency?

## Architecture Onboarding

- **Component map:**
  Input Batch B → [Outcome Reward Function] → R (sparse: 0/1/0.1/0) → [GRPO/PPO Advantage] → Âold → [IMAGINE Networks: fθP, fθT] → Prediction Error → R★₁ → [Error Gate: I(a≠o)] → R★ → [Advantage Integration] → Ânew = Âold + R★ → [Policy Update] → Updated πθ

- **Critical path:** The predictor fθP must be updated BEFORE computing R★₁ for the current batch. The error gate I(a≠o) requires ground-truth answers to be available during training.

- **Design tradeoffs:**
  - α (exploration intensity): Too low → insufficient exploration; too high → exploration dominates outcome signals. Paper uses α=0.5 as robust default.
  - γ (decay rate): Controls exploration → exploitation transition. γ=40 balances early exploration with convergence.
  - Predictor network size: Larger → better novelty discrimination but slower; paper uses minimal size (16-dim embedding).

- **Failure signatures:**
  - Loss L(q, o) drops to near-zero immediately → predictor overfitting, no novelty signal
  - Response length collapses → model exploiting outcome rewards only, exploration failed
  - Ânew variance explodes → α too large
  - GRPO advantage signs flip unexpectedly → exploration reward added before normalization

- **First 3 experiments:**
  1. Sanity check on toy dataset: Run GRPO baseline for 50 steps, then IMAGINE(GRPO) for 50 steps. Verify predictor loss decreases gradually, R★ values are distributed reasonably, and accuracy improves.
  2. Ablation of integration order: Compare adding R★ before vs. after advantage computation on Countdown-34. Expect post-hoc addition stabilizes training and improves final accuracy.
  3. Hyperparameter sensitivity sweep: Grid search α ∈ {0.1, 0.5, 0.9} and γ ∈ {20, 40, 70} on validation split. Plot accuracy contours.

## Open Questions the Paper Calls Out

### Open Question 1
Can incorporating diverse, multi-faceted reward functions further enhance IMAGINE's ability to guide exploration in large action spaces? Future work aims to incorporate diverse reward functions to evaluate multi-faceted values and enable multi-angle exploration.

### Open Question 2
Do the lightweight, low-dimensional predictor networks (embedding size=16) capture sufficient semantic novelty to distinguish high-quality reasoning paths? The current implementation relies on small networks for efficiency, but their capacity to model complex reasoning "uniqueness" beyond surface-level syntax is not analyzed.

### Open Question 3
How does the trajectory-aware exploration mechanism scale to LLMs significantly larger than 7B parameters? Performance on frontier-scale models remains unverified, though the dynamics of exploration versus exploitation may shift fundamentally in models with emergent capabilities.

## Limitations
- Dependence on predictor network's ability to maintain novelty signals; if predictor overfits (L(q, o) → 0), exploration incentives collapse entirely
- Cannot distinguish between productively novel paths versus distractor paths that increase diversity without improving accuracy
- Computational overhead claim doesn't account for maintaining and updating the dual-network system

## Confidence
- **High confidence**: Trajectory-level novelty detection reducing token-level sampling bias, supported by Figure 4's empirical demonstration
- **Medium confidence**: Error-conditioned reward allocation's effectiveness, supported by modest gains in Table 10 but limited to one dataset
- **Medium confidence**: Advantage-preserving integration preventing GRPO distortion, supported by Table 10 improvements but dependent on careful hyperparameter tuning

## Next Checks
1. **Predictor Overfitting Stress Test:** Run IMAGINE for 1000 steps while monitoring predictor loss L(q, o). Verify gradual decrease rather than crashing to zero within first 100 steps.
2. **Advantage Sign Inversion Test:** Implement variant adding exploration rewards before GRPO advantage computation. Expect higher variance, more negative advantages, and degraded performance compared to proper post-hoc integration.
3. **Exploration-Exploitation Transition Validation:** Train IMAGINE with varying γ values (10, 40, 100) on GSM8K. Verify γ=40 provides optimal balance between early exploration and late exploitation convergence.