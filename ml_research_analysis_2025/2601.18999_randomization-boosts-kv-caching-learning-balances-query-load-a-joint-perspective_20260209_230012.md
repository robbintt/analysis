---
ver: rpa2
title: 'Randomization Boosts KV Caching, Learning Balances Query Load: A Joint Perspective'
arxiv_id: '2601.18999'
source_url: https://arxiv.org/abs/2601.18999
tags:
- cache
- tokens
- normalized
- latency
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of balancing query load and
  maximizing cache hit rate in multi-LLM serving systems using KV caching. The authors
  propose a unified mathematical model that captures the core trade-offs between KV
  cache eviction and query routing.
---

# Randomization Boosts KV Caching, Learning Balances Query Load: A Joint Perspective

## Quick Facts
- arXiv ID: 2601.18999
- Source URL: https://arxiv.org/abs/2601.18999
- Reference count: 40
- Up to 6.92× higher cache hit rate, 11.96× reduction in latency, 14.06× reduction in time-to-first-token, and 77.4% increase in throughput over state-of-the-art methods

## Executive Summary
This paper tackles the challenge of optimizing multi-LLM serving systems that use KV caching by addressing two core problems: KV cache eviction and query routing. The authors present a unified mathematical framework that captures the fundamental trade-offs between these two problems. They introduce two algorithms: RL T (Randomized Leaf Token eviction) that uses randomization to improve robustness to dynamic query arrivals, and LBGR (Learning-Based Greedy Routing) that adaptively routes queries based on estimated end-to-end latency using online learning. The combined approach significantly outperforms existing methods across multiple benchmarks and prefix-sharing settings.

## Method Summary
The authors propose a unified mathematical model that jointly considers KV cache eviction and query routing in multi-LLM serving systems. The RL T algorithm uses randomization to achieve a worst-case competitive ratio of O(log n) for cache eviction, providing robustness against adversarial arrival patterns. The LBGR algorithm employs an online learning-based strategy that adaptively routes queries by estimating end-to-end latency, balancing the load across multiple LLMs. These algorithms are designed to work together, with the cache eviction strategy informing routing decisions and vice versa, creating a synergistic system that optimizes both cache hit rates and query throughput.

## Key Results
- Up to 6.92× higher cache hit rate compared to state-of-the-art methods
- 11.96× reduction in end-to-end latency
- 14.06× reduction in time-to-first-token latency
- 77.4% increase in system throughput

## Why This Works (Mechanism)
The paper's approach works by addressing the fundamental tension between maximizing cache hit rates (which requires keeping frequently accessed KV pairs in memory) and balancing query load across multiple LLMs (which requires intelligent routing decisions). The randomization in RL T provides robustness against worst-case adversarial arrival patterns, while the learning-based routing in LBGR enables adaptive load balancing based on real-time latency estimates. By jointly optimizing these two components, the system can achieve better overall performance than optimizing them separately.

## Foundational Learning
- KV caching fundamentals: Understanding how transformer models use key-value caches for efficient inference, particularly for handling prefix-sharing queries
  - Why needed: Forms the basis for understanding cache eviction and hit rate optimization
  - Quick check: Verify understanding of how KV caches store intermediate attention results

- Competitive analysis in online algorithms: The theoretical framework for analyzing algorithm performance against adversarial inputs
  - Why needed: Provides the mathematical foundation for proving RL T's performance guarantees
  - Quick check: Review competitive ratio definitions and examples in online algorithm literature

- Online learning and bandit algorithms: The framework for making sequential decisions with partial information
  - Why needed: Underpins the LBGR algorithm's adaptive routing strategy
  - Quick check: Understand the difference between full-information and bandit feedback models

## Architecture Onboarding

**Component Map:**
RL T (Cache Eviction) -> KV Cache Management -> LBGR (Query Routing) -> LLM Serving Pool

**Critical Path:**
Query Arrival -> LBGR Routing Decision -> LLM Execution -> KV Cache Update -> RL T Eviction Decision -> Response Return

**Design Tradeoffs:**
- Randomization vs. determinism in cache eviction: Randomization provides better worst-case guarantees but may sacrifice some best-case performance
- Online learning overhead vs. routing accuracy: More sophisticated learning provides better routing but increases computational overhead
- Cache size vs. hit rate: Larger caches improve hit rates but increase memory costs and eviction complexity

**Failure Signatures:**
- Poor cache hit rates despite randomization: Indicates potential issues with the eviction policy or query arrival pattern assumptions
- Suboptimal routing decisions: Suggests the online learning component is not adapting quickly enough to changing latency patterns
- Increased latency under high load: May indicate the routing algorithm is not effectively balancing load across available LLMs

**First Experiments:**
1. Test RL T alone on synthetic adversarial arrival patterns to validate the O(log n) competitive ratio
2. Evaluate LBGR routing accuracy on a controlled benchmark with known latency variations
3. Measure end-to-end system performance with both algorithms enabled on the primary benchmark

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical analysis assumes adversarial arrival patterns which may not reflect real-world query distributions with temporal correlations
- Learning-based routing component's sensitivity to model architecture changes and hardware heterogeneity not fully explored
- Performance improvements heavily dependent on specific prefix-sharing settings tested
- Computational overhead of online learning component not thoroughly characterized for practical deployment

## Confidence
- High confidence in theoretical contributions (RL T competitive ratio) - rigorous mathematical analysis
- Medium confidence in empirical evaluation results - controlled experimental conditions but limited workload diversity
- Low confidence in practical deployment implications - insufficient characterization of computational overhead and real-world challenges

## Next Checks
1. Test the algorithms on diverse, realistic query arrival patterns with temporal correlations and bursty workloads to validate robustness beyond adversarial assumptions
2. Characterize the computational overhead of the learning-based routing component across different hardware configurations and model sizes
3. Evaluate performance degradation when the latency estimation model encounters previously unseen model architectures or hardware configurations