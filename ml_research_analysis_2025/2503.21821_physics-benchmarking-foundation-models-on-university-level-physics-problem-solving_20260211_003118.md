---
ver: rpa2
title: 'PHYSICS: Benchmarking Foundation Models on University-Level Physics Problem
  Solving'
arxiv_id: '2503.21821'
source_url: https://arxiv.org/abs/2503.21821
tags:
- physics
- reasoning
- problem
- arxiv
- field
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PHYSICS, a benchmark of 1,297 expert-annotated
  university-level physics problems covering six subfields. The benchmark is designed
  to test advanced multi-step reasoning and mathematical modeling, and is notably
  more challenging than existing benchmarks.
---

# PHYSICS: Benchmarking Foundation Models on University-Level Physics Problem Solving

## Quick Facts
- **arXiv ID**: 2503.21821
- **Source URL**: https://arxiv.org/abs/2503.21821
- **Reference count**: 40
- **Primary result**: o3-mini achieves 59.9% accuracy on university-level physics problems, highlighting significant limitations in current foundation models.

## Executive Summary
This paper introduces PHYSICS, a benchmark of 1,297 expert-annotated university-level physics problems spanning six subfields. The benchmark is designed to test advanced multi-step reasoning and mathematical modeling, proving notably more challenging than existing benchmarks. An automated evaluation system using SymPy and GPT-4o is developed to assess model responses. The best-performing model, o3-mini, achieves only 59.9% accuracy, revealing substantial limitations in current models. The paper identifies five key failure patterns: inability to integrate professional knowledge, incorrect assumptions, multimodal data interpretation issues, calculation errors, and misunderstanding problem statements. These findings underscore the need for improved reasoning frameworks, mathematical precision, and better knowledge integration in AI models.

## Method Summary
The study evaluates foundation models on 1,297 university-level physics problems covering six subfields. Problems are split into validation (297) and test (1,000) sets, with 298 multimodal problems including figures. Models are prompted with Chain-of-Thought reasoning and \boxed{} answer format. Evaluation uses a pipeline: regex extracts boxed answers, SymPy performs mathematical equivalence checking, and GPT-4o serves as fallback for natural language or failed symbolic evaluations. Open-source models run via vLLM pipeline while proprietary models use official APIs. Text-only models are evaluated on the text-only subset. RAG experiments use SerpAPI to retrieve top-5 Google results for augmentation.

## Key Results
- o3-mini achieves 59.9% accuracy, the highest among tested models
- Extended CoT models significantly outperform standard models (DeepSeek-R1: 44.3%)
- RAG improves performance across all tested models but doesn't fully bridge the gap
- Physics problems average 5.38 reasoning steps with 24% exceeding 10 steps
- Models show subfield-specific performance gaps, with electromagnetism and quantum mechanics being particularly challenging

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Automated evaluation via symbolic verification (SymPy) combined with GPT-4o fallback enables reliable correctness assessment for physics solutions.
- Mechanism: Regular expressions extract boxed final answers from model outputs; SymPy's `parse_latex` and `is_equiv` functions perform rule-based mathematical equivalence checking. When SymPy fails or returns False, GPT-4o evaluates natural language explanations and conceptual correctness as a fallback.
- Core assumption: Mathematical expressions can be canonicalized sufficiently for symbolic equivalence testing, and GPT-4o provides adequate backup for cases SymPy cannot handle.
- Evidence anchors:
  - [section] "The processed LATEX answers are parsed into symbolic representations using sympy.parse_latex, which enables direct mathematical equivalence checking through the function sympy.is_equiv."
  - [section] "Specifically, GPT-4o is used to evaluate answers that rely on natural language explanations... Additionally, GPT-4o evaluation is employed as a fallback when symbolic computation (e.g., SymPy) returns False or fails to verify an answer."
  - [corpus] Limited direct corpus support; neighbor papers focus on benchmarking and reasoning but not evaluation pipelines.
- Break condition: SymPy parsing fails on non-standard notation or when answers are expressed on the left-hand side of equations, causing extraction errors (see Appendix F example).

### Mechanism 2
- Claim: Extended chain-of-thought (CoT) reasoning models substantially outperform standard models on multi-step physics problems.
- Mechanism: Models trained with explicit reasoning traces (o3-mini: 59.9%, DeepSeek-R1: 44.3%) can decompose complex physics problems into sequential subproblems, apply domain knowledge at each step, and self-correct before finalizing answers.
- Core assumption: Longer reasoning chains correlate with better problem decomposition and error self-correction.
- Evidence anchors:
  - [abstract] "Even the most advanced model, o3-mini, achieves only 59.9% accuracy, highlighting significant challenges in solving high-level scientific problems."
  - [section] "Notably, frontier models such as o3-mini, which incorporate system-2 reasoning, demonstrate a substantial leap in performance compared to other proprietary models."
  - [corpus] Neighbor paper "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs" investigates reasoning models for physics, supporting this mechanism directionally.
- Break condition: Excessively long reasoning chains exceed token limits (observed with DeepSeek-R1 at 10240+ tokens), leading to incomplete answers or computational overhead.

### Mechanism 3
- Claim: Retrieval-Augmented Generation (RAG) partially compensates for models' inability to integrate professional physics knowledge.
- Mechanism: Models generate targeted search queries based on problem content; retrieved external knowledge (top-5 Google results via SerpAPI) is concatenated to the prompt, providing formulas, principles, and context not encoded in model weights.
- Core assumption: Relevant physics knowledge exists in web-accessible sources and can be extracted without overwhelming the context window.
- Evidence anchors:
  - [section] "As shown in Figure 4, the RAG setting improves performance across all tested models."
  - [section] "These findings underscore the potential of retrieval-based augmentation in enhancing model reasoning capabilities."
  - [corpus] Neighbor paper "Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving" directly examines RAG for physics, providing strong external validation.
- Break condition: Retrieved content is irrelevant, contradictory, or introduces noise that confuses the model.

## Foundational Learning

- Concept: **Symbolic mathematics verification**
  - Why needed here: Physics solutions require exact algebraic equivalence, not semantic similarity. Models output different forms (e.g., `sqrt(2gh)` vs `(2gh)^(1/2)`), and symbolic systems handle this canonicalization.
  - Quick check question: Given `expr1 = \frac{P_0}{\rho g}` and `expr2 = \frac{P_0}{g\rho}`, would SymPy recognize them as equivalent?

- Concept: **Multi-step reasoning decomposition**
  - Why needed here: Physics problems average 5.38 reasoning steps with 24% exceeding 10 steps. Models must chain: problem parsing → principle selection → equation setup → algebraic manipulation → numerical evaluation.
  - Quick check question: For a siphon problem requiring Bernoulli's equation at two points plus a cavitation constraint, can you identify the minimum number of reasoning dependencies?

- Concept: **Physics domain priors**
  - Why needed here: Problems omit "obvious" real-world constraints (e.g., reservoir volume >> tube cross-section, non-relativistic limits). Models must infer these from context.
  - Quick check question: What unstated assumption about the reservoir enables treating water level as constant in a siphon problem?

## Architecture Onboarding

- Component map:
  - Data layer: 1,297 problems split into validation (297) / test (1,000), with 523 designated "HARD" subset; 298 problems include figures
  - Model inference layer: vLLM pipeline for open-source models; official APIs for proprietary models; text-only models tested on text-only subset
  - Answer extraction layer: Regex for `\boxed{}` extraction → notation standardization → implication-stripping (content after `⟹`)
  - Verification layer: SymPy equivalence → if False/fail → GPT-4o fallback with math equivalency prompt

- Critical path: Model response → regex extraction → SymPy parse → equivalence check → (if needed) GPT-4o evaluation → accuracy aggregation. Any break in this chain (parsing failure, token overflow, API timeout) requires fallback handling.

- Design tradeoffs:
  - SymPy-first evaluation is deterministic but brittle to notation variations; GPT-4o is flexible but introduces non-determinism
  - CoT prompting improves accuracy but increases latency and token usage
  - RAG adds external knowledge but doubles inference time and risks context dilution

- Failure signatures:
  - **Knowledge integration failure**: Model correctly applies formula but misses implicit physical constraints (Appendix D.2: relativistic correction assumes `<p⁴> = (<p²>)²` incorrectly)
  - **Hallucinated assumptions**: Model introduces conditions not in problem statement (Appendix D.3: assumes rotation cancels thrust)
  - **Calculation cascade failure**: Early algebraic error propagates through multi-step derivation (Appendix D.4: incorrect hyperbolic identity `cosh(x) + sinh(x) = 0`)
  - **Vision parsing error**: Multimodal models misread circuit diagrams or spatial relations (Appendix E: incorrect resistor values)

- First 3 experiments:
  1. **Baseline evaluation**: Run text-only subset on 3 model tiers (small open-source, large open-source, proprietary) with standard CoT prompting; measure accuracy by physics subfield to identify systematic gaps.
  2. **Ablation on reasoning depth**: Compare standard CoT vs. self-reflection prompting (see Appendix C.2) on top-4 models; quantify accuracy gain vs. token cost increase.
  3. **RAG effectiveness stratification**: Implement RAG pipeline (SerpAPI + 5-result context); measure improvement separately for problems tagged as requiring "professional knowledge integration" vs. pure mathematical derivation to identify where retrieval helps most.

## Open Questions the Paper Calls Out

- **Open Question 1**: Can retrieval-augmented generation (RAG) with domain-specific physics knowledge bases achieve parity with human expert performance (70-80% accuracy) on PhD-qualifying physics problems?
  - Basis in paper: [explicit] The authors tested RAG using Google Search and found it improved performance but "does not fully bridge the gap." They explicitly call for "effective integration of physics knowledge sources."
  - Why unresolved: The RAG experiments used general web search (SerpAPI), not curated physics databases. The ceiling for domain-specific retrieval remains unknown.
  - What evidence would resolve it: Comparative experiments using RAG with specialized physics corpora (textbooks, lecture notes, physics encyclopedias) versus general web search on the PHYSICS benchmark.

- **Open Question 2**: What is the reliability ceiling of automated SymPy-based evaluation for physics problem grading, and how does it compare to expert human evaluation?
  - Basis in paper: [explicit] The authors state in Limitations: "this approach may fail to recognize equivalent solutions and introduce subjectivity. Incorporating expert human review could enhance evaluation accuracy." They also provide an example of SymPy evaluation failure in Appendix F.
  - Why unresolved: No systematic comparison between automated and human grading was conducted. The paper acknowledges potential errors but doesn't quantify their frequency or severity.
  - What evidence would resolve it: A human expert annotation study on a subset of model responses, measuring agreement rates between SymPy, GPT-4o fallback, and human physics experts.

- **Open Question 3**: Can multi-modal foundation models be trained or fine-tuned to accurately interpret physics diagrams (circuit schematics, field line plots, experimental setups) without vision errors?
  - Basis in paper: [inferred] The error analysis in Appendix E documents vision errors where models "misinterpreted circuit connections and misread component values," leading to fundamentally incorrect solutions. The paper identifies "difficulties in handling multimodal data" as a key failure pattern but does not propose solutions.
  - Why unresolved: The paper evaluates existing models but does not investigate whether vision capabilities can be improved through physics-specific training data or architectural changes.
  - What evidence would resolve it: Training or fine-tuning experiments with physics diagram datasets, followed by evaluation on the multimodal subset of PHYSICS (298 problems with figures).

## Limitations
- Automated SymPy evaluation is brittle to non-standard notation and left-hand-side answers
- Human baseline methodology lacks detailed experimental specification
- Performance variability across subfields suggests potential dataset bias
- 1,297-problem dataset size may not capture full complexity distribution

## Confidence
- **Mechanism 1** (Automated evaluation via SymPy + GPT-4o): Medium - Evaluation pipeline is well-described but limited by SymPy parsing constraints and GPT-4o's non-deterministic fallback.
- **Mechanism 2** (Extended CoT models outperform standard): High - Performance gap is clear (59.9% vs 44.3%), though causation versus correlation with reasoning depth remains to be validated.
- **Mechanism 3** (RAG partially compensates knowledge gaps): Medium - RAG shows consistent improvement, but the magnitude and reliability of gains across problem types requires further testing.

## Next Checks
1. **Evaluate SymPy extraction robustness**: Test the regex + SymPy pipeline on a subset of problems where answers appear on the left side of equations or use non-standard notation. Measure false positive/false negative rates to quantify evaluation reliability.
2. **Stratify human baseline methodology**: Replicate the human evaluation on a random 50-problem subset with detailed scoring rubrics. Compare inter-annotator agreement and scoring variance to establish confidence intervals around the 70-80% baseline.
3. **Test RAG retrieval relevance**: For problems where RAG improves performance, manually audit the top-5 retrieved results to verify they contain relevant physics knowledge versus general web noise. Quantify the ratio of helpful versus harmful retrievals.