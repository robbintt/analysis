---
ver: rpa2
title: 'From Parameters to Prompts: Understanding and Mitigating the Factuality Gap
  between Fine-Tuned LLMs'
arxiv_id: '2505.23410'
source_url: https://arxiv.org/abs/2505.23410
tags:
- knowledge
- data
- known
- factuality
- unknown
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the factuality gap between large language
  models (LLMs) fine-tuned on known versus unknown factual knowledge. The authors
  conduct systematic experiments on two model architectures (Llama-3.1-8B and Mistral-7B-v0.3)
  and two task types (question answering and open-ended generation).
---

# From Parameters to Prompts: Understanding and Mitigating the Factuality Gap between Fine-Tuned LLMs

## Quick Facts
- **arXiv ID**: 2505.23410
- **Source URL**: https://arxiv.org/abs/2505.23410
- **Authors**: Xuan Gong; Hanbo Huang; Shiyu Liang
- **Reference count**: 30
- **Primary result**: Fine-tuning on unknown knowledge reduces model factuality compared to known knowledge, with gap widening as training progresses, but can be significantly mitigated using in-context learning techniques.

## Executive Summary
This paper investigates a critical phenomenon in LLM fine-tuning: models trained on unknown factual knowledge exhibit reduced factuality compared to those trained on known knowledge. Through systematic experiments across two model architectures (Llama-3.1-8B and Mistral-7B-v0.3) and multiple datasets, the authors demonstrate that this factuality gap persists and widens as training progresses. The gap can be substantially reduced at inference time using in-context learning techniques, particularly few-shot examples and chain-of-thought prompting. Notably, ICL can compensate for limitations in fine-tuning data quality, enabling models trained on small subsets to achieve performance comparable to those trained on full datasets. The authors provide a theoretical framework explaining this gap through a graph-theoretic lens, modeling factual knowledge as knowledge graphs where fine-tuning corresponds to edge completion.

## Method Summary
The authors fine-tune Llama-3.1-8B and Mistral-7B-v0.3 models on known versus unknown factual knowledge splits from EntityQuestions, PopQA, MMLU, and WikiBios datasets. Known/unknown classification is performed using few-shot prompting with 4 examples, greedy and random sampling. Models are trained using SFT with LoRA adapters (batch size 128, learning rates 1e-5 for Llama, 5e-6 for Mistral). Both early-stop (best validation accuracy) and convergence models are evaluated. Factuality is measured using Exact Match for QA tasks and FActScore for open-ended generation. In-context learning mitigation is tested using 4-shot examples from the known split and chain-of-thought reasoning generated by GPT-4o.

## Key Results
- Fine-tuning on unknown knowledge consistently reduces model factuality compared to known knowledge, with gaps of 10-15% on average across architectures and datasets
- The factuality gap widens as training progresses, with convergence models showing larger gaps than early-stop models
- In-context learning (ICL) significantly mitigates the gap, with unknown models gaining more from few-shot examples than known models (e.g., +13.3 vs +3.15 for Llama Base early-stop)
- The gap nearly disappears under strong distributional shift (MMLU OOD with cosine similarity 0.55)
- ICL can compensate for fine-tuning data limitations, enabling small subset models to match full dataset performance

## Why This Works (Mechanism)

### Mechanism 1: Knowledge Graph Edge-Completion via SFT
Fine-tuning adds factual edges to an internal knowledge graph, with differential connectivity explaining the factuality gap. Known knowledge connects already-dense subgraphs (high-degree entities), propagating benefits to semantically similar tokens. Unknown knowledge connects sparse subgraphs (low-degree entities), creating isolated edges with limited generalization. The gap equals the difference in edge counts (Theorem 4.1).

### Mechanism 2: ICL as Temporary Subgraph Injection
In-context learning prompts act as auxiliary knowledge graphs that augment the model's graph during inference. Few-shot examples inject edges connecting demonstration entities to their answers, while CoT prompts create multi-hop reasoning paths. The augmented graph has denser connectivity, particularly benefiting sparse unknown-knowledge graphs (Theorem 5.1).

### Mechanism 3: Distributional Distance and Gap Decay
The factuality gap decreases as test-train distributional distance increases, approaching zero under strong semantic shift. When test distribution diverges from training, both known and unknown knowledge graphs become equally disconnected from test entities, making the difference in edge coverage irrelevant.

## Foundational Learning

- **Concept**: Knowledge Graph Formalization (G = (V, Er, Esim))
  - **Why needed here**: The entire theoretical framework models factual knowledge as directed graphs with entity nodes, relation edges, and similarity edges. Understanding this is prerequisite to interpreting the edge-completion mechanism.
  - **Quick check question**: Given tokens "Paris" and "France" with embeddings ||e_Paris - e_France|| < ε, what edges would exist in Esim connecting these nodes?

- **Concept**: In-Context Learning as Gradient-Free Adaptation
  - **Why needed here**: The paper treats ICL as a test-time intervention that modifies graph connectivity without parameter updates. Distinguishing this from parameter-based learning is essential for understanding why ICL can "override" fine-tuning effects.
  - **Quick check question**: If a model fine-tuned on unknown knowledge receives a few-shot prompt with known examples, which graph structure governs prediction—the fine-tuned G_unk or the augmented G*?

- **Concept**: Knowledge Boundary and Extractability
  - **Why needed here**: The paper operationalizes "unknown knowledge" as facts the model cannot extract under any prompting (Definition 3.1). This boundary determines which training data creates sparse vs. dense graphs.
  - **Quick check question**: A model correctly answers "What is the capital of France?" with zero-shot but fails on "What is the capital of Vanuatu?" with 10-shot prompting. Which is outside the knowledge boundary per Definition 3.1?

## Architecture Onboarding

- **Component map**: EntityQuestions/PopQA/MMLU → Known/Unknown splitter (few-shot filtering) → SFT (LoRA, 1e-5 to 5e-6 LR) → Checkpoints at early-stop and convergence → Test prompt (0-shot / 4-shot / 4-shot+CoT) → Llama-3.1-8B or Mistral-7B-v0.3 → Exact Match / FActScore evaluation

- **Critical path**:
  1. **Data filtering** (Section 3.2): Use few-shot prompting (4 examples, greedy + 16 random samples) to classify triples as Known/Unknown. Critical: each relation subset must be balanced to prevent confounds.
  2. **Training to convergence** (Appendix B.1): Monitor both early-stop (best validation accuracy) and convergence (loss plateau) models, as gap behavior differs.
  3. **Test-time ICL injection** (Section 5.1): Select few-shot examples from Known training data only. For CoT, use GPT-4o to generate entity-level analysis chains.
  4. **OOD validation** (Section 4.2): Compute cosine similarity between ID and OOD test sets using all-MiniLM-L6-v2 embeddings to verify distributional shift levels.

- **Design tradeoffs**:
  - **Fixed vs. fine-tuned embeddings**: Paper assumes fixed non-orthogonal embeddings from pretraining (Appendix A.1). Alternative: updating embeddings during SFT would change Esim structure, potentially creating different generalization patterns—unexplored.
  - **Base vs. Instruct models**: Instruct variants show smaller gaps (Table 1) but are also more susceptible to prompt format changes (Figure 4). Trade-off: alignment reduces gap magnitude but increases prompt sensitivity.
  - **Early-stop vs. convergence**: Early-stop models show gaps that are more easily mitigated by ICL (Table 3). Trade-off: longer training increases memorization but makes test-time correction harder.

- **Failure signatures**:
  - **Gap inversion**: If Unknown > Known performance, check for data leakage (Unknown set contains actually-known facts per few-shot validation) or severe class imbalance.
  - **ICL negative transfer**: If few-shot decreases performance (Table 3, some Mistral-Instruct cases), verify examples are from Known split and semantically similar to test questions.
  - **OOD gap persistence**: If gap doesn't decrease with distributional shift, verify cosine similarity calculation—embeddings may not capture task-relevant semantics for your domain.

- **First 3 experiments**:
  1. **Replicate gap on single dataset**: Fine-tune Llama-3.1-8B on EntityQuestions Known vs. Unknown splits. Evaluate at early-stop and convergence with 0-shot. Verify 10-15% gap (Table 1, Llama Base: 40.30 vs 28.25).
  2. **Test ICL mitigation**: Add 4-shot prompting with Known examples to both models. Verify Unknown gains exceed Known gains (Table 3: +13.3 vs +3.15 for Llama Base early-stop).
  3. **Validate OOD decay**: Evaluate both models on mmlu_ood (cosine similarity 0.55). Confirm gap closes to <2% (Table 2: 66.11 vs 67.05 for Llama early-stop).

## Open Questions the Paper Calls Out

1. **Anomalous behavior on complex datasets**: The paper does not fully explain anomalous behavior on MMLU and WikiBios, which may involve more complex or multimodal factual structures. The simplified graph structure (one-hop connectivity) may not capture the multi-step reasoning or open-ended generation required by these benchmarks.

2. **Evaluation frameworks for data selection**: The paper notes the need to reconsider using ICL prompting to evaluate fine-tuning data selection methods, as ICL can mask the impact of training data quality on model factuality. A new metric or probing method that isolates parameterized knowledge from temporary prompt-based reasoning is needed.

3. **Fixed embedding assumptions**: The theoretical analysis assumes fixed non-orthogonal embeddings, whereas standard fine-tuning often modifies these embeddings. If embeddings shift significantly during training, the definition of similarity edges changes dynamically, potentially invalidating the static graph analysis used to prove the theorems.

## Limitations

- The graph-theoretic framework assumes simplified knowledge structures that may not capture complex reasoning required by benchmarks like MMLU and WikiBios
- The distributional shift relationship is only demonstrated on a single OOD dataset, limiting generalizability of the gap decay mechanism
- The limits of ICL mitigation are not explored—it's unclear whether prompt-based correction can compensate for arbitrarily poor fine-tuning data

## Confidence

**Known vs unknown fine-tuning consistently reduces factuality** (High): Multiple datasets and model architectures consistently show 10-15% gaps between Known and Unknown models, with the gap widening at convergence.

**ICL can significantly mitigate the factuality gap** (High): Few-shot examples and chain-of-thought prompting demonstrate consistent gap reduction, with Unknown models often gaining more than Known models (Table 3 shows +13.3 vs +3.15 for Llama Base early-stop).

**Distributional distance reduces the factuality gap** (Medium): The MMLU OOD experiment shows gap closure from 10.84% to 0.96%, but this single data point doesn't establish a general principle.

## Next Checks

1. **Validate graph-theoretic assumptions**: Test whether token embeddings actually form the Euclidean neighborhoods assumed by the theory. Measure the proportion of test entities that have semantically similar neighbors in both Known and Unknown training sets, and correlate this with factuality performance.

2. **Systematically vary distributional shift**: Beyond MMLU OOD, evaluate the gap across multiple OOD datasets with controlled distributional distances. Measure whether the gap truly follows the predicted decay pattern and identify at what cosine similarity threshold the gap becomes negligible.

3. **Test ICL limits and failure modes**: Design experiments where fine-tuning data quality is deliberately degraded (random noise, contradictory facts, severe class imbalance) and measure whether ICL can still compensate. Identify the threshold where prompt-based mitigation fails and characterize the failure patterns.