---
ver: rpa2
title: 'MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation
  for Colonoscopy Polyp Image Segmentation'
arxiv_id: '2511.01143'
source_url: https://arxiv.org/abs/2511.01143
tags:
- segmentation
- polyp
- microaunet
- attention
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MicroAUNet addresses the challenge of accurate, real-time colonoscopy
  polyp segmentation by proposing a light-weighted attention-based network that combines
  depthwise-separable dilated convolutions with a parameter-shared channel-spatial
  attention block to enhance boundary features. A progressive two-stage knowledge-distillation
  scheme transfers semantic and boundary knowledge from a high-capacity teacher model.
---

# MicroAUNet: Boundary-Enhanced Multi-scale Fusion with Knowledge Distillation for Colonoscopy Polyp Image Segmentation

## Quick Facts
- **arXiv ID**: 2511.01143
- **Source URL**: https://arxiv.org/abs/2511.01143
- **Reference count**: 9
- **Primary result**: State-of-the-art mDice scores of 0.904 (Kvasir-SEG) and 0.902 (CVC-ClinicDB) with only 0.0249M parameters and 0.148 GFLOPs

## Executive Summary
MicroAUNet is a lightweight, attention-based neural network designed for real-time colonoscopy polyp segmentation. It integrates depthwise-separable dilated convolutions and a parameter-shared channel-spatial attention block to enhance boundary features while maintaining efficiency. The model employs a progressive two-stage knowledge distillation scheme to transfer both semantic and boundary knowledge from a high-capacity teacher model. Evaluated on two benchmark datasets, MicroAUNet achieves state-of-the-art accuracy with minimal computational overhead, enabling real-time clinical deployment.

## Method Summary
MicroAUNet addresses the challenge of accurate, real-time colonoscopy polyp segmentation by proposing a light-weighted attention-based network that combines depthwise-separable dilated convolutions with a parameter-shared channel-spatial attention block to enhance boundary features. A progressive two-stage knowledge-distillation scheme transfers semantic and boundary knowledge from a high-capacity teacher model. Evaluated on Kvasir-SEG and CVC-ClinicDB datasets, MicroAUNet achieves state-of-the-art mDice scores of 0.904 and 0.902 respectively, while using only 0.0249M parameters and 0.148 GFLOPs. This demonstrates superior accuracy-efficiency trade-off, enabling real-time clinical deployment. Ablation studies confirm the effectiveness of each proposed component in improving segmentation performance and robustness.

## Key Results
- Achieves mDice scores of 0.904 on Kvasir-SEG and 0.902 on CVC-ClinicDB
- Uses only 0.0249M parameters and 0.148 GFLOPs, demonstrating superior efficiency
- Outperforms state-of-the-art models in accuracy-efficiency trade-off for real-time clinical deployment

## Why This Works (Mechanism)
MicroAUNet leverages depthwise-separable dilated convolutions to capture multi-scale contextual information while reducing computational cost. The parameter-shared channel-spatial attention block enhances boundary features by focusing on relevant spatial and channel-wise information. The progressive two-stage knowledge distillation transfers both semantic and boundary knowledge from a high-capacity teacher model, improving segmentation accuracy. These components work together to achieve high accuracy with minimal computational overhead, enabling real-time deployment.

## Foundational Learning
- **Knowledge Distillation**: A technique where a smaller student model learns from a larger teacher model, improving its performance. *Why needed*: To transfer knowledge from a high-capacity model to a lightweight one. *Quick check*: Verify that the student model achieves comparable accuracy to the teacher model.
- **Depthwise Separable Convolutions**: A convolution operation that separates spatial and channel-wise convolutions, reducing computational cost. *Why needed*: To capture multi-scale contextual information efficiently. *Quick check*: Ensure the model maintains accuracy while reducing parameters and FLOPs.
- **Attention Mechanisms**: Techniques that allow models to focus on relevant parts of the input. *Why needed*: To enhance boundary features and improve segmentation accuracy. *Quick check*: Validate that the attention block improves boundary delineation in the segmentation results.

## Architecture Onboarding
- **Component Map**: Input -> Depthwise Separable Dilated Convolutions -> Channel-Spatial Attention Block -> Segmentation Head
- **Critical Path**: The path from input to output that has the highest computational cost, primarily through the depthwise separable dilated convolutions.
- **Design Tradeoffs**: Balances accuracy and efficiency by using lightweight components and knowledge distillation to achieve high accuracy with minimal computational overhead.
- **Failure Signatures**: Poor performance on small or low-contrast polyps, and potential overfitting to the training dataset.
- **First Experiments**:
  1. Evaluate the model's performance on a held-out test set to assess generalization.
  2. Measure inference latency on target hardware to validate real-time deployment claims.
  3. Test the model's robustness to varying imaging conditions, such as motion blur and poor illumination.

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to diverse clinical environments and endoscope brands remains unverified.
- Real-time deployment claims lack explicit timing benchmarks on target hardware.
- Training data and teacher architecture details are not provided, limiting reproducibility.

## Confidence
- **High confidence**: Parameter efficiency metrics (0.0249M parameters, 0.148 GFLOPs) and comparative mDice scores on benchmark datasets.
- **Medium confidence**: Claims of superior accuracy-efficiency trade-off and real-time deployment potential, pending hardware-specific latency validation.
- **Low confidence**: Generalizability across clinical settings and endoscope hardware, due to limited dataset scope and absence of cross-domain testing.

## Next Checks
1. Conduct cross-dataset evaluation on polyp segmentation benchmarks with varying imaging conditions (e.g., EndoscopyArt, ASU-Mayo Clinic) to assess domain robustness.
2. Measure end-to-end inference latency on representative clinical hardware (e.g., NVIDIA Jetson, Intel iGPU) to validate real-time deployment claims.
3. Test model performance on polyp videos with dynamic motion and varying illumination to evaluate robustness in practical colonoscopy scenarios.