---
ver: rpa2
title: 'InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance
  via Reinforcement Fine-Tuning'
arxiv_id: '2508.19679'
source_url: https://arxiv.org/abs/2508.19679
tags:
- user
- call
- mobile
- action
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the safety challenge in fully autonomous VLM-based
  mobile agents by introducing proactive user inquiry capabilities. It presents InquireBench,
  a comprehensive benchmark with 975 annotated data across 5 categories and 22 sub-categories,
  where existing agents show near-zero performance.
---

# InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning

## Quick Facts
- arXiv ID: 2508.19679
- Source URL: https://arxiv.org/abs/2508.19679
- Reference count: 29
- Primary result: Introduces proactive user inquiry capabilities for VLM-based mobile agents, achieving 46.8% improvement in inquiry success rate (52.6%) and best overall success rate on benchmark

## Executive Summary
This paper addresses the safety challenge in fully autonomous VLM-based mobile agents by introducing proactive user inquiry capabilities. The authors present InquireBench, a comprehensive benchmark with 975 annotated data across 5 categories and 22 sub-categories, where existing agents show near-zero performance. The proposed InquireMobile model employs a two-stage training strategy: supervised fine-tuning for structured output acquisition followed by Group Relative Policy Optimization (GRPO) with rule-based rewards. The model incorporates an interactive pre-action reasoning mechanism to request human assistance at critical decision points. Experimental results demonstrate significant improvements in inquiry success rate while achieving the best overall success rate among baselines on InquireBench.

## Method Summary
InquireMobile uses a two-stage training approach with Qwen2.5-VL-3B as the base model. Stage 1 applies supervised fine-tuning (SFT) with LoRA on combined inquiry and general GUI data to learn structured output formats. Stage 2 applies GRPO with rule-based rewards (format + action type + action argument) to refine when to inquire versus act autonomously. The model generates internal reasoning in <think/> tags before deciding whether to call_user with content or execute mobile_use actions. Training uses 4 generations per sample with temperature=1 for exploration, and evaluation is conducted on InquireBench with 190 tasks across 5 categories.

## Key Results
- 46.8% improvement in inquiry success rate (reaching 52.6%) compared to baselines
- Best overall success rate among all baselines on InquireBench benchmark
- Stage 1 (SFT-only) over-inquires while Stage 2 (GRPO-only) under-inquires; combined approach achieves optimal balance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage training (SFT → GRPO) enables inquiry capability while maintaining general GUI skills.
- Mechanism: Stage 1 teaches structured output format and basic inquiry patterns; Stage 2 refines when to inquire vs. act autonomously through rule-based rewards. The combination prevents both under-inquiry (dangerous) and over-inquiry (annoying).
- Core assumption: The model can transfer inquiry patterns learned from SFT data to novel scenarios through GRPO exploration.
- Evidence anchors: Two-stage training explicitly described in abstract; ablation shows SFT-only over-inquires while GRPO-only under-inquires.

### Mechanism 2
- Claim: Rule-based rewards provide dense enough signal for GRPO without a critic network.
- Mechanism: Three-component reward R = RF + RT + RA gives immediate feedback: format correctness, action type matching, and action argument verification via BLEU/intersection. GRPO normalizes rewards across N responses to compute relative advantages.
- Core assumption: Binary/sparse rewards combined with group normalization can overcome reward sparsity without value function approximation.
- Evidence anchors: Reward function explicitly defined with three components; GRPO described as removing need for separate value function.

### Mechanism 3
- Claim: Interactive pre-action reasoning enables proactive human-in-the-loop at critical decision points.
- Mechanism: The model generates internal reasoning in <think/> tags before deciding whether to call_user with content or execute mobile_use actions. This decouples "when to ask" from "what to do."
- Core assumption: The model can learn to recognize risk/ambiguity patterns from visual context and task instruction.
- Evidence anchors: Interactive pre-action reasoning explicitly mentioned in abstract; 5 categories defined for triggering inquiry.

## Foundational Learning

- Concept: **GRPO (Group Relative Policy Optimization)**
  - Why needed here: Core RL algorithm that avoids training a separate critic by normalizing rewards across N sampled responses per task.
  - Quick check question: Can you explain why GRPO removes the need for a value function compared to PPO?

- Concept: **Rule-based Reward Design**
  - Why needed here: Enables verifiable, binary rewards without learning a reward model.
  - Quick check question: How would you design a reward for the `call_user` action that balances inquiry necessity vs. annoyance?

- Concept: **Action Space Design for Mobile Agents**
  - Why needed here: The `call_user` action must be treated as a first-class action alongside `click`, `type`, etc., with its own reward component.
  - Quick check question: What are the trade-offs between treating `call_user` as a separate action type vs. a post-hoc safety check?

## Architecture Onboarding

- Component map: Qwen2.5-VL-3B (Base VLM) -> SFT stage (LoRA fine-tuning) -> GRPO stage (online RL with rule-based rewards) -> 9-action space including `call_user(content)`, `click(x,y)`, `type(text)`, `terminate(status)`

- Critical path:
  1. Collect inquiry + general GUI data with human annotations
  2. Run SFT on combined dataset for format acquisition
  3. Run GRPO with 4 generations per sample, temperature=1
  4. Evaluate on InquireBench using ISR, SR, and GPT-4o-scored completion

- Design tradeoffs:
  - SFT-only vs. GRPO-only vs. Both: SFT-only over-inquires; GRPO-only under-inquires; both achieves balance
  - Rule-based vs. Learned Rewards: Rule-based is verifiable but may miss nuance; learned rewards are flexible but require large data
  - Small vs. Large Action Space: 9 actions cover core operations but may miss edge cases

- Failure signatures:
  - Over-inquiry: Model asks for confirmation on trivial actions → indicates SFT bias without GRPO refinement
  - Under-inquiry: Model proceeds on payment/delete screens without asking → indicates insufficient risk pattern coverage
  - Invalid coordinates: Click actions miss target bounding boxes → indicates grounding failure

- First 3 experiments:
  1. Ablate reward components: Train with RF only, RF+RT, RF+RT+RA to isolate contribution
  2. Vary inquiry data ratio: Train with 100%, 50%, 25% inquiry data to test balance
  3. Cross-category generalization: Train on 4 categories, test on held-out 5th category

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can inquiry-based training approaches generalize effectively to iOS mobile environments?
- Basis in paper: Testing conducted exclusively on Android devices and emulators
- Why unresolved: iOS has fundamentally different UI paradigms and interaction patterns
- What evidence would resolve it: Evaluation results on iOS devices using comparable benchmark tasks

### Open Question 2
- Question: How can the substantial gap between inquiry success rate (52.6%) and task success rate (7.9%) be closed?
- Basis in paper: Acknowledges unsatisfactory overall performance in task completion and trajectory evaluation
- Why unresolved: Identifies grounding, app-level priors, and in-app page comprehension as key failure modes without proposing solutions
- What evidence would resolve it: Ablation studies showing improved task success rates from interventions targeting foundational capabilities

### Open Question 3
- Question: What are the failure modes and handling strategies for sequential inquiry scenarios?
- Basis in paper: "Combination" category comprises only 8% of samples; no analysis of multi-inquiry trajectories
- Why unresolved: Real-world tasks may require multiple sequential inquiries; repeated interruptions could degrade user satisfaction
- What evidence would resolve it: Analysis of task trajectories containing multiple inquiry-requiring steps with success rates and user satisfaction scores

## Limitations

- Limited generalizability of inquiry patterns to novel domains outside the 5-category taxonomy
- Evaluation artifacts from binary rule-based rewards that may create gaming behavior
- Scale limitations of the base model and training data diversity

## Confidence

- **High Confidence**: Two-stage training achieves better balance than either stage alone (ablation study shows SFT-only over-inquires, GRPO-only under-inquires)
- **Medium Confidence**: Rule-based rewards can provide sufficient signal for GRPO without a critic network
- **Medium Confidence**: Effectiveness of the specific 5-category taxonomy for triggering inquiries

## Next Checks

1. **Cross-domain generalization test**: Train on 4 inquiry categories and test on held-out 5th category, then test on completely novel risk scenarios (medical apps, financial transactions)

2. **Reward sparsity analysis**: Compare GRPO performance with learned reward model vs. current rule-based rewards to measure impact on nuanced inquiry decisions

3. **Long-horizon task evaluation**: Test on extended workflows (10+ steps) involving multiple inquiry points to measure sustained appropriate inquiry behavior over time