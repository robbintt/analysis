---
ver: rpa2
title: Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics
arxiv_id: '2501.08415'
source_url: https://arxiv.org/abs/2501.08415
tags:
- quality
- attack
- metrics
- adversarial
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel cross-modal transferable attack method,
  IC2VQA, for video quality assessment (VQA) models. The key idea is to leverage adversarial
  perturbations generated on white-box image quality assessment (IQA) models with
  an additional CLIP module to effectively target black-box VQA models.
---

# Cross-Modal Transferable Image-to-Video Attack on Video Quality Metrics

## Quick Facts
- arXiv ID: 2501.08415
- Source URL: https://arxiv.org/abs/2501.08415
- Reference count: 15
- Key outcome: IC2VQA achieves up to 0.425 PLCC and 0.380 SRCC reduction on black-box VQA models by transferring adversarial perturbations from white-box IQA models.

## Executive Summary
This paper introduces IC2VQA, a novel cross-modal transferable attack method that generates adversarial perturbations on white-box image quality assessment (IQA) models to effectively attack black-box video quality assessment (VQA) models. The approach leverages the similarity between low-level feature spaces of images and videos, using a cross-layer loss to maximize divergence in intermediate feature representations, along with a temporal loss for stability. The addition of a CLIP module enhances transferability by capturing semantic-visual features that generalize across architectures. Extensive experiments on 12 high-resolution videos and three VQA models demonstrate significant improvements over existing black-box attack strategies.

## Method Summary
IC2VQA generates adversarial perturbations on white-box IQA models (NIMA, PaQ-2-PiQ, SPAQ) and transfers them to attack black-box VQA models (VSFA, MDTVSFA, TiVQA). The method uses a cross-layer loss that computes cosine similarities between feature outputs of IQA and CLIP models, aggregated across multiple layers to maximize feature divergence. A temporal loss ensures frame-to-frame perturbation consistency, improving both imperceptibility and rank correlation attack effectiveness. The attack is optimized using Adam with L∞ constraint, and experiments use a subset of Xiph.org videos downscaled to 540p.

## Key Results
- Achieves up to 0.425 PLCC and 0.380 SRCC reduction on average across three black-box VQA models
- Outperforms existing black-box attack strategies significantly on high-resolution videos
- Adding CLIP module increases attack success by 1.8 times compared to cross-layer loss alone
- Temporal loss improves SRCC attack success by 1.2 times while slightly decreasing PLCC

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Feature Space Similarity Enables Transfer
Adversarial perturbations generated on white-box IQA models transfer to black-box VQA models because low-level feature spaces exhibit significant correlation. The cross-layer loss maximizes divergence in intermediate feature representations across multiple layers, disrupting fundamental quality features (edges, textures, noise patterns) shared between image and video quality assessment models.

### Mechanism 2: CLIP Module Enhances Transfer via Semantic Feature Regularization
Incorporating CLIP's image encoder into the attack loss function improves transferability by capturing semantic-visual features that generalize across diverse architectures. CLIP's visual encoder encodes robust semantic-visual alignments that are less sensitive to task-specific architectural choices than hand-crafted quality features.

### Mechanism 3: Temporal Loss Improves Imperceptibility and Rank Correlation Attack
A temporal regularization term ensures frame-to-frame perturbation consistency, improving both visual imperceptibility and attack effectiveness on rank-based correlation metrics. This produces smoother perturbation sequences that avoid flickering artifacts humans detect and produce more consistent quality score degradations across frames.

## Foundational Learning

- **Concept: No-Reference Quality Assessment (NR-IQA/NR-VQA)**
  - Why needed: Understanding how blind quality metrics extract quality features from distorted content alone is essential for designing perturbations that disrupt those specific features.
  - Quick check: Why does attacking NR-VQA require different strategies than attacking full-reference metrics that compute pixel-wise differences against pristine sources?

- **Concept: Adversarial Transferability Across Modalities**
  - Why needed: The core innovation transfers attacks from image domain (white-box access) to video domain (black-box), requiring understanding of what properties enable cross-modal generalization.
  - Quick check: What must be true about the relationship between IQA and VQA feature spaces for perturbations optimized on IQA gradients to remain effective against an unseen VQA architecture?

- **Concept: Cosine Similarity for Feature-Space Attacks**
  - Why needed: The cross-layer loss uses cosine similarity rather than L2 distance or direct score maximization; understanding why angular distance in feature space is more effective for transfer is critical.
  - Quick check: Why would cosine similarity between feature vectors transfer better across architectures than minimizing the L2 norm of feature differences or maximizing score divergence directly?

## Architecture Onboarding

- **Component map**: Frame Extractor -> [IQA forward pass (save layer k_f features) + CLIP forward pass (save output)] -> Compute L_xlayer + L_CLIP + L_temp -> Adam update δ -> L∞ clip -> Repeat I iterations -> Composite perturbed frames into video

- **Critical path**: Frame extraction → [IQA forward pass (save layer k_f features) + CLIP forward pass (save output)] → Compute L_xlayer + L_CLIP + L_temp → Adam update δ → L∞ clip → Repeat I iterations → Composite perturbed frames into video

- **Design tradeoffs**: Single IQA metric vs. ensemble (single-metric version outperforms weighted ensemble); Layer selection strategy varies by model (NIMA uses post-classifier/pool; PaQ-2-PiQ uses roi-pool/body; SPAQ uses layers 1-4); Epsilon vs. iterations tradeoff affects distortion visibility vs. optimization refinement.

- **Failure signatures**: High PLCC/SRCC after attack (>0.6): Perturbation not transferring—check feature extraction hooks placement; Visible flickering/artifacts: Temporal loss weight insufficient; Attack succeeds on one VQA but not others: Transfer limited to models with shared feature correlations; Over-attacked video (unwatchable): ε too high.

- **First 3 experiments**:
  1. Reproduce baseline transfer: Implement IC2VQA with SPAQ as white-box IQA on Xiph.org subset, target VSFA, verify approximately matches Table 1 values (PLCC ~0.404, SRCC ~0.311).
  2. Ablate CLIP contribution: Compare three configurations—(a) L_xlayer only, (b) L_xlayer + L_CLIP, (c) L_xlayer + L_CLIP + L_temp)—on same videos targeting VSFA, expect ~1.8× improvement when adding CLIP.
  3. Test layer sensitivity: For SPAQ, compare attacking layer 1 vs. layer 4 vs. all four layers, plot PLCC/SRCC vs. layer depth to identify optimal k_f.

## Open Questions the Paper Calls Out

### Open Question 1
How does the selection of specific intermediate layers in the white-box IQA models impact the transferability of the attack to different black-box VQA architectures? The paper manually selects specific layers for different metrics without providing systematic justification or ablation studies.

### Open Question 2
Does the cross-modal transferability of IC2VQA persist against modern transformer-based VQA models that utilize attention mechanisms rather than pure CNN-RNN backbones? The selected black-box target models primarily rely on CNN and recurrent architectures.

### Open Question 3
How robust is the attack success rate when applied to a larger, more diverse dataset containing complex motion patterns and native high resolutions? The experiments were limited to only 10 videos downscaled to 540p.

## Limitations

- Core assumption about IQA and VQA feature space similarity lacks direct empirical validation across diverse VQA models
- Learning rate hyperparameter is unspecified, which could significantly impact reproducibility
- Focus on correlation metrics (PLCC/SRCC) rather than actual perceptual quality degradation leaves open whether attacks produce truly imperceptible videos
- Limited evaluation on small dataset (10 videos) at reduced resolution may not capture real-world variance

## Confidence

- **High confidence**: Ablation study results showing CLIP and temporal loss contributions (1.8× and 1.2× improvements respectively) are well-supported by Table 2 data
- **Medium confidence**: General transferability principle from IQA to VQA models is plausible given correlation analysis but lacks comprehensive cross-model validation
- **Low confidence**: Claim that attacks produce "imperceptible" videos is not rigorously evaluated with human studies or established perceptual metrics

## Next Checks

1. **Feature Correlation Validation**: Replicate Figure 5 analysis for all three target VQA models (VSFA, MDTVSFA, TiVQA) against each IQA model to quantify feature space overlap and predict transferability success.

2. **Human Perceptual Study**: Conduct a user study comparing attacked videos against clean videos to verify that the temporal loss actually produces imperceptible perturbations beyond the SRCC/PLCC metrics.

3. **Cross-Architecture Transfer Test**: Apply the method to VQA models with fundamentally different architectures (e.g., transformer-only vs CNN-based) to identify the boundaries of effective cross-modal transfer.