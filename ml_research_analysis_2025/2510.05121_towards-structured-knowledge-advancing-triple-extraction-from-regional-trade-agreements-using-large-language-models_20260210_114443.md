---
ver: rpa2
title: 'Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade
  Agreements using Large Language Models'
arxiv_id: '2510.05121'
source_url: https://arxiv.org/abs/2510.05121
tags:
- trade
- triples
- language
- prompt
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates Large Language Models (LLMs) for extracting
  Subject-Predicate-Object triples from regional trade agreement texts. Using Llama
  3.1 and iterative prompt engineering, the authors tested zero-shot, one-shot, few-shot,
  and negative-example configurations.
---

# Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models

## Quick Facts
- arXiv ID: 2510.05121
- Source URL: https://arxiv.org/abs/2510.05121
- Reference count: 16
- Primary result: Llama 3.1 achieved F1-score of 0.49 on triple extraction from trade agreements using negative-example prompting

## Executive Summary
This study evaluates Large Language Models for extracting Subject-Predicate-Object triples from regional trade agreement texts. Using Llama 3.1 and iterative prompt engineering, the authors tested zero-shot, one-shot, few-shot, and negative-example configurations. The model processed unstructured legal texts and extracted triples, which were evaluated against a benchmark set curated by domain experts. Results showed progressive improvements in precision, recall, and F1-scores across prompt strategies, with negative examples yielding the highest performance (precision 0.39, recall 0.66, F1 0.49). Qualitative analysis confirmed the model's ability to capture trade-relevant relationships, though challenges in coreference resolution and semantic diversity were noted.

## Method Summary
The study employed Llama 3.1 (70B parameters) with iterative prompt engineering to extract triples from 450 WTO Regional Trade Agreement XML files. The pipeline used LangChain and Ollama for orchestration, testing four prompt strategies: zero-shot, one-shot, few-shot, and negative examples. The approach focused on extracting named entities for subjects/objects and trade-related verbs for predicates. Performance was evaluated against a domain-expert curated benchmark of 100 triples using Exact Match and Semantic Similarity metrics. Data preprocessing included stopword removal and filtering of high-frequency trade terms.

## Key Results
- Progressive improvements across prompt strategies: Zero-shot F1 0.07 → Few-shot F1 0.35 → Negative examples F1 0.49
- Negative examples yielded highest performance: Precision 0.39, Recall 0.66, F1 0.49
- Semantic Match F1 (0.57) exceeded Exact Match F1 (0.49), indicating meaning capture despite phrasing differences
- Model captured trade-relevant relationships but struggled with coreference resolution and predicate diversity

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Incremental prompt complexity improves triple extraction quality in legal domains.
- **Mechanism:** Moving from zero-shot to few-shot prompting provides the LLM with progressive context, allowing it to map generic linguistic patterns to the specific formalism of legal Subject-Predicate-Object structures without weight updates.
- **Core assumption:** The LLM possesses sufficient latent semantic understanding of legal concepts from pre-training, requiring only alignment via context rather than gradient descent.
- **Evidence anchors:** [abstract] "Results showed progressive improvements in precision, recall, and F1-scores across prompt strategies." [section 5] Table 1 shows F1 Score rising from 0.07 (Zero Shot) to 0.35 (Few Shot) on Exact Match.

### Mechanism 2
- **Claim:** Negative examples reduce semantic drift and hallucination better than positive-only examples.
- **Mechanism:** Providing negated instructions explicitly constrains the output space, teaching the model to distinguish between syntactically correct but semantically shallow triples and domain-rich requirements.
- **Core assumption:** The model can effectively utilize negative constraints during inference to suppress the probability of generic or low-information tokens.
- **Evidence anchors:** [abstract] "...negative examples yielding the highest performance (precision 0.39, recall 0.66, F1 0.49)." [section 4] "We also add a few negative examples and negated instructions..."

### Mechanism 3
- **Claim:** Domain-specific verb constraints anchor the extraction process to economic semantics.
- **Mechanism:** By restricting predicates to a specific semantic class (economic trade-related verbs like "ratify", "export"), the model filters out syntactically dominant but semantically irrelevant clauses common in legal boilerplate.
- **Core assumption:** Legal texts contain a high signal-to-noise ratio where "noise" is often grammatically similar to "signal" but distinguishable by verb choice.
- **Evidence anchors:** [section 3] "...added directives for the model to focus on economic trade-related verbs... to enhance the relevance of generated triples." [section 5] Qualitative analysis notes the model captures trade-relevant relationships.

## Foundational Learning

- **Concept: Knowledge Graphs & Triples (Subject-Predicate-Object)**
  - **Why needed here:** This is the target output format. Understanding that the goal is to convert unstructured text into a structured link is fundamental to evaluating the model's success.
  - **Quick check question:** Given the sentence "The agreement was ratified by both parties," what are the likely Subject, Predicate, and Object?

- **Concept: In-Context Learning (Zero/Few-Shot)**
  - **Why needed here:** The study relies on this capability to adapt Llama 3.1 to a niche legal domain without expensive fine-tuning.
  - **Quick check question:** How does "One-shot" prompting differ from "Zero-shot" in terms of input tokens provided to the model?

- **Concept: Precision vs. Recall in Extraction**
  - **Why needed here:** The paper highlights a trade-off where negative examples boost recall but may impact precision.
  - **Quick check question:** If the model extracts 10 triples, but only 4 are correct (out of 8 total possible correct triples), what are the approximate Precision and Recall?

## Architecture Onboarding

- **Component map:** XML files -> Data Cleaning -> LangChain + Ollama -> Llama 3.1 70B -> Iterative Prompt Engineering -> Triple Extraction -> Evaluation Layer
- **Critical path:** Data Cleaning (stopword removal) → Prompt Iteration (Zero → One → Few → Negative) → Triple Filtering (1000 triples max)
- **Design tradeoffs:** Exact Match vs. Semantic Match evaluation; Recall vs. Precision prioritization; Generic vs. Specific prompts
- **Failure signatures:** Coreference Resolution (generic terms like "Parties"); Semantic Diversity (repetitive predicates); Hallucination (generic subjects/objects in zero-shot)
- **First 3 experiments:**
  1. Baseline Run: Execute the Zero-shot prompt on a single RTA document to establish lower bound performance
  2. Negative Constraint Ablation: Run Few-shot vs. Negative Examples prompt on same text to verify semantic diversity improvements
  3. Semantic Threshold Test: Compare Exact Match vs. Semantic Match scores to determine if improvement is synonym use or correct extraction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can coreference resolution be optimized within LLM pipelines to accurately link generic legal terms (e.g., "Parties") to specific sovereign entities in complex trade agreements?
- Basis in paper: [explicit] The conclusion explicitly identifies coreference resolution as a "major drawback" where the model was observed to be "insufficient"
- Why unresolved: The current prompt engineering approach failed to consistently resolve ambiguous references to specific entities
- What evidence would resolve it: A higher "Entity-Relation Coherence" score in qualitative metrics or quantitative increase in exact match recall

### Open Question 2
- Question: To what extent does domain-specific fine-tuning improve the semantic diversity and accuracy of triples compared to the few-shot and negative-example prompting strategies tested?
- Basis in paper: [explicit] The results section states that the model "may require fine-tuning to enhance the diversity of its outputs"
- Why unresolved: The study was limited to prompt engineering and did not experiment with weight updates
- What evidence would resolve it: Comparative benchmarks showing a fine-tuned model outperforming the "Negative Examples Model" (F1 0.49)

### Open Question 3
- Question: Can a standardized ontology of trade-related verbs be integrated into the extraction pipeline to reduce the "overly complex predicates" and redundancy observed in the model outputs?
- Basis in paper: [explicit] The qualitative analysis notes the model generated "overly complex predicates" that reduced usability
- Why unresolved: The current methodology relies on the LLM's free-form generation of English verbs without a restricted schema
- What evidence would resolve it: A reduction in the count of unique predicate stems mapping to the same semantic relation

## Limitations
- Small benchmark set of 100 triples creates uncertainty about generalization to full corpus
- Coreference resolution challenges lead to generic terms instead of specific entity names
- Semantic diversity issues result in repetitive predicates and redundant triples
- Preprocessing step of removing "insignificant high-frequency trade terms" not fully specified

## Confidence
- **High Confidence:** The observed improvement from zero-shot (F1 0.07) to negative examples (F1 0.49) is directly measurable and well-supported by Table 1 results
- **Medium Confidence:** The claim that negative examples reduce hallucination and improve recall is supported by quantitative results but lacks strong corpus evidence
- **Low Confidence:** The assertion that domain-specific verb constraints significantly improve semantic relevance is weakly supported, relying primarily on qualitative analysis

## Next Checks
1. **Benchmark Generalization Test:** Apply the Negative Examples prompt configuration to a different set of 50 triples from the same RTA corpus that were not used in the original evaluation to test performance consistency

2. **Ablation Study on Verb Constraints:** Run the extraction pipeline with and without the specific trade-related verb restrictions to quantify their actual contribution to precision and recall improvements

3. **Coreference Resolution Evaluation:** Manually analyze 30 output triples where the model used generic terms like "Parties" to assess whether this represents a systematic failure or acceptable trade-off given the performance metrics achieved