---
ver: rpa2
title: Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks
arxiv_id: '2510.14844'
source_url: https://arxiv.org/abs/2510.14844
tags:
- lemma
- unlearning
- proof
- have
- point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper provides theoretical analysis of gradient ascent as\
  \ a method for machine unlearning in neural networks. The authors formalize a new\
  \ success criterion called (\u03F5, \u03B4, \u03C4)-successful unlearning based\
  \ on Karush-Kuhn-Tucker (KKT) conditions, which are known to be satisfied by solutions\
  \ obtained through gradient descent on homogeneous models."
---

# Provable Unlearning with Gradient Ascent on Two-Layer ReLU Neural Networks

## Quick Facts
- **arXiv ID**: 2510.14844
- **Source URL**: https://arxiv.org/abs/2510.14844
- **Reference count**: 40
- **Primary result**: Theoretical analysis of gradient ascent for machine unlearning, proving (ϵ, δ, τ)-successful unlearning in linear models and two-layer ReLU networks under near-orthogonal data assumptions.

## Executive Summary
This paper provides theoretical analysis of gradient ascent as a method for machine unlearning in neural networks. The authors formalize a new success criterion called (ϵ, δ, τ)-successful unlearning based on Karush-Kuhn-Tucker (KKT) conditions, which are known to be satisfied by solutions obtained through gradient descent on homogeneous models. The key contributions include proving that gradient ascent with an appropriate step size is (ϵ, δ, τ)-successful for unlearning a single data point in both linear models and two-layer ReLU networks, under assumptions of high-dimensional or nearly-orthogonal data. For linear predictors, they achieve near-exact recovery of the margin-maximizing predictor. For two-layer networks, they prove approximate recovery with small ϵ, δ, and τ parameters. Additionally, the paper demonstrates that gradient ascent unlearning does not compromise generalization performance in a synthetic Gaussian-mixture setting.

## Method Summary
The method involves training a two-layer ReLU network using gradient descent on exponential/logistic loss until convergence to an approximate KKT point, then performing a single gradient ascent step on the forget point using a carefully computed step size derived from KKT multipliers. The unlearning agent is defined as $A_{GA}(\theta, S, r) = \theta + \beta \nabla_\theta \ell(y_r N(x_r, \theta))$, where the step size $\beta = -\lambda_r / \ell'(y_r N(x_r, \theta))$ depends on the KKT multiplier $\lambda_r$ for the forget point. The framework analyzes success through approximate satisfaction of KKT conditions on the retained dataset, with theoretical guarantees provided under assumptions of high-dimensional or nearly-orthogonal data.

## Key Results
- Proves gradient ascent unlearning is (ϵ, δ, τ)-successful for linear models, achieving near-exact recovery of the margin-maximizing predictor.
- Establishes approximate unlearning guarantees for two-layer ReLU networks with small approximation parameters under near-orthogonality assumptions.
- Demonstrates that gradient ascent unlearning preserves generalization performance in synthetic Gaussian-mixture settings.
- Shows empirical validation that deviating from optimal step size increases approximation error.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** A single gradient ascent step shifts the model parameters to satisfy approximate Karush-Kuhn-Tucker (KKT) conditions for the retained dataset.
- **Mechanism:** Gradient descent on exponential/logistic losses exhibits an implicit bias toward margin maximization, converging to a KKT point where weights are a weighted sum of training data gradients. By taking a gradient ascent step on the forget point, the algorithm effectively subtracts the forget point's contribution from this weighted sum, leaving a representation closer to the KKT point of the retain set.
- **Core assumption:** The original model is an $(\epsilon, \delta)$-approximate KKT point for the full dataset, and the data satisfies near-orthogonality (Assumption 2.3).
- **Evidence anchors:**
  - [abstract] "Leveraging the implicit bias of gradient descent... we quantify the quality of the unlearned model by evaluating how well it satisfies these conditions w.r.t. the retained data."
  - [section 2.1] Theorem 2.1 establishes that gradient flow converges in direction to a KKT point.
  - [corpus] "Directional Convergence, Benign Overfitting of Gradient Descent..." corroborates the convergence to KKT points in similar architectures.
- **Break condition:** Fails if the training dynamics have not converged to an approximate KKT solution (e.g., early stopping or inconsistent training).

### Mechanism 2
- **Claim:** Near-orthogonality of high-dimensional data limits the "collateral damage" to the margins of retained data points during the unlearning step.
- **Mechanism:** The stationarity condition expresses weights as a sum of data gradients. When subtracting the forget point's gradient, the change in the margin for a retained point depends on the inner product between the forget and retain point. If this inner product is small (near-orthogonality), the margin violation is bounded by $O(\frac{\epsilon_d}{m})$.
- **Core assumption:** Data points satisfy $|\langle x_i, x_j \rangle| \le \phi$ (Assumption 2.3) and norms are bounded.
- **Evidence anchors:**
  - [section 3] "Due to the near-orthogonality of the data points in S... this difference is of order $O(\frac{\epsilon_d}{m})$."
  - [section 2.4] Assumption 2.3 formalizes the near-orthogonality condition required for the proofs.
  - [corpus] *Corpus evidence for this specific mechanism is weak; the related "Ascent Fails to Forget" actually warns that gradient ascent fails when data is correlated/dependent, reinforcing the necessity of the orthogonality assumption.*
- **Break condition:** Fails if data points are highly correlated or aligned, causing the unlearning step to degrade retained margins significantly.

### Mechanism 3
- **Claim:** In synthetic Gaussian-mixture settings, gradient ascent unlearning preserves generalization because the gradient directions align with class structure.
- **Mechanism:** In the Gaussian mixture $D_{MG}$, inner products between points depend on class membership (same vs. different cluster). The stationarity condition ensures that training points contribute gradients consistent with their class sign. Unlearning preserves this sign alignment, maintaining high probability of correct classification on test data.
- **Core assumption:** Data is drawn from a mixture of Gaussians with small cluster means and large variance relative to dimension.
- **Evidence anchors:**
  - [section 6] Theorem 6.1 states that both original and unlearned models generalize with high probability.
  - [section 6] "The inner product sign is determined by whether two points belong to the same or different clusters."
- **Break condition:** Fails if the distribution structure does not align gradient contributions with class labels (e.g., non-clustered or highly non-linear manifolds).

## Foundational Learning

- **Concept: KKT (Karush-Kuhn-Tucker) Conditions**
  - **Why needed here:** This is the mathematical language used to define "success." You cannot interpret Theorem 3.1 or 4.1 without understanding that a KKT point represents an optimal solution to the margin maximization problem (Eq 2) constrained by correct classification.
  - **Quick check question:** Can you explain why satisfying the "stationarity" condition implies the weights are a linear combination of the data gradients?

- **Concept: Implicit Bias of Gradient Descent**
  - **Why needed here:** The entire proof relies on the premise that the trained model $\theta$ isn't just any predictor, but one that implicitly minimizes the margin maximization problem. Without this, the gradient ascent step has no theoretical anchor to the "retain" optimal solution.
  - **Quick check question:** Why does minimizing logistic loss on separable data lead to a max-margin solution rather than just zero loss?

- **Concept: Two-Layer ReLU Networks & Homogeneity**
  - **Why needed here:** The proofs utilize the property that $N(\alpha \theta, x) = \alpha^C N(\theta, x)$. For 2-layer ReLU nets, $C=1$. This homogeneity is crucial for Lemma B.3, allowing scaling of the unlearned weights to restore feasibility (margin $\ge 1$) without changing the direction.
  - **Quick check question:** If you scale the weights of a ReLU network by a factor of 2, how does the output change? How does the gradient change?

## Architecture Onboarding

- **Component map:** Input vector $x \in \mathbb{R}^d$ -> Two-layer ReLU network $N(\theta, x) = \sum u_j \sigma(w_j^\top x)$ -> Unlearning agent $A_{GA}(\theta, S, r) = \theta + \beta \nabla_\theta \ell(y_r N(x_r, \theta))$

- **Critical path:**
  1. **Train:** Run gradient flow/descent until the model reaches an approximate KKT point (high accuracy, low loss).
  2. **Extract Lambda:** Identify the KKT multiplier $\lambda_l$ for the forget point. Assumption: $\lambda_l$ correlates with the gradient magnitude. The paper computes the theoretical step size $\beta = -\lambda_l / \ell'(\cdot)$.
  3. **Update:** Apply one gradient ascent step with magnitude $\beta$.

- **Design tradeoffs:**
  - **Exact vs. Approximate:** The method yields an *approximate* KKT point for the retain set (parameters $\epsilon, \delta, \tau$ are small but non-zero), not the exact retrained model.
  - **Data Regime:** The proofs require near-orthogonality ($d \gg m$). Performance may degrade on dense, correlated datasets (supported by corpus warning in "Ascent Fails to Forget").
  - **Step Size Sensitivity:** Theorem 4.1 relies on a "carefully chosen" step size. Figure 1 shows deviation leads to worse approximation.

- **Failure signatures:**
  - **Activation Flipping:** In 2-layer nets, the gradient step might flip the activation status of neurons ($w_j^\top x_i$ changes sign). The proof constructs a hypothetical "fixed" vector $\tilde{\theta}$ to handle this, but the actual algorithm $A_{GA}$ does not explicitly fix activations.
  - **Batch Unlearning:** While extended to sets (Appendix B.2, C.3), the approximation errors ($\epsilon, \delta$) accumulate with the size of the forget set $k$.

- **First 3 experiments:**
  1. **Synthetic Validation:** Generate data from $N(0, \frac{1}{d} I_d)$. Train a linear model. Compute the cosine similarity between $A_{GA}$ and the retrained model. Verify it approaches $1 - O(\sqrt{\epsilon_d})$ as dimension $d$ increases.
  2. **Step Size Ablation:** Replicate Figure 1. Unlearn a point in a 2-layer ReLU net while varying the step size $\alpha \beta$. Plot the resulting KKT approximation parameter $\epsilon$ against the step size to confirm the "Goldilocks" zone.
  3. **Orthogonality Stress Test:** deliberately introduce correlation into the dataset (violating Assumption 2.3) and measure the degradation in cosine similarity between the unlearned and retrained models.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does adding a recovery phase for retained data (as in NegGrad+) affect unlearning success under the (ε, δ, τ) criterion?
- **Basis in paper:** [explicit] "while we focus on a gradient-ascent step, it would be valuable to analyze the effect of an additional recovery phase for the retain data, including those used in NegGrad+ and related variants, under the same KKT-based framework."
- **Why unresolved:** This paper analyzes only a single gradient-ascent step; recovery phases are empirically used but lack theoretical characterization in this framework.
- **What evidence would resolve it:** Theoretical analysis showing how KKT approximation parameters (ε, δ, τ) evolve with additional recovery steps, or empirical comparison of with/without recovery.

### Open Question 2
- **Question:** What is the quantitative relationship between (ε, δ, τ)-successful unlearning and membership inference attack success rates?
- **Basis in paper:** [explicit] "it would be interesting to develop tighter bounds connecting approximate KKT satisfaction with practical privacy metrics, such as membership inference risk."
- **Why unresolved:** The paper's criterion is theoretical; the connection to practical privacy guarantees remains qualitative.
- **What evidence would resolve it:** Derivation of bounds linking ε, δ, τ parameters to membership inference attack accuracy, or empirical demonstration of correlation between KKT satisfaction and attack success.

### Open Question 3
- **Question:** Can the theoretical guarantees for gradient ascent unlearning be extended to networks deeper than two layers?
- **Basis in paper:** [explicit] "extending our results to deeper architectures and additional distributions remains an important challenge."
- **Why unresolved:** The analysis relies on specific properties of two-layer ReLU networks (e.g., tractable activation pattern analysis); the proof techniques do not trivially extend.
- **What evidence would resolve it:** Theoretical results bounding (ε, δ, τ) for L-layer networks with L>2, or identification of which proof steps fail for deeper architectures.

### Open Question 4
- **Question:** How robust is gradient ascent unlearning when the training data violates the near-orthogonality assumption (Assumption 2.3)?
- **Basis in paper:** [inferred] The theoretical results depend critically on Assumption 2.3, which requires |⟨x_i, x_j⟩| ≤ ϕ for i≠j. Real-world datasets often have correlated features.
- **Why unresolved:** The bounds contain terms like O(ε_d/m) that grow with data non-orthogonality (ϕ), but the degradation rate for realistic correlations is unknown.
- **What evidence would resolve it:** Experiments on natural datasets (images, text) measuring (ε, δ, τ) parameters, or theoretical analysis with relaxed orthogonality bounds.

## Limitations
- The theoretical framework relies heavily on Assumption 2.3 (near-orthogonality of data points) and requires high-dimensional settings where $d \gg m$, limiting practical applicability to datasets with inherent low-dimensional structure or strong correlations.
- The analysis assumes the pre-unlearning model has already converged to an approximate KKT point, which may not hold with early stopping or noisy training.
- The step size $\beta$ is explicitly derived from the KKT multiplier $\lambda_r$, making the method sensitive to accurate estimation of this quantity.

## Confidence
- **High Confidence:** The directional convergence of gradient descent to KKT points for two-layer ReLU networks (Mechanism 1).
- **Medium Confidence:** The unlearning quality bounds for linear models (Theorem 3.1) and the cosine similarity approximation for two-layer networks (Theorem 4.1).
- **Low Confidence:** The generalization preservation claims for the synthetic Gaussian-mixture setting (Theorem 6.1).

## Next Checks
1. **KKT Multiplier Estimation:** Implement and validate the algorithm for computing KKT multipliers $\lambda_i$ from trained model parameters. Compare estimates against ground truth from the dual problem on synthetic data.
2. **Correlation Sensitivity:** Systematically vary the correlation structure in synthetic datasets (violating Assumption 2.3) and measure degradation in unlearning quality metrics ($\epsilon, \delta, \tau$) to quantify the near-orthogonality assumption's practical limits.
3. **Real-World Dataset Testing:** Apply the method to a high-dimensional, nearly-orthogonal dataset (e.g., text embeddings) and measure unlearning quality against full retraining, while also testing on a correlated dataset to observe performance degradation.