---
ver: rpa2
title: 'The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a
  Multi-Step Malware'
arxiv_id: '2601.09625'
source_url: https://arxiv.org/abs/2601.09625
tags:
- prompt
- injection
- data
- persistence
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a systematic framework for analyzing attacks
  on LLM-based applications, which the authors term "promptware." They argue that
  current framing of these threats as "prompt injection" is inadequate because it
  obscures the multi-step nature of real attacks, which mirror traditional malware
  campaigns. The authors propose a five-step kill chain model: Initial Access (prompt
  injection), Privilege Escalation (jailbreaking), Persistence (memory and retrieval
  poisoning), Lateral Movement (cross-system and cross-user propagation), and Actions
  on Objective (data exfiltration, unauthorized transactions, etc.).'
---

# The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware

## Quick Facts
- arXiv ID: 2601.09625
- Source URL: https://arxiv.org/abs/2601.09625
- Reference count: 11
- Primary result: Systematic framework for analyzing LLM attacks as multi-step "promptware" campaigns

## Executive Summary
This paper introduces a five-stage kill chain framework for analyzing attacks on LLM-based applications, arguing that current "prompt injection" framing is inadequate for capturing the systematic, multi-stage nature of real-world attacks. The authors demonstrate that LLM-related attacks follow sequences analogous to traditional malware campaigns, requiring security practitioners to adopt structured threat modeling approaches. By mapping recent attacks to their proposed framework, they provide a common vocabulary for addressing rapidly evolving threats across AI safety and cybersecurity communities.

## Method Summary
The paper analyzes seven documented attacks/studies and maps them to a proposed five-stage kill chain model (Initial Access, Privilege Escalation, Persistence, Lateral Movement, Actions on Objective). The method involves qualitative classification of attack behaviors and techniques against each stage's definition, validating whether documented attacks fit the framework's structure. Source materials include academic papers and blog posts referenced in Table I and footnotes.

## Key Results
- Current "prompt injection" framing obscures the multi-step nature of LLM attacks
- Five-stage kill chain (Initial Access → Privilege Escalation → Persistence → Lateral Movement → Actions on Objective) effectively categorizes documented attacks
- LLM applications' utility features (broad permissions, memory systems, cross-system integrations) become attack pathways when compromised

## Why This Works (Mechanism)

### Mechanism 1: Instruction-Data Conflation
- Claim: LLMs process all inputs as undifferentiated token sequences, creating an unpatchable vulnerability where malicious instructions cannot be architecturally distinguished from legitimate data.
- Mechanism: All input—system prompts, user messages, retrieved documents—is tokenized into unified sequences with no native enforcement boundary between trusted instructions and untrusted data. Guardrails operate at the application layer as pattern-matching defenses, not as fundamental architectural separations.
- Core assumption: This architectural property is inherent to current LLM design and cannot be comprehensively patched.
- Evidence anchors:
  - [abstract] "existing security frameworks inadequately address" this new attack surface
  - [section II] "No native mechanism currently exists to reliably enforce a distinction between instructions and data. This is not a bug amenable to patching; it is an inherent property of the architecture."
  - [corpus] "Defeating Prompt Injections by Design" proposes defensive system layers, implicitly acknowledging the underlying architectural gap.
- Break condition: If future architectures implement native instruction/data separation at the token processing level.

### Mechanism 2: Persistence via Stateful Components
- Claim: Malicious payloads can establish durable footholds by corrupting memory systems (RAG databases, agent memory) that survive session boundaries.
- Mechanism: Retrieval-dependent persistence exploits RAG systems—payloads execute when semantic similarity triggers retrieval. Retrieval-independent persistence targets agent memory features that inject content into every context window unconditionally. Command-and-control variants fetch remote instructions dynamically.
- Core assumption: LLM applications increasingly rely on stateful components for utility, creating persistence opportunities.
- Evidence anchors:
  - [abstract] "Persistence (memory and retrieval poisoning)"
  - [section IV.B] "Once persisted, this content influenced all future interactions without requiring retrieval or semantic similarity. This constitutes retrieval-independent persistence."
  - [corpus] Weak corpus evidence on persistence mechanisms specifically; corpus focuses on injection and inference attacks.
- Break condition: If memory systems implemented strict content validation, provenance tracking, or isolation boundaries.

### Mechanism 3: Cross-Boundary Propagation via Permission Exploitation
- Claim: Broad permissions granted to LLM agents for utility become attack pathways when compromised, enabling self-replicating, permission-based, and pipeline-based lateral movement.
- Mechanism: Self-replicating movement embeds payloads in outgoing content (emails, documents). Permission-based movement exploits unified authentication contexts (e.g., OS-level assistant permissions). Pipeline-based movement traverses data flows between integrated systems (Zendesk→Jira→Cursor).
- Core assumption: Utility maximization drives permission scope, and cross-boundary integrations are common in production deployments.
- Evidence anchors:
  - [abstract] "Lateral Movement (cross-system and cross-user propagation)"
  - [section V] "LLM applications are granted broad access to data and capabilities to maximize their utility, but these very cross-boundary integrations that make AI applications useful become a liability when the application is compromised."
  - [corpus] "Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems" confirms chained attack patterns in enterprise settings.
- Break condition: If applications implemented strict permission boundaries, human-in-the-loop confirmation for cross-boundary actions, or isolated integration contexts.

## Foundational Learning

- Concept: Kill Chain Methodology
  - Why needed here: Transforms monolithic "prompt injection" framing into structured multi-stage analysis, enabling targeted defensive intervention at each phase.
  - Quick check question: Can you name the five stages and explain why treating prompt injection as only "initial access" changes defensive priorities?

- Concept: Retrieval-Augmented Generation (RAG) Security Model
  - Why needed here: RAG is both an indirect injection vector (initial access) and a persistence target. Understanding retrieval triggers is essential for threat modeling.
  - Quick check question: How does semantic similarity affect the decay rate of retrieval-dependent persistence?

- Concept: LLM Alignment as Privilege Boundary
  - Why needed here: Safety training (RLHF) constrains model capabilities; jailbreaking circumvents these constraints. This reframes alignment failures as privilege escalation rather than input validation failures.
  - Quick check question: Why does the paper distinguish jailbreaking (attacking the model's safety filters) from prompt injection (exploiting context concatenation)?

## Architecture Onboarding

- Component map:
  - Context Window → unified token sequence processing point (no instruction/data boundary)
  - RAG/External Data Sources → indirect injection vectors + persistence targets
  - Agent Memory → persistence targets (retrieval-independent)
  - Tool Integrations → lateral movement channels + action enablers
  - Permission Scope → damage potential limiter

- Critical path: Untrusted Input → Context Window (concatenation) → LLM Inference → Tool/Agent Execution
  - Key vulnerability: No boundary enforcement at concatenation stage; guardrails are post-hoc pattern matching

- Design tradeoffs:
  - Utility vs. Security: Broader permissions increase utility but expand lateral movement surface
  - Automation vs. Oversight: Autonomous agents remove friction but eliminate human confirmation checkpoints
  - Statefulness vs. Isolation: Memory/RAG improves experience but enables persistence

- Failure signatures:
  - Unexpected tool invocations triggered by retrieved content (not user request)
  - Behavior changes persisting across sessions without user action
  - Cross-application actions originating from single compromised input source
  - Data exfiltration embedded within normal-looking agent activity

- First 3 experiments:
  1. **Indirect injection test**: Plant poisoned document in RAG source, query system with semantically related prompt, observe if unauthorized instructions execute.
  2. **Persistence validation**: Inject payload into agent memory, verify execution across 3+ distinct sessions without re-injection; test retrieval-dependent vs. retrieval-independent decay rates.
  3. **Lateral surface mapping**: Document all tool integrations, unified auth contexts, and cross-system data pipelines; trace propagation pathways from each entry point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can future LLM architectures be designed to natively enforce a distinction between trusted instructions and untrusted data, or is the "Initial Access" vulnerability an inherent and unfixable property of current token-processing paradigms?
- Basis in paper: [explicit] The paper states that "no patch can resolve this inherent property of LLM architecture" and "no known comprehensive solution" exists for the inability to distinguish instructions from data.
- Why unresolved: The paper argues that because LLMs process all input as undifferentiated token sequences, application-layer guardrails are insufficient, leaving a fundamental architectural flaw that security researchers have not yet overcome.
- What evidence would resolve it: The demonstration of a model architecture or training methodology that can mathematically guarantee separation between control plane (instructions) and data plane (content) inputs without relying on probabilistic filters.

### Open Question 2
- Question: What specific defensive controls are most effective at breaking the "Persistence" and "Lateral Movement" phases of the kill chain, assuming that "Initial Access" via prompt injection cannot be fully prevented?
- Basis in paper: [inferred] The authors conclude that "practitioners must focus on limiting privilege escalation, preventing persistence, [and] constraining lateral movement" because Initial Access is inevitable, but they do not validate specific technical defenses for these later stages.
- Why unresolved: While the paper provides a taxonomy for analyzing attacks, it documents the failure of current guardrails (input filters, safety training) but does not propose or test specific countermeasures for interrupting the kill chain once the context window is compromised.
- What evidence would resolve it: Empirical studies testing specific interventions—such as signed memory hashes, agent permission sandboxing, or anomaly detection in tool usage—that successfully stop the progression from Persistence to Actions on Objective.

### Open Question 3
- Question: Does the five-stage linear kill chain model sufficiently capture attack scenarios in complex, multi-agent systems where stages may execute concurrently or merge?
- Basis in paper: [explicit] The authors caveat their model by stating: "We do not claim that the five-stage model captures every possible attack scenario, nor that the boundaries between stages are always sharp. Attackers may skip stages, combine them... execute them sequentially or concurrently."
- Why unresolved: The current framework is adapted from traditional cyber kill chains (linear), but the paper notes that autonomous agents or pipeline-based movements (like AgentFlayer) might exhibit non-linear or instantaneous propagation behaviors that challenge the sequential stage model.
- What evidence would resolve it: Analysis of promptware attacks in multi-agent ecosystems to determine if attacks strictly follow the five-stage progression or if novel, non-linear attack trajectories require a revised framework.

## Limitations

- Evidence Granularity: Framework validation relies on mapping documented attacks to stages, but source attacks vary widely in documentation quality with some existing only in blog posts or preprints.
- Architectural Generalization: Claims about unpatchable instruction-data conflation assume current transformer-based architectures remain dominant.
- Cross-Domain Applicability: Framework may not uniformly apply across different LLM deployment contexts (enterprise vs. consumer vs. autonomous agents).

## Confidence

**High Confidence**: Instruction-data conflation vulnerability (Mechanism 1) with extensive technical analysis and corpus support; Cross-boundary propagation via permission exploitation (Mechanism 3) well-documented through multiple attack examples.

**Medium Confidence**: Persistence mechanisms (Mechanism 2) with solid theoretical grounding but relatively weak corpus evidence; Kill chain framework's practical utility demonstrated through attack case studies but lacking quantitative defensive validation.

**Low Confidence**: Systematic sequence claim relies heavily on qualitative mapping rather than quantitative pattern analysis across broader attack corpus.

## Next Checks

1. **Granular Attack Classification Audit**: Systematically re-classify all seven reference attacks using the paper's kill chain definitions, documenting confidence scores and edge case handling for each stage assignment. Compare classifications across multiple reviewers to establish inter-rater reliability.

2. **Architecture-Specific Persistence Validation**: Design controlled experiments testing retrieval-dependent vs. retrieval-independent persistence decay rates across three different LLM application architectures (RAG-only, agent-memory, hybrid systems) with quantitative measurements of persistence duration and propagation distance.

3. **Cross-Boundary Permission Mapping**: Document and test lateral movement surfaces across 10+ production LLM applications, measuring permission scope breadth, integration context isolation, and human-in-the-loop checkpoint frequency. Quantify the relationship between permission breadth and lateral movement attack surface area.