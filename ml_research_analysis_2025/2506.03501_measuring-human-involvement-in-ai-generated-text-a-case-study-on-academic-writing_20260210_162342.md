---
ver: rpa2
title: 'Measuring Human Involvement in AI-Generated Text: A Case Study on Academic
  Writing'
arxiv_id: '2506.03501'
source_url: https://arxiv.org/abs/2506.03501
tags:
- human
- text
- involvement
- generated
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a method to measure human involvement in
  AI-generated academic text using BERTScore and a dual-head RoBERTa-based regression
  model. Unlike traditional binary classifiers, the approach quantifies human contribution
  as a continuous value and identifies human-contributed words, addressing limitations
  of existing detectors when applied to collaborative human-AI writing.
---

# Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing

## Quick Facts
- arXiv ID: 2506.03501
- Source URL: https://arxiv.org/abs/2506.03501
- Reference count: 33
- Introduces method to quantify human contribution in AI-generated academic text with F1 score of 0.9423

## Executive Summary
This paper addresses the critical challenge of measuring human involvement in AI-generated academic text, moving beyond binary detection to quantify the degree of human contribution. The authors propose a novel approach using BERTScore to establish ground truth labels and a dual-head RoBERTa regression model to predict continuous human involvement scores. The method not only estimates overall human contribution but also identifies specific words contributed by humans, making it particularly valuable for scenarios involving collaborative human-AI writing. The approach demonstrates strong performance on a newly created Continuous Academic Set (CAS-CS) dataset with varying levels of human input.

## Method Summary
The methodology involves a two-stage process: first, establishing ground truth labels using BERTScore recall to measure similarity between human-written and mixed human-AI text; second, training a dual-head RoBERTa regression model to predict both continuous human involvement scores and identify human-contributed words. The model uses a continuous labeling approach where human involvement is quantified as a value between 0 and 1, representing the degree of human contribution in mixed text. The CAS-CS dataset was created by combining human-written abstracts with AI-generated content at varying proportions (0-100% human involvement in 10% increments) using three prompt templates. The dual-head architecture enables simultaneous prediction of overall involvement and word-level human contribution.

## Key Results
- The detector achieved an F1 score of 0.9423 and mean squared error of 0.004 on the CAS-CS dataset
- All existing detectors failed on the same dataset, highlighting the inadequacy of binary classification for collaborative writing
- The method demonstrated cross-model generalization across different LLMs (ChatGPT, GPT-4, Claude-3, Gemini) and maintained robustness across diverse prompt templates
- Human evaluation confirmed consistency between the regression label and human judgment with Spearman correlation of 0.64

## Why This Works (Mechanism)
The approach works by recognizing that human involvement in AI-generated text exists on a spectrum rather than as a binary state. By using BERTScore recall as a continuous metric for measuring semantic similarity between human-written and mixed text, the method captures nuanced degrees of human contribution. The dual-head RoBERTa architecture is specifically designed to handle this continuous nature, with one head predicting the overall involvement score and another identifying human-contributed words. This framework addresses the fundamental limitation of existing detectors that are optimized for distinguishing entirely human versus entirely AI-generated text, which fail when applied to collaborative writing scenarios where both contributions are present.

## Foundational Learning
- **BERTScore-based continuous labeling**: A method for quantifying human involvement by measuring semantic similarity between human and mixed text using BERTScore recall. Why needed: Traditional binary labeling cannot capture the nuanced degrees of human contribution in collaborative writing. Quick check: Verify that BERTScore recall values correlate with human judgments of involvement.
- **Dual-head RoBERTa regression architecture**: A neural network design with two output heads - one for continuous involvement prediction and another for word-level human contribution identification. Why needed: Different aspects of human involvement require different prediction targets in the same model. Quick check: Ensure both heads are trained with appropriate loss functions for their respective tasks.
- **Continuous Academic Set (CAS-CS) creation**: A methodology for generating a dataset with controlled levels of human involvement by mixing human-written and AI-generated abstracts. Why needed: Existing datasets only contain binary labels, making them unsuitable for training regression models. Quick check: Confirm that each involvement level (0-100%) has sufficient samples for robust training.

## Architecture Onboarding

**Component Map**: BERTScore calculation -> CAS-CS dataset creation -> Dual-head RoBERTa regression model -> Involvement prediction

**Critical Path**: The most critical sequence is BERTScore calculation → dataset creation → model training → prediction. Any error in establishing ground truth labels through BERTScore will propagate through the entire system, affecting model performance.

**Design Tradeoffs**: The use of BERTScore as a ground truth creates a dependency on the quality of the similarity metric, which may not perfectly align with human perception of involvement. The continuous labeling approach requires more complex data preparation compared to binary labeling but enables more nuanced detection. The dual-head architecture increases model complexity but provides both sentence-level and word-level insights.

**Failure Signatures**: 
- Low F1 scores on CAS-CS indicate issues with the regression model or insufficient training data
- High MSE values suggest poor correlation between predicted and actual involvement levels
- Inconsistent word-level predictions may indicate problems with the attention mechanism or token-level labeling

**Three First Experiments**:
1. Train the dual-head model on a subset of CAS-CS (e.g., 50-100% involvement levels) and evaluate performance compared to using all levels
2. Replace BERTScore with alternative similarity metrics (e.g., cosine similarity on embeddings) to assess impact on ground truth quality
3. Test the model on binary-labeled datasets to quantify performance degradation compared to existing binary detectors

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How can the methodology be adapted to accurately measure human involvement in full-length academic papers that exceed the input token limitations of the RoBERTa architecture?
- Basis in paper: [explicit] The conclusion states that "RoBERTa has an upper limit on text length, preventing it from processing long texts, such as full papers."
- Why unresolved: The current model is restricted to processing abstracts or shorter segments due to the fixed input size of the underlying transformer architecture.
- What evidence would resolve it: A modified model or architecture that processes full documents and achieves comparable Mean Squared Error (MSE) scores on a dataset of full-length papers.

### Open Question 2
- Question: Does the regression model maintain high accuracy when applied to academic writing in domains other than Computer Science?
- Basis in paper: [explicit] The authors note that the "training and validation sets are focused on computer science" and they "did not systematically create datasets for testing in other fields."
- Why unresolved: The specific vocabulary and structure of CS abstracts may have biased the model, and it is unknown if the BERTScore-based metric generalizes to disciplines with different writing conventions.
- What evidence would resolve it: Evaluation of the current model on a "Continuous Academic Set" constructed from Humanities, Social Sciences, or Natural Sciences corpora.

### Open Question 3
- Question: Can the ground-truth labeling be improved by replacing or augmenting the BERTScore-based metric with human survey data or alternative intuitive quantification methods?
- Basis in paper: [explicit] The conclusion suggests that "Future research could explore more intuitive quantification methods, such as incorporating survey data," as the current reliance is on a model-based score.
- Why unresolved: While the current method correlates with human judgment (Spearman 0.64), the label is derived purely from BERTScore recall, which is a mathematical proxy for involvement rather than a direct measure of semantic contribution.
- What evidence would resolve it: A comparative study showing that a model trained on human-annotated ground truth (e.g., crowd-sourced assessments of involvement) results in lower regression error or higher correlation with human evaluators than the BERTScore baseline.

### Open Question 4
- Question: What architectural or training modifications are required to reduce the performance variance when detecting text generated by structurally distinct LLMs?
- Basis in paper: [inferred] While the paper claims generalizability, Table V shows a significant performance drop when the model is tested on Claude-3 (MSE 0.034) and Gemini (MSE 0.025) compared to the ChatGPT training set (MSE 0.004).
- Why unresolved: The model appears to be somewhat overfitted to the generation patterns of the GPT family (ChatGPT/GPT-4), struggling more with the idiosyncrasies of Claude or Gemini.
- What evidence would resolve it: A study demonstrating a unified model that achieves consistently low MSE (e.g., <0.01) across a balanced test set containing equal parts GPT, Claude, Gemini, and Llama generations.

## Limitations
- The CAS-CS dataset is limited to English academic writing in Computer Science, raising questions about cross-domain and cross-linguistic generalizability
- The methodology assumes BERTScore similarity adequately captures "human involvement," which may not fully align with human notions of contribution or authorship style
- The study does not adequately address potential overfitting to the specific dataset structure, and the claim that "all existing detectors failed" needs more nuanced interpretation

## Confidence
- **High Confidence**: The model's superior performance on the CAS-CS dataset compared to existing detectors. The experimental methodology and evaluation metrics are sound for this specific dataset.
- **Medium Confidence**: Cross-model generalization across different LLMs. While the study tested multiple models, the range of models and diversity of prompts may not be comprehensive enough to establish robust generalization.
- **Low Confidence**: The claim that this approach solves the limitations of binary classifiers for collaborative writing. The study demonstrates effectiveness on a specific dataset but hasn't proven this in real-world collaborative writing scenarios where human and AI contributions are more complex and intertwined.

## Next Checks
1. **Cross-Domain Validation**: Test the model on non-academic domains including creative writing, technical documentation, and informal communication to assess domain transfer capability and identify domain-specific limitations.

2. **Real-World Collaborative Writing Dataset**: Create and evaluate on a dataset of actual human-AI collaborative writing samples from platforms like Google Docs or academic co-authoring tools, where the contribution boundaries are more ambiguous than the controlled CAS-CS dataset.

3. **Longitudinal Stability Analysis**: Assess model performance over time as LLMs evolve and potentially become better at mimicking human writing patterns, and evaluate whether the detection method requires periodic retraining or can maintain accuracy across model generations.