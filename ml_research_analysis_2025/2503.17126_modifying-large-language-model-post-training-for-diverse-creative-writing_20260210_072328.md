---
ver: rpa2
title: Modifying Large Language Model Post-Training for Diverse Creative Writing
arxiv_id: '2503.17126'
source_url: https://arxiv.org/abs/2503.17126
tags:
- diversity
- prompt
- quality
- instances
- while
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the challenge of diversifying LLM creative writing
  outputs while maintaining quality. Current post-training methods often boost quality
  but suppress diversity, leading to homogenized outputs that limit creative expression.
---

# Modifying Large Language Model Post-Training for Diverse Creative Writing

## Quick Facts
- **arXiv ID**: 2503.17126
- **Source URL**: https://arxiv.org/abs/2503.17126
- **Reference count**: 40
- **Primary result**: Deviation-aware DPO and ORPO achieve diverse creative writing outputs while maintaining quality, outperforming GPT-4o on both metrics

## Executive Summary
This work addresses the challenge of diversifying LLM creative writing outputs while maintaining quality. Current post-training methods often boost quality but suppress diversity, leading to homogenized outputs that limit creative expression. The authors introduce deviation-aware extensions to DPO and ORPO that incorporate a measure of how much each training instance differs from others with the same prompt. This encourages models to learn from rare high-quality examples. Experiments with Llama-3.1-8B and Mistral-7B-v0.3 on the r/writingPrompts dataset show that their DDPO and DORPO models achieve semantic and style diversity scores approaching human-written data, while maintaining comparable writing quality to top models like GPT-4o. A human evaluation confirms DDPO outputs are both higher quality and more diverse than GPT-4o and DPO.

## Method Summary
The authors modify standard DPO and ORPO training by weighting the preference loss by the "deviation" of the winning response - a measure of how different it is from other winning responses for the same prompt. Deviation is calculated as the mean pairwise cosine distance between responses in an embedding space (semantic or style). The deviation weights are normalized to preserve dataset scale. The method is tested on Llama-3.1-8B and Mistral-7B-v0.3 using LoRA fine-tuning on the r/writingPrompts dataset, with evaluation using both automated metrics (reward model score, embedding distances) and human judgment.

## Key Results
- DDPO models achieve semantic and style diversity scores approaching human-written data
- DDPO maintains comparable writing quality to top models like GPT-4o
- Human evaluation confirms DDPO outputs are both higher quality and more diverse than GPT-4o and DPO
- Ablation studies demonstrate robustness across dataset sizes and superiority over DivPO
- The approach offers a general strategy for balancing diversity and quality in LLM fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
Weighting the preference loss by the "deviation" of the winning response shifts the model's learned distribution to cover outlying modes rather than just the highest-probability mode. Standard post-training (like DPO) tends to sharpen the distribution around the single best average response. By multiplying the loss term by $\delta_w$ (deviation), the model applies stronger gradients to pairs where the winning response is semantically or stylistically distant from other winners for the same prompt. This forces the model to allocate probability mass to these rare, high-quality instances.

### Mechanism 2
The choice of embedding model for calculating deviation directly controls which dimension of creativity (semantic vs. style) is amplified. The deviation $\delta$ is calculated as the mean pairwise cosine distance in an embedding space. If the space is "semantic" (e.g., jina-embeddings), the model learns to vary plot points and content. If the space is "style" (e.g., Style-Embedding), it learns to vary tone and sentence structure.

### Mechanism 3
Normalizing deviation weights to sum to the count of responses preserves dataset scale while re-balancing importance. The paper normalizes deviations so $\sum \delta_{yi} = |Y_x|$. This ensures that while individual pairs are re-weighted, the total "pressure" applied per prompt remains comparable to standard training, preventing the model from ignoring prompts with low inherent diversity.

## Foundational Learning

- **Concept**: Direct Preference Optimization (DPO)
  - **Why needed here**: This is the substrate the paper builds upon (DDPO). You must understand how DPO optimizes a policy using a preference pair $(y_w, y_l)$ without an explicit reward model to understand where the $\delta_w$ weight is injected.
  - **Quick check question**: Can you identify where the deviation weight $\delta_w$ is multiplied into the standard DPO loss equation (Eq. 5 vs Eq. 8)?

- **Concept**: Embedding Spaces & Cosine Similarity
  - **Why needed here**: The core innovation relies on defining "diversity" mathematically. You need to understand that cosine distance measures orientation, not magnitude, and that different embedding models capture different linguistic features (semantics vs. style).
  - **Quick check question**: Why would a style-embedding give a high deviation score to two stories with the same plot but different narrators?

- **Concept**: Preference Data Structure
  - **Why needed here**: The mechanism requires grouping data by prompt $x$ to calculate deviation among responses $Y_x$. Standard datasets often treat pairs independently; this architecture requires the "prompt-grouped" view.
  - **Quick check question**: Why does the method fail if you treat every preference pair as an isolated training example?

## Architecture Onboarding

- **Component map**: Data Loader -> Deviation Engine -> Trainer (DDPO/DORPO)
- **Critical path**: The calculation of deviation must happen before the training epoch or as a distinct pre-processing step, as it requires a global view of all responses to a specific prompt
- **Design tradeoffs**:
  - Min-$\delta$ Threshold: Setting a floor (e.g., 0.1) fixes quality degradation but dampens the diversity effect
  - Embedding Choice: Using a generic vs. specialized style embedding trades off computational cost vs. fine-grained stylistic control
- **Failure signatures**:
  - Zero-Weight Collapse: If you have $\le 2$ responses per prompt, deviation is uniform or zero
  - Reward Hacking: The model might learn to generate "weird" (high deviation) but incoherent text if the quality signal is weak
- **First 3 experiments**:
  1. Ablation on Response Density: Train DDPO on subsets with Max 4, Max 8, and Max 16 responses per prompt to reproduce quality dip at low density
  2. Semantic vs. Style: Train two models using only semantic embeddings vs. only style embeddings for deviation
  3. Baseline Comparison: Compare DDPO-both against standard DPO on same dataset, checking reddit-reward score and embedding distance

## Open Questions the Paper Calls Out

### Open Question 1
Can deviation-weighted training objectives improve diversity in online post-training settings (e.g., online PPO or online DPO), where models learn from dynamically generated samples rather than fixed offline datasets? This study only tested offline training with fixed datasets. Online settings present different challenges—deviation must be computed on-the-fly from newly generated samples, and the distribution shift during training may affect how deviation signals evolve.

### Open Question 2
Do deviation-weighted objectives transfer effectively to tasks beyond creative writing, such as code generation, mathematical reasoning, or open-ended question answering where multiple valid outputs exist? The deviation metric was computed using semantic and style embeddings optimized for narrative text. Other domains may require different distance metrics, and the quality-diversity trade-off may manifest differently when correctness constraints are stronger.

### Open Question 3
How can deviation-based weighting be adapted for training methods that use numerical rewards instead of preference pairs (e.g., reinforcement learning from scalar feedback)? DDPO/DORPO weight preference pair losses by the winning response's deviation. With scalar rewards, there are no explicit pairs—the deviation signal would need to be integrated differently into the loss, and it's unclear whether weighting high-deviation samples more heavily would produce similar diversity benefits.

## Limitations
- The approach assumes datasets contain multiple valid, high-quality responses per prompt, which may not hold in many real-world datasets
- Effectiveness depends heavily on the quality and appropriateness of embedding models used to calculate deviation
- Computational overhead of calculating pairwise cosine distances could become prohibitive for large datasets
- Reliance on upvote-based preference data from Reddit introduces potential biases toward popular but not necessarily creative content

## Confidence
- **High Confidence**: The core mathematical formulation of DDPO/DORPO and its implementation details
- **Medium Confidence**: The claim that DDPO maintains quality while improving diversity, based on automated metrics and one human evaluation
- **Medium Confidence**: The assertion that deviation weighting specifically enables learning from rare high-quality instances
- **Low Confidence**: Generalization to domains beyond creative writing and Reddit-style prompts

## Next Checks
1. **Dataset Diversity Dependence**: Systematically vary the number of responses per prompt in training data (e.g., 2, 4, 8, 16) and measure how this affects both quality and diversity outcomes
2. **Cross-Domain Transfer**: Apply DDPO to a different creative domain (e.g., poetry generation) to test whether the deviation-aware approach generalizes beyond Reddit-style narrative prompts
3. **Embedding Sensitivity Analysis**: Replace jina-embeddings-v3 and Style-Embedding with alternative semantic and style embedding models to test the robustness of the deviation signal