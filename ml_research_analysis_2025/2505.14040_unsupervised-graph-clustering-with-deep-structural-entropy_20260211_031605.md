---
ver: rpa2
title: Unsupervised Graph Clustering with Deep Structural Entropy
arxiv_id: '2505.14040'
source_url: https://arxiv.org/abs/2505.14040
tags:
- graph
- structural
- clustering
- structure
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of unsupervised graph clustering
  when the original graph structure is sparse or noisy. The authors propose DeSE,
  a novel framework that enhances graph clustering by integrating deep structural
  entropy with a differentiable soft assignment scheme.
---

# Unsupervised Graph Clustering with Deep Structural Entropy

## Quick Facts
- **arXiv ID:** 2505.14040
- **Source URL:** https://arxiv.org/abs/2505.14040
- **Reference count:** 40
- **Primary result:** DeSE achieves state-of-the-art clustering performance on four benchmark datasets, outperforming eight representative baselines across multiple metrics (NMI, ACC, F1).

## Executive Summary
This paper addresses the challenge of unsupervised graph clustering when the original graph structure is sparse or noisy. The authors propose DeSE, a novel framework that enhances graph clustering by integrating deep structural entropy with a differentiable soft assignment scheme. DeSE includes a Structural Learning Layer that constructs an attributed graph using K-nearest neighbors in feature space to address sparsity, and a Clustering Assignment Layer that jointly learns node embeddings and a soft assignment matrix via GNNs. The model is optimized by minimizing structural entropy and maximizing edge consistency through cross-entropy loss. Experiments on Cora, Citeseer, Computer, and Photo datasets show that DeSE achieves state-of-the-art performance across multiple metrics (e.g., NMI, ACC, F1), outperforming eight representative baselines. It also demonstrates robustness to cluster number variations and improved interpretability.

## Method Summary
DeSE is a three-component framework for unsupervised graph clustering. First, the Structural Learning Layer (SLL) constructs an attributed graph by building a KNN graph in the feature space and fusing it with the original adjacency matrix to address sparsity and noise. Second, the Clustering Assignment Layer (ASS) uses two GNN-based learners to jointly learn node embeddings and a soft assignment matrix, optimizing them end-to-end for the clustering task. Third, the model is trained by minimizing a combined loss: the soft-assignment structural entropy (measuring clustering uncertainty) and a cross-entropy loss (ensuring edge consistency). This differentiable formulation allows stable gradient-based optimization.

## Key Results
- DeSE achieves state-of-the-art performance on Cora, Citeseer, Computer, and Photo datasets across NMI, ACC, ARI, and F1 metrics.
- The SLL's KNN-based feature graph significantly improves performance on sparse graphs, with K=1 often optimal.
- The model demonstrates robustness to cluster number variations and consistently converges to the ground-truth cluster count.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Constructing an attribute graph using K-nearest neighbors in the feature space and fusing it with the original graph structure mitigates performance degradation caused by sparse or noisy edges in the original adjacency matrix.
- **Mechanism:** The Structural Learning Layer (SLL) maps node features to a latent space using an MLP. It then builds a KNN graph ($A_f$) in this space, connecting nodes with similar features even if they are disconnected in the original graph. This attribute graph is fused with the original adjacency matrix ($A_g$) to form an enhanced graph ($W = A_g + \beta_f A_f$). This provides a richer, more reliable topological signal for clustering.
- **Core assumption:** Node feature similarity is a strong predictor for cluster membership and can recover missing or correct erroneous structural connections in the original graph.
- **Evidence anchors:**
  - [abstract] "...design a Structural Learning Layer (SLL) to generate an attributed graph from the original feature data... thereby mitigating the issue of sparse connections between graph nodes."
  - [section 4.2] "The SLL aims to enhance the original graph structure by leveraging the available feature information... This process constructs an attribute graph... to address the sparsity and missing interactions..."
  - [corpus] Related work on learnable structural augmentation supports the general principle, though specific KNN-feature fusion evidence in the provided corpus is weak.
- **Break condition:** Performance may degrade if node features are uninformative or noisy, leading the KNN graph to introduce more harmful edges than it corrects. The optimal `K` for the KNN graph is also critical.

### Mechanism 2
- **Claim:** A differentiable soft assignment formulation of structural entropy provides a stable, trainable objective for graph clustering, overcoming the limitations of discrete optimization methods.
- **Mechanism:** The framework introduces a soft assignment structural entropy ($\mathcal{L}_{se}$) calculated using a probabilistic assignment matrix ($S$). Instead of a hard partition, nodes belong to clusters with varying probabilities. This continuous formulation allows for gradient-based optimization, letting the model minimize the structural entropy (uncertainty) of the graph directly during training, which encourages the formation of stable, well-defined clusters.
- **Core assumption:** The optimal clustering corresponds to a minimum of the structural entropy, and a probabilistic representation allows the model to retain information from ambiguous boundary nodes during the learning process.
- **Evidence anchors:**
  - [abstract] "...incorporating Deep Structural Entropy... we first propose a method for calculating structural entropy with soft assignment, which quantifies structure in a differentiable form."
  - [section 4.1] "This highlights the limitations of traditional structural entropy. To address this issue, we transform... into a probabilistic relationship... This approach aligns with real-world scenarios..."
  - [corpus] Corpus evidence explicitly mentions "Hyperbolic Continuous Structural Entropy for Hierarchical Clustering," validating the direction of making structural entropy differentiable.
- **Break condition:** The optimization may fail if the soft assignment probabilities become too extreme (approaching hard assignments) too early, which can cause gradients to vanish.

### Mechanism 3
- **Claim:** Jointly learning node embeddings and cluster assignments within the Clustering Assignment Layer (ASS) yields better clusters than the traditional two-stage approach of learning embeddings first and then applying an external clustering algorithm like k-means.
- **Mechanism:** The ASS uses two GNN-based learners: an Embedding Learner ($GNN_{emb}$) and a Soft Assignment Learner ($GNN_{ass}$). Both operate on the enhanced graph from the SLL. The Embedding Learner updates node representations, while the Assignment Learner produces a soft assignment matrix. These components are trained end-to-end, allowing the learned embeddings to be directly optimized for the clustering task via the combined loss function.
- **Core assumption:** Joint optimization allows the model to capture essential relationships between node features and adaptive clusters, which is missed when the embedding and clustering steps are decoupled.
- **Evidence anchors:**
  - [abstract] "...clustering assignment method (ASS), based on GNNs, learns node embeddings and a soft assignment matrix to cluster on the enhanced graph."
  - [section 4.3] "The clustering assignment layer utilizes the initial embeddings and the adjacency matrix to learn the soft assignments and embeddings of nodes..."
  - [corpus] The corpus paper "ASIL: Augmented Structural Information Learning for Deep Graph Clustering" also proposes a joint learning approach, providing external validation for this architectural choice.
- **Break condition:** The end-to-end learning process can be sensitive to the balance between the SE loss and the cross-entropy loss. If the edge-based cross-entropy loss dominates, the model may focus more on link prediction than forming coherent clusters.

## Foundational Learning

- **Concept: Graph Structure Learning (GSL)**
  - **Why needed here:** The entire DeSE framework is predicated on the idea that the raw input graph is suboptimal. Understanding GSL is necessary to grasp the motivation behind the SLL module and its role in dynamically refining the graph topology.
  - **Quick check question:** Why is the original adjacency matrix often insufficient for high-quality clustering, and how does learning a new structure help?

- **Concept: Structural Entropy**
  - **Why needed here:** This is the core theoretical contribution. You must understand that structural entropy measures the uncertainty or information content of a graph's structure and that the paper's goal is to minimize this to find a stable clustering.
  - **Quick check question:** In the context of this paper, what does minimizing structural entropy represent for the final clustering result?

- **Concept: Soft vs. Hard Clustering Assignment**
  - **Why needed here:** The key innovation that makes the structural entropy objective trainable is its soft (probabilistic) assignment scheme. This concept is central to the loss function and optimization process.
  - **Quick check question:** What is the primary advantage of using a probabilistic (soft) assignment for the structural entropy calculation during model training?

## Architecture Onboarding

- **Component map:** Raw Features ($X$) & Original Graph ($A_g$) -> SLL (MLP + KNN) -> Enhanced Graph ($W$) -> ASS ($GNN_{emb}$ & $GNN_{ass}$) -> Node Embeddings ($H$) & Soft Assignments ($S$) -> Loss Computation ($\mathcal{L}_{se} + \lambda\mathcal{L}_{ce}$) -> Backpropagation.

- **Critical path:** Raw Features ($X$) & Original Graph ($A_g$) -> SLL (MLP + KNN) -> Enhanced Graph ($W$) -> ASS ($GNN_{emb}$ & $GNN_{ass}$) -> Node Embeddings ($H$) & Soft Assignments ($S$) -> Loss Computation ($\mathcal{L}_{se} + \lambda\mathcal{L}_{ce}$) -> Backpropagation.

- **Design tradeoffs:**
    - **SLL's KNN Graph:** Adding more neighbors (higher `K`) adds more structural information but can introduce noise. Ablation studies suggest `K=1` is often optimal.
    - **Loss Coefficients ($\lambda_{se}, \lambda_{ce}$):** Balancing the structural stability loss ($\mathcal{L}_{se}$) against the edge consistency loss ($\mathcal{L}_{ce}$) is critical and dataset-dependent.
    - **ASS Layer Stacking:** Multiple ASS layers can learn hierarchical clusters, but deeper models increase computational cost and risk over-smoothing.

- **Failure signatures:**
    - **Cluster Collapse:** The model may converge to assigning all nodes to a single cluster if the SE loss is not properly weighted.
    - **Sensitivity to Sparsity:** On extremely sparse datasets, the SLL's KNN graph might create small, disconnected components, preventing the model from learning a global structure.
    - **Noisy Feature Drift:** If input features are poor, the SLL will build a flawed attribute graph, leading the entire model astray.

- **First 3 experiments:**
    1.  **Baseline Reproduction:** Run DeSE on the Cora or Citeseer datasets and verify the reported ACC and NMI scores to establish a working baseline.
    2.  **Ablation on SLL:** Disable the SLL (set $\beta_f = 0$) and measure the performance drop. This quantifies the contribution of the feature-based graph enhancement.
    3.  **Hyperparameter Sensitivity:** Vary the key hyperparameters (`K` for the SLL, and $\lambda_{se}, \lambda_{ce}$ for the loss) to understand their impact on a validation set. Use the paper's findings (e.g., `K=1` being optimal) as a starting point.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model be refined to improve boundary detection and prevent the incorrect merging of small clusters into larger dominant clusters?
- Basis in paper: [explicit] The authors state, "Improving or fine-tuning cluster boundaries within DeSE is the next step for future research," noting that small clusters are often disregarded or merged into larger ones.
- Why unresolved: The current soft assignment mechanism tends to favor stable structural entropy over distinct minority cluster preservation, leading to collective errors at boundaries (e.g., in the Computer dataset).
- Evidence: Improved metrics (ARI/F1) specifically on minority classes in datasets like Cora or Computer, demonstrating distinct boundary separation without sacrificing overall structural stability.

### Open Question 2
- Question: How can the computational efficiency of DeSE be optimized for large-scale graphs without compromising the structural learning capabilities?
- Basis in paper: [explicit] The paper notes, "Future efficiency improvements may be achievable through enhancements in the selection of K-nearest neighbors in large-scale graphs and the computation of soft-assignment structural entropy."
- Why unresolved: The current time complexity is $O(dN \log N)$, leading to significantly higher runtimes (e.g., 2037s on Computer) compared to some baselines on dense datasets.
- Evidence: Implementation of scalable KNN or parallelized entropy calculation techniques that reduce runtime on graphs exceeding 100,000 nodes while maintaining state-of-the-art clustering accuracy.

### Open Question 3
- Question: Can the framework be made more robust to hyperparameter selection, specifically reducing the dependency on fine-tuning loss coefficients ($\lambda_{se}, \lambda_{ce}$) for varying cluster numbers?
- Basis in paper: [explicit] The authors identify "a key direction for future work is to improve the model to reduce the impact of hyperparameters on clustering accuracy across varying numbers of clusters."
- Why unresolved: The experiments show that optimal coefficients for SE loss and CE loss vary significantly across datasets (e.g., Computer vs. Cora), suggesting a lack of universal adaptability.
- Evidence: Performance stability analysis showing consistent clustering accuracy across diverse datasets without requiring dataset-specific hyperparameter search.

## Limitations
- The paper lacks critical implementation details such as exact MLP/GNN architectures, training hyperparameters (learning rate, optimizer), and negative sampling strategy for cross-entropy loss.
- Computational complexity and training time are not reported, making scalability assessment difficult.
- The sensitivity analysis focuses on SLL's K parameter and loss coefficients but does not thoroughly explore the impact of ASS layer depth or the trade-offs between SE and CE loss weights across different dataset types.

## Confidence
- **High Confidence:** The core architectural design (SLL + ASS) and the theoretical motivation for differentiable structural entropy are well-founded and supported by the literature.
- **Medium Confidence:** The experimental results showing SOTA performance on benchmark datasets, as the exact implementation details needed for verification are missing.
- **Low Confidence:** Claims about robustness to varying cluster numbers and the specific hyperparameter recommendations (K=1, λ values) due to limited ablation studies.

## Next Checks
1. **Ablation Study Replication:** Implement DeSE without the SLL (β_f = 0) and measure the performance drop on Cora and Citeseer to verify the claimed 5-10% improvement in NMI/ACC.
2. **Hyperparameter Sensitivity:** Systematically vary K (1, 3, 5) and the loss coefficients (λ_se ∈ {0.001, 0.01, 0.1}, λ_ce ∈ {1, 5, 10}) to confirm the paper's findings about optimal values and assess model stability.
3. **Cluster Number Robustness:** Run DeSE with c-1, c, c+1, and c+2 clusters on all four datasets to verify the claim that the model converges to the ground-truth cluster count and does not collapse to a single cluster.