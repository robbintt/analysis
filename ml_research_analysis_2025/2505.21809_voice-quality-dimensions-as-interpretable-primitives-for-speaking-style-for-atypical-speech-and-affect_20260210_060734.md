---
ver: rpa2
title: Voice Quality Dimensions as Interpretable Primitives for Speaking Style for
  Atypical Speech and Affect
arxiv_id: '2505.21809'
source_url: https://arxiv.org/abs/2505.21809
tags:
- speech
- voice
- atypical
- dataset
- probes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops interpretable voice quality dimension (VQD)
  probes for atypical speech and affect detection. The authors train regression and
  binary classification probes on the Speech Accessibility Project dataset (11,184
  samples from 434 speakers) using frozen pre-trained embeddings as features.
---

# Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect

## Quick Facts
- arXiv ID: 2505.21809
- Source URL: https://arxiv.org/abs/2505.21809
- Reference count: 0
- Primary result: VQD probes achieve Spearman correlations 0.33-0.72 and AUCs 0.68-0.91 across 7 voice quality dimensions, with strong zero-shot generalization to Italian and English atypical speech and affective speech datasets

## Executive Summary
This paper develops interpretable Voice Quality Dimension (VQD) probes for atypical speech and affect detection using frozen pre-trained embeddings. Seven VQDs are modeled: intelligibility, imprecise consonants, harsh voice, naturalness, monoloudness, monopitch, and breathiness. Probes are trained on the Speech Accessibility Project dataset (11,184 samples from 434 speakers) using Lasso and logistic regression on frozen HuBERT, CLAP, and RawNet3 embeddings. Results show strong performance on held-out test data and demonstrate zero-shot generalization to out-of-domain datasets including Italian atypical speech, English atypical speech, and affective speech from RAVDESS. The work establishes VQDs as transferable acoustic primitives that can explain model predictions and reveal biases in affect detection models.

## Method Summary
The study trains linear regression and binary classification probes on frozen speech embeddings extracted from pre-trained models (HuBERT Large, HuBERT Large ASR, CLAP, RawNet3). Input audio is preprocessed by trimming silence using wav2vec2 forced alignment with transcriptions. Probes are trained per VQD dimension using the Speech Accessibility Project dataset with speaker-stratified splits (train: 8,016 samples, val: 1,194, test: 2,086). Binary labels are created using a ~20% positive rate threshold. Performance is evaluated using Spearman correlation for regression and AUC for classification, with additional zero-shot testing on Italian atypical speech, English atypical speech, and affective speech datasets.

## Key Results
- Spearman correlations range from 0.33-0.72 across VQD dimensions on held-out SAP test data
- AUC scores range from 0.68-0.91 for binary classification across dimensions
- Strong zero-shot generalization to Italian and English atypical speech datasets
- VQD probes reveal affect model bias toward atypical speech, particularly misclassifying monopitch/monoloudness as sadness
- CLAP model shows strongest cross-language generalization to Italian dataset

## Why This Works (Mechanism)

### Mechanism 1
Frozen pre-trained speech embeddings encode recoverable voice quality information that linear probes can extract without fine-tuning. Self-supervised models trained on large speech corpora implicitly learn acoustic properties—including non-semantic dimensions like harshness and breathiness—alongside phonetic content. Linear regression/classification probes trained on these frozen embeddings can isolate specific VQDs by learning weighted combinations of embedding dimensions. Core assumption: The pre-training objectives create representations where voice quality correlates are linearly accessible rather than deeply entangled.

### Mechanism 2
VQDs function as transferable acoustic primitives that generalize zero-shot across languages, speech elicitation types, and task domains. Voice quality dimensions capture acoustic properties (spectral tilt, prosodic variation, phonation patterns) that manifest similarly across languages and tasks. Probes trained on English atypical speech can detect these properties in Italian speech and affective speech because the underlying acoustic signatures—harshness, monopitch, breathiness—share physical correlates regardless of linguistic or task context. Core assumption: Acoustic manifestations of VQDs are sufficiently consistent across languages and speaking contexts that English-trained probes capture transferable features rather than dataset-specific artifacts.

### Mechanism 3
VQD probes can expose systematic biases in downstream affect models by revealing confounding acoustic features. Affect detection models trained on typical speech may learn spurious correlations between voice quality features (monopitch, harshness) and emotional categories. By applying VQD probes to atypical speech evaluated by affect models, one can identify when models confuse atypical speech characteristics with emotional states—particularly "sadness" for speech with monopitch/monoloudness characteristics common in dysarthria. Core assumption: Affect models encode voice quality information implicitly, and this information can be surfaced through VQD probe analysis.

## Foundational Learning

- **Concept: Self-supervised speech representations (HuBERT, wav2vec2)**
  - Why needed here: Understanding what frozen embeddings encode determines whether probing is viable; these models learn via masked prediction, capturing both phonetic and speaker/style information.
  - Quick check question: Can you explain why HuBERT ASR (fine-tuned for speech recognition) outperforms base HuBERT on intelligibility and imprecise consonants but underperforms on harsh voice?

- **Concept: Linear probing vs. fine-tuning**
  - Why needed here: The paper deliberately uses linear probes to demonstrate that VQD information is linearly accessible; this constrains the complexity of learned representations and improves interpretability.
  - Quick check question: Why might neural probes overfit compared to linear probes when training data is limited to ~11K samples across 7 dimensions?

- **Concept: Clinical voice quality assessment scales (CAPE-V, GRBAS)**
  - Why needed here: The seven VQDs derive from speech-language pathology evaluation protocols; understanding their perceptual definitions clarifies what the models should capture.
  - Quick check question: Why might "breathiness" have the weakest model performance (Spearman 0.30-0.35) compared to "imprecise consonants" (0.68-0.69)?

## Architecture Onboarding

- **Component map:**
  Audio samples (trimmed silence via forced alignment) -> Frozen embeddings from HuBERT Large, HuBERT Large ASR, CLAP, RawNet3 -> Lasso regression + Logistic regression probes -> 7 VQD scores per sample

- **Critical path:**
  1. Silence trimming with wav2vec2 forced alignment (required for embedding quality)
  2. Embedding extraction (dimension varies: 192-1024 depending on model)
  3. Probe training with validation-set regularization selection
  4. Cross-category and cross-dataset evaluation

- **Design tradeoffs:**
  - HuBERT ASR excels at pronunciation-related dimensions (intelligibility, imprecise consonants) but underperforms on voice-quality dimensions (harshness)
  - CLAP shows best cross-language generalization (Italian dataset) due to language-aligned training
  - Linear probes chosen over neural probes despite lower capacity—interpretability and regularization prioritized
  - Binary classification threshold at 20% positive rate stabilizes task difficulty but may miss edge cases

- **Failure signatures:**
  - Probes trained only on "novel sentences" generalize poorly (avg. Spearman 0.29-0.34) despite diverse phonetics—reading may elicit less natural speech
  - Breathiness consistently underperforms across all models (Spearman 0.21-0.35)—may require different acoustic features or annotation reliability issues
  - RawNet3 (speaker ID model) underperforms overall—speaker identity features may not align with VQD correlates

- **First 3 experiments:**
  1. Replicate linear probe training on HuBERT embeddings for a single VQD (e.g., naturalness) using SAP splits; verify Spearman correlation matches reported ~0.72 before expanding to all dimensions.
  2. Test zero-shot transfer by applying trained probes to a held-out speech category not used in training (e.g., train on digital voice commands + spontaneous, evaluate on novel sentences) to validate generalization claims.
  3. Implement affect bias analysis: Apply VQD probes to RAVDESS samples, then correlate VQD scores with categorical emotion labels to verify patterns (e.g., sad speech → higher monopitch scores) before attempting to explain affect model predictions.

## Open Questions the Paper Calls Out

- **Open Question 1:** Do adapter-based fine-tuning or few-shot learning approaches yield significantly better performance for Voice Quality Dimensions (VQDs) compared to the frozen linear probes evaluated in this study? The authors identify "only evaluating frozen embeddings with VQD linear probes" as a limitation and explicitly list "few-shot learning and adapter training" as areas for future work.

- **Open Question 2:** How does the relationship between specific VQDs and perceived speech severity change in typologically distinct languages, particularly tonal languages? The paper notes that breathiness was a stronger differentiator for Italian than English, and explicitly states, "Future work will include evaluation with additional languages including tonal languages."

- **Open Question 3:** To what extent can VQD primitives disentangle the acoustic markers of atypical speech from those of affective states to reduce bias in emotion recognition models? The authors show that affect models misclassify atypical speech as "sad" and suggest "a deeper investigation of connections between VQDs and specific labels across broad tasks" is needed to improve robustness.

- **Open Question 4:** How does annotator reliability and confidence impact the performance ceiling of VQD regression probes? The authors hypothesize that performance variations across dimensions might be due to annotator confidence and state, "Future work could include obtaining annotations from different raters to quantify annotator reliability."

## Limitations
- Study relies on proprietary CLAP model for optimal cross-language generalization, limiting full reproducibility
- Binary classification uses fixed ~20% positive rate threshold without specifying optimal thresholds for clinical applications
- Breathiness detection shows consistently weak performance (Spearman 0.21-0.35), suggesting annotation reliability issues or fundamental limitations
- SAP dataset may not fully represent diversity of atypical speech patterns across all clinical conditions

## Confidence
- **High confidence**: VQD probes can be trained on frozen embeddings to predict voice quality dimensions with strong performance on held-out test data (Spearman 0.33-0.72, AUC 0.68-0.91)
- **Medium confidence**: Zero-shot generalization across languages and speech elicitation types due to acoustic primitive nature of VQDs
- **Medium confidence**: VQD probes can expose biases in affect models by revealing confounding acoustic features

## Next Checks
1. Test probe performance on additional clinical speech datasets (e.g., TORGO, UASpeech) to validate generalization across different atypical speech populations and recording conditions
2. Implement ablation studies removing individual VQD features from affect model inputs to quantify the specific contribution of voice quality features to prediction biases
3. Evaluate probe calibration and clinical utility by testing performance at different severity thresholds (e.g., ≥4 vs ≥6 on 1-7 scale) to determine optimal operating points for clinical assessment