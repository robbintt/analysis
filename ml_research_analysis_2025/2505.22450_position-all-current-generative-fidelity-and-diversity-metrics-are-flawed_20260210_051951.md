---
ver: rpa2
title: 'Position: All Current Generative Fidelity and Diversity Metrics are Flawed'
arxiv_id: '2505.22450'
source_url: https://arxiv.org/abs/2505.22450
tags:
- metrics
- synthetic
- real
- data
- metric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors systematically evaluated generative fidelity and diversity
  metrics using carefully designed sanity checks to identify failure cases. They proposed
  six desiderata for synthetic data metrics and distilled reported failures into automated
  checks with clear passing criteria.
---

# Position: All Current Generative Fidelity and Diversity Metrics are Flawed

## Quick Facts
- arXiv ID: 2505.22450
- Source URL: https://arxiv.org/abs/2505.22450
- Authors: Ossi Räisä; Boris van Breugel; Mihaela van der Schaar
- Reference count: 40
- All 12 tested generative fidelity and diversity metrics fail multiple sanity checks, undermining their reliability

## Executive Summary
This paper systematically evaluates 12 generative fidelity and diversity metrics using carefully designed sanity checks to identify fundamental failure modes. The authors propose six desiderata for synthetic data metrics and distill reported failures into automated checks with clear passing criteria. Testing across 21 checks reveals that all metrics fail multiple sanity checks, particularly in areas like outlier robustness, scaling invariance, mode collapse detection, and high-dimensional geometry issues. The authors conclude that current metrics are fundamentally flawed and urge the research community to prioritize developing and rigorously testing new metrics over improving generative models.

## Method Summary
The authors conducted a comprehensive evaluation of 12 generative fidelity and diversity metrics using six proposed desiderata as evaluation criteria. They designed 21 automated sanity checks (both "easy" and "hard") based on simple synthetic distributions where ground truth fidelity/diversity values are known analytically. The metrics were tested across these checks, with performance documented in tables showing which desiderata each metric satisfies or fails. The evaluation focused on identifying failure modes that could undermine metric reliability in practical applications.

## Key Results
- All 12 metrics fail multiple sanity checks, with most failing 3-4 of the 6 proposed desiderata
- High-dimensional geometry issues cause metrics to miss significant differences in single dimensions among many
- Dataset size and hyperparameter selection (k in k-NN) significantly affect metric values, violating scaling invariance
- No metric successfully distinguishes between continuous and discrete numerical distributions in tabular data
- Metrics relying on Euclidean distances in embedding space struggle with complex manifold structures

## Why This Works (Mechanism)

### Mechanism 1: Isolation of Failure Modes via Controlled Synthetic Distributions
The authors construct "sanity checks" (e.g., disjoint hyperspheres, Gaussian mixtures with dropped modes) where the ground truth fidelity/diversity is known analytically. If a metric fails to return the known correct value, it reveals a fundamental flaw in the metric's mathematical formulation rather than its estimation variance. The core assumption is that a metric failing on simple, isolated geometric cases will likely fail on complex, high-dimensional real data.

### Mechanism 2: Euclidean Geometry Limitations in High Dimensions
Metrics like Improved Precision, Density, and Coverage rely on Euclidean distances to define "neighborhoods" (hyperspheres). In high dimensions, a large difference in a single dimension is "drowned out" by noise from hundreds of other identical dimensions, causing metrics to report high fidelity when they should be low. The core assumption is that the embedding space preserves meaningful semantic differences that Euclidean distance should capture.

### Mechanism 3: Sensitivity to Hyperparameters and Data Scaling
Many metrics define "coverage" or "density" based on the k-th nearest neighbor. As dataset size grows, the distance to the k-th neighbor changes, causing metric scores to fluctuate solely due to larger datasets, not improved synthetic data quality. The core assumption is that evaluation metrics should be invariant to evaluation set size beyond a certain threshold.

## Foundational Learning

- **Concept: Precision (Fidelity) vs. Recall (Diversity)**
  - Why needed here: The paper evaluates metrics in pairs. Fidelity measures "Are these samples realistic?" while Diversity measures "Do we cover the whole real distribution?"
  - Quick check question: If a generator produces only a single, perfect image of a cat, does it have high Fidelity or high Diversity?

- **Concept: Manifold Hypothesis & Embeddings**
  - Why needed here: All metrics operate on an "embedding" φ (features), not raw pixels. The quality depends entirely on whether this embedding space groups similar concepts together.
  - Quick check question: Why might Euclidean distance in pixel space be a poor metric for image similarity?

- **Concept: The Curse of Dimensionality**
  - Why needed here: The paper highlights that metrics fail in high dimensions (e.g., d=64, d=128). As dimensions increase, volume explodes and notions of "distance" become less discriminative.
  - Quick check question: In a 1000-dimensional hypercube, where is most of the volume located—near the center or near the corners?

## Architecture Onboarding

- **Component map:** Real Data (X_r) -> Embedder (φ) -> Metric Core -> Output Score
- **Critical path:** The choice of Embedder (φ) and Hyperparameter (k). The embedding dictates the geometry the metric "sees."
- **Design tradeoffs:**
  - Euclidean vs. Learned Metrics: Euclidean is fast but blind to complex manifolds; Learned captures manifolds better but introduces training noise.
  - Robustness vs. Sensitivity: Large k is robust to outliers but fails to detect small mode drops; Small k detects fine details but triggers on noise.
- **Failure signatures:**
  - High-dimensional blindness: Score stays ~1.0 even when one dimension is significantly shifted
  - Outlier explosion: Metric value jumps drastically because a single outlier creates a massive "support" ball
  - Scaling variance: Metric score drifts as you add more real data samples, even if the underlying distribution is unchanged
- **First 3 experiments:**
  1. Replicate the "Gaussian Mean Difference" check. Plot metric value vs. mean shift μ. Does it form a bell curve peaking at μ=0?
  2. Add a single extreme outlier to your real data. Does the "Fidelity" score of your synthetic data change by more than 1%?
  3. Run the metric with N=100 vs N=1000 real samples (identical distribution). Does the score stabilize?

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can non-Euclidean approaches, such as topological features, resolve the high-dimensional geometry failure modes found in all current fidelity and diversity metrics?
- Basis in paper: [explicit] Section 5.4 conjectures that "future work must go beyond Euclidean distances in some way" because all current metrics fail sanity checks involving high-dimensional geometry.
- Why unresolved: Current metrics rely on Euclidean distances or embeddings, which causes the signal of a single disjoint dimension to be drowned out.
- What evidence would resolve it: Development of a metric using topological features that successfully passes the "One Disjoint Dimension" and "Uniform Hypercube" sanity checks.

### Open Question 2
- Question: Can the geometric insights from symmetric precision/recall (symPrec/symRec) be integrated into other metrics to fix the hypersphere surface issue without sacrificing basic reliability?
- Basis in paper: [explicit] Section 5.3 states it "remains an open problem" whether insights from Khayatkhoei & Abdalmageed (2023) can be applied to fix other metrics.
- Why unresolved: There is currently a trade-off where fixing the specific asymmetry of the "Hypersphere Surface" check results in failing basic distributional difference checks.
- What evidence would resolve it: A new metric that simultaneously passes the "Hypersphere Surface" sanity check and the basic "Gaussian Mean Difference" checks.

### Open Question 3
- Question: How can generative metrics be designed to reliably distinguish between continuous and discrete numerical distributions in tabular data?
- Basis in paper: [inferred] The authors introduced a novel sanity check ("Discrete Num. vs. Continuous Num.") and found that "all current metrics are bad" at this task.
- Why unresolved: Current metrics fail to penalize generators that produce continuous values for integer-valued (discrete) tabular data.
- What evidence would resolve it: A metric that successfully passes the "Discrete Num. vs. Continuous Num." sanity check across varying scales of discretization.

## Limitations

- The sanity checks use highly simplified synthetic distributions that may not capture the full complexity of real-world data manifolds
- The study's focus on Euclidean geometry may be overly restrictive, as learned embeddings showed superior performance on several checks
- The paper doesn't explore whether modified versions of existing metrics (e.g., with adaptive k-selection) could pass the sanity checks
- The conclusion that metrics are "fundamentally flawed" may be overly conservative for relative comparisons when experimental conditions are controlled

## Confidence

**High Confidence:** The empirical observation that all 12 tested metrics fail multiple sanity checks (as documented in Tables 3-4 and Section 5 results).

**Medium Confidence:** The conclusion that metrics should not be used for absolute quality assessment, though this may be overly conservative for relative comparisons.

**Low Confidence:** The claim that current metrics are "fundamentally flawed" rather than simply requiring better implementation practices or more sophisticated embeddings.

## Next Checks

1. **Real-World Generalization Test:** Apply the six proposed sanity checks to actual real-world datasets to verify whether failure modes identified in controlled settings manifest in practical scenarios.

2. **Embedding Space Validation:** Test whether metrics using learned embeddings (like DeepSVDD) can pass sanity checks that Euclidean-based metrics fail, and whether these embeddings preserve semantic similarity to avoid high-dimensional geometry issues.

3. **Practical Impact Assessment:** Conduct a controlled experiment where practitioners use flawed metrics for model selection in a downstream task, measuring whether identified metric failures actually lead to worse model choices compared to using no metrics at all.