---
ver: rpa2
title: 'FAIM: Frequency-Aware Interactive Mamba for Time Series Classification'
arxiv_id: '2512.07858'
source_url: https://arxiv.org/abs/2512.07858
tags:
- faim
- time
- series
- data
- frequency
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FAIM, a lightweight model for time series
  classification that integrates frequency-domain analysis with Mamba-based sequence
  modeling. The core idea is to use Fourier Transform for decomposing time series
  into frequency components, apply adaptive filtering to suppress noise, and reconstruct
  the signal for further processing.
---

# FAIM: Frequency-Aware Interactive Mamba for Time Series Classification

## Quick Facts
- **arXiv ID**: 2512.07858
- **Source URL**: https://arxiv.org/abs/2512.07858
- **Reference count**: 40
- **Primary result**: Achieves state-of-the-art accuracy on univariate (85 UCR) and multivariate (26 UEA) time series classification benchmarks using frequency-aware Mamba architecture.

## Executive Summary
FAIM introduces a novel lightweight architecture for time series classification that integrates frequency-domain analysis with Mamba-based sequence modeling. The model uses Fourier Transform to decompose time series into frequency components, applies adaptive filtering to suppress noise, and reconstructs signals for processing by Interactive Mamba Blocks. A self-supervised pre-training mechanism further enhances robustness across diverse domains and noisy conditions. Experimental results demonstrate superior classification accuracy compared to existing state-of-the-art methods while maintaining computational efficiency.

## Method Summary
FAIM is a hierarchical architecture combining frequency-aware preprocessing with Mamba-based sequence modeling. The model processes input time series through patching and positional embedding, followed by L layers of Adaptive Filtering Block (AFB) and Interactive Mamba Block (IMB) pairs. AFB transforms data to frequency domain using FFT, applies learnable adaptive thresholds to filter noise, and reconstructs signals via iFFT. IMB employs dual-causal convolution structures to capture multi-granularity temporal dependencies through cross-scale interaction. The architecture incorporates self-supervised pre-training with masked reconstruction to improve robustness under limited labeled data.

## Key Results
- Achieves state-of-the-art average accuracy of 88.63% on 85 UCR univariate time series datasets
- Demonstrates superior performance with average rank of 2.53 across benchmarks, outperforming existing methods
- Shows strong noise robustness, maintaining accuracy under 1.5x Gaussian noise levels on ArticularyWordRecognition dataset

## Why This Works (Mechanism)

### Mechanism 1: Adaptive Frequency Filtering
Adaptive Filtering Block (AFB) suppresses noise while preserving discriminative frequency components by transforming input to frequency domain using FFT, applying learnable thresholds (θ_high, θ_low) to create binary masks for high-frequency and low-frequency filtering, and reconstructing signals via iFFT. The core assumption is that noise manifests primarily in extreme frequency bands while discriminative information occupies separable spectral regions.

### Mechanism 2: Multi-Scale Temporal Interaction
Interactive Mamba Block (IMB) captures multi-granularity temporal dependencies through dual-causal convolution with different kernel sizes (2×2 for fine-grained, 4×4 for broader dependencies), enabling outputs to modulate each other via cross-interaction. The core assumption is that temporal patterns exist at multiple scales and cross-interaction between scale-specific representations enriches feature expressiveness.

### Mechanism 3: Self-Supervised Pre-training
The self-supervised pre-training mechanism improves robustness under limited labeled data by randomly masking portions of input and reconstructing masked segments via MSE loss, then initializing supervised training with these pre-trained weights using label-smoothed cross-entropy. The core assumption is that reconstruction forces learning of intrinsic temporal structure transferable to classification.

## Foundational Learning

- **Discrete Fourier Transform (DFT) and frequency-domain filtering**: Essential for understanding AFB's FFT/iFFT operations and how element-wise filtering enables noise suppression while preserving signal components. Quick check: Given a 100-point sequence, what is the complexity of FFT vs. naive DFT? (Answer: O(N log N) ≈ 664 vs. O(N²) = 10,000)

- **State Space Models (SSMs) and selective scan**: Critical for understanding IMB's Mamba backbone and how it achieves O(N) complexity for sequence modeling through convolutional reformulation via parallel scan. Quick check: How does SSM achieve O(N) complexity for sequence modeling? (Answer: Through convolutional reformulation via parallel scan, avoiding quadratic attention)

- **Time Series Classification evaluation metrics**: Necessary for interpreting reported Avg. ACC, Avg. Rank, F1-Score, and CD diagrams. Quick check: Why use Critical Difference (CD) diagrams instead of average accuracy alone? (Answer: To account for multiple comparisons across datasets and assess statistical significance)

## Architecture Onboarding

- **Component map**: Input → Patching → Positional Embedding → [Layer × L: AFB → IMB → LayerNorm] → Classification Head
  - AFB: FFT → HF_Filter (|M| ≤ θ_high) → LF_Filter (|M| ≥ θ_low) → Learnable Filters (ψ_G, ψ_L) → Integrate → iFFT
  - IMB: Linear → Conv_1 (2×2) → SSM_1 and Linear → Conv_2 (4×4) → SSM_2 → Cross-interaction → Conv_3

- **Critical path**:
  1. Patch size selection affects sequence length entering AFB
  2. Threshold initialization determines initial filtering aggressiveness
  3. Mask ratio controls pre-training difficulty

- **Design tradeoffs**:
  - More layers → better on large datasets, but overfitting risk on small data
  - Smaller patches → finer granularity but longer sequences and memory cost
  - Aggressive filtering → cleaner signals but potential loss of discriminative high-frequency details

- **Failure signatures**:
  - Accuracy drops sharply with added noise → check if AFB thresholds are learning
  - Performance degrades with more layers on small datasets → reduce L or add regularization
  - Training time grows non-linearly with sequence length → verify patch size not too small

- **First 3 experiments**:
  1. Run FAIM on FordA dataset with and without AFB; expect ~5-7% accuracy gap
  2. Add Gaussian noise (0.5, 1.0, 1.5) to ArticularyWordRecognition; plot accuracy vs. noise level
  3. Train on subset fractions (1%, 10%, 50%, 100%) of UWaveGestureLibraryAll with layer depths 1-4; verify FAIM maintains stability

## Open Questions the Paper Calls Out

### Multi-modal and Irregular Sampling Support
How can FAIM be adapted to effectively support multi-modal data fusion or classify irregularly sampled time series? The current architecture relies on patching and positional embeddings suited for regular, fixed-interval sampling, and lacks mechanisms for fusing heterogeneous data modalities.

### Alternative Feature Interaction Mechanisms
What alternative feature interaction mechanisms could enhance the Interactive Mamba Block (IMB) beyond the current dual-causal convolution structure? While effective, the current dual-causal convolution is a fixed structural choice; the authors suggest there may be more dynamic or flexible ways to handle multi-granularity interactions.

### Model Compression and Acceleration
What specific model compression or acceleration techniques can be applied to FAIM to optimize it for large-scale, real-world deployment? Although the model is designed to be "lightweight," the paper does not evaluate specific compression techniques required for edge devices or high-throughput systems.

## Limitations

- **Hyper-parameter ambiguity**: Key design choices including number of layers L, hidden dimension d_model, and Mamba SSM parameters are not explicitly specified
- **Threshold initialization strategy**: AFB uses learnable thresholds but initialization ranges are not provided, affecting adaptive filtering effectiveness
- **Pre-training protocol details**: Masking pattern generation method and exact reconstruction loss formulation beyond basic MSE are not detailed

## Confidence

- **Performance claims**: Medium confidence - methodologically sound with proper statistical comparisons but exact replication requires resolving hyper-parameter ambiguities
- **AFB mechanism**: High confidence - well-defined mathematically with clear FFT → masking → learnable filtering → iFFT pipeline
- **IMB effectiveness**: Medium confidence - dual-causal convolution and cross-interaction design explicitly described but impact versus standard Mamba variants cannot be independently verified

## Next Checks

1. **Ablation verification**: Implement minimal FAIM variant (L=2, d_model=64) and systematically disable AFB and IMB components to reproduce 5-7% accuracy differences on FordA and GunPoint datasets

2. **Noise robustness reproduction**: Add Gaussian noise (σ=0.5, 1.0, 1.5) to ArticularyWordRecognition and plot FAIM's classification accuracy curve against baselines (DLinear, TapNet) to match reported trends

3. **Pre-training impact isolation**: Train FAIM with and without 100-epoch pre-training stage on 50% of UWaveGestureLibraryAll data, measuring accuracy gap to quantify self-supervised contribution