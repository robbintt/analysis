---
ver: rpa2
title: On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation
  Systems
arxiv_id: '2502.14759'
source_url: https://arxiv.org/abs/2502.14759
tags:
- context
- snippets
- performance
- answer
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates Retrieval-Augmented Generation
  (RAG) systems for long-form question answering across biomedical and encyclopedic
  domains. We analyze the impact of context size, retrieval strategies, and base LLMs
  on RAG performance.
---

# On the Influence of Context Size and Model Choice in Retrieval-Augmented Generation Systems

## Quick Facts
- arXiv ID: 2502.14759
- Source URL: https://arxiv.org/abs/2502.14759
- Authors: Juraj Vladika; Florian Matthes
- Reference count: 28
- Primary result: QA performance improves up to 15 context snippets, after which it plateaus or declines

## Executive Summary
This study systematically evaluates Retrieval-Augmented Generation (RAG) systems for long-form question answering across biomedical and encyclopedic domains. We analyze the impact of context size, retrieval strategies, and base LLMs on RAG performance. Our findings show that QA performance improves steadily up to 15 context snippets, after which it plateaus or declines. Different LLMs excel in different domains: Mistral and Qwen perform best in biomedical QA, while GPT and LLaMa excel in encyclopedic QA. Open-domain retrieval is challenging, with BM25 outperforming semantic search in the biomedical domain. We observe that RAG systems often underperform compared to LLMs' internal knowledge for complex queries, highlighting knowledge conflicts between retrieved and internal information.

## Method Summary
The study evaluates long-form question answering using RAG systems across biomedical (BioASQ-QA) and encyclopedic (QuoteSum) domains. Retrieval uses both BM25 (sparse keyword matching) and semantic (dense embeddings) approaches on MEDLINE 2022 (10.6M documents) and Wikipedia. Context size varies from 1-30 snippets per query. Eight base LLMs are tested: GPT-3.5, GPT-4o, LLaMa-3 70B/8B, Mixtral 8x7B, Mistral 7B, Gemma 7B, and Qwen 1.5 7B. Evaluation uses ROUGE-L, BERTScore, and NLI entailment (DeBERTa-v3-tasksource) with zero-shot prompting template.

## Key Results
- QA performance improves steadily with up to 15 snippets but stagnates or declines beyond that threshold
- BM25 outperforms semantic search in biomedical open-domain QA by optimizing for precision over recall
- Different LLMs excel in different domains: Mistral/Qwen for biomedical QA, GPT/LLaMa for encyclopedic QA
- RAG systems often underperform compared to LLM internal knowledge for complex queries due to knowledge conflicts

## Why This Works (Mechanism)

### Mechanism 1: Context Saturation and Positional Attention
Increasing context snippets improves long-form QA performance up to approximately 15 snippets, after which returns diminish or reverse. Models allocate attention across context; as snippet count grows, relevant information competes with noise. Beyond a threshold, attention is spread too thin or mid-sequence information is underweighted.

### Mechanism 2: Domain-Conditioned Model Specialization
Optimal reader LLM choice varies by domain; biomedical QA favors Mistral/Qwen while encyclopedic QA favors GPT/LLaMa. Pre-training data composition and instruction-tuning create implicit domain priors. Models with stronger biomedical exposure better leverage specialized terminology and citation patterns.

### Mechanism 3: Retrieval Precision Over Coverage in Specialized Domains
For biomedical open-domain QA, BM25 (sparse keyword matching) outperforms semantic (dense) search in final answer quality. Biomedical queries contain precise terminology where exact lexical matches correlate with relevance. BM25 optimizes for precision; semantic search introduces false positives through conceptual similarity.

### Mechanism 4: Internal-External Knowledge Conflict
For complex queries, RAG with imperfect retrieval can underperform compared to LLM internal knowledge alone. Retrieved snippets may be semantically similar but incomplete; LLM instructed to rely only on context produces constrained answers, whereas internal knowledge synthesizes broader patterns.

## Foundational Learning

- **Sparse vs Dense Retrieval**: Why needed - The paper explicitly compares BM25 and embedding-based search; understanding lexical matching vs vector similarity is prerequisite to retriever selection. Quick check - Would a query like "EGFR inhibitor resistance mechanisms" benefit more from keyword matching or semantic similarity in a biomedical corpus?

- **Context Window and Attention Distribution**: Why needed - Performance plateau at 15+ snippets relates to how transformer attention spreads across tokens; understanding positional bias informs chunking and ordering strategies. Quick check - If a model attends primarily to start/end of context, where should the most critical evidence be placed?

- **NLI-based Evaluation for Long-Form QA**: Why needed - The paper uses entailment (NLI) as a semantic metric alongside ROUGE/BERTScore; understanding logical entailment vs lexical overlap is key to interpreting results. Quick check - Why might a generated answer have high ROUGE but low entailment relative to a reference?

## Architecture Onboarding

- **Component map**: Query → Retriever → Top-k docs → Sentence extractor → Snippets → Prompt → Reader LLM → Answer → Evaluator → Metrics

- **Critical path**: 
  1. Query → Retriever → Top-k docs
  2. Docs → Sentence extractor → Snippets
  3. Snippets + Query → Prompt → Reader LLM → Answer
  4. Answer + Reference → Evaluator → Metrics

- **Design tradeoffs**:
  - Precision vs Recall: BM25 gives precision; semantic gives coverage. Choose based on domain terminology.
  - Context length vs Noise: More snippets add signal but also noise; plateau at ~15.
  - Model size vs Domain fit: Larger models do not always outperform smaller domain-aligned models.

- **Failure signatures**:
  - Early plateau: Metrics stop improving before 10 snippets → likely retriever returning irrelevant or redundant chunks
  - Context underperformance vs zero-shot: RAG with k<10 loses to no-context → check retrieval relevance or query-snippet mismatch
  - Domain mismatch: Encyclopedic-tuned model underperforms on biomedical → switch reader or fine-tune retriever for terminology

- **First 3 experiments**:
  1. Establish baseline: Test each LLM with gold snippets (k=1, 5, 10) to isolate reader capacity without retrieval noise
  2. Retriever comparison: On open-domain setting, compare BM25 vs semantic search at k=10, 15, 20 using top-performing reader per domain
  3. Saturation probe: Increment k to 30 and identify per-model plateau point; check whether repositioning gold snippets to start/end of context mitigates "lost in the middle"

## Open Questions the Paper Calls Out

- **Query expansion and evidence re-ranking**: How do pre-retrieval strategies (like query expansion) and post-retrieval strategies (like evidence re-ranking) influence the performance saturation point in long-form RAG systems?

- **Knowledge conflict optimization**: How can RAG systems be optimized to dynamically resolve conflicts between an LLM's internal knowledge and retrieved external context?

- **Few-shot prompting effects**: Does a few-shot prompting setting reduce the performance variance between different LLM architectures in long-form QA tasks?

- **Domain generalizability of retrieval methods**: To what extent does the observed advantage of BM25 over semantic search in the biomedical domain hold true for corpora with different lexical densities?

## Limitations
- Context size plateau findings based on only two specific datasets (BioASQ-QA and QuoteSum) with their own characteristics
- Comparison between BM25 and semantic search may shift with different preprocessing pipelines or embedding models
- Knowledge conflict between retrieved and internal LLM information remains qualitative rather than quantitatively measured
- Analysis doesn't account for potential latency or cost trade-offs when selecting optimal context sizes or models

## Confidence

- **High confidence**: Context size optimization (15-snippet plateau) - supported by systematic sweeps across multiple models and datasets with clear performance curves
- **Medium confidence**: Domain-specific model selection - results show consistent patterns but could shift with different biomedical fine-tuning approaches or encyclopedic domain variations
- **Medium confidence**: BM25 vs semantic search in biomedical domains - findings are robust for the tested PubMed corpus but may not extend to other specialized domains
- **Low confidence**: Internal vs external knowledge conflict - observation is qualitative and based on limited comparisons; quantification and deeper analysis needed

## Next Checks
1. **Cross-domain replication**: Test the 15-snippet saturation point and domain-specific model preferences on at least two additional QA datasets (e.g., HotpotQA for multi-hop reasoning and a clinical decision support dataset) to verify generalization.

2. **Dynamic context optimization**: Implement and evaluate adaptive context selection strategies (like SARA or First Token Probability Guided RAG) to determine if intelligent snippet selection can push performance beyond the 15-snippet plateau or reduce noise in larger contexts.

3. **Quantitative knowledge conflict analysis**: Design experiments to measure when and why retrieved contexts harm performance compared to zero-shot internal knowledge, potentially using techniques like oracle retrieval (gold context) vs noisy retrieval to isolate the effect.