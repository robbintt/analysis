---
ver: rpa2
title: Model State Arithmetic for Machine Unlearning
arxiv_id: '2506.20941'
source_url: https://arxiv.org/abs/2506.20941
tags:
- unlearning
- forget
- arxiv
- ideal
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes MSA (Model State Arithmetic), a new approach
  for machine unlearning that leverages intermediate model checkpoints to more effectively
  remove the influence of targeted data points. MSA computes a forget vector from
  a checkpoint prior to exposure to the unlearning targets and applies it to the final
  model to reverse the effect of the target data.
---

# Model State Arithmetic for Machine Unlearning

## Quick Facts
- arXiv ID: 2506.20941
- Source URL: https://arxiv.org/abs/2506.20941
- Authors: Keivan Rezaei; Mehrdad Saberi; Abhilasha Ravichander; Soheil Feizi
- Reference count: 24
- Key outcome: MSA leverages intermediate checkpoints to compute forget vectors that remove targeted data influence more effectively than existing methods across multiple benchmarks

## Executive Summary
This paper introduces Model State Arithmetic (MSA), a novel approach to machine unlearning that uses intermediate model checkpoints to compute forget vectors for removing targeted data points. The method addresses a key challenge in data-level unlearning by aligning the post-unlearning model more closely with an ideal reference model. By leveraging influence functions and checkpoint arithmetic, MSA demonstrates superior performance in forgetting efficacy, model utility preservation, and privacy leakage across multiple benchmarks including TOFU, RESTOR, and MUSE-Books.

## Method Summary
MSA computes a forget vector from a checkpoint taken prior to exposure to unlearning targets and applies it to the final model to reverse the effect of target data. The approach leverages influence functions to estimate how much each parameter contributes to the loss on the unlearning targets. By using checkpoints from various points in training (even hundreds of billions of tokens before the targets), MSA can effectively remove the influence of targeted data while preserving model utility. The method addresses the challenge of aligning the post-unlearning model with an ideal reference model that never saw the unlearning data.

## Key Results
- MSA consistently outperforms or matches existing unlearning algorithms across multiple benchmarks (TOFU, RESTOR, MUSE-Books)
- The method achieves strong performance even when using checkpoints hundreds of billions of tokens before the unlearning targets
- MSA demonstrates robustness to checkpointing frequency while maintaining effectiveness in forgetting efficacy, model utility preservation, and privacy leakage reduction

## Why This Works (Mechanism)
The core mechanism relies on influence functions to estimate parameter contributions to loss on unlearning targets. By computing a forget vector from an earlier checkpoint (before exposure to target data) and applying it to the final model, MSA effectively reverses the influence of targeted data points. This approach leverages the mathematical relationship between model states at different training stages, allowing for precise removal of unwanted data influence while maintaining the beneficial knowledge learned from other data points.

## Foundational Learning
- **Influence Functions**: Mathematical tools for estimating how model parameters contribute to loss on specific data points; needed to identify which parameters to modify during unlearning
- **Checkpoint Arithmetic**: The concept of combining model states from different training stages; quick check: verify that parameter differences between checkpoints capture meaningful learning dynamics
- **Data-level Unlearning**: The process of removing specific data points' influence from trained models; needed because traditional retraining is computationally expensive
- **Membership Inference**: Privacy attack that determines whether specific data was used in training; quick check: measure whether unlearning actually reduces membership inference accuracy
- **Reference Model Alignment**: The goal of making post-unlearning models match models that never saw the unlearning data; needed to ensure complete data removal

## Architecture Onboarding

**Component Map**
Checkpoint Storage -> Influence Weight Computation -> Forget Vector Generation -> Final Model Update

**Critical Path**
The critical path involves computing influence weights between checkpoints and the final model, then applying the forget vector. This requires access to intermediate checkpoints and the ability to efficiently compute parameter-level influence.

**Design Tradeoffs**
The method trades increased storage requirements (for checkpoints) against computational efficiency (avoiding full retraining). Using checkpoints further from the unlearning targets requires less storage but may reduce unlearning effectiveness.

**Failure Signatures**
- Insufficient checkpoint frequency leading to poor unlearning performance
- Computational overhead becoming prohibitive for very large models
- Influence weight estimation becoming unstable when checkpoints are too far apart

**First Experiments**
1. Verify that forget vectors computed from checkpoints effectively remove influence of synthetic target data
2. Test robustness to checkpoint frequency by varying the distance between checkpoints and unlearning targets
3. Compare privacy leakage reduction against baseline unlearning methods on membership inference tasks

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Reliance on intermediate checkpoints may limit applicability when checkpoints are not available or stored
- Computational overhead of computing and storing influence weights across checkpoints could be significant for very large models
- Experimental validation focuses primarily on transformer-based language models, with unclear generalizability to other architectures

## Confidence

**High Confidence**: The core algorithmic approach and its implementation are sound, with reproducible code provided. The theoretical foundation connecting influence functions to model state arithmetic is well-established.

**Medium Confidence**: The comparative performance claims against existing methods are supported by the reported experiments, though the choice of baselines and evaluation metrics could be expanded.

**Medium Confidence**: The robustness claims regarding checkpoint frequency are demonstrated but would benefit from broader testing across different model scales and training dynamics.

## Next Checks
1. Test MSA's effectiveness on non-transformer architectures (CNNs, GNNs) and different task types (computer vision, graph learning) to assess generalizability.
2. Evaluate the method's performance when the exact influence of unlearning targets is uncertain or when dealing with noisy annotations of which data points to remove.
3. Conduct a comprehensive analysis of the computational and storage overhead across different model scales, particularly for models with hundreds of billions of parameters.