---
ver: rpa2
title: 'AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining
  Data Selection'
arxiv_id: '2505.07293'
source_url: https://arxiv.org/abs/2505.07293
tags:
- data
- attentioninfluence
- arxiv
- heads
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "AttentionInfluence is a training-free data selection method that\
  \ leverages attention head activation patterns in pretrained language models to\
  \ identify reasoning-intensive pretraining data. The method detects retrieval heads\u2014\
  attention heads critical for reasoning and in-context learning\u2014and selects\
  \ samples causing the largest performance drop when these heads are masked."
---

# AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection

## Quick Facts
- arXiv ID: 2505.07293
- Source URL: https://arxiv.org/abs/2505.07293
- Authors: Kai Hua; Steven Wu; Ge Zhang; Ke Shen
- Reference count: 40
- A training-free data selection method that leverages attention head activation patterns to identify reasoning-intensive pretraining data

## Executive Summary
AttentionInfluence is a training-free data selection method that identifies reasoning-intensive pretraining data by analyzing attention head activation patterns in pretrained language models. The method detects retrieval heads—attention heads critical for reasoning and in-context learning—and selects samples causing the largest performance drop when these heads are masked. When applied to select 73B tokens from SmolLM-Corpus using a 1.3B model, training a 7B model on the selected data improved performance by 1.4-3.5 percentage points across knowledge-intensive and reasoning-heavy benchmarks including MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval. The approach demonstrates weak-to-strong scaling where smaller models can effectively select data that improves the performance of larger models.

## Method Summary
AttentionInfluence operates by first identifying retrieval heads through masking analysis—specific attention heads whose removal causes the largest performance degradation on reasoning tasks. The method then scores pretraining samples based on the performance drop when these critical heads are masked during inference. Samples causing the largest performance drops are selected as reasoning-intensive data. The approach is training-free, requiring only a pretrained model and the pretraining corpus. When applied to select 73B tokens from SmolLM-Corpus using a 1.3B model, the selected data was used to train a 7B model, demonstrating the weak-to-strong scaling property where smaller models can effectively curate data for larger models.

## Key Results
- Training a 7B model on 73B tokens selected by AttentionInfluence from SmolLM-Corpus improved performance by 1.4-3.5 percentage points across benchmarks
- Selected data was higher quality, more diverse, and more reasoning-intensive than baseline selection methods
- Demonstrated weak-to-strong scaling property where 1.3B model effectively selects data for 7B model training
- Performance gains observed on MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval benchmarks

## Why This Works (Mechanism)
The method exploits the observation that certain attention heads in pretrained language models are critical for reasoning and in-context learning. By identifying these retrieval heads through masking experiments, AttentionInfluence can quantify how reasoning-intensive each pretraining sample is based on the performance drop when these heads are disabled. This creates a training-free proxy for reasoning complexity that can be computed efficiently without expensive full training runs. The weak-to-strong scaling emerges because the pattern of which attention heads are important for reasoning is preserved across model sizes, allowing smaller models to effectively identify reasoning-intensive data for larger models.

## Foundational Learning
**Attention Mechanisms in Transformers** - Why needed: Core to understanding how models process information and why certain heads are critical for reasoning. Quick check: Verify understanding of multi-head attention and query-key-value operations.
**In-Context Learning** - Why needed: The method specifically targets data that improves this capability, which is central to modern LLMs. Quick check: Can explain how models perform tasks without gradient updates using context.
**Retrieval Head Concept** - Why needed: These are the specific attention heads that AttentionInfluence identifies and leverages. Quick check: Understand how masking experiments reveal head importance for specific capabilities.
**Model Masking Techniques** - Why needed: The core mechanism for identifying critical attention heads through performance degradation analysis. Quick check: Can describe how systematic head masking reveals functional importance.
**Data Selection for Pretraining** - Why needed: Context for why this method matters in the broader landscape of LLM development. Quick check: Understand the trade-offs between data quantity, quality, and training efficiency.

## Architecture Onboarding

**Component Map**: Pretraining Corpus -> Attention Head Masking -> Performance Evaluation -> Sample Scoring -> Selected Dataset

**Critical Path**: The method requires (1) a pretrained language model, (2) reasoning benchmarks for head importance evaluation, (3) the pretraining corpus to be filtered, and (4) a masking mechanism to assess head importance. The core loop involves masking each attention head, measuring performance degradation on reasoning tasks, identifying retrieval heads, then scoring corpus samples based on performance drops when those heads are masked.

**Design Tradeoffs**: The training-free approach trades potential accuracy gains from fine-tuning-based selection for computational efficiency and scalability. Using smaller models for selection sacrifices some precision in identifying reasoning-intensive data but enables weak-to-strong scaling and reduces computational costs. The method assumes that attention head importance patterns are preserved across model sizes, which may not hold for all model architectures.

**Failure Signatures**: Poor performance if the pretrained model lacks sufficient reasoning capability to identify meaningful retrieval heads. Selection quality degrades if the reasoning benchmarks used for head identification don't align with the target tasks. The method may miss domain-specific reasoning patterns if the pretrained model wasn't exposed to relevant data during pretraining.

**First Experiments**: 
1. Apply head masking to a pretrained model on a reasoning benchmark and measure performance degradation to identify retrieval heads
2. Score a small sample of pretraining data using the identified retrieval heads and verify that high-scoring samples contain more complex reasoning
3. Compare AttentionInfluence-selected data against random selection by training small models and measuring performance differences

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Limited evidence for cross-domain and cross-linguistic applicability beyond reasoning-intensive tasks
- Computational efficiency gains from using smaller models for selection are not quantified
- Selection of 73B tokens may not represent full diversity of available pretraining data
- Method effectiveness for domain-specific or multilingual corpora remains unproven

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| AttentionInfluence improves reasoning performance on tested benchmarks | High |
| Selected data is higher quality and more diverse than baselines | Medium |
| Weak-to-strong scaling property demonstrated | Low |

## Next Checks

1. **Cross-Domain Generalization**: Test AttentionInfluence on domain-specific datasets (e.g., biomedical, legal, or technical domains) to evaluate its effectiveness beyond reasoning-intensive tasks.

2. **Computational Efficiency Analysis**: Quantify the computational savings of using a smaller model for data selection compared to traditional methods, including time and resource requirements.

3. **Multilingual and Cross-Lingual Evaluation**: Apply AttentionInfluence to multilingual corpora to assess its ability to identify reasoning-intensive data across languages and cultures.