---
ver: rpa2
title: Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of
  Weekly Retail Sales
arxiv_id: '2511.00552'
source_url: https://arxiv.org/abs/2511.00552
tags:
- sales
- forecasting
- data
- temporal
- weekly
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of multi-horizon probabilistic
  forecasting for weekly retail sales, aiming to improve inventory and promotion planning
  in dynamic environments. It introduces a Temporal Fusion Transformer (TFT) model
  that fuses static store identifiers with time-varying exogenous signals such as
  holidays, CPI, fuel price, and temperature.
---

# Temporal Fusion Transformer for Multi-Horizon Probabilistic Forecasting of Weekly Retail Sales

## Quick Facts
- arXiv ID: 2511.00552
- Source URL: https://arxiv.org/abs/2511.00552
- Reference count: 0
- Model achieved RMSE of $57.9k USD and R² of 0.9875 on hold-out set, outperforming XGB, CNN, LSTM, and CNN-LSTM baselines

## Executive Summary
This study addresses multi-horizon probabilistic forecasting for weekly retail sales using a Temporal Fusion Transformer (TFT) model. The approach fuses static store identifiers with time-varying exogenous signals including holidays, CPI, fuel price, and temperature to produce calibrated 90% prediction intervals for 1-5 week ahead forecasts. Tested on Walmart sales data (45 stores, 2010-2012), the TFT model achieved strong performance metrics including RMSE of $57.9k USD and R² of 0.9875, demonstrating practical value for inventory optimization and operational planning in dynamic retail environments.

## Method Summary
The methodology employs a Temporal Fusion Transformer architecture configured with 52-week encoder length and 5-week prediction horizon. The model incorporates static store identifiers through a dedicated encoder and processes time-varying features including CPI, fuel price, temperature, and holiday flags through Variable Selection Networks and Gated Residual Networks. Training uses Quantile Loss at 0.1, 0.5, and 0.9 quantiles with batch size 16, hidden size 64, and attention heads 4. The Walmart dataset from Kaggle is preprocessed with log-transformation of the target variable and standardization of features. Performance is evaluated using RMSE, MAE, R², and SMAPE metrics with both hold-out validation and 5-fold cross-validation.

## Key Results
- TFT achieved RMSE of $57,973.84 and R² of 0.9875 on fixed 2012 hold-out set
- 5-fold cross-validation averaged RMSE of $64,626.95 and R² of 0.9844
- Outperformed XGB, CNN, LSTM, and CNN-LSTM baselines across all metrics
- Model training completed in approximately 780 seconds on local hardware

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Static covariate encoding enables store-specific behavioral patterns to be learned alongside shared temporal dynamics
- Mechanism: The Static Covariate Encoder creates embeddings from categorical store identifiers; these are fused with time-varying exogenous signals through Gated Residual Networks, allowing the model to learn both location-invariant temporal patterns and store-specific biases simultaneously
- Core assumption: Store identity captures stable, unobserved factors (size, demographics, regional preferences) that systematically shift baseline sales but do not change week-to-week
- Evidence anchors: [abstract] "fuses static store identifiers with time-varying exogenous signals"; [section II.B] "Static covariates (Store ID) are incorporated using a static covariate encoder to provide global context across sequences"

### Mechanism 2
- Claim: Multi-head temporal attention selectively weights historical weeks that are most predictive for each forecast horizon
- Mechanism: The decoder's multi-head attention mechanism computes attention scores over the 52-week encoder context; this allows the model to emphasize specific past weeks rather than treating all history uniformly
- Core assumption: Relevant historical patterns are not uniformly distributed across time; specific lags contain disproportionate predictive signal for particular horizons
- Evidence anchors: [section IV] "Multi-head attention mechanisms enhance interpretability by emphasizing key time steps and variables"; [Fig. 5 description] "illustrates the attention distribution across prior time steps"

### Mechanism 3
- Claim: Quantile loss produces calibrated prediction intervals by directly optimizing for multiple output quantiles simultaneously
- Mechanism: Rather than minimizing squared error alone, TFT is trained with Quantile Loss at τ ∈ {0.1, 0.5, 0.9}, forcing the network to output the 10th, 50th (median), and 90th percentiles
- Core assumption: The conditional distribution of sales given inputs is sufficiently captured by these quantiles; the loss function properly calibrates interval coverage
- Evidence anchors: [abstract] "produces probabilistic forecasts with 90% prediction intervals"; [section II.B] "QuantileLoss (0.1, 0.5, and 0.9)" used with specified hyperparameters

## Foundational Learning

- **Quantile Regression and Loss Functions**
  - Why needed here: TFT outputs prediction intervals, not point forecasts; understanding pinball loss is essential to debug calibration
  - Quick check question: Given a forecast quantile q̂ at τ=0.9 and actual value y, what is the pinball loss if y falls below q̂?

- **Attention Mechanisms and Self-Attention**
  - Why needed here: Multi-head attention is the core driver of TFT's ability to weight historical time steps; interpreting attention weights is key to explainability
  - Quick check question: In scaled dot-product attention, what happens to attention weights when query and key vectors are orthogonal?

- **Static vs. Time-Varying Covariates in Time Series**
  - Why needed here: Distinguishing what can be known in advance (known inputs) vs. must be inferred (targets) is critical for configuring TimeSeriesDataSet objects correctly
  - Quick check question: Is "holiday flag for next week" a known or unknown input at prediction time? What about "temperature next week"?

## Architecture Onboarding

- **Component map**:
  Static Covariate Encoder -> Variable Selection Network -> LSTM Encoder -> Multi-Head Attention (Decoder) -> Gated Residual Networks -> Quantile Output Layer

- **Critical path**:
  1. Preprocess data -> log-transform target, scale features, create time index
  2. Build TimeSeriesDataSet -> specify static cats, known reals, unknown targets
  3. Configure TFT -> encoder_length=52, prediction_length=5, quantiles=[0.1, 0.5, 0.9]
  4. Train with QuantileLoss -> monitor validation loss for overfitting
  5. Evaluate -> RMSE, MAE, R², SMAPE, plus calibration checks on PI coverage

- **Design tradeoffs**:
  - Longer encoder (52 weeks) captures yearly seasonality but increases memory/compute; shorter encoders may miss annual patterns
  - More attention heads (4 used) increase expressiveness but risk overfitting on small datasets (45 stores × ~140 weeks)
  - Log-transform stabilizes training but requires exponentiation for interpretability; residuals may behave differently on original scale

- **Failure signatures**:
  - Attention weights near-uniform -> model not learning selective temporal patterns
  - PI coverage far from 90% -> quantile miscalibration, may need loss weighting adjustment
  - Residuals show systematic patterns -> missing exogenous variables or insufficient model capacity
  - Fold-to-fold variance high in cross-validation -> model sensitive to temporal split; check for regime shifts

- **First 3 experiments**:
  1. Baseline sanity check: Train TFT with only historical sales (no exogenous features, no static covariates) to isolate attention contribution
  2. Ablation on static encoder: Compare TFT with vs. without Store ID embeddings to quantify static covariate value
  3. Calibration diagnostic: On held-out 2012 data, compute empirical coverage of 90% PIs and mean interval width; flag if coverage <85% or >95%

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the TFT model maintain its superior performance and calibration across diverse retail datasets outside of the specific Walmart 2010-2012 sample?
- Basis in paper: [explicit] The authors state, "In the future, we want to verify the model across numerous benchmark datasets to examine its generalizability and resilience in various forecasting situations"
- Why unresolved: The current study is restricted to a single dataset from a specific retailer spanning 2010-2012, limiting the ability to generalize findings to other retail environments or time periods with different noise profiles
- What evidence would resolve it: Performance metrics (RMSE, R²) and calibration scores derived from applying the same TFT architecture to other public retail benchmarks (e.g., Favorita) or more contemporary datasets

### Open Question 2
- Question: Is the higher accuracy of the TFT justified by its computational cost and latency compared to much simpler baselines like XGB?
- Basis in paper: [inferred] The paper mentions a training time of ~780 seconds on a local laptop and acknowledges that other Transformer variants require "substantial processing resources," yet it omits a comparative efficiency analysis against the XGB or LSTM baselines
- Why unresolved: Without comparing training duration and inference latency, it is unclear if the accuracy gain (R² improvement of ~0.03-0.09 over baselines) is worth the computational overhead for real-time or large-scale deployment
- What evidence would resolve it: A benchmark comparison of training time, inference speed, and memory usage per forecast between TFT and the XGB/CNN/LSTM baselines on the same hardware

### Open Question 3
- Question: How robust are the probabilistic prediction intervals when facing extreme exogenous shocks or "black swan" events not present in the training data?
- Basis in paper: [inferred] The paper notes the dataset covers 2010-2012 and aims to reduce uncertainty from "unforeseen natural disasters," but the historical data likely lacks the severity of modern supply chain disruptions or pandemics to test this claim
- Why unresolved: The model's quantile loss function is trained on standard volatility; it is uncertain if the 90% prediction intervals would maintain calibration (coverage probability) under extreme market stress absent from the training set
- What evidence would resolve it: Evaluation of the model's coverage probability on out-of-sample data containing high-impact, low-frequency events (e.g., the COVID-19 pandemic or major supply chain interruptions)

## Limitations

- Small sample size (45 stores × 140 weeks) limits confidence in model stability across different retail environments
- Cross-validation results show some variance (RMSE ranging from ~60k to ~70k USD), suggesting potential sensitivity to data splits
- Static covariate encoder's effectiveness is weakly supported by direct evidence, with most validation coming from architectural design rather than controlled ablation studies

## Confidence

- High confidence in multi-head attention mechanism effectiveness (supported by attention visualization and corpus evidence)
- Medium confidence in static covariate encoding benefits (design rationale present but limited ablation evidence)
- Medium confidence in quantile calibration (nominal 90% PI coverage claimed but not empirically validated with coverage diagnostics)

## Next Checks

1. Perform quantile calibration analysis on held-out 2012 data to verify actual 90% PI coverage falls within [85%, 95%]
2. Conduct controlled ablation study comparing TFT with and without static covariate encoder to isolate its contribution
3. Test model generalization on out-of-sample time periods or different retail domains to assess temporal robustness beyond the 2012 hold-out