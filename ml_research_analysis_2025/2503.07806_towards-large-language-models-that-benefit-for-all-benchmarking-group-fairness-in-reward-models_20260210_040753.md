---
ver: rpa2
title: 'Towards Large Language Models that Benefit for All: Benchmarking Group Fairness
  in Reward Models'
arxiv_id: '2503.07806'
source_url: https://arxiv.org/abs/2503.07806
tags:
- group
- reward
- fairness
- groups
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new benchmark for evaluating group fairness
  in reward models used in large language models. Unlike previous approaches, it addresses
  the challenge of measuring fairness when users from different demographic groups
  ask different prompts.
---

# Towards Large Language Models that Benefit for All: Benchmarking Group Fairness in Reward Models

## Quick Facts
- arXiv ID: 2503.07806
- Source URL: https://arxiv.org/abs/2503.07806
- Reference count: 8
- Primary result: Statistically significant group unfairness found in all evaluated reward models, with disparities across most demographic group pairs

## Executive Summary
This paper introduces a new benchmark for evaluating group fairness in reward models used in large language models. The key innovation addresses the challenge of measuring fairness when users from different demographic groups ask different prompts by using arXiv metadataâ€”expert-written abstracts from eight academic fields as proxies for demographic groups. The authors evaluate eight top-performing reward models and find statistically significant group unfairness in all cases, with disparities present across most demographic group pairs. Interestingly, the top-performing models on the RewardBench leaderboard also demonstrate better group fairness, suggesting that fairness and performance may be positively correlated in reward modeling.

## Method Summary
The paper proposes a novel approach to benchmarking group fairness in reward models by leveraging arXiv metadata as a proxy for demographic groups. The method uses 2000 title-abstract pairs from each of eight academic fields (physics, mathematics, computer science, economics, electrical engineering, system science, quantitative biology, and quantitative finance). Standard prompts ("Write an abstract for a paper with title <Title>") are used with expert-written abstracts as ground truth responses. The evaluation measures fairness through Normalized Maximum Group Difference, ANOVA tests for statistical significance, and Tukey HSD tests for pairwise comparisons. Eight reward models from the RewardBench leaderboard are evaluated, excluding LLM-as-a-Judge and pairwise models.

## Key Results
- Statistically significant group unfairness detected in all eight evaluated reward models (ANOVA p-values < 0.0001)
- Disparities present across most demographic group pairs as identified by Tukey HSD tests
- Top-performing models on RewardBench also show better group fairness, suggesting positive correlation between canonical performance and fairness
- Systemic bias in reward models could propagate to final LLM outputs during reinforcement learning stage

## Why This Works (Mechanism)

### Mechanism 1: Proxies for assessing group fairness using domain-specific prompts
The arXiv benchmark provides a viable proxy for assessing group fairness by using expert-written abstracts from different academic fields as distinct demographic groups. The core mechanism relies on the assumption that occupational groups can serve as valid social groups. High-quality expert text from each domain is used as ground-truth responses to prompts based on paper titles, measuring if a reward model assigns equitable scores across domains.

### Mechanism 2: Pinpointing bias propagation in the RLHF pipeline
Evaluating reward models in isolation is crucial for identifying a specific source of bias before it propagates to final LLM output during reinforcement learning. Biases can be introduced at any stage of RLHF (SFT, reward modeling, RL), and an unfair reward signal will steer the LLM to generate outputs preferred by the biased reward model, thereby propagating the group unfairness.

### Mechanism 3: Statistical validation of group unfairness
ANOVA and Tukey HSD tests provide a rigorous statistical framework to confirm that observed disparities in reward scores are systematic group unfairness, not random variation. ANOVA determines if means across multiple groups are significantly different, while Tukey HSD pinpoints which specific group pairs have significantly different means.

## Foundational Learning

- **Concept: Reinforcement Learning from Human Feedback (RLHF)**
  - Why needed: This is the core training pipeline being analyzed, with three stages (SFT, Reward Modeling, RL) where bias can be introduced
  - Quick check: Can you name the three key stages of the RLHF pipeline and identify which one this work focuses on benchmarking?

- **Concept: Demographic Parity (Group Fairness)**
  - Why needed: This is the formal definition of fairness used, where a fair reward model provides approximately equal average reward scores to different social groups
  - Quick check: Based on Equation (1), what does it mean for a reward model to satisfy group fairness across a set of social groups G?

- **Concept: Bradley-Terry Model**
  - Why needed: This is the foundational preference model used to train the reward models, framing the learning objective as predicting preference probabilities
  - Quick check: In the context of training a reward model, what is the loss function derived from the Bradley-Terry model aiming to maximize?

## Architecture Onboarding

- **Component map:** arXiv Metadata Dataset -> Evaluation Prompt -> Reward Models (M) -> Fairness Evaluation Module
- **Critical path:** Starts with understanding the problem definition (group fairness in RLHF), then understanding the proxy data (arXiv) and evaluation construction, and finally interpreting statistical analysis (ANOVA/Tukey) that validates the core claim
- **Design tradeoffs:** Using academic abstracts allows for high-quality, expert-written data but limits demographic groups to occupational categories; tradeoff between improving fairness and maintaining canonical performance
- **Failure signatures:** High F-statistic and low p-value indicate group unfairness; high Normalized Maximum Group Difference (>50%) signals poor fairness; correlation patterns with known biased models suggest shared systemic bias
- **First 3 experiments:** 
  1. Establish baseline by running Nemotron-4-340B-Reward on small arXiv subset to reproduce group disparities
  2. Evaluate your own reward model using the methodology and calculate fairness metrics
  3. Perform pairwise analysis using Tukey HSD test to identify specific group pairs with most significant unfairness

## Open Questions the Paper Calls Out

- **Open Question 1:** What specific algorithms or training interventions can effectively mitigate the identified group unfairness in reward models without degrading canonical performance?
  - Basis: The conclusion states results highlight the "need for unfairness mitigation in reward models," yet provides no mitigation strategies
  - Why unresolved: Work focuses exclusively on defining the problem and establishing a benchmark for detection
  - What evidence would resolve it: A method that reduces NMGD on this benchmark while maintaining RewardBench scores

- **Open Question 2:** How does the group unfairness measured in reward models propagate to or interact with biases from other stages of the RLHF pipeline?
  - Basis: Section 3.3 outlines three stages where bias can arise but notes this work isolates only the reward modeling stage
  - Why unresolved: Evaluates reward model in isolation without measuring fairness of final LLM policy after RL
  - What evidence would resolve it: End-to-end evaluation measuring group fairness at each RLHF stage

- **Open Question 3:** Does the group unfairness observed using occupational proxies generalize to traditional protected attributes like gender, race, or geographic location?
  - Basis: Section 4.1 defines demographic groups based on academic disciplines due to lack of user demographic data
  - Why unresolved: Uncertain if "occupation" acts as sufficient proxy for social dimensions of fairness
  - What evidence would resolve it: Correlation analysis between fairness scores on arXiv benchmark and dataset with explicit protected attribute labels

## Limitations
- arXiv categories as demographic proxies restrict findings to academic domain biases rather than broader social demographic dimensions
- Assumption that expert-written abstracts across fields are of comparable quality cannot be fully validated
- Study demonstrates bias in reward models but does not empirically prove bias propagation to final LLM outputs

## Confidence
- **High confidence:** Statistical findings of group unfairness (ANOVA p-values < 0.0001, Tukey HSD pairwise differences) - directly computed from data using standard methods
- **Medium confidence:** Correlation between canonical performance and fairness - statistically measured but causal relationship unclear
- **Medium confidence:** arXiv as proxy for demographic groups - methodologically justified but represents significant simplifying assumption

## Next Checks
1. Validate cross-domain quality equivalence by conducting blinded quality assessment where domain experts rate abstracts across different arXiv categories
2. Test bias propagation empirically by running an RLHF training pipeline using a biased reward model and measuring whether resulting LLM shows corresponding group disparities
3. Extend to broader demographic proxies by replicating benchmark using datasets with explicit demographic annotations to validate whether arXiv findings generalize beyond academic domains