---
ver: rpa2
title: 'Trading Carbon for Physics: On the Resource Efficiency of Machine Learning
  for Spatio-Temporal Forecasting'
arxiv_id: '2509.24517'
source_url: https://arxiv.org/abs/2509.24517
tags:
- carbon
- physics
- data
- training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper examines how incorporating physics-based inductive
  biases into machine learning models can reduce carbon emissions while maintaining
  or improving predictive performance. The authors conduct experiments on three tasks:
  harmonic oscillator modeling, viscous Burgers'' equation approximation, and incompressible
  shear flow forecasting.'
---

# Trading Carbon for Physics: On the Resource Efficiency of Machine Learning for Spatio-Temporal Forecasting

## Quick Facts
- arXiv ID: 2509.24517
- Source URL: https://arxiv.org/abs/2509.24517
- Authors: Sophia N. Wilson; Jens Hesselbjerg Christensen; Raghavendra Selvan
- Reference count: 34
- One-line primary result: Physics-informed models achieve better accuracy-carbon trade-offs than purely data-driven approaches for spatio-temporal forecasting

## Executive Summary
This paper investigates how incorporating physics-based inductive biases into machine learning models can reduce carbon emissions while maintaining or improving predictive performance. The authors conduct experiments on three tasks: harmonic oscillator modeling, viscous Burgers' equation approximation, and incompressible shear flow forecasting. They compare models with varying levels of physics inductive bias, from purely data-driven U-nets to strongly biased Fourier neural operators and flow matching models. The key finding is that models with appropriate physics priors can achieve better accuracy-carbon trade-offs than purely data-driven approaches, with semi-supervised physics-informed neural networks (S-PINNs) dominating the Pareto frontier in most cases.

## Method Summary
The study compares models across three spatio-temporal forecasting tasks with varying physics inductive bias strengths. Experiments use Carbontracker to measure training and inference carbon emissions (kgCO₂eq) on Nvidia A40 GPUs. The harmonic oscillator task tests different activation functions with a 2-layer MLP; the viscous Burgers' equation uses 4-layer MLPs and U-net variants with physics loss terms; the incompressible shear flow task employs U-nets, Fourier neural operators (FNO), and flow matching models on The Well dataset. Models are evaluated on both accuracy (MSE, Pearson r) and carbon efficiency, with semi-supervised PINNs combining data and physics losses to optimize the accuracy-carbon trade-off.

## Key Results
- S-PINNs (semi-supervised PINNs) achieve lower test errors at lower carbon costs compared to purely data-driven neural networks on viscous Burgers' equation
- FNO variants show 2.5-7.5× lower training emissions than U-net variants for shear flow forecasting
- Fourier neural operators have higher inference costs than U-nets, creating a training-inference efficiency trade-off
- Snake activation functions achieve fastest training but poorest extrapolation on harmonic oscillator task

## Why This Works (Mechanism)

### Mechanism 1: Physics Constraints Reduce Searchable Hypothesis Space
Embedding physics inductive biases constrains the solution space, reducing computational resources needed to find accurate solutions. Physics priors act as regularization that reduces variance in the learning problem, requiring less data and fewer optimization steps to converge. This works when target systems follow embedded physics; misspecified priors introduce bias causing underfitting.

### Mechanism 2: Semi-supervised Data-Physics Fusion Dominates Pareto Frontier
Combining data-driven loss with physics loss achieves better accuracy-carbon trade-offs than purely data-driven or purely physics approaches. Data loss ensures fit to observed behavior; physics loss provides regularization and extrapolation capability. Together they require fewer total training resources than pure data approaches that must learn physics from scratch. This advantage disappears when physics constraints are trivially satisfied or overly restrictive.

### Mechanism 3: Training-Inference Efficiency Trade-off Depends on Architecture Choice
Models with strong physics inductive biases have low training costs but higher inference costs; optimal choice depends on deployment frequency. Spectral architectures learn global correlations efficiently during training via Fourier-space operations, but these operations remain computationally intensive at inference. Convolutional architectures require more training to learn global structure but execute faster at inference.

## Foundational Learning

- **Inductive Bias and Bias-Variance Tradeoff**
  - Why needed here: The paper's argument relies on understanding how physics priors shift the bias-variance tradeoff. Without this, claims about "physics reducing data requirements" appear unmotivated.
  - Quick check question: Why do periodic activation functions (Sine, Snake) improve extrapolation for harmonic oscillator modeling compared to ReLU?

- **Physics-Informed Neural Networks (PINNs)**
  - Why needed here: PINNs are a key architecture. Understanding how PDE residuals become loss terms is essential for implementing or modifying proposed approaches.
  - Quick check question: Given viscous Burgers' equation ∂u/∂t + u∂u/∂x - ν∂²u/∂x² = 0, write the PINN loss term enforcing this PDE.

- **Fourier Neural Operators and Spectral Methods**
  - Why needed here: FNOs represent the strongly-biased architecture in the main experiment. Understanding spectral representations explains the training efficiency results.
  - Quick check question: Why does the Fourier layer capture long-range dependencies more efficiently than standard convolutional layers?

## Architecture Onboarding

- **Component map:**
  ```
  Physics Bias Spectrum (weakest → strongest):
  U-Net → U-Net-CP → Flow Matching → FNO → TFNO → UFNO
  (none)   (weak)     (medium)      (strong spectral)
  
  Loss Components:
  L_total = α·L_data + β·L_PDE + γ·L_IC + δ·L_BC
  
  Architectural Differences:
  - U-Net: Local convolutions, zero/circular padding
  - FNO: Global spectral convolutions in Fourier space
  - FM: Continuous-time vector field via numerical integration
  ```

- **Critical path:**
  1. Identify available physics knowledge (governing equations, boundary conditions, symmetries)
  2. Map physics to inductive bias mechanism (loss term, architecture, or activation)
  3. Select architecture based on deployment scenario (training-heavy vs. inference-heavy)
  4. Instrument training with carbon/energy tracking (CarbonTracker)
  5. Evaluate on Pareto frontier (accuracy vs. carbon), not accuracy alone

- **Design tradeoffs:**
  - Stronger physics bias → lower training cost, potentially higher inference cost
  - Hybrid (data+physics) → Pareto-optimal, but requires both data and physics knowledge
  - Circular padding → respects periodicity, but may not match all boundary conditions
  - Flow matching → good temporal stability, but 2-5.5× higher inference cost from numerical integration

- **Failure signatures:**
  - Good training fit, poor extrapolation: Physics bias too weak or misaligned
  - Cannot fit training data: Physics constraints overly restrictive
  - Unexpectedly high inference cost: Check numerical integration steps or spectral resolution
  - Circular padding artifacts: Periodicity assumption violated

- **First 3 experiments:**
  1. Replicate harmonic oscillator: Compare ReLU vs. Sine vs. Snake activations on damped oscillator extrapolation to verify physics bias effects.
  2. Measure training carbon vs. test MSE for U-Net, FNO, and S-PINN on viscous Burgers' equation to confirm S-PINN dominates Pareto frontier.
  3. Profile inference time and carbon for all 8 models on single-step vs. 20-step rollout to quantify training-inference trade-off before deployment selection.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do development-phase carbon costs (hyperparameter search, architecture exploration, multiple training runs) affect the overall efficacy-efficiency trade-offs for physics-informed models compared to purely data-driven baselines?
- Basis in paper: The Limitations section states: "In practice, however, hyperparameter search can dominate computational cost, and complex models typically require many more training runs during development than simpler baselines." All models were trained without extensive tuning to emphasize architectural comparison over peak performance.
- Why unresolved: The study measured only training and inference emissions for final model configurations, excluding the substantial computational cost of model development and hyperparameter optimization.
- What evidence would resolve it: A comprehensive study tracking cumulative carbon emissions across the entire ML development lifecycle, including all hyperparameter tuning runs, for both physics-informed and data-driven models.

### Open Question 2
- Question: How do hardware-specific optimizations and different accelerator architectures affect the relative carbon efficiency rankings of models with varying physics inductive biases?
- Basis in paper: The Limitations section states: "our measurements do not account for hardware specific optimisations, where certain operations are more efficiently executed on specific accelerators."
- Why unresolved: All experiments were conducted on a single GPU type (Nvidia A40) without testing hardware-specific optimizations that may benefit different architectures unequally.
- What evidence would resolve it: Systematic benchmarking of the same model architectures across multiple hardware platforms with and without hardware-specific optimizations.

### Open Question 3
- Question: To what extent do the observed efficacy-efficiency trade-offs generalize to ML tasks beyond spatio-temporal forecasting when incorporating other forms of domain knowledge as inductive biases?
- Basis in paper: The Limitations section states: "we restricted our study to spatio-temporal forecasting. While this limits the scope of the contribution, the central message about taking efficiency considerations into account by including appropriate inductive biases is applicable across ML tasks."
- Why unresolved: The study only examined physics-based inductive biases for PDE-governed systems; it remains unclear whether similar carbon-accuracy trade-offs exist for linguistic, biological, or other domain-specific inductive biases.
- What evidence would resolve it: Comparative studies across diverse ML domains measuring carbon-accuracy trade-offs when incorporating various domain knowledge.

## Limitations
- All experiments conducted on single GPU type without hardware-specific optimizations
- Carbon measurements exclude development-phase costs like hyperparameter tuning
- Results may not generalize beyond the three specific PDE tasks examined

## Confidence
- **Medium** in the main claims about Pareto frontier dominance by hybrid approaches
- **Medium** in the training-inference efficiency trade-off claims
- **Low** in generalizability beyond the three specific tasks examined

## Next Checks
1. **Cross-domain validation**: Apply the same physics bias spectrum to a different physics domain (e.g., electromagnetic wave propagation) to test generalizability of the Pareto frontier findings.
2. **Long-term deployment simulation**: Extend the rollout experiments beyond 20 steps to quantify how training-inference trade-offs evolve over extended prediction horizons.
3. **Sensitivity analysis**: Systematically vary the physics loss weights to map the full Pareto frontier and identify optimal operating points for different deployment scenarios.