---
ver: rpa2
title: 'Zero-Shot Keyphrase Generation: Investigating Specialized Instructions and
  Multi-Sample Aggregation on Large Language Models'
arxiv_id: '2503.00597'
source_url: https://arxiv.org/abs/2503.00597
tags:
- keyphrases
- keyphrase
- generation
- present
- absent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  for zero-shot keyphrase generation from scientific documents. The authors systematically
  explore how task-relevant specialized instructions and multi-sample aggregation
  strategies affect performance.
---

# Zero-Shot Keyphrase Generation: Investigating Specialized Instructions and Multi-Sample Aggregation on Large Language Models

## Quick Facts
- arXiv ID: 2503.00597
- Source URL: https://arxiv.org/abs/2503.00597
- Authors: Jayanth Mohan; Jishnu Ray Chowdhury; Tomas Malik; Cornelia Caragea
- Reference count: 34
- One-line primary result: Multi-sample frequency aggregation (Frequency Order) consistently improves zero-shot keyphrase generation performance across all tested models and datasets.

## Executive Summary
This paper investigates how task-relevant specialized instructions and multi-sample aggregation strategies affect zero-shot keyphrase generation from scientific documents using large language models. The authors systematically test specialist prompts for present and absent keyphrases, additional instructions for order and length control, and various aggregation methods including union, concatenation, interleaf, and frequency-based approaches across multiple samples. Their key finding is that Frequency Order aggregation—counting normalized keyphrase frequency across samples and sorting by descending frequency—consistently outperforms baseline approaches across all models and datasets for both present and absent keyphrases.

## Method Summary
The method uses large language models (Llama-3, Phi-3, GPT-4o) with multi-sample generation (n=10, temperature=0.8) followed by various aggregation strategies. Samples are normalized (lowercase, Porter stemming, deduplication), ranked by perplexity, then aggregated using union, concatenation, interleaf, or frequency approaches. The final output is dynamically truncated based on per-sample average keyphrase counts. The Frequency Order method counts occurrences of each normalized keyphrase across all samples, sorts by frequency descending, and breaks ties using interleaf order from the perplexity-ranked samples.

## Key Results
- Frequency Order aggregation consistently improves performance across all models and datasets for both present and absent keyphrases
- Specialist prompts (present-only or absent-only) do not consistently outperform unified baseline prompts
- Order control and length control instructions show mixed results, sometimes harming performance
- LLMs perform poorly on KPTimes (news domain) compared to scientific datasets due to mismatch between generation patterns and news keyphrase conventions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-sample frequency aggregation acts as implicit majority voting, filtering stochastic noise while reinforcing semantically valid keyphrases.
- Mechanism: Independent stochastic samples produce variation in spurious outputs but converge on high-probability semantic concepts. Frequency counting rewards phrases that appear consistently across samples, demoting random artifacts.
- Core assumption: The LLM has sufficient zero-shot semantic alignment that correct keyphrases appear across multiple samples more often than incorrect ones.
- Evidence anchors: Table 3 shows Frequency Order consistently improves over baseline across all models and datasets for both present and absent keyphrases (e.g., Llama-3 absent F1@5: baseline 5.5 → Frequency Order 7.6).

### Mechanism 2
- Claim: Interleaved aggregation preserves position-based quality signals across all samples, not just the first-ranked sample.
- Mechanism: LLMs tend to generate more relevant keyphrases early. Union Concatenation would bury early keyphrases from later samples behind later keyphrases from earlier samples. Union Interleaf takes position-1 from all samples, then position-2 from all samples, etc., preserving the "first-generated = most relevant" signal across the ensemble before deduplication.
- Core assumption: LLMs inherently prioritize relevant keyphrases earlier in the generation sequence.
- Evidence anchors: RQ2 motivation notes that "metrics such as (F1@5) used in keyphrase generation, focus on some first k keyphrases, so it is important for the LLMs to generate the best keyphrases first."

### Mechanism 3
- Claim: Dynamic truncation to per-sample average counts prevents precision collapse from over-aggregation.
- Mechanism: Multi-sample union can produce arbitrarily large keyphrase sets. By computing Mpre (average present count) and Mabs (average absent count) per input, then truncating aggregated output to these ceilings, the method balances recall gains from diversity against precision losses from overgeneration.
- Core assumption: The single-sample output length is a reasonable proxy for the "correct" number of keyphrases.
- Evidence anchors: Table 6 shows LLMs generate ~9-10 keyphrases per input vs. ~3 in KPTimes ground truth, confirming systematic overgeneration.

## Foundational Learning

- Concept: **Self-consistency in LLMs** (Wang et al., 2023)
  - Why needed here: The multi-sample aggregation strategy is explicitly motivated as a "task-specific counterpart to self-consistency-style strategies." Understanding majority voting over reasoning chains helps explain why frequency ordering works for keyphrase lists.
  - Quick check question: Can you explain why self-consistency improves performance on tasks with discrete outputs, and how this translates to keyphrase lists?

- Concept: **Present vs. absent keyphrases**
  - Why needed here: The paper separates evaluation and analysis for these two types. Present keyphrases appear verbatim in source text (extraction-like); absent keyphrases are semantically implied but not explicit (requires abstraction/generation).
  - Quick check question: Given a scientific abstract about "neural keyphrase generation using reinforcement learning," would "reinforcement learning" be present or absent? What about "abstractive summarization"?

- Concept: **Keyphrase generation evaluation metrics (F1@M, F1@5, R@10, R@Inf)**
  - Why needed here: The choice of aggregation strategy interacts with evaluation. F1@5 penalizes poor ordering; F1@M penalizes overgeneration. R@Inf shows theoretical recall upper bound if perfect selection were available.
  - Quick check question: If a model generates 20 keyphrases but only the first 5 are correct, what happens to F1@5 vs. F1@M?

## Architecture Onboarding

- Component map: Input (title + abstract) -> Baseline prompt template (chat format for model) -> Multi-sample generation (n=10, temp=0.8) -> Per-sample normalization (lowercase, stem, dedupe) -> Perplexity ranking (ascending) -> Aggregation strategy (Union / Union Concat / Union Interleaf / Frequency Order) -> Dynamic truncation (Mpre, Mabs ceilings) -> Final keyphrase list

- Critical path: The aggregation strategy choice (Frequency Order recommended) and sample count (n=10 used) are the dominant levers. Temperature=0.8 is used but not extensively ablated.

- Design tradeoffs:
  - Frequency Order vs. Union Interleaf: Frequency Order slightly outperforms on most metrics; Interleaf may be preferable when preserving positional diversity matters more than consensus.
  - Sample count (n): Higher n improves recall potential but increases latency and cost linearly. Paper uses n=10 without systematic ablation.
  - Perplexity ranking: Added as preprocessing but not ablated independently; assumed to help Union Concat more than Frequency Order.

- Failure signatures:
  - Specialist prompts (present-only or absent-only) do not consistently outperform unified baseline prompts—answering RQ1 negatively.
  - Order control and length control instructions show mixed results, sometimes harming performance—answering RQ2 negatively.
  - Simple Union destroys order information and harms F1@5-style metrics.
  - LLMs perform poorly on KPTimes (news domain) vs. scientific datasets, likely due to mismatch between LLM generation patterns (many long phrases) and news keyphrase conventions (few short phrases).

- First 3 experiments:
  1. Baseline replication: Run single-sample baseline (temp=0.8) on Inspec validation split; verify F1@5 and F1@M approximate reported values (~48.3/40.5 for Llama-3 present).
  2. Frequency Order ablation: Compare n=3, n=5, n=10, n=20 on a held-out subset to identify saturation point where additional samples yield diminishing returns.
  3. Cross-domain generalization: Apply best configuration (Frequency Order, n=10) to KPTimes subset; analyze why performance degrades and whether instruction tuning for shorter keyphrases helps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed multi-sample aggregation strategies (e.g., Frequency Order) improve the performance of earlier, non-LLM keyphrase generation models?
- Basis in paper: [explicit] The Conclusion states, "Our multi-sampling aggregation strategies are also model-agnostic and can work with earlier established KPG models. We leave potential to augment earlier model strategies with multi-sampling aggregation for future work."
- Why unresolved: The paper focused exclusively on validating these strategies on LLMs (Llama-3, Phi-3, GPT-4o) and did not test their applicability or effectiveness on the smaller, fine-tuned seq2seq models (like BART or T5) discussed in the related work.

### Open Question 2
- Question: Does few-shot prompting or parameter-efficient fine-tuning (PEFT) offer performance gains over the zero-shot LLM approaches tested?
- Basis in paper: [explicit] The Limitations section states, "the effectiveness of few-shot prompting, and parameter-efficient fine-tuning for KPG are also relevant questions that are yet unanswered in this paper."
- Why unresolved: The authors strictly adhered to a zero-shot setting to establish a baseline for LLM capabilities, leaving the optimization of these models via in-context examples or weight updates unexplored.

### Open Question 3
- Question: How do LLMs perform under fine-grained semantic evaluation metrics like KPEval compared to standard F1-based matching?
- Basis in paper: [explicit] The Limitations section notes that "alternative evaluation schemes to better judge LLM’s capacities such as KPEval... are yet to be explored."
- Why unresolved: The paper relied on standard stemming and matching protocols (F1@M, F1@5), which may penalize valid semantically similar keyphrases generated by LLMs that differ lexically from ground truth.

### Open Question 4
- Question: Can specific prompt engineering or input handling mitigate the poor performance of LLMs on news-domain keyphrase generation (KPTimes)?
- Basis in paper: [inferred] The paper notes a significant performance gap in the news domain, stating LLMs "perform quite poorly in the news domain (KP-Times) compared to others," and suggests this is due to biases regarding keyphrase length and document input size.
- Why unresolved: The authors identified the failure mode (over-generation of long phrases) but did not develop or test a specialized solution for the news domain to correct this bias.

## Limitations
- Specialist prompts (present-only or absent-only) do not consistently outperform unified baseline prompts
- Order control and length control instructions show mixed results, sometimes harming performance
- LLMs perform poorly on KPTimes (news domain) compared to scientific datasets

## Confidence
- **High confidence:** The core finding that Frequency Order aggregation consistently improves performance across all models and datasets for both present and absent keyphrases.
- **Medium confidence:** The mechanism explanation for why Frequency Order works (implicit majority voting filtering stochastic noise).
- **Low confidence:** The comparative advantage of Frequency Order over Union Interleaf, as the difference is small and context-dependent.

## Next Checks
1. Implement and test both normalized and raw perplexity calculations to verify which produces better aggregation results.
2. Apply the best configuration to additional non-scientific domains (e.g., news, social media, technical documentation) to quantify generalization limits.
3. Systematically ablate n=3,5,10,15,20 to identify the optimal trade-off between performance gains and computational cost.