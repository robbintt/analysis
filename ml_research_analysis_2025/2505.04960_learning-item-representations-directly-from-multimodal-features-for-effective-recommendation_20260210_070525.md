---
ver: rpa2
title: Learning Item Representations Directly from Multimodal Features for Effective
  Recommendation
arxiv_id: '2505.04960'
source_url: https://arxiv.org/abs/2505.04960
tags:
- multimodal
- features
- item
- uni00000013
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies a fundamental issue in multimodal recommender
  systems where item ID embeddings and multimodal features are jointly learned with
  biased gradient magnitudes. The authors propose LIRDRec, a novel model that learns
  item representations directly from multimodal features without relying on item ID
  embeddings.
---

# Learning Item Representations Directly from Multimodal Features for Effective Recommendation

## Quick Facts
- arXiv ID: 2505.04960
- Source URL: https://arxiv.org/abs/2505.04960
- Reference count: 40
- Key outcome: Proposes LIRDRec, which learns item representations directly from multimodal features without ID embeddings, achieving 4.21% average improvement in NDCG@20 on five datasets.

## Executive Summary
This paper identifies a fundamental optimization issue in multimodal recommender systems where item ID embeddings converge slower than multimodal feature parameters due to gradient bias. The authors propose LIRDRec, a novel framework that eliminates ID embeddings entirely and learns item representations directly from multimodal features. The model employs a multimodal transformation mechanism with 2-D DCT for feature decorrelation, modality-specific encoders for fusion, and a progressive weight copying module to stabilize heterogeneous modality integration. Experimental results demonstrate significant performance improvements over state-of-the-art baselines, particularly in cold-start scenarios.

## Method Summary
LIRDRec learns item representations directly from multimodal features (visual and textual) without using ID embeddings. The method applies 2-D DCT to decorrelate modality-specific features, then processes them through modality-specific and shared DNN encoders. It uses LightGCN for graph learning on user-item and item-item graphs, with items represented solely by their multimodal features. A Progressive Weight Copying (PWC) module fuses heterogeneous modalities by incrementally learning weights through exponential moving averages. The model is optimized using BPR loss with L2 regularization, and can optionally leverage MLLMs to convert item images to text for enhanced semantic embeddings.

## Key Results
- LIRDRec achieves 4.21% average improvement in NDCG@20 compared to state-of-the-art baselines
- Demonstrates superior cold-start performance by eliminating dependency on ID embeddings
- Shows accelerated startup performance with faster convergence than traditional approaches
- MLLM-LLM embeddings provide additional accuracy boost but increase computational cost

## Why This Works (Mechanism)

### Mechanism 1
Optimization bias in standard MMRSs causes item ID embeddings to converge slower than multimodal feature parameters, leading to suboptimal final representations. In frameworks combining ID embeddings and multimodal features, the gradient magnitude updating multimodal transformation matrices is aggregated across all items, whereas ID embedding gradients are specific to individual items. Because the number of items far exceeds the number of interactions per item in sparse graphs, multimodal features learn faster, causing the loss to converge before ID embeddings are fully optimized.

### Mechanism 2
Initializing item representations via 2-D DCT on multimodal features improves convergence speed and quality by decorrelating modality-specific information. The DCT exploits energy compaction and decorrelation properties, concentrating useful information in the frequency domain. This preprocessing allows subsequent DNNs to synthesize cross-modal relationships more effectively than raw concatenation, particularly when modality features contain spatial or channel-wise correlations.

### Mechanism 3
Progressive Weight Copying (PWC) stabilizes fusion of heterogeneous modalities better than standard attention by smoothing weight updates via a target network. PWC partitions user representations into chunks (modalities) and calculates weights using a DNN constrained by a lightweight target network. The final weights are exponential moving averages of current predictions and the target network's state, preventing abrupt shifts in modality importance during training and enabling gradual adaptation to changing feature importance.

## Foundational Learning

- **Concept: Bayesian Personalized Ranking (BPR) Loss**
  - **Why needed here:** The paper's theoretical argument relies on decomposing BPR loss to show gradient bias between ID and multimodal features
  - **Quick check question:** Can you explain why the gradient of BPR loss depends on the difference between positive and negative sampling scores?

- **Concept: LightGCN (Light Graph Convolutional Network)**
  - **Why needed here:** LIRDRec uses LightGCN for graph learning without nonlinear transformations
  - **Quick check question:** How does LightGCN aggregate neighbor information differently from standard GCN layers?

- **Concept: Discrete Cosine Transform (DCT)**
  - **Why needed here:** The paper utilizes 2-D DCT as a core transformation mechanism for feature decorrelation
  - **Quick check question:** How does DCT differ from PCA or autoencoders in handling input signal decorrelation?

## Architecture Onboarding

- **Component map:** Input -> MFT Block (2-D DCT → Concatenation → DNN Encoders) -> Graph Encoder (LightGCN) -> Fusion Head (PWC Module) -> Optimizer (BPR Loss + L2)

- **Critical path:** The most sensitive path is Initialization → MFT → Graph Encoder. Since there are no ID embeddings, the entire model relies on the MFT block producing viable initial item representations from raw features.

- **Design tradeoffs:**
  - Cold-Start vs. Specificity: Removing ID embeddings gains instant cold-start capability but loses ability to memorize highly specific user-item interactions without multimodal correlation
  - MLLM Features: Using MLLM-LLM embeddings boosts accuracy but drastically increases feature extraction latency and cost

- **Failure signatures:**
  - Modal Collapse: If PWC weights collapse to zero for all but one modality, check the decay rate τ
  - Slow Convergence: If model trains slower than baselines, verify DCT is a pre-processing step, not part of training loop
  - Cold-Start Failure: If performance on new items is poor, ensure Item-Item graph is constructed correctly

- **First 3 experiments:**
  1. Gradient Bias Validation: Replicate Figure 1 to verify optimization bias exists in your environment
  2. Ablation on PWC: Compare "LIRDRec w/o PWC" vs. full LIRDRec to isolate PWC value
  3. Cold-Start Stress Test: Train on 80% items, test on 20% completely unseen items

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the Progressive Weight Copying (PWC) module be effectively adapted to sequential recommendation scenarios to capture temporal dynamics and multimodal interactions? The current LIRDRec framework is designed for general collaborative filtering on static graphs and lacks specific architectural components to model the sequential order or temporal evolution of user preferences.

- **Open Question 2:** Does the identified optimization gradient bias against ID embeddings persist in dense interaction graphs or under non-BPR optimization objectives? The theoretical proof relies on sparsity assumptions and specific BPR loss formulation, which might not hold in dense graphs or with alternative loss functions.

- **Open Question 3:** Does the reliance on fixed 2-D DCT for feature decorrelation limit the model's ability to capture complex, non-linear cross-modal relationships compared to learnable fusion mechanisms? While DCT effectively decorrelates signals, it is a fixed linear transformation, and it's unresolved whether a learnable, adaptive mechanism could capture nuanced multimodal dependencies better.

## Limitations

- The assumption that multimodal features are always more informative than ID embeddings may not hold for domains with weak visual/textual correlations
- The computational overhead of MLLM feature extraction is not fully addressed, potentially limiting real-world applicability
- The model's performance relies heavily on pre-extracted features from specific pipelines (FREEDOM), raising questions about generalizability

## Confidence

- **High confidence:** Core claim about optimization bias in multimodal recommender systems (mathematical proof, alignment with existing work)
- **Medium confidence:** Proposed solution's superiority (strong experimental results but rely on pre-extracted features)
- **Medium confidence:** DCT transformation mechanism effectiveness (demonstrated empirically but lacks theoretical grounding specific to recommendation)
- **Medium confidence:** PWC module benefits (ablation studies show value but adds complexity)

## Next Checks

1. **Gradient Bias Replication:** Independently verify the optimization bias claim by reproducing the gradient magnitude comparison on a standard multimodal recommender with controlled initialization.

2. **PWC Ablation Study:** Conduct a more granular ablation by comparing PWC against static weights, standard attention, and no fusion to isolate the specific contribution of progressive weighting.

3. **Domain Transfer Test:** Evaluate LIRDRec on a dataset where multimodal features are known to be less informative to test the limits of removing ID embeddings.