---
ver: rpa2
title: 'LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning'
arxiv_id: '2510.14211'
source_url: https://arxiv.org/abs/2510.14211
tags:
- generation
- stage
- layer
- accuracy
- skipping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LiteStage, a latency-aware layer skipping
  framework for multi-stage reasoning in small language models. LiteStage addresses
  the challenge of balancing efficiency and accuracy by combining an offline stage-wise
  layer budget search with an online confidence-based generation early exit.
---

# LiteStage: Latency-aware Layer Skipping for Multi-stage Reasoning

## Quick Facts
- **arXiv ID:** 2510.14211
- **Source URL:** https://arxiv.org/abs/2510.14211
- **Reference count:** 40
- **Primary result:** Achieves up to 1.70× speedup with <4.0% accuracy loss on multi-stage reasoning tasks

## Executive Summary
LiteStage introduces a latency-aware layer skipping framework for accelerating small language models in multi-stage reasoning tasks. The approach combines an offline stage-wise layer budget search with an online confidence-based generation early exit to balance efficiency and accuracy. By identifying that different reasoning stages have varying sensitivity to layer skipping, LiteStage applies aggressive compression to robust stages while protecting critical aggregation stages. Experiments demonstrate significant latency improvements across OBQA, CSQA, and StrategyQA benchmarks while maintaining accuracy within acceptable bounds.

## Method Summary
LiteStage operates through a two-phase approach: offline configuration and online inference. During offline profiling, sub-layer importance is estimated using cosine similarity of residual streams, followed by greedy search to allocate layer budgets per stage while optimizing for latency under accuracy constraints. The online inference engine applies these stage-wise budgets using dynamic layer dropping and monitors token confidence during decoding, triggering early exit when confidence drops below threshold. The framework specifically targets the three-stage reasoning structure (Recall, Analysis, Summary) common in many reasoning benchmarks, applying different compression levels to each stage based on their sensitivity profiles.

## Key Results
- Achieves up to 1.70× speedup on multi-stage reasoning benchmarks
- Maintains accuracy loss below 4.0% across tested tasks
- Outperforms prior training-free layer skipping methods
- Demonstrates effectiveness on TinyLlama-1.1B and Qwen2.5-0.5B models

## Why This Works (Mechanism)

### Mechanism 1: Stage-wise Sensitivity Isolation
The framework exploits non-uniform sensitivity across reasoning stages, applying aggressive layer skipping to robust stages while protecting critical aggregation stages. By identifying that Stage 3 (Summary) is the accuracy bottleneck and Stage 2 (Analysis) is the latency bottleneck, it allocates budgets to minimize latency while satisfying accuracy constraints. This isolation prevents accuracy collapse that would occur with uniform skipping policies.

### Mechanism 2: Confidence-Based Generation Early Exit
Layer skipping induces verbose generation with redundant output tokens, which can negate latency gains from reduced computation. The framework monitors token confidence during decoding using a sliding window cache, forcing EOS termination when mean confidence drops below threshold. This suppresses low-confidence tails that don't contribute to final reasoning outcomes, translating per-token speedups into end-to-end latency gains.

### Mechanism 3: Latency-Aware Budget Search
Instead of optimizing purely for accuracy or layer importance, LiteStage directly searches for configurations that minimize end-to-end latency within accuracy bounds. This accounts for the non-monotonic relationship between skipped layers and total inference time, pruning configurations where aggressive skipping causes rambling generation that increases total time despite fewer layers.

## Foundational Learning

**Concept: Sub-layer Granularity (MHSA vs. FFN)**
- Why needed here: LiteStage skips at the sub-layer level (Attention or Feed-Forward), not just the block level, requiring understanding of these components' different functions
- Quick check question: Does the model skip the Attention head or the FFN matrix when the cosine similarity of the residual stream is high?

**Concept: Residual Stream Cosine Similarity**
- Why needed here: This proxy metric determines "Layer Importance" by measuring how much a sub-layer changes the representation
- Quick check question: If `cos(input, output) ≈ 1.0`, is the layer likely critical or redundant?

**Concept: Greedy Search Optimization**
- Why needed here: The offline configuration uses a greedy strategy (longest stage first) to find a local optimum, not necessarily the global minimum
- Quick check question: Why does the search prioritize the longest stage (Analysis) over the shortest stage?

## Architecture Onboarding

**Component map:**
Offline Profiler -> Sub-layer Importance (cosine sim) -> Accuracy-Latency Grid Search -> Skip_Config
Online Inference Engine -> Load Skip_Config -> Dynamic Layer Dropping -> Confidence Cache Monitoring -> Early Exit

**Critical path:**
The Offline Search is the critical path for setup, requiring hours of profiling to build accuracy-latency trade-off curves. The Online Early Exit is the critical path for runtime performance, monitoring token confidence during generation.

**Design tradeoffs:**
- Search Cost vs. Reuse: Offline search takes hours (e.g., 2.8h for OBQA on TinyLlama), viable only for static model/task pairs
- Threshold Strictness: Low confidence thresholds (0.3) keep accuracy high but gain less speed; high thresholds (0.6+) maximize speed but risk cutting valid reasoning chains

**Failure signatures:**
- Latency Inversion: Applying layer skipping increases total latency (>1.0) due to verbose hallucinations, indicating early exit threshold is too lenient
- Accuracy Collapse at Stage 3: Performance drops sharply with minor skipping, requiring protection of Stage 3 with smaller skip budgets

**First 3 experiments:**
1. Establish Baseline Sensitivity: Ablate layer skipping on one stage at a time to verify Stage 3 sensitivity and Stage 2 latency bottleneck
2. Tune Early Exit Threshold: Run LiteStage with confidence thresholds [0.3, 0.5, 0.7] to find the knee of the curve
3. Search vs. Heuristic Comparison: Compare LiteStage (search-based) against static skip policy ("skip every 4th layer")

## Open Questions the Paper Calls Out

**Open Question 1:** Can LiteStage be effectively adapted for deep reasoning tasks without relying on manually designed heuristics?
- Basis: Diagnostic study concludes deep reasoning requires "complementary mechanisms" like periodic full-decoding
- Why unresolved: Current framework assumes short multi-stage structure, while deep reasoning suffers from rapid error accumulation over long horizons
- Evidence needed: Modified framework achieving high speedup on long-horizon benchmarks (e.g., AIME) without periodic full-decoding

**Open Question 2:** Do adaptive or task-specific confidence thresholds outperform the static 0.5 threshold for generation early exit?
- Basis: Authors state current confidence criterion is "simple" and "more adaptive or task-specific exit strategies may further improve robustness"
- Why unresolved: Fixed threshold may be too aggressive for some tasks and too lenient for others
- Evidence needed: Comparative experiments showing dynamic thresholds reduce accuracy loss variance across benchmarks

**Open Question 3:** How does the offline search cost scale with model size and reasoning stage complexity?
- Basis: Authors report search times in hours for small models (0.5B-1.1B) and acknowledge overhead compared to fixed heuristics
- Why unresolved: Unclear if search remains feasible for larger models (e.g., 7B+) or longer reasoning chains
- Evidence needed: Latency measurements of offline search phase on larger architectures or tasks with more than three stages

## Limitations

- The framework assumes a specific three-stage reasoning decomposition (Recall → Analysis → Summary) that may not generalize to all reasoning tasks
- Offline search phase introduces significant overhead (hours of profiling) compared to fixed heuristic approaches
- Layer importance metric relies on cosine similarity of residual streams, which may not perfectly correlate with actual accuracy impact

## Confidence

**High confidence:** The empirical observation that layer skipping causes verbose generation with redundant output tokens is well-supported by confidence decay patterns in Figure 3
**Medium confidence:** The 1.70× speedup claim is based on specific benchmarks (OBQA, CSQA, StrategyQA) with particular model sizes (1.1B, 0.5B) and may not generalize broadly
**Low confidence:** The theoretical justification for why Stage 3 is "substantially more sensitive" is based on empirical observation rather than mechanistic explanation

## Next Checks

**Check 1: Sub-layer importance validation**
Run ablation studies skipping individual sub-layers and measure actual accuracy impact, comparing ground truth sensitivity with cosine similarity proxy to verify the search framework optimizes for the correct metric.

**Check 2: Cross-task robustness**
Test LiteStage on reasoning tasks that don't naturally decompose into the three-stage format (e.g., single-stage logical inference, iterative refinement) to verify stage-wise sensitivity isolation assumptions hold.

**Check 3: Model size scaling**
Evaluate LiteStage on models larger than 1.1B parameters (e.g., 3B, 7B) to verify whether the 1.70× speedup is achievable or if larger models require different layer budgets or confidence thresholds.