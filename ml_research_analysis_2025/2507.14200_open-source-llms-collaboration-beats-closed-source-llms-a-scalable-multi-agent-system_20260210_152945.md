---
ver: rpa2
title: 'Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent
  System'
arxiv_id: '2507.14200'
source_url: https://arxiv.org/abs/2507.14200
tags:
- llms
- uni00000003
- arxiv
- question
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SMACS, a scalable multi-agent collaboration
  framework that combines fifteen open-source LLMs to outperform leading closed-source
  models. SMACS employs a retrieval-based prior selection strategy to dynamically
  choose expert LLMs per instance, then aggregates diverse responses through an exploration-exploitation
  posterior enhancement process using a hybrid quality score.
---

# Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System

## Quick Facts
- **arXiv ID**: 2507.14200
- **Source URL**: https://arxiv.org/abs/2507.14200
- **Reference count**: 32
- **Primary result**: SMACS combines fifteen open-source LLMs to achieve 76.78% average accuracy across eight benchmarks, surpassing GPT-4.1 (+5.36%), GPT-o3-mini (+5.28%), and Claude-3.7-Sonnet (+12.73%)

## Executive Summary
This paper introduces SMACS, a scalable multi-agent collaboration framework that leverages fifteen open-source LLMs to outperform leading closed-source models. The system employs a retrieval-based prior selection strategy to dynamically choose expert LLMs per instance, then aggregates diverse responses through an exploration-exploitation posterior enhancement process using a hybrid quality score. Across eight benchmarks, SMACS achieves 76.78% average accuracy, demonstrating both superior performance and excellent scalability as more models are added to the ensemble.

## Method Summary
SMACS is a scalable multi-agent collaboration framework that combines fifteen open-source LLMs to outperform leading closed-source models. The system uses a retrieval-based prior selection strategy to dynamically choose expert LLMs per instance based on task requirements. It then aggregates diverse responses through an exploration-exploitation posterior enhancement process using a hybrid quality score. The framework demonstrates consistent performance improvement as more LLMs are added to the ensemble, achieving state-of-the-art results across multiple benchmarks.

## Key Results
- SMACS achieves 76.78% average accuracy across eight benchmarks
- Outperforms GPT-4.1 by 5.36%, GPT-o3-mini by 5.28%, and Claude-3.7-Sonnet by 12.73%
- Surpasses both open-source (+2.86%) and closed-source (+2.04%) upper bounds
- Demonstrates excellent scalability with consistent performance improvement as more LLMs are added

## Why This Works (Mechanism)
The framework's effectiveness stems from its two-stage approach: first, it intelligently selects the most appropriate LLM for each task instance through retrieval-based prior selection, ensuring that each problem is routed to models with relevant expertise. Second, it aggregates responses from multiple models using an exploration-exploitation strategy that balances diverse perspectives with quality assessment. This combination allows SMACS to leverage the complementary strengths of different open-source models while mitigating individual weaknesses, resulting in superior performance compared to any single model, including closed-source alternatives.

## Foundational Learning

**Retrieval-based selection**: Why needed - to route tasks to most relevant expert models efficiently; Quick check - verify retrieval accuracy correlates with downstream task performance

**Multi-agent collaboration**: Why needed - to combine complementary strengths of different models; Quick check - measure performance gains from adding each additional model

**Hybrid quality scoring**: Why needed - to balance exploration of diverse responses with exploitation of high-quality answers; Quick check - test sensitivity to scoring parameter variations

**Posterior enhancement**: Why needed - to refine initial selections through collective intelligence; Quick check - compare with simpler averaging approaches

**Scalability principles**: Why needed - to ensure framework benefits continue with more models; Quick check - plot performance vs. number of models curve

## Architecture Onboarding

**Component map**: Input tasks -> Retrieval-based Prior Selection -> Multiple LLM executions -> Response Aggregation -> Hybrid Quality Scoring -> Final Output

**Critical path**: Task input flows through prior selection to choose expert models, which generate responses that are then aggregated using the hybrid quality score to produce the final answer

**Design tradeoffs**: The framework balances computational cost (running multiple models) against accuracy gains, while the retrieval mechanism trades some selection accuracy for speed

**Failure signatures**: Poor prior selection leads to mismatched model-task pairing, inadequate aggregation can cause diluted responses, and suboptimal quality scoring may overweight weak contributions

**First experiments**: 1) Test retrieval accuracy on held-out data 2) Measure performance improvement when adding each new model 3) Validate hybrid scoring against alternative aggregation methods

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims rely heavily on standard benchmarks that may not reflect real-world complexity
- Scalability improvements lack theoretical justification for indefinite continuation
- Hybrid quality score mechanism appears sensitive to undisclosed weight tuning parameters

## Confidence
- **High**: Architectural design and retrieval-based selection mechanism
- **Medium**: Performance claims due to potential benchmark-specific optimization and limited transparency about hyperparameters

## Next Checks
1. Test SMACS on out-of-distribution datasets and real-world tasks beyond standard benchmarks to assess generalization
2. Conduct ablation studies isolating the contributions of prior selection versus posterior aggregation to quantify each component's impact
3. Evaluate computational cost and latency trade-offs when scaling beyond fifteen models to verify claimed efficiency improvements hold at larger scales