---
ver: rpa2
title: 'Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness
  Landscape Analysis Perspective'
arxiv_id: '2509.21945'
source_url: https://arxiv.org/abs/2509.21945
tags:
- landscape
- configuration
- tuning
- performance
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper reveals that traditional accuracy metrics for surrogate\
  \ models in configuration tuning can be misleading, as higher accuracy does not\
  \ guarantee better tuning results. To address this, the authors propose a new theory\u2014\
  landscape dominance\u2014that evaluates model usefulness based on the similarity\
  \ of global landscape structure and the severity of local optima compared to the\
  \ real system."
---

# Unveiling Many Faces of Surrogate Models for Configuration Tuning: A Fitness Landscape Analysis Perspective

## Quick Facts
- **arXiv ID:** 2509.21945
- **Source URL:** https://arxiv.org/abs/2509.21945
- **Reference count:** 40
- **Primary result:** Landscape features are largely uncorrelated with traditional accuracy metrics, and classic ML models often outperform deep learning models for configuration tuning.

## Executive Summary
This paper addresses the fundamental problem that traditional accuracy metrics for surrogate models in configuration tuning can be misleading, as higher accuracy does not guarantee better tuning results. The authors propose "landscape dominance" theory, which evaluates model usefulness based on how well the emulated landscape preserves global structure and smooths local optima compared to the real system. Through extensive empirical study across 18 systems, 10 models, and 16 tuners, they demonstrate that accuracy metrics and landscape features provide largely orthogonal information, with positive correlation in less than 30% of cases. Based on these insights, they develop Model4Tune, a predictive tool that uses learning-to-rank to estimate the most useful model-tuner pairs for unseen systems, achieving up to 244% improvement over random guessing.

## Method Summary
The study evaluates 10 surrogate models across 18 configurable systems using 16 different tuners. For each system-model pair, they calculate 8 landscape features (4 global: FDC, FBD, Skewness, Kurtosis; 4 local: PLO, CL, MIE, NBC) to characterize the search space topology. They derive features including absolute deviation of global features between model and real landscapes, local feature values, and traditional accuracy metrics (MAPE, ÂµRD). These features are combined with one-hot encoded tuner characteristics and used to train a LambdaRank model that predicts the ranking of model-tuner pairs. The approach uses leave-one-out cross-validation across the 18 systems to evaluate predictive performance.

## Key Results
- Landscape features and accuracy metrics are generally uncorrelated, with positive correlation in less than 30% of cases
- No single surrogate model is universally best across all system-tuner combinations
- Classic machine learning models (RF, DT, LR) tend to be more useful than deep learning models for configuration tuning
- Model4Tune predicts the most useful model-tuner pairs with 79-82% improvement over random guessing

## Why This Works (Mechanism)

### Mechanism 1: Landscape Dominance as a Proxy for Tunability
A surrogate model is useful for configuration tuning if it preserves the global structure of the search space while smoothing local optima, rather than strictly minimizing prediction error. This occurs when a model's emulated landscape has smaller deviations in global features from the real system and exhibits less severe local optima. The search heuristic performs better on landscapes that offer clearer guidance or fewer traps, even if the point-wise accuracy is not perfect.

### Mechanism 2: Orthogonality of Accuracy and Landscape Features
Traditional accuracy metrics provide information orthogonal to landscape features, meaning low error does not imply a "tunable" landscape structure. Accuracy metrics treat configurations as independent test points, whereas landscape features analyze spatial dependencies and neighborhood structures between configurations. This divergence exists because a model can predict performance values well but still misrepresent the spatial relationships that tuners rely on.

### Mechanism 3: Option Sensitivity Determining Landscape Fidelity
Memory and queue-related configuration options disproportionately influence the "shape" of the emulated landscape compared to CPU or utility options. Removing memory or queue options causes the landscape feature vector to cluster significantly away from the baseline, suggesting these options control the "space" or "buffer" of the system, heavily impacting performance interactions.

## Foundational Learning

- **Concept: Fitness Landscape Analysis (FLA)**
  - **Why needed here:** The paper shifts evaluation from "prediction error" to "landscape topology." Understanding local optima density, ruggedness (Correlation Length), and global guidance (Fitness Distance Correlation) is essential for grasping Landscape Dominance.
  - **Quick check question:** If a landscape has a low Correlation Length, does that imply it is smooth or rugged? (Answer: Rugged)

- **Concept: Pareto Dominance**
  - **Why needed here:** The core theory uses Pareto dominance to compare models: Model A dominates Model B if A is "better" in at least one feature and "not worse" in others. This logic labels models as "useful" or "dominated."
  - **Quick check question:** If Model A has better global fidelity but worse local smoothness than Model B, can A dominate B? (Answer: No, strictly speaking, unless "worse" is within a threshold)

- **Concept: Learning-to-Rank (LambdaRank)**
  - **Why needed here:** Model4Tune is a ranker that learns to order model-tuner pairs based on derived features. Understanding this distinction is key to using the tool.
  - **Quick check question:** Why is Learning-to-Rank preferred over regression for this problem? (Answer: Practitioners care about selecting the best pair relative to others, not the absolute performance value)

## Architecture Onboarding

- **Component map:** Configurable System -> Sampling Module -> 10 Surrogate Models -> Feature Extractor -> Landscape Features + Accuracy Metrics -> Model4Tune (LambdaRank)
- **Critical path:** Feature Extraction. To use Model4Tune on a new system, you must sample the configuration space to compute the specific landscape features for that system first.
- **Design tradeoffs:**
  - **Coarse vs. Fine-grained:** Landscape Dominance (coarse, rule-based) vs. Model4Tune (fine-grained, learning-based). Dominance is interpretable; Model4Tune is more accurate but acts as a black box.
  - **Sequential vs. Batch:** Separate ranking models are trained for Sequential (Bayesian) and Batch tuners because they utilize models differently.
- **Failure signatures:**
  - **Cold Start Failure:** Cannot compute landscape features if sample size is too small or configuration space is too vast to estimate neighborhoods.
  - **Domain Mismatch:** Model4Tune performs best on systems similar to the 18 studied. Applying it to embedded systems or tiny config spaces may fail.
- **First 3 experiments:**
  1. **Sanity Check:** Take a known system. Train a high-accuracy model (DeepPerf) and a low-accuracy one (LR). Verify that the "better" model identified by Landscape Dominance actually yields better tuning results, regardless of MAPE.
  2. **Feature Ablation:** Run Model4Tune using only Accuracy features vs. only Landscape features. Confirm that the "Combined" model outperforms both, validating the orthogonality claim.
  3. **New System Inference:** Introduce a system not in the paper. Measure a small sample, compute landscape features, and use Model4Tune to recommend a model-tuner pair. Compare against a random choice.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the predictive accuracy of Model4Tune be improved by incorporating a more comprehensive set of landscape features beyond the 8 global and local metrics used in this study? (Section VIII-A states this as future work)
- **Open Question 2:** Does the theory of landscape dominance hold for emerging domains like Agentic AI or hardware configuration, where configuration interactions may differ from the software systems studied? (Section I mentions Agentic AI as motivation; Section VIII-C notes considering more systems might prove fruitful)
- **Open Question 3:** At what specific sampling budget does the cost of calculating landscape features for Model4Tune negate its benefits over simply running a baseline tuner? (While Section VII-A claims Model4Tune avoids expensive profiling, Section VII-B notes it requires sampling to compute features, creating an unstated cost trade-off)

## Limitations
- The empirical foundation rests on 18 systems and 10 surrogate models, which may not capture the full diversity of configurable systems
- The analysis assumes that the 8 selected landscape features comprehensively characterize search space quality
- The orthogonality between accuracy metrics and landscape features relies heavily on the specific feature selection and calculation methodology
- The memory/queue option influence finding requires further validation across broader system types beyond databases and big data systems

## Confidence

- **High Confidence:** The core mechanism of Landscape Dominance (evaluating global structure and local optima) is well-supported by the theoretical framework and empirical evidence. The finding that traditional accuracy metrics can be misleading for tuning purposes is robust across multiple experiments.
- **Medium Confidence:** The orthogonality claim between accuracy and landscape features, while supported by the reported correlation statistics, relies heavily on the specific feature selection and calculation methodology.
- **Medium Confidence:** The Model4Tune tool demonstrates strong performance (79-82% improvement over random), but as a black-box predictor, its internal decision-making process is less interpretable than the Landscape Dominance theory.

## Next Checks

1. **Cross-Domain Validation:** Apply Model4Tune to a fundamentally different system type (e.g., embedded systems or networking equipment) to test generalizability beyond databases and big data systems.
2. **Feature Ablation Study:** Systematically remove individual landscape features from Model4Tune to quantify their relative contribution and verify that the combination provides unique value beyond any single feature.
3. **Temporal Stability Analysis:** Evaluate whether Landscape Dominance rankings remain stable across different workload distributions or system versions, as configuration landscapes may shift over time.