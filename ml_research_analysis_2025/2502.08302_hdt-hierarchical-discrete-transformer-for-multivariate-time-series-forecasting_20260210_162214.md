---
ver: rpa2
title: 'HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting'
arxiv_id: '2502.08302'
source_url: https://arxiv.org/abs/2502.08302
tags:
- forecasting
- time
- discrete
- series
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of long-term, high-dimensional
  multivariate time series forecasting. It proposes a hierarchical discrete transformer
  (HDT) that models time series as discrete token representations using a vector-quantized
  framework.
---

# HDT: Hierarchical Discrete Transformer for Multivariate Time Series Forecasting

## Quick Facts
- arXiv ID: 2502.08302
- Source URL: https://arxiv.org/abs/2502.08302
- Authors: Shibo Feng; Peilin Zhao; Liu Liu; Pengcheng Wu; Zhiqi Shen
- Reference count: 40
- Key outcome: HDT achieves average 16.7% improvement on CRPSsum and 15.4% on NRMSEsum compared to state-of-the-art methods for long-term, high-dimensional multivariate time series forecasting

## Executive Summary
This paper addresses the challenge of long-term, high-dimensional multivariate time series forecasting by proposing a hierarchical discrete transformer (HDT). The method models time series as discrete token representations using a vector-quantized framework, which enables efficient modeling of complex temporal dependencies. HDT employs a two-stage approach that first learns discrete representations and then uses a hierarchical transformer to generate discrete tokens conditioned on both context and long-term trends.

The proposed HDT demonstrates superior performance across five real-world datasets, achieving significant improvements over existing state-of-the-art methods. By discretizing continuous time series data into tokens, the model can leverage transformer architectures more effectively while maintaining computational efficiency for high-dimensional forecasting tasks.

## Method Summary
HDT addresses long-term, high-dimensional multivariate time series forecasting through a hierarchical discrete transformer architecture. The method operates in two stages: first, it learns discrete representations of both target and downsampled time series using a vector-quantized framework; second, it employs a hierarchical transformer to generate discrete tokens conditioned on the context and the target's long-term trends. This approach allows the model to capture complex temporal dependencies while maintaining computational efficiency through discretization of continuous time series data into token representations.

## Key Results
- HDT achieves an average 16.7% improvement on CRPSsum compared to state-of-the-art methods
- The model demonstrates 15.4% improvement on NRMSEsum across five real-world datasets
- Experimental results show superior performance in long-term, high-dimensional multivariate time series forecasting

## Why This Works (Mechanism)
The hierarchical discrete transformer works by converting continuous time series data into discrete token representations, which enables more efficient processing while preserving essential temporal patterns. The two-stage approach allows the model to first learn meaningful discrete representations and then leverage transformer architectures to capture complex dependencies. The hierarchical structure helps manage computational complexity while maintaining the ability to model long-term trends effectively.

## Foundational Learning

Vector Quantization (VQ):
- Why needed: To convert continuous time series data into discrete token representations
- Quick check: Can be validated by examining reconstruction quality and information preservation metrics

Hierarchical Transformer Architecture:
- Why needed: To manage computational complexity while modeling multi-scale temporal dependencies
- Quick check: Verify through ablation studies comparing hierarchical vs flat transformer performance

Time Series Tokenization:
- Why needed: To enable efficient processing of high-dimensional time series using transformer models
- Quick check: Assess token quality through downstream task performance and reconstruction accuracy

## Architecture Onboarding

Component Map:
Input Time Series -> Vector Quantization -> Discrete Token Representation -> Hierarchical Transformer -> Output Tokens -> Forecast Generation

Critical Path:
The critical path flows from input time series through vector quantization to discrete token representation, then through the hierarchical transformer layers, and finally to output token generation for forecasting.

Design Tradeoffs:
The main tradeoff involves discretization accuracy versus computational efficiency. While vector quantization enables efficient processing, it may introduce information loss. The hierarchical structure balances model capacity with computational constraints.

Failure Signatures:
Potential failure modes include poor discretization quality leading to information loss, hierarchical collapse where higher levels fail to capture long-term dependencies, and token generation errors propagating through the forecasting pipeline.

First Experiments:
1. Test vector quantization quality by measuring reconstruction error on validation data
2. Validate hierarchical transformer performance by comparing with flat transformer baselines
3. Assess long-term forecasting accuracy at different prediction horizons

## Open Questions the Paper Calls Out
None identified in the provided corpus signals.

## Limitations
- Performance improvements are reported as aggregate metrics without detailed breakdown by dataset characteristics
- The two-stage hierarchical approach introduces additional complexity that may affect practical deployment
- Potential information loss during vector quantization is not thoroughly discussed
- Limited analysis of scalability to very high-dimensional time series or extremely long time horizons

## Confidence

High confidence: The proposed two-stage hierarchical discrete transformer architecture and its core methodology

Medium confidence: The claimed performance improvements relative to state-of-the-art methods

Low confidence: The generalizability of results across different domains and the method's robustness to varying data characteristics

## Next Checks

1. Conduct ablation studies to quantify the individual contributions of the discrete representation learning stage versus the hierarchical transformer stage

2. Evaluate the model's performance on additional real-world datasets with varying characteristics (e.g., different sampling rates, dimensionality ranges, noise levels)

3. Perform computational efficiency analysis comparing HDT with competing methods in terms of training/inference time and memory requirements