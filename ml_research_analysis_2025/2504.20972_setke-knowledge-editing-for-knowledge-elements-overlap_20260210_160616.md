---
ver: rpa2
title: 'SetKE: Knowledge Editing for Knowledge Elements Overlap'
arxiv_id: '2504.20972'
source_url: https://arxiv.org/abs/2504.20972
tags:
- knowledge
- editing
- triplets
- setke
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SetKE, a novel approach for knowledge editing
  in Large Language Models (LLMs) that addresses the Knowledge Element Overlap (KEO)
  problem. KEO occurs when multiple triplets share common elements, leading to editing
  conflicts.
---

# SetKE: Knowledge Editing for Knowledge Elements Overlap

## Quick Facts
- arXiv ID: 2504.20972
- Source URL: https://arxiv.org/abs/2504.20972
- Reference count: 34
- Primary result: SetKE achieves state-of-the-art performance on mainstream LLMs in KEO scenarios, improving efficacy score from 65.6 to 78.4

## Executive Summary
SetKE introduces a novel knowledge editing approach that addresses the Knowledge Element Overlap (KEO) problem, where multiple triplets share common elements leading to editing conflicts. The method leverages bipartite matching to optimize object set editing and multi-layer residual spreading to improve locality. Evaluated on the new EDIT SET dataset, SetKE significantly outperforms existing methods, achieving superior efficacy, generalization, and locality scores on mainstream LLMs.

## Method Summary
SetKE addresses knowledge editing for Knowledge Element Overlap (KEO) by treating the problem as set editing rather than individual triplet editing. The method uses bipartite matching (Hungarian algorithm) to optimally align predicted and target object sets, preventing overwriting. It also employs multi-layer residual spreading across FFN layers and a KL-divergence constraint on subject prompts to improve locality. The approach is evaluated on the EDIT SET dataset with 31 relations and 40,904 instances, using GPT-2 and GPT-J models.

## Key Results
- SetKE achieves state-of-the-art performance on mainstream LLMs in KEO scenarios
- Efficacy score improves from 65.6 to 78.4 compared to existing methods
- Composite score (harmonic mean of efficacy, generalization, and locality) shows significant gains
- Superior performance demonstrated on GPT-2 Large (760M), GPT-2 XL (1.5B), and GPT-J (6B)

## Why This Works (Mechanism)

### Mechanism 1: Bipartite Matching for Optimal Assignment
Treating KEO editing as a bipartite matching problem enables optimal alignment between predicted and target object sets, reducing overwriting. The method creates a bipartite graph with predicted objects as "jobs" and editing targets as "workers," using the Hungarian algorithm to find the optimal assignment that minimizes total matching cost. This cost incorporates probability scores of correct matches, ensuring that multi-object edits are coordinated rather than sequential.

### Mechanism 2: Multi-Layer Residual Spreading for Locality
Distributing the residual update vector across multiple FFN layers mitigates unintended side effects on unrelated knowledge. Instead of updating a single layer's projection matrix, SetKE spreads the residual across critical layers, computing layer-specific updates that collectively achieve the desired output change with smaller per-layer perturbations.

### Mechanism 3: KL-Divergence Constraint for Subject Preservation
A KL-divergence loss on general subject prompts preserves the model's broader understanding of the subject entity during editing. The constraint term minimizes KL divergence of predictions for generic subject prompts (e.g., "s is a"), preventing the edit from disrupting unrelated subject properties.

## Foundational Learning

- **Transformer FFN as Key-Value Memory**: The paper's "locate-and-edit" paradigm assumes facts are stored in FFN layers as (key, value) pairs; understanding this is essential to grasp why overlapping triplets share neurons. Quick check: Given a prompt "The capital of France is," which FFN layer in a 12-layer GPT-2 is most likely modified by ROME? (Answer: Middle layers, typically 5-8.)

- **Bipartite Matching / Hungarian Algorithm**: SetKE's core optimization relies on assigning predicted objects to target objects optimally; without understanding bipartite matching, the loss formulation is opaque. Quick check: For a 3×3 cost matrix with all zeros, what assignment does the Hungarian algorithm return? (Answer: Any perfect matching; all have zero cost.)

- **Causal Mediation Analysis for Knowledge Localization**: Identifies which FFN layers/neurons causally influence the prediction of a specific fact, enabling targeted edits. Quick check: If intervening on layer 10's FFN output changes the probability of "Paris" for "Capital of France" but layer 2 does not, where is the fact likely stored? (Answer: Layer 10, or downstream of it.)

## Architecture Onboarding

- **Component map**: Knowledge Locator -> Matching Module -> Loss Computer -> Multi-Layer Updater
- **Critical path**: 1. Input KEO triplets (s, r, O) → 2. Get model predictions Ŷ for prompt tr(s) → 3. Run Hungarian matching between O and Ŷ → 4. Compute combined loss L(y, ŷ) → 5. Backprop to find δi → 6. Spread δi across L layers → 7. Update W_proj in each layer
- **Design tradeoffs**: Set size vs. compute (larger N increases O(N³) matching cost); number of edit layers (more layers improve locality but dilute edit strength); constraint weight (higher L_const preserves subject knowledge better but may reduce efficacy)
- **Failure signatures**: Overwriting still occurs (check predicted set size vs target set); locality degrades (check L_const weight); edit fails (δi magnitude insufficient)
- **First 3 experiments**: 1. Replicate Figure 4 on GPT2-Large with N from 3 to 8; 2. Ablate L_const on 500 KEO instances; 3. Layer sensitivity test using single layer vs L=5 layers

## Open Questions the Paper Calls Out

### Open Question 1
How does the computational complexity of the Hungarian algorithm (O(N³)) impact SetKE's scalability when editing extremely large sets of overlapping triplets compared to constant-time batch editing methods? The cubic time complexity may become a bottleneck for real-time editing in industrial-scale knowledge bases with thousands of overlaps per subject.

### Open Question 2
Can SetKE maintain its performance advantages when applied to significantly larger, state-of-the-art LLMs (e.g., LLaMA-3, GPT-4) where knowledge storage is potentially more distributed? The localization method may differ qualitatively in larger models, potentially affecting SetKE's mechanism.

### Open Question 3
How can the set-editing formulation be adapted to handle Subject-Object Overlap (SOO) or Relation-Object Overlap (ROO), which were excluded from this study? The current method optimizes sets sharing a single prefix tr(s); overlap types without shared prefixes require a different formulation.

## Limitations
- The effectiveness relies heavily on model's initial prediction set being sufficiently complete
- Claims about "robustness to various editing scales" based on limited testing of only three model scales
- EDIT SET dataset construction process and prompt generation are not fully specified

## Confidence

- **High confidence**: The core mechanism of using bipartite matching to coordinate multi-object edits in KEO scenarios is well-supported by experimental results
- **Medium confidence**: The locality improvements are meaningful but the KL-constraint's exact contribution is difficult to isolate
- **Low confidence**: Claims about performance on "mainstream LLMs" are based on limited testing - only three model scales and one evaluation dataset

## Next Checks

1. **Stress test bipartite matching**: Systematically evaluate SetKE on KEO cases where predicted object set size varies from 50% to 200% of target set size. Measure how matching quality and edit success degrade as completeness drops.

2. **Ablate KL-constraint**: Run SetKE without the L_const term on 500 KEO instances and report Locality Score. If locality drops significantly (>10 points), this validates the constraint's contribution.

3. **Cross-dataset generalization**: Apply SetKE to a non-Wikidata knowledge source (e.g., medical triples from UMLS) and evaluate if the 12.8-point efficacy gain over MEMIT holds.