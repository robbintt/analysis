---
ver: rpa2
title: 'Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary
  Study'
arxiv_id: '2505.02142'
source_url: https://arxiv.org/abs/2505.02142
tags:
- reasoning
- https
- wang
- zhang
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates the use of simpler, more cost-effective\
  \ Offline Reinforcement Learning (Offline RL) methods\u2014specifically Direct Preference\
  \ Optimization (DPO) and its length-desensitized variant LD-DPO\u2014to enhance\
  \ reasoning capabilities in large language models (LLMs). Unlike computationally\
  \ intensive Online RL approaches, Offline RL leverages pre-collected preference\
  \ data, offering greater efficiency and stability."
---

# Exploring the Potential of Offline RL for Reasoning in LLMs: A Preliminary Study

## Quick Facts
- arXiv ID: 2505.02142
- Source URL: https://arxiv.org/abs/2505.02142
- Reference count: 40
- Primary result: LD-DPO improves LLM reasoning by 3.3% average across 5 benchmarks, with 10.1% gain on Arena-Hard

## Executive Summary
This paper investigates whether simpler, more cost-effective Offline Reinforcement Learning (Offline RL) methods can enhance reasoning capabilities in large language models (LLMs). The authors focus on Direct Preference Optimization (DPO) and its length-desensitized variant LD-DPO, which leverage pre-collected preference data rather than requiring computationally intensive online sampling. Experiments on DeepDistill-32B across five reasoning benchmarks demonstrate that LD-DPO achieves an average performance improvement of 3.3%, with a notable 10.1% gain on the challenging Arena-Hard benchmark. The study shows that LD-DPO mitigates the verbosity and instability issues of standard DPO by reducing sensitivity to response length, leading to more concise and semantically rich outputs.

## Method Summary
The study uses LD-DPO to optimize DeepDistill-32B on ~67,575 preference pairs constructed from DeepSeek-R1 distilled outputs. The preference pairs are selected based on verification scores (0 < verify_score < 1), ensuring chosen responses pass verification while rejected responses fail. LD-DPO modifies the standard DPO loss by introducing length-desensitization hyperparameter α=0.3, which reduces the contribution of excess tokens beyond the common length in each preference pair. Training uses AdamW optimizer with learning rate 5e-7, batch size 32, 1 epoch, 10% warmup, and cosine annealing. The reference policy is the DeepDistill-32B SFT checkpoint.

## Key Results
- LD-DPO achieves 3.3% average improvement across five reasoning benchmarks
- Arena-Hard shows 10.1% performance gain with LD-DPO
- IFEval instruction-following improves from 59.7% (DPO) to 68.4% (LD-DPO)
- DPO increases average generation length by ~25% (6273→7839 tokens) and degrades IFEval performance
- LD-DPO maintains moderate length increase (7444.5 tokens) while improving performance

## Why This Works (Mechanism)

### Mechanism 1: Length-Normalized Likelihood Decomposition
Standard DPO computes sequence likelihood as a product of token probabilities, inherently penalizing longer sequences. LD-DPO introduces hyperparameter α ∈ [0, 1] to reparameterize likelihood: tokens beyond the shorter response in a preference pair contribute with exponent α rather than 1. This reduces verbosity bias while preserving semantic quality differences.

### Mechanism 2: Semantic-Richness-Aligned Length Extension
Performance gains require length increases paired with semantic density, not indiscriminate verbosity. LD-DPO's controlled length sensitivity (α=0.3) allows productive length extension while penalizing redundant padding. The chosen/rejected pairs from DeepSeek-R1 distillation encode genuine quality differences, not just length artifacts.

### Mechanism 3: Static Preference Dataset Avoids Online Sampling Instability
Offline RL provides training stability by eliminating on-policy sampling variance inherent in PPO/GRPO. DPO-style methods optimize directly against a fixed preference dataset, treating it as a static supervision signal. This avoids the variance from exploration and reward model approximation errors present in online RL.

## Foundational Learning

- **Concept: Direct Preference Optimization (DPO) Loss Function**
  - Why needed: Understanding Equation 1 is prerequisite to understanding why length sensitivity emerges and how LD-DPO modifies it.
  - Quick check: Given a chosen response yw and rejected response yl, what happens to the DPO loss when yw is 2× longer than yl, assuming equal per-token probability?

- **Concept: Sequence Likelihood as Token Product**
  - Why needed: The core diagnosis of DPO's verbosity bias requires understanding why longer sequences receive lower likelihoods under autoregressive models.
  - Quick check: Why does the product of per-token probabilities decrease as sequence length increases, even if each token has high probability?

- **Concept: Preference Pair Construction from Verification**
  - Why needed: The paper constructs chosen/rejected pairs based on DeepSeek-R1 verification scores—understanding this is critical for reproducing the data pipeline.
  - Quick check: What quality signal should distinguish chosen from rejected responses in a reasoning task, and how might this signal correlate with length?

## Architecture Onboarding

- **Component map:** Query sources (5 categories: math, code, science, IF, general) → DeepSeek-R1 distillation → verification scoring → chosen/rejected pair selection → ground truth validation with o4-mini-high → LD-DPO training → benchmark evaluation

- **Critical path:**
  1. Query filtering: Remove pass_rate ∈ {0, 1} (too easy/hard)
  2. Ground truth validation: Cross-check R1 mode vs. original GT vs. o4-mini-high
  3. Length-balanced pair selection: Ensure chosen/rejected differ in quality, not just length
  4. LD-DPO training with β=0.1, α=0.3
  5. Checkpoint evaluation at 10% intervals

- **Design tradeoffs:**
  - α selection: Lower α reduces verbosity but may suppress useful length extension; paper uses α=0.3 empirically but provides no ablation
  - Single epoch: Limits overfitting but may underutilize preference signal
  - Fixed reference policy: πref from SFT provides stable baseline but cannot adapt during training

- **Failure signatures:**
  - DPO verbosity spiral: Generation length increases without performance gain; IFEval-style instruction following degrades (59.7% vs. 72.8% baseline)
  - Length desensitization overcorrection: If α too low, model may prefer shorter but incorrect responses
  - Ground truth corruption: If R1 mode disagrees with o4-mini-high and query is discarded, dataset shrinks; if kept, noise propagates

- **First 3 experiments:**
  1. Reproduce baseline: Train DeepDistill-32B with standard DPO (β=0.1) → expect length increase + IFEval degradation
  2. α ablation: Compare α ∈ {0.0, 0.3, 0.5, 1.0} on Arena-Hard → expect α=0.3 optimal, α=0.0 underperforms, α=1.0 (standard DPO) shows instability
  3. Length-performance correlation: Plot generation length vs. benchmark accuracy for DPO vs. LD-DPO → expect LD-DPO achieves higher accuracy per token

## Open Questions the Paper Calls Out

### Open Question 1
Can Offline RL methods be effectively combined with Online RL in a sequential training pipeline to achieve further performance gains? The current study isolates Offline RL as a standalone approach and does not investigate hybrid or sequential training strategies that might leverage the stability of Offline RL before the exploration of Online RL.

### Open Question 2
What is the precise performance gap between Offline RL and Online RL methods on reasoning benchmarks? The paper provides comparisons to DeepSeek-R1 (an Online RL model) as a reference point but does not conduct controlled experiments equating data, compute, and model scale.

### Open Question 3
Why does LD-DPO yield substantially different performance improvements across reasoning domains (e.g., +10.1% on Arena-Hard vs. +0.0% on GPQA-Diamond)? The analysis focuses on length sensitivity and verbosity but does not investigate whether task structure, question format, or data distribution cause certain domains to benefit more from length-desensitized optimization.

### Open Question 4
How robust is the length-desensitization hyperparameter α=0.3 across different model scales and task types? The authors note α was "empirically set to 0.3 based on preliminary experiments," but all experiments use a single 32B model without investigating whether this value generalizes.

## Limitations

- Single model scale: All experiments use 32B parameter model without investigating scalability
- Limited hyperparameter exploration: α=0.3 was empirically chosen without systematic ablation
- No online RL comparison: Performance is compared to DeepSeek-R1 but not against controlled online RL experiments
- Single epoch training: May underutilize preference signal or limit performance ceiling

## Confidence

- Claim: LD-DPO improves reasoning performance by 3.3% average
  - Evidence: Controlled experiments across 5 benchmarks with clear statistical improvements
  - Confidence: High

- Claim: Standard DPO causes verbosity and IFEval degradation
  - Evidence: Direct comparison shows length increase from 6273→7839 tokens and IFEval drop from 72.8%→59.7%
  - Confidence: High

- Claim: α=0.3 is optimal for length-desensitization
  - Evidence: Empirical choice with no ablation study
  - Confidence: Low

## Next Checks

1. Verify LD-DPO implementation correctly applies α only to excess tokens beyond common length in preference pairs
2. Confirm IFEval scores improve from baseline when using LD-DPO (vs. degradation with standard DPO)
3. Check generation length distributions to ensure LD-DPO maintains moderate increases while improving performance