---
ver: rpa2
title: Towards Fairness Assessment of Dutch Hate Speech Detection
arxiv_id: '2506.12502'
source_url: https://arxiv.org/abs/2506.12502
tags:
- counterfactual
- dutch
- hate
- speech
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates counterfactual fairness in Dutch hate speech
  detection models, a gap in the literature. It curates Dutch Social Group Terms,
  generates counterfactual data using LLMs, SLL, and MGS, and fine-tunes BERTje models.
---

# Towards Fairness Assessment of Dutch Hate Speech Detection

## Quick Facts
- arXiv ID: 2506.12502
- Source URL: https://arxiv.org/abs/2506.12502
- Reference count: 11
- Dutch hate speech detection models show improved group fairness (DPD=0.06, EOD=0.11) using counterfactual data augmentation, but toxic-class fairness degraded

## Executive Summary
This study evaluates counterfactual fairness in Dutch hate speech detection models, addressing a gap in the literature where most fairness research focuses on English. The authors curate Dutch Social Group Terms, generate counterfactual data using four methods (LLMdef, LLMlist, SLL, MGS), and fine-tune BERTje models. The SLL and MGS methods improve both counterfactual fairness and group fairness metrics, with SLL performing best (DPD=0.06, EOD=0.11), while LLM-generated data did not improve fairness. Models achieved 79% accuracy, with notable class imbalance affecting performance. Qualitative analysis revealed grammar issues in SLL/MGS outputs and context mismatches in LLM outputs.

## Method Summary
The study fine-tunes BERTje on the IMSyPP Dutch hate speech dataset with counterfactual augmentations. Four counterfactual generation methods were used: LLMdef (implicit LLM substitution), LLMlist (LLM with predefined SGT list), SLL (substitute SGTs filtered by GPT-2 log-likelihood), and MGS (dictionary-based substitution by grammatical category). The authors evaluated performance using accuracy, F1, precision, recall, and fairness using Counterfactual Token Fairness (CTF), Demographic Parity Difference (DPD), and Equalized Odds Difference (EOD). Data preprocessing included removing emojis, extra spaces, and special characters.

## Key Results
- SLL and MGS methods improved group fairness: DPD reduced from 0.38 to 0.06 (SLL) and EOD from 0.53 to 0.11
- Counterfactual token fairness worsened for toxic class across all methods (0.11 baseline → 0.20-0.32)
- BERTje models achieved 79% accuracy with significant class imbalance affecting inappropriate and violent classes
- LLM-generated counterfactuals, despite being more grammatically fluent, did not improve fairness metrics

## Why This Works (Mechanism)

### Mechanism 1
Counterfactual data augmentation reduces group fairness disparities by exposing the model to sentence pairs differing only in social group terms while preserving toxicity labels. This teaches the model that equivalent toxicity statements about different groups should receive identical classifications, reducing prediction variance across groups. The assumption is that generated counterfactuals preserve semantic meaning and toxicity levels without introducing systematic label noise. Evidence shows SLL reduced DPD from 0.38 to 0.06 and EOD from 0.53 to 0.11, though the paper notes CTF worsened for toxic class when counterfactuals failed to preserve toxicity labels.

### Mechanism 2
Dutch morphological constraints critically affect counterfactual generation quality because adjectives conjugate based on noun attributes and some terms function as both nouns and adjectives. English-designed substitution methods fail to model these morphological agreements, producing ungrammatical outputs that may introduce noise. The paper documents SLL producing "jong mensen" instead of "jongere mensen" and MGS substituting "Chinees" with "Turk" instead of "Turks." While these grammar issues are documented, the paper does not prove they directly harm fairness, only that they affect data quality.

### Mechanism 3
LLM-generated counterfactuals, despite grammatical fluency, failed to improve fairness because they made context-inappropriate substitutions that introduced semantic drift. Examples include replacing football team colors with education terms and misinterpreting idioms like "iemand zwart maken" (to defame) as "iemand wit maken." This semantic drift may introduce noise or mismatch between generated text and original labels, failing to provide the consistent contrast signal needed for fairness learning. The paper assumes the data was too noisy but did not perform ablation studies to isolate data quality versus quantity effects.

## Foundational Learning

- **Counterfactual Fairness (Kusner et al., 2017)**: The core fairness criterion where a model's prediction should remain unchanged when a sensitive attribute is changed in a counterfactual world. The paper operationalizes this via CTF, which measures prediction variance across counterfactual pairs. Quick check: If you swap "Moroccan" for "Dutch" in a hate speech example, should the model's prediction change? Under counterfactual fairness, what should happen?

- **Group Fairness Metrics (Demographic Parity, Equalized Odds)**: Complementary to individual-level counterfactual fairness. DPD measures disparity in positive prediction rates across groups. EOD measures disparity in true positive and false negative rates. The paper finds improvements in both, but trade-offs exist (toxic-class CTF worsened). Quick check: If a model flags 30% of posts mentioning Group A as hateful but only 10% for Group B, is this a DPD or EOD violation?

- **Morphological Agreement in Dutch**: Dutch adjectives conjugate (e.g., "jong" → "jonge" before plural nouns), and some SGTs are ambiguous (noun vs. adjective). Counterfactual generation methods must handle this to produce valid training data. Quick check: In Dutch, why can't you simply swap "Turk" for "Marokkaan" in every context? What grammatical rule might be violated?

## Architecture Onboarding

- **Component map**: Dutch SGT Curation (85 terms across 7 categories) -> Counterfactual Generation (4 methods: LLMdef, LLMlist, SLL, MGS) -> BERTje Fine-tuning (IMSyPP + augmentations) -> Fairness Evaluation (CTF, DPD, EOD on 2,890 template-based sentences)

- **Critical path**: Curate language-specific SGTs (Dutch social context differs from English) -> Generate counterfactuals using at least two methods (SLL + MGS recommended) -> Qualitatively assess sample counterfactuals for grammar and semantic coherence -> Fine-tune model with combined original + counterfactual data -> Evaluate on held-out test set + counterfactual evaluation set

- **Design tradeoffs**: SLL vs. MGS (SLL better fairness metrics but both had grammar issues), LLM vs. rule-based (LLMs more fluent but with semantic drift), Data volume vs. quality (SLL generated 49K counterfactuals but quality varied). The paper did not perform ablation studies on data size or fine-tuning strategy.

- **Failure signatures**: Grammar errors in SLL/MGS outputs (adjective conjugation failures, noun/adjective substitutions), Context mismatches in LLM outputs (idiom misinterpretation, unrelated substitutions), Toxic-class CTF degradation (all counterfactual models worsened CTF for toxic templates), Class imbalance (inappropriate/violent classes had lower F1 scores).

- **First 3 experiments**: 1) Validate counterfactual quality before training by manually annotating 100-200 generated counterfactuals from each method for grammatical correctness and semantic preservation. 2) Ablate data size by fine-tuning BERTje with varying proportions of counterfactual data (25%, 50%, 75%, 100%) to disentangle volume effects from method effects. 3) Isolate toxic-class CTF degradation by evaluating per-category CTF on held-out toxic templates to analyze whether specific SGT categories drive the degradation.

## Open Questions the Paper Calls Out

1. Why does counterfactual data augmentation improve counterfactual fairness for non-toxic templates but worsen it for toxic templates? The authors document this counterintuitive phenomenon but provide no explanation for the class-specific fairness degradation.

2. How does varying the size and composition of counterfactual training data affect the trade-off between model performance and fairness? The study uses fixed dataset sizes without systematic comparison across different data scaling regimes.

3. Can grammatical accuracy in Dutch counterfactual generation be improved while maintaining or improving fairness gains? The paper documents grammar issues but doesn't explore their relationship to fairness outcomes.

4. How do state-of-the-art multilingual or Dutch-specific LLMs perform in generating counterfactuals compared to the methods tested? The LLM experiments don't specify which models were used, and newer models may have better Dutch language understanding.

## Limitations

- The study lacks systematic error analysis of generated counterfactual outputs, with grammar issues documented but not quantified for their impact on model performance.
- The toxic-class CTF degradation remains unexplained, with the paper noting this counterintuitive result without investigating underlying causes.
- No fine-tuning hyperparameters, data mixing ratios, or ablation studies are reported, limiting understanding of whether improvements are robust to training choices.

## Confidence

- **High Confidence**: Experimental methodology for generating counterfactual data and computing fairness metrics (CTF, DPD, EOD) is clearly specified and reproducible. Improvement in group fairness metrics using SLL and MGS methods is well-supported by quantitative results.
- **Medium Confidence**: Qualitative analysis of grammar issues in counterfactual generation is descriptive but not systematically quantified. Claim that LLM-generated data failed due to context mismatches is plausible but lacks rigorous validation.
- **Low Confidence**: Explanation for toxic-class CTF degradation is absent. The study does not adequately address why fairness improvements for non-toxic classes do not extend to toxic classes.

## Next Checks

1. **Systematic Quality Assessment**: Manually annotate 200 randomly sampled counterfactuals from each generation method (SLL, MGS, LLMdef, LLMlist) for grammatical correctness and semantic preservation. Compute inter-annotator agreement and report the percentage of high-quality counterfactuals to quantify the relationship between generation quality and fairness outcomes.

2. **Toxic-Class Diagnostic Analysis**: Isolate the toxic-class CTF degradation by evaluating per-SGT-category CTF scores on held-out toxic templates. Identify whether specific social group categories (e.g., skin color, sexuality) drive the degradation, and analyze whether this correlates with particular generation method failures or inherent properties of toxic language.

3. **Hyperparameter Sensitivity Analysis**: Conduct controlled experiments varying fine-tuning learning rates (1e-5, 2e-5, 5e-5), batch sizes (8, 16, 32), and data mixing ratios (25%, 50%, 75%, 100% counterfactual augmentation). Report how these variations affect both performance metrics and fairness metrics to determine whether observed improvements are robust to training choices.