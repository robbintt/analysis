---
ver: rpa2
title: 'Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs'
arxiv_id: '2505.12049'
source_url: https://arxiv.org/abs/2505.12049
tags:
- utility
- lexicographic
- theorem
- expected
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends lexicographic expected utility theory to sequential
  decision-making, motivated by settings where objectives must be prioritized in a
  strict, non-compensatory order. The authors identify a simple and practical condition
  under which preferences cannot be captured by scalar rewards, necessitating lexicographically
  ordered utility vectors.
---

# Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs

## Quick Facts
- **arXiv ID:** 2505.12049
- **Source URL:** https://arxiv.org/abs/2505.12049
- **Reference count:** 35
- **Primary result:** This paper extends lexicographic expected utility theory to sequential decision-making, motivated by settings where objectives must be prioritized in a strict, non-compensatory order. The authors identify a simple and practical condition under which preferences cannot be captured by scalar rewards, necessitating lexicographically ordered utility vectors.

## Executive Summary
This paper introduces a principled framework for handling lexicographic objectives in sequential decision-making by extending the von Neumann-Morgenstern utility theory. The key insight is that when preferences violate the continuity axiom due to an "infinitely bad" outcome, standard scalar rewards fail to capture the true utility structure. The authors provide a full characterization of lexicographic utility functions in Markov Decision Processes under a memorylessness assumption, demonstrating that optimal policies retain desirable properties like stationarity and determinism, in contrast to Constrained MDPs. The framework generalizes the reward hypothesis and offers a foundation for modeling safety-critical systems where certain catastrophic outcomes must be strictly avoided regardless of potential rewards.

## Method Summary
The paper develops an axiomatic framework for lexicographic utility in MDPs by extending vNM expected utility theory. The key components include: (1) introducing Axiom 7 "Safety First" that formalizes an infinitely bad outcome violating continuity, (2) defining lexicographic utility functions as vectors where comparison proceeds dimension-by-dimension, (3) establishing a recursive Bellman equation formulation using transition-dependent lower-triangular matrices Γ rather than scalar discount factors, and (4) proving that optimal policies in Lexicographic MDPs remain stationary and deterministic when diagonal entries of Γ are less than 1. The 2D case specifically models safety probability in the first dimension and rewards in the second, with the safety probability scaling subsequent reward accumulation.

## Key Results
- Preferences violating continuity axiom necessitate 2-dimensional lexicographic utility vectors tracking safety probability and standard utility
- Lexicographic MDPs support stationary, deterministic optimal policies unlike Constrained MDPs
- The fundamental theorem of MDPs extends to lexicographic case when diagonal entries of Γ are less than 1
- General form u(e·τ) = r(e) + Γ(e)u(τ) allows coupling between utility dimensions through lower-triangular matrices

## Why This Works (Mechanism)

### Mechanism 1: Axiomatic Derivation of Non-Compensatory Utility
The paper proves that when preferences include an "infinitely bad" outcome o† violating the continuity axiom, scalar rewards cannot represent preferences. Axiom 7 ("Safety First") formalizes that any probability of o† is preferred against, necessitating a 2-dimensional utility vector where the first dimension tracks safety probability and the second tracks standard utility. This mechanism collapses to scalar utility if continuity is restored.

### Mechanism 2: Recursive Matrix Discounting
Sequential decision-making under lexicographic utility uses transition-dependent lower-triangular matrices Γ instead of scalar discount factors. The utility propagates via u(e·τ) = r(e) + Γ(e)u(τ), where Γ allows the realization of one utility dimension (e.g., safety) to scale the accumulation of subsequent dimensions. This mathematical coupling preserves the lexicographic ordering of future returns.

### Mechanism 3: Preservation of Uniform Optimality
Optimal policies in Lexicographic MDPs remain stationary and deterministic when diagonal entries of Γ are less than 1. Theorem 7 proves the lexicographic maximum of the Q-function exists under this condition, avoiding the history-dependence and randomization required in Constrained MDPs to satisfy constraints.

## Foundational Learning

- **Von Neumann-Morgenstern (vNM) Expected Utility Theory**: Why needed - This paper extends vNM; understand the four axioms to see which one is dropped (Continuity) to create lexicographic utility. Quick check - Why does the "Continuity" axiom prevent an outcome from being "infinitely bad"?
- **Lexicographic Ordering**: Why needed - This is the comparison operator for utility vectors; comparison stops at the first dimension where values differ. Quick check - Is the vector [0.9, 1000] lexicographically greater or less than [1.0, 1]?
- **Stationary vs. History-Dependent Policies**: Why needed - A major result is that LMDPs allow stationary optimal policies, whereas CMDPs often require complex history-dependent policies. Quick check - Does a stationary policy change its action based on the sequence of states visited previously?

## Architecture Onboarding

- **Component map**: Preference Space (Axioms) → Utility Function u: Δ(E*) → ℝ^d → Rewards r ∈ ℝ^d & Multipliers Γ ∈ ℝ^(d×d) → Optimal Policy π (greedy w.r.t. lexicographic Q*)
- **Critical path**: Verify axioms → Define Γ and r → Solve lexicographic Bellman equation (Eq. 14) → Extract greedy policy
- **Design tradeoffs**: Scalarization vs. Vectorization (scalarization is cheaper but violates "Safety First"); LMDP vs. CMDP (LMDPs support deterministic policies but require strict priority; CMDPs allow tradeoffs but require stochastic policies)
- **Failure signatures**: Scalar Collapse (approximating LMDP with scalar reward leads to "unsafe" behavior); Infinite Utility (setting diagonal entries of Γ ≥ 1 breaks value function convergence)
- **First 3 experiments**: 1) Implement 2D LMDP environment and verify safety-first behavior vs. scalar agent; 2) Compare LMDP vs. CMDP optimal policies to verify LMDP yields stationary deterministic policies; 3) Vary off-diagonal entries in Γ to observe coupling effects on policy aggressiveness

## Open Questions the Paper Calls Out

### Open Question 1
Can efficient algorithmic methods be developed to solve Lexicographic MDPs (LMDPs) where the reward multipliers Γ are full lower triangular matrices rather than diagonal matrices? The authors state they "have not proposed new algorithmic methods" and note their framework generalizes Γ beyond the diagonal structures typically assumed in prior work, leaving this algorithmic solution unexplored.

### Open Question 2
Can the "Safety First" axiomatic framework be operationalized to maintain strict safety guardrails, such as sandbox confinement, in AI control systems? While the theoretical utility functions are defined, they have not yet been implemented to manage the conflict between safety constraints and task performance in dynamic environments.

### Open Question 3
In known, fixed environments, is it possible to design simpler scalar rewards that approximate lexicographic utilities without sacrificing the strict priority of safety? The paper proves scalar rewards are theoretically insufficient for the general axiomatic case, but the practical distinction in finite, known MDPs remains unclear.

## Limitations
- The framework assumes perfect knowledge of the catastrophic outcome o† and its infinite disutility, which may be difficult to verify in practice
- The requirement for lower-triangular Γ matrices may not capture all hierarchical preference structures
- The paper focuses on finite MDPs and does not address function approximation or large-scale implementation challenges

## Confidence

- **High Confidence**: The axiomatic derivation showing when scalar rewards fail (Theorem 5) and the fundamental theorem of LMDPs (Theorem 7) are mathematically rigorous and well-supported by the theoretical framework
- **Medium Confidence**: The recursive matrix formulation and its connection to lexicographic utility propagation (Theorem 3, Eq. 9) is sound, but the practical implications for algorithm design require further validation
- **Low Confidence**: The claim that LMDPs are "simpler" than CMDPs in practice, while theoretically supported, lacks empirical validation across diverse constraint problems

## Next Checks

1. **Axiom Verification**: Implement the gridworld from Figure 1 and verify that a scalar agent chooses the risky high-reward path while the LMDP agent chooses the safe path, demonstrating the practical impact of violating continuity
2. **Policy Structure Analysis**: Compare optimal policies from LMDP vs. CMDP formulations on identical constraint problems to empirically verify that LMDP yields stationary deterministic policies while CMDP requires stochastic policies
3. **Matrix Sensitivity Testing**: Systematically vary off-diagonal entries in Γ (Eq. 16) to observe how coupling between safety probability and secondary rewards affects policy aggressiveness, validating the matrix formulation's expressiveness