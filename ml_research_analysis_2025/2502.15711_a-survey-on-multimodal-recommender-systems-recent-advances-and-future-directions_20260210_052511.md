---
ver: rpa2
title: 'A Survey on Multimodal Recommender Systems: Recent Advances and Future Directions'
arxiv_id: '2502.15711'
source_url: https://arxiv.org/abs/2502.15711
tags:
- multimodal
- learning
- fusion
- information
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive survey of Multimodal Recommender
  Systems (MRS), which leverage diverse multimedia information (text, images, videos,
  audio) to enhance recommendation performance. The survey systematically categorizes
  MRS technologies into four key components: Feature Extraction, Encoder, Multimodal
  Fusion, and Loss Function.'
---

# A Survey on Multimodal Recommender Systems: Recent Advances and Future Directions

## Quick Facts
- arXiv ID: 2502.15711
- Source URL: https://arxiv.org/abs/2502.15711
- Reference count: 40
- Primary result: Comprehensive survey and taxonomy of Multimodal Recommender Systems (MRS), categorizing approaches into Feature Extraction, Encoder, Multimodal Fusion, and Loss Function components.

## Executive Summary
This paper presents a comprehensive survey of Multimodal Recommender Systems (MRS), which leverage diverse multimedia information (text, images, videos, audio) to enhance recommendation performance. The survey systematically categorizes MRS technologies into four key components: Feature Extraction, Encoder, Multimodal Fusion, and Loss Function. The work provides researchers with a structured framework to understand current MRS technologies and identify potential research gaps, supported by an open-source repository for practical implementation.

## Method Summary
The survey systematically reviews 40+ Multimodal Recommender Systems by categorizing them into four key components: Feature Extraction (visual: ResNet, ViT; textual: BERT, Sentence-Transformer), Encoder (Matrix Factorization-based and Graph-based approaches), Multimodal Fusion (timing strategies: early vs. late fusion; methodology: element-wise, concatenation, attentive, heuristic), and Loss Function (supervised: pointwise and pairwise losses; self-supervised: feature-based and structure-based approaches). The paper provides detailed analysis of each component's development, motivations, and current limitations, while also discussing future research directions including unified MRS models, cold-start problem resolution, and richer modality integration.

## Key Results
- Presents a systematic taxonomy of MRS technologies into four key components
- Reviews and analyzes 40+ methods across different MRS architectures
- Identifies key limitations of current MRS approaches and proposes future research directions
- Provides an open-source repository (https://github.com/Jinfeng-Xu/Awesome-Multimodal-Recommender-Systems) for practical implementation

## Why This Works (Mechanism)
The survey works by providing a comprehensive framework that systematically categorizes MRS technologies, enabling researchers to understand the current landscape and identify gaps. By analyzing the four key components (Feature Extraction, Encoder, Multimodal Fusion, and Loss Function) separately, the paper reveals the interdependencies and limitations of current approaches. The mechanism of categorization allows for clear identification of research opportunities, particularly in unifying feature extraction and encoding, addressing cold-start problems, and integrating richer modalities beyond text and images.

## Foundational Learning
1. **Multimodal Feature Extraction** - Why needed: To convert raw multimedia data (images, text, audio) into meaningful numerical representations. Quick check: Verify that visual features use CNN-based models (ResNet, ViT) and textual features use transformer-based models (BERT, Sentence-Transformer).

2. **Encoder Architectures** - Why needed: To transform extracted features into user/item embeddings while preserving modality-specific information. Quick check: Confirm distinction between Matrix Factorization-based (e.g., VBPR) and Graph-based (e.g., MMGCN, LATTICE) encoders.

3. **Multimodal Fusion Strategies** - Why needed: To combine information from different modalities effectively. Quick check: Understand timing strategies (early vs. late fusion) and methodology (element-wise, concatenation, attentive, heuristic).

4. **Loss Function Design** - Why needed: To optimize model parameters based on recommendation objectives. Quick check: Differentiate between supervised learning (pointwise vs. pairwise losses) and self-supervised learning approaches.

## Architecture Onboarding

**Component Map**: Feature Extraction -> Encoder -> Multimodal Fusion -> Loss Function

**Critical Path**: The most critical path for MRS performance lies in the interaction between Encoder and Multimodal Fusion, where the model must effectively combine modality-specific representations while preserving important information.

**Design Tradeoffs**: Early fusion allows correlation capture but loses modality-specific features; late fusion preserves specific features but misses correlations. The choice of encoder (MF-based vs. Graph-based) affects how well the model can handle complex user-item relationships and incorporate multimodal information.

**Failure Signatures**: Inconsistent feature extraction leads to incomparable embeddings; mismatched evaluation protocols create unfair comparisons; segregated feature extraction and encoding introduce multimodal noise; traditional ID-based models fail on cold-start problems.

**First Experiments**:
1. Implement a baseline MRS using standardized feature extraction (ResNet for images, BERT for text) with a simple early fusion strategy
2. Compare MF-based encoder (VBPR) vs Graph-based encoder (MMGCN) on the same dataset with identical feature extraction
3. Test different fusion strategies (early vs. late) using the same encoder architecture to evaluate the impact on recommendation performance

## Open Questions the Paper Calls Out

**Open Question 1**: How can a unified MRS architecture be developed to seamlessly integrate feature extraction and representation encoding, thereby eliminating the noise caused by the current segregation of these processes? [explicit] Section VII.A states that current models segregate feature extraction and encoding, resulting in "inherent multimodal noise," and explicitly calls for an "urgent need for a unified model."

**Open Question 2**: How can multimodal information be leveraged to accurately infer preferences for new users or items in dynamic environments where historical interaction data is absent? [explicit] Section VII.B identifies the "cold-start problem" as a challenge where traditional ID-based models fail, suggesting the need to exploit multimodal information for new entities.

**Open Question 3**: What technical frameworks are required to effectively process and synthesize non-traditional sensory modalities, such as auditory, olfactory, and kinesthetic data, into recommendation logic? [explicit] Section VII.C notes that existing MRS models are effective for text and visuals but highlights a need to leverage a "richer variety of modalities" including olfactory and kinesthetic data.

**Open Question 4**: How can a multimodal fusion strategy be optimized to simultaneously exploit modality-specific features (typically lost in early fusion) while capturing inter-modal correlations (often missed in late fusion)? [inferred] Section V.A explicitly outlines the limitations of current timing strategies: early fusion prevents full exploitation of specific features, while late fusion fails to capture correlations between modalities.

## Limitations
- Lacks specific evaluation protocols and dataset details for practical implementation
- Does not provide concrete hyperparameter configurations for individual methods
- Focuses primarily on text and visual modalities, with limited discussion of other sensory data
- Cross-paper comparison may be inconsistent due to varying implementation details across referenced works

## Confidence
- **High** confidence in the systematic categorization of MRS components and comprehensive literature review
- **Medium** confidence in practical applicability due to lack of specific evaluation protocols and implementation details
- **Medium** confidence in reproducibility of individual methods without additional documentation from referenced papers

## Next Checks
1. Verify and standardize feature extraction methods (ResNet/ViT for visual, BERT/Sentence-Transformer for textual) across different MRS implementations to ensure comparability
2. Extract and document specific evaluation protocols (train/test splits, negative sampling strategies, metrics like precision@K, recall@K, NDCG) from the referenced papers to enable fair cross-paper comparison
3. Test the practical implementation of selected MRS methods from the open-source repository on a common dataset (e.g., Amazon or TikTok) to assess the feasibility of the survey's recommendations and identify any gaps in the provided implementation details