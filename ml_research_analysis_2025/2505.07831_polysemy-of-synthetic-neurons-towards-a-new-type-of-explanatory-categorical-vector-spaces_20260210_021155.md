---
ver: rpa2
title: Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical
  Vector Spaces
arxiv_id: '2505.07831'
source_url: https://arxiv.org/abs/2505.07831
tags:
- categorical
- neuron
- arxiv
- activation
- sub-dimensions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether the activation level of a token
  in a synthetic neuron is related to its coordinates within a vector space formed
  by the categorical sub-dimensions of that neuron. The researchers propose that neurons
  can be interpreted as categorical vector spaces whose basis consists of categorical
  sub-dimensions extracted from preceding neurons.
---

# Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces

## Quick Facts
- **arXiv ID**: 2505.07831
- **Source URL**: https://arxiv.org/abs/2505.07831
- **Reference count**: 0
- **Primary result**: 78.01% of neurons showed positive differences in dimensional proximity between high and low activation tokens, suggesting activation drives categorical convergence

## Executive Summary
This study investigates whether token activation levels in synthetic neurons correlate with their coordinates within categorical vector spaces formed by sub-dimensions of those neurons. Using GPT2-XL's second MLP layer, researchers analyzed 614 neurons with at least three taken-clusters and found that higher activation levels correlate positively with higher dimensional proximity to multiple categorical sub-dimensions. The results support a model where neuronal polysemy can be understood as a vector space structured by activation, with activation acting as an intra-neuronal attention mechanism guiding concept formation through categorical convergence.

## Method Summary
The methodology involves identifying "taken-clusters" - tokens shared between a target neuron and its top-10 connected precursor neurons - then computing dimensional proximity as mean cosine similarity between core-tokens and taken-cluster tokens using GPT-2 XL input embeddings. The study filtered for neurons with at least three taken-clusters and six tokens per cluster, then compared dimensional proximity between top-10 and bottom-10 activating core-tokens. Statistical validation used Kruskal-Wallis tests and Kendall's tau correlations to establish significance of the activation-proximity relationship.

## Key Results
- 78.01% of neurons showed positive differences in dimensional proximity between high and low activation tokens
- Kendall's τ correlations of 0.78–0.85 between activation rank and dimensional proximity (p < .0001)
- PCA analysis revealed Factor 1 capturing 93.5% variance showing positive correlations among sub-dimensions (convergence)

## Why This Works (Mechanism)

### Mechanism 1: Activation-Dimensional Proximity Correlation
- Claim: Token activation levels correlate positively with dimensional proximity to a neuron's principal categorical sub-dimensions.
- Mechanism: The aggregation function combines categorical phasing with connection weights, causing highly activated tokens to simultaneously engage multiple categorical sub-dimensions.
- Core assumption: The relationship observed in early MLP layers generalizes to deeper layers and other architectures.
- Evidence anchors: 78.01% positive differences; Kendall's τ 0.78–0.85; related work on categorical clipping.
- Break condition: If activation-proximity correlation fails to replicate in later layers or larger models.

### Mechanism 2: Non-Orthogonal Categorical Vector Space Structure
- Claim: A neuron's polysemantic category can be modeled as a categorical vector space with non-orthogonal basis of clipped sub-dimensions.
- Mechanism: Categorical clipping extracts relatively monosemantic sub-dimensions from precursor neurons; partial categorical confluence causes semantic overlap.
- Core assumption: Sub-dimensions remain linearly independent despite non-orthogonality, preserving vector space property.
- Evidence anchors: Basis consists of non-orthogonal sub-dimensions; PCA shows convergence factor (93.5% variance).
- Break condition: If sub-dimensions prove linearly dependent, vector space interpretation collapses.

### Mechanism 3: Intra-Neuronal Attention via Activation Segmentation
- Claim: Activation magnitude functions as attention mechanism identifying "critical concept-in-action" - categorically homogeneous subset at high activation levels.
- Mechanism: High-activation tokens occupy intersection of multiple categorical sub-dimensions (categorical convergence), reducing polysemy.
- Core assumption: Activation-categorical convergence relationship is causal rather than merely correlational.
- Evidence anchors: Activation acts as intra-neuronal attention; categorical convergence from reduced degrees of freedom.
- Break condition: If causal interventions fail to shift categorical proximity, attention interpretation is suspect.

## Foundational Learning

- Concept: **Superposition hypothesis in mechanistic interpretability**
  - Why needed here: The paper positions itself as alternative to superposition-based explanations of polysemy. Understanding superposition is essential to evaluate claims about intra- vs. inter-neuronal polysemy encoding.
  - Quick check question: Can you explain why quasi-orthogonal representations allow more features than dimensions, and how this differs from the paper's intra-neuronal vector space model?

- Concept: **Categorical clipping and token clustering**
  - Why needed here: Methodology depends on identifying "taken-clusters" - tokens shared between target neuron and strongly-connected precursors. Without understanding extraction process, dimensional proximity calculations are opaque.
  - Quick check question: Given a neuron in layer 1 with 10 precursor neurons in layer 0, how would you identify its principal categorical sub-dimensions operationally?

- Concept: **Non-orthogonal basis and partial confluence**
  - Why needed here: Paper's key geometric claim is that categorical sub-dimensions are non-orthogonal but linearly independent. Understanding why orthogonality fails but independence holds is critical for vector space interpretation.
  - Quick check question: If two categorical sub-dimensions have cosine similarity 0.6, are they still valid basis vectors? What conditions must hold?

## Architecture Onboarding

- Component map: Layer n-1 neurons → (categorical clipping via x, w, Σ effects) → Categorical sub-dimensions → (partial confluence) → Layer n neuron as categorical vector space → (intra-neuronal attention) → Critical concept-in-action → Layer n+1 categorical clipping

- Critical path: The aggregation function (∑(wᵢⱼxᵢⱼ) + a) is bottleneck where categorical priming, inter-neuronal attention, and phasing combine. This determines which tokens achieve high activation and thus which categorical intersections form.

- Design tradeoffs: Framework explains polysemy reduction at high activations but doesn't address why lower-activation tokens remain polysemantic - whether this is feature (flexibility) or bug (interference). Non-orthogonal basis complicates geometric interventions compared to orthogonal decomposition methods.

- Failure signatures:
  - Activation-proximity correlation breaks down in deeper layers (layers > 10)
  - Taken-clusters become too sparse (< 3 clusters per neuron) for statistical analysis
  - PCA shows no dominant convergence factor (Factor 1 < 50% variance)
  - Sub-dimensions become linearly dependent (rank deficiency detected)

- First 3 experiments:
  1. **Replication across layers**: Test activation-dimensional proximity correlation in layers 2-5 of GPT2-XL to assess whether 78% positive difference holds beyond early MLP layers.
  2. **Cross-architecture validation**: Apply same methodology to Llama 2-7B or Mistral-7B to evaluate generalization beyond GPT2-XL.
  3. **Causal intervention test**: Clamp activations of high-proximity tokens and measure changes in categorical clustering coherence and downstream neuron activations to test causal attention hypothesis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the activation-dimension proximity relationship hold across deeper layers of transformer networks beyond the early MLP layers studied?
- Basis: Study only analyzed layers 0-2 of GPT2-XL; layer-by-layer categorical restructuring was not empirically validated across full network depth.
- Why unresolved: Methodological limitation - analyses confined to early perceptron-type layers where categorical sub-dimensions are hypothesized most extractable.
- What evidence would resolve it: Replication of dimensional proximity methodology across all MLP layers, assessing whether 78% positive correlation pattern persists, strengthens, or diminishes with depth.

### Open Question 2
- Question: Is the observed relationship between activation levels and categorical convergence causal, or merely correlational?
- Basis: Paper hypothesizes "activation acts as intra-neuronal attention mechanism guiding concept formation," treating high activation as functionally driving categorical convergence.
- Why unresolved: Only observational correlations measured; no causal interventions performed to test whether manipulating activation changes categorical proximity patterns.
- What evidence would resolve it: Interventional experiments where activation levels are artificially modulated and resulting changes in dimensional proximity scores measured.

### Open Question 3
- Question: Does the intra-neuronal categorical vector space framework generalize across different LLM architectures and scales?
- Basis: Study exclusively used GPT2-XL; architectural variations could alter how categorical sub-dimensions are extracted and structured.
- Why unresolved: No cross-model validation performed.
- What evidence would resolve it: Applying same taken-cluster and dimensional proximity analysis to neurons from diverse architectures (e.g., LLaMA, Mistral, decoder-only vs. encoder-decoder models).

### Open Question 4
- Question: Can the intra-neuronal categorical vector space model be formally integrated with existing inter-neuronal superposition theories, or do they make competing predictions about polysemy?
- Basis: Section 5.2 states: "Our perspective does not seek to contradict existing theories... but rather offers an alternative interpretative approach" - relationship between frameworks remains theoretically unelaborated and empirically untested.
- Why unresolved: No comparative analysis or unifying formal framework proposed.
- What evidence would resolve it: Formal analysis of whether both perspectives yield compatible predictions, or identification of testable scenarios where they diverge.

## Limitations
- Analysis restricted to early MLP layers (0-1) of GPT2-XL, with unclear generalizability to deeper layers or other architectures
- Findings leave 21.99% of neurons unexplained by the activation-dimensional proximity mechanism
- Vector space interpretation relies on assumption that non-orthogonal but linearly independent sub-dimensions maintain proper geometric properties

## Confidence
- **High confidence (80-95%)**: Statistical finding that high-activation tokens show higher dimensional proximity in early MLP layers (Kendall's tau 0.78-0.85, p < .0001)
- **Medium confidence (60-80%)**: Causal interpretation that activation drives categorical convergence through intra-neuronal attention
- **Low confidence (40-60%)**: Generalization to deeper layers and other architectures, and claim that this represents fundamental principle of concept formation

## Next Checks
1. **Layer-depth replication study**: Apply activation-dimensional proximity analysis to MLP layers 2-5 of GPT2-XL to determine if 78% positive correlation holds in deeper layers.
2. **Cross-architecture validation**: Implement identical methodology on Llama 2-7B or Mistral-7B to test whether activation-categorical convergence relationship is architecture-dependent.
3. **Causal intervention experiment**: Perform controlled activation clamping on high-proximity tokens and measure downstream effects on categorical clustering coherence and activation patterns in subsequent layer neurons.