---
ver: rpa2
title: Confidence Estimation for LLMs in Multi-turn Interactions
arxiv_id: '2601.02179'
source_url: https://arxiv.org/abs/2601.02179
tags:
- confidence
- answer
- guess
- information
- multi-turn
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study systematically examines confidence estimation in multi-turn
  dialogues, establishing a formal framework with two key desiderata: per-turn calibration
  and monotonicity of confidence as more information becomes available. To enable
  controlled evaluation, the authors introduce length-normalized Expected Calibration
  Error (InfoECE) and a novel "Hinter-Guesser" paradigm for generating datasets with
  progressive information revelation.'
---

# Confidence Estimation for LLMs in Multi-turn Interactions

## Quick Facts
- arXiv ID: 2601.02179
- Source URL: https://arxiv.org/abs/2601.02179
- Reference count: 33
- Primary result: P(SUFFICIENT) method shows best performance in maintaining confidence monotonicity and distinguishing meaningful information gains from conversational filler in multi-turn dialogues

## Executive Summary
This study systematically examines confidence estimation in multi-turn dialogues, establishing formal desiderata for per-turn calibration and monotonicity as information accumulates. The authors introduce a novel "Hinter-Guesser" paradigm for generating controlled datasets with progressive information revelation and length-normalized Expected Calibration Error (InfoECE) for fair evaluation across dialogue lengths. Across four models and four datasets, widely-used confidence techniques struggle with both calibration and monotonicity, while the proposed P(SUFFICIENT) method—a logit-based sufficiency probe—shows comparatively better performance, particularly in maintaining confidence monotonicity and distinguishing meaningful information gains from conversational filler.

## Method Summary
The study introduces a formal framework for confidence estimation in multi-turn dialogues with two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To enable controlled evaluation, the authors develop the "Hinter-Guesser" paradigm for generating datasets with progressive information revelation, where hints are revealed turn-by-turn until the answer is guessable. They introduce length-normalized Expected Calibration Error (InfoECE) to normalize turn positions to fractional "information levels" (0-1] for fair comparison across dialogues of different lengths. Five confidence estimation methods are evaluated: verbalized prompting (VANILLA-VERB, COT-VERB), self-consistency (SC), and two logit-based probes (P(TRUE), P(SUFFICIENT)). The P(SUFFICIENT) method asks whether accumulated information uniquely entails the proposed answer, using the softmax probability on "Yes" as the confidence score.

## Key Results
- P(SUFFICIENT) achieves Kendall's τ values of 71-84 across datasets, significantly outperforming verbalized methods and self-consistency in maintaining confidence monotonicity
- InfoECE values for P(SUFFICIENT) range from 6-35 across model-dataset combinations, showing better calibration than verbalized methods which often exceed 50
- Confidence signals behave very differently between multi-turn interactions and single-turn summaries, despite comparable accuracy levels, underscoring the importance of interactive dialogue structure
- Verbalized confidence methods show instability across conditions, with some configurations showing negligible or negative placebo effects, indicating they confuse turn count with evidence strength

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** P(SUFFICIENT) probes improve monotonicity by assessing evidence sufficiency rather than answer correctness.
- **Mechanism:** The method constrains output to a binary choice (Yes/No) asking whether accumulated information *uniquely entails* the proposed answer. The softmax probability on "Yes" becomes the confidence score. This allows models to express low confidence even when guesses are accidentally correct, if alternatives remain plausible.
- **Core assumption:** Models can distinguish "information sufficiency" from "current guess happens to match ground truth."
- **Evidence anchors:**
  - [Section 3.4]: "P(SUFFICIENT) works particularly well in our under-specified settings, where the set of plausible answers shrinks with each turn. This method allows the model to express low confidence even if its current guess happens to be correct, as long as other candidates have not yet been ruled out."
  - [Section 5.2]: P(SUFFICIENT) achieves τ=83.76 on GUESS with Qwen2.5-72B and τ=71.38 on TRICKME with Llama3.1-70B.
  - [Corpus]: Related work (arXiv:2601.08064) confirms calibration alone is insufficient; discrimination under variation matters—but corpus lacks direct validation of sufficiency probing specifically.
- **Break condition:** When dialogue turns contain adversarial or misleading information that appears sufficient but isn't.

### Mechanism 2
- **Claim:** Self-consistency (SC) degrades in under-specified settings because sampling variability doesn't track information accumulation.
- **Mechanism:** SC computes confidence as the fraction of m sampled answers matching a candidate. In under-specified early turns, multiple answers are plausibly correct, so sampled responses fragment across candidates, yielding unstable confidence regardless of information gain.
- **Core assumption:** High sample agreement indicates correctness; this assumes a unique correct answer exists and is identifiable.
- **Evidence anchors:**
  - [Section 5.2]: "SC often shows weak monotonicity in under-specified settings (even single digits on GUESS)."
  - [Table 2]: SC achieves τ=2.59 on GUESS with Llama3.1-70B despite 56.88% accuracy.
  - [Corpus]: Uncorpus papers directly address multi-turn SC failure modes.
- **Break condition:** When questions have multiple valid answers or when temperature settings skew sampling distribution.

### Mechanism 3
- **Claim:** Verbalized confidence conflates conversational length with evidence strength.
- **Mechanism:** Models asked to self-report confidence via prompting (VANILLA-VERB, COT-VERB) often increase confidence simply because turn count increases, not because information quality improves. This is revealed by placebo experiments where uninformative QA pairs are inserted.
- **Core assumption:** Models can introspect and verbalize epistemic states accurately.
- **Evidence anchors:**
  - [Section 5.3]: "Verbalized confidence is unstable across conditions... in some cases they even move counterintuitively (e.g., Qwen2.5-7B on GUESS decreases by −9.29 with an informative hint, p=0.005)."
  - [Table 3]: VANILLA-VERB shows negligible or negative placebo effects in some configurations but positive in others, indicating instability.
  - [Corpus]: Related work (arXiv:2505.23912) shows RL can improve verbalized confidence, but requires expensive training—suggesting base verbalization is unreliable.
- **Break condition:** When prompt format or model scale changes significantly; verbalized confidence shows scaling-dependent format sensitivity.

## Foundational Learning

- **Concept: Expected Calibration Error (ECE)**
  - **Why needed here:** InfoECE extends ECE to multi-turn by normalizing turn positions to fractional "information levels" (0-1], enabling fair comparison across dialogues of different lengths.
  - **Quick check question:** If a 5-turn dialogue and a 10-turn dialogue both reach 50% information level, how does InfoECE ensure comparable calibration measurement?

- **Concept: Kendall's τ (rank correlation)**
  - **Why needed here:** Quantifies monotonicity by counting concordant vs. discordant confidence pairs across turns. A τ of 1 means confidence strictly increases; 0 means no trend.
  - **Quick check question:** If confidence sequence is [0.3, 0.5, 0.4, 0.6] across 4 turns, are there more concordant or discordant pairs?

- **Concept: Logit-based probing**
  - **Why needed here:** P(SUFFICIENT) and P(TRUE) both constrain output to binary choice and extract softmax probability from the "True/Yes" token. Understanding constrained generation is prerequisite.
  - **Quick check question:** Why use constrained decoding (A/B only) rather than free-form confidence verbalization?

## Architecture Onboarding

- **Component map:** Hinter-Guesser Pipeline -> Per-turn answer extraction -> Confidence estimation (parallel across methods) -> Metric computation -> Placebo comparison

- **Critical path:** Dialogue generation → Per-turn answer extraction → Confidence estimation (parallel across methods) → Metric computation → Placebo comparison

- **Design tradeoffs:**
  - **P(SUFFICIENT) vs. P(TRUE):** Sufficiency better for under-specified settings where multiple answers remain plausible; P(TRUE) may overconfident when guess is accidentally correct.
  - **SC vs. verbalized:** SC more robust to turn-count artifacts but requires m× inference cost; verbalized is single-pass but unstable.
  - **Multi-turn vs. single-turn summary:** Accuracy comparable but confidence diverges—summarization removes turn-structure cues that sufficiency probes exploit.

- **Failure signatures:**
  - τ near 0 or negative: Confidence doesn't track information (common for SC in under-specified games).
  - Large placebo-confidence increase: Method confuses turn count with evidence (seen in P(TRUE) on GUESS).
  - InfoECE > 50: Severe miscalibration (typical for verbalized methods).

- **First 3 experiments:**
  1. **Replicate P(SUFFICIENT) vs. P(TRUE) on 20Q:** Confirm τ improvement (target: +30-50 points per Table 2).
  2. **Placebo ablation on GUESS:** Verify P(SUFFICIENT) decreases confidence on placebo (target: significant drop per Table 3, e.g., Llama3.1-70B: 14.27→2.97).
  3. **Scale sweep on InfoECE:** Test whether larger models improve calibration under summarization (per Appendix C: Llama3.1-70B GUESS InfoECE 34.70→6.16).

## Open Questions the Paper Calls Out
None

## Limitations
- Limited real-world dialogue diversity: The Hinter-Guesser pipeline may not capture the full complexity of naturally occurring multi-turn interactions where users ask clarifying questions, backtrack, or introduce ambiguity outside the game rules.
- Single-answer assumption in sufficiency probing: P(SUFFICIENT) assumes that sufficiency can be reduced to binary "Yes/No" answers, which may oversimplify confidence estimation in domains with inherently fuzzy or probabilistic answers.
- Evaluation metric assumptions: Both InfoECE and Kendall's τ assume that "more information should always increase confidence monotonically," but in some real-world scenarios, additional information might reveal uncertainty.

## Confidence
- **High confidence:** P(SUFFICIENT) outperforms other methods on monotonicity metrics (τ values 71-84 range). This is consistently demonstrated across multiple model-dataset combinations and supported by clear ablation experiments.
- **Medium confidence:** P(SUFFICIENT) maintains calibration while improving monotonicity. While the InfoECE improvements are substantial, the calibration gains are less dramatic than the monotonicity improvements.
- **Medium confidence:** Multi-turn confidence estimation differs fundamentally from single-turn summarization. The comparative analysis is robust, but the "fundamental difference" claim extrapolates from specific game-like datasets to broader conversational contexts.
- **Low confidence:** Verbalized confidence methods are universally unreliable. While evidence shows instability, there are instances where verbalized methods perform adequately, and the corpus indicates potential for improvement through specialized training.

## Next Checks
1. **Cross-domain robustness test:** Apply P(SUFFICIENT) to non-game datasets like conversational QA or customer service dialogues to verify that sufficiency-based probing maintains performance outside the controlled Hinter-Guesser environment.
2. **Real-time confidence dynamics:** Implement an online experiment where users can request confidence estimates mid-dialogue, then measure whether P(SUFFICIENT) confidence scores actually help users decide whether to continue asking questions or accept answers.
3. **Adversarial information injection:** Design test cases where later turns introduce misleading information that appears sufficient but leads to incorrect answers, then measure whether P(SUFFICIENT) appropriately decreases confidence when sufficiency is illusory.