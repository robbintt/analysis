---
ver: rpa2
title: Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?
arxiv_id: '2507.10174'
source_url: https://arxiv.org/abs/2507.10174
tags:
- transformer
- learning
- offline
- decision
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper challenges the perceived advantages of Decision Transformer
  (DT) for offline reinforcement learning in sparse-reward environments. The authors
  propose Filtered Behavior Cloning (FBC), a simple method that filters low-performing
  trajectories before applying behavior cloning.
---

# Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?

## Quick Facts
- arXiv ID: 2507.10174
- Source URL: https://arxiv.org/abs/2507.10174
- Authors: Yumi Omori; Zixuan Dong; Keith Ross
- Reference count: 2
- One-line primary result: FBC outperforms DT on 7/9 D4RL sparsified tasks and achieves 3.5% aggregate improvement on Robomimic

## Executive Summary
This paper challenges the perceived advantages of Decision Transformer (DT) for offline reinforcement learning in sparse-reward environments. The authors propose Filtered Behavior Cloning (FBC), which filters out low-performing trajectories before applying behavior cloning. Through extensive experiments on Robomimic and D4RL benchmarks, FBC achieves competitive or superior performance to DT while using fewer parameters, less training data, and shorter wall-clock time. The results suggest DT offers no advantage over FBC in sparse-reward settings, and based on prior work, likely not in dense-reward environments either.

## Method Summary
The authors propose Filtered Behavior Cloning (FBC) as an alternative to Decision Transformer for offline RL in sparse-reward settings. FBC works by filtering out low-performing trajectories from the dataset, then applying vanilla behavior cloning using a simple MLP policy network. In sparse settings, only successful trajectories are retained; in sparsified settings (dense rewards summed to end), the top 10% of trajectories by final return are kept. This transforms the sparse RL problem into a high-quality supervised learning problem. The method is compared against DT and a Filtered DT (FDT) variant on D4RL and Robomimic benchmarks.

## Key Results
- FBC outperforms DT on 7 out of 9 D4RL sparsified tasks with a 4% aggregate improvement
- FBC achieves a 3.5% aggregate improvement over DT on Robomimic benchmarks
- FBC uses approximately half the parameters of DT (MLP vs Transformer) and trains 3x faster
- On D4RL, FBC outperforms vanilla BC by significant margins (e.g., Lift: 0.46 vs FBC: 0.94)

## Why This Works (Mechanism)

### Mechanism 1: Data Quality Transformation
Filtering low-performing trajectories transforms a sparse-reward policy optimization problem into a high-quality supervised learning problem. In sparse reward settings, low-return trajectories provide no positive reinforcement signal and often introduce noise. By retaining only successful trajectories (top x% returns), FBC allows the standard supervised loss to focus exclusively on state-action pairs that led to success, effectively bypassing the credit assignment problem inherent in sparse RL.

### Mechanism 2: RTG as Implicit Filter
Return-to-Go (RTG) conditioning in Decision Transformer acts effectively as a data filter rather than a continuous value function approximator in sparse settings. The paper posits that in sparse rewards, specifying a high target return is semantically equivalent to selecting the "good" subset of the dataset. Since DT fails to outperform the explicit filtering of FBC, the Transformer architecture does not appear to extract additional value from the "bad" data beyond learning to ignore it.

### Mechanism 3: Architectural Simplicity
Reduced architectural complexity (MLP vs Transformer) yields better sample efficiency and faster convergence when sequence modeling is unnecessary. Transformers generally require more data to converge than MLPs due to larger parameter counts and weaker inductive biases for single-step mapping. By simplifying the architecture to an MLP and reducing the dataset to high-quality samples, FBC minimizes the variance in the optimization landscape.

## Foundational Learning

- **Concept: Behavior Cloning (BC)** - FBC is fundamentally BC applied to a cleaned dataset. Understanding that BC reduces RL to supervised learning (mapping s â†’ a) is required to grasp why filtering works.
  - Quick check: Can you explain why BC typically fails when the dataset contains a mix of expert and random data, and how FBC modifies this premise?

- **Concept: Offline Reinforcement Learning Constraints** - The paper addresses the "offline" constraint where the agent cannot interact with the environment to explore. This creates the "distributional shift" problem that CQL tries to solve, but FBC sidesteps via filtering.
  - Quick check: Why does collecting more data (e.g., adding failed trajectories) potentially harm performance in offline RL but usually help in online RL?

- **Concept: Sparsified vs. Sparse Rewards** - The paper distinguishes between binary success signals and "sparsified" dense rewards (summing rewards to the end). This distinction determines whether filtering is a binary mask or a percentile threshold.
  - Quick check: In the "sparsified" setting, if you filter for the top 10% of returns, are you guaranteed to have only successful trajectories? (Hint: No, you might just have the "least bad" failures).

## Architecture Onboarding

- **Component map**: Dataset -> Pre-processor (filter by success/return) -> Policy Network (MLP) -> Loss (MSE/CE)
- **Critical path**: The "Filtering" step is the only non-standard addition. If this step is implemented incorrectly (e.g., leaking future information or keeping low-return data), the method reduces to vanilla BC.
- **Design tradeoffs**:
  - Data Quantity vs. Quality: FBC aggressively discards data (often 90% in sparsified settings). If the dataset is small to begin with, this may cause underfitting.
  - Simplicity vs. Generality: FBC explicitly cannot perform trajectory "stitching" (combining parts of suboptimal trajectories). CQL or Q-Transformer would be required for such capabilities.
- **Failure signatures**:
  - Performance Collapse to BC levels: Indicates the filtering threshold is too loose or the reward logic is inverted.
  - Zero-shot Generalization Failure: If the filtered data is too homogeneous, the policy may fail if the initial state distribution varies slightly.
- **First 3 experiments**:
  1. Baseline Reconstruction: Re-run BC vs. FBC on the Robomimic "Lift" task. Verify that BC scores ~0.46 and FBC ~0.94.
  2. Threshold Sensitivity: On a sparsified D4RL task (e.g., HalfCheetah), sweep the filtering percentile (top 5%, 10%, 20%, 50%) to observe the trade-off between dataset size and data quality.
  3. Architecture Ablation: Compare FBC (MLP) vs. FDT (Transformer on filtered data) on the Walker task to verify if the Transformer provides any benefit when data quality is controlled.

## Open Questions the Paper Calls Out

- **Open Question 1**: Is Decision Transformer preferable to Filtered Behavior Cloning for dense-reward offline RL environments? The authors restricted their experiments to sparse and sparsified settings, leaving the FBC-vs-DT relationship in dense settings unknown.

- **Open Question 2**: Does the finding that FBC outperforms DT generalize to vision-based or non-Markovian tasks? The authors explicitly limit their conclusion to "raw-state robotic tasks" while acknowledging that specialized transformer variants succeed in vision domains.

- **Open Question 3**: Why does Filtered Decision Transformer (FDT) underperform FBC despite using the same filtered data? In Section 5.1, the authors observe that FDT performs worse than FBC and hypothesize "overfitting and poor credit assignment," but provide no verification.

## Limitations

- The evaluation relies entirely on Robomimic and D4RL benchmarks, which may not represent the full diversity of sparse-reward robotics tasks.
- Critical implementation details like state normalization procedures and exact reward processing for sparsification are not provided.
- The paper focuses exclusively on raw-state robotic manipulation tasks, limiting generalizability to other domains.

## Confidence

- **High confidence**: Claims that FBC outperforms vanilla BC on filtered data, and that FBC achieves competitive performance to DT with fewer parameters and faster training times.
- **Medium confidence**: Claims about DT's mechanism in sparse settings being equivalent to data filtering based on empirical results.
- **Low confidence**: Generalization claims to dense-reward environments based on prior work without direct verification.

## Next Checks

1. **Cross-domain validation**: Test FBC on a non-robotic sparse-reward benchmark (e.g., Atari games with sparse rewards) to verify if the performance gains extend beyond current evaluation domains.

2. **Dataset size sensitivity**: Systematically vary the dataset size and filtering threshold to identify the minimum dataset size where FBC still outperforms vanilla BC, and the point where aggressive filtering causes underfitting.

3. **Trajectory stitching capability**: Design an experiment where optimal behavior requires combining segments from multiple failing trajectories to test whether FBC's inability to "stitch" is a fundamental limitation in certain environments.