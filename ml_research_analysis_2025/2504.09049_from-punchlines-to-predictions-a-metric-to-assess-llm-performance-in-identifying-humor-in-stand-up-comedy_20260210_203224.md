---
ver: rpa2
title: 'From Punchlines to Predictions: A Metric to Assess LLM Performance in Identifying
  Humor in Stand-Up Comedy'
arxiv_id: '2504.09049'
source_url: https://arxiv.org/abs/2504.09049
tags:
- humor
- prompt
- metric
- comedy
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study introduces a novel humor detection metric to evaluate\
  \ Large Language Models (LLMs) on their ability to identify humorous quotes from\
  \ stand-up comedy transcripts. The metric employs three scoring modules\u2014fuzzy\
  \ string matching, sentence embeddings, and subspace similarity\u2014to assess the\
  \ similarity between model predictions and ground truth punchlines derived from\
  \ audience laughter."
---

# From Punchlines to Predictions: A Metric to Assess LLM Performance in Identifying Humor in Stand-Up Comedy

## Quick Facts
- arXiv ID: 2504.09049
- Source URL: https://arxiv.org/abs/2504.09049
- Reference count: 6
- Models scored 51% vs humans at 41% on humor detection task

## Executive Summary
This study introduces a novel metric to evaluate Large Language Models on their ability to identify humorous quotes from stand-up comedy transcripts. The metric uses three scoring modules - fuzzy string matching, sentence embeddings, and subspace similarity - to assess similarity between model predictions and ground truth punchlines derived from audience laughter. Experiments across multiple LLMs reveal that while these models achieve a maximum score of 51%, they still outperform human evaluators who scored 41%, highlighting the subjective nature of humor detection.

## Method Summary
The authors develop a metric that evaluates LLMs on humor detection by extracting funny quotes from stand-up comedy transcripts. Ground truth punchlines are derived from audience laughter timestamps using a laughter detection model and forced alignment. Three scoring modules provide different perspectives on similarity: fuzzy string matching (Levenshtein distance), sentence embeddings (semantic vector comparison), and subspace similarity (PCA-derived canonical angles). A penalty mechanism prevents over-generation of quotes, and the final score is the average best-match across transcripts.

## Key Results
- LLMs achieved maximum score of 51% on humor detection task
- Human evaluators scored 41% on the same task
- ChatGPT showed highest score (51%) but lowest human agreement (28.7%)
- Models that output explanations instead of quotes performed better with embedding module than fuzzy matching

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular scoring accommodates the inherent subjectivity of humor detection by offering evaluators flexibility in how strictly to penalize model outputs.
- Mechanism: Three distinct similarity computations—fuzzy string matching (Levenshtein distance for lexical precision), sentence embeddings (semantic vector comparison via Sentence Transformers), and subspace similarity (PCA-derived canonical angles across multiple prompt variations)—produce independent scores that can be selected based on task requirements.
- Core assumption: Humor identification lacks a single "correct" answer; therefore, multiple similarity lenses provide fairer assessment than binary exact-match evaluation.

### Mechanism 2
- Claim: Ground truth derived from audience laughter provides an objective behavioral signal that reduces annotator subjectivity in identifying punchlines.
- Mechanism: Laughter detection model extracts timestamps from audio; forced alignment maps transcript sentences to audio; the sentence immediately preceding laughter onset is labeled as the humorous quote (G = {g₁, ..., gₖ}).
- Core assumption: The sentence directly before audience laughter is the comedic stimulus; laughter delay is negligible or correctable.

### Mechanism 3
- Claim: Over-generation penalty prevents models from gaming the metric by outputting excessive quotes to maximize chance matches.
- Mechanism: Penalty term p = max(n - k, 0) where n = number of model predictions and k = number of ground truth quotes; final score = max[(1/k)Σⱼtⱼ - αp, 0] with scaling factor α = 0.1.
- Core assumption: Models will not strategically under-generate; the penalty asymmetrically addresses only over-generation.

## Foundational Learning

- **Concept: Forced Alignment (Audio-Text Synchronization)**
  - Why needed here: Understanding how audio laughter timestamps map to transcript sentences is essential for grasping ground truth construction.
  - Quick check question: If laughter begins 0.3 seconds into a pause after a sentence ends, which sentence should be labeled as the humorous quote?

- **Concept: Levenshtein Distance / Edit Distance**
  - Why needed here: The fuzzy string matching module relies on this metric; understanding it clarifies why similar but non-identical quotes receive partial credit.
  - Quick check question: What is the Levenshtein distance between "punchline" and "punch line"?

- **Concept: Canonical Angles Between Subspaces**
  - Why needed here: The subspace similarity module uses SVD-derived canonical angles (cosines stored in Σ) to compare structural alignment between prediction and ground truth subspaces.
  - Quick check question: If all canonical angles are 0°, what is the subspace similarity score?

## Architecture Onboarding

- **Component map:**
  Input Layer: Stand-up transcript text + audio file
  Ground Truth Pipeline: Laughter detection model → timestamps → forced alignment → sentence-quote mapping (G)
  LLM Prompting Layer: Transcript + instruction prompt → model generates predicted quotes (M)
  Scoring Layer: Three parallel modules (fuzzy, embedding, subspace) → similarity matrix → max selection → penalty adjustment → final score
  Evaluation Layer: Score aggregation across transcripts; human agreement comparison

- **Critical path:**
  1. Audio quality directly impacts laughter detection accuracy (minimum 0.2s laughter length, 0.5 probability threshold)
  2. Forced alignment precision determines correct sentence attribution
  3. Prompt design affects model compliance (some models output explanations instead of quotes)
  4. Module selection determines evaluation strictness

- **Design tradeoffs:**
  - Fuzzy matching: Fast, interpretable, but penalizes paraphrasing; best for strict quote extraction evaluation
  - Embeddings: Captures semantics, handles explanatory outputs, but may give credit for conceptually similar non-humorous text
  - Subspace: Holistic model capability assessment across prompt variations, but requires multiple inference runs and opaque scoring
  - Penalty factor α = 0.1: Authors' choice; not empirically tuned in this paper

- **Failure signatures:**
  - Model outputs explanations instead of quotes (noted with Gemma 27b) → embedding module more appropriate than fuzzy
  - ChatGPT shows 48.9% fuzzy vs. 25.4% embedding → high lexical match but low semantic understanding
  - Human score 41% vs. LLM 51% does NOT imply LLM humor superiority; indicates text-based task alignment favoring LLM training
  - Agreement rate ≠ metric score (ChatGPT: highest score, lowest human agreement at 28.7%)

- **First 3 experiments:**
  1. Reproduce fuzzy matching scores on 5 transcripts using provided code (https://github.com/swaggirl9000/humor) with Gemma 2b-instruct; verify score calculation matches paper's methodology
  2. Ablate the penalty term (set α = 0) and measure score inflation; identify which models over-generate when unconstrained
  3. Test embedding module with models that produced explanatory outputs; compare semantic credit assignment vs. fuzzy matching punitive scores

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LLM humor detection performance change when models process multimodal inputs (video/audio) compared to text-only transcripts?
- Basis in paper: [explicit] The authors state in the Conclusion, "In the future, we aim to apply the metric to evaluate a model’s predicted quotes in a format distinct from text... By exploring live comedy performances, we hope to deploy our metric for humor detection on stand-up comedy videos."
- Why unresolved: The current study is strictly text-based, and the authors have not yet tested the metric against vision/audio-capable models or multimodal datasets.
- What evidence would resolve it: Empirical results comparing text-only scores against scores derived from models processing video/audio data on the same comedy set.

### Open Question 2
- Question: How does the exclusion of auditory laughter cues impact human agreement scores and accuracy compared to LLMs?
- Basis in paper: [explicit] The Conclusion notes, "this approach raises questions about the perception of humor among humans when they view stand-up without background laughter," and hypothesizes differing outputs.
- Why unresolved: The current human evaluation used transcripts; the specific effect of audio cues (or their absence) on the human vs. machine performance gap remains a hypothesis.
- What evidence would resolve it: A study measuring human detection performance on muted videos versus videos with laughter, compared directly to LLM text-based results.

### Open Question 3
- Question: Does narrowing human instructions to focus on textual linguistic mechanisms rather than general context improve human performance?
- Basis in paper: [explicit] Section 4.4 states, "We hypothesize that the scores for humans may differ if the evaluators were tasked with focusing on textual properties rather than general context."
- Why unresolved: It is unclear if the human performance gap (41% vs 51%) is due to the lack of delivery cues or a fundamental difference in how humans read humor versus how LLMs process text.
- What evidence would resolve it: A comparative evaluation where one group of human annotators is instructed to focus on "linguistic mechanisms" (like incongruity) while a control group is not.

## Limitations
- Ground truth validity depends on the assumption that sentences preceding laughter are the humorous content, which may not account for delivery, context, or visual elements
- No ablation studies demonstrate whether all three scoring modules independently capture humor perception
- Penalty factor α=0.1 and laughter detection thresholds were not empirically tuned for this specific task

## Confidence
- High confidence: The modular scoring framework design and its theoretical justification for accommodating humor subjectivity
- Medium confidence: The overall experimental methodology and score comparisons across models
- Low confidence: The behavioral claim that higher scores indicate better humor understanding (51% vs 41% does not imply LLM superiority)

## Next Checks
1. Manually validate 10 transcripts to confirm that laughter-triggered sentences align with human-identified punchlines, checking for systematic mismatches
2. Remove each scoring module individually and rerun on all models to identify which module drives score differences
3. Conduct controlled human evaluation on a subset of transcripts with detailed annotation guidelines to isolate whether low human scores reflect task difficulty or evaluation inconsistency