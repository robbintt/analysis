---
ver: rpa2
title: Chain-of-Thought Tokens are Computer Program Variables
arxiv_id: '2505.04955'
source_url: https://arxiv.org/abs/2505.04955
tags:
- tokens
- latent
- digit
- problems
- intermediate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper empirically studies the role of Chain-of-Thought (CoT)\
  \ tokens in large language models by treating them as computer program variables\
  \ that store intermediate values. The authors examine two compositional tasks\u2014\
  multi-digit multiplication and dynamic programming\u2014and demonstrate that CoT\
  \ is essential for solving these problems."
---

# Chain-of-Thought Tokens are Computer Program Variables

## Quick Facts
- **arXiv ID**: 2505.04955
- **Source URL**: https://arxiv.org/abs/2505.04955
- **Reference count**: 40
- **Primary result**: CoT tokens function as mutable program variables storing intermediate computational values, and preserving only these tokens achieves comparable performance to full CoT.

## Executive Summary
This paper investigates Chain-of-Thought (CoT) tokens in large language models by treating them as computer program variables that store intermediate values. Through experiments on multi-digit multiplication and dynamic programming tasks, the authors demonstrate that CoT is essential for solving compositional problems. They show that preserving only tokens storing intermediate results achieves comparable performance to full CoT, and that these values can be represented in alternative latent forms without harming model performance. Through intervention experiments where CoT values are randomly replaced, they confirm that subsequent reasoning steps and final answers change correspondingly, validating the variable-like behavior of CoT tokens.

## Method Summary
The authors study two compositional tasks: multi-digit multiplication (1×1 to 5×5 digit pairs) and dynamic programming (Maximum Path Sum in a Grid). They train Qwen-2.5-1.5B models on synthetic datasets (100k entries per problem scale) using full-parameter supervised fine-tuning. Three conditions are compared: direct prompting (no CoT), full CoT, and compressed CoT (removing non-numeric/non-symbol tokens). Latent token experiments use linear projections to encode intermediate values as special tokens. Intervention experiments involve replacing intermediate values in generated CoT and checking if subsequent tokens and final answers update accordingly. Performance is measured by exact match accuracy of final answers against ground truth.

## Key Results
- Preserving only tokens that store intermediate results achieves comparable performance to full CoT on both multiplication and DP tasks
- Randomly intervening intermediate values in CoT causes corresponding changes in subsequent tokens and final answers
- Intermediate values can be represented in latent embeddings without performance loss, but compression fails when computational complexity exceeds model capacity
- Models develop shortcuts on easy subproblems, accounting for ~49% of intervention failures

## Why This Works (Mechanism)

### Mechanism 1: Intermediate Value Storage
CoT tokens primarily function as storage for intermediate computational results rather than semantic reasoning traces. During CoT generation, the model computes and writes intermediate values to specific token positions, which are then attended to by subsequent generations for further computation, analogous to variable assignment and read operations in programs.

### Mechanism 2: Causal Value Propagation
Intermediate values stored in CoT tokens causally determine subsequent reasoning steps and final outputs. The model attends to intervened values and incorporates them into downstream computations, treating new values as ground truth inputs for subsequent operations.

### Mechanism 3: Latent Compression with Complexity Bounds
Intermediate values can be encoded in latent vectors rather than explicit tokens, but performance degrades when computational complexity between tokens exceeds model capacity. The model learns to encode multi-digit numbers into single latent vectors, but fails when merged tokens require computing across multiple intermediate states.

## Foundational Learning

- **Concept**: Autoregressive attention and KV-cache
  - Why needed: The variable-like behavior depends on later tokens attending to earlier token positions
  - Quick check: Can you explain why an intervention at position t would affect generation at position t+k but not positions before t?

- **Concept**: Linear probing of hidden states
  - Why needed: Section 4.2 uses linear probes to detect when intermediate values are no longer recoverable from hidden states
  - Quick check: If a linear probe on layer 20 achieves 90% accuracy but layer 10 achieves 20%, what does this suggest about when the model "computes" the value?

- **Concept**: Compositional generalization in Transformers
  - Why needed: The paper notes models "struggle to generalize on compositional problems" and develop shortcuts
  - Quick check: Why might a model trained on 3×3 multiplication fail on 5×5 multiplication even with CoT, if it learned the algorithm correctly?

## Architecture Onboarding

- **Component map**: Input -> Linear projection Pin -> Latent embedding -> Linear projection Pout -> Embedding space -> Standard Transformer processing

- **Critical path**: 1) Identify which tokens store intermediate values, 2) For latent experiments: design one-hot encoding scheme, 3) Train with combined loss, 4) For interventions: truncate after intervened position, regenerate, compare

- **Design tradeoffs**: Latent dimension d (higher supports larger ranges but increases parameters); compression aggression (reduces sequence length but increases per-token computation); model scale (all experiments on 1.5B model)

- **Failure signatures**: Shortcut errors (~49% of failures when model ignores intervened value for ×1 or ×0 cases); probe accuracy collapse when complexity exceeds threshold; late-layer recovery (accuracy rises only at layers 18-20+)

- **First 3 experiments**: 1) Replicate non-result token removal experiment on your task, 2) Run intervention analysis to check causal propagation, 3) Test compression limits by progressively merging latent tokens

## Open Questions the Paper Calls Out

- **Open Question 1**: How can intermediate variable tokens be systematically identified in general natural language reasoning tasks? The paper notes this is difficult in tasks where plain text elicits commonsense knowledge.

- **Open Question 2**: What is the specific computational complexity limit between CoT tokens for a given LLM architecture? The authors state it's difficult to estimate this exact limit.

- **Open Question 3**: How can models be prevented from learning "shortcuts" that bypass the intended variable logic in CoT? The paper observes shortcut errors account for ~49% of intervention failures.

- **Open Question 4**: Do the findings regarding CoT tokens as mutable variables generalize to larger models and non-synthetic datasets? The paper's experiments were restricted to Qwen-2.5-1.5B and synthetic tasks.

## Limitations

- Experiments were conducted on a single model architecture (Qwen-2.5-1.5B), limiting generalizability across different model scales
- Tasks studied (multiplication and DP) are highly structured and algorithmic, not representing open-ended reasoning tasks
- Intervention experiments show 26% failure rate, with shortcut errors suggesting the variable storage hypothesis is incomplete
- Latent compression experiments show performance degradation at specific thresholds, but generalizability to other architectures is unclear

## Confidence

- **High Confidence**: CoT tokens function analogously to program variables through causal value propagation (directly supported by intervention experiments)
- **Medium Confidence**: Intermediate value storage is the primary function of CoT tokens (supported by compressed CoT experiments but complicated by shortcut errors)
- **Low Confidence**: The computational complexity bound for latent compression (demonstrated for specific model and tasks but generalizability unclear)

## Next Checks

1. **Cross-model validation**: Replicate intervention experiments on larger models (7B, 70B) to test whether computational complexity bounds scale with model capacity

2. **Semantic reasoning tasks**: Test the variable hypothesis on tasks requiring logical inference chains where semantic connector tokens may play a more critical role

3. **Alternative attention mechanisms**: Compare variable behavior under different attention variants (linear attention, local attention) to determine if causal propagation is specific to standard softmax attention