---
ver: rpa2
title: Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models
arxiv_id: '2508.09719'
source_url: https://arxiv.org/abs/2508.09719
tags:
- concepts
- concept
- ards
- sofa
- context-aware
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of accurately diagnosing Acute
  Respiratory Distress Syndrome (ARDS) from electronic health records, which often
  lack complete labels and critical contextual information. To improve interpretability
  and reduce reliance on spurious shortcuts, the authors propose a context-aware Concept
  Bottleneck Model (CBM) that integrates structured EHR data with LLM-derived concepts
  extracted from unstructured clinical notes.
---

# Improving ARDS Diagnosis Through Context-Aware Concept Bottleneck Models

## Quick Facts
- **arXiv ID**: 2508.09719
- **Source URL**: https://arxiv.org/abs/2508.09719
- **Reference count**: 40
- **Primary result**: 8–10% performance gain over traditional models using LLM-derived concepts from clinical notes

## Executive Summary
This paper addresses the challenge of accurately diagnosing Acute Respiratory Distress Syndrome (ARDS) from electronic health records, which often lack complete labels and critical contextual information. The authors propose a context-aware Concept Bottleneck Model (CBM) that integrates structured EHR data with LLM-derived concepts extracted from unstructured clinical notes. By leveraging concepts like pneumonia, bilateral infiltrates, and cardiac failure from discharge summaries and radiology reports, the model achieves improved interpretability and reduces reliance on spurious shortcuts. The approach demonstrates an 8–10% performance gain over traditional models and baselines, with enhanced accuracy, precision, recall, and F1-score.

## Method Summary
The method employs a joint CBM architecture where a network predicts intermediate concepts from structured EHR features, and another network predicts the ARDS label from concatenated predicted concepts and LLM-extracted contextual concepts. The model is trained using a weighted sum of label and concept losses, with Llama-3 extracting 8 binary concepts from clinical text. Test-time interventions on misclassified concepts further improve performance by 12–20%. The approach addresses concept leakage by using LLM concepts that originate from distributions less conditionally dependent on labels than structured EHR variables.

## Key Results
- 8–10% performance gain over traditional models and baselines
- Improved accuracy, precision, recall, and F1-score
- Reduced concept leakage and higher mutual information between concepts and labels
- 12–20% performance improvement through interventions on misclassified concepts

## Why This Works (Mechanism)

### Mechanism 1: Distributional Regularization via Independent Text Sources
LLM-derived concepts from clinical notes reduce concept leakage because they originate from a distribution less conditionally dependent on labels than structured EHR variables. Clinical notes are authored independently of retrospective labeling processes, acting as a form of distributional regularization that encourages the model to learn generalizable features rather than dataset-specific shortcuts.

### Mechanism 2: Concept Space Completion Reduces Shortcut Reliance
Augmenting structured concepts with LLM-extracted contextual concepts improves the completeness of the intermediate representation space, reducing the model's reliance on spurious shortcuts. Adding clinically relevant concepts like "bilateral infiltrates," "pneumonia," and "cardiac failure" provides the model with information that was previously missing, forcing it to use meaningful features rather than shortcuts.

### Mechanism 3: Concept-Level Interventions Enable Test-Time Correction
The CBM architecture supports human-in-the-loop intervention at the concept level, allowing clinicians to correct mispredicted concepts and recover misclassified cases. Because predictions flow through an explicit intermediate concept layer, practitioners can identify erroneous concept predictions and override them with ground-truth, mean, or median values.

## Foundational Learning

- **Concept: Concept Bottleneck Models (CBMs)**
  - **Why needed here:** The entire architecture builds on CBMs, which decompose prediction into two stages: input→concepts and concepts→label.
  - **Quick check question:** Can you explain why a CBM's performance might be lower than an end-to-end model, and under what conditions intervention could close that gap?

- **Concept: Concept Leakage (Information Leakage)**
  - **Why needed here:** The paper's primary motivation is mitigating leakage, where intermediate concepts inadvertently encode label information.
  - **Quick check question:** If a CBM achieves 95% accuracy but its concept predictions show near-zero mutual information with the label, what does this suggest about leakage?

- **Concept: Multi-Modal Data Integration in Healthcare**
  - **Why needed here:** The method combines structured tabular EHR data with unstructured clinical text.
  - **Quick check question:** Why might discharge summaries provide different diagnostic signals than structured SOFA scores for ARDS identification?

## Architecture Onboarding

- **Component map:** Structured EHR features → Vanilla Concept Predictor → LLM Concept Extractor → Concatenated concepts → Label Predictor → ARDS prediction
- **Critical path:** Training: Extract LLM concepts offline → Train joint CBM with loss L_Y(f(g(x), c_text); y) + λΣL_Cj(g(x); c_j). Inference: Forward pass through concept predictors → concatenate → label prediction → optional intervention on erroneous concepts → updated prediction.
- **Design tradeoffs:** Llama-3 (7B) balances capability and compute; minimal prompts isolate concept contribution but may sacrifice extraction quality; median-based interventions outperform mean-based.
- **Failure signatures:** LLM hallucination introducing false positives/negatives; missing text data degrading performance; distribution shift in deployment data.
- **First 3 experiments:** 1) Baseline replication with vanilla CBM on structured features only. 2) Ablation study removing LLM concepts one at a time. 3) Intervention stress test with ground-truth, mean, and median interventions using independent and correlated strategies.

## Open Questions the Paper Calls Out

### Open Question 1
Can the context-aware CBM maintain diagnostic performance when restricted to time-limited data to reflect real-time clinical constraints? The current retrospective approach relies on complete clinical trajectories; it is unclear if the model is robust when data is incomplete or truncated early in a stay.

### Open Question 2
To what extent do LLM hallucinations or noise degrade concept quality, and how can they be systematically mitigated without solely relying on manual intervention? While test-time interventions can correct errors, the paper does not propose a method to prevent or filter LLM extraction errors before they enter the bottleneck.

### Open Question 3
Do advanced multi-modal alignment techniques (beyond concatenation) further reduce concept leakage or improve generalizability? The current implementation concatenates LLM concepts with structured concepts; it is unknown if interaction effects between modalities could be better captured to reduce shortcut learning.

## Limitations
- Independence assumption for LLM-derived concepts may fail if clinical notes systematically encode diagnostic labels
- 32.67% agreement between ClinicalBERT and Llama for pneumonia extraction raises concerns about concept reliability
- Reliance on complete clinical trajectories limits real-time applicability

## Confidence
- **High**: 8-10% performance gain on MIMIC-IV dataset with statistical significance
- **Medium**: Concept leakage reduction claims, as mechanism is theoretically sound but not empirically isolated
- **High**: Intervention effectiveness for tested scenarios, though real-world clinician accuracy remains unvalidated

## Next Checks
1. Test concept leakage independence assumption by comparing LLM concept distributions between ARDS+ and ARDS- patients
2. Evaluate intervention effectiveness with non-expert annotators to assess practical usability
3. Assess model performance on an external, geographically distinct ICU dataset to validate generalizability beyond MIMIC-IV