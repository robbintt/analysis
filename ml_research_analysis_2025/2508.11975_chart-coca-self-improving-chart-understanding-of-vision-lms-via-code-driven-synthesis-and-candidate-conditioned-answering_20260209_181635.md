---
ver: rpa2
title: 'Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven
  Synthesis and Candidate-Conditioned Answering'
arxiv_id: '2508.11975'
source_url: https://arxiv.org/abs/2508.11975
tags:
- chart
- data
- code
- reasoning
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving vision language
  models' (VLMs) understanding of charts, particularly for accurate description and
  complex reasoning. The core method involves a code-driven chart synthesis pipeline
  that generates aligned chart-question-answer triplets through code generation and
  execution, ensuring data reliability without human intervention.
---

# Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering

## Quick Facts
- arXiv ID: 2508.11975
- Source URL: https://arxiv.org/abs/2508.11975
- Reference count: 40
- One-line primary result: Achieves up to 15.50 points accuracy improvement on chart question answering via self-improvement without human-labeled data or external models.

## Executive Summary
Chart-CoCa addresses the challenge of improving vision language models' understanding of charts for accurate description and complex reasoning. The method employs a code-driven chart synthesis pipeline that generates aligned chart-question-answer triplets through code generation and execution, ensuring data reliability without human intervention. Additionally, a candidate-conditioned answering process is introduced, where the VLM generates multiple responses per query and synthesizes the final answer by contextualizing these candidates. Experiments demonstrate significant improvements over the initial VLM in a fully self-improving paradigm.

## Method Summary
The Chart-CoCa method combines code-driven synthesis and candidate-conditioned answering to create a self-improving system for chart understanding. First, a VLM describes real charts, generates plotting code, and executes it to produce synthetic charts with extracted metadata serving as ground truth. This code execution ensures data reliability by eliminating hallucinations common in direct text generation. Second, the method trains the VLM to output a final answer conditioned on multiple self-generated candidate responses, learning to discriminate correct answers from incorrect ones. The approach operates without human-labeled data or external models, using only the initial VLM and synthetic data generation.

## Key Results
- Achieves up to 15.50 points accuracy improvement over initial VLM on CharXiv validation set
- Successfully demonstrates self-improving capability without human-labeled data or external models
- Shows significant gains in both descriptive tasks (information extraction, enumeration, pattern recognition) and reasoning tasks (text/number analysis)

## Why This Works (Mechanism)

### Mechanism 1: Code as a Deterministic Grounding Intermediary
Generating executable code as an intermediate step reduces noise inherent in direct VLM text generation by anchoring data extraction in a deterministic execution environment. The ground-truth answers are extracted from executed code objects rather than the VLM's potentially hallucinated text description, ensuring mathematical alignment between questions and visualized charts.

### Mechanism 2: Candidate-Conditioned Discriminative Learning
Training a model to synthesize an answer from multiple self-generated candidates enables it to learn discrimination capabilities superior to simple majority voting. This forces the model to identify correctness among conflicting options rather than just generating one likely token sequence, leveraging the fact that correct answers often appear in the candidate distribution.

### Mechanism 3: Iterative Self-Verification via Test-Time Compute
Increasing the inference budget (number of candidates K) exposes the model's latent knowledge, as the probability of correct answers appearing in candidate lists increases significantly. The candidate-conditioned training acts as a mechanism to reliably retrieve this latent correct answer without requiring massive compute at test time.

## Foundational Learning

- **Concept: Test-Time Scaling (Pass@K)**
  - Why needed: Understanding that VLMs know more than they emit in a single pass is essential to grasp why generating candidates is useful
  - Quick check question: If a model has 40% accuracy on a single try, does adding more candidates help if the correct answer is never actually generated?

- **Concept: Synthetic Data Generation (SDG) Pipelines**
  - Why needed: The method avoids human labeling by building a pipeline (Description -> Code -> Execution); understanding data flow and "model collapse" risks is key
  - Quick check question: Why is extracting answers from *executed code objects* safer than extracting them from the VLM's *text description* of the chart?

- **Concept: Instruction Tuning / Supervised Fine-Tuning (SFT)**
  - Why needed: The final step involves fine-tuning the VLM on synthetic triplets; understanding how SFT shapes behavior to follow specific formats is crucial
  - Quick check question: How does the training objective change when the input includes "candidate answers" alongside the question?

## Architecture Onboarding

- **Component map:** Synthesizer (VLM) -> Execution Environment (Python) -> Matcher -> Candidate Generator (Initial VLM) -> Answer Model (Target VLM)
- **Critical path:** The Code Execution step serves as the "source of truth"; if code execution fails or extraction logic fails, the entire data generation pipeline produces garbage
- **Design tradeoffs:** 
  - Latency vs. Accuracy: Higher K improves accuracy but linearly increases inference latency
  - Complexity vs. Noise: Using code as intermediary adds significant architectural complexity but is necessary to eliminate hallucination noise
- **Failure signatures:** 
  - Code Loop: Getting stuck in "ValueError" or "IndexError" loops during synthesis
  - Majority Bias: Model reverts to majority voting rather than synthesizing correct answers
- **First 3 experiments:**
  1. Execution Integrity Check: Run synthesis pipeline on 100 charts and manually verify extracted metadata matches visual content
  2. Candidate Scaling (K-value): Train with K=1 vs K=5 vs K=10 to verify model learns to utilize extra context
  3. Ablation on Code: Compare training on "Code-Driven" data vs "Direct Description" data to isolate denoising impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does extending code-driven synthesis to non-Matplotlib libraries and more complex, flexible QA pairs affect model generalization?
- Basis: Current method relies solely on Python Matplotlib and focuses on basic chart elements
- Why unresolved: Implementation constrained to single plotting library and simple question templates
- What evidence would resolve it: Ablation studies showing performance gains with Seaborn, Plotly, or complex reasoning questions

### Open Question 2
- Question: Can iterative self-training loops using the model's own final predictions further enhance VLM performance?
- Basis: Conclusion proposes "Iterative Improvement" strategy reusing final-predicted data or well-trained model outputs
- Why unresolved: Current study presents single-pass improvement; stability and convergence of iterative loops untested
- What evidence would resolve it: Experiments demonstrating accuracy trends over multiple iterative training cycles

### Open Question 3
- Question: Can reinforcement learning (RL) or process reward models (PRMs) outperform supervised fine-tuning in candidate-conditioned answering?
- Basis: Authors identify future direction to explore RL and PRMs to guide reasoning process
- Why unresolved: Paper currently relies on supervised learning on synthetic data
- What evidence would resolve it: Comparative performance metrics between Chart-CoCa and RL/PRM-based optimization

## Limitations
- Code generation reliability issues with ~40% error rate during synthesis, primarily ValueErrors
- Candidate-conditioned answering effectiveness depends heavily on initial VLM's ability to generate correct candidates
- 15.50 point improvement measured only on CharXiv validation data, raising generalization concerns

## Confidence
- **High confidence**: Code execution as ground truth mechanism is well-supported by deterministic nature of code execution
- **Medium confidence**: Candidate-conditioned answering architecture's discrimination learning supported by case studies but lacks extensive ablation
- **Medium confidence**: Self-improving paradigm claim valid within experimental scope but limited by single-dataset validation

## Next Checks
1. **Code Execution Robustness**: Run synthesis pipeline on held-out test set and measure error distribution (ValueError, IndexError, etc.) to validate reproducibility against reported 40% error rate
2. **Candidate Quality Analysis**: Manually verify correct answer presence in top K candidates for subset of questions to test if model synthesizes or merely performs majority voting
3. **Cross-Benchmark Generalization**: Evaluate final model on different chart QA benchmark (e.g., ChartQA) to test if improvements transfer beyond CharXiv dataset