---
ver: rpa2
title: 'Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical
  Thinking Skill?'
arxiv_id: '2504.06514'
source_url: https://arxiv.org/abs/2504.06514
tags:
- reasoning
- questions
- question
- answer
- response
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reasoning models exhibit excessive token generation when faced
  with ill-posed questions missing critical premises, a phenomenon termed MiP-Overthinking.
  While these models can detect missing information early in their reasoning process,
  they continue generating lengthy, redundant reasoning chains rather than abstaining.
---

# Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?

## Quick Facts
- arXiv ID: 2504.06514
- Source URL: https://arxiv.org/abs/2504.06514
- Authors: Chenrui Fan; Ming Li; Lichao Sun; Tianyi Zhou
- Reference count: 39
- Reasoning models generate excessive tokens on ill-posed questions missing critical premises, failing to abstain despite early detection

## Executive Summary
Reasoning models exhibit excessive token generation when faced with ill-posed questions missing critical premises, a phenomenon termed MiP-Overthinking. While these models can detect missing information early in their reasoning process, they continue generating lengthy, redundant reasoning chains rather than abstaining. Non-reasoning models handle such questions more efficiently, producing shorter responses and correctly identifying unsolvable queries. This reveals a critical flaw in reasoning model training, where reinforcement learning and supervised fine-tuning encourage verbose thinking patterns without adequately promoting critical evaluation of question validity.

## Method Summary
The paper constructs four MiP datasets by removing critical premises from well-defined questions: MiP-Formula (50 synthetic formulas with undefined variables), MiP-SVAMP (300 body-question swaps), MiP-GSM8K (582 questions with one numerical condition removed), and MiP-MATH (58 questions with essential premise removed). The authors evaluate 7 reasoning models and 5 non-reasoning models on these datasets, measuring response length, abstention rates, and accuracy. They use GPT-4o as an LLM-as-judge to evaluate responses and detect suspicion of missing premises. A distillation experiment fine-tunes a non-reasoning model on just 50 verbose MiP responses from a reasoning model to test behavior transfer.

## Key Results
- Reasoning models detect missing premises early (In-Process Suspicion Rate 83-100%) but rarely abstain (3-16% on MiP-GSM8K)
- Reasoning models generate 10-80x more reasoning-related tokens for MiP vs. well-defined questions
- Non-reasoning models show shorter responses and higher abstention rates on MiP questions
- Distillation of just 50 verbose MiP responses transfers overthinking behavior to non-reasoning models

## Why This Works (Mechanism)

### Mechanism 1
Reasoning models detect missing premises early but fail to terminate, producing redundant tokens. Models trained with RL/SFT for thorough reasoning exhibit high "in-process suspicion rates" (83-100%) at early reasoning steps (First Suspicion Index 1.16-3.90), yet low final abstention rates (3-16% on MiP-GSM8K). The training incentivizes continued exploration over early exit. Abstention requires both detection capability AND reward signals that favor efficient termination.

### Mechanism 2
Overthinking behavior transfers through distillation from reasoning models to non-reasoning models. Fine-tuning Qwen-2.5-7B-Instruct on just 50 MiP responses from DeepSeek-R1 caused response length to increase from 262 to 2,447 tokens on MiP questions and 272 to 1,838 on well-defined questions, while abstain rate dropped from 34% to 5.2%. Verbose reasoning patterns are learned behaviors encoded in response demonstrations, not emergent from task difficulty.

### Mechanism 3
Missing premises trigger self-doubt loops characterized by repetitive reasoning patterns rather than critical evaluation. Token analysis reveals explosive growth in "alternatively," "wait," "check," "but," and hypothesis markers for MiP vs. well-defined questions. Step-level similarity heatmaps show higher intra-response redundancy (avg similarity 0.50 vs. 0.45). These patterns indicate models are "trapped" revisiting partial reasoning rather than committing to insolvability judgment.

## Foundational Learning

- **Test-time scaling law**
  - Why needed here: The paper argues MiP-Overthinking contradicts the assumption that more reasoning tokens improve outcomes; understanding this baseline clarifies why the observed behavior is anomalous.
  - Quick check question: Does generating more tokens on MiP questions improve abstention rates? (Answer: No, the paper shows inverse relationship.)

- **Reward hacking in RL**
  - Why needed here: The paper hypothesizes that rule-based RL rewards for format/accuracy without length penalties encourage excessive reasoning as a strategy to maximize reward.
  - Quick check question: What reward signal would discourage 3,000-token responses to "What is the value of a?"?

- **Distillation as behavioral transfer**
  - Why needed here: Understanding how reasoning patterns propagate from teacher to student models explains why both RL-based and SFT-based models exhibit the same failure mode.
  - Quick check question: If you filter distillation data for response length, should the student model inherit overthinking? (Answer: Evidence suggests no.)

## Architecture Onboarding

- **Component map:**
  MiP question → model response → token/step segmentation → GPT-4o evaluation → metrics aggregation (length, abstain rate, accuracy)

- **Critical path:**
  MiP question → model response → token/step segmentation → GPT-4o evaluation → metrics aggregation (length, abstain rate, accuracy)

- **Design tradeoffs:**
  Using GPT-4o as judge enables scalable evaluation but introduces model-dependent bias; authors mitigate with 3-run majority voting for suspicion detection. MiP-GSM8K/MATH require human verification to ensure unsolvability; synthetic MiP-Formula avoids this but may not reflect real-world ambiguity.

- **Failure signatures:**
  High In-Process Suspicion Rate + Low Abstain Rate = detection without commitment (key diagnostic for MiP-Overthinking). Response length ratio (MiP/Well-defined) > 2x for reasoning models, ~1x for non-reasoning models.

- **First 3 experiments:**
  1. Reproduce Figure 2 on a new dataset: Measure response length, abstain rate, and accuracy for reasoning vs. non-reasoning models on MiP-GSM8K variants.
  2. Replicate distillation transfer: Fine-tune a non-reasoning model on 50 verbose MiP responses; measure length/abstain changes before and after.
  3. Test early-exit intervention: Add explicit reward for "insufficient information" termination in RL training; compare MiP response lengths and abstention rates against baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Can explicit training interventions (e.g., length penalties, early-exit rewards) break the MiP-Overthinking pattern without degrading performance on well-defined problems? The authors state current training recipes "do not sufficiently reward critical thinking or early exit from unsolvable tasks" and call for approaches that "better balance thorough reasoning with critical thinking." This remains unresolved as no intervention experiments are conducted.

### Open Question 2
What specific mechanism causes reasoning models to continue generating tokens after detecting MiP, instead of terminating early? The paper identifies this detection-action gap empirically but does not investigate whether it stems from architectural constraints, token-level autoregressive patterns, or reward mis-specification during training.

### Open Question 3
Is MiP-Overthinking an inevitable consequence of training on reasoning-dense corpora, or can it be prevented through data curation? The paper demonstrates overthinking is "contagious through distillation" after fine-tuning on just 50 examples, and non-reasoning models are immune. However, it does not determine whether the issue originates from reasoning training data characteristics or the optimization process itself.

## Limitations
- MiP-Overthinking behavior depends heavily on the specific evaluation protocol and GPT-4o judge model, introducing potential bias
- The boundary between MiP-specific overthinking and general verbose reasoning remains somewhat fuzzy
- The paper demonstrates behavioral patterns but doesn't definitively establish whether this requires fundamental architectural changes or could be addressed through fine-tuning adjustments

## Confidence
- **High confidence**: The empirical observation that reasoning models generate significantly longer responses to MiP questions compared to non-reasoning models, and that this behavior transfers through distillation
- **Medium confidence**: The characterization of MiP-Overthinking as a distinct failure mode separate from general overthinking
- **Low confidence**: The claim that MiP-Overthinking represents a "critical flaw in reasoning model training" requiring fundamental architectural changes

## Next Checks
1. **Judge model ablation**: Repeat the core experiments using different LLM-as-judge configurations (GPT-4o, Claude-3, Llama-3-70B) to assess the stability of abstention and suspicion detection metrics across judge models
2. **Reward engineering intervention**: Implement an explicit "insufficient information" classification reward in RL training and measure its impact on MiP response lengths and abstention rates compared to the baseline reasoning models
3. **Cross-dataset generalization**: Test MiP-Overthinking behavior on naturally occurring ambiguous questions from real-world datasets to assess whether the phenomenon generalizes beyond controlled experimental conditions