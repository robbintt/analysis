---
ver: rpa2
title: 'RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks'
arxiv_id: '2511.22147'
source_url: https://arxiv.org/abs/2511.22147
tags:
- images
- image
- clean
- remedygs
- poisoned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes RemedyGS, the first effective black-box defense
  framework against computation cost attacks targeting 3D Gaussian splatting (3DGS)
  systems. The framework consists of a detector to identify poisoned input images
  and a purifier to recover benign images, with adversarial training incorporated
  to enhance recovery quality.
---

# RemedyGS: Defend 3D Gaussian Splatting against Computation Cost Attacks

## Quick Facts
- arXiv ID: 2511.22147
- Source URL: https://arxiv.org/abs/2511.22147
- Authors: Yanping Li; Zhening Liu; Zijian Li; Zehong Lin; Jun Zhang
- Reference count: 40
- Primary result: First effective black-box defense against computation cost attacks on 3DGS with up to 4 dB PSNR improvement

## Executive Summary
This paper introduces RemedyGS, a two-stage black-box defense framework that protects 3D Gaussian Splatting (3DGS) systems from computation cost attacks. The framework combines a CNN-based detector to identify poisoned images and a purifier autoencoder to recover clean images, with adversarial training to enhance reconstruction quality. Experiments demonstrate that RemedyGS effectively defends against white-box, black-box, and adaptive attacks while maintaining high reconstruction utility, achieving significant improvements in PSNR compared to baseline defenses.

## Method Summary
RemedyGS defends 3DGS against Poison-splat attacks through a two-stage pipeline: first detecting poisoned images using a 4-layer CNN classifier trained on texture artifacts, then purifying detected images with a U-Net-like autoencoder that includes adversarial training. The purifier learns to recover clean images by minimizing MSE loss while a discriminator ensures the output distribution matches natural images. The framework is trained on DL3DV dataset with 1M clean/poisoned pairs and evaluated across multiple benchmarks, showing effective defense against various attack types while preserving reconstruction quality.

## Key Results
- Effective defense against white-box, black-box, and adaptive attacks
- Up to 4 dB improvement in PSNR compared to baseline defenses
- Maintains high reconstruction utility while protecting against DoS attacks
- Successfully safeguards 3DGS systems without compromising normal user experience

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A convolutional neural network can function as a high-accuracy pre-filter to identify poisoned images based on unnatural texture artifacts introduced by the attack's optimization process.
- Mechanism: The `Poison-splat` attack maximizes the Total Variation (TV) score of images to trigger densification in 3DGS. This process introduces "unnatural high-frequency noise and more pronounced edge structures" that a 4-layer CNN can learn to detect.
- Core assumption: The attack's optimization for high computational cost necessarily leaves consistent and learnable texture artifacts.
- Evidence anchors: [abstract] "...a detector to identify the attacked input images with poisoned textures..."; [section 4.1] "...maximizes the TV score of input images... This process introduces unnatural high-frequency noise..."
- Break condition: An adaptive attack that successfully optimizes to increase Gaussians without significantly altering texture spectrum in a way the detector has learned.

### Mechanism 2
- Claim: A learnable encoder-decoder network can recover the original clean image from a poisoned one by learning a complex non-linear inverse transformation.
- Mechanism: The purifier is implemented as an autoencoder with additive skip connections. The training objective minimizes MSE between purified and original clean images, derived from maximizing their mutual information.
- Core assumption: The mapping from poisoned image distribution back to clean distribution can be effectively learned by a CNN with L2-based loss.
- Evidence anchors: [abstract] "...a purifier to recover the benign images from their attacked counterparts..."; [section 4.2] "The training objective is formulated as: `L_pur = min_{\phi,\theta} ... ||V_cln - g_\theta(f_\phi(V_poi))||^2_2`."
- Break condition: Adversarial perturbations are so extreme that they corrupt low-level features irreversibly, or distribution shift is too complex for a single network to generalize.

### Mechanism 3
- Claim: Incorporating adversarial training with a discriminator network aligns the purified image distribution with the natural image distribution, mitigating over-smoothing from MSE loss.
- Mechanism: A discriminator is trained to distinguish between purified and clean images. The purifier is then trained adversarially to fool this discriminator, encouraging generation of realistic high-frequency details.
- Core assumption: Blurring artifact is a direct result of MSE loss, and GAN-based loss can successfully recover fine details without reintroducing adversarial noise.
- Evidence anchors: [abstract] "...adversarial training... to enforce distributional alignment between the recovered and original natural images..."; [section 4.3] "...recovered images often contain overly blurred regions... we incorporate adversarial training..."
- Break condition: Training instability (mode collapse) or adaptive attack that optimizes against artifacts of this adversarially-trained purifier.

## Foundational Learning

- **Concept: 3D Gaussian Splatting (3DGS) & Densification**
  - Why needed here: The entire defense is predicated on countering an attack that exploits 3DGS's adaptive density control
  - Quick check question: Why does increasing the total variance (TV) score of an image cause the 3DGS model to use more Gaussians?

- **Concept: Adversarial Training & GANs**
  - Why needed here: The core innovation for improving purifier's output quality is adversarial training
  - Quick check question: What does the discriminator try to classify, and what is the purifier's goal in relation to the discriminator?

- **Concept: Trade-offs in Defense: Safety vs. Utility**
  - Why needed here: The paper critiques naive baselines for sacrificing too much utility
  - Quick check question: Why is "image smoothing" considered a poor defense despite reducing computational cost?

## Architecture Onboarding

- **Component map:** Input Image -> Detector (4-layer CNN) -> Router -> (if poisoned) -> Purifier (U-Net with skip connections) -> 3DGS Training. During Purifier training: Purifier Output + Clean Image -> Discriminator (4-layer CNN) -> Loss -> Purifier Update.
- **Critical path:** Input Image -> Detector -> (if poisoned) -> Purifier -> 3DGS Training
- **Design tradeoffs:** Additive skip connections chosen over concatenation to prevent propagating attacker noise; adversarial training added to fight MSE blurring but introduces GAN training complexity and instability risk
- **Failure signatures:**
  - False Positive: Detector flags clean image -> Unnecessary purification -> Potential loss of detail
  - False Negative: Detector misses poisoned image -> No purification -> 3DGS suffers DoS attack
  - Purifier Failure: Purified image still too noisy (attack persists) or too blurry (reconstruction quality low)
- **First 3 experiments:**
  1. **Detector Ablation:** Train and evaluate detector in isolation. Measure accuracy, F1, and recall on validation set of clean/poisoned images to ensure low false negative rate
  2. **Purifier Baseline (MSE-only):** Train purifier using only MSE loss. Measure PSNR/SSIM and computational cost when purified images are used for 3DGS training
  3. **Full System Integration & Adversarial Training:** Integrate adversarial training loop. Compare reconstruction quality and computational cost against MSE-only baseline to quantify benefit of distributional alignment

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness depends critically on assumption that Poison-splat attack leaves consistent, learnable texture artifacts
- Adversarial training component introduces potential instability risks common to GAN training
- Dataset-specific nature raises questions about generalization to different domains or attack variants

## Confidence
- **Mechanism 1 (Detection):** Medium - Theoretical foundation sound but empirical validation against adaptive attacks is limited
- **Mechanism 2 (Purification):** Medium - Autoencoder architecture standard but effectiveness relies heavily on training data quality and attack strength assumptions
- **Mechanism 3 (Adversarial Training):** Low-Medium - Theoretical justification provided but GAN training instability is a known issue

## Next Checks
1. **Adaptive Attack Evaluation:** Design and test attack specifically crafted to defeat the detector by minimizing texture changes while maximizing computational cost, then measure how RemedyGS performance degrades

2. **Cross-Dataset Generalization:** Train RemedyGS on DL3DV and evaluate detection/purification performance on completely different datasets to quantify domain shift effects

3. **Adversarial Training Stability:** Monitor GAN training metrics (discriminator accuracy, generator loss patterns) across multiple training runs to quantify consistency and stability of purifier's performance under adversarial training regime