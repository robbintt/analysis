---
ver: rpa2
title: 'Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture
  for Scientific Discovery'
arxiv_id: '2511.18298'
source_url: https://arxiv.org/abs/2511.18298
tags:
- scientific
- knowledge
- agent
- agents
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BioSage is a compound AI architecture that integrates LLMs, RAG,
  and specialized agents to enable cross-disciplinary scientific knowledge discovery
  and synthesis across AI, data science, biomedical, and biosecurity domains. The
  system employs retrieval agents with query planning and response synthesis, cross-disciplinary
  translation agents, and reasoning agents to facilitate transparent, traceable, and
  user-centric scientific workflows.
---

# Cross-Disciplinary Knowledge Retrieval and Synthesis: A Compound AI Architecture for Scientific Discovery

## Quick Facts
- arXiv ID: 2511.18298
- Source URL: https://arxiv.org/abs/2511.18298
- Authors: Svitlana Volkova, Peter Bautista, Avinash Hiriyanna, Gabriel Ganberg, Isabel Erickson, Zachary Klinefelter, Nick Abele, Hsien-Te Kao, Grant Engberson
- Reference count: 16
- Key outcome: BioSage agents outperform vanilla and RAG-only approaches by 13%-21% on cross-disciplinary benchmarks using Llama 3.1 70B and GPT-4o

## Executive Summary
BioSage introduces a compound AI architecture that integrates large language models, retrieval-augmented generation, and specialized agents to enable cross-disciplinary scientific knowledge discovery across AI, data science, biomedical, and biosecurity domains. The system employs retrieval agents with query planning and response synthesis, cross-disciplinary translation agents, and reasoning agents to facilitate transparent, traceable, and user-centric scientific workflows. Evaluation demonstrates that BioSage agents achieve 13%-21% performance improvements over vanilla and RAG-only approaches on established benchmarks.

## Method Summary
BioSage implements a compound AI architecture using LlamaIndex for RAG with hybrid semantic search (term-based + embedding similarity) powered by sentence-transformers/all-MiniLM-L6-v2 embeddings. The system indexes 585,902 scientific papers from S2ORC and LitQA2 across six domains using OpenSearch as the vector database. Agent framework PydanticAI runs on AWS r5.2xlarge instances with base models including GPT-4o, Llama-3 70B, and Phi4 14B. The architecture features four configurations: Vanilla LLM, Vanilla RAG, Retrieval Agent v1 (query planning), and Retrieval Agent v2 (advanced query planning). Query planning and RAG templates are provided in Appendix A.1.

## Key Results
- BioSage agents outperform vanilla and RAG-only approaches by 13%-21% on cross-disciplinary benchmarks
- GPT-4o agent achieves 29.6% accuracy on LitQA2 (+46.5% over vanilla LLM)
- GPT-4o agent achieves 92.5% accuracy on WMDP (+34.1% over vanilla LLM)
- Agent v2 with advanced query planning shows 18.1% absolute improvement over agent v1 on GPQA (65.2% vs 46.7%)

## Why This Works (Mechanism)
BioSage's compound architecture enables effective cross-disciplinary knowledge synthesis through specialized retrieval agents that decompose complex queries into domain-specific sub-queries, retrieve relevant evidence from domain-specific RAG tools, and synthesize responses through structured reasoning. The system's strength lies in its ability to bridge knowledge gaps between disparate scientific domains while maintaining traceability through interpretable conversational interfaces.

## Foundational Learning
- **Compound AI Architecture**: Integration of multiple AI components (LLMs, RAG, agents) working together to solve complex tasks that individual components cannot handle alone
  - Why needed: Single AI models struggle with cross-disciplinary knowledge synthesis requiring domain expertise across multiple fields
  - Quick check: Verify agent performance gains over individual component baselines

- **Retrieval-augmented Generation (RAG)**: Technique combining information retrieval with text generation to provide LLMs with current, relevant context
  - Why needed: LLMs have limited knowledge cutoff dates and can hallucinate facts without external verification
  - Quick check: Compare RAG performance against vanilla LLM on knowledge-intensive tasks

- **Query Planning**: Process of decomposing complex queries into sub-queries and determining optimal retrieval strategies
  - Why needed: Cross-disciplinary queries require intelligent routing to appropriate domain-specific knowledge sources
  - Quick check: Analyze query planning outputs for logical decomposition and domain relevance

- **Response Synthesis**: Method of combining multiple retrieved evidence pieces into coherent, synthesized answers
  - Why needed: Individual evidence pieces may be incomplete; synthesis creates comprehensive answers
  - Quick check: Evaluate synthesized responses for coherence and completeness

- **Domain-Specific RAG Tools**: Specialized retrieval systems optimized for particular scientific domains
  - Why needed: Different domains require different vocabularies, methodologies, and knowledge representations
  - Quick check: Verify domain-specific retrieval accuracy for representative queries

- **Causal Analysis Metrics**: Evaluation methods measuring the impact of individual architectural components on overall performance
  - Why needed: Understanding which components drive performance improvements enables targeted optimization
  - Quick check: Perform ablation studies removing individual components to measure performance impact

## Architecture Onboarding

**Component Map**: User Query -> Query Planner -> Domain-Specific RAG Tools -> Evidence Retrieval -> Response Synthesizer -> Final Answer

**Critical Path**: User query → Query planning → Domain routing → Evidence retrieval → Response synthesis → User output

**Design Tradeoffs**:
- Retrieval accuracy vs. computational cost: Higher precision retrieval requires more sophisticated models and processing
- Query planning complexity vs. response time: More sophisticated planning improves accuracy but increases latency
- Domain coverage vs. specialization: Broader domain support may reduce domain-specific optimization effectiveness

**Failure Signatures**:
- Agent underperforms vanilla models on GPQA (46.7% vs 56.1% for v1): Indicates query planning may retrieve irrelevant domains
- Modest gains on HLE-Bio despite RAG: Suggests benchmark questions require reasoning beyond retrieved evidence
- Performance degradation with increasing domain complexity: May indicate limitations in cross-disciplinary translation capabilities

**First Experiments**:
1. Build RAG pipeline with estimated chunking (512 tokens, 50 overlap) and verify retrieval accuracy on LitQA2 using provided templates
2. Implement agent v2 with advanced query planning and test against v1 on GPQA to confirm 18.1% improvement
3. Conduct ablation studies removing individual components to verify 13%-21% performance improvements

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gains rely on proprietary benchmarks and synthetic datasets not publicly available
- Key architectural decisions (domain taxonomy, chunking strategy, retrieval hyperparameters) are underspecified
- System generalization beyond four studied domains (AI, data science, biomedical, biosecurity) remains untested
- Agent v1 underperforms vanilla models on GPQA, suggesting domain-sensitive limitations

## Confidence
- **High confidence**: Retrieval agent architecture design and basic implementation using LlamaIndex and PydanticAI framework
- **Medium confidence**: Performance gains over baselines on specified benchmarks, though reproducibility depends on dataset access
- **Low confidence**: Claims about cross-disciplinary knowledge synthesis capability without evidence on truly novel interdisciplinary combinations

## Next Checks
1. Reconstruct document indexing pipeline with estimated chunking parameters (512 tokens, 50 overlap) and verify retrieval accuracy on LitQA2 using provided RAG templates and query planning prompts
2. Implement agent v2 with advanced query planning and test against agent v1 on GPQA to confirm the reported 18.1% absolute improvement (65.2% vs 46.7%)
3. Conduct ablation studies removing individual agent components (query planner, response synthesizer, evidence RAG) to verify the reported 13%-21% performance improvements attributed to the compound architecture