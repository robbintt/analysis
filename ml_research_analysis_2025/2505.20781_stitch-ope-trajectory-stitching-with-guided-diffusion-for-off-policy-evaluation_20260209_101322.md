---
ver: rpa2
title: 'STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation'
arxiv_id: '2505.20781'
source_url: https://arxiv.org/abs/2505.20781
tags:
- policy
- diffusion
- target
- behavior
- stitch-ope
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes STITCH-OPE, a diffusion-based model for long-horizon
  off-policy evaluation in high-dimensional continuous control settings. The method
  trains a conditional diffusion model on short sub-trajectories from offline data
  and generates target-policy rollouts by stitching together guided sub-trajectories.
---

# STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation

## Quick Facts
- **arXiv ID:** 2505.20781
- **Source URL:** https://arxiv.org/abs/2505.20781
- **Reference count:** 40
- **Primary result:** STITCH-OPE outperforms state-of-the-art OPE methods in mean squared error, rank correlation, and regret metrics on D4RL and Gym benchmarks.

## Executive Summary
STITCH-OPE introduces a diffusion-based model for long-horizon off-policy evaluation in high-dimensional continuous control settings. The method trains a conditional diffusion model on short sub-trajectories from offline data and generates target-policy rollouts by stitching together guided sub-trajectories. To address distribution shift, it uses negative guidance from the behavior policy during generation. Experiments show STITCH-OPE outperforms state-of-the-art OPE methods, and theoretical analysis provides bounds on bias and variance, showing exponential improvement over full-trajectory diffusion and importance sampling.

## Method Summary
STITCH-OPE trains a conditional diffusion model on short sub-trajectories (length w << T) from offline behavior data. During generation, the model produces long-horizon trajectories by stitching w-length sub-trajectories end-to-end, with each sub-trajectory conditioned on the last state of the previous one. The method applies guided diffusion using a guidance function that combines the target policy's score with negative behavior policy guidance to correct for distribution shift. A separate reward predictor estimates returns for the generated trajectories. The approach avoids compounding errors from autoregressive models while maintaining compositionality benefits.

## Key Results
- STITCH-OPE outperforms baseline OPE methods (FQE, TD, PGD) on D4RL and Gym benchmarks in LogRMSE, Spearman rank correlation, and regret metrics
- Theoretical analysis shows exponential reduction in MSE compared to importance sampling and full-trajectory diffusion methods
- The method effectively evaluates diffusion policies, demonstrating its versatility beyond standard RL policies

## Why This Works (Mechanism)

### Mechanism 1: Sub-Trajectory Stitching with Conditional Diffusion
Generating long-horizon trajectories by stitching short, conditionally generated sub-trajectories reduces compounding error compared to full-trajectory autoregressive models. The model trains on fixed-length sub-trajectories and conditions generation on the end state of the previous sub-trajectory, allowing end-to-end stitching while avoiding the compounding of errors seen in fully autoregressive dynamics models.

### Mechanism 2: Negative Behavior Policy Guidance
Subtracting the score function of the behavior policy during guided diffusion prevents over-regularization and corrects for distribution shift. The guidance function g(τ) = α∇τΣlog π(at|st) - λ∇τΣlog β(at|st) actively pushes generated trajectories away from high-density regions under the behavior policy that are unlikely under the target policy.

### Mechanism 3: Theoretical Bias-Variance Reduction
The sub-trajectory approach provides an exponential reduction in mean squared error compared to importance sampling and full-trajectory diffusion. The variance term scales with (T/w)² and κ^w rather than κ^T, where w is the fixed sub-trajectory length and T is the total horizon.

## Foundational Learning

- **Denoising Diffusion Probabilistic Models (DDPMs)**: The core generative engine. You must understand the forward (noising) and backward (denoising) processes, and how a neural network learns to predict noise at each step to reverse the diffusion.
  - Quick check: Can you explain how the model is trained to reverse a fixed Markov chain that adds Gaussian noise to data?

- **Guided Diffusion / Classifier Guidance**: The mechanism for steering the diffusion process. You need to understand how to modify the mean of the backward diffusion step using the gradient (score) of a guidance function, typically derived from a classifier or, in this case, a policy.
  - Quick check: If you have a pre-trained diffusion model p(x) and a classifier p(y|x), how do you sample from the conditional distribution p(x|y)?

- **Off-Policy Evaluation (OPE) & Importance Sampling (IS)**: The problem being solved. You must understand the distribution shift problem and why naive IS suffers from exponential variance with horizon length ("curse of horizon").
  - Quick check: Why does the variance of an Importance Sampling estimator grow exponentially with the trajectory length T?

## Architecture Onboarding

- **Component map**: 
  - Diffusion Model (ϵθ) -> Guidance Function (g(τ)) -> Stitching Controller -> Reward Predictor (R̂) -> Value Estimate (Ĵ)
  - Reward Predictor (R̂) -> Diffusion Model (ϵθ) (for training data)

- **Critical path**: The guided backward diffusion process (Algorithm 2, lines 5-11). This is where the pre-trained model, the state conditioning, and the guidance function intersect. A bug here will result in infeasible or low-quality trajectories.

- **Design tradeoffs**:
  - **Window size (w)**: Smaller w improves compositionality but may increase error compounding. Larger w is more stable but less flexible. The paper finds w=8 for D4RL tasks.
  - **Guidance coefficients (α, λ)**: λ controls regularization. λ=0 is over-regularized; λ=1 can be unstable. Empirically, 0 < λ < α works best.
  - **vs. PGD**: STITCH-OPE trades off the simplicity of full-trajectory generation (PGD) for improved generalization and error handling via stitching and negative guidance.

- **Failure signatures**:
  - **Collapse to Behavior**: If λ is too low or omitted, generated trajectories will look identical to the average behavior policy, failing to estimate the target policy value.
  - **Infeasible Trajectories**: If λ is too high, the guidance may push the denoising process out of the support of the learned behavior model, producing physically impossible states/dynamics.
  - **High Variance**: If w is poorly tuned or the reward predictor is inaccurate, the final value estimate Ĵ will be noisy.

- **First 3 experiments**:
  1. Reproduce the "GaussianWorld" toy example (Table 1): Implement a simple 2D diffusion model and verify that the negative guidance term is necessary to correctly steer the agent in the opposite direction of the behavior policy.
  2. Hyperparameter sweep for w: On a single D4RL task (e.g., Hopper-medium), train the conditional diffusion model and evaluate OPE performance (LogRMSE) while varying w (e.g., 2, 4, 8, 16, 32). Confirm the optimal w balances error and compositionality.
  3. Ablation on guidance: Run the full OPE pipeline on a standard benchmark with and without the negative behavior guidance term (λ=0 vs. optimal λ). Plot the resulting trajectories and compare the estimated value error to demonstrate the effect of over-regularization.

## Open Questions the Paper Calls Out
- **Policy Optimization Extension**: Whether the advantages of STITCH-OPE apply to offline policy optimization, not just evaluation.
- **Adaptive Guidance Tuning**: How to adaptively tune or learn guidance coefficients α and λ rather than manually selecting them.
- **Domain Generalization**: Whether STITCH-OPE generalizes to domain-specific problems outside robotics, such as healthcare or recommender systems.
- **Trajectory Fidelity Validation**: How to validate trajectory fidelity when states are partially observable or difficult to interpret.

## Limitations
- Theoretical bounds rely on strong assumptions about bounded likelihood ratios and perfect sub-trajectory distribution modeling that may not hold in practice
- Lack of architectural details for the UNet and behavior policy parameters for D4RL datasets introduces significant reproducibility barriers
- The method has not been validated on non-robotics domains like healthcare or recommender systems

## Confidence
- **High Confidence**: The mechanism of sub-trajectory stitching and its implementation details are well-supported by the text and experimental results.
- **Medium Confidence**: The effectiveness of negative behavior policy guidance is demonstrated empirically, but the theoretical justification could be more rigorous.
- **Medium Confidence**: The claimed exponential improvement in MSE bounds is theoretically derived, but the assumptions are strong and not all are empirically validated.

## Next Checks
1. **Validate Assumption 3.2**: Conduct an empirical analysis to quantify the TV divergence between the learned sub-trajectory distribution and the true behavior distribution on D4RL datasets.
2. **Architecture Reproduction**: Attempt to reproduce the UNet architecture and behavior policy parameters for the D4RL datasets based on standard practices and the paper's description.
3. **Extreme Distribution Shift Test**: Evaluate STITCH-OPE on a dataset with known, severe distribution shift to test the robustness of the method to violations of Assumption 3.1.