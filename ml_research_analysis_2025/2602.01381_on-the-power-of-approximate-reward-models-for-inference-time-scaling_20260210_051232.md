---
ver: rpa2
title: On the Power of (Approximate) Reward Models for Inference-Time Scaling
arxiv_id: '2602.01381'
source_url: https://arxiv.org/abs/2602.01381
tags:
- reward
- proposal
- approximate
- inference-time
- theorem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a theoretical foundation for understanding why
  approximate reward models are effective in inference-time scaling. The key insight
  is that the Bellman error of the reward model is the critical factor governing performance.
---

# On the Power of (Approximate) Reward Models for Inference-Time Scaling

## Quick Facts
- arXiv ID: 2602.01381
- Source URL: https://arxiv.org/abs/2602.01381
- Reference count: 40
- This work provides theoretical foundation for why approximate reward models are effective in inference-time scaling

## Executive Summary
This paper establishes theoretical foundations for understanding why approximate reward models are effective in inference-time scaling for LLM reasoning. The key insight is that the Bellman error of the reward model is the critical factor governing performance. For a reasoning process of length T, if the Bellman error is bounded by O(1/T), then combining this reward model with Sequential Monte Carlo (SMC) reduces the computational complexity from exponential to polynomial in T, yielding exponential improvement in inference efficiency despite using only approximate rewards.

## Method Summary
The paper analyzes SMC-based inference-time scaling through a Feynman-Kac formalism, where the base policy (pretrained LLM) provides proposal distribution and the reward model guides the inference. The core mechanism involves computing potential functions G_t = V̂(s_0, s_{1:t})/V̂(s_0, s_{1:t-1}) that weight particle trajectories based on the reward model's value estimates. Two main approaches are analyzed: naive-proposal SMC that uses the base policy directly, and single-particle guided SMC with Metropolis-Hastings correction that achieves logarithmic dependence on accuracy. The analysis shows that when the Bellman error of the reward model scales as O(1/T), SMC can achieve polynomial complexity rather than exponential complexity.

## Key Results
- If Bellman error scales as O(1/T), SMC reduces computational complexity from exponential to polynomial in horizon T
- Information-theoretic lower bounds show that sub-exponential complexity requires a small Bellman error regime
- Metropolis-Hastings correction achieves arbitrary TV accuracy with logarithmic dependence on accuracy on a high-probability event
- The ratio bound L of the reward model and the exponential base (1+ε) directly determine the worst-case complexity without guidance

## Why This Works (Mechanism)

### Mechanism 1: Bellman Error Control
If the Bellman error of the approximate reward model scales as ε = O(1/T), SMC can reduce computational complexity of inference-time reasoning from exponential to polynomial in horizon T. The Bellman error measures deviation of approximate value function from optimal twist function, and when this local error is sufficiently small, the variance of importance weights and bias of particle system remain bounded, preventing exponential explosion of complexity.

### Mechanism 2: Metropolis-Hastings (MH) Correction
Augmenting finite-resampling-pool approach with Metropolis-Hastings correction allows achieving arbitrary TV accuracy δ_TV with logarithmic dependence on accuracy (log(1/δ_TV)) on high-probability event. The MH step uses exact likelihood ratio computed on augmented space to accept/reject trajectories, forcing chain to target exact reward-tilted distribution and geometrically contracting TV error via Dobrushin coefficient.

### Mechanism 3: Information-Theoretic Lower Bounds
Any sampler attempting to generate samples from target distribution without intermediate guidance or with only large Bellman error must incur exponential time complexity in worst case. By constructing "hard" family of inputs where target distribution places high mass on sparse set of leaves, any algorithm with limited time budget cannot query enough paths to find high-mass region with constant probability.

## Foundational Learning

- **Feynman-Kac Formalism**: Models inference-time scaling as particle approximation of flow of probability measures defined by potentials G_t and Markov kernels M_t. Understanding this formalism is required to parse proofs of particle complexity in Section 5.
  - Quick check: Can you explain how potential function G_t relates base policy π_ref to target tilted distribution π̃?

- **Bellman Error (in Soft RL)**: Measures accuracy of twist function used for inference-time guidance, unlike standard RL where it measures policy optimality. This is the central metric of the paper.
  - Quick check: If Bellman error is ε, how does error accumulate over T steps in single-particle trajectory (Theorem 4.3)?

- **Dobrushin Ergodic Coefficient**: Used in appendices to bound stability of Feynman-Kac semigroup and contraction rate of Metropolis-Hastings kernel.
  - Quick check: How does bound on Dobrushin coefficient relate to speed of convergence (mixing) of Markov chain in Section 6?

## Architecture Onboarding

- **Component map**: Base Policy (π_ref) -> Reward/Value Model (V̂) -> SMC Engine -> MH Corrector (Optional)
- **Critical path**: Generation of potential function G_t and subsequent resampling of particles based on these weights, where Bellman error directly impacts variance and bias
- **Design tradeoffs**: Naive vs. Optimal Proposal (cheap but high complexity vs. variance-minimizing but expensive), SP-gSMC vs. SMC (fast with linear error accumulation vs. slower with constant error), Correction Method (Rejection sampling requires C_∞ vs. Resampling-pool MH requires managing augmented space)
- **Failure signatures**: Particle Collapse (all N particles resample to single trajectory), Exponential Slowdown (Bellman error ε not O(1/T)), Weight Degeneracy (potentials G_t become extreme)
- **First 3 experiments**:
  1. Measure local Bellman error of reward model across different steps of reasoning traces, plot ε vs. T to verify O(1/T) scaling
  2. Run SMC with naive proposal on synthetic task, vary horizon T and observe if particle complexity N required for constant TV accuracy scales polynomially or exponentially
  3. Compare naive resampling-pool guidance against MH-corrected version on constrained decoding task, measure number of MH steps H needed to achieve δ_TV convergence

## Open Questions the Paper Calls Out

### Open Question 1
Does theoretical characterization of SMC complexity extend directly to token-level reasoning where exact probability evaluations are tractable? The paper notes that in token-level setting, expectation is calculable within O(|S|) time, but defers analysis of this regime. This remains unresolved because the paper focuses on sentence-level reasoning where incremental weights require Monte Carlo estimation.

### Open Question 2
What is optimal particle count for SMC-based inference scaling, and is there phase transition between single-particle and multi-particle regimes? The analysis shows both can achieve polynomial complexity under ε = O(1/T) regime but doesn't characterize when increasing particles beyond single particle provides diminishing returns or negative returns.

### Open Question 3
Can dependence on δ_TV in resampling-pool MH be improved from O(log(δ_TV⁻¹)) to O(log log(δ_TV⁻¹)) or similar, or is logarithmic dependence inherent? The paper achieves O(log(δ_TV⁻¹)) MH steps but doesn't prove this is optimal or whether alternative corrections might achieve faster convergence.

## Limitations
- Theoretical analysis relies on idealized assumptions about reward model's Bellman error and ratio bounds that may not hold in practice
- Paper does not provide empirical validation of O(1/T) scaling assumption for real-world reward models
- Optimal proposal mechanism's computational overhead from Monte Carlo estimation of normalizing constants is not addressed

## Confidence

- **High Confidence**: Information-theoretic lower bounds and polynomial complexity upper bounds under controlled Bellman error conditions are mathematically rigorous
- **Medium Confidence**: Practical applicability depends on assumptions requiring empirical validation, real-world reward models may not achieve required O(1/T) scaling
- **Low Confidence**: Optimal proposal mechanism's practical implementation feasibility and MH correction's performance in high-dimensional LLM spaces remain largely theoretical

## Next Checks

1. **Empirical Bellman Error Analysis**: Measure and plot local Bellman error of various reward model architectures (CTL, Soft-RL, SIXO) across reasoning traces of varying lengths to empirically verify O(1/T) scaling assumption

2. **Practical Complexity Scaling**: Implement naive-proposal SMC with different particle counts on benchmark reasoning tasks and measure actual computational complexity scaling with horizon T, comparing against theoretical predictions

3. **MH Correction Robustness**: Test Metropolis-Hastings correction mechanism on constrained decoding tasks with varying action space dimensions to assess acceptance rate stability and identify practical limitations