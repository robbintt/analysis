---
ver: rpa2
title: 'Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained
  Settings'
arxiv_id: '2505.13718'
source_url: https://arxiv.org/abs/2505.13718
tags:
- training
- reasoning
- base
- knight
- warmup
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a two-stage training framework for reasoning-capable
  LLMs in data-scarce environments. The first stage "warms up" the model via distillation
  with reasoning traces from Knights & Knaves logic puzzles to induce general reasoning
  skills.
---

# Warm Up Before You Train: Unlocking General Reasoning in Resource-Constrained Settings

## Quick Facts
- **arXiv ID**: 2505.13718
- **Source URL**: https://arxiv.org/abs/2505.13718
- **Reference count**: 40
- **Primary result**: Two-stage training (warmup + RLVR) significantly improves reasoning performance under data scarcity, with warmup alone yielding +10.2% on MATH, +15.3% on HumanEval+, and +9.0% on MMLU-Pro for Qwen2.5-3B.

## Executive Summary
This paper introduces a two-stage training framework for developing reasoning-capable LLMs when data is scarce. The first stage "warms up" models by distilling reasoning traces from Knights & Knaves logic puzzles, inducing general reasoning skills. The second stage adapts the warmed-up model to target domains using Reinforcement Learning with Verifiable Rewards (RLVR) on limited examples (≤100). Experiments demonstrate that warmup alone yields significant gains across multiple reasoning benchmarks, and when both base and warmed-up models undergo RLVR training on limited data, the warmed-up model consistently outperforms the base model. The approach also preserves cross-domain generalization after task-specific RLVR training.

## Method Summary
The method employs a two-stage training approach. First, a "warmup" stage uses knowledge distillation on Knights & Knaves (K&K) logic puzzle reasoning traces generated by a high-quality teacher model (QwQ-32B). This stage induces general reasoning behaviors without rejection sampling, using a learning rate of 1e-6 for 3 epochs. Second, a target-domain adaptation stage applies Reinforcement Learning with Verifiable Rewards (RLVR) using Dr. GRPO, training on 50-100 examples per task with batch sizes of 4×5, 10 generations per batch, and 8K maximum completion length. The approach is evaluated across multiple benchmarks including MATH, HumanEval+, and MMLU-Pro using models ranging from 1.5B to 14B parameters.

## Key Results
- Warmup alone yields significant gains: +10.2% on MATH, +15.3% on HumanEval+, and +9.0% on MMLU-Pro for Qwen2.5-3B
- When both base and warmed-up models undergo RLVR on ≤100 examples, warmed-up model consistently outperforms base: +6.7% on MATH, +5.0% on HumanEval+
- Warmed-up models maintain longer completions and broader reasoning capabilities after task-specific RLVR training
- Method shows consistent gains across model sizes (1.5B to 14B) and diverse reasoning tasks

## Why This Works (Mechanism)
The approach works by first establishing a general reasoning foundation through distillation of structured logic puzzle reasoning traces, then fine-tuning this foundation for specific tasks. The K&K puzzles provide a controlled environment where correct reasoning leads to verifiable outcomes, creating a robust prior that transfers to more complex domains. The warmup stage creates a reasoning-capable model that RLVR can then efficiently adapt to specific tasks with limited data.

## Foundational Learning
- **Knowledge Distillation**: Transferring reasoning patterns from teacher to student model; needed to induce general reasoning capabilities from expert demonstrations
- **Reinforcement Learning with Verifiable Rewards (RLVR)**: Fine-tuning with reward signals based on verifiable outcomes; needed for task-specific adaptation with limited data
- **Knights & Knaves Logic Puzzles**: Simple constraint satisfaction problems with clear logical structure; needed as a controlled environment for establishing reasoning priors
- **Dr. GRPO Algorithm**: Group relative policy optimization variant; needed for efficient RL training with small batch sizes
- **CoT (Chain-of-Thought) Reasoning**: Step-by-step problem solving; needed to make reasoning process explicit and trainable
- **Warmup Effect**: Pre-training on reasoning tasks before fine-tuning; needed to establish general reasoning capabilities that transfer across domains

## Architecture Onboarding

**Component Map**: QwQ-32B Teacher -> K&K Dataset -> SFT Distillation -> Warmed-up Model -> RLVR (Dr. GRPO) -> Target Task Performance

**Critical Path**: The most critical components are the teacher model quality (QwQ-32B vs alternatives), the K&K distillation stage (LR=1e-6 critical), and the RLVR adaptation with proper reward functions per domain.

**Design Tradeoffs**: Using logic puzzles for warmup trades domain-specific knowledge for general reasoning capability; the fixed 100-example RLVR limit trades comprehensive training for data efficiency; rejection sampling is avoided to maintain diverse reasoning patterns.

**Failure Signatures**: K&K overfitting at LR>1e-6 (MATH drops to ~11%); short CoT generation from non-reasoning teacher; cross-domain degradation after RLVR on base model.

**First Experiments**:
1. Verify teacher model produces long, reflective CoT traces (QwQ-32B vs Qwen2.5-32B comparison)
2. Test K&K distillation at multiple learning rates to confirm 1e-6 is optimal
3. Evaluate warmup effect on a held-out reasoning task not used in RLVR training

## Open Questions the Paper Calls Out
- How well does reasoning distilled from simple logic domains transfer to more complex tasks involving richer dynamics, interaction, or domain-specific constraints (e.g., multi-agent environments)?
- What alternative synthetic environments could serve as more effective reasoning priors than K&K, and what design principles should guide their construction?
- Why does distillation with s1K cause performance degradation on the 14B model while K&K distillation yields consistent gains across all model sizes?
- Does the warmup-based reasoning approach scale effectively to larger models (beyond 14B), and how do the sample efficiency benefits evolve with model scale?

## Limitations
- Effectiveness appears tightly coupled to specific K&K dataset and teacher model (QwQ-32B)
- RLVR training uses varying step counts without clear convergence criteria
- Cross-domain generalization preservation claims based on completion length rather than systematic downstream task evaluations
- Limited to 14B parameter models due to compute constraints

## Confidence

**High confidence**: Core finding that warmup + RLVR outperforms base + RLVR on target tasks with limited data
**Medium confidence**: Generalization preservation claim (based on completion length observations)
**Medium confidence**: Data efficiency claim (training procedure and stopping criteria introduce variability)

## Next Checks
1. Test warmup mechanism with alternative reasoning teachers (QwQ-32B alternatives) to verify benefit generalizes beyond specific teacher
2. Implement convergence-based stopping criteria for RLVR training rather than fixed step counts
3. Systematically evaluate post-RLVR models on held-out reasoning tasks beyond completion length to quantitatively assess cross-domain generalization preservation