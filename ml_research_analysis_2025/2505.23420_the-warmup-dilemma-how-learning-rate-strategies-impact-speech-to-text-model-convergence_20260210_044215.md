---
ver: rpa2
title: 'The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model
  Convergence'
arxiv_id: '2505.23420'
source_url: https://arxiv.org/abs/2505.23420
tags:
- warmup
- training
- exponential
- convergence
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the impact of learning rate warmup strategies
  on the convergence of large-scale speech-to-text models. While the double linear
  warmup introduced by OWSM helped training converge, its effectiveness had not been
  compared with other policies nor its impact on final model quality been investigated.
---

# The Warmup Dilemma: How Learning Rate Strategies Impact Speech-to-Text Model Convergence

## Quick Facts
- arXiv ID: 2505.23420
- Source URL: https://arxiv.org/abs/2505.23420
- Reference count: 13
- Large-scale S2T training with complex Transformer variants requires sub-exponential LR warmup to avoid divergence

## Executive Summary
This paper investigates learning rate warmup strategies for large-scale speech-to-text models, focusing on their impact on convergence and final model quality. Through systematic experiments with a 878M-parameter Conformer model trained on 150k hours of speech data, the authors demonstrate that polynomial warmup leads to exploding gradients and model divergence, while exponential and piecewise-linear schedules ensure stable training. The study reveals that deeper Conformer encoders (>18 layers) are particularly sensitive to warmup policy choices, and that while exponential warmup accelerates early convergence, it does not improve final word error rates compared to piecewise-linear approaches.

## Method Summary
The study trains an 878M-parameter Conformer model (24 encoder layers, 12 decoder layers) on ~150k hours of multilingual speech data using Adam optimizer with gradient clipping and weight decay. Four learning rate warmup strategies are compared: polynomial (α=1.5), exponential, piecewise-linear, and standard linear. Training runs for one epoch with 5 gradient accumulation steps, monitoring validation perplexity and gradient norms throughout. The experimental setup includes label-smoothed cross-entropy loss plus dual CTC objectives, with models evaluated on multiple test sets using word error rate as the primary metric.

## Key Results
- Polynomial warmup causes exploding gradients and model divergence in deep Conformer encoders (>18 layers)
- Exponential and piecewise-linear warmup schedules ensure stable convergence
- Exponential warmup accelerates initial convergence but does not improve final model performance
- Encoder depth, not decoder depth, drives convergence instability in Conformer architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Exponential/sub-exponential warmup prevents exploding gradients in deep Conformer encoders.
- Mechanism: Complex encoder layers with multiple subcomponents wrapped in residual connections accumulate scaling effects during early training. Exponential warmup keeps LR sufficiently low in the critical 15k-50k step window where gradient instability emerges, avoiding the steep late-stage increases that polynomial warmup introduces.
- Core assumption: The gradient spikes observed at 25k-30k steps with polynomial warmup are causally linked to the LR magnitude in that window, not to other training dynamics.
- Evidence anchors:
  - [abstract] "large-scale S2T trainings with complex Transformer variants like Conformer require sub-exponential LR warmup to ensure convergence, as polynomial warmup leads to exploding gradients and model divergence"
  - [section 4.1] "models with more than 18 encoder layers were not converging...each subcomponent is wrapped in a residual connection...may indicate a need for additional normalization layers"
  - [section 4.2] "in the polynomial training there are huge spikes in the gradient norm in the range 25k-30k steps...This minimal difference is sufficient to enable model convergence"
  - [corpus] Related theoretical analysis (arXiv:2509.07972) suggests warmup benefits are not fully theoretically understood; corpus provides limited causal validation.

### Mechanism 2
- Claim: Higher early-stage LR accelerates initial convergence without improving final performance.
- Mechanism: Exponential warmup front-loads higher LR in the first ~15k steps, producing faster perplexity reduction early. However, once past warmup, the inverse-square-root decay dominates optimization, and both exponential and piecewise-linear schedules converge to similar loss basins.
- Core assumption: The learning dynamics after warmup phase are governed primarily by the decay schedule, swamping any initialization differences from warmup.
- Evidence anchors:
  - [abstract] "exponential warmup accelerates initial convergence compared to piecewise-linear warmup, it does not result in better final model performance"
  - [section 4.2] "the exponential policy – which features the highest LR in the first ~15k steps – always displays the lowest perplexity"
  - [figure 3] Learning curves converge post-50k steps with slight piecewise-linear advantage on English data
  - [corpus] No corpus papers directly validate this specific claim for S2T; limited external evidence.

### Mechanism 3
- Claim: Encoder depth, not decoder depth, drives convergence instability in Conformer models.
- Mechanism: Conformer encoder layers contain convolutional modules and additional feed-forward components absent in standard Transformer layers. Each subcomponent's residual path compounds gradient scaling; deeper encoders amplify this effect while decoder depth is less consequential.
- Core assumption: The divergence pattern observed (18+ encoder layers fail, 12 encoder + 12 decoder succeeds) generalizes beyond the specific Conformer implementation used.
- Evidence anchors:
  - [section 4.1] "models with 18 encoder layers and 6 decoder layers diverge, while models with 12 encoder and 12 decoder layers converge"
  - [section 4.1] "the number of encoder layers was the driver of the issue while adding more decoder layers was not"
  - [section 1] "deeper layers involving additional components (e.g., extra convolutional or linear layers), making them more prone to 'exploding gradient'"
  - [corpus] Corpus papers on warmup focus on LLMs/Transformers broadly; no direct validation for encoder-specific instability claim.

## Foundational Learning

- Concept: **Learning Rate Warmup**
  - Why needed here: The paper's central intervention; understanding why LR starts at 0 and gradually increases is essential before comparing linear/piecewise/exponential/polynomial variants.
  - Quick check question: Can you explain why starting with full learning rate at step 0 might destabilize deep network training?

- Concept: **Exploding Gradients**
  - Why needed here: The paper diagnoses polynomial warmup failure through gradient norm spikes; understanding this failure mode is prerequisite to interpreting the gnorm analysis.
  - Quick check question: What happens to weight updates when gradient norms exceed 100-200 during training?

- Concept: **Conformer Architecture**
  - Why needed here: The paper's findings are architecture-specific; Conformer combines self-attention with convolution, differing from vanilla Transformer and explaining why standard warmup fails.
  - Quick check question: What additional components does a Conformer block contain compared to a standard Transformer encoder block?

## Architecture Onboarding

- Component map:
  - Frontend: Two 1D convolutional layers (4× downsampling) -> Encoder: 24-layer Conformer (1024-dim embeddings, 4096 FFN hidden dim, 16 attention heads) -> Decoder: 12-layer Transformer -> Loss: Label-smoothed cross-entropy + dual CTC (layer 16 + final encoder output)

- Critical path:
  1. Verify Conformer implementation handles padding correctly (cite Papi et al. 2024 fix)
  2. Configure warmup schedule: exponential or piecewise-linear with w=50k, η=2e-4
  3. Monitor gradient norm during first 50k steps—should stay <25; spikes >100 indicate imminent divergence
  4. If divergence occurs at 25k-30k steps, reduce LR in that window or switch warmup policy

- Design tradeoffs:
  - Exponential warmup: Faster early convergence, same final quality, marginally safer against divergence
  - Piecewise-linear: Slower start, comparable final WER, more hyperparameters (η', w')
  - Polynomial: Not viable for 24-layer Conformer encoders with α=1.5
  - Assumption: Tradeoffs may differ for α values not tested

- Failure signatures:
  - Gradient norm spikes >100 during warmup → model will not converge
  - Perplexity plateaus at high values (~8-10) through 50k steps → warmup policy incompatible
  - Encoder-only depth increase (>18 layers) without warmup adaptation → divergence likely

- First 3 experiments:
  1. **Warmup ablation**: Train with exponential vs piecewise-linear warmup, log perplexity at 5k-step intervals and gradient norm throughout. Confirm both converge and compare WER at 170k steps.
  2. **Depth boundary test**: Train 18-layer vs 20-layer vs 24-layer encoder with exponential warmup to validate the >18-layer divergence threshold under improved warmup.
  3. **Learning rate sensitivity**: Run exponential warmup with peak LR at η=1e-4 vs 2e-4 vs 4e-4 to test whether the 15k-50k window LR tolerance scales with peak LR.

## Open Questions the Paper Calls Out

- **Open Question 1**: Would adding normalization layers within each Conformer encoder block mitigate the scaling effects and convergence issues observed in deep models?
  - Basis in paper: [explicit] The authors state in Section 4.1: "each subcomponent is wrapped in a residual connection... which may indicate a need for additional normalization layers within each encoder block to mitigate potential scaling effects. However, we leave this investigation for future work."
  - Why unresolved: The paper identifies the structural cause (residual connections on subcomponents) but does not experimentally test normalization layer interventions.
  - What evidence would resolve it: Training deep Conformer models (>18 encoder layers) with standard linear warmup while adding layer normalization within encoder blocks, comparing convergence rates against baseline.

- **Open Question 2**: Do these warmup strategy findings generalize to multilingual (100+ languages) and multi-task training scenarios?
  - Basis in paper: [explicit] In the Limitations section: "future works should validate that our findings extend to these scenarios."
  - Why unresolved: Experiments used only two languages and a single task (ASR), while OWSM trained on 180k hours across 100+ languages with multiple tasks.
  - What evidence would resolve it: Replicating the warmup comparisons (piecewise-linear vs. exponential vs. polynomial) on a massively multilingual, multi-task training setup.

- **Open Question 3**: Can polynomial warmup achieve convergence with a different α value than the 1.5 used in this study?
  - Basis in paper: [explicit] The Limitations section states: "by tuning α we could, for instance, obtain a converging model even with the polynomial policy, this was not the focus of our work."
  - Why unresolved: Only α=1.5 was tested; lower values may produce a sufficiently gradual warmup to prevent gradient explosion.
  - What evidence would resolve it: Systematic sweep of α values (e.g., 1.1–1.4) for polynomial warmup, monitoring gradient norms and final WER.

- **Open Question 4**: What mechanisms explain why exponential warmup accelerates early convergence without improving final model quality?
  - Basis in paper: [inferred] The paper reports this phenomenon (Figure 2, Figure 3, Table 1) but offers no theoretical or empirical explanation for the decoupling between convergence speed and final performance.
  - Why unresolved: The analysis focuses on describing behavior rather than explaining the underlying optimization dynamics.
  - What evidence would resolve it: Detailed analysis of loss landscape geometry, gradient flow, and weight trajectory differences between exponential and piecewise-linear warmup during and after warmup.

## Limitations

- Findings are specific to Conformer architecture and may not generalize to other model types
- Empirical evidence for encoder-specific depth sensitivity relies on limited ablation experiments
- Optimal parameterization for piecewise-linear warmup schedules remains unexplored
- The claim that post-warmup decay dynamics dominate assumes training duration is sufficient

## Confidence

- **High confidence**: Polynomial warmup causes gradient spikes and model divergence in deep Conformer encoders (>18 layers), supported by direct gradient norm measurements and consistent model behavior
- **Medium confidence**: Exponential warmup accelerates early convergence without improving final performance, though corpus lacks direct external validation for S2T models
- **Low confidence**: Generalizability of encoder-specific depth sensitivity claim to other Conformer implementations or complex encoder architectures, as evidence is based on single model configuration

## Next Checks

1. **Architecture Generalization Test**: Train the same model configuration (24-layer Conformer encoder) with polynomial, exponential, and piecewise-linear warmup, but vary the residual connection scaling factors or add additional normalization layers. Measure whether the 25k-30k gradient spikes persist or are mitigated, testing whether the mechanism is tied to specific architectural details.

2. **Scale and Duration Sensitivity**: Repeat the warmup ablation experiments with smaller (e.g., 500M) and larger (e.g., 1.2B) parameter models, and extend training to 2 epochs. Compare final WER and convergence patterns to determine if the exponential warmup advantage in early convergence scales with model size or training duration.

3. **Decoder Depth Interaction**: Train models with 12, 18, and 24 decoder layers (keeping encoder at 24 layers) using exponential warmup. Monitor convergence and final performance to validate whether decoder depth has negligible impact on stability, as claimed, or if there is an interaction between encoder and decoder depth not captured in the original experiments.