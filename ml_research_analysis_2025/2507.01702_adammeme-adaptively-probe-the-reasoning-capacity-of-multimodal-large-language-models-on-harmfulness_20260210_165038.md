---
ver: rpa2
title: 'AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language
  Models on Harmfulness'
arxiv_id: '2507.01702'
source_url: https://arxiv.org/abs/2507.01702
tags:
- meme
- harmfulness
- memes
- target
- harmful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes AdamMeme, an adaptive evaluation framework
  for assessing multimodal large language models (mLLMs) on harmful meme understanding.
  Existing benchmarks rely on static, accuracy-based evaluations that fail to capture
  the dynamic nature of online memes.
---

# AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness

## Quick Facts
- arXiv ID: 2507.01702
- Source URL: https://arxiv.org/abs/2507.01702
- Authors: Zixin Chen; Hongzhan Lin; Kaixin Li; Ziyang Luo; Zhen Ye; Guang Chen; Zhiyong Huang; Jing Ma
- Reference count: 40
- One-line primary result: Adaptive multi-agent framework achieves 56.7% human-agent agreement on harmful meme evaluation, with GPT-4o and Step models scoring below 10% failure rate

## Executive Summary
This paper introduces AdamMeme, an adaptive evaluation framework for assessing multimodal large language models' (mLLMs) reasoning capacity on harmful meme understanding. Unlike static benchmarks, AdamMeme uses a multi-agent system to iteratively generate challenging meme samples that expose model-specific weaknesses. The framework employs harmfulness mining with taxonomy-based categorization, model scoring using reference-based evaluation, and iterative refinement to create harder cases targeting specific misbeliefs. Experiments on 11 mLLMs show that GPT-4o and Step series models achieve the best overall performance, with failure rates below 10% on average.

## Method Summary
AdamMeme uses a three-stage pipeline: (1) Harmfulness Mining where three Miner agents with majority voting categorize memes, validated by Examiner and Judge agents, (2) Model Scoring via mLLM-as-a-Judge where three candidate agents generate analyses, a senior agent summarizes the reference answer, and a Scorer agent rates target models on a 1-10 scale, and (3) Iterative Refinement where the Refiner agent retrieves similar cases via BM25 on misbelief sentences and modifies meme text to be more euphemistic while preserving harmful concepts. The framework uses GPT-4o as the agent controller with specific temperature settings and OCR-SAM preprocessing to erase original meme text before scoring.

## Key Results
- GPT-4o and Step series models achieve the best overall performance with failure rates below 10% on average
- Iterative refinement reduces average scores by up to 4.73% and reveals concentrated weaknesses in racial stereotypes and anti-Black bias
- Human evaluation confirms 56.7% agreement on average scores and 73.8% on failure rate classification

## Why This Works (Mechanism)

### Mechanism 1: Multi-Agent Taxonomy Construction with Consensus Voting
Distributed agent roles with majority voting produce more reliable harmfulness categorization than single-model classification. Three Miner agents independently categorize memes; decisions require >50% agreement. Examiner and Judge agents validate new category proposals against meme content and taxonomy coherence before inclusion. Agent disagreement signals ambiguous cases worth filtering or flagging, and multi-role verification prevents taxonomy drift.

### Mechanism 2: Reference-Based Scoring with Wisdom-of-Crowds Ground Truth
Aggregating multiple candidate analyses before scoring produces fairer evaluation than single-reference comparison. Three agents generate candidate answers; a senior agent summarizes the best response as reference. A Scorer agent then grades the target model's answer against this synthesized ground truth on a 1-10 scale. The agent-generated reference approximates human expert judgment, and scoring divergence indicates target model weakness rather than reference quality issues.

### Mechanism 3: Weakness-Targeted Iterative Refinement via Retrieval-Augmented Text Modification
Modifying meme text to reduce explicit harmfulness signals while preserving underlying misbelief exposes model reliance on superficial cues. When a target model scores below threshold, the Refiner retrieves semantically similar cases via BM25 on misbelief sentences, then rewrites meme text to be more euphemistic while maintaining the harmful concept. Models that succeed on explicit harmfulness but fail on obscured versions lack true reasoning about multimodal harm.

## Foundational Learning

- **mLLM-as-a-Judge paradigm**: The entire scoring mechanism relies on using one mLLM (GPT-4o) to evaluate another's outputs; understanding biases in this approach is critical. *Quick check: Can you explain why the paper uses GPT-4o as the judge and what risks this creates for models trained on GPT-4o-distilled data?*

- **Taxonomy-based harm categorization**: The framework's first stage depends on building and maintaining a harmfulness taxonomy; engineers must understand how categories are proposed, validated, and expanded. *Quick check: What two new categories did Miner agents discover, and what validation steps were required before adding them?*

- **Retrieval-augmented in-context learning**: The Refiner agent uses retrieved historical samples as in-context examples; understanding BM25 retrieval and its limitations is essential for debugging refinement quality. *Quick check: Why does the paper use misbelief sentences rather than raw meme text as retrieval queries?*

## Architecture Onboarding

- **Component map**: Raw meme input → Mining stage (Miner agents ×3 with majority vote → Examiner agent → Judge agent → Narrator agent generates misbelief) → Scoring stage (Candidate generators ×3 → Senior summarizer → Reference answer → Scorer agent 1-10 rating) → Refinement stage (BM25 retrieval on misbelief → Refiner agent rewrites text → Re-score loop)

- **Critical path**: Raw meme input → Mining stage produces (image, text, category, misbelief) tuples → Scored samples feed into refinement seed pool (S=10 samples) → Each refinement iteration retrieves similar cases, modifies text, re-scores; if score drops, continues to next similar sample → Convergence when score improves or iteration limit (N=6) reached

- **Design tradeoffs**: Text-only modification preserves image but limits multimodal adversarial coverage; assumption is textual semantics are more direct (cited Akbari et al., 2019); GPT-4o as controller provides strong capabilities but introduces bias for models distilled from GPT-4o; threshold=4.0 for failure rate validated by human evaluation (73.8% agreement) but may not generalize across cultures

- **Failure signatures**: Taxonomy explosion from overly permissive new category suggestions; refinement drift where modified text loses connection to original misbelief; scoring ceiling effects where GPT-4o averages 7.38 and all models cluster near ceiling

- **First 3 experiments**: (1) Ablate the mining stage: Replace majority-vote Miner with single-agent classification; measure taxonomy coherence and category distribution shift (2) Cross-model judge swap: Use Qwen-VL-Max or Step-1o as the scoring agent instead of GPT-4o; compare failure rate rankings for correlation (3) Image-text joint refinement: Extend Refiner to modify both modalities (using inpainting for image text); test whether visual obscuring reveals additional weaknesses beyond text-only modification

## Open Questions the Paper Calls Out

### Open Question 1
**Question:** How can the evaluation framework be adapted to effectively assess mLLMs with strict safety alignment (e.g., Claude, Gemini) that frequently refuse to analyze harmful content?
**Basis in paper:** The "Limitations" section states the authors were "unable to conduct a complete evaluation of certain mainstream mLLMs... due to their inherent safety mechanisms," restricting the ability to fully assess their capabilities.
**Why unresolved:** The current methodology relies on direct prompting, which triggers refusal errors in safety-aligned models, creating a gap in the evaluation landscape.
**What evidence would resolve it:** Successfully incorporating "red-teaming" or specific prompt-engineering techniques into the AdamMeme pipeline that induce safe models to analyze harmfulness without refusal, yielding comparable failure rates.

### Open Question 2
**Question:** To what extent does using GPT-4o as the agent controller introduce systematic bias against models trained on different data distributions?
**Basis in paper:** The "Limitations" section acknowledges that employing GPT-4o as the controller introduces "inherent bias" because emerging mLLMs are often distilled from it, potentially favoring reasoning that aligns with the controller's logic.
**Why unresolved:** The paper relies on human evaluation to confirm fairness, but does not isolate or quantify the specific bias resulting from the controller-target relationship.
**What evidence would resolve it:** Ablation studies using diverse, non-GPT-4o controllers (e.g., open-source models) showing consistent scoring patterns across target models.

### Open Question 3
**Question:** Does restricting the iterative refinement stage to textual modifications limit the discovery of visual-centric reasoning failures?
**Basis in paper:** Page 5 states the Refiner agent modifies "the text $t$ while preserving the image $i$," based on the assumption that textual semantics are more direct.
**Why unresolved:** The framework currently probes reasoning by obscuring textual cues, but does not dynamically alter visual semantics to test visual-harmful comprehension.
**What evidence would resolve it:** Implementation of an image-modification refinement step that achieves a comparable or higher drop in target model scores compared to text-only refinement.

## Limitations

- Human-machine agreement threshold: Framework relies on GPT-4o as both controller and judge, creating potential bias for models trained on GPT-4o-distilled data with only 56.7% human agreement on average scores
- Text-only modification scope: Refinement focuses solely on textual modifications, leaving visual content unchanged and limiting multimodal reasoning failure discovery
- Taxonomy scalability: Framework's ability to scale to emerging harmfulness categories without human intervention remains unproven

## Confidence

- **High Confidence**: Iterative refinement successfully reduces scores by 4.73% and converges around iteration 6; GPT-4o and Step models consistently outperform others; failure rates below 10% for top models
- **Medium Confidence**: Human evaluation confirms scoring consistency (73.8% agreement on failure rate); taxonomy construction process is robust to agent disagreement; reference-based scoring provides fairer evaluation than single-reference comparison
- **Low Confidence**: Cross-cultural generalizability of the harmfulness taxonomy; long-term stability of the multi-agent consensus mechanism; effectiveness of text-only modifications for probing multimodal reasoning

## Next Checks

1. **Cross-Model Judge Validation**: Replace GPT-4o with Qwen-VL-Max and Step-1o as the scoring agent; measure correlation changes in failure rate rankings across all 11 models
2. **Visual-Text Joint Refinement**: Extend the Refiner agent to modify both image and text (using inpainting for visual text); test whether this reveals additional reasoning weaknesses beyond text-only modification
3. **Cultural Context Expansion**: Evaluate the framework using meme datasets from non-Western sources; measure changes in category distribution and model performance to assess cross-cultural generalizability