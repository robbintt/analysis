---
ver: rpa2
title: 'Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based
  (Mis)Generalization in LLMs'
arxiv_id: '2505.22630'
source_url: https://arxiv.org/abs/2505.22630
tags:
- context
- query
- class
- language
- candidates
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work examines irrelevant context hallucinations in LLMs\u2014\
  errors where misleading context cues are incorporated into predictions. The authors\
  \ hypothesize and empirically validate that these hallucinations stem from a structured\
  \ mechanism called class-based (mis)generalization, where models combine abstract\
  \ class representations with features from either the query or context to generate\
  \ answers."
---

# Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs

## Quick Facts
- **arXiv ID:** 2505.22630
- **Source URL:** https://arxiv.org/abs/2505.22630
- **Reference count:** 37
- **Primary result:** LLMs exhibit "class-based (mis)generalization" where irrelevant context shifts predictions by combining abstract class cues with features from context entities

## Executive Summary
This paper investigates irrelevant context hallucinations in LLMs—errors where misleading context cues are incorporated into predictions. Through behavioral analysis on Llama-3, Mistral, and Pythia across 39 relation types, the authors identify a structured mechanism called class-based (mis)generalization. They find that 71% of context-based candidates integrate identifiable context features with the correct abstract class. Mechanistic interpretability reveals this process is governed by two competing circuits—one favoring query-based reasoning and another integrating contextual cues—whose relative strength determines the final prediction. The findings suggest LLMs are "stochastic chameleons," dynamically adapting to contextual cues in systematic but unreliable ways.

## Method Summary
The study evaluates base models (Llama-3 8B/70B, Mistral v0.3 7B, Pythia 6.9B/12B) on the ParaRel dataset with 39 factual recall relation types. The experimental design compares Q-only queries (27.6K total) against C+Q conditions (3,900 context variations per query, totaling ~106M examples). Context is constructed by prepending random context demonstrations from other subdatasets. Candidates are classified as context-based (top-3 under C+Q, not top-10 under Q) or query-based (top under both). Mechanistic interventions include logit attribution across layers, activation patching with Gaussian noise (σ ≈ 0.3), and attention knockout at specific layers (L17+L24 for Llama-3, L18+L24 for Mistral, L19+L24 for Pythia).

## Key Results
- 71% of context-based candidates integrate identifiable context features with the correct abstract class
- Two competing circuits govern feature selection: query-based (layers ~8) and context-based (layers ~17), with competition peaking at layers 17-24
- Attention knockout at layers 17 and 24 can flip predictions from context-based to query-based candidates
- Hierarchical class-to-instance processing occurs with class representations forming in lower layers (1-15) before refining into specific answers in higher layers (17-29)

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Class-to-Instance Predictions
- **Claim:** LLMs construct abstract class representations in lower layers before refining them into specific answer instances in higher layers.
- **Mechanism:** Early layers (1-15) elevate logits for class tokens; mid-to-late layers (17-29) then promote specific candidates. The transition around layers 17-24 is decisive.
- **Core assumption:** Pre-training has encoded separable "class" and "instance" representations.
- **Evidence:** Logit attribution shows class logits peaking early; candidate logits diverge from layer ~17 onward.
- **Break condition:** Novel or under-represented target classes may cause the hierarchical decomposition to fail.

### Mechanism 2: Competing Circuits for Feature Selection
- **Claim:** Two distinct circuits—query-based and context-based—compete, and their relative strength determines the final prediction.
- **Mechanism:** Query circuit integrates query-subject information from ~layer 8; context circuit transfers context subject/object information to the last token from ~layer 17. By layers 17-24, the stronger circuit's candidate dominates.
- **Core assumption:** Context and query information flow through partially separable pathways.
- **Evidence:** Activation patching shows distinct circuits with different layer-wise profiles; context circuit activates later but can overpower query circuit.
- **Break condition:** Heavy overlap between context and query features may cause circuits to merge rather than compete.

### Mechanism 3: Class-Based (Mis)Generalization via Feature Combination
- **Claim:** When irrelevant context shifts predictions, the model combines an abstract class cued by the query relation with features extracted from context entities.
- **Mechanism:** The query relation implies an expected class (e.g., "languages"); the model extracts salient features from context (e.g., "Japan" from "Honda") and combines class + feature to produce an answer (e.g., "Japanese").
- **Core assumption:** Entities have reasonably consistent feature-class mappings in pre-training.
- **Evidence:** 71% of context-based candidates satisfy both context-feature incorporation and correct-class criteria in human annotation.
- **Break condition:** Ambiguous or multi-class features in context entities may lead to wrong or mixed feature selection.

## Foundational Learning

- **Concept:** Residual stream and layer-wise vocabulary projection (logit lens)
  - **Why needed here:** Understanding how information accumulates and becomes interpretable across layers is essential for reading the hierarchical class-to-instance trajectory.
  - **Quick check question:** If class logits are high at layer 10 but candidate logits diverge only after layer 17, where is the "decision" most plausibly occurring?

- **Concept:** Attention knockout and activation patching
  - **Why needed here:** These interventions demonstrate causality by blocking context information at layers 17 and 24 to flip predictions.
  - **Quick check question:** If you ablate attention from context tokens to the last token at layer 24, what should happen to the probability of context-based candidates?

- **Concept:** Pointwise Mutual Information (PMI) for context-candidate dependence
  - **Why needed here:** PMI quantifies whether context-candidate pairings are statistically associated beyond chance, supporting systematic generalization.
  - **Quick check question:** Why is PMI preferable to raw co-occurrence frequency when contexts are artificially constructed and uniformly sampled?

## Architecture Onboarding

- **Component map:** `[C_REL0, C_SUBJ1, C_REL2, C_OBJ3, Q_REL4, Q_SUBJ5, Q_REL6] -> Layers 1-15 (class formation) -> Layers 17-24 (context/query competition) -> Layers 25-32 (answer refinement) -> Output`

- **Critical path:**
  1. Context tokens processed in early layers; class information extracted.
  2. Context subject/object → last token transfer initiates at ~layer 17.
  3. Competition between circuits peaks between layers 17-24.
  4. Stronger circuit determines whether final answer is context-dominant or query-dominant.

- **Design tradeoffs:**
  - Intervention timing: Knocking out at layer 17 (first transfer) vs layer 24 (peak integration) yields different flip rates.
  - Candidate thresholds: `Ccand.` defined as top-3 under C+Q but not top-10 under Q balances sensitivity vs specificity.
  - Noise scale in patching: σ = 0.3 follows prior work but may require tuning for different model families/sizes.

- **Failure signatures:**
  - Knockout doesn't flip predictions → Check whether context information entered before layer 17 or via non-attention routes.
  - Post-intervention outputs are random/unidentifiable → Intervention likely too aggressive; reduce knockout scope or noise level.
  - Context-dominant behavior persists at 70B scale → Scaling alone doesn't suppress class-based (mis)generalization; circuit structure persists.

- **First 3 experiments:**
  1. Replicate logit attribution on 100 context-dominant samples: verify class-token logits peak before candidate-token logits; record transition layer.
  2. Implement attention knockout at layers 17 and 24 for 50 context-dominant samples; measure flip rate to query-based candidates and compare to random-layer baselines.
  3. Compute PMI between context terms and generated candidates for a held-out relation type (e.g., expertise); test whether dependence remains significant (p < 0.01) and compare to null distribution via permutation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the minimum model scale required for class-based (mis)generalization to emerge?
- **Basis:** From Limitations: "Do smaller models also display class-based generalization, and if so, what is the minimum size required?"
- **Why unresolved:** The study only evaluated models around 7-8B, 12B, and 70B parameters. Smaller models were not tested.
- **What evidence would resolve it:** Systematic evaluation of models below 1B parameters using the same ParaRel-based C+Q framework.

### Open Question 2
- **Question:** Can mechanistic interventions at specific attention heads reliably mitigate irrelevant context hallucinations?
- **Basis:** From Limitations: "Developing mitigation methods informed by our findings and evaluating their effectiveness is an important direction for future research."
- **Why unresolved:** Attention knockout experiments demonstrated causal influence but were designed for mechanistic validation, not mitigation.
- **What evidence would resolve it:** Testing whether targeted ablation or steering of the identified context circuit reduces hallucination rates without degrading overall model capability.

### Open Question 3
- **Question:** Does class-based generalization transfer across languages with shared or distinct class structures?
- **Basis:** From Limitations: "our study is limited to English-language datasets."
- **Why unresolved:** Abstract class representations may encode differently across languages with different grammatical or ontological structures.
- **What evidence would resolve it:** Replicating experiments on multilingual models using translated queries and contexts, then comparing whether the same layers mediate class construction.

## Limitations

- The study relies on controlled, synthetic contexts rather than naturalistic scenarios, raising questions about ecological validity.
- While 71% of context-based candidates show identifiable class-feature combinations, the remaining 29% suggest alternative mechanisms may be at play.
- The focus on factual recall relations limits generalizability to other domains where class-based generalization might manifest differently.

## Confidence

- **High confidence:** The empirical observation that irrelevant context shifts predictions in a systematic, class-based manner (supported by 71% annotation agreement and PMI significance).
- **Medium confidence:** The hierarchical layer-wise mechanism (logit attribution showing class-to-instance progression) - while consistent with the data, alternative explanations exist.
- **Medium confidence:** The competing circuits framework - the attention knockout results are compelling, but circuit separation could be more nuanced.

## Next Checks

1. Test ecological validity by evaluating on naturally occurring multi-turn conversations where context is genuinely irrelevant but semantically plausible, measuring whether the same class-based (mis)generalization pattern emerges without engineered context.

2. Validate the circuit competition hypothesis by implementing ablation studies that selectively disable either query or context pathways across different relation types, measuring whether the predicted flip rates and class-feature combination patterns hold consistently.

3. Examine scaling effects by comparing 8B and 70B models on identical relation types, testing whether the hierarchical class-to-instance transition occurs at proportionally different layers, and whether circuit competition intensifies or weakens with scale.