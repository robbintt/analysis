---
ver: rpa2
title: Learning Causality for Longitudinal Data
arxiv_id: '2512.04980'
source_url: https://arxiv.org/abs/2512.04980
tags:
- causal
- treatment
- data
- learning
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Learning Causality for Longitudinal Data

## Quick Facts
- arXiv ID: 2512.04980
- Source URL: https://arxiv.org/abs/2512.04980
- Reference count: 0
- Primary result: Proposes methods to estimate Individual Treatment Effects in longitudinal data with unobserved adjustment variables and selection bias.

## Executive Summary
This work addresses the challenge of estimating Individual Treatment Effects (ITE) in longitudinal observational data where unobserved static adjustment variables are present. The proposed framework uses a Causal Dynamic Variational Autoencoder (CDVAE) to learn a latent substitute for these variables, ensuring sequential ignorability and identifiability of the Augmented CATE. Additionally, a Causal Contrastive Predictive Coding (Causal CPC) model is introduced for long-term counterfactual prediction, and a transparency mechanism using Jacobian-based subspace clustering is developed to recover interpretable feature groupings.

## Method Summary
The framework combines three key methods: (1) CDVAE learns a static latent variable Z as a substitute for unobserved adjustment variables, using a weighted ELBO with IPM and distance match terms, trained via adversarial optimization. (2) Causal CPC uses contrastive learning (InfoNCE) with GRU encoders for long-term prediction and CLUB for adversarial balancing. (3) Transparency is achieved through Sparse Subspace Clustering on the decoder Jacobian to recover feature clusters. The methods are validated on synthetic, semi-synthetic MIMIC-III, and real-world Saint-Gobain data.

## Key Results
- CDVAE with latent substitutes significantly outperforms baselines in estimating CATE under unobserved static confounders.
- Causal CPC achieves near-oracle long-term prediction performance, outperforming Transformers in efficiency.
- The transparency mechanism successfully recovers interpretable feature clusters from learned representations.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Individual Treatment Effects (ITE) can be accurately estimated in longitudinal data even when adjustment variables are missing, by learning a latent "substitute" for them.
- **Mechanism:** A Causal Dynamic Variational Autoencoder (CDVAE) infers a static latent variable Z as a proxy for unobserved adjustment variables U. It forces the response series to follow a Conditional Markov Model (CMM) of order p given Z, ensuring sequential ignorability holds when history is augmented with Z.
- **Core assumption:** Unobserved adjustment variables are static (time-invariant) and influence the outcome but not the treatment. The response dynamics have finite memory (CMM(p)).
- **Evidence anchors:** [abstract] "By accounting for such effect modifiers... treatment effects can be estimated more accurately." [section] Section 3.5.1 proves identifiability when Z ~ CMM(p). [corpus] Related work "TV-SurvCaus" addresses time-varying treatments but this targets static missing covariates.
- **Break condition:** If unobserved adjustment variables are time-varying or the response memory is infinite (CMM fails), the substitute Z will not capture necessary causal information.

### Mechanism 2
- **Claim:** Counterfactual responses over long time horizons can be predicted efficiently using contrastive learning on recurrent representations.
- **Mechanism:** Causal CPC uses a GRU to encode history and maximizes the InfoNCE lower bound on mutual information between context and future frames, forcing the representation to capture global long-term structure. It simultaneously minimizes the CLUB upper bound on mutual information between the representation and treatment to ensure balanced representations.
- **Core assumption:** Treatment effects have a "lagged" effect (Assumption 4.1), and sequential ignorability holds.
- **Evidence anchors:** [abstract] "Causal CPC leverages temporal dynamics through contrastive predictive coding... achieving near-oracle performance." [section] Section 4.5.1 details using InfoNCE to maximize MI between context C_t and future F_{t+j}.
- **Break condition:** If the underlying data dynamics are not stationary or if the history/future split does not preserve the causal structure needed for InfoNCE, the representation will fail to predict long-term dependencies.

### Mechanism 3
- **Claim:** The generative process of a Causal Representation Learning (CRL) model can be made transparent by recovering clusters of observed variables without assuming "anchor features."
- **Mechanism:** Apply Sparse Subspace Clustering (SSC) on the Jacobian of the decoder. By assuming the Jacobian columns lie in a union of subspaces, solving a self-expression problem (Df ≈ DfC) yields a matrix C* where non-zero entries indicate shared latent parents. Spectral clustering on C* recovers the feature groupings.
- **Core assumption:** The decoder is smooth (L-smooth), latent variables are sub-Gaussian, and the subspaces induced by the Jacobian are sufficiently distinct (incoherent).
- **Evidence anchors:** [abstract] "Under mild assumptions, the learned substitutes account for all relevant adjustment variables... we show that the ACATE leveraging the substitutes is identifiable." [section] Section 5.5 guarantees subspace detection properties.
- **Break condition:** If the Jacobian columns are randomly distributed (no subspace structure) or if the subspaces overlap excessively (high affinity), the SSC algorithm will fail to separate the features.

## Foundational Learning

- **Concept:** Potential Outcomes (PO) Framework & Sequential Ignorability
  - **Why needed here:** The entire thesis frames causal inference using PO (Y(1), Y(0)). "Sequential Ignorability" is the critical assumption that treatment is independent of potential outcomes given history, allowing identification of causal effects from observational data.
  - **Quick check question:** If a treatment at time t depends on a future outcome Y_{t+1}, is sequential ignorability violated?

- **Concept:** Variational Autoencoders (VAEs) & ELBO (Evidence Lower Bound)
  - **Why needed here:** CDVAE is built on VAE principles. Understanding the trade-off between reconstruction loss and KL divergence, and the concept of the "near-deterministic regime," is essential for understanding how CDVAE avoids posterior collapse.
  - **Quick check question:** What happens to the KL term in the ELBO if the decoder variance σ goes to zero? (See Theorem 3.4/3.5).

- **Concept:** Contrastive Learning (InfoNCE) & Mutual Information
  - **Why needed here:** The "Causal CPC" model relies on Contrastive Predictive Coding. The InfoNCE loss is a lower bound on mutual information. Understanding how maximizing this bound forces the model to capture "global structure" is key to Mechanism 2.
  - **Quick check question:** Why is InfoNCE often preferred over other MI estimators (like MINE or NWJ) for high-dimensional sequential data?

## Architecture Onboarding

- **Component map:**
  - **Chapter 3 (CDVAE):** Input (History H_t) -> Encoder (GRU) -> Substitute Z (GMM prior) -> Decoder (Outcome Y + Propensity W heads). Loss = Weighted ELBO (reconstruction + KL) + IPM (Wasserstein distance for balance) + Distance Match (posterior stationarity).
  - **Chapter 4 (Causal CPC):** Input (History Frames F_t) -> Local Encoder -> Context Encoder (GRU) -> Predictor/Decoder. Loss = InfoNCE (CPC) + InfoMax (Reconstruction) + CLUB (Adversarial balancing) + Outcome NLL.
  - **Chapter 5 (Transparency):** Decoder f -> Jacobian Df -> Self-Expression Matrix C (Lasso optimization) -> Affinity Matrix A -> Spectral Clustering.

- **Critical path:**
  1. **CDVAE:** Ensure the Weighted ELBO is implemented correctly (Equation 3.13/3.18). The "near-deterministic regime" depends on learnable variance σ.
  2. **Causal CPC:** The InfoNCE loss is the driver for long-term dependencies. The CLUB loss is critical for ensuring balanced representations (treatment invariance).
  3. **Transparency:** The Self-Expression optimization problem (Equation 5.8) is the core algorithmic step; efficient implementation is needed.

- **Design tradeoffs:**
  - **CDVAE:** Static adjustment variables allow for simple GMM priors but fail for time-varying confounders.
  - **Causal CPC:** GRUs are computationally efficient but rely on contrastive pre-training to capture global structure; less expressive than Transformers but faster.
  - **Transparency:** Self-expression is computationally expensive (O(d_x^3) for the affinity matrix) but provides formal identifiability guarantees unavailable in anchor-based methods.

- **Failure signatures:**
  - **CDVAE:** "Posterior collapse" (KL term dominates) or "Bad variables" (M-colliders) being captured by the latent substitute Z.
  - **Causal CPC:** "Subspace detection failure" (Theorem 5.2 condition not met) -> poor clustering. "Gradient variance explosion" if InfoNCE is not used.
  - **Transparency:** If the Jacobian is noisy or subspaces are not incoherent, the affinity matrix A will be dense/noisy, leading to poor cluster recovery.

- **First 3 experiments:**
  1. Run CDVAE on MIMIC-III semi-synthetic data (Chapter 3): Compare "base" approach (no substitute) vs. "substitute" approach (with latent Z) vs. "oracle" (true U). Check if substitute approach bridges the gap to oracle performance.
  2. Run Causal CPC on Tumor Growth simulation (Chapter 4): Measure prediction error (NRMSE) at long horizons (ρ=10) against "Causal Transformer" baseline. Verify that Causal CPC maintains lower error and higher efficiency.
  3. Run Transparency Analysis on Saint-Gobain data (Chapter 5): Compute NMI/Homogeneity of the recovered clusters against the ground-truth product categories. Compare against a "Random Clustering" baseline.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can Bayesian causal discovery methods be extended to quantify epistemic uncertainty over latent causal representations specifically within nonlinear and time-varying contexts? Basis: The Epilogue states little work has been done on assessing uncertainty in CRL frameworks, particularly in nonlinear settings and/or temporally evolving structures.

- **Open Question 2:** Under what specific conditions of invariance or equivariance does the Augmented Conditional Average Treatment Effect (ACATE) estimated using a learned substitute Z exactly coincide with the ACATE estimated using the true unobserved adjustment variables U? Basis: Section 6.2 asks when the CATE augmented with Z and that with U coincide, noting that identifying generative factors often allows for trivial ambiguities.

- **Open Question 3:** How can the transparency framework based on modular Jacobian sparsity be adapted to handle dynamic or transient latent structures, such as seasonal shifts or regime changes, in longitudinal data? Basis: The Discussion in Chapter 5 notes that the current approach assumes a "static partitioning of features," whereas "feature associations often shift over time due to seasonality."

## Limitations

- The core assumption that unobserved adjustment variables are static is critical but potentially limiting for real-world applications where confounders may vary over time.
- The transparency mechanism depends on specific Jacobian subspace structures that may not generalize across all data distributions.
- Scalability of the transparency mechanism (SSC on Jacobian) for high-dimensional data needs further validation.

## Confidence

- **High Confidence:** The identifiability proof for Augmented CATE (Theorem 3.1) and the contrastive learning framework (InfoNCE/CLUB objectives) are mathematically rigorous and well-supported.
- **Medium Confidence:** The empirical results on synthetic and semi-synthetic data demonstrate the approach works in controlled settings, but generalization to diverse real-world scenarios needs further validation.
- **Low Confidence:** The scalability of the transparency mechanism (SSC on Jacobian) for high-dimensional data and the robustness of the CDVAE approach when the static assumption is violated.

## Next Checks

1. **Robustness Test:** Evaluate CDVAE performance when the static adjustment variable assumption is violated by introducing time-varying confounders in synthetic data. Measure how quickly identifiability breaks down.

2. **Scalability Analysis:** Benchmark the transparency mechanism on increasingly high-dimensional datasets (e.g., scaling from 50 to 500 features) to assess computational feasibility and clustering quality degradation.

3. **Real-world Application:** Apply the complete framework (CDVAE + Causal CPC + Transparency) to a new longitudinal dataset outside healthcare (e.g., educational or economic data) to test domain generalizability.