---
ver: rpa2
title: Liouville PDE-based sliced-Wasserstein flow for fair regression
arxiv_id: '2505.17204'
source_url: https://arxiv.org/abs/2505.17204
tags:
- liouville
- pde-based
- barycenter
- wasserstein
- health
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work transforms the Fokker-Planck equation-based sliced Wasserstein
  flow (SWF) into a Liouville PDE-based transport with density estimation via normalizing
  flows, eliminating the stochastic diffusive term. The method improves SWF convergence
  and reduces variance, particularly in high-dimensional settings.
---

# Liouville PDE-based sliced-Wasserstein flow for fair regression

## Quick Facts
- arXiv ID: 2505.17204
- Source URL: https://arxiv.org/abs/2505.17204
- Reference count: 40
- Primary result: Transforms stochastic Fokker-Planck SWF into deterministic Liouville PDE-based transport with Neural ODE density estimation, reducing variance and improving fairness-accuracy Pareto trade-offs in high-dimensional settings.

## Executive Summary
This work addresses fair regression by computing Wasserstein barycenters of sensitive attribute-conditioned distributions using a novel Liouville PDE-based sliced-Wasserstein flow. The key innovation transforms the stochastic Fokker-Planck equation into deterministic transport by absorbing the diffusion term into a drift velocity estimated via normalizing flows. Experiments on synthetic and real-world datasets demonstrate improved convergence, reduced variance, and better Pareto trade-offs between demographic parity (KS distance) and predictive accuracy (MSE) compared to vanilla SWF.

## Method Summary
The method computes fair regression by finding the Wasserstein barycenter of conditional distributions (partitioned by sensitive attributes) through sliced-Wasserstein gradient flow. It replaces the standard Fokker-Planck formulation with a Liouville PDE that eliminates the stochastic diffusion term. A Neural ODE estimates the evolving log-density to compute the drift velocity, which combines Kantorovich potentials from each sensitive attribute group. The approach leverages the equivalence between demographic parity and Wasserstein barycenters, computing the barycenter via superposition of individual group flows rather than expensive multi-marginal transport.

## Key Results
- Liouville-SWF achieves lower variance in gradient flow convergence compared to vanilla SWF on synthetic GMM barycenter tasks
- Pareto curves show Liouville-SWF dominates vanilla SWF, achieving lower MSE for equivalent fairness (KS distance) on Communities and Crime and Health Care Spending Cost datasets
- Performance is particularly strong when exact Wasserstein barycenters are computationally intractable, demonstrating scalability to high-dimensional data
- Regularization parameter λ controls the fairness-accuracy trade-off, with excessive regularization degrading predictive performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Replacing stochastic Fokker-Planck diffusion with deterministic Liouville transport reduces variance in gradient flow convergence
- **Mechanism:** Standard SWF uses Fokker-Planck equation with stochastic diffusive term (λΔρ). This method reformulates as Liouville PDE, integrating Neural ODE to estimate log-density (∇logρ), absorbing stochastic term into deterministic drift velocity (f*)
- **Core assumption:** Neural ODE can accurately estimate evolving density ρ and divergence of velocity field (∇·f*) without significant approximation error
- **Evidence anchors:** Abstract states "transforms... eliminating the stochastic diffusive term"; Section 4.1 shows derivation of modified drift term f* including score function ∇logρ
- **Break condition:** In extremely low dimensions or with very small particle count N, Neural ODE overhead may outweigh variance reduction benefits

### Mechanism 2
- **Claim:** Fair regression target (Wasserstein barycenter) computed via linear superposition of individual group-specific gradient flows
- **Mechanism:** Avoids full multi-marginal optimal transport by aggregating "push" forces (Kantorovich potentials) from each sensitive attribute group onto common particles. Velocity field ṽ is weighted sum of individual potentials
- **Core assumption:** Sliced Wasserstein distance (integrating over random 1D projections) provides sufficient approximation of true Wasserstein geometry for high-dimensional barycenter finding
- **Evidence anchors:** Section 5.1 derives drift term vt as superposition of potentials; Section 5.3 constructs velocity ṽ as sum over sensitive attributes
- **Break condition:** If distributions for different sensitive attributes are multi-modal and geometrically distant, simple linear superposition may fail to resolve topology, causing particle "tearing"

### Mechanism 3
- **Claim:** Strong demographic parity equivalent to moving all group-conditional distributions to common geometric center (barycenter)
- **Mechanism:** Fairness requires P(Ŷ|S=a) = P(Ŷ|S=ā). Leverages Theorem 1 (Chzhen et al. 2020) proving optimal fair regressor is transport map to Wasserstein barycenter of conditional distributions. Liouville-SWF efficiently generates this map
- **Core assumption:** Minimizing Wasserstein-2 distance to barycenter preserves predictive accuracy (MSE) better than other fairness interventions when regularization λ is properly tuned
- **Evidence anchors:** Section 3.5 cites Theorem 1 linking fair regression minimization to Wasserstein barycenters; Section 6.4 Figure 3 shows Pareto curves where Liouville-SWF achieves lower MSE for given KS distance
- **Break condition:** If initial regression model f* is fundamentally biased in way that correlates perfectly with sensitive attribute, transport map to barycenter may destroy all signal, resulting in trivial (constant) predictor

## Foundational Learning

- **Concept: Fokker-Planck vs. Liouville Equations**
  - **Why needed here:** Core contribution transforms *stochastic* Fokker-Planck equation (describes probability density evolution with diffusion) into *deterministic* Liouville equation (conservation of probability along flow lines)
  - **Quick check question:** If you remove diffusion term Δρ from PDE, does solution become more random or more deterministic?

- **Concept: Kantorovich Potentials**
  - **Why needed here:** These potentials act as "forces" that push particles from source to target. Gradient of these potentials determines velocity field ṽ
  - **Quick check question:** In optimal transport, is Kantorovich potential ψ function that defines cost of moving mass, or map of where mass moves to? (Hint: Relates to cost, but gradient ∇ψ defines map T = x - ∇ψ)

- **Concept: Neural ODEs (Continuous Normalizing Flows)**
  - **Why needed here:** Method uses Neural ODE not just to move particles, but to compute *log-density* (logρ) via "adjoint" method. Density required to calculate drift term in Liouville formulation (Eq. 20)
  - **Quick check question:** Why is calculating ρ hard in normalizing flow? (Answer: Usually requires invertibility; Neural ODEs solve this via instantaneous change of variables formula)

## Architecture Onboarding

- **Component map:** Data Loader -> Quantile Estimator -> Liouville Solver (Neural ODE + Velocity Field + Integrator) -> Evaluator
- **Critical path:** Density estimation step (Eq. 21). Velocity ṽ depends on ∇logρ. If Neural ODE fails to converge or estimate density accurately, drift term will be incorrect and particles won't flow to barycenter
- **Design tradeoffs:**
  - Exact vs. Sliced: Uses Sliced Wasserstein (integrating over 1D projections) for scalability (O(N log N)) at cost of approximation error vs exact Wasserstein (O(N³))
  - Regularization λ: Controls entropy term. High λ smooths distribution (better fairness/stability) but risks over-regularization (destroying signal/MSE). Low λ preserves signal but risks instability
  - Projections Nθ: More projections improve Sliced Wasserstein approximation but linearly increase compute time per step
- **Failure signatures:**
  - Mode Collapse: If Neural ODE density estimation fails, particles may collapse to single point (low variance, high MSE)
  - Divergence: If step size h too large relative to velocity ṽ, particle positions may explode (NaN values)
  - Over-regularization: Seen in Fig S1-S4 at λ=1.0; MSE spikes (e.g., to ~250-300) indicating model prioritizing smoothness over predictive accuracy
- **First 3 experiments:**
  1. 2D Visual Validation (Case Study 1): Run flow on 2D Gaussian Mixture Model (GMM). Plot particles at t₀, t_mid, t_end. Verify they move from source to target shape without "smearing" (which would indicate excessive diffusion)
  2. Ablation on Density Estimation: Run Liouville-SWF against version where ∇logρ is removed (reverting closer to vanilla SWF). Check variance of loss function over 5 seeds to quantify specific variance reduction claimed
  3. Pareto Curve Sweep (Case Study 3): Train on Communities and Crime dataset. Sweep λ ∈ [10⁻⁴, 10⁻¹]. Plot KS vs. MSE. Confirm Liouville-SWF dominates vanilla SWF on Pareto frontier (lower left corner)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What are formal theoretical guarantees regarding stability and convergence rates of Liouville PDE-based sliced-Wasserstein flow compared to original Fokker-Planck formulation?
- **Basis in paper:** [explicit] Conclusion states "theoretical analysis of stability and convergence of Liouville PDE-based SWF remains for the future"
- **Why unresolved:** While empirical results show reduced variance and loss, paper currently relies on theoretical framework of original SWF without providing new proofs for deterministic Liouville transport dynamics
- **What evidence would resolve it:** Formal proof showing deterministic Liouville PDE-based update scheme converges to target distribution (or barycenter) with specific bounds on convergence rate relative to step size h and neural ODE approximation error

### Open Question 2
- **Question:** Does performance improvement of Liouville PDE-based SWF generalize to significantly higher-dimensional distributions or different data modalities beyond tested tabular datasets?
- **Basis in paper:** [explicit] Conclusion notes "competence of Liouville PDE-based improvement in convergence and reduction in variance needs more exploration from diverse datasets" and mentions current tests limited to dimensions around 60 and 120
- **Why unresolved:** Paper claims to overcome "curse of dimensionality" but experiments restricted to synthetic GMMs and two specific real-world tabular datasets (Crime and Health Care)
- **What evidence would resolve it:** Benchmarking method on high-dimensional data (e.g., images with d > 1000) to verify if reduced variance and improved Pareto curves persist when sliced approximation becomes less representative of full Wasserstein distance

### Open Question 3
- **Question:** Can regularization parameter λ be adapted dynamically to mitigate performance degradation (over-regularization) observed at higher fixed values?
- **Basis in paper:** [inferred] Section 6.4 notes "strong regularization (λ=1) leads to pronounced degradation in MSE... reflecting over-regularization" yet method currently relies on fixed λ chosen via grid search
- **Why unresolved:** Results indicate sensitive trade-off where λ controls entropy but requires manual tuning to balance fairness (KS) and accuracy (MSE) without destabilizing regression performance
- **What evidence would resolve it:** Development of adaptive scheduling algorithm for λ or theoretical criterion for selecting λ based on current divergence from target distribution, demonstrating stable convergence without manual hyperparameter search

## Limitations

- Theoretical convergence guarantees for the Liouville PDE formulation remain unproven compared to the original Fokker-Planck framework
- Empirical validation is limited to specific datasets and synthetic examples, with sensitivity to hyperparameters largely unexplored
- Performance on datasets with non-linear or multi-modal sensitive attribute distributions is not demonstrated
- Method's robustness across diverse real-world datasets and sensitivity to Neural ODE architectural choices are not well-characterized

## Confidence

- **High Confidence:** Theoretical derivation of Liouville PDE formulation and its relationship to Fokker-Planck is sound (Mechanism 1). Connection between Wasserstein barycenters and demographic parity is well-established (Mechanism 3)
- **Medium Confidence:** Practical implementation of superposition mechanism for computing barycenters appears correct but empirical validation is limited (Mechanism 2). Claim of improved Pareto trade-offs is supported but needs broader validation
- **Low Confidence:** General robustness of method across diverse real-world datasets and sensitivity to architectural choices for Neural ODE are not well-characterized

## Next Checks

1. **Architecture Sensitivity Analysis:** Systematically vary Neural ODE depth, width, and activation functions to quantify impact on convergence and final fairness-accuracy trade-offs
2. **Dataset Diversity Test:** Apply method to additional fairness datasets (e.g., Adult Income, COMPAS) with different sensitive attribute structures to test generalizability beyond current examples
3. **Baseline Comparison Expansion:** Compare against more recent fairness methods (e.g., adversarial debiasing, post-processing techniques) on same Pareto frontier to establish relative performance in broader literature