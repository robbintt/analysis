---
ver: rpa2
title: Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using
  Knowledge Distillation
arxiv_id: '2507.00055'
source_url: https://arxiv.org/abs/2507.00055
tags:
- speech
- data
- emotion
- distillation
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation

## Quick Facts
- arXiv ID: 2507.00055
- Source URL: https://arxiv.org/abs/2507.00055
- Reference count: 0
- Key outcome: Student model achieves 66.47% UAR on RAVDESS and 58.42% UAR on CREMA-D, outperforming supervised baselines by leveraging unlabeled audio-visual data via multimodal knowledge distillation

## Executive Summary
This paper introduces LiSER, a knowledge distillation framework for speech emotion recognition that leverages unlabeled audio-visual data to train lightweight student models. The system uses two pre-trained teacher models (HuBERT for audio, S2D for video) to generate soft labels on an unlabeled dataset (MSP-Face), which a CNN-LSTM student then learns from via distillation. Experiments show that the student model outperforms supervised baselines on two emotion recognition benchmarks (RAVDESS and CREMA-D) while using only a fraction of the labeled data.

## Method Summary
LiSER employs multimodal knowledge distillation where a student CNN-LSTM model learns from two teacher models (HuBERT-base for audio and S2D for video) on unlabeled audio-visual data. The student receives 3-second log Mel-spectrogram inputs and is trained using a weighted combination of supervised cross-entropy loss on labeled data and mean absolute error (MAE) distillation loss on unlabeled data. The framework uses confidence-weighted instance-level weighting for the distillation loss, where samples with higher teacher prediction confidence contribute more to the loss. The model is trained jointly on both labeled and unlabeled data rather than in separate stages.

## Key Results
- Student model achieves 66.47% UAR on RAVDESS (vs 57.81% supervised baseline) and 58.42% UAR on CREMA-D (vs 55.81% supervised baseline)
- Using 50% labeled data with distillation outperforms using 100% labeled data without distillation
- MAE distillation loss performs better than cross-entropy for distillation, especially with noisy teacher predictions
- Instance-level confidence weighting improves RAVDESS performance by 15.09% but slightly degrades CREMA-D performance by 4.9%

## Why This Works (Mechanism)

### Mechanism 1: Cross-Modal Distillation from Visual to Audio
If a student model is trained to align its audio-based emotion predictions with the softmax outputs of a visual "teacher" model, it may learn to recognize audio features correlated with facial expressions without explicit video inputs during inference. The student minimizes the divergence between its prediction (from audio) and the teacher's prediction (from video). By minimizing Mean Absolute Error (MAE) between these distributions, the student effectively learns a joint audio-visual manifold, allowing the audio encoder to capture features indicative of emotion that are visible in the face.

### Mechanism 2: Semi-Supervised Data Efficiency
Leveraging unlabeled data via distillation allows the system to maintain high accuracy while reducing reliance on expensive manual annotations. The framework utilizes a large corpus of unlabeled audio-visual data (MSP-Face). By treating teacher predictions as "soft labels" or targets, the student model learns robust feature representations from the underlying data distribution, effectively performing data augmentation at the feature level.

### Mechanism 3: Instance-Level Confidence Weighting
Weighting the distillation loss by the teacher's prediction confidence may improve student performance by filtering out noisy or ambiguous pseudo-labels. The loss function scales the contribution of each unlabeled sample by the maximum softmax probability of the teacher. High-confidence predictions (e.g., clear smiles or angry shouts) influence the student more than ambiguous ones.

## Foundational Learning

- **Knowledge Distillation (KD)**: The entire LiSER architecture depends on transferring "dark knowledge" (soft labels) from large models (HuBERT, S2D) to a small student (CNN-LSTM). Without understanding KD, the rationale for matching soft outputs instead of hard labels is lost.
  - *Quick check question*: Why does the student minimize the difference between softmax distributions (e.g., using MAE) rather than just predicting the hard class label?

- **Self-Supervised Learning (SSL) Representations**: The teacher models (HuBERT, S2D) are not trained from scratch on emotion; they are pre-trained on massive general datasets (LibriSpeech, FERV39k). Understanding that these models provide robust general features is key to understanding why they can label unlabeled data effectively.
  - *Quick check question*: What pre-training datasets and objectives are used for the HuBERT and S2D teacher models?

- **Multimodal Correlation (Audio-Visual)**: The paper assumes a coupling between voice and face. The system works because emotional states generally manifest simultaneously in both modalities.
  - *Quick check question*: Does the student model require video input during inference (deployment)?

## Architecture Onboarding

- **Component map**: Log Mel-spectrogram (3s) -> CNN-LSTM Student -> Supervised Head + Speech Distill Head + Video Distill Head <- HuBERT Teacher (audio) and S2D Teacher (video)

- **Critical path**:
  1. Extract faces from MSP-Face and pre-compute S2D logits
  2. Pass unlabeled audio/video to teachers to generate soft targets
  3. Pass log Mel-spectrogram to Student CNN-LSTM
  4. Compute Weighted Sum of Supervised CE (labeled) and MAE (unlabeled distillation)
  5. Use only supervised head at inference

- **Design tradeoffs**:
  - MAE vs. CE for Distillation: Paper uses MAE (L1) for distillation loss rather than CE (Cross-Entropy). Evidence suggests MAE is more robust to noisy labels/uncertainty in teacher predictions.
  - Training Strategy: The paper argues for joint training (supervised + distillation) rather than a two-stage "pre-train then fine-tune" approach (Table 3 shows two-stage performs worse).

- **Failure signatures**:
  - Confidence Weighting Failure: On CREMA-D, confidence weighting degraded performance compared to simple joint distillation. Hypothesis: Teacher calibration on CREMA-D features may be poor, making confidence a misleading proxy for accuracy.
  - Modality Mismatch: If the video teacher fails to detect a face (S2D requires face sequences), the video distillation path fails.

- **First 3 experiments**:
  1. **Sanity Check (no-dstl)**: Train the student model using only the labeled RAVDESS/CREMA-D data with standard Cross-Entropy to establish a performance floor.
  2. **Uni-modal Ablation**: Train two separate models—one distilling from HuBERT only and one from S2D only—to isolate the contribution of visual vs. audio knowledge.
  3. **Loss Function Validation**: Compare MAE vs. Cross-Entropy as the distillation loss function on the validation set to confirm the paper's finding that MAE is more robust to the noise of "in-the-wild" pseudo-labels.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does the instance-level confidence weighting mechanism improve performance on RAVDESS but cause a performance drop on CREMA-D compared to the fixed-weight baseline?
- Basis in paper: The authors report in Section 3.6 and Table 2 that the confidence-enhanced loss improves UAR on RAVDESS but reduces performance relative to the baseline on CREMA-D (0.584 to 0.578).
- Why unresolved: The paper documents the empirical inconsistency but does not analyze the specific characteristics of the CREMA-D dataset or teacher predictions that cause the confidence weights to fail in that context.
- What evidence would resolve it: A comparative analysis of the teacher models' prediction entropy and signal-to-noise ratios on both datasets to determine if CREMA-D contains ambiguous samples where "confidence" is a misleading proxy for accuracy.

### Open Question 2
- Question: Does the temporal averaging of video teacher outputs over 3-second windows limit the student model's ability to learn short-term or transient emotional dynamics?
- Basis in paper: Section 3.4 notes that to align with the student's input, the video teacher's 0.5s clip predictions are averaged over a 3s window, which may smooth over rapid facial expression changes that occur in shorter intervals.
- Why unresolved: The evaluation uses aggregate metrics (UAR/WAR) over the full signal, leaving the impact on temporal precision or the detection of brief emotional shifts unexplored.
- What evidence would resolve it: An experiment evaluating the model on segment-level emotion recognition or measuring the correlation between the duration of emotional peaks and the model's prediction accuracy.

### Open Question 3
- Question: To what extent does the discrepancy in emotion taxonomies between the video teacher (S2D) and the target datasets hinder the transferability of knowledge?
- Basis in paper: Section 2.3 defines $K_S$ (target emotion classes) and $K_V$ (teacher emotion classes) as distinct output dimensions, implying the student learns a representation based on the teacher's taxonomy (e.g., 7 classes) to solve a different task (e.g., 8 classes).
- Why unresolved: The paper does not analyze if the semantic gap between the teacher's classes (FERV39k) and student's classes (RAVDESS/CREMA-D) introduces noise or conflicting gradients during the distillation process.
- What evidence would resolve it: An ablation study using a video teacher trained on a taxonomy aligned with the target dataset, or an analysis of the activation mappings between the two output heads.

## Limitations
- Teacher model calibration: The S2D video teacher is pre-trained on FERV39k, which may not perfectly align with emotional expressions in MSP-Face, making confidence weighting dataset-dependent.
- Domain shift in unlabeled data: MSP-Face contains "in-the-wild" data that may differ significantly from controlled RAVDESS/CREMA-D datasets, with validation limited to these specific benchmarks.
- Implementation details: Critical hyperparameters for LoRA fine-tuning of HuBERT and exact S2D model architecture/weights are not fully specified, potentially affecting reproducibility.

## Confidence
- **High Confidence**: The core mechanism of multimodal knowledge distillation from HuBERT and S2D to a CNN-LSTM student is well-supported by experimental results showing consistent UAR improvements across both RAVDESS and CREMA-D benchmarks.
- **Medium Confidence**: The efficiency claims regarding reduced dependence on labeled data are supported by ablation showing 50% labeled data with distillation outperforming 100% labeled data, but absolute magnitude may vary with different datasets.
- **Medium Confidence**: The confidence weighting mechanism shows mixed results (15.09% improvement on RAVDESS but 4.9% degradation on CREMA-D), indicating it is effective but sensitive to dataset characteristics and teacher calibration.

## Next Checks
1. **Teacher Calibration Analysis**: Compute the correlation between teacher softmax confidence scores and actual accuracy on a held-out validation set from MSP-Face to validate whether confidence weighting is statistically justified for this specific teacher-student pair.
2. **Domain Generalization Test**: Evaluate the trained LiSER model on an out-of-domain speech emotion dataset (e.g., IEMOCAP or a non-English dataset) to assess whether the distillation from unlabeled data provides genuine generalization benefits beyond benchmark memorization.
3. **Two-Stage vs. Joint Training Validation**: Conduct a controlled experiment comparing the paper's joint training approach against a two-stage "pre-train on unlabeled then fine-tune on labeled" strategy to definitively confirm which training paradigm yields superior performance and under what conditions.