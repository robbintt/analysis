---
ver: rpa2
title: 'FAME: Adaptive Functional Attention with Expert Routing for Function-on-Function
  Regression'
arxiv_id: '2510.00621'
source_url: https://arxiv.org/abs/2510.00621
tags:
- functional
- data
- fame
- should
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FAME introduces an end-to-end functional regression framework that
  directly operates on irregularly sampled continuous functions without predefined
  bases or discretization. It uses bidirectional neural controlled differential equations
  with continuous attention to capture intra-functional continuity and multi-head
  cross attention to model inter-functional dependencies.
---

# FAME: Adaptive Functional Attention with Expert Routing for Function-on-Function Regression

## Quick Facts
- **arXiv ID:** 2510.00621
- **Source URL:** https://arxiv.org/abs/2510.00621
- **Reference count:** 40
- **Key outcome:** FAME achieves state-of-the-art accuracy and strong robustness to irregular sampling and varying input resolutions, outperforming classical and deep learning baselines in function-on-function regression tasks.

## Executive Summary
FAME introduces an end-to-end functional regression framework that directly operates on irregularly sampled continuous functions without predefined bases or discretization. It uses bidirectional neural controlled differential equations with continuous attention to capture intra-functional continuity and multi-head cross attention to model inter-functional dependencies. A mixture-of-experts routing layer enhances the model's ability to handle feature heterogeneity. Evaluated on both synthetic and real-world datasets, FAME demonstrates superior performance and robustness to irregular sampling patterns.

## Method Summary
FAME employs a bidirectional Neural Controlled Differential Equation (NCDE) encoder with Mixture-of-Experts (MoE) vector fields to process irregularly sampled functional data. The architecture uses continuous attention mechanisms to compute query, key, and value trajectories, followed by multi-head cross-attention to model inter-functional dependencies. An NCDE decoder maps the fused global context back to the output function space. The model is trained using Adam optimizer with learning rate 1e-3 for 100 epochs, employing 2-layer MLPs with Tanh activation for input lifting and vector fields.

## Key Results
- Achieves state-of-the-art accuracy in function-on-function regression tasks
- Demonstrates strong robustness to irregular sampling patterns and varying input resolutions
- Outperforms classical and deep learning baselines on both synthetic and real-world datasets
- Maintains stable performance when evaluated on test grids different from training grids

## Why This Works (Mechanism)

### Mechanism 1: Intra-functional Continuity via Neural Controlled Differential Equations (NCDE)
By mapping discrete observations to continuous latent paths, the model captures the inherent dynamics of a function independent of specific sampling grids. Instead of processing discrete tokens, a Bidirectional NCDE solves a differential equation $dz = f(z)dX$ driven by the input signal $X$, creating a continuous hidden state $Z(t)$.

### Mechanism 2: Feature Heterogeneity via Mixture-of-Experts (MoE) Routing
Routing input functions to specialized vector fields allows the model to adapt to varying scales, frequencies, and noise levels across different function channels. A lightweight router computes weights $\pi$ for $K$ expert vector fields, with the effective dynamics $f(z)$ being a convex combination of these experts.

### Mechanism 3: Inter-functional Dependency Modeling via Continuous Cross-Attention
Modeling interactions between different function channels improves regression accuracy by capturing nonlinear couplings. After generating per-function latent paths, a multi-head cross-attention mechanism allows each function to attend to the others at every time step, merging them into a global context path.

## Foundational Learning

**Concept: Controlled Differential Equations (CDEs)**
- **Why needed here:** FAME uses CDEs to define state evolution where the state changes in response to the input signal, not just time
- **Quick check question:** How does the solution trajectory of a CDE differ from an ODE regarding the input data $X$?

**Concept: Young Integration / 1-Variation**
- **Why needed here:** The paper assumes inputs have finite 1-variation to ensure integrals exist, providing the mathematical foundation for continuous processing of irregular data
- **Quick check question:** Why is the "finite 1-variation" assumption critical for defining the integral $\int f(z)dX$ in this context?

**Concept: Functional Data Analysis (FoFR)**
- **Why needed here:** The model maps $X \to Y$ where both are functions, not scalars; understanding that the "target" is a continuous curve is vital for the decoder setup
- **Quick check question:** In FoFR, what are the input and output spaces $\mathcal{X}$ and $\mathcal{Y}$ defined as?

## Architecture Onboarding

**Component map:** Input Lifting -> MoE-Bi-NCDE Encoder -> Continuous Attention -> Cross-Attention Fusion -> NCDE Decoder

**Critical path:** The Continuous Attention Block. If this integration is implemented incorrectly (e.g., using standard discrete attention approximations), the model loses its robustness to irregular sampling.

**Design tradeoffs:**
- **Expert Count ($K$):** Higher $K$ handles heterogeneity better but increases parameters; paper suggests $K=3\sim5$ is optimal
- **Bi-directionality:** Essential for global context but doubles the CDE solver calls (forward + backward)

**Failure signatures:**
- **Loss Instability:** Violating Lipschitz constraints in vector fields $f_\theta$
- **Grid Sensitivity:** Performance drops significantly when evaluating on points not seen in training
- **Expert Collapse:** Router entropy drops to 0; check routing weights during training

**First 3 experiments:**
1. **Sanity Check:** Train on fixed grids and evaluate on irregular/mixed grids to prove sampling invariance
2. **Ablation (Bi-directionality):** Run "FAME w/o Bi-dir" on a dataset with long-range dependencies to confirm performance degradation
3. **Stress Test (Noise):** Evaluate on varying noise conditions to validate MoE router successfully separates signal from noise experts

## Open Questions the Paper Calls Out

**Open Question 1:** How can FAME be adapted for functional classification tasks?
- **Basis in paper:** The conclusion explicitly lists extending the functional attention principle to "functional classification" as a primary direction for future work
- **Why unresolved:** The current decoder is a Neural CDE designed to generate continuous trajectories, whereas classification requires mapping to discrete labels
- **What evidence would resolve it:** A modified decoder head evaluated on standard functional classification benchmarks showing retained robustness to irregular sampling

**Open Question 2:** Can the framework adaptively switch to simpler models for low-complexity operators to improve efficiency?
- **Basis in paper:** The authors note that "when the underlying operator varies little... simpler linear or low-rank models may offer a more attractive trade-off" in complexity
- **Why unresolved:** FAME currently employs a computationally expensive MoE and attention architecture regardless of the inherent difficulty of the regression task
- **What evidence would resolve it:** A dynamic routing mechanism that detects low-complexity inputs and falls back to linear baselines, demonstrating reduced compute time without accuracy loss

**Open Question 3:** How does computational complexity scale with the number of input functions ($d$) in high-dimensional regimes?
- **Basis in paper:** Table 4 shows FAME is significantly slower than baselines, and Case 7 reveals performance degradation as $d$ increases to only 10
- **Why unresolved:** The cross-attention mechanism involves pairwise interactions between functions, which may become a computational bottleneck as the number of input channels increases significantly
- **What evidence would resolve it:** Stress tests on high-dimensional datasets ($d \geq 50$) to analyze if the architecture requires sparse attention mechanisms to remain viable

## Limitations
- The paper does not specify batch size, exact ODE solver parameters, or the temperature value used for attention normalization
- While the theoretical framework assumes finite 1-variation inputs, real-world data may occasionally violate this assumption
- Claims about computational efficiency relative to discretization-based methods are not quantified

## Confidence

**High confidence:** The core mechanism of using NCDEs for continuous intra-functional modeling is well-supported by both theoretical foundations and empirical results. The MoE routing's effectiveness in handling feature heterogeneity is demonstrated across multiple synthetic and real datasets.

**Medium confidence:** The cross-attention mechanism's contribution is validated, but the specific architectural choices lack sensitivity analysis. The robustness claims for irregular sampling are supported but tested on limited grid variations.

**Low confidence:** The comparison with potential continuous alternatives (beyond mentioned baselines) is absent. The paper's claims about computational efficiency relative to discretization-based methods are not quantified.

## Next Checks
1. **Grid Invariance Test:** Evaluate FAME on synthetic data with increasingly irregular sampling patterns to verify the theoretical continuous property holds in practice
2. **Expert Specialization Analysis:** Visualize and quantify the router's expert assignments across different function types to confirm MoE is learning meaningful specializations
3. **ODE Solver Sensitivity:** Systematically vary the numerical solver's tolerance and step size to determine the minimum precision required for stable training and whether the continuous property is preserved across different implementations