---
ver: rpa2
title: 'CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning'
arxiv_id: '2601.15141'
source_url: https://arxiv.org/abs/2601.15141
tags:
- uni00000013
- arxiv
- uni00000018
- reasoning
- uni00000011
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of execution failures in agentic
  reinforcement learning with parameter-constrained models. It proposes CLEANER, a
  framework that uses the model's intrinsic self-correction capabilities to eliminate
  error-contaminated context directly during data collection.
---

# CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning
## Quick Facts
- arXiv ID: 2601.15141
- Source URL: https://arxiv.org/abs/2601.15141
- Reference count: 9
- Average accuracy gains of 6% on AIME, 3% on GPQA, and 5% on LiveCodeBench using one-third of training steps

## Executive Summary
CLEANER introduces a self-purification framework for agentic reinforcement learning that addresses execution failures by leveraging the model's intrinsic self-correction capabilities. The core innovation, Similarity-Aware Adaptive Rollback (SAAR), identifies and replaces failed actions with successful self-corrections during data collection, producing cleaner trajectories that improve learning efficiency. This approach significantly outperforms baselines on reasoning and code generation benchmarks while requiring fewer training iterations.

## Method Summary
CLEANER proposes an online trajectory purification framework that eliminates error-contaminated context directly during data collection rather than filtering at the batch level. The method employs a similarity-aware adaptive rollback mechanism that retrospectively replaces failed actions with successful self-corrections. During the lookahead phase, the model generates $K$ potential corrections for a failed action, and the SAAR mechanism selects the most appropriate replacement based on semantic similarity between the original and corrected code. This adaptive granularity allows the system to distinguish between minor modifications (deep repairs) and complete strategy changes (shallow repairs), optimizing the balance between preserving useful context and removing erroneous information.

## Key Results
- 6% average accuracy improvement on AIME benchmark over baselines
- 3% accuracy gain on GPQA benchmark while using one-third of training steps
- 5% improvement on LiveCodeBench for code generation tasks
- Performance matches state-of-the-art methods with significantly reduced computational resources

## Why This Works (Mechanism)
The mechanism works by recognizing that traditional RL approaches struggle with error propagation in agentic systems, where a single failed action can contaminate the entire reasoning trajectory. By implementing SAAR, CLEANER captures the model's ability to self-correct during execution, creating purified trajectories that contain only successful reasoning paths. The similarity-aware component ensures that replacements are semantically appropriate, maintaining the integrity of the original reasoning while eliminating execution failures. This approach directly addresses the challenge of parameter-constrained models by making more efficient use of limited training data.

## Foundational Learning
- **Agentic Reinforcement Learning**: Autonomous agents that can interact with environments using tools; needed to understand the context of execution failures in multi-step reasoning tasks; quick check: verify agent can call external tools and receive feedback
- **Trajectory Purification**: The process of cleaning contaminated execution paths; essential for understanding how CLEANER improves training data quality; quick check: confirm that failed trajectories can be identified and separated from successful ones
- **Similarity Metrics for Code**: Using structural similarity (like difflib.SequenceMatcher) to measure semantic equivalence; critical for the adaptive rollback mechanism; quick check: test that similar code snippets produce high similarity scores
- **Online vs. Offline RL**: Online learning during data collection versus post-hoc filtering; important for understanding CLEANER's efficiency advantage; quick check: compare performance differences between online and offline purification approaches
- **Chain-of-Thought Reasoning**: Sequential reasoning processes that can be disrupted by execution failures; fundamental to understanding why trajectory contamination is problematic; quick check: verify that reasoning traces maintain logical coherence
- **Self-Correction Capability**: The model's ability to generate successful corrections to its own failures; core assumption underlying CLEANER's effectiveness; quick check: test model's ability to fix its own code errors within K attempts

## Architecture Onboarding
- **Component Map**: Environment -> Agent Model -> Tool Execution -> Error Detection -> Lookahead Correction -> SAAR Rollback -> Clean Trajectory
- **Critical Path**: (1) Execute action → (2) Detect failure → (3) Generate K corrections → (4) Apply SAAR similarity scoring → (5) Replace with best correction → (6) Store clean trajectory
- **Design Tradeoffs**: Balance between rollback granularity (deep vs. shallow repairs) and computational overhead of lookahead phase; the system must decide when to make minor fixes versus complete strategy changes
- **Failure Signatures**: Execution errors, tool call failures, semantic inconsistencies in reasoning chains; these trigger the SAAR mechanism and determine rollback decisions
- **First Experiment 1**: Test SAAR on a simple code debugging task with known error types to verify similarity scoring works as expected
- **First Experiment 2**: Run CLEANER on a single AIME problem to observe the full pipeline from failure to self-correction to trajectory purification
- **First Experiment 3**: Compare training curves with and without SAAR to measure the impact of trajectory purification on learning efficiency

## Open Questions the Paper Calls Out
- **Open Question 1**: How can discarded erroneous actions be effectively utilized as negative samples in agentic RL without causing training instability? The authors found that applying Online-DPO to failed actions led to training collapse because penalizing tool calls without addressing the preceding Chain-of-Thought reasoning created a disconnect.
- **Open Question 2**: Can the similarity-aware rollback mechanism be adapted for non-code tool environments (e.g., web search, REST APIs) where semantic similarity is harder to quantify? The current implementation relies on code structure using `difflib.SequenceMatcher`, which may not translate to natural language or API interaction logs.
- **Open Question 3**: How does the dependency on intrinsic self-correction capability limit the applicability of CLEANER to base models with weak error-recovery skills? If the model cannot generate successful corrections within K attempts, the mechanism fails to purify trajectories, potentially filtering out difficult problems or retaining noise.

## Limitations
- Effectiveness is constrained by the base model's self-correction capabilities
- Current similarity metric is tailored for code and may not generalize to other domains
- The lookahead phase adds computational overhead during data collection

## Confidence
- **Method novelty**: High - presents a unique approach to trajectory purification
- **Empirical results**: High - shows consistent improvements across multiple benchmarks
- **Generalizability**: Medium - primarily demonstrated on code-related tasks
- **Scalability**: Medium - computational overhead of lookahead phase needs further evaluation
- **Theoretical foundation**: Medium - practical approach with limited theoretical analysis

## Next Checks
- Verify that SAAR correctly identifies and replaces failures across diverse code error types
- Test the impact of varying K (number of correction attempts) on purification quality and computational cost
- Evaluate CLEANER's performance on non-code reasoning tasks to assess domain generalizability