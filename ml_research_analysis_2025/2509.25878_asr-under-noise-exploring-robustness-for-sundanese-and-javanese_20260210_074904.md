---
ver: rpa2
title: 'ASR Under Noise: Exploring Robustness for Sundanese and Javanese'
arxiv_id: '2509.25878'
source_url: https://arxiv.org/abs/2509.25878
tags:
- clean
- noise
- sundanese
- javanese
- noisetrain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the robustness of Whisper-based ASR models
  for Javanese and Sundanese under noisy conditions. While clean audio performance
  is strong, WER degrades by 2-3x in low-SNR environments without noise-aware training.
---

# ASR Under Noise: Exploring Robustness for Sundanese and Javanese

## Quick Facts
- arXiv ID: 2509.25878
- Source URL: https://arxiv.org/abs/2509.25878
- Reference count: 40
- This work investigates the robustness of Whisper-based ASR models for Javanese and Sundanese under noisy conditions. While clean audio performance is strong, WER degrades by 2-3x in low-SNR environments without noise-aware training. Both SpecAugment and synthetic noise training improve robustness, with NoiseTrain consistently outperforming other methods across models and languages. Error analysis reveals language-specific challenges: Sundanese struggles with vowel confusion and name errors, while Javanese has more digraph and consonant issues. Overall, noise-aware fine-tuning is essential for real-world ASR performance in these languages.

## Executive Summary
This study evaluates Whisper-based ASR models for Javanese and Sundanese under varying noise conditions, comparing clean fine-tuning, SpecAugment, and noise-aware training (NoiseTrain). The experiments show that while Whisper models perform well on clean speech, WER degrades significantly under low-SNR conditions without noise-aware training. NoiseTrain, which uses synthetic noise augmentation, consistently outperforms both clean fine-tuning and SpecAugment across model sizes and languages, particularly for larger Whisper models. Error analysis reveals language-specific patterns: Sundanese exhibits vowel confusion and name errors, while Javanese shows more consonant and diacritic issues. The results demonstrate that synthetic noise training is essential for real-world ASR deployment in these low-resource languages.

## Method Summary
The study fine-tunes four Whisper model variants (Tiny, Medium, Large-v3, Large-v3-Turbo) on OpenSLR corpora for Javanese and Sundanese using three training strategies: clean fine-tuning, SpecAugment with configuration #9, and NoiseTrain with synthetic noise from AudioSet. Models are evaluated on clean and noisy test sets across SNR levels from -20 to 20 dB, measuring WER and CER. The NoiseTrain pipeline mixes clean audio with AudioSet noise using a specific SNR-based scaling formula. The evaluation includes 8 held-out noise classes not seen during training.

## Key Results
- Noise-aware training substantially improves ASR robustness under low-SNR conditions, with WER improvements of 2-3x compared to clean fine-tuning
- NoiseTrain consistently outperforms SpecAugment across all model sizes and both languages
- Larger Whisper models (Large-v3) show better robustness than smaller variants, but all benefit from noise-aware training
- Language-specific error patterns emerge: Sundanese struggles with vowel confusion and name errors, while Javanese has more consonant and diacritic issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Noise-aware training with synthetic noise augmentation substantially improves ASR robustness under low-SNR conditions compared to clean-only fine-tuning.
- Mechanism: Synthetic noise injection during fine-tuning exposes the acoustic model to degraded signal conditions, enabling the network to learn noise-invariant feature representations that reduce word error rates in real-world noisy environments.
- Core assumption: The synthetic noise distribution and SNR sampling sufficiently approximate real-world acoustic conditions encountered during deployment.
- Evidence anchors:
  - [abstract] "...noise-aware training substantially improves robustness, particularly for larger Whisper models."
  - [section] Tables 2 and 3 show WER improvements under low-SNR conditions (–SNR) when using NoiseTrain.
  - [corpus] "When De-noising Hurts" (2512.17562) discusses noise-robustness techniques, suggesting broader relevance but not direct replication.
- Break condition: Performance gains disappear if synthetic noise distribution diverges significantly from deployment noise types or if model capacity is insufficient to learn noise-invariant features.

### Mechanism 2
- Claim: SpecAugment improves robustness by regularizing the model through time and frequency masking, though its effectiveness varies compared to noise-aware training.
- Mechanism: Time warping, frequency masking, and time masking applied to spectrograms during training force the model to learn representations that tolerate localized corruptions, reducing overfitting to clean signal patterns.
- Core assumption: The augmentation distortions adequately simulate the effects of noise and channel variability without corrupting linguistic content.
- Evidence anchors:
  - [section] Appendix A, Table 5 details SpecAugment configurations; configuration #9 performed best.
  - [section] Tables 2 and 3 show SpecAug+Clean reduces WER under –SNR compared to clean fine-tuning.
  - [corpus] No direct corpus papers evaluate SpecAugment in this exact setting; evidence is limited to the current study.
- Break condition: Aggressive masking removes critical phonetic information, harming clean-condition performance without sufficient noise robustness gains.

### Mechanism 3
- Claim: Language-specific phonological characteristics drive distinct error patterns, influencing the effectiveness of robustness interventions.
- Mechanism: Sundanese vowel systems (e.g., e, è, eu) and Javanese consonant digraphs (dh, ng, ny, th) produce different confusion patterns under noise, affecting WER distribution across error types.
- Core assumption: The observed error patterns are intrinsic to the language phonology rather than artifacts of the training corpus.
- Evidence anchors:
  - [section] Table 4 and error analysis describe vowel confusion in Sundanese and consonant/diacritic errors in Javanese.
  - [corpus] No corpus papers provide comparative phonological error analysis for these languages.
- Break condition: Error patterns shift significantly if training data covers additional dialects or spontaneous speech, altering the error distribution.

## Foundational Learning

- Concept: **Signal-to-Noise Ratio (SNR)**
  - Why needed here: SNR quantifies the relative level of background noise to speech, determining the difficulty of the recognition task and the expected WER degradation.
  - Quick check question: Would a negative SNR value indicate more or less noise than a positive SNR value?

- Concept: **Word Error Rate (WER) and Character Error Rate (CER)**
  - Why needed here: WER quantifies word-level transcription accuracy, while CER captures finer-grained edits (vowels, consonants, diacritics), critical for agglutinative languages.
  - Quick check question: If a model inserts an extra space within a word, which error metric would increase more significantly in agglutinative languages?

- Concept: **Fine-tuning vs. Zero-shot Evaluation**
  - Why needed here: Zero-shot performance indicates pre-trained model generalization; fine-tuning adapts the model to low-resource languages, reducing WER.
  - Quick check question: What does high zero-shot WER (>70) suggest about the pre-trained model's exposure to a specific language?

## Architecture Onboarding

- Component map:
  - Pre-trained Whisper models (Tiny, Medium, Large-v3, Large-v3-Turbo) -> OpenSLR dataset -> SpecAugment module or NoiseTrain pipeline -> Evaluation pipeline

- Critical path:
  1. Select Whisper variant and training strategy (clean, SpecAugment, NoiseTrain)
  2. Preprocess OpenSLR data; if NoiseTrain, mix with AudioSet noise at target SNRs
  3. Fine-tune model; if SpecAugment, apply masking during training
  4. Evaluate on clean and noisy test sets; compute WER/CER and error type breakdowns

- Design tradeoffs:
  - Model size vs. robustness: Larger models (Large-v3) achieve lower WER but require more compute; Tiny degrades faster under noise
  - SpecAugment vs. NoiseTrain: NoiseTrain provides stronger low-SNR robustness but requires noise corpus curation; SpecAugment is simpler but less effective in severe noise
  - Synthetic vs. real noise: Synthetic noise enables controlled experiments but may not fully capture real-world variability

- Failure signatures:
  - WER > 200 at –SNR: Model fails to generalize under severe noise; likely insufficient noise-aware training
  - High vowel confusion in Sundanese: Check for e/è/eu confusion; may require targeted augmentation or pronunciation lexicon
  - Diacritic errors in Javanese: Sandhangan swara marks (é, è) often dropped; consider normalizing diacritics or enriching training data

- First 3 experiments:
  1. Baseline fine-tuning: Fine-tune Large-v3 on clean OpenSLR data; evaluate on clean and noisy test sets to establish WER bounds
  2. SpecAugment sweep: Run SpecAugment configurations (light to aggressive) on Medium model; select best config based on validation WER
  3. NoiseTrain evaluation: Train Large-v3 with NoiseTrain using AudioSet noise at SNRs –20 to 20 dB; compare WER curves against clean and SpecAugment baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does dialect-aware fine-tuning significantly improve ASR accuracy for the diverse regional varieties of Javanese and Sundanese compared to the generalized models used in this study?
- Basis in paper: [explicit] The conclusion explicitly states "Future work includes dialect-aware fine-tuning," while the limitations section notes the current data aggregates speakers from limited regions, failing to reflect full dialectal variation.
- Why unresolved: The current experimental setup groups all speakers by language (e.g., Javanese) rather than training or evaluating specific models for distinct varieties like Central vs. Eastern Javanese.
- What evidence would resolve it: A comparative benchmark showing WER reductions when models are fine-tuned and evaluated on region-specific subsets of data (e.g., Priangan Sundanese) versus the current general models.

### Open Question 2
- Question: Can a speech enhancement pre-processing module offer superior robustness in low-SNR environments compared to the synthetic noise injection (NoiseTrain) strategies?
- Basis in paper: [explicit] The authors list "speech enhancement for better real-world robustness" as a specific avenue for future improvements in the conclusion.
- Why unresolved: This study focused exclusively on data augmentation techniques (SpecAugment and synthetic noise mixing) and did not experiment with denoising front-ends or separate signal processing modules.
- What evidence would resolve it: An ablation study comparing the WER of NoiseTrain models against a pipeline that utilizes a dedicated speech enhancement model to clean the audio prior to transcription.

### Open Question 3
- Question: Does the robustness gained from synthetic noise training transfer to complex, non-stationary real-world environments involving conversational overlap and varied recording devices?
- Basis in paper: [inferred] The limitations section states that "noisy conditions are synthetic and cannot fully capture real-world environments such as conversational overlap or varied recording devices."
- Why unresolved: The evaluation used additive noise from AudioSet mixed with clean readings, which simulates background interference but may not replicate the acoustic complexity of "in-the-wild" spontaneous speech.
- What evidence would resolve it: Evaluation of the NoiseTrain models on a "wild" test set comprising spontaneous speech recorded in natural, unconstrained environments (e.g., crowded markets or streets).

## Limitations

- Missing training hyperparameters (learning rate, batch size, epochs, optimizer) prevent exact replication
- Unclear data selection specifics - exact training/test subsets from OpenSLR not specified
- Synthetic noise may not fully capture real-world acoustic complexity including conversational overlap and varied recording devices

## Confidence

- High Confidence: The core finding that noise-aware training improves robustness in low-SNR conditions is well-supported by comparative WER tables across multiple Whisper model sizes and both languages
- Medium Confidence: The claim that NoiseTrain consistently outperforms SpecAugment has strong empirical support but may depend on the specific synthetic noise corpus used
- Low Confidence: Claims about the exact magnitude of WER improvements and the relative effectiveness of different Whisper model sizes under various noise conditions cannot be fully verified without the missing training details

## Next Checks

1. **Hyperparameter Sweep**: Conduct a systematic search over learning rates (1e-5 to 1e-3), batch sizes (8-32), and epochs (10-50) for each training strategy to determine if performance variations stem from optimization choices rather than the methods themselves

2. **Real Noise Validation**: Replace synthetic AudioSet noise with real-world environmental recordings from the target deployment environments. Compare WER improvements between synthetic and real noise training to assess ecological validity

3. **Error Pattern Replication**: Independently verify the language-specific error patterns by conducting a blind error analysis on the noisy test sets. Cross-check whether vowel confusion in Sundanese and consonant errors in Javanese persist across different annotators and conditions