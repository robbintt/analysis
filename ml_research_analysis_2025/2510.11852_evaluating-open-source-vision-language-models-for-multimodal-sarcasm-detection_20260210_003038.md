---
ver: rpa2
title: Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection
arxiv_id: '2510.11852'
source_url: https://arxiv.org/abs/2510.11852
tags:
- sarcasm
- multimodal
- detection
- few-shot
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates seven open-source vision-language models for
  multimodal sarcasm detection using zero-, one-, and few-shot prompting strategies.
  The models (BLIP2, InstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, and Qwen-VL)
  are tested on three benchmark datasets (Muse, MMSD2.0, and SarcNet) for both binary
  classification of sarcastic image-caption pairs and generation of explanations for
  sarcastic instances.
---

# Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection

## Quick Facts
- arXiv ID: 2510.11852
- Source URL: https://arxiv.org/abs/2510.11852
- Reference count: 40
- Primary result: Open-source VLMs achieve only moderate success in multimodal sarcasm detection without task-specific fine-tuning

## Executive Summary
This study evaluates seven open-source vision-language models for multimodal sarcasm detection using zero-, one-, and few-shot prompting strategies. The models are tested on three benchmark datasets for both binary classification of sarcastic image-caption pairs and generation of explanations for sarcastic instances. Results show that while some models achieve moderate classification accuracy (up to 0.73 on MMSD2.0 with one-shot prompting), they struggle to generate high-quality, image-grounded explanations. Few-shot prompting does not improve performance, suggesting current VLMs require more than demonstration-based prompting to capture multimodal irony nuances.

## Method Summary
The researchers evaluated seven open-source vision-language models (BLIP2, InstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, and Qwen-VL) using zero-, one-, and few-shot prompting strategies. Models were tested on three benchmark datasets (Muse, MMSD2.0, and SarcNet) for both binary classification of sarcastic image-caption pairs and generation of explanations for sarcastic instances. The study employed standard classification metrics (accuracy, precision, recall, F1) and explanation quality metrics (RAG precision, BLEU, ROUGE, BERTScore) to evaluate model performance across different prompting conditions.

## Key Results
- Classification accuracy reached up to 0.73 on MMSD2.0 with one-shot prompting for top-performing models like Gemma3 and InstructBLIP
- Few-shot prompting failed to improve performance across all models and datasets, contrary to typical few-shot learning expectations
- A clear divergence emerged between models excelling at classification versus those producing better explanations, with no single model performing well at both tasks

## Why This Works (Mechanism)
The study reveals that current open-source vision-language models struggle with multimodal sarcasm detection due to the complex nature of combining visual and textual irony cues. Sarcasm detection requires understanding subtle contextual relationships between images and captions, which current VLMs cannot fully capture through simple prompting strategies alone. The failure of few-shot prompting suggests that sarcastic patterns are too nuanced and context-dependent to be learned from a few demonstrations, indicating a fundamental limitation in how these models process multimodal irony.

## Foundational Learning
- **Multimodal sarcasm detection**: Understanding irony that emerges from the interaction between visual and textual elements
  - Why needed: Sarcasm often relies on the juxtaposition or contradiction between image and text
  - Quick check: Verify that evaluation datasets contain genuine multimodal sarcastic instances
- **Zero/one/few-shot prompting**: Different approaches to task specification without fine-tuning
  - Why needed: Practical alternative to resource-intensive fine-tuning for new tasks
  - Quick check: Confirm prompt templates are consistent across models and conditions
- **Vision-language model architectures**: Integration of visual and textual processing for joint understanding
  - Why needed: Sarcasm detection requires simultaneous processing of both modalities
  - Quick check: Validate that models can process both image and text inputs effectively
- **Classification vs. explanation generation**: Different model capabilities for different task types
  - Why needed: Highlights that models may excel at recognition but struggle with reasoning
  - Quick check: Compare metrics across classification and generation tasks separately

## Architecture Onboarding
**Component Map:** Image Encoder -> Multimodal Fusion -> Text Encoder -> Output Head
**Critical Path:** Image/Caption Input → Visual Feature Extraction → Multimodal Fusion → Task-Specific Output
**Design Tradeoffs:** Balancing vision and language capabilities while maintaining efficient inference; instruction tuning improves task following but may reduce domain-specific performance
**Failure Signatures:** Poor few-shot performance indicates inability to learn nuanced patterns from limited examples; classification-explanation performance divergence suggests separate reasoning mechanisms
**First Experiments:**
1. Test zero-shot performance with refined prompt engineering to establish baseline capabilities
2. Evaluate model performance on non-sarcastic multimodal pairs to establish false positive rates
3. Compare model outputs with human annotations to identify specific failure modes in sarcasm detection

## Open Questions the Paper Calls Out
None

## Limitations
- Results may underestimate model potential since evaluation used only prompting without task-specific fine-tuning
- Focus on open-source models may not represent full capabilities of proprietary vision-language systems
- Reliance on three benchmark datasets may not capture full diversity of sarcastic expressions across cultural contexts

## Confidence
- **High confidence**: Current open-source VLMs achieve only moderate success in multimodal sarcasm classification, supported by consistent results across multiple models and datasets
- **Medium confidence**: Few-shot prompting fails to improve performance, potentially influenced by prompt engineering quality
- **Medium confidence**: Divergence between classification and explanation capabilities, based on relatively coarse evaluation metrics

## Next Checks
1. Conduct ablation studies on prompt engineering by testing different prompt structures, example selections, and instruction formats to determine whether poor few-shot performance stems from the prompting approach itself
2. Evaluate model performance on additional sarcasm detection datasets that include more diverse cultural contexts, domains, and types of multimodal irony to assess generalizability
3. Implement task-specific fine-tuning on a subset of data to establish performance ceilings and compare against zero/one/few-shot baselines to quantify the gap between prompting-only and fine-tuned approaches