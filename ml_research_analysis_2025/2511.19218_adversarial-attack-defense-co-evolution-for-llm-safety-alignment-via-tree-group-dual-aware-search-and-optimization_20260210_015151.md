---
ver: rpa2
title: Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group
  Dual-Aware Search and Optimization
arxiv_id: '2511.19218'
source_url: https://arxiv.org/abs/2511.19218
tags:
- attack
- jailbreak
- defense
- adversarial
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ACE-Safety, a co-evolutionary framework
  for joint optimization of attack and defense models in large language models (LLMs).
  The framework integrates two key components: GS-MCTS (Group-aware Strategy-guided
  Monte Carlo Tree Search) for efficient jailbreak attack exploration with adversarial
  priors and group-awareness, and AC-TGPO (Adversarial Curriculum Tree-aware Group
  Policy Optimization) for robust joint training through curriculum reinforcement
  learning.'
---

# Adversarial Attack-Defense Co-Evolution for LLM Safety Alignment via Tree-Group Dual-Aware Search and Optimization

## Quick Facts
- arXiv ID: 2511.19218
- Source URL: https://arxiv.org/abs/2511.19218
- Reference count: 36
- Key outcome: Co-evolutionary framework jointly optimizes attack and defense models, achieving higher attack success rates with fewer attempts while maintaining superior defense performance, helpfulness, and responsibility across multiple benchmarks.

## Executive Summary
This paper introduces ACE-Safety, a co-evolutionary framework that jointly optimizes attack and defense models for large language model safety alignment. The framework integrates GS-MCTS for efficient jailbreak attack exploration using adversarial priors and group-awareness, with AC-TGPO for robust joint training through curriculum reinforcement learning. ACE-Safety demonstrates significant improvements over existing methods, achieving higher attack success rates with fewer attempts while maintaining superior defense performance, helpfulness, and responsibility metrics. The approach addresses limitations of isolated attack/defense methods by enabling dynamic adaptation to evolving threats and safeguards through mutual improvement.

## Method Summary
ACE-Safety implements a co-evolutionary closed-loop training system where attack and defense models iteratively improve against each other. The framework uses GS-MCTS (Group-aware Strategy-guided Monte Carlo Tree Search) to explore jailbreak strategies with adversarial priors and group-wise evaluation, collecting jailbroken samples. These samples feed into AC-TGPO (Adversarial Curriculum Tree-aware Group Policy Optimization), which trains both models using tree-aware normalization and curriculum-based adversarial training. The system iterates through four rounds, using updated models as the base for subsequent exploration, creating a dynamic adaptation loop that captures emerging attack patterns while hardening defenses.

## Key Results
- Achieves higher attack success rates with fewer attempts compared to baseline methods
- Maintains superior defense performance with lower over-refusal rates while preserving helpfulness
- Demonstrates consistent improvements across multiple benchmarks including MergedHarm and Malicious-Instruct
- Shows dynamic adaptation capability through iterative co-evolutionary training

## Why This Works (Mechanism)

### Mechanism 1: Group-aware Strategy-guided Monte Carlo Tree Search (GS-MCTS)
- Claim: GS-MCTS improves jailbreak discovery efficiency and robustness by integrating adversarial priors and group-wise evaluation into tree search.
- Mechanism: Uses PUCT algorithm combining mean reward Q(s,a) with prior probability P(s,a), where P(s,a) derives from attack model generation confidence and defense model non-rejection likelihood. Group evaluation generates multiple queries per strategy to mitigate LLM randomness, taking max harmfulness score as node reward.
- Core assumption: LLM generation randomness significantly impacts attack success; averaging over multiple queries and using probabilistic priors can stabilize strategy selection.

### Mechanism 2: Adversarial Curriculum Tree-aware Group Policy Optimization (AC-TGPO)
- Claim: AC-TGPO enables robust mutual improvement through tree-aware normalization with curriculum-based adversarial training.
- Mechanism: Extends GRPO with tree-level normalization weighting samples by node depth and path value. Uses adversarial curriculum with Normal, Asymmetric, and Hard sample sets. Rewards maximize harmfulness/co-relevance for attack and minimize harmfulness/maximize responsibility for defense.
- Core assumption: Progressively challenging samples prevent overfitting and improve generalization; tree-aware normalization ensures fair advantage comparison across the search tree.

### Mechanism 3: Co-Evolutionary Closed-Loop Training
- Claim: Iteratively training attack and defense models against each other creates dynamic adaptation outperforming isolated training.
- Mechanism: Each training round uses updated models from previous round as base for GS-MCTS exploration, collecting new adversarial samples. Creates feedback loop where stronger attacks expose defense weaknesses and hardened defenses force attack evolution.
- Core assumption: Real-world threat landscapes evolve; static training sets cannot capture emerging attack patterns. Mutual improvement possible without destabilizing model capabilities.

## Foundational Learning

- **Monte Carlo Tree Search (MCTS) with UCT**: Why needed here: GS-MCTS extends standard MCTS. You must understand selection, expansion, simulation, and backpropagation, plus how Upper Confidence Bounds (UCT) balance exploration vs. exploitation. Quick check question: Can you explain why UCT adds an exploration bonus to the average reward?

- **Group Relative Policy Optimization (GRPO)**: Why needed here: AC-TGPO builds on GRPO. You need to grasp how it uses group-based reward normalization for advantage estimation instead of a separate value function. Quick check question: How does GRPO compute the advantage for a token sequence given a group of outputs and their rewards?

- **Curriculum Learning**: Why needed here: Framework uses adversarial curriculum (Normal, Asymmetric, Hard samples). Understanding how sample difficulty scheduling aids learning is critical. Quick check question: Why might training on only the hardest samples from the start be less effective than a progressive curriculum?

## Architecture Onboarding

- **Component map**: Attack Model (M_A) -> Defense Model (M_D) -> Judge Model (J_A) -> GS-MCTS Module -> AC-TGPO Module -> M_A/M_D update loop

- **Critical path**: Initialize M_A and M_D from same base LLM → Run GS-MCTS for N_m cycles per query to collect jailbroken node samples → Construct curriculum datasets (Normal, Asymmetric, Hard) → Train M_A and M_D via AC-TGPO using adversarial rewards → Repeat for multiple iterations using updated models as new base

- **Design tradeoffs**: Attack Search Depth vs. Compute (deeper MCTS explores more strategies but increases cost); Group Size (G) vs. Sample Efficiency (larger groups better mitigate randomness but require more calls); Judge Choice (strong judge improves label quality but adds API cost/latency)

- **Failure signatures**: Low ASR despite high search cycles (strategy space may not cover model's vulnerabilities); High over-refusal rate after training (defense model may be overfitting to refusal behavior); Unstable training loss (KL divergence constraint may be too weak, causing policy drift)

- **First 3 experiments**: Validate GS-MCTS Priors (run with/without prior probability term, compare ASR and ANA); Ablate Curriculum Components (train using only Normal, only Asymmetric, only Hard sample sets, compare defense robustness); Iterate on Judge Consistency (evaluate J_A's scoring consistency on fixed set of pairs, calculate inter-rater reliability)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does ACE-Safety perform when the attack model ($M_A$) and defense model ($M_D$) use different LLM backbones or architectures?
- Basis in paper: [explicit] The paper states: "We use the same LLM backbone for both attacker (MA) and defender (MD) models" (Section 4.1), and in Limitations notes that the scope "may not fully capture nuanced adversarial scenarios."
- Why unresolved: The co-evolution dynamics may differ substantially when the two models have different capabilities, tokenizers, or alignment bases.
- What evidence would resolve it: Experiments using cross-architecture pairs (e.g., Llama3 attacker vs. Mistral defender) measuring ASR, defense robustness, and convergence speed.

### Open Question 2
- Question: Can the method generalize to multilingual and culturally diverse adversarial contexts beyond English benchmarks?
- Basis in paper: [explicit] The Limitations section explicitly identifies extending evaluations to "diverse cultural contexts" as a promising avenue for future work to boost generalizability.
- Why unresolved: Jailbreak strategies and safety norms are language- and culture-specific; the strategy taxonomy and reward models are English-centric.
- What evidence would resolve it: Evaluation on multilingual harmful query datasets (e.g., translated HarmBench) with native-language judges to measure cross-lingual transfer and cultural robustness.

### Open Question 3
- Question: Does the fixed strategy action space $\mathcal{A}$ limit the discovery of novel, long-tail jailbreak strategies?
- Basis in paper: [inferred] While the taxonomy covers 8 strategy classes with 40 patterns (Appendix A.7), the paper acknowledges "long-tail diversity" and includes a Dynamic Exploration fallback. The constraint of a predefined space may constrain exploration.
- Why unresolved: The framework may overfit to known strategy categories and fail to adapt to emergent, unanticipated attack paradigms.
- What evidence would resolve it: Ablation studies with an expanded or dynamically grown action space, measuring success on out-of-distribution attack patterns from real-world red-teaming logs.

### Open Question 4
- Question: What are the computational and sample efficiency trade-offs for deploying ACE-Safety in resource-constrained environments?
- Basis in paper: [explicit] The Limitations section lists "resource-constrained environments" as an area for future work to improve generalizability.
- Why unresolved: The method requires significant compute (8× H800 GPUs) and multiple training iterations; feasibility for smaller organizations or edge deployment is unknown.
- What evidence would resolve it: Profiling experiments measuring ASR/defense performance versus compute budget (FLOPs), memory usage, and training data volume, with comparisons to lightweight baselines.

## Limitations
- Relies on same LLM backbone for both attack and defense models, potentially limiting exploration of cross-architecture dynamics
- Computational requirements are significant, requiring 8× H800 GPUs and multiple training iterations
- Strategy taxonomy and judge agent are English-centric, limiting generalizability to multilingual and culturally diverse contexts

## Confidence
- Co-evolutionary framework concept: High confidence - builds on established adversarial training principles with consistent benchmark improvements
- GS-MCTS mechanism: Medium confidence - novel integration of priors and group evaluation, but specific prior probability formulation may be sensitive to model changes
- AC-TGPO mechanism: Medium-Low confidence - shows curriculum benefits but lacks ablation studies isolating tree-aware normalization impact

## Next Checks
1. **Strategy Template Fidelity Test**: Reconstruct all 8 strategy categories with their 40 sub-patterns from Table 9 and validate their jailbreak effectiveness against a baseline defense model. Compare success rates with the paper's reported ANA and ASR values.

2. **Judge Consistency Audit**: Run 100 fixed (query, answer) pairs through the judge agent across multiple sessions. Calculate inter-rater reliability scores and compare against human annotations or a secondary LLM judge to verify the reward signal stability.

3. **Curriculum Ablation Study**: Train three separate defense models using only Normal, only Asymmetric, and only Hard sample sets from the first iteration. Evaluate their defense robustness (ASR-LR) and helpfulness on the Malicious-Instruct OOD set to quantify the curriculum's contribution versus other factors.