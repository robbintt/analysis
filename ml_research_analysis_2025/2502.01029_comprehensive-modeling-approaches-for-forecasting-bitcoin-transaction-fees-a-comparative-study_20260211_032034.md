---
ver: rpa2
title: 'Comprehensive Modeling Approaches for Forecasting Bitcoin Transaction Fees:
  A Comparative Study'
arxiv_id: '2502.01029'
source_url: https://arxiv.org/abs/2502.01029
tags:
- bitcoin
- prediction
- rate
- transaction
- sarimax
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents a systematic evaluation of six predictive models
  for forecasting Bitcoin transaction fees across a 24-hour horizon (144 blocks).
  The analysis integrates comprehensive feature engineering spanning mempool metrics,
  network parameters, and historical fee patterns.
---

# Comprehensive Modeling Approaches for Forecasting Bitcoin Transaction Fees: A Comparative Study

## Quick Facts
- arXiv ID: 2502.01029
- Source URL: https://arxiv.org/abs/2502.01029
- Reference count: 31
- Primary result: SARIMAX outperforms deep learning models on 91-day Bitcoin fee forecasting task

## Executive Summary
This study systematically evaluates six predictive models for forecasting Bitcoin transaction fees over a 24-hour horizon. The analysis reveals that traditional statistical approaches, particularly SARIMAX with daily seasonality, outperform sophisticated deep learning architectures when trained on constrained datasets. Through rigorous cross-validation and independent testing, the research demonstrates that model complexity does not necessarily correlate with prediction accuracy in cryptocurrency fee forecasting. The findings provide empirically-validated guidance for fee-sensitive decision making and highlight critical considerations in model selection based on data availability.

## Method Summary
The research forecasts Bitcoin median transaction fees over 144 blocks (24 hours) using data from Sept 16 - Dec 15, 2024. Six models are compared: SARIMAX (p=2,d=1,q=2 × P=1,D=1,Q=1, s=144), Prophet, Time2Vec, Time2Vec+Attention, TFT, and a Hybrid (SARIMAX + LightGBM). The dataset contains 11,809 records with 23 engineered features across mempool metrics, block information, network parameters, and temporal features. Models are evaluated using 5-fold expanding-window cross-validation and a held-out 144-block test set, with performance measured by MAE, RMSE, and Theil's U statistic.

## Key Results
- SARIMAX achieves superior accuracy with MAE of 1.2462 and RMSE of 1.5859 on the test set
- Deep learning models (Time2Vec, TFT) show comparatively lower predictive power due to data constraints
- Hybrid model combining SARIMAX residuals with LightGBM provides marginal improvements with MAE of 1.3492
- Traditional statistical approaches outperform more complex deep learning architectures on the 91-day dataset

## Why This Works (Mechanism)

### Mechanism 1
Traditional statistical models (SARIMAX) outperform deep learning architectures when forecasting Bitcoin fees in data-constrained environments (short historical windows). SARIMAX explicitly models temporal structures via seasonal differencing and autoregression (s=144 blocks ≈ 24 hours), hard-coding daily periodicity rather than learning it from scratch. This reduces variance error components on small datasets by capturing stable seasonal patterns through linear relationships without massive parameter tuning.

### Mechanism 2
Deep learning models exhibit underfitting on the 91-day dataset due to insufficient training samples relative to model capacity. Architectures like TFT and Time2Vec utilize embedding layers and multi-head attention requiring thousands to millions of weights. With only ~11,800 records, these models struggle to differentiate signal from noise, resorting to smoothing or failing to converge on volatile targets, resulting in higher MAE/RMSE than simple baselines.

### Mechanism 3
A hybrid architecture combining statistical residuals with gradient boosting improves robustness by separating stable linear trends from non-linear errors. The hybrid model first uses SARIMAX to capture baseline seasonality and trend, then passes residuals and original features to LightGBM to correct SARIMAX errors by identifying non-linear interactions in mempool/block features that the linear model missed.

## Foundational Learning

- **Seasonality in Time Series (s=144):** The paper configures SARIMAX with `s=144` because Bitcoin targets 10-minute blocks, creating a daily cycle. Understanding this is critical to why the statistical model works; the model is hard-coded to look for patterns repeating every 24 hours. *Quick check:* If Bitcoin block times sped up to 5 minutes consistently, how would you adjust the SARIMAX `s` parameter to maintain the same daily seasonality modeling?

- **Exogenous Variables (SARIMAX vs ARIMA):** The 'X' in SARIMAX stands for exogenous (external) variables. The paper uses 23 features (mempool size, price, etc.). A learner must understand that the model doesn't just look at past fees (autoregression) but correlates fee changes with current network congestion (mempool). *Quick check:* Why might using `mempool_size_mb` as an exogenous variable create a "data leakage" risk if not carefully time-aligned during feature engineering?

- **The Bias-Variance Tradeoff:** This is the central theoretical explanation for the paper's results. High-complexity models (TFT) have high variance (overfitting potential) and need more data. Low-complexity models (SARIMAX) have higher bias but lower variance. The 91-day dataset forced the team to prefer high-bias/low-variance models. *Quick check:* If you increased the dataset from 91 days to 5 years, which way would the "optimal model complexity" likely shift—towards simpler linear models or deeper transformer architectures?

## Architecture Onboarding

- **Component map:** Bitcoin Core (block/mempool stats) + CoinGecko (Price) -> Deduplication -> Ffill/Bfill (imputation) -> Percentile Clipping (outliers) -> Feature Eng (4 Categories) -> Model Layer (SARIMAX, LightGBM, TFT, Time2Vec) -> 5-fold expanding window CV
- **Critical path:** The exogenous feature alignment. You must ensure that features like `mempool_size_mb` at time $t$ are aligned with `block_median_fee_rate` at time $t$ before prediction, but strictly partitioned so future data isn't used to predict past blocks during training.
- **Design tradeoffs:** Precision vs. Stability (models favor stable trend capturing over capturing every jagged fee spike), Complexity vs. Data (choosing TFT allows capturing complex interactions but fails with <100 days of data; choosing SARIMAX works now but may plateau in performance as data grows).
- **Failure signatures:** Oversmoothing (model outputs a flat line or gentle wave, missing sharp fee spikes), Lagging (predictions simply replicate $t-1$ values), High Theil's U (> 1.0, model is worse than guessing previous block's fee).
- **First 3 experiments:** 1) Baseline Comparison: Implement Naive Drift vs. SARIMAX (p=2,d=1,q=2,s=144). Confirm SARIMAX's Theil's U is < 1.0 on 144-block test set. 2) Ablation on Exogenous Features: Run SARIMAX with only historical fees vs. SARIMAX + `mempool_size_mb` + `bitcoin_price_usd`. Quantify performance gain from adding external data. 3) Hybrid Weight Sensitivity: Test Hybrid model with fixed α=0.5 vs. dynamic EMA-weighted α. Determine if dynamic weighting provides statistically significant improvement over simple average.

## Open Questions the Paper Calls Out
1. Can deep learning architectures (TFT, Time2Vec) outperform statistical baselines when trained on datasets significantly larger than the 91-day window used in this study?
2. Does the integration of network-specific parameters, such as mining difficulty adjustments and block size dynamics, significantly improve 24-hour forecast accuracy?
3. Can adaptive hybrid architectures that dynamically adjust complexity based on real-time volatility outperform the static weighted hybrid model proposed?

## Limitations
- The 91-day dataset window may not capture regime changes or extreme market conditions
- Model comparison scope is limited to six specific models without exploring alternative approaches
- Critical implementation details like exact outlier clipping percentiles and precise feature subsets are underspecified
- Feature engineering assumptions are not validated through systematic ablation studies

## Confidence
**High Confidence (≥ 0.8):** Traditional statistical models outperform deep learning on short, constrained datasets when stable seasonal patterns exist; SARIMAX with daily seasonality effectively captures Bitcoin fee patterns in the tested timeframe; model complexity doesn't guarantee better performance on limited data.

**Medium Confidence (0.6-0.8):** The hybrid SARIMAX+GBM architecture provides marginal improvements over pure SARIMAX; deep learning underperformance is primarily due to data scarcity rather than model inadequacy; the 144-block (24-hour) horizon is appropriate for Bitcoin fee forecasting.

**Low Confidence (< 0.6):** Long-term generalization beyond the 91-day testing window; whether results would hold during extreme market conditions or protocol changes; optimality of the specific feature engineering approach.

## Next Checks
1. Extend analysis to multiple non-overlapping 90-day periods across different Bitcoin market cycles to validate SARIMAX's superiority consistency.
2. Systematically increase training window from 91 days to 1-2 years while monitoring deep learning model performance to test the data scarcity hypothesis.
3. Conduct systematic feature removal experiments for both SARIMAX and hybrid model to identify which exogenous variables contribute most to performance gains.