---
ver: rpa2
title: 'Comba: Improving Bilinear RNNs with Closed-loop Control'
arxiv_id: '2506.02475'
source_url: https://arxiv.org/abs/2506.02475
tags:
- arxiv
- state
- comba
- memory
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Comba, a novel Bilinear RNN architecture inspired
  by closed-loop control theory. The model introduces a scalar-plus-low-rank (SPLR)
  state transition with both state feedback and output feedback corrections to improve
  memory management in sequence modeling.
---

# Comba: Improving Bilinear RNNs with Closed-loop Control

## Quick Facts
- arXiv ID: 2506.02475
- Source URL: https://arxiv.org/abs/2506.02475
- Authors: Jiaxi Hu, Yongqi Pan, Jusen Du, Disen Lan, Xiaqiang Tang, Qingsong Wen, Yuxuan Liang, Weigao Sun
- Reference count: 40
- Primary result: Bilinear RNN with closed-loop control achieves 40% speedup over Gated-DeltaNet with superior performance on language and vision tasks

## Executive Summary
Comba introduces a novel Bilinear RNN architecture that incorporates closed-loop control theory to improve memory management in sequence modeling. The model uses scalar-plus-low-rank (SPLR) state transitions with both state feedback and output feedback corrections, enabling more efficient orthogonal memory retention compared to traditional gating mechanisms. Implemented with a hardware-efficient chunk-wise parallel kernel in Triton, Comba demonstrates significant performance gains in both language modeling (perplexity of 12.68 on 1.3B model) and vision tasks (80.5% ImageNet-1K accuracy), while achieving 40% faster forward propagation than competing architectures.

## Method Summary
Comba is a Bilinear RNN architecture that uses a scalar-plus-low-rank (SPLR) state transition with closed-loop control mechanisms. The model processes sequences through input correction (state feedback), state update (SPLR transition), output correction (query refinement), and memory read operations. It employs a chunk-wise parallel kernel in Triton for hardware efficiency and is trained on large-scale corpora (340M and 1.3B parameters) using AdamW optimizer with cosine learning rate schedule. The architecture distinguishes itself from Linear RNNs through its bilinear state interactions and control-theoretic feedback corrections at both input and output stages.

## Key Results
- Achieves 40% speedup in forward propagation compared to Gated-DeltaNet
- 1.3B parameter model reaches perplexity of 12.68 on SlimPajama corpus
- Outperforms Mamba2 on ImageNet-1K classification (80.5% top-1 accuracy)
- Demonstrates superior recall performance on multiple benchmarks compared to Linear RNNs

## Why This Works (Mechanism)

### Mechanism 1: Supervised Memory Management via State Feedback
Comba uses a closed-loop Delta rule that calculates correction terms based on error between new values and current state projections. This Householder transform reflects stored memories across hyperplanes orthogonal to current keys, enforcing orthogonal memory management. The assumption is that treating recurrent state updates as optimization steps (minimizing \|v_t - S_{t-1}k_t\|) is superior to data-independent gating for long-term dependency tracking.

### Mechanism 2: Output Feedback Correction (Query Refinement)
During retrieval, the query vector is modified by subtracting a scaled key vector (o_t = S_t(q_t - dk_t)), equivalent to incorporating a similarity optimization objective. This forces the query to align better with the memory structure before reading from it. The assumption is that raw queries are suboptimal for retrieval from the bilinear state and require residual correction to utilize memory effectively.

### Mechanism 3: Scalar-Plus-Low-Rank (SPLR) State Transition
The state transition is defined as (\alpha_t - \tilde{\beta}_t k_t k^T), using scalar decay rather than full diagonal matrix. This simplifies inverse matrix calculations required for chunk-wise parallelism, achieving 2x pretraining acceleration while maintaining capacity to model negative eigenvalues. The assumption is that spectral flexibility from low-rank terms is sufficient, making full diagonal decay computationally redundant overhead.

## Foundational Learning

- **Concept: Bilinear Systems**
  - Why needed here: Comba is categorized as a "Bilinear RNN" distinguishing it from Linear RNNs (Mamba/GLA). The interaction Sk makes the overall system non-linear despite linearity with respect to S and k individually.
  - Quick check question: Can you explain why S_t k_t is considered a bilinear operation rather than a linear one?

- **Concept: Control Theory (Open vs. Closed Loop)**
  - Why needed here: The core innovation moves from "open-loop" (blindly writing to memory) to "closed-loop" (writing based on input and current state error).
  - Quick check question: In this paper, what specific mathematical operation constitutes the "feedback loop" during the input phase?

- **Concept: Householder Transformations**
  - Why needed here: The paper interprets memory update as a Householder reflection, explaining how the model avoids catastrophic forgetting by reflecting memory in the hyperplane orthogonal to the new key.
  - Quick check question: How does the factor β in (I - βkk^T) control the strength of the reflection/forgetting?

## Architecture Onboarding

- **Component map:** Inputs (q, k, v, α, β, d) → Input Correction (v_new = v - αSk) → State Update (SPLR transition) → Output Correction (q_tilde = q - dk) → Read (o = Sq_tilde)

- **Critical path:** The chunk-wise parallel kernel in Triton (Eq. 11-12) for calculating the inverse matrix M (Eq. 10) via forward substitution, which is the computational bottleneck that SPLR optimizes compared to Gated DeltaNet.

- **Design tradeoffs:**
  - SPLR vs. DPLR/IPLR: SPLR is faster (40% speedup) and simpler (scalar decay) but theoretically less expressive than full diagonal decay. Paper argues SPLR prevents overfitting better.
  - Output Correction Init: Initializing d=1 is better for large models (1.3B+), while d=0.02 is required for stability in smaller models (340M).

- **Failure signatures:**
  - Training Instability: If d is initialized improperly for model scale, loss may diverge.
  - Overfitting: If using IPLR variant on vision tasks, watch for loss decrease followed by sudden increase.
  - Recall Drop-off: If "forcing forgetting" (α ≈ 1) is not applied, model may fail to generalize to longer inference contexts.

- **First 3 experiments:**
  1. Operator Benchmark: Profile Triton kernel forward/backward pass against FlashAttention and Gated DeltaNet to verify claimed 40% speedup.
  2. Ablation on Output Correction: Train 340M model with d=0 vs. learned d to measure delta in perplexity on Wikitext.
  3. MQAR Synthetic Task: Run Multi-Query Associative Recall task to verify Bilinear structure allows perfect recall unlike standard Linear RNNs.

## Open Questions the Paper Calls Out

- Why does Comba underperform Gated-DeltaNet on specific downstream tasks like summarization and code generation? The authors note this lag warrants further exploration, suggesting the state dynamics or feedback mechanisms may be less suited for structural requirements of code and summarization tasks.

- How can Gated Slot Attention (GSA) be integrated with Comba to form an elegant hybrid architecture? The paper mentions future plans to explore this integration, but the specific method to combine GSA's intra-layer hybrid mechanism with Comba's closed-loop control remains undefined.

- Do the efficiency and performance gains of Comba's SPLR structure persist at scales larger than 1.3B parameters? The authors acknowledge they couldn't extend experimental scale to larger models like 2.7B due to computational resource limitations.

## Limitations

- Critical implementation details unspecified: Model architecture dimensions for 340M and 1.3B variants are not provided, making exact reproduction difficult.
- Training stability highly sensitive to output correction initialization: Different d values (0.02 for 340M vs 1.0 for 1.3B) required for different model scales without theoretical justification.
- SPLR structure's theoretical expressiveness limitations not thoroughly analyzed: Acknowledged constraints in non-language domains lack rigorous mathematical proof of benefits for long-term dependency retention.

## Confidence

- **High confidence**: The core mechanism of closed-loop control through state and output feedback corrections is mathematically sound and well-motivated by control theory principles. The 40% forward propagation speedup claim is supported by direct comparison with Gated-DeltaNet.
- **Medium confidence**: The empirical performance improvements on language tasks are convincing but depend heavily on proper hyperparameter tuning. Vision task results are promising but lack detailed training methodology.
- **Low confidence**: The theoretical claims about SPLR preventing overfitting better than IPLR are based on limited empirical evidence. The Householder transformation interpretation lacks rigorous mathematical proof of its benefits.

## Next Checks

1. Reproduce the 40% speedup: Profile the Triton chunk-wise parallel kernel implementation against Gated-DeltaNet and FlashAttention on the same hardware, measuring forward/backward pass times across different sequence lengths and batch sizes.

2. Ablation study on output correction: Train a 340M model with d=0 (removing output correction entirely) versus learned d, measuring the delta in perplexity and recall performance to quantify the specific contribution of this mechanism.

3. Synthetic recall task validation: Implement the Multi-Query Associative Recall task with varying context lengths to verify that Comba achieves perfect recall while standard Linear RNNs fail, testing the claimed advantage of bilinear state interactions for long-term memory.