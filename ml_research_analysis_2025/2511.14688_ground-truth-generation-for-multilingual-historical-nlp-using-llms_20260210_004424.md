---
ver: rpa2
title: Ground Truth Generation for Multilingual Historical NLP using LLMs
arxiv_id: '2511.14688'
source_url: https://arxiv.org/abs/2511.14688
tags:
- historical
- french
- chinese
- llms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the challenge of applying natural language\
  \ processing (NLP) to historical and low-resource languages by using large language\
  \ models (LLMs) to generate synthetic ground-truth annotations for historical French\
  \ (16th\u201320th centuries) and Chinese (1900\u20131950) texts. The authors developed\
  \ a method where LLMs produce POS, lemmatization, and named entity recognition (NER)\
  \ annotations, which are then used to fine-tune spaCy models."
---

# Ground Truth Generation for Multilingual Historical NLP using LLMs

## Quick Facts
- arXiv ID: 2511.14688
- Source URL: https://arxiv.org/abs/2511.14688
- Reference count: 40
- Primary result: LLM-generated synthetic annotations (96-98% POS, 95-97% lemma, 94% NER) enable fine-tuning spaCy models that significantly improve historical NLP performance

## Executive Summary
This study demonstrates that large language models can generate high-quality synthetic ground-truth annotations for historical and low-resource languages, enabling substantial improvements in downstream NLP tasks. The authors developed a pipeline using GPT-4o and Gemini 2.0 Flash to produce POS, lemmatization, and NER annotations for 16th-20th century French and 1900-1950 Chinese texts, achieving 96-98% accuracy on validated samples. Fine-tuning spaCy models on this synthetic data yielded 5-8 percentage point improvements in POS accuracy and 10-15 point improvements in NER accuracy for historical texts compared to off-the-shelf models. The results validate LLM-generated synthetic data as an effective solution for bootstrapping annotation in domain-specific contexts where traditional annotation is prohibitively expensive.

## Method Summary
The method involves sampling historical sentences from ARTFL-Frantext (French) and Shanghai Library (Chinese), then using LLMs to generate token-level annotations following Universal Dependencies schemas. For French, GPT-4o produces POS, lemma, dependency relations, and IOB NER tags; for Chinese, Gemini 2.0 Flash runs twice per sentence at different temperatures and keeps only exact matches to ensure quality. The synthetic annotations are converted to spaCy format, augmented for rare classes, and used to fine-tune pre-trained models (CamemBERT for French, zh_core_web_lg for Chinese). Models are trained with early stopping and evaluated on held-out historical test sets and modern reference corpora.

## Key Results
- LLM-generated annotations achieved 96.47% POS and 97.98% lemmatization accuracy for French, 98.95% POS and 94.26% NER accuracy for Chinese
- Fine-tuned spaCy models improved POS accuracy from 90.97% to 97.20% on historical French and from 67.75% to 72.33% on historical Chinese
- Chinese NER accuracy increased from 33.44% to 43.98% on historical texts using fine-tuned models
- Normalized Chinese POS scores (accounting for segmentation errors) improved from 64.82% to 72.33%

## Why This Works (Mechanism)

### Mechanism 1
LLMs with strong modern-language coverage can generate sufficiently accurate token-level annotations for historical texts to serve as synthetic training data. LLMs implicitly encode morphological and syntactic patterns from their training corpora; when prompted with explicit annotation schemas, they project these patterns onto historical inputs even when orthography differs. Core assumption: The LLM's pre-training data included enough historical or archaic text to generalize to target periods. Evidence: 96-98% POS accuracy on manually validated samples; French POS 96.47%, lemma 97.98%; Chinese POS 98.95%, NER 94.26%. Break condition: Languages poorly represented in LLM pre-training data will likely show degraded annotation quality.

### Mechanism 2
Fine-tuning smaller, efficient models on LLM-generated synthetic data transfers annotation capability while reducing inference cost. The spaCy models learn task-specific decision boundaries from synthetic labels; because they are trained on in-domain historical text, they internalize period-specific orthographic and syntactic patterns. Core assumption: Synthetic annotations, despite ~3-4% error rates, provide sufficient signal for gradient-based learning without catastrophic memorization of noise. Evidence: Fine-tuning spaCy models yielded substantial improvements over off-the-shelf models; French historical POS: 97.20% vs. 90.97% baseline; Chinese historical POS: 72.33% vs. 67.75%. Break condition: If annotation error rate exceeds ~10-15%, model performance would likely degrade.

### Mechanism 3
Models trained on imperfect synthetic annotations can approach but not exceed the quality ceiling of their training labels. Neural models average over noisy labels; systematic errors may persist, but random errors are partially smoothed during training. Core assumption: Label errors are not concentrated in ways that systematically bias learned representations. Evidence: Custom model achieves 95.28% POS on validated test data despite 3.5% training label error; nears but does not exceed training annotation quality (96.47%). Break condition: Systematic annotation errors will propagate to trained model unless post-processed.

## Foundational Learning

- **Concept:** Universal Dependencies (UD) and treebank tagsets
  - Why needed here: All annotation and evaluation uses UD POS tags and language-specific treebank tags; understanding schema alignment is prerequisite for reproducing results.
  - Quick check question: Can you map the French word "pas" to both a UD POS tag and a French Treebank tag?

- **Concept:** IOB (Inside-Outside-Beginning) entity tagging scheme
  - Why needed here: NER annotations use IOB format; tokenization errors in Chinese cascade to IOB sequences, breaking entity recognition.
  - Quick check question: Given tokens ["Tokyo", "Institute", "of", "Technology"], what IOB sequence represents a single ORG entity?

- **Concept:** Normalized accuracy metrics for segmented languages
  - Why needed here: Chinese POS/NER scores are inflated by tokenization accuracy; normalized scores (dividing by token F1) isolate downstream task performance.
  - Quick check question: If token F1 is 75% and raw POS accuracy is 72%, what is the normalized POS score?

## Architecture Onboarding

- **Component map:** Historical corpus extraction -> LLM annotation API -> Schema normalization -> Train/dev/test split -> spaCy fine-tuning -> Dual evaluation
- **Critical path:** LLM prompt design -> annotation quality assessment -> fine-tuning with early stopping -> validation on held-out data. Prompt engineering is the highest-leverage step.
- **Design tradeoffs:**
  - LLM choice: GPT-4o higher quality but slower; Gemini Flash faster for Chinese but requires dual-temperature consensus
  - Data volume: 55K French sentences, 10K Chinese—authors suggest even "relatively limited" data suffices, but optimal volume unknown
  - Model backbone: Transformer (CamemBERT) vs. word-vector (zh_core_web_lg)—transformer better for morphologically complex French; Chinese bottlenecked by tokenizer
- **Failure signatures:**
  - Chinese segmentation errors (e.g., "Tokyo Institute of Technology" split into 3 tokens → NER failure)
  - French systematic tag confusion (e.g., "pas" as particle vs. adverb)
  - Domain shift: historical models underperform on modern web text (French drops 3+ points on UD Sequoia)
- **First 3 experiments:**
  1. Baseline replication: Run off-the-shelf spaCy models on a 100-sentence historical sample; compute POS/lemma/NER accuracy manually to establish gap.
  2. LLM annotation pilot: Prompt GPT-4o or Gemini on 50 sentences with paper's prompts; validate accuracy on 10 sentences to verify label quality.
  3. Fine-tuning loop: Train spaCy on 5K synthetic annotations with early stopping; evaluate on held-out historical subset to measure delta vs. baseline.

## Open Questions the Paper Calls Out

### Open Question 1
Does a "many-models" strategy, utilizing separate pipelines for specific eras, yield significant quantitative gains over a single multi-century historical model? Basis: The authors state in the Discussion that creating and evaluating century-specific models could quantify potential gains compared to the multi-century historic model. Why unresolved: The current study successfully trained a unified "historic" model but did not compare this approach against specialized sub-models. What evidence would resolve it: A comparative evaluation of F1 scores between the unified historical model and individual century-specific models on a temporally diverse test set.

### Open Question 2
What is the optimal volume of LLM-generated synthetic data required to fine-tune models for specific historical NLP tasks? Basis: The Discussion notes that while moderately sized datasets were effective, further research could establish clearer benchmarks for the volume of training data needed. Why unresolved: The study used fixed dataset sizes (55,000 for French, 10,000 for Chinese). It is unclear if smaller datasets would suffice or if performance saturates at these levels. What evidence would resolve it: Ablation studies training models on incremental subsets of the synthetic data (e.g., 1k, 5k, 10k) to plot performance curves and identify the point of diminishing returns.

### Open Question 3
Can LLM-generated synthetic annotation effectively bootstrap domain-specific tokenizers to resolve segmentation bottlenecks in historical Chinese? Basis: The authors identify segmentation as a "critical ceiling" for downstream tasks and state that synthetic annotation might finally be the answer to developing domain-specific tokenizers. Why unresolved: Current off-the-shelf tokenizers are tuned for modern simplified text, causing error propagation in POS and NER tasks for historical texts. What evidence would resolve it: Training a custom tokenizer exclusively on LLM-generated segmentation data and measuring the resulting improvement in downstream POS and NER accuracy.

## Limitations
- Dependence on LLM annotation quality, which is inherently tied to pre-training corpus composition and may not generalize to languages poorly represented in LLM training data
- Chinese NER system fundamentally constrained by segmentation accuracy, with Token F1 scores directly limiting NER recall
- Study conducted on relatively small datasets (55K French sentences, 10K Chinese sentences), leaving minimum effective data volume and optimal augmentation strategies unknown

## Confidence

**High Confidence (Established by direct evidence):**
- LLMs can generate synthetic annotations for historical texts with >94% accuracy across all tasks
- Fine-tuning spaCy models on synthetic data yields measurable improvements over off-the-shelf models
- The annotation quality ceiling constrains downstream model performance

**Medium Confidence (Supported by evidence but with assumptions):**
- Modest amounts of LLM-generated data are sufficient for meaningful model improvement
- The approach generalizes to other historical languages with similar representation in LLM pre-training data
- Error rates below ~10-15% do not significantly degrade fine-tuned model performance

**Low Confidence (Largely unvalidated):**
- Optimal data volume for synthetic annotation generation
- Generalization to languages poorly represented in LLM pre-training data
- Performance on historical texts from periods before 16th century or after 1950

## Next Checks
1. **Error distribution analysis**: Manually annotate 100 sentences from each historical period to characterize the types and frequencies of LLM annotation errors, particularly focusing on systematic patterns that could bias trained models.
2. **Data volume sensitivity test**: Conduct controlled experiments varying the amount of synthetic training data (1K, 5K, 10K, 50K sentences) to identify the point of diminishing returns and optimal augmentation strategies.
3. **Cross-period generalization**: Evaluate the fine-tuned historical models on modern reference corpora and on historical texts from periods not represented in the training data to assess domain shift and temporal generalization.