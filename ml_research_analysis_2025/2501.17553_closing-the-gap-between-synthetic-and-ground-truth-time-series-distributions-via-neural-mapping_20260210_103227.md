---
ver: rpa2
title: Closing the Gap Between Synthetic and Ground Truth Time Series Distributions
  via Neural Mapping
arxiv_id: '2501.17553'
source_url: https://arxiv.org/abs/2501.17553
tags:
- time
- series
- mapping
- timevqv
- synthetic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Neural Mapper for Vector Quantized Time Series
  Generator (NM-VQTSG), a novel method to address fidelity challenges in vector quantized
  (VQ) time series generation. VQ-based methods, such as TimeVQVAE, suffer from information
  loss during compression into discrete latent spaces and deviations in the learned
  prior distribution from the ground truth distribution, resulting in synthetic time
  series with compromised fidelity.
---

# Closing the Gap Between Synthetic and Ground Truth Time Series Distributions via Neural Mapping

## Quick Facts
- arXiv ID: 2501.17553
- Source URL: https://arxiv.org/abs/2501.17553
- Reference count: 28
- Authors: Daesoo Lee; Sara Malacarne; Erlend Aune
- Primary result: Achieves ~151% average FID improvement, 7% IS improvement, and 37% cFID improvement over TimeVQVAE baseline

## Executive Summary
This paper addresses fidelity challenges in vector quantized (VQ) time series generation by introducing Neural Mapper for Vector Quantized Time Series Generator (NM-VQTSG). The method tackles information loss during compression into discrete latent spaces and deviations in learned prior distributions from ground truth distributions. By leveraging a U-Net-based neural mapping model, NM-VQTSG bridges the distributional gap between synthetic and real time series, effectively refining synthetic data by addressing artifacts introduced during generation. The approach demonstrates substantial improvements across diverse UCR Time Series Classification datasets.

## Method Summary
NM-VQTSG operates in three stages: (1) Tokenization using an encoder, vector quantizer, and decoder with reconstruction loss; (2) Prior learning using a bidirectional Transformer to predict masked tokens; (3) Neural mapping training where a U-Net-based mapper refines synthetic samples toward ground truth. The method employs Stochastic Vector Quantization (SVQ) with temperature τ to approximate the synthetic sample distribution, enabling paired training. The U-Net architecture integrates 1D convolutions, self-attention layers, and Snake activation functions to capture multi-scale temporal dependencies for effective refinement.

## Key Results
- Achieves ~151% average improvement in FID across 13 UCR datasets
- Demonstrates 7% improvement in IS and 37% improvement in conditional FID
- Shows significant fidelity gains backed by visual inspection in both data and latent spaces
- Successfully reduces artifacts in synthetic time series while preserving temporal patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: A learnable neural mapping function can reduce distributional discrepancy between synthetic and ground truth time series.
- Mechanism: The U-Net-based mapper learns to identify synthetic-specific artifacts and transforms them toward the ground truth distribution through L1 reconstruction loss minimization.
- Core assumption: The distributional gap manifests as consistent, learnable patterns rather than random noise.
- Evidence anchors: Abstract states the model "effectively aligning the distributions of synthetic and real data"; Section 3 describes mapping model narrowing the gap between distributions.

### Mechanism 2
- Claim: Stochastic Vector Quantization approximates the synthetic sample distribution using ground truth data.
- Mechanism: Instead of deterministic argmin quantization, SVQ samples tokens stochastically via softmax over negative Euclidean distances with temperature τ, injecting controlled noise similar to generation-time sampling errors.
- Core assumption: The dissimilarity level between synthetic and ground truth samples can be matched by tuning τ.
- Evidence anchors: Section 3 Equation 3.1b shows SVQ implementation; Section 5 validates the approximation hypothesis experimentally.

### Mechanism 3
- Claim: U-Net architecture with self-attention captures multi-scale temporal dependencies for effective refinement.
- Mechanism: Encoder downsamples temporal resolution while doubling feature dimension; skip connections preserve fine-grained details; self-attention layers model long-range dependencies.
- Core assumption: Time series refinement benefits from both local (convolutional) and global (attention) context.
- Evidence anchors: Section 4.1 details U-Net architecture with self-attention integration; Appendix A.1 provides architecture diagram.

## Foundational Learning

- Concept: Vector Quantization (VQ) in VAEs
  - Why needed: Understanding how discrete latent spaces introduce information loss is essential for grasping why synthetic samples deviate from ground truth.
  - Quick check: Can you explain why argmin quantization creates irrecoverable information loss?

- Concept: Prior Distribution Learning in Latent Space
  - Why needed: The second bottleneck (learned prior ≠ ground truth prior) requires understanding how autoregressive/bidirectional models sample token sequences.
  - Quick check: What factors cause a learned prior to deviate from the true underlying distribution?

- Concept: U-Net Skip Connections
  - Why needed: The mapper's effectiveness depends on preserving fine-grained temporal details while transforming global patterns.
  - Quick check: Why would a pure encoder-decoder (without skips) produce blurred time series outputs?

## Architecture Onboarding

- Component map: Encoder → Vector Quantizer → Decoder (Stage 1) → Bidirectional Transformer (Stage 2) → U-Net Mapper (Stage 3)
- Critical path: Ground truth X → SVQ (with optimal τ) → synthetic-like X̃' → U-Net mapper → refined X̃'R ≈ X
- Design tradeoffs:
  - Single τ vs. per-step τ: Paper uses single τ for simplicity; varying variance across sampling steps is unmodeled
  - L1 loss vs. GAN loss: L1 causes averaging/blurring; GAN loss proposed for sharper outputs but not implemented
  - Deterministic vs. stochastic mapping: Current approach is one-to-one; stochastic mapping needed when multiple plausible refinements exist
- Failure signatures:
  - Negative FID change: Synthetic already close to ground truth; mapper introduces noise
  - IS degradation with class imbalance: Minority class samples poorly mapped
  - Blurred outputs: L1 loss averaging effect; consider multi-scale STFT or GAN loss
- First 3 experiments:
  1. Reproduce baseline TimeVQVAE FID/IS on a single UCR dataset (e.g., FordA) to validate Stage 1-2 pipeline.
  2. Implement SVQ and sweep τ ∈ {0.1, 0.5, 1, 2, 4}; compute FID between Ẋ and X̃' using ROCKET features to verify p(Ẋ) ≈ p(X̃').
  3. Train U-Net mapper with L1 loss on generated pairs; compare FID/IS/cFID before and after mapping; visualize X, Ẋ, ẊR in data space to confirm artifact removal.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can integrating a GAN-based discriminator or a multi-scale STFT reconstruction loss into the training objective enable the neural mapper to preserve fine-grained temporal details better than the current L1 loss?
- Basis: The Discussion section suggests that the L1 loss causes an "averaging effect" leading to blurred data, and explicitly proposes GAN losses or multi-scale STFT losses as future work to capture sharper features.
- Why unresolved: The current implementation relies solely on L1 loss for simplicity, which inherently optimizes for the mean prediction, potentially smoothing out distinct high-frequency patterns.
- What evidence would resolve it: Experimental results comparing the fidelity (FID/IS) and visual sharpness of generated samples using the proposed loss functions against the baseline L1 approach.

### Open Question 2
- Question: How would incorporating a dynamic temperature schedule (τ) across sampling iterations, rather than a fixed scalar value, affect the accuracy of the distributional approximation p(Ẋ) ≈ p(X̃')?
- Basis: Section 3 states that the authors used a single τ to simplify the proposal, acknowledging that variance changes during sampling iterations, and explicitly left the exploration of varying τ for future work.
- Why unresolved: The current method assumes a constant noise level for the stochastic vector quantization approximation, which does not reflect the varying variance inherent in the prior model's iterative sampling process.
- What evidence would resolve it: An ablation study evaluating the alignment between generated and stochastic distributions when a scheduled τ is applied versus a static one.

### Open Question 3
- Question: What is the impact of introducing stochasticity into the mapping model to handle scenarios where a synthetic sample corresponds to multiple plausible ground truth outcomes?
- Basis: The Discussion section identifies the "Deficiency of Stochasticity" as a limitation, noting that the current deterministic one-to-one mapping may not be suitable for all datasets.
- Why unresolved: The current U-Net architecture enforces a deterministic output, potentially failing to capture the full diversity of valid "real" mappings for a given synthetic input.
- What evidence would resolve it: Comparative analysis on multimodal datasets showing whether a stochastic mapper improves diversity metrics or better covers the ground truth distribution modes compared to the deterministic baseline.

## Limitations

- The distributional-gap hypothesis relies on a single global τ parameter, which may not capture generation-specific error patterns across different time series classes or sampling steps.
- FID-based optimization for τ selection may not fully capture semantic or structural fidelity, particularly for multi-class datasets where minority class preservation is critical.
- The current L1 loss for mapping introduces averaging artifacts that could degrade sharp transitions or periodic patterns in certain time series types.

## Confidence

- High confidence in the architectural design and three-stage training pipeline, as these follow established practices in VQ-VAE and U-Net literature.
- Medium confidence in the SVQ approximation hypothesis, as it is theoretically sound but lacks direct external validation beyond the paper's internal experiments.
- Medium confidence in the overall fidelity improvements, given the substantial quantitative gains but limited qualitative validation across diverse time series patterns.
- Low confidence in the generalization to non-UCR datasets and very long time series (>10k timesteps), as these were not tested in the evaluation.

## Next Checks

1. Conduct ablation studies on τ parameter sensitivity by testing both per-class and per-step temperature scheduling, then measuring FID stability across diverse time series types to verify the single-τ assumption.

2. Implement and compare L1 loss with GAN-based adversarial loss for the U-Net mapper, measuring both quantitative fidelity metrics and qualitative sharpness of periodic/transition regions in the refined outputs.

3. Test the complete NM-VQTSG pipeline on non-UCR datasets (e.g., medical or sensor data with different characteristics) and on long time series (10k+ timesteps) to evaluate architectural limitations and computational scalability.