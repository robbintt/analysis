---
ver: rpa2
title: 'Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting'
arxiv_id: '2601.00913'
source_url: https://arxiv.org/abs/2601.00913
tags:
- gaussians
- removal
- masks
- compression
- clean-gs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Clean-GS addresses the problem of removing background clutter
  and spurious Gaussians (floaters) from 3D Gaussian Splatting reconstructions, which
  inflate model sizes and obscure objects of interest. The method uses sparse semantic
  masks to guide a three-stage pruning process: (1) whitelist filtering that removes
  Gaussians not projecting to object regions, (2) depth-buffered color validation
  that eliminates visible floaters based on color mismatch, and (3) neighbor-based
  outlier removal that cleans isolated artifacts.'
---

# Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting

## Quick Facts
- arXiv ID: 2601.00913
- Source URL: https://arxiv.org/abs/2601.00913
- Authors: Subhankar Mishra
- Reference count: 5
- Primary result: 60-80% compression of 3D Gaussian Splatting models while preserving object quality using as few as 3 semantic masks

## Executive Summary
Clean-GS addresses the problem of removing background clutter and spurious Gaussians (floaters) from 3D Gaussian Splatting reconstructions, which inflate model sizes and obscure objects of interest. The method uses sparse semantic masks to guide a three-stage pruning process: whitelist filtering that removes Gaussians not projecting to object regions, depth-buffered color validation that eliminates visible floaters based on color mismatch, and neighbor-based outlier removal that cleans isolated artifacts. Using as few as 3 semantic masks (1% of views), Clean-GS achieves 60-80% compression while preserving object quality. On a temple dataset, the method reduces model size from 125MB to 47MB (62% reduction) with 525K to 198K Gaussians. Processing time is 2-5 minutes on 96-core CPUs, making 3DGS models practical for web deployment and AR/VR applications where bandwidth and model size are critical constraints.

## Method Summary
Clean-GS is a post-processing pipeline that prunes 3D Gaussian Splatting models using sparse semantic masks. The three-stage process begins with whitelist filtering, where each Gaussian is projected to all masked views and removed if it never appears in object regions. Next, depth-buffered color validation renders the whitelisted Gaussians per view, comparing front-layer colors to the masked images using SH DC coefficients and removing consistent mismatches. Finally, neighbor-based outlier removal computes mean k-NN distances (k=10) for each Gaussian and removes those above the 95th percentile, eliminating isolated artifacts. The method processes pre-trained 3DGS models with as few as 3 binary masks, achieving 60-80% compression while maintaining visual quality.

## Key Results
- 60-80% compression of 3DGS models using only 3 semantic masks (1% of total views)
- Temple dataset reduction: 525K→198K Gaussians (62% reduction), 125MB→47MB file size
- Processing time: 2-5 minutes on 96-core CPUs
- Maintains visual quality while eliminating background clutter and floaters

## Why This Works (Mechanism)

### Mechanism 1: Whitelist Filtering via Spatial Projection
Gaussians that never project inside object masks across all annotated views cannot belong to the target object. For each Gaussian, compute 2D projection through camera intrinsics/extrinsics. If projection falls outside white (object) pixels in every masked view, exclude from whitelist. This leverages multi-view spatial consistency: true object parts must appear in object regions from some viewpoints. Core assumption: The object is adequately covered by the sparse mask set; no object region is occluded in all masked views.

### Mechanism 2: Depth-Buffered Color Validation
Visible floaters near objects can be identified by color mismatch between rendered and expected pixel colors. Render only whitelisted Gaussians with depth buffering per view. At each pixel, only the front-most Gaussian contributes. Compare its DC color (from SH coefficients) against the masked image. Remove Gaussians that consistently mismatch across views where visible. Core assumption: Floaters have colors that differ from the true object surface; depth-buffering correctly identifies which Gaussian is front-most.

### Mechanism 3: Neighbor-Based Outlier Removal
Isolated Gaussians far from k-nearest neighbors are likely artifacts rather than coherent object geometry. Compute mean distance to k=10 nearest neighbors for each Gaussian. Remove those above 95th percentile distance—these are spatially isolated and inconsistent with dense object surfaces. Core assumption: Real object geometry produces spatially coherent Gaussian clusters; floaters are scattered and isolated.

## Foundational Learning

- **Concept**: 3D Gaussian Splatting representation
  - Why needed here: Understanding that 3DGS stores position, covariance, opacity, and SH coefficients per Gaussian is prerequisite to grasping how projection and color validation work
  - Quick check question: What are the four key attributes stored per Gaussian in 3DGS?

- **Concept**: Camera projection mathematics (world → camera → image)
  - Why needed here: Whitelist filtering requires computing where 3D Gaussians project in 2D image space across multiple camera views
  - Quick check question: Given R_c2w and t_c2w, how do you transform a 3D point from world to camera coordinates?

- **Concept**: Depth buffering / z-buffering
  - Why needed here: Stage 2 relies on determining which Gaussian is front-most at each pixel to validate colors against ground truth
  - Quick check question: In depth buffering, which primitive is retained when multiple overlap at a pixel?

## Architecture Onboarding

- **Component map**: Input: Trained 3DGS model + M semantic masks → Stage 1: Whitelist Filtering → Stage 2: Color Validation → Stage 3: Outlier Removal → Output: Pruned 3DGS model

- **Critical path**: Stage 1 → Stage 2 → Stage 3 (sequential; each reduces candidate set for next stage)

- **Design tradeoffs**:
  - Sparse masks (3 of 302 views) vs. dense masks: Sparse reduces annotation cost but risks missing occluded regions
  - Conservative color validation (remove only with clear mismatch) vs. aggressive: Conservative preserves more object detail but retains some floaters
  - Neighbor percentile (95) vs. stricter (90): Higher preserves sparse geometry; lower removes more isolated artifacts

- **Failure signatures**:
  - Missing object parts after pruning → masks didn't cover all object regions (increase mask diversity)
  - Persistent floaters near object → color threshold τ=0.40 too permissive or floaters match object color (lower τ, add more masks)
  - Over-aggressive pruning of thin structures → neighbor removal threshold too strict (increase p_neighbor)

- **First 3 experiments**:
  1. Replicate Temple result: Train 3DGS on provided data, apply Clean-GS with 3 masks, verify ~60% compression with visual quality
  2. Ablate stages: Run whitelist-only, whitelist+color, and full pipeline to confirm each stage's contribution matches Table 2
  3. Sensitivity analysis: Vary mask count (1, 3, 5, 10) to characterize compression vs. annotation cost tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
Can foundation models (SAM, GroundingDINO) enable fully automatic mask generation from text prompts without manual annotation? Current method requires manually-created semantic masks. While SAM is mentioned for mask creation, the paper does not demonstrate end-to-end automation using text prompts to specify target objects. What evidence would resolve it: Experiments showing Clean-GS working with masks generated automatically from text descriptions (e.g., "temple" or "statue"), with compression and quality metrics comparable to manually-masked baselines.

### Open Question 2
What are the formal guarantees on minimum mask count and viewpoint diversity required for reliable object isolation? The paper empirically shows 3 masks (1% of views) work for the Temple, but lacks theoretical analysis of when this sparse supervision fails. The relationship between mask coverage, occlusion patterns, and pruning completeness remains uncharacterized. What evidence would resolve it: Theoretical analysis or systematic experiments varying mask count, angular distribution, and object occlusion levels to establish bounds on pruning accuracy.

### Open Question 3
Does fine-tuning remaining Gaussians after pruning recover quality lost during aggressive compression? The paper reports 60-80% compression but does not apply any post-pruning optimization. Whether the remaining Gaussians can adaptively redistribute to fill gaps or improve rendering fidelity is untested. What evidence would resolve it: Ablation experiments applying gradient-based fine-tuning to pruned models, measuring visual quality recovery (PSNR, LPIPS) against the original unpruned scene.

### Open Question 4
How well does Clean-GS generalize beyond outdoor monuments to diverse scene types (indoor, multi-object, transparent/reflective surfaces)? The method's effectiveness on indoor scenes, multiple interacting objects, or materials with complex view-dependent appearance is unknown. Color validation relies on RGB distance thresholds (τ=0.40) calibrated on outdoor textures. Indoor scenes with different lighting statistics, transparent objects, or scenes requiring multiple isolated objects may require different parameterization. What evidence would resolve it: Experiments on diverse scene categories (indoor rooms, vehicles, transparent objects) with analysis of parameter sensitivity and failure modes.

## Limitations
- Limited evaluation to outdoor monument datasets; generalization to indoor scenes, vehicles, or multi-object scenes untested
- Sparse masks risk missing occluded object regions, potentially over-pruning valid geometry
- Color validation threshold (τ=0.40) may not generalize across diverse lighting conditions and material types

## Confidence
- **High**: 60-80% compression achieved (empirical results in Table 2); three-stage pipeline structure; neighbor-based outlier removal mechanics
- **Medium**: Color validation threshold selection (τ=0.40) and its scene dependence; mask sparsity threshold (3 masks sufficient)
- **Low**: Exact 3DGS model file parsing; SH coefficient color space interpretation; CUDA rasterizer implementation details

## Next Checks
1. **Cross-dataset validation**: Apply Clean-GS to non-temple datasets (indoor scenes, vehicles, etc.) to verify compression ratios and quality preservation hold beyond the primary example
2. **Stage ablation robustness**: Systematically vary color threshold τ (0.3-0.6) and neighbor percentile (90-99) to characterize sensitivity and establish optimal defaults
3. **Mask coverage analysis**: Quantify relationship between mask diversity/quantity and over-pruning risk, establishing minimum mask requirements for different object topologies