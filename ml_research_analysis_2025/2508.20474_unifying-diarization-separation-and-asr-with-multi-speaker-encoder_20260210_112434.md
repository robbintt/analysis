---
ver: rpa2
title: Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder
arxiv_id: '2508.20474'
source_url: https://arxiv.org/abs/2508.20474
tags:
- speech
- speaker
- training
- multi-speaker
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a unified multi-speaker encoder (UME) framework
  that jointly optimizes speaker diarization, speech separation, and multi-speaker
  automatic speech recognition tasks using a shared speech foundation model encoder.
  The key innovation is the use of residual weighted-sum encoding (RWSE) to combine
  hidden representations from multiple layers of the encoder, enabling better information
  exchange and bottom-up alignment across tasks.
---

# Unifying Diarization, Separation, and ASR with Multi-Speaker Encoder

## Quick Facts
- **arXiv ID:** 2508.20474
- **Source URL:** https://arxiv.org/abs/2508.20474
- **Reference count:** 40
- **Primary result:** Achieves state-of-the-art diarization error rates of 1.37% and 2.29% on Libri2Mix and Libri3Mix respectively, with substantial improvements across all three tasks

## Executive Summary
This paper presents a unified multi-speaker encoder (UME) framework that jointly optimizes speaker diarization, speech separation, and multi-speaker automatic speech recognition using a shared speech foundation model encoder. The key innovation is the use of residual weighted-sum encoding (RWSE) to combine hidden representations from multiple layers of the encoder, enabling better information exchange and bottom-up alignment across tasks. The framework achieves state-of-the-art performance on LibriMix evaluation sets, demonstrating substantial improvements over single-task baselines across all three tasks while maintaining strong generalizability across datasets with varying overlap characteristics.

## Method Summary
UME employs OWSMv3.1 Medium as a shared encoder, processing input mixtures through L E-Branchformer layers. The Residual Weighted-Sum Encoding (RWSE) mechanism combines representations from all layers using learnable softmax-normalized weights, which are then residually added to the final layer to produce $H_{enc}$. Three task-specific heads process this representation: an EEND-based diarization head, a Conv-TasNet-based separation head (with upsampled $H_{enc}$ concatenated to Conv-TasNet features), and a joint CTC/attention ASR head with speaker-differentiating encoders. The model is trained jointly with equal weights (λ=0.33) across all tasks, with ASR pre-initialization required for 3-speaker stability.

## Key Results
- Achieves diarization error rates of 1.37% and 2.29% on Libri2Mix and Libri3Mix respectively
- Demonstrates substantial improvements over single-task baselines across all three tasks (diarization, separation, ASR)
- Shows strong generalizability across datasets with varying overlap characteristics

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Aggregating representations across encoder layers via RWSE enables cross-task information exchange and better alignment than using only the final layer.
- **Mechanism:** Learnable softmax-normalized weights $\omega^{task}_l$ form a weighted sum $H_{ws} = \sum_{l=1}^{L} \omega^{task}_l H^{(l)}$ over all layers; this is residually added to the last layer: $H_{enc} = H_{ws} + H^{(L)}$.
- **Core assumption:** Different layers encode complementary semantic/acoustic information relevant to SD, SS, and ASR.
- **Evidence anchors:** RWSE definition and role as a bridge between tasks (Page 2, Section II-A); weak direct evidence from neighboring papers discussing multi-layer use.
- **Break condition:** If layers are highly redundant or dominated by the final layer, learnable weights may collapse toward $H^{(L)}$, negating RWSE benefits.

### Mechanism 2
- **Claim:** Joint multi-task optimization of SD, SS, and ASR with shared encoder captures interdependencies and improves per-task performance over independent training.
- **Mechanism:** Final loss $L_{all} = \lambda_{diar} L_{diar} + \lambda_{sep} L_{sep} + \lambda_{asr} L_{asr}$ combines BCE diarization loss, SI-SDR separation loss, and CTC/attention ASR loss, enabling gradient flow from all tasks into shared encoder.
- **Core assumption:** Tasks are cooperative (not conflicting) under equal-weight scalarization.
- **Evidence anchors:** Joint training approach captures inherent interdependencies among tasks (abstract); multi-task weighting and initialization described (Page 3, Equation 15).
- **Break condition:** If task gradients conflict strongly or loss scales differ significantly, equal weights can cause instability or suboptimal convergence.

### Mechanism 3
- **Claim:** Task-specific heads with concatenated/transformed RWSE features leverage shared representations while preserving task-unique processing.
- **Mechanism:** SD uses linear mapping over $H_{enc}$; SS concatenates upsampled $H_{enc}$ with Conv-TasNet features; ASR passes $H_{enc}$ through speaker-differentiating encoders.
- **Core assumption:** Shared encoder representations generalize across tasks when appropriately transformed per task.
- **Evidence anchors:** Task-specific architectures detailed (Pages 2-3, Sections II-B, II-C, II-D).
- **Break condition:** If upsampling/misalignment corrupts temporal correspondence, task heads receive poorly aligned features, degrading performance.

## Foundational Learning

- **Concept: Permutation Invariant Training (PIT)**
  - Why needed: Handles label ambiguity in multi-speaker scenarios by optimizing over all speaker permutations.
  - Quick check: Can you explain how PIT resolves the speaker-label correspondence problem during training?

- **Concept: Speech Foundation Models (SFMs)**
  - Why needed: Provides pre-trained encoder (OWSMv3.1) as initialization; understanding layer-wise semantics informs RWSE design.
  - Quick check: What types of information do early vs. late encoder layers typically encode in models like WavLM or Whisper?

- **Concept: Multi-Task Learning Balance**
  - Why needed: Proper loss weighting (λ values) prevents task dominance and training instability.
  - Quick check: How would you detect if one task is dominating training in a multi-task setup?

## Architecture Onboarding

- **Component map:** Input mixture X → OWSMv3.1 encoder (L E-Branchformer layers) → RWSE ($H_{enc}$) → three parallel heads:
  - SD: Linear layer → speaker activity probabilities
  - SS: Conv-TasNet encoder → concat with upsampled $H_{enc}$ → TCN → masks → decoder
  - ASR: Speaker-differentiating encoder → CTC/attention decoder

- **Critical path:**
  1. Encoder forward pass generates $H^{(l)}$ for all layers.
  2. RWSE computes $H_{enc}$ via weighted sum + residual.
  3. Each task head processes $H_{enc}$ (with task-specific transforms).
  4. Losses computed with PIT; gradients backprop to shared encoder.

- **Design tradeoffs:**
  - Weighted-sum vs. last-layer-only: RWSE adds parameters but enables cross-layer information use.
  - Equal λ vs. tuned λ: Equal weights simpler but may not suit all datasets; two-stage tuning degraded performance here.
  - ASR initialization: Stabilizes 3-speaker training but adds pre-training cost.

- **Failure signatures:**
  - Training divergence on 3-speaker without ASR initialization (reported in Tables I–III).
  - DER/SDR/WER gaps between RWSE and ablated versions indicate layer-combination contribution.
  - Generalization drop if training overlap characteristics mismatch evaluation.

- **First 3 experiments:**
  1. Ablate RWSE (use only $H^{(L)}$) to measure contribution of multi-layer aggregation; compare DER/SDR/WER.
  2. Train with single-task losses ($\lambda=1.0$ for each task) vs. joint training to quantify multi-task benefits.
  3. Vary task weights (e.g., $\lambda_{asr}=0.8$ vs. $0.33$) on Libri2Mix/Libri3Mix to assess sensitivity and stability.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the UME framework maintain its performance advantages when applied to challenging real-world, unsegmented datasets like CHiME-6?
- **Basis in paper:** The authors state in the conclusion, "In the future, we plan to apply this framework to more challenging multi-speaker scenarios like CHiME-6."
- **Why unresolved:** The current study relies on simulated LibriMix data which may not fully represent the reverberation, noise, and spontaneous speech patterns found in natural meetings.
- **What evidence would resolve it:** Evaluation of UME on the CHiME-6 dataset showing competitive or state-of-the-art DER and WER compared to existing cascaded or joint systems.

### Open Question 2
- **Question:** How can training stability be ensured for more than two speakers without relying on pre-trained ASR initialization?
- **Basis in paper:** Tables I and III note that three-speaker models consistently "diverged (div.) w/o ASR initialization," indicating a fundamental instability in the joint training process for higher speaker counts.
- **Why unresolved:** The paper uses initialization as a patch for divergence but does not analyze the underlying optimization conflicts causing the multi-task loss to explode in the three-speaker case.
- **What evidence would resolve it:** A modification to the loss function or learning dynamics that allows the three-speaker model to converge from a random ("flat start") initialization.

### Open Question 3
- **Question:** Can the unified encoder effectively transfer shared representations to multilingual multi-speaker scenarios?
- **Basis in paper:** The authors express interest in "extending it to a multilingual UME" in the conclusion.
- **Why unresolved:** While OWSM supports multiple languages, it is unclear if the RWSE mechanism and the specific task heads (SD, SS, ASR) generalize across language boundaries without negative transfer.
- **What evidence would resolve it:** Successful implementation of the UME framework on a multilingual overlapped speech dataset (e.g., mLibriMix) demonstrating consistent improvements over monolingual baselines.

## Limitations

- **Dataset Generalization:** Evaluation limited to LibriMix with 100% overlap; performance on datasets with varying overlap ratios or real-world conditions untested.
- **RWSE Implementation Details:** Specific implementation of upsampling for Conv-TasNet concatenation and initialization ranges for SD/SS heads not explicitly specified.
- **Loss Weight Sensitivity:** Equal weights may not be optimal for all scenarios; sensitivity to hyperparameters not fully characterized.

## Confidence

**High Confidence:**
- RWSE mechanism improves performance over using only the final encoder layer (ablation results show consistent DER/SDR/WER improvements)
- Joint training provides benefits over single-task baselines (state-of-the-art results on LibriMix)
- Model generalizes across diarization, separation, and ASR tasks using shared representations

**Medium Confidence:**
- Claim that "different layers encode complementary information" supported by ablation but not directly validated with layer analysis
- Robustness claim for LibriCSS mentioned but not quantitatively evaluated
- Mechanism by which RWSE enables "bottom-up alignment" is theoretically plausible but not empirically demonstrated

**Low Confidence:**
- Performance extrapolation to real-world datasets with partial overlap or varied noise conditions
- Optimality of equal task weights across all scenarios
- Stability of training process without careful hyperparameter tuning

## Next Checks

1. **Ablation on Layer Importance:** Systematically disable RWSE and retrain with only the final layer, then test with subsets of layers (e.g., last 2-4 layers) to quantify the contribution of each layer to each task's performance.

2. **Cross-Dataset Generalization Test:** Evaluate the pre-trained UME model on a different multi-speaker dataset (e.g., AMI, CHiME-6, or WSJ0-2mix) with varying overlap ratios and real-world conditions to assess generalization beyond LibriMix.

3. **Loss Weight Sensitivity Analysis:** Conduct a systematic grid search over task weights (e.g., varying λ_diar from 0.2-0.4, λ_sep from 0.3-0.4, λ_asr from 0.2-0.4) to identify optimal configurations and measure sensitivity to these hyperparameters.