---
ver: rpa2
title: Exploring Landscapes for Better Minima along Valleys
arxiv_id: '2510.27153'
source_url: https://arxiv.org/abs/2510.27153
tags:
- batch
- alto
- training
- size
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an optimizer adaptor "E" that enables gradient-based
  optimizers to continue exploring along valleys (regions with low and similar losses)
  even after reaching a local minimum. This approach increases the likelihood of finding
  lower and flatter local minima associated with better generalization.
---

# Exploring Landscapes for Better Minima along Valleys

## Quick Facts
- arXiv ID: 2510.27153
- Source URL: https://arxiv.org/abs/2510.27153
- Authors: Tong Zhao; Jiacheng Li; Yuanchang Zhou; Guangming Tan; Weile Jia
- Reference count: 40
- Key outcome: Optimizer adaptor E achieves 2.5% average test accuracy improvement in large-batch training by exploring valleys to find flatter minima

## Executive Summary
This paper introduces an optimizer adaptor "E" that enables gradient-based optimizers to continue exploring along valleys even after reaching local minima, increasing the likelihood of finding lower and flatter local minima associated with better generalization. The adaptor introduces an acceleration term based on gradient differences that helps escape sharp small-scale minima while being captured by large-scale flat minima. The authors demonstrate their adapted Lamb optimizer (ALTO) achieves significant improvements in large-batch training scenarios, including better performance on ImageNet with larger batch sizes and GPT-2 with improved perplexity.

## Method Summary
The method introduces an acceleration term `a_k = β₁a_{k-1} + (1-β₁)(g_k - g_{k-1})` that captures the difference between current and previous gradients. This term approximates the Hessian-gradient product and scales with curvature: large at sharp minima (promoting escape) and small at flat minima (allowing capture). The modified gradient `g_k + αa_k` is then used in a standard Lamb optimizer framework. For large-batch training (batch size ≥ 1K), the authors set α = -5 and β₁ = 0.99, enabling the optimizer to explore valleys more effectively and find flatter minima with better generalization properties.

## Key Results
- ALTO achieves an average 2.5% improvement in test accuracy across various large-batch training tasks compared to state-of-the-art
- The method finds flatter minima as validated by lower Hessian eigenvalues in adapted optimizers
- ALTO successfully scales to larger batch sizes on ImageNet while maintaining or improving performance
- GPT-2 training with ALTO shows improved perplexity compared to standard optimization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The acceleration term `a_k = β₁a_{k-1} + (1-β₁)(g_k - g_{k-1})` creates an effective repulsion from sharp minima while preserving attraction to large-scale valley structures.
- Mechanism: The gradient difference `g_k - g_{k-1}` approximates `H_k(θ_k - θ_{k-1})`, which scales with Hessian curvature. At sharp minima (large `||H_k||`), this term amplifies escape velocity; at flat minima (small `||H_k||`), the term diminishes, allowing capture.
- Core assumption: The approximation `-∇||∇f(θ_k)||² ≈ g_k - g_{k-1}` holds sufficiently well during optimization dynamics.
- Evidence anchors:
  - [abstract]: "introduces an acceleration term based on the difference between current and previous gradients, which helps the optimizer escape sharp small-scale minima while being captured by large-scale flat minima"
  - [Section 2, Method]: Equation (2) and surrounding analysis show `ḡ_k - ḡ_{k-1}` as a proxy for Hessian-gradient product
  - [corpus]: Related work "Explore the Loss space with Hill-ADAM" (FMR=0.555) addresses similar escape dynamics but uses different mechanism; corpus lacks direct validation of this specific approximation
- Break condition: If gradient differences become dominated by noise rather than curvature information (small batches, late training), the escape mechanism degrades.

### Mechanism 2
- Claim: When α < 0, the modified gradient `g_k + αa_k` preferentially converges to flatter minima, improving generalization.
- Mechanism: Flat minima have wide "openings" with small gradients and small gradient differences, causing the acceleration term to contribute minimally to the update. Sharp minima have narrow openings with large gradient magnitudes, causing the αa_k term to dominate and push parameters away.
- Core assumption: Flat minima correlate with better generalization (standard sharpness hypothesis).
- Evidence anchors:
  - [abstract]: "increases the likelihood of finding a lower and flatter local minimum, which is often associated with better generalization"
  - [Section 2, Table 1]: Direction analysis shows `⟨θ_k - θ_{k-1}, ḡ_k - ḡ_{k-1}⟩` sign determines acceleration/deceleration at different optimization stages
  - [Section 4, Figure 3]: Hessian eigenvalue measurements show adapted optimizers with α < 0 find flatter minima
  - [corpus]: SAM-related work "LightSAM" (FMR=0.535) explores flat minima via perturbation; provides indirect support for flat-minima generalization hypothesis
- Break condition: If the loss landscape has disconnected flat regions that cannot be reached via valley paths, exploration along valleys may not reach globally optimal flat minima.

### Mechanism 3
- Claim: High β₁ values (e.g., 0.99) enable information persistence from early training stages to guide later exploration when gradient signals become noisy.
- Mechanism: The EMA of gradient differences retains "memory" of directions where gradients were decaying rapidly (descending into valleys). When later-stage gradients become dominated by noise, this stored direction continues guiding valley exploration.
- Core assumption: Early-training gradient differences contain meaningful curvature information that remains relevant throughout training.
- Evidence anchors:
  - [Section 2]: "β₁ reflects the persistence of the memory of the gradients-decaying-most directions"
  - [Section 4, Hyperparameters paragraph]: "we set α = -5, β₁ = 0.99 in large batch case (batch size ≥ 1K)"
  - [Section 5]: "ALTO uses remembered informative gradient information at early training stage... as guidance during later training stage where gradient information is flooded with noise"
  - [corpus]: Limited direct validation in corpus; "Convergence, Sticking and Escape" (FMR=0.546) studies SGD noise dynamics but not EMA-based memory mechanisms
- Break condition: If early training gradient directions are misleading (poor initialization, non-representative early batches), persistent memory may lock in suboptimal exploration directions.

## Foundational Learning

- Concept: **Exponential Moving Average (EMA) in optimizers**
  - Why needed here: The adaptor relies on EMA of gradient differences. Understanding how β values control memory length is essential for tuning.
  - Quick check question: If β₁ = 0.99, approximately how many past steps significantly influence the current acceleration term? (Answer: ~100 steps, as the half-life is approximately ln(0.5)/ln(β) ≈ 69)

- Concept: **Hessian curvature and sharp/flat minima**
  - Why needed here: The mechanism's escape behavior depends on Hessian norm at different minima types.
  - Quick check question: Why would a large Hessian eigenvalue cause the acceleration term to promote escape? (Answer: Large Hessian → large gradient differences → large acceleration term magnitude → larger effective step size away from that point)

- Concept: **Large-batch training generalization gap**
  - Why needed here: ALTO specifically targets large-batch scenarios where fewer parameter updates limit exploration.
  - Quick check question: If you train with batch size 32K instead of 256, approximately how many fewer parameter updates occur for the same number of epochs? (Answer: ~125x fewer updates, reducing exploration opportunities)

## Architecture Onboarding

- Component map:
  - a_k: EMA of gradient differences (primary exploration term)
  - m_k, v_k: Standard Adam-style momentum and variance terms (modified to use g_k + αa_k)
  - r_k: Layerwise-normalized update direction (from Lamb)
  - θ^{(i)}_{k+1}: Layer-wise parameter update with trust-region scaling

- Critical path: The gradient modification `g_k ← g_k + αa_k` happens before momentum computation. The acceleration term must be computed first, then integrated into the standard optimizer flow. Incorrect ordering (e.g., adding αa_k to momentum instead of gradient) causes different dynamics.

- Design tradeoffs:
  - Memory overhead: +1 tensor of parameter dimension for a_k (approximately 2% increase vs. Lamb for large models)
  - Computation overhead: +1 gradient subtraction per step (negligible vs. forward/backward pass)
  - α sign: Negative for exploration/flat minima (large batches), positive for exploitation/faster convergence (small batches)
  - β₁ tuning: Must scale with batch size—larger batches need higher β₁ (0.99) to maintain information across fewer steps

- Failure signatures:
  - Training divergence with learning rates that work for base optimizer: Likely |α| too large (exceed 1/(1-β₁) bound)
  - No improvement over base optimizer: Likely β₁ too small for batch size, or α sign incorrect
  - Slower convergence with α < 0: Expected behavior; the paper notes slower convergence is the trade-off for finding flatter minima
  - Gradient explosion late in training: Check that bias correction terms (1 - β^k_i) are applied correctly

- First 3 experiments:
  1. **Batch size sweep**: Train same model (e.g., ResNet-20 on CIFAR-100) with batch sizes [128, 1024, 4096, 16384], keeping α = -5, β₁ = 0.99 fixed. Compare ALTO vs. Lamb accuracy curves to verify the paper's claim that larger batches show larger improvements.
  2. **Hyperparameter sensitivity**: Fix batch size at 4096, vary α ∈ [-10, -5, -1, 0.5, 5] and β₁ ∈ [0.9, 0.99, 0.999] using a small grid (9 configurations). Plot final test accuracy to identify stable operating region.
  3. **Flatness validation**: After training with ALTO (α = -5) vs. Lamb, compute top Hessian eigenvalue on a held-out batch. Verify that ALTO finds lower eigenvalues (flatter minima) as shown in Figure 3 of the paper.

## Open Questions the Paper Calls Out
None

## Limitations
- The acceleration mechanism may degrade when gradient differences become dominated by noise, particularly in late training stages or with extremely large batches
- The EMA-based memory mechanism assumes early-training gradient directions remain relevant, but poor initialization or non-representative early batches could lock in suboptimal exploration directions
- The method introduces additional hyperparameters (α, β₁) that require careful tuning based on batch size, increasing practical complexity

## Confidence
- Gradient difference approximation quality: Medium confidence (limited direct validation in corpus)
- 2.5% average accuracy improvement claim: Medium confidence (single study without independent replication)
- Flatness-generalization correlation: High confidence (well-established sharpness hypothesis)

## Next Checks
1. Test ALTO with batch sizes beyond 16K to determine if the exploration advantage persists or breaks down with extreme mini-batching
2. Measure the correlation between gradient difference magnitude and true Hessian norm at various training stages to validate the approximation quality
3. Compare ALTO against other valley-exploration methods (e.g., Hill-ADAM from the corpus) on identical tasks to isolate the specific contribution of the acceleration mechanism