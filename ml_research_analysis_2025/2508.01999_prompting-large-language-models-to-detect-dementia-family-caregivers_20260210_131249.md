---
ver: rpa2
title: Prompting Large Language Models to Detect Dementia Family Caregivers
arxiv_id: '2508.01999'
source_url: https://arxiv.org/abs/2508.01999
tags:
- dementia
- family
- tweet
- member
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a system for detecting tweets from individuals
  with a family member suffering from dementia using large language models (LLMs)
  with various prompting strategies. The authors explored zero-shot, few-shot, chain-of-thought,
  and cascaded prompting on fine-tuned Llama-3.1-8B and Mistral-7B models, addressing
  data imbalance through oversampling.
---

# Prompting Large Language Models to Detect Dementia Family Caregivers

## Quick Facts
- **arXiv ID:** 2508.01999
- **Source URL:** https://arxiv.org/abs/2508.01999
- **Reference count:** 7
- **Primary result:** Zero-shot prompting on fine-tuned Llama-3.1-8B achieved 0.95 macro F1 for detecting dementia caregiver tweets

## Executive Summary
This paper presents a system for detecting tweets from individuals with a family member suffering from dementia using large language models (LLMs) with various prompting strategies. The authors explored zero-shot, few-shot, chain-of-thought, and cascaded prompting on fine-tuned Llama-3.1-8B and Mistral-7B models, addressing data imbalance through oversampling. The best performance was achieved with zero-shot prompting, yielding a macro F1-score of 0.95 on both validation and test sets, which is competitive with the BERTweet benchmark (F1: 0.962). The system leverages LoRA-based fine-tuning with 4-bit quantization for efficiency. Error analysis revealed that the model struggles with implicit family relationships and overgeneralizes from ambiguous context. Future work includes testing larger models for improved coreference resolution.

## Method Summary
The authors fine-tuned Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3 models using QLoRA (4-bit quantization) and LoRA adapters via TRL's SFTTrainer. The SMM4H 2025 Task 3 dataset was used, with 6,724 training tweets (imbalanced: 4,523 positive, 2,201 negative) and 353 validation tweets. To address class imbalance, minority class (label 0) tweets were oversampled roughly 2x per epoch. Five epochs of training were performed with a learning rate of 2e-4 using AdamW optimizer. The system used zero-shot prompting with a specific template format, avoiding preprocessing to preserve contextual information. Model outputs were parsed by taking the first character ('0' or '1') from the generated text.

## Key Results
- Zero-shot prompting achieved 0.95 macro F1 on both validation and test sets
- Outperformed few-shot (0.875 F1) and chain-of-thought (0.863 F1) prompting strategies
- Competitive with BERTweet benchmark (0.962 F1)
- Error analysis revealed struggles with implicit family relationships and keyword overgeneralization

## Why This Works (Mechanism)

### Mechanism 1: Distribution Alignment via Dynamic Oversampling
Replicating minority class samples (label 0) per epoch prevents the model from defaulting to the majority class during gradient updates. By balancing batch composition (approx. 50/50 vs. the native 67/33 split), the loss function penalizes false negatives on the minority class more heavily, forcing the decision boundary to shift rather than collapsing to the mode. The signal in the minority class is distinct enough that repetition does not lead to overfitting but rather corrects prior probability bias.

### Mechanism 2: Semantic Preservation via Minimal Preprocessing
Retaining "noisy" raw text (stop words, hashtags) preserves syntactic and deictic cues essential for coreference resolution. In short texts like tweets, function words (e.g., "my," "the") and structure often signal the personal relationship to the disease. Removing them destroys the context required to distinguish "my mom" (caregiver) from "a mom" (general observation). The LLM's pre-training has already equipped it to handle noisy social media text better than rule-based cleaning can preserve signal.

### Mechanism 3: In-Context Redundancy in Post-Tuning Inference
Zero-shot prompting outperformed few-shot and CoT after fine-tuning, suggesting that explicit reasoning steps or examples in the prompt may conflict with the model's internalized task representation. Fine-tuning with LoRA embeds the task logic directly into the weights. Providing additional reasoning steps (CoT) or examples (Few-Shot) at inference time may effectively "distract" the model or introduce noise if the in-context examples deviate slightly from the fine-tuning distribution.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA) & QLoRA**
  - **Why needed here:** Full fine-tuning of 7B-8B parameter models is computationally prohibitive. LoRA allows training by injecting small rank-decomposition matrices, and 4-bit quantization (QLoRA) reduces memory footprint to fit on accessible hardware (e.g., A100).
  - **Quick check question:** If you freeze the base model weights and only train the LoRA adapters, how does this affect the model's ability to unlearn toxic pre-training biases compared to full fine-tuning?

- **Concept: Class Imbalance Handling**
  - **Why needed here:** With a 67/33 split, a standard model achieves high accuracy by always predicting the majority class. Oversampling or loss weighting is required to force the model to recognize the minority class.
  - **Quick check question:** Why might oversampling the minority class lead to overfitting compared to class-weighted loss functions?

- **Concept: Coreference Resolution in User-Generated Text**
  - **Why needed here:** The primary failure mode is distinguishing "my family" vs. general discussion. This requires resolving pronouns (e.g., "he," "she") to entities, which is difficult in short, informal tweets.
  - **Quick check question:** How does the lack of explicit named entities in tweets (e.g., "my mom" vs. "Mary") complicate the linking of a dementia diagnosis to the author's family circle?

## Architecture Onboarding

- **Component map:** Raw Tweet -> Prompt Template (System/User roles) -> Llama-3.1-8B-Instruct + LoRA Adapters -> Text Output -> First Character Parsing ('0'/'1')
- **Critical path:**
  1. Load raw dataset
  2. Apply oversampling logic *per epoch* (do not pre-duplicate static file)
  3. Format samples into the conversation template (System/Instruction)
  4. Train using LoRA (r=16 or similar small rank)
  5. Inference using the exact Zero-Shot template used in training
- **Design tradeoffs:**
  - Raw vs. Cleaned Data: Paper explicitly chose raw data to preserve context (Won)
  - Zero-Shot vs. CoT: Paper found Zero-Shot superior post-tuning (Won)
  - Oversampling vs. Weighted Loss: Paper found oversampling more straightforward (Chosen)
- **Failure signatures:**
  - False Positives: Triggered by co-occurrence of "family" and "dementia" keywords without direct author relationship
  - False Negatives: Triggered by implicit relationships (e.g., "mom's dad" = grandfather) where the coreference chain requires multi-step reasoning the 8B model misses
- **First 3 experiments:**
  1. Baseline Replication: Train with standard class frequencies (no oversampling) to quantify performance drop
  2. Preprocessing Ablation: Apply stop-word removal to verify claim that performance degrades due to loss of context
  3. Prompt Sensitivity: Compare Zero-Shot template against Chain-of-Thought template to confirm if CoT tokens dilute the fine-tuned signal

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can scaling to larger models (e.g., 70B parameters) resolve errors in coreference resolution for implicit family relationships?
- **Basis in paper:** The conclusion states, "We believe that using larger models (e.g., 70B) could further improve performance," specifically noting that larger models would likely better recognize implicit relationships like "mom's 80 year old dad."
- **Why unresolved:** The current study was limited to 7B and 8B models due to resource constraints, leaving the specific benefit of scale for this task unproven.
- **What evidence would resolve it:** A comparison of error rates on implicit relationship cases between the current 8B models and 70B+ models on the same test set.

### Open Question 2
- **Question:** Why does zero-shot prompting outperform complex strategies like Chain-of-Thought (CoT) and few-shot prompting when fine-tuning LLMs for this classification task?
- **Basis in paper:** Table 2 shows Zero-shot achieving the highest F1 (0.946), significantly outperforming CoT (0.863) and Few-shot (0.875). The paper reports this counter-intuitive result but does not analyze why advanced prompting degrades performance during fine-tuning.
- **Why unresolved:** The authors focused on the empirical comparison rather than the underlying mechanisms causing complex prompts to fail or overfit during instruction tuning.
- **What evidence would resolve it:** Ablation studies analyzing gradient updates or attention patterns to determine if complex prompt tokens introduce noise during the fine-tuning process.

### Open Question 3
- **Question:** How can models be refined to stop overgeneralizing based on keyword co-occurrence (e.g., "family" + "dementia")?
- **Basis in paper:** The error analysis states the model "sometimes overgeneralizes from keywords like 'family' and 'dementia'," leading to false positives in ambiguous tweets where the semantic link is missing.
- **Why unresolved:** The current LoRA-based tuning appears to rely on surface-level associations rather than verifying the specific semantic relationship required by the task.
- **What evidence would resolve it:** Development of a "hard negative" test set specifically designed to trap keyword bias, demonstrating improved precision scores.

## Limitations

- **Dataset availability:** SMM4H 2025 Task 3 dataset not publicly available at time of writing, making independent verification difficult
- **LoRA configuration:** Specific LoRA rank and alpha values not specified, requiring assumptions that may affect reproducibility
- **Error quantification:** Error analysis based on patterns but lacks quantitative breakdowns of error types

## Confidence

- **High Confidence:** Zero-shot prompting approach achieving 0.95 F1 on validation set (directly stated and supported by Table 2)
- **Medium Confidence:** Oversampling effectiveness for class imbalance (stated but comparison with weighted loss not explicitly demonstrated)
- **Low Confidence:** Larger models improving coreference resolution (presented as future work without preliminary evidence)

## Next Checks

1. **Data Availability Impact:** Attempt to reproduce 0.95 F1 score once SMM4H 2025 dataset becomes available, comparing zero-shot vs. few-shot vs. CoT prompting on exact validation split
2. **LoRA Hyperparameter Sensitivity:** Systematically vary LoRA rank (r=8, 16, 32) and alpha values to determine impact on observed zero-shot superiority
3. **Error Type Quantification:** Manually categorize 100 random false predictions from test set to verify claims about implicit family relationship failures vs. overgeneralization from ambiguous context