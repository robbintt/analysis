---
ver: rpa2
title: Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic
  Literature Review
arxiv_id: '2506.15506'
source_url: https://arxiv.org/abs/2506.15506
tags:
- adversarial
- attacks
- data
- tabular
- studies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This systematic literature review consolidates 53 studies on adversarial
  attacks targeting tabular machine learning models, the first comprehensive survey
  in this domain. It analyzes attack methodologies across four optimization strategies
  (gradient-based, gradient-free, learning-based, and hybrid), finding that most gradient-based
  attacks adapt image-based methods with added feasibility constraints.
---

# Insights on Adversarial Attacks for Tabular Machine Learning via a Systematic Literature Review

## Quick Facts
- **arXiv ID**: 2506.15506
- **Source URL**: https://arxiv.org/abs/2506.15506
- **Reference count**: 40
- **Primary result**: First comprehensive survey consolidating 53 studies on adversarial attacks targeting tabular machine learning models

## Executive Summary
This systematic literature review provides the first comprehensive survey of adversarial attacks on tabular machine learning models, analyzing 53 studies published between 2016 and 2023. The review identifies four main optimization strategies used in these attacks: gradient-based, gradient-free, learning-based, and hybrid approaches. Most gradient-based attacks adapt techniques from image-based adversarial attacks by incorporating additional feasibility constraints to handle tabular data's discrete and mixed-type nature.

The review reveals significant gaps in the field, including the lack of standardized benchmarks, inconsistent evaluation methodologies, and underexplored practical considerations like plausibility and semantic preservation. While attack effectiveness is consistently measured across studies, real-world applicability factors remain poorly addressed, limiting the practical utility of current adversarial robustness research in tabular settings.

## Method Summary
The authors conducted a systematic literature review following PRISMA guidelines, searching multiple academic databases including ACM Digital Library, IEEE Xplore, SpringerLink, and arXiv. The review process involved three screening phases: title screening, abstract screening, and full-text evaluation. From an initial pool of 2,522 papers, 53 studies were selected for final analysis based on inclusion criteria focusing on adversarial attacks specifically targeting tabular machine learning models.

The selected studies were analyzed across multiple dimensions including attack optimization strategies, target applications, practical considerations (efficacy, feasibility, transferability), and evaluation methodologies. The authors classified attacks into four categories based on their optimization approach and examined how each addresses the unique challenges of tabular data, such as mixed data types, discrete features, and the need for valid input constraints.

## Key Results
- Most gradient-based attacks adapt image-based methods with added feasibility constraints for tabular data
- Only 15 of 53 studies evaluated transferability of attacks across different model architectures
- Real-world applicability factors like plausibility and semantic preservation remain underexplored despite being critical for practical deployment
- Cybersecurity applications dominate the field, with significant gaps in other domains like healthcare and finance

## Why This Works (Mechanism)
Adversarial attacks on tabular machine learning work by exploiting the sensitivity of model decision boundaries to small perturbations in input features. The mechanism relies on the mathematical optimization of an adversarial objective function that balances attack effectiveness against the constraints imposed by the tabular data domain. Unlike continuous image data, tabular features often include discrete values, categorical variables, and domain-specific constraints that require specialized handling to maintain attack feasibility while achieving the desired misclassification.

The effectiveness of these attacks stems from the fundamental vulnerability of machine learning models to carefully crafted input modifications, regardless of the data type. By adapting gradient-based optimization techniques from computer vision and incorporating domain-specific constraints, attackers can systematically identify and exploit weaknesses in tabular models' decision surfaces. The transferability property further amplifies attack impact by enabling black-box attacks without requiring access to model internals.

## Foundational Learning
**Gradient-based optimization**: Why needed - Enables systematic search for adversarial examples by computing input gradients with respect to the loss function. Quick check - Verify gradient computation works with mixed continuous and discrete features through small perturbation tests.

**Feasibility constraints**: Why needed - Ensures adversarial examples remain valid within the problem domain (e.g., valid categorical values, range constraints). Quick check - Test constraint satisfaction by verifying all modified features meet domain requirements post-attack.

**Transferability principles**: Why needed - Allows attacks to generalize across different model architectures without requiring white-box access. Quick check - Evaluate attack success rate across multiple model types using the same adversarial examples.

**Semantic preservation metrics**: Why needed - Ensures adversarial modifications don't violate the underlying meaning or relationships in the data. Quick check - Measure feature importance changes and domain expert validation of modified samples.

## Architecture Onboarding
**Component map**: Feature preprocessing -> Attack optimization (gradient-based/gradient-free/learning-based/hybrid) -> Constraint satisfaction -> Adversarial example generation -> Model evaluation

**Critical path**: The most critical path involves gradient computation (for gradient-based attacks) or surrogate model training (for gradient-free approaches), followed by iterative optimization with constraint satisfaction checks. This path determines both attack success rate and computational efficiency.

**Design tradeoffs**: Accuracy vs. feasibility (stricter constraints reduce attack success), computational cost vs. attack strength (more iterations improve effectiveness but increase runtime), white-box vs. black-box requirements (gradient access enables stronger attacks but limits applicability), and transferability vs. target specificity (general attacks work across models but may be weaker than model-specific approaches).

**Failure signatures**: Attacks fail when constraints become too restrictive (no valid perturbations exist), when the loss landscape is too flat (gradients provide no useful direction), or when defensive mechanisms detect and reject adversarial modifications. Common indicators include oscillation in optimization, constraint violations, or convergence to local minima that don't achieve the attack objective.

**First experiments**:
1. Implement a basic gradient-based attack on a simple tabular dataset (e.g., adult income) to verify basic functionality and constraint handling
2. Compare attack success rates across different optimization strategies on the same target model to establish baseline effectiveness
3. Evaluate transferability by applying the same adversarial examples to multiple model architectures (decision tree, random forest, neural network) trained on identical data

## Open Questions the Paper Calls Out
The paper identifies several critical open questions in the field of adversarial attacks on tabular machine learning. First, how can attack methodologies be standardized to enable fair comparison and reproducibility across studies? Second, what are the most effective ways to evaluate real-world applicability factors like plausibility and semantic preservation in adversarial examples? Third, how can defense mechanisms be developed that specifically address the unique challenges of tabular data without significantly degrading model performance?

Additional open questions include the development of comprehensive benchmark datasets that represent real-world tabular scenarios, the exploration of attack transferability across different tabular domains (beyond the current focus on cybersecurity), and the investigation of adaptive attack strategies that can overcome common defensive techniques. The paper also highlights the need for more research on the intersection of data privacy and adversarial robustness in tabular settings.

## Limitations
- The review focuses exclusively on English-language publications, potentially missing relevant research in other languages
- Classification of attack methodologies relies on authors' descriptions which may vary in technical precision
- Assessment of practical considerations like plausibility is challenging due to inconsistent reporting and subjective nature of these metrics

## Confidence
- **Attack methodology classification**: Medium - Based on authors' descriptions with potential terminology variations
- **Efficacy measurement claims**: High - Consistently reported across all 53 studies with standard metrics
- **Practical applicability assessment**: Medium - Limited by inconsistent reporting and subjective evaluation criteria
- **Transferability findings**: Medium - Based on subset of 15 studies with varying experimental setups

## Next Checks
1. Conduct a replication study using the most cited attack methodologies on standardized tabular datasets to verify reported efficacy rates and assess transferability across different model architectures.

2. Develop and apply a standardized evaluation framework that incorporates plausibility and semantic preservation metrics to assess whether successful attacks in academic settings would remain effective in real-world deployments.

3. Expand the literature search to include non-English publications and conduct a citation network analysis to identify potentially relevant studies that may have been missed by keyword-based searches.