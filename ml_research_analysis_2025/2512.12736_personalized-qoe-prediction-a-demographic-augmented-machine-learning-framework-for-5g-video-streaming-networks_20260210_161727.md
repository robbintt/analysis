---
ver: rpa2
title: 'Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework
  for 5G Video Streaming Networks'
arxiv_id: '2512.12736'
source_url: https://arxiv.org/abs/2512.12736
tags:
- prediction
- learning
- user
- dataset
- demographic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of personalized Quality of Experience
  (QoE) prediction for 5G video streaming by introducing a demographic-aware machine
  learning framework. The authors propose a behavioral data augmentation strategy
  that expands a small QoE dataset sixfold using synthetic demographic profiles modeling
  varying user sensitivities to streaming impairments.
---

# Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks

## Quick Facts
- arXiv ID: 2512.12736
- Source URL: https://arxiv.org/abs/2512.12736
- Reference count: 8
- Primary result: Demographic augmentation improves QoE prediction accuracy, with TabNet achieving RMSE 6.63, MAE 5.25, R² 0.87

## Executive Summary
This paper addresses the challenge of personalized Quality of Experience (QoE) prediction for 5G video streaming by introducing a demographic-aware machine learning framework. The authors propose a behavioral data augmentation strategy that expands a small QoE dataset sixfold using synthetic demographic profiles modeling varying user sensitivities to streaming impairments. They evaluate classical ML models and advanced deep learning architectures, including an attention-based MLP and TabNet, on the augmented dataset. Experimental results show significant improvements in prediction accuracy across RMSE, MAE, and R² metrics compared to baseline models trained on the original data. TabNet achieves the strongest performance (RMSE 6.63, MAE 5.25, R² 0.87), benefiting from its feature selection and attention mechanisms. The demographic augmentation enables richer modeling of user perception diversity and enhances model robustness, demonstrating a scalable approach for personalized QoE-aware intelligence in 5G networks.

## Method Summary
The framework augments a base QoE dataset of 450 streaming sessions by generating six synthetic demographic profiles per session, each with distinct sensitivity weights for streaming impairments. Impact factors (RebuffImpact, QualityBoost, QualityVariance, Smoothness) are computed from raw features and used in MOS adjustment functions specific to each demographic. Gaussian noise (σ=2.0) is added to create perceptual variance within profiles. Classical ML models (RF, GB, SVR, KNN, DT, Linear Reg, MLP) and deep learning models (AttentionMLP, TabNet) are trained on the augmented dataset. TabNet employs sequential attention masks for feature selection across decision steps, with sparsemax activation enabling exact feature exclusion. Models are evaluated on RMSE, MAE, R², PLCC, and SRCC metrics, with cross-validation and hyperparameter tuning applied.

## Key Results
- Demographic augmentation expands N=450 to N=2,700 samples, reducing Random Forest RMSE from 6.43 to 5.16
- TabNet achieves best overall performance: RMSE 6.63, MAE 5.25, R² 0.87 on augmented data
- AttentionMLP shows significant improvement over classical models, validating deep learning for QoE prediction
- Feature impact analysis confirms demographic-specific sensitivities (e.g., gamers show -0.442 correlation with rebuffering vs. -0.284 for professionals)

## Why This Works (Mechanism)

### Mechanism 1: Demographic-Weighted Data Augmentation
- **Claim:** Synthetic demographic profiles with behavioral sensitivity weights can expand small QoE datasets while preserving perceptually meaningful variation.
- **Mechanism:** Six demographic profiles (casual viewer, quality enthusiast, mobile user, gamer, elderly, professional) are assigned distinct sensitivity weights for rebuffering, quality, bitrate, and consistency. Each original sample generates six augmented versions through demographic-specific MOS adjustment functions with Gaussian noise (σ=2.0), expanding N=450 to N=2700.
- **Core assumption:** The assigned sensitivity weights (e.g., gamer rebuffering weight=2.8 vs. elderly=0.5) approximate real perceptual differences in user populations.
- **Evidence anchors:**
  - [abstract]: "behaviorally realistic demographic profiles modeling varying user sensitivities to streaming impairments"
  - [section V.B-V.D]: Defines Profile_k = {w_rebuff, w_quality, w_bitrate, w_consistency} and MOS augmentation with ŷ_i,k = g_k(y_i, ImpactFactors) + ε
  - [corpus]: TSKAN paper addresses interpretable QoE over time series but does not validate demographic weighting schemes; corpus lacks direct validation of synthetic demographic-to-MOS mappings.
- **Break condition:** If real-user studies show sensitivity weights do not correlate with actual demographic groups, or if Gaussian noise assumption (σ=2.0) does not reflect true intra-group variance.

### Mechanism 2: Impact Factor Computation as Perceptual Features
- **Claim:** Derived impact factors (RebuffImpact, QualityBoost, QualityVariance, Smoothness) provide more learning signal than raw streaming metrics alone.
- **Mechanism:** Raw features (VMAF, SSIM, bitrate, stall duration) are transformed into normalized impact factors bounded [0,1] that directly feed demographic-specific MOS adjustments. For example, RebuffImpact = min(stall_duration/2.0, 1.0) caps the rebuffering penalty.
- **Core assumption:** The chosen transformation functions capture the perceptually relevant aspects of each impairment type.
- **Evidence anchors:**
  - [section V.C]: Explicit formulas for RebuffImpact, QualityBoost, QualityVariance, Smoothness
  - [section X.E]: Correlation analysis shows gamers have -0.442 correlation with rebuffering duration vs. -0.284 for professional users, suggesting differential sensitivity is captured
  - [corpus]: Plasticity-Aware MoE paper addresses varying QoE functions across users but uses learned rather than hand-crafted feature transformations.
- **Break condition:** If alternative feature engineering (e.g., learned embeddings) outperforms hand-crafted impact factors on held-out demographic data.

### Mechanism 3: TabNet Sequential Attention for Demographic-Aware Feature Selection
- **Claim:** TabNet's stepwise sparse attention masks enable selective focus on demographic-relevant features, improving generalization on augmented data.
- **Mechanism:** At each decision step t, TabNet computes mask M^(t) via sparsemax, processes selected features through a feature transformer, and aggregates across steps. This allows the model to attend to rebuffering features for gamers while attending to quality features for enthusiasts.
- **Core assumption:** Demographic-specific patterns are learnable through sequential feature selection rather than explicit demographic conditioning.
- **Evidence anchors:**
  - [abstract]: "TabNet achieves the strongest performance (RMSE 6.63, MAE 5.25, R² 0.87), benefiting from its feature selection and attention mechanisms"
  - [section VII.B]: Describes M^(t) ∈ [0,1]^d learned through sparsemax, with ghost batch normalization for regularization
  - [corpus]: qAttCNN applies self-attention to encrypted traffic for QoE prediction but in a different domain (video conferencing); no direct corpus validation of TabNet for demographic-aware QoE.
- **Break condition:** If TabNet's feature masks do not show interpretable demographic-specific patterns when visualized, or if simpler models (Random Forest with explicit demographic features) match performance.

## Foundational Learning

- **Concept: Mean Opinion Score (MOS) as Ground Truth**
  - **Why needed here:** The entire framework predicts MOS (0-100 scale) as the target variable. Understanding that MOS aggregates subjective user ratings helps contextualize why demographic variation matters.
  - **Quick check question:** If two users watch the same video session with identical network conditions, should they report the same MOS? (Paper assumption: No—demographic factors cause systematic variation.)

- **Concept: Data Augmentation for Tabular Data**
  - **Why needed here:** Unlike image augmentation (rotation, flip), tabular augmentation requires domain-consistent transformations. This paper uses profile-driven target modification rather than feature-space perturbation.
  - **Quick check question:** What is the risk of augmenting by adding Gaussian noise to features vs. to targets? (Target noise preserves feature-label consistency; feature noise can create implausible samples.)

- **Concept: Attention-Based Feature Selection in TabNet**
  - **Why needed here:** TabNet differs from standard MLPs by learning *which* features to use at each step, not just *how* to weight them. This matters when demographic groups prioritize different features.
  - **Quick check question:** How does sparsemax differ from softmax in feature selection? (Sparsemax produces exact zeros, enabling true feature exclusion; softmax assigns non-zero weights to all features.)

## Architecture Onboarding

- **Component map:**
  Original QoE Dataset (450 samples) → Demographic Profile Generator (6 profiles) → MOS Adjustment Engine → Augmented Dataset (2700)
  ↓
  Feature Preprocessing: Label encoding → Standardization
  ↓
  Parallel pipelines: Classical ML (RF, GB, SVR, KNN, DT, LR, MLP)
                     Deep Learning (AttentionMLP, TabNet)
  ↓
  RMSE, MAE, R², PLCC, SRCC computation
  ↓
  Model comparison tables, scatter plots, correlation analysis

- **Critical path:** The augmentation quality determines everything downstream. If MOS adjustment functions do not reflect real demographic behavior, even TabNet will learn spurious patterns. Validate augmentation before model selection.

- **Design tradeoffs:**
  - **Tree models vs. deep learning on augmented data:** Random Forest achieves lower RMSE (5.16) than TabNet (6.63) on augmented data, but TabNet provides interpretability via attention masks. Choose based on whether deployment requires explanation.
  - **Augmentation factor (6x):** Chosen to balance diversity vs. synthetic artifact risk. Higher factors increase dataset size but may amplify noise from the Gaussian assumption.
  - **Explicit demographic features vs. implicit learning:** The paper adds demographic as a feature and lets models learn; an alternative would be demographic-conditional architectures.

- **Failure signatures:**
  - **Augmentation collapse:** If augmented MOS scores cluster around the mean (insufficient variance), models will underfit demographic patterns. Check per-profile MOS distributions.
  - **Attention mask saturation:** If TabNet masks are uniformly dense across steps, the model is not performing feature selection—reduce regularization or check feature scaling.
  - **Correlation mismatch:** If feature-MOS correlations do not align with assigned sensitivity weights (e.g., gamers not more sensitive to rebuffering), the augmentation logic has errors.

- **First 3 experiments:**
  1. **Baseline validation:** Train Random Forest on original (N=450) vs. augmented (N=2700) data without demographic features. Expect RMSE reduction from ~6.4 to ~5.2. If not observed, augmentation is not adding signal.
  2. **Demographic feature ablation:** Train TabNet with and without the demographic categorical feature. Quantify the R² gap to confirm demographic conditioning matters.
  3. **Cross-profile generalization:** Hold out one demographic profile entirely during training. Evaluate per-profile RMSE to test whether models learn demographic-specific patterns vs. global averaging.

## Open Questions the Paper Calls Out

- **How well do the synthetic demographic profiles and assigned sensitivity weights correspond to actual user behavior patterns measured through controlled user studies?**
  - **Basis in paper:** [explicit] "There's probably also room to refine the user personas and sensitivity weights based on more detailed user studies."
  - **Why unresolved:** The six demographic profiles (e.g., gamer, elderly, quality enthusiast) and their sensitivity weights (e.g., 2.8 rebuffering sensitivity for gamers vs. 0.5 for elderly) were designed based on assumptions rather than empirical user research.
  - **What evidence would resolve it:** Correlation analysis between model predictions and MOS scores from real users matching each demographic profile, demonstrating that synthetic weight assignments align with measured perceptual differences.

- **How does the demographic-augmented QoE prediction framework perform when deployed in live 5G network infrastructure under real-world conditions?**
  - **Basis in paper:** [explicit] "For future work, it would be really interesting to test this with actual real-user data instead of simulated demographics, and maybe try it out in a live 5G testbed to see how it handles real network conditions."
  - **Why unresolved:** All experiments were conducted in Google Colab using pre-collected streaming session data; no real-time inference or network resource allocation was tested.
  - **What evidence would resolve it:** Deployment results from a 5G testbed showing prediction latency, accuracy under varying network loads, and integration feasibility with network slicing mechanisms.

- **To what extent does the Gaussian noise injection (σ=2.0) during MOS augmentation reflect actual within-demographic rating variability?**
  - **Basis in paper:** [inferred] The augmentation adds noise "ϵ∼ N(0, σ²), σ=2.0" without empirical justification for this parameter value.
  - **Why unresolved:** The noise magnitude directly affects dataset diversity and model training, but its calibration to real human rating variance remains unvalidated.
  - **What evidence would resolve it:** Statistical comparison between the variance in synthetic MOS scores and variance observed in repeated subjective ratings from users within the same demographic group.

## Limitations

- **Behavioral realism uncertainty:** The synthetic demographic profiles and sensitivity weights are based on assumptions rather than empirical user studies, potentially limiting the framework's generalizability to real-world populations.
- **Noise parameter arbitrariness:** The Gaussian noise magnitude (σ=2.0) used in MOS augmentation is not empirically justified and may not reflect true perceptual variance within demographic groups.
- **Limited interpretability:** While TabNet's attention masks enable feature selection, the paper does not visualize or analyze these masks to demonstrate demographic-specific learning patterns.

## Confidence

- **High confidence:** The demographic augmentation methodology is clearly specified and reproducible. The performance improvements (RMSE reduction from ~6.4 to ~5.2 on augmented data) are directly measurable and validated against multiple baseline models.
- **Medium confidence:** The claim that TabNet's sequential attention enables demographic-aware feature selection is supported by the performance gains but lacks interpretability analysis. Without mask visualizations or ablation studies, the mechanism remains inferred rather than proven.
- **Low confidence:** The behavioral realism of the synthetic demographic profiles is asserted but not empirically validated. The paper provides no user study or held-out demographic data to confirm that the assigned sensitivity weights approximate real-world perceptual differences.

## Next Checks

1. **Sensitivity weight validation:** Conduct a small-scale user study with at least two demographic groups (e.g., gamers vs. casual viewers) to measure actual MOS sensitivity to rebuffering duration. Compare the empirical weights to the synthetic profile assignments.
2. **Cross-profile generalization test:** Hold out one demographic profile entirely during training. Evaluate per-profile RMSE on the held-out profile to determine if the model learns demographic-specific patterns or simply averages across all users.
3. **Attention mask interpretability:** Visualize TabNet's feature masks (M^(t)) across decision steps for sessions from different demographic profiles. Check if masks show systematic differences (e.g., rebuffering features highlighted for gamers, quality features for enthusiasts) to validate the attention mechanism's demographic awareness.