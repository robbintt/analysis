---
ver: rpa2
title: 'Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison
  for Semantic Similarity'
arxiv_id: '2502.14620'
source_url: https://arxiv.org/abs/2502.14620
tags:
- sentence
- semantic
- similarity
- embeddings
- glove
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates RWKV, a language model with linear attention,
  for generating sentence embeddings in zero-shot settings. It performs a layer-wise
  analysis on the MRPC dataset, comparing embeddings from different hidden layers
  to a GloVe baseline using Spearman correlation.
---

# Exploring RWKV for Sentence Embeddings: Layer-wise Analysis and Baseline Comparison for Semantic Similarity

## Quick Facts
- arXiv ID: 2502.14620
- Source URL: https://arxiv.org/abs/2502.14620
- Reference count: 29
- RWKV embeddings underperform GloVe baseline in zero-shot semantic similarity (0.3498 vs 0.4326 Spearman correlation on MRPC)

## Executive Summary
This paper investigates RWKV, a language model with linear attention, for generating sentence embeddings in zero-shot settings. The study performs layer-wise analysis on the MRPC dataset, comparing embeddings from different hidden layers to a GloVe baseline using Spearman correlation. Results show RWKV embeddings underperform GloVe, with correlation decreasing as layer depth increases. While RWKV has theoretical efficiency advantages, its practical inference time is much higher than GloVe, though GPU memory usage is similar. The findings suggest RWKV's zero-shot semantic similarity capabilities need further enhancement, potentially through task-specific fine-tuning or advanced pooling strategies, to match simpler baselines.

## Method Summary
The study evaluates RWKV for zero-shot sentence embedding generation on MRPC paraphrase corpus. Hidden states are extracted from layers 1, 3, 5, 7, 9, and 11 of RWKV-v6-Finch-1B6-HF, then pooled via mean averaging across tokens. Cosine similarity is computed for sentence pairs and correlated with ground-truth labels using Spearman correlation. The approach is compared against GloVe 6B 50d baseline using whitespace tokenization and mean word pooling. Experiments run on Google Colab Tesla T4 GPU with batch size 32, processing 1000 training subset and 408 validation samples.

## Key Results
- Layer 1 embeddings achieve highest Spearman correlation (0.3498 validation), declining to 0.3073 at Layer 11
- RWKV lags GloVe baseline by 8-12 percentage points across all layers (GloVe: 0.4326)
- Inference time ~700x slower than GloVe (0.41s vs 0.0006s per pair on training set)
- GPU memory usage similar between RWKV and GloVe despite RWKV's theoretical efficiency advantages

## Why This Works (Mechanism)

### Mechanism 1: Linear Attention via Recurrent Accumulation
RWKV achieves O(nd) complexity by reformulating quadratic attention as a recurrent process with cached cumulative terms. Instead of computing full attention matrix, RWKV maintains running accumulators (A_t, B_t) that combine current key-value pairs with decayed historical contributions. The output at each position depends on exponentially decayed past information weighted by learnable relative position weights w_{t-i}. Core assumption: Learnable decay weights can compensate for long-range dependency preservation that exponential decay naturally limits. Break condition: When sequence lengths are short (like MRPC sentence pairs), recurrent overhead may exceed matrix attention costs.

### Mechanism 2: Early Layers Capture More Generalizable Semantic Features
Earlier RWKV layers produce embeddings with higher semantic similarity correlation than deeper layers. Deeper layers optimize for language modeling objectives (next-token prediction), which may specialize representations toward syntactic/fluency features rather than semantic alignment needed for paraphrase detection. Core assumption: Semantic features for similarity tasks diverge from language modeling features as depth increases (entropy decreases from ~5.2 in shallow layers to ~3.8 in deep layers). Break condition: Task-specific fine-tuning could realign deeper layer representations toward semantic similarity objectives.

### Mechanism 3: Average Pooling Dilutes Semantic Signal-to-Noise Ratio
Simple average pooling across all tokens reduces discriminative power when key semantic information concentrates in few tokens. Average pooling assumes equal contribution from all n tokens. If only m << n tokens carry task-relevant semantic signal, the effective SNR degrades proportionally to n/m. Core assumption: Semantic similarity judgments depend on sparse, task-critical tokens rather than uniform token contributions. Break condition: Attention-weighted adaptive pooling with learnable query vector could concentrate weights on semantically important tokens.

## Foundational Learning

- Concept: Linear vs Quadratic Attention Complexity
  - Why needed here: Understanding why RWKV's theoretical O(n) scaling doesn't translate to practical speedup for sentence-length inputs
  - Quick check question: For a 20-token sentence, how does the constant factor in RWKV's linear attention compare to Transformer's quadratic overhead?

- Concept: Zero-shot Transfer Misalignment
  - Why needed here: Pre-trained language models optimize for next-token prediction, not semantic similarity alignment
  - Quick check question: Why would a model trained to predict "the" after "cat sat on" encode paraphrase relationships?

- Concept: Entropy as Information Capacity Proxy
  - Why needed here: Decreasing entropy across layers suggests information bottleneck formation that may exclude semantic nuances
  - Quick check question: If Layer 12 entropy is 3.8 and Layer 3 is 5.2, what type of information might be lost?

## Architecture Onboarding

- Component map:
  Time-Mixing Block -> Channel-Mixing Block -> Hidden States (Layers 1-12+) -> Pooling -> Cosine Similarity

- Critical path:
  1. Tokenize sentence pair via RWKV tokenizer
  2. Forward pass extracting hidden states from target layer (Layer 1 preferred for zero-shot semantic tasks)
  3. Apply pooling (average used; attention-weighted proposed for improvement)
  4. Compute cosine similarity; correlate with ground-truth labels

- Design tradeoffs:
  - Layer selection: Earlier = better semantic correlation; deeper = more abstract/specialized
  - Pooling strategy: Average = simple but dilutes signal; adaptive = theoretically better but requires implementation/testing
  - Zero-shot vs fine-tuned: Zero-shot isolates intrinsic capabilities but underperforms task-adapted baselines

- Failure signatures:
  - Spearman correlation decreases with depth (validation: 0.3498 → 0.3073 from Layer 1 → 11)
  - High similarity scores for non-paraphrase pairs (qualitative analysis shows false positives)
  - Inference time ~700x GloVe baseline (0.41s vs 0.0006s per pair on training set)
  - All RWKV layers trail GloVe baseline by 8-12 percentage points

- First 3 experiments:
  1. Layer-wise extraction baseline: Extract embeddings from layers 1, 3, 5, 7, 9, 11 using average pooling; compute Spearman correlation on MRPC validation set to confirm depth-performance inverse relationship.
  2. GloVe baseline comparison: Generate sentence embeddings via word vector averaging; compare correlation (0.4326) and inference time (0.0006s) against best RWKV layer.
  3. Qualitative failure analysis: Examine similarity scores on sample paraphrase/non-paraphrase pairs to identify if high false-positive rates stem from pooling dilution or representation misalignment.

## Open Questions the Paper Calls Out

### Open Question 1
Can contrastive learning fine-tuning close the performance gap between RWKV and simpler baselines like GloVe? The study only evaluated zero-shot performance, where RWKV underperformed the simpler GloVe baseline, leaving the potential gains from task-specific adaptation unknown. Empirical results showing that RWKV fine-tuned with contrastive loss on semantic similarity datasets achieves higher Spearman correlation than the GloVe baseline would resolve this.

### Open Question 2
Do attention-weighted adaptive pooling strategies significantly outperform the simple average pooling utilized in this study? The authors identify simple average pooling as a limitation that potentially dilutes the signal-to-noise ratio if semantic information is concentrated in only a few tokens. Ablation studies on MRPC demonstrating that adaptive pooling yields statistically significant improvements in correlation scores over average pooling would resolve this.

### Open Question 3
How does RWKV's performance compare to state-of-the-art Transformer-based models like Sentence-BERT or SimCSE? The paper restricted its baseline comparison to GloVe, making it difficult to contextualize RWKV's efficacy within the broader landscape of modern sentence embedding techniques. A benchmark comparison reporting Spearman correlation and inference latency for RWKV against SBERT and SimCSE on standard datasets would resolve this.

## Limitations
- Zero-shot evaluation may underestimate RWKV's capabilities, as task-specific fine-tuning could realign deeper layer representations toward semantic similarity objectives
- Theoretical O(nd) complexity advantage doesn't translate to practical speed gains for short sequences like MRPC sentence pairs
- MRPC dataset (1000 training, 408 validation pairs) may lack sufficient statistical power to distinguish subtle differences between embedding strategies

## Confidence
- High Confidence: Layer-wise correlation trends and inference time measurements are directly reproducible
- Medium Confidence: The claim that "RWKV's zero-shot semantic similarity capabilities need further enhancement" follows logically but doesn't account for potential fine-tuning improvements
- Low Confidence: The assertion about entropy decreases from ~5.2 to ~3.8 lacks sufficient empirical validation within the paper

## Next Checks
1. Fine-tuning Experiment: Implement task-specific fine-tuning on RWKV using MRPC training data with layer-wise probing to determine if deeper layers can achieve competitive semantic similarity performance when optimized for the paraphrase detection objective.
2. Pooling Strategy Comparison: Replace average pooling with attention-weighted adaptive pooling using learnable query vectors, then measure the impact on Spearman correlation and compare against the baseline average pooling results reported in the paper.
3. Sequence Length Scaling Analysis: Repeat the layer-wise correlation analysis on longer sentence pairs (e.g., from QQP or other semantic similarity datasets) to determine if RWKV's linear attention mechanism provides practical advantages at scale that aren't visible in short MRPC sequences.