---
ver: rpa2
title: Aligning Generative Speech Enhancement with Perceptual Feedback
arxiv_id: '2507.09929'
source_url: https://arxiv.org/abs/2507.09929
tags:
- speech
- enhancement
- perceptual
- arxiv
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GSEPF, the first perceptually aligned LM-based
  speech enhancement method. It uses Direct Preference Optimization (DPO) guided by
  a neural MOS predictor (UTMOS) to steer enhancement toward human-preferred outputs.
---

# Aligning Generative Speech Enhancement with Perceptual Feedback

## Quick Facts
- arXiv ID: 2507.09929
- Source URL: https://arxiv.org/abs/2507.09929
- Reference count: 0
- Primary result: First perceptually aligned LM-based speech enhancement method using DPO with UTMOS, achieving up to 56% relative gains in perceptual metrics.

## Executive Summary
This paper introduces GSEPF, the first perceptually aligned LM-based speech enhancement method. It uses Direct Preference Optimization (DPO) guided by a neural MOS predictor (UTMOS) to steer enhancement toward human-preferred outputs. On the Deep Noise Suppression Challenge 2020 test sets, GSEPF improves speech quality metrics, achieving up to 56% relative gains in UTMOS and strong gains in NISQA and DNSMOS, while also showing better subjective naturalness in listening tests. This establishes a new paradigm for perceptually driven speech enhancement.

## Method Summary
GSEPF extends the GenSE framework with DPO alignment using UTMOS as a proxy for human preference. The method trains a two-stage language model (N2S for semantic enhancement, S2S for acoustic generation) with a combined cross-entropy and DPO loss. Preference pairs are constructed by sampling multiple candidates from a reference model, scoring them with UTMOS, and selecting the best and worst as preferred/rejected pairs. The final training objective combines token-level fidelity with perceptual alignment, improving both objective metrics and subjective naturalness.

## Key Results
- Achieves up to 56% relative improvement in UTMOS over baseline GenSE
- Shows consistent gains across multiple perceptual metrics (NISQA, DNSMOS)
- Demonstrates better subjective naturalness in human listening tests
- Maintains speaker similarity through combined CE+DPO training objective

## Why This Works (Mechanism)

### Mechanism 1
DPO aligns LM-based speech enhancement outputs with human perceptual preferences more effectively than token-level likelihood objectives alone. DPO uses a contrastive objective to maximize the relative preference margin between a preferred and a rejected acoustic token sequence under the same conditioning context. This steers the model's probability distribution toward outputs rated higher by a proxy for human judgment, bypassing the need for a complex reinforcement learning pipeline.

### Mechanism 2
Preference pairs for DPO are most effective when constructed from a reference model's own generated outputs, not from ground-truth data. The process involves generating multiple candidate sequences from a reference LM, scoring them with a perceptual metric, and selecting the best as "preferred" and worst as "rejected." This forces the model to learn a preference gradient within its own output distribution.

### Mechanism 3
Combining the original cross-entropy loss (L_CE) with the DPO loss (L_DPO) acts as an anchor, preserving signal fidelity and speaker identity while improving perceptual quality. The final training objective is a simple sum: L_overall = L_CE + L_DPO. L_CE grounds the model in the original clean speech signal, preventing it from over-optimizing for a perceptual score at the cost of hallucinating a generic "clean" speaker.

## Foundational Learning

### Concept: Direct Preference Optimization (DPO)
- Why needed here: This is the core alignment algorithm. It replaces complex reinforcement learning (RLHF) with a simpler, stable classification-style loss on preference pairs.
- Quick check question: How does DPO use a frozen reference model (π_ref) and a trainable policy (π_θ) to avoid training a separate reward model?

### Concept: Language Model-based Speech Enhancement (LM-based SE)
- Why needed here: The method is built on top of a specific generative architecture (GenSE) that formulates enhancement as a token prediction task. Understanding this is essential.
- Quick check question: In the two-stage GenSE framework, what is the output of the first stage (N2S LM) and how is it used by the second stage (S2S LM)?

### Concept: Non-Intrusive Speech Quality Assessment (MOS prediction)
- Why needed here: The entire DPO training signal depends on UTMOS, a neural network that predicts a human Mean Opinion Score. It's the "reward" function.
- Quick check question: Why is a reference-free metric like DNSMOS or UTMOS necessary for this training loop, as opposed to a reference-based metric like PESQ?

## Architecture Onboarding

### Component map
- **Pre-trained & Frozen**: WavLM-Large (feature extractor), K-means (semantic tokenizer), SimCodec (acoustic tokenizer/decoder), UTMOS (MOS predictor)
- **Learned (GenSE Baseline)**: N2S LM (Noisy semantic tokens → Clean semantic tokens), S2S LM (π_ref: Noisy/clean semantic + noisy acoustic tokens → Enhanced acoustic tokens)
- **Aligned (GSEPF)**: S2S LM (π_θ: Initialized from π_ref, trained with L_CE + L_DPO)

### Critical path
The Preference Pair Construction Pipeline. An engineer must first implement the candidate generation (sampling N sequences from π_ref), then the scoring loop (decode audio, run UTMOS), and finally the pair selection (top Z vs bottom Z). Errors here will corrupt the DPO training signal.

### Design tradeoffs
- **N (Number of candidates)**: Higher N gives a better chance of finding a good A+ pair but linearly increases compute cost during training data generation.
- **Z (Number of pairs)**: The ablation study (Table 2) suggests Z=4 and Z=1 perform similarly, so fewer pairs may be more efficient.
- **β (DPO temperature)**: Controls the sharpness of the preference. The paper uses 0.1; tuning this is critical for stability.

### Failure signatures
- **Speaker Drift**: Enhanced audio sounds clean but loses the original speaker's voice identity. This indicates L_CE is too weak or DPO is over-powerful.
- **Metric Hacking**: UTMOS scores increase, but human listening tests show no improvement or new artifacts. This suggests the model is exploiting weaknesses in the UTMOS predictor.
- **DPO Collapse**: The loss doesn't decrease, which can happen if A+ and A- are not distinguishable (e.g., all candidates are bad).

### First 3 experiments
1. **Reproduce GenSE Baseline (π_ref)**: Train the two-stage LM using only cross-entropy loss and validate baseline metrics (DNSMOS, SECS).
2. **Ablate Preference Pair Source**: Compare DPO performance using ground-truth as A+ (which should fail) versus using model-generated candidates (which should succeed) to validate the core assumption.
3. **Tune CE+DPO Balance**: Run a sweep on the relative weighting or learning rates for the combined loss to find the sweet spot that maximizes UTMOS without degrading Speaker Embedding Cosine Similarity (SECS).

## Open Questions the Paper Calls Out

### Open Question 1
How can the framework be extended to balance perceptual quality with speaker identity preservation? The conclusion identifies "extending preference alignment to speaker similarity" as a key future direction. Additionally, Table 1 shows that training with DPO alone degrades Speaker Embedding Cosine Similarity (SECS) on the "w/o Reverb" test set. This remains unresolved because the current DPO implementation optimizes single-objective human preference (UTMOS), which can implicitly trade off acoustic fidelity for noise reduction, whereas multi-objective alignment requires a more complex reward formulation.

### Open Question 2
Why does using ground-truth clean tokens as preferred pairs fail to improve performance, while model-sampled tokens succeed? Section 4.3 reports that the $Z=1$ (Ground-truth) setup is "ineffective" and suggests it pushes the model similarly to cross-entropy, diminishing DPO's unique contribution. This remains unresolved because intuitively, ground-truth audio represents the highest possible quality, yet the method relies on "imperfect" samples ranked by UTMOS. The mechanism for why out-of-distribution (ground-truth) preferences destabilize or fail to aid DPO training is not fully explained.

### Open Question 3
Can this alignment paradigm be adapted to enable controllable enhancement, such as user-defined adjustments to noise suppression intensity? The conclusion explicitly lists "controllability" as a future direction for this new paradigm. This remains unresolved because the current system is deterministic in its alignment, targeting a single scalar definition of "quality" without mechanisms for conditional generation based on user intent.

## Limitations

- Perceptual metric dependency and potential reward hacking: The entire DPO alignment relies on UTMOS as a proxy for human perception, which could lead to metric hacking where DNSMOS and NISQA scores improve but human listening tests show no real benefit.
- Generalization to unseen noise and speaker conditions: The training data covers a wide range of SNRs and noise types, but evaluation is only on DNS Challenge 2020 test sets with no evidence for out-of-distribution performance.
- Architectural specificity and transfer potential: The method is tightly coupled to the GenSE two-stage LM architecture, and it's unclear if the same approach would be as effective for other SE architectures.

## Confidence

- **High Confidence**: The core technical contribution (integrating DPO with a neural MOS predictor for LM-based SE) is novel and the reported metric improvements on the DNS Challenge 2020 test set are reproducible.
- **Medium Confidence**: The qualitative claim that the model produces "perceptually preferred" outputs is supported by listening tests, but the extent of preference (44% in one condition) is relatively modest.
- **Low Confidence**: The broader claim that this "establishes a new paradigm for perceptually driven speech enhancement" is an overstatement requiring evidence of widespread adoption and superiority across diverse conditions and architectures.

## Next Checks

1. **Out-of-Distribution Noise Robustness Test**: Evaluate GSEPF on a held-out test set with noise types not present in the training data (e.g., environmental noise from a different dataset, or real-world recordings from smart devices). Compare performance against the GenSE baseline to validate the claim of improved generalization.

2. **Ablation Study on Preference Pair Quality**: Systematically vary the number of candidates (N) and the diversity of the reference model's outputs to test the hypothesis that preference pair quality is the critical factor. For example, compare N=4, 16, and 64 candidates and measure the impact on DPO convergence and final metric scores.

3. **Human Preference Study on Speaker Identity**: Conduct a targeted listening test where human raters are explicitly asked to judge both the overall quality and the speaker similarity of the enhanced speech. This will directly validate the claim that the L_CE + L_DPO combination prevents the loss of speaker identity and will expose any potential reward hacking by the UTMOS predictor.