---
ver: rpa2
title: How Feasible is Augmenting Fake Nodes with Learnable Features as a Counter-strategy
  against Link Stealing Attacks?
arxiv_id: '2503.09726'
source_url: https://arxiv.org/abs/2503.09726
tags:
- graph
- nodes
- node
- have
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the feasibility of using graph injection attacks
  as a defensive measure against link stealing attacks on graph neural networks (GNNs).
  The authors propose a method called NARGIS that augments the original graph with
  new nodes having learnable features.
---

# How Feasible is Augmenting Fake Nodes with Learnable Features as a Counter-strategy against Link Stealing Attacks?

## Quick Facts
- **arXiv ID**: 2503.09726
- **Source URL**: https://arxiv.org/abs/2503.09726
- **Reference count**: 35
- **Primary result**: Node injection with learnable features can effectively defend against similarity-based link-stealing attacks while maintaining classification utility, though less effective against influence-based attacks.

## Executive Summary
This paper investigates using graph injection attacks as a defensive mechanism against link-stealing attacks on graph neural networks. The proposed NARGIS method augments the original graph with fake nodes having learnable features, connected to spectral cluster centers to perturb the embedding space. A tri-level optimization framework jointly learns GNN parameters, a surrogate attacker model, and the defense model. Experiments on Cora, Citeseer, and Pubmed datasets show that NARGIS achieves better fidelity-privacy tradeoffs compared to differential privacy-based defenses for similarity-based attacks, though it performs poorly against influence-based attacks like LinkTeller.

## Method Summary
NARGIS implements a defensive graph injection strategy that perturbs the posterior space through strategically placed fake nodes with learnable features. The method uses spectral clustering to identify cluster centers, then attaches fake nodes to these centers. A tri-level optimization framework alternately updates GNN parameters for node classification, trains a surrogate attacker (once), and updates the fake node features to maximize defense strength while maintaining classification accuracy. The defense specifically targets similarity-based link-stealing attacks by reshaping posterior distributions to reduce edge inference reliability.

## Key Results
- NARGIS achieves superior fidelity-privacy tradeoffs compared to differential privacy defenses on benchmark datasets for similarity-based attacks
- The method is most effective against attacks using posterior similarity metrics (Attack-0, RW, AA, PA) but shows limited effectiveness against influence-based attacks like LinkTeller
- Optimal performance is observed with SAGE architecture, with varying results across different GNN architectures
- The number of clusters (and thus fake nodes) required shows inverse relationship with graph density, supporting the theoretical Proposition 1

## Why This Works (Mechanism)

### Mechanism 1: Posterior-Space Perturbation via Node Injection
- Claim: Augmenting the graph with fake nodes that have learnable features can perturb the posterior space enough to mislead link-stealing attackers while preserving node classification utility.
- Mechanism: Link-stealing attacks exploit the heuristic that connected nodes have similar posterior distributions. NARGIS injects nodes whose learned features, when propagated through message-passing, reshape the embedding space so that the posterior similarity between connected nodes no longer reliably indicates edge existence. The defender explicitly optimizes to maximize distributional divergence (Jensen-Shannon) and correlation distance between posteriors of edges while maintaining classification accuracy on ground-truth labels.
- Core assumption: Attackers primarily use similarity-based features (cosine, correlation, posterior distance) to infer edges.
- Evidence anchors: Abstract statement about reshaping graph embedding space; motivating example showing perturbed posteriors changing distance metrics.

### Mechanism 2: Cluster-Center Attachment for Efficient Graph Coverage
- Claim: Connecting augmented nodes to spectral cluster centers enables efficient perturbation of the embedding space with fewer injected nodes, where the optimal number of clusters is inversely proportional to graph density.
- Mechanism: Spectral clustering partitions the graph so that nodes within a cluster are within a small hop distance. By attaching fake nodes to cluster centers (nodes with minimal intra-cluster average distance), each fake node influences its cluster's neighborhood through message-passing. Proposition 1 formalizes that for graphs with higher density, fewer clusters (and thus fewer fake nodes) are needed to achieve coverage under fixed constraints on cluster size and degree.
- Core assumption: The cluster-center node's influence propagates effectively through the L-hop neighborhood within the cluster.
- Evidence anchors: Proposition 1 and proof showing c ∝ 1/σ under stated constraints.

### Mechanism 3: Tri-Level Optimization for Fidelity-Privacy Tradeoff
- Claim: A tri-level optimization that alternately updates GNN parameters, surrogate attacker parameters (once), and augmented node features can jointly improve classification performance and defense strength.
- Mechanism: Stage 1 optimizes the augmented GNN for node classification (L_class). Stage 2 trains a surrogate attacker on the resulting posteriors (L_attack), but only once to avoid adaptive adversaries. Stage 3 updates the learnable node features to maximize defender loss (L_defense), which combines misclassification of the surrogate, alignment calibration to preserve ground-truth label probabilities, and distribution/correlation distance terms.
- Core assumption: The surrogate attacker trained on initial posteriors is representative of real attackers' behavior.
- Evidence anchors: Abstract statement about tri-level optimization; section 4.2.1 explaining single surrogate training to prevent adaptive adversaries.

## Foundational Learning

- **Concept: Message-Passing in Graph Neural Networks**
  - Why needed here: NARGIS relies on understanding how node embeddings are updated through aggregation and update functions across k-hop neighborhoods; this determines where and how injected nodes exert influence.
  - Quick check question: Given a 2-layer GNN, which nodes' features can influence the embedding of a target node v?

- **Concept: Link-Stealing Attacks and Posterior Similarity**
  - Why needed here: The defense is designed specifically against attackers who infer edge existence from similarity metrics on node posteriors; understanding the attack basis clarifies why perturbation works.
  - Quick check question: If two nodes have posterior vectors [0.3, 0.7] and [0.35, 0.65], would a cosine-similarity-based attacker likely infer an edge?

- **Concept: Spectral Clustering and Graph Laplacians**
  - Why needed here: The augmentation module uses spectral clustering to identify where to attach fake nodes; understanding cluster formation and center selection is critical for implementation and tuning.
  - Quick check question: How does the unnormalized Laplacian relate to graph connectivity and cluster structure?

## Architecture Onboarding

- **Component map**: Original graph G -> Spectral clustering -> Augmented graph G_aug with fake nodes -> Feature Learning GNN -> Surrogate Attacker MLP -> Tri-Optimization Loop -> Perturbed posteriors

- **Critical path**:
  1. Compute spectral clusters and identify cluster centers
  2. Augment graph with n_new fake nodes connected to cluster centers
  3. Run tri-optimization: classify → train surrogate → update fake node features
  4. Output perturbed posteriors for original nodes

- **Design tradeoffs**:
  - Higher λ_align preserves accuracy but may reduce defense; lower λ_align improves privacy but risks misclassification
  - Fewer clusters (fewer fake nodes) may under-perturb dense graphs; too many may over-perturb and harm fidelity
  - Not retraining the surrogate avoids adaptive attackers but may overfit to a weak surrogate

- **Failure signatures**:
  - Attack AUC remains near baseline despite defense → λ_dist, λ_corr, or λ_miss too low; or cluster placement ineffective
  - Node classification accuracy drops sharply → λ_align too low or n_new too high
  - LinkTeller AUC unchanged → mechanism does not address influence-based attacks

- **First 3 experiments**:
  1. Reproduce Attack-0 results on Cora with SAGE: vary λ_align and plot accuracy vs Attack-0 AUC to understand the fidelity-privacy frontier
  2. Ablate cluster number: test n_new ∈ {5, 10, 20, 40} on Citeseer and observe impact on both metrics; verify inverse density relationship
  3. Test against LinkTeller on Pubmed: confirm current limitation, then prototype an influence-based loss term added to L_defense and measure any improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a theoretical performance bound be established that links the tunable defense loss weights (λ) directly to the fidelity-privacy trade-off?
- Basis in paper: Authors state in Conclusion and Discussion they want to "theoretically find a performance bound - concerning the tunable defense loss weights in the model optimization loop."
- Why unresolved: Current adjustment of hyperparameters relies on grid search and heuristics rather than formal mathematical relationships.
- What evidence would resolve it: A derivation proving a mathematical bound that predicts defense performance (AUC) and model utility (Accuracy) based on specific λ values.

### Open Question 2
- Question: How can the NARGIS augmentation scheme be integrated directly into the message-passing mechanism to ensure consistent robustness across diverse GNN architectures like GCN and GAT?
- Basis in paper: Authors note that "NARGIS and its variants have optimal performances mostly for SAGE, and never for GCN," and propose to "investigate how to make the model performance better across different message-passing mechanisms."
- Why unresolved: Current method modifies graph input side rather than internal aggregation layers, yielding varying results depending on GNN architecture.
- What evidence would resolve it: A modified NARGIS implementation achieving comparable defense AUCs for GCN and GAT models as currently achieved for SAGE.

### Open Question 3
- Question: Can integrating node influence metrics into the clustering or optimization process successfully extend NARGIS to defend against influence-based attacks like LinkTeller?
- Basis in paper: Authors explicitly list "Integrating Node Influence" as proposed investigation because current method "considers the posterior similarity measures only," resulting in poor performance against LinkTeller.
- Why unresolved: NARGIS currently optimizes for posterior ambiguity, whereas influence-based attacks exploit feature perturbations not addressed by current loss functions.
- What evidence would resolve it: Experimental results showing significant reduction in LinkTeller attack AUC when influence-based terms are added to spectral clustering or tri-level optimization.

## Limitations

- The defense mechanism is significantly less effective against influence-based attacks like LinkTeller, which exploit feature perturbations rather than posterior similarity
- The tri-level optimization scheme lacks direct corpus validation for GNN defense applications, relying on theoretical justification
- The density-cluster relationship formalized in Proposition 1 has no direct empirical validation in the defensive node injection context
- Loss weight tuning (λ_align, λ_dist, λ_corr, λ_miss) appears critical but is not extensively explored across different graph structures

## Confidence

- **High confidence**: The mechanism of posterior-space perturbation through learnable node features is well-defined and experimentally validated for similarity-based attacks on benchmark datasets
- **Medium confidence**: The cluster-center attachment strategy's efficiency claim (inverse proportionality to density) is theoretically supported but lacks direct empirical validation for defensive applications
- **Medium confidence**: The tri-level optimization framework's design choices (single surrogate training, loss aggregation) are justified by threat model constraints but not extensively validated across attack variations

## Next Checks

1. **Expand attack surface validation**: Test NARGIS against a broader range of link-stealing attacks including hybrid approaches that combine similarity and influence metrics. Specifically, implement an attack that uses both posterior similarity and feature influence, and measure whether NARGIS degrades gracefully or catastrophically.

2. **Loss weight sensitivity analysis**: Systematically vary λ_align ∈ [0.1, 0.5, 1.0, 2.0] and λ_dist ∈ [0.01, 0.05, 0.1, 0.5] on Cora and Citeseer to map the fidelity-privacy tradeoff surface. Identify whether there are regions of hyperparameter space where both metrics remain stable or if they exhibit sharp transitions.

3. **Cross-dataset generalization test**: Apply the optimal hyperparameter configuration from Cora to Citeseer and Pubmed without retraining. Measure degradation in both attack AUC and classification accuracy to quantify how much the defense is dataset-specific versus generalizable.