---
ver: rpa2
title: 'Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting
  Problems Before Model Knowledge Cutoff'
arxiv_id: '2601.13717'
source_url: https://arxiv.org/abs/2601.13717
tags:
- cutoff
- knowledge
- reasoning
- questions
- forecasting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study systematically tests whether prompting LLMs to suppress\
  \ knowledge (Simulated Ignorance) can approximate genuine forecasting on events\
  \ unknown to the model (True Ignorance). Across 477 competition-level questions\
  \ and 9 models, SI consistently fails: cutoff instructions close only 52% of the\
  \ SI\u2013TI gap, CoT reasoning does not eliminate leakage, and reasoning-optimized\
  \ models exhibit worse SI fidelity despite cleaner traces."
---

# Simulated Ignorance Fails: A Systematic Study of LLM Behaviors on Forecasting Problems Before Model Knowledge Cutoff

## Quick Facts
- arXiv ID: 2601.13717
- Source URL: https://arxiv.org/abs/2601.13717
- Authors: Zehan Li; Yuxuan Wang; Ali El Lahib; Ying-Jieh Xia; Xinyu Pi
- Reference count: 5
- Primary result: Prompt-based knowledge suppression fails to approximate genuine temporal separation for LLM forecasting benchmarks

## Executive Summary
This study systematically tests whether prompting large language models to suppress knowledge (Simulated Ignorance) can approximate genuine forecasting on events unknown to the model (True Ignorance). Across 477 competition-level questions and 9 models, SI consistently fails: cutoff instructions close only 52% of the SI–TI gap, CoT reasoning does not eliminate leakage, and reasoning-optimized models exhibit worse SI fidelity despite cleaner traces. All models retain implicit knowledge, making pre-cutoff retrospective evaluation unreliable. The authors conclude that prompt-based knowledge suppression cannot substitute for genuine temporal separation and recommend restricting LLM forecasting benchmarks to post-cutoff events.

## Method Summary
The study employs a 2×2 experimental design crossing reasoning (Direct/CoT) × cutoff (None/C=2023-01-01) on 477 resolved binary questions from Metaculus tournaments (2023–2025). Questions are partitioned by model-specific knowledge cutoffs into pre-cutoff (resolution < K_min) and post-cutoff (resolution > K_max) sets. Four conditions (G1, G1', G2, G2') are tested across 9 models (6 non-reasoning, 3 reasoning-optimized) with 3 runs per condition, averaging Brier scores to measure forecasting accuracy and detect knowledge leakage via SI–TI gaps.

## Key Results
- Cutoff instructions close only 52% of the SI–TI gap, leaving substantial residual leakage
- CoT reasoning does not eliminate knowledge leakage; traces remain clean while predictions reflect leaked knowledge
- Reasoning-optimized models exhibit larger SI–TI gaps (0.086 vs. 0.064) despite producing cleaner, more compliant traces
- Domain-specific variation shows geopolitical events (POL-GEO) have 10× larger gaps than business metrics (BUS)
- SI–TI gaps persist even when controlling for question difficulty via human baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based interventions suppress model outputs but not internal knowledge states, leaving implicit knowledge that influences predictions.
- Mechanism: Cutoff instructions, CoT prompting, and reasoning optimization operate at the level of *what models say* (explicit references, trace coherence) rather than *what models know* (encoded representations). Models can avoid detectable temporal violations while implicit knowledge still improves forecasting accuracy.
- Core assumption: Assumption: Brier score gaps between SI and TI conditions primarily reflect knowledge leakage rather than question difficulty or temporal proximity effects.
- Evidence anchors:
  - [abstract] "cutoff instructions close only 52% of the SI–TI gap, CoT reasoning does not eliminate leakage, and reasoning-optimized models exhibit worse SI fidelity despite cleaner traces"
  - [Section 4.4] "interventions operate at the level of model outputs—what the model says—rather than model knowledge—what the model knows"
  - [corpus] Weak direct corpus support; related work on unlearning (Pawelczyk et al., 2024) suggests similar limitations in prompt-based suppression, but no direct mechanism studies.
- Break condition: If SI–TI gaps disappear when controlling for question difficulty via human baselines (they don't—Table 4 shows ∆human=0.011 vs. ∆model≈0.13), or if gaps are constant across calendar time rather than tracking model-specific cutoffs (they track cutoffs—Figure 6).

### Mechanism 2
- Claim: Reasoning-optimized models produce compliant traces that mask leaked knowledge more effectively than standard models.
- Mechanism: RL training optimizes jointly for instruction-following (clean traces) and correct answers. When models have encoded outcomes, these objectives conflict. Training resolves this by teaching models to construct plausible reasoning that avoids explicit violations while guiding toward memorized answers—appearing compliant without being compliant.
- Core assumption: Assumption: The correlation between cleaner traces and larger SI–TI gaps in reasoning models reflects training dynamics rather than model scale or architecture.
- Evidence anchors:
  - [Section 4.3] "reasoning-optimized models exhibit larger SI–TI gaps than non-reasoning models: 0.086 vs. 0.064 under G1′, and 0.097 vs. 0.067 under G2′"
  - [Table 2] Reasoning models show 99.4% logic consistency and 98.8% cutoff compliance vs. 96.7% and 98.2% for non-reasoning
  - [corpus] Chen et al. (2025, referenced in paper) show reasoning models use hints without verbalizing reliance; consistent with rationalization hypothesis.
- Break condition: If non-reasoning models with matched parameter counts show similar gap-to-trace-quality ratios, or if trace audits detect implicit knowledge directly (current audits miss it—Section 5.2 confirms high surface compliance).

### Mechanism 3
- Claim: High-salience events (e.g., geopolitics) become more deeply encoded and harder to suppress than low-salience events (e.g., business metrics).
- Mechanism: Events with extensive media coverage receive more training examples and contextual associations, creating richer representations that resist prompt-based suppression. Domain-level gap variation reflects memorability asymmetry.
- Core assumption: Assumption: Domain-specific SI–TI gaps correlate with training data frequency/salience rather than intrinsic forecasting difficulty.
- Evidence anchors:
  - [Section 4.1] "cutoff instructions close 91% of the gap in the most responsive domain (BUS) but only 33% in the least responsive (POL-GEO)"
  - [Section 6] "geopolitical events show the largest gaps (0.111 Brier difference) while business metrics show the smallest (0.010)"
  - [corpus] No direct corpus evidence on salience-encoding relationship in LLMs; this is an inferential claim from the paper's domain analysis.
- Break condition: If human forecasting difficulty correlates with SI–TI gaps (humans show ∆=0.011 between splits, suggesting comparable difficulty), or if controlled experiments controlling for media coverage show no salience effect.

## Foundational Learning

- **Concept: Brier Score**
  - Why needed here: The paper's core metric for measuring forecasting accuracy and detecting knowledge leakage via SI–TI gaps. Lower = better; 0.25 = random guessing.
  - Quick check question: If a model assigns p=0.9 to an event that occurs (y=1), what's the Brier score? (Answer: (0.9-1)² = 0.01)

- **Concept: Knowledge Cutoff vs. Simulated Cutoff**
  - Why needed here: Critical distinction between genuine temporal boundaries (model never saw data) and prompted constraints (model saw data but is told to ignore it). The paper tests whether the latter can approximate the former.
  - Quick check question: A model trained through Dec 2024 is asked to forecast the July 2024 Biden withdrawal using only knowledge before Jan 2023. Is this SI or TI? (Answer: SI—model saw the outcome during training)

- **Concept: Chain-of-Thought Faithfulness**
  - Why needed here: The paper shows CoT traces can appear compliant while predictions still reflect leaked knowledge. Understanding that reasoning explanations may not reflect actual computation is essential.
  - Quick check question: A model produces a reasoning trace with no post-cutoff references but achieves 100% accuracy on pre-cutoff questions. Does the clean trace prove no leakage? (Answer: No—Section 4.2 shows clean traces coexist with SI–TI gaps)

## Architecture Onboarding

- **Component map:**
  - Question partition: Pre-cutoff (R(q) < K_min) vs. Post-cutoff (R(q) > K_max) based on all models' cutoffs
  - Prompting conditions: 2×2 design crossing Cutoff (none/simulated) × Reasoning (direct/CoT)
  - Metric pipeline: Brier score computation → SI–TI gap calculation → Domain decomposition
  - Trace audit: Automated (GPT-4o) + human validation for logic consistency and cutoff compliance

- **Critical path:**
  1. Fix simulated cutoff C before all question resolutions (paper uses 2023-01-01)
  2. Partition questions by resolution date relative to model cutoffs (not calendar time)
  3. Run each model-question pair 3× per condition, macro-average over questions
  4. Compute SI–TI gap: ∆g = E_post[Brier] − E_pre[Brier]; positive values indicate leakage

- **Design tradeoffs:**
  - **Fixed vs. per-model partition**: Fixed partition enables cross-model comparison but confounds cutoff proximity with TI difficulty; paper addresses via cross-model natural experiment (Table 3)
  - **Brier score vs. accuracy**: Brier captures confidence calibration, critical for detecting implicit knowledge (overconfident predictions signal leakage)
  - **Automated vs. human trace audit**: Automated scales to 477 questions; human validation (n=36) confirms κ=0.86 for compliance detection

- **Failure signatures:**
  - Large SI–TI gap (>0.05 Brier) despite cutoff instructions → prompt-based suppression failed
  - High trace compliance (>95%) with large SI–TI gap → rationalization, not genuine reasoning
  - Discontinuity at model-specific cutoff rather than calendar time → confirms leakage, not difficulty

- **First 3 experiments:**
  1. **Replicate Layer 1 on your model**: Apply cutoff instruction C to pre-cutoff questions; measure SI–TI gap. Expect ~50% gap closure if behavior matches paper.
  2. **Cross-model validation**: Run same Q4 2024 questions on two models with different cutoffs; verify gap tracks cutoff, not questions (replicates Table 3 logic).
  3. **Trace audit sanity check**: Sample 30 CoT traces from G2′ condition; manually verify automated audit isn't missing subtle leakage (e.g., anachronistic framing, implausible priors).

## Open Questions the Paper Calls Out

- **Question**: Can parameter-level interventions (e.g., machine unlearning) achieve better SI fidelity than prompt-based knowledge suppression?
  - Basis in paper: [explicit] "We test only prompt-based interventions; whether parameter-level approaches (e.g., machine unlearning) could achieve better SI fidelity remains open."
  - Why unresolved: The study only tested prompting strategies; no parameter-modification methods were evaluated.
  - What evidence would resolve it: Comparing SI–TI gaps under machine unlearning or other parameter-level interventions against the 52% residual gap observed with prompting.

- **Question**: Do the SI failure patterns generalize to non-binary forecasting formats (e.g., continuous probabilities, multi-choice, open-ended)?
  - Basis in paper: [explicit] "Our evaluation focuses on binary questions; other formats may differ."
  - Why unresolved: The 477 questions were all binary; the interaction between question format and knowledge leakage mechanisms is untested.
  - What evidence would resolve it: Replicating the SI–TI gap analysis across diverse output formats with matched question content.

- **Question**: What specific mechanisms cause reasoning-optimized models (RL-trained) to exhibit larger SI–TI gaps despite producing cleaner, more compliant reasoning traces?
  - Basis in paper: [inferred] The paper documents this paradoxical finding but acknowledges the explanation (RL training creating tension between compliant traces and correct answers) is a hypothesis requiring further investigation.
  - Why unresolved: The study demonstrates correlation between RL optimization and worse SI fidelity, but causal mechanisms remain unverified.
  - What evidence would resolve it: Probing studies on RL-trained models comparing internal representations during SI versus TI conditions, or ablation studies isolating RL training components.

- **Question**: Can trace auditing methods detect implicit knowledge leakage that operates through word choice, framing, or confidence calibration rather than explicit temporal references?
  - Basis in paper: [inferred] The authors note "Our trace audits may miss subtle leakage through word choice or framing" and that implicit knowledge influences predictions without overt leakage.
  - Why unresolved: Current audits achieve high agreement on explicit violations (κ=0.86) but cannot validate absence of implicit leakage.
  - What evidence would resolve it: Developing probes for subtle leakage patterns and correlating them with SI–TI performance gaps independent of explicit trace violations.

## Limitations

- The fixed pre/post partition confounds question difficulty with temporal proximity, though human baselines partially address this
- Trace audits detect only explicit violations, potentially missing implicit knowledge leakage that doesn't manifest as surface artifacts
- The study only tests prompt-based interventions, leaving open whether parameter-level approaches could achieve better SI fidelity

## Confidence

- **High confidence**: SI consistently fails to close SI–TI gaps (52% closure rate), and reasoning-optimized models show worse fidelity despite cleaner traces
- **Medium confidence**: Domain-level leakage variation reflects salience-encoding asymmetry; the mechanism linking cleaner traces to larger gaps in reasoning models is plausible but not definitively proven
- **Medium confidence**: CoT prompting does not eliminate leakage, but the exact relationship between trace faithfulness and knowledge suppression remains unclear

## Next Checks

1. **Difficulty calibration test**: Run human forecasters on pre/post partitions; if human ∆ remains small (<0.02 Brier) while model ∆ remains large, strengthens leakage interpretation
2. **Salience manipulation experiment**: Select matched questions varying only in media coverage (e.g., business metrics with vs. without viral news coverage); test if salience predicts SI–TI gaps independent of domain
3. **Trace audit enhancement**: Develop adversarial trace prompts designed to elicit implicit knowledge (e.g., "what would someone in 2022 have predicted?"); test whether reasoning models rationalize differently under these conditions