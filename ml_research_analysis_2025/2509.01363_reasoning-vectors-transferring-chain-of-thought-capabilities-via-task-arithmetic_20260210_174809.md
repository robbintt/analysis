---
ver: rpa2
title: 'Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic'
arxiv_id: '2509.01363'
source_url: https://arxiv.org/abs/2509.01363
tags:
- reasoning
- vector
- performance
- gsm8k
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces reasoning vectors\u2014compact task vectors\
  \ that capture and transfer reasoning capabilities between models. The method extracts\
  \ a vector from two identical Qwen2.5 models trained with different optimization\
  \ strategies (SFT vs GRPO) and transfers it via simple arithmetic to enhance reasoning\
  \ in other instruction-tuned models."
---

# Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic

## Quick Facts
- arXiv ID: 2509.01363
- Source URL: https://arxiv.org/abs/2509.01363
- Reference count: 40
- Authors: Mohammad Zbeeb; Hasan Abed Al Kader Hammoud; Bernard Ghanem
- Key outcome: Reasoning vectors extracted via task arithmetic consistently improve reasoning performance across models and benchmarks, with gains up to +12.3% on BBH.

## Executive Summary
This paper introduces reasoning vectors—compact task vectors that capture and transfer reasoning capabilities between models. The method extracts a vector from two identical Qwen2.5 models trained with different optimization strategies (SFT vs GRPO) and transfers it via simple arithmetic to enhance reasoning in other instruction-tuned models. When added to the base model, the vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for 1.5B model). The improvements persist under adversarial conditions and are reversible, as subtracting the vector causes significant degradation (-11.8% on GSM8K). This demonstrates that reasoning capabilities acquired through reinforcement learning can be isolated and reused as modular components, offering a practical, training-free method to enhance model reasoning by leveraging existing computational investments.

## Method Summary
The method extracts a reasoning vector by computing the element-wise difference between two identically initialized Qwen2.5 models—one fine-tuned via supervised fine-tuning (SFT) and another via group relative policy optimization (GRPO). This vector, $v_{\text{reason}} = \theta_{\text{GRPO}} - \theta_{\text{SFT}}$, is hypothesized to capture reasoning-specific updates while factoring out shared knowledge. The vector is then added to compatible instruction-tuned models using task arithmetic (via MergeKit), with scaling factor $\alpha=1$ found optimal. The transfer relies on Linear Mode Connectivity, ensuring the addition remains in a stable, low-loss region of the optimization landscape.

## Key Results
- GSM8K accuracy improved by +4.9% when reasoning vector was added to Qwen2.5-Instruct
- HumanEval pass@1 increased by +4.3% with the same vector transfer
- BigBenchHard accuracy showed +12.3% improvement for 1.5B model
- Subtracting the vector caused 11.8% degradation on GSM8K, confirming its effectiveness

## Why This Works (Mechanism)

### Mechanism 1: Differential Isolation of Reasoning Updates
Subtracting an SFT-tuned model from an RL-tuned model isolates the specific parameter updates responsible for reasoning improvements. By computing $v_{\text{reason}} = \theta_{\text{GRPO}} - \theta_{\text{SFT}}$, the method factors out dataset-specific knowledge acquired during standard fine-tuning, leaving a residual vector that primarily encodes the reasoning capability instilled by GRPO. This assumes both models learned the same factual knowledge from the dataset, differing only in their optimization for reasoning processes.

### Mechanism 2: Linear Mode Connectivity for Safe Transfer
The reasoning vector can be safely added to a target model without catastrophic failure because the donor models reside in the same low-loss basin of the optimization landscape. This relies on Linear Mode Connectivity (LMC), which posits that models fine-tuned from an identical initialization lie in a connected region where linear interpolation maintains low loss. Adding $v_{\text{reason}}$ moves the target model along a stable trajectory within this basin.

### Mechanism 3: Domain-General Reasoning Activation
A reasoning vector derived from a specific domain (math) generalizes to improve performance in unrelated domains (code, logic). The vector is hypothesized to encode abstract problem-solving strategies rather than domain-specific facts. When "Think step by step" prompts are used, these latent pathways are activated, suggesting reasoning capabilities are partially modular and distinct from domain knowledge.

## Foundational Learning

- **Task Arithmetic**: The fundamental operation (weight addition/subtraction) used to extract and transfer reasoning capability. Without understanding that models can be merged via simple tensor math, the method appears unintuitive. *Quick check*: If you have Model A (pre-trained) and Model B (fine-tuned on sentiment), how would you create a "sentiment vector" and apply it to Model C? (Answer: $v = \theta_B - \theta_A$; $\theta_{C_{new}} = \theta_C + v$)

- **Group Relative Policy Optimization (GRPO)**: A form of reinforcement learning used to optimize for reasoning via rewards. Understanding that GRPO is specifically optimizing for reasoning (via rewards) is crucial to distinguishing $\theta_{GRPO}$ from the baseline $\theta_{SFT}$. *Quick check*: Why does the paper use the difference between a GRPO model and an SFT model, rather than just using the GRPO model directly? (Answer: To isolate the reasoning skill while removing shared dataset knowledge.)

- **Linear Mode Connectivity (LMC)**: Explains why the method doesn't break the model by providing theoretical justification that adding a vector from one model to another remains in a "safe" zone of the loss landscape. *Quick check*: Does LMC hold if the two models were initialized with different random seeds? (Answer: Generally, no; LMC requires shared initialization.)

## Architecture Onboarding

- **Component map**: Qwen2.5-SFT model -> Qwen2.5-GRPO model -> v_reason vector -> Qwen2.5-Instruct model -> Enhanced model with reasoning capabilities

- **Critical path**:
  1. Verify strict architecture and tokenizer compatibility between Donors and Target
  2. Load $\theta_{SFT}$ and $\theta_{GRPO}$ weights
  3. Compute and save $v_{\text{reason}}$ (one-time cost)
  4. Load Target model and add $v_{\text{reason}}$ (scaling $\alpha=1$ is a good default)
  5. Inference with "Think step by step" prefix for best activation

- **Design tradeoffs**:
  - Compatibility vs. Reach: Method is restricted to models with identical architectures and tokenizers, not a universal reasoning upgrade
  - Magnitude ($\alpha$): Paper finds $\alpha=1.0$ optimal; lower $\alpha$ dampens effect, higher $\alpha$ risks instability

- **Failure signatures**:
  - Catastrophic Degradation: Applying to incompatible architecture causes NaNs or gibberish outputs
  - tokenizer Misalignment: Different vocabularies cause embedding layer math misalignment
  - Negative Transfer: Subtracting the vector causes 11.8% drop on GSM8K

- **First 3 experiments**:
  1. Sanity Check (Subtraction): Compute $\theta_{base} - v_{\text{reason}}$ and confirm performance drop on GSM8K
  2. In-Domain Transfer: Apply $v_{\text{reason}}$ to Qwen2.5-Instruct and evaluate on GSM8K to replicate +4.9% gain
  3. Cross-Domain Generalization: Test enhanced model on HumanEval to verify math-derived vector improves code generation (~+4.3%)

## Open Questions the Paper Calls Out

### Open Question 1
Can reasoning vectors be effectively transferred across different model families (e.g., from Qwen to Llama), or are they strictly bound by architectural similarities? The limitations section states: "Transferring reasoning vectors across different model families (e.g., from a Llama model to a Qwen model) is not guaranteed to work and remains an important open question." This is unresolved because the study exclusively utilizes Qwen 2.5 models sharing identical architectures and tokenizers. Experiments applying the extracted $v_{\text{reason}}$ from Qwen to structurally distinct models like Llama or Mistral would resolve this.

### Open Question 2
Can multiple reasoning vectors derived from different domains be composed or added together without causing interference that degrades performance? Figure 2 illustrates "Addition" combining multiple skill vectors, and Related Work mentions TIES-Merging for reducing interference, but Table 1 only evaluates single-vector transfer. This is unresolved because the paper doesn't test simultaneous application of, for example, a math-based and code-based reasoning vector. Ablation studies applying $v_{\text{reason-math}} + v_{\text{reason-code}}$ to a single target model would resolve this.

### Open Question 3
Is shared pre-training initialization strictly necessary for effective transfer, or can vectors transfer between models with different training histories but identical architectures? Section 3.4 lists "Initialization Similarity" as a compatibility requirement, suggesting models should originate from the same pre-trained checkpoint family, but doesn't quantify failure modes if violated. This is unresolved because the authors rely on LMC theory assuming shared initialization but don't test if the vector has semantic meaning in differently initialized weight space. Transferring the vector to a target model with same architecture but different pre-training seed would resolve this.

## Limitations
- Strictly requires identical architectures and tokenizers, preventing cross-family transfers (e.g., Qwen to Llama)
- Success depends on obtaining exact public donor checkpoints (SFT and GRPO) which are not specified
- Vector sensitivity to GRPO training configuration and hyperparameters is not fully explored

## Confidence

- **High Confidence**: Core mechanism of task arithmetic for extracting and transferring reasoning capabilities is well-established and theoretically grounded in Linear Mode Connectivity. Empirical improvements (+4.9% GSM8K, +12.3% BBH for 1.5B) are substantial and ablation study confirms vector efficacy.

- **Medium Confidence**: Domain-general reasoning generalization claim is supported by results but relies heavily on paper's own evaluations. Theoretical justification via LMC is sound but specific applicability to reasoning transfer is less directly validated.

- **Low Confidence**: Practical barrier of obtaining exact public donor checkpoints is not addressed, creating a prerequisite for reproduction. Paper doesn't discuss sensitivity to GRPO training's specific hyperparameters or reward function details.

## Next Checks
1. **Replication with Public Checkpoints**: Locate and download exact Qwen2.5-SFT and Qwen2.5-GRPO checkpoints from Hugging Face, compute reasoning vector, and apply to Qwen2.5-Instruct to verify reported +4.9% GSM8K improvement.

2. **Cross-Domain Generalization Test**: After enhancing model with math-derived reasoning vector, evaluate on diverse reasoning benchmarks (HumanEval for code, BigBenchHard for logic) to independently confirm claimed domain-generalization.

3. **Architecture Compatibility Boundary**: Systematically test limits of LMC by attempting to transfer vector to target model with slightly different configuration (different hidden dimension or tokenizer size) to identify precise point where performance degrades or fails catastrophically.