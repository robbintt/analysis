---
ver: rpa2
title: 'DePass: Unified Feature Attributing by Simple Decomposed Forward Pass'
arxiv_id: '2510.18462'
source_url: https://arxiv.org/abs/2510.18462
tags:
- depass
- attribution
- tokens
- hidden
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DePass, a unified feature attribution framework
  for Transformer models based on a single decomposed forward pass. The method decomposes
  hidden states into additive components and propagates them through the network with
  attention scores and MLP activations fixed, enabling lossless, fine-grained attribution
  without auxiliary training.
---

# DePass: Unified Feature Attributing by Simple Decomposed Forward Pass

## Quick Facts
- **arXiv ID**: 2510.18462
- **Source URL**: https://arxiv.org/abs/2510.18462
- **Reference count**: 40
- **Primary result**: DePass achieves faithful, fine-grained feature attribution across token, component, and subspace levels without auxiliary training.

## Executive Summary
DePass introduces a unified feature attribution framework for Transformer models based on a single decomposed forward pass. The method decomposes hidden states into additive components and propagates them through the network with attention scores and MLP activations fixed, enabling lossless, fine-grained attribution without auxiliary training. Experiments validate DePass across token-level, model component-level, and subspace-level tasks, demonstrating superior faithfulness in identifying critical inputs, components, and representational directions. It consistently outperforms baseline methods in comprehensiveness and sufficiency metrics while being computationally efficient and broadly applicable to different model families.

## Method Summary
DePass works by decomposing hidden states into additive components at initialization, then propagating through Transformer layers with frozen attention scores and MLP activations. For token-wise attribution, each token's hidden state is split into N additive components (one per input token). For model component-level attribution, components represent attention heads or MLP neurons. For subspace-level attribution, components represent projections onto learned subspaces. The method uses RMSNorm scaling, per-component attention output computation with fixed attention scores, and softmax-based MLP neuron attribution. Summation over components exactly reconstructs the original hidden states, ensuring lossless decomposition.

## Key Results
- Token-level attribution identifies critical input tokens with superior comprehensiveness and sufficiency compared to baselines on Known_1000 dataset
- Model component-level attribution successfully identifies top attention heads and MLP neurons critical for IOI and CounterFact tasks through masking interventions
- Subspace decomposition isolates functional dimensions (language vs. semantic subspaces), with semantic subspace consistently producing factually accurate tokens while language subspace produces stylistic output
- DePass achieves 3-5x improvement in comprehensiveness metrics compared to integrated gradients and attention rollout baselines

## Why This Works (Mechanism)

### Mechanism 1
Additive decomposition of hidden states with frozen attention/MLP activations enables lossless attribution. DePass initializes decomposed hidden states where each component represents a source (e.g., token, head, subspace). During the forward pass, attention scores and MLP activations are fixed from the standard forward pass. LayerNorm scaling distributes over components via a diagonal scaling matrix. Attention outputs are computed per-component via fixed attention weights applied to each decomposed component, and MLP outputs are distributed across components via relevance scores using softmax over dot products. Summing components reconstructs the original hidden state exactly.

### Mechanism 2
Token-wise decomposition enables precise attribution of output logits to specific input tokens. At layer 0, decomposed hidden states are initialized as one-hot per-token. These propagate through all layers via the decomposed forward pass. At the LM head, the logit for target y is decomposed via dot product with the target vector, yielding per-token attribution scores.

### Mechanism 3
Subspace decomposition isolates functional dimensions (e.g., language vs. semantic subspaces) and propagates their independent effects. Given a projection matrix P_t, hidden states are decomposed into subspace and orthogonal complement components. Each component is propagated separately via DePass, allowing independent decoding from each subspace.

## Foundational Learning

- **Transformer residual stream and layer-wise computation**: DePass operates by decomposing the residual stream and tracing components through MHSA and MLP. Understanding the residual stream is essential to initialize and propagate decompositions correctly.
  - Quick check: In a standard Transformer decoder, what is the shape of the hidden state at layer ℓ, and how does the residual connection combine attention and MLP outputs?

- **Linear projection and subspace decomposition**: Subspace-wise DePass requires constructing a projection matrix P_t from a subspace basis and decomposing hidden states into in-subspace and orthogonal components.
  - Quick check: Given a matrix W ∈ ℝ^{d×c}, how would you compute an orthogonal projection matrix onto its column space?

- **Softmax normalization and relevance distribution**: MLP attribution in DePass uses softmax over dot products to distribute neuron outputs across components.
  - Quick check: Why might softmax normalization outperform linear-weighted decomposition when distributing MLP neuron contributions?

## Architecture Onboarding

- **Component map**: Input token embeddings → Decomposed Hidden States (X^{(ℓ)}_{dec}) → RMSNorm → MHSA (fixed attention scores) → MLP (softmax attribution) → LM Head (logit attribution)

- **Critical path**:
  1. Run standard forward pass to capture attention scores and MLP activations
  2. Initialize decomposed hidden states based on attribution granularity (token, component, subspace)
  3. Propagate decomposed states through each layer using fixed attention/MLP values
  4. At the LM head, compute per-component logit contributions for attribution

- **Design tradeoffs**:
  - Granularity vs. memory: Higher M increases memory; process components sequentially if needed
  - Freezing vs. accuracy: Freezing ignores QK circuit interactions, favoring efficiency and OV-only faithfulness
  - Softmax vs. alternative normalization: Empirical results favor softmax for comprehensiveness/sufficiency

- **Failure signatures**:
  - Non-additive interactions: Check if attention scores or MLP activations were incorrectly modified
  - Poor subspace projection: Verify classifier/projection matrix quality
  - Memory overflow with high M: Reduce M or batch components

- **First 3 experiments**:
  1. Token-level sanity check: On a short prompt, verify that summing per-token contributions reconstructs the original logit exactly
  2. Component-level ablation: On IOI or CounterFact, mask top-k attention heads/neurons identified by DePass; confirm sharper accuracy drop vs. baselines
  3. Subspace separation test: Train a language classifier, run subspace-wise DePass, and decode from language vs. semantic subspaces to verify functional separation

## Open Questions the Paper Calls Out

### Open Question 1
How can DePass be extended to decompose the Query-Key (QK) circuit to trace the formation of attention patterns, rather than just the Output-Value (OV) flow? The current method freezes attention scores, ignoring how inputs influence attention formation; a solution requires handling the complexity explosion of additive decomposition in the QK space.

### Open Question 2
Is the softmax-based normalization for MLP attribution theoretically optimal, or does it merely provide the best empirical fit among heuristic choices? While empirically validated, the method lacks a formal proof that softmax is the uniquely correct allocation mechanism for non-linear activations in this decomposition framework.

### Open Question 3
How can the framework be optimized to handle high-resolution decomposition (e.g., millions of components) without requiring sequential grouping due to memory constraints? Current memory limitations force the grouping of components, which compromises the granularity of the analysis for very fine-grained tasks.

## Limitations

- Additive decomposition assumption has limited validation; freezing attention scores may miss critical QK circuit interactions that influence predictions
- Subspace decomposition quality depends on projection matrix quality with no systematic evaluation of how poor projections affect attribution results
- Memory constraints for high-resolution decomposition (neuron-level) require component grouping, potentially compromising granularity

## Confidence

- **High confidence**: Token-level attribution mechanism and experimental results
- **Medium confidence**: Model component-level attribution (attention heads and MLP neurons)
- **Low confidence**: Subspace decomposition claims (functional separation on single example)

## Next Checks

1. **QK circuit error quantification**: Implement a controlled experiment comparing DePass attribution scores against ground-truth attribution on synthetic tasks where QK circuit contributions are known. Measure systematic error introduced by freezing attention scores and quantify variation across model depth and task type.

2. **Subspace projection robustness**: Systematically evaluate subspace decomposition across 50+ prompts with varying semantic content. For each prompt, train multiple subspace classifiers with different random seeds and measure attribution consistency. Test whether functional separation holds when using random vs. task-specific projection matrices.

3. **Memory-efficiency validation**: Implement the sequential component processing strategy for neuron-level attribution on Llama-2-7B. Measure actual memory usage and runtime compared to claimed efficiency. Compare attribution results when processing components in groups vs. individually to verify grouping does not significantly alter attribution quality.