---
ver: rpa2
title: 'ICPC: In-context Prompt Compression with Faster Inference'
arxiv_id: '2501.01625'
source_url: https://arxiv.org/abs/2501.01625
tags:
- compression
- prompt
- icpc
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of processing long prompts in
  large language models (LLMs) due to fixed input sizes and quadratic computational
  scaling. The proposed method, ICPC (In-context Prompt Compression), introduces a
  novel approach that leverages transformer encoders instead of LLMs to compress prompts
  by calculating word importance through entropy-based information functions and removing
  redundant tokens.
---

# ICPC: In-context Prompt Compression with Faster Inference

## Quick Facts
- arXiv ID: 2501.01625
- Source URL: https://arxiv.org/abs/2501.01625
- Reference count: 30
- Primary result: Faster prompt compression (10-16ms vs 40-52ms) with improved BLEU (43.4 vs 42.6) and ROUGE (59.1 vs 58.8) scores

## Executive Summary
This paper introduces ICPC (In-context Prompt Compression), a novel approach to address the challenge of processing long prompts in large language models (LLMs). The method leverages transformer encoders instead of LLMs to compress prompts by calculating word importance through entropy-based information functions and removing redundant tokens. By segmenting text at phrase and clause levels and using adaptive percentile thresholding, ICPC achieves faster compression speeds while maintaining or improving performance metrics compared to existing methods.

## Method Summary
ICPC segments text into participle units (phrases/clauses) and uses a transformer encoder to calculate loss values for each unit based on contextual similarity and Masked Language Modeling (MLM) probabilities. Units with loss above a dynamically computed percentile threshold are removed. The method tests multiple encoder architectures (BERT, RoBERTa, XLNet, ALBERT, T5, DeBERTa) and targets compression ratios of 0.8, 0.6, and 0.4. The approach requires implementing participle segmentation, computing loss scores, applying percentile-based filtering, and reconstructing compressed text.

## Key Results
- Achieves compression speeds of 10-16ms compared to 40-52ms for baseline methods
- Improves BLEU score from 42.6 to 43.4 compared to LLMLingua
- Improves ROUGE score from 58.8 to 59.1 compared to LLMLingua
- Maintains consistent performance across different encoder architectures
- Shows better readability preservation in compressed outputs

## Why This Works (Mechanism)

### Mechanism 1: Encoder-based Redundancy Estimation
The method uses smaller transformer encoders (BERT, RoBERTa) to identify and remove redundant tokens faster than LLMs by calculating a "loss" value for each lexical unit based on its predictability within local context. High loss indicates a token is easily predicted by neighbors (redundant). Core assumption: MLM probabilities from encoders serve as a sufficient proxy for semantic importance.

### Mechanism 2: Participle-Level Segmentation
ICPC groups tokens into "participle units" (words, phrases, or clauses) before calculating loss, ensuring that high-dependency phrases are evaluated as a whole rather than at the individual token level. This preserves linguistic integrity during compression.

### Mechanism 3: Adaptive Percentile Thresholding
The algorithm calculates a dynamic threshold based on the p-th percentile of the loss distribution, removing units with loss above this threshold. This adapts compression intensity to the specific redundancy profile of the input text rather than using fixed compression ratios.

## Foundational Learning

- **Concept: Masked Language Modeling (MLM)**
  - Why needed: Core compression logic relies on encoder's ability to predict masked tokens using bidirectional attention
  - Quick check: How does the model calculate probability of a token if it hasn't been masked during inference? (Hint: It infers likelihood based on surrounding context embeddings)

- **Concept: Shannon Entropy & Self-Information**
  - Why needed: Paper frames compression as information theory problem where low-surprise tokens are considered low-information and redundant
  - Quick check: Does high probability p(xi) result in higher or lower calculated "loss" L(xi), and does that make the token more or less likely to be deleted?

- **Concept: Tokenization & Granularity**
  - Why needed: "Participle" step involves mapping raw tokens to higher-level semantic units
  - Quick check: How would you aggregate embeddings of "sub" and "##way" to calculate loss for the single semantic unit "subway"?

## Architecture Onboarding

- **Component map:** Input Interface -> Segmentation Module -> Encoder Engine -> Scoring Layer -> Filtering Gate -> Reconstruction
- **Critical path:** The loop inside the Scoring Layer (calculating similarity and probability for every unit) is the primary inference bottleneck
- **Design tradeoffs:**
  - Encoder Size vs. Nuance: Smaller encoders (ALBERT) are faster but may miss subtle semantic dependencies compared to larger ones (DeBERTa)
  - Granularity vs. Flexibility: Clause-level segmentation preserves grammar better but reduces fine-grained compression control compared to word-level
- **Failure signatures:**
  - Grammar Collapse: Excessive compression resulting in broken sentence structures
  - Context Drift: Removal of locally redundant but task-critical keywords
- **First 3 experiments:**
  1. Verify 10-16ms compression claim on standard EC2 instance using BERT-base against LLMLingua baseline
  2. Run compression on "arXiv Papers" dataset, switching between word-level and clause-level segmentation to observe readability changes
  3. Replace BERT with DeBERTa using same compression ratio (0.6) and measure delta in BLEU/ROUGE scores

## Open Questions the Paper Calls Out
None explicitly called out in the provided content.

## Limitations
- Fixed-length chunking (512 tokens) may lose global semantic dependencies when processing long documents
- No cross-lingual evaluation to assess robustness across morphologically rich languages
- Limited evaluation of rare entity preservation for task-critical keywords

## Confidence
- Method reproducibility: Medium (Key hyperparameters like Î±, k, and segmentation algorithm not fully specified)
- Performance claims: High (Results supported by multiple metrics and encoder comparisons)
- Speed claims: High (Direct timing comparisons with baselines provided)

## Next Checks
1. Implement and verify participle segmentation using spaCy or NLTK chunker on sample text
2. Calibrate percentile threshold p to achieve target compression ratios (0.8, 0.6, 0.4)
3. Run ablation study comparing BERT vs. DeBERTa on arXiv Papers dataset measuring BLEU/ROUGE deltas