---
ver: rpa2
title: 'Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling
  Effects'
arxiv_id: '2601.02015'
source_url: https://arxiv.org/abs/2601.02015
tags:
- metaphor
- surprisal
- novelty
- novel
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between language model
  (LM) surprisal and metaphor novelty using four datasets and 16 LM variants. The
  authors find that surprisal significantly and moderately correlates with metaphor
  novelty annotations, with the strongest correlation (r=0.499) achieved using GPT-2-base
  with a novel cloze-surprisal method.
---

# Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects

## Quick Facts
- arXiv ID: 2601.02015
- Source URL: https://arxiv.org/abs/2601.02015
- Reference count: 40
- Primary result: Surprisal correlates moderately with metaphor novelty (r=0.499 maximum), showing divergent scaling effects between corpus-based and synthetic datasets

## Executive Summary
This study investigates the relationship between language model surprisal and metaphor novelty using four datasets and 16 LM variants. The authors find that surprisal significantly correlates with metaphor novelty annotations, with the strongest correlation (r=0.499) achieved using GPT-2-base with a novel cloze-surprisal method. Two key scaling effects emerge: corpus-based datasets show decreasing correlation with model size (inverse scaling), while synthetic datasets show increasing correlation (Quality-Power Hypothesis). Instruction-tuning generally fails to improve correlations. Cloze-surprisal, which incorporates right-context information, consistently improves correlations on corpus-based data but shows mixed results on synthetic datasets. Genre analysis reveals that metaphor density and LM perplexity influence surprisal-novelty correlations.

## Method Summary
The authors conducted a systematic investigation of surprisal-metaphor novelty correlations using four metaphor datasets (two corpus-based, two synthetic) and 16 language model variants spanning different architectures and sizes. They employed both standard surprisal and a novel cloze-surprisal method that incorporates right-context information. Statistical analysis included Pearson correlation, Spearman correlation, and multiple linear regression to assess the relationship between LM-based surprisal scores and human novelty annotations. The study examined scaling effects by comparing correlations across model sizes and investigated genre-specific influences on the surprisal-novelty relationship.

## Key Results
- Maximum correlation of r=0.499 achieved between GPT-2-base surprisal and metaphor novelty annotations
- Inverse scaling effect: correlations decrease with model size for corpus-based datasets
- Quality-Power Hypothesis: correlations increase with model size for synthetic datasets
- Cloze-surprisal consistently improves correlations on corpus-based data but shows mixed results on synthetic datasets

## Why This Works (Mechanism)
The moderate correlation between surprisal and metaphor novelty suggests that surprisal captures some aspects of novelty through unexpectedness, but not the full semantic richness of metaphorical expression. The divergent scaling effects indicate that larger models may be better at recognizing synthetic metaphors but worse at detecting novel metaphors in natural text, possibly because they become more conservative in their predictions. The cloze-surprisal method's effectiveness on corpus data suggests that right-context information helps disambiguate metaphorical usage that might otherwise be misclassified as standard language.

## Foundational Learning

**Language Model Surprisal**: The negative log probability assigned by an LM to a token given its context, measuring unexpectedness. *Why needed*: Core metric for quantifying linguistic predictability. *Quick check*: Calculate surprisal values for simple n-grams in a test corpus.

**Metaphor Novelty**: Human-rated measures of how unusual or creative a metaphorical expression is. *Why needed*: Ground truth for evaluating whether LM surprisal captures creative language use. *Quick check*: Compare ratings for literal vs metaphorical expressions of similar frequency.

**Quality-Power Hypothesis**: The theory that synthetic datasets improve with model size while corpus-based datasets show inverse scaling. *Why needed*: Explains divergent correlation patterns across dataset types. *Why needed*: Framework for understanding how LM capabilities affect metaphor detection. *Quick check*: Plot correlation vs model size separately for synthetic and corpus datasets.

**Inverse Scaling**: The phenomenon where performance metrics decrease as model size increases. *Why needed*: Explains why larger models sometimes perform worse on creative language tasks. *Quick check*: Compare perplexity on metaphorical vs literal sentences across model sizes.

## Architecture Onboarding

**Component Map**: Datasets (corpus-based -> synthetic) -> Language Models (16 variants) -> Surprisal Calculation (standard -> cloze) -> Statistical Analysis (correlation -> regression) -> Results

**Critical Path**: Dataset selection → LM inference → Surprisal computation → Statistical correlation → Scaling analysis → Interpretation

**Design Tradeoffs**: Standard surprisal is computationally efficient but less accurate; cloze-surprisal is more accurate but requires additional computation; corpus-based datasets are ecologically valid but smaller; synthetic datasets are larger but potentially less natural.

**Failure Signatures**: Low correlations indicate surprisal fails to capture metaphor novelty; inconsistent scaling effects suggest dataset quality issues; genre-specific anomalies point to corpus composition problems.

**First Experiments**:
1. Calculate Pearson correlation between surprisal and novelty for a single dataset and LM pair
2. Compare standard vs cloze-surprisal on corpus-based data to verify improvement
3. Plot correlation vs model size to observe scaling patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Moderate correlation magnitudes (maximum r=0.499) indicate surprisal captures only partial aspects of metaphor novelty
- Quality-Power Hypothesis relies on synthetic datasets that may not represent natural metaphor usage patterns
- Inverse scaling effect in corpus-based datasets contradicts typical LM scaling expectations but may be influenced by sample size

## Confidence
- Core correlation findings: Medium
- Scaling effects: Medium
- Quality-Power Hypothesis: Medium-low
- Genre-specific interpretations: Medium-low

## Next Checks
1. Test the Quality-Power Hypothesis on additional synthetic metaphor datasets with varying quality controls to confirm the inverse relationship between model size and correlation strength
2. Conduct cross-linguistic validation using non-English metaphor corpora to assess whether scaling effects and correlation patterns generalize across languages
3. Implement ablation studies removing specific right-context information in the cloze-surprisal method to quantify its contribution to improved correlations on corpus-based data