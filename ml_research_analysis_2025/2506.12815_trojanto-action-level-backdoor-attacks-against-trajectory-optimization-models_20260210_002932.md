---
ver: rpa2
title: 'TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models'
arxiv_id: '2506.12815'
source_url: https://arxiv.org/abs/2506.12815
tags:
- trigger
- backdoor
- trojanto
- uni00000013
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TrojanTO, the first action-level backdoor
  attack framework designed for trajectory optimization (TO) models in offline reinforcement
  learning. Existing backdoor attacks in reinforcement learning rely on reward manipulation,
  which is ineffective for TO models due to their sequence modeling nature and focus
  on minimizing reconstruction loss.
---

# TrojanTO: Action-Level Backdoor Attacks against Trajectory Optimization Models

## Quick Facts
- arXiv ID: 2506.12815
- Source URL: https://arxiv.org/abs/2506.12815
- Reference count: 40
- First action-level backdoor attack framework for trajectory optimization models in offline RL

## Executive Summary
This paper introduces TrojanTO, the first framework for action-level backdoor attacks against trajectory optimization (TO) models in offline reinforcement learning. Unlike traditional RL backdoor attacks that manipulate rewards, TrojanTO targets TO models that minimize reconstruction loss on action sequences. The method employs alternating training to strengthen trigger-action coupling, trajectory filtering to maintain benign performance, and batch poisoning to ensure context consistency. Extensive evaluations across six D4RL environments and three TO model architectures demonstrate high attack success rates (71.9% average) while maintaining benign task performance (91.4%) with minimal attack budget (0.3% of trajectories).

## Method Summary
TrojanTO implements a bi-level optimization framework that alternates between updating trigger perturbations and model parameters to create robust backdoor behavior. The method filters trajectories based on length thresholds to avoid suboptimal data contamination, then applies batch poisoning by duplicating clean batches and modifying only one transition with the trigger pattern while replacing the target action. During training, the framework employs MI-FGSM to update triggers while freezing model parameters, then updates model parameters while freezing triggers. This alternating process continues for the first half of training, after which standard training resumes. The approach specifically targets the sequential nature of TO models, which reconstruct actions rather than optimize cumulative rewards.

## Key Results
- Achieves 71.9% average attack success rate across six D4RL environments
- Maintains 91.4% benign task performance with only 0.3% attack budget
- Outperforms existing baselines significantly in both effectiveness and stealth
- Works across three different TO model architectures (Decision Transformer, Trajectory Transformer, and C51)
- Demonstrates robustness to varying trigger dimensions and target action types

## Why This Works (Mechanism)

### Mechanism 1: Alternating Training for Trigger-Action Coupling
- **Claim:** Joint optimization of trigger perturbations and model weights creates stronger causal links than fixing either component
- **Mechanism:** Bi-level optimization framework alternates between MI-FGSM updates to trigger values (minimizing backdoor loss) and model parameter updates to reinforce the behavior
- **Core assumption:** Allowing the trigger to adapt to the model's current state prevents backdoor washout by standard training dynamics
- **Evidence:** Ablation study shows w/o AT drops CP from 0.701 to 0.517 average; related work on input-model co-optimization supports general efficacy

### Mechanism 2: Batch Poisoning for Context Consistency
- **Claim:** Limiting poisoning to single transitions within batches prevents OOD issues and ensures trigger effectiveness during clean deployment
- **Mechanism:** Duplicates clean batch and poisons only one random transition in copy, maintaining consistent context with clean data
- **Core assumption:** Model learns to associate specific trigger pattern with target action more robustly when surrounding context remains consistent
- **Evidence:** Ablation results confirm removing Batch Poisoning causes ASR drop; OOD context shift analysis validates the mechanism

### Mechanism 3: Trajectory Filtering for Stealth Preservation
- **Claim:** Discarding suboptimal trajectories before poisoning preserves benign performance by preventing overfitting to low-quality data
- **Mechanism:** Filters dataset to retain trajectories exceeding minimum length threshold, avoiding degradation from learning on failed trajectories
- **Core assumption:** Longer trajectories correlate with higher quality data and more representative behavioral policies
- **Evidence:** Removing Trajectory Filtering results in BTP drop from 0.914 to 0.850; ablation study explicitly demonstrates stealth preservation

## Foundational Learning

- **Concept: Trajectory Optimization (TO) vs. Standard RL**
  - **Why needed here:** TO models minimize reconstruction loss on actions rather than maximize cumulative reward via Bellman updates, explaining why reward-based backdoors fail
  - **Quick check:** Does the model optimize for value functions (critic) or direct action prediction (sequence modeling)?

- **Concept: Action-Level vs. Policy-Level Backdoors**
  - **Why needed here:** TrojanTO targets specific actions rather than degrading overall return, important for continuous action spaces where exact matching is difficult
  - **Quick check:** Is the goal to make agent fail generally (policy) or execute specific maneuver precisely (action)?

- **Concept: Out-Of-Distribution (OOD) Context in Transformers**
  - **Why needed here:** Transformers process sequential context; training history differing from evaluation (all states perturbed vs. only current one) causes generalization failure
  - **Quick check:** During training, does trigger see history of other triggers or history of clean states?

## Architecture Onboarding

- **Component map:** Data Loader -> Filtering Layer (length threshold Îµ) -> Poisoning Module (duplicate batch, poison 1 transition, replace target action) -> Optimizer (AT: Loop A trigger update, Loop B model update)

- **Critical path:** Alternating Training schedule is most sensitive component; improper frequency causes backdoor failure (ablation on alternation frequency in Appendix J.1)

- **Design tradeoffs:**
  - **Trigger Dimensions:** Manual selection or random search currently required as gradient-based selection didn't correlate with success
  - **Budget vs. Stealth:** 0.3% data usage preserves BTP but requires precise Batch Poisoning; increasing budget eventually plateaus performance (Table 15)

- **Failure signatures:**
  - **Low BTP, High ASR:** Failed trajectory filtering, polluting general policy
  - **Low ASR, High BTP:** Trigger dimensions/values not learned; alternating frequency too high/low or trigger forgotten during clean updates
  - **High ASR training, Low eval:** Poisoned full history during training creating OOD mismatch

- **First 3 experiments:**
  1. **Baseline Implementation:** Train Decision Transformer on HalfCheetah with TrojanTO (target action '1'), verify ASR > 0.9 and BTP > 0.9
  2. **Ablation (BP):** Same setup but poison entire sequence length instead of single step to observe ASR drop from OOD context shift
  3. **Target Action Sensitivity:** Test target types '1', 'fixed random', 'arithmetic' on Hopper to replicate finding that boundary actions are easier to inject than interior actions

## Open Questions the Paper Calls Out
None

## Limitations

- Dependence on trajectory filtering assumes longer trajectories reliably indicate higher quality data, which may not generalize across all datasets
- Requires careful tuning of alternating training frequency and trigger dimension selection without definitive optimal guidance
- Batch poisoning effectiveness may degrade in environments with highly variable state dynamics or diverse contexts
- Assumes access to clean training data for filtering, which may not be available in all deployment scenarios

## Confidence

**High Confidence:**
- Alternating training mechanism for trigger-action coupling (strong ablation support)
- Trajectory filtering's role in maintaining benign performance (empirical validation)
- Fundamental premise that reward-based attacks fail for TO models (theoretical and empirical)

**Medium Confidence:**
- Generalizability of 0.3% attack budget across all TO architectures
- Manual vs. automated trigger dimension selection effectiveness
- Batch poisoning robustness in highly dynamic environments

## Next Checks

1. **Cross-Domain Applicability Test:** Evaluate TrojanTO on non-MuJoCo environments (Atari, sparse reward tasks) to assess trajectory length as quality indicator and trigger-action coupling generalizability

2. **Trigger Dimension Optimization Study:** Systematic comparison of manual versus automated trigger dimension selection across multiple environments to determine if lack of gradient correlation is general or specific

3. **Context Variability Stress Test:** Design experiments with controlled context shifts during training to test batch poisoning effectiveness across diverse state histories during deployment