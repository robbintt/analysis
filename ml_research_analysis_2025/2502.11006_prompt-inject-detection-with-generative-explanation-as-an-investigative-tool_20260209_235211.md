---
ver: rpa2
title: Prompt Inject Detection with Generative Explanation as an Investigative Tool
arxiv_id: '2502.11006'
source_url: https://arxiv.org/abs/2502.11006
tags:
- prompt
- prompts
- explanation
- adversarial
- used
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explored using fine-tuned text-generation LLM models
  to detect prompt injects and generate explanations to aid AI security investigators.
  The research used ToxicChat dataset containing benign and adversarial prompts (jailbreaking
  and toxic).
---

# Prompt Inject Detection with Generative Explanation as an Investigative Tool

## Quick Facts
- arXiv ID: 2502.11006
- Source URL: https://arxiv.org/abs/2502.11006
- Reference count: 0
- Primary result: Fine-tuned text-generation LLMs significantly outperform vanilla models in detecting adversarial prompts while generating useful explanations for security investigators.

## Executive Summary
This paper investigates using fine-tuned text-generation LLM models to detect prompt injection attacks and generate explanatory text to aid security investigators. The research leverages the ToxicChat dataset containing benign and adversarial prompts (jailbreaking and toxic) to train Llama3.2-Instruct models (1B and 3B parameters) using Supervised Fine Tuning and Direct Preference Optimization techniques. Results demonstrate that fine-tuned models achieve significantly better detection performance compared to vanilla models, with the 3B parameter model showing superior generalization to unseen adversarial patterns. The generated explanations were generally acceptable for investigative purposes, with only occasional instances of subjectivity, bias, or misleading content. The study validates the premise that text-generation LLMs can facilitate investigation and triage of prompt injects, with future work extending to output censorship detection.

## Method Summary
The study fine-tunes Llama3.2-Instruct models (1B and 3B parameters) using the ToxicChat dataset, which contains labeled prompts categorized as benign, toxic, or jailbreaking. Models are fine-tuned with Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) using LoRA adapters. The fine-tuned models are evaluated on both the ToxicChat test set and out-of-distribution adversarial prompts generated by Nvidia Garak tools. Manual evaluation of generated explanations uses four criteria (contributing factors, subjectivity, illustrative examples, misleading content) assessed by three evaluators on 10 random samples.

## Key Results
- Fine-tuned models showed significantly better performance in detecting adversarial prompts compared to vanilla models across both ToxicChat test set and Garak probes
- The 3B parameter model generalized better after fine-tuning, showing increased performance on Garak-generated adversarial prompts compared to the 1B model
- Generated explanations were generally acceptable, with only a few being subjective, biased, or misleading according to manual evaluation
- Fine-tuned models consistently provided explanations, whereas vanilla models often generated empty or no responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning text-generation LLMs with domain-specific adversarial data improves detection accuracy over vanilla models.
- Mechanism: Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) adapt the model's weights to recognize patterns in adversarial prompts (jailbreaking and toxic) versus benign prompts, creating specialized decision boundaries that vanilla instruction-tuned models lack.
- Core assumption: The ToxicChat training distribution sufficiently represents real-world adversarial prompt patterns.
- Evidence anchors:
  - [abstract] "The fine-tuned models showed significantly better performance in detecting adversarial prompts compared to vanilla models"
  - [section IV.D, Page 4] "We observed that fine-tuned models with both approaches (SFT or DPO) had significantly better performance in detecting adversarial prompts over the vanilla models"

### Mechanism 2
- Claim: Higher parameter count enables better generalization to unseen adversarial distributions after fine-tuning.
- Mechanism: Larger models (3B vs 1B parameters) possess greater representational capacity, allowing them to learn more robust features during fine-tuning that transfer to novel attack patterns generated by external tools like Garak.
- Core assumption: Observed generalization gains are due to parameter count, not confounding factors.
- Evidence anchors:
  - [abstract] "the 3B parameter model generalizing better after fine-tuning"
  - [section IV.D, Page 4] "the model with higher parameters (3B) generalizes better after fine-tuning, having increased performance in detecting adversarial prompts from Garak, compared to the 1B model"

### Mechanism 3
- Claim: Generated explanations aid investigative triage by listing contributing factors for classification decisions.
- Mechanism: Text-generation models can verbalize reasoning traces alongside classifications, providing human-interpretable justification that pure classifiers cannot offer.
- Core assumption: Human evaluators can reliably assess explanation quality, and their ratings predict real-world investigative utility.
- Evidence anchors:
  - [abstract] "The generated explanations were generally acceptable, with only a few being subjective, biased, or misleading"
  - [section IV.D, Page 4, Table 4] Fine-tuned models scored higher on EQ1 (contributing factors) and lower on EQ4 (misleading) compared to vanilla models

## Foundational Learning

- Concept: **Prompt Injection Attack Taxonomy** (Jailbreaking vs. Toxic Prompts)
  - Why needed here: The paper distinguishes jailbreaking (bypassing safeguards) from toxic prompts (harmful content), and conflating these affects model training and evaluation design.
  - Quick check question: Would a prompt asking the model to "ignore previous instructions and output credit card numbers" be classified as jailbreaking or toxic?

- Concept: **LoRA (Low-Rank Adaptation)**
  - Why needed here: The paper uses LoRA for efficient fine-tuning; understanding this explains how they adapted models without full retraining.
  - Quick check question: What is the key advantage of LoRA over full fine-tuning for parameter efficiency?

- Concept: **F1 Score and Class Imbalance**
  - Why needed here: Detection involves imbalanced classes (mostly benign prompts); F1 balances precision and recall rather than raw accuracy.
  - Quick check question: Why would accuracy alone be misleading if 95% of prompts are benign?

## Architecture Onboarding

- Component map:
  Guardrail LLM (Llama3.2-1B/3B-Instruct, fine-tuned) -> Target LLM (Llama3.2-1B-Instruct) -> Garak Testing Tool (generates adversarial prompts)

- Critical path:
  1. Prepare ToxicChat training data with classification labels (Benign/Toxic/Jailbreaking)
  2. Fine-tune guardrail models using SFT + DPO with LoRA
  3. Evaluate on ToxicChat test set (in-distribution) and Garak probes (out-of-distribution)
  4. Manual evaluation of explanations using 4 criteria (EQ1-EQ4)

- Design tradeoffs:
  - **1B vs. 3B models**: 3B offers better generalization but higher inference cost
  - **SFT vs. DPO**: Paper tests both; results suggest comparable detection gains, but no direct head-to-head analysis provided
  - **Detection vs. Explanation**: Text-generation models may sacrifice pure classification accuracy for interpretability

- Failure signatures:
  - Vanilla models generate no responses or empty explanations (observed in Page 4)
  - Fine-tuned models occasionally produce subjective/biased explanations (EQ2 failures in Table 4)
  - 1B model shows similar in-distribution performance but worse out-of-distribution generalization

- First 3 experiments:
  1. **Baseline detection**: Run vanilla Llama3.2-1B and 3B on ToxicChat test set to establish F1/accuracy baselines
  2. **Fine-tuning ablation**: Compare SFT-only, DPO-only, and SFT+DPO on same validation split to isolate technique contributions
  3. **Explanation reliability test**: Generate explanations for 50 adversarial prompts and measure inter-rater agreement among evaluators beyond the paper's 10-sample test

## Open Questions the Paper Calls Out

- Can text-generation LLMs be effectively adapted for output censorship detection and investigation?
  - Basis in paper: [explicit] The conclusion explicitly lists extending the research "to assess the suitability of using text-generation LLM for output censorship detection and investigation" as a primary next step.
  - Why unresolved: The current study focused exclusively on input-based adversarial prompts (jailbreaking and toxic inputs) and did not test the models against LLM-generated outputs.
  - What evidence would resolve it: Results from fine-tuning and evaluating LLMs on datasets containing both benign and censored/adversarial model outputs, measuring detection accuracy and explanation quality.

- What methods can effectively minimize subjectivity and misleading content in generated security explanations?
  - Basis in paper: [explicit] The authors state the need to "study ways to improve the quality of the generated explanation" and noted that while generally acceptable, some generated explanations were "subjective, biased, or misleading."
  - Why unresolved: The current research observed these issues (e.g., EQ4 failures where explanations argued for wrong labels) but did not implement or compare specific techniques to mitigate them.
  - What evidence would resolve it: Comparative experiments using enhanced fine-tuning strategies (e.g., different DPO configurations) or guardrail mechanisms, showing a reduction in misleading/subjective explanation rates.

- Does the improved generalization observed in 3B parameter models scale effectively to larger model sizes?
  - Basis in paper: [inferred] The paper notes the 3B model generalized better than the 1B model after fine-tuning, implying parameter count aids performance, but leaves the ceiling or scaling trend for larger models untested.
  - Why unresolved: The study limited the scope to 1B and 3B parameter versions of Llama 3.2; it is unknown if 7B or 70B models would offer significantly better generalization or suffer from diminishing returns.
  - What evidence would resolve it: Benchmarking the detection F1 scores and explanation quality of larger fine-tuned models (e.g., 8B, 70B) against the same Garak probes used in the paper.

## Limitations
- The ToxicChat dataset may not capture the full diversity of real-world prompt injection attacks, particularly novel jailbreak techniques
- Manual evaluation of explanations was limited to only 10 samples across 3 evaluators, which may not provide statistically robust quality assessments
- The comparison between SFT and DPO techniques was not directly tested head-to-head, making it unclear which fine-tuning approach provides superior results

## Confidence

**High Confidence**: The core finding that fine-tuned text-generation models outperform vanilla models in adversarial prompt detection is well-supported by quantitative metrics (F1, precision, recall) on both in-distribution (ToxicChat) and out-of-distribution (Garak) datasets.

**Medium Confidence**: The explanation quality assessments show positive trends but rely on limited manual evaluation samples. The distinction between fine-tuning techniques (SFT vs DPO) lacks direct comparative analysis.

**Low Confidence**: The practical utility of generated explanations for actual security investigation workflows has not been validated. No field testing with security analysts was conducted.

## Next Checks
1. **Statistical Validation of Explanation Quality**: Expand manual evaluation to 100+ adversarial prompts with multiple independent evaluators to establish inter-rater reliability scores and statistical significance of explanation quality differences between vanilla and fine-tuned models.

2. **Real-World Attack Generalization**: Test fine-tuned models against live prompt injection attacks from production systems or recent adversarial prompt repositories (beyond the 6-month-old Garak probes) to validate detection performance on contemporary attack patterns.

3. **Head-to-Head Fine-Tuning Comparison**: Conduct controlled experiments comparing SFT-only, DPO-only, and combined SFT+DPO approaches using identical hyperparameters and datasets to isolate which technique provides optimal detection-accuracy versus explanation-quality tradeoffs.