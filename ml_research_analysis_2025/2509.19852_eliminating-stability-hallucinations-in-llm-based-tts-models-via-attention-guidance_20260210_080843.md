---
ver: rpa2
title: Eliminating stability hallucinations in llm-based tts models via attention
  guidance
arxiv_id: '2509.19852'
source_url: https://arxiv.org/abs/2509.19852
tags:
- speech
- alignment
- text
- tokens
- cosyv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses stability hallucinations in LLM-based TTS
  models, where the models may generate repetitive or omitted speech when processing
  long or difficult text. The core method involves analyzing the attention mechanism
  in LLMs to identify "alignment heads" that are crucial for text-speech alignment.
---

# Eliminating stability hallucinations in llm-based tts models via attention guidance

## Quick Facts
- **arXiv ID**: 2509.19852
- **Source URL**: https://arxiv.org/abs/2509.19852
- **Reference count**: 0
- **Primary result**: OAS-guided CosyVoice2 reduces WER by 2.1% and 1.6% on hard texts vs baseline

## Executive Summary
This paper addresses stability hallucinations in LLM-based TTS models, where the models may generate repetitive or omitted speech when processing long or difficult text. The core method involves analyzing the attention mechanism in LLMs to identify "alignment heads" that are crucial for text-speech alignment. A metric called Optimal Alignment Score (OAS) is proposed to evaluate alignment quality using the Viterbi algorithm. OAS is integrated into the training of CosyVoice2 to help the model learn continuous and stable alignment. Additionally, a chain-of-thought (CoT) approach is used, leveraging pre-trained attention values to guide the training of a student CosyVoice2 model. Experiments on the Seed-TTS-Eval and CV3-Eval test sets show that the proposed methods effectively reduce stability hallucinations without introducing negative effects. Specifically, the OAS metric shows a strong correlation with word error rate (WER) in synthesized speech (correlation coefficient of 0.638). Incorporating OAS into CosyVoice2 reduces WER by 2.1% and 1.6% under hard text scenarios on Seed-TTS-Eval and CV3-Eval, respectively, while maintaining or slightly improving subjective speech naturalness. Attention-guided training further reduces stability hallucinations, with sparse text token supervision and progress bar value supervision showing better performance than full text token supervision.

## Method Summary
The approach analyzes attention patterns in LLM-based TTS to identify "alignment heads" (middle-layer heads) responsible for text-speech alignment. OAS is computed using Viterbi algorithm on attention sub-matrices to measure alignment quality. OAS is then used as a differentiable loss during training to improve alignment. Additionally, attention-guided training uses pseudo alignment labels from a teacher model to supervise a student model, employing sparse text token supervision and progress bar value prediction to avoid boundary ambiguity and improve training efficiency.

## Key Results
- OAS shows strong correlation with WER (correlation coefficient of 0.638)
- OAS-guided training reduces WER by 2.1% and 1.6% under hard text scenarios on Seed-TTS-Eval and CV3-Eval
- Attention-guided training with sparse text token supervision and progress bar achieves best performance (WER ~10% on hard texts vs ~13.5% baseline)
- No degradation in subjective speech naturalness (SIM/UTMOS maintained or improved)

## Why This Works (Mechanism)

### Mechanism 1: Alignment Head Identification via Attention Pattern Analysis
- **Claim**: Specific attention heads in middle transformer layers function as implicit "alignment heads" that control text-speech correspondence in decoder-only TTS models
- **Mechanism**: In decoder-only architectures, text and speech tokens are concatenated in a single sequence. Self-attention heads must learn implicit alignment without cross-attention. Analysis of CosyVoice2 revealed that middle-layer heads (specifically layers 8-9 of 24) exhibit monotonic forward alignment patterns from speech tokens back to text tokens, similar to cross-attention in encoder-decoder models
- **Core assumption**: Alignment quality in these specific heads causally determines stability hallucinations
- **Evidence anchors**:
  - [abstract]: "analyzed the alignment mechanism between text tokens and speech tokens in LLMs"
  - [Section 2.1]: "middle-layer heads contain specific 'alignment heads'... weight regions from output speech tokens to input text tokens exhibit global forward alignment paths"
  - [Section 3.2]: "coefficient between OAS and WER is 0.638" — correlation established but not causation
  - [corpus]: Limited corpus validation; related work (Valle-R, MegaTTS3) assumes alignment importance but through different mechanisms
- **Break condition**: Layer distribution may differ across model architectures/scales; the 8-9 pattern is specific to CosyVoice2's 24-layer Qwen-0.5B backbone

### Mechanism 2: Optimal Alignment Score (OAS) as Differentiable Alignment Quality Metric
- **Claim**: The Viterbi-based OAS metric captures alignment quality and can serve as a differentiable training objective
- **Mechanism**: 
  1. Extract sub-matrix A (speech×text) from full attention probabilities
  2. Run Viterbi dynamic programming to find optimal monotonic path maximizing cumulative attention
  3. OAS = (sum of attention along optimal path) / (sum of all attention in alignment region)
  4. Simplify to differentiable loss: L_OAS = -1/L_s × Σ log A_i,Pi
- **Core assumption**: Monotonic continuous alignment is both necessary and sufficient for stable TTS; Viterbi path captures this
- **Evidence anchors**:
  - [abstract]: "OAS, which employs the Viterbi algorithm to evaluate text-speech alignment quality"
  - [Section 2.2]: "OAS can reflect the continuity and monotonicity of the attention between whole text tokens and whole speech tokens"
  - [Section 3.3]: "CV2 OAS... WER decreased by 2.1% and 1.6% respectively under hard text scenarios"
  - [corpus]: Weak corpus support; related work uses different alignment metrics (MAD in MegaTTS3, forced alignment in Valle-R)
- **Break condition**: Highly expressive speech with intentional timing variations may violate monotonicity assumption; gradient flow through Viterbi may be unstable for non-smooth attention distributions

### Mechanism 3: Attention-Guided Training via Pseudo Labels from Teacher Model
- **Claim**: Optimal alignment paths extracted from a pre-trained teacher model can serve as pseudo forced alignment labels for student training, eliminating the need for ground-truth alignment annotations
- **Mechanism**:
  1. Run teacher inference, extract optimal path from highest-OAS head
  2. Convert durations to sparse text token targets: instead of full repetition [t1,t1,t2,t2,t2], use sparse [t1,M,M,t2,M,M] where each text token appears once at random valid position
  3. Add progress bar value prediction: p_n = (Σd_i for i≤n) / (Σd_i for all i)
  4. Train student with CoT framework using these intermediate supervision signals
- **Core assumption**: Teacher model's attention encodes useful alignment knowledge that transfers to student; sparsity handles boundary ambiguity and pseudo-label noise
- **Evidence anchors**:
  - [abstract]: "pre-trained attention value is employed to guide the training of the student CosyVoice2 via chain-of-thought"
  - [Section 2.4]: "sparse text tokens can effectively avoid issues such as insufficient accuracy of pseudo-force alignment and ambiguous boundary definition"
  - [Section 3.4, Table 3]: Sparse format achieves 97.10% train accuracy vs 90.44% for full; best WER (9.984%) with sparse + progress bar
  - [corpus]: Valle-R requires "accurate forced alignment labels [that] are difficult to obtain on large-scale datasets" — validates the problem this addresses
- **Break condition**: Teacher errors propagate to student; sparsity strategy may discard useful boundary timing information; repeated structural patterns in text may still confuse model if progress bar is insufficient

## Foundational Learning

- **Concept: Decoder-Only Self-Attention for Sequence-to-Sequence Tasks**
  - Why needed here: LLM-based TTS concatenates text and speech tokens in one sequence, relying on causal self-attention rather than explicit cross-attention. Alignment must emerge implicitly through learned attention patterns
  - Quick check question: In a decoder-only TTS model with input sequence [text_tokens, speech_tokens], why can't standard cross-attention monotonicity constraints be directly applied?

- **Concept: Viterbi Algorithm and Monotonic Alignment Search**
  - Why needed here: OAS uses Viterbi to find the maximum-probability monotonic path through the attention matrix, providing a principled way to measure alignment quality
  - Quick check question: Given attention matrix A of shape (L_speech, L_text), the Viterbi recurrence is dp[i,j] = A[i,j] + max(dp[i-1,j-1], dp[i-1,j]). What constraint does this enforce on the alignment path?

- **Concept: Chain-of-Thought (CoT) as Intermediate Supervision**
  - Why needed here: The attention-guided training injects intermediate predictions (text tokens, progress values) into the generation process, providing dense supervision for alignment learning
  - Quick check question: Why does the sparse repetition format [t1, M, M, t2, M, M] (where M=masked) outperform full repetition [t1, t1, t2, t2, t2] for pseudo-label training?

## Architecture Onboarding

- **Component map**:
  ```
  Input Text → Tokenizer
                      ↓
  [Text Tokens | Prompt Speech | Target Speech] → LLM Backbone (Qwen-0.5B, 24 layers)
                      ↓
         Layer 8-9 Attention Heads (designated "alignment heads")
                      ↓
         Attention Mask: restrict to text×speech region
                      ↓
         OAS Loss: maximize probability along Viterbi path
                      ↓
  Alternative/Combined Path:
  Teacher Model → Extract Optimal Path from Highest-OAS Head
                      ↓
         Generate Pseudo Labels (sparse text + progress bar)
                      ↓
  Student Model ← CoT Training with Pseudo Labels
                      ↓
  Speech Token Predictions → Codec Decoder → Audio Waveform
  ```

- **Critical path**:
  1. **Identify alignment heads**: Compute average top-k OAS per layer on validation set; select layers with significantly higher scores
  2. **Apply attention masking**: Restrict designated heads to alignment region (text×speech submatrix)
  3. **Add OAS regularization**: L_OAS = -1/L_s × Σ log A_i,Pi applied to masked heads
  4. **Generate pseudo-labels** (if using attention-guided training): Run teacher inference, extract Viterbi paths, convert to sparse format
  5. **Train student**: Standard next-token prediction + text token supervision + progress bar L1 loss + first-order difference loss

- **Design tradeoffs**:
  - **Fixed vs. learned head selection**: Paper manually designates layers 8-9 based on OAS analysis. Assumption: stable across training. Risk: may not generalize to other architectures
  - **OAS loss vs. attention-guided training**: OAS loss requires no teacher; attention-guided requires teacher inference but provides denser supervision. Paper combines both for best results
  - **Sparse vs. full pseudo-labels**: Sparse achieves higher prediction accuracy (97.10% vs 90.44%) by avoiding boundary ambiguity, but discards timing granularity
  - **Progress bar necessity**: Adds computational overhead but critical for repeated structural patterns; L1 + first-order difference loss enforces monotonic progress

- **Failure signatures**:
  - **High WER despite high OAS**: Alignment heads incorrectly identified for this architecture; re-analyze layer-wise OAS distribution
  - **Degraded naturalness (SIM/UTMOS drop)**: Attention constraints too restrictive; the paper reports no degradation but monitors this explicitly
  - **Training divergence with OAS loss**: Gradient flow issues through Viterbi; check attention probability smoothness
  - **Repetitive speech on hard texts**: Progress bar not effectively guiding generation; verify progress bar loss weights
  - **Error propagation in student model**: Teacher pseudo-labels have systematic errors; visualize teacher alignment paths for failure cases

- **First 3 experiments**:
  1. **Validate OAS-WER correlation on your data**: Synthesize 100-400 sentences from hard text subset, compute per-sample WER and OAS (top-5 heads averaged). Calculate Pearson correlation. Expect r ≈ 0.6; if r < 0.3, re-examine alignment head selection
  2. **Ablate layer selection for alignment heads**: Train variants with OAS loss applied to different layer groups (early: 4-6, middle: 8-9, late: 12-14). Compare WER on hard texts to validate paper's layer 8-9 choice
  3. **Compare supervision strategies**: Train three student models: (a) full text token supervision, (b) sparse text token supervision, (c) sparse + progress bar. Measure train/dev text token accuracy and final WER. Replicate finding that sparse + progress bar achieves lowest WER (~10% on hard texts vs ~13.5% baseline)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the identification of "alignment heads" be automated and dynamic during training, rather than manually designating specific layers (Layers 8 and 9) based on pre-training statistics?
- **Basis in paper:** [inferred] Section 2.3 states that the authors "designated half of the attention heads in 8-th layer and 9-th layer... as alignment heads" based on average OAS analysis. This suggests the current selection is a heuristic fixed after initial analysis, potentially limiting adaptability if optimal alignment heads shift during training or differ in other model architectures.
- **Why unresolved:** The paper does not propose a mechanism for continuous or differentiable selection of these heads; it relies on a static configuration determined prior to the OAS fine-tuning experiments.
- **What evidence would resolve it:** An experiment comparing the static head selection against a learnable gating mechanism or an adaptive selection algorithm that identifies high-OAS heads on-the-fly during the training process.

### Open Question 2
- **Question:** Does the Optimal Alignment Score (OAS) correlation with Word Error Rate (WER) hold true for non-tonal languages or languages with significantly different prosodic structures than Mandarin?
- **Basis in paper:** [inferred] The experimental validation is restricted to Mandarin Chinese (WenetSpeech4TTS, Seed-TTS-Eval, CV3-Eval). The introduction of the "progress bar" value and text repetition strategies (Section 2.4) are designed to handle hard texts, but the correlation coefficient of 0.638 between OAS and WER is only verified on this specific linguistic dataset.
- **Why unresolved:** The alignment constraints (monotonicity/continuity) are likely universal, but the specific attention patterns for tonal languages (like Mandarin) might differ from stress-timed languages (like English), affecting the OAS metric's predictive power.
- **What evidence would resolve it:** Evaluation of the OAS-guided CosyVoice2 model on a multilingual dataset (e.g., English or cross-lingual benchmarks) to verify if the 0.638 correlation coefficient and WER reduction persist.

### Open Question 3
- **Question:** To what extent does the accuracy of the "pseudo forced alignment labels" limit the student model's performance, particularly in "hard text" scenarios where the teacher model might itself be unstable?
- **Basis in paper:** [inferred] Section 2.4 describes using the pre-trained CosyVoice2 to generate pseudo-labels. While the paper notes the teacher is stable on common texts (WER < 4%), the method relies on these labels to guide the student. If the teacher hallucinates on the specific "hard text" used for training, the generated pseudo-labels would be erroneous, potentially reinforcing errors.
- **Why unresolved:** The paper demonstrates improved student performance but does not analyze failure cases where the teacher's pseudo-labels were likely incorrect or how robust the sparse text token method is to significant teacher misalignment.
- **What evidence would resolve it:** An analysis of the "teacher-student" alignment gap, specifically measuring the quality of pseudo-labels on the hard text subset before training, or an ablation study using ground-truth forced alignment (if available) vs. pseudo-labels to quantify the performance ceiling.

## Limitations
- Architecture-specificity: The alignment head identification mechanism is tightly coupled to CosyVoice2's specific architecture (Qwen-0.5B, 24 layers) and may not generalize to different models
- Correlation vs. causation: While OAS shows strong correlation with WER (r=0.638), the paper doesn't establish causal mechanisms
- Evaluation scope: Results are demonstrated on Mandarin datasets only, without validation on other languages or domains

## Confidence
- **High confidence**: The core methodology of using OAS as a differentiable alignment quality metric is well-grounded. The Viterbi-based approach for extracting optimal monotonic paths from attention matrices is standard and reliable. The correlation between OAS and WER is statistically significant and reproducible.
- **Medium confidence**: The effectiveness of OAS regularization in reducing WER (2.1% and 1.6% improvements) is demonstrated, but the causal mechanism remains somewhat unclear. The sparse text token supervision strategy shows promise, but the specific randomization approach and its optimal parameters are not thoroughly explored.
- **Low confidence**: The generalizability of the "alignment heads are in layers 8-9" claim across different architectures and scales. The assumption that monotonic alignment is always desirable for all speech patterns (expressive speech with intentional timing variations may violate this). The long-term stability of these improvements across extended training or different dataset distributions.

## Next Checks
1. **Architecture transfer validation**: Apply the OAS analysis and head selection methodology to a different LLM-based TTS architecture (different backbone, different layer count). Run the same correlation analysis between OAS and WER across layers to verify if the middle-layer alignment pattern holds. If the pattern doesn't transfer, investigate what architectural features correlate with alignment head emergence.

2. **Causal mechanism ablation**: Design an experiment where OAS is manipulated independently of other factors. For example, artificially boost OAS for specific attention heads through direct manipulation while keeping other training signals constant. Measure whether this targeted OAS increase specifically reduces WER compared to random OAS manipulation. This would help establish whether OAS is causally linked to stability rather than just correlated.

3. **Cross-linguistic generalization test**: Apply the complete OAS+attention-guided training pipeline to a non-Mandarin TTS dataset (e.g., English, multilingual, or low-resource language). Compare WER improvements and OAS-WER correlations against the Mandarin results. If performance degrades significantly, investigate whether language-specific alignment patterns require different OAS formulations or head selection strategies.