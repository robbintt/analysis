---
ver: rpa2
title: 'ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation'
arxiv_id: '2512.07178'
source_url: https://arxiv.org/abs/2512.07178
tags:
- shap
- explanations
- liver
- explanation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces contextualSHAP, a Python package that extends
  SHAP by integrating large language model (LLM) explanations to improve user comprehension.
  The package uses user-defined parameters such as feature aliases, descriptions,
  and background context to generate contextualized textual explanations alongside
  SHAP visualizations.
---

# ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation

## Quick Facts
- arXiv ID: 2512.07178
- Source URL: https://arxiv.org/abs/2512.07178
- Reference count: 31
- Primary result: Contextual explanations improved layperson understandability scores from 1.55 (SHAP-only) to 3.28, while experts showed minimal gains.

## Executive Summary
ContextualSHAP is a Python package that enhances SHAP explanations by integrating large language model (LLM) generated textual context. The tool accepts user-defined parameters including feature aliases, descriptions, and background context to generate contextualized explanations alongside standard SHAP visualizations. Tested on a liver disease classification task using XGBoost and TreeSHAP with seven participants, the system showed significant improvements in perceived understandability for non-experts while experts preferred concise outputs.

## Method Summary
The study used the UCI HCV dataset (608 records after preprocessing) to train an XGBoost classifier for binary liver disease prediction. SHAP values were computed using TreeSHAP, and the contextualSHAP package wrapped these outputs with feature aliases, descriptions, and background context. The system then called OpenAI's GPT-4o API to generate textual explanations. User evaluations were conducted with seven participants (five laypersons, two medical experts) using Likert-scale surveys across three explanation formats: standard SHAP, SHAP with generic context, and contextualSHAP with domain-specific context.

## Key Results
- Laypersons' understandability scores improved from 1.55 (SHAP-only) to 3.28 (contextualSHAP)
- Experts showed minimal gains (3.9 → 4.2) and preferred concise outputs
- Classification F1 score achieved was 0.78 on the liver disease task
- Two doctors participated alongside five non-expert participants in the user study

## Why This Works (Mechanism)

### Mechanism 1: Semantic Grounding via Context Injection
The architecture injects user-defined feature aliases and descriptions into LLM prompts, forcing interpretation of numerical SHAP values through a domain-specific lens rather than generic statistical terms. This assumes the LLM can reliably map numerical attributions to provided semantic descriptions without hallucinating false causal links.

### Mechanism 2: Multi-Modal Cognitive Offloading
Visual SHAP plots provide spatial pattern recognition while contextual text offers causal narrative. This combination lowers cognitive load for non-experts by allowing cross-referencing of "what" (the plot) with "why" (the text), though conflicting information between modalities would increase confusion.

### Mechanism 3: Audience-Specific Complexity Control
The package uses a binary "Expert/General" reader parameter to modify LLM system prompts, adjusting verbosity and technical depth. This assumes the binary classification sufficiently captures user mental models, though misalignment occurs if actual user expertise differs from the selected level.

## Foundational Learning

- **Concept: Shapley Values (SHAP)**
  - Why needed: These represent the raw material being translated
  - Quick check: If a feature has a SHAP value of +0.5, did it increase or decrease the model output compared to the average prediction?

- **Concept: TreeSHAP vs. Model-Agnostic SHAP**
  - Why needed: The study uses XGBoost, which allows TreeSHAP's efficient exact computation
  - Quick check: Why is TreeSHAP faster for XGBoost models than KernelSHAP?

- **Concept: Prompt Engineering / Context Window**
  - Why needed: LLMs have token limits requiring careful context inclusion
  - Quick check: If you have 100 features and 1000 samples, can you send all SHAP values to the LLM at once?

## Architecture Onboarding

- **Component map:** Trained Model (XGBoost) -> TreeSHAP (calculates values) -> contextualSHAP wrapper (accepts aliases, descriptions, background, reader level) -> OpenAI API (GPT-4o) -> SHAP Plot + Contextualized Text

- **Critical path:** 1) Generating SHAP values via TreeExplainer 2) Mapping raw feature names to feature_aliases 3) Constructing the "Guard Prompt" to prevent symbol misinterpretation

- **Design tradeoffs:** GPT-4o provides high-quality reasoning but introduces cost ($0.017/request) and latency (14.9s). Token limits require truncating SHAP values for large feature sets.

- **Failure signatures:** Hallucinated causality, symbol confusion (E[f(X)] vs f(x)), context drift when additional_background is too vague.

- **First 3 experiments:**
  1. Ablation on Context: Run contextualSHAP with no aliases/descriptions vs. full context on a non-technical user
  2. Guard Prompt Stress Test: Feed adversarial SHAP plots and verify text matches visual math
  3. Expert Efficiency Test: Measure time-to-insight for experts using Standard SHAP vs. ContextualSHAP (Expert Mode)

## Open Questions the Paper Calls Out

- Does contextualSHAP improve actual decision-making performance rather than only perceived understandability?
- How does contextualSHAP perform across diverse domains beyond healthcare and with larger, more representative user populations?
- Does contextualSHAP enhance user trust in model predictions, and does this trust correlate with appropriate reliance?
- How reliable are LLM-generated explanations in accurately interpreting SHAP visualizations without hallucination or misinterpretation?

## Limitations
- Sample size of seven participants provides weak statistical power for subgroup analyses
- Manual curation of feature descriptions creates potential bias and scalability challenges
- Binary "Expert/General" reader classification may oversimplify user needs
- GPT-4o integration introduces significant cost and latency constraints

## Confidence

- **High Confidence:** Semantic grounding mechanism is well-supported by architectural details and consistent improvement in layperson scores
- **Medium Confidence:** Multi-modal cognitive offloading is theoretically sound but relies on general multimedia learning theory
- **Low Confidence:** Audience-specific complexity control is supported by survey results but lacks detailed validation of binary classification adequacy

## Next Checks
1. Conduct larger-scale user study (n≥30) with stratified sampling to establish statistical significance
2. Implement ablation study comparing contextualSHAP outputs with and without feature aliases/descriptions
3. Test system robustness against adversarial inputs with negative base values and complex feature interactions