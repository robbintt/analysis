---
ver: rpa2
title: How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators
arxiv_id: '2510.26704'
source_url: https://arxiv.org/abs/2510.26704
tags:
- regularization
- reconstruction
- inverse
- training
- invertible
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces two regularization terms for training invertible
  neural networks (INNs) in linear inverse problems, bridging the gap between operator
  approximation and data-dependent regularization. The first regularization, based
  on the log-determinant of the Jacobian, connects to a smoothed posterior mean estimator,
  providing implicit denoising.
---

# How Regularization Terms Make Invertible Neural Networks Bayesian Point Estimators

## Quick Facts
- **arXiv ID**: 2510.26704
- **Source URL**: https://arxiv.org/abs/2510.26704
- **Reference count**: 29
- **One-line primary result**: Two regularization terms for INNs in linear inverse problems bridge operator approximation and data-dependent regularization, with divergence-based regularization achieving MAP-like properties and excellent reconstruction accuracy.

## Executive Summary
This work introduces two principled regularization terms for training invertible neural networks in linear inverse problems. The first, based on the log-determinant of the Jacobian, connects to a smoothed posterior mean estimator with implicit denoising properties. The second, based on the divergence of the network, closely resembles the maximum a posteriori (MAP) estimator and yields highly accurate reconstructions. Theoretical analysis shows both regularizers preserve forward operator fidelity while introducing data-dependent priors. Numerical experiments on 2D toy problems validate the findings, demonstrating that the divergence-based approach in particular achieves excellent performance in both operator approximation and reconstruction quality.

## Method Summary
The method trains invertible residual networks (iResNets) to approximate linear forward operators while incorporating two regularization terms. The first regularization term, $-\hat{\delta}^2 \log|\det D\phi_{\rightarrow}(x)|$, connects to a smoothed posterior mean estimator. The second, $-\hat{\delta}^2 \nabla \cdot \phi_{\rightarrow}(x)$, closely resembles the MAP estimator. Both are added to the standard MSE loss on the forward map. The networks use residual blocks with enforced Lipschitz constraints ($L < 1$) to guarantee invertibility via fixed-point iteration. The theoretical analysis shows how these regularizers shape the learned forward map to encode data-dependent priors while maintaining operator approximation fidelity.

## Key Results
- The divergence regularization term yields reconstructions closely resembling the MAP estimator, with exact equivalence under strongly log-concave priors
- The log-determinant regularization provides implicit denoising effects through a smoothed posterior mean interpretation
- Both regularizers enable simultaneous preservation of forward operator fidelity and data-dependent reconstruction
- Numerical experiments on 2D toy problems demonstrate superior performance, particularly for the divergence-based approach

## Why This Works (Mechanism)

### Mechanism 1
The log-determinant regularization term $-\hat{\delta}^2 \log|\det D\phi_{\rightarrow}(x)|$ yields reconstructions resembling a smoothed posterior mean estimator. The term acts as a volume-expansion penalty that counteracts contraction from ill-posed forward operators. Theorem 1 shows the minimizer satisfies $\phi_{\rightarrow}(x) = Ax - \hat{\delta}^2 \nabla_y(\log \phi_{\rightarrow\#}p_X)(\phi_{\rightarrow}(x))$, which Theorem 1's proof interprets as an implicit Euler step on a smoothed score field—similar to posterior mean denoising but with stronger smoothing.

### Mechanism 2
The divergence regularization term $-\hat{\delta}^2 \nabla \cdot \phi_{\rightarrow}(x)$ yields reconstructions with MAP-like properties, and under strong log-concavity of the prior, recovers the exact MAP estimator. Maximizing divergence in high-density regions reduces contraction there. Theorem 2 proves the loss is equivalent to fitting $\phi_{\rightarrow}(x)$ to the regularized forward operator $A^*Ax - \hat{\delta}^2 \nabla_x(\log p_X)(x)$, which matches the first-order optimality condition for MAP estimation.

### Mechanism 3
Invertible networks trained with these regularizers simultaneously preserve forward operator fidelity while enabling data-dependent reconstruction. The bijective structure guarantees a stable inverse without requiring separate inverse training. The regularizers inject prior information through the Jacobian (log-det) or divergence, shaping the learned forward map to encode data-dependent priors while the base loss $\frac{1}{2}\|\phi_{\rightarrow}(x) - z_\delta\|^2$ enforces operator approximation.

## Foundational Learning

- **MAP vs. Posterior Mean Estimators**: Why needed here: The two regularizers target different Bayesian point estimators; understanding their properties (implicit vs. explicit Euler on score fields, regression-to-mean in PM) clarifies when to use each. Quick check question: For a bimodal prior, which estimator would you expect to produce sharper but potentially mode-averaged reconstructions?

- **Score Functions and Tweedie's Formula**: Why needed here: Both regularizers operate via score functions $\nabla \log p$; the posterior mean extension of Tweedie's formula (Lemma 1) provides the theoretical basis for the denoising interpretation. Quick check question: In denoising ($A = I$), what does $\delta^2 \nabla_y(\log p_Y)(y_\delta)$ represent geometrically?

- **iResNet Invertibility via Lipschitz Constraints**: Why needed here: The architecture choice determines how inversion is computed (fixed-point iteration) and what regularization terms are tractable. Quick check question: If Lip($f_i$) = 1.1 for some layer, what guarantee is lost?

## Architecture Onboarding

- **Component map**: x -> [iResNet layers: Id - $f_i$ with Lip($f_i$) < 1] -> $\phi_{\rightarrow}(x)$ ≈ $A^*Ax$ (or $Ax$ if $Y = X$) -> z$\delta$ -> [fixed-point iteration per layer] -> $\phi_{\leftarrow}(z_\delta)$ ≈ reconstruction

- **Critical path**: 
  1. Enforce Lipschitz constraint during training (spectral normalization or similar)
  2. Compute divergence/log-det efficiently (Hutchinson estimator for high dimensions)
  3. Invert via fixed-point iteration (50-100 iterations for training stability)

- **Design tradeoffs**:
  - $\hat{\delta}$ selection: Controls regularization strength; typically set to target noise level $\delta$
  - Regularizer choice: Divergence for MAP-like (sharper, better operator fidelity); log-det for PM-like (smoother, may better handle multimodal uncertainty)
  - Iteration count: More iterations -> more memory (use deep equilibrium techniques for backward pass)

- **Failure signatures**:
  - Exploding inverses: Lip($f_i$) approaching or exceeding 1
  - Poor reconstruction despite low forward error: Regularization weight $\hat{\delta}$ too small or prior mismatch
  - Grid visualization shows no density-dependent warping: Regularizer not being computed or weighted correctly

- **First 3 experiments**:
  1. Replicate the 2D bimodal denoising experiment ($A = I$) with divergence regularization; visualize grid deformation to confirm MAP-like behavior per Figure 1
  2. Test on ill-conditioned $A_\epsilon$ ($\epsilon \to 0$) with both regularizers; plot forward and reconstruction MSE vs. noise level as in Figure 4
  3. Ablate $\hat{\delta}$: Train with $\hat{\delta} \in \{0.5\delta, \delta, 2\delta\}$ to observe regularization strength effects on the tradeoff between operator approximation and reconstruction quality

## Open Questions the Paper Calls Out

- **Can the divergence-based regularization framework be theoretically and empirically extended to nonlinear inverse problems?**
  - Basis in paper: The conclusion states, "From an applied perspective, it would be valuable to extend divergence-based regularization to nonlinear inverse problems—both in theory and in practice."
  - Why unresolved: The theoretical analysis in Section 4 (Theorems 1 and 2) and the numerical experiments in Section 5 are restricted to linear forward operators $A$ (or $A^*A$).
  - What evidence would resolve it: A derivation of the first-order optimality conditions for a nonlinear operator $\mathcal{A}$ and successful reconstruction results on a nonlinear benchmark problem (e.g., PDE-constrained optimization).

- **Does divergence-based training effectively learn invertible proximal operators for Plug-and-Play (PnP) algorithms with guaranteed convergence?**
  - Basis in paper: The conclusion proposes that "divergence-based training offers a principled way to learn invertible proximal operators in Plug-and-Play algorithms" to align with the MAP estimator rather than the posterior mean.
  - Why unresolved: The current work focuses on full network inversion for reconstruction, whereas PnP requires the learned component to function as a robust proximal step within an iterative scheme, which was not tested.
  - What evidence would resolve it: Integration of the trained network into a PnP framework (e.g., ADMM or FISTA) demonstrating convergence properties and stable denoising performance on standard image datasets.

- **Do the theoretical connections to Bayesian estimators hold in high-dimensional imaging settings where stochastic trace estimators are required?**
  - Basis in paper: Section 5.1.1 notes that exact computation of regularization terms is infeasible for high dimensions, necessitating Hutchinson's trace estimator, but Section 5 experiments are limited to 2D toy problems where exact computation is feasible.
  - Why unresolved: The stochastic approximation of the divergence or log-determinant introduces noise into the gradient, and it is unverified if the precise MAP or posterior mean equivalence is maintained in this high-dimensional, approximate regime.
  - What evidence would resolve it: Numerical experiments on high-dimensional inverse problems (e.g., CT or MRI reconstruction) showing that the network trained with stochastic estimators still approximates the theoretical behavior derived in the paper.

## Limitations
- Theoretical claims rely on restrictive assumptions: Y = X for log-determinant regularization and strong log-concavity for exact MAP equivalence
- Numerical experiments limited to 2D toy problems, leaving scalability to high-dimensional inverse problems uncertain
- Connection between regularizers and Bayesian estimators not validated against ground-truth posteriors

## Confidence
- **High**: The mechanism by which the divergence regularization mimics MAP estimation (Mechanism 2) is strongly supported by Theorem 2 and Corollary 1, with Figure 5 providing visual confirmation
- **Medium**: The log-determinant mechanism (Mechanism 1) is mathematically sound (Theorems 1 and Lemma 1), but its denoising effect is "implicit" and harder to verify quantitatively in the 2D experiments
- **Medium**: The claim that the regularizers enable "data-dependent regularization" is valid, but the strength and optimality of this prior compared to classical approaches (e.g., Tikhonov) is not rigorously compared

## Next Checks
1. **3D Scaling Test**: Replicate the denoising experiment (A = I) on a 3D bimodal prior to assess whether the regularization effects (especially the implicit smoothing from log-det) scale effectively
2. **Prior Robustness**: Train with a non-log-concave prior (e.g., mixture of three Gaussians) and evaluate whether the divergence regularizer still produces reasonable MAP-like reconstructions, or if it breaks down as the theory suggests
3. **Operator Robustness**: Test on a severely ill-conditioned operator (e.g., A_ε with ε → 0) and compare the reconstruction error of the two regularizers against a classical Tikhonov method to quantify the practical benefit of the data-dependent prior