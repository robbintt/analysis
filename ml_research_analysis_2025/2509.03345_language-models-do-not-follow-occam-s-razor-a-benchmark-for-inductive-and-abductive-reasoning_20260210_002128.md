---
ver: rpa2
title: 'Language Models Do Not Follow Occam''s Razor: A Benchmark for Inductive and
  Abductive Reasoning'
arxiv_id: '2509.03345'
source_url: https://arxiv.org/abs/2509.03345
tags:
- reasoning
- each
- hypotheses
- observations
- dalpist
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work focuses on evaluating LLMs' inductive and abductive reasoning
  capabilities, which are essential for solving real-world problems but less explored
  than deductive reasoning. The authors introduce a programmable and synthetic dataset,
  INABHYD, where each reasoning example consists of an incomplete world model and
  observations.
---

# Language Models Do Not Follow Occam's Razor: A Benchmark for Inductive and Abductive Reasoning

## Quick Facts
- arXiv ID: 2509.03345
- Source URL: https://arxiv.org/abs/2509.03345
- Reference count: 37
- Primary result: LLMs struggle with complex inductive/abductive reasoning tasks even with reasoning-enhancing techniques like RLVR and in-context learning

## Executive Summary
This paper introduces INABHYD, a synthetic benchmark for evaluating LLMs' inductive and abductive reasoning capabilities using first-order logic ontology trees. The benchmark reveals that while models can handle simple scenarios with shallow ontology trees and single hypotheses, they struggle significantly with complex world models and multiple hypotheses. Even state-of-the-art models like DeepSeek-R1 and GPT-4o show degraded performance as ontology depth increases. The authors propose a quality metric based on Occam's Razor to evaluate hypothesis parsimony, finding that LLM-generated hypotheses are notably less parsimonious than ground truth.

## Method Summary
The authors create a programmable synthetic dataset where each example consists of an incomplete world model (FOL ontology tree with hidden axioms) and observations. LLMs must generate hypotheses explaining the observations under this incomplete model. The benchmark tests three reasoning types: inferring properties (inductive), inferring membership relations (abductive), and inferring subtype relations (both). A quality metric rewards hypotheses that explain many observations with few rules, penalizing redundancy. Experiments evaluate five LLMs across different ontology tree heights (1-4) and reasoning scenarios, using zero-shot and 8-shot in-context learning with in-distribution and out-of-distribution demonstrations.

## Key Results
- Model performance drops significantly as ontology tree height increases, with accuracy falling from above 80% to below 50% when height increases from 1 to 2
- RLVR-trained models show 10-20% performance improvement, primarily through deductive verification of generated hypotheses
- In-distribution in-context demonstrations improve performance on complex ontology trees (heights 3-4), while out-of-distribution demonstrations provide minimal benefit
- LLM-generated hypotheses consistently show lower quality scores than ground truth, indicating failure to apply Occam's Razor effectively

## Why This Works (Mechanism)

### Mechanism 1: Ontology Tree Height Controls Reasoning Complexity
Performance degrades as ontology depth increases because hypothesis generation requires traversing longer inference chains while maintaining consistency across coupled hypotheses. Each layer in the ontology tree adds branching factors (2-3 children per node) that exponentially expand the hypothesis space while linearly increasing the depth of proof trees LLMs must implicitly construct.

### Mechanism 2: Verification-Oriented Training Enables Hypothesis Quality Improvement
RLVR-trained models improve at inductive/abductive tasks primarily by learning to verify their own hypotheses deductively before outputting them. Models generate candidate hypotheses, then simulate deductive proofs to check whether each hypothesis actually explains the observations—catching errors like wrong ontology direction or redundant hypotheses.

### Mechanism 3: In-Distribution Demonstrations Transfer Ontology Navigation Patterns
In-context examples matching the target ontology height help LLMs learn hierarchical traversal patterns, while out-of-distribution examples provide minimal transfer. Demonstrations with matching tree structures expose LLMs to the specific pattern of "identify common ancestor → propose generalization at that level," which they can then apply to new problems with similar depth.

## Foundational Learning

- **Concept: First-Order Logic (FOL) Ontology Trees**
  - Why needed here: The entire dataset is generated from FOL ontology trees with membership relations (e.g., `cat(Amy)`), property relations (`∀x(cat(x) → cute(x)`), and subtype relations (`∀x(ragdoll(x) → cat(x))`)
  - Quick check question: Given "All mammals are warm-blooded" and "Sam is a mammal," can you derive "Sam is warm-blooded"? If not, review universal instantiation.

- **Concept: Inductive vs. Abductive vs. Deductive Reasoning**
  - Why needed here: The paper explicitly distinguishes these: inductive generalizes from instances to rules, abductive finds explanations for observations, deductive derives conclusions from premises
  - Quick check question: Given "Amy has a rash, Sam has a rash, Tom has a rash," what type of reasoning would conclude "All members of this group have rashes"?

- **Concept: Occam's Razor / Parsimony in Hypothesis Quality**
  - Why needed here: The paper introduces a quality metric that rewards hypotheses that explain many observations with few rules, penalizing redundant or overly specific hypotheses
  - Quick check question: If observations A, B, C can be explained by either "All X are Y" or by "A is Y, B is Y, C is Y," which has higher quality and why?

## Architecture Onboarding

- **Component map:**
  Ontology Generator -> Hypothesis Hider -> Observation Generator -> Natural Language Translator -> LLM -> Evaluator

- **Critical path:**
  1. Generate ontology tree with target height
  2. Hide axioms to create incomplete world model
  3. Generate observations for each hidden axiom
  4. Translate to natural language prompt
  5. LLM produces hypotheses
  6. Parse hypotheses and compute metrics (weak/strong accuracy, quality)

- **Design tradeoffs:**
  - Fictional vs. real concepts: Fictional prevents contamination but reduces ecological validity
  - FOL expressiveness: Limits reasoning to unary predicates and Horn clauses; higher-order logic would enable more complex scenarios but complicate evaluation
  - Observation sufficiency: Observations are guaranteed to support ground truth; real-world scenarios may have ambiguous or insufficient data

- **Failure signatures:**
  - Wrong ontology direction: Hypothesizing "All mammals are cats" instead of "All cats are mammals"
  - Unnecessary hypotheses: Proposing both "All cats are cute" and "All ragdolls are cute" when only the first is needed
  - Trivial hypotheses: Reusing observations as hypotheses (e.g., "Amy is cute" to explain "Amy is cute")
  - Entity hallucination: Introducing concepts/members from demonstrations into test responses
  - Member-concept confusion: Treating "Amy" as a concept or "wumpus" as a member

- **First 3 experiments:**
  1. Reproduce the single-hypothesis baseline across all three task types (infer property, infer membership, infer subtype) at height 1-2 to validate your evaluation pipeline matches reported weak/strong accuracy numbers
  2. Test the quality metric sensitivity by manually constructing hypotheses with known redundancy levels and verifying the metric correctly penalizes non-parsimonious explanations
  3. Run ablation on in-context demonstration distribution: compare 0-shot, 8-shot in-distribution, and 8-shot out-of-distribution on height-3 multi-hypothesis examples to confirm the ~10-15% improvement pattern from in-distribution examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can reinforcement learning with verifiable rewards (RLVR) that explicitly encodes parsimony principles significantly improve LLMs' hypothesis quality on inductive and abductive reasoning tasks?
- Basis in paper: The authors state: "Another crucial future direction is improving the quality of LLMs' hypotheses. For that, LLMs may benefit from RLVR that explicitly conveys principles of parsimony in inductive and abductive reasoning with an appropriate reward."
- Why unresolved: Current RLVR methods show moderate gains but were designed for deductive reasoning. No RLVR approach yet explicitly rewards hypothesis simplicity or coverage of observations.
- What evidence would resolve it: A study comparing standard RLVR against parsimony-aware RLVR on INABHYD, measuring hypothesis quality scores rather than just accuracy.

### Open Question 2
- Question: Does fine-tuning LLMs on synthetic inductive/abductive reasoning data transfer to improved performance on real-world scientific discovery or medical diagnosis tasks?
- Basis in paper: The authors ask: "whether large language models fine-tuned on this synthetic dataset (using supervised fine-tuning or reinforcement learning from verifiable rewards) could improve their performance on real-world tasks involving inductive and abductive reasoning."
- Why unresolved: The benchmark uses fictional world models to avoid contamination, but the relationship between synthetic benchmark performance and real-world applications remains untested.
- What evidence would resolve it: Fine-tuning experiments showing that improvements on INABHYD correlate with gains on real-world tasks like hypothesis generation in scientific literature or differential diagnosis.

### Open Question 3
- Question: Would extending the benchmark to higher-order logic (HOL) representations reveal additional limitations in LLMs' reasoning capabilities beyond those observed with first-order logic?
- Basis in paper: The authors note: "One limitation of this work is that first-order logic (FOL) limits the expressiveness of the world model and the reasoning example. We leave it as a future work to use higher-order logic (HOL) to generate more complex world models."
- Why unresolved: FOL cannot express certain kinds of generalizations (e.g., properties of properties, quantification over predicates) that may be important for more sophisticated inductive reasoning.
- What evidence would resolve it: Construction of an HOL-based benchmark variant and comparison of LLM performance patterns across FOL and HOL versions.

## Limitations

- The synthetic nature of INABHYD limits ecological validity, as models trained on real-world data may not generalize to this fictional ontology space
- Focus on unary predicates and Horn clauses in FOL excludes many real-world reasoning scenarios involving higher-order logic, temporal reasoning, or numerical relationships
- The paper does not investigate how models handle ambiguous or contradictory observations, which are common in real-world abductive reasoning

## Confidence

**High Confidence**: The core finding that LLMs struggle with complex ontology trees and multiple hypotheses is well-supported by systematic experiments across multiple models and configurations. The quality metric based on Occam's Razor provides a principled way to evaluate hypothesis parsimony.

**Medium Confidence**: The claim that RLVR training improves performance through deductive verification is plausible but relies on post-hoc observation of model behavior rather than direct evidence of the verification process. The exact mechanism by which verification improves inductive/abductive performance remains somewhat speculative.

**Low Confidence**: The generalizability of findings to real-world reasoning tasks is uncertain given the synthetic nature of the benchmark. The paper does not validate whether improvements on INABHYD translate to better performance on practical abductive reasoning problems.

## Next Checks

1. Test model performance on INABHYD with intentionally ambiguous observations to assess how models handle uncertainty in abductive reasoning.
2. Evaluate whether finetuning on INABHYD examples improves performance beyond in-context learning, particularly for height-3 and height-4 ontology trees.
3. Conduct ablation studies varying the branching factor independently from tree height to isolate their individual effects on reasoning complexity.