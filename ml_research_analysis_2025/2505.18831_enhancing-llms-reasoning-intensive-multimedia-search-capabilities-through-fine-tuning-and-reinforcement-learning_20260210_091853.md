---
ver: rpa2
title: Enhancing LLMs' Reasoning-Intensive Multimedia Search Capabilities through
  Fine-Tuning and Reinforcement Learning
arxiv_id: '2505.18831'
source_url: https://arxiv.org/abs/2505.18831
tags:
- search
- arxiv
- language
- query
- searchexpert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of existing LLM-driven search
  agents in handling complex reasoning-intensive queries and multimedia content, particularly
  their reliance on prompt engineering and inefficient Python-based search plan representations.
  The authors propose SearchExpert, a two-stage training framework that introduces
  an efficient natural language representation for search plans, supervised fine-tuning
  for searching (SFTS) with automated dataset construction, and reinforcement learning
  from search feedback (RLSF) that uses search result quality as reward signals.
---

# Enhancing LLMs' Reasoning-Intensive Multimedia Search Capabilities through Fine-Tuning and Reinforcement Learning

## Quick Facts
- arXiv ID: 2505.18831
- Source URL: https://arxiv.org/abs/2505.18831
- Reference count: 40
- Primary result: SearchExpert achieves 82.33% accuracy on FinSearchBench-24 vs 76.20% for FinSearch

## Executive Summary
This paper addresses the limitations of existing LLM-driven search agents in handling complex reasoning-intensive queries and multimedia content, particularly their reliance on prompt engineering and inefficient Python-based search plan representations. The authors propose SearchExpert, a two-stage training framework that introduces an efficient natural language representation for search plans, supervised fine-tuning for searching (SFTS) with automated dataset construction, and reinforcement learning from search feedback (RLSF) that uses search result quality as reward signals. Additionally, they develop a multimedia understanding and generation agent to process visual inputs and outputs. Experiments show that SearchExpert outperforms commercial search methods, achieving 82.33% accuracy on FinSearchBench-24 (vs. 76.20% for FinSearch) and 71.50% on SearchExpertBench-25 (vs. 32.50% for Perplexity Pro), while reducing token consumption by 42-53% compared to Python-based implementations. Human evaluations confirm superior analytical completeness and reasoning capabilities.

## Method Summary
The authors propose SearchExpert, a two-stage training framework that addresses LLM-driven search limitations through supervised fine-tuning for searching (SFTS) with automated dataset construction and reinforcement learning from search feedback (RLSF). The framework introduces a novel natural language representation for search plans to improve efficiency, and includes a multimedia understanding and generation agent for handling visual inputs and outputs. The RLSF component uses search result quality as reward signals to optimize the search agent's performance over time.

## Key Results
- SearchExpert achieves 82.33% accuracy on FinSearchBench-24, outperforming FinSearch's 76.20%
- On SearchExpertBench-25, SearchExpert reaches 71.50% accuracy compared to 32.50% for Perplexity Pro
- The framework reduces token consumption by 42-53% compared to Python-based implementations
- Human evaluations confirm superior analytical completeness and reasoning capabilities

## Why This Works (Mechanism)
The framework's success stems from its two-stage training approach that combines supervised fine-tuning with reinforcement learning. The natural language search plan representation improves efficiency by reducing token consumption compared to traditional Python-based implementations. RLSF uses search result quality as reward signals, enabling the agent to learn optimal search strategies through iterative feedback. The multimedia understanding component extends the agent's capabilities beyond text-based queries to handle visual content effectively.

## Foundational Learning
- **Supervised Fine-Tuning for Searching (SFTS)**: Why needed - to provide initial training on structured search tasks; Quick check - verify dataset quality and coverage of diverse query types
- **Reinforcement Learning from Search Feedback (RLSF)**: Why needed - to optimize search strategies through iterative improvement; Quick check - validate reward signal alignment with human judgment
- **Natural Language Search Plan Representation**: Why needed - to reduce token consumption and improve efficiency; Quick check - measure token savings compared to Python-based representations
- **Multimedia Understanding Agent**: Why needed - to handle visual inputs and outputs in search queries; Quick check - evaluate visual question answering accuracy
- **Automated Dataset Construction**: Why needed - to scale training data creation for diverse search scenarios; Quick check - assess dataset diversity and difficulty distribution
- **Search Result Quality as Reward Signal**: Why needed - to align optimization with user satisfaction; Quick check - compare reward-based optimization against alternative metrics

## Architecture Onboarding

**Component Map**: User Query -> Natural Language Parser -> Search Plan Generator -> RLSF Controller -> Search API -> Result Aggregator -> Multimedia Processor -> Response Generator

**Critical Path**: User Query → Natural Language Parser → Search Plan Generator → RLSF Controller → Search API → Result Aggregator → Response Generator

**Design Tradeoffs**: The natural language representation sacrifices some programmatic precision for token efficiency and interpretability. RLSF requires careful reward design to avoid optimization for metrics that don't align with user satisfaction. The multimedia component adds complexity but enables handling of visual content.

**Failure Signatures**: Poor reward signal design leading to reward hacking, inadequate multimedia understanding causing incorrect visual interpretation, overly aggressive token reduction harming search quality, and insufficient diversity in automated dataset construction limiting generalization.

**First Experiments**:
1. Validate the natural language representation's efficiency by comparing token consumption against Python-based implementations on identical queries
2. Test RLSF reward signal effectiveness by measuring performance improvements across training iterations
3. Evaluate multimedia understanding accuracy on a diverse set of visual question answering tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The newly introduced benchmarks (FinSearchBench-24 and SearchExpertBench-25) lack independent validation and their reliability remains uncertain
- The multimedia understanding component is mentioned but lacks quantitative evaluation results
- The human evaluation methodology lacks details about rater expertise, inter-rater reliability, and specific evaluation criteria

## Confidence

**High confidence**: The general approach of combining supervised fine-tuning with reinforcement learning for search agents is technically sound and aligns with established LLM training methodologies.

**Medium confidence**: The reported benchmark results are internally consistent but depend heavily on the quality and representativeness of newly introduced evaluation datasets.

**Low confidence**: Claims about multimedia understanding capabilities and their integration into the search pipeline lack sufficient empirical validation.

## Next Checks
1. Release the benchmarks (FinSearchBench-24 and SearchExpertBench-25) with detailed annotation guidelines and difficulty distributions to enable independent evaluation and comparison
2. Conduct ablation studies comparing natural language vs Python-based search plan representations while holding other variables constant to isolate the claimed efficiency gains
3. Provide quantitative evaluation of the multimedia understanding component, including accuracy metrics for visual question answering and image generation quality assessments