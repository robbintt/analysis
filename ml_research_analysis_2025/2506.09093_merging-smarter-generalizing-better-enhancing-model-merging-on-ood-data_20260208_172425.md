---
ver: rpa2
title: 'Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data'
arxiv_id: '2506.09093'
source_url: https://arxiv.org/abs/2506.09093
tags:
- task
- performance
- merging
- lwptv
- ours
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of improving model merging performance
  on out-of-domain (OOD) data, which is often overlooked in favor of in-domain accuracy.
  The authors propose LwPTV (Layer-wise Pruning Task Vector), a method that uses a
  saliency score to measure parameter redundancy in task vectors.
---

# Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data

## Quick Facts
- **arXiv ID:** 2506.09093
- **Source URL:** https://arxiv.org/abs/2506.09093
- **Authors:** Bingjie Zhang, Hongkang Li, Changlong Shi, Guowei Rong, He Zhao, Dongsheng Wang, Dandan Guo, Meng Wang
- **Reference count:** 40
- **Primary result:** LwPTV improves OOD generalization while preserving ID performance by pruning low-saliency parameters and replacing them with pretrained model parameters.

## Executive Summary
This paper addresses the challenge of improving model merging performance on out-of-domain (OOD) data, which is often overlooked in favor of in-domain accuracy. The authors propose LwPTV (Layer-wise Pruning Task Vector), a method that uses a saliency score to measure parameter redundancy in task vectors. By pruning low-saliency parameters and replacing them with corresponding pretrained model parameters, LwPTV enhances OOD generalization while preserving ID performance. Experiments on multiple datasets and architectures show significant improvements in H-scores, demonstrating the method's effectiveness and flexibility across various model merging techniques.

## Method Summary
LwPTV enhances model merging by identifying and pruning redundant parameters in task vectors. The method computes a layer-wise saliency score based on the absolute deviation from the mean task vector, then prunes layers with low scores (default ratio η=0.7). These pruned parameters are replaced with corresponding pretrained model parameters to restore OOD generalization. A shared mask (OR operation) across all tasks ensures ID performance preservation. The approach is compatible with various merging techniques like Task Arithmetic and AdaMerging, and shows consistent improvements across different model architectures and task combinations.

## Key Results
- LwPTV consistently improves H-scores (harmonic mean of ID and OOD accuracy) across multiple model merging methods
- The method achieves better OOD generalization while maintaining or slightly improving ID performance
- Layer-wise pruning proves more effective than parameter-wise pruning for OOD tasks while being computationally efficient
- Experiments on 8 ID datasets and 13 OOD datasets demonstrate robust performance across diverse scenarios

## Why This Works (Mechanism)

### Mechanism 1: Saliency-Guided Redundancy Filtering
Pruning task vector layers with low "saliency" (low deviation from the mean across tasks) reduces interference and improves OOD generalization. The method calculates a saliency score for each layer by measuring the L1 distance between a specific task's vector and the average of all task vectors. Layers with low scores are considered "redundant" and are discarded, effectively denoising the task-specific signal. Core assumption: Parameters that exhibit low diversity across different tasks do not encode discriminative, task-specific features and are safe to remove to recover pretrained generalization.

### Mechanism 2: Pretrained Anchor Restoration
Reverting pruned parameters to their original pretrained values explicitly recovers the OOD generalization capacity lost during fine-tuning. Fine-tuning often distorts generalizable features to fit ID data. By replacing low-saliency parameters with pretrained values, the merged model is "anchored" back to the generalizable representation space of the pretrained model. Core assumption: The pretrained model contains superior generalizable features for OOD data compared to the merged or fine-tuned models.

### Mechanism 3: Conservative Critical Layer Retention (Shared Mask)
Applying a union (OR operation) of task-specific masks ensures that ID performance is preserved by keeping any layer critical to at least one task. To prevent accidental pruning of layers critical for specific ID tasks, the method constructs a final mask where a parameter is kept if any task mask indicates high saliency. This conservative strategy prioritizes ID recall over maximum OOD recovery. Core assumption: Critical layers for different tasks rarely overlap completely, but any critical layer must be preserved to maintain multi-task capability.

## Foundational Learning

- **Concept: Task Vectors**
  - Why needed here: The entire method operates on "Task Vectors" (τ_k = θ_k - θ_pre). You must understand that these vectors represent the direction and magnitude of updates required to specialize a model.
  - Quick check question: If two task vectors point in opposite directions for a specific parameter, what happens if you average them? (Answer: They cancel out, potentially removing the feature).

- **Concept: OOD vs. ID Generalization Trade-off**
  - Why needed here: The paper's central thesis is that standard merging optimizes for ID data at the cost of OOD robustness. You need to grasp why fine-tuning degrades OOD performance (feature distortion).
  - Quick check question: Why might a model with 100% accuracy on training data fail completely on a slightly perturbed test image? (Answer: It learned spurious correlations or "overfit" to ID noise).

- **Concept: Layer-wise Pruning vs. Parameter-wise Pruning**
  - Why needed here: LwPTV calculates saliency per layer, not per individual weight. This exploits the structural redundancy of Transformers.
  - Quick check question: Why might dropping an entire layer be safer for OOD than dropping random individual weights scattered across the network? (Answer: Layers often function as cohesive units; partial disruption can break functional logic more than removing a whole redundant unit).

## Architecture Onboarding

- **Component map:** Pretrained model + fine-tuned models -> Task Vectors -> Layer-wise Saliency Scores -> Binary masks -> Shared mask (OR operation) -> Masked task vectors -> Merged model
- **Critical path:** The Saliency Score Calculation (Eq 5). If this score does not correctly identify redundancy (e.g., if all layers have similar scores), the pruning becomes random or degenerate.
- **Design tradeoffs:**
  - Pruning Ratio (η): Higher η (0.8+) favors Pretrained Model (High OOD, Low ID). Lower η (0.3) favors Merged Model (Low OOD, High ID). The paper finds 0.7 to be a sweet spot.
  - Granularity: The paper uses Layer-wise pruning. Appendix B.10 shows this is faster and more effective for OOD than parameter-wise pruning, which preserves ID better but is computationally expensive.
- **Failure signatures:**
  - Catastrophic ID Drop: Likely caused by setting the pruning threshold η too high or failing to apply the shared mask (OR operation).
  - No OOD Improvement: Suggests the saliency score is not effectively discriminating redundant layers, or the base merging method is already "close enough" to the pretrained model.
- **First 3 experiments:**
  1. Baseline Establishment: Run standard Task Arithmetic on 8 tasks vs. LwPTV-enhanced Task Arithmetic. Plot ID vs. OOD accuracy to visualize the "Pareto frontier" shift.
  2. Saliency Validation: Randomly shuffle the saliency scores (ablation) and verify that performance drops, proving the content of the score matters, not just the sparsity.
  3. Hyperparameter Sensitivity: Sweep η (pruning ratio) from 0.0 to 1.0 to find the specific "knee" in the curve where ID performance starts to crash for your specific dataset domain.

## Open Questions the Paper Calls Out

### Open Question 1
Can LwPTV be adapted to merge models with heterogeneous architectures or different initializations? The method is contingent upon identical model architectures and shared initializations, restricting broader applicability. The current saliency score relies on element-wise correspondence between task vectors, which breaks down when parameter structures or dimensions differ. What evidence would resolve it: A mechanism for cross-architecture alignment (e.g., neuron matching) that allows saliency calculation without identical parameter indices.

### Open Question 2
Can the pruning strategy be refined to strictly guarantee ID performance preservation? The method may compromise ID performance and identifies preserving or augmenting ID performance as a future goal. The current "OR" operation for the shared mask is a heuristic that may fail to preserve complex parameter interdependencies required for specific ID tasks. What evidence would resolve it: A pruning technique that provides theoretical or empirical bounds on ID performance degradation while maximizing OOD gains.

### Open Question 3
How does the semantic diversity of the task set influence the reliability of the layer-wise saliency score? The method calculates saliency as deviation from the mean task vector, implicitly assuming the mean represents a stable baseline of "redundancy." If the merged tasks are highly dissimilar (orthogonal), the mean vector may not represent a meaningful shared state, potentially leading to the pruning of useful parameters. What evidence would resolve it: An ablation study on synthetic task sets with controlled variance to measure how task correlation correlates with pruning effectiveness.

## Limitations

- The method assumes identical model architectures and shared initializations, limiting applicability to heterogeneous model merging scenarios
- The fixed pruning ratio (η=0.7) may not be optimal across all task combinations, with limited systematic sensitivity analysis
- The theoretical justification for why cross-task variance indicates redundancy is weak, relying primarily on empirical observation

## Confidence

- **High confidence:** The core mechanism of using pretrained model parameters to restore OOD generalization - supported by clear empirical evidence showing pretrained models outperform merged models on OOD data
- **Medium confidence:** The saliency-based pruning approach - works well empirically but the theoretical justification for why cross-task variance indicates redundancy is weak
- **Medium confidence:** The shared mask strategy - effectively preserves ID performance, but the paper doesn't explore scenarios where task conflicts might make this approach suboptimal

## Next Checks

1. **Task Similarity Ablation:** Systematically test LwPTV on pairs of highly similar vs. highly dissimilar tasks to validate whether the saliency score remains meaningful across the full spectrum of task relationships.

2. **Pretrained Model Dependency:** Evaluate LwPTV when using a weaker pretrained model (e.g., one not pretrained on the same domain as the target tasks) to test the limits of the "pretrained anchor" assumption.

3. **Layer vs. Parameter Granularity:** Compare layer-wise pruning against parameter-wise pruning on the same tasks using identical saliency metrics to isolate whether the granularity choice or the saliency calculation itself drives performance differences.