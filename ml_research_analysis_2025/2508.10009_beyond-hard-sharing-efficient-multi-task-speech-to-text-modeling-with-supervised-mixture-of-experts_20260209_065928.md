---
ver: rpa2
title: 'Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised
  Mixture of Experts'
arxiv_id: '2508.10009'
source_url: https://arxiv.org/abs/2508.10009
tags:
- s-moe
- data
- speech
- decoder
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a Supervised Mixture of Experts (S-MoE) approach
  to mitigate task interference in multi-task speech-to-text modeling. Unlike traditional
  MoE, S-MoE eliminates learnable gating functions by using task-specific guiding
  tokens to route inputs to designated expert networks, reducing computational overhead.
---

# Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts

## Quick Facts
- arXiv ID: 2508.10009
- Source URL: https://arxiv.org/abs/2508.10009
- Reference count: 0
- Primary result: 6.35% relative WER improvement over hard-parameter sharing baseline

## Executive Summary
This paper introduces Supervised Mixture of Experts (S-MoE), a novel approach to multi-task speech-to-text modeling that eliminates learnable gating functions by using task-specific guiding tokens to route inputs to designated expert networks. Unlike traditional MoE, S-MoE maintains constant computational overhead regardless of expert count by computing only one expert per layer at inference time. The method is applied to a Transformer-based model for joint automatic speech recognition (ASR) and speech translation (ST) across mixed-bandwidth audio, achieving significant performance improvements over standard hard-parameter sharing baselines.

## Method Summary
S-MoE replaces traditional MoE gating networks with predefined routing logic based on task identity and bandwidth metadata. For a Transformer encoder-decoder architecture (12L/6L), the feedforward networks (FFNs) in each layer are replaced with two parallel experts. Encoder S-MoE routes to NB or WB experts based on input bandwidth, while decoder S-MoE routes to ASR or ST experts based on task tag tokens (<transcribe>/<translate> + language tags). The model maintains shared attention and convolution layers while specializing FFN parameters per task/bandwidth. Training uses interleaved single-task batches, and inference computes only the designated expert per layer, achieving the same active parameter count as the baseline despite having more total parameters.

## Key Results
- DecS-MoE achieves 8.06% relative WER improvement and 1.77% BLEU improvement over Base-Model
- EncDecS-MoE achieves 6.35% relative WER reduction in NB environment and 2.39% in WB
- Both models use 144M trainable parameters vs 107M for baseline, but only 107M active parameters during inference

## Why This Works (Mechanism)

### Mechanism 1
Replacing learnable gating networks with predefined routing based on task/bandwidth metadata eliminates computational overhead while preserving expert specialization. The gating function G′(x) uses hardcoded conditional logic to select exactly one expert per input, maintaining constant inference cost regardless of total expert count. Core assumption: Task identity and bandwidth information are reliably available at both training and inference time. Evidence anchors: [abstract] "eliminates the need for training gating functions by utilizing special guiding tokens to route each task to its designated expert" and [section 2.1] "S-MoE employs a predefined gating function G′, eliminating the need for additional gating network training". Break condition: If task labels are noisy, ambiguous, or unavailable at inference, routing fails.

### Mechanism 2
Isolating task-specific parameters in separate FFN experts mitigates negative transfer between ASR and ST tasks. Each task gets a dedicated FFN while self-attention layers remain shared, allowing shared representation learning while preventing gradient conflicts in the FFN. Core assumption: Task interference primarily manifests in the FFN layers rather than attention layers. Evidence anchors: [section 1] "By assigning each task to a separate feedforward network, S-MoE overcomes the limitations of hard-parameter sharing" and [section 4.1] DecS-MoE outperforms DecFFNx2 (same parameter count, no expert separation). Break condition: If tasks are highly correlated with minimal interference, S-MoE's parameter overhead may not justify marginal gains.

### Mechanism 3
Bandwidth-specific encoder experts prevent spectral mismatch degradation when processing mixed NB/WB audio. NB signals lack high-frequency content present in WB. Separate experts allow NB expert to specialize on limited spectral content while WB expert retains full-band processing. Core assumption: Bandwidth-induced performance gaps stem from FFN representation conflicts, not from earlier convolution or attention stages. Evidence anchors: [section 2.1.1] "Models trained on NB tend to underperform when processing WB speech... WB signals are processed by expert E0, whereas NB signals are handled by expert E1" and [section 4.2] EncDecS-MoE achieves 6.35% relative WER reduction in NB environment. Break condition: If spectral differences are minimal, benefits may diminish.

## Foundational Learning

- Concept: **Mixture of Experts (MoE) routing**
  - Why needed here: S-MoE is a constrained variant of MoE; understanding standard MoE gating (learned softmax over experts) clarifies what S-MoE removes and why.
  - Quick check question: Can you explain why standard MoE requires training a gating network and what computational cost this adds?

- Concept: **Task interference / negative transfer in multi-task learning**
  - Why needed here: The core problem S-MoE solves; understanding gradient conflicts helps diagnose when expert separation helps versus when shared parameters suffice.
  - Quick check question: What types of task pairs are most susceptible to negative transfer (e.g., similar inputs with different outputs vs. different inputs with similar outputs)?

- Concept: **Speech bandwidth and spectral characteristics**
  - Why needed here: The encoder S-MoE routes based on NB (8 kHz) vs WB (16 kHz); understanding frequency content differences explains why separate experts help.
  - Quick check question: What frequency range is lost when downsampling 16 kHz audio to 8 kHz, and how might this affect phoneme discrimination?

## Architecture Onboarding

- Component map: Audio input → bandwidth detection → Encoder S-MoE selects NB/WB expert per encoder block → Encoded representation → decoder input with task tag → Decoder S-MoE selects ASR/ST expert per decoder block → Output → softmax over BBPE vocabulary (40k tokens)

- Critical path: 1) Audio input → bandwidth detection → Encoder S-MoE selects NB/WB expert per encoder block; 2) Encoded representation → decoder input with task tag → Decoder S-MoE selects ASR/ST expert per decoder block; 3) Output → softmax over BBPE vocabulary (40k tokens)

- Design tradeoffs:
  - Parameter efficiency: 144M trainable params (EncDecS-MoE) vs 107M (Base), but both use 107M active params during inference
  - Routing complexity vs flexibility: S-MoE is simpler (no learned gating) but cannot dynamically discover expert assignments; requires a priori task/bandwidth labels
  - Expert count: Fixed at 2 per S-MoE layer in this work; scaling to more tasks requires linear parameter growth

- Failure signatures:
  - Missing or incorrect task tag at inference → wrong decoder expert activated → ASR outputs translation or vice versa
  - Bandwidth mislabeled during training → encoder expert receives mismatched spectral content → degraded representations
  - Interleaved batch training not properly balanced → one expert undertrained relative to the other

- First 3 experiments:
  1. Decoder-only S-MoE validation: Train DecS-MoE on ASR+ST (WB only), compare against Base-Model and DecFFNx2. Verify that expert separation outperforms simple parameter scaling.
  2. Encoder S-MoE bandwidth ablation: Train EncDecS-MoE on NB/WB mix. Test each bandwidth variant separately to confirm that routing correctness matters (i.e., swap routing and observe degradation).
  3. Inference efficiency benchmark: Measure wall-clock latency and memory for EncDecS-MoE vs Base-Model. Confirm that active parameter count matches (107M) despite larger trainable parameter pool (144M).

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance and stability of S-MoE scale when applied to a significantly larger number of tasks or languages compared to the binary setup tested? Basis: [explicit] "Future work will focus on expanding S-MoE to support more speech tasks and multilingual capabilities, further enhancing its real-world applicability." Why unresolved: The current study only validates the architecture on a binary classification of tasks and bandwidths, leaving the complexity of managing routing for dozens of tasks unexplored. What evidence would resolve it: Empirical results showing S-MoE convergence and error rates on a dataset involving >5 distinct tasks compared to a dense baseline.

### Open Question 2
Does the static routing of S-MoE provide superior performance compared to a standard Mixture of Experts model with a learned gating network? Basis: [inferred] The paper argues that S-MoE is more efficient than traditional MoE because it "eliminates the need for training gating functions," but the experiments only compare S-MoE against hard-sharing baselines and parameter-increased variants, not against a learned MoE. Why unresolved: Without a direct comparison to a learnable-gating MoE, it is unclear if the performance gains are due to the specific "supervised" routing logic or simply the addition of specialized parameters. What evidence would resolve it: A controlled ablation study comparing S-MoE against a standard MoE (with learned gating) on the same dataset, analyzing the trade-off between routing accuracy, training overhead, and final WER/BLEU scores.

### Open Question 3
Is the hard, discrete routing mechanism robust to label noise or ambiguous input characteristics? Basis: [inferred] The gating functions defined in Equations 2 and 3 rely on hard, binary thresholds based on metadata (e.g., "if x is WB signals"). Why unresolved: The paper assumes perfect metadata alignment (task tags and bandwidth labels), but real-world scenarios might involve ambiguous signals where hard routing could force an input into a suboptimal expert. What evidence would resolve it: Experiments evaluating model degradation when task tags or bandwidth labels are randomly corrupted or when processing bandwidth-ambiguous audio.

## Limitations
- Performance evaluation assumes perfect availability of task labels and bandwidth metadata at inference time
- Scalability to more than two tasks or languages remains untested
- No empirical comparison against standard MoE with learned gating to validate computational savings claims
- Bandwidth routing assumes clean separation of NB/WB signals; mixed-bandwidth streams or codec artifacts could complicate routing decisions

## Confidence
- **High Confidence**: The claim that S-MoE reduces computational overhead by eliminating learnable gating functions is well-supported by the architecture description and experimental results under stated conditions
- **Medium Confidence**: The assertion that task interference primarily manifests in FFN layers is plausible given ablation results but not definitively proven
- **Low Confidence**: The bandwidth-specific routing claim is based on a single controlled experiment; robustness to diverse or noisy bandwidth scenarios is uncertain

## Next Checks
1. **Robustness to Missing/Incorrect Labels**: Intentionally corrupt or remove task tags and bandwidth metadata during inference, and measure performance degradation to quantify sensitivity to real-world label noise.

2. **Standard MoE Baseline Comparison**: Implement and train a standard learned MoE baseline (with softmax gating) under identical conditions, and compare both parameter efficiency and task performance to empirically validate computational savings claim.

3. **Scalability to More Tasks**: Extend S-MoE to handle three or more tasks (e.g., ASR, ST, and intent classification), and measure whether performance gains scale linearly or plateau due to routing complexity or expert specialization limits.