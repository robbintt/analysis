---
ver: rpa2
title: 'Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain'
arxiv_id: '2504.13950'
source_url: https://arxiv.org/abs/2504.13950
tags:
- performance
- data
- arxiv
- training
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates data sampling strategies for RLVR training
  in the medical domain. The authors explore four sampling approaches from MedQA-USMLE:
  random sampling (baseline), and filtering using Phi-4, Gemma-3-27b-it, and Gemma-3-12b-it
  models.'
---

# Open-Medical-R1: How to Choose Data for RLVR Training at Medicine Domain

## Quick Facts
- arXiv ID: 2504.13950
- Source URL: https://arxiv.org/abs/2504.13950
- Reference count: 31
- Primary result: Models trained on filtered MedQA-USMLE samples outperform random sampling baselines in medical domain performance

## Executive Summary
This paper investigates data sampling strategies for RLVR training in the medical domain using the MedQA-USMLE dataset. The authors compare four sampling approaches: random sampling (baseline), and filtering using Phi-4, Gemma-3-27b-it, and Gemma-3-12b-it models. Using Gemma-3-12b-it as the base model with GRPO, they evaluate performance across MMLU, GSM8K, MMLU-Pro, and CMMLU benchmarks. The key finding is that filtered data generally outperforms random sampling, with self-filtered samples achieving superior medical domain performance (0.6745 MMLU) but reduced robustness across different benchmarks, while filtering with larger same-series models yields better overall robustness.

## Method Summary
The authors implement a data filtering strategy where a pre-trained model (Phi-4, Gemma-3-27b-it, or Gemma-3-12b-it) evaluates candidate samples from MedQA-USMLE and classifies them as "easy" (correct answers with proper format) or "hard" (incorrect/invalid). Training data is constructed with a 4:1 hard-to-easy ratio (400:100). The base model Gemma-3-12b-it is then trained using GRPO with three reward functions (format, accuracy, XML count). The training uses LoRA adapters with batch size 3, 3 rollouts per sample, and gradient accumulation of 4 on a single RTX 4090 GPU.

## Key Results
- Filtered sampling strategies outperform random sampling baseline across all benchmarks
- Self-filtered samples (using Gemma-3-12b-it) achieve best medical domain performance (0.6745 MMLU vs 0.6690 baseline)
- Self-filtered models show reduced robustness on non-medical benchmarks (GSM8K: 0.9189 vs 0.9219 baseline)
- Filtering with larger same-series models (Gemma-3-27b-it) provides better robustness across benchmarks
- No "Aha moment" observed with any sampling strategy, possibly due to limited dataset size

## Why This Works (Mechanism)

### Mechanism 1: Difficulty-Calibrated Sample Selection
Filtering training samples by difficulty relative to model capability improves RLVR learning efficiency. The 4:1 hard-to-easy ratio targets the model's learning boundary where gradient signals are most informative. This assumes samples at the edge of model capability provide better learning signal than random samples, which may include trivially easy or impossibly hard examples.

### Mechanism 2: Self-Filtering for Domain Specialization
When the filtering model matches the training model (self-filtering), domain-specific performance improves but cross-domain robustness degrades. This identifies samples precisely calibrated to the base model's specific weaknesses in the target domain, producing targeted improvements but creating overfitting to that particular difficulty distribution.

### Mechanism 3: Same-Family Larger Model Filtering for Robustness
Using a larger model from the same architecture family for filtering provides better robustness across benchmarks while maintaining domain improvements. Larger models from the same series share representational structures and failure modes but have broader capability boundaries, selecting challenging samples that remain learnable without overfitting.

## Foundational Learning

- **Concept: Group Relative Policy Optimization (GRPO)**
  - Why needed: Core RL algorithm that eliminates critic network by computing advantages relative to mean reward within sampled action groups
  - Quick check: Given 3 rollouts with rewards [1, 0, 1], what is the relative advantage of the first action?

- **Concept: Verifiable Reward Composition**
  - Why needed: Three reward components (format, accuracy, XML count) must interact effectively for medical reasoning tasks
  - Quick check: Why might format reward be necessary alongside accuracy reward for medical reasoning tasks?

- **Concept: Curriculum Learning via Difficulty Filtering**
  - Why needed: The 400:100 hard-to-easy ratio implements curriculum learning; understanding this balance is essential for interpreting results
  - Quick check: What might happen if you trained on 500 hard samples with zero easy samples?

## Architecture Onboarding

- **Component map**: MedQA-USMLE dataset → Filtering models (Phi-4, Gemma-3-27b-it, Gemma-3-12b-it) → Gemma-3-12b-it base model → GRPO training with format/accuracy/XML rewards → Evaluation on MMLU, GSM8K, MMLU-Pro, CMMLU

- **Critical path**:
  1. Run filtering model on MedQA-USMLE with prompt template, classify samples as hard/easy
  2. Select 400 hard + 100 easy samples for training corpus
  3. Train Gemma-3-12b-it with GRPO using DeepSeek-R1-Zero-style prompt
  4. Evaluate on all four benchmarks to measure domain performance vs robustness

- **Design tradeoffs**:
  - Self-filtering vs larger-model-filtering: Self-filter maximizes domain performance; same-family larger model preserves robustness
  - Cross-architecture vs same-architecture filtering: Phi-4 (cross-architecture) provides intermediate results; Gemma-3-27b (same-family) best for robustness
  - LoRA vs full-parameter: Authors use LoRA; note Q-LoRA (4-bit) underperforms per repository
  - Dataset size: 500 samples may be insufficient for robust learning

- **Failure signatures**:
  - Random sampling baseline underperforms filtered approaches on nearly all metrics
  - Self-filtered models degrade on non-medical benchmarks (GSM8K: 0.9189 vs 0.9219 baseline)
  - All GRPO-trained models show CMMLU degradation vs untrained baseline (single-language training harms multilingual ability)
  - No "Aha moment" observed with any sampling strategy

- **First 3 experiments**:
  1. Reproduce random baseline: Train Gemma-3-12b-it with GRPO on 500 randomly sampled MedQA-USMLE questions; verify MMLU ≈ 0.6632
  2. Implement self-filtering pipeline: Run Gemma-3-12b-it on MedQA-USMLE, classify difficulty, verify 400:100 split produces MMLU ≈ 0.6745
  3. Ablate hard/easy ratio: Test 500:0, 250:250, and 400:100 splits with Gemma-3-27b filtering to characterize the robustness-specialization tradeoff

## Open Questions the Paper Calls Out

1. **Search Tool Integration**: Can incorporating search tools or function-calling capabilities during RLVR training enhance medical knowledge reasoning and response generation? The authors suggest this as future work, noting that current training is restricted to internal parametric knowledge.

2. **Full-Parameter Training**: Do the trade-offs between domain-specific performance and robustness persist when using full-parameter training instead of LoRA adapters? The paper suggests exploring full-parameter training based on current strategy.

3. **Dataset Scale for Self-Reflection**: Does scaling the training data beyond 500 samples facilitate the emergence of the "Aha moment" (self-reflection) which was absent in this study? The authors hypothesize that limited dataset size may be insufficient for this phenomenon.

## Limitations
- Small sample size (500 samples total) may not provide sufficient learning signal for robust generalization
- Experimental setup lacks specification of training duration (epochs/steps), making convergence assessment difficult
- No quantitative analysis of filtering model accuracy in classifying hard vs. easy samples
- Observed degradation on CMMLU and non-medical benchmarks for self-filtered models suggests potential overfitting not fully explored

## Confidence

- **High Confidence**: The core finding that filtered sampling strategies outperform random sampling on medical domain benchmarks (MMLU subcategory)
- **Medium Confidence**: The robustness-specialization tradeoff between self-filtering and same-family larger-model filtering
- **Low Confidence**: The claim that same-architecture models share inductive biases in a way that makes larger models better filters

## Next Checks

1. **Ablation on Sample Size**: Systematically vary the number of training samples (100, 250, 500, 1000) for both self-filtered and larger-model-filtered approaches to quantify the scaling relationship between data volume and domain performance vs. robustness.

2. **Filter Model Capability Gap Analysis**: Train models using filtering from progressively larger capability gaps (Phi-4 → Gemma-3-12b-it → Gemma-3-27b-it) while keeping the base model constant to map the relationship between filter-base capability mismatch and resulting model performance/robustness.

3. **Distribution Shift Robustness Test**: Evaluate all trained models on domain-adjacent but distinct datasets (e.g., medical questions from different licensing exams, clinical vignettes) to quantify how well the filtering strategy generalizes beyond the exact MedQA-USMLE distribution.