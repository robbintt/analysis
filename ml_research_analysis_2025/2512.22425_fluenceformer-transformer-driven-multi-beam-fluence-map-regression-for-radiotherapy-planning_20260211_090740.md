---
ver: rpa2
title: 'FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy
  Planning'
arxiv_id: '2512.22425'
source_url: https://arxiv.org/abs/2512.22425
tags:
- fluence
- dose
- unetr
- energy
- prediction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: FluenceFormer introduces a transformer-based two-stage architecture
  for IMRT fluence map prediction, addressing the ill-posed inverse problem of mapping
  patient anatomy to beam-intensity patterns. The model first predicts a global dose
  prior, then conditions this on explicit beam geometry to regress physically calibrated
  fluence maps.
---

# FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning

## Quick Facts
- arXiv ID: 2512.22425
- Source URL: https://arxiv.org/abs/2512.22425
- Reference count: 15
- Primary result: Two-stage transformer architecture reduces Energy Error to 4.5% on prostate IMRT, outperforming single-stage and CNN baselines

## Executive Summary
FluenceFormer introduces a transformer-based two-stage architecture for IMRT fluence map prediction, addressing the ill-posed inverse problem of mapping patient anatomy to beam-intensity patterns. The model first predicts a global dose prior, then conditions this on explicit beam geometry to regress physically calibrated fluence maps. A novel Fluence-Aware Regression (FAR) loss integrates voxel fidelity, gradient smoothness, structural consistency, and energy conservation. Evaluated across four transformer backbones on a prostate IMRT dataset, FluenceFormer with Swin UNETR achieves state-of-the-art performance, reducing Energy Error to 4.5% and improving structural fidelity (SSIM) with statistical significance (p<0.05) over CNN and single-stage baselines. The framework is backbone-agnostic, demonstrating consistent gains across architectures.

## Method Summary
FluenceFormer employs a two-stage transformer architecture for IMRT fluence map regression. Stage 1 takes 128×128 CT slices concatenated with contour masks as input and predicts dose distribution using transformer backbones (Swin UNETR, UNETR, nnFormer, MedFormer). Stage 2 conditions the predicted dose on explicit beam geometry through sinusoidal encoding (sin/cos of gantry angle) to regress beam-specific fluence maps. The FAR loss combines MSE, gradient smoothness, structural correlation, and energy conservation terms computed beam-wise. The model is trained on 99 prostate IMRT patients with 9 beams each, using Adam optimizer (lr=1e-4) for 50 epochs per stage.

## Key Results
- Energy Error reduced to 4.53% (Swin UNETR) vs 6.10% (baseline)
- SSIM improved to 0.76 vs 0.70 across all backbones with p<0.05 significance
- Swin UNETR achieves best balance of global context and local fidelity
- Beam-wise FAR loss computation outperforms global aggregation
- Two-stage decomposition outperforms single-stage direct mapping

## Why This Works (Mechanism)

### Mechanism 1
The two-stage decomposition (anatomy→dose→fluence) reduces the ill-posedness of direct anatomy-to-fluence mapping by introducing an intermediate structural prior. Stage 1 learns a dose distribution conditioned on patient anatomy, which constrains the solution space for Stage 2. This intermediate representation encodes treatment intent and spatial context, reducing the ambiguity of multiple beam configurations achieving similar doses.

### Mechanism 2
Sinusoidal geometric encoding (sin/cos of gantry angle) provides explicit directional conditioning that outperforms implicit learning through convolutional operators. Beam geometry is encoded as two spatial maps (M_sin, M_cos) concatenated with the dose slice, allowing the network to disambiguate which angular projection each fluence map represents without re-processing anatomy.

### Mechanism 3
The FAR loss enforces physical consistency (energy conservation, gradient smoothness) that improves both structural fidelity and dosimetric accuracy beyond MSE alone. Four-component loss (L_MSE, L_Grad, L_Corr, L_Energy) jointly optimizes voxel accuracy, spatial smoothness (MLC constraints), structural pattern alignment, and total Monitor Unit conservation. Beam-wise computation prevents dilution of gradients.

## Foundational Learning

- **Ill-posed inverse problems in radiotherapy**: Why needed here - Fluence prediction maps volumetric anatomy to multi-beam intensity patterns, but multiple beam configurations can yield identical dose distributions. Understanding this ambiguity explains why two-stage decomposition is proposed. Quick check - Can you explain why direct CNN-based fluence prediction might produce physically unrealizable plans?

- **Vision Transformer inductive biases (global attention vs. local receptive fields)**: Why needed here - The paper argues transformers capture long-range anatomical dependencies that CNNs miss. Swin UNETR's hierarchical windowed attention is credited with balancing global context and local spatial fidelity. Quick check - Why would windowed attention (Swin) outperform flat token sequences (UNETR) for regressing sharp beam boundaries?

- **Physics-informed loss functions**: Why needed here - The FAR loss encodes domain constraints (energy conservation, MLC smoothness) directly into training. Without this, models may optimize pixel-wise accuracy while violating physical deliverability. Quick check - What physical constraint does the Energy Loss term enforce, and why would pure MSE fail to capture it?

## Architecture Onboarding

- **Component map**: CT slice + contour mask → Stage 1 Transformer → Dose slice → [sin(θ), cos(θ)] maps → Stage 2 Transformer → Fluence map (per beam)

- **Critical path**:
  1. Preprocess CT/contours to 128×128, concatenate as 2-channel input per slice
  2. Train Stage 1 with MSE to predict dose slices
  3. Freeze Stage 1
  4. For each beam b, construct 3-channel input: dose prediction + sin(θ_b) map + cos(θ_b) map
  5. Train Stage 2 with FAR loss across all B beams
  6. At inference, iterate Stage 2 for each desired beam angle

- **Design tradeoffs**:
  - Backbone selection: Swin UNETR best balances global context and local fidelity; UNETR produces blurrier boundaries
  - Resolution vs. efficiency: 96×96 increases Energy Error (4.53%→7.80%); 128×128 required for steep gradient preservation
  - Linear vs. MLP head: Linear head yields higher SSIM (0.76 vs. 0.66); deeper heads introduce regression instability
  - Beam-wise vs. global loss: Beam-wise aggregation essential; global averaging dilutes beam-specific gradients

- **Failure signatures**:
  - Sigmoid saturation: Using segmentation-style sigmoid heads causes Energy Error >20%
  - Blurred boundaries: Flat-token backbones (UNETR) produce indistinct beam edges
  - Energy drift: MSE-only training yields 6-10% Energy Error
  - Geometric confusion: Without explicit sin/cos encoding, model may conflate beams from different angles

- **First 3 experiments**:
  1. Implement single-stage Swin UNETR with MSE loss on provided prostate dataset; verify ~7% Energy Error matches Table 1
  2. Train Stage 2 with vs. without sin/cos conditioning; quantify SSIM and Energy Error difference to validate directional disambiguation
  3. Train with MSE+Energy, MSE+Grad, and full FAR; confirm that Energy Error reduction requires all components working jointly

## Open Questions the Paper Calls Out

### Open Question 1
Can FluenceFormer generalize to cancer sites beyond prostate IMRT (e.g., head-and-neck, lung, pancreas)? The framework was evaluated exclusively on prostate IMRT; anatomical complexity differs substantially across disease sites.

### Open Question 2
Does explicit differentiable dose calculation improve fluence-dose consistency compared to the current two-stage decoupled approach? Stage 1 and Stage 2 are trained independently; predicted fluence is not directly optimized for dosimetric accuracy via forward dose calculation.

### Open Question 3
Are the predicted fluence maps directly deliverable by MLC hardware without post-processing or segmentation into deliverable apertures? Continuous fluence maps require conversion to discrete MLC segments; the framework does not model mechanical collimator constraints.

## Limitations
- Evaluation confined to prostate IMRT with coplanar beams, limiting generalizability to heterogeneous anatomies and non-coplanar geometries
- Optimal loss weights (α=1, β=0.5, γ=0.3, δ=0.2) are dataset-specific and may not generalize without retraining
- Clinical acceptability of fluence maps is not validated; dosimetric plan quality and delivery efficiency require real-world testing

## Confidence
- **High Confidence**: Two-stage decomposition improves Energy Error over single-stage baselines (4.5% vs. 6-7%). Swin UNETR outperforms other backbones on SSIM. FAR loss components are complementary and improve structural fidelity.
- **Medium Confidence**: Sinusoidal geometric encoding improves beam disambiguation. Stage 1 dose prediction quality directly impacts Stage 2 fluence accuracy. FAR loss weights are optimal for prostate IMRT.
- **Low Confidence**: Generalizability to non-prostate anatomies. Robustness to non-coplanar beam geometries. Clinical acceptability of generated fluence maps without further optimization.

## Next Checks
1. Train and evaluate FluenceFormer on a head-and-neck IMRT dataset. Compare Energy Error, SSIM, and MAE against prostate results to quantify cross-anatomy performance drop.
2. Modify the beam configuration during inference (e.g., non-coplanar arcs, irregular gantry spacing). Measure degradation in SSIM and Energy Error to assess geometric encoding robustness.
3. Convert predicted fluence maps into deliverable plans. Compute dosimetric metrics (e.g., V95, Dmean for targets/Organs at Risk) and compare against clinical plans to verify clinical acceptability.