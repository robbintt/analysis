---
ver: rpa2
title: Instance-dependent Early Stopping
arxiv_id: '2502.07547'
source_url: https://arxiv.org/abs/2502.07547
tags:
- training
- learning
- loss
- instances
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Instance-dependent Early Stopping (IES), a
  method that stops training individual instances once they are mastered by the model.
  The core idea is to use the second-order differences of loss values as a criterion
  to determine when an instance has been learned sufficiently and can be excluded
  from further backpropagation.
---

# Instance-dependent Early Stopping

## Quick Facts
- arXiv ID: 2502.07547
- Source URL: https://arxiv.org/abs/2502.07547
- Authors: Suqin Yuan; Runqi Lin; Lei Feng; Bo Han; Tongliang Liu
- Reference count: 40
- One-line primary result: Reduces backpropagation instances by 10%-50% while maintaining or improving test accuracy

## Executive Summary
This paper introduces Instance-dependent Early Stopping (IES), a method that stops training individual instances once they are mastered by the model. The core innovation uses second-order differences of loss values to determine when an instance has been learned sufficiently and can be excluded from further backpropagation. This approach allows the model to focus computational resources on harder-to-learn instances, improving training efficiency. Experiments demonstrate that IES can significantly reduce backpropagation instances while maintaining or slightly improving test accuracy and transfer learning performance.

## Method Summary
IES works by computing the second-order difference of per-sample loss values across three consecutive epochs: Δ²Lᵢ(w(t)) = Lᵢ(w(t)) - 2Lᵢ(w(t-1)) + Lᵢ(w(t-2)). When |Δ²Lᵢ(w(t))| < δ for k consecutive epochs, the instance is marked as mastered and excluded from backpropagation in subsequent epochs. All instances continue to participate in forward passes to track loss history. The method includes a reversible mechanism where mastered samples can re-enter training if their second-order difference exceeds the threshold later. The approach is tested on CIFAR-10/100 and ImageNet-1k using standard architectures like ResNet-18.

## Key Results
- IES reduces backpropagation instances by 10%-50% across experiments
- On ImageNet-1k, achieved up to 40% reduction while maintaining performance
- Improved transfer learning by 1.5% on average compared to baseline methods
- Maintains test accuracy within 0.2% of baseline on CIFAR-10
- Shows robustness to threshold values across 4 orders of magnitude (1e-4 to 1e-1)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Second-order loss differences provide a more unified criterion for "mastered" status than raw loss values.
- Mechanism: The second-order difference captures loss plateau behavior regardless of absolute loss magnitude, enabling a single threshold δ across all samples.
- Core assumption: Instances with stable second-order differences are truly mastered and won't contribute meaningfully to further optimization.
- Evidence anchors: [Section 3.2] states the second-order difference effectively captures stability regardless of specific loss values. [Figure 3] shows coefficient of variation for second-order differences decreases and stabilizes in later training stages.

### Mechanism 2
- Claim: Excluding mastered instances increases average gradient norm of remaining samples, accelerating loss reduction.
- Mechanism: Mastered instances have near-zero gradients; removing them from backpropagation leaves a filtered set with higher mean gradient norm, leading to more effective parameter changes per epoch.
- Core assumption: Gradient norm correlates with learning progress; larger gradients on remaining hard samples drive faster convergence.
- Evidence anchors: [Section 3.3] shows IES consistently achieves higher average mini-batch gradient norm compared to baseline. [Figure 4] demonstrates gradient norm curves showing IES above baseline throughout training.

### Mechanism 3
- Claim: IES reduces loss landscape sharpness more rapidly than full-data training.
- Mechanism: By focusing on harder instances, IES avoids over-memorization of easy samples that can lead to sharp minima. Reduced SAM values and Hessian eigenvalues indicate flatter solutions.
- Core assumption: Flat minima generalize better; sharpness reduction correlates with improved transfer learning.
- Evidence anchors: [Section 3.3] reports IES reduces largest eigenvalue more quickly and consistently achieves lower SAM values. [Table 3] shows transfer learning improvements of 0.9%-2.5% across CIFAR and Caltech-101 downstream tasks.

## Foundational Learning

- Concept: **Second-order finite differences**
  - Why needed here: The mastered criterion relies on computing Δ²L across three consecutive epochs; understanding discrete derivatives helps interpret why this signals convergence.
  - Quick check question: Can you explain why a second-order difference approaching zero indicates a function is reaching a plateau?

- Concept: **Loss landscape sharpness (Hessian eigenvalues, SAM)**
  - Why needed here: The paper claims IES accelerates sharpness reduction; understanding flat vs. sharp minima clarifies the generalization argument.
  - Quick check question: Why might flat minima generalize better than sharp minima?

- Concept: **Catastrophic forgetting in continual/filtered learning**
  - Why needed here: Excluding samples risks forgetting; the reversible re-inclusion mechanism addresses this.
  - Quick check question: What conditions would cause a filtered-sample approach to forget previously learned patterns?

## Architecture Onboarding

- Component map: Full Dataset D(0) → Forward Pass (compute Lᵢ for all i) → Second-Order Difference Calculator → Mastered Instance Filter M(t) → Filtered Dataset D(t) = D(0) \ M(t) → Backpropagation on D(t) → Parameter Update

- Critical path:
  1. Store losses for t, t-1, t-2 epochs (requires 3-epoch buffer per instance)
  2. Compute Δ²Lᵢ and compare to threshold δ each epoch
  3. Update D(t) before backpropagation
  4. Allow re-inclusion if criterion violated later

- Design tradeoffs:
  - Forward pass overhead on all samples vs. backprop savings on filtered subset
  - Lower δ → more conservative removal, less speedup; higher δ → aggressive removal, risk of under-training
  - Annealing in final epochs (10% for ImageNet) improves stability but reduces late-stage efficiency

- Failure signatures:
  - Test accuracy drops >1%: δ too aggressive; reduce threshold
  - Minimal samples removed: δ too conservative or learning rate too high (loss oscillates)
  - Oscillating removal counts: Unstable training; check learning rate schedule
  - Transfer learning degradation: Over-removal of class-representative samples; consider class-balanced removal

- First 3 experiments:
  1. **Sanity check on small dataset**: CIFAR-10 with ResNet-18, δ=1e-3, 200 epochs. Verify ~40-55% mini-batch savings with accuracy within 0.2% of baseline.
  2. **Threshold sweep**: Test δ ∈ {1e-4, 1e-3, 1e-2, 1e-1} on CIFAR-10. Plot removal rate vs. test accuracy to find operating range; paper shows robustness across 4 orders of magnitude.
  3. **Ablation on order N**: Compare N=0 (raw loss), N=1, N=2, N=3 as mastered criteria. Paper shows N=2 generally best; validate on your architecture before committing.

## Open Questions the Paper Calls Out

- Question: Can a formal theoretical framework be developed to rigorously prove why the second-order difference of loss values provides a superior convergence criterion for "mastered" instances compared to zero-order loss or higher-order differences?
- Question: Does Instance-dependent Early Stopping (IES) inadvertently introduce or amplify bias against minority subgroups by potentially excluding them from backpropagation earlier or later than majority groups?
- Question: How can the IES criterion be adapted to ensure computational savings in datasets with significant label noise, where the current method fails to exclude samples?

## Limitations

- The mechanism assumes second-order differences reliably signal mastery, which may not hold for all loss landscapes or architectures.
- Memory overhead for storing three epochs of per-sample losses could be prohibitive for extremely large datasets.
- The adaptive thresholding strategy (annealing δ in final 10% of training) lacks comprehensive ablation showing its necessity.
- Transfer learning improvements (1.5% average) are promising but based on limited downstream tasks.

## Confidence

**High Confidence**: The core mechanism of using second-order loss differences as a mastered criterion is well-founded and mathematically sound. The empirical results showing 10-50% reduction in backpropagation instances while maintaining accuracy are reproducible given the specified methodology.

**Medium Confidence**: Claims about IES accelerating sharpness reduction and improving transfer learning generalization are supported by experiments but could benefit from more extensive validation across diverse architectures and downstream tasks.

**Low Confidence**: The assertion that IES would generalize well to extremely large-scale datasets (beyond ImageNet-1k) and highly imbalanced datasets lacks empirical validation in the paper.

## Next Checks

1. **Memory Overhead Analysis**: Measure actual memory consumption for storing per-sample loss histories on ImageNet-scale data and evaluate whether approximate methods (quantization, sampling) could reduce overhead while maintaining performance.

2. **Class Imbalance Stress Test**: Apply IES to long-tailed benchmark datasets (e.g., CIFAR-LT) with varying imbalance factors to verify it doesn't exacerbate existing biases by prematurely removing minority class samples.

3. **Architecture Generalization**: Test IES on transformer-based architectures (ViT, BERT) to determine if the second-order difference criterion remains effective when loss surfaces have fundamentally different geometries compared to CNNs.