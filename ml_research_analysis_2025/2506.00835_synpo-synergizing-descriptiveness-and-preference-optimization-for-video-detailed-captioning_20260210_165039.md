---
ver: rpa2
title: 'SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed
  Captioning'
arxiv_id: '2506.00835'
source_url: https://arxiv.org/abs/2506.00835
tags:
- preference
- video
- arxiv
- optimization
- synpo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the limitations of Direct Preference Optimization
  (DPO) in fine-grained video captioning, where existing methods struggle with capturing
  subtle video dynamics and suffer from simultaneous decreases in positive and negative
  rewards during training. The authors propose Synergistic Preference Optimization
  (SynPO), which reformulates reward computation to prevent negative preferences from
  dominating, introduces an explicit language capability term to maintain generation
  quality, and eliminates the need for a reference model, achieving 20% training efficiency
  improvement.
---

# SynPO: Synergizing Descriptiveness and Preference Optimization for Video Detailed Captioning

## Quick Facts
- **arXiv ID:** 2506.00835
- **Source URL:** https://arxiv.org/abs/2506.00835
- **Reference count:** 40
- **Key outcome:** SynPO reformulates DPO's reward computation with exponential transformations and language preservation terms, eliminating reference models to achieve 20% training efficiency improvement while outperforming DPO variants on video captioning benchmarks.

## Executive Summary
SynPO addresses critical limitations in Direct Preference Optimization (DPO) for fine-grained video captioning, where existing methods struggle with capturing subtle video dynamics and suffer from simultaneous decreases in positive and negative rewards during training. The authors propose a novel reformulation that applies exponential transformations to prevent negative preferences from dominating optimization, introduces an explicit language capability term to maintain generation quality, and eliminates the need for a reference model. Through an automated pipeline leveraging vision-language model self-consistency, SynPO constructs high-quality preference pairs and achieves consistent improvements across video captioning benchmarks (VDC, VDD, VATEX, MSR-VTT) and NLP tasks while maintaining strong language capabilities.

## Method Summary
SynPO reformulates DPO's reward computation by applying exponential transformations to the log-probability of token sequences, preventing negative preferences from dominating optimization through gradient rebalancing. The method introduces an auxiliary language capability term that uses arithmetic averaging to preserve high-probability tokens critical for fluency, counterbalancing the preference ranking term focused on semantic discrimination. Most innovatively, SynPO eliminates the reference model requirement through sigmoid-bounded formulation with implicit regularization, achieving approximately 20% training efficiency improvement. The approach is trained using LoRA fine-tuning with specific hyperparameters (rank=128, alpha=64, dropout=0.05) and optimized via AdamW with learning rate 5e-6 and warmup of 0.1.

## Key Results
- SynPO achieves consistent improvements across video captioning benchmarks (VDC, VDD, VATEX, MSR-VTT) compared to DPO variants while maintaining strong language capabilities
- Eliminates reference model requirement, achieving approximately 20% training efficiency improvement
- Introduces automated pipeline for constructing high-quality preference pairs using vision-language model self-consistency and detail-capturing ability

## Why This Works (Mechanism)

### Mechanism 1: Exponential Transformation Rebalances Gradient Dynamics
DPO's log-ratio formulation causes both positive and negative rewards to decrease simultaneously because derivatives incentivize reducing both rewards. By applying exp(log S(y)) transformation, SynPO corrects gradient direction—amplifying contributions from semantically important low-probability tokens while preventing simultaneous reward decay. This works because tokens with lower log-probability carry higher semantic importance in video captioning.

### Mechanism 2: Explicit Language Preservation Term Anchors Generation Quality
The auxiliary term β·S(yw) uses arithmetic averaging to preserve high-probability tokens critical for fluency (conjunctions, grammar tokens), counterbalancing the preference ranking term that focuses on semantic discrimination. Without this constraint, preference optimization naturally drifts toward ranking discrimination at the expense of generation fluency.

### Mechanism 3: Reference-Free Formulation Improves Efficiency Without Stability Loss
SynPO's sigmoid-bounded formulation with α parameter provides implicit regularization (sigmoid derivatives suppress extreme values), making explicit KL-divergence constraints unnecessary. This achieves ~20% training speedup while maintaining optimization stability without the reference model comparisons required in standard DPO.

## Foundational Learning

- **DPO Objective and Limitations**
  - Why needed here: SynPO is explicitly designed to fix DPO's reward collapse problem; understanding the baseline failure mode is essential.
  - Quick check question: Can you explain why DPO's log-ratio formulation causes both positive and negative rewards to decrease simultaneously?

- **Token-Level Probability vs. Semantic Importance**
  - Why needed here: SynPO's core insight relies on the inverse relationship between log-probability and semantic significance.
  - Quick check question: Why would low-probability tokens (e.g., rare nouns, action verbs) be more important for preference learning than high-probability tokens (e.g., "the", punctuation)?

- **Self-Consistency for Preference Data Construction**
  - Why needed here: The data pipeline uses VLM self-consistency to score and rank candidates without stronger models.
  - Quick check question: How can you identify high-quality captions using only the target model's own outputs across multiple samples?

## Architecture Onboarding

- **Component map:**
  Video Input → VLM (contrastive decoding + self-retrospective) → N candidate captions (sampling count 10-32) → LLM scorer (3 criteria: factuality, fluency, self-consistency) → Ranked pairs (top=positive, bottom=negative) → SynPO loss computation → LoRA fine-tuning (rank=128, alpha=64)

- **Critical path:**
  1. Data quality directly determines preference learning effectiveness—scoring criteria must match target task (Criterion 1 factuality is most important for positive selection)
  2. Hyperparameter α controls preference ranking strength; β controls language preservation—joint tuning required
  3. Gradient monitoring: track positive/negative reward separation; if both decrease, α may be too low or β too high

- **Design tradeoffs:**
  - Sampling count vs. cost: More samples improve preference pair quality but increase inference time (~2x for retrospective strategy)
  - Log-exponential vs. arithmetic averaging: Former for preference discrimination (semantic focus), latter for fluency preservation (grammar focus)
  - Reference-free efficiency vs. implicit regularization: SynPO trades explicit KL constraint for faster training

- **Failure signatures:**
  - Both rewards decreasing during training → negative preference dominance (increase α)
  - Fluency degradation in outputs → β too low or missing
  - Training instability → sigmoid saturation not working; check α range (20-50 recommended)
  - Preference pairs too similar → increase sampling temperature (0.9) or sampling count

- **First 3 experiments:**
  1. Reproduce gradient imbalance visualization: Train standard DPO on ShareGPT4Video subset; plot positive/negative reward trajectories and gradient norms to confirm the problem exists in your setup
  2. Ablate each SynPO component: Test (a) exponential transformation only, (b) language term only, (c) full SynPO; measure VDC scores to isolate contribution of each mechanism
  3. Validate data pipeline quality: Generate preference pairs using the 3-criterion scorer; manually inspect top-5 and bottom-5 ranked captions for 20 videos to verify scoring alignment with human judgment

## Open Questions the Paper Calls Out

### Open Question 1
How does SynPO generalize to other multimodal preference learning tasks beyond video captioning, such as image-text alignment or audio-language tasks? The paper demonstrates effectiveness on video captioning and NLP benchmarks but does not evaluate on other modalities where preference optimization is also relevant.

### Open Question 2
To what extent does the automated preference pair construction pipeline align with true human preferences for video captioning quality? While the paper shows downstream task improvements, it does not include human evaluation comparing pipeline-generated preferences against human-curated pairs.

### Open Question 3
What are the theoretical guarantees of SynPO regarding optimization stability and avoidance of reward collapse compared to DPO? The paper provides empirical evidence of stability but does not prove formal convergence properties or guarantees that SynPO prevents all forms of reward degradation.

## Limitations

- Core claims around exponential reward transformation rely heavily on empirical observations that may not generalize across different domains or video styles
- Reference-free efficiency claim (20% speedup) lacks comparison to other reference-free alternatives like SimPO or CPO
- The automated preference pair construction pipeline's alignment with human preferences is not validated through human evaluation studies

## Confidence

- **High Confidence:** Data pipeline quality and benchmark results are well-documented with multiple metrics showing consistent improvements across both video captioning and NLP tasks
- **Medium Confidence:** Mechanism explaining why DPO's log-ratio formulation causes simultaneous reward decay is theoretically sound but relies on specific empirical patterns
- **Medium Confidence:** Claim that exponential transformations prevent negative preference dominance is supported by gradient analysis but could be sensitive to hyperparameter choices

## Next Checks

1. **Domain Transfer Validation:** Test SynPO on non-video captioning tasks (e.g., detailed image captioning or long-form text generation) to verify whether the low-probability token importance relationship holds across modalities

2. **Component Ablation Study:** Systematically vary α (preference strength) and β (language preservation weight) across a wider range (e.g., α∈{10,20,30,40,50,60}, β∈{0.05,0.1,0.15,0.2,0.25,0.3}) to map the full performance landscape

3. **Reference Model Comparison:** Implement and compare against SimPO and CPO (other reference-free DPO variants) on the same datasets to determine whether SynPO's gains are due to the reference-free approach specifically or the additional exponential transformation and language capability terms