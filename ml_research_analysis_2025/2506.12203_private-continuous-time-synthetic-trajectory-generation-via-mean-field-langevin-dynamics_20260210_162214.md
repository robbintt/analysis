---
ver: rpa2
title: Private Continuous-Time Synthetic Trajectory Generation via Mean-Field Langevin
  Dynamics
arxiv_id: '2506.12203'
source_url: https://arxiv.org/abs/2506.12203
tags:
- data
- privacy
- private
- trajectory
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a differentially private algorithm for generating
  synthetic continuous-time trajectories. The key innovation is leveraging mean-field
  Langevin dynamics with a reformulation via entropic optimal transport, allowing
  each user to contribute only a single time-point rather than a full trajectory.
---

# Private Continuous-Time Synthetic Trajectory Generation via Mean-Field Langevin Dynamics

## Quick Facts
- arXiv ID: 2506.12203
- Source URL: https://arxiv.org/abs/2506.12203
- Authors: Anming Gu; Edward Chien; Kristjan Greenewald
- Reference count: 30
- Key outcome: Introduces a DP algorithm for continuous-time trajectory synthesis using entropic OT reformulation, enabling single time-point contribution per user while maintaining privacy and utility

## Executive Summary
This paper presents a differentially private algorithm for generating synthetic continuous-time trajectories by reformulating the problem through entropic optimal transport. The key innovation allows each user to contribute only a single time-point rather than a full trajectory, improving privacy by construction. The method combines mean-field Langevin dynamics with noisy particle gradient descent and Poisson subsampling to achieve Gaussian differential privacy guarantees while maintaining statistical consistency under dense temporal sampling.

## Method Summary
The method leverages mean-field Langevin dynamics reformulated via entropic optimal transport to generate synthetic trajectories. It uses noisy particle gradient descent with Poisson subsampling for privacy, where each user contributes only one time-point. The approach employs warm-start initialization via private means or clustering to improve convergence. Theoretical guarantees include statistical consistency under dense temporal sampling and Gaussian differential privacy via standard composition theorems. Experiments on MNIST stroke data demonstrate effectiveness in generating realistic trajectories while maintaining privacy guarantees.

## Key Results
- Achieves Gaussian differential privacy through noisy particle gradient descent with composition
- Provides statistical consistency guarantees under dense temporal sampling conditions
- Demonstrates practical utility on handwritten MNIST stroke dataset with realistic trajectory generation
- Shows strong performance even with limited temporal observations per user

## Why This Works (Mechanism)

### Mechanism 1: Entropic Optimal Transport Reformulation
Reformulating trajectory inference via entropic OT enables single time-point contribution per user while maintaining trajectory coherence. The paper reduces optimization from path space P(Ω) to particle space P(X)^T using equivalence between path-space entropy minimization F(R) = Fit(R) + τH(R|W_τ) and particle-space objective F(μ) = G(μ) + τH(μ). Temporal coupling is achieved via entropic transport plans T_{τi}(μ(i), μ(i+1)) between consecutive marginals.

### Mechanism 2: Privacy via Noisy Particle Gradient Descent
Discretized MFLD with noise injection achieves Gaussian differential privacy through composition. The approach uses (1) discretization of MFLD yielding noisy gradient descent on particles, (2) gradient sensitivity bounded by clipping threshold C, (3) Gaussian noise τ√η added per update, (4) Poisson subsampling (rate ρ) for amplification, and (5) parallel composition across disjoint time marginals.

### Mechanism 3: Statistical Consistency via Dense Temporal Sampling
The estimator recovers ground truth trajectory distribution as time density and sample size increase. Minimizer R* of entropy-regularized objective satisfies marginals converge to P_t as σ,λ → 0, T → ∞, with rate ∫₀¹ d²_H(R*_t * g_σ, P_t * g_σ)dt ≲ max{1/T, 1/(N^{2/3}T^{1/3})}.

## Foundational Learning

- Concept: Entropic Optimal Transport / Schrödinger Bridge
  - Why needed here: Core framework coupling temporal marginals into coherent trajectories
  - Quick check question: Given marginals μ, ν, can you write the entropic OT objective T_τ(μ, ν)?

- Concept: Differential Privacy Composition (Gaussian DP)
  - Why needed here: Understanding how privacy budget accumulates across K iterations and T time points
  - Quick check question: How does GDP composition improve over basic (ε,δ)-DP composition?

- Concept: Mean-Field Langevin Dynamics
  - Why needed here: Connects continuous-time SDE dynamics to discrete particle updates
  - Quick check question: What is the relationship between MFLD SDE (Eq. 8) and its discretization (Eq. 10)?

## Architecture Onboarding

- Component map: Data binning → Private initialization (means/clustering) → Core loop: [Sinkhorn solver for OT plans → Poisson subsample → gradient + clip → noisy update] × K → Trajectory synthesis via composed plans

- Critical path: (1) Compute private means per marginal (consumes budget); (2) Run Sinkhorn between consecutive marginals; (3) Subsample, compute gradients, clip, add noise; (4) Update particles; (5) Repeat; (6) Output particles + OT plans

- Design tradeoffs: Larger τ improves privacy but needs more iterations; smaller ρ improves privacy but increases variance; larger m improves accuracy but increases Sinkhorn cost; warm starts consume separate privacy budget.

- Failure signatures: Modal collapse (fix with clustering warm start); poor convergence (check initialization quality); cross-cluster trajectories in multimodal data (use exact OT for final synthesis).

- First 3 experiments:
  1. Sanity check on MNIST digit "1" with warm start; verify marginals match private means and trajectories look realistic
  2. Privacy-utility tradeoff: vary τ ∈ {1.0, 1.5, 2.0, 2.5}, measure W₂ distance vs GDP budget μ
  3. Multimodal stress test: 3-mode synthetic data; compare uniform vs clustering warm start; measure cluster weight accuracy

## Open Questions the Paper Calls Out

- Question: Can neural network approaches for trajectory inference be effectively adapted to private continuous-time synthetic trajectory generation while maintaining provable utility and privacy guarantees?
  - Basis in paper: [explicit] The conclusion states "it would be interesting to adapt neural network approaches for trajectory inference to this problem," referencing works like [TMF+23, ZLZ25] that use neural networks for trajectory inference.
  - Why unresolved: The current method relies on mean-field Langevin dynamics with particle gradient descent, and it is unclear whether neural network parameterizations would preserve the theoretical guarantees or require new analysis techniques.
  - What evidence would resolve it: A private neural trajectory inference method with provable (ε,δ)-DP guarantees and utility bounds comparable to Theorem 3.2, demonstrated on benchmark datasets.

- Question: How can private synthetic trajectory generation be achieved under online or continual release settings where data arrives as a stream rather than in batch?
  - Basis in paper: [explicit] The conclusion explicitly notes "It would be interesting to study private synthetic data generation under the online or continual release settings, where data comes as part of a stream."
  - Why unresolved: The current algorithm assumes static batch access to all temporal marginals; streaming data introduces challenges for composition, requires online entropic optimal transport updates, and complicates privacy accounting across releases.
  - What evidence would resolve it: An algorithm with bounded cumulative privacy loss under continual observation (e.g., via DP composition for streaming), with utility guarantees that degrade gracefully with the number of releases.

- Question: Can the method be extended to handle discrete-valued or Boolean trajectory data while preserving utility guarantees?
  - Basis in paper: [explicit] The authors state "We also leave for future work incorporating the sliced-Wasserstein method of [DAHY24] to allow our method to work with discrete-valued (e.g. Boolean) data."
  - Why unresolved: The current approach constructs high-dimensional Gaussians and relies on continuous path spaces; discrete spaces require different transport geometry and may not admit the same entropic regularization formulations.
  - What evidence would resolve it: An algorithm for discrete trajectory generation with DP guarantees and empirical utility on discrete-valued benchmarks (e.g., mobility traces with categorical locations).

- Question: Can finite-particle discretization guarantees be established that match the statistical consistency results currently proven only for the infinite-particle limit?
  - Basis in paper: [inferred] The authors note in Section 3.3 that "the statistical guarantees from [YNCY25] only hold for the infinite-particle limit and not the finite-particle discretization," and in Appendix C.4 they defer discretization error analysis "to future work on statistical guarantees for MFLD."
  - Why unresolved: The current analysis assumes m→∞ particles per time marginal; finite-particle effects interact with entropic OT plan estimation, stochastic gradient noise, and clipping in ways not yet characterized.
  - What evidence would resolve it: Explicit bounds on the error between finite-particle Algorithm 1 and the population minimizer, including dependence on particle count m, iteration count K, and dataset size N.

## Limitations

- The theoretical convergence rates rely heavily on assumptions about log-Sobolev inequalities holding for the Schrödinger potentials, which may not be verifiable in practice
- Statistical consistency guarantees require dense temporal sampling (T growing with N), but the relationship between T and N needed for the O(1/(N^(2/3)T^(1/3))) rate is not explicitly specified
- Experiments only validate on MNIST stroke data, limiting generalizability to other continuous-time trajectory domains

## Confidence

- **High confidence**: Privacy guarantees via noisy particle gradient descent composition (supported by established DP theory)
- **Medium confidence**: Entropic OT reformulation enables single time-point contribution (theoretically sound but empirical validation limited)
- **Low confidence**: Statistical consistency under sparse temporal sampling (theoretical conditions are strong and untested)

## Next Checks

1. Verify convergence behavior when T is held constant while N increases, to test the O(1/(N^(2/3)T^(1/3))) rate empirically
2. Test the algorithm on non-stroke trajectory datasets (e.g., animal movement data or financial time series) to assess domain generalizability
3. Experiment with varying diffusivity parameters τ to quantify the tradeoff between trajectory coherence and privacy amplification through composition