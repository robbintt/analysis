---
ver: rpa2
title: 'SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented
  Generation'
arxiv_id: '2510.15682'
source_url: https://arxiv.org/abs/2510.15682
tags:
- squai
- scientific
- retrieval
- answer
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SQuAI introduces a multi-agent retrieval-augmented generation system
  for scientific question answering. It addresses the challenge of complex, open-domain
  questions in scholarly domains by decomposing queries into sub-questions, using
  hybrid sparse-dense retrieval, and applying adaptive document filtering.
---

# SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation

## Quick Facts
- arXiv ID: 2510.15682
- Source URL: https://arxiv.org/abs/2510.15682
- Reference count: 40
- Key outcome: SQuAI improves faithfulness and answer/contextual relevancy by up to +0.088 (12%) over baseline RAG systems

## Executive Summary
SQuAI addresses the challenge of complex scientific question answering through a multi-agent retrieval-augmented generation system. The approach decomposes queries into sub-questions, employs hybrid sparse-dense retrieval, and applies adaptive document filtering to improve answer quality. The system generates fine-grained in-line citations with supporting context, ensuring faithfulness and traceability in scholarly domains. Evaluated across three benchmarks with over 2.3 million arXiv papers, SQuAI demonstrates meaningful improvements over standard RAG baselines.

## Method Summary
SQuAI introduces a multi-agent architecture that breaks down complex scientific queries into manageable sub-questions. The system uses hybrid retrieval combining sparse and dense methods to identify relevant documents, followed by adaptive filtering to refine document selection. A generation component produces answers with fine-grained in-line citations and supporting context. The approach is evaluated on three benchmarks (LitSearch, unarXive Simple, and unarXive Expert) and includes a synthetic dataset of 1,000 scientific QA triplets created for training and validation.

## Key Results
- Improves combined score of faithfulness and answer/contextual relevancy by up to +0.088 (12%) over baseline RAG
- Scales effectively to over 2.3 million arXiv papers
- Introduces a new synthetic dataset of 1,000 scientific QA triplets

## Why This Works (Mechanism)
SQuAI works by decomposing complex scientific questions into simpler sub-questions that can be addressed more effectively through targeted retrieval. The hybrid retrieval approach combines the broad coverage of sparse methods with the semantic precision of dense methods, improving document selection quality. Adaptive document filtering further refines results based on initial retrieval performance, while the multi-agent architecture allows parallel processing of sub-questions. Fine-grained citation generation ensures traceability and supports the faithfulness of generated answers.

## Foundational Learning

- **Query decomposition**: Breaking complex questions into simpler sub-questions allows for more targeted retrieval and generation. Why needed: Complex scientific questions often contain multiple facets that standard RAG systems struggle to address simultaneously. Quick check: Verify that decomposed sub-questions maintain semantic coherence with the original query.

- **Hybrid sparse-dense retrieval**: Combining traditional keyword-based retrieval with semantic embedding approaches improves both coverage and precision. Why needed: Scientific literature requires both exact term matching and semantic understanding. Quick check: Compare retrieval precision at different recall levels between hybrid and single-method approaches.

- **Adaptive document filtering**: Dynamically refining document selection based on retrieval quality improves downstream generation. Why needed: Initial retrieval may include irrelevant documents that degrade answer quality. Quick check: Measure the impact of filtering on answer faithfulness scores.

- **Fine-grained citation generation**: Providing specific document locations for claims ensures traceability and supports verification. Why needed: Scientific QA requires accountability for generated claims. Quick check: Validate that citations correspond to actual supporting evidence in source documents.

## Architecture Onboarding

Component map: Query Decomposition -> Hybrid Retrieval -> Adaptive Filtering -> Multi-Agent Processing -> Generation with Citations

Critical path: Query → Decomposition → Retrieval → Filtering → Answer Generation

Design tradeoffs: The multi-agent approach enables parallel processing but increases computational overhead compared to monolithic systems. The trade-off favors accuracy and faithfulness over latency, making it suitable for research applications where correctness is paramount.

Failure signatures: Poor query decomposition can lead to irrelevant sub-questions and cascading errors. Ineffective filtering may introduce noise into the generation pipeline. The system assumes queries can be meaningfully decomposed, which may not hold for all question types.

First experiments:
1. Test query decomposition on a diverse set of scientific questions to validate the splitting strategy
2. Compare hybrid retrieval performance against sparse-only and dense-only baselines on scientific corpora
3. Evaluate the impact of adaptive filtering by measuring answer quality with and without this component

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several areas remain unexplored. The decomposition strategy's effectiveness for questions that resist meaningful splitting requires investigation. The computational overhead of the multi-agent approach compared to simpler systems needs quantification. The generalizability of improvements beyond the three evaluated benchmarks remains uncertain. The potential for cascading errors when sub-question retrieval fails has not been thoroughly addressed.

## Limitations

- Improvements of +0.088 represent modest absolute gains over baseline RAG systems
- Evaluation focuses on three specific benchmarks without broader cross-domain validation
- Computational overhead of multi-agent approach compared to simpler baselines remains unclear

## Confidence

- High confidence: Core architecture (multi-agent RAG with citation generation) is technically sound and well-implemented
- Medium confidence: Synthetic dataset creation methodology is reasonable but lacks external validation
- Medium confidence: Benchmark performance improvements are statistically significant but may not translate to all scientific domains

## Next Checks

1. Conduct ablation studies to isolate the contribution of each component (query decomposition, hybrid retrieval, adaptive filtering) to overall performance
2. Test the system on additional scientific domains beyond the three evaluated benchmarks to assess generalizability
3. Measure computational efficiency and response latency compared to standard RAG baselines at scale