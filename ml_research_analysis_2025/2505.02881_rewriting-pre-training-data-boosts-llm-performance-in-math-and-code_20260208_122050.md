---
ver: rpa2
title: Rewriting Pre-Training Data Boosts LLM Performance in Math and Code
arxiv_id: '2505.02881'
source_url: https://arxiv.org/abs/2505.02881
tags:
- code
- data
- humaneval
- tokens
- rewriting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SwallowCode and SwallowMath are two openly licensed datasets that
  significantly enhance large language model (LLM) performance in code generation
  and mathematical reasoning. SwallowCode (approximately 16.1 billion tokens) refines
  Python snippets from The-Stack-v2 using a four-stage pipeline that includes syntax
  validation, pylint-based style filtering, and a two-stage LLM rewriting process
  to enforce style conformity and transform snippets into self-contained, algorithmically
  efficient examples.
---

# Rewriting Pre-Training Data Boosts LLM Performance in Math and Code

## Quick Facts
- **arXiv ID:** 2505.02881
- **Source URL:** https://arxiv.org/abs/2505.02881
- **Reference count:** 40
- **Primary result:** Llama-3.1-8B trained on SwallowCode/Math outperforms Stack-Edu/Finemath-4+ by +17.0/+12.4 points on code/math benchmarks.

## Executive Summary
This paper introduces SwallowCode and SwallowMath, two openly licensed datasets that significantly enhance large language model (LLM) performance in code generation and mathematical reasoning. SwallowCode (approximately 16.1 billion tokens) refines Python snippets from The-Stack-v2 using a four-stage pipeline that includes syntax validation, pylint-based style filtering, and a two-stage LLM rewriting process to enforce style conformity and transform snippets into self-contained, algorithmically efficient examples. SwallowMath (approximately 2.3 billion tokens) upgrades Finemath-4+ by removing boilerplate, restoring context, and reformatting solutions into concise, step-by-step explanations. Within a fixed 50 billion token training budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1 by +17.0 on HumanEval and +16.1 on HumanEval+ compared to Stack-Edu. Substituting SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies confirm that each pipeline stage contributes incrementally, with rewriting delivering the largest gains. All datasets, prompts, and checkpoints are publicly available.

## Method Summary
The method employs a four-stage pipeline for SwallowCode: (1) syntax validation via `compile()`, (2) Pylint filtering (â‰¥7.0 score with comment-ratio penalty), (3) Style-Guided Code Rewriting (SGCR) using Llama-3.3-70B-Instruct to enforce Google Python Style Guide, and (4) Self-Contained Optimization Rewriting (SCOR) to resolve dependencies and ensure executability. SwallowMath uses a single-stage LLM rewrite. Models are trained using Megatron-LM with a fixed 50 billion token budget, evaluating performance on HumanEval, HumanEval+, GSM8K, and MATH benchmarks.

## Key Results
- SwallowCode achieves +17.0 pass@1 on HumanEval and +16.1 on HumanEval+ vs Stack-Edu.
- SwallowMath achieves +12.4 accuracy on GSM8K and +7.6 on MATH vs Finemath-4+.
- Each pipeline stage (SGCR, SCOR) contributes incrementally, with rewriting delivering the largest gains.
- All datasets, prompts, and checkpoints are publicly available.

## Why This Works (Mechanism)

### Mechanism 1: Transform-and-Retain Utility Maximization
Transforming low-quality code into high-quality code via LLM rewriting yields higher downstream performance than discarding low-quality data, conditional on the rewriting model's accuracy. This increases effective volume of high-quality tokens without losing semantic diversity.

### Mechanism 2: Semantic Normalization via Style Guides
Enforcing consistent style (naming, type hints, docstrings) reduces the "cognitive load" required for the model to learn structural patterns, allowing it to allocate capacity to reasoning.

### Mechanism 3: Self-Containment and Dependency Resolution
Rewriting code to be "self-contained" (resolving external dependencies and inlining context) significantly improves the model's ability to generate executable code.

## Foundational Learning

- **Concept: Continual Pre-training (CPT)**
  - Why needed here: The paper uses CPT on Llama-3.1-8B to inject new data (SwallowCode/Math) without forgetting original capabilities.
  - Quick check question: How does the learning rate schedule in CPT typically differ from pre-training from scratch?

- **Concept: Static Analysis (Linting)**
  - Why needed here: The pipeline uses `pylint` as a critical filter and quality signal before rewriting.
  - Quick check question: Why is syntax filtering (compiling) a necessary precursor to linting in this pipeline?

- **Concept: Data Ablation Studies**
  - Why needed here: The paper validates each pipeline stage (SGCR, SCOR) by training separate models.
  - Quick check question: In the ablation experiments, what variable is held constant while the dataset changes?

## Architecture Onboarding

- **Component map:** The-Stack-v2 (Raw Python) -> Syntax + Pylint Filter -> Llama-3.3-70B-Instruct (SGCR for Style -> SCOR for Semantics) -> Megatron-LM (Llama-3.1-8B) -> HumanEval/HumanEval+ (Code), GSM8K/MATH (Math)

- **Critical path:** The LLM-driven rewriting stage (SGCR + SCOR) is the primary bottleneck and cost driver.

- **Design tradeoffs:** Filtering (LLM-based scoring) is computationally cheaper but discards data; rewriting is 1.22x more expensive but retains/upgrades data. Applying SGCR and SCOR sequentially yields better quality than simultaneously.

- **Failure signatures:** MBPP performance drop after SGCR due to function-name standardization mismatches; trivial snippet expansion if SCOR is not applied.

- **First 3 experiments:**
  1. Pipeline Reproduction (Scale Down): Take 1,000 samples from The-Stack-v2. Run the Syntax + Pylint filter. Analyze the distribution of scores to verify the 7.0 threshold impact.
  2. Rewrite Validation: Manually inspect 10 "bad" samples (score < 5) and pass them through the SGCR prompt. Check if the logic is preserved or hallucinated.
  3. Dependency Check: Run SCOR on a sample with missing imports. Verify if the LLM correctly identifies and inlines the missing dependency vs. removing the functionality.

## Open Questions the Paper Calls Out

### Open Question 1
Does SCOR (Self-Contained Optimization Rewriting) improve code generation performance when applied in isolation without the preceding SGCR (Style-Guided Code Rewriting) stage? The authors state "We did not conduct an ablation experiment evaluating SCOR in isolation without SGCR."

### Open Question 2
Can the SwallowCode pipeline achieve similar performance improvements when adapted to other programming languages (e.g., Java, C++, Rust)? The authors claim language-agnosticity but all experiments used Python exclusively.

### Open Question 3
Do the performance gains from rewritten pre-training data persist, diminish, or scale differently when extending training beyond the 50 billion token budget? All ablation studies used a fixed 50B-token budget.

### Open Question 4
To what extent do SwallowCode and SwallowMath inherit or amplify biases from Llama-3.3-70B-Instruct used as the rewriting model? No analysis was conducted on whether rewritten datasets exhibit systematic preferences.

## Limitations

- The core claim that rewriting > filtering lacks a direct, head-to-head ablation against a filtered-only baseline.
- The paper does not report on potential logic hallucinations introduced during rewriting.
- Long-term robustness to messy, real-world codebases is untested; MBPP failure suggests potential brittleness to domain-specific naming conventions.

## Confidence

- **High Confidence:** The SwallowCode/Math datasets are publicly released and the four-stage pipeline is fully specified. The reported performance gains over Stack-Edu and Finemath-4+ are well-supported by the ablation results.
- **Medium Confidence:** The "transform-and-retain" mechanism is supported by internal comparisons but lacks a direct, head-to-head ablation against a filtered-only baseline.
- **Low Confidence:** The long-term robustness of models trained on normalized, rewritten code to messy, real-world codebases is untested.

## Next Checks

1. **Logic Preservation Audit:** Manually inspect 50 rewritten samples (pre/post) for semantic equivalence to quantify hallucination risk.
2. **Filtered vs. Rewritten Head-to-Head:** Train a model using only filtered (high-quality) data from The-Stack-v2 and compare its HumanEval performance to the SwallowCode model.
3. **Domain Generalization Test:** Evaluate the SwallowCode model on a benchmark of real-world, non-standard Python code (e.g., from GitHub issues or legacy systems) to assess robustness to style variance.