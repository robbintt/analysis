---
ver: rpa2
title: 'MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation'
arxiv_id: '2508.16674'
source_url: https://arxiv.org/abs/2508.16674
tags:
- medical
- report
- evaluation
- recall
- structured
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MedRepBench introduces a benchmark for evaluating medical report
  interpretation using vision-language models (VLMs). It contains 1,900 real-world
  Chinese medical reports with structured ground truth and supports both objective
  (field-level recall) and subjective (LLM-based interpretability) evaluation.
---

# MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation

## Quick Facts
- arXiv ID: 2508.16674
- Source URL: https://arxiv.org/abs/2508.16674
- Reference count: 1
- Primary result: Introduces MedRepBench benchmark with 1,900 real-world Chinese medical reports; shows GRPO fine-tuning yields up to 6% recall improvement over supervised baselines.

## Executive Summary
MedRepBench introduces a benchmark for evaluating medical report interpretation using vision-language models (VLMs). It contains 1,900 real-world Chinese medical reports with structured ground truth and supports both objective (field-level recall) and subjective (LLM-based interpretability) evaluation. The study evaluates VLMs and LLMs under end-to-end and OCR-assisted conditions. Results show that VLMs lag behind text-only models under OCR settings but improve significantly when optimized with reinforcement learning. GRPO fine-tuning yields up to 6% recall improvement over supervised baselines. MedRepBench is designed to drive progress in robust, layout-aware medical document understanding.

## Method Summary
MedRepBench evaluates medical report interpretation by extracting structured clinical items (name, value, unit, range, abnormal flag) from images into JSON format. The benchmark uses 1,900 de-identified Chinese medical reports and supports two evaluation modes: objective field-level recall and subjective LLM-based scoring. The primary method involves training InternVL3-8B models with supervised fine-tuning (SFT) on 15k pairs, followed by GRPO optimization on 800 samples using field-level recall as reward. LoRA rank=128 is applied to attention layers while freezing the vision encoder. OCR+LLM pipelines are also evaluated as strong baselines despite layout blindness issues.

## Key Results
- GRPO fine-tuning yields up to 6% recall improvement over supervised baselines
- OCR+LLM pipelines achieve higher field-level recall but suffer from layout blindness and cascading errors
- VLMs improve significantly with reinforcement learning but still lag behind text-only models under OCR settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Group Relative Policy Optimization (GRPO) improves structured field extraction by using field-level recall as a reward signal, enabling mid-scale VLMs to outperform larger baselines.
- Mechanism: GRPO samples multiple candidate outputs per input, computes group-wise reward statistics (mean, variance), and optimizes policy via relative advantage—samples above group average are reinforced. The reward R = average recall across five fields (name, value, unit, range, abnormal) provides dense, task-aligned feedback.
- Core assumption: Field-level recall correlates with clinically useful extraction; the reward landscape is sufficiently smooth for policy gradient optimization.
- Evidence anchors:
  - [abstract] "GRPO fine-tuning yields up to 6% recall improvement over supervised baselines"
  - [section: Reinforcement Learning Optimization] Equations 3-7 define reward and group-relative advantage; "applied to an InternVL3-8B model... achieving +6% gain"
  - [corpus] Med-R1 and MedVLM-R1 (Lai et al. 2025, Pan et al. 2025) show similar RL gains for medical VQA, supporting generalization—though not for document interpretation specifically.
- Break condition: Reward hacking (model optimizes recall at expense of factual grounding); insufficient sample diversity within groups leads to unstable baselines.

### Mechanism 2
- Claim: OCR+LLM pipelines achieve high field-level recall but suffer from layout blindness and cascading errors that VLMs with direct image input can potentially avoid.
- Mechanism: OCR discards spatial layout and visual formatting; LLM receives flattened text, losing table structure, cell boundaries, and spatial relationships. Recognition errors (missing entries, misalignment) propagate uncorrected because LLM cannot cross-reference visual layout. End-to-end VLMs preserve spatial grounding but struggle with text recognition accuracy.
- Core assumption: Layout information is diagnostically relevant; OCR errors are non-trivial and cannot be reliably corrected by LLM reasoning alone.
- Evidence anchors:
  - [abstract] "OCR+LLM pipelines, while strong in metrics, suffer from layout blindness and cascading errors"
  - [section: Results] "On average, field-level recall drops by 10–20% when OCR input is removed"
  - [corpus] Zhang et al. 2025 (cited in Related Work) highlights limitations of vision-only models on OCR-sensitive inputs—supporting the dual challenge.
- Break condition: OCR quality is near-perfect (layout loss becomes negligible); VLM visual-text alignment is so poor that cascading OCR errors still outperform end-to-end.

### Mechanism 3
- Claim: Combining objective field-level recall with LLM-based subjective scoring creates a more reliable evaluation signal than either alone.
- Mechanism: Objective recall captures structured extraction accuracy; subjective LLM evaluation (via DeepSeek-R1) assesses factuality, reasoning validity, and ethical compliance. The LLM evaluator achieves 88.3% accuracy and κ=0.82 against human experts, providing scalable human-aligned assessment.
- Core assumption: The LLM evaluator generalizes beyond benchmark distribution; 3-point scale captures clinically relevant quality distinctions.
- Evidence anchors:
  - [abstract] "supports both objective (field-level recall) and subjective (LLM-based interpretability) evaluation"
  - [section: Agreement Between LLM-Based Evaluator and Human Experts] "Cohen's kappa coefficient of 0.82, indicating substantial inter-rater consistency"
  - [corpus] Liu et al. 2023 (LLM-as-a-Judge) supports LLM-based evaluation validity; no corpus evidence directly validates this specific medical evaluator.
- Break condition: Evaluator model exhibits systematic bias (over/under-scoring specific error types); distribution shift between evaluation prompts and clinical use cases.

## Foundational Learning

- Concept: **Field-level recall vs. surface-level text similarity (BLEU/ROUGE)**
  - Why needed here: Traditional n-gram metrics fail to capture structured medical extraction—e.g., extracting "5.22 mmol/L" correctly matters more than paraphrasing.
  - Quick check question: If a model outputs correct values in wrong JSON keys, would BLEU capture this failure? (Answer: No—field-level recall would.)

- Concept: **Group Relative Policy Optimization (GRPO)**
  - Why needed here: GRPO provides stable RL fine-tuning without a separate value function by using group statistics as baselines.
  - Quick check question: Why compute advantage relative to group mean rather than using absolute reward? (Answer: Reduces variance, stabilizes training.)

- Concept: **Vision-Language Model architecture (vision encoder → projector → LLM)**
  - Why needed here: Understanding which components are frozen vs. trained determines what capabilities transfer (e.g., freezing vision encoder preserves visual grounding).
  - Quick check question: If LoRA is only applied to LLM attention layers, what happens to visual feature quality? (Answer: Preserved—vision encoder unchanged.)

## Architecture Onboarding

- Component map:
  Input: Medical report image (photo/screenshot/e-doc) → Vision encoder (frozen) → Projector (frozen) → LLM backbone (LoRA-adapted)
  Output: Structured JSON with 5 fields per item
  Training: SFT (15k pairs) → GRPO (800 samples, 2 epochs, group size G)
  Evaluation: Field-level recall calculator + DeepSeek-R1 evaluator

- Critical path:
  1. Data preparation: De-identification, OCR extraction, JSON ground-truth annotation
  2. SFT baseline: Train on interpretation task
  3. GRPO optimization: Reward = average recall; freeze vision encoder; LoRA on Q/K/V projections
  4. Evaluation: Run both objective and subjective protocols

- Design tradeoffs:
  - End-to-end VLM vs. OCR+LLM: VLM preserves layout but struggles with text recognition; OCR+LLM has higher recall but cascading errors
  - LoRA rank (128): Higher rank = more capacity but risk of overfitting on 800 GRPO samples
  - Group size G in GRPO: Larger groups = more stable baselines but higher compute

- Failure signatures:
  - Recall improves but subjective score drops → reward hacking on field names without semantic understanding
  - Large variance across seeds → insufficient group diversity or unstable reward scaling
  - JSON parsing errors → model not properly constrained to output format

- First 3 experiments:
  1. **Baseline establishment**: Evaluate InternVL3-8B SFT on MedRepBench without OCR; record per-field recall and subjective scores.
  2. **GRPO ablation**: Train GRPO with varying group sizes (G=4, 8, 16) on 800 samples; measure recall gain and training stability (reward variance).
  3. **OCR comparison**: Run same model with OCR input; quantify the 10-20% recall gap and analyze error types (layout vs. recognition failures).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can end-to-end VLMs close the 10–20% field-level recall gap compared to OCR+LLM pipelines while maintaining layout-awareness?
- Basis in paper: [explicit] The paper states that "field-level recall drops by 10–20% when OCR input is removed" and that this "reveals substantial headroom for improving visual-text alignment and structured reasoning in VLMs."
- Why unresolved: The paper demonstrates GRPO yields +6% gains but does not achieve parity with OCR-assisted baselines, leaving the gap unresolved.
- What evidence would resolve it: A VLM achieving comparable or superior average recall to OCR+LLM pipelines (e.g., >90%) on MedRepBench without OCR input.

### Open Question 2
- Question: How can vision-based models effectively handle cascading errors and layout-blindness inherent in OCR+LLM pipelines for clinical reliability?
- Basis in paper: [explicit] The authors note that OCR+LLM models "suffer from layout-blindness and latency issues" and that recognition errors "can propagate to the LLM, leading to factual hallucinations."
- Why unresolved: The paper identifies the problem but does not propose a solution that maintains OCR+LLM performance while eliminating its architectural limitations.
- What evidence would resolve it: A unified model architecture that matches OCR+LLM recall while demonstrating spatial grounding and robustness to recognition errors.

### Open Question 3
- Question: Do MedRepBench evaluation findings generalize to non-Chinese medical reports and diverse healthcare documentation standards?
- Basis in paper: [inferred] The benchmark focuses exclusively on Chinese-language reports and notes "heterogeneous layout styles, as medical reports lack universally enforced formatting standards."
- Why unresolved: The paper does not evaluate cross-lingual or cross-institutional generalization, which is critical for global applicability.
- What evidence would resolve it: Evaluation of the same VLMs on comparable English or multilingual medical report benchmarks showing consistent performance patterns.

## Limitations
- Dataset access: MedRepBench and its 1,900 reports plus 15k SFT pairs are not publicly available, preventing independent validation.
- GRPO hyperparameter omission: Critical values such as learning rate, group size G, KL coefficient β, and batch size are unspecified.
- Evaluator generalizability: The DeepSeek-R1-based subjective scorer is validated only on the MedRepBench distribution; no cross-dataset or out-of-distribution testing is reported.

## Confidence
- **High confidence**: Core benchmark structure (1,900 reports, 5-field recall, mixed objective/subjective scoring) and reported SFT baseline results.
- **Medium confidence**: GRPO fine-tuning efficacy (+6% recall) given similar RL gains in related medical VQA work, but not directly validated for document interpretation.
- **Low confidence**: LLM-based subjective evaluation robustness across clinical settings, due to lack of external validation and prompt transparency.

## Next Checks
1. **Dataset and prompt release**: Obtain MedRepBench data and full prompt templates; verify field-level recall and subjective scores independently.
2. **GRPO ablation study**: Systematically vary group size G, KL coefficient β, and learning rate; measure recall gains and reward stability.
3. **Cross-dataset evaluator generalization**: Test the DeepSeek-R1 evaluator on external medical report datasets; compute inter-rater agreement and bias metrics.