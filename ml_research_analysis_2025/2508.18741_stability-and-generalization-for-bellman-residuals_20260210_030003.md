---
ver: rpa2
title: Stability and Generalization for Bellman Residuals
arxiv_id: '2508.18741'
source_url: https://arxiv.org/abs/2508.18741
tags:
- bellman
- stability
- lemma
- generalization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper establishes a stability and generalization theory for\
  \ Bellman Residual Minimization (BRM) in offline reinforcement learning and inverse\
  \ reinforcement learning. By reformulating BRM as a Polyak-\u0141ojasiewicz-strongly-concave\
  \ minimax problem, the authors analyze stochastic gradient descent-ascent (SGDA)\
  \ with shared-index coupling on neighboring datasets."
---

# Stability and Generalization for Bellman Residuals

## Quick Facts
- **arXiv ID:** 2508.18741
- **Source URL:** https://arxiv.org/abs/2508.18741
- **Reference count:** 40
- **Primary result:** Proves O(1/n) stability and generalization for Bellman Residual Minimization in offline RL using shared-index coupled SGDA.

## Executive Summary
This paper establishes a stability and generalization theory for Bellman Residual Minimization (BRM) in offline reinforcement learning and inverse reinforcement learning. The authors reformulate BRM as a Polyak-Łojasiewicz-strongly-concave minimax problem and analyze stochastic gradient descent-ascent (SGDA) with shared-index coupling on neighboring datasets. They prove an O(1/n) on-average argument-stability bound without requiring i.i.d. sampling assumptions, which directly yields O(1/n) generalization guarantees for BRM. The analysis accommodates neural-network parameterizations, minibatching, and standard Robbins-Monro stepsizes, providing explicit rates that improve upon existing O(n^{-1/2}) bounds for convex-concave problems.

## Method Summary
The paper analyzes Bellman Residual Minimization (BRM) by reformulating it as a minimax optimization problem using a bi-conjugate trick that avoids the "double sampling" bias. The method uses Stochastic Gradient Descent Ascent (SGDA) with shared-index coupling on neighboring datasets to achieve stability. The analysis proves O(1/n) generalization bounds for neural network parameterizations under Polyak-Łojasiewicz (PL) and strong concavity assumptions. The approach works with minibatches and harmonic stepsize schedules without requiring variance reduction or extra regularization.

## Key Results
- Proves O(1/n) on-average argument-stability bound for SGDA on BRM without i.i.d. sampling assumptions
- Achieves O(1/n) generalization guarantees that improve upon existing O(n^{-1/2}) bounds for convex-concave problems
- Accommodates neural-network parameterizations, minibatching, and standard Robbins-Monro stepsizes
- Shows empirical Bellman residual is a reliable proxy for true population risk with parametric sample complexity

## Why This Works (Mechanism)

### Mechanism 1: Bi-Conjugate Reformulation for Double Sampling
Minimizing Mean Squared Bellman Error (MSBE) is reduced to a tractable minimax optimization problem that avoids "double sampling" bias inherent in offline TD learning. The authors introduce a dual variable ζ to estimate the variance term, transforming the objective into a minimax form compatible with SGDA. The bias correction fails if the function class for ζ is insufficiently expressive.

### Mechanism 2: Shared-Index Coupling for Non-I.I.D. Stability
SGDA generalizes on offline data even when samples are correlated (non-i.i.d.) using the same random seed for minibatch selection on neighboring datasets. This isolates the impact of the differing sample, which has only O(1/n) probability of being drawn. The stability bound loosens if the data distribution is degenerate with O(1) sampling probability for specific samples.

### Mechanism 3: Lyapunov Potential Contraction under PL Geometry
The optimization landscape satisfies the PL condition and strong concavity, ensuring a Lyapunov potential contracts faster than noise accumulates. The potential combines primal suboptimality and dual gap, contracting by factor (1-cηt) at each step. If the neural network parameterization fails to satisfy the PL condition, the contraction property is lost and the O(1/n) rate is not guaranteed.

## Foundational Learning

- **Concept: Polyak-Łojasiewicz (PL) Condition**
  - Why needed here: The paper relies on the PL condition to prove convergence without convexity. PL implies ||∇f||² ≥ μ(f - f*), guaranteeing gradient descent finds the global minimum.
  - Quick check question: Does a function being PL imply it is convex? (Answer: No)

- **Concept: Algorithmic Stability (On-Average Argument Stability)**
  - Why needed here: This bridges the optimization algorithm (SGDA) and statistical generalization gap. It measures how much output changes if one training point is replaced.
  - Quick check question: If an algorithm has uniform stability ε, what is the upper bound on its generalization error? (Answer: O(ε))

- **Concept: Stochastic Gradient Descent Ascent (SGDA)**
  - Why needed here: BRM is formulated as a minimax game. SGDA is the tool used to solve it. Distinguish descent step (value function Q) from ascent step (dual variable ζ).
  - Quick check question: In a non-convex/non-concave game, does SGDA always converge to a saddle point? (Answer: No, but the paper proves it does here due to PL geometry)

## Architecture Onboarding

- **Component map:** Offline Dataset D → Bi-conjugate transformation → Minibatch SGDA with shared-index coupling → Primal network Q_θ and Dual network ζ_θ → Generalized Bellman Residual minimizer

- **Critical path:** Correct implementation of bi-conjugate objective (Eq. 4) → Ensuring PL/Strongly Concave structure holds (architecture choice) → SGDA training loop

- **Design tradeoffs:**
  - Minibatch Size (B): Larger B reduces variance but increases computation. Theory holds for any B ≥ 1.
  - Step Size: Harmonic steps (c₁/t) are theoretically required for O(1/n) rate but may be slower than constant/adaptive steps.

- **Failure signatures:**
  - Divergence: Gradients exploding implies "effective domain" constraint is violated or PL condition failed.
  - Stagnation: If dual variable ζ fails to track V_Q, bias correction term vanishes.

- **First 3 experiments:**
  1. Sanity Check (Toy MDP): Train on small tabular MDP where Q* is known. Plot ||Q_t - Q*|| and Stability Constant ε_t to verify O(1/T) and O(1/n) decay.
  2. Ablation on Coupling: Run SGDA on dataset with high autocorrelation. Compare standard analysis vs. "shared-index" implementation to verify stability holds without i.i.d. shuffling.
  3. Scale Test: Verify empirical generalization gap (Population Risk - Empirical Risk) scales as 1/n by varying dataset size n on standard offline RL benchmark.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the O(1/n) stability guarantee hold when the offline dataset consists of a single, highly correlated Markovian trajectory rather than independent samples?
- Basis in paper: [Inferred] The paper assumes fixed batch of logged trajectories and uses shared-index coupling to avoid i.i.d. assumptions, but stability analysis relies on "replace-one" neighboring datasets treating data points as exchangeable units.
- Why unresolved: Current analysis bounds impact of changing single sample, but temporal dependence within single trajectory could amplify error propagation in ways not captured by current Lyapunov potential.
- What evidence would resolve it: Stability analysis incorporating mixing time of behavior policy, or empirical validation on datasets with varying autocorrelation.

### Open Question 2
- Question: Can the bounded gradient assumption (A2) be removed or rigorously proven for unprojected SGDA on neural networks?
- Basis in paper: [Inferred] Page 7 states Assumption (A2) presumes iterates remain in bounded set Ω, justified heuristically by coercivity of landscape but not strictly proven for unprojected SGDA dynamics.
- Why unresolved: Neural network training operates in unbounded parameter spaces, and strict gradient bounds are difficult to guarantee without explicit projection or clipping.
- What evidence would resolve it: Theoretical proof showing SGDA iterates naturally remain bounded due to problem geometry, or modification of stability proof using local gradient bounds.

### Open Question 3
- Question: Is the O(1/n) excess risk rate minimax optimal for Bellman residual minimization under PL-strongly-concave geometry?
- Basis in paper: [Explicit] Abstract and introduction highlight improvement from O(1/√n) to O(1/n) but do not discuss if faster rate is theoretically possible.
- Why unresolved: Establishing statistical lower bounds for this specific saddle-point problem is necessary to confirm optimality.
- What evidence would resolve it: Derivation of statistical lower bound for BRM showing no algorithm can achieve rate faster than O(1/n) under stated assumptions.

### Open Question 4
- Question: How does the constant C_dist, which depends on PL and strong concavity parameters, scale with dimensionality of neural network parameter space?
- Basis in paper: [Inferred] Theorem 3 provides stability bound with constants depending on μ_PL, μ_QG, and ρ, but dependence on network width or depth is not characterized.
- Why unresolved: For deep learning applications, these optimization constants can degrade with dimension, potentially obscuring benefits of O(1/n) rate.
- What evidence would resolve it: Theoretical characterization of how PL and strong concavity parameters scale with over-parameterization in neural networks.

## Limitations
- Relies on PL condition and strong concavity holding for neural network parameterizations, which may not hold for all architectures
- Assumes reward structure with Gumbel-distributed noise satisfies specific technical conditions
- Shared-index coupling mechanism may not generalize to adaptive sampling schemes or prioritized replay
- Harmonic stepsize schedule is theoretically necessary but may be suboptimal in practice compared to adaptive methods

## Confidence

- **High Confidence:** The stability bound derivation and its connection to generalization error (Theorem 3.1) - the proof technique is rigorous and mathematical steps are verifiable
- **Medium Confidence:** The PL condition and strong concavity assumptions holding for practical neural network implementations - requires empirical validation across different architectures
- **Medium Confidence:** The bi-conjugate reformulation correctly avoiding double sampling bias in practice - theory is sound but empirical verification is needed

## Next Checks

1. **Architecture Sensitivity Test:** Train same BRM algorithm with different neural network architectures (varying depth/width) on standard offline RL benchmark to verify PL condition empirically holds and O(1/n) generalization rate is maintained.

2. **Reward Structure Verification:** Implement algorithm on standard RL environments with deterministic or Gaussian rewards (not Gumbel) and measure whether theoretical preconditions are violated or if algorithm still performs well empirically.

3. **Comparison with Standard TD Learning:** Implement both proposed bi-conjugate BRM and standard squared TD error minimization on datasets with varying levels of autocorrelation. Measure and compare empirical generalization gaps to verify double sampling bias is mitigated.