---
ver: rpa2
title: 'Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement
  Learning'
arxiv_id: '2506.02911'
source_url: https://arxiv.org/abs/2506.02911
tags:
- cell
- reasoning
- cells
- type
- genes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cell-o1 introduces a batch-level reasoning framework for single-cell
  RNA-seq annotation that mirrors expert workflows. By jointly assigning unique cell
  types to each cell using gene expression and contextual metadata, it moves beyond
  single-cell independent labeling.
---

# Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning

## Quick Facts
- arXiv ID: 2506.02911
- Source URL: https://arxiv.org/abs/2506.02911
- Reference count: 40
- Cell-o1 achieves 32.88% batch-level accuracy on CellPuzzles, outperforming OpenAI's o1 by 73% (19.0% vs 32.9%)

## Executive Summary
Cell-o1 introduces a batch-level reasoning framework for single-cell RNA-seq annotation that mirrors expert workflows. By jointly assigning unique cell types to each cell using gene expression and contextual metadata, it moves beyond single-cell independent labeling. The model is trained via supervised fine-tuning on distilled expert-like reasoning traces, followed by reinforcement learning with batch-level rewards. On the CellPuzzles benchmark, Cell-o1 achieves 32.88% batch-level accuracy, outperforming OpenAI's o1 by over 73% (19.0% vs 32.9%). It also demonstrates emergent reasoning behaviors like self-reflection and curriculum reasoning, generalizes to unseen diseases, and maintains stable performance across varying batch sizes.

## Method Summary
Cell-o1 uses a two-stage training approach: first supervised fine-tuning on 3,912 distilled reasoning traces from OpenAI's o1, then reinforcement learning with group-relative policy optimization (GRPO). The task formulation requires assigning N unique cell types to a batch of N cells from the same donor, using ranked gene expression and metadata. The model receives sparse batch-level rewards (1 for perfect assignment, 0 for partial, -1 for format errors) and uses KL regularization to prevent catastrophic forgetting during RL.

## Key Results
- Cell-o1 achieves 32.88% batch-level accuracy on CellPuzzles, outperforming OpenAI's o1 (19.0%) by 73%
- Joint batch-level reasoning outperforms independent cell labeling, with decomposition to single-cell task reducing accuracy to 1.1%
- Model generalizes to unseen diseases, maintaining >38% accuracy on held-out conditions
- Demonstrates emergent reasoning behaviors including self-reflection and curriculum reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Joint batch-level annotation with contextual reasoning outperforms independent cell-by-cell labeling.
- **Mechanism:** The task formulation provides N cells from the same donor with shared metadata (tissue, disease, age) and requires assigning N unique labels from a candidate set. This forces the model to perform comparative reasoning across cells and satisfy mutual exclusivity constraints, rather than treating each cell in isolation.
- **Core assumption:** Expert annotation relies on contextual comparison across related cells; modeling this improves biological validity.
- **Evidence anchors:**
  - [abstract] "requires reasoning across the batch-level cellular context to ensure label uniqueness"
  - [section 3] "each instance consists of a batch of N cells... each sampled from a distinct cell type within the same donor and experimental batch"
  - [corpus] Related work scAgent and ReCellTy similarly emphasize context-aware annotation but use different mechanisms (agents, knowledge graphs); corpus does not directly validate batch-level formulation.
- **Break condition:** If cells are processed independently (decomposed setup), batch-level accuracy collapses from 32.88% to ~1.1% even for strong models.

### Mechanism 2
- **Claim:** Supervised fine-tuning on distilled reasoning traces serves as a necessary cold-start before reinforcement learning.
- **Mechanism:** OpenAI's o1 generates reasoning traces for 10,155 instances; rejection sampling accepts only those with correct final answers and valid format (3,912 accepted, 38.52% rate). SFT on this filtered dataset teaches the model task-specific reasoning patterns and output formatting before RL policy optimization begins.
- **Core assumption:** Distilled traces from a stronger model encode transferable reasoning patterns that bootstrap learning.
- **Evidence anchors:**
  - [section 4.2.1] "A response is accepted only if it (1) adheres to the expected format and (2) yields a cell-type assignment that exactly matches the ground-truth labels"
  - [section 6.1] "Removing SFT initialization (B) leads to ineffective reward learning and a sharp increase in response length"
  - [corpus] Corpus does not provide comparative evidence for cold-start vs. direct RL in this domain.
- **Break condition:** Without SFT, early RL rollouts produce invalid formats and zero rewards, causing policy stagnation.

### Mechanism 3
- **Claim:** Sparse batch-level rewards combined with GRPO optimization incentivize global consistency across cell assignments.
- **Mechanism:** The reward function returns 1 only if ALL N cells match ground truth, 0 for partial correctness, and -1 for format violations. GRPO estimates advantages from G grouped rollouts without a separate critic, using KL regularization to prevent catastrophic forgetting.
- **Core assumption:** Sparse binary rewards at the batch level force learning of joint assignment strategies rather than local heuristics.
- **Evidence anchors:**
  - [section 4.3.2] "a reward of 1 is given only when all cell types in the batch are correctly predicted; any incorrect prediction results in a reward of 0"
  - [section 6.1] Configuration with cell+batch reward shows higher training rewards but comparable/slightly less stable test performance vs. batch-only reward.
  - [corpus] No corpus papers evaluate GRPO specifically; comparison to PPO in paper shows GRPO yields more stable training.
- **Break condition:** Dense cell-level rewards alone improve training reward but may not translate to batch-level generalization; format violations cause immediate -1 penalty regardless of partial correctness.

## Foundational Learning

- **Concept: Single-cell RNA-seq representation**
  - **Why needed here:** Inputs are ranked gene expression lists (top M genes per cell), not raw counts. Understanding that gene rankings approximate marker genes used by experts is essential.
  - **Quick check question:** Can you explain why MALAT1, RPS27, RPL10 appearing together suggests a different interpretation than IGKC, IGHA1, JCHAIN?

- **Concept: Policy gradient RL with advantage estimation**
  - **Why needed here:** GRPO uses group-relative advantages to avoid training a critic. Understanding how advantages normalize rewards across rollouts clarifies why grouped sampling matters.
  - **Quick check question:** What happens to advantage estimates if all G rollouts for a prompt receive reward 0?

- **Concept: Chain-of-thought reasoning in LLMs**
  - **Why needed here:** The model generates explicit reasoning in lbrakk...ViewSet tags before answers. Understanding CoT helps diagnose whether failures stem from reasoning errors vs. final answer mapping.
  - **Quick check question:** If a model produces correct reasoning but outputs the wrong label, which component should you investigate?

## Architecture Onboarding

- **Component map:** Input encoder -> SFT stage (Qwen2.5-7B-Instruct + LoRA) -> RL stage (GRPO) -> Reward module
- **Critical path:**
  1. Reasoning distillation (o1 → rejection sampling → 3,912 examples)
  2. SFT (10 epochs, lr=5e-5, batch=8, grad_accum=16)
  3. RL dataset assembly (3,912 SFT + 3,000 extra = 6,912 total)
  4. GRPO training (20 epochs, max_prompt=3,072, max_response=3,000)
- **Design tradeoffs:**
  - Sparse vs. dense rewards: Batch-only reward (binary) enforces global consistency but increases variance; adding cell-level reward stabilizes training but may not improve final batch accuracy
  - GRPO vs. PPO: GRPO avoids critic training overhead; PPO produces shorter, less informative outputs in this setup
  - SFT dataset size vs. quality: Rejection sampling at 38.52% acceptance trades quantity for quality; lower threshold increases data but introduces noise
- **Failure signatures:**
  - Repeated labels within batch: Indicates model not learning mutual exclusivity constraint
  - Format violations (missing/extra tags): Causes immediate -1 reward; check response length spike
  - Valid reasoning, wrong conclusion: Suggests final mapping layer error, not reasoning failure
  - Forced assignment errors: Model reasoning correct but label assigned to wrong cell due to constraint pressure
- **First 3 experiments:**
  1. **Ablate SFT initialization:** Train RL directly from base Qwen2.5-7B-Instruct; expect reward stagnation and format errors to confirm cold-start necessity
  2. **Vary batch size N (8→15):** Measure accuracy degradation curve; Cell-o1 should show smaller decline than baselines per Figure 4
  3. **Cross-disease generalization test:** Evaluate on completely held-out diseases (Breast Cancer, Melanoma, SLE, Colorectal Cancer as in Table 3); verify Cell-o1 maintains >38% batch accuracy vs. o1's ~28%

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on real-world, noisy single-cell datasets remains untested
- Computational overhead of joint reasoning across large batches has not been characterized
- Model's behavior on extremely rare cell types or novel cell states is unknown

## Confidence
**High Confidence:**
- Superiority of batch-level annotation over independent cell labeling (1.1% accuracy collapse when decomposed)
- Necessity of SFT initialization (reward stagnation and format errors in ablation)
- Effectiveness of sparse batch-level rewards (consistent performance gains over cell-level only)

**Medium Confidence:**
- Generalization to unseen diseases (promising but not definitive evidence)
- Transferability of distilled reasoning traces (plausible but not independently verified)
- Stability across batch sizes (shown but with limited size variation)

**Low Confidence:**
- Scalability to real-world datasets with variable cell counts
- Performance on rare cell types and novel cell states
- Computational efficiency of joint reasoning for large batches

## Next Checks
1. **Real-world dataset validation:** Evaluate Cell-o1 on a completely independent, real-world single-cell dataset from a new tissue type or disease not represented in CellPuzzles, including datasets with >100 cells per batch to test scalability

2. **Gene coverage sensitivity analysis:** Systematically vary M (top genes used) from 10 to 100 to determine the minimum gene coverage required for stable performance, and test whether adding gene expression values (not just rankings) improves accuracy

3. **Reasoning trace interpretability audit:** Manually examine 100 reasoning traces from Cell-o1's predictions to assess whether the generated reasoning aligns with known biological principles and whether errors stem from reasoning failures or final label assignment mistakes