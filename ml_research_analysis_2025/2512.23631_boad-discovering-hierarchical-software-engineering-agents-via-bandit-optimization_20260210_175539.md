---
ver: rpa2
title: 'BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization'
arxiv_id: '2512.23631'
source_url: https://arxiv.org/abs/2512.23631
tags:
- sub-agents
- code
- subagent
- sub-agent
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BOAD, a method for automatically discovering
  effective hierarchical multi-agent systems for software engineering tasks. BOAD
  treats agent design as a multi-armed bandit problem, where each arm represents a
  candidate sub-agent and rewards measure helpfulness when collaborating with others.
---

# BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization

## Quick Facts
- arXiv ID: 2512.23631
- Source URL: https://arxiv.org/abs/2512.23631
- Reference count: 40
- Key outcome: BOAD achieves 20.0% success rate on SWE-bench-Live, ranking second on the leaderboard and outperforming larger models like GPT-4 and Claude.

## Executive Summary
This paper introduces BOAD, a method for automatically discovering effective hierarchical multi-agent systems for software engineering tasks. BOAD treats agent design as a multi-armed bandit problem, where each arm represents a candidate sub-agent and rewards measure helpfulness when collaborating with others. By balancing exploration and exploitation, BOAD efficiently identifies useful sub-agent combinations without requiring exhaustive evaluation. On SWE-bench-Live, BOAD achieves 20.0% success rate, ranking second on the leaderboard and outperforming larger models like GPT-4 and Claude. On SWE-bench-Verified, it achieves 53.2%, setting a new state of the art among smaller models. BOAD demonstrates that automatically discovered hierarchical structures significantly improve generalization on long-horizon software engineering tasks.

## Method Summary
BOAD optimizes hierarchical multi-agent systems for software engineering by formulating agent discovery as a multi-armed bandit problem. Candidate sub-agents are stored in an archive and evaluated through UCB-based selection. The system uses hindsight credit assignment via LLM-as-judge to estimate each sub-agent's helpfulness from execution trajectories, avoiding the free-rider problem of success-rate attribution. The method runs for B=20 rounds, selecting K=3 sub-agents per round from the archive, and uses a Chinese Restaurant Process to optionally expand the sub-agent pool. The final system deploys the top-2 sub-agents by helpfulness with a customized orchestrator. BOAD was evaluated on SWE-bench-Live (300 instances) and SWE-bench-Verified (500 instances) using Seed-OSS-36B-Instruct, achieving state-of-the-art results among smaller models.

## Key Results
- BOAD achieves 20.0% success rate on SWE-bench-Live, ranking second on the leaderboard
- BOAD sets new state of the art at 53.2% on SWE-bench-Verified among smaller models
- Performance peaks with exactly 2 sub-agents (20.0%), with larger teams (3-5) showing degradation due to coordination overhead
- Helpfulness-based selection outperforms success-rate selection (20.0% vs 15.3% for top-2, 16.3% vs 11.3% for top-3)

## Why This Works (Mechanism)

### Mechanism 1
Hierarchical decomposition reduces spurious correlations by limiting irrelevant context during long reasoning chains. Sub-agents handle specialized sub-tasks (localization, editing, validation) with isolated context. The orchestrator coordinates at a higher abstraction level without retaining low-level action history. This mirrors human cognitive load management and hierarchical RL's temporal abstraction. Core assumption: Spurious correlations from context overload are a primary cause of generalization failure in single-agent SWE systems. Evidence: Abstract states "monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization." Break condition: If sub-agent boundaries are poorly defined, context isolation fails and overhead exceeds benefits.

### Mechanism 2
Bandit-based exploration with hindsight credit assignment efficiently identifies useful sub-agents under limited evaluation budgets. Each sub-agent is a bandit arm with reward = "helpfulness" judged post-hoc via LLM-as-judge on trajectories. UCB balances exploitation (high observed helpfulness) with exploration (under-sampled arms). This linearizes the combinatorial search space by evaluating sub-agents individually rather than all combinations. Core assumption: A sub-agent's contribution can be meaningfully estimated from trajectory analysis even when the full system fails. Evidence: Section 5.3 shows "helpfulness-based selection consistently outperforms success-rate selection" (16.3% vs 11.3% for top-3). Break condition: If LLM-as-judge evaluations are noisy or systematically biased toward certain sub-agent types, UCB will converge to suboptimal arms.

### Mechanism 3
Small, focused sub-agent teams outperform both minimal and large teams due to coordination overhead. Exactly 2 sub-agents achieved peak performance (20.0%). Single agents lack specialization; larger teams (3-5) suffer communication overhead and coordination failures. The orchestrator must be customized to the specific sub-agent set. Core assumption: There exists an optimal team size balancing specialization against coordination costs. Evidence: Section 5.3 states "performance peaks with exactly two sub-agents... larger teams of three (16.3%), four (16.7%), or five (13.7%) reduce performance due to communication and coordination overhead." Break condition: If task complexity varies significantly across problem instances, fixed team size may be suboptimal; adaptive team sizing could be needed.

## Foundational Learning

- **Multi-Armed Bandit (MAB) and UCB**: Core optimization framework for sub-agent selection under exploration-exploitation tradeoff. Quick check: Given 10 arms with unknown reward distributions and a budget of 50 pulls, would UCB outperform greedy selection? Why?
- **Semi-Markov Decision Process (SMDP) / Temporal Abstraction**: Theoretical grounding for hierarchical agent structures where sub-agents execute temporally extended "options." Quick check: How does temporal abstraction reduce the effective planning horizon in long-horizon tasks?
- **Credit Assignment in Multi-Agent Systems**: Understanding why naive success-rate attribution fails (free-rider problem) motivates hindsight-based evaluation. Quick check: If a 3-agent team succeeds but one agent made errors later compensated by others, would success-rate correctly credit each agent?

## Architecture Onboarding

- **Component map**: Sub-agent Archive -> Warmup Stage -> UCB Selector -> Orchestrator -> Hindsight Evaluator -> Archive Expander
- **Critical path**: Bootstrap archive → Warmup sub-agents → For each round: UCB select top-K → Instantiate orchestrator → Evaluate on design set → Hindsight scoring → Update statistics → Optionally expand archive → Return top-2 by helpfulness
- **Design tradeoffs**: Design set size: Paper found 6-24 instances yield similar results; larger sets increase cost without clear gain. K (sub-agents per round): K=3 during optimization; final deployment uses top-2. Model choice: Claude-4 for generation/judging (high capability), Seed-OSS-36B for execution (controls for fine-tuning confounds). Budget B=20 rounds: Sufficient for convergence; later-discovered sub-agents tend to be worse.
- **Failure signatures**: Error propagation: Orchestrator accepts erroneous sub-agent outputs without validation, leading subsequent steps astray. Over-editing: Single agents produce excessively large patches; mitigated by hierarchical separation. Free-riders: Sub-agents with high success-rate co-occurrence but low actual contribution; detected via hindsight scoring. Stagnation: Fixed archive without CRP expansion leads to repeated exploitation of early sub-agents.
- **First 3 experiments**: 1) Reproduce the bandit vs. success-rate ablation: Run BOAD for 10 rounds on a small design set (12 instances), compare top-2 selection by helpfulness vs. success-rate on a held-out test split. Expected: ~5 percentage point gap favoring helpfulness. 2) Ablate team size on your own task: Evaluate top-{1,2,3,4,5} sub-agents from a converged archive on 50 held-out instances. Expected: Peak at K=2, degradation at K≥3. 3) Test cross-model transfer: Apply sub-agents discovered for Model A to Model B on the same benchmark. Expected: Partial transfer with smaller gains than original model (paper reports 13.7% → 16.3% for Claude 3.7).

## Open Questions the Paper Calls Out

- Can incorporating lightweight verification mechanisms (e.g., span cross-checks, invariant tests, or dual-read localization) effectively mitigate error propagation when orchestrators unconditionally accept sub-agent outputs?
- Can adaptive team sizing dynamically determine the optimal number of sub-agents per task instance, rather than using a fixed team size?
- Do the performance gains from BOAD-discovered sub-agents scale effectively when applied to significantly larger frontier models (e.g., GPT-4, Claude 4)?

## Limitations

- Reliance on LLM-as-judge for hindsight credit assignment introduces potential noise and systematic bias
- Optimal team size finding (K=2) may not generalize across all software engineering tasks
- Study focuses on relatively small, well-defined GitHub issues, limiting confidence in performance on more complex, open-ended software engineering challenges

## Confidence

- **High Confidence**: The core claim that hierarchical decomposition improves SWE performance is well-supported by both BOAD's results and convergent evidence from TDFlow in the literature. The bandit-based exploration mechanism is technically sound and the ablation showing helpfulness-based selection outperforming success-rate attribution is convincing.
- **Medium Confidence**: The specific finding that exactly two sub-agents is optimal may be task-dependent and warrants further investigation. The claim about spurious correlation reduction is theoretically plausible but not directly measured in the paper.
- **Low Confidence**: The assertion that BOAD sets "new state of the art among smaller models" should be interpreted cautiously, as direct comparisons with other specialized architectures are limited in the paper.

## Next Checks

1. **Judge Reliability Test**: Run the hindsight evaluation using multiple judge models (Claude-3.5, GPT-4, Gemini) on the same trajectories to quantify inter-judge agreement and assess the robustness of helpfulness scoring.

2. **Cross-Domain Transfer**: Apply BOAD-discovered sub-agents to a different software engineering benchmark (e.g., HumanEval, MBPP) to test generalization beyond SWE-bench.

3. **Adaptive Team Sizing**: Modify BOAD to dynamically adjust K based on problem complexity (e.g., using problem metadata or early execution signals) and compare against fixed K=2 performance.