---
ver: rpa2
title: 'RM-Distiller: Exploiting Generative LLM for Reward Model Distillation'
arxiv_id: '2601.14032'
source_url: https://arxiv.org/abs/2601.14032
tags:
- teacher
- preference
- response
- rm-distiller
- generative
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'RM-Distiller introduces a systematic approach to distilling generative
  LLMs into reward models by exploiting three key capabilities: refinement, scoring,
  and generation. The method synthesizes highly correlated preference pairs through
  contrastive refinement, captures nuanced preference strength via margin-aware regression,
  and preserves linguistic knowledge through generative regularization.'
---

# RM-Distiller: Exploiting Generative LLM for Reward Model Distillation

## Quick Facts
- arXiv ID: 2601.14032
- Source URL: https://arxiv.org/abs/2601.14032
- Authors: Hongli Zhou; Hui Huang; Wei Liu; Chenglong Wang; Xingyuan Bu; Lvyuan Han; Fuhai Song; Muyun Yang; Wenhao Jiang; Hailong Cao; Tiejun Zhao
- Reference count: 38
- Primary result: 95.5% average accuracy on RewardBench, outperforming traditional distillation methods

## Executive Summary
RM-Distiller introduces a systematic approach to distilling generative LLMs into reward models by exploiting three key capabilities: refinement, scoring, and generation. The method synthesizes highly correlated preference pairs through contrastive refinement, captures nuanced preference strength via margin-aware regression, and preserves linguistic knowledge through generative regularization. Experiments show RM-Distiller achieves state-of-the-art performance on RewardBench and RM-Bench, demonstrating superior performance in downstream RLHF tasks with gains of 2.1-2.4 points on AlpacaEval 2.0. The approach also shows strong generalizability and data efficiency, maintaining high performance with only 1,000 training instructions.

## Method Summary
RM-Distiller trains reward models by leveraging three teacher capabilities: contrastive refinement (synthesizing correlated response pairs), margin-aware regression (capturing preference strength), and generative regularization (preserving linguistic knowledge). The method uses a teacher LLM to generate initial preference pairs, then refines rejected responses minimally while capturing precise score margins. A student model is trained on this data using a combined loss of margin-aware regression and generative regularization. The approach is evaluated on RewardBench (95.5% accuracy) and RM-Bench (62.7%), showing significant improvements over traditional distillation methods and strong performance in downstream RLHF tasks.

## Key Results
- Achieves 95.5% average accuracy on RewardBench, outperforming traditional distillation methods
- Reaches 62.7% accuracy on RM-Bench, demonstrating superior preference strength capture
- Shows 2.1-2.4 point gains on AlpacaEval 2.0 in downstream RLHF tasks
- Maintains high performance with only 1,000 training instructions (data efficiency)

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Refinement Eliminates Spurious Correlations
The teacher LLM diagnoses why a rejected response is inferior, then minimally modifies it to match teacher-quality. The resulting (y*_w, y_l) pair shares near-identical structure, differing only in correctness attributes. The RM must now discriminate on meaningful features rather than superficial shortcuts like length or format. The method uses edit distance filtering (τ_s = 3) to ensure refinements are subtle enough to preserve semantic backbone while isolating the corrective signal.

### Mechanism 2: Margin-Aware Regression Captures Preference Strength
Instead of binary labels, the teacher assigns scalar scores to both responses, and the student's reward difference is regressed toward the teacher's score margin via MSE loss. Self-calibrated scoring conditions on the rejected response's score as an anchor, improving consistency. This captures nuanced intensity of preference that traditional Bradley-Terry loss misses, providing richer supervision for the reward model to distinguish subtle improvements from categorical errors.

### Mechanism 3: Generative Regularization Prevents Representation Collapse
Joint training with generative distillation loss preserves the backbone's linguistic knowledge, preventing reward over-optimization where the model gradually loses its foundational understanding of language. The student's LM head is supervised via NLL on teacher responses and KL divergence against teacher token distributions. This anchors the shared backbone during discriminative RM training, maintaining the generative capabilities that support discriminative performance.

## Foundational Learning

- **Bradley-Terry Preference Modeling**
  - Why needed here: RM-Distiller's margin-aware regression modifies the standard BT objective; understanding the baseline clarifies what's being improved
  - Quick check question: Can you explain why Eq. 1 (BT loss) only captures ordinal preference, not strength?

- **Knowledge Distillation (Logit vs. Response-based)**
  - Why needed here: Generative regularization uses KL divergence on token distributions; this is logit-based KD applied to an RM's language head
  - Quick check question: What's the difference between distilling from logits versus from generated text only?

- **Catastrophic Forgetting in Multi-Task Learning**
  - Why needed here: The paper claims discriminative-only training degrades linguistic knowledge; regularization is the mitigation
  - Quick check question: Why does fine-tuning on a single task (reward prediction) risk degrading pre-trained capabilities?

## Architecture Onboarding

- **Component map:** Qwen2.5-3B-Instruct backbone -> Reward head (scalar output) + LM head (retained for regularization) -> GPT-4o/Qwen3-14B teacher
- **Critical path:** Generate response pool from candidate models → Initial preference annotation via teacher → Contrastive refinement: teacher produces y*_w from y_l → Self-calibrated scoring: anchor on rejected score, score refined response → Filter pairs by margin threshold (τ_s) and edit distance (τ_e) → Train student with combined L_margin + L_reg
- **Design tradeoffs:** Closed vs. open teacher (closed forces β=0, open enables full regularization); Filtering thresholds (higher τ_s yields cleaner data but fewer samples, paper uses τ_s=3, τ_e=0); Regularization weights (α, β control discriminative vs. generative balance, both at 0.2 optimal)
- **Failure signatures:** RM collapses to superficial features (length bias, format) → refinement quality issue; Reward scores become uncalibrated → margin loss convergence problem; Downstream RL policy exhibits reward hacking → RM lacks generalization
- **First 3 experiments:** 1) Reproduce ablation: Train with only BT Classifier, then incrementally add Margin-Aware Regression, Contrastive Refinement, and Generative Regularization 2) Data efficiency test: Train on 1K instructions with RM-Distiller vs. BT baseline 3) Cross-teacher validation: Swap GPT-4o for Qwen3-14B and verify performance consistency

## Open Questions the Paper Calls Out

- **Distilling from human-LLM collaboration:** The paper suggests future work on distilling preference signals from human-LLM collaboration frameworks to improve alignment efficacy beyond pure AI feedback, but doesn't define a mechanism to integrate human oversight into the distillation loop.

- **Optimizing generative regularization for closed-source teachers:** For closed-source models like GPT-4o, the coefficient β for KL divergence is set to 0, relying solely on NLL loss. The paper doesn't propose alternative mechanisms to mimic the "soft" distillation of linguistic knowledge when the teacher's probability distribution is hidden.

- **Robustness of contrastive refinement to hallucinations:** The refinement process assumes the teacher's modifications yield superior responses, but if the teacher hallucinates corrections, the resulting high-correlation pairs might reinforce factual errors. The filtering criteria don't explicitly verify factual correctness of teacher modifications.

## Limitations

- Reliance on teacher LLM consistency for refinement may introduce bias if teachers systematically miss certain error types or over-emphasize stylistic preferences
- Edit distance filtering (τ_e = 3) appears arbitrary and may discard valuable contrastive pairs requiring slightly larger modifications
- Assumption that KL regularization preserves linguistic knowledge hasn't been empirically validated through controlled forgetting experiments

## Confidence

- **High Confidence**: Contrastive refinement improves data quality over standard pairwise annotation (95.5% vs 91.8% on RewardBench)
- **Medium Confidence**: Margin-aware regression captures preference strength better than binary labels (62.7% on RM-Bench, limited ablation on margin sensitivity)
- **Medium Confidence**: Generative regularization prevents representation collapse (Table 6 shows improvement, lack of controlled forgetting experiments)
- **Low Confidence**: The 2.1-2.4 point AlpacaEval gains are downstream validation of RM quality (RLHF pipeline introduces multiple confounding factors)

## Next Checks

1. **Teacher Consistency Audit**: Analyze variance in teacher refinements and scores across multiple annotations of the same response pairs to quantify annotation reliability and potential systematic biases.

2. **Edit Distance Sensitivity Analysis**: Systematically vary τ_e from 0 to 10 and measure impact on downstream performance to determine optimal filtering threshold and understand the tradeoff between refinement quality and data quantity.

3. **Knowledge Retention Experiment**: Train a baseline RM with only discriminative objective (no generative regularization) and evaluate on a held-out generative capability test set to directly measure catastrophic forgetting.