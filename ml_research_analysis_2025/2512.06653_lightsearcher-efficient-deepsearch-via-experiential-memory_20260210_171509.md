---
ver: rpa2
title: 'LightSearcher: Efficient DeepSearch via Experiential Memory'
arxiv_id: '2512.06653'
source_url: https://arxiv.org/abs/2512.06653
tags:
- tool
- reasoning
- search
- arxiv
- lightsearcher
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LightSearcher addresses the efficiency-accuracy trade-off in DeepSearch
  systems by integrating experiential memory with adaptive reward shaping. The method
  learns from contrastive reasoning trajectories to generate interpretable guidance
  and penalizes redundant tool usage only in correct-answer scenarios.
---

# LightSearcher: Efficient DeepSearch via Experiential Memory

## Quick Facts
- arXiv ID: 2512.06653
- Source URL: https://arxiv.org/abs/2512.06653
- Reference count: 40
- Primary result: Reduces tool invocations by 39.6% while maintaining accuracy comparable to ReSearch

## Executive Summary
LightSearcher addresses the efficiency-accuracy trade-off in DeepSearch systems by integrating experiential memory with adaptive reward shaping. The method learns from contrastive reasoning trajectories to generate interpretable guidance and penalizes redundant tool usage only in correct-answer scenarios. Experiments on four multi-hop QA benchmarks demonstrate that LightSearcher maintains accuracy comparable to the state-of-the-art ReSearch while reducing search tool invocations by 39.6%, inference time by 48.6%, and token consumption by 21.2%.

## Method Summary
LightSearcher uses GRPO training on Qwen2.5-3B/7B-Instruct with contrastive experience reasoning and adaptive reward shaping. The system categorizes reasoning trajectories into Good (high reward) and Bad (low reward) sets, generates textual experience summaries via LLM comparison every 5 training steps, and applies efficiency penalties only when F1≥0.8. Experience-augmented prompts combine instructions, experience summaries, and few-shot examples during training, with the policy converging to internalize this guidance.

## Key Results
- 39.6% reduction in search tool invocations while maintaining accuracy
- 48.6% decrease in inference time
- 21.2% reduction in token consumption
- Outperforms ReSearch, OTC-GRPO, and DeepRAG across all four evaluation benchmarks

## Why This Works (Mechanism)

### Mechanism 1: Contrastive Experiential Memory Transforms Implicit Success Patterns into Explicit Guidance
By comparing high-reward and low-reward reasoning trajectories, the model distills interpretable textual guidelines that explicitly describe effective reasoning patterns, reducing over-reliance on search tools.

### Mechanism 2: Adaptive Reward Shaping Penalizes Redundancy Only After Accuracy is Achieved
Penalizing excessive tool calls only when F1≥0.8 (correct-answer scenarios) avoids sacrificing accuracy for efficiency, enabling the model to explore freely when wrong.

### Mechanism 3: Experience-Augmented Prompting Accelerates Policy Convergence During Rollout
Injecting accumulated experiential memory and few-shot examples into prompts during RL rollouts provides explicit guidance, reducing exploration variance.

## Foundational Learning

- **Policy Gradient Methods (PPO, GRPO)**: Required for understanding GRPO training and advantage function computation. *Quick check*: If all sampled trajectories for a query have negative advantages, what happens to the probability of actions taken?
- **DeepSearch/RAG Iterative Loops**: Needed to understand the Reason-Decision-Search cycle and tool-call efficiency. *Quick check*: In a 2-hop question like "Who founded the company that makes the iPhone?", what's the minimum tool calls needed if the model knows "Apple" parametrically but not "Steve Jobs"?
- **Reward Shaping for Multi-Objective RL**: Critical for understanding how scalarization weights affect optimization. *Quick check*: If W_β is set too high relative to W_α, what failure mode emerges in model behavior?

## Architecture Onboarding

- **Component map:**
  Query → Policy π_θ → [Reasoning | Decision | Search] loop → Trajectory τ
                        ↓
              Trajectory Collection (N rollouts per query)
                        ↓
              Reward Calculation (F1 + Format + Tool)
                        ↓
         ┌──────────────┴──────────────┐
         ↓                              ↓
  Good(τ)/Bad(τ)                  Adaptive Reward
  Categorization                  Shaping (Eq. 16)
         ↓                              ↓
  Experience Generation           Advantage Computation
  (LLM contrastive)
         ↓                              ↓
         └──────────────┬──────────────┘
                        ↓
              Prompt_aug Construction
              (Instructions + Experience + Few-shot + Query)
                        ↓
              GRPO Policy Update

- **Critical path:** Trajectory quality → Experience quality → Policy convergence
- **Design tradeoffs:** Experience update frequency (every 5 steps), F1 threshold θ_t=0.8 for Tool penalty, inference with/without experience
- **Failure signatures:** Tool calls drop but F1 plummets (overly aggressive reward), experience texts repeat generic advice (insufficient trajectory distinction), inference time doesn't improve (experience still injected post-convergence)
- **First 3 experiments:**
  1. Baseline replication on Musique subset with default hyperparameters
  2. Ablation sweep: disable experiential memory and measure F1 drop (~7% degradation)
  3. Cross-domain probe: train on Musique+2WikiMultihopQA, test on NQ and HotpotQA

## Open Questions the Paper Calls Out
1. Can the adaptive reward shaping mechanism effectively generalize to domains beyond multi-hop QA, such as code synthesis or strategic planning?
2. Can the computational overhead of the RL training phase be reduced to improve scalability?
3. How sensitive is the adaptive reward function to the specific F1 threshold (θ_t) used to identify correct-answer scenarios?

## Limitations
- Experiential memory mechanism depends on LLM's ability to generate actionable reasoning guidelines from trajectory comparisons
- Adaptive reward shaping assumes correct-answer trajectories exist for each query to establish minimum tool baselines
- Computational overhead of RL training phase may present challenges for broader scalability

## Confidence
- **High confidence**: Core experimental results showing 39.6% reduction in tool calls while maintaining accuracy
- **Medium confidence**: Adaptive reward shaping formulation and threshold choices
- **Low confidence**: Experiential memory's isolated contribution to efficiency gains

## Next Checks
1. Manually inspect experience summaries generated for training queries versus their application to validation queries to verify generalization
2. Systematically vary θ_t (0.6→0.95) and measure the trade-off between tool reduction and F1 stability
3. Run controlled experiments where experiences are included/excluded during inference to directly measure their marginal contribution