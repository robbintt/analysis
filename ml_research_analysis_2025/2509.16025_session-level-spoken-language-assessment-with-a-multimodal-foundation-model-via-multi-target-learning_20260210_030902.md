---
ver: rpa2
title: Session-Level Spoken Language Assessment with a Multimodal Foundation Model
  via Multi-Target Learning
arxiv_id: '2509.16025'
source_url: https://arxiv.org/abs/2509.16025
tags:
- language
- speech
- learning
- overall
- prior
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multimodal foundation model for session-level
  spoken language assessment (SLA) using multi-target learning. The model jointly
  predicts holistic and part-level proficiency scores from the entire L2 speech session,
  integrating acoustic and semantic evidence in a single pass.
---

# Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning

## Quick Facts
- arXiv ID: 2509.16025
- Source URL: https://arxiv.org/abs/2509.16025
- Reference count: 0
- Primary result: State-of-the-art RMSE 0.360 on Speak & Improve benchmark using session-level MLLM with multi-target learning and acoustic prior

## Executive Summary
This paper presents a multimodal foundation model for session-level spoken language assessment (SLA) using multi-target learning. The model jointly predicts holistic and part-level proficiency scores from the entire L2 speech session, integrating acoustic and semantic evidence in a single pass. A Whisper-based Acoustic Proficiency Prior is introduced to enhance calibration with non-lexical cues like prosody and fluency. Experiments on the Speak & Improve benchmark show the proposed approach achieves state-of-the-art performance (RMSE 0.360), outperforming ensemble and cascaded baselines, especially on long and multi-response parts. The method improves discourse-level reasoning, reduces error propagation, and offers a compact, deployable solution for CALL applications.

## Method Summary
The method uses Phi-4-multimodal-instruct with speech adapter as backbone, processing entire L2 speech sessions as dialog-style sequences interleaving text prompts with audio placeholders. A frozen Whisper-large-v3 encoder extracts acoustic representations, mean-pooled and projected through a 2-layer MLP to produce probability logits, then mapped to a token embedding prepended as Acoustic Proficiency Prior (APP). The model jointly predicts five scores (P1, P3, P4, P5, overall) via a linear regression head on the final hidden state. LoRA adapters are applied to attention and MLP projections for parameter-efficient fine-tuning, with the backbone frozen. Training uses MSE loss with AdamW optimizer, cosine decay, and gradient accumulation for batch size 1 over 3 epochs.

## Key Results
- Achieves RMSE 0.360 on Speak & Improve benchmark, outperforming ensemble and cascaded baselines
- Multi-target learning reduces per-part RMSE across all parts compared to single-target training
- APP particularly improves performance on long-audio parts (P3/P4) where prosodic patterns are more informative

## Why This Works (Mechanism)

### Mechanism 1: Session-Level Context Aggregation
Processing all responses within a session in a single forward pass enables discourse-level reasoning that utterance-level models cannot achieve. The unified model formats sessions as dialogue-style sequences with audio placeholders, allowing the attention mechanism to attend across utterances and parts. This internalizes cross-response evidence rather than relying on post-hoc score averaging. Core assumption: Proficiency cues are distributed across multiple responses; a speaker's ability manifests more reliably when the model can reference the full session context. Evidence anchors: [abstract] "By coherently processing all responses within a session, the model captures discourse-level evidence and avoids error propagation from cascaded pipelines." [Section 2.1] "MTL reduces error by more than 12% relative...improvements are consistent across all parts, with clear gains on the multi-response parts (P1/P5). We attribute these gains to session-level aggregation." Break condition: If sessions exceed the model's context window (128k tokens for Phi-4) or if individual responses contain no cross-referential cues, session-level gains diminish to utterance-level baseline.

### Mechanism 2: Multi-Target Learning for Score Regularization
Jointly predicting part-level and overall scores in a single head produces mutually reinforcing gradients that improve generalization over single-target training. A five-output regression head shares the backbone representation across all targets. The shared hidden state must encode features useful for all parts simultaneously, acting as an implicit regularizer. Core assumption: Part-level proficiency traits share underlying latent factors (e.g., grammar fluency affects all parts); joint learning exploits this shared structure. Evidence anchors: [abstract] "multi-target learning to jointly predict holistic and part-specific proficiency scores" [Table 2] Phi-4-MTL reduces per-part RMSE across all parts compared to Phi-4-CTG (single-target), with largest gains on P1 (0.556→0.494) and P5 (0.543→0.455). Break condition: If part-level scores are highly decorrelated or annotated with inconsistent standards, shared representation may introduce interference rather than synergy.

### Mechanism 3: Acoustic Proficiency Prior (APP) for Prosodic Calibration
Injecting a frozen Whisper-derived acoustic prior as a prefix token stabilizes delivery-related scoring (fluency, pausing) that the MLLM may underweight. Whisper encoder's last hidden states are mean-pooled, passed through a lightweight MLP to produce probability logits, then projected to a token embedding prepended to the input sequence. This provides an explicit acoustic cue without handcrafted features. Core assumption: The Whisper encoder encodes prosodic and disfluency cues relevant to proficiency assessment; the MLLM alone under-attends to non-lexical delivery features. Evidence anchors: [abstract] "frozen Whisper-based acoustic prior for prosodic calibration" [Section 2.2] "benefit is particularly evident in the long-audio parts (P3/P4), where prosodic and pausing patterns are more informative" [Figure 3] High correlation between APP(Whisper) and Phi-4 predictions, suggesting APP acts as calibrator rather than conflicting signal. Break condition: If the Whisper encoder is not sufficiently pre-trained on L2 speech patterns, or if the projection MLP is undertrained, the prior may introduce noise rather than signal.

## Foundational Learning

- **Concept: Multi-Target Learning (MTL)**
  - Why needed here: The model must predict five correlated scores simultaneously; understanding shared vs. task-specific representations is essential for debugging underperforming targets.
  - Quick check question: Can you explain why a shared backbone with multiple heads can outperform separate models for correlated targets?

- **Concept: Parameter-Efficient Fine-Tuning (LoRA)**
  - Why needed here: Full fine-tuning of a 128k-context MLLM on long audio sessions is impractical; LoRA enables adaptation with minimal trainable parameters.
  - Quick check question: What components of the model remain frozen during training, and which are optimized?

- **Concept: Cross-Modal Attention in MLLMs**
  - Why needed here: Audio placeholders and text instructions share a unified attention space; understanding how the model attends across modalities helps diagnose whether prosodic cues are being used.
  - Quick check question: How does interleaving audio tokens with text tokens differ from late fusion of separately encoded modalities?

## Architecture Onboarding

- **Component map**: Audio segments + text prompts → formatted as interleaved sequence → Whisper encoder (frozen) → mean-pooled hidden states → 2-layer MLP → APP token → prepended to sequence → Phi-4 backbone with LoRA adapters → final hidden state → 5-output regression head

- **Critical path**: 1. Session audio segments (16 kHz) + text prompts formatted as interleaved sequence. 2. Whisper encoder processes each audio segment independently (frozen, no gradients). 3. APP token prepended to sequence. 4. Phi-4 backbone attends over full sequence (128k context). 5. Final hidden state → regression head → 5 scores. 6. MSE loss over available targets (handles partial annotations).

- **Design tradeoffs**: Unified vs. Ensemble: Unified model reduces deployment complexity and enables cross-part reasoning, but requires long-context capability and may lose part-specialized feature engineering. Frozen Whisper vs. Fine-tuned: Freezing prevents overfitting and reduces memory, but may limit adaptation to L2-specific acoustic patterns. APP prefix vs. Late fusion: Prefix injection allows the model to learn how to use the prior; late fusion requires manual weighting.

- **Failure signatures**: RMSE degradation on long parts (P3/P4): Suggests APP is not being integrated; check projector training or Whisper feature extraction. Large variance across parts: May indicate multi-target interference; consider gradient balancing or target masking. Context overflow errors: Session exceeds 128k tokens; requires chunking or response truncation.

- **First 3 experiments**: 1. Ablate APP: Train Phi-4-MTL without the Whisper-derived prior; compare RMSE on P3/P4 to quantify acoustic prior contribution. 2. Single-target baseline: Train separate models for each part using same backbone; compare to MTL to isolate multi-target regularization effect. 3. Alternative prior sources: Replace Whisper with wav2vec 2.0 encoder; assess whether alternative self-supervised representations provide comparable acoustic cues.

## Open Questions the Paper Calls Out
- Does the session-level MLLM generalize to assessment tasks with different structures or response types without extensive retraining? (Basis: explicit conclusion stating future work will extend framework toward "cross-task generalization")
- Does the unified architecture exhibit fairness across diverse L1 backgrounds or demographic groups compared to feature-engineered baselines? (Basis: explicit conclusion identifying "fairness considerations" as future work)
- Would fine-tuning the Whisper encoder in the Acoustic Proficiency Prior (APP) yield better calibration for L2-specific prosodic features than the frozen approach? (Basis: inferred from methodology specifying "frozen Whisper model-based speech prior")
- Can the model generate explanatory feedback for the predicted scores without degrading regression accuracy? (Basis: inferred from CALL framing, yet model trained solely for MSE minimization)

## Limitations
- Single benchmark validation on Speak & Improve with no external cross-corpus testing raises generalizability concerns
- Whisper-based Acoustic Proficiency Prior assumes frozen general-purpose ASR representations capture L2-specific prosodic cues without verification
- 128k context window may not accommodate longest sessions, potentially degrading discourse-level gains through truncation
- Multi-target learning assumes part-level scores are sufficiently correlated; inconsistent annotations may cause gradient interference

## Confidence
- **High Confidence**: RMSE 0.360 result on Speak & Improve is directly reported and verifiable; unified session-level architecture and multi-target learning framework are clearly described
- **Medium Confidence**: Claims of outperforming ensemble and cascaded baselines are supported within S&I benchmark but lack cross-corpus validation; APP integration mechanism is plausible but not exhaustively tested
- **Low Confidence**: Assertion that session-level aggregation uniquely enables discourse-level reasoning is not empirically isolated from other factors; exact benefit of Whisper prior over alternative acoustic encoders is not demonstrated

## Next Checks
1. **Cross-Corpus Generalization**: Evaluate the trained model on an independent L2 speech dataset (e.g., L2-ARCTIC or TOEFL iBT) to assess robustness beyond the Speak & Improve benchmark
2. **APP Ablation and Alternatives**: Systematically compare RMSE on long parts (P3/P4) when removing the APP, and when replacing Whisper with alternative encoders (e.g., wav2vec 2.0 or HuBERT) to quantify the prior's contribution and specificity
3. **Session-Length Stress Test**: Construct test sessions that exceed 128k tokens and measure performance degradation; experiment with chunking strategies to identify the context window's practical limits