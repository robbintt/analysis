---
ver: rpa2
title: Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation
arxiv_id: '2510.06131'
source_url: https://arxiv.org/abs/2510.06131
tags:
- medical
- generation
- image
- diffusion
- medim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces MeDiM, the first medical discrete diffusion
  model that learns shared distributions across different medical modalities without
  requiring modality-specific components. The core method uses a multimodal large
  language model (MLLM) as the diffusion backbone, with two key adaptations: removing
  the causal attention mask to enable bidirectional context and injecting continuous
  timestep embeddings for diffusion awareness.'
---

# Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation

## Quick Facts
- **arXiv ID:** 2510.06131
- **Source URL:** https://arxiv.org/abs/2510.06131
- **Authors:** Jiawei Mao; Yuhan Wang; Lifeng Chen; Can Zhao; Yucheng Tang; Dong Yang; Liangqiong Qu; Daguang Xu; Yuyin Zhou
- **Reference count:** 19
- **Primary result:** First unified medical discrete diffusion model using MLLM backbone without modality-specific components, achieving state-of-the-art image and report generation across multiple medical datasets

## Executive Summary
This paper introduces MeDiM, the first medical discrete diffusion model that leverages a multimodal large language model (MLLM) as the diffusion backbone for unified medical multimodal generation. The model demonstrates state-of-the-art performance in medical image generation (FID 16.60 on MIMIC-CXR, 24.19 on PathGen), report generation (METEOR 0.2650 on MIMIC-CXR, 0.2580 on PathGen), and jointly generates high-quality image-report pairs that improve downstream VLM performance. MeDiM achieves this without requiring modality-specific components, representing a significant advance in medical multimodal generation.

## Method Summary
MeDiM adapts a pre-trained MLLM (Liquid) for discrete diffusion by removing the causal attention mask to enable bidirectional context and injecting continuous timestep embeddings via Adaptive Layer Normalization (AdaLN). The model tokenizes medical images using a VQGAN encoder and reports with LLaMA tokenizer, then concatenates them into a shared sequence. During forward diffusion, tokens are progressively replaced with [MASK] according to an absorbing transition matrix, while the MLLM backbone predicts original tokens during reverse diffusion. The model is trained for 1M steps on 8×A100 GPUs using a weighted negative log-likelihood objective with Warmup Cosine Annealing with Restarts learning rate schedule.

## Key Results
- State-of-the-art medical image generation with FID 16.60 on MIMIC-CXR and 24.19 on PathGen
- Superior report generation with METEOR 0.2650 on MIMIC-CXR and 0.2580 on PathGen
- Joint image-report pairs improve downstream VLM performance by +6.43% BLEU-1, +18.57% BLEU-2, +31.58% BLEU-3, +4.80% METEOR on PathGen
- Unified framework achieves better cross-modal consistency than specialized single-task models

## Why This Works (Mechanism)

### Mechanism 1
Removing the causal attention mask from pre-trained MLLMs enables the bidirectional context required for discrete diffusion denoising across medical image and report tokens. Standard MLLMs use autoregressive (left-to-right) attention, which prevents tokens from attending to future positions. The paper removes this constraint, allowing image and text tokens to mutually attend across the entire sequence during denoising. Evidence shows that with causal mask, mB-1 drops to 0.152 and mFID increases to 143.72, confirming the necessity of bidirectional attention for joint image-text coherence.

### Mechanism 2
Injecting continuous timestep embeddings makes the MLLM backbone aware of the diffusion process state, enabling appropriate denoising strategies at each step. Each diffusion timestep is mapped to a continuous embedding vector, injected via Adaptive Layer Normalization (AdaLN). This conditions the backbone on how much noise is present. Without timestep embeddings, mFID degrades from 20.40 to 40.03, demonstrating that the MLLM cannot infer diffusion stage from token corruption alone and needs explicit temporal signals.

### Mechanism 3
Pre-trained MLLM weights provide distribution-alignment priors that significantly improve medical multimodal generation compared to training from scratch or using non-MLLM backbones. The MLLM has already learned joint vision-language representations; fine-tuning for discrete diffusion leverages this prior rather than learning alignment from scratch. DiT backbone achieves mFID 63.22 versus MLLM backbone mFID 20.40, and without pretrained weights mFID reaches 68.27, confirming the substantial benefit of leveraging MLLM priors.

## Foundational Learning

- **Discrete Diffusion (Masked Diffusion):** MeDiM operates on discrete tokens (VQ-VAE image tokens + text tokens) rather than continuous latents. You must understand how forward diffusion replaces tokens with [MASK] and reverse diffusion predicts original tokens. Quick check: Can you explain why discrete diffusion uses an absorbing transition matrix rather than Gaussian noise?

- **VQ-VAE / VQ-GAN Tokenization:** Images must be converted to discrete tokens before being processed by the MLLM backbone. Understanding the codebook and quantization process is essential for debugging generation artifacts. Quick check: What happens to image quality if the VQ-VAE codebook is too small?

- **AdaLN (Adaptive Layer Normalization):** AdaLN dynamically generates normalization parameters from timestep embeddings, enabling context-aware feature modulation across modalities. Quick check: How does AdaLN differ from standard LayerNorm, and why does diffusion benefit from adaptive normalization?

## Architecture Onboarding

- **Component map:** VQGAN encoder → discrete image tokens → concatenated with text tokens → MLLM backbone (bidirectional attention + timestep conditioning) → reverse diffusion denoising → VQGAN decoder
- **Critical path:** Image-text pairs → tokenize both modalities → concatenate into shared sequence → forward diffusion (replace tokens with [MASK]) → reverse diffusion (MLLM predicts original tokens) → detokenize (VQGAN decoder reconstructs images)
- **Design tradeoffs:** MLLM vs DiT backbone (20.40 vs 63.22 FID), bidirectional vs causal attention (essential for joint generation; causal severely degrades quality to 143.72 FID), absorbing vs uniform transition (absorbing state used for better multimodal performance)
- **Failure signatures:** Blurred image boundaries + semantically incoherent reports (likely forgot to remove causal mask), poor convergence or unstable training (check timestep embedding injection or AdaLN configuration), domain shift artifacts (may need medical-specific fine-tuning)
- **First 3 experiments:** 1) Verify causal mask removal by running inference with and without causal mask on validation set; 2) Ablate timestep embeddings and measure FID/METEOR degradation; 3) Swap MLLM for DiT backbone on subset of data to quantify alignment prior benefit

## Open Questions the Paper Calls Out

- **Open Question 1:** Can incorporating medical domain-specific pre-trained weights into the MLLM backbone close the performance gap between MeDiM and specialized single-task expert models? The authors explicitly state in Section C (Limitation) that MeDiM "has not yet exceeded or matched expert medical models across all metrics" and identify the future direction of "incorporating MLLM backbones with medical domain background knowledge."

- **Open Question 2:** Does the 512x512 resolution and discrete VQ-VAE tokenization limit the model's ability to generate fine-grained pathological features required for high-precision diagnosis? The evaluation relies on FID and METEOR scores, which measure distribution and semantic similarity but do not specifically quantify preservation of fine-grained diagnostic details or small anomalies.

- **Open Question 3:** Does the removal of causal attention masks for bidirectional diffusion negatively impact the coherence of long-form text generation compared to standard autoregressive MLLMs? While the paper reports improved METEOR scores, it does not isolate whether the "non-causal" nature of the backbone introduces specific textual artifacts or hallucinations in longer reports compared to the original MLLM behavior.

## Limitations
- The model has not yet exceeded or matched expert medical models across all metrics, particularly in specialized diagnostic tasks
- The 512x512 resolution and discrete VQ-VAE tokenization may limit generation of fine-grained pathological features required for high-precision diagnosis
- Specific implementation details for timestep embeddings, transition matrices, and AdaLN integration are underspecified, potentially affecting reproducibility

## Confidence

- **High confidence:** Core claims about removing causal masking and adding timestep embeddings enabling MLLM-based discrete diffusion are strongly supported by ablation results
- **Medium confidence:** Claims about superior medical generation performance are supported but rely on comparisons with fewer ablations of competitor methods
- **Medium confidence:** Assertions that MLLM priors substantially outperform DiT backbones are well-supported by direct comparisons in ablation studies

## Next Checks

1. **Bidirectional attention validation:** Run a controlled experiment with and without causal mask removal on MIMIC-CXR validation set to confirm the dramatic FID/METEOR improvements reported

2. **Timestep embedding ablation:** Systematically disable timestep embeddings and measure the exact degradation in generation quality to verify the ~2× FID increase

3. **Backbone comparison validation:** Implement a DiT backbone baseline and compare against MLLM on a subset of PathGen data to confirm the ~3× FID improvement with MLLM