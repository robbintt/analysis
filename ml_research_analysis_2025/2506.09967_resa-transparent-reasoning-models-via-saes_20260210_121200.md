---
ver: rpa2
title: 'Resa: Transparent Reasoning Models via SAEs'
arxiv_id: '2506.09967'
source_url: https://arxiv.org/abs/2506.09967
tags:
- reasoning
- performance
- table
- saes
- resa
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Resa, a family of 1.5B reasoning models trained
  via a novel SAE-Tuning procedure that extracts and transfers reasoning abilities
  using sparse autoencoders. Unlike costly reinforcement learning or data-intensive
  supervised fine-tuning, SAE-Tuning uses only verified CoT-free question-answer pairs
  to elicit reasoning abilities.
---

# Resa: Transparent Reasoning Models via SAEs

## Quick Facts
- arXiv ID: 2506.09967
- Source URL: https://arxiv.org/abs/2506.09967
- Authors: Shangshang Wang; Julian Asilis; Ã–mer Faruk AkgÃ¼l; Enes Burak Bilgin; Ollie Liu; Deqing Fu; Willie Neiswanger
- Reference count: 40
- Key outcome: Resa achieves 47.28% average accuracy on reasoning benchmarks, matching RL-trained models while reducing training costs by over 2000x (to roughly $1) and time by over 450x (to around 20 minutes)

## Executive Summary
Resa introduces a novel SAE-Tuning method that extracts and transfers reasoning abilities using sparse autoencoders, achieving performance on par with RL-trained models at dramatically lower cost. The approach uses only verified CoT-free question-answer pairs to elicit reasoning abilities, avoiding the need for expensive reinforcement learning or data-intensive supervised fine-tuning. SAE-Tuning provides transparency by correlating reasoning feature distributions with model performance, offering a data-driven path to optimizing SAE-Tuning. The extracted reasoning abilities are both generalizable across datasets and modular as portable adapters, enabling reasoning transfer across models without retraining.

## Method Summary
Resa employs a two-stage SAE-Tuning procedure: First, train a sparse autoencoder (SAE) on source model activations at a hookpoint to extract reasoning-related features. Second, freeze the SAE and insert it into a target model, training LoRA adapters via KL divergence loss to minimize the difference between SAE-inserted and base outputs. This forces the target model to internalize SAE-captured reasoning patterns. The SAE is removed at test time, leaving an enhanced model with transferred reasoning abilities. The method uses CoT-free QA pairs with special Hayes/Croze tokens to trigger reasoning processes, achieving reasoning performance comparable to RL-trained models at dramatically lower cost.

## Key Results
- Achieves 47.28% average accuracy on AIME24/25, AMC23, MATH500, GPQA Diamond, and Minerva benchmarks
- Matches RL-trained models while reducing training costs by over 2000x (to roughly $1) and time by over 450x (to around 20 minutes)
- SAE-Tuning outperforms standard CoT-free SFT (39.00% vs 47.28%) while using the same data
- SAE features are transferable across models and can serve as modular reasoning adapters

## Why This Works (Mechanism)

### Mechanism 1: SAE as Reasoning Feature Compressor
SAEs extract a sparse, transferable dictionary of reasoning-related features from source model activations. The encoder projects dense activations into a higher-dimensional sparse space, then Top-k selection retains only the k most active features. This bottleneck forces the SAE to learn which latent dimensions correspond to reasoning processes rather than surface patterns. The decoder reconstructs activations from this sparse representation, with reconstruction loss ensuring fidelity. Core assumption: a subset of SAE latent dimensions corresponds to "reasoning features" that generalize beyond specific datasets.

### Mechanism 2: KL Divergence as Feature Alignment Signal
Minimizing KL divergence between target model outputs with and without SAE insertion forces the model to internalize SAE-captured reasoning patterns. During SAE-guided SFT, the frozen SAE is inserted at a hookpoint. The target model (via LoRA adapters) is trained to minimize KL(È³, y)â€”the divergence between the SAE-reconstructed output distribution and the original. This creates pressure for the target model's pre-hookpoint activations to become "compatible" with the SAE's learned feature space, effectively distilling reasoning structure into adapter weights. Core assumption: KL divergence provides a sufficient training signal for the model to learn reasoning, not just output matching.

### Mechanism 3: Layer-wise Feature Distribution as Performance Predictor
Reasoning features cluster at specific layer depths in a tri-modal distribution, and hookpoint selection within these clusters correlates with final performance. The prompt-only method identifies features that activate exclusively at reasoning tokens (ðŸŒ¢ and ). These features concentrate around layers ~5, ~15, and ~23. GMM analysis shows structural alignment between feature count distribution and reasoning performance distribution across hookpoints. Core assumption: features activated by reasoning tokens are causally related to reasoning ability, not merely correlated.

## Foundational Learning

- **Concept: Sparse Autoencoders (SAEs)**
  - Why needed here: SAEs are the core mechanism for decomposing activations into interpretable, sparse features. Understanding the encoder-decoder structure, Top-k sparsity, and reconstruction loss is essential.
  - Quick check question: Can you explain why Top-k sparsity (vs. L1 regularization) might produce more interpretable features?

- **Concept: Knowledge Distillation**
  - Why needed here: The paper frames SAE-Tuning as a distillation process where the SAE acts as a compressed teacher. Understanding teacher-student dynamics clarifies why KL divergence is the training objective.
  - Quick check question: How does distillation differ from direct fine-tuning on target outputs?

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed here: Only LoRA adapters are trained during SAE-guided SFT. Understanding rank constraints and where adapters are inserted (attention and MLP sublayers) is critical for implementation.
  - Quick check question: Why might low-rank adapters help preserve base model capabilities while learning new features?

## Architecture Onboarding

- **Component map:** Source Model â†’ SAE Training Module â†’ Trigger Dataset â†’ Target Model â†’ LoRA Adapters â†’ Elicitation Dataset
- **Critical path:** 1. Train SAE on source model activations (Stage I, ~$1 cost) 2. Freeze SAE, insert into target model at matching layer 3. Train LoRA adapters via KL divergence (Stage II) 4. Remove SAE at test time â†’ enhanced model retains reasoning
- **Design tradeoffs:** Source model selection: Lightly RL-trained source yields peak performance; base model enables fully end-to-end workflow with competitive results. SAE training mode: Trained-from-scratch eliminates dependency on pre-trained SAEs; fine-tuned may marginally improve if pre-trained SAE is high quality. Hookpoint selection: Middle layers (around 12) are default heuristic, but tri-modal analysis suggests layers ~5, ~15, ~23 as candidate clusters. Layer 18 achieved best results in ablation.
- **Failure signatures:** No performance gain over base model: Likely hookpoint mismatch or SAE not trained on reasoning-relevant activations. Verify feature counts via prompt-only method. Performance degrades: SAE reconstruction error too high; check dead feature threshold and Top-k value. Poor generalization across datasets: SAE overfitted to trigger dataset; increase data diversity or use pre-trained SAE mode.
- **First 3 experiments:** 1. Baseline replication: Train SAE on base R1-Distill with STILL dataset, insert at layer 12, train LoRA adapters. Compare to Table 4 Resa-STILL-v5 baseline (~48% avg). 2. Hookpoint sweep: Repeat experiment 1 across layers 2â€“27. Compute feature counts via prompt-only method. Validate tri-modal distribution correlation. 3. Ablation without SAE: Perform identical SFT on same CoT-free data without SAE insertion. Expect ~39% avg per Table 3 to confirm SAE necessity.

## Open Questions the Paper Calls Out

None

## Limitations

- Limited SAE Transferability Evidence: The paper claims SAE features are "transferable" across models, but the only empirical demonstration is performance on downstream datasets, not feature-level analysis.
- Prompt-Only Feature Extraction Reliability: The method for identifying reasoning features relies on correlating feature activation with specific Hayes/Croze tokens, which may not hold across different datasets or model versions.
- KL Divergence Training Signal Ambiguity: The paper doesn't clarify whether the model actually learns reasoning patterns or simply learns to match outputs through alternative pathways that don't generalize.

## Confidence

- **High Confidence**: Cost-effectiveness claims (2000x reduction), basic SAE-Tuning procedure implementation, and performance comparisons between SAE-Tuning and standard SFT are well-supported by experimental data.
- **Medium Confidence**: Claims about SAE features being "reasoning features" that transfer across models, and the correlation between layer-wise feature distribution and performance, are supported by internal analysis but lack external validation.
- **Low Confidence**: The fundamental claim that SAE-Tuning extracts and transfers reasoning abilities via sparse features is the core mechanism, but the evidence is largely correlational and doesn't demonstrate causality.

## Next Checks

1. **Feature Interpretability Analysis**: Apply feature attribution methods (e.g., integrated gradients, activation patching) to SAE features to determine if they correspond to human-understandable reasoning steps. Verify whether features that activate during reasoning problems actually encode logical relationships rather than dataset-specific patterns.

2. **Cross-Model Feature Consistency**: Test whether SAE features trained on one model architecture (e.g., R1-Distill) activate similarly on different architectures (e.g., Qwen2.5 or Llama) when processing the same reasoning problems. This would validate the "transferability" claim beyond performance metrics.

3. **Mechanism Isolation Experiment**: Perform ablation studies where SAE features are manually deactivated during inference to measure the specific contribution of each feature to reasoning performance. This would test whether the SAE is actually encoding reasoning capabilities versus other correlated factors.