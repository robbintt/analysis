---
ver: rpa2
title: 'When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability
  in Multi-Agent LLM Systems'
arxiv_id: '2601.16280'
source_url: https://arxiv.org/abs/2601.16280
tags:
- tool
- qwen2
- reliability
- error
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a systematic diagnostic framework for evaluating
  tool-use reliability in multi-agent LLM systems, addressing the critical gap in
  granular error characterization beyond aggregate success metrics. The authors propose
  a 12-category error taxonomy spanning tool initialization, parameter handling, execution,
  and result interpretation, applied to 1,980 deterministic test instances across
  both open-weight (Qwen2.5 series, Functionary) and proprietary models (GPT-4, Claude).
---

# When Agents Fail to Act: A Diagnostic Framework for Tool Invocation Reliability in Multi-Agent LLM Systems

## Quick Facts
- **arXiv ID:** 2601.16280
- **Source URL:** https://arxiv.org/abs/2601.16280
- **Reference count:** 16
- **Primary result:** 12-category error taxonomy reveals tool initialization failures as primary bottleneck; qwen2.5:32b achieves flawless 100% reliability matching GPT-4.1

## Executive Summary
This paper introduces a systematic diagnostic framework for evaluating tool-use reliability in multi-agent LLM systems, addressing the critical gap in granular error characterization beyond aggregate success metrics. The authors propose a 12-category error taxonomy spanning tool initialization, parameter handling, execution, and result interpretation, applied to 1,980 deterministic test instances across both open-weight (Qwen2.5 series, Functionary) and proprietary models (GPT-4, Claude). The framework reveals that tool initialization failures constitute the primary bottleneck for smaller models, with qwen2.5:32b achieving flawless 100% reliability matching GPT-4.1, while qwen2.5:14b represents the minimum viable production configuration at 96.6% success with 7.3s latency on RTX A6000. Hardware performance varies by 8.2× across platforms, demonstrating that mid-sized models on capable hardware offer optimal accuracy-efficiency trade-offs for resource-constrained deployments.

## Method Summary
The authors evaluate tool-use reliability in a three-agent invoice reconciliation system using 1,980 deterministic test instances. The architecture consists of an Email Agent (OCR tool), Data Engineering Agent (DB Query/Update tools), and Reconciliation Agent, orchestrated via LangGraph with 25-step recursion limits. Open-weight models are tested via Ollama v0.6.8 with Q4_K_M quantization, while proprietary models use API calls. The framework introduces a 12-category error taxonomy (4 error types × 3 tool categories) and measures success rate, execution time, process steps, and OCR F1 score across RTX A6000, RTX 4090, and Apple M3 Max hardware.

## Key Results
- Tool initialization failures dominate error distributions across all model scales, with smaller models requiring architectural augmentations
- qwen2.5:32b achieves flawless 100% reliability matching GPT-4.1, while qwen2.5:14b provides optimal 96.6% success at 7.3s latency on RTX A6000
- Hardware selection impacts planning efficiency through step reduction, not just raw inference speed (8.2× latency difference between platforms)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Model scale determines procedural reliability more than parameter precision in multi-agent tool invocation.
- **Mechanism:** Larger models (≥32B parameters) maintain stronger coupling between semantic understanding and action execution, reducing omission failures where agents produce natural-language responses instead of tool calls. Smaller models exhibit shallow procedural reasoning and weak context maintenance.
- **Core assumption:** The observed scale-reliability correlation generalizes beyond the invoice reconciliation domain tested.
- **Evidence anchors:**
  - [abstract] "procedural reliability, particularly tool initialization failures, constitutes the primary bottleneck for smaller models, while qwen2.5:32b achieves flawless performance matching GPT-4.1"
  - [section IV.C] "qwen2.5:3b: Dominated by omission failures (~89% of sampled errors), reflecting shallow procedural reasoning and weak context maintenance"
  - [corpus] ToolCritic (arXiv:2510.17052) identifies similar tool-use error patterns in dialogue systems, supporting error taxonomy generalization

### Mechanism 2
- **Claim:** Hardware selection impacts both latency and planning efficiency through step-count reduction, not just raw inference speed.
- **Mechanism:** Capable hardware enables larger models that complete tasks in fewer reasoning steps. The 8.2× latency difference between platforms compounds with planning efficiency—qwen2.5:14b requires 8.6 steps vs. qwen2.5:7b's 20.7 steps, avoiding recursion limit failures.
- **Core assumption:** Step reduction correlates with model capacity rather than hardware-specific optimization.
- **Evidence anchors:**
  - [section IV.A] "qwen2.5:14b executes in 7.3 seconds on RTX A6000 versus 60.0 seconds on M3 Max—an 8.2× difference"
  - [section IV.A] "qwen2.5:7b on M3 Max averages 85.9 seconds despite smaller size, requiring 20.7 steps per task versus 8.6 steps for the larger 14B model"
  - [corpus] Related work on edge deployment (referenced but not directly testing step-efficiency) supports hardware-accuracy trade-offs

### Mechanism 3
- **Claim:** Fine-grained error taxonomies reveal initialization failures as the dominant failure mode, which aggregate success metrics obscure.
- **Mechanism:** The 12-category taxonomy (4 error types × 3 tool categories) isolates procedural invocation failures from parameter and execution issues. This reveals that improving parameter handling or result interpretation won't address the primary bottleneck.
- **Core assumption:** Error distribution patterns persist across task variations within the tested domain.
- **Evidence anchors:**
  - [section IV.B] "Tool initialization failures (DB_UPDATE_TOOL_NOT_INITIALIZED and DB_QUERY_TOOL_NOT_INITIALIZED) dominate error distributions across all model scales"
  - [section III.C] "Not Initialized: Agent fails to properly invoke tool due to malformed calls, incorrect tool names, or invalid parameter structures"
  - [corpus] Jenius Agent (arXiv:2601.01857) notes similar limitations in "existing agent frameworks and benchmarks [that] provide limited visibility into execution-level behavior"

## Foundational Learning

- **Concept: Tool Invocation as Procedural Reasoning**
  - Why needed here: Understanding that tool failures stem from weak semantic-to-action coupling, not just syntax errors, informs model selection and orchestration design.
  - Quick check question: Can you distinguish between an agent that doesn't know to call a tool vs. one that calls it incorrectly?

- **Concept: Capacity Thresholds in Agentic Systems**
  - Why needed here: The paper identifies 14B as minimum viable and 32B as parity with proprietary models—critical for deployment planning.
  - Quick check question: What reliability target does your application require, and which model-hardware combination achieves it?

- **Concept: Cascading Failures in Multi-Agent Coordination**
  - Why needed here: Single-agent error taxonomies don't capture inter-agent communication failures and coordination breakdowns.
  - Quick check question: If one agent fails to invoke a tool, how does that propagate to downstream agents?

## Architecture Onboarding

- **Component map:** Email Agent → OCR Tool → Data Engineering Agent → DB Query Tool → DB Update Tool → Reconciliation Agent
- **Critical path:** Document input → OCR initialization → Database query → Database update → Reconciliation decision. Initialization failures at any tool invocation point cascade to task failure.
- **Design tradeoffs:**
  - 32B model + $10K hardware = flawless reliability, data sovereignty
  - 14B model + $5K hardware = 96.6% reliability, optimal SME deployment
  - Sub-14B models = require retry mechanisms, validation layers, acceptance of higher failure rates
- **Failure signatures:**
  - Omission: Agent outputs natural language instead of tool call (check for missing function invocations in logs)
  - Malformed: Incorrect tool names, invalid JSON, or hallucinated parameters (check schema validation errors)
  - Recursion limit: >25 steps indicates cyclical decision loops (check step counts for smaller models)
- **First 3 experiments:**
  1. Run 100 test instances with qwen2.5:14b on your target hardware; verify success rate ≥96% and identify dominant error categories using the taxonomy.
  2. Instrument logging for all 12 error categories; confirm initialization failures are primary (if not, your domain may differ from invoice reconciliation).
  3. Test retry logic on omission failures specifically; measure recovery rate to determine if architectural augmentation is required for sub-14B models.

## Open Questions the Paper Calls Out

- **Open Question 1:** Does the 12-category error taxonomy generalize effectively to complex domains beyond financial invoice reconciliation?
  - Basis in paper: [explicit] The conclusion identifies "cross-domain validation beyond finance" as a primary future direction.
  - Why unresolved: The current validation relies exclusively on a single use case (invoice reconciliation), leaving the taxonomy's applicability to other workflows unproven.
  - What evidence would resolve it: Application of the diagnostic framework to distinct domains (e.g., healthcare, software engineering) demonstrating consistent error categorization and bottleneck identification.

- **Open Question 2:** Can agent architectures utilize this diagnostic taxonomy to implement autonomous self-healing mechanisms?
  - Basis in paper: [explicit] The paper lists "self-healing agent architectures informed by our taxonomy" as a future research direction.
  - Why unresolved: The current framework is diagnostic (identifying failures) rather than remedial (fixing them), leaving the gap between error detection and autonomous recovery unbridged.
  - What evidence would resolve it: A system where agents detect specific errors (e.g., "Tool Not Initialized") via the taxonomy and successfully retry or alter their approach without human intervention.

- **Open Question 3:** Why does the qwen2.5:72b model underperform the smaller qwen2.5:32b model (95.1% vs. 100% success)?
  - Basis in paper: [inferred] The results section notes the larger 72B model exhibits lower success rates than the 32B model, suggesting "task-specific capacity limits" or diminishing returns, but provides no definitive causal explanation.
  - Why unresolved: The performance drop is counter-intuitive (larger models typically perform better) and suggests potential issues with quantization, instruction-following, or overfitting specific to this architecture.
  - What evidence would resolve it: Ablation studies testing the 72B model at different quantization levels or analyzing attention patterns during the specific failure modes.

## Limitations
- Framework validated on single invoice reconciliation domain with deterministic test instances, limiting generalizability to dynamic real-world scenarios
- Open-weight model performance depends heavily on quantization (Q4_K_M) and specific hardware configurations
- 12-category error taxonomy may not capture domain-specific failure modes in applications with different tool categories

## Confidence
- **High confidence** in 14B parameter minimum threshold and 32B parameter optimal performance for invoice reconciliation tasks, supported by 1,980 test instances
- **Medium confidence** in hardware-performance relationships (8.2× latency variation) as results are specific to tested model series and quantization approach
- **Medium confidence** in error taxonomy generalizability beyond invoice reconciliation, as related work identifies similar tool-use patterns in different domains

## Next Checks
1. Apply framework to 500 test instances in different multi-agent domain (e.g., customer service) to validate error taxonomy generalizability and minimum viable model thresholds
2. Test sub-14B models with architectural augmentations (retry logic, validation layers) on hardware-constrained platforms to quantify trade-off between model size and failure recovery mechanisms
3. Measure step efficiency and recursion limits for smaller models in your specific multi-agent orchestration framework to determine if 25-step recursion limit assumption holds for your use case