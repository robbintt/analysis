---
ver: rpa2
title: 'Squeeze10-LLM: Squeezing LLMs'' Weights by 10 Times via a Staged Mixed-Precision
  Quantization Method'
arxiv_id: '2507.18073'
source_url: https://arxiv.org/abs/2507.18073
tags:
- quantization
- should
- activation
- weights
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Squeeze10-LLM, a staged mixed-precision post-training\
  \ quantization framework that achieves 10\xD7 compression of 16-bit LLMs' weights\
  \ to an average of 1.6 bits per weight. The method combines 4-bit uniform quantization\
  \ with selective binarization, using two key innovations: Post-Binarization Activation\
  \ Robustness (PBAR) for refined weight salience estimation and Full Information\
  \ Activation Supervision (FIAS) to prevent cumulative quantization errors."
---

# Squeeze10-LLM: Squeezing LLMs' Weights by 10 Times via a Staged Mixed-Precision Quantization Method

## Quick Facts
- arXiv ID: 2507.18073
- Source URL: https://arxiv.org/abs/2507.18073
- Reference count: 40
- Primary result: Achieves 10× compression of 16-bit LLMs to 1.6 bits per weight, improving zero-shot accuracy from 43% to 56% on average across six tasks

## Executive Summary
Squeeze10-LLM introduces a staged mixed-precision quantization framework that compresses large language models by 10× to an average of 1.6 bits per weight. The method combines 4-bit uniform quantization with selective binarization, using two key innovations: Post-Binarization Activation Robustness (PBAR) for refined weight salience estimation and Full Information Activation Supervision (FIAS) to prevent cumulative quantization errors. Experiments on LLaMA and LLaMA2 models demonstrate state-of-the-art performance for sub-2bit weight-only quantization while maintaining comparable perplexity to full-precision models.

## Method Summary
Squeeze10-LLM is a post-training quantization method that first uniformly quantizes all weights to 4-bit, then uses PBAR to identify the most salient weights for preservation at higher precision. The salience metric combines traditional Hessian-based importance with a novel post-binarization activation range measurement. The top 20% most salient weights are kept at 4-bit while the remaining 80% are binarized to 1-bit. Throughout the process, FIAS preserves original pretrained activations rather than using updated quantized activations, preventing error propagation across layers.

## Key Results
- Achieves 10× compression (1.6 bits per weight) on LLaMA2-7B, 13B, and 70B models
- Improves zero-shot classification accuracy from 43% to 56% on average across six tasks
- Maintains WikiText2 perplexity of 9.96, comparable to full-precision models
- Outperforms GPTQ-2bit and PB-LLM-1.6bit baselines by significant margins
- Ablation studies show PBAR and FIAS each contribute 2-3% accuracy improvements

## Why This Works (Mechanism)

### Mechanism 1: PBAR
Traditional Hessian-based saliency captures weight magnitudes but misses how quantization affects activations. PBAR combines Hessian-based weight importance (V) with a measurement of how much each weight's binarization changes the activation range per output channel (B). The combined metric M = V + λB identifies weights whose binarization significantly expands activation ranges, upgrading their importance ranking.

### Mechanism 2: FIAS
Conventional PTQ uses quantized layer outputs as inputs for subsequent layer quantization, causing activation distribution shifts to compound. FIAS preserves original FP16 activations from the pretrained model and uses them consistently for all layers, preventing the "drift" that misleads weight salience estimation in deep layers.

### Mechanism 3: Staged Quantization
Rather than jumping directly from FP16 to 1-bit, the method first quantizes all weights to 4-bit. This intermediate stage reduces the precision gap and preserves distribution characteristics. Then, PBAR identifies the 20% most salient weights to retain at 4-bit while binarizing the remaining 80%.

## Foundational Learning

- Concept: **Post-Training Quantization (PTQ) for LLMs**
  - Why needed: Squeeze10-LLM is fundamentally a weight-only PTQ method; understanding baseline PTQ challenges (activation outliers, Hessian-based error compensation, layer-wise vs. global approaches) is prerequisite to appreciating the innovations.
  - Quick check: Can you explain why PTQ for LLMs differs from CNN quantization, and what makes ultra-low-bit PTQ (≤2 bits) particularly challenging?

- Concept: **Hessian-Based Sensitivity Analysis**
  - Why needed: PBAR builds directly on Hessian-based weight salience (Eq. 5-6); understanding how the Hessian matrix captures second-order information about quantization loss is essential for implementing and debugging the V component.
  - Quick check: How does the Hessian matrix H = 2XX^T relate input activations to weight importance, and what are its limitations for identifying salient weights?

- Concept: **Mixed-Precision Quantization Allocation**
  - Why needed: The core method allocates different bit-widths (4-bit vs. 1-bit) based on salience; understanding tradeoffs between compression ratio, accuracy, and storage overhead (index bits) is critical for practical deployment.
  - Quick check: What factors determine the optimal proportion of high-bit vs. low-bit weights, and how does index storage overhead affect the effective compression ratio?

## Architecture Onboarding

- Component map:
```
Input: FP16 Model + Calibration Data
         ↓
Stage 1: 4-bit Uniform Quantization (All weights → 4-bit intermediate)
         ↓
Stage 2: PBAR Salience Computation (V + λB metric)
         ↓
Stage 3: Mixed-Precision Assignment (Top 20% → 4-bit, Bottom 80% → 1-bit)
         ↓
FIAS Supervision (Throughout: original FP16 activations)
         ↓
Output: 1.6-bit Mixed-Precision Model
```

- Critical path:
  1. **PBAR metric implementation**: Computing B requires simulating per-weight binarization and measuring activation range changes—this is the most computationally sensitive step.
  2. **Salient weight threshold**: The 20% cutoff directly controls the compression-accuracy tradeoff; verify against Table 4 trends.
  3. **FIAS activation management**: Must correctly route and store original activations for all layers during quantization.

- Design tradeoffs:
  - **Salient weight proportion**: Table 4 shows 10% → 67.98% avg accuracy, 20% → 70.85%, 60% → 72.03%; diminishing returns above 30%.
  - **Intermediate bit-width**: Table 3 confirms 4-bit is optimal; 2-bit/3-bit underperform, 5-8 bit surprisingly worse.
  - **λ hyperparameter**: Appendix A.3 identifies λ=3e-4 as optimal for LLaMA2-7B; requires tuning per model family.
  - **Index overhead**: Eq. 9 accounts for 1 additional bit per weight for salient weight indexing.

- Failure signatures:
  - **Perplexity explosion (>100 on WikiText2)**: Likely FIAS not implemented correctly; activations are shifting across layers.
  - **Zero-shot accuracy collapse (<40% avg)**: Check PBAR computation; may be using wrong activation source or λ misconfigured.
  - **Layer-wise inconsistency**: If later layers show much higher quantization error than early layers, activation supervision is degrading.

- First 3 experiments:
  1. **Reproduce LLaMA2-7B baseline**: Quantize with Squeeze10-LLM, verify WikiText2 perplexity ≈9.96 and avg zero-shot accuracy ≈56% (Table 1). Compare against GPTQ-2bit and PB-LLM-1.6bit baselines.
  2. **Ablate PBAR and FIAS**: Run four variants (full, -PBAR, -FIAS, -both) on WinoGrande; expect drops per Table 5 (0.23%, 2.37%, 3.31% respectively).
  3. **Intermediate bit-width sweep**: Test 2-bit through 8-bit intermediate precision on LLaMA2-7B; confirm 4-bit achieves highest accuracy on BoolQ/HellaSwag/PIQA per Table 3.

## Open Questions the Paper Calls Out

### Open Question 1
Does Squeeze10-LLM generalize to architectures beyond the LLaMA family (e.g., encoder-decoder models, MoE architectures)?
Basis: The authors evaluate exclusively on LLaMA and LLaMA2 models (7B-70B), with no discussion of other architectures.
Why unresolved: Different architectures have distinct weight distributions, activation patterns, and attention mechanisms that may interact differently with PBAR and FIAS.
What evidence would resolve it: Experiments on models such as T5, Mistral, or DeepSeek demonstrating comparable 1.6-bit quantization performance.

### Open Question 2
Why does 4-bit intermediate quantization outperform 5-8 bit settings in the staged framework?
Basis: "This result is somewhat counterintuitive... We speculate that 4-bit serves as an effective intermediate representation," but no rigorous explanation is provided.
Why unresolved: Higher bit-widths theoretically preserve more information, yet experiments show performance degradation beyond 4-bit.
What evidence would resolve it: Systematic analysis of activation distribution alignment and gradient flow across different intermediate bit-widths, possibly with theoretical justification.

### Open Question 3
What are the actual inference speedup gains on real hardware with dedicated 1-bit/4-bit kernels?
Basis: The paper claims "accelerate inference" but only reports perplexity and accuracy; no latency or throughput measurements are provided.
Why unresolved: Mixed-precision weights (1-bit + 4-bit) require specialized dequantization and may not achieve proportional speedup without hardware support.
What evidence would resolve it: End-to-end latency benchmarks on GPU/CPU with optimized kernels for the 1.6-bit representation.

### Open Question 4
Is the fixed 80% binarization / 20% 4-bit ratio optimal across different model scales and tasks?
Basis: Table 4 shows varying performance with different ratios, but the 20% choice appears heuristically selected for the 1.6-bit target rather than systematically optimized.
Why unresolved: The optimal salient weight proportion likely depends on model capacity, task complexity, and layer depth.
What evidence would resolve it: Layer-wise or task-adaptive ratio optimization studies, potentially using learned allocation strategies.

## Limitations

- **Calibration Data Dependence**: PBAR metric's effectiveness relies heavily on representative calibration data that captures the full activation distribution; dataset size and characteristics are not specified.
- **Model Architecture Constraints**: The staged 4-bit→1-bit approach may not generalize optimally to non-transformer architectures or models with different activation distributions.
- **Per-Precision Overhead**: While index storage overhead is acknowledged, the paper doesn't quantify the practical impact on storage savings when deploying on resource-constrained hardware.

## Confidence

- **High Confidence**: The core PBAR and FIAS mechanisms are well-defined and supported by ablation studies (Table 5). The staged quantization approach (4-bit intermediate) is empirically validated against alternatives (Table 3).
- **Medium Confidence**: The specific 20% salient weight threshold is justified empirically but may not be optimal across all model scales. The λ=3e-4 value is shown to work for LLaMA2-7B but represents a hyperparameter that requires tuning.
- **Low Confidence**: The claim about 4-bit being optimal for the intermediate stage is based on limited comparison (2-bit through 8-bit). The paper doesn't explore whether task-specific tuning of intermediate precision could yield better results.

## Next Checks

1. **Cross-Model Generalization**: Apply Squeeze10-LLM to a different LLM family (e.g., OPT or Falcon) with the same λ=3e-4 and 20% threshold. Verify if perplexity and accuracy degrade significantly, indicating need for architecture-specific tuning.

2. **Calibration Data Sensitivity**: Run PBAR with 10x, 100x, and 1000x calibration data points on LLaMA2-7B. Measure impact on salient weight ranking stability and final accuracy to quantify calibration data requirements.

3. **Ablation on Intermediate Precision**: Systematically test 2-bit, 3-bit, and 5-bit intermediate stages on LLaMA2-13B (different scale than 7B). Confirm whether 4-bit remains optimal across model sizes or if the intermediate precision should scale with model capacity.