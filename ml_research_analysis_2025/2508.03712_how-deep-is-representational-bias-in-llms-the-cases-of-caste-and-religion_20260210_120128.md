---
ver: rpa2
title: How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion
arxiv_id: '2508.03712'
source_url: https://arxiv.org/abs/2508.03712
tags:
- caste
- stories
- bias
- diversity
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study quantifies deep representational bias in GPT-4 Turbo
  regarding caste and religion in India. Through 7,200 generated stories about life
  rituals, it finds that the model severely underrepresents minoritized castes and
  religions compared to actual population distributions.
---

# How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion

## Quick Facts
- **arXiv ID:** 2508.03712
- **Source URL:** https://arxiv.org/abs/2508.03712
- **Reference count:** 12
- **Primary result:** GPT-4 Turbo severely underrepresents minoritized castes and religions in Indian life ritual stories, with bias exceeding training data distributions and resisting prompt-based mitigation.

## Executive Summary
This study quantifies deep representational bias in GPT-4 Turbo regarding caste and religion in India through 7,200 generated stories about life rituals. The model consistently underrepresents minoritized castes (SC, ST, OBC) and overrepresents General Castes and Hindus compared to actual population distributions, sometimes exceeding real prevalence by over 50 percentage points. While prompt-based nudges to improve diversity show some effects, they are limited and inconsistent, particularly failing to dislodge Hindu majority dominance. The results suggest that bias is deeply embedded in LLMs and that addressing it requires more fundamental changes in model development beyond diversifying training data or prompt engineering.

## Method Summary
The study audited GPT-4 Turbo using 24 prompt combinations across 2 ritual types (Birth, Wedding, Death), 4 states (UP, Tamil Nadu, Odisha, Rajasthan), and 3 prompt conditions (Baseline, Implicit Diversity, Explicit Diversity), generating 100 stories per combination for a total of 7,200 stories. Baseline prompts reset context after each story, while diversity prompts maintained cumulative context. Stories were manually annotated for caste and religion, then compared against 2011 Census data using chi-squared goodness-of-fit tests. The study measured both over/underrepresentation and qualitative patterns like hallucination and stereotype flattening.

## Key Results
- Minoritized castes (SC, ST, OBC) are consistently underrepresented while General Castes are overrepresented, with some groups exceeding their actual prevalence by over 50 percentage points
- Religious bias strongly favors Hindus across all prompt conditions, with Muslims and other religions severely underrepresented
- Prompt-based nudges show limited and inconsistent efficacy, with diversity prompts sometimes causing over-correction or superficial changes rather than genuine demographic shifts
- When forced to represent minorities, the model often hallucinates details or produces generic stereotypes rather than accurate cultural specifics

## Why This Works (Mechanism)

### Mechanism 1: Statistical Likelihood Amplification ("Winner-Take-All")
The model amplifies majority identities beyond their statistical prevalence in training data due to probability maximization during decoding. Transformers predict the most likely next token, and if cultural dominance creates higher prior probability for majority groups, the model selects these identities disproportionately. This suggests bias is an emergent property of the likelihood maximization algorithm rather than solely missing data.

### Mechanism 2: Ineffective Contextual Nudging
Incremental prompt engineering fails to dislodge deep representational hierarchies. While prompts asking for "another" story utilize conversation history, the model interprets "different" superficially rather than shifting demographic identity distribution, particularly for religion. The model's concept of "a story" is rigidly anchored to dominant cultural defaults that resist dislodgement without explicit negative constraints.

### Mechanism 3: Diversity via Hallucination and Flattening
When forced to represent minority groups, the model hallucinates details or "flattens" diverse identities into generic stereotypes rather than retrieving accurate cultural specifics. To satisfy "diversity" requests, the model samples from lower-probability regions of its latent space, generating plausible-sounding but factually incorrect content when knowledge of these regions is sparse.

## Foundational Learning

- **Representational Harm (Crawford's Classification):** Understanding the distinction between allocative bias (who gets a loan) and representational bias (who exists in the story). The paper focuses on non-recognition and under-representation rather than resource allocation.
  - **Quick check:** Is the model refusing to service a user (allocative), or is it erasing their identity from generated narratives (representational)?

- **Indian Caste Stratification (SC/ST/OBC/GC):** Understanding the target variable is essential. "General Castes" (GC) are the dominant group, while SC (Scheduled Castes), ST (Scheduled Tribes), and OBC (Other Backward Classes) are legally recognized marginalized groups.
  - **Quick check:** If the model outputs "Brahmin," does this map to General Caste or OBC in the target state?

- **Temperature & Sampling:** The study uses `temperature=1`. Understanding how this hyperparameter controls the trade-off between coherence (sticking to the dominant "truth") and diversity (exploring low-probability "tails") is key to interpreting the "winner-take-all" results.
  - **Quick check:** Would lowering the temperature reduce the rate of hallucination in the diversity conditions, or would it just re-enforce the majority bias?

## Architecture Onboarding

- **Component map:** Prompt Templates (Baseline vs. Implicit vs. Explicit) + Context Window (Previous Stories) -> GPT-4 Turbo (Black box transformer) -> Generated Output -> Manual Coding (for caste/religion extraction) + Statistical Testing (Chi-square) -> Distribution Comparison vs. Census Data

- **Critical path:** 1) Define Ritual/State prompt, 2) Generate 100 stories per condition (resetting context for Baseline, retaining for Diversity), 3) Extract named entities (Castes/Rituals), 4) Classify entities into SC/ST/OBC/GC buckets using government lists, 5) Compare distribution against Census percentages

- **Design tradeoffs:** Narrative vs. Template (open-ended stories chosen for ecological validity despite costly analysis vs. scalable but "brittle" fill-in-the-blank templates), Black Box Constraints (auditing API output due to inaccessible model weights)

- **Failure signatures:** "Generic Diversity" (model outputs "Tribal" or "Inter-caste" without specifics), Over-correction (diversity prompts cause majority group to be underrepresented relative to population)

- **First 3 experiments:** 1) Baseline Calibration (run 100 "Tell me a story about a wedding in [State]" prompts to establish raw prior probability), 2) Nudge Saturation (test if increasing nudge strength fixes issue or triggers more hallucinations), 3) Hallucination Audit (extract ritual names from Explicit Diversity condition and cross-reference with anthropological literature)

## Open Questions the Paper Calls Out

- **Open Question 1:** Is the "winner-take-all" bias dynamic a general characteristic of contemporary LLMs, or is it specific to GPT-4 Turbo? This study only audited GPT-4 Turbo, and while the authors expect results to generalize, they lack empirical evidence for other model architectures.

- **Open Question 2:** What specific algorithmic interventions or fundamental changes in model development are effective at correcting representational bias beyond data diversification? The paper identifies the insufficiency of current mitigation strategies but does not propose or test specific algorithmic solutions.

- **Open Question 3:** Does prompting in native Indic languages (e.g., Hindi, Tamil) alter the degree of representational bias compared to English prompts? The current study was restricted by linguistic constraints, leaving this interaction untested.

## Limitations

- The exact implementation of the cumulative context mechanism for diversity prompts remains unclear due to API token limits, suggesting authors may have used sliding windows or summarization
- The GPT-4 Turbo model snapshot date/version is unspecified, making exact statistical replication challenging due to potential API updates
- Specific government caste classification lists used for mapping names to categories are not detailed, particularly given state-level variations

## Confidence

- **High Confidence:** The overall finding that GPT-4 Turbo systematically underrepresents minoritized castes and religions in life ritual narratives. The chi-squared statistical tests are robust, and census data provides reliable ground truth.
- **Medium Confidence:** The claim that prompt-based nudging has "limited and inconsistent efficacy." While statistically supported, the mechanism requires further validation.
- **Low Confidence:** The interpretation that bias amplification exceeds training data distributions. This is inferred rather than directly measured, as the actual training corpus is inaccessible.

## Next Checks

1. **Context Window Analysis:** Implement the cumulative prompt strategy with different truncation/slider window sizes to determine how the authors managed the 99-story context constraint

2. **Decoding Temperature Sweep:** Run the baseline condition at temperatures 0.0, 0.5, and 1.0 to quantify how deterministic decoding affects majority/minority representation ratios

3. **Hallucination Ground Truth:** Extract all ritual names from the Explicit Diversity condition and cross-reference with anthropological sources to create a validated hallucination/non-hallucination label set