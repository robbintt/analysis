---
ver: rpa2
title: 'Video Killed the Energy Budget: Characterizing the Latency and Power Regimes
  of Open Text-to-Video Models'
arxiv_id: '2509.19222'
source_url: https://arxiv.org/abs/2509.19222
tags:
- energy
- video
- latency
- text
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ''
---

# Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models

## Quick Facts
- arXiv ID: 2509.19222
- Source URL: https://arxiv.org/abs/2509.19222
- Reference count: 21
- Primary result: GPU energy scales quadratically with video resolution and linearly with denoising steps for text-to-video models

## Executive Summary
This paper presents the first comprehensive energy and latency characterization of open text-to-video (T2V) generation models. Through theoretical modeling and empirical validation on an NVIDIA H100 GPU, the authors demonstrate that T2V generation is predominantly compute-bound, with GPU energy scaling quadratically with spatial resolution and linearly with denoising steps. The study benchmarks multiple T2V models and provides a detailed breakdown of energy consumption across system components, revealing that the Diffusion Transformer (DiT) dominates the energy budget while other components (VAE, Text Encoder) are negligible in comparison.

## Method Summary
The authors conducted controlled scaling experiments on WAN2.1-T2V-1.3B using the Hugging Face Diffusers library with FlashAttention enabled. Energy measurements were collected using CodeCarbon with NVML/pyRAPL backend, running 2 warmup iterations plus 5 repeated measurements per configuration. Experiments varied resolution (256×256 to 3520×1980), frame count (4-100 frames), and denoising steps (1-200) on an NVIDIA H100 SXM GPU. Cross-model benchmarking included AnimateDiff, CogVideoX-2b/5b, LTX-Video, Mochi-1-preview, and WAN2.1-1.3B/14B across 50 prompts each.

## Key Results
- GPU energy scales quadratically with spatial resolution (H×W) and linearly with denoising steps
- The Diffusion Transformer (DiT) dominates energy consumption (>60% of total GPU time)
- T2V generation is predominantly compute-bound on H100, with CPU/RAM contributions being negligible (<20%)
- Theoretical scaling predictions match empirical measurements with high accuracy (R² > 0.9)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** If a Text-to-Video (T2V) architecture relies on full attention mechanisms within a Diffusion Transformer (DiT), latency and energy scale quadratically with spatial and temporal dimensions.
- **Mechanism:** The DiT token length (ℓ) grows linearly with height, width, and frame count (ℓ ∝ H × W × T). Because self-attention computes pairwise interactions between all tokens (complexity O(ℓ²)), the computational cost increases quadratically as resolution or video length increases.
- **Core assumption:** The model uses full attention without windowing or sparse factorization that would reduce the complexity class.
- **Evidence anchors:**
  - [abstract] "showing quadratic growth with spatial and temporal dimensions"
  - [section 3.6] "Predicted Scaling Regimes... self- and cross-attention terms contribute O(ℓ²) FLOPs, leading to quadratic growth"
  - [corpus] Weak direct support; neighboring papers focus on SLMs or generation quality, not T2V scaling laws.
- **Break condition:** If the architecture switches to linear attention, sparse attention, or latent compression that reduces token dependency, the quadratic relationship weakens or disappears.

### Mechanism 2
- **Claim:** In a compute-bound regime, latency and energy consumption are directly proportional to the total floating-point operations (FLOPs) required by the model.
- **Mechanism:** On modern GPUs (like the H100), high token counts in video generation saturate arithmetic throughput rather than memory bandwidth. Therefore, execution time becomes a function of the total operations divided by the sustained FLOP/s rate (D_total ≈ F_total / μ Θ_peak).
- **Core assumption:** The efficiency factor μ remains relatively constant across different input resolutions and step counts, and the workload stays compute-bound (ℓ > 295 for attention).
- **Evidence anchors:**
  - [section 3.1] "Profiling shows that the main operators... are predominantly compute-bound."
  - [section 5.1.3] "scaling with the number of denoising steps is perfectly linear... near-perfect alignment between predictions and measurements"
  - [corpus] "Characterizing and Understanding Energy Footprint..." notes energy is hardware-dependent but focuses on edge/memory-bound constraints, contrasting with the H100 compute-bound findings here.
- **Break condition:** If inputs are reduced to very short clips or low resolutions (low ℓ), the model may shift to a memory-bound regime where latency depends on bandwidth rather than FLOPs.

### Mechanism 3
- **Claim:** Total energy scales linearly with the number of denoising steps because each step incurs an identical computational cost.
- **Mechanism:** Diffusion models generate video by iteratively denoising Gaussian noise over S steps. Since the DiT processes the same volume of data in every step, the total cost is the sum of S identical forward passes.
- **Core assumption:** The denoising scheduler does not introduce variable compute loads per step (e.g., early-exit mechanisms or dynamic step sizes).
- **Evidence anchors:**
  - [abstract] "linear scaling with the number of denoising steps"
  - [section 3.6] "Each step applies the same sequence of N transformer layers, so the ideal cost scales as O(S)."
  - [corpus] No specific corpus support for T2V step scaling; related T2V papers focus on caption alignment and guidance.
- **Break condition:** If caching mechanisms (e.g., reusing attention maps across steps) are implemented, the linear scaling may degrade as later steps become computationally cheaper.

## Foundational Learning

- **Concept: Roofline Model (Compute vs. Memory Bound)**
  - **Why needed here:** The paper's entire energy model relies on the assumption that T2V inference is compute-bound. Understanding this distinction is necessary to interpret why reducing resolution saves energy (fewer FLOPs) and why the efficiency factor μ ≈ 0.456 is critical.
  - **Quick check question:** Does doubling the video resolution double the memory traffic, the FLOPs, or both quadratically, and which bottleneck limits the speed on an H100?

- **Concept: Latent Video Diffusion (DiT)**
  - **Why needed here:** The analysis breaks down energy by component (VAE, DiT, Text Encoder). To optimize the system, one must know that the DiT dominates the compute budget (specifically the attention layers), while the VAE and Text Encoder are "negligible" for large generation tasks.
  - **Quick check question:** In the WAN2.1 architecture, which component contributes the most to the quadratic scaling curve: the VAE decoder, the text encoder, or the DiT self-attention?

- **Concept: FLOP Accounting for Transformers**
  - **Why needed here:** The paper uses a theoretical FLOP model to predict latency. Understanding how 2ℓ²d (attention) compares to 4fℓd² (MLP) explains why spatial resolution (ℓ) impacts energy more than model width (d) in this specific regime.
  - **Quick check question:** If you increase the MLP expansion factor f but keep the resolution constant, does the latency scale linearly or quadratically?

## Architecture Onboarding

- **Component map:** Text Encoder (T5-XXL) -> DiT (S iterations) -> VAE Decoder -> Video
- **Critical path:** The iterative DiT loop is the bottleneck. Specifically, the self-attention kernels within the DiT layers define the scaling behavior.
  1. Prompt -> Text Encoder (Fast, One-time)
  2. Noise -> DiT Step 1...S (Slow, Iterative) -> *Critical Path*
  3. Latents -> VAE (Fast, One-time) -> Video

- **Design tradeoffs:**
  - **Resolution/Length vs. Energy:** Tradeoff is quadratic. Reducing resolution by 2x saves ~4x energy. This is the most effective lever for sustainability.
  - **Denoising Steps vs. Quality:** Tradeoff is linear. Reducing steps saves energy linearly but degrades quality.
  - **Compute-bound assumption:** Valid for "high fidelity" generation; breaks for very short/low-res clips.

- **Failure signatures:**
  - **Unexpected Linear Scaling:** If increasing resolution does not increase latency quadratically, check for memory-bound bottlenecks (CPU/GPU data transfer) or hardware throttling.
  - **High CPU Energy:** The paper notes CPU energy should be low (<20%). If high, the software pipeline (data loading/preprocessing) is inefficient.
  - **Low GPU Utilization (<40%):** Indicates you are likely memory-bound or encountering kernel overhead, violating the theoretical model's assumptions.

- **First 3 experiments:**
  1. **Resolution Sweep:** Generate a 10s video at 360p, 480p, 720p, and 1080p. Plot GPU energy vs. (H × W). Verify the quadratic fit (R² > 0.9).
  2. **Step Count Linearity:** Generate a 5s video at fixed resolution with 10, 20, 30, and 50 steps. Confirm energy increases by a constant factor per step.
  3. **Operator Profiling:** Use a profiler (e.g., PyTorch Profiler) during inference to verify that >60% of GPU time is spent in torch.matmul or attention kernels, confirming the DiT dominance assumption.

## Open Questions the Paper Calls Out

- **Open Question 1:** How do energy consumption and latency trade off against perceptual quality metrics (e.g., fidelity, motion smoothness) when varying spatial resolution and denoising steps?
  - **Basis in paper:** [explicit] The authors state in the Limitations section that they "deliberately excluded perceptual quality from our scope, leaving open the question of energy–fidelity tradeoffs."
  - **Why unresolved:** The study isolated computational behavior (FLOPs, latency, energy) without assessing the visual quality of the generated outputs, making it unclear if efficiency gains (e.g., reducing steps) proportionally degrade utility.
  - **What evidence would resolve it:** A benchmark correlating the measured energy/latency data with standard perceptual metrics (like FID or motion smoothness scores) across the tested configurations.

- **Open Question 2:** Do the predicted compute-bound scaling regimes (quadratic in resolution/frames, linear in steps) hold empirically on hardware accelerators other than the NVIDIA H100?
  - **Basis in paper:** [explicit] The Limitations section notes that while theoretical thresholds suggest the results should generalize, "this remains to be confirmed experimentally" on other hardware platforms.
  - **Why unresolved:** All empirical data was derived from a single GPU type (NVIDIA H100 SXM); different memory bandwidths or arithmetic throughputs (e.g., on consumer GPUs or TPUs) might shift the boundary between memory-bound and compute-bound regimes.
  - **What evidence would resolve it:** Replication of the scaling experiments (varying resolution, frames, and steps) on diverse hardware architectures (e.g., AMD, Intel Gaudi, or consumer-grade NVIDIA GPUs).

- **Open Question 3:** What are the actual energy and latency savings achieved by implementing inference optimizations like diffusion caching, quantization, or kernel fusion in T2V pipelines?
  - **Basis in paper:** [explicit] The Discussion and Limitations sections identify these techniques as "promising avenues," noting that the analyzed implementation lacked them.
  - **Why unresolved:** The study characterizes the "unoptimized" baseline; it does not quantify how specific optimizations (like reusing redundant attention activations) alter the theoretical or empirical scaling laws.
  - **What evidence would resolve it:** Comparative benchmarking of models with and without these optimizations enabled, measuring the deviation from the predicted compute-bound latency.

- **Open Question 4:** What is the energy contribution of audio generation relative to video synthesis in end-to-end text-to-video systems?
  - **Basis in paper:** [explicit] The authors explicitly list this as a limitation, stating that "many production T2V systems (e.g., Veo) also generate audio, whose contribution to energy cost remains unexplored."
  - **Why unresolved:** The current analysis is strictly limited to video generation, ignoring the additional computational overhead required for synchronized audio synthesis in production environments.
  - **What evidence would resolve it:** Energy profiling of integrated video and audio generation pipelines compared against video-only baselines.

## Limitations

- The study did not include perceptual quality metrics, leaving energy-fidelity tradeoffs unexplored
- All experiments were conducted on a single hardware platform (NVIDIA H100), limiting generalizability to other accelerators
- The analysis excluded audio generation, which is commonly included in production T2V systems
- The study focused on inference only, not accounting for training energy costs

## Confidence

- **Theoretical scaling predictions:** High confidence - The paper demonstrates near-perfect alignment between predicted and measured scaling behavior
- **Component energy breakdown:** High confidence - Clear experimental evidence shows DiT dominance with negligible contributions from VAE and Text Encoder
- **Compute-bound regime:** Medium confidence - Validated on H100 but requires experimental confirmation on other hardware platforms
- **Linear step scaling:** High confidence - Perfect linearity observed across all tested step counts

## Next Checks

1. **Validation on alternative hardware:** Replicate the resolution and step scaling experiments on consumer-grade GPUs (e.g., RTX 4090) to verify the compute-bound regime holds across different architectures
2. **Quality-accuracy correlation:** Generate video outputs at multiple resolutions and step counts, then evaluate perceptual quality (FID, motion smoothness) to quantify the energy-fidelity tradeoff
3. **Optimization impact assessment:** Implement diffusion caching or quantization in the inference pipeline and measure the deviation from predicted scaling laws to quantify real-world optimization benefits