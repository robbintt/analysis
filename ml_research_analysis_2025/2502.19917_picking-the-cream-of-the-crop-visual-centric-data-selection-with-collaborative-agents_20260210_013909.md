---
ver: rpa2
title: 'Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative
  Agents'
arxiv_id: '2502.19917'
source_url: https://arxiv.org/abs/2502.19917
tags:
- image
- visual
- score
- data
- images
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of noisy and misaligned data
  in multimodal large language model (MLLM) training. The authors propose ViSA (Visual-Centric
  Selection via Agents Collaboration), a method that leverages multiple visual agents
  to assess image informativeness and instruction quality.
---

# Picking the Cream of the Crop: Visual-Centric Data Selection with Collaborative Agents

## Quick Facts
- arXiv ID: 2502.19917
- Source URL: https://arxiv.org/abs/2502.19917
- Authors: Zhenyu Liu; Yunxin Li; Baotian Hu; Wenhan Luo; Yaowei Wang; Min Zhang
- Reference count: 31
- Key outcome: ViSA achieves performance comparable to or better than SOTA models on seven benchmarks using only 2.5% of the original data

## Executive Summary
This paper addresses the challenge of noisy and misaligned data in multimodal large language model (MLLM) training. The authors propose ViSA (Visual-Centric Selection via Agents Collaboration), a method that leverages multiple visual agents to assess image informativeness and instruction quality. The approach quantifies image complexity through segmentation and object detection, evaluates diverse perspectives using a multi-agent framework, and assesses instruction quality via prior token perplexity and image-text mutual information. The authors curate an 80K high-quality instruction dataset from the LLaVA-OneVision dataset. Extensive experiments show that ViSA achieves performance comparable to or better than state-of-the-art models on seven benchmarks using only 2.5% of the original data. The method demonstrates significant improvements in training efficiency, particularly for complex image understanding tasks, and shows that even well-trained large models can benefit from high-quality data.

## Method Summary
ViSA is a visual-centric data selection pipeline that filters noisy multimodal data through five sequential stages. The method quantifies image complexity using SAM2-based segmentation (SC Score) and DINO-based object detection with TF-IDF weighting (OA Score), evaluates image quality through multi-agent collaboration using Shapley-weighted scoring (DP Score), and assesses instruction quality via prior token perplexity (PT Score) and image-text mutual information (IM Score). The pipeline filters 175K samples from LLaVA-OneVision to produce 80K high-quality instruction pairs, which are used to fine-tune VLMs like Qwen2-VL-2B and InternVL-1.0-3B. The method demonstrates that high-quality data selection can achieve SOTA performance with significantly reduced computational resources.

## Key Results
- ViSA achieves comparable or superior performance to SOTA models on seven benchmarks using only 2.5% of the original data
- Significant improvements in training efficiency, particularly for complex image understanding tasks
- Even well-trained large models benefit from high-quality data curation

## Why This Works (Mechanism)

### Mechanism 1: Visual Information Quantification via Multi-Signal Detection
- **Claim:** Images with richer visual elements provide denser training signal for MLLM visual representation learning.
- **Mechanism:** Two complementary signals quantify image complexity: (1) Segmentation Complexity Score uses SAM2 with 512 anchor points to count distinct visual regions above IoU 0.75; (2) Object Alignment Score uses DINO to detect 1800+ object categories, weighted via TF-IDF to emphasize rare/discriminative objects over common ones.
- **Core assumption:** Images with more segmented regions and diverse/rare objects contain more "learnable" visual information for the vision encoder.
- **Evidence anchors:**
  - [abstract]: "quantifies image complexity through segmentation and object detection"
  - [section 3.1]: "SC Score is designed to assess the richness of graphical elements and characters... OA Score evaluates the richness of objects in an image"
  - [corpus]: Related work "Filter Images First, Generate Instructions Later" validates pre-instruction image selection, but uses different scoring approaches
- **Break condition:** Repetitive textures (e.g., fabric patterns) may score high on SC despite low semantic value—acknowledged in case study (Figure 6).

### Mechanism 2: Instruction Quality via Prior Token Perplexity and Mutual Information
- **Claim:** Instructions tightly coupled with image content provide better vision-language alignment training.
- **Mechanism:** (1) Prior Token Perplexity Score measures response coherence—lower perplexity indicates more predictable/fluent text, but extremes may indicate overly simple responses; (2) Image-Text Mutual Information Score computes I(Text;Image) = H(Text) − H(Text|Image), where higher values indicate text depends more strongly on the visual input, filtering out "weak alignment" cases where answers can be generated without seeing the image.
- **Core assumption:** High-quality instruction data requires genuine visual-textual dependency, not just well-written text.
- **Evidence anchors:**
  - [abstract]: "assesses instruction quality via prior token perplexity and image-text mutual information"
  - [section 3.3]: "A higher mutual information value indicates a stronger association between the text and the image"
  - [corpus]: "Cream of the Crop" paper explores similar data curation principles for instruction fine-tuning
- **Break condition:** Complex reasoning tasks with multiple valid responses may have lower measured MI despite genuine image dependence.

### Mechanism 3: Multi-Agent Collaboration via Shapley-Weighted Aggregation
- **Claim:** Combining multiple visual agents with correlation-weighted scoring produces more robust quality assessments than single-agent evaluation.
- **Mechanism:** Three VLM agents (InternVL, QwenVL, LLaVA) evaluate images from diverse perspectives (culture, emotion, dynamics, composition, etc.). Shapley values based on Pearson correlation weight agents by their contribution to score consistency—agents whose scores correlate better with consensus receive higher weights.
- **Core assumption:** Different agents capture complementary quality dimensions; consistency-weighted aggregation reduces individual agent biases.
- **Evidence anchors:**
  - [abstract]: "leverages multiple visual agents to assess image informativeness and instruction quality"
  - [section 3.2]: "we introduce a Shapley value based on the Pearson correlation coefficient to weigh the reliability of each agent's assessment"
  - [corpus]: Limited direct corpus evidence for multi-agent data selection specifically
- **Break condition:** If all agents share systematic biases (e.g., favoring certain image styles), correlation-based weighting cannot correct this.

## Foundational Learning

- **Concept: TF-IDF Weighting for Object Frequency**
  - Why needed here: Prevents common objects (e.g., "person") from dominating the OA Score, ensuring rare/discriminative objects contribute more to image quality assessment.
  - Quick check question: If "car" appears in 50% of images but "vintage_roadster" appears in 0.5%, which should receive higher weight in the OA Score formula?

- **Concept: Mutual Information for Multimodal Dependency**
  - Why needed here: Quantifies whether instruction text genuinely requires image context, filtering weakly-aligned pairs.
  - Quick check question: If H(Text) = 10 bits and H(Text|Image) = 3 bits, what does I = 7 bits suggest about the instruction-image relationship compared to I = 1 bit?

- **Concept: Shapley Values for Fair Attribution**
  - Why needed here: Provides principled weighting of agent contributions based on marginal improvement to coalition performance (measured via correlation consistency).
  - Quick check question: Why would simple averaging fail if one agent consistently produces noisy scores while others are reliable?

## Architecture Onboarding

- **Component map:** Raw image-instruction pairs → SAM2 (SC Score) → DINO + TF-IDF (OA Score) → 3 VLM agents + Shapley (DP Score) → Perplexity calculation (PT Score) → Entropy calculations (IM Score) → 80K filtered pairs

- **Critical path:**
  1. Sample 175K from source dataset
  2. Apply SC filter (remove bottom 15%) → ~148K
  3. Apply OA filter (remove bottom 20%) → ~118K
  4. Apply DP filter (remove bottom 13%) → 100K images
  5. Apply PT + IM filter (select top 10%) → 80K final pairs

- **Design tradeoffs:**
  - Filtering thresholds (15%/20%/13%/10%) are empirical, not theoretically derived
  - SC Score overweights repetitive textures; OA Score better captures semantic richness
  - DP Score tends toward extreme ratings (agents assign very high or very low)
  - Computational cost: running SAM2, DINO, and 3 VLMs per image

- **Failure signatures:**
  - High SC scores for texture-heavy but semantically empty images
  - Well-written but image-independent instructions passing MI filter if model entropy estimates are noisy
  - Significant agent disagreement on ambiguous images (check per-agent variance)

- **First 3 experiments:**
  1. Reproduce score distributions on 10K sample—verify SC/OA/DP histograms match Figure 3 patterns before full pipeline run
  2. Ablation study: train Qwen2-VL-2B with each filter removed sequentially (Table 2 shows instruction quality removal causes largest drop: −3.1%)
  3. Cross-dataset validation: apply ViSA scoring to ShareGPT4V vs. llava_gpt4_20k—confirm SC/IM distributions reflect known quality differences (Figure 3, Appendix B)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can a rigorous theoretical foundation be established for the proposed image information richness metrics to ensure generalizability across diverse data sources?
- Basis in paper: [explicit] The Limitations section states that the method "currently lacks a solid theoretical foundation," which restricts the generalizability of the heuristic quantification approaches used.
- Why unresolved: The current metrics (SC, OA, DP) rely on empirical aggregation of agent outputs without a formal information-theoretic or cognitive definition of "richness."
- What evidence would resolve it: Deriving a mathematical framework that correlates the proposed scores with generalizable visual representation learning, validated across heterogeneous image distributions.

### Open Question 2
- Question: Does the efficiency of the ViSA selection pipeline persist when applied to the full scale of multi-million sample datasets used in state-of-the-art MLLM training?
- Basis in paper: [explicit] The authors note in the Limitations section that "resource constraints" limited experiments to a 200K sample subset, restricting exploration of ViSA on "larger datasets."
- Why unresolved: It is untested whether the 2.5% selection ratio remains optimal or if the filtering metrics saturate/break down when the source data volume increases by an order of magnitude.
- What evidence would resolve it: Applying ViSA to the complete LLaVA-OneVision dataset (or similar scale) and analyzing the performance-to-data ratio compared to the subset experiments.

### Open Question 3
- Question: How can the Diversity Perspective Score (DP Score) methodology be refined to produce nuanced distributions rather than the observed "extreme trend" of binary ratings?
- Basis in paper: [explicit] Section 7 notes that the DP Score distribution is "notably extreme," with agents often assigning only the highest or lowest ratings, a challenge the authors admit "remains an open issue."
- Why unresolved: The current prompting and aggregation strategy encourages categorical judgments rather than fine-grained differentiation, making it difficult to distinguish between "good" and "excellent" images.
- What evidence would resolve it: Modifying the agent evaluation protocols (e.g., comparative ranking) to yield a normal distribution of scores rather than a bimodal one.

## Limitations
- The method currently lacks a solid theoretical foundation for the proposed image information richness metrics
- Resource constraints limited experiments to a 200K sample subset, restricting exploration on larger datasets
- The DP Score methodology shows an "extreme trend" where agents often assign only the highest or lowest ratings

## Confidence
- **Mechanism validity:** High - The three proposed mechanisms (visual quantification, mutual information, Shapley weighting) are well-established in their respective domains
- **Implementation feasibility:** Medium - Multiple technical details are underspecified (entropy computation, agent identities for PT/IM)
- **Scalability claims:** Low - Limited to 200K samples; performance on full-scale datasets untested
- **DP Score refinement:** Low - The extreme rating distribution is acknowledged as an open issue

## Next Checks
1. Reproduce score distributions on 10K sample—verify SC/OA/DP histograms match Figure 3 patterns before full pipeline run
2. Ablation study: train Qwen2-VL-2B with each filter removed sequentially (Table 2 shows instruction quality removal causes largest drop: −3.1%)
3. Cross-dataset validation: apply ViSA scoring to ShareGPT4V vs. llava_gpt4_20k—confirm SC/IM distributions reflect known quality differences (Figure 3, Appendix B)