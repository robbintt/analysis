---
ver: rpa2
title: "Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charit\xE9, and\
  \ Aignostics"
arxiv_id: '2501.05409'
source_url: https://arxiv.org/abs/2501.05409
tags:
- pathology
- cancer
- images
- dataset
- foundation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Atlas is a pathology foundation model trained on 1.2 million histopathology\
  \ whole slide images from Mayo Clinic and Charit\xE9. It uses a ViT-H/14 architecture\
  \ and the RudolfV training approach."
---

# Atlas: A Novel Pathology Foundation Model by Mayo Clinic, Charité, and Aignostics

## Quick Facts
- arXiv ID: 2501.05409
- Source URL: https://arxiv.org/abs/2501.05409
- Reference count: 40
- Outperforms Virchow2 and H-Optimus-0 on 21 public pathology benchmarks with 61.9% average balanced accuracy

## Executive Summary
Atlas is a pathology foundation model trained on 1.2 million histopathology whole slide images from Mayo Clinic and Charité. Using a ViT-H/14 architecture and RudolfV self-supervised training, it achieves state-of-the-art performance across 21 public benchmark datasets. The model excels particularly in morphology-related tasks while remaining competitive in molecular-related benchmarks, outperforming larger models like Virchow2 and H-Optimus-0 despite having fewer parameters.

## Method Summary
Atlas uses a ViT-H/14 backbone (632M parameters) trained via an adapted RudolfV approach based on DINOv2's self-supervised framework. The model was trained on 1.2M WSIs from two institutions, covering 70+ tissue types and 100+ staining protocols. Tiles were extracted at multiple resolutions (5×-40× magnification) and sampled using RudolfV's algorithm. The pretrained model generates embeddings evaluated through linear probing on 21 downstream tasks, with dual-token assessment (CLS and CLS+Mean) to maximize performance across different task types.

## Key Results
- Achieves 61.9% average balanced accuracy across 21 public benchmark datasets
- Outperforms Virchow2 and H-Optimus-0 despite smaller model size
- Excels in morphology tasks (BACH, CRC-100k, MHIST) while remaining competitive in molecular tasks (HEST-IDC, HEST-LUAD)
- Demonstrates particular strength at 10× magnification compared to 20× for certain tasks

## Why This Works (Mechanism)

### Mechanism 1
Multi-institutional, multi-stain training diversity improves generalization across morphology tasks. Training on 1.2M WSIs from Mayo Clinic and Charité with >70 tissue types and >100 staining protocols exposes the model to broader biological and technical variance than single-source training, encoding representations that transfer more robustly to unseen downstream tasks.

### Mechanism 2
RudolfV/DINOv2 self-supervised framework enables effective representation learning without task-specific labels. The ViT-H/14 backbone is trained via self-distillation with no labels, forcing the model to learn semantically meaningful patch-level features that cluster by morphological similarity and can be linearly probed for downstream tasks.

### Mechanism 3
Dual-token evaluation (CLS vs. CLS+Mean) captures complementary representational information. Vision Transformers encode global image information in the CLS token and local patch information in patch tokens. Evaluating both representations and reporting the better performance compensates for unknown optimal token extraction strategies.

## Foundational Learning

- **Self-supervised learning (SSL) with vision transformers**: Atlas is trained without labels; understanding contrastive/self-distillation objectives is essential to interpret what the model learns. Quick check: Can you explain how DINOv2's self-distillation loss differs from contrastive learning?

- **Linear probing vs. fine-tuning evaluation**: All benchmarks use frozen backbones with linear classifiers; this tests representation quality, not end-to-end adaptation. Quick check: Why might linear probing underrepresent a foundation model's potential compared to full fine-tuning?

- **Multiple instance learning (MIL) for slide-level tasks**: CAMELYON16 and PANDA use ABMIL to aggregate patch embeddings into slide-level predictions. Quick check: How does attention-based MIL weight individual patches differently than mean-pooling?

## Architecture Onboarding

- **Component map**: WSI collection -> Tile extraction (5×-40× magnification) -> RudolfV self-supervised pretraining -> Embedding extraction (CLS or CLS+Mean) -> Linear probing or ABMIL evaluation

- **Critical path**: 1) Tile extraction from WSIs at multiple magnifications, 2) RudolfV self-supervised pretraining on ~520M sampled tiles, 3) Embedding extraction (CLS or CLS+Mean), 4) Task-specific probing (linear classifier or ABMIL head)

- **Design tradeoffs**: Tile-based vs. slide-based (Atlas is tile-based; slide-level tasks require separate aggregation), Multi-stain vs. H&E-only (broader stain coverage may improve IHC tasks but adds preprocessing complexity), Model size vs. compute (ViT-H/14 is smaller than H-Optimus-0 but still requires significant GPU resources)

- **Failure signatures**: Poor performance on TCGA Uniform (20×) while strong at 10× suggests magnification-specific representation gaps, Large CLS vs. CLS+Mean disparities indicate task-specific token sensitivity, High molecular task variance (HEST std. deviations) suggests inconsistent gene expression encoding

- **First 3 experiments**: 1) Reproduce linear probing on 2-3 benchmark datasets (e.g., CRC-100k, PCAM) using extracted Atlas embeddings to validate pipeline, 2) Compare CLS vs. CLS+Mean performance on a morphology task (BACH) and a molecular task (HEST-IDC) to characterize token sensitivity, 3) Run ABMIL on CAMELYON16 with frozen Atlas embeddings to establish slide-level baseline before any fine-tuning attempts

## Open Questions the Paper Calls Out

- **Open Question 1**: Does further scaling of model parameters and training data volume (beyond 1.2M WSIs and 632M parameters) yield substantial performance improvements for pathology foundation models? The paper notes that related work using larger datasets or more parameters suggests scaling might yield additional improvements, but Atlas achieves state-of-the-art results despite being smaller than competitors.

- **Open Question 2**: How does Atlas perform on clinical tasks involving rare diseases and diverse staining protocols not represented in the current 21 public benchmarks? The authors acknowledge that foundation model development would benefit from a larger and more diverse pool of benchmark datasets to better understand robustness and generalization capabilities.

- **Open Question 3**: What causes the specific performance discrepancy where Atlas excels at 10× magnification tasks but underperforms on the TCGA Uniform 20× benchmark? The paper highlights this as an "interesting" variance but doesn't provide an explanation for why the model struggles specifically at 20× resolution for this cancer typing task.

## Limitations

- Proprietary 1.2M WSI dataset from Mayo Clinic and Charité prevents exact reproduction
- RudolfV training approach specifics beyond "adapted from DINOv2" are not detailed
- Model shows performance gaps on molecular tasks compared to morphology tasks
- Superior performance claims rely on multi-institutional diversity but lack direct ablation studies

## Confidence

**High confidence**: General architecture (ViT-H/14), training framework (RudolfV/DINOv2), and evaluation methodology (linear probing, dual-token assessment) are clearly specified.

**Medium confidence**: Claims that multi-institutional training diversity drives generalization improvements are plausible but not directly proven through ablation studies.

**Low confidence**: Specific advantages of RudolfV over vanilla DINOv2 remain unclear, and claims about optimal tile sampling strategy cannot be independently verified.

## Next Checks

1. **Replicate linear probing pipeline**: Using public tile datasets (TCGA or PANDA), extract CLS and CLS+Mean embeddings from a pretrained DINOv2 model and evaluate on 2-3 benchmark tasks (e.g., CRC-100k, BACH, PCAM) to validate the evaluation methodology matches reported results.

2. **Characterize token sensitivity**: Systematically compare CLS-only versus CLS+Mean performance across a morphology task (BACH) and molecular task (HEST-IDC) to quantify whether token selection strategy explains performance variance or reveals task-specific representation properties.

3. **Establish slide-level baseline**: Implement ABMIL aggregation on CAMELYON16 using frozen embeddings from any pretrained pathology model to confirm the pipeline works before attempting to replicate Atlas's specific slide-level performance.