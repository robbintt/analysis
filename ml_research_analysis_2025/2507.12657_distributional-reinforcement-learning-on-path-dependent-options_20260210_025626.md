---
ver: rpa2
title: Distributional Reinforcement Learning on Path-dependent Options
arxiv_id: '2507.12657'
source_url: https://arxiv.org/abs/2507.12657
tags:
- learning
- distribution
- quantile
- distributional
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applies Distributional Reinforcement Learning to estimate
  the full payoff distribution of path-dependent options, focusing on Asian options.
  Unlike traditional pricing methods that estimate only expected payoffs, the proposed
  framework learns the entire conditional distribution of payoffs using quantile regression
  and radial basis function (RBF) approximations.
---

# Distributional Reinforcement Learning on Path-dependent Options

## Quick Facts
- arXiv ID: 2507.12657
- Source URL: https://arxiv.org/abs/2507.12657
- Reference count: 18
- Primary result: Applied distributional RL with quantile regression and RBF approximation to learn full payoff distributions of Asian options, achieving absolute errors below 1 for in-the-money options

## Executive Summary
This study applies Distributional Reinforcement Learning to estimate the full payoff distribution of path-dependent options, focusing on Asian options. Unlike traditional pricing methods that estimate only expected payoffs, the proposed framework learns the entire conditional distribution of payoffs using quantile regression and radial basis function approximations. The approach frames option pricing as a recursive distributional estimation problem, where the Markovian state includes current price, running average, and time index. The distributional Bellman operator ensures stable propagation of uncertainty over time.

## Method Summary
The framework constructs a state vector $s_t = (S_t, A_t, t)$ representing current price, running average, and time index to maintain Markov property for Asian options. It uses RBF networks with 40 centers to approximate quantile functions, predicting 50 quantiles through linear weights. The quantile regression loss (pinball loss) trains the model via semi-gradient TD updates with backward Bellman recursion. The method includes gradient clipping and payoff clipping for stability, normalizing inputs by dividing by 200. Training proceeds over 100 epochs with 100 paths per epoch.

## Key Results
- Accurately approximates payoff distributions with absolute errors typically below 1 for in-the-money options
- Successfully learns key distributional moments including mean and skewness
- Framework generalizes well to out-of-sample scenarios
- Tail behavior (kurtosis) remains challenging to capture precisely
- Direct access to quantiles and tail probabilities without strong parametric assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Path-dependent options can be priced using recursive distributional learning if the path history is compressed into a finite-dimensional Markov state.
- **Mechanism:** The framework constructs a state vector $s_t = (S_t, A_t, t)$ (current price, running average, time). This "path-augmented representation" acts as a sufficient statistic for the future payoff distribution, transforming a non-Markovian history into a Markovian state. This allows the Bellman operator to function without tracking the infinite-dimensional price trajectory.
- **Core assumption:** The running average $A_t$ captures all relevant information for the Asian option payoff required by the conditional distribution.
- **Break condition:** If the payoff functional depends on path features not included in $s_t$ (e.g., barrier levels not tracked in the state), the Markov property fails, and recursive updates will diverge or bias.

### Mechanism 2
- **Claim:** Propagating full distributions backward through time stabilizes the learning of uncertainty compared to single-value expectations.
- **Mechanism:** The Distributional Bellman Operator ($T^\pi Z$) updates the entire probability distribution of payoffs rather than just the expected value. Because this operator is a $\gamma$-contraction in the Wasserstein metric, it theoretically ensures that errors in the estimated distribution shrink over recursive updates, preventing the "moment drift" seen in mean-based methods.
- **Core assumption:** The Wasserstein metric accurately reflects the geometric structure of the probability space, and the discount factor $\gamma < 1$ ensures the contraction property holds.
- **Break condition:** If the underlying asset process has heavy tails such that the required moments for Wasserstein convergence are infinite, the contraction guarantee may fail.

### Mechanism 3
- **Claim:** Radial Basis Function (RBF) networks can approximate quantile functions for option payoffs more stably than deep networks in low-data regimes.
- **Mechanism:** Instead of predicting a single value, the architecture predicts $N$ quantiles ($\theta_i(s)$). The Quantile Regression Loss (pinball loss) penalizes over- and under-estimation asymmetrically based on the quantile level $\tau_i$. The RBF features provide a smooth, localized approximation basis, mapping states to quantiles via linear weights, which prevents the gradient instability often found in deep neural networks.
- **Core assumption:** The state space can be adequately covered by the RBF centers, and the payoff distribution is smooth enough to be approximated by the selected number of quantiles.
- **Break condition:** If the payoff distribution is highly complex (e.g., discontinuous barriers) or the state space is high-dimensional, the fixed RBF centers may fail to generalize, requiring excessive manual feature engineering.

## Foundational Learning

- **Concept: Quantile Regression & The Pinball Loss**
  - **Why needed here:** The core learning signal. Unlike standard Mean Squared Error (MSE), which targets the average, the pinball loss targets specific percentiles (e.g., the 5th percentile for VaR). Understanding this is required to see how the model learns the *shape* of the distribution.
  - **Quick check question:** If I want to learn the median (50th percentile) vs. the 95th percentile tail, how does the loss function penalize positive vs. negative errors differently?

- **Concept: Markov State Augmentation**
  - **Why needed here:** Path-dependent options (like Asians) naturally violate the Markov assumption because history matters. You must understand how to compress history (e.g., keeping a running average) into the current state vector to make the problem solvable by RL methods.
  - **Quick check question:** If you were pricing a Lookback option (payoff depends on max price), what single variable must you add to the state vector $s_t$ to maintain the Markov property?

- **Concept: Wasserstein Distance**
  - **Why needed here:** This is the metric used to prove the convergence of the algorithm. It measures the "cost" of transforming one distribution into another. It is superior to KL-divergence here because it handles non-overlapping supports (e.g., a distribution with a mass at 0 vs. a continuous distribution) without infinite error.
  - **Quick check question:** Why is the Wasserstein distance preferred over KL-divergence when comparing two distributions that might not overlap (e.g., a predicted Gaussian vs. a true distribution with a probability mass at zero)?

## Architecture Onboarding

- **Component map:** Input Layer (State vector $(S_t, A_t, t)$) -> Normalizer (Scales inputs to $[0,1]^3$) -> Feature Layer (Fixed RBF features $\phi(s)$) -> Output Layer (Linear weights $W$ producing $N$ quantile estimates $\theta_1(s) \dots \theta_N(s)$) -> Loss (Asymmetric Quantile Huber/Pinball loss)

- **Critical path:** The Normalizer $\to$ RBF Center Alignment is the most fragile component. The paper notes that inputs are divided by fixed scaling constants (e.g., dividing price by 200). If market data moves outside this range, the RBF features will saturate or vanish, breaking the approximation.

- **Design tradeoffs:**
  - **RBF vs. Deep Neural Networks:** The paper chooses RBF for interpretability and stability in low-data regimes. **Tradeoff:** RBFs scale poorly with dimensionality (curse of dimensionality) compared to DNNs.
  - **Quantile Count ($N=50$):** Choosing too few quantiles smooths over tail risks (kurtosis); too many increases computational cost and variance.

- **Failure signatures:**
  1. **Negative Prices:** Occurs if gradient clipping is disabled or payoffs are not clipped. Large TD-errors in tails cause exploding gradients (Section 3, Figure 3).
  2. **Mode Collapse / Degenerate Distribution:** Initializing all quantiles to the mean can lead to slow convergence where the model fails to "spread" the quantiles to match the true variance if learning rate is too low.
  3. **Tail Underestimation:** The paper admits kurtosis is hard to capture. Expect the model to underestimate extreme tail risks (rare events) compared to Monte Carlo benchmarks.

- **First 3 experiments:**
  1. **Sanity Check (ITM Asian):** Train on deeply in-the-money paths (where $S_0 \gg K$). Verify that the learned mean approximates the Black-Scholes/Monte-Carlo price and that absolute error < 1.
  2. **Gradient Explosion Test:** Disable gradient clipping on out-of-the-money paths. Observe if negative price predictions appear in the quantiles (replicating Figure 3 behavior).
  3. **State Sensitivity:** Remove the running average $A_t$ from the state vector. Attempt to train. The failure to converge will demonstrate the necessity of the path-augmented state representation for Asian options.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the Distributional RL framework be extended to price American options by incorporating policy optimization?
- **Basis in paper:** The conclusion states that "even American options could be priced... if extend it with policy optimization. We leave this important future perspective as a further study."
- **Why unresolved:** The current formulation is policy-free (uncontrolled), relying on exogenous dynamics, whereas American options require optimizing an early exercise policy (stopping rule).
- **What evidence would resolve it:** A modified algorithm that learns a stopping policy alongside the value distribution and converges to known American option prices.

### Open Question 2
- **Question:** How does the framework perform under stochastic volatility and interest rate models compared to the constant parameters used in the study?
- **Basis in paper:** The conclusion explicitly remarks on the "necessity of testing our framework on stochastic volatility and interest rate scenarios along with real life option payoffs."
- **Why unresolved:** The numerical experiments were limited to geometric Brownian motion with constant interest rate ($r$) and volatility ($\sigma$).
- **What evidence would resolve it:** Successful approximation of payoff distributions when the underlying asset follows models like Heston or Hull-White dynamics.

### Open Question 3
- **Question:** Does the quantile-based approach provide robust adaptability to jumps and enable gradient-based model calibration?
- **Basis in paper:** The introduction notes that "remarks of ours on both jumps and advantages of calibration require further research, and we leave such matters aside as a further study."
- **Why unresolved:** The study focused on continuous paths; the hypothesis that the method handles discontinuities (jumps) and allows for gradient-based parameter recovery was not empirically verified.
- **What evidence would resolve it:** Performance analysis on jump-diffusion processes and the successful recovery of implied parameters via gradient descent on learned quantiles.

## Limitations
- **Tail behavior accuracy:** The framework explicitly struggles with kurtosis estimation, systematically underestimating extreme tail risks for risk management applications.
- **State representation complexity:** While effective for Asian options, the approach may not generalize well to options requiring multiple complex path features.
- **Distributional convergence guarantees:** The theoretical convergence relies on assumptions about finite moments and complete state coverage that may not hold in practice.

## Confidence
- **Distributional convergence guarantees:** Medium - theoretically proven but practical assumptions may fail
- **Tail behavior accuracy:** Low for extreme quantiles - framework struggles with kurtosis estimation
- **Generalization to complex path-dependencies:** Medium - approach may not scale to high-dimensional state spaces

## Next Checks
1. **Out-of-sample stress test:** Evaluate performance on asset parameters (volatility, drift) not seen during training, specifically with σ ∈ [0.3, 0.5] and K values outside the training range.

2. **Tail risk quantification:** Compare estimated 1% and 99% quantiles against extremely large Monte Carlo samples (10M+ paths) to measure systematic bias in tail estimates.

3. **Alternative state representations:** Implement and compare performance using different sufficient statistics (e.g., geometric vs. arithmetic averages for Asian options, or max price tracking for lookback options).