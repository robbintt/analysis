---
ver: rpa2
title: 'When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue
  Generation'
arxiv_id: '2505.24613'
source_url: https://arxiv.org/abs/2505.24613
tags:
- biography
- interlocutor
- dialogue
- speaker
- dialogues
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study examines how dialogue agents adapt to both target and
  interlocutor personas during generation. It introduces a systematic evaluation framework
  that masks or reveals interlocutor information to assess persona alignment and generalization
  across familiar/unfamiliar speaker pairings and topics.
---

# When Harry Meets Superman: The Role of The Interlocutor in Persona-Based Dialogue Generation

## Quick Facts
- arXiv ID: 2505.24613
- Source URL: https://arxiv.org/abs/2505.24613
- Reference count: 40
- The study examines how dialogue agents adapt to both target and interlocutor personas during generation, finding that models adapt effectively to interlocutor biographies but struggle more with unfamiliar interlocutors than unfamiliar topics.

## Executive Summary
This paper investigates how dialogue agents adapt to both target speaker and interlocutor personas during generation. The authors introduce a systematic evaluation framework that masks or reveals interlocutor information to assess persona alignment and generalization across familiar/unfamiliar speaker pairings and topics. Through extensive experiments with Llama 3.1 8B models, the study reveals that models effectively adapt to interlocutor biographies, generalize better across topics than across unfamiliar interlocutor pairings, and that fine-tuning yields deeper persona capture compared to zero-shot generation which tends to copy biographical details for easier recognition.

## Method Summary
The study uses the PRODIGy dataset (5,660 dialogues with dual-speaker profile annotations) and 100 synthetic Non-PRODIGy characters to evaluate persona-based dialogue generation. Llama 3.1 8B Instruct is fine-tuned with QLoRA (lr=3e-4, lora_r=16, lora_alpha=8) on next-turn prediction, generating 8-turn dialogues. The evaluation framework uses author identification (3-way classification) under four disclosure configurations: BothDisc, BioDisc, TurnsDisc, and BothMask. An LLM-as-a-judge (fine-tuned Llama 3.1 8B) and human annotators evaluate persona alignment. Topics are annotated using Llama 3.1 and clustered into 100 stems for experimental conditions.

## Key Results
- Access to interlocutor persona improves target speaker recognition (BothDisc: 0.820 vs BothMask: 0.577)
- Models generalize better across topics (0.561) than unfamiliar interlocutor pairings (0.496)
- Zero-shot models show high rare-word overlap (40.69%) with biographies while fine-tuned models show low overlap (5.18%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Disclosing interlocutor biography improves target speaker identifiability because models condition responses on interlocutor context.
- Mechanism: The model generates target speaker turns that reference or adapt to the interlocutor's background; evaluators use this cross-speaker alignment to infer the target persona. When interlocutor information is masked, these contextual cues disappear, reducing recognition accuracy.
- Core assumption: Author identification accuracy proxies for how well the model integrated interlocutor information into its generation process.
- Evidence anchors:
  - Gold dialogues: BothDisc accuracy 0.820 vs BothMask 0.577; BioDisc (0.805) outperforms TurnsDisc (0.588)
  - Related work confirms persona-response consistency is central to the challenge
- Break condition: If evaluator accuracy improves when masking interlocutor info, the mechanism fails.

### Mechanism 2
- Claim: Models generalize better across topics than across unfamiliar interlocutor pairings because topic vocabulary is more directly accessible than relational dynamics between mismatched personas.
- Mechanism: Topic words can be injected into dialogue without requiring deep persona restructuring; unfamiliar interlocutors demand implicit reasoning about how two disparate personas would interact, which the model handles less robustly.
- Core assumption: Accuracy differences between topic conditions and speaker-pairing conditions reflect differential adaptation difficulty.
- Evidence anchors:
  - Fine-tuned model: familiar topics 0.566 vs unfamiliar 0.561 (near-identical); familiar pairings 0.703 vs unfamiliar 0.496 (large gap)
- Break condition: If unfamiliar topics cause larger accuracy drops than unfamiliar interlocutors, the claimed asymmetry reverses.

### Mechanism 3
- Claim: Fine-tuning reduces surface-level biography copying and enables deeper persona internalization, while zero-shot generation relies on verbatim reproduction of prompt content.
- Mechanism: Zero-shot models attend strongly to prompt tokens and reproduce rare biographical terms directly; fine-tuned models learn to encode persona traits into latent representations, producing dialogue that reflects personality without explicit lexical overlap.
- Core assumption: Lower lexical overlap between biography and generated turns indicates deeper, more distributed persona representation.
- Evidence anchors:
  - Rare word overlap: zero-shot 40.69% vs fine-tuned 5.18% vs gold 5.84%; METEOR: zero-shot 0.140 vs fine-tuned 0.043
- Break condition: If fine-tuned models show higher rare-word overlap than zero-shot while maintaining similar accuracy, the mechanism is falsified.

## Foundational Learning

- Concept: **Persona-conditioned dialogue generation**
  - Why needed here: The entire framework assumes models can accept and use persona descriptions as context for generating in-character responses.
  - Quick check question: Given a biography "I am a glider pilot who loves camaraderie," does the model generate dialogue mentioning flying and teamwork, or generic small talk?

- Concept: **Author identification as evaluation proxy**
  - Why needed here: The paper frames persona fidelity as a classification task—can an evaluator pick the correct biography from three options?
  - Quick check question: If a dialogue mentions "collaborative problem-solving" but the biography only says "petroleum engineer," would an evaluator correctly match them?

- Concept: **Instruction drift in multi-turn dialogue**
  - Why needed here: The paper limits dialogues to 8 turns, citing prior work that persona alignment decays beyond this point.
  - Quick check question: After 12 turns, does the target speaker still sound like themselves, or have they drifted toward generic responses?

## Architecture Onboarding

- Component map:
  Persona Encoder -> Dialogue Generator (Llama 3.1 8B) -> Evaluator Module (LLM/human judge) -> Distractor Generator (SBERT)

- Critical path:
  1. Format biography prompts (target + interlocutor + topic) → 2. Generate 8-turn dialogue → 3. Select distractor biographies via SBERT → 4. Run evaluator under each disclosure config → 5. Compute accuracy/F1 across configs and conditions

- Design tradeoffs:
  - **Zero-shot vs Fine-tuned**: Zero-shot yields higher identification accuracy (superficial copying) but lower dialogue quality; fine-tuned yields lower accuracy but deeper persona capture
  - **Evaluator choice**: LLM-as-a-judge is scalable and consistent but may overestimate accuracy vs human evaluators
  - **Disclosure granularity**: BioDisc and TurnsDisc isolate which interlocutor signal matters more

- Failure signatures:
  - **Copy-paste detection**: High rare-word overlap (>30%) between biography and generated turns indicates zero-shot superficiality
  - **Unfamiliar-pairing collapse**: Accuracy near random baseline (0.333) on unfamiliar interlocutor conditions
  - **Evaluator miscalibration**: Human accuracy dramatically lower than LLM-as-a-judge suggests the judge model is overconfident

- First 3 experiments:
  1. **Baseline disclosure sweep**: Run all four disclosure configs on gold dialogues to establish upper bounds
  2. **Zero-shot vs fine-tuned comparison**: Generate dialogues for identical speaker pairs in both modes; compute rare-word overlap and METEOR scores
  3. **Unfamiliar interlocutor stress test**: Pair PRODIGy characters with Non-PRODIGy characters; confirm accuracy drop and analyze failure modes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the performance drop associated with unfamiliar interlocutor pairings be mitigated without requiring exposure to those specific pairings during training?
- Basis in paper: The authors find that models "struggle with unfamiliar interlocutors" and that "performance drops with unfamiliar pairings," identifying this as a key limitation.
- Why unresolved: The study quantifies the generalization gap but does not propose architectural or training interventions to close it.
- What evidence would resolve it: A modified training regime or architecture that achieves statistically similar accuracy scores on unfamiliar pairings compared to familiar ones.

### Open Question 2
- Question: Can zero-shot generation strategies be optimized to capture deep persona traits rather than relying on superficial "copy-pasting" of biographical details?
- Basis in paper: Section 4.4 demonstrates that while zero-shot models yield higher identification accuracy, they do so by copying rare words (40.69% overlap) rather than capturing deeper characteristics.
- Why unresolved: The paper highlights the trade-off between recognition ease and depth of persona capture but offers no solution to improve depth without fine-tuning.
- What evidence would resolve it: A zero-shot prompting method that maintains high identification accuracy while achieving low lexical overlap and high human ratings for persona depth.

### Open Question 3
- Question: To what extent do findings based on fictional movie characters generalize to real-world, non-fictional dialogue agents?
- Basis in paper: The Limitations section states that reliance on the PRODIGy dataset (movie dialogues) "may introduce stereotyped roles and inherent biases."
- Why unresolved: The entire experimental framework relies on fictional or synthetic personas, leaving the efficacy of interlocutor adaptation in realistic, non-stereotypical interactions unverified.
- What evidence would resolve it: Replicating the systematic masking evaluation framework on a dataset of genuine human-human interactions or non-fictional role-playing scenarios.

## Limitations

- The evaluation framework relies heavily on author identification accuracy as a proxy for persona fidelity, which may not fully capture nuanced aspects of persona alignment
- The study focuses exclusively on 8-turn dialogues, limiting generalizability to longer conversations where persona drift may become more pronounced
- The LLM-as-a-judge evaluation shows systematic overconfidence compared to human annotators (0.594 vs 0.509 accuracy), potentially inflating performance estimates

## Confidence

- **High Confidence**: Mechanism 1 (interlocutor biography disclosure improves identification) - directly supported by experimental results showing substantial accuracy differences across disclosure conditions
- **Medium Confidence**: Mechanism 2 (generalization across topics vs interlocutors) - supported by accuracy patterns but not extensively validated against alternative explanations
- **Medium Confidence**: Mechanism 3 (zero-shot copying vs fine-tuned internalization) - lexical overlap metrics support the claim, but deeper semantic analysis of persona representation is not provided

## Next Checks

1. Conduct a semantic similarity analysis (beyond rare-word overlap) to verify that fine-tuned models encode persona traits in latent representations rather than simply reducing surface-level copying
2. Extend dialogue length beyond 8 turns to empirically validate the persona drift claim and test whether interlocutor adaptation effects persist over longer conversations
3. Perform ablation studies removing interlocutor information from fine-tuned models to isolate the specific contribution of interlocutor biography to target persona identifiability