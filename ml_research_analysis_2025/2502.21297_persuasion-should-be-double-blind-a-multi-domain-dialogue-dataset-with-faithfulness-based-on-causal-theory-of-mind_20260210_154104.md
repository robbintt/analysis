---
ver: rpa2
title: 'Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset With Faithfulness
  Based on Causal Theory of Mind'
arxiv_id: '2502.21297'
source_url: https://arxiv.org/abs/2502.21297
tags:
- persuadee
- persuader
- generative
- desire
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of creating realistic persuasive
  dialogue datasets by introducing a novel multi-agent framework guided by causal
  Theory of Mind (ToM). The proposed ToMMA framework ensures "double-blind" conditions
  between persuader and persuadee agents while generating dialogues, preventing information
  leakage and enhancing alignment with human-like persuasion dynamics.
---

# Persuasion Should be Double-Blind: A Multi-Domain Dialogue Dataset With Faithfulness Based on Causal Theory of Mind

## Quick Facts
- arXiv ID: 2502.21297
- Source URL: https://arxiv.org/abs/2502.21297
- Authors: Dingyi Zhang; Deyu Zhou
- Reference count: 40
- Key outcome: Introduces ToMMA framework with double-blind conditions between persuader and persuadee agents, generating CToMPersu dataset of 6,275 dialogues across 35 domains with superior evaluation scores (Context-Coherence 4.97, Logical-Coherence 4.97, Helpfulness 4.93)

## Executive Summary
This paper addresses the challenge of creating realistic persuasive dialogue datasets by introducing a novel multi-agent framework guided by causal Theory of Mind (ToM). The proposed ToMMA framework ensures "double-blind" conditions between persuader and persuadee agents while generating dialogues, preventing information leakage and enhancing alignment with human-like persuasion dynamics. The framework is evaluated using a new causal ToM-based metric, which demonstrates superior performance compared to conventional evaluation methods. The resulting CToMPersu dataset comprises 6,275 dialogues across 35 domains and 6,257 unique scenarios, showing better alignment with real human dialogues and achieving higher scores on multiple evaluation metrics.

## Method Summary
The paper proposes the ToMMA (Theory of Mind Multi-Agent) framework that generates persuasive dialogues using three specialized agents: a Mental State Generator, a Persuadee Agent, and a Persuader Agent, with an optional Observer Agent for quality control. The framework enforces "double-blind" conditions where the persuader can only access dialogue history, not the persuadee's private mental state. Dialogues follow a structured causal ToM protocol: rounds 1-2 handle preventative beliefs/desires, while rounds 3-4 address generative beliefs and desires sequentially. The Observer Agent reviews and corrects persuader inferences when necessary, improving dialogue coherence through an iterative feedback loop.

## Key Results
- CToMPersu dataset achieves Context-Coherence score of 4.97, Logical-Coherence score of 4.97, and Helpfulness score of 4.93
- Outperforms existing datasets on multiple evaluation metrics including Relevance (4.94), Logical Consistency (4.88), and Generation Satisfaction (28.38)
- Demonstrates superior alignment with human-like persuasion dynamics through causal ToM-based evaluation
- Successfully prevents information leakage between persuader and persuadee agents

## Why This Works (Mechanism)

### Mechanism 1
Enforcing double-blind conditions between persuader and persuadee agents prevents information leakage and produces more realistic dialogue dynamics. The persuadee agent holds private mental state (beliefs, desires) that the persuader agent cannot directly access. The persuader must infer this state from dialogue history alone, mirroring real human persuasion where mental states are not transparently shared. Core assumption: Realistic persuasion requires asymmetric information; humans do not explicitly instruct persuaders on which strategies to employ.

### Mechanism 2
Structuring persuasion around Causal Theory of Mind (addressing beliefs and desires sequentially) improves logical coherence. For preventative behavior, altering either belief OR desire suffices. For generative behavior, BOTH belief AND desire must be addressed. The framework enforces this: rounds 1-2 handle preventative beliefs/desires; rounds 3-4 address generative beliefs (round 3) then desires (round 4). Core assumption: Human persuasion follows this causal structure where action emerges from belief + desire conjunctions.

### Mechanism 3
An Observer Agent that reviews and corrects mental state inferences improves dialogue quality by catching misattributions. After the persuader infers the persuadee's mental state, the Observer compares this against the ground-truth mental state. If misaligned, it provides specific corrective feedback. The persuader then regenerates their response. Core assumption: Persuader agents frequently misinterpret utterances; explicit correction improves downstream coherence.

## Foundational Learning

- **Theory of Mind (ToM)**: Why needed here: The entire framework assumes persuaders must model others' mental states (beliefs, desires) to persuade effectively. Without ToM, the causal intervention logic makes no sense. Quick check: Can you explain why knowing someone's belief without their desire is insufficient for predicting their action?

- **BDI Model (Belief-Desire-Intention)**: Why needed here: The paper's Causal ToM builds directly on BDI-style mental state representation. Understanding how beliefs and desires combine to produce intentions is foundational. Quick check: If someone believes the gym is open but has no desire to exercise, will they go? What does this imply for persuasion?

- **Double-Blind Protocol**: Why needed here: Borrowed from experimental design, this ensures neither party has access to the other's private information. Understanding why this matters for validity is essential. Quick check: In a persuasion study, if the persuadee knows the persuader's strategy, how might this change their responses?

## Architecture Onboarding

- **Component map**: Mental State Generator -> Persuadee Agent -> Persuader Agent -> (Optional) Observer Agent

- **Critical path**: 1. Filter unique scenarios from DailyPersuasion (6,257 scenarios) 2. Generate mental state components per scenario 3. Run multi-turn dialogue with double-blind separation 4. Observer reviews each persuader inference; if incorrect, prompt regenerates

- **Design tradeoffs**: Fixed round count (3 for generative-only, 4 for preventative+generative) limits flexibility but ensures causal structure is followed. Persuadee is cooperative (explicitly expresses concerns) rather than adversarial; may not reflect all real persuasion contexts. Observer adds inference cost; trade-off between quality and generation speed

- **Failure signatures**: Persuader repeats the same argument across rounds (belief/desire confusion). Persuadee mentions persuader's arguments in their own stance (information leakage). Observer loop fails to converge (persistent misinterpretation)

- **First 3 experiments**: 1. Ablate the Observer Agent: Compare dialogue quality scores with and without Observer feedback to quantify its contribution. 2. Vary persuader model: Test GPT-3.5 vs. GPT-4o-mini vs. GPT-4o on the CToMPersu test set using both Fixed and Dynamic Persuadee evaluation protocols. 3. Cross-dataset evaluation: Train a persuasion model on CToMPersu and evaluate on PersuasionForGood (human dataset) to test generalization to real human dialogues.

## Open Questions the Paper Calls Out

### Open Question 1
How would incorporating explicit persuadee personality traits (e.g., openness, resistance level) affect the diversity and realism of generated persuasive dialogues? Basis in paper: "Furthermore, defining the persuadee's personality can also be implemented, as persuadees with different personalities may have distinct ways of responding." Why unresolved: Current framework assumes persuadees "are more likely to explicitly express their thoughts and concerns" rather than modeling varied personality-driven response patterns.

### Open Question 2
Why does GPT-3.5 outperform GPT-4o in Generative Satisfaction (28.38 vs 17.90) despite GPT-4o's superior performance in other metrics? Basis in paper: The paper reports this counterintuitive finding in Table 2 but offers only speculative explanations about prompt design and turn numbers influencing scores. What evidence would resolve it: Ablation studies isolating prompt structure, conversation length, and model-specific biases in ToM reasoning.

### Open Question 3
How does the Observer Agent's correction rate scale with dialogue complexity and does it introduce systematic biases in the final dataset? Basis in paper: The paper demonstrates a single successful Observer intervention case but does not quantify failure rates, correction types, or whether Observer feedback consistently improves outcomes. What evidence would resolve it: Statistical analysis of Observer interventions across the full dataset, including inter-annotator agreement on whether corrections genuinely improved dialogue quality.

### Open Question 4
Can the ToMMA framework maintain double-blind fidelity when extended to multi-party persuasion scenarios involving more than two agents? Basis in paper: The framework is designed for dyadic persuader-persuadee interactions; real-world persuasion often involves multiple stakeholders with partially shared/hidden mental states. What evidence would resolve it: Extending ToMMA to 3+ agent scenarios and measuring information leakage rates and dialogue coherence.

## Limitations
The framework's dependence on structured mental state generation may not generalize to scenarios where beliefs and desires are ambiguous or conflicting. The fixed round structure (3-4 turns) may constrain more complex persuasion dynamics. The Observer Agent introduces computational overhead and may not scale efficiently for larger datasets.

## Confidence
**High Confidence**: The double-blind mechanism's effectiveness in preventing information leakage is well-supported by the design and corpus analysis. The improved evaluation metrics (Context-Coherence, Logical-Coherence, Helpfulness) are directly measurable and consistently higher than baselines.

**Medium Confidence**: The causal ToM framework's theoretical foundation is sound, but the assumption that all persuasion scenarios cleanly separate into preventative/generative belief/desire components may not hold universally. The Observer Agent's impact is measurable but may be context-dependent.

**Low Confidence**: Claims about CToMPersu's alignment with human-like persuasion dynamics are primarily based on model-based evaluation rather than direct human comparison. The dataset's ability to capture nuanced human persuasion strategies remains to be validated.

## Next Checks
1. **Human Evaluation**: Conduct human studies comparing CToMPersu dialogues with human-human persuasion conversations to validate naturalness and effectiveness claims.

2. **Ablation Studies**: Systematically remove each component (Observer Agent, causal ToM structure, double-blind conditions) to quantify their individual contributions to dialogue quality.

3. **Generalization Testing**: Evaluate CToMPersu-trained models on diverse real-world persuasion tasks and human datasets to assess transfer learning capabilities.