---
ver: rpa2
title: Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining
  and Data Filtering
arxiv_id: '2506.04981'
source_url: https://arxiv.org/abs/2506.04981
tags:
- data
- hours
- labeled
- speech
- incremental
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of fine-tuning ASR models for
  specific domains when labeled data is scarce, proposing an incremental semi-supervised
  learning pipeline. The method integrates a small in-domain labeled set with auxiliary
  data from a related domain, then applies filtering strategies (multi-model consensus
  via CER or named entity recognition) to iteratively refine pseudo-labels.
---

# Better Semi-supervised Learning for Multi-domain ASR Through Incremental Retraining and Data Filtering

## Quick Facts
- **arXiv ID:** 2506.04981
- **Source URL:** https://arxiv.org/abs/2506.04981
- **Reference count:** 0
- **Primary result:** Consensus-based filtering achieves up to 22.3% relative improvement on Wow and 24.8% on Fisher

## Executive Summary
This paper addresses the challenge of fine-tuning ASR models for specific domains when labeled data is scarce, proposing an incremental semi-supervised learning pipeline. The method integrates a small in-domain labeled set with auxiliary data from a related domain, then applies filtering strategies (multi-model consensus via CER or named entity recognition) to iteratively refine pseudo-labels. Evaluated on Wow call center and Fisher English corpora using Zipformer and Whisper-medium models, the approach significantly outperforms single-step fine-tuning.

## Method Summary
The method uses incremental semi-supervised learning with pseudo-label filtering. It starts by fine-tuning a base ASR model on a small in-domain labeled set (S_core) plus optional auxiliary labeled data (S_aux). The fine-tuned model generates pseudo-labels for a large unlabeled corpus (U). These pseudo-labels are filtered using either CER-based consensus (three-model agreement) or NER detection. The filtered subsets are then incrementally merged, with the model re-decoding accumulated data and fine-tuning on progressively larger pseudo-labeled datasets across K iterations. This iterative refinement aims to improve model performance while avoiding error accumulation from poor pseudo-labels.

## Key Results
- Consensus-based filtering achieves up to 22.3% relative improvement on Wow and 24.8% on Fisher
- Incremental retraining shows slower performance saturation compared to random selection
- NER filtering provides competitive results at lower computational cost than consensus
- Zipformer outperforms Whisper-medium in both datasets, though Whisper degrades after iteration 2

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-model consensus filtering identifies higher-quality pseudo-labels than random selection.
- Mechanism: Three diverse ASR models decode the same audio. Low inter-model character error rate (CER < 5%) indicates transcription agreement, which correlates with correctness. Filtered segments show 5.6% WER vs 22.9% for unfiltered (Table 1).
- Core assumption: Model agreement predicts pseudo-label accuracy across domains.
- Evidence anchors:
  - [abstract] "Consensus-based filtering achieves up to 22.3% relative improvement on Wow and 24.8% on Fisher"
  - [section 2.1.2] Details the CER computation: CER = (CER(T1,T2) + CER(T1,T3) + CER(T2,T3)) / 3
  - [corpus] LESS paper confirms pseudo-label quality is critical for SSL success with in-the-wild data
- Break condition: When models systematically agree on errors (shared failure modes), consensus fails to filter.

### Mechanism 2
- Claim: Incremental retraining with progressive data addition delays performance saturation.
- Mechanism: Rather than adding all pseudo-labeled data at once, subsets are introduced across K iterations. Model_i-1 re-decodes accumulated buffer U'_i, refining pseudo-labels as the model improves. This allows progressive correction.
- Core assumption: Earlier iterations improve the model enough to generate better pseudo-labels for later iterations.
- Evidence anchors:
  - [abstract] "showing slower performance saturation compared to random selection"
  - [section 2, Algorithm 1] Lines 10-12 show the incremental loop structure
  - [corpus] NIRANTAR framework demonstrates similar incremental learning benefits for multilingual ASR
- Break condition: If initial pseudo-labels are too noisy, error amplification occurs before refinement can help.

### Mechanism 3
- Claim: NER-based filtering provides competitive quality at lower computational cost than consensus.
- Mechanism: Segments containing named entities are retained; empty segments are discarded. The intuition is that entity presence indicates coherent, non-hallucinated transcription. WER drops from 22.9% to 17.0% for NER-filtered subset.
- Core assumption: Named entities correlate with transcription quality; entity absence indicates noise or incomplete utterances.
- Evidence anchors:
  - [abstract] "NER provides competitive results at lower computational cost"
  - [section 2.1.3] "Any segment containing at least one recognized entity is retained"
  - [corpus] No direct corpus evidence for NER specifically; assumption remains weakly validated
- Break condition: Domains with few named entities (e.g., purely functional commands) will over-filter valid data.

## Foundational Learning

- Concept: Semi-supervised learning with pseudo-labeling
  - Why needed here: The entire pipeline depends on generating and refining pseudo-labels from unlabeled audio. Understanding confirmation bias (error reinforcement) is critical.
  - Quick check question: Can you explain why poor pseudo-labels degrade model performance over iterations?

- Concept: Character Error Rate (CER) and Word Error Rate (WER)
  - Why needed here: CER quantifies model agreement for filtering; WER evaluates final ASR quality. The 5% CER threshold is arbitrary but empirically validated.
  - Quick check question: Why use CER for filtering but WER for evaluation?

- Concept: Fine-tuning vs. training from scratch
  - Why needed here: The pipeline starts from pretrained models (Zipformer, Whisper) rather than random initialization. Domain adaptation behavior differs from cold-start training.
  - Quick check question: What risks arise when fine-tuning on small in-domain datasets?

## Architecture Onboarding

- Component map:
  - model_base -> S_core ∪ S_aux (optional) -> model_0
  - model_0 -> pseudo-labels on U -> Filter module (CER consensus or NER)
  - Filtered subsets -> Incremental trainer (K iterations) -> model_K

- Critical path:
  1. Fine-tune model_base on S_core (+ S_aux if available) → model_0
  2. Generate pseudo-labels for U using model_0
  3. Apply filtering (CER < 5% OR NER detection) → partition into K subsets
  4. For i=1 to K: merge U_i into buffer, re-decode with model_i-1, fine-tune model_base on S_core + buffer
  5. Return model_K as final

- Design tradeoffs:
  - CER filtering: Highest quality (10.8% WER on Wow) but requires 3x decoding compute
  - NER filtering: Competitive (11.1% WER), lower cost, but may over-filter entity-sparse domains
  - Random baseline: Simplest, no filtering overhead, but faster saturation (11.3% WER)
  - Auxiliary data S_aux: Helps iteration 0 (12.0% vs 12.5% WER) but shows no benefit in later iterations; may introduce domain mismatch

- Failure signatures:
  - WER increases in later iterations (observed with Whisper after iteration 2): indicates learning rate or step size issues
  - Small filtered subset (CER < 5% retains only 17.7% of 100h Fisher subset): may indicate model mismatch or threshold too strict
  - Performance degradation vs. single-step baseline: suggests error accumulation from noisy pseudo-labels

- First 3 experiments:
  1. Baseline replication: Run incremental pipeline with random selection on Fisher (50h labeled + 250h unlabeled, 2 iterations) using Zipformer. Target: ~11.9% WER per Figure 1b.
  2. CER filtering validation: Implement 3-model consensus (NeMo Parakeet, Whisper-medium, Zipformer Gigaspeech) with 5% threshold. Compare filtered subset WER against Table 1 values (expect ~5.6% on held-out Fisher subset).
  3. Ablation on S_aux: Train with and without auxiliary DefinedAI data on Wow setup. Measure iteration 0 and iteration 1 WER to confirm 4% initial gain and subsequent neutral/negative effect.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can incremental semi-supervised pipelines be adapted to prevent performance degradation in large encoder-decoder models (like Whisper) during later training iterations?
- Basis in paper: [explicit] The authors note that while Zipformer improves consistently, Whisper performance degrades after Iteration 2, specifically stating: "Future work may explore adaptations to the process, to maintain Whisper improvements in subsequent iterations."
- Why unresolved: The paper identifies the instability (WER increasing from 11.2% to 11.6%) but does not investigate architectural or optimization causes, such as catastrophic forgetting or data distribution shifts specific to encoder-decoder models.
- What evidence would resolve it: Ablation studies testing varying learning rates, smaller incremental data subsets, or regularization techniques (e.g., Elastic Weight Consolidation) applied specifically to the Whisper model in later iterations.

### Open Question 2
- Question: Can a dynamic data re-weighting mechanism mitigate the negative transfer observed when retaining auxiliary labeled data alongside generated in-domain pseudo-labels?
- Basis in paper: [inferred] The paper observes that while auxiliary data (DefinedAI) helps Iteration 0, including it in Iteration 1 degrades performance (12.3% WER vs 11.7%), suggesting that "domain mismatch... could increase confusion relative to the newly generated in-domain pseudo-labels."
- Why unresolved: The current solution is a binary choice (include or exclude), leaving unexplored whether a soft-weighting approach could retain the benefits of auxiliary diversity without the confusion caused by domain mismatch.
- What evidence would resolve it: Experiments utilizing curriculum learning or dynamic sampling probabilities that decay the influence of auxiliary data as the volume of high-confidence in-domain pseudo-labels increases.

### Open Question 3
- Question: Is there a hierarchical or sequential integration strategy for NER and CER filtering that yields cumulative gains over single-method baselines?
- Basis in paper: [explicit] The authors attempted basic combinations (checkpoint averaging and 50/50 data splitting) but reported that "Neither approach surpassed the CER-only baseline."
- Why unresolved: The failed combinations were simplistic; it remains unclear if the semantic validity checking of NER could effectively refine the subset selected by CER if applied as a secondary pass or weighted feature.
- What evidence would resolve it: Results from a pipeline where CER selects a broad candidate set which is then further pruned or re-ranked by NER confidence scores, compared against CER-only baselines.

## Limitations

- Hyperparameter Specification: The paper does not specify fine-tuning learning rates, batch sizes, optimizer choices, or epoch counts for either the initial fine-tuning or incremental steps.
- Domain Generalization: The effectiveness of NER-based filtering assumes named entity presence correlates with transcription quality, which remains weakly validated for entity-sparse domains.
- Data Split Reproducibility: Fisher corpus splits reference prior work without specifying exact segment identifiers, making exact replication impossible without accessing the original dataset partitioning.

## Confidence

- High Confidence: Claims about consensus-based filtering achieving 22.3% relative improvement on Wow and 24.8% on Fisher are well-supported by quantitative results (WER reductions from 22.9% to 5.6% for high-quality filtered subsets).
- Medium Confidence: Incremental retraining showing slower performance saturation compared to random selection is supported by Figure 1 trends, though absolute performance gains vary by model (Zipformer outperforms Whisper).
- Low Confidence: NER filtering's competitive performance at lower computational cost is claimed but lacks direct empirical validation against the consensus approach on identical datasets.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates (1e-5, 5e-5, 1e-4) and subset sizes across 2-4 iterations on Fisher dataset to determine optimal configuration preventing WER degradation in later iterations.

2. **NER Filtering Domain Robustness**: Apply NER-based filtering to a low-entity domain (e.g., numerical database queries or technical commands) and measure both WER improvement and data retention rate to validate the filtering assumption across diverse domain types.

3. **Error Rate Correlation Study**: Create a small manually-annotated validation set from pseudo-labeled data. Compute correlation between CER-based consensus scores and actual transcription accuracy to quantify how well model agreement predicts pseudo-label quality.