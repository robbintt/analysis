---
ver: rpa2
title: 'SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and
  Evidence-Based Reasoning'
arxiv_id: '2511.16198'
source_url: https://arxiv.org/abs/2511.16198
tags:
- citation
- verification
- citations
- classification
- evidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SemanticCite introduces an AI-powered system for automated citation
  verification that addresses semantic citation errors and AI-generated hallucinated
  references through full-text source analysis. The system employs a four-class classification
  scheme (Supported, Partially Supported, Unsupported, Uncertain) combined with hybrid
  retrieval-augmented generation using both dense semantic search and sparse keyword
  matching.
---

# SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning

## Quick Facts
- **arXiv ID**: 2511.16198
- **Source URL**: https://arxiv.org/abs/2511.16198
- **Authors**: Sebastian Haan
- **Reference count**: 30
- **Primary result**: Qwen3-4B model achieves 83.64% weighted accuracy and 90.01% character similarity in citation verification

## Executive Summary
SemanticCite introduces an AI-powered system for automated citation verification that addresses semantic citation errors and AI-generated hallucinated references through full-text source analysis. The system employs a four-class classification scheme (Supported, Partially Supported, Unsupported, Uncertain) combined with hybrid retrieval-augmented generation using both dense semantic search and sparse keyword matching. Fine-tuned Qwen3 models (4B parameters) achieve performance comparable to large commercial systems while requiring significantly lower computational resources.

## Method Summary
The system uses a hybrid retrieval approach combining dense semantic search with sparse keyword matching (BM25) to locate relevant evidence snippets from source documents. It employs a 4-class ordinal classification scheme for citation verification and uses QLoRA fine-tuning on Qwen3 models. The architecture includes document ingestion via PyMuPDF, chunking into 512-character segments, hybrid retrieval with reranking, and LLM-based classification with confidence scoring.

## Key Results
- Qwen3-4B model achieves 83.64% weighted accuracy and 90.01% character similarity in citation verification
- System outperforms larger 8B model on length calibration and character similarity metrics
- Hybrid retrieval approach improves recall for citations containing specific numerical data and terminology

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Combining dense semantic search with sparse keyword matching (BM25) increases recall of relevant evidence snippets compared to single-method retrieval.
- **Mechanism**: Dense retrieval captures conceptual relationships and paraphrased content, while sparse retrieval ensures exact term correspondence (e.g., specific numerical values or terminology) that semantic vectors might miss.
- **Core assumption**: The relevant evidence required to verify a citation exists within discrete, semi-overlapping text chunks (512 characters) of the source document.

### Mechanism 2
- **Claim**: A 4-class ordinal classification (Supported, Partially Supported, Unsupported, Uncertain) enables more actionable remediation than binary verification.
- **Mechanism**: By penalizing errors based on semantic distance (e.g., classifying "Unsupported" as "Supported" is penalized higher than "Partially Supported"), the model learns to distinguish between critical failures and minor nuance omissions.
- **Core assumption**: "Partially Supported" citations—those with accurate cores but missing caveats—are distinct error categories requiring revision rather than deletion.

### Mechanism 3
- **Claim**: Fine-tuning lightweight models (Qwen3 4B) with QLoRA allows for computational efficiency while maintaining high text-generation fidelity (reasoning quality).
- **Mechanism**: Specialized training on citation-specific alignments reduces the "over-parameterization effects" seen in larger general-purpose models, allowing a 4B model to outperform an 8B variant in length calibration and character similarity.
- **Core assumption**: The ground truth data generated by GPT-4.1 is sufficiently accurate and free from systematic bias to serve as a training signal for the smaller student model.

## Foundational Learning

- **Concept: Hybrid Retrieval (Dense + Sparse)**
  - **Why needed here**: Standard semantic search often misses exact numerical data (e.g., "p < 0.05") or specific proper nouns. BM25 is added to guarantee these "lexical anchors" are retrieved.
  - **Quick check question**: If a user cites a specific chemical concentration that appears in a table footnote, which retrieval component ensures it isn't missed?

- **Concept: Ordinal Classification**
  - **Why needed here**: Citation accuracy is not binary. A claim might be "technically true but misleadingly context-free." The system needs to predict a degree of support, not just a boolean.
  - **Quick check question**: Why is a "weighted accuracy" metric superior to standard accuracy when evaluating a system that distinguishes between "Unsupported" and "Partially Supported"?

- **Concept: RAG (Retrieval-Augmented Generation)**
  - **Why needed here**: The LLM cannot verify a citation without the source text. RAG injects the relevant chunks of the PDF into the context window so the LLM can "read" the source before judging.
  - **Quick check question**: What is the failure mode if the chunk size (512 characters) is too small for a document with complex, long-form arguments?

## Architecture Onboarding

- **Component map**: Ingestion (PyMuPDF extraction) -> Recursive Character Splitter (512 chars) -> Indexing (ChromaDB + BM25) -> Hybrid Retrieval (top 30) -> FlashRank Reranker (top 3) -> LLM Synthesis (JSON output)

- **Critical path**: The Neural Reranking step is the quality gate. If the reranker fails to promote the specific contradictory sentence to the top 3, the LLM will likely classify the citation as "Supported" due to lack of evidence to the contrary.

- **Design tradeoffs**:
  - 4B vs. 8B Models: The paper explicitly argues for the 4B model. It offers better "length calibration" (sticking to the expected output size) and character similarity than the larger 8B model, likely due to less over-fitting during QLoRA.
  - Abstract Fallback: The system falls back to abstracts if full-text PDFs are unavailable. This increases coverage but significantly reduces verification depth.

- **Failure signatures**:
  - "Hallucinated Ground Truth": If the training data (generated by GPT-4) contains subtle errors, the fine-tuned Qwen models will output confident but incorrect reasoning.
  - Multi-reference Aggregation: The system currently evaluates references one-by-one. A claim supported by the sum of two papers but not either individually will be flagged as "Unsupported."

- **First 3 experiments**:
  1. Retrieval Ablation: Disable the BM25 (sparse) component and verify citations containing specific numerical data (e.g., "50% reduction") to observe the drop in retrieval accuracy.
  2. Calibration Check: Compare the JSON output length of the Qwen3-4B vs. Qwen3-8B on a held-out set to verify the paper's claim that the 4B model has superior length calibration.
  3. Adversarial "Uncertain" Test: Feed the system a vaguely worded citation and a source document with ambiguous phrasing to test if the model correctly outputs the "Uncertain" class rather than forcing a binary decision.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model performance compare when ground truth annotations come from human experts versus automated LLM-based generation?
- Basis in paper: [explicit] "The evaluation framework relies exclusively on OpenAI GPT-4.1 ground truth annotations, potentially introducing systematic biases inherent in commercial language models. Future work should incorporate human expert annotations and inter-annotator agreement studies to validate automated ground truth generation."
- Why unresolved: No human validation was performed; only GPT-4.1 annotations were used for evaluation, which could introduce model-specific biases into the assessment.
- What evidence would resolve it: Human expert annotation study with inter-annotator agreement metrics (e.g., Cohen's kappa) comparing classifications against both GPT-4.1 and human ground truth.

### Open Question 2
- Question: Can collective evidence synthesis across multiple references improve verification accuracy for citations that combine partial support from individual sources?
- Basis in paper: [explicit] "The framework currently processes multi-reference citations by evaluating each referenced source individually... This approach can result in citations being classified as PARTIALLY SUPPORTED when individual references each provide partial evidence, even though their combined evidence may constitute full support."
- Why unresolved: Current implementation treats each reference independently without mechanisms for aggregating or synthesizing evidence across multiple sources.
- What evidence would resolve it: Development and evaluation of evidence synthesis algorithms that assess whether partial support from individual sources aggregates to comprehensive validation.

### Open Question 3
- Question: How can citation verification systems be extended to handle multimodal content including figures, tables, and mathematical expressions?
- Basis in paper: [explicit] "The system's reliance on text-based analysis limits applicability to multimodal citations involving figures, tables, or mathematical expressions common in scientific literature. Extending the framework to incorporate visual and mathematical content represents a significant technical challenge."
- Why unresolved: Current system processes only text; visual and mathematical content extraction and semantic analysis remain unimplemented.
- What evidence would resolve it: Evaluation of multimodal verification capabilities on a dataset containing figure citations, tabular data references, and mathematical equation citations.

## Limitations

- **Ground Truth Quality Risk**: The training data is generated by GPT-4.1, which may contain systematic biases that propagate to the fine-tuned student models.
- **Multi-reference Complexity**: The current architecture evaluates each citation-source pair independently, failing to handle citations that require synthesis across multiple sources.
- **Chunk Boundary Issues**: The discrete 512-character chunking approach fails when citations depend on relationships between non-adjacent sections or when key evidence is split across chunks.

## Confidence

- **Hybrid Retrieval Effectiveness**: Medium confidence. The mechanism is theoretically sound but lacks direct comparison against alternative hybrid approaches.
- **Ordinal Classification Utility**: Medium confidence. The 4-class scheme is well-justified conceptually but lacks evidence that real-world citation errors follow this ordinal pattern.
- **4B Model Superiority**: High confidence. The paper provides clear quantitative evidence showing 4B model outperforms 8B on specific metrics (length calibration, character similarity).

## Next Checks

1. **Chunk Boundary Analysis**: Systematically identify citations where critical evidence spans chunk boundaries, then measure how often the reranker fails to surface all necessary chunks simultaneously.

2. **Ground Truth Bias Audit**: Create a small validation set with human-verified ground truth for challenging citations, then compare GPT-4.1 generated labels against human judgment to measure systematic bias.

3. **Multi-reference Simulation**: Construct test cases where citations require synthesis across multiple sources, then evaluate whether the single-source evaluation approach produces meaningful false negatives.