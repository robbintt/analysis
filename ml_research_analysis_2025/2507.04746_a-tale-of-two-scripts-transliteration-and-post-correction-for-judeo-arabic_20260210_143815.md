---
ver: rpa2
title: 'A Tale of Two Scripts: Transliteration and Post-Correction for Judeo-Arabic'
arxiv_id: '2507.04746'
source_url: https://arxiv.org/abs/2507.04746
tags:
- arabic
- transliteration
- hebrew
- dotted
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of transliterating Judeo-Arabic\
  \ texts\u2014written in Hebrew script\u2014into Arabic script, enabling access for\
  \ Arabic readers and compatibility with Arabic NLP tools. The authors propose a\
  \ two-step method: (1) rule-based character mapping for initial transliteration,\
  \ and (2) post-correction using state-of-the-art Arabic grammatical error correction\
  \ models to fix orthographic and grammatical errors."
---

# A Tale of Two Scripts: Transliteration and Post-Correction for Judeo-Arabic

## Quick Facts
- arXiv ID: 2507.04746
- Source URL: https://arxiv.org/abs/2507.04746
- Reference count: 9
- Primary result: 90.4% word-level accuracy on Judeo-Arabic transliteration using rule-based mapping + GEC post-correction

## Executive Summary
This paper addresses the challenge of transliterating Judeo-Arabic texts—written in Hebrew script—into Arabic script, enabling access for Arabic readers and compatibility with Arabic NLP tools. The authors propose a two-step method: (1) rule-based character mapping for initial transliteration, and (2) post-correction using state-of-the-art Arabic grammatical error correction models to fix orthographic and grammatical errors. The approach requires no training on Judeo-Arabic data and is evaluated on the first publicly available benchmark dataset. Results show that the system significantly outperforms previous work, achieving 90.4% word-level accuracy when post-correction is applied to dotted input, and enables effective morphosyntactic tagging and machine translation on transliterated texts.

## Method Summary
The method consists of a rule-based character mapping system (CharMapper) that converts Hebrew script to Arabic script using a predefined mapping table, followed by post-correction using pretrained Arabic grammatical error correction models. The transliteration handles seven Hebrew letters with upper dot diacritics that represent Arabic letters without Hebrew phonological equivalents. Post-correction is performed using either traditional GEC models (Seq2Seq, SWEET) or large language models (GPT-3.5-turbo, GPT-4o). The system processes texts by first identifying and excluding Hebrew words (biblical quotations), then applying character mapping, and finally applying GEC to correct orthographic and grammatical errors.

## Key Results
- CharMapper achieves 64.9% word-level accuracy on dotted input and 53.0% on dotless input
- Post-correction with GPT-4o improves accuracy to 90.4% for dotted input and 78.4% for dotless input
- Morphological tagging accuracy reaches 93.9% (POS) and 91.5% (lemma) for CharMapper + GPT-4o on dotted input
- MT BLEU improves from 24.1 to 28.3 with post-correction on dotted input

## Why This Works (Mechanism)

### Mechanism 1: Upper Dot Diacritic Disambiguation
Preserving the upper dot diacritic in Hebrew script substantially improves transliteration accuracy. The Hebrew abjad (22 letters) maps incompletely to Arabic (28+ letters). Seven Hebrew letters use an upper dot diacritic to represent Arabic letters without Hebrew phonological equivalents (e.g., gimel with dot → ghayn). Retaining this signal enables correct disambiguation during rule-based mapping. Assumption: The upper dot usage is consistent within a given JA text (validated for Al-Khazari but may not hold across all JA corpora). Evidence: CharMapper correctly maps 71.8% relative accuracy for dotted words vs. 63.5% for undotted; performance drops by ~12 absolute points without post-correction.

### Mechanism 2: GEC Post-Correction as Orthographic Repair
Applying pretrained Modern Standard Arabic grammatical error correction models to transliteration output recovers orthographic and morphological errors without JA-specific training. Rule-based transliteration produces orthographically invalid Arabic (missing hamzas, incorrect letter variants). GEC models—trained on native Arabic error patterns—correct these by leveraging learned morphological and contextual knowledge. Assumption: Errors from transliteration resemble errors made by native Arabic speakers (typographic, dialectal). Evidence: GPT-4o achieves highest correction quality, likely due to broader exposure to Arabic text variation during pretraining; recovers 93.4% → 72.1% accuracy on words containing dotted letters when dots are removed.

### Mechanism 3: Script Normalization Enables Tool Compatibility
Converting JA to Arabic script activates existing Arabic NLP pipelines (tagging, MT) that would fail on Hebrew-script input. Standard Arabic NLP tools assume Arabic script tokenization and morphology. Transliteration creates linguistically valid Arabic surface forms, allowing pretrained models to generalize from MSA training data to JA content. Assumption: JA morphosyntax is sufficiently similar to MSA/dialects that Arabic-trained models transfer effectively. Evidence: Morphological tagging accuracy reaches 93.9% (POS) and 91.5% (lemma); MT BLEU improves from 24.1 → 28.3 with post-correction.

## Foundational Learning

- **Abjad orthography and letter-to-phoneme mapping**: Understanding that Hebrew and Arabic abjads have incomplete one-to-one correspondences is essential for grasping why transliteration is ambiguous and why diacritics matter. Quick check: Why does Hebrew gimel (ג) need an upper dot variant to represent Arabic ghayn (غ)?

- **Grammatical Error Correction (GEC) as sequence-to-sequence or text-editing**: The post-correction stage reformulates transliteration repair as a GEC task; knowing how GEC models are trained (edit operations, precision-weighted loss) explains why they can over-correct. Quick check: What does the F0.5 metric prioritize over recall, and why is this relevant for error correction?

- **Code-switching in historical texts**: JA texts embed Hebrew citations and religious terms; distinguishing transliteration targets from translation targets is a core preprocessing challenge. Quick check: Why were Hebrew words excluded from transliteration evaluation but retained during GEC evaluation?

## Architecture Onboarding

- **Component map**: Preprocessor → CharMapper → Post-Corrector → Evaluator
- **Critical path**: CharMapper → Post-Correction → Downstream Task
  - If post-correction is skipped, accuracy drops 26 absolute points (64.9% → 90.4% for dotted)
  - If dots are stripped, accuracy drops 12 points (64.9% → 53.0%) before post-correction
- **Design tradeoffs**:
  - Dotted vs. dotless preprocessing: Dotted input yields higher accuracy but requires source texts to preserve diacritics; dotless requires stronger GEC models
  - Specialized vs. general GEC: Seq2Seq+MG (139M params) is efficient and reproducible; GPT-4o achieves best results but is closed-source
  - Hebrew handling: Current system does not explicitly translate code-switched Hebrew; future work requires code-switching detection
- **Failure signatures**:
  - Alif-Hamza errors (18% of residual errors): GEC models mishandle hamza placement on Alif variants
  - Unnecessary changes (21%): GEC over-corrects valid transliterations, often replacing words with near-synonyms
  - Hebrew false cognates (46%): GEC mistranslates Hebrew words as Arabic false cognates
- **First 3 experiments**:
  1. Reproduce CharMapper with your own mapping table: Implement the Table 2 mapping, test on a 100-sentence subset from the released dataset, verify 64.9% baseline on dotted input
  2. Ablate post-correction models: Compare Seq2Seq vs. SWEET vs. GPT-4o on the same transliteration output; measure F0.5 and accuracy to confirm GPT-4o advantage (59.5 → 63.1 F0.5 gain)
  3. Test dot recovery on held-out text: Remove dots from a dotted test set, run CharMapper + GPT-4o, quantify per-letter recovery rates to identify which dot types (j/gh, k/kh) remain hardest

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can models effectively combine transliteration with selective translation to handle Hebrew segments embedded in Judeo-Arabic?
- Basis in paper: [explicit] The authors state a plan to "develop models that combine transliteration with selective translation, converting non-Arabic segments into Arabic script."
- Why unresolved: The current pipeline treats Hebrew segments as unknowns or translates them independently, failing to address cases where Hebrew roots are morphologically integrated into Arabic.
- Evidence: A model that correctly processes and conjugates integrated Hebrew roots (e.g., *tγyr*) using Arabic verbal systems.

### Open Question 2
- Question: Can the proposed pipeline maintain performance across diverse Judeo-Arabic texts with varying orthographic conventions?
- Basis in paper: [explicit] The authors intend to "investigate alternative modeling strategies... across diverse Judeo-Arabic texts."
- Why unresolved: The current study relied on a single text (Al-Khazari) specifically chosen for its consistent diacritic usage; the rule-based mapper may not generalize to texts with different dotting conventions.
- Evidence: Evaluation results on heterogeneous Judeo-Arabic corpora, such as works by Maimonides or Saadia Gaon, demonstrating robust accuracy despite orthographic inconsistency.

### Open Question 3
- Question: Does script transliteration improve performance on Named Entity Recognition (NER) for Judeo-Arabic texts?
- Basis in paper: [explicit] The authors list plans to "assess the utility of transliteration on other downstream Arabic NLP tasks, such as named entity recognition."
- Why unresolved: The paper demonstrated downstream utility only for morphosyntactic tagging and machine translation, leaving NER unverified.
- Evidence: Comparative benchmark scores on NER tasks run on the original Hebrew script versus the transliterated Arabic script.

## Limitations
- Data Scope Uncertainty: The evaluation relies entirely on one Judeo-Arabic text (Al-Khazari), raising questions about generalizability to other JA dialects or genres.
- GEC Model Generalization: GPT-4o's performance may vary with model updates or different API versions, limiting reproducibility.
- Code-Switching Handling: The current approach excludes Hebrew code-switched segments from transliteration evaluation but retains them during GEC evaluation, where models sometimes mistranslate Hebrew words as Arabic false cognates.

## Confidence

**High Confidence**: The core mechanism of using upper dot diacritics for disambiguation is well-supported by the 12-point accuracy difference between dotted and dotless inputs, and the 71.8% vs 63.5% relative accuracy for dotted vs undotted words.

**Medium Confidence**: The effectiveness of GEC post-correction is demonstrated but depends heavily on model choice and may not generalize to other JA texts or GEC model versions. The 21% unnecessary change rate indicates room for improvement.

**Low Confidence**: Claims about downstream task performance (morphosyntactic tagging, machine translation) are based on a single text and may not transfer to other JA genres or dialects.

## Next Checks
1. **Cross-Dataset Validation**: Test the complete transliteration pipeline on at least two additional Judeo-Arabic texts from different authors/time periods to verify the upper dot diacritic assumption and GEC effectiveness across diverse JA corpora.

2. **Error Type Analysis**: Conduct a detailed linguistic analysis of the 21% unnecessary changes and hamza placement errors to identify whether these represent systematic issues that could be addressed through rule refinement or specialized training data.

3. **Code-Switching Robustness Test**: Evaluate the system's performance on JA texts with varying levels of Hebrew code-switching to quantify the impact of mistranslated Hebrew words and develop explicit Hebrew detection/preprocessing steps.