---
ver: rpa2
title: Learning from positive and unlabeled examples -Finite size sample bounds
arxiv_id: '2507.07354'
source_url: https://arxiv.org/abs/2507.07354
tags:
- learning
- such
- sample
- distribution
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the statistical complexity of positive-unlabeled\
  \ (PU) learning under minimal assumptions. Unlike prior work, it does not assume\
  \ the class prior \u03B1 is known to the learner."
---

# Learning from positive and unlabeled examples -Finite size sample bounds

## Quick Facts
- arXiv ID: 2507.07354
- Source URL: https://arxiv.org/abs/2507.07354
- Reference count: 40
- Primary result: Novel finite sample complexity bounds for PU learning without requiring knowledge of class prior α

## Executive Summary
This paper analyzes the statistical complexity of positive-unlabeled (PU) learning under minimal assumptions, departing from prior work that requires knowledge of the class prior α. The authors provide comprehensive finite sample complexity bounds across multiple PU learning scenarios including realizable and agnostic cases, with different data generating assumptions (SCAR, SAR, PCS, APDS). The work establishes lower bounds for positive and unlabeled samples based on combinatorial parameters, and extends generalization guarantees to agnostic PU learning when α is unknown.

## Method Summary
The authors develop theoretical bounds by analyzing different data generating scenarios in PU learning. For realizable cases, they derive sample complexity bounds for positive and unlabeled samples separately, with the unlabeled bound based on a novel combinatorial parameter called claw number. For agnostic cases, they establish generalization bounds that depend on the approximation error and the unknown class prior α. The analysis covers multiple data generation assumptions including SCAR (selected completely at random), SAR (selected at random), PCS (positive and chosen randomly), and APDS (arbitrary positive and distinguished randomly) setups.

## Key Results
- Lower bounds on sample complexity for positive examples nearly matching Liu et al. (2002) in the SCAR setup
- Novel lower bounds for unlabeled samples based on combinatorial claw number parameter
- First finite sample complexity bounds for SAR and PCS setups without prior knowledge of α
- Generalization bounds for agnostic PU learning when α is unknown, achieving error at most max(α,1−α)/min(α,1−α) times the approximation error
- First generalization bounds for APDS setup

## Why This Works (Mechanism)
The paper's theoretical framework works by carefully analyzing the statistical properties of PU learning under different assumptions about how positive and unlabeled examples are generated. By relaxing the requirement to know the class prior α, the authors develop bounds that depend on alternative parameters like claw number and approximation error. The mechanism leverages the fact that in PU learning, unlabeled examples contain both positive and negative examples, allowing for statistical inference without explicit negative labels.

## Foundational Learning
- **SCAR assumption**: Positive examples are selected completely at random from the positive distribution. Why needed: Provides baseline for analyzing PU learning when positive sampling is unbiased. Quick check: Verify that positive and unlabeled distributions satisfy αP = Q_P.
- **Claw number**: Combinatorial parameter measuring the maximum number of "claws" in a hypothesis class. Why needed: Enables derivation of lower bounds for unlabeled samples. Quick check: Calculate claw number for simple hypothesis classes like intervals on the line.
- **Agnostic learning**: Framework where no perfect classifier exists. Why needed: Real-world scenarios rarely satisfy realizability assumptions. Quick check: Compare generalization bounds with and without approximation error terms.
- **Class prior α**: Proportion of positive examples in the population. Why needed: Fundamental parameter affecting sample complexity in PU learning. Quick check: Test sensitivity of bounds to different values of α.
- **Generalization error**: Difference between true and empirical error. Why needed: Core metric for evaluating learning algorithm performance. Quick check: Verify that bounds scale appropriately with sample size.

## Architecture Onboarding
- **Component map**: Hypothesis class → Empirical risk minimization → Generalization bounds (SCAR/SAR/PCS/APDS variants)
- **Critical path**: Data generation assumption → Sample complexity bound derivation → Generalization guarantee
- **Design tradeoffs**: Known vs unknown α (simplicity vs generality), realizable vs agnostic (optimism vs realism)
- **Failure signatures**: Violation of data generation assumptions, hypothesis class too complex relative to sample size
- **3 first experiments**:
  1. Calculate claw numbers for benchmark hypothesis classes
  2. Verify sample complexity bounds empirically for SCAR setup with synthetic data
  3. Test agnostic generalization bounds with varying approximation errors

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on combinatorial parameters like claw number whose practical significance in real-world PU learning remains unclear
- Bounds assume access to finite samples without addressing computational complexity of achieving these sample complexities
- Generalization bounds for agnostic PU learning when α is unknown depend heavily on approximation error without concrete examples of tight cases
- Extension to APDS setup lacks empirical validation to confirm theoretical predictions

## Confidence
High confidence: Realizable case bounds under SCAR assumption matching Liu et al. (2002), generalization bounds for agnostic PU learning with known α.
Medium confidence: Novel lower bounds based on claw number, bounds for SAR and PCS setups without prior knowledge, generalization bounds for APDS setup.
Low confidence: Practical implications of claw number-based bounds, tightness of agnostic bounds when α is unknown, computational feasibility of achieving theoretical sample complexities.

## Next Checks
1. Implement empirical studies comparing claw number-based bounds against alternative PU learning methods across benchmark datasets to assess practical relevance.
2. Construct specific hypothesis classes where the agnostic generalization bound with unknown α can be shown to be tight or nearly tight.
3. Develop and test algorithmic approaches that achieve the theoretical sample complexities for SAR and PCS setups, particularly focusing on computational efficiency.