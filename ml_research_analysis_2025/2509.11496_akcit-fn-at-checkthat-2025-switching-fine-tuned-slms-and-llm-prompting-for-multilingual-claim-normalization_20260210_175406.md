---
ver: rpa2
title: 'AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for
  Multilingual Claim Normalization'
arxiv_id: '2509.11496'
source_url: https://arxiv.org/abs/2509.11496
tags:
- languages
- language
- data
- claim
- zero-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents AKCIT-FN's participation in the CLEF-2025 CheckThat!
  Task 2 on multilingual claim normalization, addressing the challenge of converting
  informal social media posts into concise, self-contained statements across 20 languages.
---

# AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization

## Quick Facts
- **arXiv ID:** 2509.11496
- **Source URL:** https://arxiv.org/abs/2509.11496
- **Reference count:** 39
- **Primary result:** Top three METEOR scores in 15/20 languages; 2nd in 8 (5 zero-shot), 3rd in 7

## Executive Summary
This paper presents AKCIT-FN's participation in the CLEF-2025 CheckThat! Task 2 on multilingual claim normalization, addressing the challenge of converting informal social media posts into concise, self-contained statements across 20 languages. The approach employed a dual-strategy framework: fine-tuning Small Language Models (SLMs) for 13 supervised languages with training data, and using Large Language Model (LLM) prompting for 7 zero-shot languages without training data. Initial development and experimentation focused on Portuguese before extending to other languages. The methodology included data cleaning to handle repetitive content and cross-split deduplication, and leveraged both monolingual and multilingual encoder-decoder architectures for fine-tuning, alongside few-shot and zero-shot prompting with various LLMs. Results showed strong performance with podium finishes (top three) in 15 of 20 languages, including second place in eight languages (five of which were zero-shot) and third place in seven. For Portuguese, the system achieved an average METEOR score of 0.5290, ranking third. Fine-tuned SLMs excelled in supervised settings, while LLM prompting demonstrated effective generalization for zero-shot languages.

## Method Summary
The methodology employed a dual-strategy framework: (1) Fine-tuning encoder-decoder SLMs for 13 supervised languages using a comprehensive hyperparameter search (epochs {3,5,10,20}, learning rates {3e-3,1e-3,5e-4}, warm-up 90, batch 32, beam 15, dynamic max length), and (2) Zero-shot LLM prompting for 7 languages without training data. The approach began with Portuguese development before scaling to other languages. Critical preprocessing involved Algorithm 1 to handle repetitive content (removing "None" suffixes, deduplicating triplicated patterns) and cross-split deduplication prioritizing test > dev > train retention. Language-specific models were used for supervised tasks (Mono PTT5 for Portuguese, Varta T5 for Indic languages, Flan-T5 for English), while few-shot and zero-shot prompting used GPT-4o mini and Qwen 2.5 Instruct (3B). The zero-shot prompt template was translated from English to target languages.

## Key Results
- Achieved podium finishes (top three) in 15 of 20 languages
- Second place in eight languages (five of which were zero-shot)
- Third place in seven languages
- Portuguese METEOR score: 0.5290 (third place)

## Why This Works (Mechanism)
The dual-strategy approach effectively addresses the data availability challenge across 20 languages. Fine-tuned SLMs leverage supervised training data to achieve high precision on languages with sufficient examples, while LLM prompting enables effective zero-shot generalization for low-resource languages. The comprehensive preprocessing handles the unique noise patterns in social media data, particularly the triplicated content and "None" suffixes that could otherwise corrupt model training. The choice of language-specific SLMs over multilingual models suggests that specialized architectures better capture language-specific nuances in normalization tasks.

## Foundational Learning
- **Claim normalization concept**: Converting informal social media posts to concise, self-contained statements - needed for misinformation detection pipelines; quick check: can the normalized claim stand alone as a verifiable statement
- **Encoder-decoder architectures**: Seq2seq models for text generation - needed for transforming posts to normalized claims; quick check: input-output sequence length and structure
- **Few-shot vs zero-shot prompting**: In-context learning with limited vs no examples - needed for handling zero-shot languages; quick check: template format and example selection strategy
- **METEOR metric**: Evaluation metric for text generation quality - needed for task-specific performance measurement; quick check: punctuation removal during evaluation preprocessing
- **Cross-split deduplication**: Preventing data leakage between train/dev/test sets - needed for valid model evaluation; quick check: triplicated content patterns in dataset
- **Language-specific vs multilingual models**: Tradeoff between specialization and generalization - needed for optimizing across diverse languages; quick check: performance comparison on dev sets

## Architecture Onboarding

**Component map:** Data Cleaning -> Preprocessing -> SLM Fine-tuning (supervised) / LLM Prompting (zero-shot) -> Evaluation

**Critical path:** Raw social media posts → Algorithm 1 preprocessing → (SLM fine-tuning or LLM prompting) → Normalized claim outputs → METEOR evaluation

**Design tradeoffs:** Language-specific SLMs vs multilingual models (specialization vs efficiency), few-shot vs zero-shot prompting (resource requirement vs generalization), complex preprocessing vs raw data usage (accuracy vs simplicity)

**Failure signatures:** Overfitting on small datasets (large dev/train METEOR gap), unhandled triplicated content in outputs, empty LLM API responses, poor cross-linguistic generalization

**First experiments to run:**
1. Implement Algorithm 1 preprocessing on sample data to verify handling of "None" suffixes and triplicated content patterns
2. Fine-tune a small encoder-decoder model (e.g., Flan-T5 base) on English training data with specified hyperparameters
3. Test zero-shot prompting with GPT-4o mini on held-out supervised language data using the translated prompt template

## Open Questions the Paper Calls Out
The paper explicitly states a key limitation: "A limitation of our work, however, is the lack of a qualitative error analysis or a discussion of failure cases, which would be a valuable direction for future investigation." The authors report aggregate METEOR scores but do not manually inspect or categorize the types of errors (e.g., hallucination, loss of nuance, grammatical errors) produced by either the fine-tuned SLMs or the LLM prompting across different linguistic families.

## Limitations
- Final hyperparameter configurations per language are not specified
- Few-shot example selection strategy and shot count details are incomplete
- Exact train/dev/test splits after deduplication are not provided
- No qualitative error analysis or discussion of failure cases

## Confidence

**High confidence:** Overall dual-strategy framework effectiveness and strong results across 15/20 languages
**Medium confidence:** Preprocessing methodology and hyperparameter search spaces
**Low confidence:** Exact reproducibility due to unspecified hyperparameter choices and incomplete experimental details

## Next Checks
1. Implement and test Algorithm 1 preprocessing on sample data to verify correct handling of "None" suffixes and triplicated content patterns before training
2. Conduct ablation study comparing monolingual vs multilingual SLM fine-tuning approaches on a subset of supervised languages
3. Test zero-shot LLM prompting with both GPT-4o mini and Qwen 2.5 Instruct (3B) on held-out supervised language data to verify effective generalization performance