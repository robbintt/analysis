---
ver: rpa2
title: 'Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go'
arxiv_id: '2511.10868'
source_url: https://arxiv.org/abs/2511.10868
tags:
- test
- unit
- code
- fine-tuning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Go-UT-Bench, a fine-tuning dataset of 5,264
  {code, unit test} pairs from 10 permissively licensed Go repositories spanning domains
  like cloud infrastructure, blockchain, and API development. To address the scarcity
  of Go-specific datasets, the authors curate and release a reproducible dataset with
  metadata and commit hashes.
---

# Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go

## Quick Facts
- arXiv ID: 2511.10868
- Source URL: https://arxiv.org/abs/2511.10868
- Reference count: 15
- Primary result: Fine-tuned models using Go-UT-Bench achieve 81.9% win rate over base models for Go unit test generation

## Executive Summary
Go-UT-Bench is a fine-tuning dataset containing 5,264 {code, unit test} pairs from 10 permissively licensed Go repositories across domains including cloud infrastructure, blockchain, and API development. The dataset addresses the scarcity of Go-specific training data for LLM-based unit test generation by providing reproducible, curated examples with metadata and commit hashes. The authors evaluate their dataset by fine-tuning two LLM families—DeepSeek-Coder-V2-Lite-Instruct (using LoRA) and Llama-3.2-3B-Instruct (full fine-tuning)—demonstrating that fine-tuned models outperform base models in over 75% of tasks, with DeepSeek-Coder-V2-Lite-Instruct achieving an 81.9% win rate.

## Method Summary
The authors curate Go-UT-Bench by collecting permissively licensed Go code and corresponding unit tests from 10 repositories spanning diverse domains. Each pair includes metadata and commit hashes for reproducibility. The dataset is then used to fine-tune two LLM architectures: DeepSeek-Coder-V2-Lite-Instruct with LoRA adapters and Llama-3.2-3B-Instruct through full fine-tuning. Performance is evaluated by comparing fine-tuned models against their base counterparts on unit test generation tasks, measuring win rates across different test scenarios.

## Key Results
- Fine-tuned models outperform base models in over 75% of unit test generation tasks
- DeepSeek-Coder-V2-Lite-Instruct achieves an 81.9% win rate when fine-tuned on Go-UT-Bench
- LoRA fine-tuning shows competitive performance compared to full fine-tuning approaches

## Why This Works (Mechanism)
The effectiveness of Go-UT-Bench stems from its domain-specific focus on Go unit testing patterns. By curating examples from diverse Go repositories, the dataset captures idiomatic testing approaches, error handling patterns, and assertion styles specific to Go development. The fine-tuning process allows models to internalize these patterns, reducing the need for extensive prompting and improving generation quality for Go-specific test cases.

## Foundational Learning
- Go testing framework (testing package) - why needed: Understanding Go's built-in testing package is essential for generating idiomatic tests; quick check: Can identify testify vs standard library patterns
- Table-driven tests in Go - why needed: Table-driven tests are a Go idiom; quick check: Recognizes when to generate multiple test cases in table format
- Mock and stub patterns in Go - why needed: Testing often requires mocking dependencies; quick check: Identifies common Go mocking libraries and patterns
- Go-specific error handling - why needed: Go's explicit error handling requires special test consideration; quick check: Can generate tests that properly assert error conditions
- Integration vs unit testing distinction - why needed: Dataset focuses on unit tests; quick check: Differentiates test types and their appropriate scope

## Architecture Onboarding

**Component map**: Code repositories -> Dataset curation -> {code, unit test} pairs -> Model fine-tuning -> Performance evaluation

**Critical path**: Repository selection → Test extraction → Pair curation → Fine-tuning → Evaluation → Results analysis

**Design tradeoffs**: The dataset prioritizes permissively licensed content over completeness, limiting size to 5,264 pairs but ensuring legal usability. Fine-tuning uses both LoRA (parameter-efficient) and full fine-tuning approaches to compare resource efficiency versus performance.

**Failure signatures**: Poor test generation quality may indicate insufficient coverage of specific Go patterns, inadequate fine-tuning duration, or domain mismatch between training repositories and evaluation tasks. Models may overfit to specific repository styles if diversity is insufficient.

**First experiments**: 1) Generate unit tests for simple Go functions using base vs fine-tuned models; 2) Compare test coverage metrics between generated and human-written tests; 3) Evaluate performance on Go code from repositories not included in the training set.

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset size (5,264 pairs) may not capture full diversity of Go testing patterns across all domains
- Evaluation focuses solely on unit test generation, leaving open questions about performance on integration tests or other Go coding tasks
- Comparison between LoRA and full fine-tuning uses different base model families, introducing architectural differences that confound results

## Confidence
- Dataset creation methodology: High
- Performance improvements: Medium (limited evaluation tasks and confounding factors in model comparisons)
- Novelty claim of being "first comprehensive fine-tuning dataset": Low

## Next Checks
1. Test whether performance gains persist when using the same base model family for both LoRA and full fine-tuning comparisons
2. Evaluate model performance on integration tests and other non-unit-test Go coding tasks to assess generalization
3. Validate dataset coverage by sampling additional Go repositories from the same domains to check for representation gaps