---
ver: rpa2
title: 'Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models'
arxiv_id: '2601.18734'
source_url: https://arxiv.org/abs/2601.18734
tags:
- teacher
- student
- on-policy
- reasoning
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving reasoning capabilities
  in large language models (LLMs) while maintaining computational efficiency. It introduces
  On-Policy Self-Distillation (OPSD), a novel framework where a single LLM acts as
  both teacher and student by conditioning on different contexts - the teacher has
  access to ground-truth solutions while the student only sees the problem.
---

# Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models
## Quick Facts
- arXiv ID: 2601.18734
- Source URL: https://arxiv.org/abs/2601.18734
- Reference count: 16
- The paper introduces On-Policy Self-Distillation (OPSD) framework that achieves 4-8× token efficiency improvement over reinforcement learning for mathematical reasoning tasks

## Executive Summary
This paper addresses the challenge of improving reasoning capabilities in large language models (LLMs) while maintaining computational efficiency. The authors introduce On-Policy Self-Distillation (OPSD), a novel framework where a single LLM acts as both teacher and student by conditioning on different contexts - the teacher has access to ground-truth solutions while the student only sees the problem. The model samples its own trajectories and receives dense token-level supervision from its teacher version through minimizing per-token divergence. Evaluated on competition-level mathematical reasoning benchmarks, OPSD achieves 4-8× token efficiency compared to reinforcement learning methods like GRPO while outperforming both traditional knowledge distillation and supervised fine-tuning approaches.

## Method Summary
The On-Policy Self-Distillation (OPSD) framework enables a single LLM to serve as both teacher and student by conditioning on different contexts. The teacher version receives ground-truth solutions as context, while the student only sees the problem prompt. During training, the model samples its own trajectories and receives dense token-level supervision from the teacher version through minimizing per-token divergence. This approach differs from traditional knowledge distillation by using a single model with different conditioning rather than separate teacher and student models. The method leverages the teacher's ground-truth-augmented context to provide high-quality supervision while maintaining the student's problem-solving autonomy.

## Key Results
- Achieves 4-8× token efficiency improvement compared to reinforcement learning methods like GRPO
- Outperforms both traditional knowledge distillation and supervised fine-tuning approaches on mathematical reasoning benchmarks
- Demonstrates superior performance across multiple model scales (1.7B to 8B parameters), with larger models benefiting more from the self-distillation approach

## Why This Works (Mechanism)
The method works by creating a controlled gap between teacher and student capabilities through context conditioning. By giving the teacher access to ground-truth solutions while restricting the student to only the problem prompt, the framework creates a natural teacher-student dynamic within a single model. The dense token-level supervision from the teacher provides high-quality learning signals that guide the student's reasoning process more effectively than sparse rewards from reinforcement learning. This approach combines the benefits of self-training with the efficiency of supervised learning.

## Foundational Learning
- **Knowledge Distillation**: A training paradigm where a smaller student model learns from a larger teacher model - needed for transferring reasoning capabilities; quick check: teacher provides supervision signals to student
- **Reinforcement Learning for LLMs**: Methods that use rewards to guide model behavior - needed as baseline comparison; quick check: uses environmental feedback rather than direct supervision
- **On-policy Learning**: Training where the agent samples its own trajectories - needed for self-distillation; quick check: agent learns from its own generated data
- **Token-level Supervision**: Dense supervision at individual token level rather than sequence-level - needed for precise learning signals; quick check: per-token loss computation
- **Context Conditioning**: Modifying model behavior by providing different context - needed for teacher-student separation; quick check: same model with different input contexts

## Architecture Onboarding
- **Component Map**: Problem prompt -> Student model -> Solution trajectory -> Teacher model (with ground truth) -> Token-level supervision -> Loss computation -> Model update
- **Critical Path**: The core training loop where the student generates solutions, the teacher provides supervision with ground truth context, and the model updates based on token-level divergence
- **Design Tradeoffs**: Single-model approach reduces complexity but requires careful context management; ground-truth dependency limits scalability to domains without abundant solutions
- **Failure Signatures**: Poor performance may indicate inadequate ground-truth supervision quality, improper context conditioning, or insufficient diversity in sampled trajectories
- **First Experiments**: 1) Validate teacher-student context separation by comparing outputs with and without ground truth, 2) Test token-level supervision effectiveness against sequence-level alternatives, 3) Measure efficiency gains on simple reasoning tasks before scaling to competition problems

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- The methodology's reliance on ground-truth solutions for teacher conditioning presents a practical limitation - obtaining such solutions at scale remains challenging for many reasoning domains
- The paper's focus on mathematical competition problems, while rigorous, raises questions about generalization to other reasoning tasks
- The claim of 4-8× token efficiency assumes equivalent quality metrics and doesn't account for potential differences in solution quality that might justify additional tokens in RL approaches

## Confidence
- 4-8× token efficiency improvement over RL: High confidence (well-supported by experimental results)
- Larger models benefit more from self-distillation: Medium confidence (consistent trends but relationship beyond tested scales unclear)
- Outperforming traditional knowledge distillation and supervised fine-tuning: High confidence (well-supported within tested domain)

## Next Checks
1. Test OPSD performance on non-mathematical reasoning tasks to evaluate domain generalization
2. Conduct ablation studies removing the ground-truth teacher access to quantify its contribution versus the distillation mechanism itself
3. Evaluate the method's performance on datasets where ground-truth solutions are unavailable to assess real-world applicability