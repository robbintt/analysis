---
ver: rpa2
title: Closing the Sim2Real Performance Gap in RL
arxiv_id: '2510.17709'
source_url: https://arxiv.org/abs/2510.17709
tags:
- policy
- real-world
- parameters
- in-sim
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a bi-level RL framework to close the Sim2Real
  performance gap by directly optimizing simulator parameters to maximize real-world
  policy performance, rather than using simulator accuracy as a proxy. The method
  derives sensitivity gradients for in-simulation policy training via Stochastic Policy
  Gradient methods, enabling outer-level RL to adapt simulation models and reward
  functions using real-world data.
---

# Closing the Sim2Real Performance Gap in RL

## Quick Facts
- **arXiv ID**: 2510.17709
- **Source URL**: https://arxiv.org/abs/2510.17709
- **Reference count**: 29
- **Primary result**: Introduces bi-level RL framework that optimizes simulator parameters directly for real-world policy performance, closing the Sim2Real gap.

## Executive Summary
This paper introduces a bi-level reinforcement learning framework to address the Sim2Real performance gap by directly optimizing simulator parameters to maximize real-world policy performance, rather than using simulation accuracy as a proxy. The method derives sensitivity gradients for in-simulation policy training via Stochastic Policy Gradient methods, enabling outer-level RL to adapt simulation models and reward functions using real-world data. Experiments on discrete and continuous MDPs validate the approach, showing convergence of the outer-level RL in five randomized trials. The framework theoretically achieves Sim2Real optimality and overcomes objective mismatch issues in model-based RL.

## Method Summary
The method frames Sim2Real transfer as a bi-level RL problem where the inner loop trains a policy in simulation using Stochastic Policy Gradient methods, while the outer loop adapts simulation parameters to maximize real-world performance. The key innovation is deriving implicit function sensitivities that allow gradient-based optimization of simulator parameters through the converged inner-loop solution. The framework computes critic sensitivities and Markov chain sensitivities from sampled trajectories, then uses these to estimate gradients for updating simulator parameters. This approach achieves Sim2Real optimality by finding simulator configurations that make the optimal simulated policy also optimal in the real world.

## Key Results
- Converged outer-level RL in five randomized trials on both discrete (3-state, 2-action MDP) and continuous (LQR) domains
- Achieved normalized return approaching 1.0 in both domains, indicating successful Sim2Real transfer
- Demonstrated theoretical proof of Sim2Real optimality under the bi-level optimization framework
- Showed convergence speed of 6k iterations for discrete and 200 iterations for continuous settings

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: A bi-level RL framework can close the Sim2Real gap by optimizing simulator parameters directly for real-world policy performance rather than proxy metrics.
- **Mechanism**: The outer-level RL treats simulator parameters θ as actions, receiving reward signals from real-world policy performance. The inner-level trains π_φ in simulation conditioned on θ. This creates a feedback loop: θ → π_φ(θ) → real-world return → θ update. The key insight is that Sim2Real optimality (π_φ* = π*) can be achieved even with imperfect simulators by finding θ* that satisfies argmax_a Q̂*_θ(s,a) = argmax_a Q*(s,a) for all states.
- **Core assumption**: The solution θ* to the outer-level optimization is not unique, allowing multiple simulator configurations to yield equivalent real-world optimal policies. This relies on the theoretical result that prediction accuracy doesn't correlate with policy performance for stochastic systems.
- **Evidence anchors**:
  - [abstract]: "We frame this problem as a bi-level RL framework: the inner-level RL trains a policy purely in simulation, and the outer-level RL adapts the simulation model and in-sim reward parameters to maximize real-world performance of the in-sim policy."
  - [section 4]: "To close the Sim2Real performance gap, we propose framing the identification of the optimal simulator parameters θ* satisfying the necessary condition for Sim2Real optimality in (3) as a policy gradient RL problem."
  - [corpus]: Related papers address Sim2Real gaps via domain randomization (paper 21416) and visual adaptation (paper 7489), but don't employ bi-level optimization.
- **Break condition**: Real-world data collection budget insufficient for outer-level RL convergence; simulator parameter space dimensionality too high relative to available feedback signal.

### Mechanism 2
- **Claim**: Implicit differentiation of the converged stochastic policy gradient enables gradient-based optimization of simulator parameters through the inner-loop solution.
- **Mechanism**: At inner-loop convergence (φ̂(φ, θ) = 0), policy parameters φ become an implicit function of θ. The Implicit Function Theorem yields ∇_θ φ = -(∇_φ φ̂)^(-1) ∇_θ φ̂. This sensitivity ∇_θ φ propagates through the outer-level gradient via chain rule: ∇_θ log π_φ = (∇_θ φ)(∇_φ log π_φ). Theorem 1 decomposes this into tractable components: critic sensitivities (∇_θ Q̂, ∇_φ Q̂) and Markov chain distribution sensitivities.
- **Core assumption**: Inner-loop policy converges to local optimality (φ̂ = 0). The paper acknowledges non-convergence adds noise but is tolerable except near optimal θ*. The Jacobian ∇_φ φ̂ must be invertible.
- **Evidence anchors**:
  - [section 4.1]: "Under local convergence, i.e., φ̂(φ, θ) = 0, SPG provides a locally optimal policy π_φ. We consider this locally converged φ in the rest of our analysis."
  - [theorem 1]: Full sensitivity expression with critic recursions (Eq. 13-14) and distribution gradient approximations (Eq. 15-16).
  - [corpus]: No corpus papers employ implicit differentiation of SPG for Sim2Real.
- **Break condition**: Jacobian singularity when policy is at a saddle point or flat region; numerical instability in iterative Jacobian inversion.

### Mechanism 3
- **Claim**: Markov chain sensitivity can be estimated from sampled trajectories using accumulated log-derivative weights.
- **Mechanism**: Lemma 1 shows ∂E_{s~ρ}[η(s_k,a_k)] = E[η(s_k,a_k) · (∂log π_φ(a_k|s_k) + Σ_{i<k} ∂log f_θ(s_{i+1}|s_i,a_i))]. Practically, this accumulates as W_k = W_{k-1} + ∂log f_θ(s_k|s_{k-1},a_{k-1}) during trajectory generation. The gradient estimate becomes (1/nN) Σ η(s_k^j,a_k^j) ⊗ W_k^j. This avoids explicit density computation.
- **Core assumption**: Differentiable simulator f_θ and policy π_φ. The paper notes differentiable simulators are becoming standard (contact robotics, discrete spaces handled).
- **Evidence anchors**:
  - [section 5]: "Using an on-policy, in-simulation trajectory of length 1000 drawn from the DP solution, we compute the Markov-chain sensitivities (Eq. 15 and 16)."
  - [figure 3]: Convergence curves show 5 randomized trials reaching normalized return ~1.0 in both discrete (6k iterations) and continuous (200 iterations) settings.
  - [corpus]: Paper 68869 discusses closed-loop simulation evaluation but not gradient-based sensitivity.
- **Break condition**: High-variance estimates from insufficient trajectory samples; numerical underflow in log-probability accumulation for long horizons.

## Foundational Learning

- **Concept: Bi-level optimization and implicit differentiation**
  - Why needed here: The entire framework depends on treating the inner-loop solution φ*(θ) as differentiable w.r.t. θ. You must understand constrained optimization, Lagrangians, and the Implicit Function Theorem to implement the sensitivity computations.
  - Quick check question: Given g(x, y) = 0 defining y*(x), derive dy*/dx using IFT. What happens if ∂g/∂y is near-zero?

- **Concept: Stochastic Policy Gradient (SPG) and the policy gradient theorem**
  - Why needed here: The inner loop uses SPG, and all sensitivity derivations start from φ̂(φ, θ) = E_{s~ρ}[∇_φ log π_φ · Q̂^π_φ]. Understanding why log π appears (score function estimator) and how the critic Q̂ enters is essential.
  - Quick check question: Derive the policy gradient ∇_φ J(π_φ) for a stochastic policy. Why does REINFORCE use log π_φ(a|s)?

- **Concept: Markov chain stationary distributions and discounted state visitation**
  - Why needed here: The paper uses discounted Markov chain density ρ^π_φ rather than stationary distribution. Sensitivity of expectations under ρ requires understanding how transition dynamics affect visitation frequencies.
  - Quick check question: For a 2-state MDP with transition matrix P, compute the discounted state visitation d^π(s) starting from s_0. How does changing P(s'|s,a) affect d^π(s')?

## Architecture Onboarding

- **Component map**:
  Simulation environment (f_θ, R_θ) → Inner-level SPG learner (π_φ, Q̂^π_φ, V̂^π_φ) → Sensitivity computation module → Jacobian solver → Outer-level SPG learner → Real-world interface

- **Critical path**:
  1. Initialize f_θ, R_θ from system identification or random sampling near real parameters
  2. Run inner-loop SPG until ||φ̂|| < ε (Algorithm 1, line 2)
  3. Generate n in-sim trajectories of length N; accumulate W^φ_k, W^θ_k during sampling
  4. Compute critic sensitivities ∇_θ Q̂, ∇_φ Q̂ iteratively until convergence (Eq. 13-14)
  5. Assemble ∇_φ φ̂ and ∇_θ φ̂ using Eq. 15-16; solve for ∇_θ φ via Eq. 12
  6. Deploy π_φ in real world; collect trajectories
  7. Compute outer gradient φ(φ, θ) via Eq. 9a; update θ
  8. Iterate until real-world return converges

- **Design tradeoffs**:
  - **Exact vs. approximate critic sensitivities**: Exact iteration (as in paper examples) scales poorly with state dimension; NN approximation adds bias but scales. Trade accuracy for tractability.
  - **Trajectory budget allocation**: More in-sim trajectories improve ∇_θ φ accuracy but cost compute; more real-world trajectories improve outer gradient but cost deployment time.
  - **Parameter scope**: Adapting only R_θ (reward shaping) is simpler but may be insufficient; adapting both f_θ and R_θ is more expressive but larger search space.
  - **Jacobian inversion method**: Direct inversion O(dim(φ)³) is prohibitive; iterative methods (conjugate gradient) are O(kn) but require good preconditioning.

- **Failure signatures**:
  - **Outer loop divergence**: Real-world return decreases—check if ∇_θ φ has correct sign (test against finite differences on simple MDP)
  - **Inner loop non-convergence**: ||φ̂|| never approaches 0—simulator parameters may yield degenerate MDP (zero rewards, absorbing states)
  - **Sensitivity explosion**: ||∇_θ φ|| → ∞—Jacobian ∇_φ φ̂ near-singular; reduce learning rate or regularize
  - **High variance in outer gradient**: Real-world trajectory returns have high variance—increase samples or use baseline subtraction
  - **Sim2Real gap persists after convergence**: Real return plateaus below optimal—θ* may not exist within parameterized family; expand f_θ/R_θ parameterization

- **First 3 experiments**:
  1. **Discrete MDP reproduction**: Implement the 3-state, 2-action MDP from Section 5.1 with tabular f_θ, R_θ. Use exact DP for inner loop. Verify outer-level convergence from 5 random seeds matches Figure 3 (left). Success criterion: normalized return > 0.95 within 6k iterations.
  2. **Sensitivity validation via finite differences**: For the continuous LQR example, compute ∇_θ φ analytically (Theorem 1) and compare to finite difference approximation ∇_θ φ ≈ (φ*(θ+δ) - φ*(θ-δ))/(2δ) for small δ. Measure relative error as function of trajectory length N and count n.
  3. **Reward-only adaptation ablation**: Fix f_θ = f_real (perfect dynamics), adapt only R_θ. Compare convergence speed and final performance to full (f_θ, R_θ) adaptation in continuous MDP. Hypothesis: reward-only is faster but may hit ceiling if dynamics mismatch is critical.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can the computational cost of estimating critic sensitivities be reduced to enable the scaling of the bi-level RL framework to high-dimensional systems?
- **Basis in paper**: [explicit] The authors state in Section 6.1 that "efficient approximations of the critic sensitivities are required to make the framework efficiently scalable" because the burden increases with policy size.
- **Why unresolved**: The current method requires computing complex derivatives (Eqs. 13-14) which are iteratively approximated using Deep Neural Networks, creating a significant computational bottleneck.
- **What evidence would resolve it**: A scalable algorithm variant that maintains convergence speed on high-dimensional benchmarks (e.g., Humanoid locomotion) without prohibitive memory or processing requirements.

### Open Question 2
- **Question**: Can the bi-level RL framework be extended to operate directly on partial or noisy observation spaces (e.g., pixels) rather than assuming identical state representations?
- **Basis in paper**: [explicit] Section 6.1 lists this as an "important direction for future work," noting the current limitation is the assumption that the simulator and real system share the same state representation.
- **Why unresolved**: The mathematical derivation of the sensitivity analysis (Theorem 1) relies on explicit state transitions and value functions, which become latent and ambiguous under partial observability.
- **What evidence would resolve it**: Successful derivation and application of the bi-level gradient on a vision-based control task where the state is not explicitly available.

### Open Question 3
- **Question**: How can offline RL algorithms be effectively adapted for the outer-level optimization loop to avoid risky or expensive online real-world interactions?
- **Basis in paper**: [explicit] The discussion in Section 6 notes that "offline RL is a suitable and promising framework for the outer-level RL algorithms" and calls for "algorithmic development" in this area.
- **Why unresolved**: Algorithm 1 currently requires rolling out the current policy on the real world (P) in every iteration to compute the outer-level gradient, which implies online data collection.
- **What evidence would resolve it**: A modified bi-level algorithm that converges to Sim2Real optimality using only a static dataset of real-world transitions, without requiring active exploration.

## Limitations

- Computational expense from bi-level optimization and iterative Jacobian inversion
- Reliance on differentiable simulators and sufficient real-world data collection budget
- Limited empirical validation to small-scale MDP experiments and lack of high-dimensional continuous control testing

## Confidence

- **Theoretical formulation**: High confidence in the proof of Sim2Real optimality under the bi-level optimization framework
- **Sensitivity gradient derivations**: Medium confidence in the correctness and practical implementation of Theorem 1, particularly Markov chain sensitivity approximations
- **Empirical validation**: Medium confidence due to small-scale MDP experiments and limited ablation studies
- **Scalability claims**: Low confidence in the framework's ability to scale to high-dimensional systems without addressing computational bottlenecks

## Next Checks

1. **Scalability Test**: Apply the framework to a higher-dimensional continuous control task (e.g., half-cheetah or humanoid locomotion) and evaluate whether outer-level RL converges within reasonable computation time and data budget.

2. **Simulator Expressiveness Ablation**: Compare performance when adapting only reward parameters versus both dynamics and rewards in a domain where dynamics mismatch is substantial (e.g., altered friction or mass distributions).

3. **Finite-Difference Gradient Validation**: For the continuous LQR experiment, verify analytical sensitivities from Theorem 1 against finite-difference approximations across varying trajectory lengths and sample counts to quantify estimation error.