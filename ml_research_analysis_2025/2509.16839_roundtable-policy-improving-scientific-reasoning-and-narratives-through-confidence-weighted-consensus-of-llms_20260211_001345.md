---
ver: rpa2
title: 'Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted
  Consensus of LLMs'
arxiv_id: '2509.16839'
source_url: https://arxiv.org/abs/2509.16839
tags:
- gain
- count
- scientific
- player
- materials
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Roundtable Policy, an inference-time reasoning
  framework that aggregates responses from multiple LLMs through confidence-weighted
  consensus. The approach treats LLMs as members of a scientific committee, where
  each model proposes candidate responses and a set of graders evaluates their performance
  to generate a confidence-weight table.
---

# Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs

## Quick Facts
- arXiv ID: 2509.16839
- Source URL: https://arxiv.org/abs/2509.16839
- Reference count: 40
- Primary result: 13.01% average performance gain over individual models on heterogeneous scientific tasks

## Executive Summary
Roundtable Policy introduces an inference-time reasoning framework that aggregates responses from multiple LLMs through confidence-weighted consensus. The approach treats LLMs as members of a scientific committee, where each model proposes candidate responses and a set of graders evaluates their performance to generate a confidence-weight table. At inference, this table guides fusion of responses into a single consensus answer. Experiments on two regimes—MultiTask (heterogeneous scientific problems) and SingleTask (structured scientific proposals)—show that Roundtable Policy consistently improves factual reliability and narrative quality. On MultiTask, it achieves an average 13.01% performance gain over individual models, while on SingleTask it improves rubric scores by 11.04%. The method is interpretable, requiring only black-box access to models and producing explicit, auditable aggregation signals.

## Method Summary
Roundtable Policy operates through a training phase where multiple player LLMs generate candidate responses to scientific queries, which are then evaluated by grader LLMs producing quality scores and uncertainty intervals. These evaluations update a confidence-weight table that tracks each player's reliability across different task types. During inference, the fusion agent synthesizes a final answer using the confidence-weighted contributions from all players, conditioned on the learned reliability table. The framework requires no model fine-tuning, works with black-box access to all models, and maintains interpretability through explicit confidence weights and uncertainty quantification.

## Key Results
- Achieves 13.01% average performance improvement over individual models on MultiTask regime (UCLA PhD exam questions)
- Improves SingleTask scientific proposal rubric scores by 11.04% (Creativity, Rigor, Coherence)
- Shows robustness to grader noise and maintains performance when players exhibit complementary error patterns
- Confidence-weight table stabilizes after sufficient training rounds, providing reliable guidance for fusion decisions

## Why This Works (Mechanism)

### Mechanism 1: Domain-Specific Reliability Tracking
Roundtable Policy improves performance by learning which models excel at which task types, then weighting contributions accordingly. The confidence-weight table ϑ ∈ R^(Lp×N) accumulates historical quality scores and uncertainties per player per subtask. During inference, the fusion agent conditions on this table to emphasize contributions from players with demonstrated strengths in the relevant domain. Core assumption: Model strengths are consistent enough across rounds that historical performance predicts future reliability on similar task types.

### Mechanism 2: Complementary Error Pattern Exploitation
Aggregating diverse models captures correct reasoning fragments while suppressing isolated errors. Different LLMs trained on different data exhibit different failure modes. When graders evaluate responses, they identify which parts of which answers are reliable. The fusion agent can then synthesize correct elements from multiple partial answers rather than depending on any single model's complete response. Core assumption: Errors across models are not perfectly correlated; at least one model provides correct information for each critical reasoning step.

### Mechanism 3: Uncertainty-Calibrated Trust
Incorporating confidence intervals into the weight table enables risk-aware fusion. Each grader outputs not just a quality score but a 95% confidence interval. The fusion agent can downweight contributions with high uncertainty even if their point score is high, favoring more reliable (tighter interval) responses. This prevents over-reliance on spuriously confident but incorrect answers. Core assumption: Graders can produce meaningful uncertainty estimates that correlate with actual error rates.

## Foundational Learning

- **Concept: Ensemble diversity**
  - Why needed here: Roundtable Policy's gains depend on models making different mistakes. Understanding why ensembles work (variance reduction, error independence) helps diagnose when the approach will fail.
  - Quick check question: If all your player models were fine-tuned from the same base model on similar data, would you expect larger or smaller gains from Roundtable Policy?

- **Concept: Uncertainty quantification in LLMs**
  - Why needed here: The confidence-weight table incorporates uncertainty intervals. You need to understand how LLMs express confidence (verbalized, logit-based, epistemic vs aleatoric) to design effective grader prompts.
  - Quick check question: Can an LLM reliably estimate its own uncertainty without external calibration? What evidence supports your answer?

- **Concept: Multi-agent coordination protocols**
  - Why needed here: Roundtable Policy is a specific coordination pattern (parallel generation → evaluation → weighted fusion). Contrasting with debate-style (iterative refinement) and tool-use (sequential delegation) helps understand tradeoffs.
  - Quick check question: In what scenarios would iterative debate outperform one-round weighted fusion? When would fusion be preferred?

## Architecture Onboarding

- **Component map:**
  - Players (L_p LLMs) -> Grader LLMs -> Confidence-weight table (ϑ) -> Fusion agent -> Consensus answer

- **Critical path:**
  1. Training phase: Sample queries → players generate → graders evaluate → update ϑ (repeat R rounds)
  2. Inference phase: New query → players generate → fusion agent synthesizes using pretrained ϑ → output consensus answer

- **Design tradeoffs:**
  - More players increase diversity but raise compute cost linearly. Paper uses L_p=9.
  - More graders improve evaluation robustness but add latency. Paper uses L_g=4.
  - Longer training (more rounds) improves ϑ calibration but delays deployment. MultiTask needs hundreds of rounds; SingleTask stabilizes faster.

- **Failure signatures:**
  - Performance plateaus below individual best model: Check if ϑ has converged to uniform weights (graders not discriminating).
  - High variance across rounds: Player strengths may be inconsistent; consider task-specific ϑ or more graders.
  - Fusion outputs generic/averaged text: Fusion agent may be ignoring ϑ; verify prompt includes table and enforces non-verbatim synthesis.

- **First 3 experiments:**
  1. Baseline comparison: Run Roundtable Policy vs. best-single-model policy (BSMP) vs. simple majority voting on held-out MultiTask queries. Expect RT > BSMP > majority.
  2. Ablate confidence table: Replace learned ϑ with uniform weights. Quantify performance drop to isolate contribution of reliability tracking.
  3. Grader consistency audit: Compute pairwise Kendall's Tau between graders on a sample of rounds. High agreement (τ > 0.6) validates evaluation reliability; low agreement signals need for more graders or rubric refinement.

## Open Questions the Paper Calls Out

- **Adaptive arbitrator strategies:** Can the confidence-weight table evolve dynamically with task complexity and real-time feedback? Current implementation uses static table learned before inference.

- **Reliability in open-ended tasks:** How to ensure reliability when AI graders exhibit low inter-grader agreement on subjective criteria like creativity and logical coherence? SingleTask shows very low grader agreement (τ ≈ 0.15-0.25).

- **Domain transfer limits:** To what extent does the "institutional memory" of the confidence-weight table transfer across distinct scientific domains without retraining? Unclear if weights learned for one domain generalize to new fields.

- **Coupling with external validation:** Can fusion be effectively coupled with RLHF or experimental feedback to optimize weight assignments autonomously? Current relies entirely on AI graders for supervision.

## Limitations

- Convergence properties and sensitivity to training data distribution remain unclear, particularly for novel domains
- Assumption that grader uncertainty estimates are meaningful and well-calibrated lacks empirical validation
- Performance gains may diminish if models share systematic biases or are fine-tuned from similar base models

## Confidence

- **High Confidence:** General architecture and implementation details are well-specified. Weighted fusion based on historical performance is clearly described and reproducible.
- **Medium Confidence:** Reported performance improvements (13.01% MultiTask, 11.04% SingleTask) have statistical significance but generalizability across domains needs validation.
- **Low Confidence:** Assumption that grader uncertainty estimates are meaningful and well-calibrated. Paper proposes mechanism but provides no empirical validation.

## Next Checks

1. **Grader Calibration Audit:** Run controlled experiment where graders evaluate responses with known ground truth quality. Compute calibration curves and Brier scores to quantify uncertainty calibration quality.

2. **Domain Transfer Test:** Apply Roundtable Policy trained on MultiTask to a different scientific domain (e.g., physics problems) without retraining ϑ. Measure performance drop to assess domain generalization limits.

3. **Ablation on Player Diversity:** Systematically reduce player diversity (e.g., use models fine-tuned from same base model) and measure performance degradation. Quantify how much gains depend on complementary error patterns versus individual model strength.