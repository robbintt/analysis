---
ver: rpa2
title: Partial Parameter Updates for Efficient Distributed Training
arxiv_id: '2509.22418'
source_url: https://arxiv.org/abs/2509.22418
tags:
- training
- parameters
- memory
- flops
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses memory and compute bottlenecks in large-scale
  distributed language model training over low-bandwidth networks. The authors propose
  restricting backpropagation to only a subset of parameters on each node, keeping
  the rest frozen during local steps.
---

# Partial Parameter Updates for Efficient Distributed Training

## Quick Facts
- arXiv ID: 2509.22418
- Source URL: https://arxiv.org/abs/2509.22418
- Reference count: 40
- Key outcome: Restricting backpropagation to a subset of parameters on each node reduces memory usage and FLOPs by up to 47% and 15% respectively, while matching prior low-communication approaches in perplexity.

## Executive Summary
This paper addresses memory and compute bottlenecks in large-scale distributed language model training over low-bandwidth networks by restricting backpropagation to only a subset of parameters on each node, keeping the rest frozen during local steps. The method reduces peak memory usage and training FLOPs without increasing communication. Experiments on a 1.3B-parameter model across 32 nodes show the method matches prior low-communication approaches in perplexity while using 15% fewer FLOPs and up to 47% less memory.

## Method Summary
The method partitions model parameters into N disjoint slices, with each node updating only its assigned slice during local training steps while performing full forward passes. After H local steps, nodes synchronize parameter updates through all-reduce operations with normalization based on how many nodes updated each parameter. The approach specifically targets MLP and attention weight matrices, partitioning them row-wise and column-wise to maintain computational flow while reducing memory and compute requirements.

## Key Results
- Memory savings of up to 47% compared to full-parameter distributed training
- 15% reduction in training FLOPs while maintaining comparable perplexity
- Method matches prior low-communication approaches in model quality
- MLP and attention projection slicing can be done independently without requiring cross-node activation exchange

## Why This Works (Mechanism)

### Mechanism 1
Restricting backpropagation to a subset of parameters reduces memory and compute without degrading convergence under low-communication training. Each node computes gradients only for its assigned parameter slice, storing no optimizer state or gradient buffers for frozen parameters. The forward pass remains full, so activations flow through all parameters, but backward skips frozen parameter gradients entirely. The frozen parameters still contribute meaningfully to gradient flow via their input Jacobians, and the optimization landscape tolerates fewer updates per parameter.

### Mechanism 2
MLP and attention projections can be sliced independently without requiring cross-node activation exchange. MLP weights are partitioned row-wise and column-wise so each slice contributes additively, while attention heads are partitioned into disjoint groups. Forward computes all slices; backward only accumulates parameter gradients for the local slice. MLP blocks operate sufficiently independently and attention heads can be trained semi-independently.

### Mechanism 3
Parameter assignments can overlap across nodes without divergence, enabling redundancy in updates. With N slices and K nodes, each slice is trained on K/N nodes. Updates are aggregated via all-reduce and normalized by a count vector tracking how many nodes update each parameter. Averaging overlapping updates from multiple nodes provides sufficient gradient signal quality.

## Foundational Learning

- **Distributed Data Parallelism (DDP) with local SGD**: The method builds on low-communication variants where nodes perform H local steps before synchronization. Quick check: Can you explain why gradient synchronization at every step becomes a bottleneck on Wi-Fi/Ethernet?
- **Optimizer state memory footprint**: Adam maintains 2 moment tensors per parameter; savings come from not allocating these for frozen parameters. Quick check: For a 1.3B parameter model in bfloat16 with Adam, roughly how much GPU memory do optimizer states consume?
- **Block coordinate descent**: The method is a distributed variant where each node optimizes a different parameter block. Quick check: What happens to convergence if blocks are chosen randomly each step versus fixed throughout training?

## Architecture Onboarding

- **Component map**: Inner optimizer (AdamW) -> Parameter slicer -> Outer optimizer (Nesterov SGD) -> Count normalizer -> Streaming synchronizer
- **Critical path**: 1) Partition model into N slices; assign each node its slice index. 2) Forward pass: compute with all parameters. 3) Backward pass: compute gradients only for local slice; skip frozen parameter gradient computation. 4) Local updates: inner optimizer updates only active slice. 5) After H steps: all-reduce sparse deltas, normalize by count vector, outer optimizer step.
- **Design tradeoffs**: More slices (smaller trainable fraction) â†’ more memory savings but slower convergence. Slicing attention heads saves additional memory but risks degradation; MLP-only is safer. Horizontal layer slicing caused gradient explosion - avoid.
- **Failure signatures**: Perplexity unexpectedly high: check that frozen slice contributions are still included in input Jacobian. Gradient explosion: likely using horizontal layer partitioning or sliced WO; revert to MLP-only vertical slicing. Memory not reduced: verify optimizer states are not being allocated for frozen parameters.
- **First 3 experiments**: 1) Baseline sanity check: Train 1.3B model with N=2 (1/2 MLPs) on 8 nodes, H=100 local steps; compare perplexity to full-model DiLoCo after 1B tokens. 2) Memory validation: Profile peak GPU memory with N=4 (1/4 MLPs) vs. full model; confirm ~30% reduction matches theoretical estimates. 3) Slice ablation: Compare N=2 vs. N=4 vs. N=8 on same token budget; plot perplexity vs. memory to characterize tradeoff curve.

## Open Questions the Paper Calls Out

### Open Question 1
Can lightweight adaptive strategies dynamically reassign trainable parameter subsets during training without sacrificing the memory and communication efficiency achieved by fixed assignment? Dynamic reassignment typically requires replicating full optimizer states or transferring states between nodes, which eliminates the memory savings and low-bandwidth advantages the method targets.

### Open Question 2
Can horizontal parameter slicing (partitioning by layer) be stabilized through hyperparameter tuning or architectural modifications to match the performance of vertical slicing? Initial experiments partitioning the model horizontally resulted in gradient explosion and immediate divergence.

### Open Question 3
To what extent can the backward pass be further optimized by dropping Jacobian components corresponding to frozen parameters without causing training divergence? While the standard method skips parameter gradients, it computes full activation gradients. Naively dropping these components led to gradient explosion.

## Limitations
- Method effectiveness on non-GPT architectures, very small or very large models, and interaction with other low-communication strategies remains untested
- Attention head slicing performance is less validated than MLP slicing
- Horizontal layer slicing caused gradient explosion and is not currently viable

## Confidence
- **High confidence**: Memory and compute savings from parameter slicing, and the feasibility of the method on the tested GPT-3 XL model
- **Medium confidence**: The claim that attention head slicing can be done without significant degradation, given that only MLP slicing is extensively validated
- **Low confidence**: The method's effectiveness on non-GPT architectures, very small or very large models, and its interaction with other low-communication strategies

## Next Checks
1. Run the same 1.3B model with N=8 (1/8 MLPs) for 2B tokens and measure perplexity to quantify the claimed "noticeable performance degradation" and verify the 50% more tokens to recover claim
2. Apply the slicing method to a ResNet-50 or ViT-B/16 model, training on ImageNet-1K, and compare peak memory and top-1 accuracy to a full-model baseline
3. Repeat the 1.3B GPT experiment with SGD with momentum (instead of AdamW) for the inner loop, keeping all other hyperparameters fixed, and report the final perplexity to assess the method's robustness to optimizer choice