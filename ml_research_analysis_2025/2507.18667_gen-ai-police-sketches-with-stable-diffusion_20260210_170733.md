---
ver: rpa2
title: Gen-AI Police Sketches with Stable Diffusion
arxiv_id: '2507.18667'
source_url: https://arxiv.org/abs/2507.18667
tags:
- clip
- diffusion
- sketches
- stable
- alignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates multimodal AI approaches for automating
  suspect sketching in law enforcement. Three models were developed and evaluated:
  (1) a baseline Stable Diffusion image-to-image model, (2) the same model integrated
  with a pre-trained CLIP model for text-image alignment, and (3) a novel approach
  using LoRA fine-tuning of CLIP integrated with Stable Diffusion.'
---

# Gen-AI Police Sketches with Stable Diffusion

## Quick Facts
- arXiv ID: 2507.18667
- Source URL: https://arxiv.org/abs/2507.18667
- Reference count: 10
- Primary result: Baseline Stable Diffusion image-to-image model outperforms CLIP-augmented variants on structural fidelity metrics for police sketch generation

## Executive Summary
This paper investigates multimodal AI approaches for automating suspect sketching in law enforcement, developing and evaluating three models using Stable Diffusion and CLIP. The baseline image-to-image model (Model 1) achieved superior structural similarity metrics compared to CLIP-integrated variants, though iterative refinement improved perceptual quality in the text-guided models. An ablation study identified that fine-tuning both self-attention and cross-attention layers of CLIP yielded optimal alignment between text descriptions and sketches. The research highlights the tradeoff between model simplicity and semantic control in automated sketch generation.

## Method Summary
The study developed three models using Stable Diffusion v1.5 and CLIP for police sketch generation from text descriptions and reference sketches. Model 1 used direct image-to-image latent diffusion, Model 2 combined CLIP text and image embeddings with Stable Diffusion, and Model 3 applied LoRA fine-tuning to both self-attention and cross-attention layers of CLIP before integration. Training used 295 (description, sketch) pairs from the CUHK Face Sketch FERET Database with 80/20 train/validation split. Inference employed strength=0.3, guidance_scale=7.5, and 5 iterations of refinement with prompts truncated to 77 tokens. Models were evaluated using SSIM, PSNR, CLIP Score, and LPIPS metrics.

## Key Results
- Model 1 achieved highest structural similarity with SSIM of 0.72 and PSNR of 25 dB, outperforming Models 2 and 3
- Iterative refinement enhanced perceptual similarity (LPIPS), with Model 3 showing improvement over Model 2
- LoRA fine-tuning of both self-attention and cross-attention layers yielded optimal text-sketch alignment in ablation study
- Qualitative evaluation confirmed Model 1 produced clearest facial features despite being the simplest approach

## Why This Works (Mechanism)

### Mechanism 1: Direct Image-to-Image Latent Diffusion for Structural Preservation
The baseline Stable Diffusion image-to-image model preserves structural fidelity by compressing input images into latents that are iteratively denoised while conditioned on image structure rather than text embeddings. With strength parameter 0.3, this approach maintains more input structure than text-guided variants, explaining why Model 1 achieved 20.84% better SSIM than Model 2 and 19.72% better than Model 3.

### Mechanism 2: CLIP Embedding Fusion for Text-Guided Iterative Refinement
Text and image embeddings from CLIP are normalized, combined, and projected to match Stable Diffusion's conditioning dimension, enabling dynamic prompt-based adjustments across 5 refinement iterations. This fusion allows semantic control while preserving structural information, though the 77-token limit constrains description richness.

### Mechanism 3: LoRA Fine-Tuning on Dual Attention Layers for Domain Adaptation
Low-rank adapters added to both self-attention and cross-attention layers of CLIP enable efficient adaptation to sketch-specific visual patterns and textual descriptions. While this configuration improved alignment over single-layer fine-tuning, it did not translate to outperforming the baseline on standard metrics, suggesting domain adaptation benefits may be outweighed by loss of general representation capacity.

## Foundational Learning

- **Latent Diffusion Models**: Understanding how Stable Diffusion compresses images to latents, applies denoising, and reconstructs outputs is essential for debugging generation quality and setting strength/guidance parameters. Quick check: Can you explain why a lower strength value (0.3) preserves more input structure than a higher value (0.8)?

- **CLIP Contrastive Learning**: CLIP's text-image alignment underpins Models 2 and 3. Understanding its embedding space, tokenization limits, and contrastive loss helps diagnose alignment failures. Quick check: What happens to a 100-token description when processed through CLIP's standard tokenizer?

- **LoRA (Low-Rank Adaptation)**: Model 3 applies LoRA to CLIP attention layers. Understanding rank selection, target modules, and fine-tuning dynamics is critical for reproducing or extending this approach. Quick check: Why does LoRA enable fine-tuning with fewer trainable parameters than full model fine-tuning?

## Architecture Onboarding

- **Component map**: Input images → VAE encoder → latents → U-Net → VAE decoder → output sketches; Text prompts → CLIP text encoder → text embeddings; Reference sketches → CLIP vision encoder → image embeddings; Combined embeddings → projection layer → conditioning vectors; LoRA adapters attached to CLIP attention layers (Model 3 only)

- **Critical path**: Prepare (description, sketch) pairs → For Model 3: Fine-tune CLIP with LoRA on attention layers → Encode inputs through appropriate pipeline → Run 5-iteration refinement → Evaluate with SSIM, PSNR, CLIP Score, and LPIPS

- **Design tradeoffs**: Simplicity vs. Control (Model 1 vs Models 2-3); Dataset size vs. Overfitting (295 pairs for LoRA); Token limit vs. Description richness (77-token CLIP constraint)

- **Failure signatures**: Heavy distortion by iteration 3 in Model 3; SSIM/PSNR degradation when text contradicts image structure; Training loss plateau before convergence; Top-25 accuracy >90% but poor qualitative results suggesting overfitting

- **First 3 experiments**: 1) Reproduce baseline metrics with strength=0.3, guidance_scale=7.5 targeting SSIM ≈ 0.72, PSNR ≈ 25 dB; 2) Ablate attention layer targets (self-only, cross-only, both) to confirm alignment findings; 3) Test token limit impact using 50, 77, and 100+ token descriptions to quantify 77-token constraint

## Open Questions the Paper Calls Out

- **Open Question 1**: Can methods to extend or bypass CLIP's 77-token input limit improve the capture of nuanced facial differences in suspect descriptions? The authors identify this constraint as hindering nuanced facial difference capture but have not tested alternatives or quantified information loss.

- **Open Question 2**: Would masking-based iterative refinement targeting specific facial features improve CLIP embedding alignment over global refinement? The authors propose this as future work but have only implemented global refinement across all iterations.

- **Open Question 3**: What factors cause Model 3 (fine-tuned CLIP + Stable Diffusion) to underperform the simpler baseline despite higher training complexity? The paper notes Model 3 shows promise but doesn't analyze why additional fine-tuning degrades output quality relative to Model 1.

## Limitations

- LoRA hyperparameters (rank, learning rate, optimizer settings, batch size) were not specified, creating reproducibility challenges for Model 3
- The 77-token CLIP constraint was identified as limiting but not empirically validated through systematic testing of longer descriptions
- Training dataset of 295 pairs may not capture sufficient variation in suspect descriptions to ensure robust generalization

## Confidence

- **High Confidence**: Model 1's superior SSIM (0.72) and PSNR (25 dB) metrics; iterative refinement improving LPIPS scores
- **Medium Confidence**: LoRA fine-tuning both attention layers improving alignment; effectiveness of CLIP embedding fusion for text-guided refinement
- **Low Confidence**: Model 3 showing improvement over Model 2 despite trailing Model 1; 77-token limit as critical constraint without experimental validation

## Next Checks

1. **Ablation Validation**: Implement and test all three LoRA configurations (self-attention only, cross-attention only, both) on the same validation set to quantitatively confirm that fine-tuning both attention layers yields statistically significant improvements.

2. **Token Limit Impact Study**: Systematically test sketch generation quality using descriptions at 50, 77, 100, and 150 tokens while keeping all other parameters constant to empirically quantify the practical impact of the 77-token constraint.

3. **Cross-Architecture Comparison**: Reproduce the baseline Model 1 approach using a different diffusion model architecture (e.g., SDXL or another img2img implementation) on the same CUFSF dataset to determine whether structural fidelity advantages are architecture-specific.