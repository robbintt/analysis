---
ver: rpa2
title: How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning
  through Evolutionary Domain Knowledge Adaptation
arxiv_id: '2510.21148'
source_url: https://arxiv.org/abs/2510.21148
tags:
- prompt
- reasoning
- arxiv
- causal
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces EGO-Prompt, a framework that integrates
  domain knowledge graphs with prompt optimization to improve LLM reasoning for domain-specific
  tasks. It decomposes reasoning into two stages: generating instance-specific guidance
  from a Semantic Causal Graph (SCG), then predicting conditioned on that guidance.'
---

# How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation

## Quick Facts
- **arXiv ID:** 2510.21148
- **Source URL:** https://arxiv.org/abs/2510.21148
- **Reference count:** 40
- **Key outcome:** EGO-Prompt achieves 7.32%–12.61% higher F1 than state-of-the-art baselines, enabling small models to match performance of much larger models at under 20% of the cost.

## Executive Summary
This paper introduces EGO-Prompt, a framework that integrates domain knowledge graphs with prompt optimization to improve LLM reasoning for domain-specific tasks. It decomposes reasoning into two stages: generating instance-specific guidance from a Semantic Causal Graph (SCG), then predicting conditioned on that guidance. Textual gradients are used to iteratively refine both the SCG and system prompts. Evaluated on three real-world datasets, EGO-Prompt achieves 7.32%–12.61% higher F1 than state-of-the-art baselines, enabling small models to match the performance of much larger models at under 20% of the cost. It also outputs an interpretable, refined SCG.

## Method Summary
EGO-Prompt is a framework for optimizing prompts using domain knowledge graphs to improve LLM reasoning. It uses a two-stage process: first, a Graph Description Model extracts instance-specific causal guidance from a Semantic Causal Graph (SCG); second, a Prediction Model uses this guidance to make predictions. The framework iteratively refines the SCG and prompts using "textual gradients" - feedback generated by a Backward Engine comparing predictions to ground truth. This evolutionary process updates the SCG structure (add/edit/delete operations) and system prompts based on validation performance.

## Key Results
- EGO-Prompt achieves 7.32%–12.61% higher F1 than state-of-the-art baselines on three real-world datasets
- Small models (e.g., GPT-4o mini) can match the performance of significantly larger models (e.g., o1) at under 20% of the cost
- The framework produces an interpretable, refined Semantic Causal Graph that evolves from expert knowledge
- On the Pandemic dataset, EGO-Prompt achieves 89.3% F1 compared to 82.2% for Expert-Organized Prompt

## Why This Works (Mechanism)

### Mechanism 1: Two-Stage Factorization
- **Claim:** Decomposing reasoning into guidance generation followed by conditioned prediction improves domain task performance
- **Mechanism:** The architecture separates the task into two distinct passes. First, a Graph Description Model ($M_F$) extracts instance-specific causal guidance ($z^*$) from the SCG and input ($x$). Second, a Prediction Model ($M_F'$) takes the original input alongside this distilled guidance to produce the final output
- **Core assumption:** The relationship between input and target can be approximated by a DAG where "nearly deterministic" reasoning guidance can be extracted
- **Evidence anchors:** [abstract] "It decomposes reasoning into two stages: generating instance-specific guidance... then predicting conditioned on that guidance"
- **Break condition:** If $M_F$ hallucinates connections not present in the SCG, or if the SCG is fundamentally incomplete, the guidance $z^*$ will mislead $M_F'$

### Mechanism 2: Textual Gradient Optimization
- **Claim:** Treating the knowledge graph as a mutable variable that can be optimized via textual gradients allows correction of expert biases
- **Mechanism:** The system iteratively refines the SCG by analyzing prediction errors. A Backward Engine ($M_B$) generates natural language feedback comparing predictions to ground truth, proposing specific Add/Edit/Delete operations on SCG nodes and edges
- **Core assumption:** The Backward Engine can diagnose why a prediction failed and propose structural graph edits that generalize
- **Evidence anchors:** [section 6.2] "EGO-Prompt automatically refines a human-designed SCG... by restricting the graph description updates to three operations"
- **Break condition:** If the validation set is small or noisy, graph updates may overfit to specific validation samples

### Mechanism 3: Small Model Performance Compression
- **Claim:** Small models can match performance of significantly larger reasoning models when equipped with optimized, graph-guided prompts
- **Mechanism:** By externalizing reasoning structure into the SCG and optimizing the prompt to utilize this structure, the model doesn't need to rely solely on internalized weights for domain logic
- **Core assumption:** The performance bottleneck is the alignment of the model's reasoning process with domain causalities, rather than raw parameter count
- **Evidence anchors:** [abstract] "enabling small models to match the performance of much larger models at under 20% of the cost"
- **Break condition:** If the domain task requires complex symbolic logic not contained in the SCG, the small model will lack capacity to execute reasoning

## Foundational Learning

- **Concept: Chain-of-Thought (CoT) & Reasoning Decomposition**
  - **Why needed here:** EGO-Prompt is essentially a structured, graph-constrained form of CoT. Understanding CoT is necessary to see why the "two-stage" generation helps
  - **Quick check question:** Can you explain why generating a causal description (guidance) *before* predicting the label might reduce hallucination compared to a zero-shot prediction?

- **Concept: Semantic Causal Graph (SCG) vs. Knowledge Graph (KG)**
  - **Why needed here:** The paper uses a specific graph definition (SCG) that is fault-tolerant and does not strictly adhere to causal Markov assumptions
  - **Quick check question:** What is the structural difference between a standard Knowledge Graph used for RAG and the Semantic Causal Graph used here, particularly regarding "completeness" requirements?

- **Concept: Textual Gradient Descent**
  - **Why needed here:** The optimization loop relies on using an LLM to generate text feedback (gradients) rather than numerical backpropagation
  - **Quick check question:** In the context of Algorithm 1, what acts as the "loss function" and what acts as the "optimizer"?

## Architecture Onboarding

- **Component map:** Graph Description Model ($M_F$) -> Prediction Model ($M_F'$) -> Backward Engine ($M_B$) -> Semantic Causal Graph ($G$)
- **Critical path:**
  1. Initialize with expert-defined SCG and System Prompts
  2. Forward Pass: $M_F$ generates $z^*$; $M_F'$ generates Prediction
  3. Backward Pass: $M_B$ compares Prediction to Ground Truth -> generates "Textual Gradients"
  4. Validation Step: Apply proposed changes to Prompt/Graph -> Run on Validation Set
  5. Commit: Keep changes only if Validation F1 improves

- **Design tradeoffs:**
  - **Determinism vs. Performance:** Forces "nearly deterministic" guidance to simplify the probabilistic chain, improving stability but suppressing creative reasoning nuances
  - **Cost vs. Accuracy:** Must select a strong Backward Engine ($M_B$) even if forward model is cheap; optimization phase itself has cost (~$2-$5 per task)
  - **Graph Rigidity:** SCG restricted to Add/Edit/Delete operations; if domain requires different graph topology, evolutionary approach might get stuck in local minimum

- **Failure signatures:**
  - **Overfitting to Validation Set:** Textual gradients become too specific to validation samples; Symptom: Validation F1 fluctuates wildly or drops after 6-12 steps
  - **Graph Drift:** "Delete" operation removes true causal links because they don't manifest clearly in small validation set; Symptom: Final SCG is sparse and performance drops on edge cases
  - **API Instability:** Non-deterministic LLM outputs cause gradient direction to flip between iterations; Symptom: Validation F1 oscillates without converging

- **First 3 experiments:**
  1. **Baseline Check:** Run "Expert Organized Prompt" (no graph) vs. "Static SCG" (no optimization) to isolate value of graph structure
  2. **Ablation on Graph Quality:** Provide "Reversed" or "33% complete" SCG to verify optimizer recovers performance
  3. **Cost/Model Scaling:** Run full EGO loop with weak backward engine vs. strong one to validate dependency on $M_B$ quality

## Open Questions the Paper Calls Out
1. How can the iterative textual gradient optimization process be stabilized to mitigate the observed 20% performance variance caused by LLM API non-determinism?
2. How can the framework prevent overfitting to the validation set when only small sample sizes are available for optimization?
3. Can the EGO-Prompt framework be effectively adapted for Dynamic RAG and automated causal discovery without requiring an initial expert-constructed SCG?
4. How can the computational efficiency of the causal-guided textual gradient process be improved to scale effectively to large datasets?

## Limitations
- **Performance variance:** Up to 20% variability in performance due to LLM API non-determinism
- **Validation overfitting:** Risk of overfitting to validation set when only small sample sizes are available
- **Computational cost:** Additional computational resources required, especially when scaling to larger sample sets

## Confidence
- **High Confidence:** Observed F1 and accuracy improvements over baselines are reproducible given described datasets and metrics
- **Medium Confidence:** Textual gradient loop works as described, but generalization beyond validation set is uncertain due to overfitting risk
- **Low Confidence:** Causal graph's interpretability claim is plausible but not quantitatively validated; SCG refinement could introduce hallucinated edges

## Next Checks
1. **Ablation on Backward Engine Quality:** Run full EGO loop with weak Backward Engine (same size as forward model) versus strong one (GPT-4o). Measure whether gradient quality directly correlates with final F1 and whether overfitting risk increases with weaker engines.

2. **SCG Necessity Test:** For each dataset, create a flat prompt containing all information currently in the SCG (without graph structure). Compare EGO-Prompt's performance to this "flattened" baseline to isolate value of graph topology versus content.

3. **Cross-Dataset Transfer of Refined SCGs:** Take optimized SCG from one dataset (e.g., Pandemic) and apply as initialization for another (e.g., TrafficSafe). Measure performance drop to quantify how much refinement is dataset-specific versus capturing generalizable causal priors.