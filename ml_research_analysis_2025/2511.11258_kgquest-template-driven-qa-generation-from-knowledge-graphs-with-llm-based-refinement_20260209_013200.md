---
ver: rpa2
title: 'KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based
  Refinement'
arxiv_id: '2511.11258'
source_url: https://arxiv.org/abs/2511.11258
tags:
- question
- generation
- knowledge
- questions
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces KGQuest, a scalable pipeline for generating
  multiple-choice QA pairs from knowledge graphs. The method clusters triplets by
  relation, creates reusable natural language templates using deterministic rules
  based on entity types, and employs a lightweight LLM refinement step to improve
  linguistic quality.
---

# KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement

## Quick Facts
- arXiv ID: 2511.11258
- Source URL: https://arxiv.org/abs/2511.11258
- Reference count: 0
- One-line primary result: Template-based pipeline produces 80-90% correct questions, with LLM refinement improving accuracy to over 95% while reducing inference time from hours to minutes

## Executive Summary
KGQuest introduces a scalable pipeline for generating multiple-choice QA pairs from knowledge graphs by clustering triplets based on their relations and creating reusable natural language templates. The method uses deterministic rules based on entity types to construct question templates, followed by a lightweight LLM refinement step to improve linguistic quality. Distractors are selected from the same relation cluster for semantic relevance. Evaluations on three knowledge graphs show that the template-based approach significantly outperforms direct LLM generation in both efficiency (minutes vs. hours) and factual consistency while maintaining high linguistic quality.

## Method Summary
The KGQuest pipeline processes knowledge graph triplets by first clustering them according to their relations, then extracting the most common object type per cluster using Named Entity Recognition. Deterministic rules map these entity types to question prefixes (e.g., PersonName â†’ "Who"), constructing templates of the form "prefix + verbalized relation + <SUBJECT>". An optional LLM refinement step samples one triplet per cluster, generates a question, and prompts the LLM to improve it while preserving factual integrity. The refined output is generalized back to a template for all instances in that cluster. Questions are instantiated by filling the <SUBJECT> placeholder, and N-1 distractors are randomly selected from the same relation cluster to ensure semantic relevance.

## Key Results
- Template-based pipeline achieves 80-90% correct questions compared to direct LLM generation
- LLM refinement improves accuracy to over 95% while requiring only single inference per template
- The approach reduces inference time from hours to minutes while maintaining factual consistency
- Distractor selection from the same relation cluster ensures semantic relevance and plausibility

## Why This Works (Mechanism)

### Mechanism 1: Relation-Based Abstraction
If triplets sharing the same relation r are grouped, the pipeline can generate a single reusable template for an entire category of facts rather than processing each triplet individually. The system clusters triplets by predicate and extracts the most common object type via Named Entity Recognition, then maps these to deterministic natural language rules. This works because the relation r and dominant object type E_k are sufficient to determine grammatical structure. Break condition: If a relation is polysemous or spans multiple distinct entity types, a single template may produce syntactically awkward questions.

### Mechanism 2: Decoupled Refinement
If the LLM is applied only to the abstract template rather than the instantiated question, it improves fluency without introducing factual hallucinations associated with generative AI. A single sample instance is generated from the template and fed to an LLM with instructions to fix grammar/syntax, then the refined output is re-templatized by replacing the specific entity with a generic placeholder. This works because syntactic corrections on one sample generalize effectively to all entities in that cluster. Break condition: If the LLM ignores the "refine-only" instruction and hallucinates new constraints in the sample output, the re-templatization step might bake an error into the entire cluster.

### Mechanism 3: Semantic Proximity for Distractors
If distractors are sampled from entities sharing the same relation cluster as the correct answer, they are semantically relevant enough to be plausible while remaining factually incorrect. For a question derived from triplet (s, r, o), the system randomly selects N-1 distractors from objects o' that appear in other triplets with relation r. This works because entities appearing in the same relation cluster are of comparable type and granularity. Break condition: If the Knowledge Graph is sparse or the cluster is small, random selection may pick entities that are semantically related but syntactically distinct, revealing the answer via mismatch.

## Foundational Learning

- **RDF Triplets (Subject-Predicate-Object)**: Why needed: The entire pipeline relies on decomposing knowledge into these atomic units to cluster and templatize information. Quick check: Given the triplet `(Paris, CapitalOf, France)`, which element serves as the "Subject" in the generated question "What is the capital of France?"
- **Slot Filling / Template Instantiation**: Why needed: This is the core generation step where static text structures are combined with dynamic data. Quick check: If a template is "Who wrote <SUBJECT>?" and the subject is "Hamlet", what is the instantiated question?
- **Hallucination in LLMs**: Why needed: The paper explicitly designs the pipeline to avoid this specific failure mode of direct LLM generation. Quick check: Why does the paper claim that direct LLM generation is "unpredictable" compared to template-based instantiation?

## Architecture Onboarding

- **Component map**: KG Parser -> Clusterer -> Rule Engine -> LLM Refiner -> Instantiator -> Distractor Module
- **Critical path**: The Refinement Prompt. If the prompt to the LLM does not explicitly constrain it to preserve the subject placeholder or avoid adding facts, the entire cluster fails verification.
- **Design tradeoffs**: Scalability vs. Diversity (using deterministic templates ensures scalability but limits linguistic diversity), Efficiency vs. Accuracy (smaller LLMs sufficient for refinement trading capability for speed).
- **Failure signatures**: Format Errors (missing punctuation/capitalization), Syntactic Awkwardness (mechanical-sounding templates), Hallucination in Refinement (LLM adding details not in original triplet).
- **First 3 experiments**: 1) Cluster Extraction Audit: Verify triplets are grouped strictly by relation with sensible entity types. 2) Refinement Isolation Test: Verify <SUBJECT> placeholder is preserved and no new facts introduced. 3) Latency Benchmark: Measure time for 1,000 questions using Template vs. Direct LLM pipeline.

## Open Questions the Paper Calls Out

### Open Question 1
Can knowledge-aware scoring or adaptive calibration strategies for distractor selection effectively enable difficulty-aware QA generation with personalized or leveled question design? Basis: The conclusion explicitly states this as an avenue for future work. Unresolved because current framework uses random distractor selection without difficulty control. Evidence needed: Experiments comparing random vs. knowledge-aware distractor selection across difficulty levels with human ratings.

### Open Question 2
How well does the template-based pipeline generalize to diverse, cross-domain knowledge graphs with substantially different schemas and relation types? Basis: The conclusion mentions exploring cross-domain QA generation for generalization. Unresolved because evaluation was limited to three specific KGs. Evidence needed: Systematic evaluation on additional domain-specific KGs measuring template coverage and error rates.

### Open Question 3
To what extent do human expert judgments align with LLM-as-jury evaluations for assessing grammatical, syntactic, and semantic correctness? Basis: The paper relies entirely on LLM-as-judge paradigm without human validation. Unresolved because LLM judges may have systematic biases different from human annotators. Evidence needed: Correlation analysis between LLM-jury scores and human expert ratings.

## Limitations

- Template Rule Coverage: Complete deterministic rule mapping between entity types and question prefixes is not provided, potentially limiting reproducibility.
- Small Cluster Fragility: The semantic proximity distractor selection mechanism may fail when clusters contain too few entities.
- Single-Sample Generalization Risk: The refinement mechanism relies on one sample per cluster, potentially propagating errors to all questions in that cluster.

## Confidence

- Template-Based Scalability: High confidence - Well-supported by 80-90% correctness rate and 90% efficiency gain in Table 2.
- LLM Refinement Effectiveness: Medium confidence - Correctness improves to 95%+, but intermediate results comparing raw vs. refined templates are not shown.
- Relation Clustering Validity: Medium confidence - Clustering mechanism appears sound but NER accuracy for dominant entity type extraction is not demonstrated across diverse relations.

## Next Checks

1. **Cluster Coverage Analysis**: Compute distribution of cluster sizes and percentage of triplets in clusters with fewer than 10 entities to assess distractor selection robustness.
2. **Hallucination Detection Benchmark**: Run direct LLM generation baseline and compare factual consistency rates to template-based approach.
3. **Rule Completeness Audit**: Create test suite of diverse relation-entity type combinations to verify deterministic rules produce grammatically correct questions for all combinations.