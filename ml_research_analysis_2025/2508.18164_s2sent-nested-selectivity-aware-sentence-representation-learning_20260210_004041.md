---
ver: rpa2
title: 'S2Sent: Nested Selectivity Aware Sentence Representation Learning'
arxiv_id: '2508.18164'
source_url: https://arxiv.org/abs/2508.18164
tags:
- sentence
- semantic
- representation
- spatial
- s2sent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of effective sentence representation
  learning using Transformer-based encoders with contrastive learning. While current
  methods rely on the last Transformer block, different blocks have varying semantic
  perception abilities that are not fully exploited.
---

# S2Sent: Nested Selectivity Aware Sentence Representation Learning

## Quick Facts
- arXiv ID: 2508.18164
- Source URL: https://arxiv.org/abs/2508.18164
- Reference count: 15
- Key outcome: S2Sent achieves 78.50% average Spearman correlation across seven STS tasks with negligible parameter overhead (0.4% of backbone)

## Executive Summary
This paper addresses the problem of effective sentence representation learning using Transformer-based encoders with contrastive learning. While current methods rely on the last Transformer block, different blocks have varying semantic perception abilities that are not fully exploited. The proposed S2Sent method introduces a parameterized nested selector downstream of the encoder that performs Spatial Selection (SS) and Frequency Selection (FS). SS uses a spatial squeeze-based self-gating mechanism to obtain adaptive weights for cross-block fusion, modeling dependencies between embedding features. FS replaces global average pooling with low-frequency DCT basis functions to reduce semantic loss during spatial squeeze. Experimental results show S2Sent significantly improves performance across seven STS tasks while introducing negligible additional parameters (less than 0.5% of backbone) and minimal inference latency. The method demonstrates strong integrability and scalability, not relying on specific hyperparameter choices for the number of Transformer blocks or frequency components.

## Method Summary
S2Sent introduces a nested selector module that sits downstream of a Transformer encoder to perform cross-block representation fusion. The method stacks the last N hidden states from the encoder into a 3D tensor, then applies Frequency Selection (FS) using low-frequency DCT basis functions for spatial squeeze, followed by Spatial Selection (SS) using a squeeze-and-excitation mechanism to learn adaptive weights for each block. The final sentence representation is obtained through a weighted sum of the original block hidden states. The method uses mean pooling (not [CLS] pooling) for the final 1D sentence vector. S2Sent was evaluated on seven STS benchmarks using both BERT and RoBERTa backbones with SimCSE and PromptCSE contrastive learning frameworks, showing consistent improvements over baseline methods.

## Key Results
- Achieves 78.50% average Spearman correlation across seven STS tasks, improving upon BERT-base-SimCSE baseline by 1.5-2.3 points
- Introduces only 0.4% additional parameters compared to backbone models (less than 0.5% overhead)
- Demonstrates strong integrability with different backbone architectures and contrastive learning frameworks
- Performance is robust across different hyperparameter choices for number of blocks (N=3 optimal) and frequency components (m=4-8)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adaptively fusing representations from multiple Transformer blocks yields richer sentence embeddings than using only the final block's hidden states.
- Mechanism: The Spatial Selection (SS) module stacks the hidden states from the last `n` Transformer blocks into a 3D tensor. A squeeze-and-excitation style network then learns adaptive weights for each feature dimension across these blocks, creating a weighted sum that emphasizes the most semantically relevant features from any layer.
- Core assumption: Different Transformer blocks capture distinct semantic information (e.g., syntax vs. abstract meaning). Standard approaches that use only the last block or a simple average fail to optimally leverage this hierarchical information.
- Evidence anchors:
  - [abstract] "...different blocks exhibit varying degrees of semantic perception ability... thus rational cross-block representation fusion is a direction worth optimizing."
  - [section 1] "...lower-level blocks encode basic/structural semantic information... while later layers encode more 'semantic' information. Treating the hidden states from different blocks equally is insufficient to model this characteristic."
  - [corpus] Related work (SimCSE, PromptCSE) relies on the last block, while others (Su et al., 2021) use simple linear fusion, providing a contrast.

### Mechanism 2
- Claim: A spatial squeeze-based self-gating mechanism generates effective fusion weights with lower redundancy than traditional token-level gating.
- Mechanism: The SS module first uses a Frequency Selection (FS) module to squeeze the 3D tensor into a 1D feature descriptor. This descriptor passes through an excitation network (FC layers with a bottleneck) to produce per-block excitation vectors, which are softmaxed to create the final adaptive weights. This models dependencies between embedding features.
- Core assumption: Traditional token-level self-gating is redundant with Transformer's internal feed-forward and residual connections. A feature-level squeeze-and-excitation approach can capture dependencies not modeled by the Transformer itself.
- Evidence anchors:
  - [abstract] "SS innovatively employs a spatial squeeze based self-gating mechanism... which not only achieves fusion with low information redundancy but also captures the dependencies between embedding features."
  - [section 3.1.1] Contrasts traditional self-gating (Fig. 2) with spatial squeeze self-gating (Fig. 3), stating the latter "avoids the information redundancy caused by conflicts between the previous self-gating mechanisms and the residual connections in Transformers."

### Mechanism 3
- Claim: Using selected low-frequency Discrete Cosine Transform (DCT) basis functions for spatial squeeze preserves more semantic information than Global Average Pooling (GAP).
- Mechanism: The FS module replaces standard GAP by splitting the input feature tensor and applying different learnable 2D DCT basis functions (specifically low-frequency ones like `Low-4` or `Low-8`) to each part. The results are concatenated to form the final squeezed feature descriptor.
- Core assumption: GAP is equivalent to using only the DC component (zero frequency) of a DCT and thus discards valuable higher-frequency components (`x0,1`, etc.) that carry semantic nuance. A multi-spectral squeeze using a broader set of low frequencies is more expressive.
- Evidence anchors:
  - [abstract] "The nested FS replaces GAP with different DCT basis functions to achieve spatial squeeze with low semantic loss."
  - [section 3.2.1] Equations 6-8 mathematically derive GAP as a single component of DCT, demonstrating information loss.
  - [section 5.1.2] Figure 5 shows `Low-4` and `Low-8` DCT basis functions outperform `GAP` (`Low-1`) on average STS scores.

## Foundational Learning

- Concept: **Squeeze-and-Excitation (SE) Networks**
  - Why needed here: This is the core pattern of the SS module. It involves a "squeeze" (global information embedding) and an "excitation" (adaptive recalibration via learned weights).
  - Quick check question: For a feature map of shape `(H, W, C)`, what is the shape of the tensor after the squeeze operation and after the excitation operation?

- Concept: **Discrete Cosine Transform (DCT)**
  - Why needed here: This signal processing technique is the foundation of the FS module, used to decompose spatial features into frequency components for more expressive squeezing.
  - Quick check question: If you perform a 1D DCT on a constant sequence like `[5, 5, 5, 5]`, what is the value of the zero-frequency (DC) component, and what are the values of all other frequency components?

- Concept: **Sentence Representation Learning with Contrastive Learning**
  - Why needed here: This is the downstream task (e.g., SimCSE) used to evaluate S2Sent. The goal is to map semantically similar sentences closer in vector space.
  - Quick check question: In unsupervised contrastive learning for sentences, what typically constitutes a "positive" pair for a given anchor sentence during training?

## Architecture Onboarding

- Component map: Transformer Encoder -> Stacked Concatenation -> Frequency Selection (FS) Module -> Spatial Selection (SS) Module -> Final Sentence Representation
- Critical path: Inputs from last N blocks -> Stacking -> FS Module (squeezing) -> SS Module (weight learning & fusion)
- Design tradeoffs:
  - **n (number of blocks)**: Larger `n` uses more information but adds compute. The paper finds performance saturates, with `n=3` being a strong, lightweight choice.
  - **m (frequency components)**: Larger `m` captures more frequency info but may add noise. `Low-4` to `Low-8` is recommended.
  - **Pooling**: **Always use mean pooling** for the final 1D sentence vector. The paper shows S2Sent's adaptive fusion breaks the [CLS] token's semantic integrity.
- Failure signatures:
  - **Performance drop compared to baseline**: May indicate incorrect DCT basis implementation or weight calculation.
  - **Significant underperformance with [CLS] pooling**: A strong sign the architecture is not being applied as intended; switch to mean pooling.
  - **Sensitivity to reduction ratio `r`**: If `r` is too small, overfitting may occur; if too large, expressive power drops. Stick to the 4-24 range.
- First 3 experiments:
  1. **Baseline Reproduction**: Implement `Stack–S2Sent(2D SS)` with `n=Last3` blocks on BERT-SimCSE. Compare its STS-B score against the `Last1 –Base` and `Last3 –Avg.` baselines from Table 2 to validate the SS module.
  2. **FS Ablation**: Compare the full S2Sent against a variant where the FS module is replaced by standard Global Average Pooling (GAP). This validates the core claim that DCT-based squeeze is superior and is the experiment shown in Figure 5.
  3. **Pooling Method Validation**: Train two S2Sent models, one using [CLS] pooling and one using mean pooling for the final sentence vector. Observe the drastic performance drop with [CLS] pooling to confirm the architectural constraint reported in Table 3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can S2Sent be effectively adapted for sentence representation learning using generative Large Language Models (LLMs)?
- Basis in paper: [explicit] Section 7 (Limitations) states that S2Sent is currently limited to Transformer-based encoders and asks: "Whether generative LLM-based sentence representations can support such dynamic cross-layer representation fusion remains an open question worth exploring."
- Why unresolved: The current method relies on the specific architectural properties of encoders like BERT/RoBERTa, whereas generative LLMs utilize different training paradigms and structural mechanics.
- What evidence would resolve it: Successful implementation and performance evaluation of S2Sent on decoder-only or encoder-decoder generative models.

### Open Question 2
- Question: Is the S2Sent framework transferable to stack-based or Inception-based architectures in non-NLP domains?
- Basis in paper: [explicit] The conclusion posits that the framework is applicable to "stack-based or Inception-based encoding structures, revealing its potential in other application scenarios."
- Why unresolved: The paper validates the method strictly on NLP tasks (Semantic Textual Similarity) using Transformer backbones, leaving its utility in computer vision or other domains unproven.
- What evidence would resolve it: Experiments applying the nested selector to CNNs or Vision Transformers (ViTs) demonstrating performance gains in tasks like image classification.

### Open Question 3
- Question: Can the S2Sent mechanism be modified to prevent the performance degradation observed when using [CLS] token pooling?
- Basis in paper: [inferred] Appendix A highlights a "surprisingly sharp decline" in performance when using [CLS] pooling with S2Sent, which the authors attribute to the loss of semantic information during weighted fusion.
- Why unresolved: The paper identifies the failure mode—where [CLS] representations lose semantic meaning under weighted fusion—but does not offer a structural solution to mitigate this loss.
- What evidence would resolve it: A variant of the selection mechanism that preserves semantic integrity for [CLS] tokens or achieves performance parity with average pooling.

## Limitations

- **Mechanism 3 (DCT-based spatial squeeze) has the weakest evidential support**, with modest empirical margins and limited theoretical justification for specific frequency selection.
- **Core assumption about hierarchical semantic decomposition across Transformer blocks is plausible but not rigorously tested**, with alternative explanations possible for observed improvements.
- **Limited ablation studies create uncertainty about individual mechanism contributions**, lacking thorough isolation of Spatial Selection versus Frequency Selection effects.

## Confidence

- **High confidence**: The overall approach of using multi-block fusion outperforms single-block baselines, as evidenced by consistent improvements across all seven STS benchmarks and multiple backbone architectures (BERT and RoBERTa).
- **Medium confidence**: The spatial squeeze-and-excitation mechanism for cross-block fusion is effective, supported by the consistent performance gains and the architectural distinction from existing methods.
- **Medium confidence**: The integrability and scalability claims are well-supported, with negligible parameter overhead (<0.5%) and robust performance across different hyperparameter settings.
- **Low confidence**: The specific claim that DCT-based frequency selection is superior to GAP for spatial squeeze, due to limited theoretical justification and modest empirical margins.

## Next Checks

1. **Independent replication on a new dataset**: Train and evaluate S2Sent on a different semantic similarity benchmark not used in the original paper (e.g., PAWS or Quora Question Pairs) to verify generalization beyond the seven STS tasks.

2. **Ablation study isolating mechanisms**: Implement and compare three variants - (a) S2Sent with both SS and FS modules, (b) S2Sent with only SS module using GAP instead of FS, and (c) S2Sent with only FS module using fixed uniform weights instead of adaptive SS. This would quantify the individual contributions of spatial selection and frequency selection.

3. **Analysis of learned fusion weights**: Visualize and analyze the adaptive weights learned by the SS module across different sentence pairs to verify they actually capture meaningful semantic relationships between blocks, rather than simply averaging or memorizing patterns.