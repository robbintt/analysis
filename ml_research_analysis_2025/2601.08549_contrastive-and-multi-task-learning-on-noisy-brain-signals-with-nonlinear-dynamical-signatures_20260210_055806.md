---
ver: rpa2
title: Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical
  Signatures
arxiv_id: '2601.08549'
source_url: https://arxiv.org/abs/2601.08549
tags:
- learning
- contrastive
- loss
- classification
- chaos
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a two-stage multitask learning framework for
  EEG analysis that combines denoising, dynamical modeling, and contrastive representation
  learning. A denoising autoencoder suppresses artifacts and stabilizes temporal dynamics,
  while a multitask Transformer jointly performs motor imagery classification, chaotic/non-chaotic
  regime discrimination via Lyapunov exponents, and self-supervised contrastive learning
  with NT-Xent loss.
---

# Contrastive and Multi-Task Learning on Noisy Brain Signals with Nonlinear Dynamical Signatures

## Quick Facts
- **arXiv ID:** 2601.08549
- **Source URL:** https://arxiv.org/abs/2601.08549
- **Reference count:** 40
- **Primary result:** Introduces a two-stage multitask learning framework for EEG analysis that combines denoising, dynamical modeling, and contrastive representation learning.

## Executive Summary
This work introduces a two-stage multitask learning framework for EEG analysis that combines denoising, dynamical modeling, and contrastive representation learning. A denoising autoencoder suppresses artifacts and stabilizes temporal dynamics, while a multitask Transformer jointly performs motor imagery classification, chaotic/non-chaotic regime discrimination via Lyapunov exponents, and self-supervised contrastive learning with NT-Xent loss. The architecture leverages a convolutional backbone and Transformer encoder to capture spatial-temporal structure and dynamical signatures. Empirical results show the framework improves robustness and generalization, outperforming strong baselines and state-of-the-art methods on motor imagery EEG decoding tasks. Ablation studies confirm the complementary contributions of denoising, dynamical features, and self-supervised learning, with the full model achieving balanced performance gains across classification and dynamical characterization tasks.

## Method Summary
The framework consists of two stages: (1) a Denoising Autoencoder (DAE) that suppresses artifacts and stabilizes temporal dynamics using SmoothL1 and Spectral losses, and (2) a Multitask Transformer that performs motor imagery classification, chaotic/non-chaotic regime discrimination via Lyapunov exponents, and self-supervised contrastive learning with NT-Xent loss. The DAE is trained first on raw EEG data with clean targets generated via bandpass filtering. Chaos labels are generated using GTF-shPLRNN to estimate Lyapunov exponents. The MTL stage uses a CNN backbone followed by a Transformer encoder with three heads: classification (real vs. imagery), dynamics (chaotic vs. non-chaotic), and contrastive learning. The model is trained on BCI2000 and BNCI Horizon 2020 datasets with augmentations including jitter, scaling, and time masking.

## Key Results
- The full multitask framework outperforms strong baselines and state-of-the-art methods on motor imagery EEG decoding tasks.
- Ablation studies confirm the complementary contributions of denoising, dynamical features, and self-supervised learning.
- The denoising autoencoder effectively suppresses artifacts while preserving physiological EEG features (Alpha/Beta peaks).
- Contrastive learning with lighter augmentations (jitter/scaling) improves robustness without harming dynamical detection.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Separating signal reconstruction (denoising) from discriminative tasks appears to stabilize the learning of temporal dynamics.
- **Mechanism:** The Denoising Autoencoder (DAE) suppresses non-neural artifacts and enforces smoothness before the signal reaches the classifier. This prevents the downstream Transformer from overfitting to high-frequency noise, allowing the attention mechanism to focus on long-range neural dependencies rather than artifact correction.
- **Core assumption:** Assumes that noise in EEG signals (muscle artifacts, blinks) is spectrally or temporally distinct from the underlying neural "dynamical signatures" (chaotic vs. non-chaotic patterns).
- **Evidence anchors:** [abstract] Mentions the DAE is trained to "suppress artifacts and stabilize temporal dynamics." [section 4.1] States the DAE effectively distinguishes real from imagined movements in low SNR conditions. [corpus] *Data Denoising and Derivative Estimation for Data-Driven Modeling of Nonlinear Dynamical Systems* supports the general principle that denoising is a prerequisite for reliable nonlinear dynamic modeling.
- **Break condition:** If the noise is structured in a way that resembles the underlying chaotic attractor (e.g., adversarial noise), the DAE may inadvertently remove relevant neural features or fail to separate the signal components.

### Mechanism 2
- **Claim:** The auxiliary task of classifying chaotic vs. non-chaotic regimes likely acts as a specialized regularizer for the shared encoder.
- **Mechanism:** By forcing the model to predict Lyapunov exponent-based labels (a measure of sensitivity to initial conditions), the encoder must learn representations that preserve the nonlinear structure of the time series. This steers the model away from learning superficial linear correlations that may not generalize across subjects.
- **Core assumption:** Assumes that Motor Imagery (MI) tasks are correlated with specific dynamical regimes (chaotic/non-chaotic) and that these dynamics carry unique information not fully captured by standard spectral features.
- **Evidence anchors:** [abstract] States the dynamical task "encourages sensitivity to nonlinear brain dynamics." [section 5.2] Ablation study shows performance drops when the dynamical component is removed, indicating its complementary contribution. [corpus] *UniMind: Unleashing the Power of LLMs for Unified Multi-Task Brain Decoding* generally supports multi-task learning for robustness, though specific dynamical regularization is unique to this paper.
- **Break condition:** If the Lyapunov exponent labels (derived via GTF-shPLRNN or energy-based tagging) are noisy or inaccurate, this auxiliary task could act as a distractor rather than a regularizer, confusing the primary gradient signal.

### Mechanism 3
- **Claim:** Contrastive learning with EEG-specific augmentations enforces invariance to session-specific artifacts.
- **Mechanism:** The NT-Xent loss pulls together augmented views of the same trial (e.g., with jitter or channel dropout) while pushing apart different trials. This forces the shared encoder to learn features that are robust to input perturbations, effectively treating noise variability as a nuisance to be ignored rather than a signal to be learned.
- **Core assumption:** Assumes that the chosen augmentations (jitter, scaling, masking) simulate realistic noise without altering the semantic content (e.g., the imagined motor class) of the EEG signal.
- **Evidence anchors:** [abstract] Highlights that contrastive learning improves "robustness and generalization." [section 4.3] Details the use of NT-Xent loss to align latent representations of augmented views. [corpus] *CoSupFormer : A Contrastive Supervised learning approach for EEG signal Classification* validates the efficacy of contrastive paradigms in handling EEG noise and channel variability.
- **Break condition:** If augmentations are too aggressive (e.g., large time masks) or the batch size is too small (limiting negative pairs), the contrastive objective may fail to converge or learn trivial features.

## Foundational Learning

### Lyapunov Exponents (Dynamical Systems)
- **Why needed here:** Understanding that a positive exponent indicates chaos (sensitivity to initial conditions) while a negative/zero one implies order is critical for interpreting the "chaos detection" head of the model.
- **Quick check question:** If a system's maximum Lyapunov exponent is negative, does it suggest a chaotic attractor or a periodic orbit?

### NT-Xent Loss (Contrastive Learning)
- **Why needed here:** This is the mathematical engine of the self-supervised component; knowing it optimizes cosine similarity helps explain why the model needs a projection head and L2 normalization.
- **Quick check question:** In a batch of size N, does NT-Xent treat the other 2N-2 samples in the batch as positives or negatives for a given pair?

### Denoising Autoencoders (DAE)
- **Why needed here:** The paper relies on a two-stage pipeline where the DAE cleans the signal. You must understand that the DAE learns to map corrupted inputs to clean targets, not just dimensionality reduction.
- **Quick check question:** During inference, does the DAE require the clean signal target to reconstruct the input?

## Architecture Onboarding

### Component map:
Raw Multi-channel EEG ($B \times C \times T$) -> 1D Convolutional DAE (Encoder: $C \to 32$; Decoder: $32 \to C$) -> Denoised Signal -> CNN backbone (kernels [5,3], filters [32,64]) -> Transformer Encoder ($L$ layers, $H$ heads) -> Classification Head (Linear layer -> Softmax) -> Real vs. Imagery labels; Dynamics Head (Linear layer -> Softmax) -> Chaotic vs. Non-chaotic labels; Contrastive Head (Projection MLP -> L2 Norm) -> NT-Xent Loss

### Critical path:
The flow of gradients from the joint loss back to the Transformer backbone. The performance relies on the DAE successfully cleaning the input so the Transformer can attend to temporal dependencies without being distracted by artifacts.

### Design tradeoffs:
- Full vs. Light Contrastive: The paper notes that "Full Contrastive" (with time masking/dropout) can hurt dynamical (chaos) detection slightly compared to "Light Contrastive" (jitter/scaling only), likely because masking destroys the precise temporal structure needed for Lyapunov estimation.
- Pseudo-label quality: The chaos detection head relies on unsupervised Lyapunov estimation (GTF-shPLRNN), which introduces label noise but removes the need for manual dynamical annotation.

### Failure signatures:
- DAE Collapse: Output is a zero-mean constant line (SmoothL1 loss plateau).
- Contrastive Collapse: All embeddings converge to a single point (check temperature $\tau$ or batch size).
- Negative Transfer: Chaos detection accuracy is high (>90%) but MI classification drops below baseline (Transformer overfitting to dynamical features irrelevant to motor imagery).

### First 3 experiments:
1. **DAE Validation:** Train the DAE (Stage 1) alone and visualize PSD (Power Spectral Density) of raw vs. reconstructed signals to ensure physiological bands (alpha/beta) are preserved while high-frequency noise is dampened.
2. **Ablation by Head:** Train the backbone with only the MI classification head, then add the Contrastive head. Observe the delta in validation F1 score to quantify the contribution of self-supervision.
3. **Chaos Label Sensitivity:** Vary the threshold used to generate "chaos" pseudo-labels from the Lyapunov exponents and plot the correlation between Chaos Head accuracy and MI Head accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does expanding the dynamical classification beyond binary chaotic/non-chaos labels to include periodic, quasi-periodic, and no-attractor states improve the robustness of motor imagery decoding?
- **Basis in paper:** [explicit] The conclusion states: "Future work could enhance the classification by including additional signal types – e.g. periodic, quasi-periodic, no attractor – offering deeper insights."
- **Why unresolved:** The current framework relies on a binary auxiliary task (chaos vs. non-chaos) derived from Lyapunov exponents, potentially oversimplifying the dynamical complexity of brain states.
- **What evidence would resolve it:** Comparative performance metrics (Accuracy/F1) showing whether a multi-class dynamical regularization task outperforms the proposed binary setup on the BCI2000 and BNCI datasets.

### Open Question 2
- **Question:** Can Graph Neural Networks (GNNs) replace the CNN backbone to more effectively capture the spatial relationships among EEG channels?
- **Basis in paper:** [explicit] The authors propose: "we might explore alternative backbone models, such as Graph Neural Networks (GNNs), to better represent spatial relationships among EEG channels."
- **Why unresolved:** The current architecture uses a CNN-Transformer hybrid, which captures spatial features but may not explicitly model the graph-like topology of electrode placement as effectively as GNNs.
- **What evidence would resolve it:** Empirical results from a Leave-One-Subject-Out (LOSO) comparison between the current CNN-based model and a GNN-based variant using identical training protocols.

### Open Question 3
- **Question:** Can specific explainable AI (XAI) techniques be integrated to validate that the model relies on neurophysiologically relevant features rather than artifacts?
- **Basis in paper:** [inferred] The Limitations section notes: "the deep learning model lacks interpretability, highlighting the need for explainable AI techniques to enhance transparency and clinical relevance."
- **Why unresolved:** While the model demonstrates high predictive accuracy, the specific features learned by the Transformer and the contribution of the Lyapunov exponent task to the latent space remain opaque.
- **What evidence would resolve it:** Visualizations (e.g., attention maps or saliency plots) that correlate high-confidence predictions with established motor imagery spectral patterns, confirming clinical relevance.

## Limitations

- The primary uncertainty lies in the stability and quality of the pseudo-labels for chaotic vs. non-chaotic regime classification, as the GTF-shPLRNN implementation details for stable Lyapunov spectrum calculation are critical and potentially fragile.
- There is a significant discrepancy in reported hyperparameters between the main text and appendix (e.g., training epochs: 30 vs. 200-250; learning rate: 1e-4 vs. 1e-3), which could materially affect reproducibility and performance comparisons.
- The effectiveness of the denoising autoencoder depends on the assumption that EEG artifacts are spectrally or temporally separable from underlying neural dynamics—a condition that may not hold for all noise types, particularly structured or adversarial noise.

## Confidence

- **High Confidence:** The denoising autoencoder effectively suppresses artifacts and stabilizes temporal dynamics, as evidenced by qualitative improvements in low SNR conditions and preservation of physiological frequency bands (Alpha/Beta peaks) in PSD analysis.
- **Medium Confidence:** The contrastive learning component improves robustness and generalization, particularly when using lighter augmentations (jitter/scaling). The ablation study shows performance drops when the dynamical component is removed, supporting its complementary role, though the exact mechanism of regularization via Lyapunov-based labels remains partially speculative.
- **Low Confidence:** The claim that the full multitask framework significantly outperforms strong baselines on motor imagery EEG decoding tasks is undermined by the lack of explicit baseline comparisons in the main text and conflicting hyperparameter reports that prevent direct replication.

## Next Checks

1. **Label Quality Validation:** Perform a sensitivity analysis by varying the threshold for generating chaos vs. non-chaos labels from the Lyapunov exponents. Plot the correlation between Chaos Head accuracy and MI Head accuracy to ensure that the dynamical auxiliary task is not acting as a distractor.

2. **Hyperparameter Reconciliation:** Conduct experiments to determine the optimal training epochs and learning rate for the MTL stage by testing the full range reported (30 vs. 200 epochs; 1e-4 vs. 1e-3 learning rate) and comparing convergence curves and final performance.

3. **DAE Artifact Separation Test:** Systematically add structured noise types (e.g., simulated muscle artifacts, EOG artifacts) to clean EEG signals and measure the DAE's ability to suppress these artifacts while preserving underlying neural dynamics. Use PSD and time-domain visualizations to confirm artifact removal without oversmoothing.