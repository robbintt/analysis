---
ver: rpa2
title: 'Decoding Memes: Benchmarking Narrative Role Classification across Multilingual
  and Multimodal Models'
arxiv_id: '2506.23122'
source_url: https://arxiv.org/abs/2506.23122
tags:
- memes
- other
- victim
- multimodal
- hvvmemes-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study benchmarks narrative role classification in memes across
  multilingual and multimodal models, focusing on identifying Hero, Villain, Victim,
  and Other roles in English and code-mixed (English-Hindi) content. The authors evaluate
  25+ models including fine-tuned multilingual transformers, sentiment-tuned classifiers,
  instruction-tuned LLMs, and multimodal vision-language models under zero-shot settings.
---

# Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models

## Quick Facts
- **arXiv ID**: 2506.23122
- **Source URL**: https://arxiv.org/abs/2506.23122
- **Reference count**: 40
- **Primary result**: Larger multilingual and multimodal models outperform smaller ones in narrative role classification for memes, but struggle consistently with identifying "Victim" roles, especially in code-mixed (English-Hindi) content.

## Executive Summary
This study benchmarks 25+ models for narrative role classification in memes, focusing on identifying Hero, Villain, Victim, and Other roles across English and code-mixed (English-Hindi) content. The authors evaluate both fine-tuned multilingual transformers and zero-shot multimodal vision-language models using four prompt strategies. Key findings show that larger models like DeBERTa-v3 and Qwen2.5-VL achieve better performance, but consistent challenges remain in identifying Victim roles and generalizing across cultural and code-mixed content. Results highlight the importance of cultural grounding and multimodal reasoning for nuanced narrative understanding in meme content.

## Method Summary
The study evaluates narrative role classification using three meme datasets: HVVMemes (English, skewed), HVVMemes-2 (English, balanced), and HVVMemes-2 (EnHi, code-mixed). Twenty-five models are tested under zero-shot settings, including fine-tuned multilingual transformers, sentiment-tuned classifiers, instruction-tuned LLMs, and multimodal vision-language models. Four prompt strategies (P1-P4) are systematically compared, ranging from basic instructions to hybrid prompts combining structured instructions with explicit role definitions. Performance is measured using macro F1-score, precision, and recall.

## Key Results
- Larger models like DeBERTa-v3-large achieve F1 scores of 0.54, outperforming smaller multilingual transformers
- Multimodal models (Qwen2.5-VL, LLaVA-NeXT) show consistent gains over text-only baselines by integrating visual-contextual cues
- Victim role classification remains persistently challenging across all models, with F1 scores approaching zero even with hybrid prompt engineering

## Why This Works (Mechanism)

### Mechanism 1
Multimodal vision-language models outperform text-only models on nuanced narrative role classification by integrating visual-contextual cues with textual semantics. VLMs align visual features with language representations, enabling interpretation of satirical or culturally grounded imagery that text-only models miss. This cross-modal grounding helps disambiguate implicit portrayals. Break condition: When OCR quality is poor or visual content is non-informative, multimodal advantage degrades to text-only baseline.

### Mechanism 2
Scale improves narrative role classification, but gains are class-dependent—larger models excel at "villain" detection while "victim" remains consistently difficult. Larger models capture more nuanced semantic patterns and have stronger inductive biases from pre-training, but victim roles require subtle contextual reasoning about suffering/harm that lacks explicit lexical markers. Break condition: When class boundaries are semantically ambiguous, scale alone insufficient.

### Mechanism 3
Hybrid prompt design combining structured instructions with role definitions yields marginal but consistent improvements across roles. Prompt P4 provides task framing and semantic grounding, reducing ambiguity in model interpretation without over-constraining outputs. Break condition: Overly rigid prompts may constrain flexibility, hurting generalization on ambiguous cases.

## Foundational Learning

- **Code-mixed language processing (Hinglish)**: Needed because HVVMemes-2 (EnHi) test set contains transliterated, idiomatic expressions requiring models to handle linguistic mixing. Quick check: Can you explain why a model trained only on English might misclassify "karne wale bohot hai" as non-informative?

- **Narrative role semantics (Hero/Villain/Victim/Other)**: Needed because classification depends on understanding implicit framing—not sentiment or hate speech directly, but how entities are positioned narratively. Quick check: How does "victim" differ from "other" when an entity is mentioned but not actively harmed in the meme context?

- **Zero-shot multimodal evaluation**: Needed because all models evaluated under zero-shot settings; understanding how to assess generalization without task-specific fine-tuning is critical. Quick check: What metrics would indicate a model is generalizing vs. memorizing class distributions?

## Architecture Onboarding

- **Component map**: Input (Meme image + OCR text + entity query) -> Encoder (Vision encoder + Text encoder) -> Fusion (Cross-modal attention/concatenation) -> Classifier (Role prediction head or LLM generation)

- **Critical path**: 1) OCR quality directly impacts text-only baselines; 2) Visual-text alignment determines multimodal gains; 3) Prompt design governs LLM instruction-following

- **Design tradeoffs**: Text-only vs. multimodal (multimodal better for nuanced memes but adds complexity); Fine-tuned vs. zero-shot LLM (fine-tuned more accurate for specific domains; zero-shot more generalizable); Prompt complexity (more structure helps ambiguous cases but may over-constrain)

- **Failure signatures**: High "Other" recall, low precision (model defaulting to safe class); Near-zero "Victim" F1 (semantic ambiguity not resolved); Disparate performance across English vs. code-mixed (insufficient multilingual grounding)

- **First 3 experiments**: 1) Establish baseline with DeBERTa-v3-large on HVVMemes (En) to reproduce reported F1=0.54; 2) Compare LLaVA-NeXT with P3 vs. P4 prompts on code-mixed test set to isolate prompt effects; 3) Ablate visual input from Qwen2.5-VL-7B (text-only mode) to quantify multimodal contribution

## Open Questions the Paper Calls Out

- **How can deep contextual modeling or fine-tuning overcome the specific inability of current models to identify the "Victim" role?** The authors state victim detection emerged as a persistent challenge, calling for deeper contextual modelling. Across all evaluated models, the F1 score for "Victim" consistently approached zero even with hybrid prompt engineering. Evidence: A model achieving "Victim" F1 comparable to "Villain" (>0.45) would resolve this.

- **To what extent does the inclusion of socio-pragmatic features improve generalization in code-mixed (Hinglish) narrative role detection?** The Conclusion lists incorporation of socio-pragmatic features as a specific future direction to address difficulty of modeling subtle narrative framings. Current models fail to capture idiomatic phrasing and cultural references essential to code-mixed content. Evidence: A benchmark study showing augmented model outperforms standard multilingual transformers on code-mixed semantic role classification would resolve this.

- **Can role-adaptive training objectives effectively resolve the semantic ambiguity between the "Other" and narrative roles?** The Conclusion suggests developing role-adaptive training objectives to handle nuanced and culturally nuanced content where models currently struggle. Confusion matrices reveal systematic misclassification, where models frequently confuse "Other" with "Villain" or "Victim". Evidence: A role-adaptive model demonstrating reduced false-positive rate for "Other" while maintaining high recall for distinct narrative roles would resolve this.

## Limitations
- Study relies entirely on zero-shot evaluation, leaving true upper bounds of performance unknown
- Datasets are relatively small and sourced from constrained shared tasks (CONSTRAINT 2022, CLEF 2024), raising generalization concerns
- OCR quality and preprocessing steps for text-only baselines are unspecified, introducing potential variability

## Confidence
- **High Confidence**: Larger models (DeBERTa-v3, Qwen2.5-VL) outperform smaller models in F1 score
- **Medium Confidence**: Multimodal models provide consistent advantages over text-only baselines
- **Low Confidence**: Victim class is inherently harder to identify due to semantic ambiguity

## Next Checks
1. Reproduce baselines with controlled OCR preprocessing: Run text-only models on HVVMemes using identical OCR-extracted text across all models to isolate the impact of visual input
2. Ablate prompt structure systematically: Compare P1-P4 prompts across all model sizes to quantify marginal contribution of hybrid prompting
3. Evaluate cross-dataset generalization: Test top-performing models on a held-out meme dataset outside HVVMemes/CLEF to assess transfer beyond curated test sets