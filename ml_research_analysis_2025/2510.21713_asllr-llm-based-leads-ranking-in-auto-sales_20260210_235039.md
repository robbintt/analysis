---
ver: rpa2
title: 'asLLR: LLM based Leads Ranking in Auto Sales'
arxiv_id: '2510.21713'
source_url: https://arxiv.org/abs/2510.21713
tags:
- asllr
- sales
- features
- performance
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: asLLR introduces a large language model-based lead ranking system
  for auto sales, addressing the challenge of effectively processing both structured
  tabular and unstructured textual features in CRM data. The model combines CTR loss
  and QA loss within a decoder-only LLM architecture, enabling simultaneous modeling
  of both data types.
---

# asLLR: LLM based Leads Ranking in Auto Sales

## Quick Facts
- **arXiv ID:** 2510.21713
- **Source URL:** https://arxiv.org/abs/2510.21713
- **Authors:** Yin Sun; Yiwen Liu; Junjie Song; Chenyu Zhang; Xinyuan Zhang; Lingjie Liu; Siqi Chen; Yuji Cao
- **Reference count:** 36
- **Primary result:** asLLR achieves AUC of 0.8127 on auto sales lead ranking, outperforming traditional CTR models by 0.0231

## Executive Summary
This paper introduces asLLR, a large language model-based system for ranking sales leads in the auto industry. The core innovation is integrating both structured tabular data and unstructured text communication logs through a decoder-only LLM architecture, enabling the model to capture complex interactions between numerical features and conversational content. The system achieves a 9.5% increase in sales volume in real-world A/B testing compared to traditional methods, demonstrating practical business impact beyond academic performance metrics.

The key technical contribution is the dual-loss training objective combining CTR loss with QA loss, which addresses the overfitting problem commonly encountered when fine-tuning LLMs on small, domain-specific datasets. This approach allows the model to leverage its pre-trained linguistic knowledge while learning the specific patterns of customer purchase intent in the auto sales domain.

## Method Summary
asLLR is a point-wise learning-to-rank model that processes both tabular CRM features and text communication logs through a unified LLM interface. The system converts 31 tabular features into hierarchical text prompts and feeds them alongside ASR transcripts into a decoder-only LLM (Qwen1.5 family). The model is fine-tuned using LoRA (rank=16) with a dual-loss objective: CTR loss for binary classification and QA loss for semantic understanding. Text summarization is applied to long inputs to reduce perplexity and noise, with the entire system achieving state-of-the-art performance on a proprietary dataset of 300K samples.

## Key Results
- Achieved AUC of 0.8127 on test set, outperforming traditional CTR models by 0.0231
- 9.5% increase in sales volume compared to traditional methods in real-world A/B testing
- Text summarization module improved performance on long-context inputs by reducing input perplexity
- Single epoch training with dropout=0.5 prevented overfitting while maintaining strong performance

## Why This Works (Mechanism)

### Mechanism 1: Dual-Loss Regularization
The QA loss function acts as a regularizer that prevents overfitting when fine-tuning LLMs on numerical prediction tasks. By forcing the model to optimize for both binary classification and semantic token generation, it compels the LLM to utilize its pre-trained linguistic knowledge rather than relying on shallow statistical correlations. This is evidenced by the performance drop when using only CTR loss (0.7921 AUC) versus the combined approach (0.8127 AUC).

### Mechanism 2: Unified Text Interface for Cross-Feature Interactions
Converting structured tabular features into hierarchical natural language prompts enables the Transformer's self-attention to model cross-feature interactions between numerical statistics and unstructured communication logs. This serialization approach allows the model to weigh relationships between specific communication keywords and statistical behaviors directly, capturing complex patterns that traditional embedding layers might miss.

### Mechanism 3: Text Summarization for Long-Context Performance
The summarization module reduces input perplexity and noise in long ASR transcripts, recovering performance that degrades when processing extensive, low-quality transcripts. By compressing context length (approximately 60% reduction) while preserving decisive purchase indicators, the system maintains performance on long inputs where raw transcripts would overwhelm the model's attention capacity.

## Foundational Learning

- **Concept: Low-Rank Adaptation (LoRA)**
  - Why needed: Enables efficient fine-tuning of large models (up to 14B parameters) by freezing pre-trained weights and injecting trainable rank-decomposition matrices, making industrial deployment feasible
  - Quick check: Understand how LoRA reduces trainable parameters compared to full fine-tuning and why low rank (16) was chosen to combat overfitting

- **Concept: AUC (Area Under Curve)**
  - Why needed: Measures model's ability to rank positive samples higher than negative ones, crucial for imbalanced datasets where positive to negative ratio is ~1.45%
  - Quick check: If model predicts "Yes" for every sample, accuracy might look high but AUC would be low due to inability to discriminate

- **Concept: Point-wise Learning to Rank**
  - Why needed: Scores each lead independently based on purchase probability, compatible with existing CRM workflows despite limitations in capturing relative lead value
  - Quick check: Identify why point-wise approach is preferred for CRM compatibility despite potential limitations in relative ranking

## Architecture Onboarding

- **Component map:** Input Layer (prompt constructor) -> Pre-processor (text summarization module) -> Core (decoder-only LLM with LoRA adapters) -> Output Layer (3 linear heads)

- **Critical path:** 1) Ingest raw CRM tabular + text data 2) Route long text through summarization module 3) Serialize tabular features into prompt template 4) Process through frozen LLM + LoRA 5) Extract hidden state at `<bos>` token 6) Compute CTR and QA losses 7) Backpropagate to LoRA weights and output heads

- **Design tradeoffs:** Context window vs. density (summarization reduces perplexity), latency vs. accuracy (14B models perform best but may have seconds-scale inference), generalization vs. overfitting (aggressive regularization with single epoch and high dropout)

- **Failure signatures:** Performance plateau near 0.79-0.80 (check QA loss activation), long-context degradation (verify summarization module activity), catastrophic forgetting (check QA head output coherence)

- **First 3 experiments:** 1) Train with only CTR loss vs. combined CTR+QA to validate overfitting phenomenon 2) Evaluate on test samples binned by token length with/without summarization to measure perplexity correlation 3) Run DeepFM/DCN baselines on tabular only vs. asLLR on tabular+text to quantify textual feature value

## Open Questions the Paper Calls Out

- **Open Question 1:** Does performance degradation with increased training data stem primarily from ASR noise or catastrophic forgetting of base LLM knowledge? The paper identifies inverse scaling trend but hasn't isolated root cause.

- **Open Question 2:** What regularization strategies can effectively mitigate LLM overfitting to allow training beyond single epoch? Current solution of truncating training prevents full convergence or learning complex patterns.

- **Open Question 3:** At what specific input token length does text summarization benefit outweigh information loss risk? The paper establishes trade-off but hasn't defined operational threshold for when summarization should trigger.

## Limitations

- Relies on proprietary dataset that cannot be independently verified or reproduced
- Results specific to single auto sales CRM system with unknown sampling biases
- Dual-loss mechanism depends critically on quality and consistency of ASR transcripts
- Summarization module's effectiveness contingent on preserving key purchase intent signals

## Confidence

**High Confidence:** Fundamental architecture combining LLM with LoRA for structured data, dual-loss training objective, and performance metrics are well-documented and internally consistent

**Medium Confidence:** Real-world A/B testing results showing 9.5% sales volume improvement lack detailed methodology description

**Low Confidence:** Generalizability to other industries or CRM systems, and long-term stability in production environments with shifting communication patterns

## Next Checks

1. Apply asLLR framework to different industry vertical (real estate or insurance) using publicly available CRM data to test whether 0.0231 AUC improvement holds across domains

2. Systematically vary quality of input transcripts (human-transcribed vs. ASR-generated) to quantify how transcription errors impact performance and determine if summarization adequately compensates

3. Deploy model in production environment for 3-6 months while monitoring AUC drift and lead conversion rates, comparing performance against periodic retraining to assess stability requirements