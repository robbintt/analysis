---
ver: rpa2
title: Chinese Morph Resolution in E-commerce Live Streaming Scenarios
arxiv_id: '2512.23280'
source_url: https://arxiv.org/abs/2512.23280
tags:
- morph
- live
- data
- morphs
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the problem of detecting morphs\u2014words\
  \ altered to evade censorship\u2014in Chinese e-commerce live streaming, focusing\
  \ on health and medical domains. The authors introduce the Live Auditory Morph Resolution\
  \ (LiveAMR) task, distinguishing it from prior text-based morph research by targeting\
  \ pronunciation-based evasion in live broadcasts."
---

# Chinese Morph Resolution in E-commerce Live Streaming Scenarios

## Quick Facts
- arXiv ID: 2512.23280
- Source URL: https://arxiv.org/abs/2512.23280
- Authors: Jiahao Zhu; Jipeng Qiang; Ran Bai; Chenyu Liu; Xiaoye Ouyang
- Reference count: 13
- Key outcome: Reformulates morph resolution as text-to-text generation, achieving 94% F1 on in-distribution test set and 82% on challenging out-of-distribution test set

## Executive Summary
This paper introduces Live Auditory Morph Resolution (LiveAMR), a novel task addressing pronunciation-based censorship evasion in Chinese e-commerce live streaming. Unlike prior text-based morph research, LiveAMR targets audio morphs that exploit homophones and similar pronunciations. The authors construct the first LiveAMR dataset (86,790 samples) and reformulate the task as text-to-text generation using T5 models. By leveraging LLM-based data augmentation and systematic morph substitution, they achieve state-of-the-art performance while demonstrating significant improvements in detecting regulatory violations.

## Method Summary
The approach transforms morph resolution into a text-to-text generation problem using T5/Mengzi-T5 architecture. Training data is constructed through multi-modal annotation combining ASR output, LLM suggestions, and video verification. A morph dictionary (430 original words → 2,688 morphs) enables systematic data augmentation where LLMs generate sentences containing original words, then morphs are substituted. The model is fine-tuned with specific hyperparameters (learning rate 1e-4, batch size 32, 20 epochs) and evaluated on two test sets with different ASR systems to assess robustness.

## Key Results
- T5 model achieves 94% F1 score on Test1 (in-distribution ASR)
- Performance remains strong at 82% F1 on Test2 (out-of-distribution ASR)
- LLM data augmentation improves Test2 performance by 23.47%
- Outperforms BERT-based classification approaches (52.6% vs 88.8% F1)

## Why This Works (Mechanism)

### Mechanism 1: Text-to-Text Generation for Variable-Length Morphs
The seq2seq architecture naturally handles morphs that change character count between original and variant forms. Unlike token-level correction, the encoder-decoder structure learns context-aware restoration through cross-entropy loss minimization across decoder positions. This approach shares structural similarities with grammar correction tasks where context informs restoration.

### Mechanism 2: LLM-Based Data Augmentation with Systematic Substitution
Rather than direct LLM morph generation, the approach generates diverse sentences containing original words then systematically substitutes morphs from a dictionary. This preserves contextual validity while creating synthetic training pairs. The method improves generalization on out-of-distribution test data by exposing the model to varied morph contexts.

### Mechanism 3: Multi-Modal Annotation for Quality Labels
Combining ASR output, LLM suggestions, and video verification enables higher-quality morph labeling than text-only approaches. Video context is essential because audio clarifies ambiguous cases that text cannot resolve. Human-machine collaboration iteratively improves annotation quality through LLM-assisted candidate generation and human verification.

## Foundational Learning

**Seq2seq generation with encoder-decoder transformers (T5 architecture)**
- Why needed here: The core model architecture; understanding attention mechanisms for context-aware text transformation is essential for debugging and improvement.
- Quick check question: Can you explain why T5 handles variable-length input-output pairs better than BERT-based approaches for this task?

**Chinese linguistic features: homophones, pinyin, and character structure**
- Why needed here: Morph strategies exploit Chinese phonological and orthographic properties; understanding "k糖" → "抗糖" requires recognizing pinyin similarity despite different characters.
- Quick check question: Given "手某术" → "手术", what linguistic mechanism is the morph exploiting, and why does this evade voice-based censorship?

**ASR variability and noise in live streaming environments**
- Why needed here: Different ASR systems produce different transcriptions of the same audio, creating test distribution shift; this explains why Test2 performance drops.
- Quick check question: Why might FunASR, Kaldi, and Whisper transcribe the same morph utterance differently, and how does this affect model evaluation?

## Architecture Onboarding

**Component map:**
Raw video → ASR (FunASR) → Transcribed text → LLM label suggestions → Human annotation (video + LLM hints) → Morph dictionary (430 original → 2,688 morphs) → Training data (X: morph text, Y: restored text) → Data augmentation (LLM generates sentences → morph substitution) → T5/Mengzi-T5 fine-tuning (seq2seq) → Inference: morph text → restored text

**Critical path:**
1. ASR quality directly determines training data quality; test with multiple ASR systems early
2. Morph dictionary coverage limits both annotation and augmentation; prioritize high-frequency health/medical terms
3. Video context is non-negotiable for annotation; budget for video-based verification infrastructure

**Design tradeoffs:**
- T5 vs. BERT-based methods: T5 handles length-changing transformations but requires more compute; BERT-based Seq2Edit is faster but limited to fixed-length edits (F1: 52.6% vs. 88.8% on Test1)
- LLM augmentation vs. manual annotation only: Augmentation improves Test2 performance by 23.47% but may introduce synthetic artifacts; validate augmented samples
- Strict vs. lenient evaluation: Current metric requires 100% morph restoration; consider partial credit for downstream violation detection

**Failure signatures:**
- Low recall on new ASR output: Model overfits to specific ASR error patterns; solution is diversifying ASR systems during training
- Hallucinated restorations: Model generates plausible but incorrect original words; check confidence thresholds and add negative samples
- Over-correction of negative samples: Model modifies text without morphs; increase negative sample ratio in training

**First 3 experiments:**
1. Baseline replication: Fine-tune Mengzi-T5 on released dataset, verify F1 scores (94% Test1, 82% Test2)
2. ASR robustness test: Evaluate model on transcripts from 3 different ASR systems (FunASR, Kaldi, Whisper)
3. Ablation on augmentation ratio: Train with 0%, 50%, and 100% of LLM-augmented data

## Open Questions the Paper Calls Out

**Open Question 1:** Can the LiveAMR approach generalize to other domains beyond health and medical live streaming, such as general retail, finance, or entertainment contexts where morphs may serve different purposes?

**Open Question 2:** How robust is morph resolution when deployed with diverse ASR systems beyond FunASR, particularly under varying acoustic conditions and speaking styles?

**Open Question 3:** Can the morph resolution model detect and resolve entirely novel morph patterns not present in the training dictionary or LLM-generated augmentation data?

**Open Question 4:** Is the proposed approach computationally efficient enough for real-time violation detection in live streaming scenarios?

## Limitations

- Data representation bias with severe class imbalance (7.9% positive samples) may limit performance on rare morphs
- ASR dependency creates significant generalization challenges (94% → 82% F1 between test sets)
- Strict evaluation metric requiring perfect morph restoration may overstate practical limitations

## Confidence

**High Confidence:**
- Seq2seq generation approach outperforms classification-based methods
- LLM data augmentation improves model robustness
- Video context is essential for accurate morph annotation

**Medium Confidence:**
- LiveAMR task represents novel research direction
- Proposed data augmentation strategy is optimal
- Morph dictionary provides sufficient coverage

**Low Confidence:**
- Dataset fully represents morph diversity in Chinese e-commerce live streaming
- Model performance on unseen ASR systems will match reported metrics
- Annotation quality is consistent across all samples

## Next Checks

1. **ASR Robustness Validation:** Evaluate trained model on transcripts from at least three different ASR systems (FunASR, Kaldi, Whisper) with varying noise profiles. Measure F1 score degradation and identify specific ASR error patterns that cause failures.

2. **Class Imbalance Impact Assessment:** Stratify model performance by morph frequency and health/medical domain. Identify whether rare morphs or underrepresented conditions show significantly lower recall. Consider implementing class-weighted training or oversampling strategies.

3. **Partial Credit Evaluation:** Implement a relaxed evaluation metric that awards partial credit for partially correct morph restoration (e.g., Levenshtein distance-based scoring). Compare violation detection performance using strict vs. relaxed morph resolution.