---
ver: rpa2
title: 'OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation'
arxiv_id: '2507.05965'
source_url: https://arxiv.org/abs/2507.05965
tags:
- atomic
- factscore
- fact
- facts
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces OpenFActScore, an open-source implementation
  of the FActScore framework for evaluating the factuality of text generated by large
  language models (LLMs). FActScore evaluates factual accuracy by extracting individual
  factual claims (Atomic Facts) and validating them against a trusted knowledge source.
---

# OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation

## Quick Facts
- **arXiv ID**: 2507.05965
- **Source URL**: https://arxiv.org/abs/2507.05965
- **Reference count**: 12
- **Primary result**: Open-source implementation enabling any Hugging Face model to perform FActScore factuality evaluation with 0.99 Pearson correlation to original closed-source results

## Executive Summary
This paper introduces OpenFActScore, an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models. FActScore evaluates factual accuracy by extracting individual factual claims (Atomic Facts) and validating them against a trusted knowledge source. Unlike the original FActScore, which relies on closed-source models like InstructGPT and ChatGPT, OpenFActScore enables the use of any Hugging Face-compatible model for both Atomic Fact Generation (AFG) and Atomic Fact Validation (AFV). The authors provide a detailed technical overview and evaluate multiple open-source LLMs using the original FActScore benchmark, reporting BERTScore-F1 for AFG and Error Rate for AFV. Results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance. The final setup achieves a 0.99 Pearson correlation with the original FActScore experiments, demonstrating that OpenFActScore is a reliable and cost-effective alternative for factuality evaluation.

## Method Summary
OpenFActScore implements the two-stage FActScore pipeline: Atomic Fact Generation (AFG) extracts individual factual claims from text, and Atomic Fact Validation (AFV) verifies each claim against a knowledge source. The open-source implementation replaces closed-source models with any Hugging Face-compatible model, using chat templates and system prompts compatible with causal language models. For AFG, models generate atomic facts from sentences, evaluated using BERTScore-F1 against human-annotated facts. For AFV, the system retrieves top-5 relevant Wikipedia passages via a GTR retriever and prompts the model with "<context>\nTrue or False?" The recommended configuration uses OLMo for AFG and Gemma for AFV, achieving 0.99 Pearson correlation with original FActScore results.

## Key Results
- Open models can approximate closed-source FActScore performance with 0.99 Pearson correlation
- Gemma achieves best overall performance for both AFG (0.867 BERTScore-F1) and AFV (36.7% Error Rate)
- OLMo shows strong AFG quality (0.864 BERTScore-F1) but poor AFV reliability (55.6% Error Rate)
- Qwen exhibits verbosity issues, producing explanations instead of atomic facts, resulting in lower AFG scores

## Why This Works (Mechanism)

### Mechanism 1: Atomic Fact Decomposition for Granular Evaluation
Breaking long-form text into atomic facts enables more precise factuality assessment than sentence-level evaluation. A model (e.g., Olmo or Gemma) receives a sentence and generates short statements, each containing one piece of information. This decomposition avoids "subjective partial support labels" by forcing binary judgments on discrete claims.

### Mechanism 2: Retrieval-Augmented Validation Against Knowledge Sources
Validating atomic facts against retrieved Wikipedia passages approximates human fact-checking. For each atomic fact, a GTR retriever fetches the top-5 relevant Wikipedia passages. These are prepended to a prompt asking the evaluator model (e.g., Gemma) "True or False?" The model's output is parsed for the first occurrence of either token.

### Mechanism 3: Open Model Substitution Preserves Ranking Correlation
Open-source models can replace closed-source models while maintaining high correlation with human judgments. By implementing chat templates and system prompts compatible with HuggingFace models, OpenFActScore enables any causal LM to perform AFG and AFV. The paper reports 0.99 Pearson correlation between OpenFActScore (Olmo+Gemma) and original FActScore configurations.

## Foundational Learning

- **Atomic Facts vs. Sentences**: Why needed here - The entire pipeline hinges on decomposing text into claims smaller than sentences. Without this, you cannot understand why FActScore is precision-oriented. Quick check: Given "Marie Curie was born in Warsaw and won two Nobel Prizes," what are two atomic facts?

- **Retrieval-Augmented Generation (RAG) for Verification**: Why needed here - AFV relies on retrieving passages before prompting the model. Understanding how retrievers (BM25, GTR) work helps debug low validation accuracy. Quick check: Why might a retriever fail to surface relevant evidence for a recently-created entity?

- **Correlation vs. Absolute Score Agreement**: Why needed here - The paper claims success via 0.99 correlation, but absolute scores differ. Understanding this distinction prevents misinterpreting results. Quick check: If two scoring systems have 0.99 correlation but one consistently outputs scores 10 points lower, are they interchangeable for model selection?

## Architecture Onboarding

- **Component map**: Biography prompt → LLM_subject generates text → Sentence parser → BM25 retrieves demo → HFModel (Olmo/Gemma) generates atomic facts → GTR retrieves Wikipedia passages → HFModel validates with "True or False?" prompt → Count supported facts / total facts → FActScore

- **Critical path**: Load HFModel with correct chat template (system prompt + user prompt) → Ensure Wikipedia dump is indexed for GTR retrieval → Parse model outputs for "True"/"False" tokens (first occurrence only)

- **Design tradeoffs**: Olmo vs. Gemma for AFG: Olmo is fully open (training data included) but slightly lower BERTScore (0.864 vs. 0.867). Choose based on licensing/transparency needs. Retrieval-only vs. ensemble validation: Original FActScore ensembles retrieval with non-parametric scoring; OpenFActScore defaults to retrieval-only for simplicity. Error Rate vs. BERTScore-F1: Use BERTScore for AFG quality, Error Rate for AFV alignment with humans.

- **Failure signatures**: Qwen verbosity: Produces explanations instead of atomic facts, lowering BERTScore (0.619 average). Fix: Adjust system prompt to suppress reasoning. Low retrieval recall: If Wikipedia lacks entity coverage, AFV produces false negatives. Check retrieval hits before validation. Token parsing errors: If model outputs "True, but..." the parser may miss the token. Log raw outputs during debugging.

- **First 3 experiments**: Reproduce correlation claim: Run OpenFActScore on the 183-entity benchmark with Olmo (AFG) + Gemma (AFV). Verify 0.99 correlation against original scores. Ablate AFG model: Swap Olmo for Llama3.1 and measure BERTScore-F1 change. Expect ~5% drop based on Table 1. Test domain shift: Apply the pipeline to a non-biography task (e.g., scientific summaries). Report correlation with human annotations to probe generalization limits.

## Open Questions the Paper Calls Out

### Open Question 1
Can improved prompting strategies enable models with "thinking" behaviors (e.g., Qwen) to perform competitively on Atomic Fact Generation? The authors state: "Qwen would benefit from a better suited prompting strategy, which we leave for future work." Current prompts were adapted from the original FActScore and may not accommodate models trained to output reasoning chains before answers.

### Open Question 2
Does OpenFActScore generalize to domains beyond biography writing, such as technical documentation, medical text, or news articles? The evaluation is limited to biographies, justified as "non-subjective and contain[ing] non-vague information," but no other tasks are tested. Different domains may have different atomic fact structures, knowledge source availability, and subjectivity thresholds.

### Open Question 3
What explains the divergence between Olmo's strong AFG performance (0.864 BERTScore-F1) and its poor AFV reliability (55.6% Error Rate)? The authors note this discrepancy but do not investigate causes: "Despite performing well during AFG... this suggests Olmo's factual verification may not reflect human judgments reliably." The paper does not analyze whether the issue stems from retrieval quality, prompt misalignment, or intrinsic model reasoning limitations.

## Limitations

- **Model Version Dependencies**: The paper evaluates multiple open models but does not specify exact HuggingFace model IDs or versions, creating uncertainty about reproducibility.

- **Knowledge Base Coverage**: The AFV component relies on Wikipedia retrieval, but the paper does not specify which Wikipedia dump version was used or how entities with sparse coverage were handled.

- **Generalization Boundaries**: While the paper demonstrates 0.99 Pearson correlation on biography data, it does not test whether this correlation holds for other domains.

## Confidence

- **High Confidence**: The core contribution of providing an open-source implementation of FActScore is well-supported. The technical architecture is clearly described, and the code is publicly available.

- **Medium Confidence**: The claim that open models can approximate closed-source performance is supported by the benchmark results, but absolute score differences suggest the approximation is not perfect.

- **Low Confidence**: The assertion that OpenFActScore is "cost-effective" is made without providing actual computational cost comparisons between the open and closed-source implementations.

## Next Checks

1. **Version Verification**: Reproduce the benchmark using specific HuggingFace model versions (exact model IDs) to confirm that results are consistent and not dependent on particular model releases.

2. **Cross-Domain Testing**: Apply OpenFActScore to at least two non-biography domains (e.g., scientific abstracts, news articles) and measure correlation with human judgments to validate domain generalization claims.

3. **Cost Benchmarking**: Implement both OpenFActScore and original FActScore on identical hardware and measure actual inference time and API costs to empirically validate the cost-effectiveness claim.