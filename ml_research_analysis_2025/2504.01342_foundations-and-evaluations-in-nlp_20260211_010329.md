---
ver: rpa2
title: Foundations and Evaluations in NLP
arxiv_id: '2504.01342'
source_url: https://arxiv.org/abs/2504.01342
tags:
- korean
- sentence
- evaluation
- page
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This memoir addresses two fundamental aspects of Natural Language
  Processing (NLP): the creation of linguistic resources and the evaluation of system
  performance. It introduces a morpheme-based annotation scheme for the Korean language
  that captures linguistic properties from morphology to semantics, achieving state-of-the-art
  results in various NLP tasks.'
---

# Foundations and Evaluations in NLP

## Quick Facts
- **arXiv ID:** 2504.01342
- **Source URL:** https://arxiv.org/abs/2504.01342
- **Authors:** Jungyeul Park
- **Reference count:** 0
- **Primary result:** Introduces morpheme-based annotation scheme for Korean and jp-algorithm for robust end-to-end NLP evaluation

## Executive Summary
This memoir presents two fundamental contributions to Natural Language Processing: a morpheme-based annotation scheme for Korean that achieves state-of-the-art results across multiple NLP tasks, and a novel jp-algorithm for robust evaluation of end-to-end NLP systems. The work addresses critical challenges in processing morphologically rich languages by introducing fine-grained segmentation that captures linguistic properties from morphology to semantics. The jp-algorithm overcomes limitations of traditional evaluation methods by incorporating linear-time alignment to handle preprocessing mismatches in tokenization and sentence boundary detection.

## Method Summary
The work introduces a morpheme-based annotation scheme for Korean that segments words into individual morphemes, capturing linguistic properties from morphology to semantics. This approach converts existing eojeol-based corpora to a morpheme-level CoNLL-U format using Multi-Word Token (MWT) annotations. The jp-algorithm implements a linear-time alignment method that aligns sentences and words between system outputs and gold standards before applying standard evaluation metrics. This alignment-based preprocessing corrects for mismatches in tokenization and sentence boundaries, enabling more accurate end-to-end evaluation across various NLP tasks.

## Key Results
- Morpheme-based segmentation achieves state-of-the-art results in Korean POS tagging, NER, and dependency parsing
- The jp-algorithm enables robust end-to-end evaluation by incorporating linear-time alignment to handle preprocessing mismatches
- System achieves >96% accuracy in POS tagging and >88 F1 in NER using the proposed morpheme-based format
- Provides generalizable framework for evaluating diverse NLP systems with preprocessing variations

## Why This Works (Mechanism)

### Mechanism 1: Morpheme-Based Granularity Improves Task Performance in Agglutinative Languages
- **Claim:** Fine-grained morpheme segmentation (Level 5) improves performance over coarser segmentations in POS tagging, NER, and dependency parsing for Korean.
- **Mechanism:** Reduces data sparsity from combinatorial word forms, exposes functional morphemes as independent tokens, and enables clearer pattern learning for functional roles.
- **Core assumption:** Morphological analyzer correctly identifies and segments morphemes, and downstream models effectively leverage this fine-grained input.
- **Evidence anchors:** POS tagging accuracy increases from 83.18% (Level 1) to 96.01% (Level 5); syntactic parsing F1 improves from 76.69% to 82.23%; MT BLEU improves from 5.86 to 7.98.
- **Break condition:** Performance gains may diminish with noisy segmentation or if downstream tasks require word-level context.

### Mechanism 2: Alignment-Based Preprocessing Enables Robust End-to-End Evaluation
- **Claim:** Sentence and word alignment before standard metrics corrects for tokenization and SBD mismatches between system and gold.
- **Mechanism:** jp-algorithm performs monolingual alignment by merging system and gold sequences character-by-character until matches are found, then re-indexing positions for standard metric calculation.
- **Core assumption:** Text content between system and gold is fundamentally the same; differences are only in segmentation boundaries.
- **Evidence anchors:** jp-algorithm overcomes limitations of traditional evaluation, enabling robust end-to-end evaluations across various NLP tasks.
- **Break condition:** Alignment heuristic may fail with substantive content differences (paraphrasing, insertions/deletions).

### Mechanism 3: Unified CoNLL-U Format with Morpheme MWTs Facilitates Multi-Task Learning
- **Claim:** CoNLL-U format with MWTs preserves surface word boundaries while exposing morphological segmentation for consistent multi-task learning.
- **Mechanism:** Allows sequence-labeling models to jointly learn from surface forms and fine-grained linguistic features, improving generalization.
- **Core assumption:** Annotation conversion is accurate and model architecture can handle variable-length sequences within MWTs.
- **Evidence anchors:** NER results higher with morpheme-based CoNLL-U format (88.16 F1) than original format (86.72 F1) using same model.
- **Break condition:** Format complexity may introduce errors in data parsing; unnecessary overhead for models not utilizing sub-token information.

## Foundational Learning

- **Concept: Morphological Analysis & Granularity in Agglutinative Languages**
  - Why needed here: Understanding that one "word" can contain multiple morphemes is essential to grasp why fine-grained segmentation helps.
  - Quick check question: Given a Korean word like "먹었다" (ate), can you identify the stem (먹) and the attached past tense (었) and declarative (다) morphemes?

- **Concept: Monolingual Alignment vs. Cross-Lingual Alignment**
  - Why needed here: jp-algorithm adapts alignment techniques from Machine Translation; distinguishing between aligning different languages vs. same language with different segmentations is crucial.
  - Quick check question: In cross-lingual alignment, you align English with French translation. In jp-algorithm, what do sequences L and R represent?

- **Concept: PARSEVAL Measures for Constituency Parsing**
  - Why needed here: Understanding that PARSEVAL measures (Precision, Recall, F1 on labeled brackets) are standard for evaluating constituency parse trees is prerequisite for jp-evalb discussion.
  - Quick check question: To calculate PARSEVAL precision, what constitutes a "relevant constituent"?

## Architecture Onboarding

- **Component map:** Raw System Text + Raw Gold Text → Sentence Alignment → Word Alignment → Aligned, Re-indexed System & Gold → Standard Evaluation Metric → Robust Score

- **Critical path:** The pipeline sequentially processes raw text through sentence alignment, word alignment, and finally standard evaluation metrics to produce robust scores.

- **Design tradeoffs:**
  - **Alignment Heuristic:** Simple character-level equality and length-based merging is linear-time but assumes identical content; more complex similarity measures would be robust to edits but computationally expensive.
  - **MWT Complexity:** Morpheme-based CoNLL-U format is complex to parse but provides rich linguistic features; simpler word-level format would be easier but lose benefits for morphologically rich languages.

- **Failure signatures:**
  - **Incorrect Merges:** Alignment produces non-matching merged sentences/tokens, leading to wrong re-indexing and inaccurate evaluation scores, often from low similarity thresholds or system paraphrasing.
  - **Evaluation Failure:** Standard metrics fail due to index mismatches if re-indexing after alignment is buggy, manifesting as out-of-bound token reference errors.

- **First 3 experiments:**
  1. **Sanity Check on Perfect Input:** Run jp-algorithm on system output matching gold tokenization exactly; aligned output should be identical and scores should match traditional method.
  2. **Controlled Mismatch Test:** Create test case where gold splits "can't" as "ca" and "n't" but system splits as "can" and "not"; verify alignment correctly maps these and evaluation computes correct TP/FP/FN.
  3. **Real-World End-to-End Test:** Apply full jp-errant pipeline to raw text processed by modern GEC system (gector or t5); compare F0.5 score to traditional errant on gold-tokenized input.

## Open Questions the Paper Calls Out
None

## Limitations
- Morpheme-based benefits demonstrated specifically for Korean, may not generalize to languages with different morphological structures
- jp-algorithm assumes identical text content between system and gold outputs, may fail with substantial content edits or paraphrasing
- Limited direct comparisons to state-of-the-art systems beyond those mentioned, making it difficult to assess relative contribution

## Confidence
**High Confidence:**
- Morphological analysis improvements for Korean NLP tasks are well-supported by experimental results with clear quantitative improvements

**Medium Confidence:**
- Morpheme-based CoNLL-U format benefits for multi-task learning lack external validation across different model architectures
- jp-algorithm performance in real-world scenarios beyond controlled experiments remains to be fully validated

**Low Confidence:**
- Generalizability to other morphologically rich languages beyond Korean is not demonstrated
- Long-term impact and adoption potential cannot be assessed without broader implementation studies

## Next Checks
1. **Cross-Lingual Generalization Test:** Apply the morpheme-based annotation scheme and evaluation methodology to another agglutinative language (Turkish or Finnish) to validate generalization beyond Korean.

2. **Stress Test for Alignment Algorithm:** Create comprehensive test suite with controlled content differences (insertions, deletions, paraphrasing) between system and gold outputs to systematically evaluate jp-algorithm's robustness and identify failure modes.

3. **Benchmark Comparison Study:** Compare jp-algorithm evaluation results against traditional metrics across multiple state-of-the-art systems for various NLP tasks (parsing, GEC, NER) to quantify practical impact on evaluation scores and system rankings.