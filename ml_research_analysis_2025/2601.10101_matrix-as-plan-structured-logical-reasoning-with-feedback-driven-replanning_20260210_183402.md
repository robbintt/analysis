---
ver: rpa2
title: 'Matrix as Plan: Structured Logical Reasoning with Feedback-Driven Replanning'
arxiv_id: '2601.10101'
source_url: https://arxiv.org/abs/2601.10101
tags:
- plan
- reasoning
- step
- 'true'
- lion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MatrixCoT introduces a structured chain-of-thought framework for
  logical reasoning that uses typed symbolic representations and a matrix-based dependency
  plan to improve reliability and interpretability. It replaces unstructured text-based
  reasoning with a dependency matrix encoding prerequisite-successor relationships,
  enabling explicit, machine-checkable execution.
---

# Matrix as Plan: Structured Logical Reasoning with Feedback-Driven Replanning

## Quick Facts
- arXiv ID: 2601.10101
- Source URL: https://arxiv.org/abs/2601.10101
- Reference count: 40
- Primary result: Introduces MatrixCoT, a structured chain-of-thought framework using typed symbolic representations and dependency matrices for logical reasoning, achieving state-of-the-art accuracy across five benchmarks and five LLMs.

## Executive Summary
MatrixCoT introduces a structured chain-of-thought framework for logical reasoning that uses typed symbolic representations and a matrix-based dependency plan to improve reliability and interpretability. It replaces unstructured text-based reasoning with a dependency matrix encoding prerequisite-successor relationships, enabling explicit, machine-checkable execution. A feedback-driven replanning mechanism repairs execution traces by minimally editing the matrix under semantic equivalence constraints. Evaluated across five logical reasoning benchmarks and five LLMs (both closed- and open-source), MatrixCoT achieves the highest or tied-best accuracy and shows stable performance across models, with gains of 5–9 percentage points over ablations and baselines.

## Method Summary
MatrixCoT is a four-stage pipeline that transforms natural language premises and queries into logical reasoning traces. First, the Symbol Translator converts inputs into structured symbolic representations with typed predicates and entities. Second, the Matrix-based Planner generates a plan dictionary and binary dependency matrix encoding prerequisite-successor relationships. Third, the Problem Solver executes steps in topological order using forward-chaining to reach deductive closure. Fourth, the Feedback-driven Replanner diagnoses execution failures, minimally edits the matrix under semantic equivalence constraints, and re-executes. The framework is evaluated on five benchmarks using five LLMs without training, relying on inference-time prompting with JSON schema enforcement.

## Key Results
- Achieves highest or tied-best accuracy across five logical reasoning benchmarks (PrOntoQA, ProofWriter, FOLIO, LogicalDeduction, AR-LSAT) and five LLMs.
- Matrix-based planning and feedback-driven replanning each contribute 5–9 percentage point gains over ablations.
- Shows stable performance across model scales, with largest gains on longer reasoning chains.
- Outperforms unstructured chain-of-thought and other structured baselines in both accuracy and interpretability.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Typed, structured symbolic representations reduce semantic drift and enable static verification that unstructured prose cannot support.
- Mechanism: The Symbol Translator converts natural language premises into a unified namespace with explicit type annotations, predicate signatures, and referential links. This replaces loose inline annotations (e.g., `:::` markers) with keyed, indexable fields that can be statically checked for type consistency and variable scope before execution begins.
- Core assumption: LLMs generate fewer semantic errors when constrained to fill structured schemas than when producing free-form reasoning text.
- Evidence anchors:
  - [abstract] "Specifically, we normalize and type natural language expressions and attach explicit citation fields... The plan thus becomes a verifiable artifact"
  - [section 2.1] "Compared with free-text representations that maintain only a loosely aligned structure via inline markers (e.g., ':::'), this representation reduces semantic ambiguity and the propagation of redundancy"
  - [corpus] IFDNS (arxiv 2601.07464) similarly addresses faithfulness in logical reasoning through structured approaches, but the paper does not directly compare to MatrixCoT's schema design
- Break condition: If the LLM fails to follow the JSON schema or produces malformed type annotations, the static checks cannot run and the mechanism degrades to unstructured text generation.

### Mechanism 2
- Claim: A dependency matrix encoding prerequisite-successor relationships enables explicit, machine-checkable execution ordering that prevents skipped steps and broken reasoning chains.
- Mechanism: The Matrix-based Planner outputs a binary adjacency matrix A where A[i][j]=1 means step i must complete before step j. Execution selects steps whose prerequisites are satisfied (pred(s_j) ⊆ S_done) and processes them in topological order. This eliminates ambiguity about which step depends on which, replacing implicit textual ordering.
- Core assumption: Explicit dependency encoding reduces execution errors compared to LLMs inferring dependencies from linear prose.
- Evidence anchors:
  - [abstract] "A dependency matrix encoding prerequisite-successor relationships, enabling explicit, machine-checkable execution"
  - [section 2.2] "It turns 'dependency discovery' into 'dependency execution.' Through topological scheduling and transitive reduction, it preserves the minimal sufficient dependencies. This reduces broken chains, premature use, and omissions"
  - [section 4.3] Ablation shows removing matrix-based planning (w/o MP) causes the largest accuracy drop (~9 percentage points)
  - [corpus] LogiPlan (arxiv 2506.10527) evaluates LLM planning over relational structures but focuses on benchmarking rather than the matrix encoding mechanism
- Break condition: If the generated matrix contains cycles or missing dependencies, topological execution fails; if the matrix is too sparse or dense, execution may stall or run steps out of order.

### Mechanism 3
- Claim: Feedback-driven replanning repairs execution traces by making minimal edits to the dependency matrix under semantic equivalence constraints, improving final answer accuracy.
- Mechanism: The Feedback-driven Replanner takes the execution log τ and provisional answer ŷ, applies a diagnostic function f_diag to identify failure categories (missing prerequisites, rule misuse, premature termination, redundancy), then uses edit operators (AddEdge, DelEdge, Merge, InsertGuard) to produce a revised matrix A'. Normalization removes cycles and transitively redundant edges before re-execution.
- Core assumption: Execution failures can be diagnosed and repaired through localized matrix edits rather than full re-generation.
- Evidence anchors:
  - [abstract] "A feedback-driven replanning mechanism repairs execution traces by minimally editing the matrix under semantic equivalence constraints"
  - [section 2.3] "Under semantic-equivalence constraints, it identifies omissions and defects, rewrites and compresses the dependency matrix, and produces a more trustworthy final answer"
  - [section 4.3] Ablation shows removing feedback-driven replanning (w/o EFR) causes ~5-6 percentage point drops, with larger effects on long-chain tasks
  - [corpus] IFDNS (arxiv 2601.07464) also uses iterative feedback for logical reasoning; direct comparison to MatrixCoT's matrix-edit approach is not available
- Break condition: If the diagnostic mapping misclassifies the failure type or the edit operators introduce new cycles, re-execution may produce worse results than the original.

## Foundational Learning

- **First-Order Logic (FOL) notation and inference rules**
  - Why needed here: The entire framework represents premises and queries as FOL formulas using quantifiers (∀, ∃), connectives (¬, ∧, ∨, →, ↔), and predicates. Understanding modus ponens, universal instantiation, and forward-chaining is required to follow the solver's execution logic.
  - Quick check question: Given `∀x: (IsRough(x) → IsCold(x))` and `IsRough(gary)`, what can you infer?

- **Directed acyclic graphs (DAGs) and topological ordering**
  - Why needed here: The dependency matrix A defines a DAG over reasoning steps. Execution requires understanding that steps are processed only when all predecessors are complete, and cycles must be detected and removed.
  - Quick check question: If matrix A has A[1][2]=1, A[2][3]=1, and A[3][1]=1, why is this invalid for execution?

- **Structured output prompting and schema constraints**
  - Why needed here: Each module (Translator, Planner, Solver, Replanner) outputs JSON with specific keys. Understanding how to design and enforce schemas is critical for ensuring machine-readable outputs.
  - Quick check question: What happens if an LLM outputs `{"plan": {...}, "matrix": [[0,1], [1,0]]}` when the schema expects a 3×3 matrix?

## Architecture Onboarding

- **Component map:** Symbol Translator → Matrix-based Planner → Problem Solver → Feedback-driven Replanner
- **Critical path:** Translator → Planner → Solver → Replanner. If Solver produces a correct answer with high confidence, Replanner may be skipped. The matrix A is the central artifact: it must be a valid DAG before Solver execution.
- **Design tradeoffs:**
  - Matrix size vs. granularity: More steps increase matrix dimension (O(n²) entries) but provide finer-grained dependency control
  - Replanning iterations vs. latency: Each replanning cycle adds LLM calls; the paper does not specify a maximum iteration count
  - Schema strictness vs. LLM compliance: Stricter schemas improve verifiability but may increase formatting error rates on weaker models
- **Failure signatures:**
  - **Broken chain:** Solver outputs "Unknown" when answer should be T/F; typically caused by missing edges in A (pred(s_j) incomplete)
  - **Cycle detection error:** Execution hangs or throws topological sort failure; check A for self-loops or circular dependencies
  - **Schema violation:** JSON parsing fails; LLM did not follow output format (common on smaller models per Table 2 variance)
  - **Premature termination:** Forward-chaining stops before deductive closure; diagnostic should flag "premature termination" and add missing edges
- **First 3 experiments:**
  1. **Schema compliance test:** Run Translator on 50 samples from PrOntoQA with GPT-4o-mini and Qwen2.5-72B; measure JSON parse success rate and type annotation completeness. This establishes baseline LLM capability for structured output.
  2. **Matrix validity check:** Run Planner on samples with known ground-truth dependency structures; measure precision/recall of edge presence in A. Manually inspect cases where A has cycles or missing critical edges.
  3. **Replanning benefit stratification:** Run full pipeline on ProofWriter depth-0 through depth-5 problems with and without Replanner; plot accuracy delta by depth to verify the paper's claim that EFR benefits increase with reasoning chain length.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can the matrix-based dependency planning mechanism be effectively adapted for dynamic, interactive environments such as web agents, where premises and dependencies evolve in real-time?
  - **Basis in paper:** [explicit] The introduction explicitly motivates the work by citing "web-based applications" and "cross-site web agents" as key targets, but all experimental evaluations are limited to static logical reasoning benchmarks (e.g., AR-LSAT, ProofWriter).
  - **Why unresolved:** The current methodology assumes a fixed context $P$ to generate a static plan $\Pi$, whereas interactive agents require the ability to update the dependency matrix $\mathbf{A}$ dynamically as new states are observed or actions fail.
  - **What evidence would resolve it:** An evaluation of the framework on interactive web-browsing benchmarks (e.g., WebShop) to test if the replanning mechanism can handle non-static dependencies.

- **Open Question 2:** What is the trade-off between the accuracy gains provided by the feedback-driven replanning loop and the associated computational overhead (latency and token cost)?
  - **Basis in paper:** [inferred] The methodology section describes an iterative process where the system diagnoses execution traces and re-executes the solver. While the paper reports accuracy improvements, it does not report the average number of replanning iterations, token consumption, or wall-clock time per query.
  - **Why unresolved:** It is unclear if the significant accuracy gains over single-pass baselines like CoT come at a prohibitive computational cost that would limit scalability in production environments.
  - **What evidence would resolve it:** A comparative analysis reporting the average LLM calls, token counts, and latency per problem instance for MatrixCoT versus baselines.

- **Open Question 3:** To what extent does the initial Symbol Translation phase act as a bottleneck for the framework, and can the replanner effectively correct errors originating from translation rather than planning?
  - **Basis in paper:** [inferred] The pipeline is strictly sequential (Translation $\to$ Planning $\to$ Solving), and the feedback mechanism is defined as repairing the dependency matrix and plan (Equation 10). The diagnostic mapping (Equation 9) is described in the context of execution traces and decisions, leaving the robustness of the initial translation step unexamined.
  - **Why unresolved:** If the initial translation of natural language to FOL is semantically incorrect (e.g., wrong predicate typing), the resulting matrix plan will be logically unsound but structurally valid, potentially evading the current replanning logic which focuses on dependency repairs.
  - **What evidence would resolve it:** An ablation study analyzing the source of remaining errors (Translation vs. Planning vs. Execution) to determine if a translation-feedback loop is needed.

- **Open Question 4:** How does the performance of MatrixCoT scale on extremely long reasoning chains where the size of the dependency matrix exceeds the context window or processing reliability of current LLMs?
  - **Basis in paper:** [inferred] The paper demonstrates stability up to reasoning depth 5 (Figure 4), but acknowledges that LLM-driven approaches face challenges with "long chains" and "semantic drift." The matrix representation grows as $N \times N$, potentially introducing scaling limits for proofs requiring hundreds of steps.
  - **Why unresolved:** The complexity of managing and attending to a large binary matrix in the LLM's context window for highly complex problems (beyond the tested benchmarks) remains unknown.
  - **What evidence would resolve it:** Testing the framework on datasets specifically constructed for deep multi-step reasoning (depth > 20) to identify the context limit where matrix management itself becomes the bottleneck.

## Limitations
- No code or exact prompt templates are released, making faithful reproduction challenging.
- Performance depends on LLM's ability to follow strict JSON schemas, which may degrade on smaller models.
- Scalability to extremely long reasoning chains (depth > 20) is untested and may hit context window limits.
- The symbolic representation schema is proprietary and not directly compared to alternative structured formats.

## Confidence
- **High confidence:** The core mechanism—structured symbolic representations + dependency matrix + replanning—is clearly described and empirically validated across five benchmarks.
- **Medium confidence:** The ablation results showing relative gains from each component are reproducible, but absolute gains depend on exact prompting and hyperparameters not specified.
- **Low confidence:** Generalization to domains outside first-order logic or to models with weaker reasoning capabilities without additional schema enforcement.

## Next Checks
1. **Schema Compliance Test**: Run the Translator on 50 samples from PrOntoQA with GPT-4o-mini and Qwen2.5-72B; measure JSON parse success rate and type annotation completeness to establish baseline LLM capability for structured output.
2. **Matrix Validity Check**: Run the Planner on samples with known ground-truth dependency structures; measure precision/recall of edge presence in the matrix and manually inspect cases where the matrix has cycles or missing critical edges.
3. **Replanning Benefit Stratification**: Run the full pipeline on ProofWriter depth-0 through depth-5 problems with and without the Replanner; plot accuracy delta by depth to verify the paper's claim that feedback-driven replanning benefits increase with reasoning chain length.