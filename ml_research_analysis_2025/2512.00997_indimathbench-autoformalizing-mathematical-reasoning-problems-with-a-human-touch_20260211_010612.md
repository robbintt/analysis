---
ver: rpa2
title: 'IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human
  Touch'
arxiv_id: '2512.00997'
source_url: https://arxiv.org/abs/2512.00997
tags:
- lean
- theorem
- problems
- reasoning
- problem
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces IndiMathBench, a new benchmark of 312 human-verified
  Lean 4 theorems derived from Indian Mathematical Olympiad problems. The benchmark
  is created using a scalable AI-powered pipeline combining category-based retrieval,
  iterative compiler feedback, and multi-model ensemble generation.
---

# IndiMathBench: Autoformalizing Mathematical Reasoning Problems with a Human Touch

## Quick Facts
- arXiv ID: 2512.00997
- Source URL: https://arxiv.org/abs/2512.00997
- Authors: Param Biyani; Shashank Kirtania; Yasharth Bajpai; Sumit Gulwani; Ashish Tiwari
- Reference count: 40
- Key outcome: New benchmark of 312 human-verified Lean 4 theorems from Indian Mathematical Olympiad problems; current frontier models achieve low success rates (up to 51.2% BEq pass rate for autoformalization, up to 11% pass@1 for theorem proving), highlighting benchmark difficulty.

## Executive Summary
This paper introduces IndiMathBench, a new benchmark of 312 human-verified Lean 4 theorems derived from Indian Mathematical Olympiad problems. The benchmark is created using a scalable AI-powered pipeline combining category-based retrieval, iterative compiler feedback, and multi-model ensemble generation. Human experts efficiently validate generated formalizations through an interactive VS Code dashboard. Evaluation shows that current frontier models achieve low success rates in both autoformalization (up to 51.2% BEq pass rate) and theorem proving (up to 11% pass@1 in 10-turn setting), highlighting the benchmark's difficulty. The IndiMathBench pipeline significantly reduces annotation time compared to manual formalization, and the benchmark and dashboard are released as open resources to advance research in neural theorem proving and autoformalization.

## Method Summary
The IndiMathBench pipeline consists of four stages: (1) PreProcess - categorizes problems into four domains and retrieves static Mathlib context for each category via a Claude Sonnet 4 agent; (2) Formalize - uses iterative LLM generation with up to 6 rounds of compiler feedback per problem per model; (3) PrepDashboard - aggregates 12 model outputs and generates GPT-5 summary with rankings and error analysis; (4) VS Code Dashboard - provides human annotation interface with Lean compilation, candidate comparison, and LLM notes display. The pipeline evaluates autoformalization using BEq (bidirectional equivalence) and GTED (syntactic similarity) metrics, and theorem proving using pass@1 in single-turn and 10-turn ReAct settings.

## Key Results
- Autoformalization success: 51.2% of problems had at least one model pass BEq check in 10-turn setting
- Single-model performance: Claude Opus 4 achieved 77.9% compilation rate and 21.5% BEq pass rate
- Theorem proving: Top model (o3) achieved 11% pass@1 in 10-turn setting, 5% in single-turn
- Annotation efficiency: Pipeline reduced human annotation time by 3.5× compared to manual formalization
- Geometry challenge: No geometry problems were solved by any model, highlighting domain-specific difficulties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative compiler feedback substantially improves syntactic validity of autoformalized theorems.
- Mechanism: After each LLM generation, the Lean compiler validates the code. Parse and type errors are extracted and fed back to the model for correction over up to six iterations, allowing the model to refine syntax without human intervention.
- Core assumption: LLMs can effectively interpret and act on compiler error messages to fix formalization errors.
- Evidence anchors:
  - [abstract]: "iterative compiler feedback" is listed as a core pipeline component
  - [Table 3]: Claude Opus 4 improves from 4.1% to 77.9% compilation success with documentation+feedback; GPT-5 from 30.5% to 75.3%
  - [corpus]: FMC paper (arXiv 2507.11275) uses similar feedback-based autoformalization
- Break condition: If models cannot interpret Lean error messages or if errors are semantically rather than syntactically rooted, iteration yields diminishing returns.

### Mechanism 2
- Claim: Domain-specific documentation retrieval reduces hallucination of non-existent imports and incorrect syntax.
- Mechanism: Problems are first categorized into one of four domains. A Claude Sonnet 4 agent pre-extracts relevant Mathlib definitions and notations for each category, creating static context reused across all problems in that domain.
- Core assumption: Olympiad problems within a category share sufficient structural similarities that shared documentation provides useful priors.
- Evidence anchors:
  - [Section 4.1]: "This context anchors the model with domain knowledge within each category, preventing hallucinations and syntax errors"
  - [Algorithm 1]: ctxt ← p.Cat.Ctxt retrieved per problem based on labeled category
  - [corpus]: No direct corpus comparison for category-based retrieval in theorem proving; mechanism is paper-specific
- Break condition: If a problem spans multiple domains or uses uncommon constructs not in the static context, retrieval may miss critical definitions.

### Mechanism 3
- Claim: Multi-model ensemble generation increases the probability that at least one candidate is semantically correct.
- Mechanism: Twelve models generate formalizations independently. A summary model (GPT-5) ranks candidates and identifies shared errors. Human annotators can merge correct fragments from different outputs.
- Core assumption: Models have complementary strengths; errors are not perfectly correlated across model families.
- Evidence anchors:
  - [Section 5.1]: "In 160 of the 312 problems, at least one generation passed the BEq check" (51.2%)
  - [Figure 5]: Venn diagram shows limited overlap between Claude Opus 4, Gemini 2.5 Pro, and GPT-5 BEq-passing problems
  - [corpus]: DeepSeek-Prover-V2 uses single-model approach; ensemble comparison not directly available
- Break condition: If all models share systematic biases (e.g., geometry formalization difficulty), ensemble provides no benefit for those problem classes.

## Foundational Learning

- Concept: Lean 4 type theory and theorem statement structure
  - Why needed here: Autoformalization requires expressing natural language problems as valid Lean theorem signatures with correct type annotations (e.g., distinguishing `N` from `N+` for natural numbers including/excluding zero).
  - Quick check question: Given the INMO 2018 problem in Section 4.4, why does using `N` instead of `N+` make the theorem unprovable?

- Concept: Bidirectional Extended Definitional Equivalence (BEq)
  - Why needed here: The primary evaluation metric for autoformalization quality requires understanding that BEq attempts to prove logical equivalence in both directions using heuristic and LLM-guided tactics.
  - Quick check question: Why is BEq preferred over syntactic metrics like BLEU for evaluating autoformalization correctness?

- Concept: Mathlib library organization and import conventions
  - Why needed here: The category-based retrieval mechanism depends on understanding how Mathlib structures geometric, algebraic, number-theoretic, and combinatorial definitions across modules.
  - Quick check question: What type of Lean 4 construct would you need to formalize "point D lies on segment BC" as shown in Appendix A.1.1?

## Architecture Onboarding

- Component map:
  PreProcess -> Formalize -> PrepDashboard -> VS Code Dashboard

- Critical path:
  1. Problem categorization → 2. Static context retrieval → 3. Multi-model generation with feedback → 4. Dashboard summary → 5. Human verification → 6. BEq/GTED evaluation

- Design tradeoffs:
  - Static vs. dynamic context: Static context is computed once per category (faster) but may miss problem-specific constructs that dynamic retrieval would capture
  - 12-model ensemble vs. single model: Higher compute cost and latency, but increases probability of at least one correct formalization (51.2% vs. max single-model 21.5% for Claude Opus 4)
  - Human-in-the-loop vs. fully automated: 3.5× faster than manual, but still requires Lean expertise for verification

- Failure signatures:
  - High compilation rate but low BEq score: Claude Opus 4 achieves 77.9% compilation but only 21.5% semantic correctness—indicates models learn syntax but struggle with meaning preservation
  - Geometry domain underperformance: Only 12 of 108 BEq-passing formalizations from top 3 models were geometry problems (Section 5.1)
  - o3 "graceful surrender": ~25% of o3 attempts refused to produce a proof rather than hallucinating (Appendix E)

- First 3 experiments:
  1. **Ablation on feedback iterations**: Measure BEq and compilation success at 0, 2, 4, 6 iterations to identify point of diminishing returns (preliminary data in Table 3 shows gains from zero-shot to 6 iterations).
  2. **Category-specific analysis**: Run pipeline on each domain separately to quantify which categories benefit most from documentation retrieval (geometry expected to show largest gap due to Mathlib limitations noted in Section 2).
  3. **Single-model vs. ensemble efficiency**: Compare time-to-correct-formalization using only Claude Opus 4 (best single model) vs. full 12-model ensemble with human annotation to validate the 3.5× speedup claim from Section 4.3.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the reported 3.5x speed-up in annotation efficiency statistically significant when controlling for annotator bias?
- Basis in paper: [explicit] Section 4.3 states the efficiency study is "preliminary and not tested for statistical significance" and acknowledges a "potential source of bias arising from the same annotator participating across all three workflows."
- Why unresolved: The controlled study relied on a single annotator and a limited sample size (36 problems), limiting the generalizability of the time-savings claims.
- What evidence would resolve it: A large-scale user study with diverse annotators confirming statistical significance and reduced bias.

### Open Question 2
- Question: Under what specific conditions does human-AI collaboration provide measurable value over fully automated or manual formalization?
- Basis in paper: [explicit] Section 2 notes that "no prior work has systematically evaluated these hybrid pipelines... This gap limits understanding of when and how human–AI collaboration provides measurable value."
- Why unresolved: While the paper demonstrates efficiency gains, it does not isolate the specific problem types or error categories where the collaboration is most critical versus superfluous.
- What evidence would resolve it: A breakdown of annotation edits by category (e.g., syntax vs. semantic) to quantify where human input corrects AI failures.

### Open Question 3
- Question: How can autoformalization systems be improved to handle Olympiad-level geometry given the complete failure of current models?
- Basis in paper: [explicit] Section 5.3 notes "no geometry problems were solved by any model," and Appendix B.5.3 highlights "systematic difficulties with spatial reasoning and coordinate-free geometric formalization."
- Why unresolved: Current models fail to leverage Mathlib effectively for geometry, and the pipeline's documentation retrieval does not sufficiently bridge the gap for complex spatial reasoning.
- What evidence would resolve it: Successful evaluation results on the geometry subset of IndiMathBench using a modified pipeline or specialized model.

## Limitations

- BEq metric limitations: The semantic equivalence metric relies on heuristic tactics and LLM guidance, which may not capture true logical equivalence for all problem types
- Geometry domain weakness: Complete failure on geometry problems (0/108 BEq passes) highlights systematic difficulties with spatial reasoning and limited Mathlib geometry support
- Statistical significance concerns: The 3.5× speedup claim lacks rigorous statistical validation due to small sample size and potential annotator bias

## Confidence

- High Confidence: Compilation success rates (77.9% for Claude Opus 4) and 3.5× speedup claim are directly measurable and well-supported
- Medium Confidence: BEq pass rates (51.2%) and theorem proving performance (11% pass@1) depend on external implementations not fully detailed in paper
- Low Confidence: Claims about documentation retrieval effectiveness versus dynamic retrieval would require ablation studies not fully presented

## Next Checks

1. **BEq Implementation Verification**: Obtain and validate the external BEq and GTED implementations (Liu et al., 2025b/c) to ensure the semantic equivalence metrics are correctly applied and interpreted for the IndiMathBench formalizations.

2. **Category-Specific Ablation Study**: Run the pipeline with and without the category-based documentation retrieval on each domain separately to quantify the actual benefit for geometry problems, where the paper suggests it would be most significant.

3. **Geometry Domain Stress Test**: Manually verify a sample of geometry formalizations that failed BEq but passed compilation to determine if the failures are due to Mathlib limitations (as claimed) or other factors in the autoformalization process.