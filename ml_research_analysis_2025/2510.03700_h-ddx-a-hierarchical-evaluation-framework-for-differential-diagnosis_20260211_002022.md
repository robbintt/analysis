---
ver: rpa2
title: 'H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis'
arxiv_id: '2510.03700'
source_url: https://arxiv.org/abs/2510.03700
tags:
- clinical
- diagnosis
- hierarchical
- evaluation
- hdf1
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces H-DDx, a hierarchical evaluation framework
  for assessing LLMs' differential diagnosis capabilities. Traditional flat metrics
  like Top-k accuracy fail to capture clinically meaningful near-misses, treating
  taxonomically related errors as severe failures.
---

# H-DDx: A Hierarchical Evaluation Framework for Differential Diagnosis

## Quick Facts
- arXiv ID: 2510.03700
- Source URL: https://arxiv.org/abs/2510.03700
- Reference count: 40
- Introduces H-DDx, a hierarchical evaluation framework using ICD-10 taxonomy to assess LLM differential diagnosis capabilities

## Executive Summary
H-DDx addresses the limitation of flat evaluation metrics in differential diagnosis tasks by leveraging the ICD-10 taxonomy to provide hierarchical credit assignment. Traditional Top-k accuracy treats taxonomically related diagnoses as equally wrong as distant ones, failing to capture clinically meaningful near-misses. The framework maps free-text diagnoses to ICD-10 codes using a retrieval-reranking pipeline and computes HDF1 (Hierarchical F1), which rewards predictions that share taxonomic branches with ground-truth diagnoses. Experiments on 22 leading models reveal that domain-specialized models like MediPhi are significantly undervalued by flat metrics but excel under HDF1, with up to 18 rank improvements. The framework also exposes performance differences across ICD-10 chapters and provides interpretable insights into model strengths and weaknesses.

## Method Summary
The H-DDx framework evaluates differential diagnosis capabilities by mapping free-text predictions to ICD-10 codes through a two-stage pipeline: text-embedding retrieval (top-15 candidates) followed by LLM reranking (top-1 selection). For each case, the framework expands both predicted and ground-truth diagnosis sets to include all ancestor codes in the ICD-10 hierarchy (Chapter → Section → Category → Subcategory). Hierarchical precision (HDP) and recall (HDR) are computed over these augmented sets, then combined into HDF1 via harmonic mean. The metric is macro-averaged across cases to treat each case equally regardless of diagnosis frequency. Validation on 730 DDXPlus cases shows the mapping pipeline achieves 93.1% accuracy, with errors being systematically clinically adjacent near-misses.

## Key Results
- Domain-specialized models like MediPhi are undervalued by flat metrics but excel under HDF1, improving by up to 18 ranks
- Models often identify correct clinical contexts (same ICD-10 chapter) even when missing precise diagnoses
- HDF1 reveals performance differences across ICD-10 chapters, with fine-tuning benefits varying by specialty
- The framework provides clinically relevant assessments and interpretable hierarchical error patterns

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Credit Assignment via ICD-10 Taxonomy Expansion
Expanding diagnoses to include ancestral nodes enables partial credit for clinically related near-misses. For each prediction and ground truth, all ancestor codes are added to their sets. HDF1 then computes precision/recall over these augmented sets, rewarding predictions that share taxonomic branches. This assumes ICD-10 proximity correlates with clinical relevance. Break condition: If taxonomy doesn't align with clinical relevance for specific disease pairs, credit assignment misleads.

### Mechanism 2: Retrieval-Reranking Pipeline for High-Fidelity Mapping
A two-stage pipeline (embedding retrieval + LLM reranking) achieves 93.1% accuracy. First, text-embedding-3-large retrieves top-15 ICD-10 candidates. Second, gpt-4o reranker selects the best match. All unique diagnoses are mapped once to a canonical reference table. Core assumption: Mapping errors are clinically adjacent near-misses. Break condition: If errors include taxonomically distant misclassifications, HDF1 scores become unreliable.

### Mechanism 3: Revealing Hidden Strengths of Domain-Specialized Models
Domain-specialized models produce clinically coherent differential lists within correct medical domains, undervalued by flat metrics but rewarded by HDF1. Medical fine-tuning strengthens broad clinical category understanding even when rare subcategory diagnoses remain underrepresented. HDF1's partial credit captures this coherence. Break condition: If domain fine-tuning improves hierarchical scores but not actual clinical utility, the metric may overvalue specialist models.

## Foundational Learning

- **ICD-10 Taxonomy Structure (4 levels: Chapter → Section → Category → Subcategory)**: Understanding the hierarchy is essential to interpreting HDF1 scores and where models fail in the "hierarchical cascade." Quick check: Can you explain why "Influenza (J11)" and "Pneumonia (J18)" receive partial credit against each other under HDF1 but not under flat Top-k accuracy?

- **Hierarchical Precision/Recall in Multi-Label Classification**: HDF1 extends standard F1 to hierarchical multi-label settings; understanding ancestral expansion is key to implementing or modifying the metric. Quick check: If a model predicts {J18.1, J40} and ground truth is {J11.1, J47}, what augmented sets would HDF1 compare?

- **Macro-Averaging per Patient Case**: HDF1 macro-averages across cases, meaning each case contributes equally regardless of diagnosis frequency—critical for understanding score distributions on imbalanced clinical data. Quick check: Why might macro-averaging produce different model rankings than micro-averaging on a dataset where respiratory diseases dominate?

## Architecture Onboarding

- **Component map**: Patient case (chief complaint + history) → LLM generates top-5 free-text DDx → text-embedding-3-large (retrieval, top-15) → gpt-4o (rerank, top-1) → ICD-10 codes → Ancestral expansion → HDF1 scores → Per-level breakdown

- **Critical path**: Free-text DDx → Canonical mapping table (pre-computed for all models) → ICD-10 codes → Ancestral expansion → HDF1 scores

- **Design tradeoffs**: ICD-10 vs. SNOMED CT (chosen for global standard and reproducibility, but may miss clinical nuances); Static DDx list vs. sequential reasoning (evaluates snapshot predictions, not iterative process); Synthetic (DDXPlus) vs. real EHR (current validation on synthetic data; real-world correlation unknown)

- **Failure signatures**: Mapping pipeline errors (predictions outside ICD-10 coverage or ambiguous); Hierarchical cascade breakdown (model performs well at Chapter but collapses at Category)

- **First 3 experiments**: 1) Replicate mapping pipeline validation: Test retrieval + reranking on your own clinical vocabulary sample (≥100 diagnoses with clinician-mapped ground truth) to verify 93.1% accuracy holds. 2) Compare flat vs. hierarchical metrics on your model: Generate top-5 DDx for 100+ cases, compute both Top-5 Accuracy (with LLM judge) and HDF1; analyze rank correlation and divergence cases. 3) Per-chapter HDF1 breakdown: Stratify evaluation by ICD-10 chapter to identify where your model excels or fails (e.g., respiratory vs. neurological); correlate with fine-tuning data coverage.

## Open Questions the Paper Calls Out

- **Real-World Correlation**: Does the H-DDx framework's evaluation on synthetic benchmarks correlate with diagnostic utility in real-world clinical environments? Current results rely on DDXPlus, which may not fully represent the noise and complexity of actual Electronic Health Records (EHR). A study establishing correlations between high HDF1 scores and improved diagnostic efficiency or patient outcomes in live clinical settings would resolve this.

- **Ontology Richness**: Would adopting richer ontologies like SNOMED CT improve H-DDx's ability to capture clinical nuances compared to ICD-10? ICD-10 provides a standardized tree but lacks the semantic granularity required to distinguish between subtle variations in clinical presentation. Comparative benchmarking of the HDF1 metric using SNOMED CT mappings versus ICD-10 mappings on complex, ambiguous medical cases would resolve this.

- **Interactive Diagnostic Agents**: Can the H-DDx framework be effectively extended to evaluate interactive diagnostic agents that perform sequential reasoning? Current evaluation assesses a snapshot prediction, ignoring the dynamic process of information gathering and hypothesis refinement. A modified HDF1 metric that successfully tracks and scores the evolution of diagnostic accuracy across multiple turns of a clinical dialogue would resolve this.

## Limitations
- Framework evaluates static differential lists rather than the iterative diagnostic reasoning process used in clinical practice
- Current validation relies on synthetic DDXPlus data rather than real-world clinical cases
- ICD-10 taxonomy may not capture all clinical nuances and taxonomic proximity may not always align with clinical relevance

## Confidence
- **High**: Core mechanism of hierarchical credit assignment and formal definition of HDF1 are well-specified and mathematically sound
- **Medium**: Clinical relevance of taxonomic proximity is plausible but not empirically validated
- **Low**: Generalizability of findings from synthetic data to real clinical settings

## Next Checks
1. Apply H-DDx to a dataset of real clinical cases with ground-truth diagnoses to validate whether HDF1 scores correlate with actual diagnostic accuracy and clinical utility
2. Conduct a clinician study to validate whether ICD-10 taxonomic proximity consistently aligns with clinical similarity across diverse disease pairs
3. Extend the framework to evaluate sequential diagnostic reasoning rather than static differential lists, incorporating temporal dynamics of the diagnostic process