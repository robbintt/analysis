---
ver: rpa2
title: 'GCPO: When Contrast Fails, Go Gold'
arxiv_id: '2510.07790'
source_url: https://arxiv.org/abs/2510.07790
tags:
- arxiv
- training
- reasoning
- gcpo
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of reinforcement learning for
  reasoning in large language models, specifically the issue where existing methods
  like Group Relative Policy Optimization (GRPO) cannot learn from samples where all
  responses are incorrect or all correct, limiting the model's ability to improve
  its reasoning capabilities. The authors propose Group Contrastive Policy Optimization
  (GCPO), a method that incorporates external standard reference answers as golden
  answers (GA) to guide model updates.
---

# GCPO: When Contrast Fails, Go Gold

## Quick Facts
- **arXiv ID**: 2510.07790
- **Source URL**: https://arxiv.org/abs/2510.07790
- **Reference count**: 40
- **Primary result**: GCPO achieves 36.95% accuracy vs DAPO's 30.37% on six reasoning benchmarks using DeepSeek-R1-Distill-Qwen-1.5B

## Executive Summary
This paper addresses a fundamental limitation in reinforcement learning for reasoning in large language models: existing methods like Group Relative Policy Optimization (GRPO) cannot effectively learn when all responses are incorrect or all correct. The authors propose Group Contrastive Policy Optimization (GCPO), which introduces external golden answers as reference points to guide model updates. When all rollouts fail, GCPO replaces one response with the golden answer, providing clear optimization direction. Experiments show substantial improvements over baseline methods, with GCPO achieving 36.95% accuracy compared to DAPO's 30.37% on six benchmark datasets.

## Method Summary
GCPO builds upon GRPO by incorporating external standard reference answers as golden answers (GAs) to guide model updates when all rollouts fail. The method replaces one response with the GA in such scenarios, providing a clear optimization direction. Key technical innovations include sequence-level importance sampling and removal of KL divergence for improved stability. The approach enables the model to learn reasoning strategies from larger models by leveraging external reference solutions as optimization anchors.

## Key Results
- GCPO achieves 36.95% accuracy compared to DAPO's 30.37% on six benchmark datasets
- 25% improvement over DAPO on the AIME 2024 dataset
- Roughly 54% gain on the MathQA dataset compared to baseline model

## Why This Works (Mechanism)
Assumption: GCPO works by addressing GRPO's fundamental limitation of failing to learn when all responses are uniformly incorrect or correct. By introducing golden answers as external reference points, the method provides clear optimization direction in failure scenarios. The replacement of one response with the golden answer creates a meaningful contrast that enables gradient-based learning. Sequence-level importance sampling helps focus on more informative sequences, while removing KL divergence reduces optimization conflicts that might otherwise destabilize training.

## Foundational Learning
- **Group Relative Policy Optimization (GRPO)**: Baseline method that optimizes LLM reasoning through group comparisons
  - Why needed: Understanding GRPO limitations is essential to grasp GCPO's motivation
  - Quick check: Verify GRPO fails to learn when all responses are uniformly correct or incorrect

- **Reinforcement Learning for Reasoning**: Framework for improving LLM reasoning through trial-and-error
  - Why needed: GCPO operates within RL framework but addresses its specific failure modes
  - Quick check: Confirm RL-based reasoning differs from supervised fine-tuning approaches

- **Golden Answer Integration**: Using external reference solutions to guide optimization
  - Why needed: Central to GCPO's approach of providing optimization direction when rollouts fail
  - Quick check: Validate that GA replacement provides clear gradient signals

## Architecture Onboarding
- **Component Map**: Model -> Rollout Generator -> Response Evaluator -> GCPO Optimizer -> Updated Model
- **Critical Path**: Input problem → Multiple rollouts → Response evaluation → GCPO update (with GA replacement if needed) → Updated parameters
- **Design Tradeoffs**: 
  - GCPO vs DAPO: Removes KL divergence for stability but adds dependency on external golden answers
  - Single model vs multi-model: Uses only one model (1.5B) but could potentially scale
- **Failure Signatures**: 
  - Poor performance when golden answers are low quality or unavailable
  - Limited generalizability across different model architectures and sizes
- **3 First Experiments**:
  1. Verify GCPO improves over GRPO on datasets where all responses are uniformly incorrect
  2. Test GCPO with synthetic vs. human-annotated golden answers to assess GA quality sensitivity
  3. Run ablation study removing each GCPO component to isolate contributions

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly identify open questions or limitations in its discussion sections.

## Limitations
- Single-model evaluation raises generalizability concerns; improvements may not transfer to larger models
- Baseline comparison issues obscure which components drive performance gains
- External reference dependence creates practical deployment challenges and potential bias

## Confidence
- **High confidence**: GRPO's inability to learn from uniform response scenarios is well-established
- **Medium confidence**: Performance improvements are based on controlled experiments but lack broader validation
- **Low confidence**: Relative contributions of individual GCPO components are not experimentally isolated

## Next Checks
1. Conduct ablation studies isolating golden answer replacement, sequence-level importance sampling, and KL divergence removal
2. Test GCPO on at least three additional model sizes (7B, 13B, 34B) to assess generalizability
3. Systematically vary golden answer quality and source to determine method robustness