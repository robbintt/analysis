---
ver: rpa2
title: 'ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware
  Filtering'
arxiv_id: '2510.20036'
source_url: https://arxiv.org/abs/2510.20036
tags:
- tool
- tools
- toolscope
- seal-tools
- selection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces ToolScope, a framework that enhances large
  language model (LLM) agent tool selection by addressing two key challenges: tool
  overlap and input context limits. ToolScope consists of ToolScopeMerger, which automatically
  merges semantically redundant tools using a graph-based approach with Auto-Correction
  to ensure precision, and ToolScopeRetriever, which employs hybrid retrieval and
  reranking to select the most relevant tools for each query.'
---

# ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering

## Quick Facts
- arXiv ID: 2510.20036
- Source URL: https://arxiv.org/abs/2510.20036
- Authors: Marianne Menglin Liu; Daniel Garcia; Fjona Parllaku; Vikas Upadhyay; Syed Fahad Allam Shah; Dan Roth
- Reference count: 40
- Key outcome: Improves LLM agent tool selection accuracy by 8.38% to 38.6% through tool merging and context-aware filtering

## Executive Summary
ToolScope addresses two critical challenges in LLM agent tool use: tool overlap and input context limits. The framework introduces ToolScopeMerger, which automatically merges semantically redundant tools using a graph-based approach with Auto-Correction for precision, and ToolScopeRetriever, which employs hybrid retrieval and reranking to select the most relevant tools for each query. Evaluations on three state-of-the-art LLMs and three open-source benchmarks demonstrate significant improvements in tool selection accuracy while reducing context length by up to 99%.

## Method Summary
ToolScope operates through an offline merging pipeline and an online retrieval pipeline. The offline ToolScopeMerger constructs an undirected graph of semantically equivalent tools, extracts connected components, and prunes them to canonical representations using synthesized descriptions. The online ToolScopeRetriever uses hybrid search (BM25 + dense embeddings) to find candidates, then applies cross-encoder reranking to filter noise before passing a compressed tool set to the LLM agent. The system achieves up to 99% context reduction while improving selection accuracy across multiple benchmarks.

## Key Results
- Improves tool selection accuracy by 8.38% to 38.6% over strong baselines
- Reduces context length by up to 99% (from 32,563 to 469 tokens on BFCL)
- Shows consistent gains across three state-of-the-art LLMs and three open-source benchmarks
- Particularly effective on large, noisy toolsets where reranking provides +1.3% gains

## Why This Works (Mechanism)

### Mechanism 1: Graph-Based Semantic Consolidation
Consolidating semantically redundant tools into a single canonical representation reduces retrieval ambiguity and improves selection accuracy. ToolScopeMerger constructs an undirected graph where nodes are tools and edges represent semantic equivalence (determined by an LLM classifier). By extracting connected components and pruning them to a single "canonical" tool via a synthesized description, the system reduces the search space density. This prevents the retriever from splitting attention across multiple near-duplicate vectors. The core assumption is that the LLM-based binary classifier accurately identifies functional equivalence, and the synthesized description preserves the full utility of the merged cluster.

### Mechanism 2: Cross-Encoder Reranking for Noise Filtering
A secondary cross-encoder reranker significantly improves precision by re-evaluating the top-k candidates identified by the initial bi-encoder retrieval. The Retriever first uses a faster bi-encoder (dense embeddings) to cast a wide net (e.g., top-50). A computationally heavier cross-encoder then processes these specific query-tool pairs to capture fine-grained interactions. This two-stage filtering ensures only the most contextually relevant tools occupy the limited context window. The core assumption is that the computational cost of the cross-encoder is acceptable during inference, and the top-k cutoff in the first stage does not exclude the ground-truth tool.

### Mechanism 3: Context Window Compression via Selective Injection
Drastically reducing the number of tool tokens in the prompt mitigates "lost-in-the-middle" phenomena and focus drift in LLMs. By strictly limiting the passed tools to the top-k (e.g., 5 to 30) after reranking, the system removes distractor tools. This compresses the prompt (up to 99% reduction cited), allowing the LLM to allocate attention more efficiently to the relevant tool definitions. The core assumption is that the LLM's reasoning capability is inversely correlated with the amount of irrelevant context (noise) provided.

## Foundational Learning

- **Concept: Bi-Encoders vs. Cross-Encoders**
  - **Why needed here:** ToolScopeRetriever relies on a bi-encoder (gte-large) for speed and a cross-encoder (ms-marco) for accuracy. Understanding the latency/accuracy trade-off is critical for tuning the `top_m` and `alpha` parameters.
  - **Quick check question:** Why can't you use a cross-encoder directly on a toolset of 4,000 tools for every query? (Answer: Latency/O(N) complexity).

- **Concept: Graph Connected Components**
  - **Why needed here:** ToolScopeMerger treats tool redundancy as a clustering problem solved via graph connected components. Understanding this helps in debugging why two seemingly unrelated tools might be merged (transitive closure via an intermediate tool).
  - **Quick check question:** If Tool A is similar to B, and B is similar to C, but A is not similar to C, will ToolScope merge them all? (Answer: Yes, if edges are formed, they become one connected component).

- **Concept: LLM-as-a-Judge / Validation**
  - **Why needed here:** The "Auto-Correction" module uses an LLM to validate merges. This is a "meta-cognitive" pattern where a model audits its own output or the output of a weaker process.
  - **Quick check question:** What is the primary risk when the validator LLM is the same model family as the classifier that created the merge proposal? (Answer: Consistent bias or failure modes may not be caught).

## Architecture Onboarding

- **Component map:** Offline Pipeline (Merger): Tool Indexer (FAISS + GTE-Large) -> Candidate Generator (Cosine Sim > 0.82) -> LLM Classifier -> Graph Builder -> Auto-Corrector (GPT-4o). Online Pipeline (Retriever): Query Input -> Hybrid Search (BM25 + Dense) -> Global Reranking (Cross-Encoder) -> Min-Max Normalization -> Top-k Truncation.

- **Critical path:** The Online Retriever latency is the bottleneck. The Merger is an offline job. You must optimize the `top_m` (candidates sent to reranker) to balance latency against the context limit of your serving infrastructure.

- **Design tradeoffs:**
  - **Alpha (α) Setting:** The paper found α=1 (Dense-only) optimal for their benchmarks. However, for code-heavy or keyword-specific tools, setting α < 1 to include BM25 weighting may be necessary in production.
  - **Threshold (0.82):** The merging threshold is "robust" but brittle at the extremes. Higher thresholds preserve distinct tools but keep noise; lower thresholds aggressively compress but risk merging distinct functions.

- **Failure signatures:**
  - **High Recall, Low Selection (CSR):** The Merger is working, but the Retriever is passing the correct tool in a list of distractors that confuse the agent. Fix: Increase reranker weight or decrease k.
  - **Zero Recall:** The Merger has likely over-merged, destroying the specific tool needed. Fix: Increase threshold or audit Auto-Correction logs.

- **First 3 experiments:**
  1. **Threshold Sweep:** Run ToolScopeMerger on your toolset with thresholds [0.77, 0.82, 0.90] and measure the % reduction in toolset size vs. a hold-out set of valid distinct tools to find the "safe" compression zone.
  2. **Alpha Tuning:** Compare retrieval Recall@10 using α=0 (BM25), α=0.5 (Hybrid), and α=1 (Dense) to verify the paper's claim that dense retrieval dominates for your specific domain data.
  3. **Ablation on k:** Measure the LLM agent's accuracy (CSR) with k=[5, 10, 20] to determine the optimal context window fill-rate before "lost-in-the-middle" degradation sets in.

## Open Questions the Paper Calls Out
None

## Limitations
- The offline ToolScopeMerger assumes static tool sets, limiting applicability to dynamic environments where tools are frequently added or modified.
- The LLM-based classifier and validator introduce black-box decision points that are difficult to audit for correctness, particularly when merging tools with subtle functional differences.
- The effectiveness of the graph-based merging approach depends heavily on the quality of the semantic equivalence classifier, with specific failure conditions not fully characterized.

## Confidence
- **High Confidence (9/10):** Claims about context reduction (up to 99%) and retrieval accuracy improvements (8.38% to 38.6% gains) are well-supported by ablation studies and comparison with strong baselines.
- **Medium Confidence (6/10):** The effectiveness of the graph-based merging approach depends heavily on the quality of the semantic equivalence classifier.
- **Medium Confidence (6/10):** The claim that ToolScope is "scalable for real-world use" is primarily supported by context reduction metrics, with computational overhead not fully characterized.

## Next Checks
1. **Dynamic Tool Set Evaluation:** Test ToolScopeMerger's performance on tool sets that change over time, measuring merge quality degradation rates and the overhead of periodic re-merging operations.

2. **Cross-Domain Generalization:** Evaluate the system on tool sets from different domains (e.g., medical, legal, technical support) to determine whether the 0.82 threshold and Auto-Correction process maintain effectiveness across semantically diverse vocabularies.

3. **Latent Capability Preservation:** Conduct user studies or automated functional testing to verify that merged tool representations preserve all distinct capabilities of their constituent tools, particularly for tools that are semantically similar but have critical functional differences in edge cases.