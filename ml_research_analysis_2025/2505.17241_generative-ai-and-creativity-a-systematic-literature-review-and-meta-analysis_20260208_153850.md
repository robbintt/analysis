---
ver: rpa2
title: 'Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis'
arxiv_id: '2505.17241'
source_url: https://arxiv.org/abs/2505.17241
tags:
- creative
- genai
- creativity
- effect
- humans
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study addresses the scattered empirical evidence on the effect
  of generative AI on creativity by conducting a meta-analysis of 28 studies (n =
  8214 participants). The authors compare creative performance and diversity across
  three dimensions: GenAI vs.'
---

# Generative AI and Creativity: A Systematic Literature Review and Meta-Analysis

## Quick Facts
- arXiv ID: 2505.17241
- Source URL: https://arxiv.org/abs/2505.17241
- Reference count: 40
- Primary result: Human-GenAI collaboration boosts creative performance (g = 0.27) but reduces idea diversity (g = -0.86)

## Executive Summary
This meta-analysis of 28 studies (n = 8214 participants) examines how generative AI affects creativity across three dimensions: standalone AI performance vs. humans, human-AI collaboration vs. humans alone, and idea diversity in collaborative settings. The findings reveal that GenAI achieves creative performance parity with average humans on standardized tasks, but human-AI collaboration provides a modest performance boost (g = 0.27) while significantly reducing collective idea diversity (g = -0.86). The benefits are asymmetric, favoring laypeople over domain experts, and vary substantially by task type, with AI excelling on divergent thinking tasks but underperforming on complex compositional work.

## Method Summary
The authors conducted a systematic literature review following PRISMA guidelines, identifying studies from Web of Science, SSRN, and arXiv. They extracted effect sizes (Hedges' g) for creative performance and diversity metrics across multiple dimensions. Analysis used random-effects meta-analysis with DerSimonian-Laird estimation, implemented in R using the metafor package. The dataset includes 127 effect sizes from 28 studies covering various creativity tasks, user expertise levels, and AI model types.

## Key Results
- No significant difference in creative performance between GenAI and humans (g = -0.05)
- Human-GenAI collaboration significantly outperforms humans alone (g = 0.27)
- GenAI collaboration significantly reduces idea diversity (g = -0.86)
- Laypeople benefit more from collaboration (g = 0.654) than domain experts (g = -0.057)

## Why This Works (Mechanism)

### Mechanism 1
- GenAI achieves creative performance parity with average humans on standardized creativity tasks, but this parity is task-contingent and model-contingent.
- GenAI models trained on large corpora can generate outputs that match human-rated creativity on divergent thinking tasks but underperform on complex, compositional tasks requiring tacit knowledge or multi-step reasoning.
- Core assumption: Standardized creativity tasks capture core creative dimensions; the effect generalizes to real-world creative work only conditionally.
- Evidence anchors: "no significant difference in creative performance between GenAI and humans (g = -0.05)"; "AUT favors GenAI (g = 0.855)... creative-writing tasks favor humans (g = -0.717)"
- Break condition: Tasks requiring deep domain expertise, tacit knowledge, or compositional reasoning.

### Mechanism 2
- Human-GenAI collaboration yields a small but robust boost in creative performance, with asymmetric benefits favoring laypeople over domain experts.
- GenAI provides generic facilitation—faster drafting, broader initial ideation, reduced cognitive load—rather than task-specific augmentation.
- Core assumption: The performance gain stems from reduced cognitive friction and expanded initial search space, not from genuinely novel recombination by the AI.
- Evidence anchors: "humans collaborating with GenAI significantly outperform those working without assistance (g = 0.27)"; "laypeople (g = 0.654)... academia (g = -0.057)"
- Break condition: When users already possess high domain expertise or when task requires integration of tacit organizational knowledge.

### Mechanism 3
- Human-GenAI collaboration systematically reduces idea diversity at the collective level, even while boosting individual performance.
- GenAI models converge toward high-probability outputs from their training distribution; when multiple users collaborate with the same or similar models, their outputs become more homogeneous.
- Core assumption: The diversity reduction is driven by model-level convergence rather than user behavior changes.
- Evidence anchors: "GenAI has a significant negative effect on the diversity of ideas for such collaborations between humans and GenAI (g = -0.86)"; "collaboration with GenAI reduces the diversity of ideas, indicating a risk of creative outputs that could become more homogeneous"
- Break condition: Not established—this is identified as a research gap.

## Foundational Learning

- **Effect size interpretation (Hedges' g)**: The paper reports all findings in standardized effect sizes; misinterpreting magnitude leads to over- or under-estimating practical significance. Quick check: Given g = 0.27 for collaboration benefit, would you describe this as "small," "medium," or "large"? What practical performance change does this imply?

- **Heterogeneity (I², τ²)**: All meta-analyses show high heterogeneity (I² > 90%), meaning effects vary substantially across contexts; pooled estimates are conditional averages, not universal laws. Quick check: If I² = 94%, can you generalize the pooled effect to a new task/model without validation?

- **Creativity dimensions (novelty vs. originality vs. diversity)**: The paper collapses multiple constructs; understanding their conceptual overlap prevents misapplying findings to the wrong outcome. Quick check: A system improves novelty scores but reduces diversity. Is this a success or a failure? What context determines the answer?

## Architecture Onboarding

- **Component map**: Task type classifier (AUT/CT vs. creative writing vs. business ideation) -> User profiler (layperson vs. domain expert) -> Model selector (GPT-4 for standalone, GPT-3.5 for collaboration) -> Diversity monitor (semantic distance metric) -> Feedback loop (periodic homogenization risk evaluation)

- **Critical path**: 1. Classify task complexity -> 2. Route to appropriate model/modality -> 3. Inject diversity-preserving constraints if cohort size > N -> 4. Log semantic distances for post-hoc analysis

- **Design tradeoffs**: Performance vs. diversity: Collaboration boosts individual creativity (g = 0.27) but reduces collective diversity (g = -0.86). Systems optimizing for both must explicitly trade off or use multi-objective optimization. Model choice: GPT-4 superior for standalone generation; GPT-3.5 better for collaboration augmentation. User expertise: System should adapt prompt complexity and intervention intensity based on user expertise.

- **Failure signatures**: High I² context: Pooled effects don't generalize; system fails silently on novel task types. Homogenization cascade: Cohort outputs converge semantically over time; detect via declining semantic distance variance. Expert disengagement: Domain experts show minimal gain (g ≈ 0); may abandon system if not adapted.

- **First 3 experiments**: 1. Task-type validation: Run the system on AUT vs. creative writing tasks; measure whether performance patterns match meta-analytic predictions. 2. Diversity decay test: Have 20 users complete the same ideation task with GenAI support; measure semantic distance across outputs over 5 rounds to quantify homogenization rate. 3. Expertise moderation check: Recruit 10 laypeople and 10 domain experts; measure collaboration benefit delta. Confirm that laypeople show larger gains (g ~0.65) while experts show minimal gains (g ~0).

## Open Questions the Paper Calls Out

- What psychological mechanisms (e.g., cognitive load, motivation, agency) mediate the performance gains observed in human-GenAI creative collaboration? The authors state it is unclear whether gains stem from "broader ideation... deeper engagement... or heightened motivation," calling for work on "psychological mediators."

- How do iterative design workflows (e.g., critique-refine) compare to simple one-shot prompting in supporting human creativity? The authors note the absence of studies comparing design choices and suggest future work explore "elaborate workflows" versus "naive GenAI setups."

- To what extent do GenAI creativity findings generalize to real-world professional tasks requiring tacit knowledge and strategic alignment? The paper highlights a reliance on simplified tasks like the Alternate Uses Test and calls for "more relevant, real-world-inspired study designs."

## Limitations
- High heterogeneity (I² > 90%) across studies limits generalizability of pooled effects to specific contexts
- Limited representation of visual creativity tasks despite growing GenAI capabilities in this domain
- Lack of longitudinal studies makes it difficult to assess long-term creative development with GenAI

## Confidence
- High confidence: Human-GenAI collaboration boosts individual creative performance (g = 0.27)
- Medium confidence: GenAI-human parity on standardized creativity tasks, with task-contingent variation
- Medium confidence: Creative homogenization in human-GenAI collaboration (g = -0.86)
- Low confidence: Mechanisms explaining why certain task types favor humans or AI

## Next Checks
1. **Task-specific validation**: Test the system on both AUT-style tasks and creative writing to verify the meta-analytic prediction of domain-dependent performance patterns
2. **Homogenization monitoring**: Implement semantic diversity tracking across user cohorts to detect and quantify creative convergence over time
3. **Expertise-moderation study**: Recruit both domain experts and laypeople to confirm the asymmetric benefit pattern (g ~0.65 for laypeople vs. g ~0 for experts)