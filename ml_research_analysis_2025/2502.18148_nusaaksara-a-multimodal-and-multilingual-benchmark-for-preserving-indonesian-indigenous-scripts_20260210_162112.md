---
ver: rpa2
title: 'NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian
  Indigenous Scripts'
arxiv_id: '2502.18148'
source_url: https://arxiv.org/abs/2502.18148
tags:
- script
- scripts
- language
- languages
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces NusaAksara, a benchmark dataset for Indonesian
  languages featuring their original scripts, which are often neglected in NLP research.
  The dataset includes both text and image modalities, covering tasks such as image
  segmentation, OCR, transliteration, translation, and language identification.
---

# NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts

## Quick Facts
- **arXiv ID**: 2502.18148
- **Source URL**: https://arxiv.org/abs/2502.18148
- **Reference count**: 40
- **Primary result**: NLP models achieve near-zero performance on Indonesian indigenous scripts due to lack of training data representation

## Executive Summary
NusaAksara is a benchmark dataset for Indonesian languages featuring their original scripts, which are often neglected in NLP research. The dataset includes both text and image modalities, covering tasks such as image segmentation, OCR, transliteration, translation, and language identification. It encompasses eight scripts across seven languages, including low-resource languages and one script unsupported by Unicode. Expert-annotated and validated data were collected from historical and educational sources. Benchmarking experiments using models like GPT-4o, Llama 3.2, and Aya 23 show that most NLP systems perform poorly on tasks involving local scripts, with many achieving near-zero performance, highlighting the urgent need for better support for indigenous scripts in NLP pipelines.

## Method Summary
The NusaAksara benchmark covers 9 tasks across 8 Indonesian indigenous scripts (Bali, Batak, Jawa, Jawi, Lampung, Lontara, Pegon, Sunda) from 75 scanned books (~7,137 pages). The dataset includes ~1,000 segmented images per script and is available at https://huggingface.co/datasets/NusaAksara/NusaAksara. The benchmark evaluates OCR/transliteration using CER and WER metrics, translation using BLEU and chrF++, LID using accuracy, and segmentation using IoU. The evaluation uses zero-shot VLMs and LLMs (GPT-4o, Gemini-Flash, Llama-3.2-11B, InternVL2.5-8B, LLaVA-v1.6-7B, Cendol-7b, Sailor-7B, Aya-23-8B, Llama-3.1-8B) and fine-tuned PP-OCRv3 detection with DBResNet-50 for segmentation.

## Key Results
- Most NLP technologies cannot handle Indonesia's local scripts, with many achieving near-zero performance
- Token coverage of local scripts in major models is extremely low (0.0-0.018% of tokens)
- GPT-4o shows partial success on Jawi but fails on other scripts
- Fine-tuned segmentation achieves IoU up to 0.91 but OCR CER remains >1 for all scripts
- Expert annotation is necessary as models cannot reliably transcribe low-resource scripts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lack of training data representation in local scripts causes near-zero model performance.
- Mechanism: Pretrained models tokenize text based on their vocabulary; when local scripts are absent or extremely rare (<0.02% of tokens), the model cannot form meaningful representations, leading to near-random outputs.
- Core assumption: Tokenization failure propagates to all downstream tasks (OCR, transliteration, translation).
- Evidence anchors:
  - [abstract]: "most NLP technologies cannot handle Indonesia's local scripts, with many achieving near-zero performance"
  - [section] Table 1 shows 0.0% native script tokens in NLLB, Bloomz, Llama, Aya; Sailor and Cendol show ~0.01–0.018%
  - [corpus] Related work on African and low-resource languages confirms similar representation gaps, but no direct mechanistic study for Indonesian scripts.
- Break condition: If models were evaluated on romanized versions of the same text, performance should improve dramatically (observed: romanized translation chrF++ jumps from ~10–20 to ~30–50).

### Mechanism 2
- Claim: Multi-stage pipelines (segmentation → OCR → transliteration → translation) compound errors.
- Mechanism: Each stage is a potential failure point; errors propagate multiplicatively. Fine-tuned segmentation (IoU ~0.6–0.9) does not guarantee OCR success (CER >1 for most models).
- Core assumption: Stage independence—errors don't cancel out.
- Evidence anchors:
  - [abstract]: "benchmark covers both text and image modalities and encompasses diverse tasks such as image segmentation, OCR, transliteration, translation"
  - [section] Table 4 shows PP-OCRv3 segmentation IoU up to 0.91 but OCR CER >1 for all scripts
  - [corpus] No direct corpus evidence on pipeline error propagation in this specific context.
- Break condition: If any stage were robust (e.g., perfect OCR), downstream tasks should improve (not tested in paper).

### Mechanism 3
- Claim: Expert annotation is necessary because models cannot reliably transcribe low-resource scripts.
- Mechanism: Native speakers and cultural experts provide ground truth that models cannot generate; inter-annotator agreement is high (CER <0.1, WER <0.3) for most scripts, validating data quality.
- Core assumption: Human experts have script knowledge that current models lack.
- Evidence anchors:
  - [abstract]: "data is constructed by human experts through rigorous steps"
  - [section] Table 11 shows low annotator-validator CER/WER and high BLEU/chRF++ for most scripts
  - [corpus] No corpus evidence comparing expert vs. crowd annotation for these scripts.
- Break condition: If models could pre-annotate with acceptable accuracy, human effort could focus on validation rather than full transcription.

## Foundational Learning

- **Multilingual NLP and script representation**
  - Why needed here: Understanding why training data imbalance leads to performance gaps.
  - Quick check question: What percentage of Llama-3.1 tokens are in Javanese script vs. Latin script?

- **OCR and document understanding pipelines**
  - Why needed here: The benchmark evaluates OCR as a critical first step for scanned manuscripts.
  - Quick check question: Why does high segmentation IoU not guarantee low OCR CER?

- **Low-resource language evaluation design**
  - Why needed here: Proper benchmarking requires appropriate metrics (CER, WER, BLEU, chrF++) and understanding of zero-shot vs. fine-tuned settings.
  - Quick check question: Which metric is most appropriate for transliteration vs. translation tasks?

## Architecture Onboarding

- **Component map**: Scanned image → segmentation → OCR → transliteration → translation
- **Critical path**: Scanned image → segmentation → OCR → transliteration → translation. Each stage must succeed for downstream tasks to work.
- **Design tradeoffs**:
  - Fine-tuning PP-OCRv3 detection vs. using zero-shot SAM-ViT (fine-tuning better for segmentation, but still fails at OCR)
  - Using opaque models (GPT-4o, Gemini) vs. open models (Llama, InternVL)—opaque models perform better but are not reproducible
  - Including Lampung script despite no Unicode support—requires custom fonts and separate annotation workflow
- **Failure signatures**:
  - OCR hallucination: models output wrong script (e.g., Devanagari instead of Balinese)
  - Translation hallucination: models generate plausible but unrelated Indonesian text
  - LID confusion: Arabic-script varieties (Jawi, Pegon) misclassified as Arabic
- **First 3 experiments**:
  1. **Baseline OCR evaluation**: Run PP-OCRv3 (zero-shot) on all 8 scripts; record CER per script to identify which scripts are hardest.
  2. **Fine-tune segmentation**: Use annotated bounding boxes to train DBResNet-50; measure IoU improvement vs. zero-shot SAM-ViT.
  3. **Romanized vs. script comparison**: Evaluate translation on romanized text vs. original script; quantify the performance gap to isolate the script representation problem.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized model architectures or training strategies (e.g., glyph-aware pretraining, multi-script adapters) achieve non-trivial OCR and transliteration performance for Indonesian indigenous scripts, and how close to human-level accuracy can they get?
- Basis in paper: [explicit] The authors state: "We also analyze current NLP data and models... revealing their underperformance for this task" and show near-zero performance for most models. They conclude: "Our findings reveal the urgent need of integrating indigenous scripts into NLP pipelines."
- Why unresolved: All benchmarked models (including fine-tuned PP-OCR) achieve CER >1 (i.e., >100% error) on OCR for almost all scripts. Only proprietary models like GPT-4o show partial success on Jawi, but results are inconsistent. The fundamental challenge of architectural design and training methodology for low-resource, non-Latin scripts remains unaddressed.
- What evidence would resolve it: A systematic study comparing different architectural approaches (e.g., character-level vs. subword-level, vision-language pretraining objectives, multi-script joint training) on the NusaAksara benchmark, showing statistically significant improvements over current baselines with qualitative analysis of error modes.

### Open Question 2
- Question: To what extent can transfer learning from related high-resource scripts (e.g., Arabic for Jawi/Pegon, Devanagari for Brahmic-derived scripts) improve performance, and what are the limits of cross-script transfer?
- Basis in paper: [explicit] The authors observe: "Again, Jawi is among the scripts where most models perform somewhat well... primarily because it is one of the highest-resource" and note Jawi's connection to Arabic script. They also mention: "These overlaps pose interesting questions for... NLP resources that accurately reflect each language."
- Why unresolved: While models show relatively better performance on Jawi (likely due to Arabic script exposure), performance on other scripts (especially Brahmic-derived ones) remains near-zero despite potential structural similarities. The paper does not explore systematic transfer learning from related script families.
- What evidence would resolve it: Experiments fine-tuning models pre-trained on related scripts (e.g., Arabic models for Jawi/Pegon, Indic script models for Javanese/Balinese) with ablation studies showing performance gains and analyzing negative transfer cases. Evidence should include cross-script similarity metrics and their correlation with transfer success.

### Open Question 3
- Question: How does dataset scale affect performance for these scripts, and what is the minimum amount of expert-annotated data required for usable OCR/transliteration?
- Basis in paper: [inferred] The authors limit annotation to 10% of books due to copyright and note "gathering more resources would be beneficial." They also mention OCR failure is "likely due to the extremely limited training data, which is insufficient for effective learning."
- Why unresolved: The current dataset contains only ~1,000 segmented images per script, which may be insufficient for training robust models. The relationship between data quantity, quality, and performance for these complex scripts is unexplored.
- What evidence would resolve it: A data scaling study varying training set sizes (e.g., 100, 500, 1000, 5000 samples) with learning curve analysis, combined with synthetic data augmentation experiments. Results should identify critical mass thresholds and whether quality improvements can compensate for quantity limitations.

### Open Question 4
- Question: What multimodal learning approaches can bridge the performance gap between direct image-to-text tasks (OCR, image transliteration) and text-to-text tasks (transliteration, translation)?
- Basis in paper: [inferred] Tables show models perform significantly worse on image-based transliteration (CER >1) compared to script-to-Latin transliteration (e.g., GPT-4o: CER 0.17-0.33). The authors note: "Transliterating directly from images presents an even greater challenge."
- Why unresolved: The performance disparity suggests vision-language models fail to properly align visual script representations with linguistic understanding. Current multimodal models appear to lack adequate script-specific visual-linguistic grounding.
- What evidence would resolve it: Comparative experiments between pipeline approaches (OCR → transliteration) and end-to-end multimodal models, combined with analysis of visual attention patterns during script processing. Success would require demonstrating that integrated vision-language training on script images improves performance over pipelined approaches.

## Limitations

- The custom font requirement for Lampung script (unsupported by Unicode) creates reproducibility barriers for the broader community.
- Expert annotation quality is validated through inter-annotator agreement, but comparisons to crowd-sourced alternatives are absent.
- The paper doesn't test whether adding script data to training corpora would improve results, leaving the causal mechanism indirect.

## Confidence

- **High confidence**: Claims about dataset construction methodology, task coverage (9 tasks across 8 scripts), and benchmark results showing poor performance of existing models on local scripts.
- **Medium confidence**: Claims about the mechanism linking token representation gaps to performance failures. While plausible and supported by token statistics, the causal chain isn't directly tested.
- **Low confidence**: Claims about the broader implications for NLP research on low-resource languages, which extrapolate from a single language family without systematic comparison to other low-resource scenarios.

## Next Checks

1. **Token distribution validation**: Verify the reported token percentages for each script in the evaluated models' vocabularies. Specifically, check Llama-3.1's token distribution to confirm that Javanese script tokens are indeed below 0.02%.

2. **Script-specific OCR failure analysis**: For each script, run zero-shot PP-OCRv3 and record CER. Identify whether failures are script-agnostic (all scripts fail similarly) or script-specific (certain scripts perform worse), which would indicate whether the problem is representation vs. script complexity.

3. **Romanized vs. native script performance gap**: Select 3 scripts and evaluate translation performance on both romanized and native script versions of the same text. Measure the performance delta to quantify how much of the failure is attributable to script representation vs. other factors like language modeling.