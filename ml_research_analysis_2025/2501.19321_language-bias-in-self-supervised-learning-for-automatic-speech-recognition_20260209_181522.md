---
ver: rpa2
title: Language Bias in Self-Supervised Learning For Automatic Speech Recognition
arxiv_id: '2501.19321'
source_url: https://arxiv.org/abs/2501.19321
tags:
- english
- data
- language
- languages
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates language bias in self-supervised learning
  (SSL) for multilingual automatic speech recognition (ASR). While SSL models like
  XLS-R are trained on 128 languages, the pretraining data is heavily imbalanced with
  English comprising 15.9% of the total data.
---

# Language Bias in Self-Supervised Learning For Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2501.19321
- Source URL: https://arxiv.org/abs/2501.19321
- Authors: Edward Storey; Naomi Harte; Peter Bell
- Reference count: 0
- Key outcome: English subnetworks in XLS-R SSL models consistently outperform other language subnetworks across multilingual ASR tasks, regardless of linguistic relationships.

## Executive Summary
This paper investigates language bias in self-supervised learning (SSL) for multilingual automatic speech recognition using the Lottery Ticket Hypothesis. The authors find that XLS-R, despite being trained on 128 languages, exhibits strong English bias due to imbalanced pretraining data where English comprises 15.9% of total data. Through language-specific subnetwork analysis via L1-norm pruning, they demonstrate that English subnetworks consistently outperform subnetworks for other languages across all downstream tasks, even when the target language is linguistically unrelated to English. The paper concludes that imbalanced pretraining data leads to inefficient multilingual SSL ASR, with models over-relying on features learned from high-resource languages.

## Method Summary
The methodology involves fine-tuning XLS-R 300M on upstream languages (English, German, French, Spanish, Polish) for 100 epochs, selecting checkpoints with lowest CTC loss. L1-norm unstructured one-shot global weight pruning is then applied to the encoder at 70-90% sparsity to identify language-specific subnetworks. These pruned models are fine-tuned on downstream languages (including Catalan, Asturian, Xhosa) for 10 epochs, with a freeze-then-unfreeze strategy when upstream and downstream languages differ. Performance is measured via Character Error Rate (CER), and subnetwork overlap is quantified using Intersection Over Union (IOU) metrics.

## Key Results
- English subnetworks achieve 26.92% lower average CER than worst-performing subnetworks when used across non-matching downstream languages
- Catalan and Asturian subnetworks show higher IOU overlap with English than with Spanish despite being Spanish variants
- At 90% sparsity, English subnetwork consistently outperforms other language subnetworks regardless of downstream language family relationships
- IOU overlap between English and other language subnetworks ranges from 80-85%, yet this small difference causes substantial performance variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Imbalanced pretraining data causes SSL models to develop language-specific weight dominance that persists through fine-tuning
- Mechanism: When English comprises 15.9% of pretraining data while other languages each represent ≤5.8%, gradient updates during SSL disproportionately optimize weights for English phonetic patterns. These weights become "anchored" and resist modification during downstream fine-tuning.
- Core assumption: Assumption: L1-norm magnitude reflects functional importance of weights for a given language task.
- Evidence anchors:
  - [abstract] "XLS-R builds only on weights learned from the languages with the largest data contribution to the pretraining data"
  - [Section 4.3] English subnetwork has highest IOU with base XLS-R weights (before fine-tuning), implying "the reliance on English has been taught to the model at the pretraining stage"
  - [corpus] Related work on Arabic-centric SSL models (HARNESS) suggests language-specific pretraining improves low-resource performance, indirectly supporting the data-composition hypothesis
- Break condition: If pretraining data were balanced across languages, language-specific subnetworks should show comparable performance to their matched downstream languages

### Mechanism 2
- Claim: Language-specific subnetworks extracted via Lottery Ticket Hypothesis reveal functional weight groupings, but their transfer efficiency is constrained by pretraining distribution
- Mechanism: L1-norm global pruning identifies weights with highest magnitude for a given language task. At 70-90% sparsity, remaining weights form a "winning ticket" that captures language-specific representations. However, since English dominates pretraining, the English subnetwork contains more universally useful features.
- Core assumption: Assumption: Weight magnitude after fine-tuning indicates causal contribution to that language's recognition.
- Evidence anchors:
  - [Section 3.3] "We apply L1-norm unstructured one-shot global weight pruning to the encoder of XLS-R"
  - [Section 4.1] At 90% sparsity, English subnetwork achieves 26.92% lower CER than worst-performing subnetwork when averaged across non-matching downstream languages
  - [corpus] Limited direct corpus evidence on LTH for multilingual ASR; primarily supported by this paper's novel contribution
- Break condition: If pruning were applied before fine-tuning (to base model), subnetworks should show no language-specific performance patterns

### Mechanism 3
- Claim: SSL models bypass linguistic relationships during transfer, preferring statistically dominant pretraining features
- Mechanism: Despite Catalan and Asturian being linguistically closer to Spanish (both Latin-family languages of Spain), their subnetworks show higher weight overlap with English than Spanish. The model ignores phylogenetic language relationships in favor of features learned from high-resource pretraining data.
- Core assumption: Assumption: IOU overlap indicates which pretraining features the model recruits for new languages.
- Evidence anchors:
  - [Section 4.3, Figure 8] "Asturian has an IOU of 81.01% with Spanish and 84.66% with English. This shows that XLS-R builds on weights learned for English when learning new or low-resource languages regardless of their language families"
  - [Section 5] "XLS-R weights learned in pretraining from English language data are more impactful to training. This is regardless of the linguistic relation to English"
  - [corpus] Paper on tone recognition in North-East Indian languages (Angami, Ao, Mizo) shows similar transfer challenges with SSL models pretrained on non-tonal languages
- Break condition: If linguistic proximity drove transfer, Catalan/Asturian should show higher IOU with Spanish than English

## Foundational Learning

- **Self-Supervised Learning (wav2vec 2.0 architecture)**
  - Why needed here: XLS-R uses this architecture; understanding how SSL pretraining creates representations is essential for diagnosing bias
  - Quick check question: Can you explain how masked prediction during pretraining creates representations that may favor certain phonetic patterns?

- **Lottery Ticket Hypothesis and L1-norm Pruning**
  - Why needed here: The paper's methodology depends on extracting subnetworks via pruning; understanding what pruning reveals about weight importance is critical
  - Quick check question: At 90% sparsity, what does it mean if a weight survives pruning for English but not Spanish?

- **Intersection Over Union (IOU) for Weight Analysis**
  - Why needed here: IOU quantifies subnetwork overlap; this is the primary tool for demonstrating English-weight dominance
  - Quick check question: If two language subnetworks have 85% IOU overlap, what does the remaining 15% represent functionally?

## Architecture Onboarding

- **Component map:**
  ```
  XLS-R (300M parameters)
  ├── Encoder: 24 transformer layers (target of pruning)
  ├── Feature extractor (CNN frontend)
  └── Quantization module (for SSL contrastive loss)

  Fine-tuning adds:
  └── Language-specific output head (CTC-based)
  ```

- **Critical path:**
  1. Load pretrained XLS-R (do NOT modify encoder weights initially)
  2. Fine-tune on upstream language (100 epochs, select by lowest CTC loss)
  3. Apply L1-norm global pruning to encoder at target sparsity
  4. Fine-tune pruned model on downstream language (freeze encoder 1 epoch if upstream≠downstream, then unfreeze 10 epochs)
  5. Evaluate via Character Error Rate (CER)

- **Design tradeoffs:**
  - Higher sparsity (80-90%) reveals language-specific patterns but increases CER
  - Freezing encoder initially for cross-language transfer stabilizes but may limit adaptation
  - Global vs. layer-wise pruning: Global preserves cross-layer connections but may unevenly prune layers

- **Failure signatures:**
  - CER >90% at high sparsity with non-English subnetworks (Table 3: Spanish subnetwork on English at 90% sparsity = 97.97% CER)
  - Large CER gap between matching and non-matching language pairs (>20% difference suggests pretraining bias)
  - High variance across languages when using same subnetwork (indicates poor generalization)

- **First 3 experiments:**
  1. **Baseline sparsity sweep**: Fine-tune XLS-R on your target language, prune from 0-90% sparsity in 10% increments, plot CER curve. This establishes whether your target language shows the same English-dominance pattern or has usable language-specific subnetwork.
  2. **Cross-language subnetwork swap**: Train upstream models on 2+ languages from the paper (e.g., English, Spanish), then swap subnetworks at 80% sparsity. If English subnetwork outperforms Spanish subnetwork on your target, pretraining bias is active.
  3. **IOU analysis with base model**: Compute IOU between your target language's subnetwork and the base XLS-R (unfine-tuned) at 90% sparsity. High overlap (>85%) suggests your language relies on English-dominant features; low overlap suggests genuine language-specific learning.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would balancing pretraining data across languages eliminate the English subnetwork dominance, or does the bias persist due to other factors in the SSL training objective?
- Basis in paper: [explicit] The authors conclude by recommending that "pretraining data by language and linguistic relationships" be balanced, but do not experimentally validate whether this intervention would resolve the observed bias.
- Why unresolved: The study diagnoses the problem using an already-trained XLS-R model but cannot isolate whether data imbalance is the sole cause or whether SSL objectives like masked prediction inherently favor high-resource languages.
- What evidence would resolve it: Pretraining wav2vec 2.0 or similar architectures on deliberately balanced multilingual datasets and repeating the LTH subnetwork analysis to compare language-specific subnetwork performance.

### Open Question 2
- Question: Do these findings generalize to larger SSL ASR models (e.g., XLS-R 1B, w2v-BERT, or MMS), or is the English subnetwork dominance specific to the 300M parameter model?
- Basis in paper: [inferred] The authors evaluated only "the smallest iteration of XLS-R with 300M parameters" without testing whether model scale affects the degree of language bias in subnetworks.
- Why unresolved: Larger models have greater capacity and may develop more distinct language-specific representations, potentially reducing over-reliance on dominant-language features.
- What evidence would resolve it: Repeating the pruning and cross-lingual subnetwork experiments on XLS-R 1B or other large multilingual SSL models and comparing the performance gaps between language-specific subnetworks.

### Open Question 3
- Question: What acoustic or linguistic properties do the non-overlapping weights in language-specific subnetworks encode, and why do English-specific weights transfer more effectively to unrelated languages?
- Basis in paper: [inferred] The IOU analysis shows 80–85% overlap between subnetworks, yet the remaining weights cause substantial performance differences; the paper does not investigate what these weights represent functionally.
- Why unresolved: Understanding whether these weights capture phonetic, prosodic, or language-agnostic acoustic features would clarify why English subnetworks generalize better despite linguistic unrelatedness.
- What evidence would resolve it: Probing experiments on the non-overlapping weights using phonetic classification tasks or representational similarity analysis to identify what properties are encoded differentially across language-specific subnetworks.

## Limitations
- The Lottery Ticket Hypothesis framework used for subnetwork analysis has primarily been validated for vision tasks and simple architectures, not complex multilingual ASR with 300M+ parameters
- The study uses only XLS-R 300M architecture and FLEURS dataset, limiting generalizability to other SSL models or datasets
- The causal mechanism linking imbalanced pretraining data to performance bias is plausible but not definitively proven through ablation studies

## Confidence
- **High confidence**: The empirical observation that English subnetworks outperform other language subnetworks across all downstream tasks is directly measurable and consistently demonstrated
- **Medium confidence**: The causal mechanism linking imbalanced pretraining data (15.9% English) to observed performance bias is plausible but not definitively proven
- **Low confidence**: The generalizability of these findings beyond the XLS-R architecture and FLEURS dataset

## Next Checks
1. **Pre-pruning control experiment**: Apply the same L1-norm pruning to the base XLS-R model (before any fine-tuning) and evaluate whether language-specific performance patterns still emerge
2. **Cross-architecture replication**: Test the same methodology on a different SSL architecture (e.g., Wav2Vec2.0) pretrained on the same FLEURS data but with balanced language distribution
3. **Linguistic feature analysis**: Conduct a systematic comparison of phonetic inventory overlap between English and target languages versus performance correlation