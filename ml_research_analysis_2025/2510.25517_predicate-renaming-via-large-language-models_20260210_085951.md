---
ver: rpa2
title: Predicate Renaming via Large Language Models
arxiv_id: '2510.25517'
source_url: https://arxiv.org/abs/2510.25517
tags:
- names
- predicate
- predicates
- llms
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores using Large Language Models (LLMs) to automatically
  rename unnamed predicates in logic rules, a common issue in Inductive Logic Programming
  (ILP) that hampers interpretability. The authors propose a pipeline where multiple
  LLMs generate name suggestions for each predicate, which are then judged by other
  LLMs to select the most suitable one.
---

# Predicate Renaming via Large Language Models

## Quick Facts
- arXiv ID: 2510.25517
- Source URL: https://arxiv.org/abs/2510.25517
- Reference count: 40
- Key outcome: LLMs can automatically generate interpretable names for unnamed predicates in logic rules, with GPT models showing the best performance on math predicates

## Executive Summary
This paper addresses the interpretability challenge in Inductive Logic Programming (ILP) where logic rules often contain unnamed predicates, making them difficult for humans to understand. The authors propose using Large Language Models (LLMs) to automatically rename these predicates through a multi-stage pipeline. Multiple LLMs generate name suggestions for each predicate, which are then evaluated by other LLMs to select the most appropriate name. Experiments demonstrate that GPT models (ChatGPT-4o, ChatGPT-o3mini) and Gemini perform well on math predicates, with GPT models correctly renaming 15 out of 16 predicates. Human judges confirmed the effectiveness of this approach, validating that the LLM-selected names were appropriate in most cases.

## Method Summary
The proposed method uses a pipeline approach where multiple LLMs generate name suggestions for each unnamed predicate in logic rules. These suggestions are then passed to other LLMs for evaluation and selection of the most suitable name. The process involves: (1) generating multiple name candidates from different LLM models, (2) having other LLMs judge and rank these suggestions, and (3) selecting the highest-ranked name as the final predicate name. The approach was tested on hand-crafted logic rules, particularly focusing on math predicates, and evaluated both by LLM judgments and human verification.

## Key Results
- GPT models (ChatGPT-4o, ChatGPT-o3mini) and Gemini showed the best performance in predicate renaming
- GPT models correctly renamed 15 out of 16 math predicates
- Human judges confirmed LLM performance, with correct names selected in most cases
- LLMs struggled significantly with real-world datasets, suggesting domain-specific fine-tuning may be needed

## Why This Works (Mechanism)
The mechanism leverages LLMs' strong natural language understanding and generation capabilities to interpret the semantic meaning of logic predicates and generate appropriate names. By using multiple LLMs for both generation and evaluation, the approach creates a form of ensemble intelligence where different models can compensate for each other's weaknesses. The iterative evaluation process helps filter out inappropriate suggestions and select names that are both semantically accurate and human-readable. This approach is particularly effective for well-defined domains like mathematics where predicates have clear, unambiguous meanings that LLMs can recognize from their training data.

## Foundational Learning
**Inductive Logic Programming (ILP)**: A machine learning approach that learns logical rules from data. Why needed: Provides the context for why predicate naming matters for interpretability. Quick check: Can you explain how ILMs learn rules from examples?

**Logic Programming Syntax**: Understanding predicate-argument structure in logical rules. Why needed: Essential for grasping what needs to be renamed and how. Quick check: Can you identify predicates and their arguments in a simple rule?

**Large Language Model Capabilities**: LLMs' ability to understand semantics and generate natural language. Why needed: Core to understanding how LLMs can infer meaning from logical predicates. Quick check: Can you list three capabilities LLMs demonstrate in this paper?

**Ensemble Methods**: Using multiple models to improve overall performance. Why needed: Explains why multiple LLMs are used for both generation and evaluation. Quick check: Can you describe how using multiple models improves reliability?

## Architecture Onboarding

**Component Map**: Logic Rules -> Multiple LLM Generators -> Name Suggestions -> Multiple LLM Judges -> Selected Names -> Human Verification

**Critical Path**: The pipeline from predicate identification through LLM generation, evaluation, and selection represents the critical path. Each stage depends on the previous one, with no parallel processing or alternative routes that could bypass stages.

**Design Tradeoffs**: The approach trades computational cost (multiple LLM calls) for improved accuracy and interpretability. Using multiple LLMs for both generation and evaluation increases reliability but also increases API costs and latency. The choice to use LLMs rather than rule-based systems sacrifices transparency for flexibility and potentially better semantic understanding.

**Failure Signatures**: Poor performance on real-world datasets indicates domain adaptation challenges. Inconsistent LLM judgments suggest variability in model understanding. Complete failure to generate meaningful names would indicate the predicate semantics are too complex or abstract for current LLMs to capture.

**Three First Experiments**:
1. Test the pipeline on a small set of 5-10 simple math predicates to verify basic functionality
2. Compare single LLM generation vs. multiple LLM generation to quantify the benefit of ensemble approach
3. Test different prompt formulations to optimize name generation quality

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the significant drop in performance on real-world datasets suggests several implicit questions about domain adaptation and the generalizability of the approach to complex, real-world logic programs.

## Limitations
- Evaluation limited to only 16 hand-crafted math predicates, lacking statistical significance
- Poor performance on real-world datasets with only two test cases (UMLS and Kinship)
- No quantitative metrics for LLM judgments, relying on qualitative assessment
- Absence of comparison with rule-based or dictionary-based renaming baselines
- No evaluation of whether renamed predicates actually improve downstream task performance

## Confidence

**High confidence**: The general approach of using LLMs for predicate renaming is novel and technically sound. The observation that LLMs can generate plausible names for well-defined math predicates is supported by evidence.

**Medium confidence**: The relative performance ranking of different LLM models (GPT-4o and o3mini performing best) appears consistent within the limited test set, though the small sample size (16 predicates) limits statistical significance.

**Low confidence**: The claims about LLM effectiveness on real-world datasets and the superiority of LLM-based renaming over alternative approaches are not well-supported due to insufficient empirical evidence and lack of proper baselines.

## Next Checks

1. Conduct systematic evaluation on a larger, diverse set of logic rules (minimum 100 predicates across multiple domains) to establish statistical significance of performance claims.

2. Implement and compare against rule-based renaming baselines (e.g., pattern matching, dictionary-based approaches) to determine whether the LLM approach provides meaningful advantages.

3. Perform user studies to assess whether LLM-generated predicate names actually improve human comprehension and downstream task performance compared to original or alternative naming schemes.