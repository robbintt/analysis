---
ver: rpa2
title: 'MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image
  Encoding and Multiple Trajectory Prediction'
arxiv_id: '2511.22181'
source_url: https://arxiv.org/abs/2511.22181
tags:
- trajectory
- driving
- embeddings
- scene
- mtr-vp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MTR-VP, a vision-first adaptation of the
  Motion Transformer framework for trajectory planning in autonomous driving. The
  model uses a Vision Transformer to encode egocentric camera images and past vehicle
  states, replacing the HD map features used in prior methods.
---

# MTR-VP: Towards End-to-End Trajectory Planning through Context-Driven Image Encoding and Multiple Trajectory Prediction

## Quick Facts
- arXiv ID: 2511.22181
- Source URL: https://arxiv.org/abs/2511.22181
- Reference count: 28
- Primary result: Vision-based trajectory planner using transformer architecture; outperforms baseline on ADE while matching RFS

## Executive Summary
This paper introduces MTR-VP, a vision-first adaptation of the Motion Transformer framework for trajectory planning in autonomous driving. The model uses a Vision Transformer to encode egocentric camera images and past vehicle states, replacing the HD map features used in prior methods. It then fuses these visual and motion embeddings via cross-attention with a route intent embedding, predicting multiple future trajectories with associated probabilities. The approach is evaluated on the Waymo End-to-End Driving Dataset, which emphasizes rare and complex driving scenarios. Results show that while visual embeddings are not fully leveraged in the current architecture—evidenced by similar performance with blank images—the model still outperforms the Waymo baseline on ADE and matches it on Rater Feedback Score. Multi-trajectory prediction improves performance over single-trajectory prediction. However, attempts to enhance query embeddings with CLIP and DINOv2 representations degraded results, suggesting challenges in effectively integrating large-scale vision-language features into the planning task. The work demonstrates the potential of transformer-based motion prediction and highlights opportunities for better visual-motion fusion in autonomous driving planning.

## Method Summary
MTR-VP is a transformer-based end-to-end trajectory planner that predicts future vehicle trajectories using camera images and past kinematic states. The model encodes panoramic camera views via a ViT backbone and past vehicle states via a temporal transformer. These embeddings are fused using cross-attention with a discrete routing intent (left/straight/right), and a decoder outputs K=20 candidate trajectories with associated probabilities. Training uses a hybrid loss combining cross-entropy for mode selection and L2 regression for trajectory refinement. The architecture is evaluated on the Waymo End-to-End Driving Dataset, with performance measured by Average Displacement Error (ADE) and Rater Feedback Score (RFS).

## Key Results
- MTR-VP achieves 1.4232 ADE at 3 seconds, outperforming the Waymo baseline (3.0182 ADE)
- Multi-trajectory prediction improves performance, with top-10 ADE of 0.3204 at 3 seconds
- Visual embeddings are not effectively utilized, as blank-image ablations yield nearly identical performance (1.4238 ADE)
- CLIP and DINOv2 foundation model embeddings degrade performance (2.0142 ADE) when added to query embeddings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-trajectory prediction with probability distribution reduces prediction variance and improves planning performance compared to single-trajectory output.
- Mechanism: The decoder generates K=20 candidate trajectories alongside a learned probability distribution. During training, cross-entropy loss increases probability for the trajectory closest to ground truth, while L2 regression refines only that closest trajectory. This allows the model to mode-separate distinct plausible futures rather than averaging them.
- Core assumption: Valid driving plans cluster into discrete modes; averaging across modes produces worse trajectories than selecting from individually coherent candidates.
- Evidence anchors:
  - [abstract] "predicting a distribution over multiple futures instead of a single future trajectory boosts planning performance"
  - [Table II] ADE improves from 0.6762 (top-1) to 0.3204 (top-10) at 3 seconds on validation split
  - [corpus] ResAD paper addresses spatio-temporal imbalance in trajectory data, suggesting multi-modal outputs help with optimization burden
- Break condition: If candidate trajectories collapse to near-duplicates or probability distribution becomes uniform, the mechanism degrades to single-trajectory behavior with computational overhead.

### Mechanism 2
- Claim: Temporal transformer encoding of past kinematic states captures sufficient motion priors to predict future trajectories even without effective visual input.
- Mechanism: A 4-layer, 8-head transformer encoder processes 16 timesteps of 6D state vectors (position, velocity, acceleration). Self-attention learns temporal dependencies and motion patterns. The paper's ablation shows nearly identical ADE with blank images (1.4238 vs 1.4232), indicating the model relies primarily on this kinematic encoding.
- Core assumption: Past motion patterns are strongly predictive of near-future motion; scene context provides marginal improvement for typical prediction horizons.
- Evidence anchors:
  - [Section IV.A] "Self attention in this block of the model helps encode the relationships between the past states"
  - [Table I] MTR-VP with blank images achieves ADE 1.4238 vs 1.4232 with real images (3-second), demonstrating visual features are not being leveraged
  - [corpus] SpaRC-AD and related work emphasize sensor fusion, suggesting this paper's finding that vision isn't utilized represents an integration failure rather than vision being unnecessary
- Break condition: In scenarios requiring scene-aware decisions (lane curvatures, construction zones, obstacles), kinematic-only prediction will produce unsafe trajectories.

### Mechanism 3
- Claim: Cross-attention between intent embeddings and scene context enables goal-conditioned trajectory generation.
- Mechanism: Intent (left/straight/right) is encoded to 128D, then serves as the query in cross-attention against scene context embeddings (keys/values). This conditions trajectory generation on high-level routing goals.
- Core assumption: The intent signal is reliable and the scene context embeddings contain information relevant to achieving that intent.
- Evidence anchors:
  - [Section IV.B] "The intent embedding is used as the query while the scene context embeddings are used as the keys and values"
  - [Section VI] Model outperforms Waymo baseline on ADE (1.4232 vs 3.0182), suggesting intent conditioning provides meaningful guidance
  - [corpus] Map-World paper notes multi-modal trajectory prediction typically relies on handcrafted anchors; MTR-VP's intent-driven queries offer a learned alternative
- Break condition: If scene context lacks relevant information (as the blank-image ablation suggests), intent conditioning operates on impoverished features, limiting its effectiveness to simple extrapolation.

## Foundational Learning

- Concept: Vision Transformer (ViT) patch embedding and CLS token aggregation
  - Why needed here: The model uses ViT to encode panoramic camera views; understanding how patches become scene embeddings is critical for debugging vision fusion failures.
  - Quick check question: Can you explain why replacing the image with blank inputs yields nearly identical performance, and what this implies about the CLS token's information content?

- Concept: Cross-attention query-key-value semantics for conditional generation
  - Why needed here: The decoder uses intent as query and scene context as key/value; misunderstanding this direction will cause incorrect architecture modifications.
  - Quick check question: If you wanted the model to attend more to visual features when generating trajectories, would you modify the query, key, or value—and why?

- Concept: Multi-modal trajectory prediction with classification + regression losses
  - Why needed here: Training uses cross-entropy over trajectory modes plus L2 regression on the closest mode; this hybrid objective requires careful balancing.
  - Quick check question: Why does the paper apply L2 loss only to the closest trajectory rather than all K trajectories?

## Architecture Onboarding

- Component map:
  - Inputs (camera images, kinematic history, intent) -> ViT Encoder -> Temporal Transformer -> Fusion (concatenation/cross-attention) -> Intent Embedding -> Cross-Attention Decoder -> K trajectories + probabilities

- Critical path: Kinematic encoder -> fusion -> decoder. The blank-image ablation reveals vision is off the critical path in current implementation.

- Design tradeoffs:
  - Concatenation vs cross-attention fusion: Concatenation is simpler but doesn't force interaction; cross-attention attempts explicit integration but still shows blank-image equivalence
  - K=20 trajectories: Higher K improves top-K metrics but increases inference cost and may dilute training signal
  - Foundation model embeddings (CLIP/DINOv2): Intended to provide semantic scene understanding, but degraded performance—possibly due to dimension mismatch with training data scale

- Failure signatures:
  - Blank images yielding identical ADE -> vision not utilized
  - CLIP/DINOv2 degrading performance -> feature space misalignment or overfitting in higher-dimensional space
  - Top-1 ADE significantly worse than top-10 -> good trajectories generated but selection mechanism weak

- First 3 experiments:
  1. Vision gradient check: Backprop through ViT and verify non-zero gradients reach image patches. If near-zero, the fusion module is blocking learning.
  2. Kinematic-only baseline: Train with intentional blank images from start. If performance matches full model, confirm vision contributes nothing.
  3. Attention visualization: Extract cross-attention weights from decoder. Determine if intent query attends to visual tokens or only kinematic tokens.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How can transformer-based architectures be modified to effectively fuse visual scene embeddings with kinetic history so that the model relies on visual context rather than solely on past trajectory data?
- **Basis in paper:** [explicit] The authors note that "visual embeddings are not being combined properly" because ablations with blank images yielded similar performance to full inputs (Section IV.C, Section VI).
- **Why unresolved:** Current concatenation and cross-attention mechanisms failed to leverage visual data, leaving the model "lacking 'vision'" and essentially performing trajectory prediction based only on kinematics.
- **What evidence would resolve it:** A model architecture where performance metrics (ADE/RFS) significantly degrade when visual inputs are removed or corrupted, demonstrating a functional reliance on scene features.

### Open Question 2
- **Question:** What specific fusion strategies allow large-scale vision-language foundation models (e.g., CLIP, DINOv2) to enhance trajectory planning without degrading performance?
- **Basis in paper:** [explicit] The authors attempted to augment query embeddings with CLIP and DINOv2 features, but this "degraded results," suggesting current integration methods are insufficient (Section IV.D, Section VI).
- **Why unresolved:** The paper suggests the failure may be due to a higher-dimensional input space relative to training data volume or the specific inability of these models to provide spatial grounding for driving decisions.
- **What evidence would resolve it:** A training regime where foundation model embeddings explicitly improve generalization to rare scenarios (long-tail) or complex semantic reasoning without increasing displacement error.

### Open Question 3
- **Question:** Can integrating Large Language Model (LLM)-based intent priors improve the generation of trajectories for complex, composite maneuvers?
- **Basis in paper:** [explicit] The conclusion identifies the limitation to "short-horizon single-instruction maneuvers" and proposes that "integrating LLM-based intent priors may lead to richer routing understanding" (Section VI).
- **Why unresolved:** The current discrete intent encoding (left/right/straight) is too simple for high-level planning, but the integration of semantic reasoning into continuous trajectory generation remains unexplored in this work.
- **What evidence would resolve it:** Successful trajectory generation for multi-step instructions (e.g., "merge left then exit immediately") in a unified planning model.

## Limitations

- Visual embeddings are not effectively utilized, as blank-image ablations yield nearly identical performance, suggesting the model operates primarily as a motion predictor
- CLIP and DINOv2 foundation model embeddings degrade performance when added to query embeddings, indicating challenges in integrating large-scale vision-language features
- The model is limited to short-horizon single-instruction maneuvers and may struggle with complex, composite routing scenarios

## Confidence

- **High confidence**: Multi-trajectory prediction improves performance over single-trajectory output; the hybrid CE+L2 loss objective functions as described
- **Medium confidence**: The motion transformer framework can predict trajectories using kinematic history alone; intent conditioning provides meaningful guidance
- **Low confidence**: Vision features can be effectively integrated into motion prediction; foundation model embeddings enhance performance

## Next Checks

1. **Vision Gradient Validation**: Instrument the training loop to monitor gradient magnitudes flowing from fusion layers back to ViT patches. If gradients to image patches remain near-zero while gradients to kinematic encoder are substantial, this confirms the visual stream is effectively frozen during training.

2. **Attention Pattern Analysis**: Extract and visualize cross-attention weights from the decoder for both vision-based and blank-image conditions. Compare whether intent queries attend to visual tokens versus purely kinematic tokens, and quantify the distribution of attention mass.

3. **Feature Space Compatibility Test**: Evaluate CLIP/DINOv2 embeddings on a held-out validation set of scenes without training, measuring cosine similarity between these embeddings and the model's learned visual representations. This will reveal whether the performance degradation stems from feature space misalignment or overfitting in high-dimensional space.