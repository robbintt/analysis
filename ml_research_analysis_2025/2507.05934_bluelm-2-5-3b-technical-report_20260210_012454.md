---
ver: rpa2
title: BlueLM-2.5-3B Technical Report
arxiv_id: '2507.05934'
source_url: https://arxiv.org/abs/2507.05934
tags:
- data
- training
- arxiv
- reasoning
- multimodal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BlueLM-2.5-3B is a compact multimodal large language model with
  2.9 billion parameters that supports both thinking and non-thinking modes while
  allowing explicit control over reasoning token budget. It achieves this through
  diversified data curation, key data resampling, hybrid heterogeneous reinforcement
  learning, and high-performance training infrastructure.
---

# BlueLM-2.5-3B Technical Report

## Quick Facts
- arXiv ID: 2507.05934
- Source URL: https://arxiv.org/abs/2507.05934
- Reference count: 40
- Primary result: 2.9B parameter multimodal model matching Qwen3-4B on text tasks and trailing Kimi-VL-A3B-16B by only 5% on multimodal tasks

## Executive Summary
BlueLM-2.5-3B is a compact multimodal large language model with 2.9 billion parameters that supports both thinking and non-thinking modes while allowing explicit control over reasoning token budget. It achieves this through diversified data curation, key data resampling, hybrid heterogeneous reinforcement learning, and high-performance training infrastructure. The model uses 23% less total training data than comparable models while achieving superior reasoning capabilities. In thinking mode, it matches Qwen3-4B on text benchmarks and trails Kimi-VL-A3B-16B by only 5% on multimodal tasks. In non-thinking mode, it outperforms Qwen2.5-VL-3B on most multimodal benchmarks.

## Method Summary
BlueLM-2.5-3B uses a three-component architecture: SigLIP2 vision encoder, 2-layer MLP adapter, and a 2.5B dense LLM backbone distilled from a 7B teacher. Training proceeds through five stages: pure-text pre-training (9.3T tokens), joint pre-training (4T tokens with 40% text ratio), reasoning-enhanced training (2.5T tokens with synthetic CoT), long-context activation (32K via YaRN), and joint SFT with thinking control token. Post-training uses hybrid heterogeneous RL with GRPO, combining rule-based rewards for verifiable tasks and learned reward models for open-ended tasks. The model achieves its efficiency through careful data curation and a unified thinking/non-thinking architecture.

## Key Results
- Matches Qwen3-4B performance on text benchmarks in thinking mode
- Trails Kimi-VL-A3B-16B by only 5% on multimodal tasks while using 2.9B parameters
- Outperforms Qwen2.5-VL-3B on most multimodal benchmarks in non-thinking mode
- Uses 23% less total training data than comparable models
- Demonstrates exceptional data efficiency with strong general-purpose and reasoning performance

## Why This Works (Mechanism)

### Mechanism 1: Thinking Mode Control via Special Token Positioning
Placing the thinking control token `[|BlueThink|]` after the query (rather than before) may reduce interference from strong instruction cues and stabilize mode activation. When placed at query end, it avoids competition with task-specific instruction prefixes that could suppress reasoning behavior.

### Mechanism 2: Data Efficiency via High Text-to-Multimodal Ratio
Increasing pure text proportion to 40% during joint pre-training may help preserve language capabilities while learning multimodal alignment. Higher text ratio acts as regularizer against catastrophic forgetting of linguistic knowledge when visual modules are trained.

### Mechanism 3: Hybrid RL with Task-Specific Rewards
Combining rule-based rewards (for verifiable tasks) with learned reward models (for open-ended tasks) may improve generalization across heterogeneous task types. Different reward sources provide complementary signals—rules give sparse but accurate feedback; models give dense but potentially noisy feedback.

## Foundational Learning

- **Catastrophic forgetting in multimodal training**
  - Why needed here: The paper's 40% text ratio directly addresses this; without understanding it, the data mixture rationale is opaque.
  - Quick check question: Can you explain why adding visual data might degrade a model's grammar?

- **Group Relative Policy Optimization (GRPO)**
  - Why needed here: The RL phase uses GRPO instead of PPO; understanding the baseline estimation difference is critical.
  - Quick check question: How does GRPO estimate advantage without a value model?

- **Context parallelism for long sequences**
  - Why needed here: 32K context training requires splitting sequences across GPUs; this differs from standard data parallelism.
  - Quick check question: What gets communicated between GPUs in context parallelism vs. data parallelism?

## Architecture Onboarding

- **Component map:**
  Vision Encoder: SigLIP2 (400M params, 378×378 tiles via AnyRes) -> Adapter: 2-layer MLP with 2×2 spatial downsampling -> LLM Backbone: Dense 2.5B transformer (in-house, distilled from 7B) -> Position Encoding: RoPE → YaRN for 4K→32K extension

- **Critical path:**
  1. Pure-text pre-training (9.3T tokens with distillation)
  2. Joint pre-training (4T tokens, 60:40 multimodal:text)
  3. Reasoning-enhanced stage (2.5T tokens, synthetic CoT)
  4. Long-context activation (YaRN, 32K sequences)
  5. Joint SFT with `[|BlueThink|]` control
  6. Hybrid RL (GRPO + heterogeneous rewards)

- **Design tradeoffs:**
  - Tile-based vision (AnyRes) vs. native resolution: Tiles enable parallel processing and fixed memory, but lose global context per tile
  - 2.9B params vs. 4B baseline: Smaller model fits edge devices but requires more careful data curation
  - Unified model (thinking + non-thinking) vs. separate models: Single deployment but risk of mode confusion

- **Failure signatures:**
  - Thinking mode fails to activate: Check `[|BlueThink|]` token position (should be post-query)
  - Text capability degradation: Verify text ratio didn't drop below ~35% during joint training
  - RL instability: Check if advantage normalization is applied across heterogeneous rewards
  - Long-context quality drop: Ensure YaRN adaptation was applied; RoPE alone won't extrapolate

- **First 3 experiments:**
  1. Ablate text ratio (20%, 40%, 60%) on a small joint training run; measure text benchmark retention vs. multimodal performance.
  2. Test `[|BlueThink|]` position (pre-query vs. post-query) on a mixed SFT dataset with business-style instructions.
  3. Compare GRPO with single reward type vs. hybrid rewards on a held-out validation set spanning math, code, and instruction-following.

## Open Questions the Paper Calls Out

1. Why did curriculum learning, high-temperature sampling, and entropy bonuses fail to improve RL training outcomes for this compact MLLM?
   - The authors report negative results but do not analyze whether these techniques are inherently unsuited for small-scale MLLMs, or if hyperparameter tuning or implementation differences caused the failures.

2. Can the thinking mode activation mechanism generalize robustly to speech and other modalities without requiring task-specific tag repositioning?
   - The current thinking mode depends on a carefully positioned control token ([|BlueThink|]), whose stability was sensitive to instruction format in text/image domains; cross-modal consistency is unexplored.

3. What are the theoretical or empirical limits of data efficiency for compact MLLMs, and does the 23% reduction in training data generalize to other architectures?
   - Without controlled comparisons isolating each factor, it is unclear if similar efficiency can be achieved by other 3B-scale MLLMs or if BlueLM-2.5-3B's design is uniquely suited.

## Limitations

- Exact LLM backbone hyperparameters remain unspecified, preventing complete reproduction
- Data mixture proportions within categories lack granularity for precise replication
- YaRN extension parameters for 32K context scaling are not documented
- Real-world latency and memory usage on specific edge devices remain untested

## Confidence

- **High confidence** in text benchmark claims (Qwen3-4B matching): Supported by clear comparative methodology and established baselines
- **Medium confidence** in multimodal performance (5% gap to Kimi-VL-A3B-16B): Performance claims are specific but depend on exact data mixture and reward model quality
- **Medium confidence** in edge-device suitability: Parameter count and token budget constraints are explicit, but real-world testing is absent
- **Low confidence** in long-context reasoning stability: While 32K context is implemented via YaRN, no ablation studies compare alternative methods

## Next Checks

1. **Data Ratio Ablation Study**: Train three models with 20%, 40%, and 60% text ratios during joint pre-training. Measure catastrophic forgetting on MMLU-Pro and text generation quality while tracking multimodal task performance to validate the 40% threshold claim.

2. **Thinking Mode Trigger Reliability**: Create a benchmark of 100 diverse prompts (business, technical, creative) with ground truth answers. Test both `[|BlueThink|]` positioning strategies (pre-query vs post-query) and measure activation consistency, response quality, and hallucination rates across modes.

3. **Hybrid RL Reward Attribution**: Implement ablation variants: (a) single reward type for all tasks, (b) rule-based only, (c) learned reward model only. Compare final performance across math, code, and instruction-following tasks to quantify each reward mechanism's contribution to the hybrid approach.