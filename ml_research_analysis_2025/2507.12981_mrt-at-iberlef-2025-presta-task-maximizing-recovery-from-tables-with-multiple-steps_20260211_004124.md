---
ver: rpa2
title: 'MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple
  Steps'
arxiv_id: '2507.12981'
source_url: https://arxiv.org/abs/2507.12981
tags:
- column
- columns
- instructions
- value
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a multi-step system for answering questions
  over Spanish tables using large language models and Python code generation. The
  approach improves upon prior work by adding column filtering, clarification instructions,
  custom functions, and fuzzy matching to handle ambiguities and naming issues.
---

# MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps

## Quick Facts
- arXiv ID: 2507.12981
- Source URL: https://arxiv.org/abs/2507.12981
- Reference count: 25
- Primary result: 85% accuracy on IberLEF-2025 PRESTA task for Spanish table QA

## Executive Summary
This paper presents a multi-step system for answering questions over Spanish tables using large language models and Python code generation. The approach improves upon prior work by adding column filtering, clarification instructions, custom functions, and fuzzy matching to handle ambiguities and naming issues. It achieves 85% accuracy on the IberLEF-2025 PRESTA task, with the system producing more interpretable intermediate steps for debugging. Manual analysis shows most errors stem from column selection and instruction generation, while ablation tests suggest the new features offer modest gains, limited by dataset size. The system is open-source and aims to balance performance with explainability in table question answering.

## Method Summary
The system implements a seven-module pipeline: Column Descriptor analyzes table metadata, Column Selector filters relevant columns in batches of 25, Explainer generates natural language instructions with clarification hints, Coder translates instructions to Python code using pre-defined custom functions, Runner executes code with exception handling, Interpreter extracts answers, and Formatter converts to expected data types. The system uses Qwen 2.5 14B for analysis and Qwen 2.5 Coder 14B for code generation, with 8 repetitions and majority voting for final answers.

## Key Results
- Achieves 85% accuracy on IberLEF-2025 PRESTA test set
- Manual error analysis shows 31% of errors from wrong column selection, 37.9% from wrong natural language instructions
- Ablation study suggests custom functions and fuzzy matching provide modest improvements, limited by small dataset size
- System produces interpretable intermediate steps for debugging, with 8-run ensemble majority voting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing table QA into sequential specialized modules reduces cognitive load on the LLM and improves explainability.
- Mechanism: The system splits the task into: (1) column description/selection, (2) natural language instruction generation, (3) code generation, (4) execution, and (5) answer formatting. Each module operates on the output of the previous one with focused prompts, preventing the model from needing to reason about the entire problem simultaneously.
- Core assumption: Intermediate natural language instructions provide a meaningful abstraction that is easier for LLMs to generate and debug than direct code synthesis from raw tables.
- Evidence anchors:
  - [abstract]: "The process consists of multiple steps: analyzing and understanding the content of the table, selecting the useful columns, generating instructions in natural language, translating these instructions to code, running it, and handling potential errors or exceptions."
  - [section]: "In comparison with end-to-end solutions, our approach is more explainable as it is easy to debug and trace which was the cause of a good or bad response." (Page 1, Section 1)
  - [corpus]: The original MRT paper (SemEval-2025) uses a similar multi-step decomposition, suggesting the pattern generalizes across table QA tasks.
- Break condition: If the intermediate natural language instructions are consistently ambiguous or incorrect, the downstream code generation will fail regardless of module specialization.

### Mechanism 2
- Claim: Column pre-filtering prevents context overflow and reduces irrelevant information that could mislead instruction generation.
- Mechanism: The column selector module uses an LLM to identify relevant columns by processing them in batches of 25 with names and descriptions, explicitly instructed to include doubtful columns rather than exclude them. This reduces the average column count from 174.1 to a manageable subset before the explainer module.
- Core assumption: The column selector can identify all relevant columns with high recall; false negatives here are unrecoverable in later stages.
- Evidence anchors:
  - [abstract]: Not explicitly mentioned, but implied by the multi-step filtering process.
  - [section]: "This new module tries to filter the columns to avoid crashes by large prompts whilst leaving the relevant questions untouched." (Page 4, Section 4.2.1)
  - [section]: "Wrong column filtering" accounts for 31.0% of errors in manual analysis (Page 8, Table 5).
  - [corpus]: Corpus papers do not provide direct evidence on column filtering effectiveness for table QA; this is a gap.
- Break condition: If tables have highly correlated columns with subtle semantic differences (e.g., "Edad" vs "Edad_recodificada"), the selector may include irrelevant or exclude relevant columns.

### Mechanism 3
- Claim: Pre-coded custom functions with fuzzy matching reduce code generation errors and handle value representation mismatches.
- Mechanism: Instead of generating raw pandas code, the coder module is prompted to use pre-defined functions (e.g., filter_rows_by_column_equals_or_less_than_numeric_value) that internally implement fuzzy matching (threshold=75) for categorical values. This abstracts common operations and handles cases where question terms (e.g., "Obama") differ from table values (e.g., "Barack Obama").
- Core assumption: The set of pre-coded functions covers the majority of required operations, and fuzzy matching thresholds generalize across datasets.
- Evidence anchors:
  - [abstract]: "The approach improves upon prior work by adding column filtering, clarification instructions, custom functions, and fuzzy matching to handle ambiguities and naming issues."
  - [section]: "These functions perform sequentially the following steps: 1. Identifying the target column and value. 2. Recognize empirically that exact matches are often not achievable... 3. If an exact match is unavailable, apply fuzzy matching..." (Page 6, Section 4.2.5)
  - [section]: Ablation study shows removing custom functions drops score from 0.71 to 0.72 (inconclusive due to small sample size), but removing fuzzy substitution drops score from 0.71 to 0.69 (Page 9, Table 6).
  - [corpus]: ExpliCIT-QA extends MRT with multimodal components, suggesting the custom function approach is reusable but requires adaptation for new modalities.
- Break condition: If questions require operations not covered by the custom function library, the system must fall back to general code generation, which is more error-prone.

## Foundational Learning

- Concept: **Table Question Answering (Table QA)**
  - Why needed here: Understanding the specific challenges of querying structured tabular data versus unstructured text, including context length limits, ambiguous column names, and value representation mismatches.
  - Quick check question: Given a table with columns "Edad" and "Edad_recodificada" (age recoded), how would you determine which is relevant for a question asking for "respondents over 65"?

- Concept: **LLM Code Generation**
  - Why needed here: The system relies on LLMs to generate executable Python code from natural language instructions, requiring understanding of both prompt engineering for code tasks and common failure modes (e.g., incorrect pandas operations).
  - Quick check question: What are two common code generation errors that pre-coded custom functions help avoid?

- Concept: **Modular Pipeline Design**
  - Why needed here: The MRT system's effectiveness depends on understanding how to decompose a complex task into sequential modules with clear interfaces, and how errors propagate through the pipeline.
  - Quick check question: If the column selector module incorrectly filters out a relevant column, can downstream modules recover? Why or why not?

## Architecture Onboarding

- Component map: Column Descriptor → Column Selector → Explainer → Coder → Runner → Interpreter → Formatter
- Critical path: Column Descriptor → Column Selector → Explainer → Coder → Runner → Interpreter → Formatter. The system can retry from the Coder module if exceptions occur during execution.
- Design tradeoffs:
  - **Explainability vs. End-to-End Performance**: Modular design enables debugging but introduces potential error accumulation across modules.
  - **Precision vs. Recall in Column Selection**: The column selector is biased toward inclusion ("in case of doubt, return the column"), which prevents false negatives but may include irrelevant columns.
  - **Custom Functions vs. General Code Generation**: Pre-coded functions reduce errors for common operations but limit flexibility for novel queries.
- Failure signatures:
  - **Wrong column selection** (31.0% of errors): Often occurs with semantically similar column names or columns with long, complex names (e.g., survey question text as column headers).
  - **Wrong natural language instructions** (37.9% of errors): Explainer adds unnecessary filters or misinterprets question intent.
  - **Formatting issues** (10.3% of errors): Interpreter incorrectly strips symbols (e.g., "+65" → "65").
- First 3 experiments:
  1. **Unit test the column selector**: Run the column selector module in isolation on tables with known relevant columns to measure recall and identify patterns in false negatives.
  2. **Trace a complete pipeline execution**: Select a single question from the validation set, run it through all modules with logging enabled, and manually verify each module's output against expected behavior.
  3. **Ablate the fuzzy matching**: Run the system with fuzzy matching disabled (threshold=100) on a subset of questions to quantify its contribution to overall accuracy and identify specific value mismatch cases it handles.

## Open Questions the Paper Calls Out

- **Open Question 1**: Do the auxiliary modules (column selector, custom functions, fuzzy matching) yield statistically significant improvements when evaluated on larger datasets?
  - Basis in paper: [inferred] The ablation study shows minimal differences (1-5 questions) between configurations. The authors state the dataset "lacks the size and diversity in order to be statistically relevant."
  - Why unresolved: The validation set contained only 100 questions across 4 tables, which was insufficient to confirm if the new modules improved performance or if the small deltas were random noise.
  - What evidence would resolve it: A statistically powered ablation study on a benchmark with significantly more tables and question pairs, demonstrating clear performance deltas for each module.

- **Open Question 2**: How can the "Explainer" module be refined to reduce the primary error source of incorrect natural language instruction generation?
  - Basis in paper: [inferred] Table 5 identifies "Wrong Instructions" as the leading cause of errors (37.9%), and the analysis notes frequent unnecessary filtering steps.
  - Why unresolved: While the paper introduces clarification instructions and JSON formatting, the logic responsible for generating the reasoning steps remains the biggest bottleneck.
  - What evidence would resolve it: An error analysis showing a reduction in logic-based instruction errors following specific architectural changes (e.g., iterative self-correction or different model fine-tuning).

- **Open Question 3**: Does the system maintain robustness when applied to datasets with fewer ambiguous column names or different linguistic structures than the PRESTA survey data?
  - Basis in paper: [explicit] "In the future, we plan to test the system against larger datasets in order to gain more insights into the relevance of each block in the final answer."
  - Why unresolved: The current results are specific to the IberLEF 2025 PRESTA task, which features specific challenges like survey data with ambiguous column names (e.g., "N_R").
  - What evidence would resolve it: Benchmark results on diverse table domains (e.g., financial, scientific) showing the generalizability of the fuzzy matching and column selection strategies.

## Limitations
- Reliance on 14B-parameter LLMs limits reproducibility to researchers with high-end GPUs
- Small validation set (100 questions) makes statistical significance of ablation results questionable
- Effectiveness of fuzzy matching and custom functions not extensively tested across diverse value representation scenarios

## Confidence
- **High Confidence**: The modular pipeline architecture improves explainability and enables debugging, as evidenced by the clear error analysis showing 31% of errors from column selection and 37.9% from instruction generation.
- **Medium Confidence**: The system achieves 85% accuracy on the PRESTA test set, but this is on a small dataset (100 questions) and does not directly compare to end-to-end alternatives on the same data.
- **Low Confidence**: The ablation study conclusions are limited by sample size, and the generalization of custom functions and fuzzy matching to other datasets remains unproven.

## Next Checks
1. **Unit test the column selector** on tables with known relevant columns to measure recall and identify false negative patterns.
2. **Trace a complete pipeline execution** on a single validation question, logging each module's output to verify expected behavior.
3. **Ablate the fuzzy matching** on a subset of questions to quantify its specific contribution to accuracy and identify value mismatch cases it handles.