---
ver: rpa2
title: 'Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors
  and Capabilities'
arxiv_id: '2601.21702'
source_url: https://arxiv.org/abs/2601.21702
tags:
- unlearning
- https
- machine
- arxiv
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores representation misdirection (RM), a class\
  \ of LLM unlearning methods that manipulates forget-representations toward a target\
  \ vector to achieve selective forgetting. While previous work used random vectors,\
  \ this paper revisits RM through the linear representation hypothesis, which suggests\
  \ that high-level concepts can be represented as one-dimensional vectors in the\
  \ model\u2019s latent space."
---

# Beyond Forgetting: Machine Unlearning Elicits Controllable Side Behaviors and Capabilities

## Quick Facts
- arXiv ID: 2601.21702
- Source URL: https://arxiv.org/abs/2601.21702
- Reference count: 40
- Primary result: Representation misdirection with concept vectors elicits controllable side behaviors (e.g., 12.7-point TruthfulQA improvement) while maintaining unlearning effectiveness

## Executive Summary
This paper challenges the conventional view of machine unlearning as merely forgetting unwanted information. Through representation misdirection (RM) methods, the authors demonstrate that unlearning can be engineered to elicit controllable side behaviors and enhanced capabilities aligned with specific concepts. By adding or ablating high-level concept vectors during the unlearning process, the model not only forgets target knowledge but also develops structured behaviors—becoming more truthful, more positive, or more likely to refuse instructions. The work establishes a theoretical framework connecting linear representation hypothesis to controllable behavior modification in LLMs.

## Method Summary
The paper introduces Representational Addition (RAd) and Representational Ablation (RAb) as extensions to representation misdirection unlearning. RAd adds scaled concept vectors to forget-representations, while RAb removes components aligned with concept directions. Concept vectors are extracted using Logistic Regression probes trained on contrastive prompt pairs. The method fine-tunes limited layers (typically layers 5-7) on combined forget and retain sets, manipulating representations at the MLP output of layer 7. The approach leverages the linear representation hypothesis to predictably influence output distributions toward desired concepts while suppressing target knowledge.

## Key Results
- RAd with truthfulness direction improves TruthfulQA BLEU by 12.7 points for Mistral-7B while maintaining unlearning (WMDP drops to 25.0)
- RAd with sentiment direction increases positive sentiment by 38.6-39.9 percentage points
- RAd with refusal direction induces refusal even to harmless instructions
- RAd with task-specific directions improves in-context learning capabilities by 32.9-58.5 percentage points
- RAd models resist Logitlens and enhanced GCG attacks but remain vulnerable to finetuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding a high-level concept vector to forget-representations during unlearning shifts the model's output distribution toward that concept while suppressing target knowledge.
- Mechanism: Under the linear representation hypothesis, RAd computes: λ' = λ_f + c·λ̄_W. This additive operation multiplies the odds of generating the concept-aligned output by exp(αc·λ̄_W^⊤ γ̄_W), monotonically increasing probability of Y=1 outcomes (e.g., truthful answers).
- Core assumption: High-level concepts are represented linearly in latent space and can be extracted via linear probes trained on contrastive prompt pairs.
- Evidence anchors:
  - [section 3.2.1]: Derivation showing additive intervention increases odds of target outcome Y=1
  - [Table 1]: RAd with truthfulness direction improves TruthfulQA BLEU by +12.7 for Mistral-7B while maintaining unlearning (WMDP drops to 25.0)
  - [corpus]: Corpus lacks direct mechanistic validation; related work (AUVIC) focuses on adversarial unlearning in multimodal settings, not representation-level mechanisms
- Break condition: If concept vectors are not approximately one-dimensional or if the linear representation hypothesis fails for the target concept, RAd effects will be unpredictable.

### Mechanism 2
- Claim: Ablating concept-aligned components from forget-representations reduces probability of generating concept-related outputs while preserving off-target information.
- Mechanism: RAb computes: λ' = λ_f - c(λ_f^⊤ λ̄_W/||λ̄_W||²)λ̄_W, projecting out the component aligned with concept direction. This divides the odds by exp(αc(λ_f^⊤ λ̄_W)(λ̄_W^⊤ γ̄_W)), reducing probability of Y=1 outcomes.
- Core assumption: Forget-representations contain positive evidence for the concept (λ_f^⊤ λ̄_W > 0); if this inner product is negative or near-zero, ablation has minimal effect.
- Evidence anchors:
  - [section 3.2.2]: Mathematical derivation of ablative intervention reducing target outcome probability
  - [Table 1]: RAb with truthfulness direction degrades TruthfulQA MC1 by -12.9 and MC2 by -15.0 for Zephyr-7B
  - [corpus]: No corpus papers directly test ablative vs. additive interventions; validation limited to this work
- Break condition: If forget-representations have minimal alignment with the concept direction, ablation produces negligible behavioral change.

### Mechanism 3
- Claim: Random vectors in high-dimensional representation spaces are nearly orthogonal to concept directions with high probability, explaining why random target vectors in prior RM methods do not induce structured side effects.
- Mechanism: By Proposition 3.2 (derived from Lévy's Lemma), for a random unit vector u on S^(d-1), P(|⟨u, λ̄_W⟩| ≤ ε) ≥ 1 - 2exp(-(d-1)ε²/2). In high dimensions, random vectors rarely align with any specific concept direction.
- Core assumption: The representation space dimension d is sufficiently large (modern LLMs have d > 4096) for concentration of measure to apply.
- Evidence anchors:
  - [section 3.2.3]: Proposition 3.2 with proof using Lévy's Lemma
  - [Table 1]: RAd with random vector yields only +0.8 average improvement on TruthfulQA vs. +12.7 with truthfulness direction (Mistral-7B)
  - [Figure 12]: Empirical cosine similarity between random and concept directions is concentrated near zero
  - [corpus]: No corpus papers test random vs. concept vector orthogonality
- Break condition: In low-dimensional representation spaces or if concept directions span most of the space, random vectors may have non-negligible alignment with concepts.

## Foundational Learning

- Concept: **Linear Representation Hypothesis**
  - Why needed here: This hypothesis underpins the entire theoretical framework—without linear encoding of concepts, neither RAd nor RAb would produce predictable effects.
  - Quick check question: Can you explain why Theorem 2.2 (log-odds linearity) implies that adding a concept vector changes output probabilities monotonically?

- Concept: **Representation Misdirection (RM)**
  - Why needed here: RM is the broader class of unlearning methods this work analyzes; understanding the baseline (random vectors) clarifies why concept vectors matter.
  - Quick check question: What is the key difference between RM with random vectors vs. concept vectors in terms of side effects?

- Concept: **Knowledge Recovery Attacks (Logitlens, Finetuning, Orthogonalization)**
  - Why needed here: Section 4.5 evaluates robustness; RAd models resist Logitlens and enhanced GCG but are vulnerable to finetuning. Understanding these attack vectors is critical for deployment.
  - Quick check question: Why does RAd resist Logitlens but remain vulnerable to finetuning attacks?

## Architecture Onboarding

- Component map:
  - Concept extraction -> Forward hook -> Loss computation -> Gradient update
  - Logistic Regression probe -> Extract forget-representations at layer 7 -> RAd/RAb loss combining forget/retain terms -> Update layers {l, l-1, l-2}

- Critical path:
  1. Identify concept direction via probe training on labeled contrastive data
  2. Extract representations at layer l=7 (MLP output) during unlearning forward pass
  3. Compute RAd/RAb loss combining forget-set manipulation and retain-set preservation
  4. Update limited layers to prevent catastrophic forgetting on retain-set

- Design tradeoffs:
  - RAd vs. RAb: RAd achieves better unlearning (lower WMDP) and induces useful side effects but is brittle to benign perturbations (perturbed MMLU drops from 60.2 to 23.5). RAb is more robust but less effective at unlearning.
  - Scaling coefficient c: Higher c increases concept influence but risks overwriting retain knowledge. Paper uses c ∈ [14, 60] depending on task/model.
  - Layer selection: Layer 7 is used across experiments; middle layers show vulnerability to Logitlens for RAb models (Figure 1).

- Failure signatures:
  - RAd with high c on small retain-set: Catastrophic utility collapse—MMLU drops significantly
  - RAb on concept with λ_f^⊤ λ̄_W ≈ 0: Minimal behavioral change, ineffective unlearning
  - Benign perturbation with forget-tokens in retain queries: RAd models produce near-random outputs (Table 10)

- First 3 experiments:
  1. Validate concept extraction: Train probe on your target concept (e.g., truthfulness), verify probe accuracy on held-out contrastive pairs before unlearning
  2. Baseline unlearning comparison: Run RAd with random vs. concept vectors on WMDP-Biology, measure WMDP accuracy drop and MMLU retention—expect random to show minimal side effects
  3. Robustness stress test: After RAd unlearning, apply benign perturbation (insert forget-tokens into retain queries) and measure MMLU accuracy to quantify fragility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can linear representations (concept vectors) extracted from one model architecture be transferred to effectively guide unlearning and elicit side behaviors in a different model architecture?
- Basis in paper: [explicit] Section 5 explicitly identifies exploring side effects in different model architectures via the "linear representation transferability hypothesis" as a promising future direction.
- Why unresolved: The paper only validates RAd and RAb using concept vectors extracted from the same model being unlearned (e.g., Mistral-7B vectors for Mistral-7B).
- What evidence would resolve it: Successful unlearning and behavioral control in a target model (e.g., Llama-3) when using concept vectors derived solely from a source model (e.g., Mistral-7B).

### Open Question 2
- Question: Can the vulnerability of Representational Addition (RAd) models to benign perturbations (e.g., overlapping forget-tokens) be mitigated without compromising unlearning effectiveness?
- Basis in paper: [inferred] Table 10 and Appendix F show RAd models suffer significant utility collapse (near-random accuracy) on perturbed MMLU benchmarks, whereas RAb models are robust but less effective at unlearning.
- Why unresolved: The authors identify this "fundamental trade-off" between unlearning effectiveness and robustness to benign perturbation but do not propose a method to overcome it.
- What evidence would resolve it: A modified RAd technique that maintains high accuracy on perturbed retain-sets while simultaneously achieving low accuracy on forget-sets (e.g., WMDP).

### Open Question 3
- Question: How does the specific method used to extract concept vectors (e.g., Logistic Regression vs. PCA vs. diff-in-means) influence the magnitude and reliability of the elicited side behaviors?
- Basis in paper: [inferred] Section 3.3 utilizes a "simple Logistic Regression probe" to identify concept directions, assuming this suffices to capture the linear representation.
- Why unresolved: The paper does not ablate the vector extraction method; thus, it is unclear if the quality of the side effects (e.g., the +12.7 BLEU score in TruthfulQA) is dependent on the probe's specific noise or accuracy.
- What evidence would resolve it: A comparative analysis showing that alternative extraction methods yield significantly different side effect intensities or unlearning efficiencies.

## Limitations

- **Representation fragility**: RAd models show extreme brittleness to benign perturbations, with MMLU accuracy collapsing from 60.2 to 23.5 when forget-tokens are inserted into retain queries
- **Generalization uncertainty**: All experiments use specific LLMs (Zephyr-7B, Mistral-7B) and targeted concepts, without establishing whether RAd/RAb generalize to other model families or more nuanced concepts
- **Attack resistance trade-offs**: While RAd resists Logitlens and enhanced GCG attacks, it remains vulnerable to finetuning, creating a false sense of security against one attack class while shifting vulnerabilities to others

## Confidence

- **High Confidence**: The core observation that concept vectors produce structured side effects (Tables 1-2, Figures 12) is empirically robust and theoretically grounded in the linear representation hypothesis
- **Medium Confidence**: Claims about RAd superiority over RAb for unlearning (WMDP comparisons) and robustness against Logitlens hold within tested parameters but may not generalize
- **Low Confidence**: The claim that RAd "improves" capabilities (e.g., +58.5 points on ICL tasks) conflates induced behavior with genuine capability enhancement

## Next Checks

1. **Robustness Stress Testing**: Systematically vary benign perturbations (token insertion positions, forget-token counts, retain-set diversity) to characterize the exact failure boundary of RAd models. Compare fragility across concept directions to determine if certain concepts induce more stable representations.

2. **Cross-Concept Transferability**: Apply RAd with one concept direction (e.g., truthfulness) to unlearning tasks for different concepts (e.g., toxicity). Measure whether learned representations transfer or if each concept requires independent extraction and fine-tuning.

3. **Long-term Stability Evaluation**: Track RAd model performance over extended inference periods and across diverse prompts to detect gradual degradation or concept drift. This addresses whether induced capabilities persist or decay under normal usage patterns.