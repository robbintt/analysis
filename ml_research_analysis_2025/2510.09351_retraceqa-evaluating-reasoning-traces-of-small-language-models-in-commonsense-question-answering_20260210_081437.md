---
ver: rpa2
title: 'ReTraceQA: Evaluating Reasoning Traces of Small Language Models in Commonsense
  Question Answering'
arxiv_id: '2510.09351'
source_url: https://arxiv.org/abs/2510.09351
tags:
- reasoning
- error
- question
- evaluation
- correct
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReTraceQA, a novel benchmark designed to
  evaluate reasoning traces of small language models (SLMs) in commonsense question
  answering tasks. The key problem addressed is that current evaluation practices
  rely solely on final answer accuracy, overlooking flawed reasoning processes that
  still yield correct answers.
---

# ReTraceQA: Evaluating Reasoning Traces of Small Language Models in Commonsense Question Answering

## Quick Facts
- arXiv ID: 2510.09351
- Source URL: https://arxiv.org/abs/2510.09351
- Reference count: 27
- Primary result: Current evaluation practices overestimate SLM reasoning capability by 14-25% by focusing only on final answers rather than the reasoning process

## Executive Summary
This paper introduces ReTraceQA, a novel benchmark designed to evaluate reasoning traces of small language models in commonsense question answering tasks. The key problem addressed is that current evaluation practices rely solely on final answer accuracy, overlooking flawed reasoning processes that still yield correct answers. ReTraceQA provides expert-annotated reasoning traces with step-level error locations and categorizations, revealing that 14-24% of instances contain flawed reasoning despite correct final answers. The authors show that when employing strong LLMs as automated judges for reasoning-aware evaluation rather than answer-only metrics, SLM performance drops significantly by up to 25% across all models and datasets.

## Method Summary
ReTraceQA is a benchmark for evaluating reasoning traces of small language models in commonsense question answering. The authors annotate 2,421 instances from four commonsense QA datasets (CSQA, OBQA, QASC, StrategyQA) with step-level reasoning traces generated via zero-shot CoT prompting from seven SLMs. Expert annotators identify the first error position and categorize errors as hallucination, reasoning errors, or misinterpretations. The benchmark is evaluated using strong LLMs as judges and Process Reward Models, comparing reasoning-aware evaluation (process error detection) against traditional answer-only evaluation.

## Key Results
- 14-24% of instances contain flawed reasoning despite correct final answers
- Answer-only metrics overestimate SLM reasoning capability by 14-25%
- When evaluated with reasoning-aware metrics, SLM performance drops significantly across all models and datasets
- Hallucination errors constitute the majority of reasoning failures (41.9-62.5%)
- Strong LLMs can classify reasoning trace validity but struggle to localize exact error positions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Answer-only metrics systematically overestimate SLM reasoning capability by 14-25% because correct final answers can emerge from flawed intermediate reasoning.
- Mechanism: SLMs use shallow heuristics (pattern matching, surface-level associations) that bypass coherent multi-step inference. When evaluated only on final answers, these heuristics produce "lucky correct" outputs that mask underlying reasoning failures. Process-level annotation exposes this by requiring step-by-step validation.
- Core assumption: Correct reasoning should be coherent at each step, not just at the endpoint.
- Evidence anchors:
  - [abstract] "14-24% of instances contain flawed reasoning despite correct final answers"
  - [section 3.6] "17.9% of instances arrive at correct final answers despite containing a reasoning error"
  - [corpus] JudgeBoard confirms SLMs struggle with self-evaluation of reasoning correctness

### Mechanism 2
- Claim: Strong LLMs can classify reasoning trace validity but struggle to localize exact error positions.
- Mechanism: LLMs develop global coherence representations that detect "something is wrong" but lack fine-grained step-level attribution. This creates asymmetric performance: high accuracy on binary correct/incorrect classification (~80-90%), lower accuracy on pinpointing first error step (~50-70%).
- Core assumption: Larger models internalize reasoning patterns that transfer to evaluation without task-specific training.
- Evidence anchors:
  - [section 5.1] "models detect trace correctness better than localizing errors... o1-mini achieves 74.3% error classification on OBQA, still lagging behind its correctness detection ability"
  - [table 2] Reference-based F1 scores for error localization range 31.7-79.2% across models and datasets
  - [corpus] Project Aletheia shows SLMs produce "linear, overconfident reasoning traces that do not recover from early mistakes"

### Mechanism 3
- Claim: Hallucination dominates SLM reasoning errors in commonsense (41.9-62.5%), indicating factual grounding failures rather than pure logical errors.
- Mechanism: Commonsense reasoning requires world knowledge retrieval. SLMs with limited parametric knowledge generate plausible-sounding but false statements when knowledge gaps occur. Unlike math (where reasoning errors dominate), commonsense exposes knowledge-encoding limitations.
- Core assumption: Error type distribution reflects underlying capability gaps rather than annotation artifact.
- Evidence anchors:
  - [section 3.6] "hallucination errors constitute the majority of failures (41.9%-62.5%), followed by reasoning errors (27.9%-35.4%)"
  - [section 5.1] "PRMs originally developed and trained for mathematical tasks perform significantly worse... highlighting domain-specific gaps"

## Foundational Learning

- Concept: **Chain-of-Thought (CoT) Prompting**
  - Why needed here: ReTraceQA generates all reasoning traces using zero-shot CoT. Understanding how CoT elicits step-by-step reasoning is prerequisite to evaluating trace quality.
  - Quick check question: Can you explain why "Let's think step by step" changes model output structure compared to direct prompting?

- Concept: **Process Reward Models (PRMs) vs. Outcome Reward Models (ORMs)**
  - Why needed here: The paper explicitly contrasts PRMs (step-level evaluation) with ORMs (final-answer evaluation). This distinction is central to the benchmark's purpose.
  - Quick check question: Given a 5-step reasoning trace with an error at step 2 but correct final answer, would a PRM or ORM give a higher score?

- Concept: **LLM-as-a-Judge Paradigm**
  - Why needed here: The evaluation methodology uses strong LLMs as automated judges. Understanding biases, calibration, and limitations of judge models is essential for interpreting results.
  - Quick check question: What are two failure modes when using GPT-4 to evaluate reasoning traces from smaller models?

## Architecture Onboarding

- Component map: Commonsense questions from CSQA, OBQA, QASC, StrategyQA → 7 SLMs producing zero-shot CoT traces → Step segmentation at "\n\n" boundaries → 3 expert annotators labeling first error position + error type → LLM judges (GPT-4o, o1-mini, Qwen2.5-72B) and PRMs evaluating traces → Compare judge predictions vs. human labels

- Critical path: Question → SLM generation → Step segmentation → Human annotation (ground truth) → LLM judge evaluation → Compare judge predictions vs. human labels

- Design tradeoffs:
  - Reference-free vs. Reference-based evaluation: Reference-free tests real-world deployment; reference-based tests maximum judge capability
  - First-error-only annotation: Ignores downstream error propagation but provides unambiguous training signal
  - Binary F1 vs. accuracy: Harmonic mean balances permissive vs. overly critical judges

- Failure signatures:
  - Judge over-assigns blame to later steps: Indicates model captures error consequences rather than origins
  - PRM scores near-random on commonsense: Math-trained PRMs don't transfer; expect F1 < 25%
  - High process error rate in smaller SLMs: Llama-3.2-1B and Qwen2.5-1.5B show 25-40% flawed-correct traces

- First 3 experiments:
  1. Baseline validation: Run your SLM on ReTraceQA questions, extract final answers, compute accuracy. Then use o1-mini as judge for reasoning-aware evaluation. Expect 15-25% accuracy drop.
  2. Error localization probe: Feed ReTraceQA traces to your judge model, compare predicted error positions against ground truth. Compute separate accuracy for correct-trace-detection vs. error-localization.
  3. Domain transfer test: Apply a math-trained PRM (e.g., Math-Shepherd-7B) to ReTraceQA traces. Verify the ~4-28% F1 degradation reported in Table 2.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized Process Reward Models (PRMs) trained specifically on commonsense reasoning traces close the performance gap observed with math-trained PRMs?
- Basis in paper: The Limitations section states that mathematical PRMs fail to transfer to commonsense domains, motivating the need to develop specialized PRMs for non-STEM domains.
- Why unresolved: Current PRMs rely on the formal verifiability of math, which is absent in the ambiguous, world-knowledge-dependent nature of commonsense tasks.
- What evidence would resolve it: Training a new PRM on ReTraceQA annotations and demonstrating superior F1 scores in error localization compared to Math-Shepherd on commonsense benchmarks.

### Open Question 2
- Question: What prompting strategies or fine-tuning methods can align LLM-judge error localization accuracy with their high trace-level correctness detection?
- Basis in paper: Section 5.1 and the Conclusion note that while models like o1-mini detect incorrect traces effectively, they struggle to pinpoint the exact error step, often assigning blame to later consequences.
- Why unresolved: Judges currently identify global semantic inconsistencies but lack the granularity to isolate the specific causal break in the reasoning chain.
- What evidence would resolve it: An ablation study showing a specific prompting technique that significantly increases "error" metric scores without degrading "correct" trace detection.

### Open Question 3
- Question: Does the prevalence of "process errors" (correct answer, flawed reasoning) remain consistent across multilingual settings?
- Basis in paper: The Limitations section identifies the English-only focus as a constraint and suggests extending the framework to multilingual settings is necessary.
- Why unresolved: It is unclear if the 14-24% error rate is unique to English SLMs or if different languages induce varying rates of hallucination versus reasoning errors.
- What evidence would resolve it: Applying the ReTraceQA annotation protocol to a multilingual benchmark to compare error distributions across languages.

## Limitations
- Answer-only metrics systematically overestimate SLM reasoning capability by focusing only on final answers rather than the reasoning process
- Hallucination errors constitute the majority of reasoning failures in commonsense tasks (41.9-62.5%)
- Strong LLMs can classify reasoning trace validity but struggle to localize exact error positions

## Confidence
- **Medium**: In the 14-24% flawed-correct trace estimate due to potential annotation subjectivity
- **Low**: In cross-dataset generalizability - current benchmark covers limited slice of real-world applications
- **Medium**: In LLM judge reliability - judges themselves can exhibit reasoning errors and biases

## Next Checks
1. **Annotation reliability study**: Re-annotate a random 10% sample of traces with fresh annotators blind to original labels to measure inter-annotator agreement and potential bias in error classification.

2. **Domain transfer experiment**: Apply ReTraceQA evaluation to a non-commonsense domain (e.g., simple scientific reasoning or everyday decision-making scenarios) to test whether hallucination dominance persists outside the current scope.

3. **Judge model calibration**: Test whether providing judge models with confidence scores for each reasoning step improves error localization accuracy compared to binary correct/incorrect predictions.