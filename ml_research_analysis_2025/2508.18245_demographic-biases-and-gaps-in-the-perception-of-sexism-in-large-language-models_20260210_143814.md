---
ver: rpa2
title: Demographic Biases and Gaps in the Perception of Sexism in Large Language Models
arxiv_id: '2508.18245'
source_url: https://arxiv.org/abs/2508.18245
tags:
- sexism
- demographic
- gender
- profiling
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates demographic biases in LLMs for sexism detection
  using the EXIST 2024 dataset, which includes annotations from six demographic profiles
  per tweet. The authors systematically compare prompt designs for inducing demographic
  profiles in LLMs and evaluate alignment with human annotators' perceptions.
---

# Demographic Biases and Gaps in the Perception of Sexism in Large Language Models

## Quick Facts
- arXiv ID: 2508.18245
- Source URL: https://arxiv.org/abs/2508.18245
- Reference count: 19
- This paper investigates demographic biases in LLMs for sexism detection using the EXIST 2024 dataset, finding that while LLMs can detect sexism at the population level, they do not accurately replicate diverse demographic perspectives.

## Executive Summary
This paper systematically evaluates how well large language models (LLMs) can replicate human demographic perspectives on sexism detection. Using the EXIST 2024 dataset with annotations from six demographic profiles per tweet, the authors compare three prompt designs for inducing demographic profiles in LLMs and measure alignment with human annotators' perceptions. Results show that while LLMs achieve reasonable population-level detection, they fail to accurately represent diverse demographic viewpoints, with alignment varying significantly by demographic group. The best-performing prompt achieved correlations up to 0.5552 with human consensus, but this alignment was inconsistent across different demographic combinations.

## Method Summary
The study uses the EXIST 2024 dataset containing 7,958 tweets annotated by six demographic profiles each (spanning gender, age, and country). The authors test three prompt templates on Llama-3.1-8B-Instruct and Gemini-1.5-Flash-8B, generating 6 responses per tweet at different temperatures to create soft predictions. They measure alignment between model outputs and human annotations using Pearson correlation, comparing performance across demographic groups and prompt variations. All prompts were issued in English regardless of tweet language, and model refusals were classified as sexist cases.

## Key Results
- The best-performing prompt (Gupta et al., 2023) achieved correlations up to 0.5552 with human consensus
- LLMs aligned more closely with female annotators overall compared to male annotators
- Multi-attribute demographic profiling (gender + age + country) yielded higher alignment than single-attribute profiling
- The neutral (no-profile) prompt outperformed profiled outputs for Llama-3.1-8B-8B in some cases

## Why This Works (Mechanism)

### Mechanism 1
- Multi-attribute demographic profiling yields higher alignment than single-attribute profiling
- Joint demographic constraints narrow the output distribution space by providing intersecting contextual signals
- Core assumption: The model's training data contains learnable associations between demographic combinations and language patterns
- Evidence: Table 3 shows Gender-Age-Country profiling achieving 0.5552 correlation vs. 0.4134 for gender-only
- Break condition: If target demographic combinations are underrepresented in training data, profiling may degrade

### Mechanism 2
- The instruction "Your responses should closely mirror the knowledge and abilities of this persona" is the active component enabling profile induction
- This phrasing explicitly constrains the model to retrieve persona-associated knowledge patterns
- Core assumption: The model has encoded demographic-correlated knowledge patterns that can be selectively activated
- Evidence: P1 and P2 (both containing the mirror instruction) outperform P3 by >6 points
- Break condition: If the target task requires reasoning beyond surface-level demographic associations

### Mechanism 3
- Soft prediction aggregation enables comparison with human annotation distributions but limits insight into within-group variance
- Converting 6 binary judgments per tweet into a proportion creates a scalar that can correlate with human soft labels
- Core assumption: Running the same profile multiple times captures meaningful variance rather than noise
- Evidence: "sof t_prediction = #Y ES / 6" — explicit aggregation formula
- Break condition: If temperature=0 is used, soft predictions collapse to binary

## Foundational Learning

- **Persona/persona prompting in LLMs**
  - Why needed here: The entire methodology depends on inducing demographic perspectives via prompt-based role assignment
  - Quick check: Can you explain why "You are a 45-year-old woman from Spain" might produce different outputs than "Classify this tweet" with no persona?

- **Soft labels and annotation disagreement**
  - Why needed here: The EXIST dataset captures annotator disagreement; understanding soft labels as distributions rather than ground truth is essential
  - Quick check: If 4 of 6 annotators label a tweet sexist, what does the soft label represent, and what information is lost?

- **Pearson correlation for alignment measurement**
  - Why needed here: The study uses correlation—not accuracy—to measure whether model outputs follow the same pattern as human demographic groups
  - Quick check: Why might a model achieve high correlation but low accuracy, and what would that mean for demographic alignment?

## Architecture Onboarding

- **Component map**: Prompt template library -> Demographic profile matrix -> Inference loop -> Output extraction -> Aggregation layer -> Evaluation module
- **Critical path**: Prompt template → Profile assignment → Model inference (×6) → Binary extraction → Soft aggregation → Demographic-stratified correlation
- **Design tradeoffs**: Temperature 0 vs. 1: Determinism vs. capturing natural variance; Single vs. joint profiling: Interpretability vs. ecological validity; English-only prompts for Spanish tweets: Control vs. potential cross-lingual signal loss
- **Failure signatures**: Low correlation for specific demographics (e.g., 18-22 group: 0.38-0.48 range); Profiling prompt performing well on one attribute but poorly on others; High refusal rates for certain content types; Neutral profile outperforming profiled outputs
- **First 3 experiments**:
  1. Prompt ablation: Remove the "mirror knowledge and abilities" sentence from P1/P2 and measure correlation drop
  2. Language-matched profiling: Issue prompts in the same language as the tweet and compare alignment
  3. Refusal analysis: Manually annotate a sample of refusals to validate whether classifying them as sexist introduces systematic bias

## Open Questions the Paper Calls Out

- Would native-language prompts for non-English content improve demographic alignment in sexism detection?
- Do larger LLMs (>8B parameters) more accurately replicate diverse demographic perspectives in sexism detection?
- Why does gender-only profiling produce notably different alignment patterns compared to multi-attribute profiling?
- How should model refusals be classified in sexism detection tasks?

## Limitations
- Geographic representativeness is limited to USA, India, Spain, and Nigeria
- All prompts were issued in English even for Spanish tweets, potentially influencing model outputs
- The study used smaller model versions (8B parameters), leaving behavior for much larger models unexplored
- The soft prediction aggregation masks within-group variance and may conflate genuine demographic differences with random variation

## Confidence

- **High confidence**: Population-level sexism detection capability of LLMs and superiority of multi-attribute demographic profiling
- **Medium confidence**: Specific mechanism by which Gupta's prompt achieves better alignment through the "mirror knowledge and abilities" instruction
- **Low confidence**: Generalizability of demographic alignment patterns to underrepresented regions, languages, and intersectional identities

## Next Checks

1. Prompt ablation study: Systematically remove the "mirror knowledge and abilities" sentence from Gupta's prompt to isolate the active instruction component
2. Language-matched profiling: Run identical experiments with prompts in the same language as input tweets to quantify cross-linguistic effects
3. Refusal behavior analysis: Manually annotate a stratified sample of model refusals to determine whether classifying them as sexist systematically biases results