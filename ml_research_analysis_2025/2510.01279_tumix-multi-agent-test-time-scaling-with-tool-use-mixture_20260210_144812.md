---
ver: rpa2
title: 'TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture'
arxiv_id: '2510.01279'
source_url: https://arxiv.org/abs/2510.01279
tags:
- agent
- code
- agents
- search
- tumix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TUMIX introduces a multi-agent test-time scaling framework that
  enhances LLM reasoning by combining textual reasoning, code execution, and web search
  in parallel. It runs diverse agents with distinct tool-use strategies, iteratively
  sharing and refining answers to improve accuracy.
---

# TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture

## Quick Facts
- arXiv ID: 2510.01279
- Source URL: https://arxiv.org/abs/2510.01279
- Reference count: 40
- TUMIX achieves up to 3.55% higher accuracy than state-of-the-art baselines on HLE, GPQA, and AIME benchmarks

## Executive Summary
TUMIX introduces a multi-agent test-time scaling framework that enhances LLM reasoning by combining textual reasoning, code execution, and web search in parallel. It runs diverse agents with distinct tool-use strategies, iteratively sharing and refining answers to improve accuracy. Experiments show TUMIX achieves up to 3.55% higher accuracy than state-of-the-art baselines on benchmarks like HLE, GPQA, and AIME, with near-equal inference costs. It also reduces inference costs to 49% using an LLM-as-Judge termination strategy. Agent diversity and quality are critical to performance, and automatically generating agents via LLMs further improves results.

## Method Summary
TUMIX employs 15 heterogeneous agents (CoT, Code, Search, Dual-Tool, Guided variants) running in parallel across multiple refinement rounds. Each agent receives the original question plus all prior answers as context. After minimum 2 rounds, an LLM-as-Judge evaluates answer consensus for early termination. Final answers are selected via majority voting. The framework integrates code execution (60s timeout) and web search (three variants) as tools, with agents allowed up to 5 tool interactions per problem.

## Key Results
- TUMIX achieves 3.55% higher accuracy than state-of-the-art baselines on HLE, GPQA, and AIME benchmarks
- Coverage reaches 65% while accuracy plateaus at 34% on HLE, highlighting selection bottleneck
- LLM-as-Judge termination reduces inference costs to 49% while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Heterogeneous agent strategies outperform repeated sampling from a single best-performing agent in tool-augmented test-time scaling.
- Mechanism: Different agents (text-only, code-focused, search-enabled, dual-tool, guided) explore distinct reasoning paths. Their correlated-but-not-identical error distributions increase solution space coverage more efficiently than homogeneous sampling, as measured by the coverage metric (probability at least one agent is correct).
- Core assumption: Agent diversity correlates with answer diversity, and diverse answers improve selection quality when combined with effective aggregation.
- Evidence anchors:
  - [abstract] "Agent diversity and quality are crucial"
  - [Section 5.1, Figure 5] Comparisons of 15 diverse agents vs. 3 agents vs. 1 agent show both coverage and average score improve with diversity
  - [corpus] Related work on multi-agent debate (MALLM, arXiv:2509.11656) supports diversity benefits, though Self-MoA (arXiv:2502.00674) finds limited benefit from LLM diversity in non-tool settings—suggesting tool augmentation changes diversity dynamics
- Break condition: If all agents converge to identical strategies or error modes, diversity provides no benefit. Observed in later refinement rounds where coverage drops (Figure 3, 6).

### Mechanism 2
- Claim: Iterative answer-sharing across rounds improves average agent accuracy but reduces coverage, creating an optimal stopping point.
- Mechanism: Each round, agents receive the original question plus all prior answers. This acts as message passing—agents converge toward consensus solutions. Early rounds benefit from cross-pollination of reasoning approaches; later rounds suffer from premature convergence where correct minority answers get discarded.
- Core assumption: LLMs can integrate and build upon others' reasoning, but lack robustness to distinguish correct minority answers from incorrect noise.
- Evidence anchors:
  - [Section 3.2] "Coverage decreases monotonically across all benchmarks"
  - [Figure 3, 4] Show coverage drops while average score rises then plateaus; Sankey diagram shows convergence dynamics
  - [corpus] COMPASS (arXiv:2510.08790) identifies context management as bottleneck in long-horizon reasoning, supporting the degradation pattern
- Break condition: If refinement continues past convergence point, accuracy degrades (GPQA shows this decline). Excessive sharing causes "groupthink" where correct answers are lost.

### Mechanism 3
- Claim: LLM-as-Judge termination preserves accuracy while reducing inference cost to ~49% compared to fixed-round termination.
- Mechanism: After minimum rounds (2), an LLM evaluates answer consistency. If answers show "clear and strong consensus" in reasoning/logic, refinement stops. This prevents over-refinement where diversity collapses and correct answers get discarded.
- Core assumption: The judge LLM can reliably assess answer consensus, and consensus correlates with having found the best achievable answer.
- Evidence anchors:
  - [Section 5.2, Figure 7] Term_LLM achieves near-optimal accuracy at 49% inference cost; Term_Rule (majority stabilization) performs worse
  - [Table 4] Judge prompt explicitly instructs conservative continuation if "any differences in reasoning, phrasing, emphasis, conclusions"
  - [corpus] Corpus lacks direct evidence for or against LLM-as-Judge termination strategies in multi-agent settings
- Break condition: If judge is overconfident and terminates too early (round 1), performance degrades (Figure 7 green curve). Minimum round constraint mitigates this.

## Foundational Learning

- Concept: Test-time scaling (TTS)
  - Why needed here: TUMIX builds on the principle that allocating more compute at inference improves reasoning, but extends it to tool-augmented multi-agent settings. Understanding TTS helps explain why running 15 agents over multiple rounds helps.
  - Quick check question: Can you explain why scaling inference compute differs from scaling training compute, and what the two-stage TTS process (generation + selection) entails?

- Concept: Tool augmentation tradeoffs (Code Interpreter vs. Search vs. Text)
  - Why needed here: TUMIX's core contribution is mixing these modalities. Text handles semantics/commonsense; code handles precise computation; search handles knowledge retrieval. Knowing when each fails helps design agent strategies.
  - Quick check question: For a question asking "What is the 50th Fibonacci number modulo 7?", which tool would you prioritize and why?

- Concept: Coverage vs. accuracy in ensemble methods
  - Why needed here: TUMIX explicitly tracks both metrics. Coverage = P(at least one agent correct). Accuracy = P(final selected answer correct). Understanding this distinction explains why diversity helps coverage but selection remains the bottleneck.
  - Quick check question: If coverage is 65% but accuracy is 34%, what does this tell you about the selection mechanism?

## Architecture Onboarding

- Component map:
  - Agent Pool -> Round Controller -> Tool Execution Layer -> Termination Module -> Answer Selector
  - Agents generate answers in parallel, results flow to round controller which builds joint prompt, tools execute, judge evaluates, selector finalizes

- Critical path:
  1. Question → all 15 agents in parallel (each with tool access, max 5 tool interactions)
  2. Collect 15 answers → build joint prompt for round 2
  3. Repeat refinement until termination criteria met (minimum 2 rounds)
  4. Aggregate final answers via majority vote

- Design tradeoffs:
  - **Agent count**: 12-15 agents shows diminishing returns (Figure 10); more agents increase coverage but complicate selection
  - **Refinement rounds**: Early rounds help (Figure 3), but GPQA shows degradation after round 2
  - **Tool access**: Full Code+Search access yields best coverage/accuracy (Figure 6), but increases per-inference cost
  - **LLM-generated vs. human-designed**: LLM-generated agents add +1.2% accuracy (Section 5.3) but require validation

- Failure signatures:
  - **Coverage collapse**: All agents converge to same wrong answer (Figure 4 "all wrong" category grows in later rounds)
  - **Premature termination**: Judge stops at round 1 → worse accuracy
  - **Tool failures**: Code timeout (>60s) returns error; agents must recover or fail
  - **Selection failures**: Correct answer in candidate set but majority vote selects wrong one (coverage 65% vs accuracy 34% gap on HLE)

- First 3 experiments:
  1. **Baseline validation**: Run TUMIX with just 3 agents (CoT, Code, Search) on 50 HLE questions. Compare coverage and accuracy across rounds 1-5 to reproduce Figure 3 patterns.
  2. **Termination ablation**: Compare Term_LLM vs. fixed 5-round vs. Term_Rule on GPQA. Verify that LLM-judge termination reduces cost without accuracy loss (targeting ~50% cost reduction).
  3. **Diversity stress test**: Replace 15 diverse agents with 15 copies of single best agent. Measure coverage drop on AIME problems to quantify diversity contribution.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the final answer selection mechanism be improved to bridge the gap between high candidate coverage and final accuracy?
- Basis: [explicit] The Introduction states that "accuracy plateaus at about 34% because LLMs struggle to identify the correct answer among noisy candidates" despite achieving a coverage of ≥65%.
- Why unresolved: Current methods like majority voting fail to effectively distinguish the single correct answer from the diverse, noisy set of candidates generated during test-time scaling.
- Evidence: A novel selection method that significantly increases accuracy without increasing coverage, or one that scales accuracy linearly with coverage.

### Open Question 2
- Question: Can the iterative refinement protocol be redesigned to prevent the monotonic decrease in coverage where correct answers are mistakenly discarded?
- Basis: [explicit] Section 3.2 observes that "Coverage decreases monotonically across all benchmarks" and "correct answers are mistakenly discarded during iterative refinement."
- Why unresolved: The message-passing mechanism encourages convergence (consensus), which inherently reduces diversity and causes agents to abandon correct reasoning paths.
- Evidence: A refinement strategy where coverage remains stable or increases across rounds, leading to higher peak accuracy.

### Open Question 3
- Question: Can the LLM-as-Judge termination strategy be calibrated to avoid overconfidence-induced early stopping without manual minimum round constraints?
- Basis: [explicit] Section 5.2 notes that "LLMs tend to be overconfident," which necessitates a hard-coded minimum of two refinement rounds to preserve performance.
- Why unresolved: The reliance on a "min round" heuristic indicates the judge is not inherently reliable at determining solution maturity, limiting potential cost savings.
- Evidence: A self-terminating strategy that matches the performance of fixed-round baselines without the need for external parameter tuning.

### Open Question 4
- Question: Do the optimal agent strategies generated by a strong LLM transfer effectively to weaker models or distinct architectures?
- Basis: [inferred] Section 5.3 demonstrates that LLM-generated agents improve performance, but the agents were generated by Gemini-2.5-Pro specifically for Gemini models.
- Why unresolved: It is unclear if the automatically generated "diverse" reasoning strategies are universal or overfitted to the specific reasoning patterns of the generator model.
- Evidence: Evaluation of Gemini-designed agent prompts on alternative model families (e.g., Llama, Claude) showing retained performance gains.

## Limitations

- LLM-as-Judge termination mechanism's reliability lacks ablation studies on judge accuracy or comparison with stronger oracles
- Tool execution failures (60s timeout) and recovery strategies are under-specified, potentially creating brittleness
- Answer selection process's vulnerability to "all wrong" consensus states not fully addressed, with coverage dropping from 65% to 34% on HLE between rounds 1 and 5

## Confidence

- **High Confidence**: Agent diversity benefits (5.1, Figure 5), iterative refinement dynamics (5.1, Figure 3), and cost reduction with LLM-as-Judge (5.2, Figure 7)
- **Medium Confidence**: Coverage vs accuracy tradeoff mechanisms, optimal agent count (12-15), and search backend comparisons
- **Low Confidence**: LLM-generated agent performance claims, long-term refinement dynamics beyond 5 rounds, and cross-benchmark generalization

## Next Checks

1. **Judge Reliability Test**: Implement an oracle judge on 100 held-out questions to measure true consensus accuracy vs. LLM-as-Judge predictions
2. **Coverage-Stability Analysis**: Track per-round answer entropy and correctness to identify when refinement begins harming coverage retention
3. **Agent Diversity Stress Test**: Compare 15 identical agents vs. 15 diverse agents on AIME problems, measuring coverage gap and convergence speed