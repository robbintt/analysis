---
ver: rpa2
title: Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness
  of RAG Against Misleading Retrievals
arxiv_id: '2502.16101'
source_url: https://arxiv.org/abs/2502.16101
tags:
- misleading
- documents
- retrieval
- dataset
- claim
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces RAGUARD, the first benchmark designed to
  evaluate the robustness of retrieval-augmented generation (RAG) systems against
  misleading evidence. Unlike prior datasets that use synthetic noise, RAGUARD incorporates
  naturally occurring misinformation from Reddit discussions to challenge RAG systems
  with realistic retrieval conditions.
---

# Worse than Zero-shot? A Fact-Checking Dataset for Evaluating the Robustness of RAG Against Misleading Retrievals

## Quick Facts
- **arXiv ID:** 2502.16101
- **Source URL:** https://arxiv.org/abs/2502.16101
- **Reference count:** 40
- **Primary result:** RAG systems perform worse than zero-shot baselines when exposed to misleading retrievals (accuracy drops: 31.7%-58.0%)

## Executive Summary
This paper introduces RAGUARD, the first benchmark designed to evaluate the robustness of retrieval-augmented generation (RAG) systems against misleading evidence. Unlike prior datasets that use synthetic noise, RAGUARD incorporates naturally occurring misinformation from Reddit discussions to challenge RAG systems with realistic retrieval conditions. The dataset contains 2,648 political claims from PolitiFact, each paired with documents categorized as supporting, misleading, or unrelated. Experimental results across eight LLM backbones show that all models perform worse than their zero-shot baselines when exposed to misleading retrievals, with accuracy drops ranging from 31.7% to 58.0%. Human annotators consistently outperform LLMs, demonstrating superior ability to handle noisy context. The benchmark reveals that even state-of-the-art RAG systems struggle to distinguish factual content from subtle misinformation, highlighting the need for improved robustness methods in real-world applications.

## Method Summary
RAGUARD is a fact-checking dataset containing 2,648 political claims from PolitiFact, each paired with Reddit documents categorized as supporting, misleading, or unrelated. Documents are annotated using LLM-as-annotator methodology where GPT-4 predicts claim verdicts given documents. The evaluation tests five settings: zero-context prediction, standard RAG (top-1 and top-5 retrievals), oracle retrieval with all associated documents, and oracle retrieval with only misleading documents. Eight LLM backbones are evaluated across all settings using temperature=0.1. Retrieval uses OpenAI's text-embedding-ada-002 embeddings, and prompts include two-shot examples with explicit warnings that context may be incorrect.

## Key Results
- All eight tested LLM-powered RAG systems perform worse than their zero-shot baselines when exposed to misleading retrievals
- Accuracy drops range from 31.7% to 58.0% across different models
- Retrieval Recall for misleading documents increases from 21.3% (RAG-1) to 44.8% (RAG-5), correlating with larger accuracy drops
- Human annotators outperform LLMs on 64 claims from the test set, showing better robustness to noisy context
- Models struggle to distinguish factual content from subtle misinformation like opinion-stated-as-fact and temporal misalignment

## Why This Works (Mechanism)

### Mechanism 1
Misleading retrievals cause accuracy drops below zero-shot because LLMs overweight retrieved context over internal knowledge. When LLMs receive documents in-context, they exhibit retrieval bias—treating provided information as authoritative even when explicitly cautioned otherwise. This causes them to abandon correct prior knowledge when documents contain subtle distortions (framing, omissions, temporal misalignment).

### Mechanism 2
Human robustness stems from explicit source credibility assessment and contextual framing detection. Humans naturally distinguish between opinion-stated-as-fact (e.g., "Is it normal to be taxed this much?") and verified claims. LLMs instead interpret surface-level signals (numbers, names, rhetorical patterns) as factual support without assessing source intent or temporal context.

### Mechanism 3
More retrieved documents increase exposure to misleading content, compounding degradation. RAG-5 has 44.8% misleading retrieval recall vs. RAG-1's 21.3%. Since LLMs struggle to discount misleading documents even when supporting evidence is present, additional context increases harm probability rather than providing redundancy benefits.

## Foundational Learning

- **Concept:** Retrieval-Augmented Generation (RAG)
  - Why needed here: The paper assumes understanding of how RAG combines retrieval with generation.
  - Quick check question: Can you explain why adding retrieved context would ever decrease vs. increase accuracy?

- **Concept:** Zero-shot vs. retrieval-augmented inference
  - Why needed here: The paper's core finding is that zero-shot outperforms retrieval-augmented in misleading conditions.
  - Quick check question: What does it mean when zero-shot beats RAG? What does that imply about retrieved content quality?

- **Concept:** Fact-checking as binary classification with noisy evidence
  - Why needed here: The task frames verification as true/false with supporting/misleading/unrelated documents.
  - Quick check question: How would you design a prompt that cautions against trusting retrieved context?

## Architecture Onboarding

- **Component map:** Google Search (Reddit-restricted) via GPT-4-generated keywords -> LLM-as-annotator (GPT-4 predicts claim verdict given document) -> Evaluation Settings (Zero-Context, RAG-1, RAG-5, Oracle All, Oracle Misleading) -> Baseline Models (OLMo-1B, Llama 3, Mistral, Gemini 1.5, GPT-4o, Claude 3.5, DeepSeek, o4-mini)

- **Critical path:** Extract keywords from claim → retrieve Reddit documents → label each document via LLM prediction behavior → evaluate models across all settings with temperature=0.1 → compare accuracy drops relative to zero-shot baseline

- **Design tradeoffs:**
  - LLM-guided annotation enables scale but may encode model-specific failure patterns (addressed via cross-model agreement checks: κ=0.789 with Claude, κ=0.650 with Gemini)
  - Reddit corpus provides naturalistic misinformation but introduces platform-specific bias
  - Binary verdict simplification (collapsing 6-point scale) reduces nuance but improves annotation clarity

- **Failure signatures:**
  - Confusing opinion with fact: Model interprets "less likely" as falsification of absolute claim
  - Temporal misalignment: Document from 2024 used to verify 2000s claim
  - Surface-level number matching: "300 million" document supports "300 billion" claim

- **First 3 experiments:**
  1. Reproduce baseline: Run zero-context vs. oracle-misleading on any model to confirm 30-58% drop range
  2. Test retrieval filtering: Implement credibility scoring (e.g., subreddit reputation, upvote ratio) and measure whether filtered RAG-5 outperforms unfiltered
  3. Ablate prompt warning: Remove "context is NOT ALWAYS relevant or correct" instruction to quantify its protective effect

## Open Questions the Paper Calls Out

- Can adversarial retrieval training or multi-step reasoning integration effectively mitigate the susceptibility of RAG systems to misleading evidence? The paper suggests future research should focus on these methods but doesn't test them.

- How can human evaluation protocols be scaled to provide robust statistical comparisons of human versus machine robustness in fact-checking tasks? The current study used only four annotators on a 64-instance subset, limiting generalization.

- Does the susceptibility of LLMs to misleading retrievals in political discourse generalize to other high-stakes domains like medicine or law? The dataset is restricted to political claims, leaving domain generalizability uncertain.

## Limitations

- The use of LLM-as-annotator for document labeling may encode model-specific failure patterns, though cross-model agreement provides some validation
- The Reddit corpus introduces platform-specific bias toward opinion-heavy discourse that may not generalize to other information sources
- The binary verdict simplification loses nuance from PolitiFact's six-point scale, potentially oversimplifying complex claims

## Confidence

- **High confidence:** Empirical finding that all tested RAG systems perform worse than zero-shot baselines when exposed to misleading retrievals (accuracy drops: 31.7%-58.0%)
- **Medium confidence:** Mechanism explanation that LLMs overweight retrieved context due to retrieval bias
- **Low confidence:** Human study comparison due to small sample size (n=4 on 64 instances)

## Next Checks

1. Run the document labeling pipeline with a second LLM annotator (e.g., Claude) on a random 10% sample to measure inter-annotator agreement beyond reported κ statistics.

2. Implement a timestamp-filtering condition where only documents from the same decade as the claim are used, then measure whether accuracy degradation decreases compared to unfiltered RAG.

3. Create a variant that removes the explicit warning about context unreliability, run zero-shot vs. RAG comparisons, and measure the exact accuracy difference attributable to this prompt engineering.