---
ver: rpa2
title: Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated
  Rationale Enhancements
arxiv_id: '2508.07598'
source_url: https://arxiv.org/abs/2508.07598
tags:
- event
- keycp
- trigger
- text
- prompting
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of one-shot event detection
  using large language models (LLMs), where LLMs struggle with accurate trigger identification
  and over-interpretation. The authors propose KeyCP++, a keyword-centric chain-of-thought
  prompting approach that enhances conventional in-context learning (ICL) by automatically
  generating rationales for demonstration examples.
---

# Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements

## Quick Facts
- **arXiv ID**: 2508.07598
- **Source URL**: https://arxiv.org/abs/2508.07598
- **Reference count**: 40
- **Key outcome**: Proposes KeyCP++, a keyword-centric chain-of-thought prompting approach that achieves up to 37% relative improvement over vanilla prompting and 12.8% over previous SOTA methods in one-shot event detection settings.

## Executive Summary
This paper addresses the challenge of one-shot event detection using large language models (LLMs), where LLMs struggle with accurate trigger identification and over-interpretation. The authors propose KeyCP++, a keyword-centric chain-of-thought prompting approach that enhances conventional in-context learning (ICL) by automatically generating rationales for demonstration examples. KeyCP++ incorporates exemplary triggers (keywords) into prompts and employs a proposal-judgment workflow where LLMs propose candidate triggers and justify their relevance to event definitions. Extensive experiments on ACE2005 and WikiEvents datasets demonstrate that KeyCP++ significantly outperforms prior ICL methods and supervised fine-tuning approaches.

## Method Summary
KeyCP++ is a keyword-centric chain-of-thought prompting approach for one-shot event detection that enhances conventional in-context learning by automatically generating rationales for demonstration examples. The method operates entirely within the ICL paradigm without requiring gradient updates. It first generates keywords from event definitions via GPT-3.5 (5 repetitions, majority vote >3), then performs candidate probing on training examples to generate trigger proposals (zero-shot detection, 5 repetitions). Negative examples are sampled with probability proportional to the number of candidates they contain, and judgments are generated comparing the gold trigger to alternatives. The approach queries one event type per forward pass and parses triggers via pattern-matching.

## Key Results
- KeyCP++ achieves up to 37% relative improvement over vanilla ICL and 12.8% over previous SOTA methods in one-shot settings
- Outperforms supervised fine-tuning approaches across multiple LLMs (LLaMA2-13B, Mistral-7B, GPT3.5, DeepSeek-V3)
- Ablation studies show keyword detection contributes 22.3% F1 improvement, proposal-judgment contributes 10.3% F1 improvement, and negative sampling contributes 7.0% F1 improvement

## Why This Works (Mechanism)

### Mechanism 1: Keyword-Centric Trigger Anchoring
- **Claim**: Incorporating exemplary triggers (keywords) into prompts provides concrete anchor points that constrain LLM trigger profiling and reduce false positives from over-interpretation.
- **Mechanism**: Keywords derived from event definitions are inserted into the event description and demonstration examples. String-matching detects keyword presence in query instances, forcing LLMs to first consider these anchor words before proposing alternatives.
- **Core assumption**: LLMs possess latent event knowledge but lack precise trigger boundaries; explicit exemplars calibrate this knowledge without requiring gradient updates.
- **Evidence anchors**: [abstract] states keywords are used as anchors to "simply trigger profiling"; [Section 4.4, Table 4] shows removing keyword detection drops F1 from 38.6 to 16.5 with LLaMA2-13B.

### Mechanism 2: Proposal-Judgment Rationale Chain
- **Claim**: A two-stage generate-then-evaluate workflow produces richer rationales than direct explanation prompts, improving detection rule learning through explicit candidate discrimination.
- **Mechanism**: Candidate probing generates trigger proposals via zero-shot detection. These proposals (plus detected keywords) form a candidate set. The LLM then generates judgments explaining why the gold trigger is appropriate and why other candidates fail—this comparison-based reasoning is inserted as demonstration rationale.
- **Core assumption**: Direct rationale generation produces superficial definition restatements; requiring explicit comparison against alternatives forces deeper semantic reasoning.
- **Evidence anchors**: [Section 3.3] notes LLMs tend to "reproduce surface-level definitions without meaningful interpretation" when directly explaining examples; [Section 5.2, Table 4] shows removing judgment drops F1 from 45.1 to 34.8.

### Mechanism 3: Candidate-Density Weighted Negative Sampling
- **Claim**: Prioritizing negative examples with more trigger candidates increases demonstration informativeness by presenting harder discrimination cases.
- **Mechanism**: Negative examples are sampled with probability proportional to `e^|C_t(x)|/τ` where |C_t(x)| is the candidate count. Examples with multiple candidates receive higher sampling probability, creating demonstrations that show how to reject plausible-but-incorrect triggers.
- **Core assumption**: Examples containing near-miss candidates provide more learning signal than examples with no trigger-like words.
- **Evidence anchors**: [Section 3.3, Equation 2] shows the sampling distribution explicitly weights by candidate count; [Section 5.2, Table 4] demonstrates removing negative sampling drops F1 from 45.1 to 38.3.

## Foundational Learning

- **In-Context Learning (ICL)**
  - Why needed here: KeyCP++ operates entirely within ICL paradigm—no gradient updates. Understanding ICL constraints (context length, demonstration sensitivity) is prerequisite.
  - Quick check question: Can you explain why ICL performance varies with demonstration ordering and format?

- **Chain-of-Thought (CoT) Prompting**
  - Why needed here: KeyCP++ extends CoT to event detection by inserting reasoning chains (rationales) before answers. The proposal-judgment structure is a specialized CoT variant.
  - Quick check question: What distinguishes CoT from standard few-shot prompting, and why does CoT require rationale annotations?

- **Event Detection Task Formulation**
  - Why needed here: The task requires identifying trigger words (not just event presence) that signify predefined event types. Misunderstanding triggers vs. events leads to incorrect evaluation.
  - Quick check question: In the sentence "The bomb killed three civilians," what is the trigger for a Life.Die event, and how does it differ from event arguments?

## Architecture Onboarding

- **Component map**: Keyword Generation Module -> Candidate Probing Module -> Negative Sampling Module -> Judgment Generation Module -> KeyCP++ Prompt Assembler -> Answer Parser

- **Critical path**: Keywords generated once → Candidate probing for all training examples → Negative sampling per query type → Judgment generation for selected examples → Prompt assembled → LLM inference → Parse trigger

- **Design tradeoffs**:
  - Keyword quality vs. robustness: Poor keywords reduce anchoring effectiveness, but method shows tolerance through proposal mechanism
  - Inference cost vs. coverage: Querying one event type per forward pass improves focus but increases total API calls
  - Negative sample count vs. conservatism: S=5 optimal; S>5 causes over-conservative predictions

- **Failure signatures**:
  - Keyword over-reliance: KeyCP shows increased keyword false positives; KeyCP++ mitigates but doesn't eliminate
  - Semantic drift: Case 3 (Table 5) shows all methods incorrectly identifying "divorce" as Life.Marry trigger
  - Proposal noise: Low-quality proposals dilute judgment rationales; mitigated by 5-repetition voting

- **First 3 experiments**:
  1. Run vanilla prompting and KeyCP on ACE05-E with LLaMA2-13B. Expect ~30-point F1 gap. Verify keyword detection is dominant contributor.
  2. Run KeyCP++ with and without judgment rationales on held-out split. Expect ~10-point F1 drop, primarily from precision loss.
  3. Compare uniform vs. candidate-density sampling. Track which negative examples are selected and their candidate counts.

## Open Questions the Paper Calls Out
None

## Limitations
- The approach relies on LLM-generated demonstrations and rationales, which may not generalize to domains without clear event definitions or where zero-shot detection is unreliable
- Performance depends critically on the quality of candidate probing and judgment generation modules
- The method doesn't address scalability concerns for real-world applications with thousands of event types
- Assumes access to event definitions and zero-shot detection capability, which may not be available in all domains

## Confidence
- **High Confidence**: Experimental results demonstrating KeyCP++'s superiority over vanilla ICL and KeyCP are well-supported by data with consistent improvements across multiple LLMs and datasets
- **Medium Confidence**: Mechanism explanations are logically coherent and supported by ablation results, but some assumptions about why each component works remain inferred rather than directly proven
- **Low Confidence**: Generalizability to domains without clear event definitions or where zero-shot detection is unreliable

## Next Checks
1. Apply KeyCP++ to a different event detection domain (e.g., biomedical event extraction from literature) without retraining keyword generation or candidate probing modules
2. Intentionally generate low-quality keywords and measure how KeyCP++ performance degrades compared to KeyCP and vanilla ICL
3. Modify inference procedure to query multiple event types in a single forward pass and measure performance degradation or efficiency gains