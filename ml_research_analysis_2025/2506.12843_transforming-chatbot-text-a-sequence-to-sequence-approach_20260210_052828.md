---
ver: rpa2
title: 'Transforming Chatbot Text: A Sequence-to-Sequence Approach'
arxiv_id: '2506.12843'
source_url: https://arxiv.org/abs/2506.12843
tags:
- text
- seq2seq
- human
- your
- gpt-generated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a sequence-to-sequence approach to transform
  GPT-generated text into a more human-like form using T5-small and BART models. The
  method fine-tunes these models to modify GPT output by introducing more natural
  sentence structures, transitions, and linguistic diversity.
---

# Transforming Chatbot Text: A Sequence-to-Sequence Approach

## Quick Facts
- arXiv ID: 2506.12843
- Source URL: https://arxiv.org/abs/2506.12843
- Reference count: 40
- Primary result: Seq2Seq models fine-tuned on paired GPT-human text can transform AI-generated text to evade detection, but transformed text remains distinguishable with appropriate training.

## Executive Summary
This paper presents a method to transform GPT-generated text into more human-like forms using sequence-to-sequence models T5-small and BART. The approach fine-tunes these models on paired GPT-human datasets with a "humanize" prefix to modify linguistic features like sentence structure and transitions. Experiments show that while these transformations significantly degrade the performance of classifiers trained on standard GPT data (accuracy drops of 7.7%-19.3%), classifiers retrained on the transformed data regain high accuracy, indicating the transformed text remains distinguishable from human text despite evading initial detection.

## Method Summary
The paper fine-tunes T5-small and BART models to transform GPT-generated text into human-like text using paired datasets with a "humanize" prefix instruction. The fine-tuning uses AdamW optimizer, linear learning rate scheduling with warm-up, and FP16 precision with early stopping. Classification experiments employ six models (LR, RF, XGB, MLP, DNN, LSTM) across three embedding types (Word2Vec, GloVe, BERT) to evaluate detection evasion. The study tests both attack effectiveness (degrading baseline classifier accuracy) and defense effectiveness (restoring accuracy through retraining on transformed data).

## Key Results
- Classification accuracy drops significantly when tested on Seq2Seq-transformed GPT text, with LSTM models showing up to 19.3% decrease
- BERT embeddings prove most robust to transformation attacks compared to Word2Vec and GloVe
- Retraining classifiers on transformed data restores accuracy to baseline levels, indicating transformed text remains distinguishable
- T5-small transformations generally cause larger accuracy drops than BART transformations across most classifier and embedding combinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Seq2Seq fine-tuning can degrade classifier performance on AI-generated text by altering detectable surface features.
- Mechanism: A pretrained encoder-decoder (T5-small or BART) is fine-tuned on paired (GPT-generated, human-written) examples with a "humanize" prefix, learning to shift sentence structure, transitions, and linguistic diversity toward the human training distribution.
- Core assumption: The classifier has learned features from the original GPT distribution that do not fully transfer to the transformed output.
- Evidence anchors:
  - [abstract] "We experiment with the Seq2Seq models T5-small and BART which serve to modify GPT-generated sentences to include linguistic, structural, and semantic components that may be more typical of human-authored text."
  - [section 4] "Our Seq2Seq models are fine-tuned in a supervised manner using paired datasets. Each input is a GPT-generated sentence prefixed with a task-specific instruction (i.e., 'humanize') and the corresponding target is the human-written version."
- Break condition: If classifiers are trained or fine-tuned on the transformed distribution, evasion effectiveness collapses.

### Mechanism 2
- Claim: Classifier accuracy drops under distribution shift when test inputs are transformed by an unseen Seq2Seq model.
- Mechanism: Baseline classifiers learn decision boundaries over embeddings using features implicit in GPT text. T5/BART transformation alters those features (e.g., verbosity, phrasing), causing systematic misclassification.
- Core assumption: The learned decision boundaries generalize poorly to the shifted feature distribution.
- Evidence anchors:
  - [abstract] "Classification models trained to detect GPT-generated text show significant drops in accuracy when tested on our transformed outputs—up to 19.3% accuracy decrease for LSTM with T5-small."
  - [section 5.2] "We observe that for Word2Vec embedding there is a consistent decrease in accuracy, with the minimum decrease being for the XGB model and T5-small transformation which yields a change of 16.2%... the maximum decrease in accuracy occurs for the LSTM model and T5-small transformation... 19.3%."
- Break condition: If embedding type or model architecture is more robust to the transformation, evasion effectiveness is reduced.

### Mechanism 3
- Claim: Retraining on transformed data restores classification accuracy, indicating transformed text remains distinguishable from human text.
- Mechanism: When classifiers are retrained on Seq2Seq-transformed GPT examples vs human text, they learn new discriminative features. That accuracy returns to baseline levels implies the transformation creates a distinct, learnable style rather than indistinguishable human-like text.
- Core assumption: The retraining data is representative and the model capacity is sufficient.
- Evidence anchors:
  - [abstract] "when retrained on transformed data, models regain high accuracy, indicating the transformations are not fully human-like but are effective at evading detection."
  - [section 5.3] "The accuracies in this case were essentially the same as those obtained for the baseline problem of distinguishing GPT-generated text from human-generated text."
- Break condition: If transformation quality improves (e.g., via adversarial fine-tuning loops), retraining may not fully recover performance.

## Foundational Learning

- Concept: Word embeddings (Word2Vec, GloVe, BERT)
  - Why needed here: The paper evaluates classifiers across three embedding types; understanding their differences (static vs contextual, co-occurrence vs prediction-based) is essential for interpreting why BERT embeddings proved more robust.
  - Quick check question: Given the same word appearing in two different sentences, which embedding technique would assign different vectors?

- Concept: Encoder-decoder (Seq2Seq) architectures
  - Why needed here: T5-small and BART are encoder-decoder transformers; grasping how they encode input sequences and autoregressively decode output clarifies how style transfer is achieved.
  - Quick check question: In a Seq2Seq model, during inference the decoder generates tokens one at a time—what information conditions each step's prediction?

- Concept: Distribution shift and adversarial robustness
  - Why needed here: The core phenomenon (accuracy drop then recovery) is a case of distribution shift followed by adversarial training; recognizing this framing helps generalize the findings.
  - Quick check question: If a classifier trained on data from distribution P is tested on data from Q, what conditions would cause accuracy to degrade?

## Architecture Onboarding

- Component map: GPT text → tokenizer → fine-tuned T5-small/BART (encoder-decoder) → decoded "humanized" text → embedding model (Word2Vec/GloVe/BERT) → classifier (LR/RF/XGB/MLP/DNN/LSTM) → prediction

- Critical path:
  1. Fine-tune T5-small and BART on paired (GPT, human) data with "humanize" prefix
  2. Generate transformed datasets by running GPT data through each Seq2Seq model
  3. Train classifiers on original data; evaluate on transformed data (attack phase)
  4. Retrain classifiers on transformed data; evaluate again (defense phase)

- Design tradeoffs:
  - T5-small vs BART: T5 caused larger accuracy drops for most models; BART slightly more robust to detection
  - Embedding choice: BERT embeddings most robust to transformation; Word2Vec/GloVe more vulnerable
  - Sequence length: 512 chosen to balance coverage and overfitting risk

- Failure signatures:
  - Overfitting on short sequences (mitigated by max_length=512 and early stopping)
  - LSTM underperforming with BERT embeddings (0.8495 baseline accuracy vs 0.9840 for DNN)
  - Transformed text detectable after retraining—transformation not truly human-like

- First 3 experiments:
  1. Replicate baseline: Train DNN with BERT embeddings on human vs GPT data; confirm ~98% accuracy.
  2. Attack evaluation: Apply fine-tuned T5-small to held-out GPT samples; test baseline DNN on transformed text—expect ~9–15% accuracy drop.
  3. Defense evaluation: Retrain DNN on T5-transformed vs human data; verify accuracy returns to near-baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can larger Seq2Seq models (e.g., Flan-T5, Mistral-7B, LLaMA 2-13B) produce transformations that evade even retrained classifiers?
- Basis in paper: [explicit] Section 6 states: "other Seq2Seq models, such as Flan-T5, Mistral-7B, or LLaMA 2-13B, might prove more effective" at improving human-like qualities.
- Why unresolved: Only T5-small and BART were tested; both produced transformations detectable by retrained classifiers.
- What evidence would resolve it: Experiments showing Seq2Seq-transformed text from larger models that maintains low detection accuracy even after classifier retraining.

### Open Question 2
- Question: Would adversarial fine-tuning loops with classifier feedback produce transformations that more effectively evade detection?
- Basis in paper: [explicit] Section 6 proposes "adversarial fine-tuning loops where feedback from classifiers is used to guide the text transformation process" as future work.
- Why unresolved: Current approach uses one-pass transformation without iterative refinement based on classifier feedback.
- What evidence would resolve it: Implementation of feedback-loop training showing transformed text achieving sustained low detection rates against progressively retrained classifiers.

### Open Question 3
- Question: Does the Seq2Seq transformation approach generalize to AI-generated text from other LLMs (Claude, PaLM, LLaMA)?
- Basis in paper: [inferred] The paper only tests GPT-generated text from a specific dataset; the introduction mentions multiple LLMs but experiments are limited to ChatGPT output.
- Why unresolved: Different LLMs may have distinct stylistic signatures that require different transformation strategies.
- What evidence would resolve it: Experiments applying T5-small and BART transformations to text generated by Claude, PaLM, and LLaMA, measuring detection accuracy drops across each source.

## Limitations

- Dataset Transparency: Relies on a paired human-GPT dataset with unspecified prompt templates and generation methods, limiting reproducibility.
- Hyperparameter Opacity: Critical training parameters for both Seq2Seq models and classifiers are unspecified, preventing exact replication.
- Limited Model Diversity: Only two small Seq2Seq architectures are evaluated, without comparison to larger models or alternative transformation approaches.

## Confidence

**High Confidence**: The core finding that Seq2Seq fine-tuning can degrade classifier performance on AI-generated text is well-supported by the experimental results with clear accuracy drops and recovery patterns.

**Medium Confidence**: The claim that transformed text remains distinguishable from human text after retraining is supported but limited by the narrow scope of tested models and lack of comparison to truly human-written text across diverse domains.

**Low Confidence**: The paper's assertion that this represents a significant advancement lacks comparative analysis against other transformation methods or human evaluation of transformation quality.

## Next Checks

1. **Ablation on Hyperparameters**: Systematically vary learning rates, batch sizes, and sequence lengths during Seq2Seq fine-tuning to determine which factors most influence transformation effectiveness and classifier evasion.

2. **Cross-Dataset Generalization**: Test the same transformation and classification pipeline on a different human-GPT dataset to verify whether observed accuracy drops and recovery patterns hold across domains.

3. **Human Evaluation of Transformation Quality**: Conduct a blind study where human annotators rate transformed text for human-likeness and detectability, comparing T5-small vs BART outputs and against ground-truth human text.