---
ver: rpa2
title: 'Recomposer: Event-roll-guided generative audio editing'
arxiv_id: '2509.05256'
source_url: https://arxiv.org/abs/2509.05256
tags:
- audio
- sound
- target
- input
- events
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Recomposer, a generative audio editing system
  that enables precise deletion, insertion, and enhancement of individual sound events
  within complex audio scenes. The method leverages a transformer-based encoder-decoder
  model trained on synthetic audio examples, where isolated sound events are mixed
  with real-world background scenes under various conditions.
---

# Recomposer: Event-roll-guided generative audio editing

## Quick Facts
- arXiv ID: 2509.05256
- Source URL: https://arxiv.org/abs/2509.05256
- Reference count: 35
- This paper introduces Recomposer, a generative audio editing system that enables precise deletion, insertion, and enhancement of individual sound events within complex audio scenes.

## Executive Summary
This paper presents Recomposer, a transformer-based generative audio editing system that can modify specific sound events within complex audio scenes based on textual instructions and timing information. The system is trained on synthetic mixtures of isolated sound events with real-world background scenes, learning to perform precise edits like deleting a dog bark or inserting a siren while preserving the rest of the scene. Evaluation shows consistent improvements over unprocessed input across deletion, insertion, and enhancement operations, with ablation studies confirming the importance of both timing and action conditioning.

## Method Summary
Recomposer uses a transformer-based encoder-decoder architecture that operates on SoundStream-encoded audio tokens. The system takes as input an audio scene, textual edit instructions (e.g., "delete dog bark"), and an event roll (binary mask indicating when the target event occurs). A frozen Sentence-T5 encodes the text, which is then conditioned across time using the event roll via inner product. The core model is an RQ-Transformer with separate temporal and depth decoders that generate SoundStream RVQ tokens. The system is trained on synthetic mixtures where isolated sound events from Freesound are mixed with AudioSet backgrounds under various target-to-background ratios, with ground truth pairs created for deletion (present→absent) and insertion (absent→present) operations.

## Key Results
- The model consistently improves over unprocessed input across all three operations (delete, insert, enhance)
- Largest spectral distortion reduction of 2.3 for deletion operations
- Substantial classifier divergence reduction for insertion operations
- Ablation experiments show timing conditioning is crucial for precise edits while class labels are most important for insertions

## Why This Works (Mechanism)

### Mechanism 1: Synthetic Contrastive Pairing
The model learns to manipulate specific sound events by training on synthetic (input, output) pairs where the only difference is the presence, absence, or amplitude of the target event. The system creates training examples by mixing isolated "target" sounds with dense "background" scenes. By feeding the model an input with a target present (for Deletion) or absent (for Insertion) and training it to predict the inverse state, the model learns a causal mapping between the edit instruction and the required spectral changes.

### Mechanism 2: Time-Aligned Instruction Fusion
Precise editing relies on fusing the semantic edit instruction with a high-resolution binary time mask (activity roll), ensuring the transformer attends to specific frames. Text instructions are encoded into a single vector and then projected across time via an inner product with a binary matrix indicating when the event occurs. This conditions the encoder-decoder to apply the operation only at specific time steps.

### Mechanism 3: Residual Hierarchical Generation (RQ-Transformer)
High-fidelity reconstruction is achieved by decoupling temporal coherence from acoustic detail generation via a depth-aware autoregressive loop. Instead of generating raw audio directly, the decoder uses a "Temporal Transformer" to define frame-level structure, followed by a "Depth Transformer" to generate the stack of Residual Vector Quantization tokens for that frame.

## Foundational Learning

- **Concept: Residual Vector Quantization (RVQ)**
  - Why needed here: The model does not output waveforms directly; it outputs discrete tokens from a SoundStream codec. Understanding that audio is represented as a hierarchy of codebooks is essential to debugging why an output might sound metallic or distorted vs. temporally jumbled.
  - Quick check question: If the "Depth Transformer" fails but the "Temporal Transformer" succeeds, what artifact would you expect in the audio?

- **Concept: Sound Event Detection (SED) / Activity Rolls**
  - Why needed here: The system relies on a pre-existing "event roll" (time-frequency map of events) to function. The paper explicitly disclaims solving the detection part, but the conditional generation depends entirely on this input.
  - Quick check question: Does the Recomposer model generate the event roll, or does it consume it as a control signal? (Answer: It consumes it).

- **Concept: Target-to-Background Ratio (TBR)**
  - Why needed here: The model's ability to "Enhance" is explicitly trained on specific TBR ranges (-30dB to 0dB). Evaluating performance requires comparing the input TBR to the model's training distribution to predict if it will hallucinate detail or successfully separate the source.
  - Quick check question: If an input event is at -40 dB but the model was trained on Enhancement for -30 dB to 0 dB, will it likely enhance the sound or leave it untouched?

## Architecture Onboarding

- **Component map:** SoundStream Encoder -> RQ-Transformer (Encoder-Decoder) -> SoundStream Decoder
- **Critical path:** The `Activity Roll` is the single point of failure for *localization*. If the roll is empty or misaligned, the `Conditioning Matrix` is zeroed or offset, and the Transformer (which cross-attends to this) will effectively perform a "no-op" or modify the wrong segment.
- **Design tradeoffs:** Synthetic vs. Real Data: Training on synthetic mixes guarantees perfect ground truth but risks a "sim-to-real" gap. Autoregressive vs. Diffusion: The choice of an RQ-Transformer allows precise temporal alignment but may be slower to infer than diffusion models.
- **Failure signatures:** "No-op" behavior: Model outputs the input exactly. Hallucinated Insertions: Model inserts sounds at random times. Wrong Class Insertion: Model inserts a "Cat" instead of a "Dog".
- **First 3 experiments:**
  1. Baseline Codec Pass: Pass a pure speech + background mix through the *frozen* SoundStream encoder and decoder to measure the distortion floor.
  2. Timing Ablation Validation: Zero out the activity roll for a known "Delete Dog" task. Verify if the dog remains in the output.
  3. Enhancement TBR Sweep: Feed the "Enhance" model inputs with targets at -5dB, -15dB, and -25dB. Plot MSD improvement vs. TBR.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can generative audio editing models be conditioned to control specific attributes of generated events, such as loudness or timbre? The authors note that generated output events are always at a fixed high TBR used in training, suggesting the need for mechanisms to specify generated event properties.

- **Open Question 2:** Can the Recomposer framework be extended to support rich, free-form text descriptions for sound transformation? The paper notes the current vocabulary is "strictly limited to the subset of fixed AudioSet class labels" and suggests training with more diverse and detailed text descriptions.

- **Open Question 3:** Can an objective metric be designed to isolate the perceptual success of a specific edit while ignoring irrelevant processing artifacts? The authors observe that current metrics were sensitive to "distortion intrinsic to model processing that did not seem perceptually significant" and suggest designing metrics to measure this "more directly."

## Limitations

- The synthetic training data creates a significant sim-to-real gap, potentially causing the model to fail on natural acoustic complexities like overlapping harmonics, reverberation, or environmental noise.
- The system's reliance on pre-computed event rolls makes it brittle to upstream detection errors, as inaccuracies propagate directly to editing quality.
- The model's generalization capability to real-world scenarios beyond synthetic test sets is uncertain, with no qualitative examples or user studies demonstrating robustness to challenging acoustic conditions.

## Confidence

- **High Confidence:** The ablation study results showing timing conditioning is essential for precise edits.
- **Medium Confidence:** The claim that the model "consistently improves over unprocessed input" across all three operations, though improvements are modest.
- **Low Confidence:** The generalization capability to real-world scenarios beyond the synthetic test set.

## Next Checks

1. **Real-world Generalization Test:** Evaluate the model on a held-out set of real-world recordings (not synthetically mixed) containing the target sound events to quantify the sim-to-real gap.

2. **End-to-End Pipeline Robustness:** Test the complete system (detection + editing) on audio where the event roll is generated by an independent SED model to measure how detection errors propagate to editing quality.

3. **Perceptual Validation Study:** Conduct a human listening test comparing the model's outputs against unprocessed input and baseline editing methods to assess whether spectral improvements translate to perceptible enhancements.