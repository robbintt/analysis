---
ver: rpa2
title: 'The Price of Linear Time: Error Analysis of Structured Kernel Interpolation'
arxiv_id: '2502.00298'
source_url: https://arxiv.org/abs/2502.00298
tags:
- error
- kernel
- interpolation
- points
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Structured Kernel Interpolation (SKI) accelerates Gaussian Process
  inference by approximating kernel matrices via interpolation at inducing points,
  achieving linear complexity. This paper provides the first rigorous error analysis
  for SKI, deriving bounds for the SKI kernel, Gram matrix, and cross-kernel matrix
  errors.
---

# The Price of Linear Time: Error Analysis of Structured Kernel Interpolation

## Quick Facts
- **arXiv ID:** 2502.00298
- **Source URL:** https://arxiv.org/abs/2502.00298
- **Reference count:** 40
- **Key outcome:** First rigorous error analysis for SKI shows that for d ≤ 3, linear-time GP inference with any fixed error tolerance is possible for large n, while for d > 3, error must increase with sample size to maintain linear time.

## Executive Summary
This paper provides the first rigorous error analysis for Structured Kernel Interpolation (SKI), a method that accelerates Gaussian Process inference by approximating kernel matrices through interpolation at inducing points. The analysis derives bounds for kernel approximation error, Gram matrix spectral norm error, and cross-kernel matrix error. A critical finding is that the number of inducing points must grow as n^(d/3) to control error, creating two distinct dimensionality regimes: for d ≤ 3, linear-time inference with any fixed error tolerance is achievable for sufficiently large sample sizes, while for d > 3, the error must increase with sample size to maintain linear computational complexity. The paper also analyzes the impact of SKI on hyperparameter estimation and posterior inference.

## Method Summary
The method uses Structured Kernel Interpolation with cubic convolutional interpolation on a regular grid of inducing points. The approximation constructs sparse weight matrices that map training points to inducing points, enabling fast matrix-vector products through structured kernel matrices. The analysis assumes three-times continuously differentiable kernels and derives error bounds for the SKI kernel, Gram matrix, and cross-kernel matrix. For hyperparameter estimation, the paper analyzes gradient ascent on the SKI log-likelihood and shows convergence to a neighborhood of a stationary point of the true log-likelihood.

## Key Results
- SKI kernel approximation error decays as O(m^(−3/d)) with m inducing points using cubic convolutional interpolation
- Gram matrix spectral norm error scales as O(n·m^(−3/d)), creating distinct computational regimes at d = 3
- For d ≤ 3, linear-time inference with any fixed error tolerance is achievable for large sample sizes
- For d > 3, error must increase with sample size to maintain linear computational complexity
- Gradient ascent on SKI log-likelihood converges to a neighborhood of a stationary point of the true log-likelihood

## Why This Works (Mechanism)

### Mechanism 1: Cubic Interpolation Error Rate
The SKI kernel approximation error decays as O(m^(−3/d)) because cubic convolutional interpolation achieves O(h³) local error per dimension. With d dimensions and grid spacing h ~ m^(−1/d), the tensor-product structure yields O(h³)^d = O(m^(−3/d)). This requires the true kernel to be three times continuously differentiable.

### Mechanism 2: Dimensionality-Dependent Computational Regimes
Gram matrix spectral norm error ‖K − K̃‖₂ scales as O(n·c^(2d)·m^(−3/d)). Setting m ~ n^(d/3) yields constant error. For d ≤ 3, this maintains O(n + m log m) = O(n) complexity, while for d > 3, m grows superlinearly, forcing error to increase with n for linear time.

### Mechanism 3: Inexact Gradient Ascent Convergence
Gradient ascent on SKI log-likelihood converges to a neighborhood of a stationary point because the score function error is bounded by terms involving Gram matrix error. Using Stonyakin et al. (2023) results for non-convex optimization with additive gradient noise, the algorithm achieves O(1/K) convergence rate to a neighborhood whose size depends on the error tolerance.

## Foundational Learning

- **Concept:** Spectral norm vs. elementwise matrix norms
  - **Why needed here:** The error analysis converts spectral norm bounds through infinity norm (row sums) using symmetry. Understanding this conversion is essential for interpreting the bounds.
  - **Quick check question:** For a symmetric matrix A, what is the relationship between ‖A‖₂ and ‖A‖_∞?

- **Concept:** Tensor-product interpolation and curse of dimensionality
  - **Why needed here:** The c^(2d) exponential factor and m^(−3/d) decay emerge from tensor-product structure. Understanding why error compounds multiplicatively across dimensions clarifies the d ≤ 3 vs. d > 3 regime split.
  - **Quick check question:** If 1D cubic interpolation has error O(h³), why does d-dimensional tensor-product interpolation have error O(h³)·c^d rather than O(h^(3d))?

- **Concept:** Inexact gradient descent convergence theory
  - **Why needed here:** Hyperparameter optimization guarantees rely on Stonyakin et al. (2023) results for non-convex optimization with additive gradient noise. Understanding the O(1/K) rate and neighborhood size ε_g²/(2μ) is critical for interpreting practical convergence.
  - **Quick check question:** In inexact gradient ascent with error ε_g, does the algorithm converge to a stationary point or to a neighborhood? What determines the neighborhood radius?

## Architecture Onboarding

- **Component map:** Data points -> Weight matrix W -> Inducing points grid -> Kernel matrix K_U -> SKI approximation K̃

- **Critical path:**
  1. Construct inducing point grid (depends on domain [−D, D]^d and desired m)
  2. Compute W matrices for train and test points (sparse, O(nL) storage)
  3. Form K_U using chosen kernel (structured, exploit FFT if applicable)
  4. Optimize hyperparameters via SKI log-likelihood gradient ascent
  5. Compute posterior mean/covariance via iterative linear solvers (CG) on K̃ + σ²I

- **Design tradeoffs:**
  - **m vs. accuracy:** Larger m reduces error O(m^(−3/d)) but increases memory O(m) and compute O(m log m)
  - **Dimension d vs. feasibility:** For d > 3, linear-time guarantee requires accepting growing error; consider alternative interpolations (permutohedral lattice, sparse grids)
  - **Interpolation degree L:** Higher L improves approximation but increases sparsity pattern complexity

- **Failure signatures:**
  - **Gram matrix not positive definite:** Inducing points too sparse relative to kernel lengthscale; increase m or reduce domain size
  - **Posterior variance explodes:** Spectral norm error too large relative to σ²; check m ≥ (n/ε · const)^(d/3)
  - **Hyperparameter divergence:** Score function error too large; ensure ‖y‖ is normalized and m grows appropriately with n
  - **Slow convergence in CG:** K_U ill-conditioned; kernel lengthscale may be too small for grid spacing

- **First 3 experiments:**
  1. **Empirical error validation:** On synthetic 1D/2D data, vary m and measure ‖K − K̃‖₂ / ‖K‖₂. Verify O(n m^(−3/d)) scaling matches theory. Plot error vs. m for fixed n.
  2. **Dimension regime probe:** For d = 2, 3, 4, 5, fix target error ε and measure runtime scaling with n. Confirm linear-time holds for d ≤ 3 but degrades for d > 3.
  3. **Hyperparameter recovery:** Generate data from GP with known θ*. Run SKI gradient ascent, measure distance to θ* as function of m. Verify convergence neighborhood shrinks as m increases.

## Open Questions the Paper Calls Out

### Open Question 1
Can the error bounds for the SKI posterior covariance matrix be tightened to guarantee decay with the number of inducing points m in dimensions d ≥ 3? The current proof relies on standard techniques resulting in error that only decays with m at rate m^(3/d−1) (positive only if d < 3).

### Open Question 2
How does the choice of interpolation degree L (e.g., higher-order polynomials or Lagrange interpolation) jointly affect the error bounds and the required number of inducing points m? The current analysis restricts to convolutional cubic interpolation (L=4), fixing the interpolation scheme while varying inducing points.

### Open Question 3
Can the analysis of gradient ascent on the SKI log-likelihood be strengthened to guarantee convergence of the sequence (e.g., last iterate or average) rather than just the existence of a single "good" iterate? The current theoretical guarantee is limited by properties of the non-convex inexact gradient ascent lemma used.

### Open Question 4
What are the theoretical error bounds for SKI when using irregularly placed inducing points or non-stationary kernel functions? The current analysis depends on regular grid spacing for interpolation error proofs and computational complexity guarantees.

## Limitations

- The analysis assumes three-times continuously differentiable kernels, which may not hold for all commonly used kernels (e.g., Matérn-1.5)
- The exponential c^(2d) factor's dependence on dimensionality is treated as dimension-independent in proofs but may grow with d in practice
- The computational regime split at d = 3 may not capture all practical considerations, as other factors like kernel lengthscale and data distribution also affect performance

## Confidence

- **High confidence:** The O(m^(−3/d)) interpolation error rate and the d ≤ 3 vs. d > 3 computational regime split are mathematically rigorous and well-supported by the proofs
- **Medium confidence:** The hyperparameter estimation convergence guarantees assume well-behaved log-likelihood and compact parameter spaces, which may not hold for all kernels or initialization schemes
- **Low confidence:** The practical impact of the c^(2d) factor is unclear without empirical validation across different kernel families and dimensionalities

## Next Checks

1. **Empirical scaling verification:** Generate synthetic data for dimensions d = 1, 2, 3, 4. For each d, vary m and measure ‖K − K̃‖₂ / ‖K‖₂. Confirm the O(m^(−3/d)) scaling and identify the computational regime shift at d = 3.

2. **Non-smooth kernel testing:** Repeat the error analysis using Matérn kernels with ν < 3 (e.g., Matérn-1.5). Measure whether error rates deviate from O(m^(−3/d)) and quantify the degradation in approximation quality.

3. **Non-uniform data distribution:** Generate clustered or manifold-distributed data rather than uniform. Compare SKI performance against adaptive inducing point methods (e.g., variational sparse GPs) and measure relative accuracy/computation trade-offs.