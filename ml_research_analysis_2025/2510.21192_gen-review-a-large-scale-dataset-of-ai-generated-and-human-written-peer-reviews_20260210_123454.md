---
ver: rpa2
title: 'Gen-Review: A Large-scale Dataset of AI-Generated (and Human-written) Peer
  Reviews'
arxiv_id: '2510.21192'
source_url: https://arxiv.org/abs/2510.21192
tags:
- reviews
- review
- llms
- papers
- iclr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents Gen-Review, a large-scale dataset of over 80,000
  AI-generated peer reviews created by issuing three different prompts (positive,
  negative, neutral) to ChatPDF for all papers submitted to ICLR from 2018-2025. The
  dataset includes the generated reviews alongside the original human reviews and
  paper metadata, enabling comprehensive analysis of LLM-generated content in peer
  review contexts.
---

# Gen-Review: A Large-scale Dataset of AI-Generated (and Human-written) Peer Reviews

## Quick Facts
- arXiv ID: 2510.21192
- Source URL: https://arxiv.org/abs/2510.21192
- Reference count: 40
- Key outcome: Gen-Review dataset of 81,850 AI-generated peer reviews reveals significant positive bias and detectability using state-of-the-art methods

## Executive Summary
This work presents Gen-Review, a large-scale dataset of over 80,000 AI-generated peer reviews created by issuing three different prompts (positive, negative, neutral) to ChatPDF for all papers submitted to ICLR from 2018-2025. The dataset includes the generated reviews alongside the original human reviews and paper metadata, enabling comprehensive analysis of LLM-generated content in peer review contexts. The study reveals that LLM-generated reviews exhibit significant positive bias, with neutral prompts predominantly recommending accept scores, and that a state-of-the-art detector (Binoculars) successfully identifies 100% of LLM-generated reviews with a low false positive rate.

## Method Summary
The dataset was generated by retrieving all ICLR submissions from 2018-2025 via OpenReview API, then using ChatPDF API (with GPT-4o backend) to generate three reviews per paper using neutral, positive, and negative prompts. Each prompt requested a 100-300 word summary plus 800-1000 word main review with specific evaluation elements (soundness, novelty, clarity, significance) and ICLR 1-10 rating. The generated reviews were stored alongside original human reviews and paper metadata. Detection experiments used Binoculars to classify both LLM and human reviews based on statistical patterns.

## Key Results
- LLM-generated reviews show significant positive bias, with neutral prompts predominantly recommending accept scores (8 being most common)
- LLMs follow semantic content instructions reliably (>99% keyword presence) but fail length constraints (500-700 vs. 800-1000 words)
- LLM recommendations align with human decisions for accepted papers but diverge sharply for rejected papers
- Binoculars detector identifies 100% of LLM-generated reviews with measurable false positives on human text

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit substantial positive bias in peer review generation, even with neutral prompts
- Mechanism: When ChatPDF (using GPT-4o) generates reviews without explicit valence instructions, it defaults to positive assessments
- Core assumption: Bias reflects training data distribution and RLHF alignment favoring agreeable outputs
- Evidence anchors: [abstract] "LLM-generated reviews exhibit significant positive bias, with neutral prompts predominantly recommending accept scores"; [section 4, RQ1] 35 neutral reviews use score 5 vs. all others above threshold

### Mechanism 2
- Claim: LLMs follow semantic content instructions reliably but fail quantitative constraints
- Mechanism: Model processes keyword requirements and generates text containing those terms (>99% presence) but poorly satisfies length constraints (500-700 vs. 800-1000 words)
- Core assumption: Token-level prediction favors content coherence over meta-level instruction tracking
- Evidence anchors: [abstract] "LLMs generally follow prompt instructions regarding content elements (mentioning required keywords...in over 99% of reviews) but struggle with length constraints"

### Mechanism 3
- Claim: LLM recommendations align with human decisions for accepted papers but diverge sharply for rejected papers
- Mechanism: Intrinsic positive bias causes LLMs to over-recommend acceptance, aligning for accepted papers but diverging for rejected ones
- Core assumption: Human reviewers apply domain expertise and critical judgment that LLMs cannot replicate
- Evidence anchors: [abstract] "The alignment between LLM recommendations and actual paper outcomes is notably poor for rejected papers"; [section 4, RQ2] Fig. 3b shows prevalent disagreement for rejected papers

### Mechanism 4
- Claim: Binoculars detector identifies 100% of LLM-generated reviews with measurable false positives on human text
- Mechanism: Binoculars measures statistical artifacts in text generation that LLM outputs exhibit distinctly
- Core assumption: GPT-4o model leaves detectable statistical signatures not easily removed by minor editing
- Evidence anchors: [abstract] "A state-of-the-art detector (Binoculars) successfully identifies 100% of LLM-generated reviews with a low false positive rate"; [section 4, RQ4] 6 human reviews flagged as potentially AI-generated

## Foundational Learning

- Concept: LLM API interaction and prompt engineering
  - Why needed here: Entire dataset depends on programmatically calling ChatPDF's API with structured prompts for PDF ingestion and text generation
  - Quick check question: Can you explain how a one-shot prompt differs from multi-turn conversation, and why length constraints might degrade without explicit enforcement?

- Concept: Peer review scoring systems (ICLR 1-10 scale)
  - Why needed here: Understanding rating taxonomy is essential for interpreting bias (why score 8 is "clear accept" and 5 is "below threshold")
  - Quick check question: What score range constitutes "accept" vs. "reject" territory, and how does LLM bias shift this distribution?

- Concept: Statistical text detection (zero-shot detection)
  - Why needed here: Binoculars operates without training data, using intrinsic LLM signaturesâ€”understanding this clarifies both its effectiveness and limitations
  - Quick check question: Why might a detector flag human-written text, and how would you distinguish false positives from actual LLM use?

## Architecture Onboarding

- Component map: OpenReview API -> ChatPDF API (GPT-4o) -> Prompt templates -> Storage layer -> Binoculars detection
- Critical path: 1) Retrieve papers from OpenReview (exclude >32MB PDFs; 695 discarded) 2) For each paper, issue three API calls to ChatPDF with respective prompts 3) Store generated reviews with paper ID, prompt type, and extracted metadata 4) Apply Binoculars scoring to both LLM and human reviews for detection analysis
- Design tradeoffs: ChatPDF chosen for convenience (free, PDF-native, API access) vs. control (cannot modify underlying model or sampling parameters); "honest-but-lazy" reviewer model maximizes reproducibility but may not reflect sophisticated LLM use cases; single LLM (GPT-4o via ChatPDF) limits generalizability
- Failure signatures: Truncated reviews due to network timeouts (sanitization attempted, some remain); Missing scores in ~0.35% of reviews (pattern-matching extraction failed); False positives in detection (6 flagged human reviews pre-2023)
- First 3 experiments: 1) Run Binoculars and alternative detectors on all 35K human reviews from 2018-2022 to establish false positive rates 2) Replicate review generation using Claude, Gemini, or local models with identical prompts to test mechanism generalizability 3) Apply post-hoc score adjustment based on historical ICLR acceptance rates to test whether LLM alignment with rejected papers improves

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed positive bias in LLM-generated peer reviews persist across different large language models (e.g., Gemini, Claude, or open-source alternatives)?
- Basis in paper: [explicit] Section 5.3 suggests expanding the dataset using "other LLMs" to see if "findings can also map to other... LLMs."
- Why unresolved: Dataset generation was restricted to ChatPDF, which relies on GPT-4o models
- What evidence would resolve it: Generating reviews for the same papers using different models and comparing their score distributions and alignment with human decisions

### Open Question 2
- Question: To what extent do LLM-generated reviews contain factual inaccuracies or "hallucinations" regarding the paper's technical content?
- Basis in paper: [explicit] Section 5.3 notes it would be intriguing to explore "if generated reviews contain hallucinations" or provide a "factual account."
- Why unresolved: Current analysis prioritized quantitative metrics (score alignment, keyword presence) over semantic accuracy assessment
- What evidence would resolve it: A qualitative or automated evaluation comparing claims made in generated reviews against the ground truth text of submitted papers

### Open Question 3
- Question: Can LLM-generated reviews evade state-of-the-art detectors (like Binoculars) if adversarial prompting or post-processing techniques are applied?
- Basis in paper: [explicit] Section 5.2 endorses taking into account the possibility that "adversarial reviewers" may attempt to evade detection
- Why unresolved: Experiments assumed an "honest-but-lazy" reviewer who used simple prompts without attempting to mask AI generation
- What evidence would resolve it: Testing detection performance on reviews generated with adversarial instructions or modified to obscure AI-specific statistical patterns

## Limitations
- Dataset relies on ChatPDF API using GPT-4o, limiting control over model parameters and sampling strategies
- "Honest-but-lazy" reviewer model may not reflect sophisticated LLM use in practice
- Detection shows 100% accuracy for LLM-generated reviews but flags 6 human reviews from 2019-2022 as potentially AI-generated

## Confidence

- High confidence: LLM keyword compliance (>99%), length constraint failures (500-700 vs. 800-1000 words), Binoculars detection performance (100% recall, measurable false positives)
- Medium confidence: Positive bias mechanism (inferred from score distributions and related work), alignment asymmetry between accepted/rejected papers (correlational analysis)
- Low confidence: Generalization to other LLMs/models, long-term detection reliability as LLMs evolve, extent of actual LLM use in human reviews flagged by Binoculars

## Next Checks
1. Manually review the 6 human reviews flagged by Binoculars from 2019-2022 to determine if they represent early LLM use or detector errors
2. Generate reviews for the same papers using Claude, Gemini, or local models with identical prompts to test mechanism generalizability
3. Apply explicit negative prompting to rejected papers and measure whether score alignment with human decisions improves