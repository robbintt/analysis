---
ver: rpa2
title: 'VeriTrail: Closed-Domain Hallucination Detection with Traceability'
arxiv_id: '2505.21786'
source_url: https://arxiv.org/abs/2505.21786
tags:
- evidence
- claim
- veritrail
- nodes
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses closed-domain hallucination detection in multi-step
  generative processes, proposing VeriTrail as the first method to provide traceability
  through provenance and error localization. The method traces claims through intermediate
  outputs, identifies error stages where hallucinations likely occur, and generates
  evidence trails.
---

# VeriTrail: Closed-Domain Hallucination Detection with Traceability

## Quick Facts
- **arXiv ID:** 2505.21786
- **Source URL:** https://arxiv.org/abs/2505.21786
- **Authors:** Dasha Metropolitansky; Jonathan Larson
- **Reference count:** 40
- **Primary result:** VeriTrail achieves macro F1 scores of 74-77% on FABLES+ and DiverseSumm+ datasets for closed-domain hallucination detection

## Executive Summary
This paper addresses closed-domain hallucination detection in multi-step generative processes, proposing VeriTrail as the first method to provide traceability through provenance and error localization. The method traces claims through intermediate outputs, identifies error stages where hallucinations likely occur, and generates evidence trails. The authors construct FABLES+ and DiverseSumm+ datasets with intermediate outputs and human annotations for multi-step processes. VeriTrail outperforms baseline methods including RAG, NLI approaches, and long-context LMs, achieving macro F1 scores of 74-77% on both datasets compared to baselines at 40-75%.

## Method Summary
VeriTrail detects hallucinations in multi-step generative processes by modeling them as Directed Acyclic Graphs (DAGs) where nodes represent text spans and edges represent information flow. The system traces claims backward from terminal outputs through intermediate nodes, using an LM to select evidence via sentence IDs and generate verdicts on support. When a claim cannot be fully supported, the system identifies the last stage where it was supported as the error stage. This approach enables both detection and localization of hallucinations while providing traceable evidence trails. The method is evaluated on two newly constructed datasets, FABLES+ and DiverseSumm+, which include intermediate outputs and human annotations for multi-step processes.

## Key Results
- VeriTrail achieves macro F1 scores of 74-77% on FABLES+ and DiverseSumm+ datasets
- Baseline methods (RAG, NLI, long-context LMs) achieve 40-75% macro F1 scores
- The tracing mechanism and evidence selection both contribute to performance gains
- Traceability enables users to understand how outputs derive from sources and where errors are introduced

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Verifying claims against intermediate outputs in a reverse traversal improves detection accuracy over comparing final outputs directly to source documents.
- **Mechanism:** The system models the generative process as a Directed Acyclic Graph (DAG). Instead of searching the entire source corpus for a claim, it traces the claim backward from the terminal node through the specific intermediate nodes that actually produced it. If support is found at an intermediate node, it continues to that node's sources; if not, it flags an error at that specific stage.
- **Core assumption:** The DAG structure accurately captures the information flow, and valid claims must be traceable through this specific lineage rather than just present somewhere in the root documents.
- **Evidence anchors:** [Abstract]: "Traces claims through intermediate outputs, identifies error stages where hallucinations likely occur, and generates evidence trails." [Section 5]: Analysis shows that tracing (VT-RAG vs RAG) contributes to performance gains, particularly on the DiverseSumm+ dataset.
- **Break condition:** If the intermediate outputs are high-quality but the source documents contain the error, the mechanism may misattribute the error source.

### Mechanism 2
- **Claim:** Constraining the Language Model (LM) to select evidence via sentence IDs rather than free-text generation prevents the fabrication of evidence.
- **Mechanism:** During the Evidence Selection step, the model is prompted to return specific sentence IDs (e.g., `[12]`) rather than quoting text. The system programmatically verifies these IDs exist. This forces the "verifier" to ground its decision in actual text spans, eliminating the risk that the verifier hallucinates a justification that doesn't exist.
- **Core assumption:** The LM is capable of accurately mapping logical entailment to specific sentence indices within the provided context.
- **Evidence anchors:** [Section 3.3]: "This approach guarantees that the sentences included in the evidence trail are not hallucinated." [Section 3.1]: Describes the segmentation of nodes into sentences with unique IDs for selection.
- **Break condition:** If the model generates invalid IDs or misinterprets the text at a specific index, the evidence trail may be empty or misleading.

### Mechanism 3
- **Claim:** Iterative verification with a "break" condition on consecutive failures allows for error localization.
- **Mechanism:** The system traverses the DAG iteratively. It tracks the last stage where a claim was deemed "Fully Supported" before receiving a "Not Fully Supported" verdict. This "last known good" state allows the system to pinpoint the exact generative step where information was likely introduced or distorted.
- **Core assumption:** Hallucinations are introduced at specific transformation steps and persist or propagate, rather than being random noise that appears and disappears arbitrarily.
- **Evidence anchors:** [Section 3.2]: "To find the error stages, VeriTrail identifies iteration n, the last iteration where the claim received an interim 'Fully Supported' verdict..."
- **Break condition:** In scenarios of "False Positives" (where a claim is actually true but marked false due to missing context), this mechanism incorrectly blames a valid stage.

## Foundational Learning

### Concept: Directed Acyclic Graphs (DAGs) in NLP Pipelines
- **Why needed here:** The paper models Multi-Step Generative (MGS) processes not as a linear sequence, but as a DAG of nodes (text spans) and edges (input/output relationships). Understanding `src(v)` (source nodes) and `stage(v)` is required to implement the traversal logic.
- **Quick check question:** Given a terminal node representing a final summary, how do you identify its "source nodes" in the DAG?

### Concept: Natural Language Inference (NLI) vs. LM-based Verification
- **Why needed here:** The paper positions VeriTrail against NLI baselines (like AlignScore). NLI typically checks for entailment between two static texts. VeriTrail uses an LM as an active verifier that must reason over selected evidence, requiring an understanding of why NLI fails on long contexts.
- **Quick check question:** Why does standard NLI struggle with the "needle in the haystack" problem in long documents compared to VeriTrail's segmented approach?

### Concept: Closed-Domain Hallucination
- **Why needed here:** The system is strictly "closed-domain," meaning it assumes a fixed set of root nodes (source documents) are the only ground truth. It does not verify against world knowledge.
- **Quick check question:** If a claim is factually true in the real world but not in the provided root nodes, what verdict should VeriTrail return?

## Architecture Onboarding
- **Component map:** DAG Constructor -> Claimify -> Evidence Selector -> Verdict Generator -> Traceability Module
- **Critical path:** The critical path is the Reverse Traversal Loop. If the Evidence Selector fails to retrieve correct IDs, the Verdict Generator has no input, and the Traceability Module returns inconclusive results.
- **Design tradeoffs:** The hyperparameter `q` (max consecutive "Not Fully Supported" verdicts). A low `q` reduces cost (stops checking early) but increases the risk of false positives (flagging a claim as hallucinated when it was actually supported in root nodes not yet checked). A high `q` ensures thoroughness but increases latency and token cost.
- **Failure signatures:**
  - High False Positives: Often caused by "omission of relevant context" during Evidence Selection (see Appendix G)â€”the model selects a sentence but misses the header or previous sentence needed to interpret it.
  - Invalid Inferences: The Verdict Generator implies a logical connection (e.g., "anticipation implies marriage") that isn't strictly in the text.
- **First 3 experiments:**
  1. Baseline Comparison: Run VeriTrail vs. a standard RAG approach on a small set of claims to measure the delta in "Not Fully Supported" precision.
  2. Ablation on `q`: Test `q=1` vs `q=3` on the DiverseSumm+ subset to visualize the trade-off between cost (token usage) and recall of hallucinations.
  3. Input Size Limit Test: Vary the sentence limit per prompt (e.g., 20 vs 160 sentences) for Evidence Selection to find the tipping point where "context loss" outweighs "recall loss" (see Appendix D.1).

## Open Questions the Paper Calls Out
None

## Limitations
- The claim that VeriTrail is "the first" closed-domain hallucination detection method with traceability requires careful qualification
- The system's effectiveness critically depends on the quality of intermediate outputs and the DAG construction
- The closed-domain assumption limits applicability to scenarios where comprehensive source documents are available
- The sentence ID selection mechanism may struggle with nuanced context that spans multiple sentences or requires understanding document structure

## Confidence

**High Confidence:** The core performance claims comparing VeriTrail to RAG and NLI baselines are well-supported by the experimental results on FABLES+ and DiverseSumm+. The macro F1 scores of 74-77% versus 40-75% for baselines are clearly demonstrated and reproducible.

**Medium Confidence:** The mechanism claims about why VeriTrail works (DAG traversal, sentence ID selection, iterative verification) are theoretically sound but rely on assumptions about the generative process structure that may not generalize.

**Low Confidence:** The error localization claims - specifically identifying the exact stage where hallucinations are introduced - assume a deterministic information flow that may not hold in practice.

## Next Checks
1. **DAG Structure Validation:** Conduct experiments varying the DAG construction methodology (e.g., different node granularity, alternative edge definitions) to assess how sensitive VeriTrail's performance is to the underlying graph structure assumptions.

2. **Cross-Domain Generalization:** Test VeriTrail on domains beyond the FABLES+ and DiverseSumm+ datasets, particularly in areas with different content structures (scientific literature, technical documentation, or conversational AI) to evaluate robustness across domains.

3. **False Positive Analysis:** Perform detailed error analysis on false positive cases to understand when and why VeriTrail incorrectly flags supported claims, particularly focusing on cases involving implicit information or multi-sentence context requirements.