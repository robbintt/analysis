---
ver: rpa2
title: Deep learning for music generation. Four approaches and their comparative evaluation
arxiv_id: '2504.02586'
source_url: https://arxiv.org/abs/2504.02586
tags:
- music
- transformer
- neural
- used
- melodies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper presents four deep learning methods for generating\
  \ melodies and compares them on both aesthetic quality and application suitability.\
  \ The methods include: (1) a modified visual transformer as a language model, (2)\
  \ a transformer-based sonification of chat data, (3) a transformer combined with\
  \ Schillinger\u2019s rhythm theory, and (4) a fine-tuned GPT-3 Curie model."
---

# Deep learning for music generation. Four approaches and their comparative evaluation

## Quick Facts
- **arXiv ID:** 2504.02586
- **Source URL:** https://arxiv.org/abs/2504.02586
- **Authors:** Razvan Paroiu; Stefan Trausan-Matu
- **Reference count:** 0
- **Primary result:** Four deep learning methods compared for melody generation; GPT-3 fine-tuned on 400 melodies achieved highest aesthetic scores (3.62/5) and was most preferred overall.

## Executive Summary
This paper evaluates four deep learning approaches for melody generation using the Lakh MIDI Dataset and human aesthetic judgment. The methods include a modified vision transformer with dual encoding, a transformer-based sonification of chat data, a transformer combined with Schillinger's rhythm theory, and a fine-tuned GPT-3 Curie model. All methods were trained to generate 3-voice polyphonic music and evaluated by 108 students on a 1-5 aesthetic scale. The study found significant differences between methods, with GPT-3 achieving the highest average score and Schillinger's theory improving quality over pure sonification approaches.

## Method Summary
The study presents four deep learning methods for melody generation: (1) a modified vision transformer with dual encoders for pitch and duration, (2) a standard transformer with sonification encoding, (3) a standard transformer with Schillinger's rhythm theory for duration generation, and (4) GPT-3 Curie fine-tuned on 400 melodies in ABC format. Methods 1-3 were trained on 175,000 MIDI files from the Lakh MIDI Dataset using various preprocessing pipelines, while Method 4 used minimal fine-tuning on 400 selected melodies. All methods generated 3-voice polyphonic music sequences, with Methods 1 and 4 using more compact encodings than Methods 2 and 3. Human evaluation involved 108 participants rating generated melodies on aesthetic quality (1-5 scale) and suitability for different applications.

## Key Results
- GPT-3 fine-tuned on 400 melodies achieved the highest average aesthetic score (3.62/5) among all methods
- Schillinger's rhythm theory combined with a standard transformer produced significantly better results than pure sonification (Method 2)
- The modified vision transformer (Method 1) scored comparably to the Schillinger method but significantly higher than sonification
- Friedman test confirmed significant differences between methods with small effect size (Kendall's W = 0.081)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Large-scale pre-training transfers to music generation with minimal fine-tuning.
- **Mechanism:** GPT-3 Curie's pre-training on massive text corpora enables learning sequential patterns that transfer to symbolic music when encoded as ABC notation. Fine-tuning on only 400 melodies in compact ABC format (27 tokens vs. 90 for MusicXML for equivalent content) allows the model to adapt its sequential reasoning to musical structure without catastrophic forgetting.
- **Core assumption:** Sequential pattern recognition learned from text generalizes to symbolic music representation.
- **Evidence anchors:**
  - [abstract]: "GPT3 produced the most pleasing melodies"
  - [section 3.3]: "GPT3 Curie transformer is limited to an input of maximum 1000 tokens... the same 90 consecutive notes can be encoded in ABC format using only 27 tokens"
  - [corpus]: Weak—no direct corpus evidence on pre-training transfer efficiency for music; neighboring papers focus on specialized music models rather than general LLM transfer
- **Break condition:** If target musical genre requires domain-specific knowledge absent from pre-training corpus (e.g., non-Western scales, microtonal systems), fine-tuning on 400 melodies may be insufficient.

### Mechanism 2
- **Claim:** Injecting music-theoretic constraints into neural generation improves perceived aesthetic quality.
- **Mechanism:** Schillinger's permutation-based rhythm theory provides mathematically structured temporal patterns that replace learned duration distributions. The transformer generates pitches while Schillinger rules govern durations, creating "temporal continuity by permuting parts of a melody inside a single voice" (p. 18). This constraint reduces the search space and eliminates implausible rhythmic sequences.
- **Core assumption:** Mathematical rhythm theory captures aesthetic preferences better than data-learned duration distributions when training data is limited or noisy.
- **Evidence anchors:**
  - [abstract]: "Schillinger method proved to generate better sounding music than previous sonification methods"
  - [section 4]: "melodies generated by combining a neural network with Schillinger's theory of rhythm had a tendency to fit as background music to a movie"
  - [corpus]: YuE (2503.08638) similarly combines symbolic constraints with neural generation for long-form coherence, suggesting hybrid approaches are actively explored
- **Break condition:** If musical genre requires irregular or highly expressive rhythmic variation (e.g., rubato, syncopated jazz), rigid permutation rules may produce mechanistic results.

### Mechanism 3
- **Claim:** Dual-encoder architecture with cross-attention improves pitch-duration coordination in polyphonic music.
- **Mechanism:** The modified Vision Transformer separates pitch and duration into parallel encoding streams, then uses cross-attention where "queries generated by the notes encoder are combined with the keys and values generated by the durations encoder" (p. 22). This allows the model to learn correlations between pitch patterns and rhythmic patterns across 3 voices without concatenating them into a single token sequence.
- **Core assumption:** Pitch and duration have distinct statistical properties that benefit from separate processing before integration.
- **Evidence anchors:**
  - [section 3.1]: "Each voice consists of notes that have a corresponding pitch and duration, so two separate sequences of pitches and durations can be made"
  - [section 3.1]: Architecture diagram shows dual encoder-decoder pairs with cross-stream attention
  - [corpus]: Yin-Yang (2501.17759) addresses long-term structure but uses single-stream tokenization; no direct corpus comparison to dual-stream approaches
- **Break condition:** If pitch-duration correlations are highly genre-specific or require longer context windows than 50 consecutive notes per voice, the 150-token total context may be insufficient.

## Foundational Learning

- **Concept: Transformer attention mechanism**
  - **Why needed here:** All four methods use transformers; understanding how self-attention learns positional relationships between notes is essential for debugging generation quality.
  - **Quick check question:** Explain why transformers can parallelize training while RNNs cannot, and what tradeoff this creates for modeling long musical sequences.

- **Concept: Positional encoding (sine/cosine)**
  - **Why needed here:** The paper explicitly uses Vaswani's positional encoding formulas (p. 19); this is how the model knows note order without recurrence.
  - **Quick check question:** What happens to positional encoding if you double the sequence length beyond training distribution?

- **Concept: Fine-tuning vs. training from scratch**
  - **Why needed here:** Method 4 (GPT-3) succeeds with 400 melodies/5 epochs while other methods train on 175,000 melodies for 10 epochs; understanding transfer learning efficiency is critical.
  - **Quick check question:** Why does ABC format make fine-tuning more efficient than MusicXML for token-limited models?

## Architecture Onboarding

- **Component map:**
  Lakh MIDI Dataset (175K MIDI files) → [music21 preprocessing] → pitch integers (126 vocab) + duration integers (833 vocab) → ┌─────────────────────────────────────────────────────┐ │ Method 1: Modified Vision Transformer │ │   - Dual encoders (pitches, durations) │ │   - Cross-attention between streams │ │   - Dual decoders → 3-voice polyphonic output │ │   - 3.5M params, 256 embedding, 8 heads │ ├─────────────────────────────────────────────────────┤ │ Method 2: Standard Transformer │ │   - Single encoder-decoder (Vaswani architecture) │ │   - 128 embedding, 8 heads, 2 stacked layers │ │   - Sonification encoding │ ├─────────────────────────────────────────────────────┤ │ Method 3: Standard Transformer │ │   - Same transformer as Method 2 │ │   - Schillinger rules replace duration generation │ ├─────────────────────────────────────────────────────┤ │ Method 4: GPT-3 Curie (fine-tuned) │ │   - 400 melodies in ABC format (pretrained) │ │   - 5 epochs fine-tuning │ │   - Input: first 10 notes → output: continuation │ └─────────────────────────────────────────────────────┘

- **Critical path:**
  1. MIDI → ABC conversion quality (GPT-3 method bottleneck)
  2. Pitch/duration vocabulary coverage (126/833 values)
  3. Sequence length at inference (50 notes/voice × 3 voices)
  4. Fine-tuning data selection for GPT-3

- **Design tradeoffs:**
  - **Compact encoding (ABC) vs. expressive encoding (MusicXML):** ABC enables longer context within GPT-3's 1000-token limit but loses some structural annotations.
  - **Dual-stream vs. single-stream:** Dual encoders add complexity but may better model pitch-rhythm independence; paper doesn't ablate this.
  - **Schillinger constraints vs. learned distributions:** Constraints guarantee musicality but reduce diversity; learned distributions are flexible but require more data.

- **Failure signatures:**
  - Generated music loses coherence after ~50 notes (original transformer limitation cited in p. 16)
  - GPT-3 produces invalid ABC syntax if fine-tuning data has encoding inconsistencies
  - Schillinger rhythms sound repetitive over long sequences (permutation cycles are finite)
  - High variance in participant scores (1.17–1.66) suggests subjective disagreement on quality

- **First 3 experiments:**
  1. **Ablate dual-encoder architecture:** Train Method 1 with concatenated pitch-duration tokens vs. dual-stream to isolate cross-attention contribution.
  2. **Scale fine-tuning data:** Test GPT-3 with 100, 400, and 1000 melodies to find minimum viable fine-tuning set size.
  3. **Hybrid Schillinger-GPT:** Apply Schillinger rhythm constraints to GPT-3 pitch output to test whether theoretic constraints improve already-strong LLM generation.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does integrating Schillinger's rhythm theory with the GPT-3 architecture improve melody quality compared to GPT-3 alone?
- **Basis in paper:** [explicit] The conclusion states the authors plan on "exploring the integration of Schillinger's theory of rhythm with diverse music generation techniques."
- **Why unresolved:** The study only evaluated Schillinger's theory combined with a "classic transformer" (Method 3), not with the highest-scoring GPT-3 model.
- **What evidence would resolve it:** A comparative evaluation of standard GPT-3 outputs against GPT-3 outputs constrained by Schillinger's rhythms.

### Open Question 2
- **Question:** How does fine-tuning dataset size impact the aesthetic quality of GPT-3 generated music?
- **Basis in paper:** [inferred] The paper compares methods trained on the full Lakh MIDI Dataset against a GPT-3 model fine-tuned on only 400 melodies (Section 3.3), creating a confounding variable regarding training volume.
- **Why unresolved:** It is unclear if GPT-3's superior performance was due to its architecture or despite its limited fine-tuning data.
- **What evidence would resolve it:** An evaluation of GPT-3 models fine-tuned on varying dataset sizes (e.g., 400 vs. 175,000 melodies).

### Open Question 3
- **Question:** Do the reported preferences persist given the small effect size and specific student demographic?
- **Basis in paper:** [inferred] The authors warn the conclusion is "not definitive" due to a "small Kendall effect size" and used a cohort of 108 students (Section 4).
- **Why unresolved:** Small effect sizes indicate weak distinctions, and student preferences may not reflect general audiences.
- **What evidence would resolve it:** A replication of the study with a larger, more diverse participant pool.

## Limitations
- Subjective evaluation methodology with limited demographic reporting and high variance in participant scores
- Confounding variables from vastly different training dataset sizes across methods (175K vs. 400 melodies)
- Reliance on human aesthetic judgment without quantitative musical metrics for objective validation

## Confidence
- **High confidence:** Relative ranking of methods (GPT-3 > Schillinger > Modified Vision Transformer > Sonification) is well-supported by Friedman test results and multiple pairwise comparisons
- **Medium confidence:** Claim that Schillinger integration improves quality over sonification is supported but limited by small sample size in Method 2 training
- **Low confidence:** Assertion that GPT-3's success stems from pre-training transfer efficiency lacks direct evidence of comparison against specialized music transformers trained on same data

## Next Checks
1. **Ablation study on Schillinger constraints:** Generate melodies using the same Method 2 transformer but with learned duration distributions (not Schillinger rules) on the same 400 melodies used for GPT-3 fine-tuning to isolate whether improvements come from data quantity or theoretical constraints.

2. **Cross-cultural evaluation:** Repeat the human evaluation with participants from diverse musical backgrounds to test whether aesthetic preferences and method rankings hold across different cultural contexts and musical training levels.

3. **Objective metric validation:** Implement quantitative measures for tonal stability, rhythmic regularity, and harmonic consonance to supplement subjective ratings and identify whether human preferences align with measurable musical properties.