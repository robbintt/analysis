---
ver: rpa2
title: 'Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language
  Models'
arxiv_id: '2509.05719'
source_url: https://arxiv.org/abs/2509.05719
tags:
- datasets
- dataset
- language
- sentiment
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews 110 studies on subjective NLP tasks in Farsi,
  focusing on sentiment analysis, emotion analysis, and toxicity detection. It finds
  that despite over 1.3 million Wikipedia articles, Farsi is low-resource for subjective
  tasks due to scarce and low-quality datasets.
---

# Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models

## Quick Facts
- **arXiv ID:** 2509.05719
- **Source URL:** https://arxiv.org/abs/2509.05719
- **Reference count:** 31
- **Primary result:** Fine-tuning XLM-RoBERTa significantly outperforms zero-shot LLMs on Farsi subjective tasks, especially toxicity detection (F1 = 0.85), while emotion analysis remains challenging (avg. F1 ≈ 0.37).

## Executive Summary
This survey systematically reviews 110 studies on subjective NLP tasks in Farsi, focusing on sentiment analysis, emotion analysis, and toxicity detection. Despite Farsi having over 1.3 million Wikipedia articles, the language remains low-resource for subjective tasks due to scarce and low-quality datasets. The study identifies only 15 publicly available datasets, most lacking demographic information and inter-annotator agreement documentation. Evaluations using encoder-only models and open-source LLMs (Llama3-8B, Mixtral-7B, Qwen2-7B) on nine datasets show that fine-tuned XLM-RoBERTa achieves strong performance on toxicity detection (F1 = 0.85) but struggles with emotion analysis (F1 ≈ 0.37). Translating data to English yields negligible gains, confirming that cultural nuances and idiomatic expressions cannot be effectively captured through translation.

## Method Summary
The study evaluates nine Farsi datasets across three subjective NLP tasks: emotion analysis (ArmanEmo, LetHerLearn, EmoPars), sentiment analysis (ParsABSA, SentiPers, MirasOpinion), and toxicity detection (Phate, Pars-OFF, PHICAD). Datasets were preprocessed to filter multilabel samples and subsample large corpora. Zero-shot LLMs (Llama3-8B-Instruct, Mixtral-8x7B-Instruct-v0.1, Qwen2-7B-Instruct) were evaluated using source-aware prompt templates, while XLM-RoBERTa was fine-tuned with 3 epochs, batch size 16, learning rate 2e-5, and Adam optimizer. Translation experiments used an NLBB model for Farsi-to-English conversion. Performance was measured using macro F1-score across all tasks.

## Key Results
- Fine-tuning XLM-RoBERTa significantly outperforms zero-shot LLMs across all tasks, especially toxicity detection (F1 = 0.85)
- LLMs perform poorly on emotion analysis (avg. F1 ≈ 0.37) but better on sentiment (F1 ≈ 0.56) and toxicity tasks (F1 ≈ 0.81)
- Translating Farsi text to English yields negligible performance improvements
- Only 15 public datasets identified across 110 reviewed studies, most lacking demographic metadata and IAA documentation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Fine-tuning encoder-only models outperforms zero-shot LLMs on subjective tasks in Farsi
- **Mechanism:** Task-specific training on limited Farsi data allows the model to learn language-specific patterns and cultural nuances that generic multilingual pre-training captures less effectively. The classification head optimized via cross-entropy loss adapts representations to subjective label boundaries.
- **Core assumption:** Sufficient signal exists in the training data for the model to generalize, despite dataset limitations.
- **Evidence anchors:**
  - [abstract] "Fine-tuning XLM-RoBERTa improves results, especially for toxicity detection (F1 = 0.85)"
  - [section - Table 4] XLM-RoBERTa achieves avg F1 of 0.554 (EA), 0.758 (SA), 0.851 (TD), outperforming all LLMs
  - [corpus] Related survey (arXiv:2508.07959) confirms LLMs face systematic challenges with subjective language understanding
- **Break condition:** When training data is too small, poorly annotated, or exhibits high label noise, fine-tuning may overfit or learn incorrect patterns.

### Mechanism 2
- **Claim:** Emotion analysis is more challenging than sentiment and toxicity detection across all model architectures
- **Mechanism:** Emotion classification requires finer-grained distinctions among multiple categories (e.g., anger vs. hate vs. disgust) compared to binary/ternary sentiment or toxicity tasks. Cultural specificity of emotion expressions (e.g., "delshooreh dārad" = "salty heart" for anxiety) compounds difficulty.
- **Core assumption:** Label granularity and cultural grounding directly correlate with task difficulty.
- **Evidence anchors:**
  - [abstract] "LLMs perform poorly on emotion analysis (avg. F1 ≈ 0.37) but better on sentiment (F1 ≈ 0.56) and toxicity tasks (F1 ≈ 0.81)"
  - [section - Results] XLM-RoBERTa achieves only 0.380 F1 on EmoPars; all models show EA as hardest task
  - [corpus] Limited corpus support specifically for Farsi emotion analysis challenges
- **Break condition:** When emotion taxonomies are simplified or when datasets use more culturally universal emotion labels.

### Mechanism 3
- **Claim:** Translating Farsi text to English does not meaningfully improve model performance
- **Mechanism:** Cultural nuances and idiomatic expressions lose meaning in literal translation (e.g., "jāye to khālie" = "your place is empty" implies affection in Farsi but suggests loneliness in English). LLMs trained primarily on English still lack the cultural context needed for subjective interpretation.
- **Core assumption:** Translation quality is acceptable, but the core issue is loss of cultural/linguistic specificity.
- **Evidence anchors:**
  - [abstract] "Translating data to English yields negligible gains"
  - [section 5.1] English translation shows minimal EA impact (two models unchanged); SA scores often decrease
  - [corpus] Corpus evidence on translation strategies for low-resource subjective NLP is weak/missing
- **Break condition:** When source expressions are more literal or culturally universal, translation may help.

## Foundational Learning

- **Concept: Subjective vs. Objective NLP Tasks**
  - Why needed here: Understanding that sentiment, emotion, and toxicity involve interpretive labeling (multiple valid perspectives) rather than factual extraction explains why inter-annotator agreement is critical and why demographic annotator information matters.
  - Quick check question: Why would two annotators from different cultural backgrounds label "aghlet kame" ("you're not very smart") differently depending on context?

- **Concept: Resource Availability Discrepancy**
  - Why needed here: Recognizing that "low-resource" is task-specific—not just about raw text volume—explains why Farsi (1.3M Wikipedia articles) remains low-resource for subjective tasks: labeled data scarcity and quality gaps are the bottleneck.
  - Quick check question: What makes a language "middle-resource" in general but "low-resource" for subjective NLP specifically?

- **Concept: Zero-Shot vs. Fine-Tuning Trade-offs**
  - Why needed here: Choosing between LLM zero-shot inference and encoder fine-tuning depends on data availability, latency constraints, and acceptable performance thresholds.
  - Quick check question: Under what conditions would you accept LLM zero-shot F1 ≈ 0.37 rather than investing in fine-tuning?

## Architecture Onboarding

- **Component map:** Datasets (15 public) → Preprocessing (filtering, subsampling) → Model layer (XLM-RoBERTa fine-tuned; Llama3-8B, Mixtral-7B, Qwen2-7B zero-shot) → Evaluation (macro F1-score) → Translation layer (NLBB model for Farsi→English)

- **Critical path:**
  1. Dataset preprocessing: filter multilabel samples (EmoPars), subsample large corpora (MirasOpinion 30K, PHICAD 132K)
  2. Prompt engineering: source-aware templates (Template II) outperform generic prompts for SA
  3. Inference: zero-shot LLMs vs. fine-tuned XLM-RoBERTa with classification head
  4. Evaluation: macro F1 across 3 tasks × 3 datasets

- **Design tradeoffs:**
  - LLMs: easier deployment, no training required, but lower EA performance (≤0.37 F1)
  - Fine-tuning: higher performance (up to 0.85 TD F1), but requires labeled data and compute
  - Translation: adds pipeline complexity without measurable gain

- **Failure signatures:**
  - EA F1 <0.40 across all models indicates fundamental taxonomy or annotation issues
  - High cross-dataset variance (XLM-RoBERTa: 0.380 EmoPars vs. 0.653 LetHerLearn) signals dataset-specific quality problems
  - LLMs failing on fine-grained labels ("very negative/positive") suggests label granularity mismatch

- **First 3 experiments:**
  1. Replicate baseline: run Qwen2-7B zero-shot on all 9 datasets using Template II prompts to confirm reported F1 scores
  2. Fine-tuning ablation: train XLM-RoBERTa on each dataset independently and quantify per-task improvement margins
  3. Error analysis: manually inspect 100 misclassified EmoPars samples to categorize error sources (annotation noise, cultural nuance, label ambiguity)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** To what extent does incorporating annotator demographics (e.g., age, gender) into datasets improve the accuracy and fairness of subjective NLP models in Farsi?
- **Basis in paper:** [explicit] The authors explicitly identify a gap where "existing datasets often lack essential demographic factors... that are crucial for accurately modeling subjectivity in language."
- **Why unresolved:** Current datasets do not include this metadata, making it impossible to test how demographic context influences the labeling of subjective content like emotion or toxicity.
- **What evidence would resolve it:** The creation and release of new Farsi datasets annotated with demographic details, followed by experiments comparing models trained with and without this information.

### Open Question 2
- **Question:** Can standardizing annotation frameworks (e.g., adhering to Ekman's basic emotions) and improving inter-annotator agreement significantly close the performance gap between Emotion Analysis (avg. F1 0.37) and Toxicity Detection (avg. F1 0.81)?
- **Basis in paper:** [inferred] The authors note that existing emotion datasets often lack adherence to standard frameworks and fail to document inter-annotator agreement, which may explain why LLMs perform poorly on this specific task.
- **Why unresolved:** It is unclear if the low performance on Emotion Analysis is due to the inherent complexity of the task or the low quality and inconsistency of the available training data.
- **What evidence would resolve it:** Constructing a high-quality Farsi emotion dataset with strict annotation guidelines and high inter-annotator agreement, then re-evaluating current SOTA models.

### Open Question 3
- **Question:** Does training on data from diverse domains beyond social media (X, Instagram) and e-commerce (Digikala) improve the generalizability of Farsi subjective task models?
- **Basis in paper:** [explicit] The survey notes a "narrow focus" in data sources and states that this lack of diversity "may limit the generalizability of models trained on them to other contexts."
- **Why unresolved:** The field lacks datasets from other modalities or formal domains, so it is unknown how current models would perform on Farsi subjective tasks in news, literature, or formal discourse.
- **What evidence would resolve it:** Collecting subjective data from underrepresented Farsi sources and conducting cross-domain validation tests.

## Limitations
- Limited to 15 public datasets across three subjective tasks, with most lacking inter-annotator agreement documentation and demographic information about annotators
- Emotion analysis datasets show particularly high variance in performance (F1 ranging from 0.380 to 0.653), suggesting inconsistent annotation quality or task definitions
- Translation experiments to English yielded negligible improvements, but the underlying reasons (cultural nuance loss vs. translation quality) remain unclear

## Confidence

- **High Confidence:** Dataset scarcity characterization (110 studies reviewed, only 15 public datasets identified), toxicity detection performance (F1 ≈ 0.81), translation ineffectiveness
- **Medium Confidence:** Fine-tuning superiority claim (requires replication across more datasets), emotion analysis difficulty (though consistent across all models tested)
- **Low Confidence:** Specific numerical comparisons between models when dataset sizes and quality vary significantly

## Next Checks
1. Conduct inter-annotator agreement analysis on the three emotion analysis datasets to quantify annotation consistency and identify whether low performance stems from task ambiguity or label noise
2. Test additional instruction-tuned models (e.g., newer versions of Llama, Mistral, or domain-adapted models) to determine if poor emotion analysis performance is architecture-specific or fundamental to the task in Farsi
3. Perform ablation studies on dataset size and quality by training on subsets of the larger sentiment and toxicity datasets to establish performance scaling relationships and identify minimum viable dataset sizes