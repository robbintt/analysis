---
ver: rpa2
title: 'Chitranuvad: Adapting Multi-Lingual LLMs for Multimodal Translation'
arxiv_id: '2502.20420'
source_url: https://arxiv.org/abs/2502.20420
tags:
- translation
- arxiv
- multimodal
- language
- stage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We present Chitranuvad, a multimodal model that effectively integrates
  Multilingual LLM and a vision module for Multimodal Translation. Our method uses
  a ViT image encoder to extract visual representations as visual token embeddings
  which are projected to the LLM space by an adapter layer and generates translation
  in an autoregressive fashion.
---

# Chitranuvad: Adapting Multi-Lingual LLMs for Multimodal Translation

## Quick Facts
- **arXiv ID:** 2502.20420
- **Source URL:** https://arxiv.org/abs/2502.20420
- **Reference count:** 25
- **Primary result:** SOTA Hindi multimodal translation results; competitive across Bengali and Malayalam tracks in WAT2024

## Executive Summary
Chitranuvad presents a multimodal translation system that integrates a frozen ViT image encoder with a multilingual LLM backbone through a learned adapter layer. The system follows a three-stage training pipeline: first aligning visual-text features, then performing instruction tuning, and finally task-specific translation fine-tuning. While achieving SOTA results for Hindi translation, the model shows limited impact from the visual modality on translation quality itself, with the vision component contributing more to general agent capabilities than translation-specific performance.

## Method Summary
Chitranuvad uses a ViT encoder to extract visual representations that are projected into the LLM's embedding space via a learned adapter (single or two-layer MLP). The multilingual LLM backbone generates translations autoregressively conditioned on both text and visual tokens. Training proceeds in three stages: (1) feature alignment with frozen vision encoder and LLM, (2) instruction tuning with unfrozen LLM, and (3) task-specific translation fine-tuning. The system targets English-to-Indic language translation (Hindi, Bengali, Malayalam) using the WAT2024 dataset, with YOLOv8 object tags augmenting the visual prompts.

## Key Results
- Achieved SOTA BLEU and RIBES scores for Hindi translation across all three WAT2024 tracks
- Multilingual joint training prevented catastrophic forgetting compared to bilingual specialists
- Vision modality had minimal impact on translation quality, echoing prior findings
- Full fine-tuning outperformed LoRA adapters in the final translation stage

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Projecting visual tokens into LLM embedding space enables grounded translation without modifying the vision encoder
- **Mechanism:** ViT encoder extracts patch-level visual representations; a learned adapter (single or two-layer MLP) maps these to the LLM's token dimension. The LLM conditions autoregressive generation on both text and visual tokens (576 per image)
- **Core assumption:** Frozen vision encoder's representations are sufficiently general; alignment occurs primarily through the adapter
- **Evidence anchors:** Abstract states the method uses ViT encoder with adapter layer; section 3 describes single-layer and two-layer MLP options
- **Break condition:** If visual features are misaligned with linguistic concepts (e.g., abstract words), grounding degrades

### Mechanism 2
- **Claim:** Multi-stage training prevents catastrophic forgetting while building translation capability progressively
- **Mechanism:** Stage 1 aligns vision-text via projector-only updates; Stage 2 adds instruction tuning with LLM unfrozen; Stage 3 applies task-specific translation fine-tuning
- **Core assumption:** Indic language capability from the base Krutrim LLM transfers through instruction tuning without dilution
- **Evidence anchors:** Section 3 describes three-stage pipeline; section 5.4 notes catastrophic forgetting when training bilingual specialists
- **Break condition:** Skipping Stage 2 yields comparable baseline performance but loses general-purpose agent capability

### Mechanism 3
- **Claim:** Multilingual joint training regularizes translation learning across Indic languages better than bilingual specialization
- **Mechanism:** Training with Hindi, Bengali, and Malayalam data mixed prevents overfitting to any single language's idiosyncrasies
- **Core assumption:** Positive transfer exists among these Indic languages; they share structural or lexical properties exploitable by the model
- **Evidence anchors:** Section 5.4 states multilingual mix acts as regularization; Table 6 shows zero-shot multilingual model outperforms English-only or bilingual variants
- **Break condition:** If languages are linguistically distant or dataset sizes are highly imbalanced, negative transfer may occur

## Foundational Learning

- **Vision-Language Models (VLMs) with late fusion**
  - *Why needed:* Chitranuvad follows LLaVA-style architecture where vision is encoded separately and fused via projection
  - *Quick check:* Can you explain why late-fusion VLMs differ from early-fusion multimodal models like Chameleon?

- **Autoregressive language generation with multimodal conditioning**
  - *Why needed:* The model generates translations token-by-token conditioned on both visual tokens and source text
  - *Quick check:* How does conditioning on 576 visual tokens affect the attention mechanism during decoding?

- **Instruction tuning vs. task-specific fine-tuning**
  - *Why needed:* The paper separates general multimodal instruction following from translation-specific adaptation
  - *Quick check:* What is updated in Stage 2 vs. Stage 3, and why might Stage 2 be optional for pure translation tasks?

## Architecture Onboarding

- **Component map:** ViT encoder -> 2-layer MLP adapter -> Multilingual LLM backbone
- **Critical path:** Stage 1 alignment → Stage 2 instruction tuning → Stage 3 translation fine-tuning
- **Design tradeoffs:**
  - Full fine-tuning vs. LoRA: Full fine-tuning outperforms LoRA ("LoRA learns less" per Table 7)
  - Bilingual vs. multilingual Stage 3: Multilingual mix prevents catastrophic forgetting; bilingual specialists fail dramatically
  - Object tag enrichment: Including all detected object tags decreased performance vs. IoU-filtered tags
- **Failure signatures:**
  - Rapid overfitting after 1 epoch in Stage 3 (learning rate 1e-4 optimal)
  - Catastrophic forgetting when training bilingual specialists (Table 7 shows near-zero scores on held-out languages)
  - Back-translation degradation: Adding reverse translation (Hi/Bn/Ml → En) hurt automatic metrics
- **First 3 experiments:**
  1. Reproduce Stage 1-3 pipeline with publicly available Visual Genome data; validate "Only Stage 1, 3" baseline
  2. Ablate projector design: Compare single-layer vs. two-layer MLP adapters on Hindi Challenge BLEU
  3. Test zero-shot transfer: After Stage 2 (English-only instruction tuning), evaluate on all three target languages

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does unfreezing the vision encoder with a Perceiver Resampler improve representation learning compared to the current frozen-encoder setup?
- **Basis:** Authors state in "Limitations and Future Work" that recent works show unfreezing vision encoder with Perceiver Resampler helps learn better representations
- **Why unresolved:** Current architecture freezes ViT during all training stages to save resources or maintain stability
- **Evidence needed:** Comparative ablation study showing BLEU/RIBES scores for Chitranuvad trained with unfrozen vs. frozen vision encoder

### Open Question 2
- **Question:** Why did the vision modality have negligible impact on translation performance despite architectural integration?
- **Basis:** Conclusion notes "vision modality had little impact on the translation, echoing observations from (Grönroos et al., 2018...)"
- **Why unresolved:** MMT's core premise is that visual context grounds language to resolve ambiguities, but if vision adds little value, the model may rely on LLM's linguistic priors
- **Evidence needed:** Analysis of "ambiguous" test cases where visual context is strictly required for correct disambiguation

### Open Question 3
- **Question:** Why does including back translation in the training mix lead to decreased performance in this specific architecture?
- **Basis:** Section 5.4 states including reverse translation tasks (Hi/Bn/Ml → En) "showed decreased performance in terms of automatic metrics"
- **Why unresolved:** Back translation typically acts as data augmentation to improve performance; its negative effect suggests issues with noise injection or conflict with LLM's pre-existing knowledge
- **Evidence needed:** Comparative loss analysis or error type categorization of models trained with and without back translation

### Open Question 4
- **Question:** Why does filtering object tags by IoU decrease performance compared to using all detected object tags?
- **Basis:** Section 4 mentions authors attempted to select relevant tags using IoU but "found decreased performance against including labels of all detected objects"
- **Why unresolved:** Intuitively, filtering for relevance should reduce noise, but results suggest "noise" might provide necessary scene context or bounding box alignment is too noisy
- **Evidence needed:** Qualitative review of instances where "all tags" succeeded and "IoU tags" failed to see if successful translations relied on objects outside provided bounding boxes

## Limitations

- The proprietary Krutrim LLM backbone prevents direct replication of reported results
- Vision modality's minimal impact on translation quality questions the necessity of multimodal architecture for pure translation tasks
- Training configuration is highly sensitive, requiring exactly 1 epoch at learning rate 1e-4 to avoid rapid overfitting
- Limited explanation for why certain design choices (like back translation degradation) produce unexpected results

## Confidence

- **High Confidence:** Effectiveness of multilingual joint training over bilingual specialization (controlled experiments in Table 7)
- **Medium Confidence:** SOTA Hindi translation results (systematic evaluation across tracks, though direct baseline comparison is limited)
- **Low Confidence:** Three-stage training pipeline optimality for translation quality (comparable performance achievable with just Stages 1 and 3)

## Next Checks

1. **Stage Ablation Validation:** Replicate "Only Stage 1, 3" baseline to verify whether three-stage pipeline is essential for translation quality versus general capability

2. **Vision Modality Impact Test:** Conduct controlled experiment comparing translation quality with and without visual tokens (setting 576 visual token positions to empty)

3. **Generalization Cross-Lingual Transfer:** Evaluate zero-shot performance of models trained on bilingual pairs (En-Hi only) on Bengali and Malayalam to measure cross-lingual transfer extent