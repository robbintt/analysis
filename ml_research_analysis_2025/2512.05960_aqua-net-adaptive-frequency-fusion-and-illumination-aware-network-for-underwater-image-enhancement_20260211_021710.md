---
ver: rpa2
title: 'AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater
  Image Enhancement'
arxiv_id: '2512.05960'
source_url: https://arxiv.org/abs/2512.05960
tags:
- underwater
- image
- color
- illumination
- enhancement
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces AQUA-Net, a novel underwater image enhancement
  model that integrates spatial, frequency, and illumination-aware processing. The
  model employs a hierarchical residual encoder-decoder with dual auxiliary branches:
  a frequency fusion encoder that enriches spatial representations with frequency
  cues to preserve fine textures, and an illumination-aware decoder that performs
  adaptive exposure correction through a learned illumination map to separate reflectance
  from lighting effects.'
---

# AQUA-Net: Adaptive Frequency Fusion and Illumination Aware Network for Underwater Image Enhancement

## Quick Facts
- arXiv ID: 2512.05960
- Source URL: https://arxiv.org/abs/2512.05960
- Authors: Munsif Ali; Najmul Hassan; Lucia Ventura; Davide Di Bari; Simonepietro Canese
- Reference count: 40
- Combines spatial, frequency, and illumination-aware processing for underwater image enhancement

## Executive Summary
AQUA-Net introduces a novel underwater image enhancement model that integrates spatial, frequency, and illumination-aware processing. The model employs a hierarchical residual encoder-decoder with dual auxiliary branches: a frequency fusion encoder that enriches spatial representations with frequency cues to preserve fine textures, and an illumination-aware decoder that performs adaptive exposure correction through a learned illumination map to separate reflectance from lighting effects. The method addresses underwater image degradation caused by wavelength-dependent light absorption and scattering, including color distortion, low contrast, and haze. AQUA-Net is validated on multiple benchmark datasets and a newly introduced deep-sea dataset from the Mediterranean Sea, demonstrating performance on par with state-of-the-art methods while using fewer parameters. Ablation studies confirm the complementary contributions of the frequency and illumination branches. The model shows strong generalization capability and robustness across diverse underwater conditions.

## Method Summary
AQUA-Net addresses underwater image degradation through a hierarchical residual encoder-decoder architecture with dual auxiliary branches. The spatial encoder uses Residual Enhancement Modules (REMs) with depthwise separable convolutions and Leaky ReLU activations. The frequency fusion branch applies FFT to the input, normalizes magnitude using learned scaling, processes the normalized spectrum with a lightweight CNN, and applies inverse FFT to generate a correction map that fuses with encoder input. The illumination-aware decoder predicts an illumination map L = σ(α)·(1 + tanh(β)) that modulates skip connections to perform adaptive exposure correction based on Retinex theory. The model is trained end-to-end using L1 loss on the UIEB dataset (890 images) and evaluated on multiple benchmarks including a newly introduced deep-sea dataset.

## Key Results
- Achieves state-of-the-art performance on underwater image enhancement benchmarks while using only 0.333M parameters
- Demonstrates strong generalization across diverse underwater conditions and lighting scenarios
- Shows superior texture preservation and color correction compared to existing methods
- Ablation studies confirm complementary contributions of frequency and illumination branches

## Why This Works (Mechanism)
The method works by addressing underwater image degradation through three complementary approaches. First, the frequency fusion branch preserves high-frequency details and textures that are often lost in underwater images by extracting and processing frequency information before combining it with spatial features. Second, the illumination-aware decoder applies adaptive exposure correction based on Retinex theory, separating reflectance from lighting effects through a learned illumination map. Third, the hierarchical residual architecture allows the model to capture multi-scale features while the dual branches provide complementary information for enhanced color correction and detail preservation. The integration of these approaches enables effective handling of color distortion, low contrast, and haze while maintaining fine textures and natural appearance.

## Foundational Learning

### Frequency Domain Processing
- Why needed: Underwater images suffer from frequency-specific degradation due to light absorption and scattering
- Quick check: Verify FFT magnitude normalization and inverse FFT operations preserve spatial consistency

### Retinex Theory
- Why needed: Separates reflectance (scene content) from illumination (lighting effects) for adaptive exposure correction
- Quick check: Confirm illumination map L produces values in expected range [0, 1]

### Residual Enhancement Modules (REMs)
- Why needed: Capture multi-scale features while preserving spatial information through residual connections
- Quick check: Validate depthwise separable convolutions maintain feature integrity

## Architecture Onboarding

### Component Map
Input -> Frequency Fusion Encoder -> Spatial Encoder (REMs) -> Decoder -> Output
                         ↓                           ↑
                 Illumination Map            Skip Connections

### Critical Path
1. Input image processed by frequency branch (FFT → normalization → CNN → IFFT)
2. Frequency correction map fuses with spatial encoder input
3. Spatial encoder extracts hierarchical features using REMs
4. Decoder reconstructs enhanced image with illumination map modulating skip connections

### Design Tradeoffs
- Frequency processing adds computational overhead but preserves fine textures
- Illumination map enables adaptive correction but requires additional parameter learning
- Residual connections improve gradient flow but increase memory usage

### Failure Signatures
- Frequency artifacts or ringing indicate improper FFT magnitude normalization
- Over-smoothed outputs suggest inadequate illumination map range or poor frequency feature fusion
- Color bleeding indicates incorrect illumination map application to skip connections

### First 3 Experiments
1. Visualize frequency correction map Rf to verify proper magnitude normalization and learned scaling
2. Check illumination map L range and distribution to ensure [0, 1] values
3. Compare outputs with and without frequency branch to assess texture preservation

## Open Questions the Paper Calls Out

### Open Question 1
How can the model complexity be further optimized to enable real-time deployment on resource-constrained embedded devices? The paper explicitly states that future studies may focus on further reduction of model complexity to enable deployment on low-power or embedded underwater devices. While AQUA-Net is lightweight (0.333M parameters), further optimization is required to meet the strict power and latency constraints of autonomous underwater vehicles (AUVs) and remote sensors. Evidence would come from benchmarks measuring frame rates and energy consumption on specific edge hardware compared to current implementation.

### Open Question 2
Does training the model specifically on deep-sea data significantly improve performance in extreme conditions compared to training on shallow-water benchmarks? The paper introduces the DeepSea dataset to enable robust evaluation, but the model is trained exclusively on the UIEB dataset (shallow/lab environments) while DeepSea data is used only for testing. It is unclear if the frequency and illumination branches can fully bridge the domain gap between shallow-water training data and the unique artificial-lighting/low-light conditions of deep-sea environments without domain-specific training. Evidence would come from a comparative study analyzing performance when fine-tuned or trained from scratch on DeepSea versus baseline UIEB training.

### Open Question 3
Would integrating perceptual or adversarial loss functions improve texture restoration and visual realism beyond the current L1 optimization? The model relies on standard L1 loss for pixel-wise consistency. While the frequency branch aims to preserve textures, L1 losses often result in overly smooth reconstructions, potentially limiting the perceptual realism the authors aim to restore. The paper does not analyze whether the frequency branch's ability to recover fine textures is bottlenecked by the simple loss function, nor does it compare against perceptual losses used in other SOTA methods. Evidence would come from an ablation study contrasting L1 with perceptual or adversarial losses, evaluated using no-reference metrics and high-frequency detail analysis.

## Limitations

- Missing architectural specifications: exact channel dimensions of REMs across three stages are unspecified
- Lightweight CNN architecture details (layer counts, kernel sizes, channel numbers) are not provided
- Deep-sea dataset lacks benchmark comparisons from other methods, limiting independent verification
- Performance evaluation relies primarily on PSNR/SSIM/UIQM/UCIQE metrics without perceptual studies

## Confidence

- State-of-the-art performance claims: High confidence (detailed mathematical formulations and ablation studies provided)
- Architectural novelty: High confidence (comprehensive descriptions of frequency fusion and illumination-aware components)
- Reproducibility: Medium confidence (missing critical implementation details despite clear conceptual framework)
- Empirical validation: Medium confidence (robust across multiple datasets but limited independent verification on new deep-sea benchmark)

## Next Checks

1. Implement and visualize frequency correction map Rf to verify proper FFT magnitude normalization and learned scaling
2. Validate illumination map L range and distribution to ensure [0, 1] values are correctly produced
3. Perform ablation study comparing outputs with and without frequency branch to assess texture preservation impact