---
ver: rpa2
title: 'Apertus: Democratizing Open and Compliant LLMs for Global Language Environments'
arxiv_id: '2509.14233'
source_url: https://arxiv.org/abs/2509.14233
tags:
- data
- training
- arxiv
- tokens
- apertus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Apertus is a fully open LLM suite trained on 15T tokens from 1811
  languages, prioritizing data compliance, multilingual representation, and transparency.
  It uses retroactive robots.txt filtering, PII/toxicity removal, and Goldfish loss
  to suppress memorization while maintaining performance.
---

# Apertus: Democratizing Open and Compliant LLMs for Global Language Environments

## Quick Facts
- arXiv ID: 2509.14233
- Source URL: https://arxiv.org/abs/2509.14233
- Reference count: 40
- Apertus achieves state-of-the-art multilingual performance among open models, with 8B and 70B variants approaching or surpassing open-weight counterparts on cultural, knowledge, and instruction-following benchmarks.

## Executive Summary
Apertus is a fully open LLM suite trained on 15T tokens from 1811 languages, prioritizing data compliance, multilingual representation, and transparency. It uses retroactive robots.txt filtering, PII/toxicity removal, and Goldfish loss to suppress memorization while maintaining performance. Post-training yields Apertus-8B/70B-Instruct models with 40% non-English pretraining data and alignment to Swiss constitutional values. Apertus achieves state-of-the-art multilingual performance among open models, with 8B and 70B variants approaching or surpassing open-weight counterparts on cultural, knowledge, and instruction-following benchmarks. All training data, code, and artifacts are publicly released under permissive licenses.

## Method Summary
Apertus consists of three main phases: data preparation, pretraining, and post-training alignment. The data pipeline applies retroactive robots.txt filtering, removes PII and toxic content, and stages a multilingual curriculum (40% non-English, 1811 languages). Pretraining uses a Transformer decoder with xIELU activation, AdEMAMix optimizer, and Goldfish loss (2% token masking) on 15T tokens. Post-training applies Supervised Fine-Tuning (SFT) on multilingual instruction data, followed by Constitutional Preference Optimization (CPR) using a Swiss AI Charter-aligned judge.

## Key Results
- Achieves state-of-the-art multilingual performance among open models on XCOPA, INCLUDE, and Global-MMLU benchmarks
- 8B and 70B variants approach or surpass open-weight counterparts on cultural, knowledge, and instruction-following tasks
- Successfully suppresses memorization through Goldfish loss while maintaining downstream task performance
- Provides fully open access to all training data, code, and model weights under permissive licenses

## Why This Works (Mechanism)

### Mechanism 1: Goldfish Loss for Memorization Mitigation
- **Claim:** Goldfish loss reduces verbatim memorization of training data without significantly degrading downstream performance.
- **Mechanism:** During pretraining, a random binary mask `G` is applied to tokens in a sequence based on a hash of the preceding context (e.g., 50 tokens). Loss is only computed on unmasked tokens (`k=50` masking rate, `h=50` window). This prevents the model from learning continuous long-range token-to-context mappings that anchor verbatim recall.
- **Core Assumption:** Selectively dropping ~2% of tokens during training disrupts the formation of memorization anchors while preserving the model's ability to learn general language patterns from the remaining 98% of tokens.
- **Evidence anchors:**
  - [abstract]: "To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance."
  - [section 2.3]: Describes the loss function `L(θ) = -1/|G| Σ Gi(xi) log Pθ(xi|x<i)` and notes the configuration of k=50, h=50.
  - [section 5.4]: "Apertus models are pretrained using the Goldfish objective (Hans et al., 2024), constraining the model's ability to regenerate text. We demonstrate that this approach effectively suppresses verbatim recall even at a large model scale and after 128 exposures during training."
- **Break condition:** Memorization returns if (a) the masking rate is too low, (b) the hash context window is too short to break up common phrases, or (c) near-duplicate data with formatting differences causes the hash to apply inconsistently.

### Mechanism 2: Retroactive Robots.txt Compliance
- **Claim:** Applying robots.txt exclusions retroactively to historic web crawls increases data compliance while maintaining model performance on compliant data.
- **Mechanism:** The paper takes web crawl snapshots from 2013-2024 but filters them using robots.txt directives from January 2025. Domains blocking AI-specific user agents (e.g., GPTBot, CCBot) are removed from all snapshots. This respects owner consent as of a recent checkpoint.
- **Core Assumption:** Current robots.txt preferences are a reasonable proxy for long-term content owner intent, and the token loss (e.g., ~8% English, ~4% multilingual) does not critically harm model quality on the remaining data.
- **Evidence anchors:**
  - [abstract]: "Apertus models are pretrained exclusively on openly available data, retroactively respecting `robots.txt` exclusions and filtering for non-permissive, toxic, and personally identifiable content."
  - [section 3.1.1]: Details the method of applying January 2025 robots.txt to historic data, listing the filtered bots and resulting token loss.
- **Break condition:** Compliance is undermined if (a) robots.txt changes after the snapshot date are ignored, or (b) the filtering is incomplete (e.g., not all AI-specific user agents are checked).

### Mechanism 3: Multilingual Data Mixture for Broad Language Coverage
- **Claim:** A pretraining corpus with 40% non-English content (1811 languages) enables strong multilingual performance without sacrificing English capability.
- **Mechanism:** The model is trained on 15T tokens, with a large fraction sourced from FineWeb-2 (1811 languages). The data mixture is staged, with early phases focusing on general language modeling and later phases increasing code/math content. This broad exposure is post-trained with 149-language instruction data.
- **Core Assumption:** Massive multilingual pretraining data, even of mixed quality, allows the model to learn transferable representations that support low-resource languages.
- **Evidence anchors:**
  - [abstract]: "The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content."
  - [section 3.2.2 & 3.3]: Describes the FineWeb-2 dataset and the staged curriculum. Section 5.1 results show strong multilingual performance on XCOPA and INCLUDE benchmarks.
- **Break condition:** Multilingual gains may not materialize if (a) low-resource language data is too noisy, or (b) post-training alignment is English-centric, causing performance collapse.

## Foundational Learning

- **Concept: Pretraining Curriculum**
  - **Why needed here:** The model's capabilities are built in stages (language foundation → quality/diversity → math/code), so understanding this staging is critical for debugging capability gaps.
  - **Quick check question:** If a model fails at complex reasoning but succeeds at general language tasks, which pretraining stage might have been insufficiently trained?

- **Concept: Post-training Alignment (SFT + QRPO)**
  - **Why needed here:** The base model is transformed into an instruction-following assistant via SFT and then aligned for safety and helpfulness. This pipeline is a key determinant of final user-facing behavior.
  - **Quick check question:** What is the primary difference between the data used for Supervised Fine-Tuning (SFT) and the data used for preference alignment via QRPO?

- **Concept: xIELU Activation Function**
  - **Why needed here:** This is a novel architectural choice (replacing SwiGLU) that is claimed to improve efficiency. Engineers working on the model internals or efficiency must understand it.
  - **Quick check question:** How does xIELU differ from standard ReLU or SwiGLU, and what is the paper's claimed benefit for LLM training?

## Architecture Onboarding

- **Component map:** Data Pipeline -> Pretraining Core (Transformer with xIELU, AdEMAMix, Goldfish loss) -> Post-training Stack (SFT + QRPO)
- **Critical path:** The most critical path for model quality is the Pretraining Core. Specifically, the AdEMAMix optimizer's performance is sensitive to gradient clipping settings, and training stability was a major engineering challenge. The Goldfish loss implementation must be correct to achieve memorization mitigation.
- **Design tradeoffs:**
  - Goldfish Loss: Trading off a slight potential degradation in modeling accuracy for a major gain in copyright/privacy safety (memorization reduction)
  - Retroactive Compliance: Trading off ~6-8% of pretraining tokens for higher data legality and ethical standing
  - Multilingual Coverage: Trading off per-language data quality (due to web data noise) for massive breadth of language support
- **Failure signatures:**
  - Training Instability: Large loss spikes or gradient issues
  - Memorization: Verbatim reproduction of training data
  - OOM: Context parallelism issues during long-context extension
- **First experiments:**
  1. Verify Goldfish loss masking logic is correctly implemented with h=50 context window
  2. Test AdEMAMix optimizer stability with gradient clipping at 0.1
  3. Validate multilingual data quality by sampling outputs across 10+ languages

## Open Questions the Paper Calls Out
None

## Limitations

- Data Quality and Generalization: Reliance on web-derived data introduces quality variance across languages, with uncertain robustness for low-resource languages
- Compliance Mechanisms Verification: Retroactive robots.txt filtering is innovative but untested in legal contexts and may not fully reflect long-term owner intent
- Memorization Mitigation Effectiveness: Limited quantitative evidence of Goldfish loss effectiveness beyond the claim that it works "even at a large model scale and after 128 exposures"

## Confidence

**High Confidence:**
- Architectural specifications (xIELU, AdEMAMix, QK-Norm) are clearly defined and implementable
- Pretraining data composition (15T tokens, 40% non-English) is accurately reported
- Benchmark results on standard multilingual tests are verifiable

**Medium Confidence:**
- Effectiveness of Goldfish loss in preventing memorization (limited quantitative validation)
- Quality of multilingual representation across all 1811 languages (web data quality variance not fully characterized)
- Legal compliance of retroactive robots.txt filtering (no legal review or independent validation)

**Low Confidence:**
- Long-term stability of model's memorization suppression (only tested up to 128 exposures)
- Generalizability to languages not represented in test benchmarks
- Actual user-facing safety improvements from constitutional alignment (limited evaluation of controversial scenarios)

## Next Checks

1. **Memorization Test Suite:** Conduct controlled experiments to quantify memorization reduction by comparing Apertus with a baseline model trained without Goldfish loss on datasets containing sensitive content. Measure both exact string recall and semantically similar reproduction.

2. **Low-Resource Language Quality Analysis:** Perform detailed evaluation of model performance on languages with minimal web presence (e.g., indigenous languages) using human evaluation and task-specific benchmarks to determine if the 1811-language claim reflects genuine capability or superficial coverage.

3. **Legal Compliance Audit:** Engage legal experts to review the retroactive robots.txt implementation and assess whether it meets current data usage regulations (e.g., GDPR, EU AI Act). Additionally, test the robustness of the filtering by attempting to reconstruct PII or copyrighted content from model outputs.