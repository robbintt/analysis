---
ver: rpa2
title: 'Improved Mean Flows: On the Challenges of Fastforward Generative Models'
arxiv_id: '2512.02012'
source_url: https://arxiv.org/abs/2512.02012
tags:
- training
- original
- meanflow
- which
- flow
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper identifies key challenges in the MeanFlow framework
  for one-step generative modeling. The first issue is that the original MeanFlow's
  training target depends on the network itself, making it non-standard regression.
---

# Improved Mean Flows: On the Challenges of Fastforward Generative Models

## Quick Facts
- arXiv ID: 2512.02012
- Source URL: https://arxiv.org/abs/2512.02012
- Reference count: 40
- Key outcome: iMF achieves 1.72 FID on ImageNet 256×256 with single-step sampling, representing 50% relative improvement over original MeanFlow

## Executive Summary
This paper addresses fundamental challenges in the MeanFlow framework for one-step generative modeling. The authors identify two critical issues: (1) the original MeanFlow's training objective depends on the network itself, creating a non-standard regression problem with high variance, and (2) the fixed classifier-free guidance scale during training limits flexibility. They propose a reformulated objective that treats the instantaneous velocity as the regression target, parameterized through a network predicting the average velocity, and introduce guidance scale as an explicit conditioning variable. These improvements enable their iMF method to achieve state-of-the-art performance of 1.72 FID with single-step sampling on ImageNet 256×256, closing the gap with multi-step models.

## Method Summary
The authors reformulate MeanFlow's training objective to create a standard regression problem by re-parameterizing the instantaneous velocity v. Instead of using the conditional velocity (e-x) directly in the Jacobian-Vector Product (JVP) computation, they predict the marginal velocity v_θ first via a boundary condition v_θ = u_θ(z_t, t, t), which has lower variance and provides more stable optimization. They also formulate guidance scale ω as an explicit conditioning variable, allowing the network to support varying guidance strengths during both training and inference. The method uses in-context conditioning with multi-token embeddings for diverse conditions (class, time steps, guidance scales) instead of the parameter-heavy adaLN-zero module, reducing parameters by ~33% while maintaining performance.

## Key Results
- Achieves 1.72 FID on ImageNet 256×256 with single-step sampling
- 50% relative improvement over original MeanFlow's 3.43 FID
- Outperforms other one-step models including Distillation 1.67, Flow 1.77, and AlphaFlow 1.81
- Reduces parameters from 133M to 89M while improving FID from 4.57 to 4.09
- Sets new state-of-the-art for one-step generative models trained from scratch

## Why This Works (Mechanism)

### Mechanism 1: v-loss Re-parameterization via Predicted Marginal Velocity
The reformulation replaces conditional velocity (e-x) with predicted marginal velocity v_θ as JVP input, reducing training variance and stabilizing optimization. The Jacobian amplifies variance in conditional samples, causing non-decreasing loss. By predicting v_θ(z_t) first, the JVP input has lower variance, yielding stable regression toward the true marginal target. This assumes the predicted marginal velocity has lower variance than conditional velocity when used as a tangent vector in JVP computation.

### Mechanism 2: Guidance Scale as Inference-Time Condition
Conditioning the network on CFG scale ω allows single-model support for varying guidance strengths at inference. Instead of baking fixed ω into training targets, ω is embedded like time steps and fed as input. The network learns u_θ(z_t|c, ω), enabling ω sampling from a distribution during training (power-law biased toward small values) and flexible selection at inference. This assumes optimal CFG scale varies with model strength and cannot be predetermined.

### Mechanism 3: Multi-Token In-Context Conditioning
Replacing adaLN-zero modulation with concatenated condition tokens reduces parameters by ~33% while maintaining performance. Each condition type is converted to multiple tokens (8 for class, 4 for others) and concatenated with image tokens along the sequence dimension. The transformer processes all jointly via self-attention. This assumes multi-token representation provides sufficient representational capacity to replace adaptive normalization.

## Foundational Learning

- **Jacobian-Vector Products (JVP)**: Core computation for MeanFlow identity; efficiently computes d/dt u_θ = ∂_z u · v + ∂_t u without forming full Jacobian. Quick check: Given f: R^n → R^m and tangent vector v, what is the complexity difference between JVP(f; v) and computing J_f · v explicitly?

- **Conditional vs Marginal Velocity**: The true regression target is marginal v(z_t) = E[v_c|z_t], not conditional v_c = e-x; using v_c in JVP injects noise. Quick check: For z_t = (1-t)x + t·e, why can multiple (x, e) pairs produce the same z_t at a given t?

- **Classifier-Free Guidance (CFG)**: CFG interpolates between conditional and unconditional predictions; v_cfg = ω·v(z_t|c) + (1-ω)·v(z_t). Quick check: What is the effective behavior when ω=1 vs ω>1?

## Architecture Onboarding

- **Component map**: 
  Input: z_t (latent), r, t (time steps), c (class), Ω={ω, t_min, t_max} (guidance)
  ├── Condition tokenizers: 2-layer MLPs → multi-token embeddings
  ├── Patch embedding: z_t → sequence tokens
  ├── Concatenation: [img_tokens | cond_tokens] along sequence axis
  ├── Transformer blocks: SwiGLU + RMSnorm + RoPE (no adaLN)
  ├── Main head u_θ: predicts average velocity
  └── Optional auxiliary v-head: shares layers, predicts marginal v (training only)

- **Critical path**:
  1. Sample batch: t, r from logit-normal; ω from power distribution p(ω)∝ω^(-β); x from data; e ~ N(0,I)
  2. Construct z_t = (1-t)x + t·e
  3. Compute v_θ via boundary condition: `v_c = fn(z, t, t, ω, c)`; `v_u = fn(z, t, t, ω, None)`
  4. JVP call: `u, dudt = jvp(fn, (z, r, t, ω, c), (v_c, 0, 1, 0, 0))`
  5. Compound function: `V = u + (t-r) * stopgrad(dudt)`
  6. CFG target: `v_g = (e-x) + (1-1/ω)(v_c - v_u)`
  7. Loss and backprop

- **Design tradeoffs**:
  - Boundary condition (0 extra params, good for small models) vs auxiliary v-head (+training params, better for large models)
  - In-context tokens (compact, -33% params) vs adaLN-zero (standard baseline, heavier)
  - Flexible ω (inference control) vs fixed ω (simpler but limited)

- **Failure signatures**:
  - Loss non-decreasing with high variance: likely using e-x directly in JVP instead of predicted v_θ
  - CFG ineffective at inference: verify ω is passed as condition, not hardcoded
  - Model larger than expected: check for residual adaLN-zero layers

- **First 3 experiments**:
  1. Reproduce baseline: Train original MF-B/2 formulation, confirm ~6.17 FID with fixed CFG
  2. Isolate v-loss fix: Change only JVP input from `(e-x)` to `fn(z, t, t)`; plot loss variance over training
  3. Validate CFG conditioning: Train with ω~p(ω), evaluate FID across ω∈[1,4] at inference to confirm flexibility

## Open Questions the Paper Calls Out
- Can the iMF framework be effectively extended to pixel-space generation to mitigate the non-negligible inference costs associated with VAE tokenizers?
- Does the proposed in-context conditioning mechanism scale efficiently to dense, high-dimensional conditioning inputs such as text embeddings for text-to-image generation?
- Can the reformulated training objective of iMF be combined with knowledge distillation to further close the performance gap with multi-step models?

## Limitations
- The v-loss reparameterization depends critically on numerically stable JVP computation, with no ablation showing sensitivity to implementation details
- The fixed power-law training distribution for ω may not cover the most useful range if optimal scales vary significantly between models or datasets
- The multi-token in-context approach trades parameter efficiency for potentially increased sequence length and attention complexity, with no reported training/inference wall-clock times

## Confidence
- **High Confidence**: The reformulated v-loss objective is mathematically correct and provides a more stable training target than original MeanFlow; iMF achieves state-of-the-art FID of 1.72 on ImageNet 256×256 with single-step sampling; in-context conditioning achieves comparable performance to adaLN-zero with 33% fewer parameters
- **Medium Confidence**: The v-loss reparameterization consistently improves training stability across different model scales and datasets; the CFG conditioning generalizes to a wide range of optimal guidance scales beyond those seen during training; the multi-token in-context approach maintains performance for complex conditions beyond class labels
- **Low Confidence**: The JVP computation details are sufficient for faithful reproduction without additional hyperparameter tuning; the optimal CFG scale range [1.0, 8.0] with β=1 or 2 covers all practical use cases; the parameter reduction translates directly to wall-clock speedups in both training and inference

## Next Checks
1. **JVP Sensitivity Analysis**: Systematically vary the JVP implementation (e.g., tangent vector scaling, stop-gradient placement) and measure the impact on training loss variance and final FID to validate robustness to implementation details

2. **CFG Generalization Benchmark**: Train iMF models on diverse datasets (e.g., LSUN, CIFAR-10) and evaluate optimal ω ranges; test whether models trained with fixed ω distributions can adapt to optimal scales outside their training range through fine-tuning or inference-time adjustment

3. **In-Context Conditioning Scaling Study**: Evaluate iMF with increasing sequence lengths (e.g., 1024, 2048 tokens) on tasks requiring complex conditions (e.g., text-to-image, segmentation-to-image); measure attention self-attention patterns to verify condition information is effectively routed despite the parameter reduction