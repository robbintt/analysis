---
ver: rpa2
title: 'CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical
  Grammar Competence of Large Language Models'
arxiv_id: '2504.13261'
source_url: https://arxiv.org/abs/2504.13261
tags:
- grammar
- language
- llms
- evaluation
- pedagogical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CPG-EVAL, the first benchmark for assessing
  large language models' (LLMs) competence in pedagogical grammar within Chinese language
  teaching. Grounded in a validated pedagogical grammar framework, the benchmark comprises
  five task types evaluating grammar recognition, fine-grained distinction, categorical
  discrimination, and resistance to linguistic interference.
---

# CPG-EVAL: A Multi-Tiered Benchmark for Evaluating the Chinese Pedagogical Grammar Competence of Large Language Models

## Quick Facts
- **arXiv ID:** 2504.13261
- **Source URL:** https://arxiv.org/abs/2504.13261
- **Reference count:** 7
- **Primary result:** Introduces first benchmark for evaluating LLMs' competence in pedagogical grammar for Chinese language teaching.

## Executive Summary
This study introduces CPG-EVAL, the first benchmark for assessing large language models' (LLMs) competence in pedagogical grammar within Chinese language teaching. Grounded in a validated pedagogical grammar framework, the benchmark comprises five task types evaluating grammar recognition, fine-grained distinction, categorical discrimination, and resistance to linguistic interference. Evaluation of 13 models shows smaller models struggle with negative instance identification and multiple-instance tasks, while larger models exhibit better resistance to interference but still have room for accuracy improvement. Average performance across tasks was 81%, with task-specific accuracies ranging from 73% to 95%. The results highlight the need for improved instructional alignment and rigorous benchmarks to guide LLM deployment in educational contexts.

## Method Summary
The benchmark uses 739 grammar items from the Chinese Grammar Learning Manual (CGLM) and 6,651 synthetic sentences generated by DeepSeek-v3 and verified by native speakers. Five task types evaluate different aspects of pedagogical grammar competence: SINGLE (single instance mapping), BATCH (batch grammar-instance mapping), SIM-GRA (similarity grammar discrimination), CAT-GRA (category grammar selection), and CON-INS (confusing instance discrimination). Zero-shot evaluation is conducted with regex-based answer extraction. Models are evaluated on their ability to map language instances to grammar items and distinguish between similar grammatical constructs.

## Key Results
- Smaller models struggle with negative instance identification and multiple-instance tasks
- Larger models exhibit better resistance to linguistic interference but still have accuracy improvement room
- Average performance across tasks was 81%, with task-specific accuracies ranging from 73% to 95%
- Significant performance gaps exist between positive and negative instance tasks for smaller models

## Why This Works (Mechanism)

### Mechanism 1: Pedagogical Pattern Recognition (P-GPR) Abstraction
- Claim: The benchmark assesses whether LLMs apply abstract grammatical rules rather than relying solely on statistical co-occurrence.
- Mechanism: By formalizing the task as $f(G_i, L_i) \rightarrow \{0, 1\}$, the framework forces the model to map a language instance ($L_i$) to a pedagogical grammar item ($G_i$). This exposes the gap between predicting the next word and verifying meta-linguistic rules.
- Core assumption: LLMs process "pedagogical grammar" differently from raw text generation, and this discrepancy is measurable via binary classification tasks.
- Evidence anchors:
  - [section 3.1] Defines P-GPR as the core evaluation function.
  - [abstract] Notes that LLM capabilities are based on statistical learning, necessitating a specific pedagogical assessment.
  - [corpus] Consistent with AraLingBench and GRILE, which isolate specific linguistic competencies (morphology, syntax) rather than general perplexity.
- Break condition: If models succeed only by matching surface-level keywords in $G_i$ and $L_i$ without understanding the rule logic, the mechanism degrades to simple semantic similarity.

### Mechanism 2: Scale-Dependent Interference Resistance
- Claim: Model scale correlates with the ability to suppress "false positive" errors in the presence of confusing or negative instances.
- Mechanism: Larger models (e.g., Doubao-1.5-pro, GPT-4o) demonstrate higher robustness in CON-INS (Confusing Instance) and BATCH-F (Negative Batch) tasks. This suggests that parameter count improves the "resolution" of decision boundaries between similar grammatical constructs.
- Core assumption: The performance gap between SINGLE-T (positive) and SINGLE-F (negative) tasks in smaller models indicates a structural inability to verify the *absence* of a rule, rather than just data sparsity.
- Evidence anchors:
  - [section 5.1] Reports significant accuracy drops in smaller models (e.g., InternLM2.5-7B falling to 31.6% on BATCH-F) and explicit "false-positive issues."
  - [section 5.3] Details how reducing confusing instances (F10 vs T5F5) improves accuracy, linking difficulty to interference volume.
  - [corpus] AraLingBench similarly segments evaluation into fine-grained categories, implying that high performance requires distinct handling of similar linguistic features.
- Break condition: If a smaller model uses specialized grammar fine-tuning to outperform a larger generalist, the mechanism shifts from scale-dependence to domain-specific adaptation.

### Mechanism 3: Structured Discrimination via Distractors
- Claim: Evaluation validity is maintained by generating distractors based on specific error typologies (semantic similarity vs. categorical similarity).
- Mechanism: The benchmark differentiates between SIM-GRA (distractors from embedding similarity) and CAT-GRA (distractors from the same grammatical category). This forces the model to distinguish "literal resemblance" from "functional equivalence."
- Core assumption: Embedding-based similarity effectively simulates the "confusing" linguistic forms learners encounter.
- Evidence anchors:
  - [section 3.3] Explicitly defines SIM-GRA and CAT-GRA distractor generation methods.
  - [section 5.2] Shows models perform worse on CAT-GRA (76.0%) than SIM-GRA (87.7%), proving the mechanism successfully tests different depth levels.
  - [corpus] Irish-BLiMP uses minimal pairs to test specific linguistic constraints; CPG-EVAL extends this by using categorical distractors for pedagogical context.
- Break condition: If distractors are not reviewed by experts (as done in section 3.2.2), they may fail to represent actual learner errors, reducing the benchmark to a test of random noise discrimination.

## Foundational Learning

- Concept: **Pedagogical Grammar vs. Linguistic Grammar**
  - Why needed here: The paper evaluates "pedagogical grammar," which focuses on rules useful for *teaching* and *acquisition* rather than theoretical linguistics. Understanding this distinction is crucial for interpreting why CGLM was chosen as the knowledge core.
  - Quick check question: Does the task require the model to act as a linguist (describing structure) or a teacher (identifying learner-appropriate rules)?

- Concept: **Zero-Shot Evaluation**
  - Why needed here: The evaluation setup explicitly uses zero-shot prompting (no examples provided) to test "intrinsic knowledge." This contrasts with few-shot setups often used in general NLP benchmarks.
  - Quick check question: If I provide examples in the prompt, am I testing the model's pre-trained grammar competence or its in-context learning ability?

- Concept: **False Positive Bias in Negation**
  - Why needed here: The study highlights that models struggle with "negative instances" (identifying when a rule does *not* apply). This is a specific failure mode where models are "agreeable" or over-predict rule presence.
  - Quick check question: In a binary classification task (T/F), does the model exhibit >50% false positives on "F" type questions?

## Architecture Onboarding

- Component map:
  Knowledge Core: CGLM (Chinese Grammar Learning Manual) -> 739 Grammar Items
  Data Engine: DeepSeek-v3 (Synthetic Sentence Generation) + Human Expert Review -> 6,651 Language Instances
  Task Layer: 5 Task Types (SINGLE, BATCH, SIM-GRA, CAT-GRA, CON-INS)
  Evaluator: Regex Scoring (Extracts strict T/F or option labels)

- Critical path:
  1. Grammar Selection: Map target domain to a validated framework (e.g., CGLM)
  2. Instance Synthesis: Generate instances and *confusing* instances
  3. Distractor Mining: Use embedding models for SIM-GRA and category lookup for CAT-GRA
  4. Prompt Engineering: Enforce strict output control (e.g., "Output only T or F")

- Design tradeoffs:
  - Synthetic vs. Human Data: The study uses synthetic data verified by humans. This scales faster than pure human authoring but risks "model-in-the-loop" bias if the generator and evaluator share similar failure modes.
  - Regex vs. LLM-Judge: The study uses regex for scoring to ensure objectivity. This requires rigid prompt adherence but eliminates the variability of using an LLM to grade another LLM.

- Failure signatures:
  - Batch Failure: Accuracy on BATCH-F drops significantly (e.g., <40%) while SINGLE-T remains high (>90%). Indicates context window contamination or inability to process multiple negations.
  - Category Confusion: High SIM-GRA but low CAT-GRA scores. Indicates the model relies on surface semantics rather than structured grammatical logic.
  - False Positive Spiral: In CON-INS-F10, smaller models score <50% (random baseline), suggesting they are systematically fooled by confusing forms.

- First 3 experiments:
  1. Bias Baseline: Run a baseline of SINGLE-T vs. SINGLE-F on your target model. If SINGLE-F accuracy is <80%, the model is unsuitable for grammar error explanation tasks without fine-tuning.
  2. Interference Stress Test: Evaluate on CON-INS-F10. If performance collapses compared to CON-INS-T5F5, the model lacks robustness to the specific "distractors" found in learner language.
  3. Semantic vs. Structural: Compare SIM-GRA and CAT-GRA scores. A large gap confirms the model relies on semantic similarity rather than formal grammatical categorization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can targeted fine-tuning significantly improve the ability of smaller-scale models to resist linguistic interference in pedagogical grammar tasks?
- Basis in paper: [explicit] The discussion section highlights the "importance of efforts targeted at enhancing grammatical discrimination and interference resistance capabilities in small-scale, cost-effective language models."
- Why unresolved: The current study only evaluated models in a zero-shot setting, revealing that smaller models fail significantly at interference-heavy tasks (CON-INS) compared to large models.
- What evidence would resolve it: A comparison of zero-shot versus fine-tuned performance for 7B-parameter models on the CON-INS and BATCH tasks within the CPG-EVAL benchmark.

### Open Question 2
- Question: To what extent can the CPG-EVAL framework be generalized to evaluate pedagogical grammar competence in languages other than Chinese?
- Basis in paper: [explicit] The conclusion explicitly states the "necessity to continuously refine benchmark designs from diverse angles, including target instructional language."
- Why unresolved: This benchmark is specifically constructed using the "Chinese Grammar Learning Manual" (CGLM) and is tailored to the unique challenges of Teaching Chinese as a Second Language.
- What evidence would resolve it: Successful construction and evaluation of a similar multi-tiered benchmark (using SINGLE, BATCH, and CON-INS tasks) based on an English grammar framework like the English Grammar Profile.

### Open Question 3
- Question: Does high performance on recognition-based benchmark tasks correlate with the ability to generate accurate grammatical explanations or correct student errors?
- Basis in paper: [inferred] The paper notes that LLMs' capabilities are based on "statistical learning" rather than "human grammatical competence," and the benchmark primarily tests recognition (mapping) rather than the ability to teach or generate pedagogical content.
- Why unresolved: While the paper assesses the ability to *identify* grammar items (P-GPR), it does not evaluate the model's ability to *explain* the rules or correct mistakes, which is a core requirement for instructional alignment.
- What evidence would resolve it: A study correlating CPG-EVAL scores with human expert ratings of LLM-generated grammar explanations and error corrections for the same grammar items.

## Limitations

- The benchmark relies on synthetic data generated by DeepSeek-v3, even with human verification, which may not fully capture the diversity of learner errors or natural language variations encountered in real pedagogical contexts.
- Zero-shot evaluation, while avoiding prompt engineering bias, may underestimate model capabilities that could be unlocked through minimal instruction tuning or few-shot examples.
- The evaluation framework focuses on binary classification accuracy, which may not capture nuanced aspects of pedagogical grammar competence such as error explanation quality or adaptive scaffolding.

## Confidence

- **High Confidence:** The benchmark construction methodology (task design, distractor generation, human verification) is well-documented and follows established practices in linguistic evaluation.
- **Medium Confidence:** The performance patterns across model scales and task types are consistent and interpretable, though the absolute performance differences may be influenced by factors not controlled for in the study.
- **Low Confidence:** The generalizability of results to non-Chinese pedagogical contexts or different grammatical frameworks remains uncertain due to the specialized nature of the CGLM framework.

## Next Checks

1. Test whether models that score poorly on CPG-EVAL can be improved through targeted fine-tuning on pedagogical grammar datasets, and measure the correlation between benchmark scores and actual teaching effectiveness.
2. Replicate the benchmark using a different pedagogical grammar framework (e.g., from a different Chinese teaching tradition) to assess framework dependency.
3. Evaluate whether the identified performance gaps persist when models are given minimal examples in-context, helping distinguish between knowledge absence and instruction-following limitations.