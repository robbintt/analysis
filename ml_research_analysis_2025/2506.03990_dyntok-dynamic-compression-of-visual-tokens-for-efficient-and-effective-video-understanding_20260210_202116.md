---
ver: rpa2
title: 'DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video
  Understanding'
arxiv_id: '2506.03990'
source_url: https://arxiv.org/abs/2506.03990
tags:
- video
- visual
- tokens
- token
- dyntok
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the inefficiency of video understanding models
  that process large numbers of visual tokens from long videos. To reduce computational
  overhead, the authors propose DynTok, a dynamic token compression strategy that
  adaptively groups and merges adjacent visual tokens based on their similarity.
---

# DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding

## Quick Facts
- arXiv ID: 2506.03990
- Source URL: https://arxiv.org/abs/2506.03990
- Reference count: 9
- Primary result: Reduces video tokens to 44.4% size while maintaining accuracy; achieves 65.3% on Video-MME, 72.5% on MLVU

## Executive Summary
DynTok addresses the inefficiency of video understanding models that process large numbers of visual tokens from long videos. The authors propose a dynamic token compression strategy that adaptively groups and merges adjacent visual tokens based on their similarity, achieving high compression in low-information-density regions while preserving essential content. This approach maintains performance while significantly reducing computational overhead.

## Method Summary
DynTok is a dynamic token compression strategy for video understanding that merges adjacent visual tokens based on their similarity. The method processes video frames through a SigLIP encoder, applies bilinear pooling to reduce tokens from 28×28 to 14×14 per frame, then merges tokens within each row when their cosine similarity exceeds a threshold. A learnable grid marker token is appended per row, and the sequence is flattened for LLM processing. The approach is trained in two stages: first on single-image data following LLaVA-OneVision, then on a large video dataset with random similarity thresholds.

## Key Results
- Reduces visual tokens to 44.4% of original size while maintaining performance
- Achieves 65.3% accuracy on Video-MME benchmark
- Achieves 72.5% accuracy on MLVU benchmark

## Why This Works (Mechanism)
DynTok exploits temporal and spatial redundancy in video frames by dynamically grouping visually similar tokens. By merging adjacent tokens within rows based on cosine similarity of their CLIP features, the method reduces computational load while preserving semantic information. The adaptive threshold allows for aggressive compression in uniform regions while maintaining detail in complex areas.

## Foundational Learning
- **Visual token compression**: Reducing token count while preserving semantic information - needed to understand efficiency gains; quick check: compare token counts before/after compression
- **Cosine similarity in embedding space**: Measuring similarity between visual features - needed to grasp merging criteria; quick check: compute similarity scores between adjacent tokens
- **Two-stage training**: Separate pretraining on images then fine-tuning on videos - needed to understand training methodology; quick check: verify training progression through stages
- **Grid marker tokens**: Special tokens indicating token group boundaries - needed to understand spatial information preservation; quick check: confirm marker token placement after merging
- **Bilinear pooling**: Downsampling technique for token reduction - needed to understand initial compression; quick check: verify token dimensions after pooling
- **Random threshold sampling**: Varying compression during training - needed to understand robustness; quick check: track threshold distribution during training

## Architecture Onboarding

**Component Map**: SigLIP Encoder -> Bilinear Pooling -> DynTok (Similarity-based Merging) -> Grid Marker Addition -> LLM

**Critical Path**: Video frames → SigLIP → 28×28 tokens → Bilinear pooling → 14×14 tokens → DynTok similarity check → Token grouping/merging → Grid marker addition → 1D sequence → LLM

**Design Tradeoffs**: Row-wise merging preserves spatial relationships within rows but may disrupt vertical dependencies; random threshold sampling during training improves robustness but adds hyperparameter complexity

**Failure Signatures**: Zero-shot application without training causes OCR/action recognition drops; aggressive compression (Sth=0.4) degrades accuracy below baseline

**First Experiments**: 1) Verify token reduction works correctly at different similarity thresholds, 2) Test grid marker preservation of spatial information, 3) Measure computational savings vs accuracy trade-off

## Open Questions the Paper Calls Out
- Can DynTok be effectively generalized to single-image understanding tasks without degrading performance?
- Does coupling DynTok with layer-wise token reduction inside the LLM backbone yield cumulative efficiency gains?
- Does restricting token merging to horizontal rows limit performance on tasks requiring vertical spatial reasoning?

## Limitations
- Dataset composition and download procedures for video datasets are not fully specified
- Grid marker token implementation details are ambiguous
- Results may be sensitive to specific hyperparameters and training setup
- Performance on single-image tasks remains unexplored

## Confidence
- High: Core concept of dynamic token compression and overall architectural approach
- Medium: Reported performance metrics and token reduction results
- Low: Exact behavior and impact of grid marker tokens, robustness to varying thresholds

## Next Checks
1. Verify the composition and source of training datasets to confirm approximately 1.79M samples
2. Implement and test DynTok with multiple similarity thresholds to confirm reported trade-offs
3. Train the model for one epoch using specified hyperparameters and evaluate on the six benchmarks to confirm performance metrics