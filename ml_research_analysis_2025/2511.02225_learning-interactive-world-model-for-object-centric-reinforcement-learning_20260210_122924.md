---
ver: rpa2
title: Learning Interactive World Model for Object-Centric Reinforcement Learning
arxiv_id: '2511.02225'
source_url: https://arxiv.org/abs/2511.02225
tags:
- learning
- policy
- objects
- world
- interaction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FIOC-WM, a framework that jointly learns
  object-centric latents and their interaction structures from raw pixel observations
  using pre-trained vision encoders. It factors the state into dynamic and static
  components per object, learns interaction graphs to model pairwise object dynamics,
  and uses these to train an interaction-centric hierarchical policy.
---

# Learning Interactive World Model for Object-Centric Reinforcement Learning

## Quick Facts
- arXiv ID: 2511.02225
- Source URL: https://arxiv.org/abs/2511.02225
- Authors: Fan Feng; Phillip Lippe; Sara Magliacane
- Reference count: 40
- Key outcome: FIOC-WM framework jointly learns object-centric latents and interaction structures, achieving LPIPS reconstruction scores of 0.007-0.038 and success rates exceeding 0.81 in most generalization tasks on robotic and embodied-AI benchmarks.

## Executive Summary
This paper introduces FIOC-WM, a framework that jointly learns object-centric latents and their interaction structures from raw pixel observations using pre-trained vision encoders. It factors the state into dynamic and static components per object, learns interaction graphs to model pairwise object dynamics, and uses these to train an interaction-centric hierarchical policy. Experiments on robotic and embodied-AI benchmarks show that FIOC-WM improves policy learning sample efficiency and generalization over strong world-model baselines, with LPIPS reconstruction scores of 0.007-0.038 and success rates exceeding 0.81 in most generalization tasks.

## Method Summary
FIOC-WM combines pre-trained vision encoders with slot attention to extract object-centric representations, then factors these into dynamic (position, velocity) and static (color, shape) components using separate encoders. The framework learns interaction graphs via GRU encoders and Gumbel-softmax sampling to parameterize pairwise object dynamics, and uses these graphs to condition a transition model. A hierarchical policy decomposes tasks into interaction primitives, with a high-level policy selecting target interaction graphs and a low-level policy executing actions via MPC. The entire system is trained offline with object-centric reconstruction and interaction graph learning objectives.

## Key Results
- LPIPS reconstruction scores of 0.007-0.038 on benchmark tasks
- Success rates exceeding 0.81 in most generalization tasks
- Outperforms strong world-model baselines in sample efficiency and generalization
- Achieves nSHD scores indicating accurate interaction graph learning on synthetic data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factoring object states into dynamic and static components may reduce redundancy in world model learning by isolating time-varying properties from invariant attributes.
- Mechanism: Separate encoders extract dynamic features (position, velocity) and static features (color, shape). A temporal consistency loss L_static enforces that static features remain stable across timesteps, while contrastive loss separates static features across object slots.
- Core assumption: Objects possess attributes that can be cleanly categorized as either time-invariant or time-varying, and this factorization aligns with underlying dynamics.
- Evidence anchors:
  - [abstract] "It factors the state into dynamic and static components per object"
  - [section] Equation (1) and (2) define L_static and contrastive loss; "we regularize the output of fc(s) to remain temporally consistent"
  - [corpus] SPARTAN (2411.06890) supports that "explicitly represent[ing] the structure of interactions" improves adaptation
- Break condition: Environments where attributes shift categories (e.g., color changes due to lighting) or where static/dynamic boundaries are ambiguous.

### Mechanism 2
- Claim: Learning explicit interaction graphs appears to improve policy sample efficiency by modeling pairwise object dynamics rather than holistic state transitions.
- Mechanism: For each object pair (i,j), a GRU encoder produces pairwise embeddings u_ij that parameterize a distribution over interaction graphs G_t. These graphs modulate the transition prior p_s(d_t|d_{t-1}, a_{t-1}, G_t), allowing selective information flow only between interacting objects.
- Core assumption: Interactions are sparse (few objects interact per timestep) and can be captured by binary adjacency matrices.
- Evidence anchors:
  - [abstract] "learns interaction graphs to model pairwise object dynamics"
  - [section] "We represent interactions between objects with a sequence of time-varying graphs G={G_1, ..., G_T}"
  - [corpus] Object-Centric World Models for Causality-Aware RL (2511.14262) notes "most world models learn holistic representations" which "limits" performance in multi-object settings
- Break condition: Dense or continuous interactions (fluid dynamics, smoke) where binary graph representations oversimplify relationships.

### Mechanism 3
- Claim: Hierarchical decomposition of tasks into interaction primitives conditionally enables better generalization to novel skill compositions.
- Mechanism: High-level policy π_h selects target interaction graphs G^g_t; low-level policy π_l learns inverse dynamics to achieve those interactions. The high-level policy receives diversity rewards to explore novel graph configurations.
- Core assumption: Long-horizon tasks decompose into sequences of discrete interaction steps, and the space of useful interactions is tractably small.
- Evidence anchors:
  - [abstract] "decomposes tasks into composable interaction primitives"
  - [section] "The high-level policy π_h selects a sequence of intermediate subgoal graphs that gradually transform G_t into G^g"
  - [corpus] AXIOM (2505.24784) supports that "priors about objects and their interactions" improve data efficiency; corpus evidence on skill composition generalization is limited
- Break condition: Tasks requiring continuous coordination or where interaction primitives don't naturally compose (e.g., concurrent multi-object manipulation).

## Foundational Learning

- Concept: **Slot Attention**
  - Why needed here: Converts pre-trained visual embeddings into object-centric slots that serve as the basis for downstream factorization.
  - Quick check question: Can you explain how slot attention iteratively binds features to discrete slots through competitive attention?

- Concept: **Variational Autoencoders for Dynamics**
  - Why needed here: The world model uses VAE structure to learn latent states with regularized posteriors matching interaction-conditioned priors.
  - Quick check question: What role does the KL divergence loss L_KL play in aligning posterior and prior distributions?

- Concept: **Model Predictive Control (MPC) with Learned Models**
  - Why needed here: Low-level policy execution uses MPC to optimize action sequences by minimizing discrepancy between predicted and target states.
  - Quick check question: How does CEM (Cross-Entropy Method) sample and refine action sequences in MPC?

## Architecture Onboarding

- Component map: Pre-trained vision encoder (DINO-v2/R3M) -> Slot Attention -> VAE encoder (object latents) -> Attribute factorization (dynamic/static encoders) -> Interaction graph learner (GRU encoder + Gumbel-softmax/codebook) -> Transition model (p_s conditioned on G_t) -> Hierarchical policy (π_h for graph selection, π_l for action execution via MPC/PPO)

- Critical path: Slot Attention -> Attribute factorization -> Interaction graph -> Transition model. If interaction graphs are inaccurate, downstream policy learning degrades significantly (ablation: -18% success rate without interaction modeling).

- Design tradeoffs:
  - Variational masks (categorical) vs. codebook vs. conditional independence testing: Categorical achieves best nSHD but requires temperature tuning; CIT is parameter-free but threshold-sensitive.
  - Number of slots: Must exceed maximum object count; too many slots wastes capacity.
  - Offline vs. online world model updates: Paper defaults to frozen offline model; online tuning provides marginal gains but increases compute.

- Failure signatures:
  - High reconstruction error (LPIPS > 0.05): Likely slot attention failing to separate objects; check slot initialization.
  - Poor interaction graph accuracy (nSHD > 0.3): Temperature τ too high/low, or insufficient training data for rare interactions.
  - Policy doesn't generalize to new object counts: Slot number mismatch; retrain with appropriate slot count.

- First 3 experiments:
  1. **Validate slot separation**: Visualize slot attention masks on sample frames; confirm each object maps to one slot with minimal overlap.
  2. **Interaction graph sanity check**: On a simple 2-3 object environment with known ground-truth interactions (e.g., colliding balls), compare learned G_t against ground truth using nSHD.
  3. **Ablation of factorization**: Train without dynamic/static factorization; measure reconstruction quality (LPIPS) and single-task success rate degradation to quantify contribution.

## Open Questions the Paper Calls Out
- Can the FIOC-WM framework be effectively transferred to real-world robotic platforms? (Basis: authors list "extending the framework to real-world robotic settings" as primary future work)
- How well does the model generalize to novel object categories not present during world model training? (Basis: Limitations section states "interaction models primarily generalize to seen object categories")
- How robust is the hierarchical policy to errors in the object discovery module? (Basis: authors acknowledge framework "relies on a pretrained object-centric model for object discovery" as specific limitation)

## Limitations
- Strong assumptions about dynamic/static attribute categorization may not hold in all environments
- Binary interaction graph representation may oversimplify dense or continuous interactions
- Offline training protocol prevents world model adaptation to distribution shifts during policy learning
- Limited evaluation on complex real-world scenarios with visual noise and latency

## Confidence
- **High Confidence**: Core architecture components (slot attention, VAE dynamics, interaction graphs) are technically sound and well-supported by existing literature
- **Medium Confidence**: Benefits of dynamic/static factorization and hierarchical decomposition are demonstrated empirically but lack isolation ablation studies
- **Low Confidence**: Generalization claims based on limited test environments; performance on more diverse scenarios uncertain

## Next Checks
1. **Ablation of Factorization**: Train without dynamic/static factorization; measure reconstruction quality (LPIPS) and single-task success rate degradation to quantify contribution
2. **Interaction Graph Robustness**: Test on environments with non-sparse interactions (e.g., fluid dynamics, smoke) where binary graph representations may oversimplify relationships
3. **Distribution Shift Adaptation**: Evaluate online world model updates during policy learning to assess adaptation to novel object appearances or dynamics not seen during offline training