---
ver: rpa2
title: 'Stairway to Fairness: Connecting Group and Individual Fairness'
arxiv_id: '2508.21334'
source_url: https://arxiv.org/abs/2508.21334
tags:
- fairness
- group
- https
- individual
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the relationship between group fairness
  and individual fairness in recommender systems. Prior work has evaluated these fairness
  types using different measures, making proper comparison difficult.
---

# Stairway to Fairness: Connecting Group and Individual Fairness

## Quick Facts
- arXiv ID: 2508.21334
- Source URL: https://arxiv.org/abs/2508.21334
- Authors: Theresia Veronika Rampisela; Maria Maistro; Tuukka Ruotsalo; Falk Scholer; Christina Lioma
- Reference count: 40
- One-line primary result: Group fairness measures often mask unfairness within groups and between individuals, highlighting the need to evaluate both group and individual fairness to ensure truly fair recommender systems.

## Executive Summary
This study investigates the relationship between group fairness and individual fairness in recommender systems. Prior work has evaluated these fairness types using different measures, making proper comparison difficult. The authors evaluate both using the same families of measures (e.g., Gini Index, Standard Deviation) across three datasets (MovieLens-1M, JobRec, LFM-1B) and eight LLM-based recommender runs. The results show that group fairness measures often mask unfairness within groups and between individuals, highlighting the need to evaluate both group and individual fairness to ensure truly fair recommender systems.

## Method Summary
The study evaluates LLM-based recommender systems using three datasets: MovieLens-1M, JobRec, and LFM-1B. Users are grouped by sensitive attributes (gender, age, occupation/degree/country) and intersectional combinations thereof. The authors compute effectiveness metrics (HR, MRR, P@10, NDCG@10) and fairness measures (Min, Range, SD, MAD, Gini, CV, FStat, KL, GCE, Atkinson Index) at both group and individual levels. Eight LLM runs are tested using different models (Llama-3.1-8B, Qwen2.5-7B, GLM-4-9B, Ministral-8B) with sensitive and non-sensitive prompts.

## Key Results
- No individual fairness measure consistently agrees with group fairness measures in ranking recommender models
- Even when recommendations are fair for groups, they can be highly unfair for individuals
- Within-group unfairness is consistently worse than between-group unfairness, yet individual fairness is comparable to within-group fairness regardless of how users are grouped

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregating effectiveness scores to the group level masks individual-level disparities that persist within groups.
- Mechanism: Group fairness measures compute average effectiveness per group, then compare across groups. When groups have similar means but high internal variance, the between-group metric appears fair while many individuals receive poor recommendations. The Atkinson Index decomposes into between-group and within-group components without residuals, revealing this hidden unfairness.
- Core assumption: Fairness requires equitable outcomes at the individual level, not just balanced group averages.
- Evidence anchors:
  - [abstract] "group fairness measures often hide unfairness within groups and between individuals"
  - [Figure 1] Shows two groups with similar mean NDCG (0.546 vs 0.471) but individual Gini of 0.446, revealing high within-group variance masked by group aggregation
  - [corpus] "Trade-offs Between Individual and Group Fairness" review confirms this tension is recognized across ML domains, though empirical RS evidence was lacking
- Break condition: If recommendation effectiveness has very low variance across all users, group and individual measures would converge; this mechanism operates primarily when effectiveness distributions are skewed.

### Mechanism 2
- Claim: Intersectional group fairness is strictly harder to achieve than single-attribute group fairness.
- Mechanism: As sensitive attributes are combined (e.g., gender × age × occupation), the number of intersectional groups grows multiplicatively. Maintaining similar effectiveness across 29–36 intersectional groups requires satisfying more constraints than across 2–7 single-attribute groups. The fairness scores systematically worsen as more attributes are considered.
- Core assumption: The underlying recommendation model does not explicitly optimize for intersectional fairness constraints.
- Evidence anchors:
  - [Section 3, RQ2] "fairness worsens as more attributes are used to form groups, highlighting the importance of considering intersectionality"
  - [Figure 3] Shows monotonic increase in SD/Gini/Atk unfairness as grouping moves from 1 attribute (2–7 groups) to 3 attributes (12–36 groups)
  - [corpus] Weak direct evidence—corpus papers address individual/group tradeoffs but not specifically intersectional complexity scaling
- Break condition: If group sizes remain large enough that law of large numbers smooths effectiveness estimates, intersectional penalties may be smaller.

### Mechanism 3
- Claim: Individual fairness tracks within-group fairness, not between-group fairness, across different grouping strategies.
- Mechanism: Regardless of how users are partitioned into groups, the variance within each group remains high and accounts for most of the individual-level unfairness. Between-group disparities are smaller than within-group disparities, so individual fairness scores (Gini_ind, Atk_ind) correlate more strongly with within-group components than between-group components.
- Core assumption: User groups defined by sensitive attributes do not fully explain effectiveness variance—other factors (interaction history, item popularity) dominate.
- Evidence anchors:
  - [Section 3, RQ4] "within-group unfairness is almost as high as individual unfairness, and always higher than between-group unfairness"
  - [Figure 4] For JobRec and LFM-1B, within-group bars nearly equal individual bars across all grouping strategies; between-group bars are consistently lower
  - [corpus] "On the use of graph models to achieve individual and group fairness" suggests structural relationships matter, supporting the idea that grouping strategy alone doesn't capture individual variance sources
- Break condition: If sensitive attributes are highly predictive of effectiveness (e.g., occupation strongly determines job recommendation quality), between-group variance would dominate.

## Foundational Learning

- Concept: **Gini Index decomposition**
  - Why needed here: The paper relies on Gini (and Atkinson Index) as measures that can be computed for both groups and individuals and decomposed into between/within components. Understanding that Gini = Gini_between + Gini_within enables interpreting Figure 4.
  - Quick check question: If Gini_between = 0.05 and Gini_within = 0.40, what is the total Gini, and which component dominates unfairness?

- Concept: **Intersectionality in fairness**
  - Why needed here: The paper evaluates fairness for intersectional groups (gender × age × occupation) and shows fairness degrades with more attributes. This requires understanding that intersectional groups are defined by multiple simultaneous sensitive attributes.
  - Quick check question: With 2 genders, 3 age bands, and 5 countries, what is the maximum number of intersectional groups possible?

- Concept: **LLM-as-recommender prompting strategies**
  - Why needed here: The experiments use LLMRecs with sensitive vs. non-sensitive prompts. The paper finds minimal difference between them, suggesting including sensitive attributes in prompts doesn't automatically improve or worsen fairness.
  - Quick check question: What are the two prompt types used, and what components differ between them?

## Architecture Onboarding

- Component map:
  - Data preprocessing: Load dataset → filter users/items by interaction count → temporal train/val/test split → remove cold-start users from val/test
  - User grouping: Map sensitive attributes → discretize continuous attributes (age, experience) → form non-overlapping groups for each attribute combination
  - LLMRec inference: Build ICL prompt with train history + val examples → generate top-k recommendations → fuzzy string matching to align with catalog
  - Effectiveness computation: Per-user HR/MRR/P@k/NDCG → aggregate to group means
  - Fairness computation: Compute base scores (P, NDCG) per user → aggregate via SD/Gini/Atk/CV/etc. for groups and individuals → decompose where applicable

- Critical path:
  1. Temporal splitting must precede LLMRec inference to prevent data leakage
  2. Fuzzy matching threshold (≥0.75) gates whether generated items count as hits
  3. Fairness measure selection determines whether within-group variance is visible (Gini/Atk: yes; Min/Range: no)

- Design tradeoffs:
  - **5-core vs 50-core filtering**: 5-core preserves more users but increases sparsity; 50-core reduces noise but may exclude minority user segments
  - **Sensitive vs. non-sensitive prompts**: Including attributes enables fairness analysis but showed minimal effectiveness difference; may introduce model-specific biases
  - **Cut-off k=10**: Standard but arbitrary; different k values may change fairness rankings

- Failure signatures:
  - **Zero Min scores**: Common when most users have P=0 or NDCG=0 (sparse domains like JobRec); Min becomes uninformative
  - **KL/GCE explosion**: These measures can reach extreme values (e.g., 659–2052 in Table 2) when effectiveness distributions are sparse or zero-inflated
  - **Empty intersectional groups**: Some attribute combinations may have 0–1 users; the paper excludes empty groups but notes min subgroup size of 1–2 in some datasets

- First 3 experiments:
  1. Replicate Figure 1 on a held-out subset: Compute group-level NDCG and Gini for two user segments, then compute individual-level Gini to confirm within-group variance exceeds between-group variance.
  2. Ablate grouping granularity: For a single model/dataset, compute fairness scores with 1, 2, and 3 attributes to verify the "stairway" pattern (fairness degrades with more intersectional groups).
  3. Measure agreement across families: Compute Kendall's τ between Gini_ind and all group measures to confirm no individual measure is a reliable proxy for group fairness (target τ < 0.9 for all).

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow scope of recommender model families tested (only LLM-based systems)
- Focus on effectiveness-fairness trade-offs without exploring whether certain recommendation algorithms inherently produce fairer outcomes
- Assumes sensitive attributes are available and known, which may not hold in real-world deployments

## Confidence
- **High Confidence**: The finding that no individual fairness measure consistently agrees with group fairness measures in ranking models is robust across datasets and model types.
- **Medium Confidence**: The observation that within-group unfairness consistently exceeds between-group unfairness, as this depends on the specific attribute distributions and dataset characteristics.
- **Medium Confidence**: The claim that individual fairness is comparable to within-group fairness regardless of grouping strategy, though this may vary with different attribute granularities.

## Next Checks
1. **Cross-method validation**: Test whether the observed disagreement between group and individual fairness measures holds for traditional collaborative filtering and content-based recommenders, not just LLMs.
2. **Intersectional group size sensitivity**: Systematically vary the minimum group size threshold for intersectional groups to determine at what point the fairness degradation pattern breaks down due to statistical noise.
3. **Temporal fairness stability**: Track fairness measures across multiple time periods to verify whether the observed patterns persist or shift as user interaction patterns evolve.