---
ver: rpa2
title: 'They want to pretend not to understand: The Limits of Current LLMs in Interpreting
  Implicit Content of Political Discourse'
arxiv_id: '2506.06775'
source_url: https://arxiv.org/abs/2506.06775
tags:
- implicit
- content
- correct
- political
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates the ability of Large Language Models (LLMs)
  to interpret manipulative implicit content in Italian political discourse. Using
  the IMPAQTS corpus, which contains annotated political speeches, researchers tested
  models on two tasks: multiple-choice selection of correct explanations and open-ended
  generation of explanations.'
---

# They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse

## Quick Facts
- arXiv ID: 2506.06775
- Source URL: https://arxiv.org/abs/2506.06775
- Reference count: 40
- Primary result: GPT4o-mini correctly explains implicit political content in only 27% of cases, falling far short of human performance

## Executive Summary
This study investigates Large Language Models' ability to interpret manipulative implicit content in Italian political discourse using the IMPAQTS corpus. Researchers tested models on two tasks: multiple-choice selection of correct explanations and open-ended generation of explanations. Results show that even the best-performing model (GPT4o-mini) achieves only 27% fully correct explanations, falling more than 20 percentage points short of estimated human performance. Chain-of-Thought prompting significantly improves results, suggesting reasoning mechanisms can aid interpretation. The findings highlight current LLMs' limited pragmatic capabilities in understanding highly implicit language and point toward future work involving contextual and world knowledge enrichment.

## Method Summary
The study evaluates four LLMs (GPT4o-mini, Aya Expanse 8B, LLAMA3.1 8B, LLAMA3.2 3B) on the IMPAQTS-PID dataset containing 31,822 annotated political speech samples. Two tasks are used: Multiple-Choice Generation (MCG) where models select correct explanations from four options, and Open-Ended Generation (OEG) where models generate free-form explanations. Three prompting strategies are tested for OEG: zero-shot, few-shot (4 examples), and Chain-of-Thought (CoT) with reasoning instructions. MCG uses topic modeling to generate distractors from same topic class, while OEG is evaluated by human experts on a 5-point scale. Greedy decoding is used with temperature=0, and token limits are 25 for MCG and 500/1000 for OEG.

## Key Results
- GPT4o-mini achieves 70% accuracy on MCG but only 27% "Totally Correct" on OEG with Chain-of-Thought prompting
- Chain-of-Thought prompting significantly improves open-ended generation performance across all models tested
- Performance varies with distractor plausibility, showing approximately 8% accuracy difference between hard and easy negative distractors
- LLAMA models exhibit refusal patterns and positional biases not seen in other models
- Current models fall more than 20 percentage points short of estimated human-level performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Chain-of-Thought prompting enhances implicit content interpretation by eliciting structured reasoning.
- Mechanism: CoT prompting provides step-by-step guidance adapted from expert annotation instructions, potentially directing model attention to relevant pragmatic cues that zero-shot prompting miss.
- Core assumption: Performance improvement stems from reasoning scaffolding rather than prompt length or example quantity.
- Evidence anchors: Abstract states "Chain-of-Thought prompting improved performance, suggesting reasoning mechanisms can aid interpretation"; Section 5.2 shows "The use of the CoT strategy leads to a noticeable improvement... totally correct answers increase."

### Mechanism 2
- Claim: Current LLMs exhibit limited pragmatic capabilities for manipulative implicit content due to insufficient integration of contextual and world knowledge.
- Mechanism: Models process linguistic patterns but lack mechanisms to incorporate speaker identity, political context, historical background, and real-world knowledge needed to resolve context-dependent implicatures.
- Core assumption: Performance gap between LLMs and humans stems from missing knowledge integration rather than fundamental architectural limitations.
- Evidence anchors: Abstract notes "current LLMs' limited pragmatic capabilities in understanding highly implicit language"; Section 5.1 states "even the best-performing model still falls more than 20 percentage points short of the estimated ceiling accuracy."

### Mechanism 3
- Claim: Implicit content interpretation performance varies with distractor plausibility, suggesting models may rely on surface-level similarity rather than deep pragmatic understanding.
- Mechanism: When distractors share topic similarity with correct answers, models perform worse, indicating potential reliance on semantic matching strategies that break down when fine-grained pragmatic distinctions are required.
- Core assumption: Performance degradation with harder distractors reflects reasoning limitations rather than evaluation artifact.
- Evidence anchors: Section 5.1 shows "GPT4o-mini shows an approximately 8% performance difference between [hard and easy negatives] subsets"; "distractor selection can significantly impact model performance."

## Foundational Learning

- **Pragmatics vs. Semantics**
  - Why needed here: The paper distinguishes between literal meaning (semantics) and implied meaning (pragmatics), central to understanding why models fail at interpreting implicatures and presuppositions.
  - Quick check question: Can you explain why "Italy needs another government" (semantics) differs from "Monti's government served the banks" (pragmatic implicature)?

- **Conversational Implicature & Presupposition**
  - Why needed here: These are the two primary implicit content types tested. Understanding Gricean maxims and presupposition triggers is essential for interpreting the task design and results.
  - Quick check question: How does "stopped smoking" presuppose prior smoking, versus implying it through context?

- **Chain-of-Thought Prompting**
  - Why needed here: This technique shows the most promise for improving performance. Understanding its mechanism helps assess whether improvements reflect genuine reasoning gains or prompt engineering.
  - Quick check question: Why might step-by-step instructions about conversational maxims help a model identify implicatures that zero-shot prompting misses?

## Architecture Onboarding

- Component map: Raw political text -> Context window construction -> Implicit content detection -> Pragmatic inference -> Explanation generation
- Critical path: The pragmatic inference step lacks explicit architectural support, creating the bottleneck
- Design tradeoffs: Model scale vs. deployment cost (tested 3B-8B models); CoT prompt length vs. inference speed; task framing: MCG enables automatic evaluation but introduces distractor artifacts
- Failure signatures: Attribute implicit content to wrong entities; generate plausible but incorrect explanations; refuse to answer politically sensitive content; conflate implicatures with presuppositions
- First 3 experiments:
  1. Baseline replication: Test GPT4o-mini on 100 random samples with zero-shot, few-shot, and CoT prompting to verify reported performance gaps (21-27% fully correct)
  2. Context ablation: Systematically vary number of preceding sentences (0-4) to quantify contextual dependency
  3. Knowledge enrichment pilot: For 50 samples, prepend explicit metadata (speaker, date, political party) and measure performance change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does enriching LLMs with specific extra-linguistic context—such as speaker identity, political affiliation, and historical setting—significantly improve their ability to interpret manipulative implicit content in political discourse?
- Basis in paper: Authors state they plan to explore enriching LLMs with contextual and world knowledge to strengthen interpretive capacities
- Why unresolved: Current study isolated text from external metadata, resulting in models missing inferences requiring specific real-world knowledge
- What evidence would resolve it: Experiment comparing baseline OEG performance against prompts including metadata fields for speaker, date, and party affiliation

### Open Question 2
- Question: How does the reasoning profile of humans interpreting political implicit content compare to that of LLMs, particularly regarding reliance on linguistic cues versus world knowledge?
- Basis in paper: Authors note in Limitations that a parallel study on human understanding and explanation capabilities should be developed
- Why unresolved: Study only estimated human ceiling performance but didn't analyze qualitative reasoning paths humans take
- What evidence would resolve it: A "think-aloud" study with human annotators performing the same OEG task, followed by comparative error analysis

### Open Question 3
- Question: Can a systematic, rule-based method for generating distractors using an external LLM improve the validity of the Multiple-Choice Generation (MCG) task?
- Basis in paper: Authors acknowledge distractor selection significantly impacts model performance and plan to design more systematic distractor generation
- Why unresolved: Current topic-similarity method produced performance variance suggesting models sometimes relied on surface-level topic matching
- What evidence would resolve it: New iteration of MCG task utilizing rule-based distractors that correlates more strongly with OEG performance

### Open Question 4
- Question: Can a reliable automated evaluation metric be established for the Open-Ended Generation task that correlates strongly with expert human judgment?
- Basis in paper: Authors relied on human expert evaluation for 150 samples because automatic measures introduce uncertainty regarding reliability
- Why unresolved: Lack of scalable automated evaluator limited OEG experiment scope
- What evidence would resolve it: Development and validation of specialized "LLM-as-a-judge" prompt achieving high correlation (>0.8) with human grading criteria

## Limitations

- Task design sensitivity to distractor construction affects result validity, with hard-negative distractors reducing accuracy by approximately 8%
- Lack of direct comparison with human performance on the same dataset prevents quantifying true performance gap
- OEG task's human evaluation introduces inter-rater variability not fully characterized
- Findings may not transfer to other languages or political discourse domains

## Confidence

**High Confidence:**
- Chain-of-Thought prompting improves performance across all models tested in open-ended generation tasks
- Model performance varies significantly with distractor plausibility, with hard-negative distractors reducing accuracy by approximately 8%
- LLAMA models exhibit refusal patterns and positional biases not seen in other models

**Medium Confidence:**
- GPT4o-mini achieves 70% MCG accuracy and 27% OEG "Totally Correct" rate represents current ceiling for implicit content interpretation
- Context window (up to 4 preceding sentences) is essential for performance but optimal context length remains unclear
- 20+ percentage point gap between best model and estimated human performance reflects genuine pragmatic reasoning limitations

**Low Confidence:**
- Whether performance improvements stem from reasoning scaffolding versus prompt engineering remains unresolved
- Transferability of findings to other languages or political discourse domains is unestablished
- Long-term architectural modifications versus prompt engineering represent optimal path forward

## Next Checks

1. **Human Benchmark Validation:** Have three independent political discourse experts evaluate 100 randomly selected samples using the same OEG criteria. Compare inter-rater agreement and performance distribution against GPT4o-mini's 27% "Totally Correct" rate to establish true human performance ceiling.

2. **Context Window Optimization:** Systematically test context window lengths (0, 1, 2, 3, 4 preceding sentences) on 200 samples to identify the point of diminishing returns. If performance plateaus at 2 sentences, it would validate the current approach while suggesting optimization opportunities.

3. **Metadata Enrichment Experiment:** For 100 samples, prepend explicit metadata (speaker identity, political party, year, legislative context) and measure performance change. A 10%+ improvement would directly support the world-knowledge integration hypothesis and suggest a viable path for immediate performance gains without architectural modifications.