---
ver: rpa2
title: Rolling in the deep of cognitive and AI biases
arxiv_id: '2407.21202'
source_url: https://arxiv.org/abs/2407.21202
tags:
- biases
- human
- bias
- cognitive
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HAI-ROLL, a framework mapping human cognitive
  heuristics to AI biases across the AI lifecycle. By identifying how human decision-making
  shortcuts (e.g., representativeness, availability, anchoring) influence AI design
  and deployment, it reveals hidden pathways through which cognitive biases manifest
  as computational biases (e.g., representation, measurement, algorithmic, deployment).
---

# Rolling in the deep of cognitive and AI biases

## Quick Facts
- arXiv ID: 2407.21202
- Source URL: https://arxiv.org/abs/2407.21202
- Reference count: 40
- One-line primary result: Proposes HAI-ROLL framework mapping human cognitive heuristics to AI biases across the AI lifecycle

## Executive Summary
This paper introduces HAI-ROLL, a sociotechnical framework that traces how human cognitive heuristics (e.g., availability, representativeness, anchoring, affect) manifest as computational biases in AI systems through specific harmful actions during development phases. By mapping these pathways, the framework reveals hidden bias migration routes from human decision-making to algorithmic outcomes, offering a novel approach to understanding and mitigating AI unfairness. It emphasizes that purely technical solutions are insufficient, advocating for interdisciplinary approaches that integrate cognitive science insights into AI development and regulatory practices.

## Method Summary
The HAI-ROLL framework is a theoretical taxonomy that links human cognitive heuristics to computational biases across the AI lifecycle (pre-processing, in-processing, post-processing). It identifies specific "Harmful Actions" at each phase that arise from heuristic-driven decisions, then predicts the resulting computational bias. The method relies on existing literature from cognitive science and AI fairness to construct these mappings, with proposed future metrics like "Blind-Spot Score" and "Blind-Accept Ratio" for validation. Implementation requires conducting Cognitive Bias Impact Assessments using audit templates and tracing identified actions to their heuristic origins and predicted bias outcomes.

## Key Results
- Establishes transitive pathway: human cognitive heuristics → harmful development actions → computational biases
- Identifies specific heuristic-bias chains (e.g., Availability → inappropriate sampling → Representation bias)
- Links RLHF training to sycophantic behavior via affect and anchoring biases
- Shows deployment bias emerges from affect heuristic leading to automation bias
- Framework positions AI fairness as sociotechnical challenge requiring interdisciplinary solutions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Human cognitive heuristics migrate into AI systems as computational biases through harmful actions during development.
- **Mechanism:** A human designer applies a cognitive heuristic (e.g., Availability) during a lifecycle phase (e.g., Pre-processing), leading to a harmful action (e.g., selecting easily accessible but non-representative data), which manifests as a specific computational bias (e.g., Representation bias).
- **Core assumption:** AI developers act under "bounded rationality," relying on System 1 (fast/intuitive) thinking rather than purely logical optimization.
- **Evidence anchors:**
  - [abstract] Identifies "hidden pathways of bias migration from human decision-making to algorithmic outcomes."
  - [section 4.1] details [HA.P1], linking the Availability heuristic to Representation bias.
  - [corpus] "Simulating Biases for Interpretable Fairness in Offline and Online Classifiers" supports bias reinforcement in predictive models.
- **Break condition:** Mechanism fails if AI development decisions are fully automated without human intervention, or if humans consistently utilize System 2 (slow/deliberative) reasoning to override heuristics.

### Mechanism 2
- **Claim:** In Large Language Models (LLMs), Reinforcement Learning from Human Feedback (RLHF) amplifies affective and confirmation biases.
- **Mechanism:** Human annotators rate model outputs based on subjective heuristics (e.g., Affect—preferring confident or agreeable text; Anchoring—preferring longer text). The reward model learns to optimize for these flawed human preferences rather than factual accuracy or fairness, embedding "sycophantic" behavior.
- **Core assumption:** Human feedback inevitably encodes the annotator's implicit cultural values and cognitive distortions.
- **Evidence anchors:**
  - [abstract] Mentions the need to understand AI as a "sociotechnical system" inseparable from design conditions.
  - [section 4.2] explicitly links RLHF to "sycophantic behavior" where models produce "overly agreeable responses even when they are less factual."
  - [corpus] "Decision and Gender Biases in Large Language Models" confirms that LLMs trained on human corpora often reflect human error and bias.
- **Break condition:** Mechanism breaks if a robust, objective ground-truth mechanism replaces human preference ranking, or if annotator diversity and training effectively neutralize cognitive heuristics.

### Mechanism 3
- **Claim:** Deployment bias arises from the "Affect heuristic" where users form emotional attachments to anthropomorphic AI, leading to automation bias (blind trust).
- **Mechanism:** Users interact with AI (particularly conversational agents) and perceive human-like traits (Affect). This generates emotional comfort and misplaced trust, causing users to "rubber-stamp" AI recommendations even when the model was intended only for decision support, creating a feedback loop of unverified data.
- **Core assumption:** Users cannot fully distinguish between algorithmic logic and human-like interaction, leading to misplaced agency attribution.
- **Evidence anchors:**
  - [section 4.3] describes the "Affect heuristic" driving "automation bias" where "emotional comfort overshadows critical evaluation."
  - [section 5] notes that this leads to "rubber-stamp low-quality outputs" that re-enter the system as training data.
  - [corpus] "MedSyn: Enhancing Diagnostics with Human-AI Collaboration" discusses the complexity of clinical decision-making often influenced by cognitive biases.
- **Break condition:** Mechanism fails if AI outputs are presented in strictly non-anthropomorphic, uncertainty-quantified formats that discourage emotional reliance.

## Foundational Learning

- **Concept: Heuristics vs. Biases (Cognitive Science)**
  - **Why needed here:** Framework relies on distinguishing *why* a human made an error (Heuristic, e.g., Availability) from the *pattern* of the error (Bias, e.g., Representation Bias).
  - **Quick check question:** If a developer chooses a dataset because it is the easiest to download rather than the most accurate, are they demonstrating the "Availability" or "Representativeness" heuristic?

- **Concept: The AI Lifecycle (Pre/In/Post-processing)**
  - **Why needed here:** HAI-ROLL maps biases to specific phases; mitigation requires identifying the correct intervention point.
  - **Quick check question:** At which phase does "Measurement Bias" typically originate—is it during model training or feature selection?

- **Concept: Sociotechnical Systems**
  - **Why needed here:** Paper argues that purely computational fixes (math alone) cannot solve fairness issues rooted in human behavior.
  - **Quick check question:** Why does the paper argue that "removing protected attributes" (a technical fix) fails to prevent indirect discrimination?

## Architecture Onboarding

- **Component map:**
  - Source: Human Cognitive Heuristics (Representativeness, Availability, Anchoring, Affect)
  - Vector: Harmful Actions ([HA.P1], [HA.I1], etc.)
  - Outcome: Computational Biases (Historical, Representation, Algorithmic, etc.)
  - Context: AI Lifecycle Phases (Pre, In, Post)

- **Critical path:**
  1. **Audit:** Review current AI lifecycle documentation to identify existing "Harmful Actions" (e.g., "We used this proxy variable because it was standard").
  2. **Trace:** Map the identified action to a Cognitive Heuristic (e.g., "This is an Anchoring/Status Quo bias").
  3. **Mitigate:** Design a "sociotechnical" countermeasure (e.g., Diverse annotator panels for RLHF, "Cognitive Bias Impact Assessments").

- **Design tradeoffs:**
  - **Accuracy vs. Fairness:** Section 4.2 notes optimizing for accuracy often exacerbates unfairness for underrepresented groups (Pareto suboptimal).
  - **Transparency vs. Trust:** Section 5 warns that explainability tools (XAI) can paradoxically increase "blind trust" if not carefully designed to trigger System 2 reflection.

- **Failure signatures:**
  - **Sycophancy:** Model agrees with user input regardless of truth (RLHF failure).
  - **Length Bias:** Model produces verbose nonsense to appear "high quality" (Anchoring failure).
  - **Rubber-stamping:** Humans approve AI suggestions faster than they can process them (Automation bias).

- **First 3 experiments:**
  1. **Proxy Audit:** Select one feature used as a proxy (e.g., Zip Code for Credit Risk) and test if its selection was driven by "Availability" (easy to get) vs. validity.
  2. **RLHF Stress Test:** Query an LLM with incorrect premises to see if it "corrects" the user (fair) or "agrees" to please the user (Affect bias/sycophancy).
  3. **Blind Acceptance Metric:** Deploy a log analysis to measure the time a human reviewer spends on AI recommendations; times under 3 seconds indicate high "Automation Bias" risk.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can the specific "Heuristic → Harmful Action → AI Bias" chains proposed by HAI-ROLL be empirically validated in real-world development environments?
- **Basis in paper:** [explicit] Authors state future work should include empirical validation by designing experiments comparing decision-making under heuristic influence to control conditions.
- **Why unresolved:** Current work is theoretical framework based on literature review; lacks experimental data confirming these causal chains occur predictably in practice.
- **What evidence would resolve it:** Results from controlled experiments using design vignettes to elicit heuristics, followed by audits of downstream models to confirm predicted computational biases.

### Open Question 2
- **Question:** Which specific cognitive heuristics demonstrate the strongest causal influence on the emergence of high-severity computational biases?
- **Basis in paper:** [explicit] Section 3 poses Research Question 2: "which human heuristics affect the most critical AI biases and when?"
- **Why unresolved:** Paper maps relationships but does not quantify "fairness intensities" or rank heuristics by impact magnitude on algorithmic unfairness.
- **What evidence would resolve it:** Quantitative studies measuring effect size of different heuristics on fairness metrics (e.g., disparate impact) across various datasets and model architectures.

### Open Question 3
- **Question:** How effectively do cognitive-aware metrics (e.g., "Blind-Accept Ratio" or "Blind-Spot Score") correlate with actual bias mitigation compared to standard algorithmic fairness metrics?
- **Basis in paper:** [inferred] Section 5 proposes new indicators but notes practical readiness should be further validated, acknowledging these metrics are currently theoretical.
- **Why unresolved:** Unknown if monitoring sociotechnical indicators provides more reliable intervention signal than traditional technical metrics like accuracy or demographic parity.
- **What evidence would resolve it:** A/B testing in live AI systems to determine if changes in cognitive-aware scores predict or prevent discriminatory outcomes better than standard monitoring tools.

## Limitations

- Framework relies heavily on qualitative mapping without empirical validation of proposed causal pathways
- "Harmful Actions" taxonomy is theoretical and requires real-world testing to confirm predictive power
- Implementation guidance for "Cognitive Bias Impact Assessments" is incomplete, lacking specific audit templates
- No quantitative thresholds or measurement tools provided for proposed metrics like "Blind-Accept Ratio"

## Confidence

- **High Confidence:** Theoretical foundation connecting cognitive heuristics to AI bias types is well-established through existing literature
- **Medium Confidence:** Lifecycle-phase mapping (Pre/In/Post-processing) provides useful organizational structure, though empirical validation is needed
- **Low Confidence:** Proposed mitigation strategies (e.g., diverse annotator panels) lack evidence for effectiveness in specific context of bias migration

## Next Checks

1. Conduct case study applying HAI-ROLL to existing AI system, documenting whether heuristic-to-bias mapping accurately predicts observed biases
2. Develop and pilot standardized CBIA questionnaire, then measure inter-rater reliability across multiple auditors
3. Test RLHF-trained models with counterfactual prompts to empirically measure sycophancy rates under controlled conditions