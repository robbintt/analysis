---
ver: rpa2
title: Language Generation with Infinite Contamination
arxiv_id: '2511.07417'
source_url: https://arxiv.org/abs/2511.07417
tags:
- density
- generation
- enumeration
- language
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies language generation in the limit under contaminated
  data, where an algorithm observes an adversarial enumeration of strings from an
  unknown target language and must eventually generate new, unseen strings from that
  language. The key contribution is characterizing how much contamination (noisy insertions
  and omissions) language generation can tolerate.
---

# Language Generation with Infinite Contamination

## Quick Facts
- arXiv ID: 2511.07417
- Source URL: https://arxiv.org/abs/2511.07417
- Reference count: 34
- Primary result: Characterizes how much contamination language generation can tolerate, showing generation possible under o(1)-noise but not dense generation

## Executive Summary
This paper studies language generation in the limit under contaminated data, where an algorithm observes an adversarial enumeration of strings from an unknown target language and must eventually generate new, unseen strings from that language. The key contribution is characterizing how much contamination (noisy insertions and omissions) language generation can tolerate. The main results show that under o(1)-noise (noise fraction converging to zero) and arbitrary omissions, all countable collections are generable in the limit, while under constant noise rate c and arbitrary omissions, generability requires specific conditions. The paper also introduces a beyond-worst-case model inspired by curriculum learning where bounded displacement allows dense generation even with infinite contamination.

## Method Summary
The paper develops algorithms for language generation under contaminated data using three main approaches: finite expansion for finite contamination (reducing to standard generation by expanding candidate languages), priority-based intersection algorithms for infinite/vanishing noise (dynamically re-ordering languages based on empirical consistency), and bounded displacement models for dense generation (constraining adversarial enumeration to follow curriculum-like orderings). The algorithms rely on membership and density oracles to track empirical noise rates and maintain language priorities, with the key insight that target languages eventually stabilize in priority while inconsistent languages are deprioritized.

## Key Results
1. Under o(1)-noise and arbitrary omissions, all countable collections are generable in the limit
2. Dense generation is strictly less robust to contamination than standard generation - no countable collection can achieve non-trivial density under constant noise or o(1)-noise with arbitrary omissions
3. Dense generation is achievable under infinite contamination if the input stream respects bounded displacement (curriculum-like ordering)

## Why This Works (Mechanism)

### Mechanism 1: Finite Expansion for Finite Contamination
- **Claim:** Language generation under finite contamination can be reduced to standard generation by expanding the collection of candidate languages.
- **Mechanism:** The algorithm constructs an expanded collection fL consisting of all original languages plus their finite modifications (unions with finite noise sets and differences by finite omission sets). A standard generator run on fL treats the contaminated input stream as a clean stream of a modified language K_{A,B}.
- **Core assumption:** The noise and omissions are strictly finite; the generator has oracle access to the original collection.
- **Evidence anchors:**
  - [abstract] "resolves an open question... by showing that generation is possible with only membership oracle access under finitely many contaminated examples."
  - [Section 4.2] "construct the expanded collection fL using the expansion subroutine... corresponds to an enumeration of K_{A,B} without noise or omissions."
  - [corpus] Limited direct corpus support for this specific theoretical reduction; related work focuses on practical contamination detection (e.g., arXiv:2505.22251).
- **Break condition:** Fails immediately if contamination is infinite or if the generator cannot track the expanded countable collection indices.

### Mechanism 2: Priority-Based Intersection for Infinite/Vanishing Noise
- **Claim:** Robustness to infinite contamination ($o(1)$-noise) is achievable by dynamically re-ordering candidate languages based on empirical consistency rather than static indices.
- **Mechanism:** The algorithm assigns a priority to each language that penalizes it whenever the empirical noise rate exceeds a threshold. By taking the intersection of the highest-priority languages (that yield an infinite set), the algorithm filters out languages that fluctuate inconsistently with the observed stream while retaining the target language which eventually stabilizes in priority.
- **Core assumption:** The target language $K$ has a noise rate that eventually falls below specific thresholds (e.g., $c_i$); the noise rate of non-target languages fluctuates enough to deprioritize them.
- **Evidence anchors:**
  - [Section 4.1] "Our algorithm assigns each language a priority... each time Li fails our check we penalize the language by decreasing its priority."
  - [Section 5.1] "The key observation is that for both c-noise and o(1)-noise, the target language K will only be penalized for a finite amount of time."
  - [corpus] This theoretical priority mechanism lacks direct empirical validation in the provided corpus of practical LLM contamination studies.
- **Break condition:** Fails if constant noise rate $c$ is too high relative to the separation between languages in the collection.

### Mechanism 3: Bounded Displacement for Dense Generation
- **Claim:** Dense generation (covering a positive fraction of the target language) is achievable under infinite contamination if the input stream respects a "curriculum" ordering (bounded displacement).
- **Mechanism:** This constrains the adversary such that the $n$-th input element cannot be "too far" from the $n$-th element in a canonical ordering. This structural constraint allows the algorithm to estimate densities reliably using empirical intersection sizes, overcoming the "mode collapse" typical in worst-case adversarial settings.
- **Core assumption:** The data generation process follows a "curriculum" where simpler/canonical examples appear roughly in order (M-boundedness).
- **Evidence anchors:**
  - [Section 7] "introduce a model where the adversary's enumeration must be 'close' to a canonical ordering... inspired by curriculum learning."
  - [Section 7.2] "Change of Density formula... we can estimate the density of a language up to a multiplicative factor by simply computing the empirical 'density'."
  - [corpus] Indirect support from corpus suggesting curriculum learning is crucial (e.g., "Overestimation in LLM Evaluation"), though specific "bounded displacement" metrics are not discussed.
- **Break condition:** Fails if the input enumeration is adversarial and unbounded (non-curriculum), reducing density guarantees to 0.

## Foundational Learning

- **Concept: Language Generation in the Limit**
  - **Why needed here:** The entire framework shifts from "identifying" a language (Gold) to "generating" from it eventually. You must understand that the algorithm is allowed to be wrong finitely many times before stabilizing.
  - **Quick check question:** Can you explain why an algorithm that identifies a language in the limit necessarily generates from it, but the reverse is not true?

- **Concept: Asymptotic Density (Liminf/Limsup)**
  - **Why needed here:** The paper distinguishes between "generation" (outputting valid strings) and "dense generation" (outputting a representative subset). Understanding $\mu_{low}$ vs $\mu_{up}$ is critical to grasping the "mode collapse" problem and the results in Section 6.
  - **Quick check question:** If a generator outputs the set $W = \{2, 4, 6, \dots\}$ and the target is $K = \mathbb{N}$, what is the element-based density? What if $K$ was just the even numbers?

- **Concept: Empirical Noise Rate**
  - **Why needed here:** This is the core observable signal used by the algorithms to filter languages. It is the fraction of observed strings that do not belong to a specific candidate language.
  - **Quick check question:** Given a stream $x_1, \dots, x_n$, how would you compute the empirical noise rate for a candidate language $L_i$? How does this differ from the true noise rate?

## Architecture Onboarding

- **Component map:** Input stream -> Membership Oracle -> Priority Engine -> Intersection Module -> Output Generator
- **Critical path:**
  1. Receive input $x_n$.
  2. Update empirical noise rates $R(L_i; x_{1:n})$ for active languages.
  3. Update Priorities $P_i^{(n)}$ (de-prioritize high noise).
  4. Sort languages by priority.
  5. Find largest prefix $J_n$ with infinite intersection.
  6. Output an unseen element from that intersection.
- **Design tradeoffs:**
  - **Finite Expansion vs. Priority:** Finite Expansion is computationally cleaner but fails on infinite noise. Priority methods handle infinite noise but require tracking dynamic state and complex thresholds.
  - **Density vs. Coverage:** Enforcing density (via stopping rule $J_n$) is strictly less robust to noise than standard generation; it requires structural assumptions (Section 7) or specific collection properties.
- **Failure signatures:**
  - **Empty Intersection:** If $J_n$ collapses or intersection is finite, the generator cannot output a valid unseen string.
  - **Mode Collapse:** Generator outputs valid strings, but they belong to a subset with density 0 in the target (e.g., generates only "even numbers" when target is "all numbers").
- **First 3 experiments:**
  1. **Sanity Check (Finite Noise):** Implement the Finite Expansion subroutine (Algo 2) on a collection of regular languages. Inject 5% static noise. Verify generation succeeds where standard [KM24] fails.
  2. **Vanishing Noise Tolerance:** Implement the Priority Intersection (Algo 4) with noise that decays as $1/n$. Plot the stabilization time $n^*$ vs. the noise decay rate.
  3. **Curriculum vs. Adversarial:** Implement the Bounded Displacement generator (Algo 9). Feed it data in canonical order (Success) vs. reverse order (Failure). Measure the achieved density $\mu_{low}$.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are tight bounds for element-based density under M-bounded displacement enumerations with vanishing noise?
- Basis in paper: [explicit] "Moreover, it would be nice to obtain tight bounds in the beyond-worst-case setting we introduced in our work." Also: "We leave as an open question the tight density guarantee given an M-bounded enumeration."
- Why unresolved: The algorithm achieves lower density ≥ (1−ε)/(2M), but the lower bound only rules out upper density > 1/M, leaving a factor-of-2 gap.
- What evidence would resolve it: Either an improved algorithm achieving density 1/M, or a strengthened lower bound showing density ≤ 1/(2M) is unavoidable.

### Open Question 2
- Question: Can generation under infinite contamination be achieved using only membership oracle access to the language collection?
- Basis in paper: [explicit] "While we have developed an algorithm that uses only membership oracle access to L in the setting of finite contamination, we have developed several algorithms that require more complicated oracles. It is an interesting open direction to understand what can be done using simpler oracles in this setting."
- Why unresolved: Algorithms for infinite contamination regimes require density oracles and bounded-displacement oracles, which are stronger than basic membership queries.
- What evidence would resolve it: Algorithms matching current guarantees using only membership oracles, or impossibility results showing stronger oracles are necessary.

### Open Question 3
- Question: How does the landscape of noisy generation change if the generator is allowed to output a vanishing fraction of hallucinations?
- Basis in paper: [explicit] "perhaps a way to circumvent some of the lower bounds we have shown, other than restricting the adversary, is to relax the requirement of the learner: how does the landscape of (noisy) generation look like we allow the generator to output a vanishing amount of hallucinations?"
- Why unresolved: Current model requires zero hallucinations after finite time; relaxing this could potentially overcome impossibility results for dense generation under infinite contamination.
- What evidence would resolve it: Characterization of which density levels become achievable under this relaxed model.

### Open Question 4
- Question: Which language collections are identifiable in the limit under M-bounded displacement enumerations?
- Basis in paper: [explicit] "We leave a full characterization of identification for M-bounded adversaries as an interesting open question."
- Why unresolved: Theorem 7.6 shows identification remains impossible for some collections even with M-bounded adversaries (M > 1), but a complete characterization is unknown.
- What evidence would resolve it: A theorem characterizing exactly which collections admit identification under M-boundedness, analogous to Angluin's characterization for unbounded adversaries.

## Limitations

- The theoretical characterization relies on idealized assumptions including exact oracle access and precise control of empirical noise rates that may not hold in practical settings
- The priority-based algorithms require tracking and updating priorities across potentially infinite language collections, raising questions about algorithmic feasibility beyond theoretical computability
- The curriculum learning assumption (bounded displacement) represents a significant departure from worst-case adversarial models and may not be satisfied by real-world data distributions

## Confidence

- **High confidence:** The characterization of generability under finite contamination (Section 4.2) and the reduction to standard generation via finite expansion. The proof techniques are standard and well-established.
- **Medium confidence:** The priority-based algorithm for vanishing noise (Section 5) and the separation between generation and dense generation under contamination. While the theoretical framework is sound, the algorithm's practical implementation complexity is not fully addressed.
- **Low confidence:** The density results under bounded displacement (Section 7) and the impossibility results for constant noise. These rely heavily on specific adversarial constructions and the practical applicability of curriculum learning assumptions is uncertain.

## Next Checks

1. Implement the finite expansion wrapper (Algorithm 2) on a benchmark collection of regular languages with synthetic contamination at 5-15% levels. Measure success rates compared to baseline [KM24] and identify failure modes.

2. Conduct a controlled experiment varying the decay rate of vanishing noise (e.g., 1/n, 1/n², exp(-n)) to empirically validate the stabilization time claims in Section 5.1 and identify practical thresholds for the priority update rules.

3. Design a curriculum vs. adversarial data generation experiment to test the bounded displacement assumption. Generate datasets with controlled displacement bounds and measure achieved density against theoretical predictions in Section 7.