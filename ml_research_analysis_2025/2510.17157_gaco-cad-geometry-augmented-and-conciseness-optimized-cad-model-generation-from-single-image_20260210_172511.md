---
ver: rpa2
title: 'GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation
  from Single Image'
arxiv_id: '2510.17157'
source_url: https://arxiv.org/abs/2510.17157
tags:
- geometric
- depth
- code
- arxiv
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GACO-CAD, a two-stage post-training framework
  for generating editable, parametric CAD models from a single image. The method addresses
  the challenge of limited spatial reasoning in current multi-modal large language
  models (MLLMs) by leveraging dense geometric priors (depth and surface normal maps)
  during supervised fine-tuning to enhance 3D spatial understanding.
---

# GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image

## Quick Facts
- **arXiv ID**: 2510.17157
- **Source URL**: https://arxiv.org/abs/2510.17157
- **Reference count**: 5
- **Primary result**: Achieves 86.49% IoU and 0.18 CD on DeepCAD with 93.1 average token length, outperforming existing methods in code validity, geometric accuracy, and modeling conciseness

## Executive Summary
GACO-CAD introduces a two-stage post-training framework that significantly advances the generation of editable, parametric CAD models from single images. By addressing the inherent spatial reasoning limitations of multi-modal large language models, the method leverages dense geometric priors (depth and surface normal maps) during supervised fine-tuning to enhance 3D spatial understanding. A reinforcement learning stage with a group length reward mechanism jointly optimizes geometric accuracy and modeling conciseness, supported by a dynamic weighting strategy for stable training. Experiments demonstrate state-of-the-art performance on benchmark datasets, with improvements across all metrics under the same MLLM backbone.

## Method Summary
GACO-CAD employs a two-stage post-training approach to enhance CAD model generation from single images. The first stage uses supervised fine-tuning with dense geometric priors (depth and surface normal maps) to improve the 3D spatial understanding of multi-modal large language models. The second stage applies reinforcement learning with a group length reward mechanism to jointly optimize geometric accuracy and modeling conciseness. A dynamic weighting strategy ensures stable training throughout. This approach addresses the limited spatial reasoning capabilities of current MLLMs by incorporating geometric information and optimizing for both accuracy and efficiency in the generated CAD code.

## Key Results
- Achieves 86.49% IoU and 0.18 CD on DeepCAD dataset
- Generates CAD models with 93.1 average token length, demonstrating improved conciseness
- Outperforms existing methods in code validity, geometric accuracy, and modeling conciseness across benchmark datasets

## Why This Works (Mechanism)
The method's effectiveness stems from addressing two critical limitations of current MLLMs in CAD generation: limited spatial reasoning and lack of optimization for modeling conciseness. By incorporating dense geometric priors (depth and surface normal maps) during supervised fine-tuning, the framework enhances the model's 3D spatial understanding, enabling more accurate reconstruction of object geometry. The reinforcement learning stage with group length reward mechanism ensures that generated CAD code is not only geometrically accurate but also concise and efficient, avoiding overly verbose or redundant modeling steps. The dynamic weighting strategy in RL training further stabilizes the optimization process, balancing geometric fidelity and code brevity effectively.

## Foundational Learning
- **Supervised Fine-Tuning with Geometric Priors**: Fine-tuning MLLMs using depth and surface normal maps to enhance 3D spatial understanding. *Why needed*: Current MLLMs struggle with spatial reasoning from 2D images alone. *Quick check*: Verify that depth and normal maps are correctly aligned and preprocessed before fine-tuning.
- **Reinforcement Learning with Group Length Reward**: Optimizing CAD generation for both geometric accuracy and modeling conciseness via RL. *Why needed*: Ensures generated models are efficient and not overly complex. *Quick check*: Monitor token length and geometric metrics during RL training to ensure balance.
- **Dynamic Weighting in RL Training**: Adjusting reward weights dynamically to stabilize training. *Why needed*: Prevents training collapse and ensures smooth convergence. *Quick check*: Track loss curves and reward trends to confirm stable optimization.
- **Parametric CAD Representation**: Generating editable, structured CAD code from image inputs. *Why needed*: Enables downstream editing and customization of generated models. *Quick check*: Validate that generated code is syntactically correct and executable in CAD software.

## Architecture Onboarding
- **Component Map**: Image Input -> Dense Geometric Prior Extraction (Depth/Normal) -> Supervised Fine-Tuning -> Reinforcement Learning (Group Length Reward + Dynamic Weighting) -> Generated Parametric CAD Code
- **Critical Path**: Geometric Prior Extraction → Supervised Fine-Tuning → RL Optimization → CAD Code Generation
- **Design Tradeoffs**: The use of dense geometric priors improves spatial reasoning but adds preprocessing overhead; RL optimization balances accuracy and conciseness but requires careful reward design and dynamic weighting for stability.
- **Failure Signatures**: Poor depth/normal map estimation leads to inaccurate geometry; RL instability can cause overfitting to reward metrics; overly aggressive conciseness optimization may reduce geometric fidelity.
- **First 3 Experiments**:
  1. Ablation study removing geometric priors to quantify their impact on spatial reasoning and IoU.
  2. RL ablation isolating group length reward vs. geometric accuracy reward contributions.
  3. Dynamic weighting ablation to demonstrate its role in training stability and final performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance claims are based on benchmark datasets that may not fully represent real-world complexity in object diversity, occlusion, and lighting conditions.
- Reliance on dense geometric priors introduces dependencies on additional data acquisition and preprocessing steps, limiting practical deployment.
- Evaluation metrics focus on geometric accuracy and code validity but do not explicitly measure semantic consistency or alignment with design intent.
- Dynamic weighting strategy lacks detailed ablation studies to demonstrate the necessity of each component.

## Confidence
- **High Confidence**: Claims about state-of-the-art performance on DeepCAD and Fusion360 datasets (86.49% IoU, 0.18 CD) are well-supported by quantitative results.
- **Medium Confidence**: The assertion that the two-stage framework consistently improves spatial reasoning is supported by ablation studies, but the mechanism could benefit from additional qualitative analysis.
- **Medium Confidence**: The claim of superior modeling conciseness is substantiated by token length metrics, but a more comprehensive comparison of geometric fidelity versus conciseness trade-offs would strengthen this claim.

## Next Checks
1. Test the framework on a broader set of real-world images with varying occlusion, lighting, and object complexity to evaluate generalization beyond benchmark datasets.
2. Conduct a user study with CAD professionals to assess whether the generated models align with design intent and functional requirements, not just geometric accuracy.
3. Perform an ablation study isolating the contributions of the group length reward mechanism and dynamic weighting strategy to quantify their individual impact on model performance and training stability.