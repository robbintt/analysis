---
ver: rpa2
title: "Extraction multi-\xE9tiquettes de relations en utilisant des couches de Transformer"
arxiv_id: '2502.15619'
source_url: https://arxiv.org/abs/2502.15619
tags:
- pour
- relations
- dans
- entra
- extraction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The BTransformer18 model combines pre-trained French language models
  (CamemBERT and FlauBERT) with Transformer encoders for multi-label relation extraction
  in French intelligence reports. The architecture uses contextual embeddings from
  pre-trained models followed by Transformer layers to capture long-term dependencies,
  ending with a classification layer using mean pooling and sigmoid activation.
---

# Extraction multi-étiquettes de relations en utilisant des couches de Transformer

## Quick Facts
- arXiv ID: 2502.15619
- Source URL: https://arxiv.org/abs/2502.15619
- Reference count: 0
- The BTransformer18 model achieves 0.654 macro F1 score on French multi-label relation extraction using CamemBERT-Large

## Executive Summary
The BTransformer18 model addresses multi-label relation extraction in French intelligence reports by combining pre-trained French language models with additional Transformer encoder layers. The architecture leverages contextual embeddings from CamemBERT or FlauBERT, processes them through transformer layers to capture long-term dependencies, and uses mean pooling for sequence aggregation before classification. Evaluated on the TextMine'25 dataset of 800 French reports, the model demonstrates that the choice of pre-trained language model significantly impacts performance, with CamemBERT-Large outperforming FlauBERT-Large by 3.4 percentage points in macro F1 score.

## Method Summary
The BTransformer18 architecture processes French text through a pre-trained language model (CamemBERT or FlauBERT), extracts contextual token embeddings, and passes them through 2 additional transformer encoder layers to capture long-range dependencies. Entity pairs are generated from annotated mentions in each document, and their corresponding token representations are aggregated using mean pooling. The pooled representations are then classified into 37 possible relation types using a dense layer with sigmoid activation, enabling multi-label prediction where entities can have multiple simultaneous relations.

## Key Results
- CamemBERT-Large achieved 0.654 macro F1 score, outperforming FlauBERT-Large (0.620)
- The architecture demonstrates the importance of pre-trained French language model quality for relation extraction
- Mean pooling was selected over [CLS] pooling based on noise robustness claims
- The model successfully handles multi-label classification with 37 relation types

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained French language model quality directly impacts multi-label relation extraction performance.
- Mechanism: CamemBERT-Large produces contextual embeddings that capture French linguistic specificities (morphology, syntax), which are then refined through transformer layers. The richer the initial representations, the more effectively downstream layers can model entity-entity relations.
- Core assumption: The pre-training corpus and architecture of CamemBERT provide representations better suited to French intelligence reports than FlauBERT's pre-training.
- Evidence anchors:
  - [abstract] "CamemBERT-Large, with a macro F1 score of 0.654, surpassing the results obtained with FlauBERT-Large"
  - [section 3.3] "amélioration de 3,4 points de pourcentage grâce à l'intégration de CamemBERT-Large"
  - [corpus] Weak direct evidence; neighbor papers confirm transformer-based extraction works but don't compare French PLMs.

### Mechanism 2
- Claim: Additional transformer encoder layers on top of frozen/fine-tuned PLM embeddings improve long-range dependency capture for relation classification.
- Mechanism: Each transformer encoder layer applies multi-head self-attention followed by position-wise feed-forward networks with residual connections. This allows tokens representing entity mentions to attend to distant contextual cues across the sequence, enriching relation-relevant signals before pooling.
- Core assumption: Two transformer layers (L=2) provide sufficient additional context modeling beyond the PLM's internal layers without overfitting on 800 documents.
- Evidence anchors:
  - [section 2.2] "Les embeddings contextuels H sont ensuite transmis à travers L couches d'encodeurs Transformer... pour capturer les dépendances à long terme entre les tokens"
  - [table 1] "Nombre de couches Transformer: 2"
  - [corpus] Neighbor paper "Transformer Enhanced Relation Classification" suggests transformer components aid relation tasks, though specific layer counts vary.

### Mechanism 3
- Claim: Mean pooling over token representations produces robust sequence-level embeddings for multi-label classification compared to [CLS]-based approaches.
- Mechanism: Averaging all token vectors (z_pool = 1/T Σ z_t) aggregates information across entity mentions and context, smoothing local noise while preserving global semantic structure. The sigmoid output then independently thresholds each of 37 relation classes.
- Core assumption: Relation signals are distributed across multiple tokens rather than concentrated in a single special token.
- Evidence anchors:
  - [section 2.3] "souvent moins sensible au bruit que d'autres méthodes d'agrégation (telles que l'utilisation d'un jeton spécial [CLS])"
  - [section 2.3] "La sortie ŷ ∈ [0,1]^C est alors interprétée comme un vecteur de probabilités indiquant la présence ou l'absence de chaque relation"
  - [corpus] No direct comparative evidence in corpus; this remains an architectural choice requiring empirical validation.

## Foundational Learning

- Concept: **Multi-label vs. multi-class classification**
  - Why needed here: Each entity pair can have multiple simultaneous relations (37 classes, not mutually exclusive). Binary cross-entropy with sigmoid outputs is required, not softmax.
  - Quick check question: Can entity A be both "located_in" and "works_for" entity B? If yes, this is multi-label.

- Concept: **Transformer encoder stack (pre-layer vs. post-layer norm, residual connections)**
  - Why needed here: The paper's transformer layers use residual connections and layer normalization to stabilize training. Understanding Add & Norm operations helps debug convergence issues.
  - Quick check question: What happens to gradient flow if you remove residual connections in a 2-layer transformer?

- Concept: **Contextual vs. static word embeddings**
  - Why needed here: CamemBERT produces different vectors for the same word in different contexts. This matters for ambiguous terms common in intelligence reports.
  - Quick check question: Would "banc" (bench/bank) receive the same embedding in "banc du parc" vs. "banc d'investissement"?

## Architecture Onboarding

- Component map:
  - Input → Tokenizer (CamemBERT-specific) → PLM Encoder (frozen or fine-tuned) → 2-layer Transformer Encoder (d=1024, heads=8) → Mean Pooling → Dense (C=37) → Sigmoid → Multi-label predictions

- Critical path:
  1. Entity pair construction from annotated mentions (generate all pairs per document)
  2. Token alignment between raw text and PLM subword tokens
  3. Label vector construction (y_ij ∈ {0,1}^37 for each pair)

- Design tradeoffs:
  - CamemBERT-Large vs. FlauBERT-Large: +3.4 F1 but larger memory footprint (1024 hidden dim)
  - L=2 transformer layers: More layers may overfit on 800 docs; fewer may undercapture dependencies
  - Mean pooling vs. [CLS]: Mean pooling claimed more robust but lacks explicit position encoding

- Failure signatures:
  - Validation loss diverges from training loss early → check dropout (p=0.1), reduce learning rate
  - F1 plateaus below 0.5 → inspect class imbalance; macro F1 equally weights rare classes
  - Sequence length errors → max_length=150; longer documents are truncated

- First 3 experiments:
  1. **Ablation on transformer layers**: Run with L=0 (direct pooling from PLM), L=1, L=2, L=4 to validate that 2 layers is optimal for this data scale.
  2. **Pooling strategy comparison**: Compare mean pooling vs. [CLS] token vs. entity-pair span pooling to test the paper's claim about noise sensitivity.
  3. **PLM comparison with statistical significance**: Run 5 seeds each for CamemBERT-Large and FlauBERT-Large; report mean ± std to confirm the 3.4-point gap is robust.

## Open Questions the Paper Calls Out
- **Question**: Would incorporating knowledge graphs or graph-based learning methods improve the modeling of complex entity relations beyond the current Transformer-based approach?
  - Basis in paper: [explicit] The conclusion explicitly states this as a future direction: "l'incorporation de graphes de connaissances ou l'utilisation de modèles d'apprentissage par graphes pourrait améliorer la modélisation des relations complexes entre entités."
  - Why unresolved: No graph-based approaches were explored in the current work; the architecture relies solely on sequential Transformer processing.
  - What evidence would resolve it: Comparative experiments integrating knowledge graph embeddings or GNN layers into the architecture, evaluated on the same TextMine'25 dataset.

## Limitations
- The evaluation is based on a single dataset (TextMine'25) of 800 French intelligence reports, representing a specific domain with potentially unique linguistic patterns and relation types.
- The claim that CamemBERT-Large outperforms FlauBERT-Large by 3.4 percentage points requires further validation through statistical significance testing and multiple random seeds.
- The paper does not report variance or confidence intervals for the F1 scores, making it difficult to assess whether the performance difference is robust.

## Confidence
- **High Confidence**: The core architecture of combining pre-trained French PLMs with transformer encoders for multi-label relation extraction is technically sound and well-grounded in established NLP practices. The use of sigmoid activation for multi-label classification is appropriate given the 37 relation types are not mutually exclusive.
- **Medium Confidence**: The reported macro F1 score of 0.654 for CamemBERT-Large is plausible given the task complexity and dataset size, though the exact performance depends on the quality of entity pair generation and class balance in the TextMine'25 dataset. The claim that mean pooling is more robust than [CLS] pooling is reasonable but unverified.
- **Low Confidence**: The assertion that the 3.4-point performance gap between CamemBERT-Large and FlauBERT-Large is meaningful without statistical significance testing or multiple seed runs. The claim that 2 transformer layers is optimal without systematic ablation across layer depths.

## Next Checks
1. **Statistical Significance Validation**: Run 5 random seeds for both CamemBERT-Large and FlauBERT-Large models, calculate mean ± standard deviation for macro F1 scores, and perform paired t-tests to determine if the 3.4-point difference is statistically significant (p < 0.05).

2. **Transformer Layer Ablation**: Systematically evaluate BTransformer18 with 0, 1, 2, 3, and 4 transformer encoder layers while keeping all other parameters constant to empirically determine the optimal depth for this dataset size and task complexity.

3. **Pooling Strategy Comparison**: Implement and compare mean pooling against [CLS] token pooling and entity-pair-specific pooling strategies (e.g., max pooling over entity span tokens) to directly test the paper's claim about noise sensitivity and determine the most effective aggregation method for this relation extraction task.