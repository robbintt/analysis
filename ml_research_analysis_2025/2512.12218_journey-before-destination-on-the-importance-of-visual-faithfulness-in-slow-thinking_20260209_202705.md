---
ver: rpa2
title: 'Journey Before Destination: On the importance of Visual Faithfulness in Slow
  Thinking'
arxiv_id: '2512.12218'
source_url: https://arxiv.org/abs/2512.12218
tags:
- reasoning
- visual
- faithfulness
- wang
- unfaithful
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces visual faithfulness as a distinct evaluation
  dimension for reasoning-augmented vision-language models, focusing on whether perception
  steps in reasoning chains are grounded in the image. It proposes a training- and
  reference-free framework using off-the-shelf VLM judges to decompose chains into
  perception and reasoning steps, validated through human meta-evaluation.
---

# Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking

## Quick Facts
- **arXiv ID:** 2512.12218
- **Source URL:** https://arxiv.org/abs/2512.12218
- **Reference count:** 40
- **Primary result:** Introduces visual faithfulness as a distinct evaluation dimension for reasoning-augmented vision-language models, focusing on whether perception steps in reasoning chains are grounded in the image

## Executive Summary
This paper introduces visual faithfulness as a distinct evaluation dimension for reasoning-augmented vision-language models, focusing on whether perception steps in reasoning chains are grounded in the image. It proposes a training- and reference-free framework using off-the-shelf VLM judges to decompose chains into perception and reasoning steps, validated through human meta-evaluation. A lightweight self-reflection procedure is introduced that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, the method reduces Unfaithful Perception Rate while preserving final-answer accuracy, demonstrating that improving intermediate visual grounding can enhance both reasoning transparency and reliability.

## Method Summary
The method introduces visual faithfulness as a distinct evaluation dimension for reasoning-augmented VLMs by decomposing chains into perception versus reasoning steps. Off-the-shelf VLM judges evaluate only perception steps for grounding in the image. The approach uses a two-stage self-reflection procedure: an auxiliary VLM detector (Claude 3.7) flags the first unfaithful perception step, then the model regenerates that step with grounding instructions (retry limit K=3). This targeted intervention preserves reasoning integrity while correcting hallucinations. The framework is validated on perception-heavy benchmarks (MMEval-Pro, MMVP, HallusionBench) across multiple reasoning-trained VLMs, showing UPR reduction from 13.4% to 4.8% while maintaining accuracy.

## Key Results
- **UPR Reduction:** Self-reflection reduces Unfaithful Perception Rate from 13.4% to 4.8% on MMEvalPro while preserving final-answer accuracy
- **Judge Correlation:** Claude 4 achieves ICC 0.69 correlation with human judgment for faithfulness evaluation, significantly outperforming white-box methods (F1 < 30)
- **Detector Performance:** Auxiliary VLM detector (Claude 3.7) achieves 97.8 F1 vs. 25.4-91.5 for white-box alternatives, remaining robust across chain lengths

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Decomposing reasoning chains into perception versus reasoning steps enables targeted faithfulness evaluation that final-answer accuracy cannot capture.
- **Mechanism:** A VLM judge first segments chains into individual steps, then classifies each as PERCEPTION (claims about visual content) or REASONING (logical inference). Only perception steps are evaluated for faithfulness, since visual grounding is only well-defined for claims that assert something about the image.
- **Core assumption:** Off-the-shelf VLM judges can reliably distinguish perception from reasoning and assess grounding without task-specific training.
- **Evidence anchors:** [abstract] "decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness"; [section 3.2] "Visual faithfulness is meaningful only for Perception steps, since these directly claim to ground information in the image"; [corpus] "On the Faithfulness of Visual Thinking" observes visual information in reasoning chains is often inaccurate yet yields correct answers.
- **Break condition:** If the judge cannot reliably distinguish perception from reasoning, faithfulness labels become noisy and downstream interventions misfire.

### Mechanism 2
- **Claim:** Localized regeneration of unfaithful perception steps preserves reasoning integrity while correcting hallucinations.
- **Mechanism:** When the detector flags step ri as unfaithful, the model regenerates only that step with explicit grounding instructions, then resumes generation from the corrected prefix. This avoids global rewriting that could corrupt faithful downstream reasoning.
- **Core assumption:** The model possesses sufficient visual knowledge to produce a faithful description when prompted correctly; errors stem from insufficient attention rather than capability gaps.
- **Evidence anchors:** [abstract] "lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training"; [section 4] "interventions should be targeted only at Perception steps that are identified as unfaithful, thereby minimizing collateral effects on downstream reasoning"; [corpus] "More Thinking, Less Seeing?" documents that longer reasoning chains increase hallucination frequency.
- **Break condition:** If the model lacks the visual knowledge to describe the image faithfully, regeneration fails regardless of retry count (paper notes ~10% of cases remain unresolved).

### Mechanism 3
- **Claim:** Strong auxiliary VLM detectors outperform white-box and training-based methods for identifying when to intervene.
- **Mechanism:** White-box methods (attention, hidden states) provide coarse signals that degrade in 7B models. Training-based detectors overfit early steps and suffer temporal context drift as chains lengthen. An auxiliary VLM judge evaluates each step against the image directly.
- **Core assumption:** The auxiliary model is sufficiently stronger than the generator to catch errors the generator cannot self-detect.
- **Evidence anchors:** [section 6] "Auxiliary-model detectors remain robust and achieve the highest F1, so we adopt this approach as our when detector"; [table 3] Claude 3.7 auxiliary model achieves 97.8 F1 vs. 25.4-91.5 for alternatives; [corpus] Related work on reasoning faithfulness in LLMs lacks multimodal grounding evaluation.
- **Break condition:** Accessibility and latency constraints may preclude closed-source auxiliary models in production settings.

## Foundational Learning

- **Concept: Perception vs. Reasoning Decomposition**
  - Why needed here: Visual faithfulness only applies to perception steps; conflating them with reasoning steps dilutes evaluation signal.
  - Quick check question: Given "The image shows a red circle. Since red often indicates danger, the answer is A," which sentence is perception?

- **Concept: Shortcut Reasoning in VLMs**
  - Why needed here: Models can map hidden representations directly to answers via priors, generating post-hoc justifications rather than causal reasoning traces.
  - Quick check question: If a model correctly answers "Is there a dog?" without looking at the image, what failure mode does this represent?

- **Concept: Temporal Context Drift in Long Chains**
  - Why needed here: Detectors trained on early reasoning steps degrade as chain length increases, limiting where interventions can be applied reliably.
  - Quick check question: Why might a hallucination detector trained on steps 1-2 fail to generalize to steps 5-6?

## Architecture Onboarding

- **Component map:** Generator VLM (θ) -> VLM Judge (J) -> Detector (D) -> Regeneration prompt (pr)
- **Critical path:** Generate → Segment → Classify Type → (if Perception) Assess Faithfulness → (if Unfaithful) Regenerate → Resume from corrected prefix
- **Design tradeoffs:** Closed-source judges (Claude 4) offer highest correlation with human judgment (ICC 0.69) but introduce API dependency; Qwen2.5-VL-72B offers open alternative (ICC 0.66). Retry limit K caps latency but may leave ~10% of unfaithful steps uncorrected. Grounding prompt (describe image first) improves judge correlation vs. vanilla prompting.
- **Failure signatures:** Detector over-triggering: Regenerates faithful steps, degrades accuracy. Detector under-triggering: Unfaithful steps propagate, UPR unchanged. Regeneration loop: Model cannot produce faithful description after K retries.
- **First 3 experiments:** 1) Reproduce ICC correlation study: Sample 100 examples from each MMEval-Pro split, compare judge labels against human annotations. 2) Ablate detector: Replace Claude 3.7 with weaker detector (Table 4) and measure UPR/accuracy delta. 3) Profile latency overhead: Measure forward pass count per example with retry limit K=3 vs. vanilla generation.

## Open Questions the Paper Calls Out

- **Open Question 1:** Can visual faithfulness improvements from self-reflection generalize to broader multimodal settings such as planning tasks or dialogue systems?
  - **Basis in paper:** [explicit] The authors state in the Limitations section: "Scope of evaluation: we study perception-heavy reasoning tasks; generalizing to broader settings such as planning or multimodal dialogue is deferred to future work."
  - **Why unresolved:** The evaluation is limited to perception-heavy benchmarks (MMEvalPro, MMVP, HallusionBench) where perception vs. reasoning step distinctions are clear; planning and dialogue may involve different types of grounding requirements.
  - **What evidence would resolve it:** Applying the self-reflection framework to embodied planning benchmarks and multimodal dialogue datasets, analyzing whether perception/reasoning decomposition and regeneration remain effective.

- **Open Question 2:** Can lightweight self-checking detectors match the performance of large auxiliary VLM judges for identifying unfaithful perception steps?
  - **Basis in paper:** [explicit] The authors note: "exploring lighter detectors or self-checking mechanisms would make the approach more widely usable" and acknowledge their method "relies on a strong external VLM. While effective, this may limit accessibility."
  - **Why unresolved:** White-box methods (SAPLMA, HaloScope, kNN) achieve F1 scores below 0.31 on the unfaithful class, while smaller models like ThinkLite-VL 7B achieve only 0.65 F1 compared to Claude 3.7's 0.98.
  - **What evidence would resolve it:** Development of lightweight detectors achieving F1 > 0.85 on unfaithful step detection, deployable on consumer hardware without API dependencies.

- **Open Question 3:** Can training-based approaches (SFT or RL with faithfulness rewards) achieve better visual faithfulness than training-free self-reflection?
  - **Basis in paper:** [inferred] The authors deliberately focus on "training-free mitigation strategies" described as "modular, lightweight, and easily applicable," but provide no comparison to what gains might be achievable through supervised fine-tuning or reinforcement learning on faithfulness objectives.
  - **Why unresolved:** The paper shows training-free self-reflection improves UPR from 13.4% to 4.8% on MMEvalPro, but whether training-based methods could achieve lower UPR or better accuracy remains unexplored.
  - **What evidence would resolve it:** Training a VLM with faithfulness-aware RL rewards and comparing UPR and accuracy against the self-reflection baseline on identical benchmarks.

- **Open Question 4:** Can KV-cache reuse or adaptive stopping reduce the latency overhead of self-reflection while preserving faithfulness gains?
  - **Basis in paper:** [explicit] The authors state: "Optimizations such as KV-cache reuse, partial decoding, or adaptive stopping could further reduce runtime" as a concrete direction for addressing inference efficiency.
  - **Why unresolved:** Self-reflection adds extra forward passes with bounded retries (K=3), but the computational overhead remains non-trivial; no latency-accuracy tradeoff analysis is provided.
  - **What evidence would resolve it:** Benchmarking latency reduction (ms per query) from KV-cache reuse and adaptive stopping, with corresponding UPR and accuracy metrics compared to the full approach.

## Limitations
- **Dependency on proprietary models:** The approach relies on strong external VLM judges (Claude 4) and detectors (Claude 3.7), limiting accessibility and increasing operational costs
- **Sentence-level segmentation assumption:** The framework assumes simple sentence-level splitting for step decomposition, which may not capture complex visual descriptions spanning multiple sentences
- **Capability vs. attention distinction:** The method cannot distinguish between attention failures and genuine capability gaps, with ~10% of cases remaining uncorrected after regeneration

## Confidence

- **High confidence:** The UPR metric formulation and its decomposition from final-answer accuracy; the auxiliary detector superiority over white-box methods (97.8 F1 vs. 25.4-91.5); the empirical observation that perception-heavy benchmarks show systematic unfaithfulness while preserving accuracy
- **Medium confidence:** The generalizability of Claude 4's faithfulness judgment to other judge models; the effectiveness of the 3-retry regeneration limit across diverse model capabilities; the claim that perception errors are primarily attention-driven rather than capability-limited
- **Low confidence:** The absolute threshold for "acceptable" UPR reduction (paper doesn't establish ground truth faithfulness standards); the scalability of closed-source judge dependencies to production environments; the robustness of sentence-level segmentation for complex visual descriptions

## Next Checks

1. **Judge generalization test:** Replace Claude 4 with Qwen2.5-VL-72B or other open VLM judges and measure ICC correlation decay to establish robustness of the faithfulness framework beyond proprietary models.

2. **Capability boundary validation:** On samples where regeneration fails after 3 retries, analyze whether the generator lacks visual knowledge of the image content versus failing to attend properly—this distinguishes attention interventions from capability gaps requiring model scaling.

3. **Segmentation granularity stress test:** Evaluate performance when using semantic sentence clustering or dependency parsing instead of simple punctuation-based splitting to determine if the sentence-level assumption holds across diverse reasoning chain structures.