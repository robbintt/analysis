---
ver: rpa2
title: 'InnateCoder: Learning Programmatic Options with Foundation Models'
arxiv_id: '2505.12508'
source_url: https://arxiv.org/abs/2505.12508
tags:
- options
- agent
- innate
- coder
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents INNATE CODER, a system that learns programmatic
  options from foundation models to improve reinforcement learning efficiency. Unlike
  previous approaches that learn options from environment interaction, INNATE CODER
  extracts them from foundation models in a zero-shot manner by breaking generated
  programs into sub-programs representing different agent behaviors.
---

# InnateCoder: Learning Programmatic Options with Foundation Models

## Quick Facts
- arXiv ID: 2505.12508
- Source URL: https://arxiv.org/abs/2505.12508
- Authors: Rubens O. Moraes; Quazi Asif Sadmine; Hendrik Baier; Levi H. S. Lelis
- Reference count: 40
- Primary result: FM-extracted programmatic options outperform online learning and random options in MicroRTS and Karel tasks

## Executive Summary
INNATE CODER learns programmatic options from foundation models to improve reinforcement learning efficiency without requiring environment interaction. Unlike previous approaches that learn options from experience, INNATE CODER extracts them from foundation models in a zero-shot manner by breaking generated programs into sub-programs representing different agent behaviors. These options induce a semantic search space where neighbor programs encode different behaviors, allowing search algorithms to explore diverse policies. Tested on MicroRTS and Karel the Robot, INNATE CODER outperformed baselines that didn't use options, learned options from experience, or sampled options randomly.

## Method Summary
INNATE CODER uses foundation models to generate candidate programs in a domain-specific language (DSL), then decomposes each program into sub-programs representing different agent behaviors. These sub-programs become options (Iω, πω, Tω) that are filtered by behavioral distinctness using action signatures collected from sampled states. The system then performs stochastic hill climbing with restarts, mixing syntax and semantic neighborhood functions with probability ε=0.4 to explore the search space. The approach requires only a small number of foundation model queries as a preprocessing step, making it computationally accessible while achieving superior sample efficiency compared to online option learning methods.

## Key Results
- INNATE CODER outperformed baselines including SHC without options, LISS-o (online option learning), and LISS-r (random options) across 8 domains
- Benefits plateau at 5000+ options but no degradation at 30,000 options
- Robust to ε values between 0.3 and 0.6 for mixing syntax and semantic neighborhoods
- Competitive with state-of-the-art algorithms and previous competition winners in MicroRTS

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Sub-programs extracted from foundation model outputs encode useful temporal abstractions even when full programs fail.
- **Mechanism**: Foundation models generate candidate programs → each program is decomposed into sub-trees rooted at non-terminal symbols → each sub-tree becomes an option ω = (Iω, πω, Tω). The decomposition preserves behavioral fragments (e.g., "harvest resources", "train units") while discarding incorrect global composition.
- **Core assumption**: Foundation models trained on code corpora encode semantically meaningful subroutines for decision-making domains, even without domain-specific training data.
- **Evidence anchors**:
  - [abstract]: "InnateCoder learns [options] from the general human knowledge encoded in foundation models in a zero-shot setting, and not from the knowledge the agent gains by interacting with the environment."
  - [Section 3.1]: "Each program p is broken into one sub-program for each sub-tree rooted at a non-terminal symbol in the AST of p."
- **Break condition**: If FM generates programs with no valid sub-trees (syntax errors, empty programs), or if sub-programs encode trivial/no-op behaviors across all states.

### Mechanism 2
- **Claim**: Options induce a semantic neighborhood structure where local search moves between behaviorally distinct programs.
- **Mechanism**: Filter options by action signatures (vectors of actions taken across sampled states) → keep only behaviorally unique options → define semantic neighbor Nm_k(n) by replacing a random AST node with an option matching the non-terminal type. This bypasses syntax-space redundancy where neighbors are syntactically different but semantically identical.
- **Core assumption**: Action signatures on 300-700 sampled states sufficiently characterize behavioral distinctness for discrete action spaces.
- **Evidence anchors**:
  - [Section 3.2]: "Moraes and Lelis [2024] showed that searching in the syntax space can be inefficient because often the neighbors n′ of a candidate n encode policies that are semantically identical to n."
  - [Section 4.3]: "The three lines with the highest winning rate are for option sets of sizes 5000, 7000, and 30000... INNATE CODER can benefit from thousands of options."
- **Break condition**: If action signatures fail to distinguish options (e.g., continuous action spaces, sparse reward structures where most actions are equivalent).

### Mechanism 3
- **Claim**: Hybrid syntax-semantic search with ε-greedy mixing preserves completeness while exploiting semantic structure.
- **Mechanism**: With probability ε = 0.4, use syntax neighborhood Nx_k (random AST mutation); with probability 1-ε, use semantic neighborhood Nm_k (option insertion). Syntax moves ensure access to all programs in the DSL; semantic moves exploit the option library for efficient behavioral exploration.
- **Core assumption**: The optimal policy lies within the reachable space of programs constructible from both random mutations and option insertions.
- **Evidence anchors**:
  - [Section 3.3]: "INNATE CODER does not search solely in the semantic space, but mixes both syntax and semantic spaces... with probability ε, SHC uses the syntax neighborhood function... and with probability 1 − ε, it uses the semantic one."
  - [Appendix G]: "There are two groups of lines at 80000 games played: ε-values of 0.3, 0.4, 0.5, and 0.6 (top lines) and 0.1, 0.2, and 0.7 (bottom lines)."
- **Break condition**: If ε is too low (semantic space too constrained) or too high (syntax-space inefficiency dominates); plateau observed at ε ∈ [0.3, 0.6].

## Foundational Learning

- **Concept: Options Framework (Sutton et al., 1999)**
  - Why needed here: INNATE CODER represents skills as temporally extended actions with (Iω, πω, Tω) tuples; understanding option initiation, policy, and termination is essential.
  - Quick check question: Can you explain why an option with initiation set Iω = S (all states) might still fail to execute in some states?

- **Concept: Context-Free Grammars and Abstract Syntax Trees**
  - Why needed here: Programs are generated via CFG production rules; decomposition into options relies on AST sub-tree extraction; neighbor generation operates on AST nodes.
  - Quick check question: Given a CFG with production ρ → if h then a, what sub-programs can be extracted from "if markersPresent then pickMarker"?

- **Concept: Stochastic Hill Climbing with Restarts**
  - Why needed here: The search algorithm iterates: sample neighbors → evaluate via rollouts → move to best → restart at local optima. Understanding restart strategy is critical for budget allocation.
  - Quick check question: Why might SHC with restarts outperform gradient-based optimization for programmatic policy search?

## Architecture Onboarding

- **Component map**: Natural language description + DSL BNF → Foundation Model → m programs → AST decomposition → filter by action signatures → option set Ω → semantic neighborhood function Nm_k → Stochastic Hill Climbing (SHC) with mixed neighborhoods

- **Critical path**: FM query quality → option behavioral diversity (action signature filtering) → semantic neighbor quality → SHC convergence speed. The filtering step (<1% computation) determines search space quality.

- **Design tradeoffs**:
  - ε = 0.4: balances exploration (syntax) vs. exploitation (semantic); robust in [0.3, 0.6]
  - Option set size: plateau at 5000+ options; no degradation at 30000 but diminishing returns
  - Action signature sample size: 300-700 states; tradeoff between behavioral characterization accuracy and computation

- **Failure signatures**:
  - FM generates syntactically invalid programs → no options extracted
  - All options have identical action signatures → semantic space collapses to single point
  - ε too low (≤0.1) or too high (≥0.7) → search stuck in constrained space or inefficient syntax exploration
  - Continuous action spaces → action signature filtering undefined (current implementation assumes discrete)

- **First 3 experiments**:
  1. **Baseline ablation**: Run INNATE CODER vs. SHC (no options) vs. LISS-o (online option learning) vs. LISS-r (random options) on a single Karel task; plot episodic return vs. episodes to quantify sample efficiency gains.
  2. **Option set size sweep**: Fix ε = 0.4, vary |Ω| ∈ {300, 1400, 5000, 7000} on LetMeOut map; identify plateau point and verify no performance degradation at large |Ω|.
  3. **ε sensitivity analysis**: Fix |Ω| = 5000, vary ε ∈ {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7}; plot winning rate at fixed game budget to confirm robustness band.

## Open Questions the Paper Calls Out

- **Open Question 1**: How can the method's behavioral similarity metric be adapted for continuous action spaces?
  - Basis in paper: [explicit] The authors state in Section 3.2, "Future research will investigate different ways of measuring behavior in continuous action spaces," noting that the current filtering relies on discrete action signatures.
  - Why unresolved: The current mechanism for filtering options into a set of behaviorally distinct programs relies on matching exact vectors of discrete actions ("action signatures"). This approach cannot be directly applied to continuous domains where action outputs are infinite and require similarity metrics rather than exact matches.
  - What evidence would resolve it: An extension of InnateCoder tested in a continuous control domain (e.g., MuJoCo) utilizing a distance-based metric (like cosine similarity or L2 distance) to cluster or filter options.

- **Open Question 2**: Can the programmatic options be effectively integrated as callable sub-routines for neural network policies?
  - Basis in paper: [explicit] The authors state in Section 5, "While we use options to define a search space, future work will explore their use as functions neural policies can call."
  - Why unresolved: The current implementation restricts the search to programmatic policies constructed from the DSL. The interaction between these symbolic options and sub-symbolic neural networks (e.g., in a hierarchical RL setup) remains unexplored.
  - What evidence would resolve it: Empirical results from a hybrid system where a high-level neural policy learns to select and execute InnateCoder's programmatic options as temporally extended actions.

- **Open Question 3**: Does the assumption of environment determinism limit the reliability of the induced semantic space?
  - Basis in paper: [inferred] The paper notes in Section 4, "Both MicroRTS and Karel are deterministic, so the value of E for policies can be computed with a single roll-out."
  - Why unresolved: The method induces a semantic space by observing option behaviors in specific states. In stochastic environments, an option's behavior may vary between rollouts, potentially causing the semantic neighborhood function to generate inconsistent or misleading neighbors during the search.
  - What evidence would resolve it: An evaluation of InnateCoder in a stochastic environment (e.g., a noisy grid world) to determine if the single-rollout evaluation and state-sampling techniques remain robust.

## Limitations

- The behavioral filtering mechanism using action signatures may not generalize to continuous or sparse-reward domains
- The 2L self-play algorithm details for MicroRTS are not fully specified in the paper
- Sample efficiency gains may be domain-specific to MicroRTS and Karel rather than broadly applicable

## Confidence

- **High**: The core mechanism of FM-extracted options outperforming online learning (tested across 30 seeds, 8 domains)
- **Medium**: The claim that thousands of options provide advantage (based on MicroRTS results showing plateau at 5000+)
- **Low**: Generalization to other domains with different DSLs or action spaces

## Next Checks

1. **Action signature robustness test**: Apply INNATE CODER to a domain with continuous actions or sparse rewards where action signatures cannot characterize behavior; measure whether semantic search collapses.
2. **2L self-play replication**: Implement the missing 2L self-play algorithm details from Moraes et al. 2023 and verify MicroRTS results are reproducible.
3. **Cross-DSL generalization**: Apply the same FM (GPT-4o) to a third, structurally different DSL (e.g., MuJoCo control primitives) and test whether option behavioral diversity still plateaus at 5000+.