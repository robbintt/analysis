---
ver: rpa2
title: Large Language Models Encode Semantics and Alignment in Linearly Separable
  Representations
arxiv_id: '2507.09709'
source_url: https://arxiv.org/abs/2507.09709
tags:
- hidden
- representations
- harmful
- language
- across
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study shows that high-level semantic knowledge and alignment
  behaviors are encoded in linearly separable, low-dimensional subspaces within transformer
  models' hidden states. Representations of scientific domains become progressively
  more separable toward deeper layers, independent of surface lexical features, and
  remain robust even after masking domain-specific keywords.
---

# Large Language Models Encode Semantics and Alignment in Linearly Separable Representations

## Quick Facts
- **arXiv ID:** 2507.09709
- **Source URL:** https://arxiv.org/abs/2507.09709
- **Reference count:** 37
- **Key outcome:** High-level semantic knowledge and alignment behaviors are encoded in linearly separable, low-dimensional subspaces within transformer models' hidden states, enabling effective safety interventions without modifying model weights.

## Executive Summary
This study reveals that transformer models organize semantic content into progressively more linearly separable subspaces as representations move through deeper layers. The research demonstrates that domain-specific knowledge (e.g., scientific disciplines) and alignment-related behaviors (e.g., safety intents) form distinct geometric clusters in hidden state space that remain robust even when domain-specific keywords are masked. These findings enable a novel safety intervention: a lightweight MLP probe trained on final-layer hidden states that outperforms external token-level filters, reducing harmful responses by more than 2× on adversarial prompts while maintaining high benign utility with 96.7% overall accuracy and 0.98+ ROC-AUC for harm detection.

## Method Summary
The study analyzes hidden states from 11 decoder-only transformer models across layers 0-48, extracting final token representations before first generation. For semantic analysis, arXiv abstracts from 6 STEM domains are processed (capped at 750 tokens, 20K samples/topic max), and linear separability is measured via hard-margin SVM classification across 15 pairwise domain combinations. For safety applications, a 6-layer MLP probe (13.9M parameters) is trained on final-layer hidden states from the WildJailbreak dataset, classifying prompts into 4 categories: direct benign, direct harmful, benign injections, and harmful injections. The probe is evaluated on an augmented test set plus HarmBench, with training using AdamW optimizer (lr=2.5×10⁻⁴, weight decay=10⁻²) for 40 epochs with early stopping.

## Key Results
- Final-layer hidden states achieve 96.7% 4-class accuracy for intent classification, outperforming external token-level filters
- SVM classification accuracy on domain pairs improves with layer depth, confirming progressive semantic refinement
- Domain separability persists under 50-60% keyword masking, demonstrating distributed semantic encoding
- The guardrail reduces harmful responses by more than 2× on adversarial prompts while maintaining 0.98+ ROC-AUC for harm detection

## Why This Works (Mechanism)

### Mechanism 1: Progressive Layer-wise Semantic Refinement
Transformer layers progressively rotate representations so semantic subspaces become more linearly separable in deeper layers. Each layer refines hidden states toward the model's output objective, organizing semantic content into near-orthogonal, separable subspaces since final-layer hidden states must project linearly onto vocabulary logits. This mechanism is supported by the observation that SVM accuracy increases with layer depth and that larger models with higher hidden dimensionality show improved separability.

### Mechanism 2: Distributed Semantic Encoding Beyond Keywords
High-level semantic information is distributed across implicit structural cues (syntax, discourse patterns) rather than concentrated in domain-specific keywords. Self-attention aggregates contextual information across tokens, spreading domain signals throughout the representation. This is evidenced by maintained separability up to 50-60% keyword masking, suggesting the semantic subspace is defined by distributed patterns rather than individual terms.

### Mechanism 3: Latent-Space Safety Geometry
Safety-aligned models encode intent (benign vs. harmful) and narrative framing (direct vs. adversarial) in linearly separable clusters. Safety training pushes harmful and benign representations into distinct regions of hidden space, with adversarial framing shifting representations toward benign clusters but residual structure remaining detectable. This enables the lightweight probe to identify malicious inputs based on geometric clustering in hidden space.

## Foundational Learning

- **Concept: Linear separability in high-dimensional spaces**
  - **Why needed here:** The entire paper relies on SVM classification accuracy as a proxy for whether semantic clusters are linearly separable.
  - **Quick check question:** Given two Gaussian clusters in 1000-dimensional space with means 0.5 units apart, will a hard-margin SVM achieve >95% accuracy?

- **Concept: Hidden states in transformer architectures**
  - **Why needed here:** The analysis operates on h ∈ R^d extracted from each layer before the first generated token.
  - **Quick check question:** In a 32-layer decoder-only model, which layer's hidden state would you extract to capture the most refined semantic representation?

- **Concept: Probing classifiers**
  - **Why needed here:** Both the SVM separability tests and the MLP guardrail are probes that predict properties from hidden states.
  - **Quick check question:** A probe achieves 99% accuracy on training data but 60% on test data. Is this a "good" probe for interpretability?

## Architecture Onboarding

- **Component map:** Input → Tokenizer → Transformer layers (L=12-48) → Final-layer hidden state h ∈ R^d → MLP probe (6-layer, 13.9M params) → 4-class classification (direct/injection × benign/harmful)

- **Critical path:** 1) Extract final-layer hidden states from base model for each prompt 2) Train MLP probe on WildJailbreak-labeled data 3) At inference, route to refusal if probe predicts "harmful" intent

- **Design tradeoffs:**
  - **Layer selection:** Earlier layers capture syntax; final layers capture semantics. Paper shows final layers are most separable, but intermediate layers may offer different signal for some architectures.
  - **Probe complexity:** 6-layer MLP (13.9M params) vs. linear probe. Paper uses MLP for capacity; linear probe may suffice if clusters are truly linearly separable.
  - **Training data diversity:** WildJailbreak covers 13 risk categories, but generalization to new attack patterns is unverified.

- **Failure signatures:**
  - High false-positive rate on benign injections (55 misclassified in paper's test set)
  - CS-EESS domain pair not separable—closely related domains may entangle
  - GPT-2 Small shows near-random separability—mechanism requires sufficient model scale

- **First 3 experiments:**
  1. Replicate SVM separability on a held-out domain pair (e.g., medicine vs. law) to validate generalization beyond arXiv categories.
  2. Train the MLP guardrail on WildJailbreak, then evaluate on a separate jailbreak dataset (e.g., HarmBench) to test out-of-distribution robustness.
  3. Ablate the probe: replace 6-layer MLP with a linear classifier and compare ROC-AUC. If performance is similar, the linear structure is confirmed.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Can chain-of-thought (CoT) reasoning be causally manipulated via a single linear vector universally?
- **Basis in paper:** The authors state (p. 9) that current steering evidence is "preliminary" and requires "adversarial perturbation or axis-orthogonality tests" to generalize.
- **Why unresolved:** While initial steering induced CoT-style responses, the authors lack rigorous causal validation that this behavior is strictly linearly encoded across contexts.
- **What evidence would resolve it:** Successful adversarial perturbation studies or axis-orthogonality tests confirming consistent, monotonic shifts in reasoning behavior.

### Open Question 2
- **Question:** How do specific architectural mechanisms (e.g., grouped-query attention) causally influence the linear separability of semantic representations?
- **Basis in paper:** The authors note (p. 9) that while they identify correlations between architecture (like Mistral/Gemma) and non-separability, "isolating and ablating these components is necessary."
- **Why unresolved:** Current evidence links geometric patterns to architecture observationally but does not isolate individual components to prove causality.
- **What evidence would resolve it:** Ablation studies that isolate specific attention heads or mechanisms to observe the direct effect on representation geometry.

### Open Question 3
- **Question:** Does the latent-space guardrail generalize effectively across diverse model scales and prompt injection datasets?
- **Basis in paper:** The authors acknowledge (p. 9-10) that findings depend solely on the WildJailbreak dataset and require extension to "more models, across varying scales."
- **Why unresolved:** The guardrail's efficacy is currently proven only on a specific Llama 3.1-8B configuration and a single dataset source.
- **What evidence would resolve it:** Evaluation of the probe's performance (ROC-AUC, refusal rates) on different model families and distinct prompt injection benchmarks.

## Limitations

- The study relies on SVM classification accuracy as a proxy for meaningful semantic structure without direct validation that geometric clusters correspond to human-understandable semantic distinctions
- Generalization claims for the safety guardrail remain unverified on truly out-of-distribution adversarial datasets beyond the WildJailbreak benchmark
- The keyword masking methodology details are incomplete, with some domain pairs (CS-EESS) showing separability failure under masking

## Confidence

- **High Confidence:** The empirical finding that final-layer hidden states enable strong linear separability (96.7% accuracy) and superior performance to token-level filters. This is directly measurable and reproducible.
- **Medium Confidence:** The mechanistic claim that progressive layer-wise refinement creates linearly separable semantic subspaces. While supported by layer-wise analysis, alternative explanations cannot be fully ruled out.
- **Low Confidence:** The claim that linear separability persists under heavy keyword masking (50-60%). The paper shows this for some domain pairs but not all, and the masking methodology details are incomplete.

## Next Checks

1. **Out-of-distribution safety evaluation:** Test the MLP guardrail on HarmBench and other independent jailbreak datasets to verify generalization beyond WildJailbreak. A drop below 0.85 ROC-AUC would indicate insufficient robustness for deployment.

2. **Ablation of keyword masking methodology:** Implement the exact keyword masking procedure (using English Word Frequency dataset) and verify separability persistence across all domain pairs, particularly CS-EESS where the paper reports failure. Document the exact bucketing algorithm and mask token usage.

3. **Linear probe comparison:** Replace the 6-layer MLP with a linear classifier on final-layer hidden states and compare performance. If accuracy remains above 90%, this would confirm the geometric structure is truly linearly separable rather than requiring nonlinear transformation.