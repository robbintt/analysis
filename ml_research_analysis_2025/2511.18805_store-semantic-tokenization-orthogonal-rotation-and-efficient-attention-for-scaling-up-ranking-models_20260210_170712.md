---
ver: rpa2
title: 'STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention
  for Scaling Up Ranking Models'
arxiv_id: '2511.18805'
source_url: https://arxiv.org/abs/2511.18805
tags:
- feature
- store
- attention
- features
- semantic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: STORE introduces a unified token-based ranking framework to address
  scalability bottlenecks in recommendation systems. It employs semantic tokenization
  to decompose high-cardinality features into stable semantic tokens, orthogonal rotation
  to enhance feature interactions, and efficient attention to reduce computational
  complexity.
---

# STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention for Scaling Up Ranking Models

## Quick Facts
- arXiv ID: 2511.18805
- Source URL: https://arxiv.org/abs/2511.18805
- Reference count: 19
- Primary result: Improves AUC by 1.195%, CTR by 2.71%, and training throughput by 1.84× over state-of-the-art methods

## Executive Summary
STORE introduces a unified token-based ranking framework that addresses two critical bottlenecks in recommendation systems: representation bottlenecks from high-cardinality sparse features and computational bottlenecks from quadratic attention complexity. The framework employs semantic tokenization to convert sparse features into stable discrete tokens, orthogonal rotation to enhance feature interactions, and efficient attention to reduce computational complexity. Experiments demonstrate consistent improvements across both public (Avazu) and industrial datasets, with STORE achieving higher accuracy while being significantly faster to train than existing methods.

## Method Summary
STORE is a token-based ranking framework that processes recommendation features through three key innovations. First, the Semantic Tokenizer uses OPMQ (Orthogonal, Parallel, Multi-expert Quantization) to convert high-cardinality sparse features like Item IDs into K semantic tokens via parallel experts and VQ-VAE style quantization. Second, the Orthogonal Rotation component processes low-cardinality static features by partitioning them into K groups, applying MLPs, and rotating them with learned orthogonal matrices to create diverse feature representations. Third, Efficient Attention employs MoBA-style block routing to reduce the quadratic attention complexity to near-linear. The unified token sequence from these components is fed into a transformer backbone for CTR prediction.

## Key Results
- Improves AUC by 1.195% and CTR by 2.71% over state-of-the-art methods on industrial dataset
- Achieves 1.84× faster training throughput while maintaining or improving accuracy
- Consistent performance gains across public (Avazu) and industrial datasets
- Efficient attention maintains accuracy with only 1/2 sparsity while being significantly faster

## Why This Works (Mechanism)
STORE addresses representation bottlenecks by converting high-cardinality sparse features into stable semantic tokens through OPMQ, which uses parallel experts and quantization to create discrete, robust representations that mitigate the "One-Epoch" phenomenon. The orthogonal rotation enhances feature interactions by creating diverse views of static features through learned orthogonal transformations. Efficient attention with MoBA block routing reduces computational complexity from O(L²) to near-linear while preserving critical interactions through selective token routing. The unified token framework enables end-to-end training with stable gradients and allows each component to specialize in handling different types of features effectively.

## Foundational Learning
- Concept: Tokenization in Recommendation Systems
  - Why needed here: The core of STORE is converting sparse, high-cardinality features (like Item IDs) into dense, discrete tokens. Understanding why this is done (to stabilize and compress the feature space) is crucial before diving into the specific OPMQ method.
  - Quick check question: Can you explain, in your own words, the problem with using raw high-cardinality IDs directly in a deep ranking model, and how converting them to semantic tokens might help?

- Concept: Attention Mechanisms and Complexity
  - Why needed here: The paper addresses the O(L²) computational bottleneck of standard self-attention. Grasping the basics of how attention scales with the number of tokens is essential to appreciate the motivation behind the "Efficient Attention" component.
  - Quick check question: If you have 100 feature tokens, how does the number of attention computations compare to having 1000 feature tokens in a standard self-attention layer?

- Concept: Regularization for Diversity
  - Why needed here: The paper uses orthogonal regularization on both the token experts and the rotation matrices to ensure they learn different things. This concept is key to preventing the model from collapsing into a single, less expressive solution.
  - Quick check question: What could go wrong if all K experts in the Semantic Tokenizer learned to extract the exact same information from an item embedding?

## Architecture Onboarding
- Component map: Input Layer -> Semantic Tokenizer (OPMQ) -> Orthogonal Rotation -> Unified Token Sequence -> STORE Backbone (Efficient Attention + FFNs) -> Output Layer

- Critical path: The data flow from Raw Features -> Semantic Tokenizer & Rotation -> Unified Token Sequence -> Efficient Attention -> Prediction is the critical path. The interplay between the tokenizer's output quality and the attention mechanism's efficiency is the core architectural innovation.

- Design tradeoffs:
  - Tokenizer Fidelity vs. Compression: Choosing the number of SIDs (K) and codebook size. Smaller values are more efficient but risk losing fine-grained details of rare items.
  - Attention Sparsity vs. Accuracy: The block size and routing in the efficient attention layer directly trade off computational speed against the model's ability to capture complex, long-range interactions. The ablation study suggests this tradeoff is favorable.
  - Number of Rotation Matrices (K): This determines the number of "views" of the static features and must be balanced against model complexity and training stability.

- Failure signatures:
  - Tokenizer Collapse: If the orthogonal regularization fails, all experts may produce similar SIDs, making the token sequence redundant and harming performance.
  - Rotation Collapse: If the diversity regularization on rotation matrices is too weak, they may converge to the same transformation, reducing the richness of static feature representations.
  - Attention Routing Failures: If the routing mechanism is poorly tuned, it might consistently ignore critical feature interactions, leading to a drop in AUC despite faster training.
  - Poor Generalization for New Items: The tokenizer relies on pre-trained embeddings. Items not in the pre-trained vocabulary or with poor initial embeddings may be represented poorly.

- First 3 experiments:
  1. Baseline Comparison: Implement STORE and compare it against a standard baseline (e.g., a simple DNN or Wide & Deep) on a public dataset like Avazu. Measure AUC, Logloss, and training throughput to confirm the paper's central claims.
  2. Tokenizer Ablation: Replace the proposed OPMQ tokenizer with a simpler alternative (e.g., a single learned embedding) while keeping the rest of the architecture constant. This isolates the contribution of the semantic tokenization mechanism.
  3. Attention Efficiency Test: Profile the training time and memory usage of the "Efficient Attention" component versus a standard, full self-attention mechanism on a sequence of tokens with varying lengths. This validates the claimed efficiency gains.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important unresolved issues emerge from the methodology and results.

## Limitations
- The paper does not fully specify critical hyperparameters including embedding dimensions, number of attention layers, and optimizer configurations
- Feature engineering details are incomplete, particularly the criteria for partitioning low-cardinality features into K groups
- Training procedure ambiguity exists around the alternating optimization schedule and learning rates for different components

## Confidence
- High Confidence: The STORE framework effectively addresses both representation and computational bottlenecks; semantic tokenization via OPMQ provides stable representations; efficient attention achieves substantial speedups with minimal accuracy loss
- Medium Confidence: The specific gains attributed to each component are difficult to isolate precisely; industrial dataset results cannot be independently verified; scalability claims are based on reported metrics
- Low Confidence: Exact computational complexity improvements across different hardware configurations; generalization performance on datasets with different characteristics; model behavior with varying numbers of feature tokens

## Next Checks
1. Component Isolation Experiment: Implement STORE on Avazu but systematically disable each major component (OPMQ tokenizer, orthogonal rotation, efficient attention) one at a time to measure individual contributions to AUC, LogLoss, and training throughput.

2. Tokenization Robustness Test: Evaluate STORE's performance when trained on a subset of items (e.g., 50% of unique items) and tested on held-out items to assess whether the semantic tokenization approach generalizes to unseen items.

3. Efficiency Scaling Analysis: Profile STORE's training time and memory usage across different sequence lengths (L=100, 500, 1000, 2000 tokens) and compare against standard self-attention baselines to validate the claimed O(L²) complexity reduction.