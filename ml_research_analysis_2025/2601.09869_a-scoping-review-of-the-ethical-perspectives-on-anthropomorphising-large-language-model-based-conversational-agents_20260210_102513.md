---
ver: rpa2
title: A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language
  Model-Based Conversational Agents
arxiv_id: '2601.09869'
source_url: https://arxiv.org/abs/2601.09869
tags:
- anthropomorphisation
- ethical
- social
- conversational
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This scoping review systematically mapped ethically oriented work
  on anthropomorphizing large language model (LLM)-based conversational agents (CAs).
  It analyzed 22 studies from 2021-2025 to understand how anthropomorphism is defined,
  the ethical challenges and opportunities it presents, and the methodological approaches
  used.
---

# A Scoping Review of the Ethical Perspectives on Anthropomorphising Large Language Model-Based Conversational Agents

## Quick Facts
- arXiv ID: 2601.09869
- Source URL: https://arxiv.org/abs/2601.09869
- Authors: Andrea Ferrario; Rasita Vinay; Matteo Casserini; Alessandro Facchini
- Reference count: 40
- Primary result: This scoping review systematically mapped ethically oriented work on anthropomorphizing large language model (LLM)-based conversational agents (CAs).

## Executive Summary
This scoping review systematically maps ethically oriented research on anthropomorphizing LLM-based conversational agents published between 2021-2025. The analysis of 22 studies reveals that while there is convergence on attribution-based definitions of anthropomorphism, significant divergence exists in how it is operationalized across different studies. The review identifies a predominant risk-focused ethical framing in the literature, with limited empirical work linking observed effects to concrete governance guidance. The authors conclude that anthropomorphism in LLM-based CAs is a complex, context-dependent phenomenon requiring further empirical research and tailored governance strategies.

## Method Summary
The review employed PRISMA-ScR guidelines to systematically identify and analyze ethically oriented studies on anthropomorphization of LLM-based conversational agents. The corpus was limited to publications from 2021-2025, yielding 22 studies that were analyzed for their definitions, ethical perspectives, methodological approaches, and governance implications. The analysis focused on how anthropomorphism is conceptualized, what ethical challenges and opportunities it presents, and how these findings could inform governance frameworks.

## Key Results
- Convergence on attribution-based definitions but divergence in operationalization of anthropomorphism across studies
- Predominantly risk-focused ethical framing with limited attention to potential benefits
- Underdeveloped empirical foundation linking observed effects to governance recommendations
- Need for context-sensitive, tailored governance strategies rather than one-size-fits-all approaches

## Why This Works (Mechanism)
The review's systematic mapping approach effectively identifies patterns and gaps in the emerging literature on LLM-based CA anthropomorphism. By applying PRISMA-ScR guidelines, the authors ensure methodological rigor in their scoping review, allowing them to synthesize diverse perspectives and identify convergence and divergence points across studies. The temporal clustering of publications (2021-2025) reflects the field's rapid development and helps contextualize the findings within the current state of research.

## Foundational Learning
- Attribution-based definitions of anthropomorphism: Why needed - provides conceptual clarity across diverse studies; Quick check - verify definitions align with psychological theories of human attribution
- Risk-focused ethical framing: Why needed - identifies primary concerns but may overlook opportunities; Quick check - assess balance of risk/benefit discussions in full corpus
- Governance-ready evaluation pipelines: Why needed - bridges empirical findings to actionable policy; Quick check - map proposed mitigations to measurable risk indicators
- Long-term interaction effects: Why needed - captures cumulative dynamics beyond single-session studies; Quick check - identify existing longitudinal datasets for adaptation
- Low-stakes vs. role-liability distinction: Why needed - establishes ethical boundaries for design choices; Quick check - create decision matrix linking context to permissible anthropomorphism

## Architecture Onboarding
Component map: Literature identification -> Conceptual analysis -> Ethical framing assessment -> Methodological gap identification -> Governance implications
Critical path: Systematic search (2021-2025) → Definition convergence analysis → Risk/benefit mapping → Governance gap identification → Recommendations synthesis
Design tradeoffs: Narrow temporal scope (comprehensive but potentially incomplete) vs. broader search (more inclusive but less focused)
Failure signatures: Overreliance on conceptual work, lack of empirical validation, risk-only framing without opportunity consideration
First experiments:
1. Replicate search with extended timeframe to verify temporal clustering
2. Cross-disciplinary comparison of anthropomorphism operationalization
3. Pilot empirical study linking specific design choices to measurable user outcomes

## Open Questions the Paper Calls Out
### Open Question 1
- Question: What are the cumulative, long-term effects of anthropomorphisation on user reliance, dependency, and deskilling?
- Basis in paper: [explicit] The authors explicitly call for "longitudinal research capable of capturing cumulative and interactional dynamics, such as reliance trajectories, dependency/attachment indicators, [and] deskilling."
- Why unresolved: The reviewed literature predominantly focuses on short-term interactions, leaving the impacts of sustained exposure to anthropomorphic cues in high-stakes settings unknown.
- What evidence would resolve it: Longitudinal empirical studies that track user behavioral and psychological changes over extended periods of interaction with LLM-based conversational agents.

### Open Question 2
- Question: How can specific interaction effects (e.g., trust shifts) be linked to testable, concrete governance actions?
- Basis in paper: [explicit] The paper identifies an under-specified "empirical→normative bridge" and calls for "governance-ready evaluation pipelines" that connect measured mechanisms to redesign or policy interventions.
- Why unresolved: Governance proposals currently outpace empirical grounding; recommendations (like "transparency") remain high-level and are not tied to specific, measurable risk triggers.
- What evidence would resolve it: Empirical validation of mitigation strategies (e.g., frictional design, inoculation) that demonstrates a causal link between specific design changes and reductions in harm.

### Open Question 3
- Question: How do cognitive, affective, and behavioral dimensions of anthropomorphism co-constitute one another during interaction?
- Basis in paper: [explicit] The review notes that the corpus "neglects how these attributional components co-constitute one another in interaction" and recommends investigating their interplay.
- Why unresolved: Existing studies often isolate single dimensions (e.g., empathy) rather than examining how epistemic, affective, and social attributions function together dynamically.
- What evidence would resolve it: Interactional studies that model the synergistic effects of multi-dimensional anthropomorphic cues on user perception and behavior.

### Open Question 4
- Question: What criteria distinguish ethically permissible "low-stakes" humanization from harmful, role- and virtue-laden deception?
- Basis in paper: [explicit] The authors call for a "principled way to treat 'low-stakes' humanization versus role- and virtue-laden relational framings," noting the literature fails to define when deception is acceptable.
- Why unresolved: The field lacks a shared normative framework to evaluate the permissibility of anthropomorphic design choices, often treating the phenomenon as uniformly risky or beneficial.
- What evidence would resolve it: Theoretical frameworks coupled with empirical user studies that define clear boundaries for acceptable anthropomorphic cues based on context and user vulnerability.

## Limitations
- Narrow temporal scope (2021-2025) may miss foundational work from earlier periods or other disciplines
- Predominance of conceptual/theoretical work over empirical research limits evidence-based conclusions
- Lack of consensus on defining and operationalizing anthropomorphism creates synthesis challenges
- Risk-focused ethical framing may underrepresent potential benefits or opportunities

## Confidence
- High: Systematic mapping approach and PRISMA-ScR methodology appropriately applied
- Medium: Identification of methodological gaps and risk-focused framing well-supported but generalizability uncertain
- Low: Conclusions about context-dependent governance require more robust empirical evidence

## Next Checks
1. Conduct broader literature search extending beyond 2021-2025 to capture foundational work and identify systematic reviews
2. Perform comparative analysis of anthropomorphism operationalization across disciplines (computer science, psychology, philosophy)
3. Design empirical studies testing relationship between anthropomorphization effects and user outcomes in high-stakes domains