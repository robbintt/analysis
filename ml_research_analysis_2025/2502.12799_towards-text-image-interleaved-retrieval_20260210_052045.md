---
ver: rpa2
title: Towards Text-Image Interleaved Retrieval
arxiv_id: '2502.12799'
source_url: https://arxiv.org/abs/2502.12799
tags:
- image
- text
- interleaved
- query
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the text-image interleaved retrieval (TIIR)
  task, where both queries and documents contain interleaved text and images. To enable
  research in this direction, the authors construct the wikiHow-TIIR dataset by generating
  high-quality interleaved queries from wikiHow tutorials.
---

# Towards Text-Image Interleaved Retrieval

## Quick Facts
- arXiv ID: 2502.12799
- Source URL: https://arxiv.org/abs/2502.12799
- Reference count: 32
- Key outcome: Introduces TIIR task and MME model achieving state-of-the-art on wikiHow-TIIR benchmark

## Executive Summary
This paper introduces the Text-Image Interleaved Retrieval (TIIR) task, where both queries and documents contain interleaved text and images. To enable research in this direction, the authors construct the wikiHow-TIIR dataset by generating high-quality interleaved queries from wikiHow tutorials. They find that adapting existing single-image or text-only models to this task is challenging, motivating the need for specialized approaches. To address this, they propose a Matryoshka Multimodal Embedder (MME) that compresses visual tokens at different granularities, improving both performance and efficiency. Experiments show that MME outperforms strong baselines while using fewer visual tokens. Extensive analyses highlight the importance of preserving interleaved context and reveal how visual token compression impacts model behavior. The work advances multimodal retrieval research by tackling interleaved data and proposing a scalable, effective solution.

## Method Summary
The authors introduce the TIIR task and construct the wikiHow-TIIR dataset from wikiHow tutorials. They propose MME, which uses Matryoshka-style visual token compression via average pooling to reduce visual token count while preserving semantic information. MME employs DeepSeek-VL-1.3B as backbone with InfoNCE contrastive learning, trained with LoRA (rank 16) at batch size 32, LR 5×10⁻⁵ for 3 epochs. The method compresses 576 visual tokens per image down to N² tokens (optimal N=3) using a 24×24 grid rearrangement and average pooling, maintaining performance while reducing visual dominance in the embedding space.

## Key Results
- MME with N=3 visual tokens achieves 0.648 MRR@10 on wikiHow-TIIR, outperforming baseline DPR-DeepSeek-VL (0.619 MRR@10)
- Visual token compression reduces computation by 64× while maintaining or improving retrieval accuracy
- Shuffling both image ordering and positional alignment causes significant performance degradation, validating importance of interleaved context preservation
- Mean Learning training strategy consistently outperforms random sampling across all N values

## Why This Works (Mechanism)

### Mechanism 1: Interleaved Context Preservation
- Claim: Preserving the sequential ordering and positional alignment between text chunks and images is causally necessary for effective TIIR.
- Mechanism: Interleaved MLLM backbones (e.g., DeepSeek-VL) maintain the original text-image sequence order in the input, allowing self-attention operations to capture cross-modal dependencies at specific positions. This preserves semantic context like "step 1 image follows step 1 text."
- Core assumption: The semantic relevance between query and document depends on the positional correspondence between text and images, not just their pooled content.
- Evidence anchors:
  - Figure 4 shows shuffling both image ordering and position causes significant MRR@10 degradation for both DPR-DeepSeek-VL and MME models.
  - Section 4.4 RQ1 states "shuffling both image ordering and position leads to significant performance degradation, indicating that both the order among images and the alignment between images and text affect the context semantics."
  - Related work on interleaved pretraining (Flamingo, OBELICS) supports that interleaved attention patterns improve multimodal understanding, though specific retrieval benefits are not directly cited.
- Break condition: If tasks do not require positional reasoning (e.g., bag-of-words multimodal matching), interleaved structure may not provide gains over pooled representations.

### Mechanism 2: Matryoshka Visual Token Compression
- Claim: Aggressively compressing visual tokens via average pooling into fewer tokens (optimal N=3 grid, yielding 9 tokens per image) improves retrieval accuracy by reducing visual dominance in the embedding space while maintaining sufficient visual semantics.
- Mechanism: Vision encoder outputs are projected to visual tokens, rearranged into a grid, and average-pooled with kernel size 24/N to produce N² tokens. This prevents the 576 tokens per image (baseline) from disproportionately influencing the [EOS] embedding through sequence position or magnitude effects.
- Core assumption: Visual dominance in the embedding space harms retrieval when visual tokens outnumber text tokens, biasing the representation toward image similarity rather than interleaved semantic matching.
- Evidence anchors:
  - "MME achieves significant improvements over the baseline by substantially fewer visual tokens."
  - Figure 5 shows an inverted U-shaped performance curve across N values, with peak MRR@10 at N=3 across all training strategies (Random, MRL, Mean).
  - Figure 6 shows N=3 yields the most balanced distribution between text-dominant and image-dominant embedding distances.
  - Matryoshka representation learning (Cai et al., 2024, cited in paper) demonstrates similar multi-granularity benefits for efficiency, though retrieval-specific visual compression is not directly validated externally.
- Break condition: If downstream tasks require fine-grained visual detail (e.g., distinguishing similar objects within images), N=3 compression may lose discriminative information.

### Mechanism 3: Mean Learning Training Strategy
- Claim: Training with cross-granularity contrastive losses (Mean Learning) outperforms single-granularity random sampling by enforcing embedding consistency across visual token resolutions.
- Mechanism: Mean Learning computes losses for all M×M combinations of query and document embedding sizes, forcing the model to produce aligned representations regardless of the visual token granularity used during inference.
- Core assumption: Embeddings at different granularities should be semantically equivalent; enforcing this improves generalization and allows flexible inference-time token selection.
- Evidence anchors:
  - Figure 5 shows Mean Learning consistently outperforms Random and MRL strategies, particularly at lower N values.
  - Section 3.2 describes Mean Learning as computing "losses between query and document embeddings of different sizes."
  - No direct external validation for cross-granularity contrastive learning in retrieval; this is a novel contribution.
- Break condition: If inference always uses a fixed N, the additional training complexity of Mean Learning may not justify marginal gains over Random sampling.

## Foundational Learning

- Concept: Dense Retrieval with Contrastive Learning (InfoNCE)
  - Why needed here: The TIIR task formulates retrieval as embedding similarity search; InfoNCE loss trains the model to maximize similarity between query-positive pairs while minimizing similarity with negatives.
  - Quick check question: Can you explain why in-batch negatives and hard negatives are both used in Equation 1?

- Concept: Vision Transformer (ViT) Tokenization
  - Why needed here: The MME compression mechanism operates on ViT patch outputs (24×24 = 576 tokens per image); understanding patch-to-token conversion is essential for implementing the pooling-based compression.
  - Quick check question: How does rearranging 576 flattened tokens into a 24×24 grid enable spatial average pooling?

- Concept: Multimodal LLM Interleaved Inputs
  - Why needed here: The baseline and MME use DeepSeek-VL, which natively supports interleaved text-image sequences; the [EOS] token aggregates cross-modal context through self-attention.
  - Quick check question: Why does the [EOS] token position matter for representing the full interleaved sequence?

## Architecture Onboarding

- Component map:
  - Input: Interleaved sequence X = [text₁, image₁, text₂, image₂, ...]
  - Vision Encoder (ViT-L-16-SigLIP-384): 384×384 image → 24×24 patches → 576 features
  - Projection Layer: 576 features → 576 visual tokens (baseline) or variable via pooling
  - Matryoshka Compression: Rearrange to 24×24 grid → Average pool with kernel 24/N → N² tokens
  - LLM Backbone (DeepSeek-VL-1.3B): Processes concatenated text + compressed visual tokens
  - Pooling: [EOS] token hidden state → L2-normalized embedding
  - Training: InfoNCE loss with temperature τ=0.05, in-batch + 1 hard negative

- Critical path: The Matryoshka compression module (average pooling after projection) is the novel intervention; incorrect pooling implementation will break the multi-granularity training and inference flexibility.

- Design tradeoffs:
  - N=1 (1 token/image): Maximum efficiency, poorest visual semantics
  - N=3 (9 tokens/image): Optimal balance per experiments, 64× compression vs baseline
  - N=24 (576 tokens/image): Baseline DPR, highest cost, visual dominance issues
  - Training strategy: Mean Learning > MRL > Random sampling for accuracy, but higher compute cost

- Failure signatures:
  - Visual dominance: If MRR@10 degrades at high N values, check embedding distance distributions (Figure 6) for imbalance
  - Sequence truncation: If input exceeds 4096 tokens, semantic content is lost; reduce N or document length
  - Shuffled context: If ablation shows no degradation from shuffling, the interleaved structure is not being modeled

- First 3 experiments:
  1. Reproduce the baseline vs MME comparison (Table 2) with N=24 (baseline) and N=3 (MME) on a subset of wikiHow-TIIR test queries to validate the setup.
  2. Run the shuffle ablation (Figure 4) to confirm the model is leveraging interleaved context; if shuffling doesn't degrade performance, the attention mechanism may not be capturing positional alignment.
  3. Sweep N ∈ {1, 2, 3, 4, 6, 8} on a validation split to find the domain-specific optimal granularity before full training; the paper notes "the best visual token size is dataset/domain dependent."

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the optimal visual token compression ratio (N=3) universal across domains, or does it vary significantly for different types of interleaved content (e.g., scientific papers, social media posts, product manuals)?
- Basis in paper: "all strategies reach the peak performance at N = 3, which implies the best visual token size is dataset/domain dependent."
- Why unresolved: The paper only evaluates on wikiHow tutorials; no experiments were conducted on other interleaved domains to test generalization of the optimal N value.
- What evidence would resolve it: Experiments applying MME to diverse interleaved corpora (e.g., web pages, slide decks, medical records) with systematic comparison of optimal N values per domain.

### Open Question 2
- Question: Can interleaved queries generated by future MLLMs with native image generation capabilities replace the text-centric pipeline, and would such queries better reflect real user search behavior?
- Basis in paper: "Given that current interleaved MLLMs are not yet sufficiently capable of handling complex text and image generation, our pipeline centers on the text modality."
- Why unresolved: The pipeline was designed around current MLLM limitations; the paper does not evaluate whether MLLM-generated interleaved queries would be more realistic or effective.
- What evidence would resolve it: A comparative study using next-generation interleaved MLLMs (e.g., Chameleon successors) to generate queries directly, with human evaluation of realism and retrieval benchmark comparisons.

### Open Question 3
- Question: How does the performance of TIIR models degrade as the number of images in queries or documents increases beyond the current benchmark's average of ~3 images?
- Basis in paper: The dataset has an average of 2.88 images per query (min 2, max 4), and documents average 4.97 images (min 2, max 64). The paper does not analyze performance scaling with image count.
- Why unresolved: No ablation or analysis was provided on how retrieval accuracy changes with increasing interleaved sequence length or image density.
- What evidence would resolve it: Controlled experiments varying image count in queries/documents (e.g., 1, 3, 5, 10, 20 images) while measuring MRR and Recall metrics.

### Open Question 4
- Question: What is the relative contribution of visual-text alignment versus image ordering to retrieval accuracy, and can models be designed to be robust to shuffled inputs?
- Basis in paper: "shuffling both image ordering and position leads to significant performance degradation, indicating that both the order among images and the alignment between images and text affect the context semantics."
- Why unresolved: The paper demonstrates both factors matter but does not disentangle their individual contributions or explore mechanisms to improve robustness to misaligned inputs.
- What evidence would resolve it: Factorial experiments isolating each perturbation type with quantitative attribution analysis, plus architectural modifications (e.g., position-aware attention) tested on shuffled data.

## Limitations
- Dependence on synthetic interleaved queries may not capture real user search behavior patterns
- MME performance may vary substantially across different domains with varying visual-textual content ratios
- Computational efficiency claims lack wall-clock time measurements and GPU memory usage comparisons

## Confidence
- **High Confidence**: TIIR task definition and dataset construction, MME with N=3 outperforms baseline, interleaved context preservation is important
- **Medium Confidence**: Visual token compression reduces visual dominance, Mean Learning outperforms random sampling, 64× visual token reduction maintains performance
- **Low Confidence**: Generalizability of N=3 optimal compression across domains, real-world applicability to user search scenarios, superiority over specialized multimodal retrieval architectures

## Next Checks
1. Implement MME on a different multimodal retrieval dataset (e.g., Flickr30k, MSCOCO with text queries, or a custom collection of tutorial-style documents) to verify that N=3 remains optimal and that the performance improvements generalize beyond wikiHow-TIIR.

2. Conduct controlled experiments measuring actual wall-clock inference time and GPU memory usage for MME (N=3) versus baseline DPR-DeepSeek-VL (N=24), including the impact of different batch sizes and sequence lengths to validate the 64× visual token reduction claim with real resource metrics.

3. Implement and evaluate Flamingo or OBELICS architectures on the TIIR task to determine whether interleaved pretraining provides advantages over the proposed MME approach, addressing the gap in comparison against models specifically designed for interleaved multimodal processing.