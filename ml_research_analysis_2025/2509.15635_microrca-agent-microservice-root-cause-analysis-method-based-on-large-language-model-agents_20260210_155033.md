---
ver: rpa2
title: 'MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language
  Model Agents'
arxiv_id: '2509.15635'
source_url: https://arxiv.org/abs/2509.15635
tags:
- data
- fault
- anomaly
- error
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents MicroRCA-Agent, a microservice root cause
  analysis system that combines large language model agents with multimodal data fusion.
  The system employs a modular architecture with five core components: data preprocessing
  for timestamp standardization, log fault extraction using Drain algorithm with multi-level
  filtering, trace anomaly detection using Isolation Forest and status code validation,
  metric fault summarization with two-stage LLM analysis, and multimodal root cause
  analysis through cross-modal LLM reasoning.'
---

# MicroRCA-Agent: Microservice Root Cause Analysis Method Based on Large Language Model Agents

## Quick Facts
- **arXiv ID**: 2509.15635
- **Source URL**: https://arxiv.org/abs/2509.15635
- **Reference count**: 2
- **Primary result**: Microservice RCA system combining LLM agents with multimodal data fusion, achieving 50.71 score in 2025 CCF AIOps Challenge

## Executive Summary
MicroRCA-Agent is a microservice root cause analysis system that employs large language model agents with multimodal data fusion to localize faulty components and explain root causes. The system processes logs, traces, and metrics through modality-specific compression and filtering, then uses cross-modal LLM reasoning to generate structured root cause explanations. A modular architecture enables targeted analysis of each data type while maintaining complementary information for final reasoning. The solution demonstrates superior performance in complex microservice fault scenarios and addresses LLM hallucination challenges through structured output validation.

## Method Summary
The system follows a pipeline approach: input JSON triggers data preprocessing with timestamp unification, followed by parallel processing of logs (Drain algorithm with multi-level filtering), traces (Isolation Forest anomaly detection with status code validation), and metrics (symmetric ratio filtering with two-stage LLM summarization). The compressed multimodal outputs feed into a cross-modal LLM for root cause analysis, generating structured JSON responses with component identification, root cause explanation, and reasoning trace. The approach balances computational efficiency through compression with analytical depth through staged LLM reasoning.

## Key Results
- Final score of 50.71 in 2025 CCF International AIOps Challenge evaluation
- Ablation studies show metric-only module achieves 42.78, demonstrating importance of infrastructure data
- Log+Metric combination outperforms full three-modal system (51.27 vs 50.71), suggesting trace module introduces noise
- Two-stage LLM summarization reduces context while maintaining fault discrimination capability

## Why This Works (Mechanism)

### Mechanism 1: Modality-Specific Compression Preserves Diagnostic Signal
Tailored compression of logs, traces, and metrics reduces LLM context overhead while maintaining fault discrimination capability. Each modality applies targeted filtering—Drain templates compress log semantics; Isolation Forest selects anomalous call chains; symmetric ratios filter metrics—before fusion, reducing token count by ~50% per the metric module alone. Core assumption: Fault-relevant signals survive compression; redundancy is modality-independent. Evidence anchors: Abstract states "compress massive logs into high-quality fault features"; section 3.4.3 confirms "approximately 50%" reduction. Break condition: If compression thresholds eliminate distinguishing features between fault types, accuracy degrades.

### Mechanism 2: Dual Detection Strategy Captures Complementary Trace Anomalies
Combining Isolation Forest (performance) with status code inspection (errors) identifies more fault patterns than either alone. Isolation Forest detects duration outliers using trained baselines per parent-child-operation combination; status code checks capture explicit failures (HTTP 500, timeouts) without training. Core assumption: Performance anomalies and explicit errors represent partially disjoint fault populations. Evidence anchors: Abstract mentions "dual anomaly detection approach that integrates Isolation Forest...with status code validation"; ablation shows Trace-only: 31.09 → Trace+Metric: 48.58. Break condition: If most faults manifest only in one dimension (duration OR status), dual strategy adds noise without benefit.

### Mechanism 3: Two-Stage LLM Phenomenon Summarization Controls Cost While Maintaining Depth
Hierarchical LLM analysis (service-pod first, then node-service-pod) balances computational cost with cross-level correlation capability. Stage 1 summarizes APM+TiDB metrics; Stage 2 integrates infra metrics with Stage 1 outputs, enabling correlation across application-container-node hierarchy. Core assumption: Service-level phenomena can be summarized independently before infrastructure context; LLMs can aggregate staged summaries without information loss. Evidence anchors: Abstract mentions "two-stage LLM analysis strategy to enable full-stack phenomenon summarization"; section 3.4.4 details Stage 1 generates ~2000-word summaries. Break condition: If cross-stage dependencies are critical (e.g., node failure only detectable via simultaneous service-pod analysis), staging may miss correlations.

## Foundational Learning

- **Drain Algorithm (Log Parsing)**
  - Why needed here: Converts unstructured logs with variable parameters into fixed templates; enables semantic deduplication before LLM processing
  - Quick check question: Given logs `[ERROR] connection timeout to 10.0.0.1:8080` and `[ERROR] connection timeout to 10.0.0.2:8080`, what template does Drain produce?

- **Isolation Forest (Anomaly Detection)**
  - Why needed here: Provides unsupervised detection of duration anomalies using normal-period baselines; handles unlabeled fault data
  - Quick check question: If contamination=0.01 and normal_avg_duration=113ms but fault_avg_duration=56375ms, would Isolation Forest flag this as anomalous (-1)?

- **Microservice Observability Triad (Logs/Traces/Metrics)**
  - Why needed here: Each modality captures different fault aspects—logs (semantic errors), traces (call chain latency), metrics (resource saturation); fusion requires understanding their temporal and causal relationships
  - Quick check question: If frontend service shows high error_ratio but backend traces are normal, which modality would you investigate first for root cause?

## Architecture Onboarding

- **Component map:** Input → Data Preprocessing (timestamp unification) → Parallel Processing: Log Module (Drain + 7-step filtering), Trace Module (IsolationForest × N detectors + status check), Metric Module (symmetric ratio filter + 2-stage LLM) → Multimodal RCA (cross-modal prompts → structured JSON)

- **Critical path:** Metric module's Stage 2 LLM summary → Multimodal RCA prompt construction → LLM inference. Bottleneck is LLM latency (~2-5s per fault sample depending on token count).

- **Design tradeoffs:**
  - Precision vs. Recall: Top-20 anomaly filtering in traces may miss low-frequency faults; ablation shows Log-only scores 23.59 vs. Log+Metric 51.27
  - Cost vs. Depth: Two-stage LLM reduces per-call tokens but risks information loss between stages
  - Generality vs. Specificity: Pre-trained Drain model (156 templates from phaseone data) may miss novel fault patterns in new environments

- **Failure signatures:**
  - LLM hallucination (Section 4.1): Model invents reasoning about trace call chains not present in input → incorrect component localization
  - Template mismatch: Novel log patterns not covered by Drain templates → filtered out before analysis
  - Baseline drift: Isolation Forest trained on post-fault "normal" data that's actually degraded → missed anomalies

- **First 3 experiments:**
  1. **Reproduce ablation on validation split:** Run single/dual/tri-modal configurations; verify Log+Metric=51.27, confirm metric module dominance
  2. **Stress test compression thresholds:** Vary symmetric ratio filter (5% → 10% → 20%); measure token reduction vs. accuracy drop
  3. **Inject LLM hallucination countermeasures:** Add retrieval-augmented generation or jury mechanism per Section 4.1 suggestions; compare output stability across 10 identical fault samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the integration of trace data be optimized to prevent the observed performance degradation when combined with log and metric data?
- Basis in paper: The ablation study in Section 4.2 shows the Log+Metric combination scores 51.27, while the full three-modal system (Log+Trace+Metric) scores lower at 50.71.
- Why unresolved: The paper notes the trace module has "unique value" in complex scenarios but acknowledges the performance drop, leaving the specific noise or conflict introduced by trace features in the LLM reasoning process unexplained.
- What evidence would resolve it: A comparative error analysis showing where trace data misleads the LLM agent, or a revised fusion mechanism that weights trace evidence conditionally based on call-chain complexity.

### Open Question 2
- Question: To what extent can Retrieval-Augmented Generation (RAG) or jury mechanisms effectively mitigate the hallucination issues encountered during multimodal root cause reasoning?
- Basis in paper: Section 4.1 states that in "bad cases," the LLM "exhibited hallucination" by inventing reasoning chains for traces not actually input, and suggests RAG or jury mechanisms as potential optimizations.
- Why unresolved: The paper identifies the hallucination problem and suggests solutions but does not implement or validate these specific mitigation strategies within the current architecture.
- What evidence would resolve it: Experimental results comparing the hallucination rate and accuracy of the current prompt-based approach against a RAG-enhanced or jury-based implementation.

### Open Question 3
- Question: Can an LLM-driven semantic filtering approach outperform the current Drain algorithm and regex-based rules for log fault extraction?
- Basis in paper: Section 4.1 notes that future work involves "enhancing the large language model's semantic judgment" to generate more effective keyword filtering templates.
- Why unresolved: The current system relies on a pre-trained Drain model and static rules (multi-level filtering) which may miss semantic nuances that an LLM could capture.
- What evidence would resolve it: Ablation studies comparing the precision and recall of the current "template + rule" log extraction versus a dynamic LLM-based semantic extraction module.

## Limitations
- Performance claims cannot be independently verified without access to exact evaluation framework and dataset splits
- Trace module integration degrades overall performance compared to Log+Metric combination (50.71 vs 51.27)
- LLM hallucination in reasoning trace represents significant failure mode requiring mitigation strategies

## Confidence

- **High Confidence**: The core architecture combining modality-specific compression with LLM summarization is sound and addresses real microservice observability challenges. The ablation results showing complementary value of logs, traces, and metrics are internally consistent.
- **Medium Confidence**: The two-stage LLM approach for cost control appears reasonable given the token reduction claims (~50%), but the risk of information loss between stages is not empirically validated beyond the final score.
- **Low Confidence**: Claims about specific performance numbers (50.71 score, 51.27 Log+Metric ablation) cannot be independently verified without access to the exact evaluation framework and dataset splits used in the CCF challenge.

## Next Checks

1. **Ablation Replication**: Re-run the single/dual/tri-modal configurations on the validation split to verify reported scores (Log+Metric=51.27, Metric-only=42.78) and confirm the metric module's dominance.

2. **Compression Sensitivity Analysis**: Systematically vary the symmetric ratio threshold (5%→10%→20%) and measure the trade-off between token reduction and accuracy degradation to validate the claimed 50% reduction preserves diagnostic signal.

3. **Hallucination Detection Implementation**: Add a jury mechanism or RAG-based verification to the multimodal RCA stage, then compare output stability across 10 identical fault samples to quantify and mitigate LLM hallucination frequency.