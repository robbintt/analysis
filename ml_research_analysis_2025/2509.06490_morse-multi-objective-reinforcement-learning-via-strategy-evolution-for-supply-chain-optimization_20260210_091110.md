---
ver: rpa2
title: 'MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply
  Chain Optimization'
arxiv_id: '2509.06490'
source_url: https://arxiv.org/abs/2509.06490
tags:
- multi-objective
- policy
- policies
- optimization
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses multi-objective optimization in supply chain
  management, where traditional methods struggle with real-time adaptability and uncertainty.
  It introduces MORSE, a novel framework that combines Reinforcement Learning (RL)
  and Multi-Objective Evolutionary Algorithms (MOEAs) to search the parameter space
  of policy neural networks, generating a Pareto front of policies.
---

# MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization

## Quick Facts
- arXiv ID: 2509.06490
- Source URL: https://arxiv.org/abs/2509.06490
- Reference count: 11
- One-line primary result: MORSE outperforms state-of-the-art methods in inventory management by generating a Pareto front of policies that enable dynamic switching under uncertainty.

## Executive Summary
MORSE introduces a novel framework that combines Reinforcement Learning and Multi-Objective Evolutionary Algorithms to optimize supply chain management under multiple objectives. The method searches the parameter space of policy neural networks using evolutionary strategies, producing a diverse Pareto front of policies rather than just solutions. This enables decision-makers to dynamically switch policies based on changing objectives and incorporates Conditional Value-at-Risk (CVaR) for risk-sensitive decision-making. Through case studies, MORSE demonstrates superior adaptability to supply chain dynamics and uncertainty compared to existing approaches.

## Method Summary
MORSE uses Multi-Objective Evolutionary Algorithms (specifically NSGA-II) to optimize the weights of a neural network policy directly, rather than using gradient descent. The framework maintains a population of policies, evaluates them through episodic rollouts in a multi-echelon supply chain environment, and applies non-dominated sorting and crowding distance to select and evolve the best-performing policies. The method incorporates CVaR in fitness evaluation for risk-aware optimization and outputs a Pareto front of policies that can be dynamically switched based on current system objectives. The policy network has separate heads for continuous order replenishment and discrete transportation mode selection.

## Key Results
- MORSE generates a diverse Pareto front of policies that outperform state-of-the-art methods like CAPQL and MONES in inventory management scenarios
- Dynamic policy switching enables adaptation to new disruptions (e.g., emission tax introduction) without retraining
- CVaR-based fitness evaluation produces more risk-aware policies that reduce extreme high values in worst-case performance
- The framework demonstrates superior adaptability to supply chain dynamics and uncertainty while improving decision-making efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using evolutionary algorithms to directly search policy parameter space produces a diverse Pareto front of policies without requiring gradient-based optimization.
- Mechanism: Population-based evolution (NSGA-II with non-dominated sorting and crowding distance) operates on neural network weights θ, evaluating each policy via episodic rollouts, then selecting/reproducing based on multi-objective fitness. This avoids local optima common in gradient-based methods and inherently maintains solution diversity.
- Core assumption: The parameter space contains regions corresponding to desirable trade-offs, and evolutionary operators (crossover, mutation) can effectively traverse this space.
- Evidence anchors:
  - [abstract] "Our method leverages MOEAs to search the parameter space of policy neural networks, generating a Pareto front of policies."
  - [section 3.3] "In this work, we leverage MOEA to directly optimize the parameter space of the policies, building a Pareto set of policies rather than a Pareto set of solutions."
  - [corpus] Limited direct corpus validation for EA-based policy search specifically in supply chains; neighbor papers focus on standard RL or heuristics-based approaches.

### Mechanism 2
- Claim: Maintaining a Pareto front of policies (not just solutions) enables real-time policy switching without retraining when objectives shift.
- Mechanism: MORSE outputs multiple Pareto-optimal policies π_i, each representing different trade-offs. When external conditions change (e.g., emission tax, geopolitical cost increase), decision-makers select a different policy from the pre-computed set rather than re-optimizing.
- Core assumption: Future objective shifts can be anticipated or fall within the coverage of the learned Pareto front.
- Evidence anchors:
  - [abstract] "...provides decision-makers with a diverse population of policies that can be dynamically switched based on the current system objectives."
  - [section 4.2.1] Demonstrates switching policies at t=200 when emission tax is introduced, protecting profit while reducing emissions.
  - [corpus] Neighbor papers on multi-objective RL in supply chains (arXiv:2507.19788) similarly emphasize multi-objective trade-offs but do not explicitly address dynamic policy switching.

### Mechanism 3
- Claim: Replacing expected return with CVaR in the fitness evaluation produces risk-aware policies that mitigate tail risks.
- Mechanism: Instead of optimizing f_i = (1/n_e) Σ R_j, MORSE computes CVaR_α as the average of the worst (1-α) percentile of episodic returns for each objective. This explicitly penalizes policies with poor worst-case performance.
- Core assumption: The empirical distribution from n_e episodes adequately represents the true return distribution, and α appropriately captures the decision-maker's risk tolerance.
- Evidence anchors:
  - [abstract] "We also introduce Conditional Value-at-Risk (CVaR) to incorporate risk-sensitive decision-making."
  - [section 5.1] "CVaR quantifies the expected loss in the tail beyond the Value at Risk (VaR)... CVaR-trained policies reduce these extreme high values."
  - [corpus] Weak corpus validation; no neighbor papers explicitly address CVaR in multi-objective RL for supply chains.

## Foundational Learning

- Concept: **Pareto Dominance and Pareto Fronts**
  - Why needed here: MORSE's core output is a Pareto front of non-dominated policies. Understanding dominance (π_i ≺ π_l iff f_i ≤ f_l and f_i ≠ f_l) is essential to interpret results.
  - Quick check question: Given two policies with objectives [profit: 100, emissions: 50] and [profit: 90, emissions: 40], which dominates the other?

- Concept: **Evolutionary Strategies for RL (ES-RL)**
  - Why needed here: MORSE uses black-box optimization over policy parameters rather than gradient-based policy gradients. Understanding population-based search, crossover, and mutation is prerequisite to modifying the algorithm.
  - Quick check question: Why might ES-RL outperform policy gradient methods in highly non-convex objective spaces?

- Concept: **Conditional Value-at-Risk (CVaR)**
  - Why needed here: The paper introduces CVaR for risk-sensitive optimization. CVaR captures expected loss in the worst α-tail, unlike VaR which only identifies a threshold.
  - Quick check question: For a minimization objective, does a lower CVaR indicate more or less tail risk?

## Architecture Onboarding

- Component map:
  Policy Network -> Environment (MO-MDP) -> Evolutionary Loop -> CVaR Module

- Critical path:
  1. Define supply chain configuration (nodes, products, demand model, objectives)
  2. Initialize population of policies (He initialization, nπ policies)
  3. Evaluate each policy over ne episodes, collecting episodic returns for each objective
  4. Apply non-dominated sorting → assign fronts → compute crowding distance
  5. Select parents → crossover/mutation → generate offspring
  6. Survival selection: top N from parents ∪ offspring
  7. Repeat for ng generations; extract final Pareto front
  8. (Optional) Re-run with CVaR-based fitness for robust policies

- Design tradeoffs:
  - Population size (nπ) vs. generations (ng): Larger populations improve diversity but increase per-generation cost; more generations improve convergence
  - Episode count (ne): More episodes reduce variance in fitness estimates but increase training time
  - CVaR confidence level (α): Higher α focuses on more extreme tail; lower α is closer to expected return
  - State history window (nt): Longer windows capture more temporal context but increase input dimensionality

- Failure signatures:
  - **Collapsed Pareto front**: All policies converge to similar trade-offs → increase population diversity or mutation rate
  - **High variance in fitness**: Noisy dominance comparisons → increase ne or use variance-reduction techniques
  - **Poor adaptation to new disruptions**: Learned policies don't cover the required trade-off → expand training scenarios or use online adaptation
  - **CVaR estimates unstable**: Inconsistent tail-risk metrics → increase ne to at least 100–500 episodes per policy

- First 3 experiments:
  1. **Baseline validation**: Run MORSE on Configuration A (3-node, seasonal demand) with nπ=50, ng=100, ne=20. Plot the Pareto front and verify non-domination visually.
  2. **Ablation on episode count**: Repeat with ne=5, 20, 50. Measure Pareto front stability (hypervolume variance across seeds) to determine sufficient sampling.
  3. **CVaR robustness test**: Train one Pareto front with mean fitness and one with CVaR (α=0.9). Compare worst-case performance (5th percentile returns) across 1000 test episodes.

## Open Questions the Paper Calls Out

- **Question**: How can human expertise be effectively integrated into the evolutionary search process to improve convergence efficiency?
  - Basis in paper: [explicit] Section 7 states, "we plan to enhance the proposed approach by integrating human expertise to improve the search efficiency of the evolutionary algorithms."
  - Why unresolved: The current framework relies solely on automated evolutionary mechanisms (NSGA-II) without incorporating domain knowledge to guide the exploration of the policy parameter space.
  - What evidence would resolve it: A comparative study demonstrating faster convergence or higher quality Pareto fronts when expert heuristics or constraints are injected into the mutation/crossover operators.

- **Question**: How does the framework scale to decentralized multi-agent supply chain environments with collaborative decision-making?
  - Basis in paper: [explicit] Section 7 outlines the plan to "expand our method to multi-agent settings, allowing for collaborative decision-making within supply chains."
  - Why unresolved: The current methodology is evaluated in single-agent contexts; the dynamics of multiple agents simultaneously evolving policies and influencing each other's environments remain unexplored.
  - What evidence would resolve it: Successful application of MORSE in a multi-echelon simulation where distinct agents control different nodes and must coordinate to achieve global Pareto optimality.

- **Question**: Would utilizing Recurrent Neural Networks (RNNs) to handle partial observability yield superior performance compared to the current fixed-window state representation?
  - Basis in paper: [inferred] Section 3.2 acknowledges that including historical data violates the Markov property and notes that RNNs capable of capturing temporal dependencies were omitted "for the sake of simplicity."
  - Why unresolved: It is unclear if the fixed window of past observations sufficiently captures the necessary temporal context or if the sequential modeling capabilities of RNNs would provide a significant advantage in stochastic environments.
  - What evidence would resolve it: Benchmarking the current agent architecture against an RNN-based agent (e.g., using LSTMs) on complex scenarios requiring long-term memory.

## Limitations
- The framework's effectiveness depends heavily on accurate representation of tail risk distributions via CVaR, which requires sufficient episode sampling
- The evolutionary approach may struggle with very high-dimensional policy networks
- The learned Pareto front may not cover unforeseen objective shifts
- Performance claims are based on synthetic environments, limiting direct real-world applicability without further validation

## Confidence
- **High confidence**: MORSE successfully generates diverse Pareto fronts of policies in multi-echelon inventory environments (validated through non-dominated sorting and comparison to CAPQL/MONES baselines)
- **Medium confidence**: The CVaR-based fitness evaluation produces more risk-aware policies (supported by worst-case performance analysis but limited corpus validation)
- **Low confidence**: Dynamic policy switching without retraining is universally effective (based on single case study scenario; generalizability to other disruption types untested)

## Next Checks
1. Test MORSE's adaptability to a new disruption type not present during training (e.g., sudden capacity constraints at a node) to assess policy switching robustness.
2. Vary the CVaR confidence level α across a range (0.7, 0.8, 0.9) and measure the trade-off between expected return and tail-risk mitigation to determine optimal settings.
3. Conduct a scalability test by increasing the supply chain complexity (more nodes, products) and measuring how population size and generations need to scale to maintain Pareto front quality.