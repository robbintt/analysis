---
ver: rpa2
title: 'Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs'
arxiv_id: '2601.18706'
source_url: https://arxiv.org/abs/2601.18706
tags:
- health-score
- rubrics
- evaluation
- rubric
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Health-SCORE, a scalable rubric-based framework
  for evaluating and improving healthcare-focused large language models (LLMs). Health-SCORE
  reduces the development cost of rubric-based evaluation by abstracting and clustering
  human-authored evaluation criteria into generalizable forms, making rubric creation
  more efficient while maintaining precision.
---

# Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs

## Quick Facts
- arXiv ID: 2601.18706
- Source URL: https://arxiv.org/abs/2601.18706
- Reference count: 15
- Primary result: Health-SCORE achieves improved healthcare LLM performance through adaptive rubric-based evaluation and reward signals

## Executive Summary
Health-SCORE is a scalable rubric-based framework that addresses the high cost of developing evaluation criteria for healthcare-focused LLMs. By clustering and abstracting human-authored rubrics from HealthBench into 29 generalizable criteria, Health-SCORE maintains evaluation precision while reducing development overhead. The framework employs an adaptive selection mechanism that identifies contextually relevant rubrics for each prompt, improving both training efficiency and inference-time performance. When used as a reward signal during reinforcement learning, Health-SCORE demonstrates improved alignment with human-authored evaluation criteria and enhanced response quality via in-context learning.

## Method Summary
Health-SCORE transforms instance-level human-authored evaluation criteria into semantically-clustered, generalizable rubrics through text embedding and manual refinement. For each prompt, an LLM-as-judge scores the relevance of all 29 Health-SCORE criteria, selecting only those exceeding a threshold for evaluation. During training, these selected rubrics provide discrete satisfaction scores (+1/0/-1) that serve as reward signals for GRPO policy optimization with KL regularization. At inference, the adaptive selection mechanism guides response generation by injecting relevant rubrics as system prompts, enabling context-aware improvements without retraining.

## Key Results
- Models trained with Health-SCORE achieve stronger performance than those using single-axis, multi-axis, or automatically generated rubrics
- Adaptive selection improves performance across in-domain and out-of-distribution evaluations (Qwen3-8B: 0.051→0.345 on HealthBench HealthData; 0.263→0.418 on CSEDB)
- Health-SCORE improves training stability and sample efficiency, with reduced KL divergence volatility during optimization

## Why This Works (Mechanism)

### Mechanism 1: Semantic Clustering for Rubric Abstraction
Human-authored rubrics from HealthBench are embedded using text-embedding-3, clustered by semantic similarity, manually refined for coherence, and consolidated into 29 Health-SCORE criteria that capture shared evaluative intent across surface-level phrasing differences.

### Mechanism 2: Adaptive Context-Aware Rubric Selection
An LLM-as-judge scores each Health-SCORE criterion on a 5-point relevance scale given the prompt context, selecting only rubrics exceeding threshold for evaluation or prompting to prevent irrelevant criteria from diluting the signal.

### Mechanism 3: Rubric-Satisfaction as Structured RL Reward
Discrete rubric satisfaction scores (+1 for satisfied positive rubrics, -1 for satisfied negative rubrics, 0 otherwise) provide interpretable, stable reward signals for policy optimization, with normalization producing sequence-level rewards for GRPO updates.

## Foundational Learning

- **RLHF/PPO-style policy optimization with KL constraints**
  - Why needed here: Health-SCORE uses GRPO for policy updates; understanding advantage estimation, reference models, and KL penalties is essential for debugging training instability
  - Quick check question: Can you explain why keeping the policy close to the reference model via KL regularization prevents reward hacking?

- **LLM-as-a-judge evaluation paradigms**
  - Why needed here: The adaptive selection mechanism and reward computation both rely on LLM judges; understanding calibration limitations and prompt sensitivity is critical
  - Quick check question: What failure modes might arise when using the same LLM family for both generation and judging?

- **Rubric-based vs. scalar reward formulations**
  - Why needed here: Health-SCORE explicitly contrasts with single-axis approaches; understanding why structured multi-criteria rewards improve interpretability and controllability informs design decisions
  - Quick check question: Why might a scalar reward fail to capture nuanced improvements in specific dimensions like safety vs. completeness?

## Architecture Onboarding

- **Component map**: Rubric Construction Pipeline (HealthBench rubrics → text-embedding-3 → clustering → manual refinement → 29 Health-SCORE criteria) -> Adaptive Selector (Prompt + Health-SCORE rubrics → LLM relevance scorer → threshold filtering → selected rubrics) -> Reward Computation (Response + selected rubrics → LLM judge → point accumulation → normalization) -> Training Loop (GRPO with 8 rollouts per prompt, async judge evaluation, KL-penalized policy updates) -> Inference Path (Adaptive selection → system prompt injection → guided generation)

- **Critical path**: 1) Validate rubric clustering quality before training (manual cluster inspection) 2) Calibrate adaptive selection threshold on held-out prompts 3) Monitor reward distribution during early training for signal quality 4) Evaluate against independent human-authored rubrics (not Health-SCORE) to avoid circular evaluation

- **Design tradeoffs**: 29 rubrics vs. fewer (more rubrics capture more dimensions but increase complexity), Discrete vs. graded satisfaction (binary simplifies training but loses nuance), Equal rubric weighting (simple but ignores severity differences)

- **Failure signatures**: Reward collapse (check judge consistency), Selection over-filtering (lower threshold), KL divergence explosion (increase KL penalty), OOD generalization gap (rubrics may be overfit)

- **First 3 experiments**: 1) Rubric ablation (train with subsets to identify driving criteria) 2) Selection threshold sweep (vary 3/5, 4/5 thresholds) 3) Judge model comparison (GPT-4.1 vs. others for reward computation)

## Open Questions the Paper Calls Out

### Open Question 1
How do alternative reward formulations incorporating graded satisfaction or learned rubric weights compare to the discrete, equal-weight approach currently used in Health-SCORE? The authors identify more expressive reward formulations as a promising direction for future work.

### Open Question 2
Does the Health-SCORE framework generalize effectively to non-medical domains or substantially different evaluation ontologies without re-clustering? The authors note their findings may not imply universal applicability across all healthcare tasks or non-medical domains.

### Open Question 3
To what degree does incorporating human-in-the-loop validation or multi-judge consensus mitigate the biases inherent in the LLM-based components of the Health-SCORE pipeline? The authors acknowledge automated evaluation may still exhibit biases and suggest this as future work.

## Limitations

- Cross-domain generalizability uncertainty: The rubric clustering approach may not transfer equally well to domains with different evaluation structures
- Judge reliability dependency: Heavy reliance on LLM-as-a-judge creates potential bias propagation without systematic evaluation of judge consistency
- Long-tail coverage gaps: Clustering may struggle with rare but important evaluation criteria that don't fit well into the 29 generalized rubrics

## Confidence

- **High confidence**: The core mechanism of adaptive rubric selection improving signal-to-noise ratio is well-supported by experimental results
- **Medium confidence**: The claim that Health-SCORE reduces development costs while maintaining precision lacks quantified cost reduction evidence
- **Medium confidence**: Training stability improvements are compelling but limited to same architecture comparisons

## Next Checks

1. Judge consistency analysis: Systematically evaluate agreement rates between different judge models (GPT-4.1, Claude-3.5-Sonnet, Qwen2.5-72B-Chat) across the full range of Health-SCORE rubrics

2. Rare criterion stress test: Construct a benchmark of prompts requiring evaluation criteria that don't map well to the 29 Health-SCORE rubrics to measure coverage limitations

3. Cross-domain transfer experiment: Apply the Health-SCORE clustering methodology to a non-healthcare domain (e.g., legal reasoning) and evaluate whether the same abstraction principles work or require modifications